---
ver: rpa2
title: Integrating Wearable Sensor Data and Self-reported Diaries for Personalized
  Affect Forecasting
arxiv_id: '2403.13841'
source_url: https://arxiv.org/abs/2403.13841
tags:
- affect
- diary
- forecasting
- status
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of accurately forecasting affective
  states, specifically positive and negative affect, using multimodal data. It proposes
  a novel deep learning model that integrates objective data from wearable sensors
  and self-reported diaries.
---

# Integrating Wearable Sensor Data and Self-reported Diaries for Personalized Affect Forecasting

## Quick Facts
- arXiv ID: 2403.13841
- Source URL: https://arxiv.org/abs/2403.13841
- Reference count: 32
- Primary result: 82.50% accuracy for positive affect and 82.76% for negative affect forecasting one week in advance

## Executive Summary
This study presents a deep learning model that integrates wearable sensor data and self-reported diary entries to forecast positive and negative affect one week in advance. The model employs a transformer encoder and DistilBERT to analyze multimodal data including physiological, environmental, sleep, metabolic, and physical activity data alongside textual diary content. The approach demonstrates that personalized models significantly outperform generic ones, achieving 82.50% accuracy for positive affect and 82.76% for negative affect. Feature analysis reveals sleep patterns and diary submission frequency as key predictors.

## Method Summary
The method integrates multimodal data through a sequential training approach: DistilBERT is first fine-tuned on affect-relevant diary text, then combined with objective sensor features through a transformer encoder. The model processes diary content, submission frequency, and objective data (sleep, physiological, activity, environmental) into a shared representation space. Leave-one-subject-out cross-validation evaluates both generalization and personalization benefits, with personalized models trained on individual historical data showing superior performance.

## Key Results
- Personalized models achieved 82.50% accuracy for positive affect and 82.76% for negative affect forecasting
- Personalized models significantly outperformed generic models, demonstrating the importance of individual behavioral patterns
- Sleep patterns, particularly deep sleep duration, and diary submission frequency were identified as the most impactful features for predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The transformer encoder + DistilBERT fusion enables multimodal affect forecasting by mapping heterogeneous inputs into a shared representation space
- Mechanism: DistilBERT extracts semantic features from diary content while the transformer encoder learns temporal dependencies across sensor modalities. Concatenating these streams before transformer encoding aligns cross-modal signals
- Core assumption: Cross-attention can reconcile disparate feature scales and temporal alignments from objective sensors and textual data
- Evidence anchors: Abstract mentions the model combines transformer encoder with DistilBERT for one-week affect forecasting; Section III.B describes the transformer encoder aggregating features through self-attention
- Break condition: If sensor and diary time-series are misaligned or missing, cross-attention cannot properly align signals, degrading accuracy

### Mechanism 2
- Claim: Sequential training of DistilBERT first and then fine-tuning the full model avoids early-stage gradient conflicts that would otherwise suppress diary feature contributions
- Mechanism: DistilBERT is fine-tuned independently to adapt embeddings to affect-related text, then the entire model is jointly trained so the transformer encoder learns to weigh both objective and diary features without overriding early textual signal extraction
- Core assumption: Fine-tuning DistilBERT on affect-relevant text before joint training ensures its embeddings are already semantically aligned with the forecasting task
- Evidence anchors: Section III.C.a observes challenges when training DistilBERT and predictive model jointly, requiring alignment to affect forecasting task; Section III.C.b describes Step 2 joint training after initial DistilBERT fine-tuning
- Break condition: If fine-tuning data is too small or not affect-specific, DistilBERT may overfit or produce misaligned embeddings, and joint training will fail to correct this

### Mechanism 3
- Claim: Personalization boosts accuracy by capturing individual behavioral patterns, such as diary submission frequency, which reflect mood states
- Mechanism: Personalized cross-validation uses half of a participant's own historical data for training, allowing the model to learn subject-specific sleep and activity-affect relationships not captured in generic models
- Core assumption: Individual affect-affecting factors are stable enough over the study period to be learned from limited personal history
- Evidence anchors: Section IV shows personalized models outperform generic ones; Section V suggests models tailored to individual behavioral and emotional patterns yield more precise predictions
- Break condition: If personal patterns change rapidly or are too sparse, the personalized model may overfit noise and underperform compared to a pooled model

## Foundational Learning

- Concept: Multimodal deep learning with transformer encoders
  - Why needed here: The model must integrate physiological sensor streams and free-text diary content into a joint representation for forecasting
  - Quick check question: Why is a transformer encoder preferred over simple concatenation + dense layer for this multimodal fusion?

- Concept: Pre-trained language model fine-tuning for domain adaptation
  - Why needed here: DistilBERT is pre-trained on general text; fine-tuning aligns its embeddings to the affect-forecasting context
  - Quick check question: What would happen if DistilBERT was used without fine-tuning on diary text?

- Concept: Leave-one-subject-out and personalized cross-validation
  - Why needed here: To evaluate both generalization across subjects and the benefit of personalization
  - Quick check question: How does personalized cross-validation differ structurally from standard k-fold cross-validation?

## Architecture Onboarding

- Component map: DistilBERT (diary content encoder) → diary submission frequency scaler → objective feature vector (sleep, physiology, activity, environment) → concatenation layer → transformer encoder (cross-modal attention) → MLP head (2-layer) → affect status output

- Critical path: DistilBERT → diary content feature → concat → transformer encoder → MLP → output

- Design tradeoffs:
  - Using DistilBERT instead of full BERT trades accuracy for speed and lower memory, acceptable due to moderate text volume
  - Fine-tuning DistilBERT first reduces joint training instability but requires separate training phase
  - Leave-one-subject-out cross-validation ensures unbiased evaluation but is computationally expensive for large N

- Failure signatures:
  - Low accuracy on both personalized and non-personalized runs → DistilBERT embeddings not capturing affect-relevant semantics
  - Personalized model much better than non-personalized → strong individual variation; pooled model loses signal
  - Degradation when removing diary features → objective features alone insufficient for one-week forecasting

- First 3 experiments:
  1. Run the model with only objective features (no diary) to establish baseline accuracy
  2. Add diary content via DistilBERT without fine-tuning to measure impact of embedding alignment
  3. Compare personalized vs non-personalized models on full multimodal setup to quantify personalization benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific diary keywords with different emotional valences (positive vs. negative) differentially impact the accuracy of affect forecasting?
- Basis in paper: The paper analyzes attention scores of keywords and finds that emotionally intense words, particularly negative ones, have higher attention scores. It also notes that the highest attention scores for emotion-tied terms have a negative valence
- Why unresolved: While the paper identifies that emotionally intense words are significant, it does not delve into the differential impact of positive versus negative keywords on forecasting accuracy
- What evidence would resolve it: Comparative analysis of forecasting accuracy when using only positive keywords versus only negative keywords in diary entries

### Open Question 2
- Question: How does the frequency of diary submissions correlate with the accuracy of affect forecasting across different demographic groups?
- Basis in paper: The paper finds that higher frequency of diary submissions is associated with an increased likelihood of positive affect status. However, it does not explore how this correlation varies across different demographic groups
- Why unresolved: The study focuses on a homogeneous sample of college students, and generalizability to other demographic groups is not addressed
- What evidence would resolve it: Conducting the study with a more diverse sample and analyzing the correlation between diary submission frequency and forecasting accuracy across different demographic groups

### Open Question 3
- Question: What is the impact of different sleep patterns (e.g., REM sleep, deep sleep) on the accuracy of affect forecasting?
- Basis in paper: The paper identifies that sleep patterns, particularly deep sleep duration, have a significant impact on affect forecasting. However, it does not differentiate between impacts of various sleep stages
- Why unresolved: The study aggregates sleep data without distinguishing between different sleep stages, leaving specific impact of each stage on forecasting accuracy unclear
- What evidence would resolve it: Detailed analysis of how each sleep stage (e.g., REM, deep sleep, light sleep) individually contributes to the accuracy of affect forecasting

## Limitations
- Findings based on specific cohort and data collection period, limiting generalizability to other populations or longer time horizons
- Sequential training approach for DistilBERT and transformer encoder may not be optimal for all datasets or affect-forecasting scenarios
- Model's performance on individual affect dimensions (e.g., anxiety vs. happiness) is not explicitly reported, which could be important for clinical applications

## Confidence

- High confidence: The model architecture and training procedure are well-defined, and reported accuracy metrics are consistent with described methodology
- Medium confidence: The personalization benefit is demonstrated, but extent of individual variation and its impact on model performance across different affect dimensions is unclear
- Low confidence: Long-term stability of model's predictions and its performance on out-of-distribution data (e.g., different demographics or cultural contexts) are unknown

## Next Checks

1. Evaluate the model's performance on a separate test set from a different population or time period to assess generalizability
2. Analyze the model's predictions on individual affect dimensions (e.g., anxiety, happiness) to identify potential biases or limitations
3. Investigate the impact of varying the fine-tuning duration and data size for DistilBERT on the overall model performance