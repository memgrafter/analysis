---
ver: rpa2
title: 'Transformer-Based Approaches for Sensor-Based Human Activity Recognition:
  Opportunities and Challenges'
arxiv_id: '2410.13605'
source_url: https://arxiv.org/abs/2410.13605
tags:
- performance
- training
- recognition
- transformer-based
- activity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transformers have been increasingly applied to sensor-based Human
  Activity Recognition (HAR), motivated by their success in natural language processing
  and computer learning. This study investigates whether transformers offer advantages
  over traditional convolutional and recurrent architectures in HAR, particularly
  given the field's data scarcity and need for efficient computation on resource-constrained
  devices.
---

# Transformer-Based Approaches for Sensor-Based Human Activity Recognition: Opportunities and Challenges

## Quick Facts
- **arXiv ID**: 2410.13605
- **Source URL**: https://arxiv.org/abs/2410.13605
- **Authors**: Clayton Souza Leite; Henry Mauranen; Aziza Zhanabatyrova; Yu Xiao
- **Reference count**: 20
- **Primary result**: Transformers consistently underperformed non-transformer models in HAR, requiring 2.5-26x more training time and showing up to 85% performance degradation under quantization

## Executive Summary
This comprehensive study investigates transformer-based architectures for sensor-based Human Activity Recognition, motivated by their success in natural language processing. Through over 500 experiments across six datasets, the research reveals that transformers consistently underperformed compared to traditional architectures like ResBiLSTM and InnoHAR. Transformers struggled particularly with complex activities, required significantly more computational resources, converged to sharper minima in the loss landscape, and showed poor robustness to both quantization and adversarial attacks.

The findings suggest that transformers may not be well-suited for HAR applications without significant modifications. While Sharpness-Aware Minimization (SAM) improved transformer performance, it wasn't sufficient to match non-transformer models and further increased training time. The study provides valuable insights into the limitations of transformers in resource-constrained, data-scarce environments typical of HAR applications.

## Method Summary
The study conducted extensive experiments comparing transformer-based models against traditional HAR architectures across six datasets (Opportunity, PAMAP2, SKODA, USC-HAD). The methodology involved training 20-epoch models using Adam optimizer, evaluating weighted F1-scores, analyzing loss landscape topography through Hessian eigenvalue analysis, testing quantization impact with INT8 conversion, evaluating adversarial robustness using FGSM attacks, and measuring computational costs. The experimental design included both baseline non-transformer models (ResBiLSTM, InnoHAR) and various transformer variants (ViT, CA-ViT, TEHAR, TTN, RetNet, TASKED, HART).

## Key Results
- Transformers required 2.5-26x more training time and showed higher inference costs compared to non-transformer models
- Transformer models experienced up to 85% performance degradation when quantized for resource-constrained deployment
- Sharpness-Aware Minimization improved transformer performance but couldn't match non-transformer models while further increasing training time
- Transformers demonstrated lower robustness to adversarial attacks compared to traditional architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers struggle in HAR due to sharp minima leading to poor generalization
- Mechanism: Transformer architectures converge to sharper loss landscape minima compared to traditional CNNs/LSTMs, which correlates with worse performance and reduced robustness to quantization and adversarial attacks
- Core assumption: Sharper minima inherently lead to worse generalization in HAR contexts
- Evidence anchors:
  - [abstract] "transformers demonstrated a tendency to converge to sharper minima (Chen et al., 2021b), which is associated with sub-optimal generalization"
  - [section] "Our findings underscore that, to achieve the success seen in CV and NLP, transformer architectures necessitate a considerably larger dataset for navigating the sharp minima effectively"
  - [corpus] Weak evidence - corpus papers don't directly address loss landscape sharpness in HAR
- Break condition: If SAM or similar techniques can sufficiently flatten minima without excessive computational cost

### Mechanism 2
- Claim: Transformers require excessive computational resources that are impractical for HAR
- Mechanism: Transformer architectures need 2.5-26x more training time and higher inference costs, making them unsuitable for resource-constrained edge devices common in HAR deployments
- Core assumption: Resource constraints are a primary bottleneck for HAR deployment
- Evidence anchors:
  - [abstract] "Training transformers demands significantly more computational resources, ranging from 2.5 to 26 times more effort"
  - [section] "Transformers require more computational efforts. To attain performance comparable to non-transformer models... there is approximately 20% more computational effort during inference"
  - [corpus] Weak evidence - corpus papers don't address computational resource requirements in detail
- Break condition: If specialized transformer variants (like RetNet) can match performance with reduced computational overhead

### Mechanism 3
- Claim: Transformers show poor robustness to quantization and adversarial attacks in HAR
- Mechanism: Transformer models experience up to 85% performance degradation after quantization and are more susceptible to adversarial perturbations compared to non-transformer models
- Core assumption: HAR systems need to maintain performance under various perturbations for real-world deployment
- Evidence anchors:
  - [abstract] "transformers experience significant performance degradation when quantized to accommodate resource-constrained devices" and "transformers demonstrate lower robustness to adversarial attacks"
  - [section] "Concerning quantization (refer to Table 4), transformer-based models experience extreme degradation, reaching up to 85% (as observed in TTN on OpportunityML)"
  - [corpus] Weak evidence - corpus papers don't specifically address quantization or adversarial robustness in HAR
- Break condition: If quantization-aware training or adversarial training can sufficiently improve robustness

## Foundational Learning

- Concept: Loss landscape topography and its relationship to generalization
  - Why needed here: Understanding why sharp minima lead to poor generalization is crucial for interpreting transformer performance in HAR
  - Quick check question: Why do flatter minima generally lead to better generalization in deep learning?

- Concept: Quantization effects on neural network performance
  - Why needed here: Transformers show extreme degradation under quantization, making this knowledge essential for understanding deployment challenges
  - Quick check question: What is the typical performance degradation range when converting 32-bit models to 8-bit quantization?

- Concept: Adversarial attack mechanisms and robustness
  - Why needed here: Transformers show higher susceptibility to adversarial attacks, which is critical for HAR applications where trust is paramount
  - Quick check question: How does the Fast Gradient Sign Method (FGSM) create adversarial examples?

## Architecture Onboarding

- Component map: Input (sliding window) -> Tokenization/Conv1D -> Transformer Encoder/Attention layers -> Classification head
- Critical path: Data preprocessing -> Model architecture selection -> Training with appropriate optimizer -> Loss landscape analysis -> Robustness evaluation
- Design tradeoffs: Performance vs. computational cost, generalization vs. sharpness, robustness vs. model complexity
- Failure signatures: Sharp loss landscapes, extreme quantization degradation (>5%), high sensitivity to adversarial perturbations
- First 3 experiments:
  1. Replicate loss landscape visualization comparing transformers vs. non-transformers on PAMAP2 dataset
  2. Measure training/inference time and quantization degradation for ViT and ResBiLSTM on Opportunity dataset
  3. Evaluate adversarial attack robustness using FGSM with varying epsilon values on USC-HAD dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications could make transformers more effective for HAR applications?
- Basis in paper: [explicit] The paper concludes that transformers require significant modifications to address their current limitations in HAR
- Why unresolved: The study identifies the problems but does not propose specific solutions or test potential architectural modifications
- What evidence would resolve it: Empirical testing of various transformer architectures with different modifications (attention mechanisms, hybrid architectures, etc.) showing improved performance on HAR datasets

### Open Question 2
- Question: How does the performance gap between transformers and non-transformer models change with increasing dataset size?
- Basis in paper: [explicit] The authors note that transformers typically require abundant data to outperform other architectures, but HAR datasets are inherently limited
- Why unresolved: The study uses fixed, relatively small datasets and does not explore performance scaling with dataset size
- What evidence would resolve it: Systematic experiments varying dataset sizes to identify the threshold where transformers become competitive

### Open Question 3
- Question: Can quantization techniques be specifically designed for transformer architectures to reduce performance degradation?
- Basis in paper: [explicit] The paper finds transformers suffer extreme performance degradation during quantization, with losses up to 85%
- Why unresolved: While the problem is identified, potential solutions or alternative quantization methods for transformers are not explored
- What evidence would resolve it: Development and testing of transformer-specific quantization approaches showing reduced performance degradation while maintaining computational benefits

### Open Question 4
- Question: What is the relationship between transformer sharpness and adversarial robustness in HAR applications?
- Basis in paper: [explicit] The study finds transformers converge to sharper minima and are more susceptible to adversarial attacks
- Why unresolved: While correlation is observed, the causal relationship and potential mitigation strategies are not investigated
- What evidence would resolve it: Experiments systematically varying sharpness (through different training methods) while measuring adversarial robustness to establish causality and identify effective defenses

## Limitations
- The study's findings are based on six datasets, which may not fully capture the variability in real-world HAR deployments
- The specific transformer variants tested may not represent the full spectrum of possible transformer architectures optimized for sensor data
- The computational cost comparisons assume standard implementations without specialized hardware acceleration

## Confidence

**High Confidence**: The computational resource requirements of transformers (2.5-26x training overhead) and the extreme quantization degradation (up to 85%) are well-supported by the experimental data across multiple datasets. These findings are consistent with the fundamental architectural differences between transformers and traditional HAR models.

**Medium Confidence**: The relationship between sharp minima and poor generalization is theoretically sound but requires further validation across different HAR scenarios. The specific mechanisms by which transformers struggle with complex activities need more granular investigation to determine if these are inherent limitations or architecture-specific issues.

**Low Confidence**: The assertion that transformers are inherently unsuitable for HAR without significant modifications may be premature. The study didn't explore hybrid approaches that combine transformer strengths with traditional HAR techniques, which could potentially address the identified limitations.

## Next Checks

1. **Loss Landscape Verification**: Replicate the loss landscape visualization methodology (α,β ∈ [-3,3], step 0.2) comparing transformers vs. non-transformers on PAMAP2 dataset to confirm the sharp minima observation across different activity complexity levels.

2. **Quantization-Aware Training Experiment**: Implement quantization-aware training during transformer model optimization and measure whether this approach reduces the extreme degradation observed in post-training quantization, potentially validating or challenging the claim about inherent quantization vulnerability.

3. **Hybrid Architecture Evaluation**: Design and test hybrid models that combine transformer attention mechanisms with traditional CNN/LSTM feature extractors to determine if the performance issues stem from pure transformer architectures or if modified architectures could overcome the identified limitations.