---
ver: rpa2
title: 'JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark
  of Generated Images'
arxiv_id: '2409.12953'
source_url: https://arxiv.org/abs/2409.12953
tags:
- image
- images
- journeybench
- dataset
- unusual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JourneyBench is a comprehensive human-annotated vision-language
  benchmark of generated images designed to rigorously test models' multimodal reasoning
  abilities. It addresses limitations in existing benchmarks that rely on images in
  usual contexts, allowing models to succeed through language bias rather than true
  visual understanding.
---

# JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images

## Quick Facts
- arXiv ID: 2409.12953
- Source URL: https://arxiv.org/abs/2409.12953
- Reference count: 40
- Primary result: Even state-of-the-art models like GPT-4 achieve relatively low performance on JourneyBench tasks (57.89% accuracy on multi-image VQA, 62.18% on MCOT), demonstrating the benchmark's difficulty and models' limited visual reasoning capabilities.

## Executive Summary
JourneyBench is a comprehensive human-annotated vision-language benchmark using generated images to rigorously test multimodal models' reasoning abilities. Unlike existing benchmarks that rely on everyday images allowing models to succeed through language bias, JourneyBench features five challenging tasks (complementary multimodal chain-of-thought, multi-image VQA, imaginary image captioning, VQA with hallucination triggers, and fine-grained retrieval) in unusual and fictional scenarios requiring fine-grained multimodal reasoning. The benchmark employs an adversarial human-machine-in-the-loop framework to create high-quality data with sample-specific distractors. Even top-performing models show significantly lower performance compared to traditional benchmarks, indicating that current models' visual reasoning capabilities are not as strong as previously thought.

## Method Summary
JourneyBench evaluates multimodal large language models (MLLMs) on five vision-language tasks using ~13.5K image-text pairs (12.4K unique images, 13.7K unique texts) of generated images. The benchmark uses zero-shot evaluation with state-of-the-art models (GPT-4o, GPT-4V, LLaVA, BLIP2, InternVL, etc.) and provides inference prompts. Performance is measured using task-specific metrics: retrieval tasks use Recall@k, captioning uses BLEU/ROUGE/CIDEr/Meteor scores, and MCOT/MMCOT/HaloQuest tasks use accuracy after answer extraction and verification with Llama-3-8B and solution verification with Llama-3-70B. The dataset is built through an adversarial human-machine-in-the-loop framework that iteratively refines challenging questions and distractors.

## Key Results
- GPT-4 achieves only 57.89% accuracy on multi-image VQA and 62.18% on MCOT tasks
- Retrieval tasks show R@1 scores in the 50-60% range even for best models, significantly lower than existing benchmarks
- Models consistently hallucinate on VQA tasks with false premise questions, with accuracy dropping to 22.67% for the best model
- Fine-grained retrieval with sample-specific distractors proves particularly challenging, with most models performing near random chance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: JourneyBench addresses the bias in existing VLU benchmarks that rely on everyday images by using generated images in unusual and fictional contexts.
- **Mechanism**: Generated images bypass copyright constraints and allow for diverse, controllable visual content, enabling the creation of challenging scenarios that require fine-grained multimodal reasoning rather than relying on background language biases.
- **Core assumption**: Models trained on everyday images develop strong language priors that allow them to perform well without true visual understanding.
- **Evidence anchors**:
  - [abstract] "Unlike existing benchmarks, JourneyBench explicitly requires fine-grained multimodal reasoning in unusual imaginary scenarios where language bias and holistic image gist are insufficient."
  - [section] "We aim to create a VLU benchmark containing challenging and diverse imaginary images, including unusual, abstract, and complex ones by leveraging the advantages of prompt-based generated images."
- **Break condition**: If models can still perform well on JourneyBench tasks using only language priors without visual understanding, this mechanism would be invalidated.

### Mechanism 2
- **Claim**: JourneyBench's five tasks (MCOT, multi-image VQA, imaginary image captioning, VQA with hallucination triggers, and fine-grained retrieval) collectively test different aspects of multimodal reasoning that existing benchmarks miss.
- **Mechanism**: Each task targets specific weaknesses in current models - for example, MCOT requires co-referencing across modalities, while fine-grained retrieval tests the ability to differentiate similar images at a detailed level.
- **Core assumption**: Different multimodal reasoning capabilities require different evaluation approaches to be properly tested.
- **Evidence anchors**:
  - [abstract] "JourneyBench features five challenging tasks: complementary multimodal chain-of-thought, multi-image VQA, imaginary image captioning, VQA with hallucination triggers, and fine-grained retrieval with sample-specific distractors."
  - [section] "This enables models to excel by leveraging previously acquired common-world knowledge without necessarily understanding the actual content of the images."
- **Break condition**: If models can perform well on all five tasks without demonstrating genuine multimodal reasoning, this mechanism would be invalidated.

### Mechanism 3
- **Claim**: The human-machine-in-the-loop (HMIL) framework creates higher quality data than either humans or machines could create alone.
- **Mechanism**: The iterative process between LLMs and MLLMs reveals "blind spots" that humans can then identify and collect as challenging distractors or questions.
- **Core assumption**: The errors that propagate through iterative LLM-MLLM dialogue reveal aspects of the task that are genuinely difficult for models to handle.
- **Evidence anchors**:
  - [section] "With each iteration, the MLLM-LLM's errors propagate, making the hallucinated predictions more difficult to overturn and thus revealing 'blind spots' to humans."
  - [section] "These 'blind spots' are not merely imagined by the generators but empirically demonstrated on the task."
- **Break condition**: If the HMIL process doesn't consistently produce more challenging data than human-only annotation, this mechanism would be invalidated.

## Foundational Learning

- **Concept**: Multimodal chain-of-thought reasoning
  - Why needed here: Understanding how models integrate information across visual and textual modalities to solve complex problems is central to evaluating JourneyBench's effectiveness
  - Quick check question: Can you explain the difference between complementary multimodal reasoning and redundant visual information in existing benchmarks?

- **Concept**: Fine-grained visual discrimination
  - Why needed here: JourneyBench's retrieval tasks require models to differentiate between highly similar images, which is critical for understanding the benchmark's difficulty
  - Quick check question: What distinguishes fine-grained retrieval from traditional image retrieval tasks?

- **Concept**: Hallucination in multimodal models
  - Why needed here: JourneyBench specifically tests models' hallucination tendencies through carefully designed questions that trigger false responses
  - Quick check question: How do false premise questions differ from visually challenging questions in testing hallucination?

## Architecture Onboarding

- **Component map**: Image generation/retrieval → Filtering for unusual/fictional content → Task-specific annotation (via HMIL) → Model evaluation → Performance analysis
- **Critical path**: The core workflow is: image generation/retrieval → filtering for unusual/fictional content → task-specific annotation (via HMIL) → model evaluation → performance analysis
- **Design tradeoffs**: Generated images offer diversity and control but may introduce artifacts; human annotation ensures quality but is expensive; HMIL balances quality and scalability but adds complexity
- **Failure signatures**: Models performing well on traditional benchmarks but poorly on JourneyBench suggests language bias; models failing on fine-grained tasks suggest limitations in visual discrimination; models consistently hallucinating suggest issues with grounding
- **First 3 experiments**:
  1. Run baseline models on traditional benchmarks vs. JourneyBench to quantify the performance gap
  2. Test model performance across different categories within each JourneyBench task to identify specific weaknesses
  3. Compare human performance on JourneyBench tasks to establish upper bounds and validate task difficulty

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum size of a vision-language model (VLM) required to reliably perform well on JourneyBench's fine-grained cross-modal retrieval tasks with sample-specific distractors?
- Basis in paper: [explicit] The paper shows that even the best models struggle significantly on retrieval tasks with distractors, achieving R@1 scores in the 50-60% range, compared to much higher scores on existing benchmarks.
- Why unresolved: The paper evaluates a range of models but does not systematically test the performance of models at different scales to determine the minimum size required for acceptable performance on JourneyBench's retrieval tasks.
- What evidence would resolve it: A systematic ablation study testing models of progressively smaller sizes (e.g., 100M, 500M, 1B parameters) on the retrieval tasks with and without distractors to identify the threshold where performance drops significantly.

### Open Question 2
- Question: How does the performance of VLMs on JourneyBench tasks change when trained specifically on generated images versus real images?
- Basis in paper: [inferred] The paper highlights that JourneyBench uses generated images to create challenging scenarios, and current models are not well-adapted to these images. It suggests that models trained on real images may struggle with the diversity and unusualness of generated content.
- Why unresolved: The paper evaluates models trained on general datasets but does not investigate the impact of specialized training on generated images for JourneyBench performance.
- What evidence would resolve it: Training multiple VLMs from scratch on datasets consisting solely of generated images (like JourneyBench) and comparing their performance on JourneyBench tasks against models trained on real image datasets.

### Open Question 3
- Question: What is the long-term effectiveness of the adversarial human-machine-in-the-loop (HMIL) framework in generating challenging data for JourneyBench?
- Basis in paper: [explicit] The paper describes using an adversarial HMIL framework to create high-quality data with sample-specific distractors for the retrieval task, but does not evaluate its effectiveness over time or compare it to other data generation methods.
- Why unresolved: The paper implements the framework but does not assess its sustainability or compare its output quality to alternative approaches for generating challenging data.
- What evidence would resolve it: A longitudinal study comparing the quality and diversity of data generated by the HMIL framework over multiple iterations, and a comparison with data generated using other methods (e.g., pure human annotation, pure machine generation) on the same tasks.

## Limitations

- The benchmark's focus on generated images may not fully represent real-world applications where models encounter everyday photographs
- Substantial computational resources are required for evaluating larger models (e.g., LLaVA-NeXT QWEN-110B needs 4 A100 GPUs for 2 days)
- Exact inter-annotator agreement rates and quality control procedures for the HMIL framework are not fully specified

## Confidence

- **High Confidence**: The observation that models perform significantly worse on JourneyBench compared to traditional benchmarks is well-supported by the data across all five tasks and multiple model families.
- **Medium Confidence**: The claim that JourneyBench reveals fundamental limitations in models' multimodal reasoning capabilities is supported but could be influenced by the specific nature of generated images versus real photographs.
- **Low Confidence**: The assertion that generated images are superior to real images for testing multimodal reasoning capabilities requires further validation, as the trade-offs between controlled generation and real-world complexity are not fully explored.

## Next Checks

1. **Cross-Domain Validation**: Test whether models that perform poorly on JourneyBench also struggle with real-world images requiring similar fine-grained reasoning to determine if the benchmark's difficulty stems from the generated image domain or genuine reasoning challenges.

2. **Human Performance Benchmark**: Establish human performance baselines on JourneyBench tasks to validate that the low model performance reflects genuine difficulty rather than task design issues.

3. **Bias Analysis**: Conduct a systematic analysis of whether models are still leveraging language priors or other biases to achieve their (low) performance on JourneyBench, which would challenge the claim that it successfully tests true multimodal reasoning.