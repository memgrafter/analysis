---
ver: rpa2
title: How to Train the Teacher Model for Effective Knowledge Distillation
arxiv_id: '2407.18041'
source_url: https://arxiv.org/abs/2407.18041
tags:
- teacher
- student
- distillation
- knowledge
- bcpd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that training a teacher model using mean
  squared error (MSE) loss is more effective for knowledge distillation (KD) than
  the conventional cross-entropy (CE) loss. This is because the teacher's role is
  to provide an estimate of the Bayes conditional probability density (BCPD) close
  to the true BCPD in the MSE sense.
---

# How to Train the Teacher Model for Effective Knowledge Distillation

## Quick Facts
- arXiv ID: 2407.18041
- Source URL: https://arxiv.org/abs/2407.18041
- Authors: Shayan Mohajer Hamidi; Xizhen Deng; Renhao Tan; Linfeng Ye; Ahmed Hussein Salamah
- Reference count: 40
- Primary result: MSE-trained teachers consistently improve student accuracy by up to 2.6% compared to CE-trained teachers across various KD methods

## Executive Summary
This paper challenges the conventional approach of using cross-entropy loss for training teacher models in knowledge distillation. Through theoretical analysis and comprehensive experiments, the authors demonstrate that training teachers with mean squared error (MSE) loss leads to better student performance. The key insight is that MSE loss directly minimizes the distance between the teacher's output and the Bayes Conditional Probability Density (BCPD), providing a more effective supervisory signal for students. The approach is validated across multiple state-of-the-art KD methods, datasets (CIFAR-100, ImageNet), and architectures, showing consistent improvements of up to 2.6% in student accuracy.

## Method Summary
The paper proposes training teacher models using MSE loss instead of the conventional cross-entropy loss. The authors prove that minimizing MSE loss is equivalent to minimizing the expected MSE between the teacher's output and the Bayes Conditional Probability Density (BCPD). This MSE-trained teacher then provides knowledge distillation to a student model using various KD methods. The approach requires only changing the teacher's training loss function while keeping all other aspects of the KD pipeline unchanged. Experiments are conducted on CIFAR-100 and ImageNet datasets, comparing student performance when using MSE-trained versus CE-trained teachers across multiple KD methods.

## Key Results
- Replacing CE-trained teachers with MSE-trained teachers consistently improves student accuracy by up to 2.6% across various state-of-the-art KD methods
- The student's accuracy is almost inversely proportional to the MSE between the teacher's output and BCPD, but not for CE
- Despite a slight decrease in teacher performance when using MSE loss, student accuracy consistently increases, demonstrating that optimizing teacher performance is not necessary for effective knowledge distillation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training a DNN with MSE loss minimizes the expected MSE between its output and the Bayes Conditional Probability Density (BCPD)
- Mechanism: Theorem 1 establishes that minimizing MSE loss is equivalent to minimizing the expected MSE between the model's output and the true BCPD
- Core assumption: The model's output can be interpreted as an estimate of the BCPD
- Evidence anchors: Abstract, Theorem 1 section
- Break condition: If the model's output does not accurately represent the BCPD

### Mechanism 2
- Claim: Minimizing MSE between teacher's output and BCPD leads to better student performance than minimizing CE between them
- Mechanism: Student accuracy is inversely proportional to MSE but not CE between teacher output and BCPD
- Core assumption: Student learning is more sensitive to MSE proximity than CE proximity in KD
- Evidence anchors: Abstract, Figure 1 section
- Break condition: If student learning process changes to prioritize CE proximity

### Mechanism 3
- Claim: Teacher's classification accuracy is not directly correlated with its effectiveness in knowledge distillation
- Mechanism: MSE-trained teachers show slight performance decrease but improve student accuracy
- Core assumption: Teacher's role is to estimate BCPD, not maximize its own accuracy
- Evidence anchors: Abstract, MSE loss observation section
- Break condition: If student learning becomes dependent on teacher's raw accuracy

## Foundational Learning

- Concept: Bayes Conditional Probability Density (BCPD)
  - Why needed here: The BCPD is the target that the teacher model aims to estimate and provide to the student
  - Quick check question: What is the difference between the BCPD and the one-hot vector typically used in classification tasks?

- Concept: Mean Squared Error (MSE) vs Cross-Entropy (CE) loss
  - Why needed here: The paper argues that MSE loss is more effective than CE loss for training the teacher model
  - Quick check question: How does minimizing MSE loss differ from minimizing CE loss in terms of the resulting model's output?

- Concept: Knowledge Distillation (KD)
  - Why needed here: The entire paper is focused on improving the effectiveness of knowledge distillation
  - Quick check question: In traditional KD, what role does the teacher model play, and how is this role different from what the paper proposes?

## Architecture Onboarding

- Component map: Teacher Model -> KD Method -> Student Model
- Critical path: 1. Train teacher using MSE loss; 2. Use teacher's output to train student; 3. Evaluate student performance; 4. Compare with CE-trained teacher
- Design tradeoffs: MSE loss may lead to slightly lower teacher accuracy but improved student performance
- Failure signatures: Student performance doesn't improve with MSE teacher; teacher's performance degrades significantly without student improvement
- First 3 experiments:
  1. Generate synthetic dataset to verify relationship between MSE proximity and student accuracy
  2. Implement MSE loss training for teacher and compare student performance on CIFAR-10
  3. Integrate MSE teacher into vanilla KD and evaluate on CIFAR-100

## Open Questions the Paper Calls Out

- Open Question 1: How does MSE loss affect the teacher's own accuracy and performance metrics? The paper observes decreased teacher performance but doesn't explore underlying reasons or quantify impact across architectures.
- Open Question 2: Is there a threshold or optimal condition where MSE-trained teachers outperform CE-trained teachers? The paper demonstrates general improvements but doesn't systematically vary conditions to identify optimal scenarios.
- Open Question 3: How does MSE-trained teacher effectiveness compare to other training strategies like regularization or early stopping? The paper compares MSE to CE but not to other proposed KD strategies.

## Limitations
- The assumption that model output represents BCPD may not hold for all architectures or datasets
- The relationship between MSE proximity and student performance lacks rigorous theoretical explanation
- Effectiveness of MSE teachers is only demonstrated for image classification tasks

## Confidence
- High confidence: Equivalence between minimizing MSE loss and minimizing MSE between model output and BCPD
- Medium confidence: MSE-trained teachers consistently improve student performance across various KD methods
- Low confidence: Optimizing teacher performance is not necessary for effective knowledge distillation

## Next Checks
1. Conduct experiments on non-image classification tasks (NLP, speech recognition) to assess generalizability
2. Investigate impact of different model architectures on MSE teacher effectiveness
3. Perform ablation studies to determine relative importance of MSE vs CE proximity in different KD scenarios