---
ver: rpa2
title: 'Fish-bone diagram of research issue: Gain a bird''s-eye view on a specific
  research topic'
arxiv_id: '2407.01553'
source_url: https://arxiv.org/abs/2407.01553
tags:
- issue
- research
- ontology
- fish-bone
- diagram
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the challenge novice researchers face in\
  \ understanding complex research topics by developing a fish-bone diagram that provides\
  \ a bird\u2019s-eye view of research issues. The method constructs a causal knowledge\
  \ graph from academic paper introductions using issue ontology classification, clustering,\
  \ and summarization."
---

# Fish-bone diagram of research issue: Gain a bird's-eye view on a specific research topic

## Quick Facts
- arXiv ID: 2407.01553
- Source URL: https://arxiv.org/abs/2407.01553
- Reference count: 20
- Primary result: Achieves 78% classification accuracy for issue ontology classification

## Executive Summary
This study addresses the challenge novice researchers face in understanding complex research topics by developing a fish-bone diagram that provides a bird's-eye view of research issues. The method constructs a causal knowledge graph from academic paper introductions using issue ontology classification, clustering, and summarization. Sentences are annotated and classified into prelude, improvable, and emphasize issues, then clustered into tasks and linked via logical chains. The resulting fish-bone diagram visually organizes research topics into joints (tasks), backbones (issue ontologies), fine-bones (factors), and child-bones (logical connections). Evaluation shows the approach achieves 78% classification accuracy, demonstrating its effectiveness in creating structured, interpretable overviews to support efficient research survey navigation.

## Method Summary
The method processes academic paper introductions by first segmenting text into sentences using spaCy. Sentences are then classified into three issue types (prelude, improvable, emphasize) using Sentence-BERT embeddings and an SVM classifier. Prelude sentences are clustered into research tasks using k-means clustering. Task names and issue summaries are generated using ChatGPT prompt engineering. The fish-bone diagram is constructed by organizing tasks as joints, issue ontologies as backbones, factors as fine-bones, and logical connections as child-bones. The visualization is rendered using Pyvis.

## Key Results
- Achieves 78% classification accuracy for issue ontology classification
- Successfully creates fish-bone diagrams with logical task-task and task-issue relationships
- Provides interpretable overviews that help novice researchers navigate research topics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classifying sentences into prelude, improvable, and emphasize issues enables structured mapping of research logic.
- Mechanism: Issue ontology classification organizes sentences into three types, then clustering groups prelude sentences into tasks. The other two types form the backbone and fine-bones, capturing causal relationships.
- Core assumption: Sentences in introductions consistently follow a pattern of historical context → problems → solutions, and can be reliably classified into these three categories.
- Evidence anchors:
  - [abstract] The method constructs a causal knowledge graph from academic paper introductions using issue ontology classification, clustering, and summarization.
  - [section] III.B Mufti-modal Issue ontology in 'introduction' defines three issue types: Prelude issue, Improvable issue, and Emphasize issue.
  - [corpus] Corpus neighbors show papers about "bird's eye view" and "research survey" methods, suggesting topical relevance but no direct validation of sentence-level classification accuracy.
- Break condition: If introductions do not follow the assumed pattern, or if sentence classification accuracy drops significantly, the causal mapping will fail.

### Mechanism 2
- Claim: Linking emphasize issues to improvable issues via child-bones creates interpretable cause-effect chains.
- Mechanism: After clustering and naming tasks, emphasize issues (solutions) are linked to improvable issues (problems) from the same paper. This creates a logical chain: task → improvable issue → emphasize issue.
- Core assumption: Each emphasize issue in a paper directly addresses a specific improvable issue mentioned in the same paper.
- Evidence anchors:
  - [abstract] The diagram is constructed using the issue ontology from academic papers, and it offers a broad, highly generalized perspective of the research field, based on relevance and logical factors.
  - [section] III.C.3 Child-bone: We build the child-bone, the most basic unit of the fish-bone design, derived from the fine-bone. This is done using the 'Improvable issue' ←→ 'Emphasize issue' logic chain within the research task.
  - [corpus] Weak or missing - no direct evidence in corpus about linking emphasize to improvable issues.
- Break condition: If emphasize and improvable issues are not clearly paired, or if authors discuss multiple problems without direct solutions, the cause-effect chain will be incomplete.

### Mechanism 3
- Claim: Fish-bone visualization provides a bird's-eye view that helps novice researchers navigate research topics more efficiently.
- Mechanism: By organizing research into joints (tasks), backbones (issue ontologies), fine-bones (factors), and child-bones (logical connections), the diagram visually represents the research landscape and causal relationships.
- Core assumption: Visual hierarchical structures are more effective than keyword lists for novice researchers to understand research topics.
- Evidence anchors:
  - [abstract] Evaluation shows the approach achieves 78% classification accuracy, demonstrating its effectiveness in creating structured, interpretable overviews to support efficient research survey navigation.
  - [section] II. Related Work discusses how keyword-based knowledge graphs are "too vague for novice researchers to understand the inherent logical connections among multiple academic papers."
  - [corpus] Corpus neighbors include papers on "bird's eye view" methods, suggesting topical relevance but no direct validation of fish-bone diagram effectiveness.
- Break condition: If the visual complexity overwhelms users, or if the classification errors propagate to the diagram, the bird's-eye view benefit will not materialize.

## Foundational Learning

- Concept: Issue Ontology Classification
  - Why needed here: To extract structured causal relationships from unstructured text in paper introductions.
  - Quick check question: Can you explain the difference between an improvable issue and an emphasize issue in the context of academic paper introductions?

- Concept: Sentence Embedding with Sentence-BERT
  - Why needed here: To convert text sentences into numerical vectors that can be clustered and compared for similarity.
  - Quick check question: What is the role of the 'Distilbert-base-uncased' model in the sentence embedding process?

- Concept: K-means Clustering for Task Identification
  - Why needed here: To group similar prelude sentences together and identify distinct research tasks within the topic.
  - Quick check question: How does clustering prelude sentences help in forming the joints (tasks) of the fish-bone diagram?

## Architecture Onboarding

- Component map:
  - Data Processing: SpaCy sentence segmentation
  - Issue Ontology Classification: Sentence-BERT embedding + SVM classifier
  - Task Clustering: K-means clustering on prelude sentences
  - Diagram Generation: ChatGPT prompt engineering for naming and summarization
  - Visualization: Pyvis for fish-bone diagram rendering

- Critical path: Sentence segmentation → Issue classification → Task clustering → Logical chain generation → Diagram visualization

- Design tradeoffs:
  - Using SVM vs. more complex models: Simpler, faster, but potentially less accurate
  - Manual annotation vs. fully automated: Higher quality but slower development
  - ChatGPT for summarization vs. rule-based: More flexible but dependent on API availability

- Failure signatures:
  - Low classification accuracy (<60%) indicates poor sentence categorization
  - Empty or nonsensical task names suggest clustering or prompt engineering issues
  - Disconnected or illogical chains in the diagram point to problems in the emphasize/improvable issue pairing

- First 3 experiments:
  1. Run the pipeline on a small, manually annotated dataset to verify the classification and clustering steps work as expected.
  2. Test the prompt engineering for task naming and issue summarization with a few sample sentences to ensure meaningful outputs.
  3. Generate a fish-bone diagram for a simple, well-understood research topic (e.g., HotpotQA) and validate its logical structure against expert knowledge.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the classification accuracy of issue ontology types change when expanding the dataset beyond the introduction section to include other sections like related work and conclusion?
- Basis in paper: [explicit] The authors note that their current dataset is limited to the introduction section and mention that "issues discussed in other sections...are not covered" as future work, implying this could affect classification performance.
- Why unresolved: The study only evaluated classification accuracy on introduction sentences (78% total accuracy), but did not test how well the model would perform on issue ontology sentences from other paper sections with potentially different linguistic patterns and contexts.
- What evidence would resolve it: A systematic evaluation comparing classification accuracy on introduction sentences versus related work and conclusion sentences using the same trained model, including precision, recall, and F1-scores for each issue type across all sections.

### Open Question 2
- Question: What is the optimal number of fine-bones (factors) to generate for each task in the fish-bone diagram to balance comprehensiveness and readability?
- Basis in paper: [explicit] The authors mention using prompt engineering with "n themes" but do not specify how n is determined, stating only that "n represents the number of fine-bone to be generated" without justification for this parameter choice.
- Why unresolved: The paper does not provide criteria for selecting the number of fine-bones per task, which could significantly impact the diagram's usefulness - too few might miss important factors while too many could overwhelm novice researchers.
- What evidence would resolve it: User studies comparing comprehension and navigation efficiency of fish-bone diagrams with different numbers of fine-bones per task, measuring task completion time, recall of key concepts, and subjective clarity ratings from novice researchers.

### Open Question 3
- Question: How does the logical chain construction between emphasize and improvable issues handle cases where multiple emphasize issues address the same improvable issue or vice versa?
- Basis in paper: [explicit] The authors describe building child-bones using "Improvable issue ←→ Emphasize issue logic chain" but do not detail how the system resolves ambiguous or redundant connections when multiple issue pairs could form valid chains.
- Why unresolved: The methodology section mentions creating logical connections but lacks specifics on conflict resolution or disambiguation strategies when multiple issue pairs could form valid chains, which could lead to incomplete or confusing diagrams.
- What evidence would resolve it: A detailed analysis of edge cases where multiple emphasize issues relate to single improvable issues (or vice versa), showing the current system's output versus an ideal resolution, and evaluation of user comprehension when presented with different resolution strategies.

## Limitations
- Limited to introduction sections of papers, missing context from related work and conclusions
- Dependency on ChatGPT introduces variability and potential reproducibility issues
- Assumes consistent structure in paper introductions that may not generalize across all research fields

## Confidence
- **High Confidence**: The general framework of using issue ontology classification to structure research topics
- **Medium Confidence**: The effectiveness of the fish-bone diagram visualization for novice researchers (limited empirical validation)
- **Low Confidence**: The scalability and generalizability of the approach beyond the HotpotQA domain

## Next Checks
1. Test the classification accuracy on a diverse set of research topics from different domains to assess generalizability
2. Conduct a user study with novice researchers to evaluate the actual effectiveness of the fish-bone diagram for survey navigation
3. Perform ablation studies to determine the impact of ChatGPT dependency by comparing with rule-based or alternative summarization methods