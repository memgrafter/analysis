---
ver: rpa2
title: LLM-based Frameworks for API Argument Filling in Task-Oriented Conversational
  Systems
arxiv_id: '2407.12016'
source_url: https://arxiv.org/abs/2407.12016
tags:
- argument
- filling
- llms
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-based frameworks for API argument filling
  in task-oriented conversational systems. The authors address the challenge of grounding
  LLM outputs to strictly follow pre-defined API schemas and dialogue histories during
  argument filling.
---

# LLM-based Frameworks for API Argument Filling in Task-Oriented Conversational Systems

## Quick Facts
- arXiv ID: 2407.12016
- Source URL: https://arxiv.org/abs/2407.12016
- Authors: Jisoo Mok; Mohammad Kachuee; Shuyang Dai; Shayan Ray; Tara Taghavi; Sungroh Yoon
- Reference count: 10
- One-line primary result: LLM-based frameworks significantly improve API argument filling in task-oriented conversational systems through instruction-tuning and multi-step prompting.

## Executive Summary
This paper addresses the challenge of grounding LLM outputs to strictly follow pre-defined API schemas and dialogue histories during argument filling in task-oriented conversational systems. The authors propose two main approaches: a two-step instruction-tuning framework for open-sourced LLMs and a multi-step prompting scheme for closed-sourced LLMs. Experimental results on STAR and SGD datasets demonstrate significant improvements in argument filling performance, with the instruction-tuned LLAMA-v1-7B model outperforming larger zero-shot baselines.

## Method Summary
The paper proposes two complementary approaches to improve LLM-based API argument filling. For open-sourced LLMs, they introduce a two-step instruction-tuning framework consisting of supervised fine-tuning (SFT) followed by rejection sampling with a custom reward function. For closed-sourced LLMs, they implement a multi-step prompting scheme that provides more granular and informative prompts by filling one argument slot at a time. The framework is evaluated on STAR and SGD datasets using BLEU, fuzzy matching, and F1 score metrics to measure argument filling accuracy.

## Key Results
- The instruction-tuned LLAMA-v1-7B model outperforms larger zero-shot baselines in API argument filling
- Rejection sampling with custom reward function reduces all error categories, particularly hallucinated values
- Multi-step prompting for closed-sourced LLMs improves grounding compared to single-step prompts
- The framework shows improved performance on both in-domain and out-of-domain benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rejection sampling with a custom reward function grounds LLM outputs to follow API schemas and dialogue histories.
- Mechanism: The authors define four error types (non-existent key, missing key, schema-grounded but incorrect value, hallucinated value) and compute a normalized reward: R = 1 − 2 × NError/NTotal. Outputs with positive reward are selected for fine-tuning, which reduces all error types in the instruction-tuned model compared to the baseline.
- Core assumption: The reward function captures semantic alignment with the ground-truth API argument structure and dialogue context.
- Evidence anchors:
  - [abstract] "Our initial investigation reveals that LLMs require an additional grounding process to successfully perform argument filling..."
  - [section] "We propose a two-step instruction-tuning framework that is comprised of supervised fine-tuning (SFT) and rejection sampling (RS)."
  - [corpus] Weak/no direct neighbor papers discuss rejection sampling for grounding API arguments; this appears to be a novel methodological contribution.
- Break condition: If the reward function fails to differentiate between semantically valid but structurally incorrect arguments and truly hallucinated ones, rejection sampling may reinforce incorrect patterns.

### Mechanism 2
- Claim: Multi-step prompting reduces hallucinations by constraining generation to one argument slot at a time.
- Mechanism: Instead of generating all arguments in a single prompt, the model is prompted to fill one slot at a time, providing explicit schema information for each step. This reduces schema-grounded but incorrect values and hallucinations.
- Core assumption: Granular prompts reduce the cognitive load and contextual ambiguity for the model, improving adherence to schema constraints.
- Evidence anchors:
  - [abstract] "we demonstrate that their performance can be improved by replacing the plain prompt design with a 'multi-step prompting' scheme."
  - [section] "By using this more targeted prompt design, we are providing the LLM with additional information about required slots and effectively restricting its generative behavior..."
  - [corpus] No direct corpus neighbor discusses multi-step prompting; this is likely a novel prompting technique.
- Break condition: If the argument slots are highly interdependent, stepwise prompting may miss contextual cues, leading to inconsistent values across slots.

### Mechanism 3
- Claim: Instruction tuning bootstraps LLM behavior to produce dictionary-formatted outputs before applying rejection sampling.
- Mechanism: The first fine-tuning phase uses cross-entropy loss to shape the model's generation to match the dictionary format of API arguments, enabling the reward function to compute errors at the key-value level.
- Core assumption: The model can learn the formatting constraints without overfitting to the training set.
- Evidence anchors:
  - [abstract] "We first bootstrap the LLM's responses on argument filling prompts..."
  - [section] "Following the conventional fine-tuning scheme, we fine-tune the LLM using the cross entropy loss..."
  - [corpus] No neighbor papers mention bootstrapping LLM formatting; this is likely an essential methodological step.
- Break condition: If the bootstrapping phase overfits to a narrow set of formatting patterns, the model may fail on unseen schema structures.

## Foundational Learning

- Concept: API schema grounding
  - Why needed here: The LLM must strictly follow a predefined schema for API arguments; understanding schema parsing and validation is essential.
  - Quick check question: What is the difference between a "non-existent key" error and a "missing key" error in the context of API argument filling?

- Concept: Prompt engineering and in-context learning
  - Why needed here: LLMs rely on carefully structured prompts to perform well; designing prompts that provide sufficient context without overwhelming the model is critical.
  - Quick check question: How does multi-step prompting differ from a single-step prompt in terms of information provided to the model?

- Concept: Rejection sampling and reward-based fine-tuning
  - Why needed here: Standard supervised fine-tuning may not capture the nuances of schema adherence; rejection sampling with a custom reward can selectively improve the model's outputs.
  - Quick check question: What is the range of the reward function used in the paper, and how is it normalized?

## Architecture Onboarding

- Component map:
  Input: Dialogue history, API schema, instruction prompt -> LLM (open-source or closed-source) -> Fine-tuning pipeline: SFT → Rejection Sampling → SFT -> Output: Dictionary-formatted API arguments

- Critical path:
  1. Parse API schema and dialogue history
  2. Generate candidate arguments via LLM
  3. Score candidates with custom reward function
  4. Fine-tune on high-reward samples
  5. Evaluate with BLEU, fuzzy matching, F-1 score

- Design tradeoffs:
  - Open-source LLMs allow fine-tuning but require computational resources; closed-source LLMs offer inference-only access but rely on prompting tricks.
  - Rejection sampling increases robustness but adds complexity and latency.
  - Multi-step prompting improves accuracy but increases inference steps.

- Failure signatures:
  - High "non-existent key" error → schema parsing issue
  - High "hallucinated value" error → prompt not constraining generation
  - Low BLEU but high fuzzy matching → semantic correctness despite phrasing differences

- First 3 experiments:
  1. Run zero-shot prompt on LLAMA-v1-7B with STAR dataset; record all error types.
  2. Fine-tune LLAMA-v1-7B with SFT only; compare error rates.
  3. Apply rejection sampling to SFT model; evaluate improvement on both in-domain and out-of-domain splits.

## Open Questions the Paper Calls Out

- How does the proposed instruction-tuning framework generalize to other task-oriented dialogue datasets beyond STAR and SGD?
  - Basis in paper: [explicit] The paper validates the approach on STAR and SGD datasets but does not explore other datasets.
  - Why unresolved: The paper does not provide evidence of the framework's performance on other task-oriented dialogue datasets, leaving its generalizability uncertain.
  - What evidence would resolve it: Experiments demonstrating the framework's effectiveness on a diverse range of task-oriented dialogue datasets, such as MultiWOZ or ConvAI2, would provide evidence of its generalizability.

- What is the impact of varying the number of rejection-sampled outputs (K) on the performance of the fine-tuned LLM?
  - Basis in paper: [explicit] The paper mentions using K number of outputs for rejection sampling but does not explore the effect of varying K.
  - Why unresolved: The optimal value of K for maximizing performance is not determined, and different values may lead to varying degrees of improvement.
  - What evidence would resolve it: Experiments varying K and analyzing its impact on performance metrics like BLEU, FM, and F-1 scores would help identify the optimal K value.

- How does the proposed framework perform when applied to open-sourced LLMs of different sizes, such as LLAMA-v1-13B or LLAMA-v1-33B?
  - Basis in paper: [explicit] The paper focuses on the LLAMA-v1-7B model but does not explore the framework's effectiveness on larger LLMs.
  - Why unresolved: The scalability of the framework to larger LLMs is unknown, and performance may vary depending on model size.
  - What evidence would resolve it: Experiments applying the framework to larger LLMs and comparing their performance to the LLAMA-v1-7B model would provide insights into scalability.

## Limitations
- The evaluation relies on a custom reward function for rejection sampling, but the exact implementation details for computing the four error types are not fully specified in the paper.
- The performance of the instruction-tuned model on truly out-of-distribution schemas (beyond the domain splits in STAR and SGD) remains untested.
- The multi-step prompting approach for closed-source LLMs may introduce latency issues in real-time conversational systems due to the sequential nature of argument filling.

## Confidence
- **High confidence**: The two-step instruction-tuning framework (SFT + rejection sampling) improves grounding performance for open-source LLMs, as evidenced by reduced error rates across all categories in the experimental results.
- **Medium confidence**: The multi-step prompting scheme for closed-source LLMs reduces hallucinations, but the improvement may be dataset-specific and requires validation on additional benchmarks.
- **Medium confidence**: The claim that the LLAMA-v1-7B model outperforms larger zero-shot baselines is supported by the reported metrics, but the comparison lacks ablation studies to isolate the impact of each fine-tuning component.

## Next Checks
1. Implement the custom reward function to compute the four error types (NK, MK, SV, HV) and verify that the normalized reward R = 1 − 2 × NError/NTotal aligns with the reported error reductions during rejection sampling.
2. Evaluate the instruction-tuned model on a held-out set of API schemas not present in STAR or SGD to assess generalization beyond in-domain and out-of-domain splits.
3. Measure the inference latency of the multi-step prompting approach for closed-source LLMs and compare it to single-step prompting to quantify the trade-off between accuracy and response time.