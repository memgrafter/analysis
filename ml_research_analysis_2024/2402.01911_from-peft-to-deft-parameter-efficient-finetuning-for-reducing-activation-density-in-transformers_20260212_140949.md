---
ver: rpa2
title: 'From PEFT to DEFT: Parameter Efficient Finetuning for Reducing Activation
  Density in Transformers'
arxiv_id: '2402.01911'
source_url: https://arxiv.org/abs/2402.01911
tags:
- density
- activation
- deft
- peft
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DEFT (Density-Efficient Fine-Tuning), a novel
  method to reduce activation density in transformer models by adding a density loss
  to the fine-tuning objective. This encourages higher activation sparsity in the
  intermediate outputs of MLP blocks, enabling efficient inference on sparsity-aware
  hardware.
---

# From PEFT to DEFT: Parameter Efficient Finetuning for Reducing Activation Density in Transformers

## Quick Facts
- arXiv ID: 2402.01911
- Source URL: https://arxiv.org/abs/2402.01911
- Authors: Bharat Runwal; Tejaswini Pedapati; Pin-Yu Chen
- Reference count: 40
- Primary result: DEFT reduces activation density by up to 44.94% on RoBERTaLarge and 53.19-90.60% on Flan-T5XXL with minimal impact on downstream performance

## Executive Summary
This paper introduces DEFT (Density-Efficient Fine-Tuning), a method that reduces activation density in transformer models by adding a density loss to the fine-tuning objective. The approach encourages higher activation sparsity in MLP intermediate outputs, enabling more efficient inference on sparsity-aware hardware. DEFT is fully compatible with mainstream PEFT techniques like Adapter, LoRA, Prefix-Tuning, and Prompt-Tuning, and can reduce activation density by up to 90.60% on large models with minimal performance degradation. An adaptive variant, ADA-DEFT, further achieves significant runtime and memory savings during inference by selectively skipping MLP blocks.

## Method Summary
DEFT modifies the PEFT fine-tuning objective by adding a density loss that encourages sparsity in MLP intermediate activations. The density loss uses smooth approximations (tanh or l0) to count non-zero activations and penalizes dense outputs. The method trains only PEFT parameters plus optional adaptive layerwise weights, leaving pretrained weights frozen. ADA-DEFT extends this with trainable scalars per MLP block, allowing selective skipping during inference. Experiments use GLUE, SQuAD datasets and models from RoBERTaLarge to Flan-T5XXL, measuring both task performance and activation density reduction.

## Key Results
- DEFT reduces activation density by 44.94% on RoBERTaLarge compared to PEFT
- On Flan-T5XXL, DEFT achieves 53.19-90.60% density reduction
- ADA-DEFT achieves significant runtime and memory savings during inference
- Minimal impact on downstream task performance across all tested models and tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding a density loss encourages activation sparsity in MLP intermediate outputs.
- Mechanism: The density loss approximates the count of non-zero activations using a smooth function (tanh or l0-approximation), then penalizes dense activations by backpropagating through this approximation. This drives the network to produce sparser intermediate activations during fine-tuning.
- Core assumption: Sparse activations in MLP intermediate layers do not significantly degrade downstream task performance.
- Evidence anchors:
  - [abstract]: "we propose a novel density loss that encourages higher activation sparsity (equivalently, lower activation density) in the pre-trained models."
  - [section 3.2]: Defines the density loss using tanh or l0-approximation to measure non-zero activations.
  - [corpus]: Weak evidence; no direct mention of density loss in corpus.

### Mechanism 2
- Claim: PEFT techniques provide a small number of trainable parameters that are sufficient to induce activation sparsity.
- Mechanism: By freezing the bulk of the transformer parameters and only training a small subset (e.g., adapters, LoRA matrices), the gradient signal through these parameters is enough to reshape the activation patterns toward sparsity without full fine-tuning.
- Core assumption: The frozen transformer layers retain enough flexibility to support sparse activation patterns when only PEFT modules are updated.
- Evidence anchors:
  - [abstract]: "we demonstrate the effectiveness of our approach by utilizing mainstream PEFT techniques... to facilitate efficient model adaptation across diverse downstream tasks."
  - [section 3.4]: Explains how DEFT is fully compatible with PEFT and only trains additional parameters.
  - [corpus]: Weak evidence; no direct mention of PEFT's role in sparsity induction in corpus.

### Mechanism 3
- Claim: Adaptive layerwise weights allow selective skipping of MLP blocks, yielding runtime and memory savings.
- Mechanism: ADA-DEFT introduces a trainable scalar per MLP block that scales the density loss locally; during inference, blocks with low weights can be skipped, reducing computation.
- Core assumption: The learned layerwise weights correlate with the importance of each MLP block for the task, so skipping low-weight blocks is safe.
- Evidence anchors:
  - [abstract]: "We also introduce ADA-DEFT, an adaptive variant of our DEFT approach, which achieves significant memory and runtime savings during inference."
  - [section 3.5]: Describes the optimization with adaptive weights and skipping of MLP blocks.
  - [corpus]: Weak evidence; no direct mention of adaptive skipping in corpus.

## Foundational Learning

- Concept: Activation sparsity and its measurement in neural networks.
  - Why needed here: The paper's core contribution relies on inducing and measuring activation sparsity; understanding how to quantify sparsity is essential.
  - Quick check question: How would you compute the activation density of a tensor in a transformer layer?

- Concept: Parameter-efficient fine-tuning (PEFT) techniques.
  - Why needed here: DEFT is built on top of PEFT; knowing how adapters, LoRA, etc., work is necessary to implement DEFT.
  - Quick check question: What is the key difference between LoRA and Adapter in terms of where trainable parameters are added?

- Concept: Differentiable approximations of non-differentiable functions (e.g., l0 norm).
  - Why needed here: The density loss requires a smooth approximation of the count of non-zero activations for backpropagation.
  - Quick check question: Why can't we use a hard threshold (step function) to count non-zero activations in the loss?

## Architecture Onboarding

- Component map:
  - Pre-trained transformer backbone (frozen)
  - PEFT module (adapter/LoRA/prefix/prompt) (trainable)
  - Density loss module (smooth approximation + scaling)
  - Optional: Adaptive layerwise weights per MLP block

- Critical path:
  1. Forward pass: Input → PEFT module → transformer → MLP intermediate output
  2. Compute density loss on intermediate output
  3. Backpropagate through PEFT modules and adaptive weights
  4. Update only PEFT and adaptive parameters

- Design tradeoffs:
  - Sparsity vs. accuracy: Higher density loss weight (α) increases sparsity but may hurt performance.
  - Approximation choice: tanh works for ReLU; l0-approximation needed for GeLU.
  - Runtime savings vs. complexity: ADA-DEFT adds inference logic to skip blocks.

- Failure signatures:
  - Performance collapse: Activation density too low or adaptive weights set to zero for critical blocks.
  - Unstable training: Poor choice of approximation or hyperparameters (α, β, ϵ).
  - No speedup: Adaptive weights never close to zero, so no blocks skipped.

- First 3 experiments:
  1. Run DEFT with Adapter on a small GLUE task (e.g., SST-2) and compare density vs. PEFT baseline.
  2. Vary α in DEFT and plot accuracy and density to find the sweet spot.
  3. Enable ADA-DEFT and measure runtime/memory savings vs. PEFT on a large model (e.g., Flan-T5XXL).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of DEFT compare to other sparsity-inducing techniques like structured pruning or quantization-aware training?
- Basis in paper: [inferred] The paper mentions that DEFT can work complementarily with quantized and pruned models, but does not directly compare its effectiveness to these methods.
- Why unresolved: The paper focuses on demonstrating DEFT's effectiveness compared to PEFT baselines, but does not benchmark against other sparsity-inducing techniques.
- What evidence would resolve it: Empirical results comparing DEFT's activation sparsity and downstream performance to structured pruning, quantization-aware training, and other sparsity-inducing methods on the same datasets and models.

### Open Question 2
- Question: What is the impact of DEFT on the robustness of models to adversarial attacks or out-of-distribution data?
- Basis in paper: [inferred] The paper does not discuss the impact of DEFT on model robustness, which is an important consideration for practical deployment.
- Why unresolved: The paper focuses on the efficiency gains of DEFT but does not explore its effects on model robustness.
- What evidence would resolve it: Experiments evaluating the robustness of DEFT models compared to PEFT models on adversarial attack benchmarks and out-of-distribution data.

### Open Question 3
- Question: How does the choice of approximation function (tanh, l0, sigmoid, l1) in the density loss affect the convergence and stability of training?
- Basis in paper: [explicit] The paper mentions that they compared DEFT with different approximations and found that tanh works best for ReLU-based models, but does not provide a detailed analysis of the impact of these choices on training dynamics.
- Why unresolved: The paper only briefly mentions the comparison of approximation functions and does not delve into the details of how these choices affect training.
- What evidence would resolve it: A detailed analysis of the convergence and stability of training with different approximation functions, including metrics like training loss, validation loss, and activation density over time.

## Limitations
- Limited experimental scope: Only tested on RoBERTaLarge and Flan-T5XXL with GLUE and SQuAD benchmarks
- Approximation errors: Density loss uses smooth approximations that may not perfectly capture true activation sparsity
- No robustness analysis: Paper does not evaluate impact on model robustness to adversarial attacks or out-of-distribution data

## Confidence
- High confidence: DEFT consistently reduces activation density when applied to RoBERTaLarge and Flan-T5XXL with PEFT techniques
- Medium confidence: ADA-DEFT achieves significant runtime and memory savings during inference
- Low confidence: DEFT will perform similarly across all transformer architectures and task types

## Next Checks
1. Ablation study on approximation functions: Compare DEFT performance using tanh vs. l0-approximation across different activation functions (ReLU, GeLU, SwiGLU) to quantify the impact of approximation choice on both density reduction and downstream accuracy.

2. Cross-architecture generalization test: Apply DEFT to BERT, GPT-2, and OPT models on diverse task types (summarization, translation, code generation) to validate the scalability and robustness of the density loss across different transformer architectures and downstream applications.

3. Adaptive skipping safety analysis: Implement a controlled experiment that systematically varies the threshold for block skipping in ADA-DEFT and measures the relationship between skipped blocks and performance degradation, identifying safe skipping zones for different task types.