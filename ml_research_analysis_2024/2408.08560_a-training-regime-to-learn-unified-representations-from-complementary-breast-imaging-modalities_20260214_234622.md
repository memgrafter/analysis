---
ver: rpa2
title: A training regime to learn unified representations from complementary breast
  imaging modalities
arxiv_id: '2408.08560'
source_url: https://arxiv.org/abs/2408.08560
tags:
- images
- ffdm
- breast
- detection
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a training regime to learn unified representations
  from complementary breast imaging modalities, specifically Full Field Digital Mammograms
  (FFDMs) and Digital Breast Tomosynthesis (DBT) Synthetic Mammograms (SMs). The goal
  is to reduce reliance on FFDM by leveraging the complementary diagnostic signal
  from both modalities.
---

# A training regime to learn unified representations from complementary breast imaging modalities

## Quick Facts
- arXiv ID: 2408.08560
- Source URL: https://arxiv.org/abs/2408.08560
- Authors: Umang Sharma; Jungkyu Park; Laura Heacock; Sumit Chopra; Krzysztof Geras
- Reference count: 40
- Primary result: Proposed method outperforms models trained on either FFDM or SM alone by learning unified representations from complementary breast imaging modalities

## Executive Summary
This paper proposes a three-stage training regime to learn unified representations from complementary breast imaging modalities, specifically Full Field Digital Mammograms (FFDMs) and Digital Breast Tomosynthesis (DBT) Synthetic Mammograms (SMs). The approach aims to reduce reliance on FFDM by leveraging the complementary diagnostic signal from both modalities. The method involves training a machine learning model to learn high-level representations that capture information from both FFDM and SM images, ultimately improving lesion detection accuracy in breast cancer screening.

## Method Summary
The authors propose a three-stage training methodology using the EfficientDet architecture. First, individual models are trained on SMs and FFDMs separately. Second, a new model is trained to produce SM representations that borrow knowledge from FFDM representations through knowledge distillation. Third, fused representations are obtained by concatenating the SM-derived features with the FFDM-mimicked features. These fused representations are then used to make predictions during inference. The method is evaluated on a large-scale dataset of 1,239,372 pairs of FFDM and SM images.

## Key Results
- The proposed method outperforms models trained on either modality alone
- Fused representations from concatenated SM-derived and FFDM-mimicked features show improved detection performance
- Knowledge distillation effectively transfers diagnostic information from FFDM to SM representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Complementary diagnostic signals from FFDM and SM images improve lesion detection accuracy
- Mechanism: The model learns to distill knowledge from FFDM representations into SM-based representations through a knowledge distillation stage, creating fused representations that encode information from both modalities
- Core assumption: The high-level representations learned from FFDM and SM contain complementary information about breast lesions that can be effectively combined
- Evidence anchors:
  - [abstract] "learns high-level representations leveraging the complementary diagnostic signal from both DBT and FFDM"
  - [section] "our analysis shows that FFDMs and SMs contain complementary information regarding estimating the presence, or absence, of suspicious lesions"
  - [corpus] Weak evidence - related papers focus on multimodal integration but don't explicitly validate complementary signal assumption
- Break condition: If FFDM and SM representations capture largely overlapping information, knowledge distillation provides minimal benefit

### Mechanism 2
- Claim: Knowledge distillation from FFDM to SM representations improves SM-only inference performance
- Mechanism: Module B learns to mimic the FFDM-derived representations (hffdm) using only SM images, effectively transferring knowledge about lesion patterns that may be better captured in FFDM
- Core assumption: The relationships between FFDM and SM representations can be learned during training and applied during SM-only inference
- Evidence anchors:
  - [section] "Module B is trained to mimic the outputs of module C... The goal of training is to bring h'\_sm close to hffdm"
  - [section] "our method outperforms models that simply learn and predict using either FFDM or SM alone"
  - [corpus] No direct evidence - knowledge distillation is used but not specifically validated for cross-modal transfer
- Break condition: If the relationship between FFDM and SM representations is too complex or non-stationary to learn effectively

### Mechanism 3
- Claim: Fused representations from concatenated SM-derived and FFDM-mimicked features outperform single-modality representations
- Mechanism: The final model combines the original SM representation (hsm) with the FFDM-mimicked representation (h'\_sm) through concatenation, providing a richer feature space for detection
- Core assumption: Concatenating complementary feature representations creates a more discriminative feature space than either representation alone
- Evidence anchors:
  - [section] "fused representations are obtained by concatenating the features from A and B"
  - [section] "our method outperforms models that simply learn and predict using either FFDM or SM alone"
  - [corpus] Weak evidence - similar concatenation approaches exist but not validated for this specific application
- Break condition: If the concatenated feature space becomes too high-dimensional or redundant, degrading model performance

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Enables transfer of diagnostic information from FFDM to SM representations without requiring FFDM at inference
  - Quick check question: What loss function is used to train the distillation module to mimic FFDM representations?

- Concept: Multi-Modal Learning
  - Why needed here: Combines complementary information from different imaging modalities to improve detection accuracy
  - Quick check question: How does the model ensure that SM-only inference benefits from FFDM knowledge learned during training?

- Concept: Object Detection with Fused Representations
  - Why needed here: Uses the enriched feature space from both modalities to improve lesion localization and classification
  - Quick check question: What happens to the detection performance if the fused representations are not fine-tuned after concatenation?

## Architecture Onboarding

- Component map:
  Module A: EfficientDet trained on SM images (produces hsm) -> Module B: EfficientDet trained to mimic C using only SM input (produces h'\_sm) -> Predictor: Detection head trained on concatenated [hsm, h'\_sm] features
  Module C: EfficientDet trained on FFDM images (produces hffdm)

- Critical path: SM image -> Module A -> hsm; SM image -> Module B -> h'\_sm -> Concatenation -> Predictor -> Detection output
- Design tradeoffs:
  - Increased complexity vs. performance gain
  - Training time for three-stage process vs. single-stage baseline
  - Memory requirements for storing and processing multiple representations
- Failure signatures:
  - Distillation failure: h'\_sm remains dissimilar to hffdm despite training
  - Overfitting: Model performs well on validation but poorly on test set
  - Representation collapse: Concatenated features lose discriminative power
- First 3 experiments:
  1. Train baseline models on SM and FFDM independently, compare to fused model
  2. Evaluate knowledge distillation effectiveness by measuring h'\_sm vs hffdm similarity
  3. Test ablation of each component (remove B, remove A, etc.) to quantify contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed methodology be generalized to other medical imaging modalities beyond FFDM and DBT+SM?
- Basis in paper: [explicit] The authors state that "We believe this setup can be generalized to broader applications in medical imaging."
- Why unresolved: The paper focuses solely on the application of the proposed method to FFDM and DBT+SM. While the authors suggest potential generalizability, they do not provide empirical evidence or theoretical justification for its application to other modalities.
- What evidence would resolve it: Empirical studies applying the proposed methodology to other medical imaging modalities, such as MRI, CT, or ultrasound, and demonstrating improved performance compared to single-modality models would provide strong evidence for generalizability.

### Open Question 2
- Question: How does the proposed methodology handle situations where the FFDM and SM images are not perfectly aligned or registered?
- Basis in paper: [inferred] The authors assume that the FFDM and SM images are paired and corresponding, as they are used to learn the relationships between the two modalities. However, the paper does not explicitly address the issue of image alignment or registration.
- Why unresolved: Misalignment or misregistration between the FFDM and SM images could lead to incorrect learning of relationships between the modalities, potentially degrading the performance of the fused model.
- What evidence would resolve it: Experiments evaluating the performance of the proposed method under different levels of image misalignment or misregistration, as well as techniques to improve image alignment or registration, would provide insights into the robustness of the method to these issues.

### Open Question 3
- Question: How does the proposed methodology compare to other multi-modal fusion techniques in terms of performance and computational efficiency?
- Basis in paper: [inferred] The authors compare their method to single-modality models (Modelsm and Modelffdm) and a model that uses both FFDM and SM images during inference (BaseUB). However, they do not compare their method to other multi-modal fusion techniques, such as early fusion, late fusion, or attention-based fusion.
- Why unresolved: Different multi-modal fusion techniques may have varying levels of performance and computational efficiency, and it is unclear how the proposed method compares to these alternatives.
- What evidence would resolve it: Empirical studies comparing the performance and computational efficiency of the proposed method to other multi-modal fusion techniques on the same dataset would provide a comprehensive evaluation of its strengths and weaknesses relative to existing approaches.

## Limitations

- The assumption that FFDM and SM images contain truly complementary diagnostic information requires further validation
- The computational overhead of the three-stage training process versus performance gains needs more rigorous cost-benefit analysis
- The scalability of the approach to other imaging modalities or different medical imaging domains remains unclear

## Confidence

- High confidence: The three-stage training methodology and model architecture are clearly specified and reproducible
- Medium confidence: The performance improvements over baseline models are demonstrated, but the statistical significance and clinical relevance need more thorough evaluation
- Low confidence: The assumption of complementary diagnostic signals between FFDM and SM images lacks direct empirical validation

## Next Checks

1. Conduct ablation studies to quantify the individual contribution of each training stage to overall performance improvements
2. Perform feature space analysis to explicitly validate whether FFDM and SM representations capture complementary versus redundant information
3. Evaluate the model's performance across different patient subgroups and lesion types to assess generalizability