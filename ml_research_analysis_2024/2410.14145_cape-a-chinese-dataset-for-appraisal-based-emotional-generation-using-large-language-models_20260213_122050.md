---
ver: rpa2
title: 'CAPE: A Chinese Dataset for Appraisal-based Emotional Generation using Large
  Language Models'
arxiv_id: '2410.14145'
source_url: https://arxiv.org/abs/2410.14145
tags:
- emotion
- emotional
- emotions
- arxiv
- situation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel two-stage automatic data generation
  framework, CAT-BEAR, to create CAPE, a Chinese dataset for emotion-aware conversational
  AI based on cognitive appraisal theory. The framework generates multi-turn dialogues
  by simulating the appraisal process, considering personality, goals, situational
  construal, and beliefs to produce contextually appropriate emotional responses.
---

# CAPE: A Chinese Dataset for Appraisal-based Emotional Generation using Large Language Models

## Quick Facts
- arXiv ID: 2410.14145
- Source URL: https://arxiv.org/abs/2410.14145
- Authors: June M. Liu; He Cao; Renliang Sun; Rui Wang; Yu Li; Jiaxing Zhang
- Reference count: 40
- Primary result: Introducing CAT-BEAR framework that generates emotionally appropriate dialogues by simulating cognitive appraisal process

## Executive Summary
This study introduces CAPE, a Chinese dataset for emotion-aware conversational AI based on cognitive appraisal theory. The dataset is generated using a novel two-stage automatic data generation framework called CAT-BEAR, which simulates the cognitive appraisal process to create multi-turn dialogues with contextually appropriate emotional responses. The framework considers personality, goals, situational construal, and beliefs to produce emotionally intelligent interactions. Manual refinement and human evaluation ensure high-quality data with accurate emotion labels and coherent utterances. The dataset enables emotion prediction and next utterance generation tasks, with fine-tuned models showing significant performance improvements over baseline models.

## Method Summary
The study employs a two-stage automatic data generation framework called CAT-BEAR to create CAPE. First, the framework generates beliefs and knowledge based on personality traits, goals, and situational construal using GPT-4-turbo. Second, it simulates the appraisal process to generate emotion labels and corresponding utterances by evaluating interactions across six dimensions (unpleasantness, control, responsibility, certainty, effort, attention). The generated data undergoes manual refinement by native Chinese speakers to ensure quality, including accuracy in emotion labeling, contextual coherence, and conversational fluency. The refined dataset is then used to fine-tune a base model (GLM-4-9B-Chat) for emotion prediction and next utterance generation tasks.

## Key Results
- Fine-tuned model (ChatEMO) achieves 36% accuracy and 28% F1 score in emotion prediction
- ChatEMO demonstrates high scores in BLEU and BERTScore for next utterance generation
- Human evaluation confirms that the appraisal process significantly improves emotional dialogue quality compared to omitting this step

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAT-BEAR generates emotionally appropriate responses by simulating the cognitive appraisal process and integrating intra-individual factors like personality, goals, beliefs, and situational construal.
- Mechanism: The framework assigns each speaker specific personality traits, goals, and situational construal, then uses these to generate personalized beliefs and knowledge. During the appraisal process, the model evaluates the interaction across six dimensions (unpleasantness, control, responsibility, certainty, effort, attention) to predict the most likely emotion and generate an emotion-aligned utterance.
- Core assumption: Emotional responses in dialogue can be accurately modeled by simulating the cognitive appraisal process as described in Lazarus's Cognitive Appraisal Theory.
- Evidence anchors:
  - [abstract]: "This corpus facilitates the generation of dialogues with contextually appropriate emotional responses by accounting for diverse personal and situational factors."
  - [section]: "Building on CAT, we introduce CAT-BEAR...comprising three main components: 1) intra-individual factors...2) the appraisal process...and 3) appraisal outcomes."
- Break condition: The mechanism would fail if the cognitive appraisal process does not accurately capture the complexity of human emotional responses, or if the intra-individual factors are not sufficiently diverse or representative of real human traits.

### Mechanism 2
- Claim: Fine-tuning on CAPE significantly improves model performance on emotion prediction and next utterance generation tasks compared to baseline models.
- Mechanism: The CAPE dataset, generated using CAT-BEAR and refined through manual quality control, provides comprehensive information about personality, goals, beliefs, situational context, and appropriate emotional responses. Fine-tuning a model (ChatEMO) on this dataset enables it to learn the complex mapping between these factors and emotional expressions.
- Core assumption: The synthetic data generated by CAT-BEAR, when refined and evaluated for quality, is sufficient to train models to generate human-like emotional responses.
- Evidence anchors:
  - [abstract]: "Our fine-tuned model (ChatEMO) significantly outperforms baseline models, achieving 36% accuracy and 28% F1 score in emotion prediction, and high scores in BLEU and BERTScore for utterance generation."
  - [section]: "We utilize evaluation metrics from related studies...and enlist three raters to assess data quality. This process ensures accuracy in emotion labeling, contextual coherence, and overall conversational fluency."
- Break condition: The mechanism would fail if the fine-tuning process overfits to the specific patterns in CAPE without generalizing to new, unseen contexts, or if the evaluation metrics do not adequately capture the quality of emotional responses.

### Mechanism 3
- Claim: Incorporating the appraisal process into the data generation framework improves the quality of generated dialogues compared to omitting this step.
- Mechanism: The appraisal process provides detailed guidelines for generating appropriate emotions and actions based on six dimensions. This ensures that emotions align logically with the dialogue context and intra-individual factors, leading to more coherent and contextually relevant responses.
- Core assumption: Explicitly modeling the appraisal process as a sequential evaluation across multiple dimensions leads to more appropriate emotional responses than alternative methods.
- Evidence anchors:
  - [section]: "We investigate how our data-generative framework enhances emotional dialogue quality...Human evaluation result (Table 6) demonstrates that omitting the appraisal process leads to notable decreases in correctness, contextual, and emotional relevance."
  - [section]: "The appraisal process offers detailed guidelines for generating appropriate emotions and actions, ensuring that emotions align logically and dialogues fit the context."
- Break condition: The mechanism would fail if the sequential evaluation across the six dimensions is not sufficient to capture the nuances of human emotional responses, or if the appraisal process introduces biases that limit the diversity of generated emotions.

## Foundational Learning

- Concept: Cognitive Appraisal Theory (CAT) and its application to emotion generation
  - Why needed here: CAT provides the theoretical foundation for the CAT-BEAR framework, explaining how emotions are generated through the appraisal of external stimuli based on personal and situational factors.
  - Quick check question: What are the three main components of the CAT-BEAR framework, and how do they relate to Cognitive Appraisal Theory?

- Concept: Multi-turn dialogue generation and emotion prediction tasks
  - Why needed here: The study involves generating multi-turn dialogues with appropriate emotional responses and evaluating models on two tasks: emotion prediction and next utterance generation.
  - Quick check question: What are the two evaluation tasks proposed in the study, and what do they aim to measure?

- Concept: Data quality control and human evaluation in NLP datasets
  - Why needed here: The CAPE dataset undergoes manual refinement and human evaluation to ensure accurate emotion labels, contextual coherence, and conversational fluency.
  - Quick check question: What are the six dimensions used in the human evaluation of the CAPE dataset, and what aspects of data quality do they assess?

## Architecture Onboarding

- Component map:
  - CAT-BEAR Framework -> CAPE Dataset -> ChatEMO Model -> Evaluation Metrics
  - CAT-BEAR Framework: Generates synthetic dialogues by simulating the cognitive appraisal process and integrating intra-individual factors
  - CAPE Dataset: The refined dataset containing multi-turn dialogues with emotion labels and utterances
  - ChatEMO Model: The fine-tuned model trained on CAPE for emotion prediction and next utterance generation tasks
  - Evaluation Metrics: Automated metrics (Accuracy, F1, BLEU, ROUGE, BERTScore) and human evaluation for assessing model performance

- Critical path:
  1. Generate synthetic dialogues using CAT-BEAR
  2. Refine the dataset through manual quality control and human evaluation to create CAPE
  3. Fine-tune a base model (GLM-4-9B-Chat) on CAPE to create ChatEMO
  4. Evaluate ChatEMO on emotion prediction and next utterance generation tasks using automated metrics and human evaluation

- Design tradeoffs:
  - Using synthetic data generation vs. collecting real human dialogues: Synthetic data allows for greater control over personality, goals, and situational factors but may lack the naturalness of real conversations
  - Focusing on Chinese language vs. multilingual approach: Focusing on Chinese allows for tailoring to cultural nuances but limits the dataset's applicability to other languages
  - Manual refinement vs. fully automated quality control: Manual refinement ensures higher data quality but is more time-consuming and expensive

- Failure signatures:
  - Low accuracy or F1 score on emotion prediction task: Indicates that the model is not effectively learning the mapping between intra-individual factors and emotions
  - Low BLEU, ROUGE, or BERTScore on next utterance generation task: Suggests that the generated utterances are not similar enough to the ground truth in terms of content or style
  - Poor performance on human evaluation metrics: Implies that the generated dialogues lack coherence, naturalness, or emotional appropriateness

- First 3 experiments:
  1. Generate a small set of synthetic dialogues using CAT-BEAR with a limited set of personality traits and situational construals. Manually evaluate the quality of these dialogues to assess the effectiveness of the framework.
  2. Fine-tune a base model on a subset of CAPE and evaluate its performance on the emotion prediction task using automated metrics. Compare the results to baseline models.
  3. Generate next utterances using the fine-tuned model and evaluate their quality using automated metrics and human evaluation. Compare the results to baseline models and the ground truth.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CAT-BEAR framework's performance vary across different cultural contexts beyond Chinese?
- Basis in paper: [inferred] The paper specifically focuses on Chinese language and culture, noting that "Research on emotional expression in other languages and cultures has received limited attention."
- Why unresolved: The study only evaluated the framework on Chinese data and didn't test its cross-cultural applicability or adaptability to other languages and cultural contexts.
- What evidence would resolve it: Comparative studies testing CAT-BEAR's performance on emotion generation datasets from different languages/cultures (e.g., English, Japanese, Spanish) while measuring accuracy, coherence, and naturalness across these contexts.

### Open Question 2
- Question: What is the long-term impact of using AI-generated emotional data on the emotional intelligence development of conversational agents?
- Basis in paper: [inferred] The paper discusses using synthetic data generation with GPT-4-turbo and fine-tuning models on this data, but doesn't address potential long-term effects on model development.
- Why unresolved: The study focuses on immediate performance improvements but doesn't examine whether continuous training on AI-generated emotional data might lead to model degradation or emotional expression biases over time.
- What evidence would resolve it: Longitudinal studies tracking model performance and emotional expression quality across multiple training iterations using both human-generated and AI-generated emotional data.

### Open Question 3
- Question: How does the granularity of situational construal affect the quality of generated emotional responses?
- Basis in paper: [explicit] The paper uses 89 situational construals from the Riverside Situational Q-sort but doesn't systematically test how varying the number or specificity of these construals impacts dialogue quality.
- Why unresolved: While the paper employs a comprehensive set of situational construals, it doesn't investigate whether more or fewer construals would improve or degrade the framework's performance.
- What evidence would resolve it: Controlled experiments testing the framework's performance with different numbers of situational construals (e.g., 30, 50, 89, 120) while measuring emotion prediction accuracy and response quality.

## Limitations

- The study relies heavily on synthetic data generation using GPT-4-turbo, which may introduce biases and may not fully capture the complexity of natural human emotional responses.
- The dataset focuses specifically on Chinese language and culture, limiting generalizability to other languages and cultural contexts.
- Evaluation primarily uses automated metrics and limited human evaluation, which may not comprehensively capture the nuances of emotional appropriateness and naturalness in dialogues.

## Confidence

- **High Confidence**: The framework's theoretical foundation based on Cognitive Appraisal Theory is well-established, and the reported improvements in automated metrics (36% accuracy, 28% F1 score) are specific and measurable.
- **Medium Confidence**: The human evaluation results and qualitative assessments are supported by the methodology but rely on subjective judgments that may vary across different evaluator pools.
- **Medium Confidence**: The claim about the appraisal process's importance is based on comparative experiments but lacks detailed ablation studies showing the contribution of individual appraisal dimensions.

## Next Checks

1. Conduct cross-cultural validation by testing ChatEMO's performance on emotion prediction and generation tasks using dialogues from different cultural contexts to assess the model's generalizability beyond Chinese cultural norms.
2. Perform a detailed ablation study on the appraisal process by systematically removing or modifying individual dimensions (unpleasantness, control, responsibility, certainty, effort, attention) to quantify their specific contributions to emotional response quality.
3. Test the model's performance on out-of-distribution scenarios by evaluating ChatEMO on dialogues with personality traits, goals, and situational construals that were not present in the training data to assess its ability to generalize to novel contexts.