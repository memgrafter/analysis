---
ver: rpa2
title: Do Llamas Work in English? On the Latent Language of Multilingual Transformers
arxiv_id: '2402.10588'
source_url: https://arxiv.org/abs/2402.10588
tags:
- layer
- language
- token
- english
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether multilingual transformer models,
  specifically the Llama-2 family, use English as an internal pivot language when
  processing non-English prompts. The authors analyze the internal latent states of
  the models using a technique called the "logit lens," which applies the model's
  output layer prematurely to intermediate layers, revealing next-token distributions
  at each stage.
---

# Do Llamas Work in English? On the Latent Language of Multilingual Transformers

## Quick Facts
- arXiv ID: 2402.10588
- Source URL: https://arxiv.org/abs/2402.10588
- Reference count: 40
- Key result: Multilingual transformers use English as an internal semantic pivot language when processing non-English prompts

## Executive Summary
This paper investigates whether multilingual transformer models, specifically the Llama-2 family, use English as an internal pivot language when processing non-English prompts. The authors analyze the internal latent states of the models using the "logit lens" technique, which reveals next-token distributions at intermediate layers. They observe a consistent three-phase pattern across model sizes and tasks: early layers show high entropy and low language-specific probabilities, middle layers show a sharp rise in English probabilities before declining, and final layers show high probability for the correct next token in the input language. The findings suggest that the abstract "concept space" lies closer to English than to other languages, indicating an English bias in the model's semantic processing.

## Method Summary
The study uses Llama-2 family models (7B, 13B, 70B parameters) with carefully constructed non-English prompts designed to elicit specific single-token continuations. The logit lens technique is applied to intermediate layers, revealing next-token distributions at each stage. Language probabilities are tracked across layers for three tasks (translation, repetition, and cloze), and entropy of next-token distributions is measured. The evolution of language probabilities and token energy through transformer layers is visualized and analyzed to identify patterns in how the models process multilingual input.

## Key Results
- Llama-2 models show a consistent three-phase processing pattern when handling non-English prompts
- Middle layers exhibit a sharp rise in English probabilities before declining, suggesting English as a semantic pivot
- The abstract "concept space" lies closer to English than to other languages in the model's semantic representation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Llama-2 uses English as a semantic pivot language during multilingual processing
- Mechanism: The model maps non-English input embeddings through an abstract "concept space" that is closer to English token embeddings than to non-English ones
- Core assumption: English token embeddings occupy a privileged position in the transformer's semantic space due to training data imbalance
- Evidence anchors:
  - [abstract] "our evidence suggests that the abstract 'concept space' lies closer to English than to other languages"
  - [section] "the abstract 'concept space' lies closer to English than to other languages, which may have important consequences regarding the biases held by multilingual language models"
  - [corpus] Weak - only related papers about multilingual model biases, no direct evidence of semantic pivot mechanisms
- Break condition: If English token embeddings were repositioned to be equidistant from all languages during training

### Mechanism 2
- Claim: Logit lens reveals intermediate layer behavior by prematurely applying the output layer
- Mechanism: By applying the unembedding matrix to intermediate latents, we can observe the model's next-token predictions at each layer, revealing the semantic trajectory
- Core assumption: The unembedding matrix learned during training is meaningful when applied to intermediate states
- Evidence anchors:
  - [abstract] "applying the model's output layer prematurely to intermediate layers, revealing next-token distributions at each stage"
  - [section] "any latent can in principle be turned into a token distribution, by treating it as though it were a final-layer latent"
  - [corpus] Strong - the paper cites Nostalgebraist (2020) for logit lens and Belrose et al. (2023) for tuned lens as established techniques
- Break condition: If intermediate latents are in a representation space that's fundamentally incompatible with the output layer

### Mechanism 3
- Claim: Three-phase processing pattern emerges from transformer architecture and training
- Mechanism: Early layers build feature representations (high entropy, low token energy), middle layers operate in concept space (low entropy, English bias), final layers map concepts to target language tokens (low entropy, high token energy)
- Core assumption: The transformer's residual architecture naturally creates this progression through semantic abstraction
- Evidence anchors:
  - [abstract] "three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an input-language-specific region"
  - [section] "Phase 1 is focused on building up a better feature representation... Phase 2, latents live in an abstract 'concept space'... In Phase 3, the model maps abstract concepts to concrete words/tokens"
  - [corpus] Weak - related papers discuss multilingual capabilities but don't describe this specific three-phase pattern
- Break condition: If the residual connections were modified to skip the concept space phase entirely

## Foundational Learning

- Concept: Logit lens technique
  - Why needed here: Allows us to peek into intermediate transformer states by applying the output layer prematurely
  - Quick check question: What happens if you apply the unembedding matrix to a random vector instead of an intermediate latent?

- Concept: Residual connections in transformers
  - Why needed here: Understanding how latents are incrementally transformed layer by layer is crucial for interpreting the three-phase pattern
  - Quick check question: If residual connections were removed, how would the trajectory through embedding space change?

- Concept: Token energy metric
  - Why needed here: Helps quantify how much of a latent's information is relevant for next-token prediction
  - Quick check question: What would token energy look like if all information in latents was immediately relevant for prediction?

## Architecture Onboarding

- Component map: Input tokens → embedding layer → 80 residual transformer blocks → unembedding matrix → softmax → output probabilities
- Critical path: Token → embedding → 40+ layers of self-attention/FFN → concept space → 10-20 layers → language-specific space → output
- Design tradeoffs: Using English as pivot reduces training complexity but introduces semantic bias; logit lens provides interpretability but may not capture all information in latents
- Failure signatures: If token energy never spikes in final layers, the model isn't properly mapping concepts to language-specific tokens; if English probability doesn't decline after rising, the model may be stuck in concept space
- First 3 experiments:
  1. Apply logit lens to a single layer and verify it produces meaningful token distributions
  2. Measure token energy progression across layers for a simple translation task
  3. Test whether the three-phase pattern persists when training data language balance is modified

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the English bias in concept space change when training Llama-2 on a more balanced multilingual corpus?
- Basis in paper: [inferred] The authors note that "Future work should study how English bias changes when decreasing the dominance of English during training" and suggest applying their method to Llama-2 derivatives with different language mixes.
- Why unresolved: The current study only examines Llama-2, which was trained on an English-dominated corpus (89.70% English). The extent to which the English bias in concept space is a product of the training data distribution remains unknown.
- What evidence would resolve it: Comparing the language probabilities and token energy patterns across layers for Llama-2 models trained on corpora with varying proportions of English vs. non-English data, using the same experimental methodology.

### Open Question 2
- Question: Do the three-phase patterns observed in Llama-2 also occur in autoregressive transformer models with different architectures (e.g., different attention mechanisms, activation functions)?
- Basis in paper: [explicit] The authors state that "the methods outlined in this paper can be straightforwardly applied to other autoregressive transformers" and provide preliminary evidence that Mistral-7B shows similar patterns.
- Why unresolved: While the authors demonstrate consistency across Llama-2 model sizes and provide initial evidence for Mistral-7B, the generalizability of the three-phase pattern to other transformer architectures remains unexplored.
- What evidence would resolve it: Applying the logit lens and energy analysis to a diverse set of autoregressive transformer models (e.g., GPT-2, OPT, BLOOM) and comparing the layer-wise language probabilities and energy patterns across different architectural choices.

### Open Question 3
- Question: What is the structure and nature of the "concept space" in high-dimensional space, and how does it relate to semantic representations in multilingual models?
- Basis in paper: [explicit] The authors propose a conceptual model where transformers operate in "input space," "concept space," and "output space," but acknowledge that "we have limited understanding of the structure of this space in its original high-dimensional form."
- Why unresolved: The current study provides evidence for the existence of a concept space but does not characterize its high-dimensional structure or explore its relationship to established semantic representation theories.
- What evidence would resolve it: Conducting a detailed geometric analysis of latent embeddings in high-dimensional space, including techniques like principal component analysis, clustering, and comparison with semantic similarity measures, to characterize the structure and properties of the concept space.

## Limitations
- The study focuses exclusively on the Llama-2 family, limiting generalizability to other multilingual transformers
- Analysis is constrained to specific non-English languages and particular prompt structures
- Geometric interpretation of latent spaces relies on high-dimensional projections that may oversimplify semantic relationships

## Confidence
- High Confidence: Logit lens technique reliability, three-phase pattern existence in Llama-2, English token embeddings' privileged position
- Medium Confidence: Abstract concept space closer to English, three-phase pattern as semantic abstraction
- Low Confidence: English bias in concept space will have important downstream consequences

## Next Checks
1. Apply the same analysis to other multilingual transformers (e.g., mBERT, XLM-R, GPT-4) to determine if the three-phase pattern and English bias are universal architectural features or specific to Llama-2.

2. Train controlled versions of multilingual transformers with deliberately balanced or English-skewed training data to test whether the English bias in concept space is caused by training data imbalance or is an emergent property of the architecture.

3. Design specific downstream tasks where English bias in intermediate representations could affect performance, then measure whether the strength of this bias correlates with performance degradation in non-English tasks.