---
ver: rpa2
title: Can Transformers Predict Vibrations?
arxiv_id: '2402.10511'
source_url: https://arxiv.org/abs/2402.10511
tags:
- forecasting
- input
- vibration
- transformer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Resoformer, a transformer-based model for
  predicting torsional resonance in electric vehicles (EVs). Torsional resonance,
  caused by vibrations between the motor and tires, can put excessive loads on the
  vehicle's drive shaft.
---

# Can Transformers Predict Vibrations?

## Quick Facts
- arXiv ID: 2402.10511
- Source URL: https://arxiv.org/abs/2402.10511
- Reference count: 40
- Primary result: Resoformer, a transformer-based model, predicts torsional resonance in EV drive shafts with state-of-the-art accuracy on simulated data.

## Executive Summary
This paper introduces Resoformer, a transformer-based model designed to predict torsional resonance in electric vehicles. Torsional resonance, caused by vibrations between the motor and tires, can create excessive loads on the drive shaft. The model uses time-series data of motor rotation speed to forecast vibration amplitudes, enabling early detection and control measures. By combining recurrent neural networks (RNNs) and temporal convolutional networks (TCNs) within a transformer architecture, Resoformer improves long-term vibration forecasting accuracy compared to traditional approaches.

## Method Summary
Resoformer combines parallel RNN and TCN streams to capture both long-term temporal dependencies and short-term local patterns in vibration data. The model processes motor rotation speed time series through position encoding, then simultaneously through RNN and TCN layers. Co-attention mechanisms between these feature streams allow the model to focus on the most relevant temporal patterns, while gating mechanisms adaptively suppress irrelevant components. The architecture is evaluated on the VIBES dataset containing 2,600 simulated vibration sequences, with Resoformer achieving state-of-the-art results using mean absolute error (MAE) as the primary metric.

## Key Results
- Resoformer achieves state-of-the-art performance on the VIBES dataset of simulated EV vibration sequences
- While traditional transformer architectures show low performance for torsional resonance forecasting, Resoformer's combination of RNN and TCN features improves accuracy
- The best results were achieved by the TCN model, with Resoformer outperforming LSTM but falling short of TCN in long-term predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The parallel combination of RNN and TCN allows the model to capture both long-term temporal dependencies and short-term local patterns in vibration data.
- Mechanism: RNN layers process sequential data to maintain context over long time horizons, while TCN layers extract local temporal features through stacked dilated convolutions. The model fuses these two feature streams before attention processing.
- Core assumption: Both long-term dependencies and short-term local patterns are important for accurately predicting torsional resonance vibrations.
- Evidence anchors:
  - [abstract]: "Resoformer combines recurrent neural networks (RNNs) and temporal convolutional networks (TCNs) within a transformer architecture to improve the accuracy of long-term vibration forecasting."
  - [section 3.2]: "The RNN estimates an effective distribution P(x0:n) of the sequences of data points"
  - [section 3.3]: "The TCN consists of one-dimensional stacked convolutional layers that enable the efficient and effective processing of temporal data"
- Break condition: If the vibration patterns are purely local or purely long-term, the dual-stream approach may add unnecessary complexity without performance gains.

### Mechanism 2
- Claim: Attention mechanisms between RNN and TCN features enable the model to focus on the most relevant temporal patterns for vibration prediction.
- Mechanism: After extracting features from both RNN and TCN streams, the model uses co-attention to compute attention scores between these different feature representations, allowing the model to dynamically weigh their contributions.
- Core assumption: The interaction between long-term and short-term features contains discriminative information for vibration forecasting.
- Evidence anchors:
  - [abstract]: "By calculating the attention between recursive and convolutional features extracted from the measured data points, Resoformer improves the accuracy of vibration forecasting."
  - [section 4.2]: "Given input representations X, the Transformer components undergo a sternward layer normalization (LN), and the Transformer output is denoted by O"
- Break condition: If one feature stream consistently dominates the other, attention may simply learn to ignore the weaker stream rather than meaningfully combine them.

### Mechanism 3
- Claim: Gating mechanisms help the model adaptively suppress irrelevant components and focus on useful features for vibration prediction.
- Mechanism: GLU (Gated Linear Units) are applied after attention processing, allowing the model to control information flow through element-wise multiplication with learned gates.
- Core assumption: Not all features extracted by RNN, TCN, and attention are equally useful for predicting torsional resonance.
- Evidence anchors:
  - [section 3.5]: "GLUs provide our model with the flexibility to suppress any parts of the architecture that may not be useful when capturing the relevant features."
  - [section 4.3]: "In Resoformer, we utilize multi-head pairwise attention to handle pairs of sequence data. However, this approach may not fully capture the significance of individual features."
- Break condition: If gating becomes too aggressive, it may discard useful information, or if too permissive, it may allow noise to propagate through the network.

## Foundational Learning

- Concept: Torsional resonance in EV drive shafts
  - Why needed here: The model predicts a specific physical phenomenon where vibrations between motor and tires create destructive loads on the drive shaft.
  - Quick check question: What causes torsional resonance in electric vehicle drive trains?

- Concept: Time series forecasting with quantile regression
  - Why needed here: The model predicts not just point estimates but distributions of vibration amplitudes at specified quantiles, useful for safety-critical control systems.
  - Quick check question: How does quantile regression differ from standard regression in time series forecasting?

- Concept: Transformer attention mechanisms
  - Why needed here: Attention allows the model to capture long-range dependencies in vibration patterns that traditional RNNs struggle with.
  - Quick check question: What advantage does self-attention provide over recurrent layers for sequence modeling?

## Architecture Onboarding

- Component map: Input → Position Encoding → Parallel RNN/TCN streams → Co-attention Transformer → Gating → MLP Output
- Critical path: Data flows through both RNN and TCN simultaneously, their outputs are combined via attention, then gated before final prediction
- Design tradeoffs: Parallel RNN/TCN adds complexity but improves performance; attention layers increase computation but capture long-range dependencies; gating adds flexibility but requires careful tuning
- Failure signatures: Poor long-term predictions indicate attention mechanism issues; inability to capture local patterns suggests TCN problems; vanishing gradients may indicate gating issues
- First 3 experiments:
  1. Train Resoformer on VIBES with input length L=192, output T=96, using MAE loss to establish baseline performance
  2. Compare Resoformer performance against pure LSTM and pure TCN baselines on the same dataset
  3. Vary prediction length T (96, 192, 336, 720) to evaluate long-term forecasting capabilities and identify performance degradation points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Resoformer's performance compare to TCN when predicting torsional resonance in real-world EV driving scenarios, not just simulated data?
- Basis in paper: [explicit] The paper states "While traditional transformer architectures show low performance in forecasting torsional resonance waves, our findings indicate that combining recurrent neural network and temporal convolutional network using the transformer architecture improves the accuracy of long-term vibration forecasting." However, it also notes that "the best results were achieved by the TCN model, with Resoformer outperforming LSTM but falling short of TCN."
- Why unresolved: The paper's experiments were conducted on a simulated dataset, and real-world conditions may differ significantly, affecting model performance.
- What evidence would resolve it: Real-world testing of Resoformer and TCN on EV vibration data collected from actual driving conditions, with performance metrics compared directly.

### Open Question 2
- Question: Can the Resoformer model be adapted to predict other types of vehicle vibrations or mechanical resonances beyond torsional resonance in EVs?
- Basis in paper: [inferred] The paper focuses specifically on torsional resonance in EVs but does not explore the model's applicability to other vibration types or vehicle systems.
- Why unresolved: The model's architecture and training were tailored to torsional resonance prediction, and its generalization capabilities to other vibration types are unknown.
- What evidence would resolve it: Experiments applying Resoformer to predict different types of vehicle vibrations (e.g., tire vibrations, suspension oscillations) and comparing its performance to specialized models for those vibration types.

### Open Question 3
- Question: How does the inclusion of additional sensor data (e.g., accelerometer, gyroscope) impact the Resoformer's accuracy in predicting torsional resonance compared to using only motor rotation speed?
- Basis in paper: [explicit] The paper states "Resoformer utilizes time-series of the motor rotation speed as input and predicts the amplitude of torsional vibration at a specified quantile occurring in the shaft after the input series."
- Why unresolved: The model was trained and evaluated using only motor rotation speed data, and the potential benefits of incorporating other sensor data are not explored.
- What evidence would resolve it: Training and testing Resoformer with additional sensor data inputs and comparing its performance to the baseline model using only motor rotation speed data.

## Limitations

- The evaluation is based on simulated rather than real-world EV data, potentially limiting generalizability to actual vehicle conditions
- The VIBES dataset generation parameters are partially specified but not fully detailed, affecting reproducibility
- The comparison primarily emphasizes Resoformer versus LSTM and TCN baselines, with limited analysis of pure transformer performance

## Confidence

**High confidence** in the architectural contribution: The combination of RNN, TCN, and transformer components is clearly specified and implemented according to the described methodology.

**Medium confidence** in performance claims: While the paper reports state-of-the-art results on the VIBES dataset, the simulated nature of the data and limited comparison to alternative approaches reduce confidence in real-world applicability.

**Low confidence** in the claimed superiority over pure transformers: The paper states that "traditional transformer architectures show low performance" but does not provide quantitative comparisons or detailed analysis of why transformers struggle with this specific task.

## Next Checks

1. **Real-world validation**: Test Resoformer on actual EV drive train vibration data to verify that simulated performance translates to physical systems.

2. **Architectural ablation study**: Systematically remove components (RNN, TCN, attention, gating) to quantify their individual contributions and validate the claimed mechanisms.

3. **Cross-simulation comparison**: Evaluate Resoformer on vibration datasets generated with different physical parameters and road conditions to assess robustness across operating scenarios.