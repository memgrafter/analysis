---
ver: rpa2
title: 'MemeMQA: Multimodal Question Answering for Memes via Rationale-Based Inferencing'
arxiv_id: '2405.11215'
source_url: https://arxiv.org/abs/2405.11215
tags:
- meme
- answer
- party
- because
- democratic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MemeMQA introduces a multimodal question-answering task for memes,
  aiming to generate accurate answers and coherent explanations. The authors propose
  ARSENAL, a two-stage framework leveraging LLM-generated rationales to address this
  task.
---

# MemeMQA: Multimodal Question Answering for Memes via Rationale-Based Inferencing

## Quick Facts
- **arXiv ID**: 2405.11215
- **Source URL**: https://arxiv.org/abs/2405.11215
- **Reference count**: 40
- **Primary result**: ARSENAL achieves ~18% higher answer prediction accuracy and superior text generation quality on MemeMQA task.

## Executive Summary
MemeMQA introduces a multimodal question-answering task for memes, aiming to generate accurate answers and coherent explanations. The authors propose ARSENAL, a two-stage framework leveraging LLM-generated rationales to address this task. They curate MemeMQACorpus, a dataset of 1,880 questions and answer-explanation pairs for 1,122 memes. ARSENAL outperforms competitive baselines, achieving ~18% higher answer prediction accuracy and superior text generation quality across various metrics. The model's robustness is analyzed through diversified question sets, confounding-based evaluations, and modality-specific assessments.

## Method Summary
The ARSENAL framework consists of three main components: a Rationale Generation Module using LLaVA-7B to produce generic (offline) and answer-specific (online) rationales, an Answer Prediction Module using a two-stage MM-CoT model with DETR visual encoder and T5-large decoder, and an Explanation Generation Module using fine-tuned T5-large to distill answer-specific rationale into concise explanations. The model processes meme images, OCR-extracted text, questions, and multiple-choice options to generate answers with "BECAUSE" explanations. The approach leverages LLM-generated reasoning chains to improve multimodal understanding and explanation quality.

## Key Results
- ARSENAL achieves ~18% higher answer prediction accuracy compared to competitive baselines.
- Superior text generation quality across BLEU-1, BLEU-4, ROUGE-L, METEOR, CHRF, and BERTScore metrics.
- Demonstrates robustness to diversified question formulations with smaller performance drops compared to baselines.
- Shows effectiveness in handling multimodal reasoning through thought chains and OCR integration.

## Why This Works (Mechanism)

### Mechanism 1
Using LLM-generated rationales (both generic and answer-specific) as intermediate steps in a two-stage pipeline improves multimodal reasoning and explanation quality for meme question answering. The first stage uses a multimodal CoT model trained to generate an intermediate rationale (capturing semantic context) and predict the answer. The second stage uses an LLM to generate an answer-specific rationale and a T5 model to distill it into a concise explanation. This design leverages the reasoning strength of LLMs while controlling for generation quality.

### Mechanism 2
Incorporating OCR-extracted text improves multimodal model performance on memes, especially for understanding complex visual-linguistic incongruities. OCR text is concatenated with image embeddings in the multimodal encoder, allowing models to ground visual features with textual cues. This is particularly useful for memes where the text and image meanings are mismatched or require contextual interpretation.

### Mechanism 3
Prompt diversification via LLM rephrasing improves robustness to varied question formulations in real-world use. Original structured questions are rewritten by Llama-2-7b-chat into five variations; one is randomly selected per sample. This emulates free-form questioning and reduces overfitting to a fixed schema.

## Foundational Learning

- **Concept: Visual-linguistic incongruity in memes**
  - Why needed here: Memes often combine text and image in mismatched or sarcastic ways; models must detect and reconcile these to infer correct answers.
  - Quick check question: How does a meme like "image of a smiling politician + text saying 'worst leader ever'" require understanding beyond literal interpretation?

- **Concept: Multimodal reasoning and attention fusion**
  - Why needed here: The architecture fuses image embeddings with OCR text via gated cross-attention; understanding this fusion is key to debugging multimodal models.
  - Quick check question: What happens to the fused representation when λ in the gated attention layer is close to 0 vs. close to 1?

- **Concept: Prompt engineering for multimodal LLMs**
  - Why needed here: ARSENAL relies on carefully structured prompts (e.g., "Explain this meme in detail", "How is [answer] [rephrased question]?") to elicit rationales.
  - Quick check question: Why is removing the first two words from the question ("Who is" → "How is") part of the answer-specific prompt?

## Architecture Onboarding

- **Component map**: Input (meme image, OCR text, question, options) -> Rationale Generation Module (LLaVA-7B) -> Answer Prediction Module (MM-CoT with DETR+T5) -> Explanation Generation Module (T5-large) -> Output (Answer + "BECAUSE" + explanation)

- **Critical path**:
  1. OCR extraction (preprocessed).
  2. Generic rationale generation (offline preprocessing).
  3. Answer prediction via MM-CoT (with generic rationale as context).
  4. Answer-specific rationale generation (online, post-answer prediction).
  5. Explanation distillation via T5.

- **Design tradeoffs**:
  - Using two separate fine-tuning stages (answer prediction vs. explanation) trades complexity for modularity and interpretability.
  - Offline generic rationale generation reduces runtime but may miss dynamic context.
  - Fine-tuning vs. zero-shot LLM usage balances accuracy vs. cost and adaptability.

- **Failure signatures**:
  - Low BLEU/ROUGE scores in explanation module → poor distillation or noisy rationale.
  - Accuracy drop in answer module → insufficient multimodal fusion or flawed rationale context.
  - Inconsistent outputs across runs → stochasticity in LLM rationale generation or training instability.

- **First 3 experiments**:
  1. Validate OCR quality by comparing Tesseract vs. Google OCR on a subset of memes.
  2. Run ARSENAL with generic rationale only (skip answer-specific rationale) to measure impact of second-stage reasoning.
  3. Evaluate ARSENAL on diversified question set to confirm robustness gain over baselines.

## Open Questions the Paper Calls Out
- The paper identifies multimodal bias in LLaVA as a limitation, where the model tends to anchor its explanations across multiple modalities without clear emphasis.
- Errors in ARSENAL's outputs due to semantically inconsistent and factually incorrect rationales generated by LLaVA are noted as a concern.

## Limitations
- Reliance on LLM-generated rationales may introduce factual errors or hallucinations that propagate through the pipeline.
- The MemeMQACorpus dataset contains only 1,122 memes with 1,880 QA pairs, which may not capture full real-world meme diversity.
- Offline rationale generation may miss context-specific information important for accurate reasoning.

## Confidence
- **High confidence**: The ARSENAL framework architecture and two-stage approach are well-specified and technically sound. The reported accuracy improvements (~18%) and text generation metrics are credible.
- **Medium confidence**: Claims about robustness to diversified questions are supported by the data but rely on a specific set of five rephrasings. Real-world generalizability remains uncertain.
- **Low confidence**: Evaluation of robustness to confounding variables is promising but based on synthetic modifications rather than naturally occurring variations.

## Next Checks
1. Conduct a human evaluation of LLaVA-7B-generated rationales for factual accuracy, relevance, and coherence before they enter the answer prediction pipeline.
2. Test ARSENAL on established multimodal QA benchmarks (e.g., VQA, Hateful Memes) to assess performance beyond the MemeMQACorpus domain.
3. Compare performance between offline generic rationale generation and online generation to quantify the impact of missing dynamic context.