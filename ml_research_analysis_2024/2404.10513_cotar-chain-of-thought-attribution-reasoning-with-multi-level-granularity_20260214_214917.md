---
ver: rpa2
title: 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity'
arxiv_id: '2404.10513'
source_url: https://arxiv.org/abs/2404.10513
tags:
- answer
- citation
- span
- gpt-4
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CoTAR, a Chain-of-Thought (CoT) reasoning
  method that improves attribution accuracy in question-answering (QA) tasks. The
  approach focuses on generating attribution-centric outputs by incorporating CoT
  guidance at three levels of granularity: span, sentence, and passage.'
---

# CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity

## Quick Facts
- **arXiv ID**: 2404.10513
- **Source URL**: https://arxiv.org/abs/2404.10513
- **Reference count**: 28
- **Primary result**: CoTAR improves attribution accuracy in QA by guiding models to generate span/sentence/passage citations through Chain-of-Thought reasoning, with finetuned smaller models sometimes outperforming GPT-4.

## Executive Summary
This paper introduces CoTAR, a Chain-of-Thought (CoT) reasoning method that improves attribution accuracy in question-answering (QA) tasks. The approach focuses on generating attribution-centric outputs by incorporating CoT guidance at three levels of granularity: span, sentence, and passage. Evaluations on two datasets using GPT-4 show that CoTAR enhances both answer quality and citation accuracy. Additionally, finetuning smaller models (Mistral 7B and Flan-T5) with CoTAR improves their performance, with some models outperforming GPT-4 in specific metrics. The results demonstrate the effectiveness of CoTAR in improving attribution quality and enabling smaller models to compete with larger ones.

## Method Summary
CoTAR applies Chain-of-Thought reasoning at three granularity levels (span, sentence, passage) to guide attribution-focused answer generation. The method requires identifying relevant portions of input passages before generating answers, with the CoT step aligned to the desired citation level. The approach was evaluated using GPT-4 on QuoteSUM and MS MARCO datasets, and smaller models were finetuned with Lora using CoTAR examples. Citation quality was measured using SEM-F1, ALCE F1, CSCA, and DOC F1 metrics, while answer quality used ROUGE-L, BERTScore, and HEM1.

## Key Results
- CoTAR significantly improves citation accuracy across all three granularity levels compared to baseline approaches
- Smaller models (Mistral 7B, Flan-T5) finetuned with CoTAR achieve competitive performance and sometimes outperform GPT-4 on specific metrics
- Aligning CoT reasoning granularity with citation level yields optimal performance, with span CoT best for span citations and sentence CoT best for sentence citations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoT reasoning guides the model to extract relevant spans/sentences/passages before generating the answer, improving citation accuracy.
- Mechanism: The model is instructed to first identify which portions of the input are most relevant to the question and cite them in the appropriate granularity. This focused reasoning step reduces the chance of missing relevant information or citing irrelevant passages.
- Core assumption: The model can effectively perform the CoT reasoning step and use the identified attributions to generate a more accurate and faithful answer.
- Evidence anchors:
  - [abstract] "This approach focuses the reasoning process on generating an attribution-centric output."
  - [section] "The process involves identifying the most crucial aspects of the given context for answering the question, by incorporating direct citations to the referenced parts."
- Break condition: If the model fails to correctly identify the relevant portions during the CoT step, or if the CoT step introduces additional noise or confusion, the citation accuracy may not improve.

### Mechanism 2
- Claim: Finetuning smaller models with CoTAR improves their performance and allows them to compete with or even outperform GPT-4 in some cases.
- Mechanism: The smaller models are trained on examples of how to use the CoTAR method to generate answers with appropriate attributions. This finetuning adapts the models to the specific task and reasoning approach.
- Core assumption: The finetuned models can effectively learn the CoTAR method and apply it to new questions and passages.
- Evidence anchors:
  - [abstract] "the combination of our method with finetuning enhances the response and attribution accuracy of two smaller LLMs, showing their potential to outperform GPT-4 in some cases."
  - [section] "In addition to fewshot instruction, we also finetune various models using our method, resulting in models that are competitive with and even outperform GPT-4 in some cases."
- Break condition: If the finetuning data is insufficient, noisy, or not representative of the target task, the smaller models may not effectively learn the CoTAR method.

### Mechanism 3
- Claim: Using the appropriate CoT method for each citation level (span, sentence, passage) yields the best results.
- Mechanism: The model is guided to reason at the same granularity level as the desired citation. For example, when citing specific spans, the model first identifies the relevant spans before generating the answer.
- Core assumption: Aligning the CoT reasoning granularity with the citation level helps the model focus its attention and produce more accurate attributions.
- Evidence anchors:
  - [section] "We observe that the GPT-4 model performed best in the span level when using the span CoT method, and the same applies for the sentence level with sentence CoT."
  - [section] "In the answer metrics, the differences between the various CoT methods and the standard run (shown as 'None') are not significant, aside from the sentence level, where the sentence CoT method produces better scores across all metrics."
- Break condition: If the model is not capable of effectively reasoning at the desired granularity level, or if the CoT step is not well-aligned with the citation task, the performance may not improve.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT is used to guide the model's reasoning process and improve its ability to generate accurate attributions.
  - Quick check question: What is the main idea behind using CoT in the CoTAR method?

- Concept: Attribution levels (span, sentence, passage)
  - Why needed here: The CoTAR method operates at three different levels of granularity for attributions, and understanding these levels is crucial for implementing and evaluating the method.
  - Quick check question: What are the three levels of attribution used in the CoTAR method, and how do they differ in granularity?

- Concept: Finetuning
  - Why needed here: Finetuning is used to adapt smaller models to the CoTAR method and improve their performance on the attribution task.
  - Quick check question: How does finetuning help smaller models learn and apply the CoTAR method?

## Architecture Onboarding

- Component map: Question -> CoT reasoning (identify relevant spans/sentences/passages) -> Answer generation (using identified attributions) -> Output
- Critical path: Question → CoT reasoning → Answer generation → Output
- Design tradeoffs:
  - Granularity vs. efficiency: Higher granularity (span) may yield more accurate attributions but could be more computationally expensive.
  - Model size vs. performance: Smaller models may be less capable but can be more efficient when finetuned with CoTAR.
- Failure signatures:
  - Poor citation accuracy: The model fails to correctly identify and cite relevant portions of the input.
  - Hallucination: The model generates incorrect or unsupported information in the answer.
  - Inefficiency: The CoT step takes too long or the overall process is too slow for practical use.
- First 3 experiments:
  1. Compare the performance of GPT-4 with and without the CoTAR method on a sample of questions from the QuoteSUM dataset.
  2. Finetune a smaller model (e.g., Mistral 7B) with the CoTAR method and evaluate its performance on the QuoteSUM dataset.
  3. Compare the performance of the finetuned smaller model with GPT-4 on a set of questions from the MSMARCO dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CoTAR approach perform on datasets with different citation styles or formats, such as those requiring inline citations or different citation conventions?
- Basis in paper: [inferred] The paper mentions that CoTAR was evaluated on two datasets with specific citation formats, but does not explore its performance on datasets with varying citation styles.
- Why unresolved: The study focused on datasets with specific citation formats, and the performance of CoTAR on other citation styles is not investigated.
- What evidence would resolve it: Evaluating CoTAR on datasets with different citation styles and comparing its performance to the results presented in the paper would provide insights into its adaptability and generalizability.

### Open Question 2
- Question: How does the choice of the Chain-of-Thought (CoT) method (span, sentence, or passage) impact the model's performance when dealing with complex questions that require multi-step reasoning or integrating information from multiple sources?
- Basis in paper: [explicit] The paper presents the results of using different CoT methods on the evaluated datasets, but does not specifically investigate their performance on complex questions.
- Why unresolved: The study focused on the overall performance of the CoT methods, and their effectiveness in handling complex questions is not explicitly addressed.
- What evidence would resolve it: Designing and evaluating the CoTAR approach on datasets with complex questions that require multi-step reasoning or integrating information from multiple sources would provide insights into the strengths and limitations of each CoT method.

### Open Question 3
- Question: How does the performance of CoTAR compare to other attribution-based methods that use different techniques, such as reinforcement learning or attention mechanisms, for improving citation accuracy?
- Basis in paper: [inferred] The paper compares CoTAR to a baseline without CoT, but does not explore its performance relative to other attribution-based methods that use different techniques.
- Why unresolved: The study focused on evaluating CoTAR with CoT methods, and its performance compared to other attribution-based approaches is not investigated.
- What evidence would resolve it: Conducting a comparative study between CoTAR and other attribution-based methods that use different techniques, such as reinforcement learning or attention mechanisms, would provide insights into the relative strengths and weaknesses of each approach.

## Limitations

- Limited empirical validation across diverse domains: Evaluation restricted to Wikipedia-style passages from QuoteSUM and MS MARCO datasets
- Algorithm 1 implementation gaps: Exact implementation details for span attribution generation are not specified
- Evaluation metric gaps: Missing factual consistency measurements between cited passages and generated answers

## Confidence

**High confidence** in the claim that CoTAR improves citation accuracy: Multiple quantitative results show consistent improvements across all three granularity levels with statistical significance.

**Medium confidence** in smaller models outperforming GPT-4: While results show competitive performance, sample size is limited to specific metrics on two datasets.

**Medium confidence** in the mechanism of aligned CoT granularity: Evidence shows performance differences when CoT methods match citation levels, but alternative explanations haven't been fully ruled out.

## Next Checks

**Validation Check 1**: Conduct ablation studies removing the CoT reasoning step entirely versus removing only the attribution generation component to isolate the specific contribution of guided reasoning versus citation requirements.

**Validation Check 2**: Test CoTAR performance on adversarial examples where relevant information is distributed across multiple passages or deliberately obscured.

**Validation Check 3**: Implement cross-domain evaluation using scientific abstracts, legal documents, and technical documentation to measure performance degradation and identify domain-specific limitations.