---
ver: rpa2
title: 'Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large Language
  Models'
arxiv_id: '2412.15265'
source_url: https://arxiv.org/abs/2412.15265
tags:
- safety
- chinese
- knowledge
- language
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Chinese SafetyQA introduces a short-form factuality benchmark for
  evaluating the safety knowledge of large language models in Chinese contexts, covering
  seven primary categories with over 2,000 high-quality examples. The benchmark addresses
  the gap in systematic safety knowledge evaluation by providing challenging, diverse,
  and region-specific questions on legal, ethical, and policy-related topics.
---

# Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large Language Models

## Quick Facts
- **arXiv ID**: 2412.15265
- **Source URL**: https://arxiv.org/abs/2412.15265
- **Reference count**: 40
- **Primary result**: Most LLMs exhibit significant deficiencies in safety factuality, with only three models achieving passing scores above 60%

## Executive Summary
Chinese SafetyQA introduces a novel benchmark for evaluating the safety knowledge of large language models in Chinese contexts. The benchmark addresses a critical gap in systematic safety knowledge evaluation by providing over 2,000 high-quality, short-form question-answer pairs across seven primary safety categories. Through comprehensive evaluation of 30+ models, the study reveals that while most LLMs demonstrate significant deficiencies in safety factuality, certain patterns emerge including overconfidence in responses and a "Tip-of-the-Tongue" phenomenon where multiple-choice formats yield higher accuracy than open-ended questions.

## Method Summary
The Chinese SafetyQA benchmark employs a short-form question-answer format to precisely evaluate factuality in safety domains, using concise questions with definitive answers to minimize contextual interpretation. The methodology involves GPT-4o for question generation and validation, with RAG verification for accuracy and safety rule verification for harmlessness. The evaluation metrics include Correct (CO), Not Attempted (NA), Incorrect (IN), Correct Given Attempted (CGA), and F-score. The benchmark covers seven safety categories including rumor detection, legal compliance, health, hate speech, discrimination, ethics, and theoretical safety knowledge.

## Key Results
- Only three models achieved passing scores above 60% on the Chinese SafetyQA benchmark
- Most LLMs showed overconfidence, with high stated confidence but low F-scores
- The "Tip-of-the-Tongue" phenomenon was observed, with models performing better on MCQ than open-ended questions
- RAG improved safety factuality but less significantly than in general knowledge domains
- Self-reflection strategies showed minimal benefits (under 5% improvement) for safety factuality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Short-form question-answer format reduces ambiguity and enables precise evaluation of factuality in safety domains
- Mechanism: Concise questions with definitive answers minimize contextual interpretation, while MCQ format further constrains the response space, allowing for clear correctness judgments
- Core assumption: Safety knowledge can be effectively assessed through short, factual questions with unambiguous answers
- Evidence anchors:
  - [abstract] "Chinese SafetyQA possesses the following essential features: Chinese, Diverse, High-quality, Static, Easy-to-evaluate, Safety-related, Harmless"
  - [section 2.1] "The dataset features concise questions and standardized answers, minimizing the input and output tokens required for GPT evaluations"
  - [section 3.3.2] "This indicates that the improved accuracy of these LLMs is not simply a result of the reduced search space afforded by MCQs, but rather due to their ability to produce certain and definitive results"
- Break condition: If safety knowledge requires complex contextual understanding that cannot be captured in short-form questions

### Mechanism 2
- Claim: Retrieval-Augmented Generation (RAG) improves safety factuality by providing access to accurate, up-to-date information
- Mechanism: RAG supplements the model's parametric knowledge with external information retrieval, reducing hallucinations and providing authoritative references for safety-related queries
- Core assumption: Safety knowledge has reliable external sources that can be retrieved and integrated into model responses
- Evidence anchors:
  - [abstract] "Retrieval-Augmented Generation (RAG) enhances safety factuality"
  - [section 3.3.4] "We find that RAG benefits the safety factuality of LLMs, although the improvement is less significant compared to the general knowledge domain"
  - [section 2.3] "RAG will be utilized to verify the accuracy of the standard answers in our Chinese SafetyQA dataset"
- Break condition: If external retrieval sources are unreliable or if RAG introduces latency that affects practical deployment

### Mechanism 3
- Claim: Self-reflection strategies do not significantly improve safety factuality because they cannot correct knowledge errors in training data
- Mechanism: Chain-of-thought reasoning helps with logical consistency but cannot identify factual errors that originated during pretraining
- Core assumption: Knowledge-based questions rely primarily on memorized facts rather than reasoning ability
- Evidence anchors:
  - [abstract] "self-reflection strategies show minimal benefits for enhancing factual accuracy in safety-related responses"
  - [section 3.3.3] "These issues arise because LLMs generate responses based on statistical patterns in their training data. Knowledge-based questions rely more on the model's breadth and comprehension than on its reasoning abilities"
  - [section 3.3.3] "If the training data contains factual errors, the model cannot identify them through chain-of-thought (COT) and tends to retain incorrect answers"
- Break condition: If safety knowledge requires complex reasoning rather than factual recall

## Foundational Learning

- Concept: Safety knowledge evaluation in specific cultural and legal contexts
  - Why needed here: The benchmark focuses on Chinese safety knowledge including laws, policies, and cultural norms that differ from Western contexts
  - Quick check question: What are the seven primary safety categories covered in Chinese SafetyQA?

- Concept: Factuality vs. safety alignment distinction
  - Why needed here: The paper distinguishes between factual accuracy (correct information) and safety alignment (harmful content avoidance), which are different evaluation dimensions
  - Quick check question: How does the paper differentiate between factuality evaluation and traditional safety red-teaming?

- Concept: Tip-of-the-Tongue phenomenon in LLMs
  - Why needed here: Models perform better on MCQ than open-ended questions for the same content, suggesting knowledge exists but cannot be accessed directly
  - Quick check question: What behavioral pattern describes when models know answers but cannot retrieve them in open-ended format?

## Architecture Onboarding

- Component map: Data collection -> RAG verification -> Safety rule verification -> Difficulty filtering -> Human expert verification -> Model evaluation
- Critical path: Benchmark creation (data collection and verification) -> Model evaluation (inference and scoring) -> Analysis (confidence assessment and RAG/self-reflection comparison)
- Design tradeoffs: Short-form questions provide precision but may miss complex safety scenarios; MCQ format improves accuracy but may not reflect real-world open-ended queries
- Failure signatures: Overconfidence without accuracy (high stated confidence but low F-score), Tip-of-the-Tongue pattern (better MCQ than QA performance), Knowledge errors persisting despite RAG
- First 3 experiments:
  1. Run baseline evaluation on 3-5 models using Chinese SafetyQA to establish performance distribution
  2. Implement RAG comparison (active vs passive) on subset of questions to measure effectiveness
  3. Test self-reflection on sample questions to confirm minimal impact on factuality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed "Tip-of-the-Tongue" phenomenon in safety knowledge persist across different languages and cultural contexts beyond Chinese?
- Basis in paper: [inferred] The paper demonstrates this phenomenon specifically in Chinese SafetyQA but suggests it may be due to knowledge conflicts in training data rather than language-specific factors.
- Why unresolved: The study only evaluates Chinese language models, leaving open whether this phenomenon is universal across linguistic and cultural contexts.
- What evidence would resolve it: Cross-linguistic experiments comparing safety knowledge factuality across models trained on different language datasets and cultural contexts.

### Open Question 2
- Question: What specific types of knowledge conflicts in training data most strongly contribute to the "Tip-of-the-Tongue" phenomenon in safety-related questions?
- Basis in paper: [explicit] The paper mentions that "knowledge conflicts within the pre-training data" impede models' ability to generate certain answers for safety knowledge.
- Why unresolved: The paper identifies knowledge conflicts as a cause but doesn't specify which types (e.g., contradictory regulations, ambiguous ethical guidelines, conflicting cultural norms) have the strongest impact.
- What evidence would resolve it: Detailed analysis of training data sources showing specific conflict patterns correlated with incorrect safety responses.

### Open Question 3
- Question: Why does self-reflection strategy show minimal improvement for safety factuality when it has demonstrated effectiveness for general knowledge tasks?
- Basis in paper: [explicit] The paper states "self-reflection resulted in minimal improvements (under 5%) across all evaluated LLMs" for safety knowledge.
- Why unresolved: The paper suggests knowledge errors in training data as a reason, but doesn't explore whether safety knowledge has unique characteristics that make self-reflection less effective.
- What evidence would resolve it: Comparative analysis of self-reflection effectiveness on safety vs. general knowledge tasks, examining whether safety knowledge requires different reasoning approaches.

## Limitations

- **Dataset Availability**: The Chinese SafetyQA benchmark dataset is not publicly available, requiring either direct access from authors or recreation following the described methodology.
- **Cultural and Linguistic Specificity**: The benchmark's focus on Chinese safety knowledge creates uncertainty about generalizability to other languages and cultures.
- **Evaluation Prompt Design**: The paper provides limited detail on the exact evaluation prompts and instructions used for model assessment.

## Confidence

**High Confidence (Likelihood >80%)**:
- Models exhibit significant deficiencies in safety factuality across all evaluated categories
- The Tip-of-the-Tongue phenomenon (better MCQ than open-ended performance) is a real behavioral pattern in LLMs
- RAG provides measurable improvements to safety factuality, though less pronounced than in general knowledge domains

**Medium Confidence (Likelihood 50-80%)**:
- Self-reflection strategies show minimal benefits for safety factuality improvements
- Overconfidence without accuracy represents a systematic issue in LLM safety evaluations
- The seven safety categories adequately capture the most critical safety knowledge domains

## Next Checks

1. **Dataset Recreation Validation**: Create a smaller validation set (100-200 examples) following the paper's methodology and evaluate 3-5 models to verify the reported performance distribution and category-level weaknesses.

2. **Prompt Engineering Sensitivity Test**: Systematically vary evaluation prompts (temperature, instruction clarity, safety disclaimers) to determine the stability of results and identify whether observed patterns persist across prompt variations.

3. **Cross-Cultural Generalization Study**: Translate a subset of Chinese SafetyQA questions into English and other languages, then evaluate multilingual models to assess the extent to which safety factuality results depend on linguistic and cultural specificity versus universal safety knowledge patterns.