---
ver: rpa2
title: 'Rethinking Bradley-Terry Models in Preference-Based Reward Modeling: Foundations,
  Theory, and Alternatives'
arxiv_id: '2411.04991'
source_url: https://arxiv.org/abs/2411.04991
tags:
- reward
- annotation
- annotations
- training
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of Bradley-Terry (BT) models in preference-based
  reward modeling for LLM alignment. While BT models have been widely adopted, their
  theoretical foundations and practical necessity remain unclear.
---

# Rethinking Bradley-Terry Models in Preference-Based Reward Modeling: Foundations, Theory, and Alternatives

## Quick Facts
- arXiv ID: 2411.04991
- Source URL: https://arxiv.org/abs/2411.04991
- Reference count: 15
- Primary result: Classification-based reward models offer greater flexibility and robustness compared to BT models while achieving comparable or better performance

## Executive Summary
This paper challenges the necessity of Bradley-Terry models in preference-based reward modeling for LLM alignment. While BT models have been widely adopted for their ability to predict comparison probabilities, the authors argue that reward models only need to preserve correct ranking predictions through monotonic transformations of true rewards. They propose order consistency as a core objective that can be achieved through both BT models and simpler classification-based methods. The authors provide asymptotic theory for neural network-based BT regression and empirically evaluate BT versus classification approaches across diverse experimental setups, demonstrating that classification models offer comparable or superior performance with greater flexibility.

## Method Summary
The authors compare Bradley-Terry (BT) and classification-based reward models using preference annotations from 6 base LLMs (Gemma2b, Gemma7b, LLaMA3-8b and their SFT-ed versions) across 2 datasets. They generate 10 responses per training prompt and 500 per test prompt, creating embeddings with Gemma2b. Three reward models are trained: BT-MLP (Siamese structure with 3-layer MLP), CLF-MLP (MLP with BCELoss), and CLF-LGB (LightGBM with binary objective). Performance is evaluated using Best-of-N sampling with N=500 against golden reward values, testing both same-prompt and cross-prompt comparison setups.

## Key Results
- Classification-based reward models achieve comparable or better performance than BT models while offering greater flexibility
- Cross-prompt comparisons outperform same-prompt comparisons, especially when same-prompt responses lack diversity
- BT models are not essential for reward modeling as order consistency suffices for downstream optimization tasks

## Why This Works (Mechanism)

### Mechanism 1
BT models preserve correct ranking predictions through monotonic transformations of true reward. The BT model outputs softmax probabilities over pairwise comparisons, which depend only on the relative difference between rewards. Since monotonic transformations preserve order, the model's ranking remains valid even if the absolute scale differs from true rewards. Core assumption: The BT model's anti-symmetric structure ensures that flipping comparison order flips prediction, preserving the correct ranking.

### Mechanism 2
Classification-based reward models offer greater flexibility and robustness compared to BT models. Classification models predict preferences directly using standard binary classifiers, which can be implemented with diverse off-the-shelf algorithms (MLP, LightGBM). This avoids BT's requirement for Siamese network structure and allows leveraging established ML techniques. Core assumption: Order consistency (preserving ranking) is sufficient for reward modeling, not calibrated probability estimation.

### Mechanism 3
Cross-prompt comparisons improve annotation quality by increasing response utility diversity. When responses from different prompts are compared, the expected difference in their utility values increases compared to same-prompt comparisons. This larger utility gap makes it easier for annotators to distinguish between responses, improving annotation quality. Core assumption: Utility differences between cross-prompt comparisons are larger than within-prompt differences, and annotators have better accuracy when distinguishing larger differences.

## Foundational Learning

- **Concept**: Order consistency in reward modeling
  - Why needed here: The paper establishes that reward models only need to preserve ranking order, not predict exact reward values. This insight enables both the theoretical analysis and the proposed classification-based alternative.
  - Quick check question: Can you explain why a reward model that preserves ranking order is sufficient for downstream LLM optimization tasks?

- **Concept**: Bradley-Terry model foundations
  - Why needed here: Understanding the original BT model's assumptions and limitations is crucial for extending it to the LLM alignment setting and identifying why simpler alternatives might work.
  - Quick check question: What are the key differences between using BT models for multi-player arenas versus preference-based reward modeling?

- **Concept**: Nonparametric logistic regression with neural networks
  - Why needed here: The theoretical analysis builds on nonparametric logistic regression theory to establish convergence rates for neural network-based BT reward models.
  - Quick check question: How does the truncated KL risk help overcome the divergence problem in KL risk for BT reward modeling?

## Architecture Onboarding

- **Component map**: prompt generation → response generation → preference annotation → embedding creation → reward model training → evaluation via BoN sampling

- **Critical path**: 
  1. Generate diverse prompt-response pairs from base LLMs
  2. Create embeddings using a frozen model (e.g., Gemma2b)
  3. Collect preference annotations (with noise modeling)
  4. Train reward model with BT or classification loss
  5. Evaluate via Best-of-N sampling against golden rewards

- **Design tradeoffs**:
  - BT vs classification: BT requires Siamese structure but provides calibrated probabilities; classification offers flexibility but only order consistency
  - Same-prompt vs cross-prompt: Same-prompt controls for prompt effects but may lack diversity; cross-prompt increases diversity but loses prompt-specific control
  - Embedding quality: Higher-quality embeddings improve reward model performance but increase computational cost

- **Failure signatures**:
  - Poor ranking preservation: If learned rewards don't correlate with true preferences despite order consistency
  - Overfitting to noise: If model learns to predict annotation noise rather than underlying preferences
  - Insufficient diversity: If cross-prompt comparisons don't provide enough utility variation

- **First 3 experiments**:
  1. Implement BT and classification reward models with identical MLP architectures on same dataset, compare ranking accuracy
  2. Vary annotation noise levels and compare robustness of BT vs classification models
  3. Test same-prompt vs cross-prompt annotation setups with controlled utility diversity scenarios

## Open Questions the Paper Calls Out

### Open Question 1
How does the convergence rate of Bradley-Terry models change when using embeddings of varying dimensionality in reward modeling? The paper provides theoretical bounds but does not empirically test how different embedding dimensions affect convergence rates in practical reward modeling scenarios. What evidence would resolve it: Empirical studies comparing BT model performance and convergence across various embedding dimensions (e.g., 64, 128, 256, 512) using the same reward modeling tasks.

### Open Question 2
Under what conditions do cross-prompt comparisons outperform same-prompt comparisons in reward modeling, and how does response diversity affect this relationship? The paper demonstrates the superiority of cross-prompt comparisons but does not fully characterize the threshold of response diversity at which this advantage emerges or disappears. What evidence would resolve it: Controlled experiments varying response diversity within prompts while measuring the performance gap between cross-prompt and same-prompt reward models.

### Open Question 3
How do classification-based reward models compare to BT models in terms of overoptimization risk and robustness to distribution shifts in the test data? The paper shows classification models are more robust to annotation quality changes but does not investigate their behavior under distribution shifts or overoptimization. What evidence would resolve it: Comparative studies of BT and classification models on distributionally shifted test sets and during iterative optimization processes, measuring reward hacking and generalization.

## Limitations
- Empirical evaluation relies on simulated preference annotations rather than real human judgments, which may not capture actual annotation noise characteristics
- Theoretical analysis assumes a fixed embedding function, which may not hold when embeddings are learned or when different embedding models are used
- Cross-prompt comparison benefits depend on assumptions about utility distributions that may not generalize to all domains

## Confidence
- **High confidence**: The theoretical framework for BT reward modeling with neural networks is well-established, drawing on nonparametric logistic regression literature
- **Medium confidence**: The empirical comparisons between BT and classification models are convincing within the simulated setting, but generalization to real human preference data remains uncertain
- **Low confidence**: The specific architectural choices and their impact on performance differences between BT and classification approaches are not thoroughly explored

## Next Checks
1. Conduct a small-scale experiment with actual human preference annotations to verify that classification-based models maintain their performance advantage over BT models when faced with real-world noise patterns
2. Evaluate reward model performance when using different embedding models (not just frozen Gemma2b) to assess the sensitivity to embedding quality and domain alignment
3. Systematically measure annotation efficiency (preferences collected per unit time) for same-prompt versus cross-prompt setups with real annotators to quantify the practical benefits claimed theoretically