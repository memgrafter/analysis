---
ver: rpa2
title: Private prediction for large-scale synthetic text generation
arxiv_id: '2407.12108'
source_url: https://arxiv.org/abs/2407.12108
tags:
- data
- privacy
- private
- synthetic
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a differentially private method for generating
  synthetic text data using large language models (LLMs). The approach, called private
  prediction, ensures privacy by only requiring the synthetic output to satisfy differential
  privacy guarantees, rather than training a model on sensitive data.
---

# Private prediction for large-scale synthetic text generation

## Quick Facts
- **arXiv ID**: 2407.12108
- **Source URL**: https://arxiv.org/abs/2407.12108
- **Reference count**: 40
- **Primary result**: Differentially private method generating thousands of high-quality synthetic text examples using LLMs

## Executive Summary
This paper presents a differentially private method for generating synthetic text data using large language models (LLMs) called private prediction. Unlike traditional approaches that train models on sensitive data, private prediction ensures privacy by only requiring the synthetic output to satisfy differential privacy guarantees. The authors make several key improvements enabling generation of thousands of high-quality synthetic data points, compared to prior work limited to fewer than 10 examples. These improvements include using the exponential mechanism for private token selection, avoiding repeated prefix re-sampling through disjoint data partitioning, and leveraging public predictions via the sparse vector technique to avoid privacy costs for predictable tokens. Experiments on benchmark datasets demonstrate improved downstream task performance and data quality compared to prior private prediction methods, with the ability to generate enough synthetic data for fine-tuning models.

## Method Summary
The method uses a private prediction framework where sensitive prompts are provided to an LLM, and the next-token predictions are made differentially private. Key innovations include: (1) using the exponential mechanism instead of Gaussian noise for private token selection, leveraging the equivalence between softmax sampling and the exponential mechanism; (2) partitioning sensitive data into disjoint batches to avoid repeated prefix re-sampling and enable parallel composition; and (3) using the sparse vector technique to compare sensitive and public token distributions, sampling from public distributions when sufficiently similar to avoid privacy costs. The method generates synthetic examples token-by-token using a fixed batch of sensitive prompts, with each token either sampled privately from sensitive data or from public predictions based on SVT comparisons.

## Key Results
- Generates thousands of synthetic examples compared to prior methods limited to fewer than 10 examples
- Improves downstream task performance on multiple benchmark datasets (AGNews, TREC, IMDB, Yelp)
- Successfully generates structured JSON data with high parsing rates while preserving privacy
- Sparse vector technique effectively reduces privacy costs by leveraging public predictions for predictable tokens

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using the exponential mechanism for private token selection induces less distortion than Gaussian noise-based approaches.
- **Mechanism:** Instead of adding Gaussian noise to the entire token distribution, we sample tokens using the exponential mechanism after clipping logits. This leverages the inherent randomness of softmax sampling to provide privacy.
- **Core assumption:** The softmax sampling operation can be viewed as an instance of the exponential mechanism, which is differentially private.
- **Evidence anchors:**
  - [abstract] "Our improvements come from an improved privacy analysis and a better private selection mechanism, which makes use of the equivalence between the softmax layer for sampling tokens in LLMs and the exponential mechanism."
  - [section 6] "Instead of protecting the privacy of the entire token distribution with Gaussian or Laplace noise, we leverage the uncertainty inherent in sampling to ensure privacy."
  - [corpus] Weak - corpus neighbors don't directly discuss exponential mechanism vs Gaussian noise tradeoffs.
- **Break condition:** When the token distribution is highly concentrated (deterministic), the exponential mechanism's privacy guarantee degrades significantly.

### Mechanism 2
- **Claim:** Using fixed disjoint data partitioning instead of repeated random subsampling avoids re-computation overhead and improves privacy efficiency.
- **Mechanism:** Instead of using a different random subset of prompts for each token generation, we partition the dataset into disjoint batches and use one batch per synthetic example. This allows KV cache acceleration and leverages parallel composition.
- **Core assumption:** The privacy analysis holds when each prompt is assigned to exactly one batch deterministically.
- **Evidence anchors:**
  - [abstract] "Furthermore, we introduce a novel use of public predictions via the sparse vector technique, in which we do not pay privacy costs for tokens that are predictable without sensitive data."
  - [section 6] "Our method generates each synthetic example using a fixed disjoint subset of the sensitive prompts, allowing us to leverage parallel composition in our analysis."
  - [corpus] Missing - corpus doesn't discuss disjoint partitioning vs repeated subsampling tradeoffs.
- **Break condition:** When the dataset size is small relative to the number of tokens needed, disjoint partitioning may lead to insufficient diversity in the training data.

### Mechanism 3
- **Claim:** Using public predictions via the sparse vector technique reduces privacy cost by avoiding sampling predictable tokens from sensitive data.
- **Mechanism:** We compare the sensitive token distribution to a public token distribution. If they're sufficiently similar, we sample from the public distribution at no privacy cost. We use the sparse vector technique to privately determine when to use public vs private sampling.
- **Core assumption:** The distance between public and private token distributions can be computed privately without revealing sensitive information.
- **Evidence anchors:**
  - [abstract] "Furthermore, we introduce a novel use of public predictions via the sparse vector technique, in which we do not pay privacy costs for tokens that are predictable without sensitive data."
  - [section 6] "Our method uses an auxiliary token distribution from an LLM without access to sensitive data, and draws the next token from that distribution whenever it is very similar to the token distribution induced by the sensitive data."
  - [corpus] Weak - corpus neighbors don't discuss sparse vector technique for reducing privacy costs in text generation.
- **Break condition:** When the sensitive data contains unique tokens not present in public data, or when the structure of sensitive data is very different from public data.

## Foundational Learning

- **Concept:** Differential Privacy
  - **Why needed here:** The entire method relies on providing formal privacy guarantees for synthetic text generation. Understanding DP definitions, composition theorems, and privacy accounting is essential for analyzing the method's guarantees.
  - **Quick check question:** What's the difference between ε-DP and (ε, δ)-DP? When would you use one vs the other?

- **Concept:** Exponential Mechanism
  - **Why needed here:** The method uses the exponential mechanism for private token selection. Understanding how it works and its privacy guarantees is crucial for analyzing the method's effectiveness.
  - **Quick check question:** How does the exponential mechanism achieve privacy when selecting from a utility distribution?

- **Concept:** Sparse Vector Technique
  - **Why needed here:** The method uses SVT to decide when to use public vs private token distributions. Understanding SVT's mechanism and privacy properties is essential for analyzing the method's efficiency.
  - **Quick check question:** How does SVT allow us to perform multiple comparisons while preserving privacy?

## Architecture Onboarding

- **Component map:** Sensitive data -> Data partitioning module -> Private token selection module (exponential mechanism) -> Public prediction module (sparse vector technique) -> Output collection and formatting

- **Critical path:**
  1. Partition sensitive prompts into disjoint batches
  2. For each batch, generate synthetic examples token-by-token
  3. For each token: compute sensitive and public logits
  4. Use SVT to decide whether to sample from public or private logits
  5. If private, use exponential mechanism with clipping
  6. Append token to output and continue until <eos>

- **Design tradeoffs:**
  - Batch size vs privacy budget: Larger batches improve privacy efficiency but increase computation per example
  - Temperature vs privacy: Higher temperature provides better privacy but may reduce output quality
  - Vocabulary size: Using full vocabulary provides better coverage but requires more privacy budget

- **Failure signatures:**
  - Degenerate outputs (repeated tokens, nonsense) → Check temperature and clipping parameters
  - Insufficient diversity in outputs → Check batch size and data partitioning
  - High privacy cost → Check SVT threshold and noise parameters

- **First 3 experiments:**
  1. Generate synthetic data for AGNews with ε=1 using default parameters (s=255, τ=2, c=10)
  2. Test effect of batch size by comparing s=127 vs s=511 on downstream classification accuracy
  3. Evaluate SVT effectiveness by comparing with and without SVT on structured data generation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the impact of using different hash functions for batch assignment on the quality and privacy guarantees of the generated synthetic data?
- **Basis in paper:** [explicit] The paper mentions that using hash functions for batch assignment can lead to batches whose sizes differ from the expected batch size s, but this does not impact the validity of Theorem 1.
- **Why unresolved:** The paper does not explore the effects of different hash functions on batch sizes, synthetic data quality, or privacy guarantees.
- **What evidence would resolve it:** Empirical results comparing different hash functions for batch assignment, showing their effects on batch sizes, synthetic data quality, and privacy guarantees.

### Open Question 2
- **Question:** How does the choice of temperature parameter τ affect the balance between privacy and utility in the generated synthetic data?
- **Basis in paper:** [explicit] The paper mentions that the temperature parameter τ flattens or sharpens the distribution, but does not explore its impact on privacy and utility.
- **Why unresolved:** The paper does not provide a detailed analysis of how different temperature settings affect the trade-off between privacy and utility.
- **What evidence would resolve it:** Empirical results showing the effects of different temperature settings on privacy guarantees and synthetic data quality.

### Open Question 3
- **Question:** Can the proposed method be extended to handle structured data with more complex schemas, such as nested JSON objects or relational databases?
- **Basis in paper:** [inferred] The paper demonstrates the method's effectiveness on structured data in the form of JSON records, but does not explore more complex schemas.
- **Why unresolved:** The paper does not address the challenges of handling more complex structured data formats.
- **What evidence would resolve it:** Empirical results showing the method's performance on more complex structured data formats, such as nested JSON objects or relational databases.

## Limitations

- Effectiveness heavily depends on availability of high-quality public token distributions for SVT to work effectively
- Privacy guarantees degrade when token distributions are highly concentrated (common for structured data)
- Disjoint data partitioning may lead to insufficient diversity when sensitive dataset is small relative to tokens needed

## Confidence

- **High Confidence**: Differential privacy guarantees and composition analysis are well-established; improvement in downstream task performance is well-supported
- **Medium Confidence**: Practical effectiveness of SVT for reducing privacy costs is supported but conditions for meaningful savings are not fully characterized
- **Low Confidence**: Scalability to larger models and effectiveness on extremely large datasets remain untested; quality of generated structured data beyond syntactic validity is uncertain

## Next Checks

1. **SVT Effectiveness Validation**: Conduct ablation studies comparing the method with and without SVT across datasets with varying degrees of similarity to public data distributions. Quantify the actual privacy budget savings and identify conditions where SVT provides diminishing returns.

2. **Structured Data Quality Assessment**: Evaluate the semantic correctness of generated JSON records beyond parsing rates by implementing schema validation and downstream task evaluation. Test whether the generated records contain meaningful relationships and accurate data representations, not just syntactically valid JSON.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary batch size, temperature, clip bound, and SVT parameters across multiple datasets to identify optimal settings and their impact on the privacy-utility tradeoff. Determine whether there are universal parameter guidelines or if dataset-specific tuning is required for best performance.