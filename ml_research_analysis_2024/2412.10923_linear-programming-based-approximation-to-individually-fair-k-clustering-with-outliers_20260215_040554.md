---
ver: rpa2
title: Linear Programming based Approximation to Individually Fair k-Clustering with
  Outliers
arxiv_id: '2412.10923'
source_url: https://arxiv.org/abs/2412.10923
tags:
- outliers
- fair
- clustering
- algorithm
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of individually fair k-means clustering
  in the presence of outliers. The authors propose a novel linear programming formulation
  that identifies and excludes outliers while ensuring fairness for the remaining
  inlier points.
---

# Linear Programming based Approximation to Individually Fair k-Clustering with Outliers

## Quick Facts
- **arXiv ID:** 2412.10923
- **Source URL:** https://arxiv.org/abs/2412.10923
- **Reference count:** 0
- **Key outcome:** Proposes an LP formulation to identify outliers while ensuring individual fairness, achieving constant-factor approximations for fair radius and clustering cost.

## Executive Summary
This paper addresses the challenge of individually fair k-means clustering in the presence of outliers. The authors develop a novel linear programming formulation that simultaneously identifies outliers and computes fair cluster centers. Their approach involves solving an LP, detecting outliers through a rounding procedure, and then applying a fair clustering algorithm to the inlier points. The method provides theoretical guarantees on both the fairness radius (16-approximation) and clustering cost (12-approximation for k-means), while empirical results show improved performance compared to isolation forest baselines.

## Method Summary
The method consists of three main steps: First, solve a linear program that assigns points to centers under fairness constraints while identifying outliers through binary variables. Second, apply the OutRound algorithm to threshold the outlier variables and reassign any outlier centers to nearby inlier centers. Third, run FairRound on the remaining inlier points to compute the final cluster centers. The LP formulation ensures that non-outlier points are assigned to centers within their fair radius, while the rounding steps preserve these fairness guarantees with bounded approximation factors.

## Key Results
- Achieves 12-approximation for k-means and 6-approximation for k-median clustering cost
- Provides 16-approximation to the fair radius for inlier points
- OutRound marks at most 3m points as outliers (empirically observed)
- Consistently yields lower clustering costs compared to isolation forest baseline on UCI datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LP identifies outliers by setting z_v=1 for points far from any cluster center within their fair radius.
- Mechanism: The LP assigns points to centers under fairness constraints (LP2, LP3, LP5). If no center is within the α·r(v) ball, the solver marks the point as an outlier (z_v=1) to satisfy feasibility.
- Core assumption: The LP is feasible with some assignment of outliers that allows the rest to satisfy fairness.
- Evidence anchors:
  - [abstract]: "We define and solve a linear program (LP) that helps us identify the outliers."
  - [section]: LP constraints enforce outlier detection via z_v variables.
- Break condition: If all points require being outliers for feasibility, the method fails.

### Mechanism 2
- Claim: OutRound preserves the fairness radius bound by reassigning outliers to the nearest inlier center within 2r(v).
- Mechanism: When a center u_out is marked an outlier, OutRound moves it to the nearest inlier center u'. By Lemma 1, any reassigned point's distance to u' is ≤2r(v), so the α=2 bound holds.
- Core assumption: Reassignment to nearest inlier center guarantees 2-approximation to the fair radius.
- Evidence anchors:
  - [section]: Lemma 1 proves d(v,u') ≤ 2d(v,u_out) ≤ 2r(v).
  - [abstract]: "the constraint in (LP6) is satisfied with α=2" after OutRound.
- Break condition: If the nearest inlier is >2r(v) away, the bound fails.

### Mechanism 3
- Claim: The overall cost after rounding is at most 3× the LP optimum, enabling a 12-approximation for k-means.
- Mechanism: OutRound increases cost in two ways: (1) A: keeping original assignments for inliers, (2) B: doubling costs for points reassigned from outliers. Summing bounds yields 3×LP. FairRound then multiplies by 4.
- Core assumption: The LP cost dominates both A and B terms in the analysis.
- Evidence anchors:
  - [section]: Equation (10) shows LP_α=2(x',y') ≤ 3LP_α=1(x*,y*,z*).
  - [abstract]: "we show that the cost of the LP obtained after rounding the outlier variables is not much worse than the original LP cost."
- Break condition: If B term > 2×LP, the bound fails.

## Foundational Learning

- Concept: Linear programming relaxation for clustering
  - Why needed here: Provides a tractable way to assign points to centers under fairness constraints while detecting outliers via z variables.
  - Quick check question: What LP constraints ensure every non-outlier point is assigned to exactly one center?

- Concept: Fair radius r(v) as distance to n/kth nearest neighbor
  - Why needed here: Defines the fairness constraint: each inlier must have a center within r(v).
  - Quick check question: If k=4 and n=100, how many neighbors define r(v)?

- Concept: Rounding LP solutions to integral assignments
  - Why needed here: LP solutions are fractional; OutRound and FairRound convert them to feasible integer solutions with bounded approximation.
  - Quick check question: Why can't we simply round all fractional assignments to nearest integer without bounds?

## Architecture Onboarding

- Component map:
  LP Solver (CPLEX) -> OutRound -> FairRound -> Evaluation

- Critical path:
  1. Solve LP with all points → get z* values
  2. Run OutRound → detect outliers, recompute (x',y')
  3. Run FairRound on inliers → produce final centers
  4. Evaluate cost and fairness radius

- Design tradeoffs:
  - Using LP gives theoretical guarantees but is computationally heavy (quadratic variables)
  - Threshold τ=0 maximizes outlier detection but may flag too many points
  - Reassignment in OutRound guarantees 2-approximation but may increase cost

- Failure signatures:
  - LP infeasible → no solution possible
  - OutRound produces >3m outliers → likely too aggressive threshold
  - FairRound cost >> LP cost → rounding lost too much

- First 3 experiments:
  1. Run on Bank dataset with k=5, m=10; verify number of outliers ≤3m
  2. Compare max fair radius: IFXO vs iForest baseline
  3. Measure LP*, LP', FR costs for k=10 to confirm 3× bound empirically

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical bounds on the number of outliers detected by the OutRound algorithm?
- Basis in paper: [explicit] The paper states, "We note here that we do not show any bounds for the number of outliers detected."
- Why unresolved: The authors empirically observed that OutRound marks at most 3m points as outliers, but this is not proven theoretically.
- What evidence would resolve it: A formal proof establishing upper bounds on the number of outliers detected by OutRound for any given dataset and threshold τ.

### Open Question 2
- Question: How does the choice of threshold τ in OutRound affect the number of outliers detected and the resulting clustering quality?
- Basis in paper: [explicit] "Varying the value of τ controls the number of outliers detected. For higher threshold values τ, the number of outliers detected will be lower."
- Why unresolved: While the paper mentions the effect of τ on outlier detection, it does not provide a detailed analysis of how different τ values impact clustering performance.
- What evidence would resolve it: Experimental results showing clustering quality (e.g., cost, fairness) for different τ values across various datasets and outlier ratios.

### Open Question 3
- Question: Can the approximation guarantees for the fair radius be improved beyond the 16-approximation mentioned in the paper?
- Basis in paper: [explicit] "Thus, we will have a 16-approximation to the fair radius for the inliers, both for k-means as well as k-median clustering."
- Why unresolved: The paper provides a 16-approximation for the fair radius, but does not explore whether tighter bounds are achievable.
- What evidence would resolve it: A new algorithm or analysis technique that provides a tighter approximation guarantee for the fair radius, either theoretically or through empirical evidence.

## Limitations
- The LP approach becomes computationally expensive for large datasets due to quadratic number of variables
- Empirical evaluation is limited to three UCI datasets, which may not represent all real-world outlier scenarios
- The aggressive threshold τ=0 for outlier detection may not generalize well to different data distributions

## Confidence

- **High confidence**: The theoretical 12-approximation guarantee for k-means and 6-approximation for k-median, as these follow directly from proven lemmas and established rounding algorithms.
- **Medium confidence**: The practical effectiveness on real datasets, as the evaluation is limited in scope and scale.
- **Medium confidence**: The LP-based outlier detection mechanism, as it relies on the assumption that the LP is feasible with some outlier assignment.

## Next Checks
1. Test the LP solver on larger datasets (e.g., >10,000 points) to verify scalability claims and identify computational bottlenecks.
2. Compare the outlier detection performance against multiple baselines (e.g., DBSCAN, LOF) on synthetic datasets with known outlier patterns.
3. Implement the algorithm with varying τ thresholds to determine the optimal value for different data distributions and clustering objectives.