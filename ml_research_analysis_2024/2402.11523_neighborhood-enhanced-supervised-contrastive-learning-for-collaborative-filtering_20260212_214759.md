---
ver: rpa2
title: Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative Filtering
arxiv_id: '2402.11523'
source_url: https://arxiv.org/abs/2402.11523
tags:
- loss
- proposed
- neighbors
- contrastive
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to enhance supervised contrastive
  learning for collaborative filtering by incorporating collaborative neighbors into
  the positive sample set. The authors introduce two novel loss functions, Lin NESCL
  and Lout NESCL, that treat the nearest neighbors and interacted neighbors of the
  anchor node as positive samples.
---

# Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative Filtering

## Quick Facts
- arXiv ID: 2402.11523
- Source URL: https://arxiv.org/abs/2402.11523
- Authors: Peijie Sun; Le Wu; Kun Zhang; Xiangzhi Chen; Meng Wang
- Reference count: 40
- Primary result: NESCL outperforms SGL by 10.09%, 7.09%, and 35.36% on NDCG@20 across three datasets

## Executive Summary
This paper addresses the limitation of supervised contrastive learning in collaborative filtering where positive samples are often too distant from anchor nodes, causing suboptimal embeddings. The authors propose Neighborhood-Enhanced Supervised Contrastive Learning (NESCL) which incorporates collaborative neighbors - both nearest neighbors and interacted neighbors - as additional positive samples in the contrastive loss. Two novel loss functions, Lin NESCL and Lout NESCL, are introduced that dynamically weight the importance of different positive samples based on their similarity to both the anchor and negative samples.

The method demonstrates significant performance improvements over the state-of-the-art SGL model, achieving 10.09%, 7.09%, and 35.36% gains in NDCG@20 on Yelp2018, Gowalla, and Amazon-Book datasets respectively. The authors also find that smaller temperature values in the contrastive loss yield better performance when multiple positive samples are incorporated, suggesting this can offset the negative impact of false negative samples.

## Method Summary
NESCL enhances supervised contrastive learning by treating collaborative neighbors of anchor nodes as positive samples. The method uses LightGCN as the backbone model and generates two augmented views of the user-item interaction graph through node/edge dropout or random walk. For each anchor node, it identifies nearest neighbors using memory-based methods like ItemKNN and interacted neighbors as positive samples. The proposed Lin NESCL and Lout NESCL loss functions then jointly optimize the anchor node and its positive neighbors by dynamically weighting their influence based on similarity to both the anchor and negative samples. The final loss combines the ranking loss with either Lin NESCL or Lout NESCL, with smaller temperature values recommended for better performance with multiple positive samples.

## Key Results
- NESCL outperforms SGL by 10.09%, 7.09%, and 35.36% on NDCG@20 for Yelp2018, Gowalla, and Amazon-Book datasets respectively
- The method performs better with smaller temperature values (τ), suggesting multiple positive samples can offset false negative impacts
- Both Lin NESCL and Lout NESCL versions show consistent improvements over baseline methods
- NESCL demonstrates robustness across datasets with different characteristics and sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating collaborative neighbors as positive samples improves recommendation by keeping related users/items close in embedding space.
- Mechanism: NESCL loss functions dynamically weight the influence of positive neighbors based on their similarity to both anchor and negative samples, counteracting the tendency of contrastive learning to push away collaborative neighbors.
- Core assumption: Nearest neighbors from memory-based methods and interacted neighbors are true collaborative neighbors that should be pulled closer to the anchor.
- Evidence anchors: Abstract mentions treating collaborative neighbors as positive samples; Section 4.4.1 shows anchor embedding depends on multiple positive samples simultaneously.
- Break condition: If identified neighbors are not truly collaborative (false positives), incorporating them could introduce noise and degrade performance.

### Mechanism 2
- Claim: Lin NESCL and Lout NESCL differ in how they weigh positive sample importance based on influence from anchor and negative samples.
- Mechanism: Lin NESCL computes influence capacity λin values considering anchor and positive-negative relationships, while Lout NESCL compounds this by including anchor's influence on other positive samples.
- Core assumption: Relative distances between positive samples and negative samples are meaningful signals for adjusting positive sample weights in updating anchor representation.
- Evidence anchors: Section 4.4.2 shows λout values should be smaller than λin and are affected by both anchor-positive and positive-positive relationships.
- Break condition: If influence calculations become too noisy with many false negatives, weighting may become unstable and hurt learning.

### Mechanism 3
- Claim: Smaller temperature values improve performance when multiple positive samples are incorporated by amplifying true negative signals and mitigating false negative impacts.
- Mechanism: Smaller τ sharpens softmax distribution in contrastive loss, making model more sensitive to differences between positive and negative samples. Multiple positives help focus on true positives while false negatives have less relative impact.
- Core assumption: Multiple positive samples can offset negative effects of some false negatives in negative sample set.
- Evidence anchors: Abstract mentions method performs better with smaller temperature values; Section 5.4.2 states small τ better utilizes information from non-hard negative samples.
- Break condition: If τ is too small, gradients may explode leading to unstable training.

## Foundational Learning

- **Graph Neural Networks (GNNs) for collaborative filtering**
  - Why needed: LightGCN backbone captures high-order collaborative signals from user-item bipartite graph
  - Quick check: What is the main difference between LightGCN and NGCF in terms of message passing?

- **Contrastive learning and InfoNCE loss**
  - Why needed: NESCL builds on contrastive learning by modifying InfoNCE loss to include collaborative neighbors as positive samples
  - Quick check: In standard contrastive learning, what are considered positive and negative samples?

- **Memory-based collaborative filtering (e.g., ItemKNN)**
  - Why needed: Used to identify nearest neighbors of items/users based on historical interaction patterns, treated as positive samples
  - Quick check: How is item-item similarity calculated in ItemKNN?

## Architecture Onboarding

- **Component map**: Input interaction graph G -> Data augmentation (Node/Edge Dropout or Random Walk) -> LightGCN backbone -> Neighbor identification (ItemKNN/UserKNN) -> NESCL loss computation -> Backpropagation

- **Critical path**: 1. Data augmentation → 2. LightGCN forward pass → 3. Neighbor indexing → 4. NESCL loss computation → 5. Backpropagation

- **Design tradeoffs**: 
  - More neighbors (larger K) increases positive sample diversity but risks incorporating noise
  - Smaller temperature τ sharpens positive/negative separation but risks gradient explosion
  - Including ranking loss LR stabilizes training but may conflict with contrastive objectives if not balanced

- **Failure signatures**:
  - Performance degrades with large K or very small τ → neighbor noise or gradient instability
  - Model underperforms SGL on datasets with strong implicit feedback → contrastive neighbors may not help in those cases
  - High variance in results across runs → sensitivity to neighbor quality or augmentation

- **First 3 experiments**:
  1. Ablation: Train with only ranking loss vs. ranking + NESCL (Lin) vs. ranking + NESCL (Lout) to confirm collaborative neighbors help
  2. Temperature sweep: Test τ ∈ [0.05, 0.30] to find optimal value and verify smaller τ helps with multiple positives
  3. Neighbor sampling: Compare random sampling vs. weighted sampling of nearest neighbors to test robustness to neighbor quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed similarity-incorporating strategies be improved to effectively utilize similarity values between neighboring and anchor nodes in the supervised contrastive loss function?
- Basis in paper: Authors mention similarity-incorporating strategies don't work and suggest more advanced memory-based methods could provide more accurate similarity values
- Why unresolved: Paper doesn't provide detailed analysis of why strategies fail or propose alternative approaches
- What evidence would resolve it: Empirical results demonstrating effectiveness of alternative similarity-incorporating strategies or theoretical analysis explaining limitations

### Open Question 2
- Question: Can the proposed loss function be effectively applied to other types of input data beyond GNN-based models, such as item sequences or social relation graphs?
- Basis in paper: Authors mention aspiration to explore application to other types of input data in future
- Why unresolved: Paper focuses on GNN-based models and doesn't provide insights into potential effectiveness with other data types
- What evidence would resolve it: Experimental results comparing performance on different input data types or theoretical analysis of adaptability to various data structures

### Open Question 3
- Question: How can more efficient graph augmentation techniques be developed to address memory consumption issue associated with the proposed model?
- Basis in paper: Authors mention memory consumption issue and plan to investigate more efficient graph augmentation techniques
- Why unresolved: Paper doesn't provide specific suggestions or insights into potential approaches to reduce memory consumption
- What evidence would resolve it: Empirical results demonstrating effectiveness of memory-efficient graph augmentation techniques or theoretical analysis of impact on model performance

## Limitations

- Performance heavily depends on quality of identified collaborative neighbors, which may introduce noise if neighbors are not truly collaborative
- Influence weighting mechanism (λin/λout) is theoretically derived but lacks ablation studies showing individual component contributions
- Method's sensitivity to hyperparameters like temperature τ and regularization α is mentioned but not thoroughly explored across diverse datasets

## Confidence

- **High Confidence**: General claim that incorporating collaborative neighbors improves contrastive learning performance is supported by consistent NDCG@20 improvements (10.09%, 7.09%, 35.36% gains) across all three datasets
- **Medium Confidence**: Theoretical analysis of Lin NESCL vs Lout NESCL weighting mechanisms is sound, but practical significance of differences needs more empirical validation
- **Low Confidence**: Claim about temperature sensitivity offsetting false negative impacts is based on limited experimental evidence (only τ=0.1 mentioned) and lacks systematic temperature sweep analysis

## Next Checks

1. Conduct neighbor quality analysis by measuring overlap between identified neighbors and actual positive items in held-out test sets across all datasets
2. Perform systematic ablation studies isolating contributions of Lin NESCL vs Lout NESCL, and ranking loss vs contrastive components
3. Execute comprehensive hyperparameter sensitivity analysis, particularly for temperature τ and regularization α, across wider range of values and datasets