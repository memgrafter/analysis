---
ver: rpa2
title: How Certain are Uncertainty Estimates? Three Novel Earth Observation Datasets
  for Benchmarking Uncertainty Quantification in Machine Learning
arxiv_id: '2412.06451'
source_url: https://arxiv.org/abs/2412.06451
tags:
- uncertainty
- noise
- dataset
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the lack of benchmark datasets for evaluating
  uncertainty quantification (UQ) methods in Earth observation (EO) machine learning.
  It introduces three novel datasets designed for UQ in regression, image segmentation,
  and classification tasks in EO: RegressionUQ (simulated biomass dataset), SegmentationUQ
  (rendered building segmentation dataset), and ClassificationUQ (multiple label votes
  for LCZ classification).'
---

# How Certain are Uncertainty Estimates? Three Novel Earth Observation Datasets for Benchmarking Uncertainty Quantification in Machine Learning

## Quick Facts
- arXiv ID: 2412.06451
- Source URL: https://arxiv.org/abs/2412.06451
- Reference count: 40
- Key outcome: Introduces three benchmark datasets for evaluating uncertainty quantification methods in EO ML regression, segmentation, and classification tasks

## Executive Summary
This paper addresses a critical gap in Earth observation machine learning by introducing three novel benchmark datasets specifically designed for evaluating uncertainty quantification (UQ) methods. The datasets provide reference uncertainty estimates through Monte Carlo simulation, enabling transparent comparison of different UQ approaches. The authors demonstrate baseline performance across regression, image segmentation, and classification tasks, showing that while model predictions can be accurate, uncertainty estimates vary significantly between methods and improve with more training data. The datasets are publicly available and represent a valuable resource for advancing UQ in EO applications.

## Method Summary
The paper introduces three datasets for benchmarking uncertainty quantification in Earth observation: RegressionUQ for biomass estimation using allometric equations with Monte Carlo input noise, SegmentationUQ for building segmentation using rendered aerial imagery with Shannon entropy uncertainty measures, and ClassificationUQ for LCZ classification using multiple human votes converted to distributional labels. Each dataset provides reference uncertainty estimates through Monte Carlo simulation and modeling of probability distributions. The authors implement baseline models including fully-connected networks, U-Net, and deep learning with distributional labels, using Bayesian neural networks, test time augmentation, and distributional learning methods. Performance is evaluated using R-squared, RMSE, cross-entropy, and calibration error metrics.

## Key Results
- The three datasets successfully enable benchmarking of UQ methods across regression, segmentation, and classification tasks
- UQ methods show varying performance, with epistemic and aleatoric uncertainties often conflated in predictions
- Increasing training data improves both prediction accuracy and uncertainty quantification, though UQ requires more data than signal prediction alone
- Monte Carlo-based reference uncertainty generation provides a reliable baseline for method comparison

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Monte Carlo simulation of input noise enables reference uncertainty generation in EO regression tasks
- Mechanism: By defining a known physical model (allometric equations), noise can be added to inputs (diameter, height), and the variance of outputs calculated via Monte Carlo sampling to create ground truth aleatoric uncertainty
- Core assumption: The physical model perfectly represents the true relationship between inputs and outputs
- Evidence anchors:
  - [abstract] "Monte Carlo simulation of input noise and modeling softmax probability distributions"
  - [section] "With a defined ground truth physical model, it is straightforward to propagate uncertainties from the input tree height and diameter to the output biomass"
  - [corpus] Weak - no direct evidence of Monte Carlo in corpus
- Break condition: If the assumed physical model deviates significantly from reality, propagated uncertainties become unreliable

### Mechanism 2
- Claim: Shannon entropy of softmax probability distributions provides a continuous uncertainty measure for segmentation tasks
- Mechanism: Instead of modeling binary segmentation outputs as Bernoulli distributions, calculating entropy across multiple Monte Carlo samples of softmax probabilities creates a continuous uncertainty metric that can be compared with reference uncertainty
- Core assumption: The baseline U-Net model has minimal epistemic uncertainty after training on noise-free data
- Evidence anchors:
  - [abstract] "We employ a similar MC sampling approach to the regression problem"
  - [section] "We choose the comparison to be on the distribution of the softmax probabilities...we use the Shannon entropy...as the uncertainty measure"
  - [corpus] Weak - corpus contains no discussion of Shannon entropy for uncertainty
- Break condition: If the baseline model retains significant epistemic uncertainty, the entropy measure conflates epistemic and aleatoric uncertainty

### Mechanism 3
- Claim: Multiple human votes can be converted to distributional labels for improved classification calibration
- Mechanism: By aggregating 10 independent human votes per image into a probability distribution (rather than one-hot encoding), the model learns to represent inherent label uncertainty, reducing overconfidence
- Core assumption: Human disagreement reflects true label ambiguity rather than simple labeling errors
- Evidence anchors:
  - [abstract] "We developed a method to turn the multiple votes to a distributional label"
  - [section] "The distributional label captures the inherent uncertainty in the label"
  - [corpus] No direct evidence - corpus contains no papers about distributional labels from multiple votes
- Break condition: If human votes are systematically biased or inconsistent, the distributional labels may misrepresent true uncertainty

## Foundational Learning

- Concept: Monte Carlo uncertainty propagation
  - Why needed here: Enables creation of reference uncertainty estimates by simulating input noise and observing output variance
  - Quick check question: If you add Gaussian noise with σ=0.1 to input diameter in the biomass model, how does this affect the variance of the biomass output?

- Concept: Aleatoric vs epistemic uncertainty
  - Why needed here: The datasets focus on aleatoric (data) uncertainty, requiring engineers to distinguish between inherent data variability and model uncertainty
  - Quick check question: Which type of uncertainty decreases as you add more training data to a well-specified model?

- Concept: Shannon entropy for probability distributions
  - Why needed here: Provides a continuous uncertainty measure for categorical predictions that can be compared across different noise levels
  - Quick check question: What is the entropy of a binary classification where softmax probabilities are [0.9, 0.1]?

## Architecture Onboarding

- Component map: Dataset generation (Blender simulation + Monte Carlo sampling) → Reference uncertainty calculation (variance/entropy) → UQ method evaluation (BNN, TTA, distributional learning)
- Critical path: Generate noise-free baseline → Simulate noise variations → Train baseline model → Calculate reference uncertainty → Evaluate UQ methods
- Design tradeoffs: Simulated datasets provide perfect reference uncertainty but may not capture all real-world complexities; multiple human votes capture label uncertainty but are expensive to obtain
- Failure signatures: High bias in UQ predictions across noise levels; poor correlation between predicted and reference uncertainty; overconfidence in model predictions
- First 3 experiments:
  1. Generate a single dataset variation (e.g., 4° viewpoint variation) and verify the reference uncertainty calculation pipeline
  2. Train a baseline U-Net on the noise-free dataset and test uncertainty propagation on one noise type
  3. Implement the distributional label approach on the classification dataset and compare calibration error with one-hot encoding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the conflation of aleatoric and epistemic uncertainties affect the performance of uncertainty quantification methods in Earth observation applications, and what strategies could effectively disentangle these two types of uncertainties?
- Basis in paper: [explicit] The authors demonstrate that epistemic and aleatoric uncertainties can barely be disentangled in the UQ methods tested, and reducing epistemic uncertainty can potentially improve the prediction of aleatoric uncertainty.
- Why unresolved: The paper shows that increasing training data improves both aleatoric and epistemic uncertainty predictions, suggesting these uncertainties are conflated in current methods. The authors note this is a limitation but don't provide a clear solution for separating them.
- What evidence would resolve it: Experimental results comparing UQ methods specifically designed to separate aleatoric and epistemic uncertainties, along with quantitative metrics showing improved performance in uncertainty estimation when these components are properly disentangled.

### Open Question 2
- Question: How can the reference aleatoric uncertainty calculation methodology be extended to classification problems where there is no well-defined physical model to propagate input noise through?
- Basis in paper: [inferred] The authors mention that for classification problems, "an inaccurate model causes the conflation of aleatoric and epistemic uncertainties" and note this is a limitation of their approach, which relies on having a ground truth physical model.
- Why unresolved: The paper demonstrates the reference uncertainty calculation for regression and segmentation but explicitly states this approach is not feasible for classification problems due to the lack of physical models, yet doesn't propose an alternative methodology.
- What evidence would resolve it: A validated methodology for calculating reference aleatoric uncertainty in classification tasks that doesn't rely on physical models, along with benchmark datasets and experimental results demonstrating its effectiveness.

### Open Question 3
- Question: What is the optimal number of training samples needed to achieve reliable uncertainty quantification in Earth observation machine learning models, and how does this requirement scale with different types of uncertainty and data complexity?
- Basis in paper: [explicit] The authors show that increasing training data size improves both biomass estimation and uncertainty quantification performance, with the uncertainty quantification requiring far more training data than signal prediction alone.
- Why unresolved: While the paper demonstrates that more training data helps, it doesn't establish specific thresholds or scaling relationships for different problem types, uncertainty levels, or data complexities.
- What evidence would resolve it: Systematic experiments across different EO tasks showing the relationship between training data size and UQ performance, including specific recommendations for minimum training data requirements based on task complexity and desired uncertainty quantification accuracy.

## Limitations

- The Monte Carlo-based reference uncertainty generation assumes the underlying physical models perfectly represent reality, which may not hold for complex Earth observation scenarios
- The simulated nature of RegressionUQ and SegmentationUQ datasets may not capture all real-world complexities, potentially limiting generalizability to operational EO applications
- The distributional label approach for ClassificationUQ relies on human vote consistency, but systematic biases in labeling could misrepresent true uncertainty

## Confidence

- **High confidence**: The methodology for generating reference uncertainty estimates through Monte Carlo simulation is technically sound and well-explained
- **Medium confidence**: The performance evaluation results are internally consistent but may not generalize to real-world datasets
- **Low confidence**: Claims about method superiority are preliminary, as only three UQ methods were compared, and the study doesn't comprehensively evaluate the full landscape of available approaches

## Next Checks

1. Validate the distributional label approach by testing on a real-world EO dataset with multiple expert annotations to confirm the assumption that human disagreement reflects true label ambiguity
2. Implement additional UQ methods (e.g., ensemble methods, evidential deep learning) on the three datasets to provide a more comprehensive comparison of method performance
3. Conduct sensitivity analysis on RegressionUQ by varying the assumed physical model parameters to assess robustness of the reference uncertainty estimates