---
ver: rpa2
title: A Fresh Look at Generalized Category Discovery through Non-negative Matrix
  Factorization
arxiv_id: '2410.21807'
source_url: https://arxiv.org/abs/2410.21807
tags: []
core_contribution: This paper addresses the challenge of Generalized Category Discovery
  (GCD), which aims to classify both base (known) and novel (unknown) categories in
  unlabeled data using only labeled base data. Existing GCD approaches inadequately
  optimize the co-occurrence matrix based on cosine similarity, failing to achieve
  zero base-novel regions and adequate sparsity.
---

# A Fresh Look at Generalized Category Discovery through Non-negative Matrix Factorization

## Quick Facts
- arXiv ID: 2410.21807
- Source URL: https://arxiv.org/abs/2410.21807
- Reference count: 40
- Primary result: Achieves 66.1% average accuracy on Semantic Shift Benchmark, outperforming prior methods by 4.7%

## Executive Summary
This paper addresses the challenge of Generalized Category Discovery (GCD), where the goal is to classify both known (base) and unknown (novel) categories in unlabeled data using only labeled base data. The authors identify limitations in existing GCD approaches that rely on cosine similarity-based co-occurrence matrix optimization, which fail to achieve zero base-novel regions and adequate sparsity. To address these issues, they propose a Non-Negative Generalized Category Discovery (NN-GCD) framework that leverages Symmetric Non-negative Matrix Factorization (SNMF) to reformulate the problem as Non-negative Contrastive Learning (NCL). The framework introduces GELU activation, NMF NCE loss, and hybrid sparse regularization to satisfy non-negative constraints and improve convergence.

## Method Summary
The NN-GCD framework employs SNMF as a mathematical medium to prove the equivalence of optimal K-means clustering with optimal SNMF, and the equivalence of SNMF solver with non-negative contrastive learning (NCL) optimization. This reframes the optimization of the co-occurrence matrix and K-means clustering as an NCL problem. The method uses ViT-B/16 as the backbone, with an MLP projection head containing non-negative activated neurons using GELU activation. The training involves SimCLR loss, SupCon loss, cross-entropy loss, NMF NCE loss, and Hybrid Sparse Regularization. The framework is evaluated on seven image recognition benchmarks including CIFAR-10/100, ImageNet-100, CUB, Stanford Cars, Aircraft, and Herbarium 19, using clustering accuracy as the primary metric.

## Key Results
- Achieves 66.1% average accuracy on the Semantic Shift Benchmark
- Outperforms state-of-the-art methods by 4.7% on average
- Demonstrates consistent improvements across multiple datasets including CIFAR-100, ImageNet-100, and domain-specific benchmarks
- Shows effectiveness in handling both base and novel categories with improved sparsity and base-novel insulation

## Why This Works (Mechanism)

### Mechanism 1
Symmetric Non-negative Matrix Factorization (SNMF) and Kernel K-means clustering are theoretically equivalent under optimal conditions. When the kernel matrix from the optimal neural network is factorized via SNMF, the resulting clustering aligns with Kernel K-means clustering. This equivalence is formalized by rewriting the Kernel K-means objective as minimizing the Frobenius norm of the difference between the kernel matrix and the product of the cluster indicator matrix and its transpose.

### Mechanism 2
Non-negative Contrastive Learning (NCL) and SNMF are equivalent when applied to the normalized adjacency matrix. NCL, which enforces non-negativity constraints on network outputs, optimizes the same objective as SNMF when applied to the normalized co-occurrence matrix of augmented data samples. The spectral contrastive loss, under non-negativity constraints, is shown to be equivalent to the SNMF objective up to an additive constant.

### Mechanism 3
The combination of GELU activation, NMF NCE loss, and Hybrid Sparse Regularization enables the model to converge to a near-optimal solution with improved sparsity and base-novel insulation. GELU activation mitigates the dead neuron problem by allowing non-zero activations, NMF NCE loss balances the learning between known and new categories by assigning weights to negative instances, and Hybrid Sparse Regularization maintains feature sparsity during optimization.

## Foundational Learning

- **Symmetric Non-negative Matrix Factorization (SNMF)**
  - Why needed here: SNMF is used to prove the equivalence between Kernel K-means clustering and NCL, and to decompose the co-occurrence matrix for improved class separability
  - Quick check question: What is the difference between SNMF and standard NMF, and why is symmetry important in this context?

- **Non-negative Contrastive Learning (NCL)**
  - Why needed here: NCL is used to optimize the co-occurrence matrix and achieve base-novel insulation by enforcing non-negativity constraints on network outputs
  - Quick check question: How does NCL differ from standard contrastive learning, and what are the benefits of the non-negativity constraint?

- **Sparsity Regularization**
  - Why needed here: Sparsity regularization is used to maintain feature sparsity during optimization, which improves class separability and generalization
  - Quick check question: What are the different types of sparsity regularization, and how do they affect the model's performance?

## Architecture Onboarding

- **Component map**: Pre-training (DINOv1/DINOv2) -> ViT-B/16 backbone -> MLP projection head (GELU activated) -> NMF NCE loss + Hybrid Sparse Regularization -> Evaluation
- **Critical path**: Pre-training (DINOv1 or DINOv2) → Fine-tuning with NMF NCE loss and Hybrid Sparse Regularization → Evaluation
- **Design tradeoffs**: The choice of pre-training method, activation function, and sparsity regularization parameters can affect the model's performance
- **Failure signatures**: Poor performance on new categories, high similarity between base and novel classes, or lack of sparsity in the co-occurrence matrix
- **First 3 experiments**:
  1. Ablation study: Remove GELU activation and observe the impact on performance
  2. Ablation study: Replace NMF NCE loss with standard InfoNCE loss and compare results
  3. Ablation study: Adjust the sparsity regularization parameters and evaluate their effect on feature sparsity and classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the NN-GCD framework perform when extended to multi-modal data scenarios, such as combining image and text features for generalized category discovery? The paper mentions that future directions could involve integrating SNMF with high-dimensional, multi-modal data, and that this synergy could enhance the capacity to uncover complex patterns across diverse domains.

### Open Question 2
What is the impact of different pre-training strategies (e.g., self-supervised vs. supervised) on the performance of the NN-GCD framework across various datasets? While the paper mentions the importance of pre-training and provides some results, it does not systematically analyze the impact of different pre-training strategies on the overall performance of NN-GCD.

### Open Question 3
How does the NN-GCD framework handle extremely imbalanced datasets, where some classes have very few samples compared to others? The paper mentions the Herbarium 19 dataset, which introduces the challenge of data imbalance, but it does not provide a detailed analysis of how NN-GCD handles this specific challenge.

## Limitations

- The theoretical equivalence claims between SNMF and Kernel K-means clustering, and between SNMF and NCL, are derived mathematically but their practical implications in the GCD setting require further empirical validation
- Performance improvements are primarily demonstrated on image classification datasets, and the method's effectiveness on other data modalities remains untested
- The magnitude of improvement varies across datasets, and the method's robustness to different dataset characteristics needs further investigation

## Confidence

- **High Confidence**: The effectiveness of the GELU activation function in mitigating dead neurons and the Hybrid Sparse Regularization approach in improving sparsity and base-novel insulation
- **Medium Confidence**: The theoretical equivalence between SNMF and Kernel K-means clustering, and between SNMF and NCL
- **Medium Confidence**: The overall performance improvement of NN-GCD over state-of-the-art methods

## Next Checks

1. Conduct a comprehensive ablation study to isolate the contributions of the SNMF-based optimization and NCL reformulation to the overall performance
2. Test NN-GCD on datasets from different domains (e.g., text, audio) to assess its generalizability beyond image classification
3. Perform a sensitivity analysis of the key hyperparameters to understand their impact on performance and identify optimal settings for different scenarios