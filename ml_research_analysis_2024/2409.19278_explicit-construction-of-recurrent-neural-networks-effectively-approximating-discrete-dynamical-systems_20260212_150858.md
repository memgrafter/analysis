---
ver: rpa2
title: Explicit construction of recurrent neural networks effectively approximating
  discrete dynamical systems
arxiv_id: '2409.19278'
source_url: https://arxiv.org/abs/2409.19278
tags:
- dynamical
- then
- such
- discrete
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents an explicit construction of recurrent neural\
  \ networks (RNNs) that effectively approximate bounded discrete dynamical systems\
  \ with recursivity. The authors consider discrete time series generated by dynamical\
  \ systems in delay coordinates and construct RNNs that can approximate these systems\
  \ with an error bound of (2t + 1)e^(\u03BBt)\u221ALC/K for t \u2265 0, where \u03BB\
  \ is the maximum Lyapunov exponent and K is a discretization parameter."
---

# Explicit construction of recurrent neural networks effectively approximating discrete dynamical systems

## Quick Facts
- arXiv ID: 2409.19278
- Source URL: https://arxiv.org/abs/2409.19278
- Authors: Chikara Nakayama; Tsuyoshi Yoneda
- Reference count: 7
- One-line primary result: Explicit construction of RNNs that approximate bounded discrete dynamical systems with error bound (2t + 1)e^(λt)√LC/K

## Executive Summary
This paper presents an explicit construction of recurrent neural networks that effectively approximate bounded discrete dynamical systems with recursivity. The authors consider discrete time series generated by dynamical systems in delay coordinates and construct RNNs that can approximate these systems with a theoretically bounded error. The key innovation is a dictionary-based approach using key-value pairs constructed from discretized training data, combined with carefully designed weight matrices and activation functions.

## Method Summary
The paper constructs RNNs by first discretizing the range [-1, 1] into K intervals, then building a dictionary of key-value pairs from the discretized time series data where keys are permutation operators representing delayed state configurations. The RNN weight matrices are explicitly constructed using the formula W = YX^(-1), where X and Y are derived from the permutation operators. The activation function is chosen to ensure matrix X is invertible for almost all cases, and the RNN parameters are set to guarantee exact reproduction of the training time series with a bounded error proportional to the Lyapunov exponent and discretization parameter.

## Key Results
- Explicit construction of RNNs that approximate bounded discrete dynamical systems with error bound (2t + 1)e^(λt)√LC/K
- Proof that the construction works for almost all choices of activation function
- Demonstration that the RNNs exactly reproduce the desired time series through explicit parameter choices
- Theoretical guarantee of approximation error bounded by discretization parameter K and system's Lyapunov exponent λ

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The explicit construction provides RNNs that can approximate any bounded discrete dynamical system with a bounded error proportional to the Lyapunov exponent and discretization parameter.
- Mechanism: The paper constructs a dictionary of key-value pairs from discretized training data, where each key is a permutation operator representing a delayed state configuration, and each value is the corresponding next state. The RNN is then explicitly built using carefully designed weight matrices and activation functions that guarantee exact reproduction of the desired time series y*(t).
- Core assumption: The dynamical system has a finite Lyapunov exponent, satisfies recursivity, and the discretization points can be chosen to avoid conflicts with the time series values.
- Evidence anchors:
  - [abstract] "we provide an explicit construction of recurrent neural networks which effectively approximate the corresponding discrete dynamical systems."
  - [section 2] "Theorem 1. Let y : Z → [−1, 1] be a time series generated by the discrete dynamical system in a delay coordinate Φ : [−1, 1]L → [−1, 1]L... there exist N ≤ KL, h, explicit W, W in and W out... such that the following holds: |y(t) - y(t)| ≤ (2t + 1)e^λt √LC/K for t ≥ 0."
- Break condition: If the Lyapunov exponent is infinite or the discretization condition (4) cannot be satisfied, the construction fails. Also if the dictionary cannot be built due to insufficient training data.

### Mechanism 2
- Claim: The dictionary-based approach allows exact reproduction of the training time series through carefully constructed weight matrices.
- Mechanism: The weight matrix W is constructed as YX^(-1) where Y and X are derived from the permutation operators and their adjoint types. This specific construction ensures that when combined with the chosen activation function and input/output weight vectors, the RNN exactly reproduces the desired time series y*(t).
- Core assumption: The matrix X is regular (invertible) for almost all choices of the activation function h.
- Evidence anchors:
  - [section 2] "Lemma 3. X is a regular matrix for almost all h : R → [−1, 1] in the following sense... there is a closed set Z of RM whose Lebesgue measure is zero such that X is regular as soon as (h(γ1), ..., h(γM)) is not in Z."
  - [section 2] "Now we explicitly construct effective recurrent neural networks... Let W be such that W := YX^(-1) with Y := [L-1∑ℓ=1 σ*i(ℓ)σj(ℓ)]i,j."
- Break condition: If the activation function h falls into the measure-zero set Z where X becomes singular, the construction fails.

### Mechanism 3
- Claim: The approximation error bound (2t + 1)e^λt √LC/K is controlled by the discretization parameter K and the system's Lyapunov exponent λ.
- Mechanism: The error accumulates over time as e^λt due to the system's dynamics, but this growth is offset by the discretization parameter K which appears in the denominator as √K. The constant C/K represents the maximum discretization error.
- Core assumption: The discretization satisfies the condition sup{(aK_k+1 - aK_k)/2} ≤ C/K for some C slightly larger than 1.
- Evidence anchors:
  - [section 2] "Also we assume recursivity to y... For any ε > 0, there is {tj}∞j=1 ⊂ Z (tj → -∞ for j → -∞) such that |Y0 - Ytj| < ε."
  - [section 2] "Let us discretize the range [-1, 1] as follows: For K ∈ Z≥1, we choose {aK_k}Kk=1 ⊂ [-1, 1]... sup{(aK_k+1 - aK_k-1)/2, (aK_1+aK_2)/2+1, 1-(aK_K-1+aK_K)/2} ≤ C/K"
- Break condition: If K is not chosen sufficiently large, the discretization error becomes too large relative to the desired approximation accuracy.

## Foundational Learning

- Concept: Discrete dynamical systems and delay coordinates
  - Why needed here: The paper explicitly constructs RNNs to approximate discrete dynamical systems represented in delay coordinates, where the state at time t depends on the current and past values.
  - Quick check question: What is the dimension L of the delay coordinate representation for a dynamical system that depends on the current value and 4 previous values?

- Concept: Lyapunov exponents and stability analysis
  - Why needed here: The approximation error bound explicitly depends on the maximum Lyapunov exponent λ of the original system, which measures the rate of divergence of nearby trajectories.
  - Quick check question: If two initial conditions differ by δ in a system with Lyapunov exponent λ, approximately how much will they differ after t time steps?

- Concept: Permutation operators and dictionary-based representations
  - Why needed here: The construction relies on creating a dictionary of permutation operators that map indices to discretized state values, which are then used to construct the weight matrices.
  - Quick check question: What is the maximum number of unique permutation operators of length L using K distinct values?

## Architecture Onboarding

- Component map: Dictionary construction -> Matrix X construction -> Matrix W construction -> Parameter initialization -> RNN execution
- Critical path: Dictionary construction → Matrix X construction → Matrix W construction → Parameter initialization → RNN execution
- Design tradeoffs: Larger K provides better discretization but increases computational complexity. The choice of activation function affects the invertibility of matrix X. The dimension L of the delay coordinate affects the size of the dictionary.
- Failure signatures: Non-invertible matrix X (singular), inability to construct the dictionary due to insufficient training data, violation of discretization conditions, infinite Lyapunov exponent.
- First 3 experiments:
  1. Test the dictionary construction with synthetic time series data to verify that all required permutation operators can be found in the training data.
  2. Verify the invertibility of matrix X for different activation functions using random permutation operators.
  3. Implement the full RNN construction with a simple discrete dynamical system (e.g., logistic map) and verify that the approximation error follows the theoretical bound.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of activation function h affect the convergence rate and stability of the constructed RNNs in practice, beyond the theoretical "almost all" result?
- Basis in paper: [explicit] The paper states "The construction is proven to work for almost all choices of the activation function" but does not provide specific guidance on optimal choices or their impact on convergence
- Why unresolved: The theoretical proof only guarantees existence for "almost all" activation functions but doesn't characterize which specific functions work best or how they affect practical performance
- What evidence would resolve it: Systematic empirical comparison of different activation functions on benchmark dynamical systems showing convergence rates and stability properties

### Open Question 2
- Question: What is the minimum required discretization parameter K for the construction to achieve a desired error bound ε, and how does this scale with the dimensionality L and Lyapunov exponent λ?
- Basis in paper: [inferred] The error bound (2t + 1)e^(λt)√LC/K depends on K, but the paper doesn't provide explicit guidance on choosing K for practical applications
- Why unresolved: The theoretical construction requires K but doesn't provide practical guidelines for choosing it based on desired accuracy or system characteristics
- What evidence would resolve it: Empirical studies showing the relationship between K, L, λ, and achieved accuracy across various dynamical systems

### Open Question 3
- Question: How does the construction perform when the dynamical system exhibits chaotic behavior with higher Lyapunov exponents, and what are the practical limitations of the approximation?
- Basis in paper: [explicit] The error bound contains the term e^(λt) which suggests exponential growth with the Lyapunov exponent
- Why unresolved: The theoretical bound shows exponential dependence on λ, but practical limitations and performance degradation with increasing chaos are not explored
- What evidence would resolve it: Numerical experiments on increasingly chaotic systems showing the practical limits of the construction's accuracy

## Limitations
- The theoretical error bound contains the term e^(λt) which grows exponentially with the Lyapunov exponent, potentially limiting applicability to highly chaotic systems.
- The construction requires precise discretization conditions that may be difficult to satisfy exactly in practice.
- The theoretical guarantee of "almost all" activation functions avoiding singularity provides limited practical guidance for choosing h.

## Confidence
- Theoretical construction validity: High
- Practical applicability across diverse systems: Medium
- Numerical implementation stability: Low

## Next Checks
1. Implement the construction on the logistic map at different parameter values to verify the theoretical error bound holds empirically and quantify how quickly the error grows with prediction horizon t.

2. Test matrix X invertibility across a wide range of activation functions (sigmoid, tanh, ReLU, random functions) to empirically verify that singular cases are indeed measure-zero and characterize the distribution of condition numbers.

3. Apply the construction to a higher-dimensional chaotic system (e.g., Lorenz attractor) and measure how the approximation quality degrades as the system dimension and Lyapunov exponent increase relative to the discretization parameter K.