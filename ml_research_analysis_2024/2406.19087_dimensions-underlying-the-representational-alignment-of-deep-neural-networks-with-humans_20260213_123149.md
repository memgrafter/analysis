---
ver: rpa2
title: Dimensions underlying the representational alignment of deep neural networks
  with humans
arxiv_id: '2406.19087'
source_url: https://arxiv.org/abs/2406.19087
tags:
- dimensions
- human
- visual
- dimension
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a framework to identify interpretable dimensions
  underlying human and artificial intelligence (AI) representations by using the same
  behavioral task and computational method for both. The authors applied this approach
  to human similarity judgments and a deep neural network (DNN) model trained on natural
  images, focusing on the triplet odd-one-out task.
---

# Dimensions underlying the representational alignment of deep neural networks with humans

## Quick Facts
- arXiv ID: 2406.19087
- Source URL: https://arxiv.org/abs/2406.19087
- Reference count: 26
- Humans prioritize semantic properties while DNNs show visual bias, with only 40 DNN dimensions needed to explain 95% of human similarity variance

## Executive Summary
This work proposes a framework to identify interpretable dimensions underlying human and artificial intelligence (AI) representations by using the same behavioral task and computational method for both. The authors applied this approach to human similarity judgments and a deep neural network (DNN) model trained on natural images, focusing on the triplet odd-one-out task. While the DNN embedding revealed many interpretable dimensions reflecting visual and semantic properties, a direct comparison with humans showed a striking visual bias in the DNN relative to the semantic dominance in humans. Despite similar global representational similarity (Pearson r = 0.55), only 40 DNN dimensions were needed to explain 95% of the variance in human similarity judgments. The results highlight divergent representational strategies between humans and DNNs and demonstrate that interpretability methods alone cannot reveal these differences, emphasizing the importance of direct comparison for understanding human-AI alignment.

## Method Summary
The study compares human and DNN representations using a triplet odd-one-out task where both humans and a pretrained VGG-16 network generate similarity judgments for images from the THINGS dataset. The authors employ a variational embedding technique (VICE) with sparsity and non-negativity constraints to learn low-dimensional embeddings from these judgments, then map DNN features to these embeddings using linear regression. Interpretability is assessed through human ratings and visualization techniques (Grad-CAM, StyleGAN-XL activation maximization), while representational similarity is quantified via correlation of representational similarity matrices between humans and DNNs.

## Key Results
- DNNs exhibit clear visual dominance over semantic properties, while humans show semantic dominance in their similarity judgments
- Despite moderate global alignment (RSA Pearson r = 0.55), only 40 DNN dimensions can explain 95% of variance in human similarity judgments
- Interpretability methods alone cannot reveal the misalignment between human and DNN representations

## Why This Works (Mechanism)

### Mechanism 1
Behavioral task-driven embeddings can reveal interpretable dimensions that align with human representations. The triplet odd-one-out task generates similarity judgments that serve as a proxy for internal cognitive representations. By optimizing embeddings to predict these judgments, the resulting dimensions capture the core features humans use to judge similarity. Core assumption: Human similarity judgments reflect stable, meaningful internal representations that can be modeled computationally.

### Mechanism 2
Visual bias in DNN representations explains misalignment with human semantic dominance. DNNs trained on image classification tasks develop representations that prioritize visual features over semantic ones. When these representations are compared to human-derived dimensions, the visual dominance becomes apparent as misalignment in the relative importance of visual vs. semantic dimensions. Core assumption: Visual features are computationally simpler to extract and thus more heavily weighted in early layers, persisting through the network architecture.

### Mechanism 3
Interpretability methods alone cannot reveal representational misalignment without direct comparison. Standard interpretability techniques can make individual DNN dimensions appear meaningful and consistent. However, these methods don't reveal whether the dimensions correspond to human representations - only direct comparison across domains exposes the misalignment. Core assumption: Visual explanations and dimension maximization provide surface-level interpretability but don't capture the functional correspondence between human and DNN representations.

## Foundational Learning

- **Representational Similarity Analysis (RSA)**: Provides the global metric (Pearson r = 0.55) that establishes whether human and DNN representations are aligned at all, before examining specific dimensions. Quick check: If two models have RSM correlation of 0.8, does this guarantee their internal dimensions are semantically aligned?

- **Variational Inference with Sparsity Constraints**: The VICE method uses spike-and-slab priors to enforce sparsity and non-negativity, creating interpretable dimensions that correspond to specific object properties. Quick check: Why does imposing non-negativity on embedding dimensions help interpretability?

- **Cross-validated prediction of embedding dimensions**: Linear regression mapping between penultimate features and embedding dimensions allows testing how manipulations affect specific dimensions, validating interpretability. Quick check: If R² between penultimate features and embedding dimension is 0.9, what does this tell us about the dimension's reliability?

## Architecture Onboarding

- **Component map**: Image loading → VGG-16 feature extraction → triplet sampling → embedding optimization → Interpretability module (Grad-CAM, activation maximization) → Comparison module (RSA, dimension correlation)
- **Critical path**: Triplet sampling → embedding optimization → dimension labeling → RSA comparison
- **Design tradeoffs**: Using VGG-16 vs. newer architectures trades representational alignment for computational efficiency and interpretability; using larger embedding dimensionality improves variance capture but reduces interpretability
- **Failure signatures**: Poor dimension reproducibility across seeds indicates optimization instability; low correlation between human and DNN RSMs suggests fundamental representational differences; uninterpretable dimensions indicate the sparsity constraints aren't working
- **First 3 experiments**: 
  1. Run embedding optimization on a subset of data to verify convergence and check dimension reproducibility
  2. Apply Grad-CAM to a few dimensions to verify the interpretability pipeline works before full analysis
  3. Perform pairwise dimension correlation between human and DNN embeddings to identify the most aligned dimensions

## Open Questions the Paper Calls Out

### Open Question 1
What specific architectural or training modifications to DNNs would effectively reduce their visual bias and improve alignment with human semantic representations? The paper discusses that CLIP retained a visual bias despite training on semantic image descriptions, suggesting that classification objective alone is not sufficient. Systematic ablation studies comparing different DNN architectures and training objectives would be needed to identify which modifications most effectively reduce visual bias while maintaining performance.

### Open Question 2
To what extent does human core object recognition rely on a visual bias similar to DNNs, and how does this compare to high-level semantic processing? The paper notes that DNN visual bias may reflect how our visual system solves core object recognition, but questions whether this bias is also found in anterior ventral-temporal cortex involved in high-level object processing. Neuroimaging studies comparing visual and semantic processing strategies across different levels of the human visual hierarchy would be needed.

### Open Question 3
How do task instructions (e.g., focus on object vs. entire image) affect the alignment between human and DNN representations? The paper conducted a task validation showing that instruction to focus on the entire image did not significantly affect triplet choice behavior compared to focusing on the object, but notes this as an area for future work. Systematic comparison of human and DNN representations across multiple tasks with varying instructions would be needed.

## Limitations

- Results are based on VGG-16 architecture, which may not reflect representational strategies of modern DNNs
- Findings are limited to the THINGS dataset and odd-one-out task, potentially limiting generalizability
- Human labeling process for DNN dimensions introduces subjective interpretation
- Assumes interpretability methods are applied optimally to reveal surface-level interpretability

## Confidence

- **High Confidence**: Technical implementation of VICE embedding method and overall framework for comparing human and DNN representations are well-supported
- **Medium Confidence**: Global representational similarity finding is strong but interpretation depends on field-specific benchmarks
- **Low Confidence**: Generalizability to other architectures, datasets, or tasks remains uncertain

## Next Checks

1. Apply the same framework to alternative DNN architectures (ResNet, Vision Transformer) to determine whether visual bias is architecture-specific
2. Test whether the semantic-visual bias pattern holds for non-object datasets (scenes, abstract patterns)
3. Compare results using different triplet sampling strategies and similarity metrics (Euclidean distance vs. dot product)