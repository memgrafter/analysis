---
ver: rpa2
title: Initialization using Update Approximation is a Silver Bullet for Extremely
  Efficient Low-Rank Fine-Tuning
arxiv_id: '2411.19557'
source_url: https://arxiv.org/abs/2411.19557
tags:
- gradient
- lora-xs
- lora
- arxiv
- initialization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LoRA-SB, a low-rank fine-tuning method that
  approximates full fine-tuning performance while using 27-90x fewer parameters. The
  core innovation is an initialization strategy that captures the first step of full
  fine-tuning using truncated SVD on averaged gradients, combined with a closed-form
  gradient approximation that eliminates the need for scaling factor tuning.
---

# Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning

## Quick Facts
- arXiv ID: 2411.19557
- Source URL: https://arxiv.org/abs/2411.19557
- Reference count: 40
- Achieves 27-90x parameter reduction while outperforming LoRA-XS and matching LoRA performance

## Executive Summary
This paper introduces LoRA-SB, a low-rank fine-tuning method that achieves full fine-tuning performance using 27-90x fewer parameters. The key innovation is an initialization strategy that approximates the first step of full fine-tuning using truncated SVD on averaged gradients, combined with a closed-form gradient approximation that eliminates scaling factor tuning. The method consistently outperforms existing parameter-efficient fine-tuning approaches across 4 models and 16 datasets spanning arithmetic reasoning, commonsense reasoning, and natural language understanding tasks.

## Method Summary
LoRA-SB approximates full fine-tuning performance by initializing the low-rank update matrices B and A to capture the first update step of full fine-tuning. The method computes averaged gradients over a small subset of training data, applies truncated SVD to obtain optimal low-rank matrices, and then trains only the R matrix while keeping B and A fixed. This initialization strategy guarantees optimal gradient approximation and eliminates the need for scaling factor tuning, achieving significant parameter efficiency while maintaining or improving performance compared to standard LoRA methods.

## Key Results
- LoRA-SB achieves 27-90x parameter reduction compared to full fine-tuning
- Outperforms LoRA-XS by 3-5% on GSM8K and MATH benchmarks
- Matches or exceeds LoRA performance while using significantly fewer parameters
- Initialization using 0.1% of training data achieves near-optimal performance

## Why This Works (Mechanism)

### Mechanism 1
LoRA-SB's initialization approximates the first step of full fine-tuning, capturing the most relevant adaptation direction for the target task. The method computes the sign of averaged gradients over a subset of training data to simulate AdamW's first update step, then uses truncated SVD to obtain an optimal low-rank approximation of this update. This creates initialization matrices B_init and A_init that form orthonormal bases capturing the initial adaptation direction. The core assumption is that the first update step contains the most informative direction for task-specific adaptation.

### Mechanism 2
LoRA-SB guarantees optimal gradient approximation throughout training by maintaining orthonormal bases in the update matrices. The initialization ensures B_init and A_init form orthonormal bases (B⊤B = AA⊤ = I), which simplifies the optimal gradient update formula to g_R = g_R^LoRA-XS. This eliminates the need for complex matrix inversions and scaling factor tuning while preserving the approximation quality. The core assumption is that orthonormal initialization can be maintained throughout training.

### Mechanism 3
LoRA-SB eliminates scaling factor sensitivity by approximating the full fine-tuning gradient. Through the gradient approximation approach, the equivalent gradient ˜g = sBg_RA becomes independent of the scaling factor s when B and A are orthonormal. This means updates to W depend solely on the gradient approximation, making additional scaling redundant. The core assumption is that when the low-rank approximation accurately captures the full fine-tuning gradient, the natural scaling of the gradient is sufficient without manual adjustment.

## Foundational Learning

- **Truncated Singular Value Decomposition (SVD)**
  - Why needed here: Used to compute the optimal low-rank approximation of the initial full fine-tuning update
  - Quick check question: What mathematical property guarantees that truncated SVD provides the best rank-r approximation of a matrix?

- **Gradient approximation in constrained subspaces**
  - Why needed here: The core insight that updating R with g_R is equivalent to updating W with a low-rank equivalent gradient
  - Quick check question: How does the chain rule relationship between ∂L/∂R and ∂L/∂W enable gradient approximation in LoRA-XS?

- **Orthonormal matrix properties**
  - Why needed here: Orthonormal bases (B⊤B = I, AA⊤ = I) simplify gradient calculations and eliminate the need for matrix inversions
  - Quick check question: Why does having orthonormal matrices B and A make the gradient update formula g_R = g_R^LoRA-XS particularly simple?

## Architecture Onboarding

- **Component map**: Pre-trained model weights (W0) -> Low-rank update components (B, R, A) -> Gradient approximation layer -> Training loop
- **Critical path**: 1) Compute initial gradient approximation using 0.1% of training data, 2) Perform truncated SVD to initialize B, R, A, 3) Train model, updating only R while keeping B and A fixed, 4) Use optimal gradient approximation formula throughout training
- **Design tradeoffs**: Fixed B and A provide stability but reduce flexibility compared to trainable LoRA; Orthonormal initialization simplifies math but may limit expressive power; Single initialization step trades computational overhead for consistent performance
- **Failure signatures**: Poor performance on tasks requiring very different adaptation directions than captured in initialization; Sensitivity to noisy or unrepresentative initialization data subset; Degradation when task requires high-rank updates beyond what the subspace can capture
- **First 3 experiments**: 1) Compare initialization quality: Run truncated SVD on different matrices and measure resulting performance, 2) Test gradient approximation importance: Remove optimal gradient approximation and compare loss curves, 3) Ablation on initialization data size: Vary the percentage of training data used for initialization

## Open Questions the Paper Calls Out

- **Open Question 1**: How does LoRA-SB's initialization strategy generalize to tasks requiring domain-specific knowledge versus general language understanding? The paper doesn't analyze domain-specific adaptation effectiveness across specialized domains.

- **Open Question 2**: What is the theoretical relationship between initialization dataset size and LoRA-SB's convergence speed versus final performance? The paper mentions using 0.1% of training data but doesn't provide a mathematical framework for optimal initialization sample selection.

- **Open Question 3**: How does LoRA-SB's performance scale with model size beyond the 9B parameter models tested? The paper demonstrates effectiveness on medium-sized models but doesn't address computational implications for models 30B+ parameters.

- **Open Question 4**: What is the impact of different optimizers beyond AdamW on LoRA-SB's initialization and performance? The paper specifically mentions AdamW is used but doesn't explore alternative optimizers or how they would affect the approximation quality.

## Limitations

- The assumption that orthonormal bases remain optimal throughout training is unverified beyond the reported experiments
- Initialization using only 0.1% of training data could lead to noisy or unrepresentative subspace selection
- The rigid structure of fixed B and A matrices may limit adaptability to complex tasks requiring more flexible adaptation patterns

## Confidence

- **High confidence**: Parameter efficiency claims (27-90x reduction) and LoRA-SB outperforming LoRA-XS are directly verifiable through experimental results
- **Medium confidence**: The mathematical justification for initialization approximating first update step is sound, but practical significance for subsequent training needs more investigation
- **Low confidence**: The guarantee that loss reduction and optimal gradient approximation are preserved throughout training is the weakest claim, as there's no mechanism to maintain optimality as training progresses

## Next Checks

1. **Training dynamics validation**: Track the angle between the actual gradient and the approximated gradient throughout training to verify that the approximation quality is maintained, not just at initialization

2. **Initialization sensitivity analysis**: Systematically vary the initialization data percentage (0.01%, 0.1%, 0.5%, 1%) and compute the resulting performance variance across multiple runs to quantify the method's sensitivity to initialization quality

3. **Cross-task generalization test**: Evaluate LoRA-SB initialized on one task when fine-tuned on a related but different task to test whether the initialization captures broadly useful adaptation directions or is overly task-specific