---
ver: rpa2
title: A Novel Two-Step Fine-Tuning Pipeline for Cold-Start Active Learning in Text
  Classification Tasks
arxiv_id: '2407.17284'
source_url: https://arxiv.org/abs/2407.17284
tags:
- fine-tuning
- classification
- data
- labeled
- instances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DoTCAL, a novel two-step fine-tuning pipeline
  for BERT-based contextual embeddings in cold-start active learning (AL) scenarios
  for text classification. Traditional AL approaches often struggle due to limited
  labeled data at the start.
---

# A Novel Two-Step Fine-Tuning Pipeline for Cold-Start Active Learning in Text Classification Tasks

## Quick Facts
- arXiv ID: 2407.17284
- Source URL: https://arxiv.org/abs/2407.17284
- Reference count: 40
- Primary result: DoTCAL pipeline achieves up to 33% improvement in Macro-F1 while reducing labeling efforts by half in cold-start AL scenarios

## Executive Summary
This paper introduces DoTCAL, a novel two-step fine-tuning pipeline for BERT-based contextual embeddings in cold-start active learning (AL) scenarios for text classification. Traditional AL approaches struggle due to limited labeled data at the start, but DoTCAL addresses this by first adapting the model to the domain using unlabeled data through masked language modeling, then fine-tuning it further with actively labeled instances selected by AL strategies. Experiments on eight diverse text classification datasets demonstrate that DoTCAL significantly outperforms traditional one-step fine-tuning, achieving up to a 33% improvement in Macro-F1 while reducing labeling efforts by half. The study also reveals that traditional text representations like Bag-of-Words (BoW) and Latent Semantic Indexing (LSI) can be more effective than BERT-based embeddings in certain low-budget and hard-to-classify scenarios.

## Method Summary
DoTCAL implements a two-step fine-tuning pipeline that first adapts BERT to the target domain using masked language modeling on all available unlabeled data, then further fine-tunes the model using actively labeled instances selected by density-weighted diversity-based query strategies. The approach is compared against traditional one-step fine-tuning and tested with four different text representations (BoW, LSI, FastText, BERT) across eight benchmark datasets. Experiments vary budget sizes from 50 to 1000 labeled instances, measuring macro-F1 classification performance while also evaluating labeling effort reduction.

## Key Results
- DoTCAL achieves up to 33% improvement in Macro-F1 compared to one-step fine-tuning
- The pipeline reduces labeling efforts by half while maintaining or improving classification performance
- Traditional representations (BoW, LSI) outperform BERT-based embeddings in low-budget scenarios and hard-to-classify tasks, with LSI showing up to 59% improvement over BERT
- DoTCAL is particularly effective when combined with appropriate text representations for the selection stage

## Why This Works (Mechanism)

### Mechanism 1
Domain adaptation via masked language modeling (MLM) provides a strong starting representation for task-specific fine-tuning, reducing the need for labeled data. The two-step fine-tuning pipeline first adapts the BERT model to the target domain vocabulary using MLM on all unlabeled data, creating a domain-specific language model. This pre-trained model is then further fine-tuned using actively labeled instances, leading to better task-specific performance with fewer labeled examples.

### Mechanism 2
LSI representation reduces dimensionality in a way that emphasizes shared latent terms across documents, improving active learning instance selection with limited budgets. By compressing the high-dimensional BoW space into a lower-dimensional space using singular value decomposition, LSI aggregates information from many instances into fewer dimensions. This compression helps identify representative and diverse instances more effectively when labeling budgets are small, as it emphasizes common patterns rather than noise from sparse high-dimensional features.

### Mechanism 3
The density-weighted diversity-based query strategy (DWDS) effectively selects representative and diverse instances when combined with appropriate text representations. DWDS selects instances with high density (representative) while ensuring diversity by avoiding similar instances already selected. This combinatorial optimization approach, enhanced with beam search, works best when the underlying text representation captures semantic similarity well, allowing the algorithm to identify truly representative and complementary instances.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the core mechanism for domain adaptation in DoTCAL's first step, allowing the model to learn domain-specific vocabulary patterns from unlabeled data.
  - Quick check question: What is the objective function used in MLM, and how does it differ from standard language modeling?

- Concept: Singular Value Decomposition (SVD) and dimensionality reduction
  - Why needed here: SVD is the mathematical foundation of LSI, which compresses high-dimensional text representations into lower-dimensional spaces that can improve AL performance with limited budgets.
  - Quick check question: How does SVD reduce dimensionality while attempting to preserve the most important relationships between terms and documents?

- Concept: Active Learning query strategies (density and diversity)
  - Why needed here: Understanding density and diversity measures is crucial for implementing and tuning the DWDS algorithm used for instance selection in cold-start scenarios.
  - Quick check question: How do density and diversity measures complement each other in selecting informative instances for active learning?

## Architecture Onboarding

- Component map: raw text → preprocessing → representation (BoW/LSI/FastText/BERT) → DWDS selection → DoTCAL fine-tuning (MLM + task adaptation) → SVM classifier → macro-F1 evaluation
- Critical path: 1. Preprocess text data 2. Generate representations 3. Run DWDS selection algorithm 4. Apply DoTCAL fine-tuning pipeline 5. Train SVM classifier 6. Evaluate macro-F1
- Design tradeoffs: BERT offers state-of-the-art performance but requires more computational resources; BoW/LSI are simpler but may underperform on complex tasks; DoTCAL reduces labeling effort but adds complexity
- Failure signatures: Poor performance across all budgets (text representation or preprocessing issue); good performance only at high budgets (fine-tuning not effective for cold-start); inconsistent results across folds (instability in instance selection or fine-tuning)
- First 3 experiments: 1. Compare macro-F1 of untuned vs tuned BERT across all datasets with budget=200 2. Test DoTCAL vs one-step fine-tuning on WebKB with varying budgets 3. Evaluate BoW, LSI, and BERT representations in selection stage while fixing classification representation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DoTCAL perform with other transformer-based language models like XLNET or GPT-3, especially in comparison to RoBERTa?
- Basis in paper: [explicit] The authors mention plans to test other Transformers (XLNET, GPT-3) as future work.
- Why unresolved: The paper only compares BERT and RoBERTa, leaving performance with other models unknown.
- What evidence would resolve it: Experiments comparing DoTCAL's performance with XLNET and GPT-3 across the same datasets and budget scenarios.

### Open Question 2
- Question: What is the optimal number of latent dimensions (d) for LSI in low-budget AL scenarios, and how does this vary across different datasets and tasks?
- Basis in paper: [inferred] The authors observe that using only 96 latent dimensions can lead to gains in macro-F1 for small budgets, but they don't determine an optimal value.
- Why unresolved: The paper only tests a few specific values of d and doesn't explore the full range or provide a general rule for determining the optimal value.
- What evidence would resolve it: Systematic experiments varying d across a wider range and analyzing the relationship between d, budget, and dataset characteristics.

### Open Question 3
- Question: How does DoTCAL perform in domains with extremely limited data, such as medical or legal text classification?
- Basis in paper: [explicit] The authors mention plans to apply their approach to other domains with data scarcity, including medical, as future work.
- Why unresolved: The paper only evaluates on general-purpose text classification datasets, leaving performance in specialized domains unknown.
- What evidence would resolve it: Experiments applying DoTCAL to medical or legal text classification tasks with limited labeled data.

## Limitations

- The effectiveness of domain adaptation via MLM depends heavily on the quality and quantity of unlabeled data available, which the paper doesn't fully explore
- The analysis of traditional representation advantages lacks deeper exploration of when and why these approaches outperform modern embeddings
- The paper doesn't address scenarios where domain shift exists between unlabeled and target data, which could affect MLM adaptation effectiveness

## Confidence

- DoTCAL pipeline effectiveness: High - well-supported by experimental results across multiple datasets
- MLM domain adaptation mechanism: Medium - theoretical justification is strong but limited empirical validation of the adaptation step itself
- Traditional representation advantages: Medium - results show clear patterns but lack deeper analysis of underlying mechanisms

## Next Checks

1. Test DoTCAL with varying amounts of unlabeled data to determine the minimum threshold needed for effective domain adaptation
2. Compare the two-step pipeline performance when domain shift exists between unlabeled and target data versus when they align well
3. Conduct ablation studies isolating the contribution of each fine-tuning step to quantify their individual impact on performance