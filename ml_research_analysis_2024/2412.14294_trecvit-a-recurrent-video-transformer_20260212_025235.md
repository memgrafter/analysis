---
ver: rpa2
title: 'TRecViT: A Recurrent Video Transformer'
arxiv_id: '2412.14294'
source_url: https://arxiv.org/abs/2412.14294
tags:
- video
- trecvit
- recurrent
- frames
- frame
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TRecViT, a hybrid video architecture that
  combines gated linear recurrent units (LRUs) for temporal modeling with ViT blocks
  for spatial and channel processing. By factorizing video modeling across time, space,
  and channels, TRecViT achieves strong performance on both sparse (video classification)
  and dense (point tracking) tasks while being significantly more efficient than pure
  attention models.
---

# TRecViT: A Recurrent Video Transformer

## Quick Facts
- arXiv ID: 2412.14294
- Source URL: https://arxiv.org/abs/2412.14294
- Authors: Viorica Pătrăucean; Xu Owen He; Joseph Heyward; Chuhan Zhang; Mehdi S. M. Sajjadi; George-Cristian Muraru; Artem Zholus; Mahdi Karami; Ross Goroshin; Yutian Chen; Simon Osindero; João Carreira; Razvan Pascanu
- Reference count: 40
- Key outcome: TRecViT achieves strong performance on both sparse and dense tasks while being significantly more efficient than pure attention models

## Executive Summary
TRecViT introduces a hybrid video architecture that combines gated linear recurrent units (LRUs) for temporal modeling with ViT blocks for spatial and channel processing. By factorizing video modeling across time, space, and channels, this architecture achieves strong performance on both sparse (video classification) and dense (point tracking) tasks while being significantly more efficient than pure attention models. The approach demonstrates superior efficiency metrics including 3× fewer parameters, 12× smaller memory footprint, and 5× lower FLOPs compared to ViViT-L while maintaining competitive accuracy.

## Method Summary
TRecViT presents a novel hybrid video architecture that factorizes video processing into three distinct stages: temporal modeling using gated linear recurrent units (LRUs), spatial processing using standard ViT blocks, and channel mixing using 1x1 convolutions. This factorization allows the model to efficiently process videos by avoiding expensive full 4D attention over space and time. The LRU component processes frames sequentially in time, while the ViT blocks handle spatial relationships within each frame. The architecture demonstrates versatility by excelling at both sparse tasks like video classification and dense tasks like point tracking, with particular efficiency advantages making it suitable for deployment in resource-constrained environments.

## Key Results
- Outperforms or matches ViViT-L on Kinetics400 and SSv2 datasets with 3× fewer parameters, 12× smaller memory footprint, and 5× lower FLOPs
- Demonstrates strong results in self-supervised masked autoencoding and long video memorization tasks
- Shows versatility across both sparse (video classification) and dense (point tracking) tasks

## Why This Works (Mechanism)
The architecture's efficiency stems from factorizing the video modeling problem across three dimensions: time, space, and channels. By using LRUs for temporal modeling, the model avoids the quadratic complexity of full attention across time steps. The ViT blocks handle spatial relationships within each frame, while 1x1 convolutions manage channel mixing. This decomposition allows the model to capture temporal dependencies efficiently while maintaining spatial reasoning capabilities, resulting in both performance and efficiency gains over pure attention-based approaches.

## Foundational Learning
- **Gated Linear Recurrent Units (LRUs)**: A variant of recurrent neural networks that use gated mechanisms to control information flow, essential for processing sequential data efficiently without the memory overhead of attention mechanisms
- **Video Transformer Architectures**: Neural network designs that apply transformer blocks to video data, enabling spatial and temporal reasoning but often suffering from high computational costs
- **Factorized Video Modeling**: The decomposition of video processing into separate temporal, spatial, and channel operations, which reduces computational complexity while maintaining representational power
- **Self-Supervised Masked Autoencoding**: A training paradigm where models learn to reconstruct masked portions of input data, useful for learning rich representations without labeled data
- **Dense vs Sparse Video Tasks**: Dense tasks (like point tracking) require per-pixel or per-point predictions, while sparse tasks (like classification) require global video-level predictions, demanding different architectural considerations

## Architecture Onboarding

Component Map:
Input Frames -> LRU Temporal Processing -> ViT Spatial Processing -> 1x1 Channel Mixing -> Output

Critical Path:
The critical computational path involves sequential processing through the LRU layers for temporal modeling, followed by parallel ViT processing for spatial reasoning, and finally channel mixing. The LRU bottleneck becomes the primary sequential constraint, while ViT blocks can be parallelized across spatial dimensions.

Design Tradeoffs:
- **Efficiency vs. Expressiveness**: Factorizing across time, space, and channels reduces computational complexity but may miss some cross-dimensional interactions that full attention could capture
- **Sequential vs. Parallel Processing**: LRUs enable efficient temporal modeling but introduce sequential dependencies that limit parallelization compared to pure attention approaches
- **Memory vs. Performance**: The 12× memory reduction comes at the cost of potentially reduced temporal context compared to full attention models, though the LRU design mitigates this

Failure Signatures:
- Poor temporal modeling on datasets requiring long-range dependencies
- Suboptimal performance on tasks requiring complex spatial-temporal interactions that cross the factorization boundaries
- Potential degradation when input frame rates are very high, overwhelming the LRU's capacity

First Experiments:
1. Ablation study removing LRU component to quantify temporal modeling contribution
2. Varying the number of recurrent steps to find the optimal tradeoff between efficiency and performance
3. Testing with different frame rates to understand the temporal modeling limits

## Open Questions the Paper Calls Out
None

## Limitations
- Efficiency comparisons appear selective, focusing primarily on ViViT-L without thorough benchmarking against other efficient video architectures like TimeSformer
- The 12× memory reduction claim needs clarification about which specific memory metric is being measured (activation memory vs. parameter memory)
- Generalization of LRU-based temporal modeling to longer temporal contexts beyond evaluated datasets is not thoroughly explored

## Confidence

High confidence:
- The architectural design combining LRUs with ViT blocks is sound and the ablation studies showing the importance of the recurrent component are convincing

Medium confidence:
- The efficiency claims are well-supported by the presented metrics, though the comparison methodology could be more comprehensive
- The self-supervised masked autoencoding results demonstrate capability but lack comparison to state-of-the-art methods in this specific domain

## Next Checks
1. **Extended temporal validation**: Evaluate TRecViT on datasets with significantly longer video sequences (e.g., ActivityNet, long-form video understanding tasks) to validate the LRU component's effectiveness for extended temporal modeling

2. **Efficiency benchmark expansion**: Compare TRecViT against additional efficient video architectures (TimeSformer, MViT, X3D) using standardized efficiency metrics including wall-clock inference time across different hardware platforms

3. **Memory analysis detail**: Provide detailed breakdown of memory usage (activation memory, parameter memory, gradient memory) during training and inference, along with analysis of memory scaling behavior with respect to input resolution and temporal length