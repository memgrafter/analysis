---
ver: rpa2
title: Scaling Laws for Linear Complexity Language Models
arxiv_id: '2406.16690'
source_url: https://arxiv.org/abs/2406.16690
tags:
- mode
- score
- hgrn2
- llama
- haystack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper establishes scaling laws for linear complexity language
  models, focusing on three architectures: TNL (data-independent decay), HGRN2 (data-dependent
  decay), and cosFormer2 (no decay). The study trains models ranging from 70M to 7B
  parameters on a 300B-token corpus and evaluates them on downstream tasks including
  validation loss, commonsense reasoning, and information retrieval.'
---

# Scaling Laws for Linear Complexity Language Models

## Quick Facts
- arXiv ID: 2406.16690
- Source URL: https://arxiv.org/abs/2406.16690
- Authors: Xuyang Shen; Dong Li; Ruitao Leng; Zhen Qin; Weigao Sun; Yiran Zhong
- Reference count: 40
- Key outcome: Linear complexity language models exhibit scaling laws similar to transformers while showing superior linguistic proficiency and knowledge retention, though weaker retrieval performance.

## Executive Summary
This study establishes scaling laws for linear complexity language models across three architectures (TNL, HGRN2, cosFormer2) trained on a 300B-token corpus ranging from 70M to 7B parameters. The research demonstrates that these models follow similar scaling behaviors to traditional transformers while achieving better linguistic performance and knowledge retention. However, linear models show limitations in retrieval tasks due to their fixed hidden space size, unable to leverage the "Going Through a Book" mechanism that softmax attention employs. The study also reveals that aspect ratio and context length significantly impact model capacity for linear models, contrary to previous scaling laws.

## Method Summary
The study trains linear complexity language models (TNL, HGRN2, cosFormer2) with 70M-7B parameters on a 300B-token bilingual corpus using fixed learning rate scheduling, Adam optimizer (3e-4 lr, 0.1 weight decay), and 4M token global batch size. Models are evaluated on validation loss, WIKITEXT-2/LAMBADA perplexity, CSR benchmarks (BoolQ, PIQA, HellaSwag, WinoGrande, ARC, OpenBookQA), NIAH retrieval, and SCROLLS tasks. Scaling laws are established by regressing training loss against FLOPs to derive power-law relationships, following Hoffmann et al. (2022)'s methodology.

## Key Results
- Linear complexity models exhibit similar scaling capabilities to conventional transformers for validation loss, commonsense reasoning, and generation tasks
- Data-dependent decay in HGRN2 shows no significant advantage over data-independent decay for most tasks, though it benefits retrieval tasks
- Linear models struggle with retrieval tasks due to fixed hidden space size, unlike softmax attention which can recompute information via "Going Through a Book" mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear complexity models exhibit scaling behaviors similar to traditional transformers when optimized for compute budget
- Mechanism: Training loss L follows power-law relationship with compute budget C: L(C) ∝ C^α, where optimal model size N_opt and dataset size D_opt scale as N_opt ∝ C^a and D_opt ∝ C^b respectively
- Core assumption: Training loss is unbiased proxy for test loss, and scaling laws from small models extrapolate to larger models
- Evidence anchors: [abstract] "exhibit similar scaling capabilities as conventional transformer-based models"; [section] "used training loss as regression target to establish power law equations against FLOPs"
- Break condition: If training loss no longer correlates with test loss at larger scales, or power-law relationship breaks down due to architectural constraints

### Mechanism 2
- Claim: Data-dependent decay in HGRN2 provides no significant advantage over data-independent decay in most tasks, but benefits retrieval tasks
- Mechanism: Both decay types influence attention mechanism's ability to weigh past information; data-dependent adapts based on input data while data-independent uses fixed decay schedule
- Core assumption: Decay mechanism's effectiveness is task-dependent, with retrieval tasks benefiting more from adaptive weighting
- Evidence anchors: [abstract] "data-dependent decay in HGRN2 shows no significant difference from data-independent decay in most tasks, though it benefits retrieval tasks"; [section] "HGRN2 displays performance on par with no-decay linear attention in retrieval and generation tasks"
- Break condition: If retrieval advantage of data-dependent decay diminishes at larger scales or with different data distributions

### Mechanism 3
- Claim: Linear models struggle with retrieval tasks due to fixed hidden space size, unlike softmax attention which can recompute information via "Going Through a Book (GTB)"
- Mechanism: Softmax attention can be reformulated as additive linear RNN that recomputes hidden states from initial time step at each update, allowing precise retention of input information; linear models lack this recomputation
- Core assumption: Ability to recompute hidden states from beginning is crucial for accurate information retrieval in long contexts
- Evidence anchors: [section] "linear complexity models have limited capacity in retrieval tasks. This is because linear models maintain a fixed size of hidden space, which makes it difficult to retain input information precisely... For linear models, there is no recomputation"
- Break condition: If linear models develop alternative mechanisms for precise information retention that negate GTB advantage of softmax attention

## Foundational Learning

- Concept: Power-law relationships and scaling laws
  - Why needed here: Understanding how model performance scales with parameters, data, and compute is crucial for optimizing resource allocation and predicting model capabilities
  - Quick check question: If a model's loss scales as L ∝ N^(-α) with parameters N, what happens to the loss if you double the number of parameters?

- Concept: Attention mechanisms and their computational complexity
  - Why needed here: Linear complexity models aim to reduce O(N^2) complexity of traditional attention to O(N) or O(N log N), which is fundamental to their scalability
  - Quick check question: What is the computational complexity of standard softmax attention, and how do linear attention variants reduce this complexity?

- Concept: Decay mechanisms in sequence models
  - Why needed here: Different decay types (data-independent, data-dependent, no decay) significantly impact model performance, especially in retrieval tasks
  - Quick check question: How does the choice of decay mechanism affect the model's ability to weigh past information in long sequences?

## Architecture Onboarding

- Component map: Input embedding layer -> Linear attention with decay -> Gating -> Feed-forward -> Output embedding layer
- Critical path: Token embedding → Linear attention with decay → Gating → Feed-forward → Output projection
- Design tradeoffs:
  - Linear attention vs. softmax attention: Computational efficiency vs. retrieval capability
  - Decay type: Fixed (data-independent) vs. adaptive (data-dependent) vs. none
  - Model shape (aspect ratio): Impact on capacity and task performance
- Failure signatures:
  - Retrieval task failures: Likely due to fixed hidden space size in linear models
  - Degraded performance with longer context: Possible decay mechanism issues or insufficient model capacity
  - Unexpected scaling behavior: Potential break in power-law relationships
- First 3 experiments:
  1. Verify scaling law: Train small models (70M-410M params) and plot loss vs. compute to confirm power-law relationship
  2. Compare decay types: Train HGRN2 with both decay types on retrieval and non-retrieval tasks to quantify performance differences
  3. Test aspect ratio impact: Train 1B parameter models with varying hidden dimensions and layer counts to observe effects on CSR and validation perplexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific modifications to linear attention mechanisms could enable them to match the retrieval capabilities of softmax attention?
- Basis in paper: [explicit] The paper explicitly states that linear models struggle with retrieval tasks because they cannot precisely retain input information without a "Going Through a Book" process that softmax attention employs
- Why unresolved: The paper identifies the problem but does not propose or test specific architectural modifications to address this limitation
- What evidence would resolve it: Experiments comparing modified linear attention mechanisms (e.g., with recomputation strategies) against standard softmax attention on NIAH and SCROLLS benchmarks

### Open Question 2
- Question: How does the data distribution and composition affect the scaling laws for linear complexity models compared to transformer models?
- Basis in paper: [inferred] The paper acknowledges that all models were trained on a fixed dataset, potentially overlooking the influence of data distribution on scaling laws
- Why unresolved: The study used a single dataset, preventing analysis of how different data characteristics impact scaling behavior
- What evidence would resolve it: Scaling law experiments across multiple datasets with varying characteristics (domain, quality, redundancy) for both linear and transformer models

### Open Question 3
- Question: What is the relationship between pre-training context length and the effective context length during inference for linear complexity models?
- Basis in paper: [explicit] The paper shows that linear models fail to retrieve information from contexts longer than their pre-training length, unlike LLaMA which scales better
- Why unresolved: The study only tested a few context lengths and did not explore the underlying mechanisms of this limitation
- What evidence would resolve it: Detailed experiments mapping pre-training context length to maximum effective inference context length, and analysis of attention patterns across different context scales

## Limitations

- Corpus and Data Limitations: The study uses a proprietary 300B-token bilingual corpus processed through a "self-cleaning scheme," but details about composition, domain distribution, and cleaning methodology are not specified
- Scaling Law Extrapolation Uncertainty: While scaling laws are established for 70M-7B parameters, there is inherent uncertainty in extrapolating power-law relationships to much larger models
- Retrieval Task Performance Gap: The study identifies linear models' struggle with retrieval tasks but provides limited mechanistic explanation beyond the "fixed hidden space size" hypothesis

## Confidence

- High Confidence: Scaling laws for linear complexity models follow power-law relationships with compute budget, similar to transformers
- Medium Confidence: Linear complexity models exhibit similar scaling capabilities to conventional transformers for validation loss, commonsense reasoning, and generation tasks
- Low Confidence: Data-dependent decay in HGRN2 shows no significant difference from data-independent decay in most tasks, though it benefits retrieval tasks

## Next Checks

1. Verify Power-Law Extrapolation: Train a 14B-parameter variant of each architecture (TNL, HGRN2, cosFormer2) to test whether established scaling laws accurately predict performance at twice the largest studied scale
2. Mechanistic Study of Retrieval Performance Gap: Implement a modified linear attention mechanism that incorporates a GTB-like recomputation step and evaluate its impact on retrieval task performance
3. Comprehensive Analysis of Decay Mechanisms: Conduct ablation studies on HGRN2 with different decay schedules across diverse task types and sequence lengths, including statistical significance testing