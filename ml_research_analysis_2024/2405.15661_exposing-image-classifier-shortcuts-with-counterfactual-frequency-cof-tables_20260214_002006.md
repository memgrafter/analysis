---
ver: rpa2
title: Exposing Image Classifier Shortcuts with Counterfactual Frequency (CoF) Tables
arxiv_id: '2405.15661'
source_url: https://arxiv.org/abs/2405.15661
tags:
- image
- explanations
- segments
- counterfactual
- shortcuts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Counterfactual Frequency (CoF) tables, a
  method to aggregate local counterfactual explanations into global insights for detecting
  shortcuts in image classifiers. The approach builds on Semantic Counterfactuals
  for Accurate Picture (SCAP) explanations, which use segmentation models and editing
  functions to generate semantically meaningful counterfactuals.
---

# Exposing Image Classifier Shortcuts with Counterfactual Frequency (CoF) Tables

## Quick Facts
- arXiv ID: 2405.15661
- Source URL: https://arxiv.org/abs/2405.15661
- Reference count: 31
- One-line primary result: CoF tables aggregate semantic counterfactual explanations to expose model shortcuts, even those with low frequency.

## Executive Summary
This paper introduces Counterfactual Frequency (CoF) tables, a method to aggregate local counterfactual explanations into global insights for detecting shortcuts in image classifiers. The approach builds on Semantic Counterfactuals for Accurate Picture (SCAP) explanations, which use segmentation models and editing functions to generate semantically meaningful counterfactuals. CoF tables quantify how often specific segments cause changes in model predictions, exposing shortcuts. The method is demonstrated on datasets like biased MNIST, Biased Action Recognition (BAR), and ImageNet, successfully identifying background biases and watermark shortcuts.

## Method Summary
The method generates Semantic Counterfactuals for Accurate Picture (SCAP) explanations using segmentation and editing functions, then aggregates these into CoF tables to quantify the impact of semantically similar segments on model decisions. SCAP explanations are built by segmenting images, editing specific segments, and recording whether the model's prediction changes. CoF tables aggregate these explanations by segment label, revealing which segments most frequently cause prediction changes, thus exposing shortcuts.

## Key Results
- CoF tables successfully identified background color as the sole predictor in biased MNIST.
- In ImageNet, watermarks in horse images were detected as a shortcut, even though present in only 10% of images.
- The method effectively revealed background biases in Biased Action Recognition (BAR) dataset, such as grass for "climbing" and water for "swimming."

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoF tables aggregate local counterfactual explanations into global insights by quantifying the frequency with which specific semantically labeled segments cause prediction changes.
- Mechanism: For each image, segmentation labels segments (e.g., "water," "grass," "rock"). An edit function modifies these segments. If the model's prediction changes, that segment is recorded as causing a counterfactual. CoF tables then count how often each segment label across all images causes such changes, highlighting shortcuts.
- Core assumption: Semantic labels from segmentation models are meaningful and consistent across images, enabling valid aggregation.
- Evidence anchors:
  - [abstract]: "CoF tables quantify how often specific segments cause changes in model predictions, exposing shortcuts."
  - [section]: "For each image ix within I, a segmentation model delineates a set of m segments... each assigned a name based on its semantic characteristics."
  - [corpus]: Weak. Related works focus on counterfactual explanations but not on aggregation via frequency tables.
- Break condition: If segmentation labels are inconsistent or lack semantic meaning, aggregation becomes invalid and shortcuts may be missed.

### Mechanism 2
- Claim: Semantic Counterfactuals for Accurate Picture (SCAP) explanations provide labels for counterfactual-causing segments, which are essential for CoF table construction.
- Mechanism: SCAP combines segmentation, editing, classification, and counterfactual search. The segmentation step produces labeled segments; editing modifies them; if the classifier's output changes, the segment label and edit are recorded as a SCAP explanation. These labeled explanations feed directly into CoF tables.
- Core assumption: The segmentation model reliably identifies semantically meaningful segments, and the edit function meaningfully alters the segment without destroying image context.
- Evidence anchors:
  - [abstract]: "These labels must have semantic meaning to ensure meaningful aggregation based on the label."
  - [section]: "SCAP explanations provide labels for the segments in an image that cause counterfactuals, which we can then use to create CoF tables."
  - [corpus]: Weak. Related works discuss counterfactuals but not SCAP's specific segmentation-and-edit pipeline.
- Break condition: If the edit function fails to create valid counterfactuals (e.g., blurring water doesn't obscure it), SCAP explanations and thus CoF tables lose fidelity.

### Mechanism 3
- Claim: CoF tables expose shortcuts even when they are infrequent by aggregating evidence across many images.
- Mechanism: Even if a shortcut (e.g., watermark) appears in only 10% of images, SCAP explanations will still record it when present. CoF tables aggregate these rare events; if the frequency of that segment causing counterfactuals is high when it does appear, the shortcut is exposed.
- Core assumption: Shortcuts, even when rare, consistently cause counterfactuals when the segment is edited.
- Evidence anchors:
  - [abstract]: "The results show that CoF tables can effectively reveal model shortcuts, even those with low frequency."
  - [section]: "Watermarks are present in only 10% of images... The removal of watermarks results in counterfactuals in approximately 1% of images, indicating that manual detection would require extensive analysis."
  - [corpus]: Weak. No corpus evidence directly addresses low-frequency shortcut detection.
- Break condition: If a shortcut's effect is inconsistent or masked by other features, its frequency in CoF tables may be diluted and go unnoticed.

## Foundational Learning

- Concept: Semantic segmentation and labeling of image regions.
  - Why needed here: CoF tables require consistent, meaningful labels to aggregate counterfactual evidence across images.
  - Quick check question: Can you explain how a segmentation model like DETR assigns labels to image regions, and why label consistency matters for aggregation?

- Concept: Counterfactual explanations in machine learning.
  - Why needed here: SCAP explanations are built on counterfactual reasoningâ€”identifying minimal changes that flip a model's prediction.
  - Quick check question: What distinguishes a counterfactual explanation from a saliency map, and why is that distinction important for shortcut detection?

- Concept: Frequency-based aggregation of local model behavior.
  - Why needed here: CoF tables summarize many local counterfactual events into global insights, revealing patterns invisible at the instance level.
  - Quick check question: How does counting the frequency of segment-induced counterfactuals help expose shortcuts that might be missed by inspecting individual explanations?

## Architecture Onboarding

- Component map:
  Segmentation module -> Edit function module -> Classifier wrapper -> Counterfactual search -> CoF table builder -> Output visualizer

- Critical path:
  1. Segment image -> label regions
  2. For each labeled region, apply edit
  3. Classify original and edited image
  4. If prediction changes, record (label, edit, outcome)
  5. Aggregate records into CoF table
  6. Visualize results

- Design tradeoffs:
  - Segmentation granularity vs. semantic clarity: finer segments give more detail but may lack clear labels.
  - Edit function aggressiveness vs. realism: stronger edits may better expose shortcuts but risk unrealistic images.
  - Aggregation scope vs. noise: broader aggregation reveals global trends but may dilute rare but important shortcuts.

- Failure signatures:
  - CoF table shows no dominant segments -> either no shortcuts or segmentation/editing too weak.
  - CoF table dominated by generic segments (e.g., "unrecognized") -> segmentation model failing to assign meaningful labels.
  - Inconsistent counterfactual triggers across similar images -> edit function or segmentation unstable.

- First 3 experiments:
  1. **MNIST color bias test**: Apply CoF tables to a simple dataset where background color is the only shortcut; verify CoF table shows 100% frequency for background segments.
  2. **ImageNet boat water vs. grass**: Use generative inpainting to replace water segments with grass; check if CoF table shows high frequency for water causing counterfactuals.
  3. **Watermark shortcut in horses**: Add watermarks to 10% of horse images; run SCAP with inpainting to remove watermarks; verify CoF table exposes watermark as a frequent counterfactual cause.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important considerations for future research, such as the optimal choice of edit functions, the extension to multi-modal data, and integration into the model development lifecycle.

## Limitations
- Relies heavily on the quality and consistency of semantic segmentation labels; inconsistent or ambiguous labeling could undermine the validity of CoF table aggregation.
- Effectiveness of edit functions is critical; if edits fail to meaningfully alter the segment, SCAP explanations and subsequent CoF tables lose fidelity.
- Limited corpus evidence directly supporting the effectiveness of CoF tables in low-frequency shortcut detection.

## Confidence
- **High** confidence in the mechanism of aggregating local counterfactual explanations into global insights via CoF tables, supported by clear evidence in the abstract and method sections.
- **Medium** confidence in the effectiveness of SCAP explanations for generating labeled counterfactuals, as the segmentation and editing process is well-described but lacks extensive validation.
- **Medium** confidence in the ability to detect low-frequency shortcuts, as the claim is supported by a single example but lacks broader empirical validation.

## Next Checks
1. Validate segmentation accuracy and consistency across images to ensure reliable label aggregation in CoF tables.
2. Test the edit function's ability to generate meaningful counterfactuals by applying it to a controlled dataset with known shortcuts.
3. Evaluate the method's performance on datasets with rare shortcuts to confirm its ability to detect low-frequency patterns.