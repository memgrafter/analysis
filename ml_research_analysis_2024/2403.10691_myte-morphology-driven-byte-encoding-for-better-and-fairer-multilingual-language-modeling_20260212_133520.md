---
ver: rpa2
title: 'MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language
  Modeling'
arxiv_id: '2403.10691'
source_url: https://arxiv.org/abs/2403.10691
tags:
- languages
- language
- myte
- across
- utf-8
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MYTE, a morphology-driven byte encoding method
  designed to address the bias in multilingual language modeling where texts in underrepresented
  languages are over-segmented into long sequences of meaningless units. The core
  idea is to replace character-based byte encoding with morpheme-based encoding, leveraging
  unsupervised morphological segmentation to produce shorter, more equitable segmentations
  across languages.
---

# MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling

## Quick Facts
- arXiv ID: 2403.10691
- Source URL: https://arxiv.org/abs/2403.10691
- Reference count: 30
- Primary result: Morphology-driven byte encoding reduces sequence lengths and improves multilingual language modeling performance, especially for non-European and non-Latin script languages

## Executive Summary
MYTE (Morphology-driven sYnthetic TEnsor encoding) addresses a fundamental bias in multilingual language modeling where texts in underrepresented languages are over-segmented into long sequences of meaningless units. By replacing character-based byte encoding with morpheme-based encoding using unsupervised morphological segmentation, MYTE produces shorter, more equitable segmentations across languages. The method demonstrates significant improvements in sequence compression, computational efficiency, and language modeling performance across 99 typologically diverse languages.

## Method Summary
The MYTE method replaces standard UTF-8 byte encoding with morphology-driven encoding using Morfessor for unsupervised morphological segmentation. For each of 99 languages, MYTE creates a morpheme inventory (~4096 morphemes per language) that maps meaningful linguistic units to fixed-size byte sequences. This encoding reduces sequence lengths by replacing variable-length UTF-8 character encodings (1-4 bytes) with shorter, balanced morpheme representations. MyT5 models are then trained on this morphology-driven representation using the T5 architecture with span corruption tasks, showing improved performance and efficiency compared to standard ByT5 models.

## Key Results
- MYTE reduces sequence lengths for all 99 analyzed languages, with the most significant improvements for non-European languages and non-Latin scripts
- MyT5 models achieve faster inference speeds and match or exceed ByT5 performance across diverse low-resource languages and tasks
- The method diminishes the perplexity gap across languages, providing more equitable language modeling performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MYTE reduces sequence lengths by replacing variable-length UTF-8 character encodings with shorter, morphology-based encodings
- Mechanism: UTF-8 uses 1-4 bytes per character, with non-Latin scripts requiring more bytes. MYTE maps morphemes to fixed-size byte sequences, compressing text representation.
- Core assumption: Morphemes are more balanced in distribution across languages than characters, allowing for more uniform encoding lengths.
- Evidence anchors:
  - [abstract] "texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units"
  - [section] "UTF-8 convention produces longer byte sequences for some languages due to the development choices"
  - [corpus] Weak - the corpus shows related work on multilingual tokenization but doesn't directly support the morpheme distribution claim
- Break condition: If morphological segmentation introduces more tokens than characters for certain languages, or if morpheme inventories aren't sufficiently balanced

### Mechanism 2
- Claim: MYTE improves multilingual language modeling performance by using meaningful linguistic units instead of orthographic symbols
- Mechanism: Language models trained on byte sequences can better predict meaningful morpheme units compared to arbitrary byte sequences representing characters, leading to lower perplexity.
- Core assumption: Meaningful linguistic units are easier for language models to predict than arbitrary orthographic symbols
- Evidence anchors:
  - [abstract] "diminishes the perplexity gap throughout diverse languages"
  - [section] "MYTE uses codepoints based on morphemes that are inherently meaningful language units in contrast to orthographic symbols"
  - [corpus] Weak - corpus shows related work on multilingual tokenization but doesn't directly validate the prediction difficulty claim
- Break condition: If the model fails to learn meaningful patterns from morpheme-based encodings, or if the morphological segmentation quality degrades

### Mechanism 3
- Claim: MYTE provides computational efficiency benefits through sequence compression
- Mechanism: Shorter sequences reduce the computational complexity of attention mechanisms (quadratic in sequence length), leading to faster inference times.
- Core assumption: The quadratic reduction in attention computation from shorter sequences outweighs any additional complexity from morphological processing
- Evidence anchors:
  - [abstract] "MyT5 achieve faster inference speeds"
  - [section] "decrease in sequence length, as shown in the last section, will render up to a quadratic reduction of forward-pass time"
  - [corpus] Weak - corpus mentions related work on multilingual efficiency but doesn't validate the specific computational claim
- Break condition: If the morphological processing overhead exceeds the computational savings from shorter sequences

## Foundational Learning

- Concept: Unicode and UTF-8 encoding
  - Why needed here: Understanding how text is represented as bytes is fundamental to grasping why MYTE provides improvements
  - Quick check question: Why do non-Latin scripts typically require more bytes in UTF-8 encoding than Latin scripts?

- Concept: Morphological segmentation
  - Why needed here: The core innovation relies on using morphemes instead of characters for encoding
  - Quick check question: What is the difference between a morpheme and a character, and why does this distinction matter for encoding?

- Concept: Language modeling and perplexity
  - Why needed here: Evaluating the effectiveness of MYTE requires understanding how language models measure prediction difficulty
  - Quick check question: How does perplexity relate to sequence length, and why does MYTE's approach potentially improve this metric?

## Architecture Onboarding

- Component map:
  - UTF-8 byte transcoder → Morfessor morphological analyzer → MYTE encoder → Language model (MyT5)

- Critical path: Input text → Morfessor segmentation → Morpheme inventory mapping → MYTE byte encoding → MyT5 model training/fine-tuning

- Design tradeoffs:
  - Morpheme inventory size (~4096) balances granularity and computational efficiency
  - Fixed-size byte encoding (2-4 bytes) provides compression while maintaining representational capacity
  - Unsupervised segmentation avoids dependency on annotated morphological data

- Failure signatures:
  - Over-segmentation by Morfessor leading to longer sequences than UTF-8
  - Missing morphemes for rare scripts causing encoding failures
  - Performance degradation when morphological boundaries are ambiguous

- First experiments to run:
  1. Compare sequence lengths of UTF-8 vs MYTE encoding for representative languages from different scripts
  2. Measure perplexity of MyT5 vs ByT5 on held-out validation data
  3. Benchmark inference speed of MyT5 models across different batch sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the efficiency of MYTE encoding scale when applied to even larger models (e.g., models with billions of parameters) compared to smaller models?
- Basis in paper: [inferred] The paper mentions that MyT5’s inference speed gains over ByT5 improve with model size, but does not provide results for models larger than 1.23 billion parameters.
- Why unresolved: The paper only evaluates models up to 1.23 billion parameters, leaving uncertainty about scalability to much larger models.
- What evidence would resolve it: Experimental results comparing MYTE-based models to ByT5 models across a range of scales, including models with tens or hundreds of billions of parameters, would clarify scalability.

### Open Question 2
- Question: Does MYTE encoding maintain its advantages in languages with highly complex morphological systems, such as polysynthetic languages, where morpheme boundaries may be less clear?
- Basis in paper: [inferred] The paper focuses on 99 languages but does not explicitly address polysynthetic languages or languages with extremely complex morphology.
- Why unresolved: The paper does not include a detailed analysis of languages with highly complex morphological systems, leaving uncertainty about performance in these cases.
- What evidence would resolve it: Evaluating MYTE encoding on a diverse set of polysynthetic languages and comparing results to other encoding methods would provide clarity.

### Open Question 3
- Question: How does the choice of morphological segmentation granularity (e.g., number of morphemes per language) affect the performance and efficiency of MYTE encoding?
- Basis in paper: [explicit] The paper mentions that 4096 morphemes per language were chosen to balance segmentation granularity, but does not explore the impact of varying this number.
- Why unresolved: The paper does not provide experiments or analysis on how different morpheme inventory sizes affect MYTE’s performance.
- What evidence would resolve it: Experiments varying the number of morphemes per language and measuring the impact on encoding efficiency and language model performance would clarify this.

### Open Question 4
- Question: How robust is MYTE encoding to errors in unsupervised morphological segmentation, particularly in low-resource languages where data quality may be poor?
- Basis in paper: [explicit] The paper acknowledges that Morfessor, the unsupervised morphological analyzer used, is prone to errors such as over-segmentation.
- Why unresolved: The paper does not provide quantitative analysis of how segmentation errors impact MYTE’s performance or efficiency.
- What evidence would resolve it: Systematic evaluation of MYTE encoding using gold-standard morphological annotations or error analysis in low-resource languages would quantify the impact of segmentation errors.

## Limitations

- The evaluation focuses primarily on compression and perplexity metrics without extensive analysis of how morphological segmentation quality affects downstream task performance across all 99 languages
- While the method claims to improve fairness, the parity metric may not fully capture linguistic complexity differences between languages
- The computational efficiency claims rely on theoretical quadratic reduction without comprehensive ablation studies comparing the overhead of morphological processing versus savings from shorter sequences

## Confidence

- **High confidence**: The claim that MYTE reduces sequence lengths for all 99 languages is well-supported by empirical results showing consistent compression across diverse scripts and language families
- **Medium confidence**: The claim that MYTE improves multilingual language modeling performance is supported by perplexity and BP EB metrics, but the relationship between morphological encoding and model generalization could be more thoroughly examined
- **Low confidence**: The computational efficiency claims regarding inference speed improvements lack sufficient empirical validation

## Next Checks

1. **Ablation study on morphological segmentation quality**: Conduct controlled experiments varying Morfessor parameters to determine the sensitivity of MYTE performance to segmentation quality, particularly for languages with rich morphology versus isolating languages.

2. **Extended computational analysis**: Measure wall-clock training and inference times for MyT5 versus ByT5 across different batch sizes and sequence lengths to empirically validate the claimed computational efficiency improvements.

3. **Downstream task robustness evaluation**: Perform comprehensive analysis of MyT5 performance on the full XTREME-UP benchmark across all 99 languages, with particular focus on low-resource languages where morphological segmentation might introduce errors or ambiguity.