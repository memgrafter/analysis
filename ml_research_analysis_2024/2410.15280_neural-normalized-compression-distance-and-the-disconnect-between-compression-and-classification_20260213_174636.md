---
ver: rpa2
title: Neural Normalized Compression Distance and the Disconnect Between Compression
  and Classification
arxiv_id: '2410.15280'
source_url: https://arxiv.org/abs/2410.15280
tags:
- compression
- neural
- classification
- compressors
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the connection between compression and classification
  performance in text classification. While compression-based distance metrics like
  Normalized Compression Distance (NCD) have been successfully used for classification
  tasks, the authors question whether better compression rates necessarily lead to
  better classification accuracy.
---

# Neural Normalized Compression Distance and the Disconnect Between Compression and Classification

## Quick Facts
- arXiv ID: 2410.15280
- Source URL: https://arxiv.org/abs/2410.15280
- Authors: John Hurwitz; Charles Nicholas; Edward Raff
- Reference count: 31
- Key outcome: Compression rate alone does not predict classification accuracy when using neural compressors for text classification

## Executive Summary
This work investigates the connection between compression and classification performance in text classification tasks. While compression-based distance metrics like Normalized Compression Distance (NCD) have been successfully used for classification, the authors question whether better compression rates necessarily lead to better classification accuracy. They propose using large language models as neural compressors to calculate a "Neural NCD" and compare its performance against traditional compressors like gzip. The results show that classification accuracy is not predictable by compression rate alone, even when neural compressors achieve consistently superior compression rates. Furthermore, the choice of LLM architecture affects the balance between NCD and latent representation-based approaches.

## Method Summary
The paper uses pretrained language models (RWKV 169M, GPT-2 117M, OPT 125M) with arithmetic coding for neural compression, comparing against traditional compressors (gzip, zstd, lzma) on text classification tasks. NCD is calculated as a distance metric for kNN classification on few-shot subsets of AGNews, 20News, and DBpedia datasets. The approach measures both classification accuracy and compression rate to investigate the relationship between these metrics.

## Key Results
- Classification accuracy is not predictable by compression rate alone, even when neural compressors achieve consistently superior compression rates
- The choice of LLM architecture affects the balance between NCD and latent representation-based approaches
- Neural compressors can outperform traditional compressors in compression rate while yielding varying relative performance in classification accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compression rate alone does not predict classification accuracy for neural compressors
- Mechanism: Different neural architectures identify different types of redundancies in concatenated sequences, leading to varying NCD quality despite similar compression ratios
- Core assumption: The way a neural network identifies and exploits redundancies is architecture-dependent and affects classification performance
- Evidence anchors:
  - [abstract] "we find that classification accuracy is not predictable by compression rate alone, among other empirical aberrations not predicted by current understanding"
  - [section 4.1] "Despite the RWKV neural compressor achieving superior compression rates than traditional compressors on each dataset, Neural NCD yields varying relative performance when compared to using NCDs of traditional compressors"
  - [corpus] Weak - related papers focus on molecular sequences and EEG signals, not text classification

### Mechanism 2
- Claim: Model architecture affects the relationship between compression rate and classification accuracy
- Mechanism: Different neural architectures have varying abilities to capture long-range dependencies in concatenated sequences, which impacts NCD quality
- Core assumption: The architecture's ability to identify similarities between concatenated sequences determines NCD effectiveness
- Evidence anchors:
  - [section 4.1] "The internal workings of each compression algorithm are important in determining NCD quality. For example, gzip searches for repeated byte sequences only within a 32 KB sliding window"
  - [section 4.2] "We can see that all neural models outperform gzip in the same way on AGNews, and underperform it in the same way on DBpedia"
  - [corpus] Weak - no direct evidence about architecture differences affecting NCD quality

### Mechanism 3
- Claim: Latent representations from the same model can outperform or underperform Neural NCD depending on the architecture
- Mechanism: The geometry of latent space and how it captures semantic similarity differs across architectures, making Euclidean distance sometimes more effective than NCD
- Core assumption: Latent representations encode information differently than compression-based distances
- Evidence anchors:
  - [section 4.3] "we compared Neural NCD with latent representations from the same model, showing that Neural NCD outperforms latent representations for some models and under-performs them for others"
  - [abstract] "Furthermore, the choice of LLM architecture affects the balance between NCD and latent representation-based approaches"
  - [corpus] Weak - related work focuses on compression for molecular sequences, not latent representation comparison

## Foundational Learning

- Concept: Kolmogorov complexity and its relationship to compression
  - Why needed here: Understanding why compression-based distance metrics theoretically work for similarity measurement
  - Quick check question: What is the fundamental limitation of using compression algorithms to approximate Kolmogorov complexity?

- Concept: Arithmetic coding and its use with language models
  - Why needed here: Understanding how language models can be turned into lossless compressors
  - Quick check question: How does arithmetic coding use probability distributions from language models to achieve compression?

- Concept: k-nearest neighbors algorithm and its computational complexity
  - Why needed here: Understanding the practical limitations of NCD-based classification
  - Quick check question: What is the computational complexity of calculating pairwise distances for kNN, and why does this limit dataset size?

## Architecture Onboarding

- Component map: Language model -> Arithmetic coding utility -> kNN classifier -> Dataset preprocessing pipeline -> Compression rate calculation utility

- Critical path:
  1. Load pretrained language model
  2. For each pair of sequences, concatenate and compress using arithmetic coding
  3. Calculate NCD using compressed lengths
  4. Build distance matrix for all pairs
  5. Apply kNN with NCD distances
  6. Calculate classification accuracy and compression rate

- Design tradeoffs:
  - Model size vs. compression rate: Larger models achieve better compression but increase computational cost quadratically
  - Traditional vs. neural compressors: Traditional compressors are faster but achieve worse compression rates
  - k value in kNN: Higher k values smooth classification but may reduce sensitivity to local patterns

- Failure signatures:
  - NCD values clustered near 0 or 1: May indicate poor discrimination between classes
  - Compression rate inconsistent across datasets: May indicate model doesn't generalize well
  - Training accuracy much higher than test accuracy: May indicate overfitting to specific dataset characteristics

- First 3 experiments:
  1. Reproduce the AGNews dataset results with gzip and RWKV 169M to verify the compression rate vs. accuracy disconnect
  2. Swap out RWKV 169M for GPT-2 117M and compare compression rates and classification accuracy on the same dataset
  3. Compare Neural NCD performance with Euclidean distance on latent representations for the same model to verify the architecture-dependent tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does improved compression rate lead to better NCD-based classification accuracy?
- Basis in paper: [explicit] The paper states "compression rate alone is not enough to predict the differences in NCD-based classification accuracy when using neural compressors across various datasets"
- Why unresolved: The paper shows cases where compression rate doesn't predict accuracy, but doesn't identify the specific factors that determine when compression rate does or doesn't correlate with classification performance
- What evidence would resolve it: Systematic experiments varying compression algorithms, datasets, and sequence characteristics to identify which factors (e.g., dataset size, sequence length, redundancy patterns) determine the relationship between compression rate and classification accuracy

### Open Question 2
- Question: What architectural features of neural compressors make them more or less effective for NCD-based classification beyond their compression rates?
- Basis in paper: [explicit] "The internal workings of each compression algorithm are important in determining NCD quality" and "Differences in neural network architectures among neural compressors may yield different abilities to identify these long range similarities"
- Why unresolved: The paper identifies that architecture matters but doesn't investigate which specific architectural choices (attention mechanisms, recurrence, context windows, etc.) contribute to better or worse NCD quality
- What evidence would resolve it: Comparative analysis of different neural compressor architectures (transformers, RNNs, CNNs) on the same datasets, measuring not just compression rates but also the quality of NCD distance calculations and their impact on classification

### Open Question 3
- Question: How do the latent representations from different neural architectures compare in their effectiveness for distance-based classification versus their performance in NCD-based classification?
- Basis in paper: [explicit] "Depending on the choice of neural network model used for neural compression, Neural NCD can either outperform or underperform the Euclidean distance-based approach on sequence latent representations"
- Why unresolved: The paper shows variability in whether NCD or latent representations work better for different models, but doesn't explain why certain architectures favor one approach over the other or what properties make an architecture better suited for each method
- What evidence would resolve it: Detailed analysis of the properties of latent representations (linear separability, cluster structure, etc.) across different architectures, correlating these properties with whether NCD or Euclidean distance performs better for classification

## Limitations
- The disconnect between compression rate and classification accuracy remains poorly understood theoretically
- The paper demonstrates empirical results but does not provide a comprehensive theory for why compression rate doesn't predict classification performance
- Limited to text classification datasets; results may not generalize to other data types

## Confidence

- **High Confidence**: The empirical observation that compression rate alone does not predict classification accuracy is well-supported by experimental results across multiple datasets and architectures
- **Medium Confidence**: The claim that model architecture affects the balance between NCD and latent representation approaches is supported by comparative results, though the theoretical reasons for this effect are not fully articulated
- **Low Confidence**: The broader claim that our understanding of what constitutes effective compression for classification is incomplete is largely a statement about gaps in current theory rather than a testable hypothesis

## Next Checks
1. Conduct ablation studies on sliding window sizes and dependency ranges across different neural architectures to isolate architectural factors affecting NCD quality versus pure compression rate
2. Test the proposed mechanisms on additional text classification datasets (e.g., IMDb reviews, Amazon product reviews) to verify whether the observed patterns generalize beyond the three datasets studied
3. Compare Neural NCD performance against contrastive learning approaches using the same latent representations to better understand when and why compression-based distances might be preferable to learned similarity metrics