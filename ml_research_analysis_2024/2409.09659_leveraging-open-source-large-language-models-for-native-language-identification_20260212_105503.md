---
ver: rpa2
title: Leveraging Open-Source Large Language Models for Native Language Identification
arxiv_id: '2409.09659'
source_url: https://arxiv.org/abs/2409.09659
tags:
- llms
- language
- open-source
- native
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of open-source large
  language models (LLMs) for Native Language Identification (NLI), comparing them
  to closed-source models like GPT-4. While open-source LLMs show poor performance
  out-of-the-box, fine-tuning them on labeled training data significantly improves
  their accuracy, enabling them to match the performance of commercial LLMs.
---

# Leveraging Open-Source Large Language Models for Native Language Identification

## Quick Facts
- arXiv ID: 2409.09659
- Source URL: https://arxiv.org/abs/2409.09659
- Authors: Yee Man Ng; Ilia Markov
- Reference count: 12
- Key outcome: Open-source LLMs achieve comparable NLI performance to GPT-4 when fine-tuned on labeled data

## Executive Summary
This study investigates the effectiveness of open-source large language models for Native Language Identification (NLI), comparing them to closed-source models like GPT-4. While open-source LLMs show poor performance out-of-the-box, fine-tuning them on labeled training data significantly improves their accuracy, enabling them to match the performance of commercial LLMs. Experiments on benchmark datasets (TOEFL11 and ICLE-NLI) demonstrate that fine-tuned models like Gemma achieve accuracy scores of 90.3% and 96.6%, respectively, comparable to GPT-4. The study highlights the potential of open-source LLMs for NLI when appropriately adapted, offering a cost-effective and transparent alternative to closed-source models.

## Method Summary
The study fine-tunes open-source LLMs (LLaMA-2, LLaMA-3, Gemma, Mistral, Phi-3) on NLI training data using QLoRA with 4-bit quantization. Models are evaluated on TOEFL11 and ICLE-NLI datasets using both zero-shot and fine-tuned settings. Zero-shot evaluation uses carefully crafted prompts, while fine-tuning employs standard hyperparameters (learning rate 1e-4, batch size 16, 3 epochs). Performance is compared against GPT-3.5 and GPT-4 baselines from prior work.

## Key Results
- Open-source LLMs achieve only 13.6% to 75.8% accuracy in zero-shot NLI settings
- Fine-tuned Gemma reaches 90.3% (±1.2) accuracy on TOEFL11, matching GPT-4 performance
- Gemma achieves 96.6% accuracy on ICLE-NLI dataset, comparable to GPT-4
- Zero-shot performance gap suggests potential training data overlap concerns for closed-source models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning open-source LLMs on labeled NLI data enables them to learn language-specific interference patterns that are otherwise not captured by general pretraining.
- Mechanism: Domain-specific fine-tuning adapts the model's parameters to recognize L1-specific linguistic features (e.g., spelling errors, syntactic patterns) that transfer from L1 to L2 writing.
- Core assumption: The NLI task requires recognition of subtle linguistic transfer patterns that are not present in general pretraining corpora.
- Evidence anchors:
  - [abstract] "when fine-tuned on labeled training data, open-source LLMs can achieve performance comparable to that of commercial LLMs"
  - [section 3.3] "The results indicate that the performance of open-source LLMs improves substantially after task-specific fine-tuning"
  - [corpus] Weak evidence - no specific linguistic pattern examples provided
- Break condition: If the training data lacks sufficient linguistic diversity across L1s, fine-tuning may not capture all relevant interference patterns.

### Mechanism 2
- Claim: Open-source LLMs can achieve comparable performance to closed-source models because they can be fine-tuned with full access to training procedures and data.
- Mechanism: Fine-tuning allows optimization of open-source models on task-specific data, compensating for potential gaps in pretraining coverage compared to closed-source models.
- Core assumption: Performance differences between open and closed models stem primarily from access to training data and fine-tuning capabilities, not fundamental model architecture differences.
- Evidence anchors:
  - [abstract] "open-source LLMs can achieve performance comparable to that of commercial LLMs"
  - [section 3.3] "Fine-tuned Gemma achieves an accuracy score of 90.3% (±1.2) on the TOEFL11 dataset, nearly matching the results of GPT-4"
  - [corpus] Weak evidence - no comparison of pretraining data coverage between model families
- Break condition: If closed-source models have seen the evaluation datasets during pretraining, performance comparisons become invalid.

### Mechanism 3
- Claim: Open-source LLMs show lower zero-shot performance because they lack exposure to NLI-specific linguistic patterns during pretraining.
- Mechanism: General pretraining corpora may not contain sufficient examples of L1 transfer phenomena, limiting zero-shot generalization to NLI tasks.
- Core assumption: NLI requires specialized knowledge of language transfer patterns that are rare in general text corpora.
- Evidence anchors:
  - [section 3.2] "We note a surprisingly low performance of open-source LLMs when used out-of-the-box"
  - [section 3.2] "The large performance gap raises the concern that closed-source LLMs might have seen the NLI benchmark datasets in training"
  - [corpus] Weak evidence - no analysis of pretraining data composition
- Break condition: If zero-shot performance improves significantly with better prompt engineering, the pretraining coverage assumption may be incorrect.

## Foundational Learning

- Concept: Language transfer phenomena
  - Why needed here: Understanding how L1 influences L2 writing is fundamental to the NLI task and model performance
  - Quick check question: Can you explain why Hindi and Telugu essays might be confused by NLI models?

- Concept: Supervised multi-class classification
  - Why needed here: NLI is framed as assigning one of multiple L1 labels to each text sample
  - Quick check question: What are the advantages and disadvantages of using accuracy as the primary evaluation metric for NLI?

- Concept: Fine-tuning methodology (QLoRA)
  - Why needed here: Fine-tuning is the key mechanism that enables open-source LLMs to match closed-source performance
  - Quick check question: How does QLoRA enable efficient fine-tuning of large language models?

## Architecture Onboarding

- Component map: Open-source LLM -> Fine-tuning pipeline -> NLI classifier
  - Model loading (4-bit quantization)
  - Fine-tuning adapter integration (QLoRA)
  - Prompt engineering for NLI task
  - Evaluation on benchmark datasets

- Critical path: Data preparation -> Model fine-tuning -> Evaluation
  - Data preparation: Ensure balanced representation of all L1s
  - Model fine-tuning: Apply QLoRA with appropriate hyperparameters
  - Evaluation: Compare against baseline and closed-source models

- Design tradeoffs: Fine-tuning vs zero-shot performance
  - Fine-tuning provides better performance but requires labeled data
  - Zero-shot is more flexible but shows poor performance on NLI
  - Open-source offers transparency but may require more computational resources

- Failure signatures: Poor fine-tuning convergence, overfitting on training data, confusion between linguistically similar L1s
  - Monitor training loss for convergence issues
  - Check validation performance to detect overfitting
  - Analyze confusion matrices to identify problematic L1 pairs

- First 3 experiments:
  1. Fine-tune Gemma on TOEFL11 training set and evaluate on test set
  2. Compare zero-shot vs fine-tuned performance on ICLE-NLI dataset
  3. Analyze confusion patterns between linguistically similar L1 pairs (e.g., Hindi/Telugu)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features or patterns do open-source LLMs fail to capture when used out-of-the-box for NLI, leading to their poor performance compared to closed-source models?
- Basis in paper: [explicit] The paper states that open-source LLMs achieve much lower accuracy scores (13.6% to 75.8%) compared to closed-source models (91.7% for GPT-4) when used out-of-the-box, and that some models tend to predict mostly one or two languages.
- Why unresolved: The paper does not provide a detailed analysis of the specific linguistic features or patterns that open-source LLMs fail to capture. It only mentions general factors like spelling errors, word choice, syntactic patterns, and grammatical errors as clues used in NLI.
- What evidence would resolve it: A detailed error analysis of open-source LLM predictions, identifying specific linguistic features or patterns that are consistently missed or misinterpreted by these models when used out-of-the-box.

### Open Question 2
- Question: How does the performance of fine-tuned open-source LLMs for NLI vary across different L1s, and are there specific L1s that remain challenging even after fine-tuning?
- Basis in paper: [inferred] The paper mentions that both GPT-4 and fine-tuned Gemma tend to misclassify Hindi texts as Telugu and Japanese essays as Korean, suggesting that some L1 pairs remain challenging. However, a comprehensive analysis of performance across all L1s is not provided.
- Why unresolved: The paper does not provide a detailed breakdown of model performance for each L1, making it difficult to identify specific L1s that remain challenging even after fine-tuning.
- What evidence would resolve it: A detailed analysis of model performance for each L1, including accuracy scores and confusion matrices, to identify specific L1s that remain challenging even after fine-tuning.

### Open Question 3
- Question: What is the impact of different fine-tuning strategies (e.g., learning rate, batch size, number of epochs) on the performance of open-source LLMs for NLI, and how can these strategies be optimized for this specific task?
- Basis in paper: [explicit] The paper mentions that open-source LLMs were fine-tuned using specific hyperparameters (learning rate of 1e-4, batch size of 16, 3 epochs), but does not explore the impact of different fine-tuning strategies on model performance.
- Why unresolved: The paper does not provide an analysis of how different fine-tuning strategies affect model performance, making it difficult to determine the optimal approach for this specific task.
- What evidence would resolve it: An ablation study or sensitivity analysis of different fine-tuning strategies (e.g., learning rate, batch size, number of epochs) to determine their impact on model performance and identify the optimal approach for NLI.

## Limitations

- The zero-shot performance gap between open and closed-source models raises concerns about potential training data overlap, though the paper acknowledges this limitation without providing definitive evidence
- The study focuses primarily on accuracy metrics without deeper investigation into model confidence or error patterns across linguistically similar language pairs
- Computational cost of fine-tuning versus using closed-source APIs is not addressed, limiting practical applicability assessments

## Confidence

- **High confidence**: The experimental methodology for fine-tuning and evaluation is clearly described and reproducible
- **Medium confidence**: The comparison between open and closed-source models is valid, though the zero-shot performance discrepancy remains unexplained
- **Low confidence**: Claims about pretraining data coverage differences between model families lack direct evidence

## Next Checks

1. **Data overlap verification**: Conduct a comprehensive analysis of closed-source model outputs on held-out samples from the evaluation datasets to detect potential memorization or training set contamination
2. **Linguistic error analysis**: Perform detailed analysis of confusion matrices to identify systematic errors, particularly between linguistically similar language pairs (e.g., Hindi/Telugu, Japanese/Korean)
3. **Cost-benefit analysis**: Measure the total computational cost of fine-tuning open-source models versus API costs for closed-source models across different usage scenarios and scales