---
ver: rpa2
title: 'Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?'
arxiv_id: '2404.03411'
source_url: https://arxiv.org/abs/2404.03411
tags: []
core_contribution: This paper presents a comprehensive evaluation of jailbreak attacks
  against large language models (LLMs) and multimodal large language models (MLLMs).
  The authors construct a dataset with 1,445 harmful questions covering 11 safety
  policies and evaluate 11 different models, including both proprietary models like
  GPT-4 and GPT-4V and open-source models like Llama2 and Qwen-VL-Chat.
---

# Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?

## Quick Facts
- **arXiv ID**: 2404.03411
- **Source URL**: https://arxiv.org/abs/2404.03411
- **Reference count**: 19
- **Key outcome**: Comprehensive evaluation of jailbreak attacks on LLMs and MLLMs, finding GPT-4/GPT-4V significantly more robust than open-source alternatives

## Executive Summary
This paper presents a systematic evaluation of jailbreak attacks against large language models and multimodal large language models. The authors construct a comprehensive dataset of 1,445 harmful questions across 11 safety policies and test 11 different models using 32 jailbreak methods. Their findings reveal that proprietary models like GPT-4 and GPT-4V demonstrate significantly better robustness against jailbreak attacks compared to open-source models. Among open-source options, Llama2 and Qwen-VL-Chat show the best performance, while visual jailbreak methods exhibit limited transferability across models.

## Method Summary
The authors construct a dataset with 1,445 harmful questions covering 11 safety policies and evaluate 11 different models, including both proprietary models like GPT-4 and GPT-4V and open-source models like Llama2 and Qwen-VL-Chat. They test 32 jailbreak methods, including 29 textual and 3 visual approaches. The evaluation measures success rates of jailbreak attempts and analyzes transferability between models, comparing the performance of different attack methods like AutoDAN and GCG.

## Key Results
- GPT-4 and GPT-4V demonstrate significantly better robustness against jailbreak attacks compared to open-source models
- AutoDAN shows better transferability than GCG across models
- Visual jailbreak methods exhibit limited transferability compared to textual methods

## Why This Works (Mechanism)
The paper identifies that proprietary models like GPT-4 and GPT-4V have built-in safety mechanisms that make them more resistant to jailbreak attacks compared to open-source alternatives. The specific mechanisms behind this enhanced robustness are not fully explored in the paper, but the results suggest that the safety training and fine-tuning processes employed by OpenAI may be more effective at preventing jailbreak attempts.

## Foundational Learning
The study highlights the importance of comprehensive safety training and fine-tuning for large language models. The findings suggest that open-source models may benefit from adopting similar safety training approaches as proprietary models like GPT-4 and GPT-4V. Additionally, the limited transferability of visual jailbreak methods indicates that multimodal models may require specialized safety measures to address visual prompt injection attacks.

## Architecture Onboarding
None

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses primarily on OpenAI's GPT-4 and GPT-4V models, which may not generalize to other proprietary models
- Evaluation dataset covers only 11 safety policies, potentially missing edge cases or emerging safety concerns
- Visual jailbreak methods show limited transferability, but reasons for this limitation are not thoroughly explored
- The study does not investigate the specific safety mechanisms employed by GPT-4 and GPT-4V that contribute to their enhanced robustness
- The effectiveness of jailbreak methods may vary depending on the specific implementation details, which are not fully explored in the paper

## Confidence
- **High confidence**: Relative robustness ranking of GPT-4/GPT-4V versus open-source models
- **Medium confidence**: Effectiveness rankings of different jailbreak methods due to potential variability in implementation details
- **Medium confidence**: Transferability findings, as the study doesn't explore the underlying mechanisms driving differences between AutoDAN and GCG

## Next Checks
1. Test the jailbreak methods against additional proprietary MLLMs beyond GPT-4V to verify if the robustness patterns hold
2. Expand the safety policy coverage beyond the 11 tested categories to include emerging safety concerns
3. Conduct ablation studies on the AutoDAN and GCG methods to identify specific components that drive their different transferability behaviors
4. Investigate the specific safety mechanisms employed by GPT-4 and GPT-4V that contribute to their enhanced robustness
5. Explore the implementation details of different jailbreak methods to better understand their effectiveness rankings