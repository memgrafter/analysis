---
ver: rpa2
title: 'Curse of Attention: A Kernel-Based Perspective for Why Transformers Fail to
  Generalize on Time Series Forecasting and Beyond'
arxiv_id: '2412.06061'
source_url: https://arxiv.org/abs/2412.06061
tags:
- definition
- step
- define
- specified
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first theoretical explanation for why transformers
  underperform simple linear models in time series forecasting. The authors attribute
  this to "Asymmetric Learning" during training, where attention mechanisms fail to
  learn residual features when the sign of the previous step is inconsistent with
  the current step in next-step prediction.
---

# Curse of Attention: A Kernel-Based Perspective for Why Transformers Fail to Generalize on Time Series Forecasting and Beyond

## Quick Facts
- **arXiv ID**: 2412.06061
- **Source URL**: https://arxiv.org/abs/2412.06061
- **Reference count**: 40
- **One-line primary result**: Provides theoretical explanation for why transformers underperform linear models in time series forecasting due to "Asymmetric Learning" during training.

## Executive Summary
This paper provides the first theoretical explanation for why transformers underperform simple linear models in time series forecasting. Through NTK analysis, the authors prove that over-parameterized attention networks cannot generalize on out-of-distribution data with sign-inconsistent next-step prediction, while linear residual networks can easily accomplish this task. The core issue stems from "Asymmetric Learning" during training, where attention mechanisms fail to learn residual features when the sign of the previous step is inconsistent with the current step in next-step prediction.

## Method Summary
The authors use NTK analysis to study a two-layer attention network with gradient descent training on data generated from a State Space Model (SSM). They prove that when sign inconsistencies exist between consecutive time steps, attention networks fail to learn the necessary residual features for generalization, while linear residual networks maintain their ability to generalize. The analysis focuses on over-parameterized networks and uses gradient flow to study the training dynamics.

## Key Results
- Over-parameterized attention networks cannot generalize on OOD data with sign-inconsistent next-step prediction
- Linear residual networks can easily accomplish sign-inconsistent next-step prediction tasks
- NTK analysis shows attention networks cannot effectively learn the mapping needed for sign-inconsistent next-step prediction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Over-parameterized attention networks fail to generalize on out-of-distribution (OOD) data when the sign of the previous step is inconsistent with the current step in next-step prediction.
- **Mechanism**: Asymmetric Learning during training causes attention weights to become misaligned with residual features. When the previous step's sign conflicts with the current step, the attention mechanism fails to learn these residual features effectively, making generalization difficult.
- **Core assumption**: The sign-inconsistency between consecutive steps in time series data creates conditions where attention cannot properly weight the residual features.
- **Evidence anchors**:
  - [abstract]: "When the sign of the previous step is inconsistent with the sign of the current step in the next-step-prediction time series, attention fails to learn the residual features."
  - [section 6.1]: "In this section, we present a new task named Sign-Inconsistent Next-step-prediction."
  - [corpus]: Weak evidence - related papers focus on universal approximation and expressivity rather than this specific sign-inconsistency mechanism.
- **Break condition**: If the time series data has consistent sign patterns between consecutive steps, the attention mechanism may perform adequately.

### Mechanism 2
- **Claim**: Linear residual networks can easily accomplish sign-inconsistent next-step prediction tasks while attention networks cannot, regardless of model size or training duration.
- **Mechanism**: Linear residual networks explicitly preserve the residual structure through direct linear transformations, while attention networks rely on learned attention weights that become ineffective when signs are inconsistent.
- **Core assumption**: The residual structure of time series data is essential for next-step prediction, and linear models can capture this structure directly.
- **Evidence anchors**:
  - [abstract]: "a linear residual network could easily accomplish it" referring to sign-inconsistent next-step prediction.
  - [section 6.3]: "Part 2 shows that a set of parameters exists for the residual linear model that can reduce the OOD risk to the same bound in this task."
  - [corpus]: Moderate evidence - several papers discuss transformer limitations in time series forecasting, supporting the general claim about linear models performing better.
- **Break condition**: If the time series data does not follow a residual structure or if the residual features are not crucial for prediction.

### Mechanism 3
- **Claim**: The Neural Tangent Kernel (NTK) analysis shows that attention networks cannot effectively learn the mapping needed for sign-inconsistent next-step prediction.
- **Mechanism**: The NTK framework demonstrates that the kernel matrix induced by attention networks becomes inadequate for capturing the necessary relationships when signs are inconsistent, while linear models maintain effective kernels.
- **Core assumption**: NTK analysis provides a valid framework for understanding the generalization capabilities of attention versus linear models in this specific task.
- **Evidence anchors**:
  - [abstract]: "Through NTK analysis, they prove that over-parameterized attention networks cannot generalize on out-of-distribution data with sign-inconsistent next-step prediction."
  - [section 5.1]: "Neural Tangent Kernel (NTK)[JGH18] provides a powerful tool for understanding gradient descent in neural network training."
  - [corpus]: Weak evidence - corpus papers focus on universal approximation rather than NTK-specific analysis of attention mechanisms.
- **Break condition**: If the NTK approximation breaks down or if other factors dominate the learning dynamics.

## Foundational Learning

- **Concept**: Neural Tangent Kernel (NTK) analysis
  - Why needed here: NTK provides the theoretical framework for understanding why attention networks fail to generalize on OOD data with sign inconsistencies.
  - Quick check question: Can you explain how NTK relates to the training dynamics of over-parameterized neural networks?

- **Concept**: State Space Models (SSM) for time series data generation
  - Why needed here: SSM provides the data generation model used to create the theoretical examples demonstrating attention failure.
  - Quick check question: What properties of SSM make it suitable for modeling time series data in this theoretical framework?

- **Concept**: Asymmetric Learning in neural networks
  - Why needed here: Asymmetric Learning describes the phenomenon where attention weights update in ways that prevent learning of residual features when signs are inconsistent.
  - Quick check question: How does Asymmetric Learning differ from symmetric learning patterns in neural network training?

## Architecture Onboarding

- **Component map**: SSM data generation (x ∈ Rd) -> Two-layer attention network with m neurons -> Linear combination output -> Gradient descent training -> OOD risk evaluation
- **Critical path**: Data generation (SSM) → Attention network training (GD with NTK analysis) → Evaluation on sign-inconsistent OOD data → Comparison with linear residual model
- **Design tradeoffs**: 
  - Attention vs. Linear: Attention captures complex patterns but fails on sign-inconsistent data; Linear is simpler but more robust for this specific task.
  - Over-parameterization: More parameters don't help attention networks overcome the sign-inconsistency limitation.
  - NTK analysis complexity: Provides theoretical guarantees but requires careful mathematical setup.
- **Failure signatures**:
  - Attention weights converging to values that suppress residual features
  - Poor generalization on OOD data with sign inconsistencies
  - NTK kernel matrix becoming inadequate for the learning task
  - Linear models outperforming attention models on residual-structured data
- **First 3 experiments**:
  1. Generate SSM data with consistent signs and compare attention vs. linear model performance.
  2. Generate SSM data with sign-inconsistent patterns and measure attention network failure to generalize.
  3. Vary the degree of sign inconsistency and plot the performance degradation of attention models versus linear models.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do the proposed theoretical insights about attention's failure in time series forecasting extend to multi-dimensional time series with non-linear relationships?
- **Basis in paper**: Explicit - The authors discuss asymmetric learning in the multi-dimensional case in Section 7.1 and mention it as an open direction
- **Why unresolved**: The paper focuses primarily on d=1 case for theoretical analysis, with only brief discussion of d>1 case
- **What evidence would resolve it**: Theoretical analysis showing how the asymmetric learning phenomenon manifests in multi-dimensional attention mechanisms, or empirical validation on real-world multi-dimensional time series datasets

### Open Question 2
- **Question**: Can the proposed solutions (differential transformer, patching, RoPE, gradient correction) effectively mitigate asymmetric learning in practice?
- **Basis in paper**: Explicit - The authors discuss potential solutions in Section 7.2
- **Why unresolved**: The paper only provides theoretical discussion without empirical validation of these solutions
- **What evidence would resolve it**: Experimental results showing improved performance on time series forecasting tasks when applying these solutions to transformer architectures

### Open Question 3
- **Question**: What are the precise conditions under which linear residual models outperform attention-based models in time series forecasting?
- **Basis in paper**: Explicit - The paper proves that linear models can generalize better than attention networks in certain settings
- **Why unresolved**: The theoretical results focus on specific synthetic data generation models, not real-world conditions
- **What evidence would resolve it**: Empirical studies identifying the characteristics of time series data where linear models consistently outperform attention-based approaches, including analysis of stationarity, linearity, and residual feature importance

## Limitations
- Analysis focuses on a specific data generation model (SSM) with particular properties that may not generalize to all time series forecasting scenarios
- NTK analysis assumes infinite-width networks and may not accurately capture finite-width behavior
- Sign-inconsistency mechanism is a constructed scenario that may not represent real-world time series data distributions

## Confidence
- **High Confidence**: The claim that linear residual networks can perform sign-inconsistent next-step prediction is well-supported by the theoretical analysis and proofs provided in the paper.
- **Medium Confidence**: The assertion that attention networks fail to generalize on OOD data with sign inconsistencies is theoretically proven but relies on specific assumptions about data generation and network initialization.
- **Low Confidence**: The broader claim that transformers universally underperform simple linear models in time series forecasting is not fully established, as the analysis focuses on a specific constructed scenario rather than real-world data.

## Next Checks
1. **Empirical validation on real-world data**: Test the theoretical predictions on actual time series datasets (e.g., weather, financial, or sensor data) to verify if the sign-inconsistency problem manifests in practice.

2. **Finite-width network analysis**: Extend the NTK analysis to finite-width networks to determine if the theoretical guarantees hold for practical implementation scales.

3. **Alternative attention mechanisms**: Investigate whether modifications to the attention mechanism (such as relative positional encoding or residual attention) can overcome the sign-inconsistency limitation while maintaining other benefits of attention-based models.