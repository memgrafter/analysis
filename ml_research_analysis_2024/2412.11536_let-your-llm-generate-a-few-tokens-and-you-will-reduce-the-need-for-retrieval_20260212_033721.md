---
ver: rpa2
title: Let your LLM generate a few tokens and you will reduce the need for retrieval
arxiv_id: '2412.11536'
source_url: https://arxiv.org/abs/2412.11536
tags:
- tokens
- retrieval
- score
- task
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates training large language models (LLMs) to
  determine whether they can answer a question using only their parametric memory
  or require retrieval-augmented generation (RAG). The authors propose a method called
  "IK" (I Know), where an LLM is trained to predict its accuracy in answering questions
  by generating a binary score indicating whether retrieval is necessary.
---

# Let your LLM generate a few tokens and you will reduce the need for retrieval

## Quick Facts
- arXiv ID: 2412.11536
- Source URL: https://arxiv.org/abs/2412.11536
- Authors: Hervé Déjean
- Reference count: 35
- Primary result: IK classifier achieves 80% accuracy, reducing retrieval steps by 50% on some datasets

## Executive Summary
This paper presents a method to train large language models (LLMs) to predict when they can answer questions using only their parametric memory versus when they need retrieval-augmented generation (RAG). The approach uses an LLM-as-a-judge to create binary labels indicating whether retrieval is necessary, then trains a classifier (IK) to predict these labels. By including a few generated answer tokens as input, the method achieves good performance with only 20,000 training samples. Experiments show the IK classifier can reduce retrieval frequency by over 50% on certain datasets while maintaining QA quality.

## Method Summary
The method involves training an LLM to predict whether it can answer questions from its parametric memory or needs retrieval. An LLM-as-a-judge evaluates the LLM's responses to create binary labels (Yes/No) for whether retrieval is needed. The IK classifier is then trained to predict these labels, with the option to include the first few generated tokens as input features. The approach is evaluated across multiple QA datasets using standardized benchmarks, showing that the IK classifier can achieve 80% accuracy and significantly reduce unnecessary retrieval operations.

## Key Results
- IK classifier achieves approximately 80% accuracy in predicting when retrieval is needed
- Including generated answer tokens (32-64 tokens) improves performance over using questions alone
- The method reduces retrieval frequency by 50% or more on certain datasets without quality degradation
- IK scores serve as useful tools for characterizing dataset difficulty and LLM knowledge gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The IK classifier learns to detect when the LLM's parametric memory contains the answer without retrieval
- Mechanism: The LLM-as-a-judge evaluates the LLM's generated answer against the gold answer, creating binary labels (Yes/No) indicating whether retrieval is needed. The IK classifier is then trained to predict these labels
- Core assumption: The LLM-as-a-judge provides reliable binary labels for whether the LLM can answer without retrieval
- Evidence anchors:
  - [abstract] "We distill an LLM-as-a-judge to compute the IK (I Know) score"
  - [section 3.1] "The score generated by the LLM-as-a-judge is used as a binary label (Yes or No)"
  - [corpus] Weak - corpus papers don't directly address IK score methodology

### Mechanism 2
- Claim: Including generated answer tokens as input to the IK classifier improves accuracy
- Mechanism: The first 32 tokens of the LLM's generated answer provide contextual information that helps the classifier determine if the answer is likely correct or if retrieval is needed
- Core assumption: The early tokens of a generated answer contain sufficient signal about the LLM's confidence/knowledge
- Evidence anchors:
  - [abstract] "through the inclusion of response tokens as input, our results suggest that only about 20,000 training samples are required to achieve good performance"
  - [section 3.1] "we also allow the model to generate just a few tokens, making the detection task easier"
  - [section 4.3] "Considering the full length is also not mandatory. A length of 32 or 64 tokens is helpful enough"
  - [corpus] Weak - corpus papers don't address token inclusion methodology

### Mechanism 3
- Claim: The IK classifier can generalize from one dataset to others for determining when retrieval is needed
- Mechanism: The classifier learns general patterns about when an LLM is confident vs uncertain, which transfers across different question-answering datasets
- Core assumption: The patterns of LLM confidence/uncertainty are consistent across different types of questions and domains
- Evidence anchors:
  - [section 4.2] "We examine how the number of tokens in the generated answer affects performance as well as the amount of training material"
  - [section 4.2] "Table 2 shows the application of the IK classifier obtained with the Mistral model (row 4 of Table 1) on our other datasets"
  - [section 4.2] "Three distinct patterns or behaviors emerge from this visualization" when applied to different datasets
  - [corpus] Weak - corpus papers don't address cross-dataset generalization of IK classifiers

## Foundational Learning

- Concept: Binary classification with LLM-as-judge
  - Why needed here: The core training process relies on converting LLM judgments into binary labels for whether retrieval is needed
  - Quick check question: How does the LLM-as-a-judge determine whether to output "Yes" or "No" for each question?

- Concept: Token inclusion for confidence detection
  - Why needed here: The method requires understanding how to use generated tokens as additional input features for the classifier
  - Quick check question: What is the optimal number of generated tokens to include for best performance?

- Concept: Cross-dataset generalization
  - Why needed here: The IK classifier is trained on one dataset (NQ) but evaluated on multiple others, requiring understanding of transfer learning principles
  - Quick check question: Why might a classifier trained on NQ perform differently on TriviaQA vs HotpotQA?

## Architecture Onboarding

- Component map:
  Question input pipeline → LLM generator → Generated answer (up to 128 tokens) → IK classifier → "Yes/No" prediction → RAG decision → (Optional) Retrieval pipeline → Reranker → Final answer

- Critical path: Question → IK classifier prediction → RAG decision → (if RAG) Retrieval → Reranker → Final answer generation

- Design tradeoffs:
  - Number of tokens to include: More tokens provide better accuracy but increase latency and computation
  - Threshold selection: Higher thresholds reduce retrieval frequency but may miss cases where retrieval would help
  - Judge selection: Different judges produce different training labels, affecting classifier behavior
  - Training data size: Smaller datasets work with token inclusion but may reduce generalization

- Failure signatures:
  - Low accuracy on IK task but good QA results: Classifier is conservative, retrieving too often
  - High accuracy on IK task but poor QA results: Classifier is overconfident, missing cases where retrieval helps
  - Classifier performance varies dramatically across datasets: Overfitting to training data patterns
  - Including tokens doesn't improve accuracy: Early tokens don't contain useful confidence signals

- First 3 experiments:
  1. Train IK classifier on NQ with 0 tokens vs 32 tokens and compare accuracy
  2. Test cross-dataset generalization by evaluating trained classifier on TriviaQA and HotpotQA
  3. Vary the IK threshold from 0.3 to 0.7 and measure impact on retrieval percentage and final QA accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different types of judges (e.g., LLM-as-a-judge vs. string-based metrics) on the robustness and generalizability of the IK classifier?
- Basis in paper: [explicit] The paper investigates the impact of different judges on the performance of the IK classifier, finding that LLM-as-a-judge performs better than string-based metrics.
- Why unresolved: While the paper shows that LLM-as-a-judge is more effective, it does not explore the reasons behind this difference or the potential biases introduced by different types of judges.
- What evidence would resolve it: Further experiments comparing the performance of IK classifiers trained with different judges on various datasets and analyzing the types of errors made by each classifier.

### Open Question 2
- Question: How does the size of the training dataset affect the performance of the IK classifier, and is there a minimum dataset size required for effective training?
- Basis in paper: [explicit] The paper explores the impact of training dataset size on IK classifier performance, finding that a small dataset (5K samples) can achieve reasonable results when using generated tokens as input.
- Why unresolved: The paper does not investigate the optimal training dataset size or the potential trade-offs between dataset size and classifier performance.
- What evidence would resolve it: Experiments varying the training dataset size and analyzing the corresponding performance of the IK classifier, potentially using techniques like active learning to identify the most informative samples.

### Open Question 3
- Question: Can the IK classifier be adapted to different LLM families and model sizes, and how does its performance vary across different configurations?
- Basis in paper: [explicit] The paper evaluates the IK classifier on different LLM families (Meta-Llama, Mistral, Gemma, SOLAR) and finds that it is generally robust across these models.
- Why unresolved: While the paper shows that the IK classifier is robust, it does not explore the potential for further adaptation or the impact of model size on classifier performance.
- What evidence would resolve it: Experiments adapting the IK classifier to different LLM families and model sizes, potentially using techniques like transfer learning or fine-tuning to optimize performance for each configuration.

## Limitations

- The reliability of the LLM-as-a-judge for generating binary training labels is uncertain and directly impacts classifier quality
- Cross-dataset generalization is based on limited evaluation and may not hold for significantly different domains
- The optimal threshold values for different datasets are not systematically characterized
- The method requires access to an LLM-as-a-judge, which may not be available for all use cases

## Confidence

**High Confidence:**
- The IK classifier can achieve approximately 80% accuracy on the binary task of predicting when retrieval is needed
- Including generated answer tokens as input consistently improves classifier performance across datasets
- The method reduces retrieval frequency by 50% or more on certain datasets without significant quality degradation

**Medium Confidence:**
- The IK score serves as a useful tool for characterizing dataset difficulty and LLM knowledge gaps
- The approach generalizes across different LLM families (Mistral, LLaMA, Zephyr, SOLAR)
- Training on 20,000 samples is sufficient for good performance when including answer tokens

**Low Confidence:**
- The specific threshold values (0.3-0.7) for different datasets are optimal
- The LLM-as-a-judge provides consistent and reliable binary labels across all question types
- The observed patterns would generalize to domains significantly different from the KILT benchmark

## Next Checks

1. **Judge Consistency Validation**: Run the LLM-as-a-judge evaluation on a held-out validation set of 1,000 samples multiple times with different random seeds and judge configurations. Measure label consistency and identify question types where the judge shows high variance. This will quantify the reliability of the training data generation process.

2. **Cross-Domain Generalization Test**: Train the IK classifier on NQ but evaluate on completely different domains such as biomedical QA (PubMedQA), legal questions, or code generation tasks. Compare performance degradation to the in-domain results to understand the limits of cross-dataset transfer.

3. **Retrieval-Accuracy Tradeoff Analysis**: Systematically sweep the IK threshold from 0.1 to 0.9 in increments of 0.1 across all evaluation datasets. Plot retrieval percentage versus final QA accuracy to identify optimal operating points for different use cases and quantify the precision-recall tradeoff inherent in the approach.