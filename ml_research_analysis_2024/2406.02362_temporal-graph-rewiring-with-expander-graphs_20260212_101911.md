---
ver: rpa2
title: Temporal Graph Rewiring with Expander Graphs
arxiv_id: '2406.02362'
source_url: https://arxiv.org/abs/2406.02362
tags:
- graph
- temporal
- node
- graphs
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TGR (Temporal Graph Rewiring), the first temporal
  graph rewiring method that constructs message-passing highways between temporally
  distant nodes in CTDGs. TGR leverages expander graph propagation to alleviate under-reaching
  and memory staleness in TGNNs.
---

# Temporal Graph Rewiring with Expander Graphs

## Quick Facts
- arXiv ID: 2406.02362
- Source URL: https://arxiv.org/abs/2406.02362
- Reference count: 20
- TGR achieves state-of-the-art results on four TGB datasets, with up to 50.5% improvement in MRR over base TGN

## Executive Summary
This paper introduces TGR (Temporal Graph Rewiring), the first method to address under-reaching and memory staleness in Temporal Graph Neural Networks (TGNNs) through expander graph-based temporal graph rewiring. TGR constructs message-passing highways between temporally distant nodes in continuous-time dynamic graphs, alleviating the limitations of strict time-ordering constraints in message paths. The method integrates seamlessly into existing TGNN architectures by enhancing node features with expander embeddings computed through memory mixing. On the challenging TGB benchmark, TGR demonstrates significant performance improvements, particularly on datasets with high surprise indices and bipartite structures.

## Method Summary
TGR is a temporal graph rewiring framework that enhances TGNNs by constructing message-passing highways between temporally distant nodes using expander graph propagation. The method integrates with existing TGNN architectures (TGN and TNCN) by augmenting node features with expander embeddings computed through a memory mixing step. During each batch, TGR extracts temporal memory states for observed nodes, computes expander embeddings via a GNN over a precomputed expander graph, and concatenates these embeddings with temporal memory states for input to the base TGNN. This approach alleviates under-reaching by enabling information propagation across temporal gaps and mitigates memory staleness by allowing consistent information flow between inactive nodes.

## Key Results
- TGR achieves 50.5% improvement in MRR over base TGN on tgbl-review dataset
- TGR achieves 22.2% improvement in MRR over base TNCN on tgbl-coin dataset
- Significant improvements demonstrated on bipartite datasets (tgbl-wiki, tgbl-review) and high-surprise-index datasets (tgbl-review, tgbl-comment)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal graph rewiring alleviates under-reaching by constructing message-passing highways between temporally distant nodes.
- Mechanism: TGR uses expander graph propagation to create a virtual topology independent of the input graph. By mixing expander embeddings into node features, information can propagate across nodes that would otherwise be temporally unreachable due to the strict time-ordering constraint on message paths.
- Core assumption: Expander graphs provide a fixed, sparse, and high-connectivity topology that can efficiently propagate information globally without requiring dynamic preprocessing.
- Evidence anchors:
  - [abstract] "TGR constructs message passing highways between temporally distant nodes in a continuous-time dynamic graph by utilizing expander graph propagation"
  - [section 4.1] "expander graphs satisfy four desirable criteria for graph rewiring: 1) the ability to efficiently propagate information globally within the input graph, 2) relieving bottlenecks and over-squashing, 3) subquadratic space and time complexity, and 4) no additional pre-processing of the input graph"
  - [corpus] Weak; related works discuss general graph rewiring but not temporal-specific expander usage.
- Break condition: If the expander graph topology is too sparse or poorly constructed, the message-passing highways may not connect relevant node pairs, reducing effectiveness.

### Mechanism 2
- Claim: Temporal graph rewiring mitigates memory staleness by enabling consistent information flow between inactive nodes.
- Mechanism: By augmenting observed nodes' features with expander embeddings derived from a global memory mixing step, TGR allows inactive nodes to retain and exchange information even when they are not directly involved in recent interactions. This counters the TGNN tendency for node states to become stale when nodes are inactive.
- Core assumption: The expander embeddings capture long-range dependencies and can be meaningfully integrated into the base TGNN's message-passing process without disrupting local temporal dynamics.
- Evidence anchors:
  - [abstract] "memory staleness in TGNNs" is explicitly mentioned as a problem TGR addresses
  - [section 1] "By memory staleness we refer to a process occurring in the temporal memory of TGNNs: The temporal memory is only updated if a node of interest is observed to interact with another node in a graph, and this causes inactive nodes' states to become stale."
  - [corpus] Weak; related works discuss static graph rewiring but not memory staleness in temporal contexts.
- Break condition: If the expander embeddings dominate over local temporal features, the model may lose sensitivity to recent interactions and degrade performance.

### Mechanism 3
- Claim: Temporal graph rewiring improves performance on bipartite and high-surprise-index datasets by overcoming structural limitations of TGNNs.
- Mechanism: In bipartite graphs, TGNNs struggle to mix information between nodes of the same class due to lack of direct edges. TGR's expander graph introduces shortcuts that bypass this limitation. Similarly, high-surprise-index datasets contain edges between nodes with no obvious training-time connections; expander-based rewiring enables these links to be predicted by providing a richer latent structure.
- Core assumption: The expander graph can model latent relationships not captured by observed temporal edges, and these relationships are relevant for the downstream link prediction task.
- Evidence anchors:
  - [section 5] "TGR shows higher improvement across datasets with high surprise indices such are tgbl-review and tgbl-comment" and "TGR achieves significant improvement over bipartite datasets such are tgbl-wiki and tgbl-review"
  - [corpus] Weak; related works discuss static graph rewiring benefits but not specifically for bipartite or high-surprise-index temporal graphs.
- Break condition: If the expander graph introduces spurious connections that do not correspond to meaningful temporal relationships, performance may degrade or plateau.

## Foundational Learning

- Concept: Expander graphs
  - Why needed here: Expander graphs provide the theoretical foundation for efficient global information propagation without bottlenecks, which is essential for the rewiring mechanism.
  - Quick check question: What property of expander graphs ensures no bottlenecks and efficient mixing of information?

- Concept: Message passing in GNNs
  - Why needed here: Understanding how information flows through layers in GNNs is crucial to grasp why temporal under-reaching occurs and how rewiring addresses it.
  - Quick check question: How many layers are typically needed for a node to receive information from another node at distance k in a static GNN?

- Concept: Temporal memory in TGNNs
  - Why needed here: The temporal memory stores node states over time; knowing how it updates is key to understanding memory staleness and how expander embeddings integrate with it.
  - Quick check question: When does a TGNN update the memory vector of a node?

## Architecture Onboarding

- Component map:
  - Node Bank -> Temporal Memory -> Expander Memory -> Memory Mixing Module -> Base TGNN -> Decoder

- Critical path:
  1. At each batch, identify new vs observed nodes
  2. Extract temporal memory states for observed nodes
  3. Compute expander embeddings via GNN over expander graph
  4. Concatenate expander embeddings with temporal memory states for input features
  5. Run base TGNN forward pass
  6. Update temporal memory and expander memory

- Design tradeoffs:
  - Precomputing a large expander graph vs. computing on-the-fly: precomputation is faster per batch but uses more memory
  - Concatenating vs. adding expander embeddings: concatenation preserves both signals but increases dimensionality
  - Number of attention heads in expander GNN: more heads increase expressiveness but add compute

- Failure signatures:
  - Degraded performance on datasets with low surprise index or dense temporal connectivity
  - Memory usage spikes due to large precomputed expander graph
  - Training instability if expander embeddings dominate temporal features

- First 3 experiments:
  1. Replace expander graph with a random graph of same sparsity; compare MRR to baseline
  2. Vary the number of attention heads in the expander GNN; plot performance vs. compute
  3. Test on a synthetic temporal graph with known under-reaching patterns; verify TGR recovers missing links

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TGR scale with increasing graph size and temporal complexity?
- Basis in paper: [inferred] The paper demonstrates TGR's effectiveness on four TGB datasets of varying sizes but doesn't provide systematic scaling analysis.
- Why unresolved: The experiments only cover a limited range of dataset sizes, and no explicit analysis of computational complexity or memory requirements as a function of graph size is provided.
- What evidence would resolve it: Experiments on a wider range of TGB datasets, including larger-scale ones, combined with analysis of runtime and memory usage as a function of graph size and temporal complexity.

### Open Question 2
- Question: What is the impact of different expander graph constructions on TGR's performance?
- Basis in paper: [explicit] The paper mentions using Cayley graphs but doesn't compare TGR's performance with other expander graph constructions.
- Why unresolved: Only one type of expander graph (Cayley graph) is evaluated, and the choice of expander graph construction is presented as a hyperparameter without exploring alternatives.
- What evidence would resolve it: Experiments comparing TGR's performance using different expander graph constructions, such as random regular graphs or Ramanujan graphs, on the same datasets.

### Open Question 3
- Question: How does TGR's performance compare to other temporal graph rewiring methods, if any exist?
- Basis in paper: [explicit] The paper claims TGR is the first approach for graph rewiring on temporal graphs, making direct comparisons impossible.
- Why unresolved: There are no existing temporal graph rewiring methods to compare against, as TGR is presented as the first of its kind.
- What evidence would resolve it: Development and evaluation of alternative temporal graph rewiring methods, followed by comparison of their performance with TGR on the same datasets and tasks.

## Limitations
- Limited experimental validation beyond the TGB benchmark; results on other temporal graph datasets are not reported
- No ablation studies isolating the contribution of expander graph propagation vs. memory mixing vs. base TGNN architecture
- Theoretical guarantees of expander graph efficiency are assumed but not empirically verified for the specific temporal settings used

## Confidence
- High: TGR framework architecture and integration with base TGNNs
- Medium: Performance improvements on TGB datasets; the magnitude of gains depends on specific dataset characteristics
- Low: Claims about under-reaching and memory staleness mechanisms; direct evidence linking these phenomena to observed improvements is weak

## Next Checks
1. Perform ablation study removing the expander graph component to measure its isolated contribution to MRR gains
2. Test TGR on a temporal graph dataset with known under-reaching patterns to verify the rewiring mechanism addresses the stated problem
3. Evaluate memory usage and training time overhead introduced by TGR's expander graph propagation to assess scalability beyond the TGB benchmark