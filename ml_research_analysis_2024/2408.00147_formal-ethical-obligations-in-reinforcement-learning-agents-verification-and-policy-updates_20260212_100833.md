---
ver: rpa2
title: 'Formal Ethical Obligations in Reinforcement Learning Agents: Verification
  and Policy Updates'
arxiv_id: '2408.00147'
source_url: https://arxiv.org/abs/2408.00147
tags:
- policy
- agent
- gradient
- probability
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents formal methods for ensuring reinforcement learning
  agents meet ethical and social obligations. The authors extend Expected Act Utilitarian
  deontic logic with strategic modalities to specify obligations that must be satisfied
  by an agent's optimal policy.
---

# Formal Ethical Obligations in Reinforcement Learning Agents: Verification and Policy Updates

## Quick Facts
- arXiv ID: 2408.00147
- Source URL: https://arxiv.org/abs/2408.00147
- Authors: Colin Shea-Blymyer; Houssam Abbas
- Reference count: 3
- Presents formal methods for ensuring RL agents meet ethical and social obligations

## Executive Summary
This paper introduces formal methods to verify and modify reinforcement learning agent policies to satisfy ethical and social obligations. The authors extend Expected Act Utilitarian deontic logic with strategic modalities to specify obligations that must be met by an agent's optimal policy. They develop two key algorithms: a model checker that verifies if an MDP's optimal policy satisfies given strategic obligations, and a policy update method that modifies reward-optimal policies to meet specified obligations while maintaining high expected utility. Experiments demonstrate these algorithms on scenarios including a windy-drone delivery system and cartpole balancing, showing they can effectively verify and modify policies to satisfy obligations like avoiding playgrounds or maintaining safety constraints.

## Method Summary
The authors develop a formal framework combining Expected Act Utilitarian deontic logic with strategic modalities to specify obligations for RL agents. They implement two core algorithms: (1) a model checker that exhaustively computes optimal policies for all possible transitions and verifies if these policies satisfy specified strategic obligations using model checking techniques, and (2) a policy update method that uses alternating gradient descent to modify reward-optimal policies to satisfy obligations while maintaining utility. The approach assumes perfect knowledge of the MDP and uses a wind model with state-dependent transition probabilities to create realistic scenarios where agents must balance utility maximization with obligation satisfaction.

## Key Results
- Successfully verified obligation satisfaction on a 20-state windy-drone delivery scenario
- Policy update algorithms achieved good trade-offs between utility and obligation satisfaction
- Alternating gradient update method performed particularly well in maintaining high expected utility while meeting obligations
- Demonstrated ability to modify policies to avoid playgrounds and maintain safety constraints

## Why This Works (Mechanism)
The approach works by formally encoding ethical obligations as strategic formulas in an extended deontic logic framework, then using model checking to verify if optimal policies satisfy these obligations. The policy update mechanism iteratively modifies the reward structure to incorporate obligation satisfaction as a soft constraint while maintaining high expected utility through gradient-based optimization. This dual approach of verification and modification allows for both ensuring compliance with specified obligations and adapting existing policies to meet new ethical requirements.

## Foundational Learning
- **Expected Act Utilitarian deontic logic**: A formal logical framework for specifying ethical obligations and permissions, needed to precisely encode what constitutes acceptable behavior for agents; quick check: verify logical formulas correctly capture intended ethical constraints
- **Strategic modalities**: Extensions to deontic logic that incorporate strategic reasoning about other agents' behaviors, needed to specify obligations that depend on multi-agent interactions; quick check: ensure formulas properly account for all possible agent strategies
- **Model checking for MDPs**: Exhaustive verification technique that computes all optimal policies and checks them against logical specifications, needed to provide mathematical guarantees of obligation satisfaction; quick check: confirm all reachable states and transitions are properly enumerated
- **Alternating gradient descent**: Optimization technique that alternately updates policy and reward parameters, needed to balance obligation satisfaction with utility maximization; quick check: monitor convergence and ensure neither objective dominates
- **Wind model with state-dependent transitions**: Probabilistic transition model that varies based on agent position, needed to create realistic uncertainty scenarios; quick check: validate transition probabilities sum to 1 and reflect intended behavior
- **Soft constraint optimization**: Method for incorporating multiple objectives by treating some as soft constraints, needed to maintain high utility while satisfying obligations; quick check: tune weighting parameters to achieve desired balance

## Architecture Onboarding

Component map: Ethical obligations -> Logical specification -> Model checking -> Verification result -> Policy update (if needed) -> Modified policy

Critical path: Formal obligation specification → Model checking verification → If failed → Policy update optimization → Verified modified policy

Design tradeoffs: Exhaustive model checking provides mathematical guarantees but doesn't scale to large state spaces; soft constraint optimization balances objectives but requires careful parameter tuning; perfect MDP knowledge assumption simplifies verification but limits real-world applicability.

Failure signatures: Model checking fails when state space is too large for exhaustive computation; policy updates fail when obligations are incompatible with high utility; verification fails when logical specifications are too restrictive or improperly formulated.

First experiments: 1) Verify simple grid world with playground avoidance obligation; 2) Test policy update on cartpole balancing with safety constraints; 3) Evaluate wind model behavior on drone delivery scenario with multiple waypoints.

## Open Questions the Paper Calls Out
None

## Limitations
- Model checking approach requires exhaustive computation of optimal policies, making it computationally prohibitive for larger state spaces
- Assumes perfect knowledge of the MDP, which may not hold in real-world scenarios where models are learned or partially observable
- Experiments are limited to relatively simple environments like grid worlds and cartpole, not demonstrating scalability to complex real-world applications

## Confidence
- Formal logical framework: High
- Model checking approach: Medium (scalability concerns)
- Policy update algorithms: Medium (limited experimental scope)
- Practical applicability: Low (real-world complexity not addressed)

## Next Checks
1. Test model checking scalability on larger MDPs (e.g., 100+ states) to establish practical limits
2. Evaluate performance when transition dynamics are imperfectly known or learned from data
3. Validate obligation specification process with domain experts to assess usability in real ethical scenarios