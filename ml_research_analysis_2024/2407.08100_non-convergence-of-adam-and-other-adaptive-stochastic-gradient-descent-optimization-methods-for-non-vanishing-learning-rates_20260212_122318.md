---
ver: rpa2
title: Non-convergence of Adam and other adaptive stochastic gradient descent optimization
  methods for non-vanishing learning rates
arxiv_id: '2407.08100'
source_url: https://arxiv.org/abs/2407.08100
tags:
- holds
- lemma
- proof
- random
- every
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work establishes non-convergence results for adaptive stochastic
  gradient descent (SGD) optimization methods, including the popular Adam optimizer,
  when learning rates are asymptotically bounded away from zero. The authors prove
  that such methods fail to converge to any possible random limit point under these
  conditions.
---

# Non-convergence of Adam and other adaptive stochastic gradient descent optimization methods for non-vanishing learning rates

## Quick Facts
- arXiv ID: 2407.08100
- Source URL: https://arxiv.org/abs/2407.08100
- Reference count: 40
- Primary result: Establishes non-convergence of adaptive SGD methods (including Adam) when learning rates remain bounded away from zero

## Executive Summary
This work proves that popular adaptive stochastic gradient descent methods, including Adam, fail to converge to any random limit point when learning rates are asymptotically bounded away from zero. The authors develop a generalized framework for conditional expectations and variances beyond improper integrable random variables, and establish pathwise a priori bounds for accelerated and adaptive SGD methods. These technical tools are then applied to prove that the expected squared error remains bounded away from zero under the stated conditions, demonstrating non-convergence.

## Method Summary
The authors prove non-convergence through a three-step approach: first establishing pathwise a priori bounds for adaptive SGD methods using their generalized conditional expectation framework; second, developing factorization lemmas for these generalized expectations and variances; and third, applying these tools to show that the expected squared error does not vanish when learning rates are bounded away from zero. The proof relies on careful analysis of the adaptive learning rate mechanism and its interaction with gradient estimates, using both theoretical quadratic optimization problems and deep neural network settings.

## Key Results
- Adaptive SGD methods fail to converge to any random limit point if learning rates remain bounded away from zero
- The expected squared error remains bounded away from zero under bounded mini-batch sizes and non-vanishing learning rates
- The authors develop a generalized conditional expectation framework that extends standard theory to improper integrable random variables

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive SGD methods fail to converge to any random limit point if learning rates remain bounded away from zero
- Mechanism: The adaptive learning rate mechanism introduces persistent oscillations in parameter updates, preventing convergence. The proof shows expected squared error remains bounded away from zero
- Core assumption: Mini-batch sizes bounded from above and learning rates both bounded from above and asymptotically bounded away from zero
- Evidence anchors:
  - [abstract]: "This work establishes non-convergence results for adaptive stochastic gradient descent (SGD) optimization methods... if learning rates are asymptotically bounded away from zero."
  - [section 4.1]: "In Proposition 4.3 we establish suitable lower bounds for variances of appropriately scaled random variables."
  - [corpus]: Weak - neighboring papers discuss non-convergence but don't provide the specific variance-based proof structure
- Break condition: If learning rates decrease to zero or mini-batch sizes grow unboundedly

### Mechanism 2
- Claim: The proof relies on establishing pathwise a priori bounds for accelerated and adaptive SGD methods
- Mechanism: Pathwise bounds on the optimization process behavior prevent error from vanishing, established through analysis of adaptive learning rate updates and gradient estimates
- Core assumption: Gradients are bounded and adaptation mechanism has bounded adaptation rates
- Evidence anchors:
  - [abstract]: "In our proof of this non-convergence result we establish suitable pathwise a priori bounds for a class of accelerated and adaptive SGD methods, which are also of independent interest."
  - [section 2.3]: Contains the specific bounds for Adam and other adaptive methods
  - [corpus]: Missing - no neighboring papers discuss the specific pathwise bounding technique
- Break condition: If gradients become unbounded or adaptation mechanism violates boundedness assumptions

### Mechanism 3
- Claim: The authors develop generalized conditional expectations and variances beyond improper integrable random variables
- Mechanism: This framework handles stochastic processes when standard integrability conditions fail, with factorization lemmas as key technical tools
- Core assumption: Suitable sigma-algebras and random variables satisfy generalized integrability conditions
- Evidence anchors:
  - [abstract]: "The authors also present a generalized variant of the standard concepts of conditional expectations... and develop factorization lemmas for such generalized conditional expectations and variances."
  - [section 3]: Detailed development of the generalized conditional expectation framework
  - [corpus]: Weak - neighboring papers don't discuss this generalized conditional expectation approach
- Break condition: If generalized integrability conditions cannot be satisfied for specific stochastic processes

## Foundational Learning

- Concept: Stochastic Gradient Descent (SGD) optimization
  - Why needed here: The paper analyzes convergence properties of SGD and its adaptive variants
  - Quick check question: What is the main difference between standard SGD and adaptive methods like Adam?

- Concept: Conditional expectations and variances
  - Why needed here: The proof relies heavily on conditional expectations and variances of stochastic processes
  - Quick check question: How do generalized conditional expectations extend the standard theory?

- Concept: Pathwise analysis of stochastic processes
  - Why needed here: The convergence analysis requires bounding the behavior of sample paths of the optimization process
  - Quick check question: What is the difference between pathwise bounds and bounds in probability?

## Architecture Onboarding

- Component map: Mathematical framework setup -> Pathwise bounds establishment -> Generalized conditional expectations development -> Non-convergence proof application
- Critical path: Establish pathwise bounds → Develop generalized conditional expectations → Apply factorization lemmas → Prove non-convergence
- Design tradeoffs: Proving non-convergence for broad class of adaptive methods increases generality but also complexity
- Failure signatures: Proof fails if learning rates decrease to zero, mini-batch sizes grow unboundedly, or gradient estimates become too volatile
- First 3 experiments:
  1. Implement the simple quadratic optimization problem from Theorem 1.1 and verify non-convergence with constant learning rates
  2. Apply Adam to the same problem with asymptotically constant learning rates and observe lack of convergence
  3. Modify learning rate schedule to decay to zero and verify that convergence is restored

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do adaptive SGD methods like Adam converge for non-vanishing learning rates in specific problem classes (e.g., strongly convex or specific neural network architectures)?
- Basis in paper: [explicit] The paper proves non-convergence for Adam and other adaptive SGD methods with non-vanishing learning rates in general settings, but notes that convergence results exist for certain problem classes
- Why unresolved: The paper's non-convergence results are general, but do not rule out convergence in specific problem classes or under certain conditions
- What evidence would resolve it: Analytical or empirical evidence demonstrating convergence (or lack thereof) of adaptive SGD methods for non-vanishing learning rates in specific problem classes

### Open Question 2
- Question: Can the non-convergence results for adaptive SGD methods be extended to more realistic settings, such as stochastic optimization problems with noise or non-stationary data?
- Basis in paper: [inferred] The paper's results are based on idealized stochastic optimization problems. The authors do not address the impact of noise or non-stationarity
- Why unresolved: The paper's theoretical analysis does not account for practical challenges in real-world optimization problems
- What evidence would resolve it: Analytical or empirical studies examining the behavior of adaptive SGD methods in the presence of noise or non-stationary data

### Open Question 3
- Question: Are there alternative adaptive learning rate strategies that can overcome the non-convergence issues identified in the paper?
- Basis in paper: [inferred] The paper focuses on specific adaptive SGD methods (RMSprop, Adam) and their non-convergence. It does not explore alternative strategies
- Why unresolved: The paper's results do not preclude the possibility of developing new adaptive learning rate methods that avoid the identified issues
- What evidence would resolve it: Development and analysis of alternative adaptive learning rate methods that provably converge in the presence of non-vanishing learning rates

## Limitations
- The proof relies on the assumption that mini-batch sizes remain bounded, which may not hold in practical large-scale implementations
- The generalized conditional expectation framework requires verification that all technical conditions hold for specific stochastic processes in deep learning applications
- The results are asymptotic in nature, and finite-time behavior may differ significantly from the limiting behavior established in the theorems

## Confidence
- **High**: The non-convergence result for quadratic problems (Theorem 1.1) - the proof is complete and the mechanism is well-understood
- **Medium**: The extension to deep neural networks (Theorem 1.2) - while the approach is sound, the technical conditions require careful verification in practice
- **Medium**: The pathwise bounds for adaptive SGD methods - the framework is novel and its applicability beyond the specific cases considered needs further exploration

## Next Checks
1. **Empirical verification**: Implement the simple quadratic optimization problem from Theorem 1.1 and empirically verify non-convergence with constant learning rates across multiple random seeds
2. **Boundary analysis**: Systematically vary the learning rate schedule to identify the precise threshold where convergence is restored, testing the sharpness of the theoretical bounds
3. **Practical assessment**: Apply the analysis to real-world deep learning tasks with varying mini-batch sizes to determine the practical relevance of the bounded mini-batch assumption