---
ver: rpa2
title: 'GraphIC: A Graph-Based In-Context Example Retrieval Model for Multi-Step Reasoning'
arxiv_id: '2410.02203'
source_url: https://arxiv.org/abs/2410.02203
tags:
- reasoning
- graphic
- examples
- similarity
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GraphIC addresses the challenge of selecting effective in-context\
  \ examples for multi-step reasoning tasks by introducing a graph-based retrieval\
  \ model. Unlike traditional text embedding methods that capture superficial semantic\
  \ similarity, GraphIC constructs thought graphs\u2014directed, node-attributed graphs\
  \ explicitly modeling reasoning steps and dependencies."
---

# GraphIC: A Graph-Based In-Context Example Retrieval Model for Multi-Step Reasoning

## Quick Facts
- arXiv ID: 2410.02203
- Source URL: https://arxiv.org/abs/2410.02203
- Authors: Jiale Fu; Yaqing Wang; Simeng Han; Jiaming Fan; Xu Yang
- Reference count: 8
- Key outcome: GraphIC achieves up to 2.5% higher accuracy than training-based approaches on multi-step reasoning tasks by using graph-based in-context example retrieval

## Executive Summary
GraphIC introduces a novel approach to in-context learning for multi-step reasoning tasks by constructing thought graphs that explicitly model reasoning steps and dependencies. Unlike traditional text embedding methods that capture superficial semantic similarity, GraphIC converts reasoning processes into structured graph representations where nodes represent reasoning steps and edges capture dependencies. The model employs a novel asymmetric similarity metric that estimates reasoning patterns from one graph and assesses their applicability to another, enabling more effective retrieval of relevant in-context examples. Comprehensive evaluations across mathematical reasoning, code generation, and logical reasoning tasks demonstrate significant performance improvements over 10 baseline methods.

## Method Summary
GraphIC constructs thought graphs—directed, node-attributed graphs that explicitly model reasoning steps and dependencies—for both candidate examples and queries. For candidate examples with ground-truth reasoning processes, the model generates formalized reasoning representations (FRRs) that can be parsed into graphs. For queries without ground-truth reasoning, pseudo reasoning processes are generated. A novel similarity metric captures sequential reasoning patterns and asymmetry between examples through an iterative aggregation process that allows information from multiple prior steps and occasional backtracking to initial premises. The model retrieves top-k most relevant examples based on computed similarities and feeds them to a large language model for final answer generation.

## Key Results
- GraphIC achieves up to 2.5% higher accuracy than training-based approaches on mathematical reasoning tasks
- The method outperforms 10 baseline approaches across four benchmark datasets: GSM8K, AQUA, MBPP, and ProofWriter
- GraphIC demonstrates robustness to inaccuracies in generated reasoning processes and maintains effectiveness even when reasoning steps are imperfect
- The graph-based approach significantly outperforms traditional text embedding methods that capture only semantic similarity

## Why This Works (Mechanism)

### Mechanism 1
GraphIC's thought graphs explicitly model reasoning steps and dependencies, filtering out irrelevant semantic information while preserving essential reasoning patterns. The model converts reasoning processes into directed, node-attributed graphs where nodes represent reasoning steps and edges represent dependencies between steps. This structured representation captures the sequential nature of reasoning rather than just semantic similarity. The core assumption is that multi-step reasoning tasks benefit more from structured representations of reasoning processes than from semantic similarity alone.

### Mechanism 2
The novel similarity metric captures sequential reasoning patterns and asymmetry between examples by estimating reasoning patterns from one graph and assessing their applicability to another. For two thought graphs G1 and G2, the model first estimates the reasoning pattern parameter W1 for G1, then assesses how well this pattern applies to G2. This creates an asymmetric similarity measure that better reflects real-world reasoning relationships. The core assumption is that reasoning patterns are directional and asymmetric - understanding how to solve problem A doesn't necessarily mean understanding how to solve problem B, even if B can help solve A.

### Mechanism 3
The iterative aggregation process in the similarity metric allows information from multiple prior steps and occasional backtracking to initial premises. The model uses an iterative formula Z^(h+1) = [(1-λ)A + λB + I]Z^(h) where A aggregates information from direct parent nodes, B enables backtracking to root nodes, and I maintains current information. This captures more complex reasoning patterns than simple parent-child relationships. The core assumption is that human reasoning often incorporates information from multiple prior steps and occasionally revisits initial premises, which should be reflected in the similarity measure.

## Foundational Learning

- **Graph theory and graph representations**: Understanding how to construct and manipulate thought graphs is fundamental to implementing GraphIC's core approach. Quick check: Can you explain the difference between directed and undirected graphs, and why directed graphs are appropriate for modeling reasoning dependencies?

- **Embedding models and similarity metrics**: The model uses BERT embeddings for nodes and develops a novel similarity metric based on these embeddings. Quick check: How do you compute cosine similarity between two embedding vectors, and what are the limitations of using simple cosine similarity for graph-structured data?

- **Optimization and matrix operations**: The similarity metric involves solving optimization problems and matrix decompositions (SVD) to compute reasoning patterns. Quick check: What is the purpose of the Frobenius norm constraint in optimization problems, and how does rank-1 approximation simplify matrix computations?

## Architecture Onboarding

- **Component map**: Query processing → Pseudo reasoning generation → Thought graph construction → Similarity computation with all candidate graphs → Top-k retrieval → Output to LLM

- **Critical path**: The system processes queries by first generating pseudo reasoning, constructing thought graphs, computing similarities with all candidate graphs, selecting top-k examples, and passing them to the LLM for final answer generation.

- **Design tradeoffs**: The model trades computational complexity for reasoning-aware retrieval. While graph construction and similarity computation are more expensive than simple text embeddings, they provide significantly better performance on reasoning tasks.

- **Failure signatures**: Poor performance on semantic-centric tasks, high computational cost for large candidate sets, sensitivity to inaccuracies in pseudo reasoning generation, overfitting to specific reasoning patterns.

- **First 3 experiments**:
  1. Implement thought graph construction for a simple mathematical reasoning task and visualize the resulting graphs
  2. Compare GraphIC's similarity metric with standard cosine similarity on a small set of thought graphs
  3. Test the full retrieval pipeline on a subset of the GSM8K dataset with ground truth reasoning processes

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of GraphIC compare when using different graph embedding dimensions, particularly when the number of vertices is much smaller than the embedding dimension? The paper mentions that the number of vertices in thought graphs is typically much smaller than the embedding dimension (e.g., fewer than 10 vertices versus 768 dimensions), and discusses rank-1 constraints to address computational efficiency, but does not provide experimental results comparing GraphIC's performance across different embedding dimensions.

### Open Question 2
Can the thought graph representation be effectively adapted for non-text modalities, such as image-based reasoning tasks? The paper focuses on text-based reasoning tasks (mathematical, code, logical), but the concept of modeling reasoning steps and dependencies as graphs could potentially extend to other modalities.

### Open Question 3
What is the impact of imperfect pseudo-reasoning process generation on GraphIC's performance, and how robust is the model to varying levels of reasoning quality? While the paper states that GraphIC is robust to inaccuracies in generated reasoning processes and does not require correctness, it lacks systematic evaluation of how different levels of reasoning quality affect retrieval performance.

## Limitations

- The method requires ground-truth reasoning processes for candidate examples, which may not always be available in real-world scenarios
- Computational complexity of constructing thought graphs and computing the novel similarity metric could limit scalability to very large candidate pools
- The evaluation focuses primarily on accuracy metrics without extensive analysis of computational overhead or robustness to noise in the reasoning processes

## Confidence

**High confidence** in the core mechanism: The theoretical foundation for using structured thought graphs to capture reasoning dependencies is well-established, and the empirical results consistently show improvements across multiple tasks and datasets.

**Medium confidence** in the iterative aggregation mechanism: While the mathematical formulation is sound, the practical benefits of the backtracking component are not clearly isolated from the overall improvement.

**Medium confidence** in generalizability: The method shows strong performance on the tested datasets, but these tasks share common characteristics. Performance on more diverse reasoning tasks remains untested.

## Next Checks

1. **Ablation study of similarity metric components**: Systematically disable individual components of the similarity metric to quantify the contribution of each to overall performance.

2. **Robustness testing with imperfect reasoning**: Evaluate GraphIC's performance when candidate examples have incomplete, noisy, or partially incorrect reasoning processes.

3. **Scalability and computational overhead analysis**: Measure the actual computational cost of GraphIC compared to baseline methods, particularly for large candidate pools.