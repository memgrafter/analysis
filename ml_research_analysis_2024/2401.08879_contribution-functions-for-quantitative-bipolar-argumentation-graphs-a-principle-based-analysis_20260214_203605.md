---
ver: rpa2
title: 'Contribution Functions for Quantitative Bipolar Argumentation Graphs: A Principle-based
  Analysis'
arxiv_id: '2401.08879'
source_url: https://arxiv.org/abs/2401.08879
tags:
- semantics
- strength
- contribution
- args
- argument
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a principle-based analysis of contribution
  functions for quantitative bipolar argumentation graphs (QBAGs). The authors introduce
  four contribution functions - removal-based, intrinsic removal-based, Shapley values-based,
  and gradient-based - and a set of principles including directionality, contribution
  existence, faithfulness, and counterfactuality.
---

# Contribution Functions for Quantitative Bipolar Argumentation Graphs: A Principle-based Analysis

## Quick Facts
- arXiv ID: 2401.08879
- Source URL: https://arxiv.org/abs/2401.08879
- Reference count: 38
- Key result: Principle-based analysis of four contribution functions for QBAGs across five argumentation semantics, revealing no single function satisfies all principles but identifying optimal choices for specific use cases.

## Executive Summary
This paper provides a comprehensive principle-based analysis of contribution functions for quantitative bipolar argumentation graphs (QBAGs). The authors introduce four distinct contribution functions - removal-based, intrinsic removal-based, Shapley values-based, and gradient-based - and evaluate them against a set of four core principles: directionality, contribution existence, faithfulness, and counterfactuality. The analysis spans five argumentation semantics and is presented in a structured tabular format showing which principles each function satisfies or violates for each semantics. The key finding is that no single contribution function universally satisfies all principles, but the Shapley values-based function emerges as optimal for quantitative contribution existence across all semantics, while the gradient-based function excels in quantitative local faithfulness for differentiable modular semantics.

## Method Summary
The authors establish a principle-based framework for analyzing contribution functions in QBAGs. They define four contribution functions through formal mathematical formulations: removal-based (comparing acceptability values before and after removing an argument), intrinsic removal-based (similar but using intrinsic acceptability values), Shapley values-based (leveraging cooperative game theory concepts), and gradient-based (computing partial derivatives of acceptability functions). These functions are then systematically evaluated against four principles - directionality (positive/negative contributions align with argument polarity), contribution existence (contributions are non-zero when present), faithfulness (preservation of argument relationships), and counterfactuality (sensitivity to argument changes) - across five argumentation semantics (preferred, grounded, complete, stage, and ideal). The analysis uses formal proofs and counterexamples to establish principle satisfaction relationships.

## Key Results
- No single contribution function satisfies all principles across all argumentation semantics
- Shapley values-based contribution function satisfies quantitative contribution existence for all semantics
- Gradient-based contribution function satisfies quantitative local faithfulness for all differentiable modular semantics
- Removal-based and intrinsic removal-based functions show limited principle satisfaction but may be computationally simpler
- The analysis provides a decision framework for selecting appropriate contribution functions based on semantic requirements

## Why This Works (Mechanism)
The principle-based approach works by establishing formal criteria that contribution functions must satisfy to be considered meaningful and reliable in the context of argumentation graphs. By defining clear mathematical properties and systematically testing each function against these properties across different semantics, the analysis reveals inherent trade-offs and limitations in existing approaches. The use of multiple semantics ensures the findings are not tied to a specific argumentation framework, while the structured comparison highlights the nuanced behavior of each function under different conditions.

## Foundational Learning
**Quantitative Bipolar Argumentation Graphs (QBAGs)**: Weighted argumentation graphs that incorporate both attack and support relations with numerical weights, enabling nuanced modeling of argumentative relationships (needed to understand the problem domain; check: can represent both positive and negative influence between arguments with weights).

**Argumentation Semantics**: Formal rules for determining which arguments are acceptable in a graph (needed to contextualize the analysis; check: includes preferred, grounded, complete, stage, and ideal semantics as evaluation targets).

**Contribution Functions**: Mathematical mappings that quantify how much individual arguments contribute to the overall acceptability of arguments in a graph (needed to understand the evaluation targets; check: four types analyzed with distinct computational properties).

**Directionality Principle**: Ensures that positive contributions correspond to supporting arguments and negative contributions correspond to attacking arguments (needed to validate semantic consistency; check: violated by removal-based function for non-monotonic semantics).

**Faithfulness Principle**: Requires that contribution functions preserve the relative relationships between arguments (needed to ensure meaningful interpretations; check: violated by gradient-based function in non-differentiable semantics).

## Architecture Onboarding

**Component Map**: QBAG graph -> Argumentation semantics engine -> Contribution function -> Principle satisfaction evaluation -> Result matrix

**Critical Path**: Input graph and semantics → Computation of argument acceptabilities → Application of contribution function → Verification against principles → Tabular result presentation

**Design Tradeoffs**: Computational complexity vs. principle satisfaction (Shapley values are computationally expensive but satisfy more principles) vs. simplicity (removal-based is simple but satisfies fewer principles)

**Failure Signatures**: 
- Removal-based function fails directionality in non-monotonic semantics
- Gradient-based function fails non-quantitative principles in non-differentiable semantics
- Intrinsic removal-based function exhibits limited faithfulness in complex support structures

**First Experiments**:
1. Apply all four contribution functions to a simple 3-argument bipolar graph with mixed support/attack relations
2. Test principle satisfaction on a graph where monotonicity fails (e.g., A supports B, B supports C, A attacks C)
3. Compare computational runtimes of Shapley values-based vs. gradient-based functions on a 10-argument graph

## Open Questions the Paper Calls Out
None

## Limitations
- Absence of empirical validation or real-world case studies to demonstrate practical utility
- Focus on a specific set of principles without exploring potentially relevant alternative principles
- Differentiability assumption for gradient-based methods restricts applicability to certain semantics
- Computational complexity of Shapley values-based function may limit scalability

## Confidence
- Principle-based framework: High
- Characterization of contribution functions: High
- Comparative analysis across semantics: High
- Practical implications and recommendations: Medium
- Empirical validation: Low

## Next Checks
1. Conduct empirical studies comparing the proposed contribution functions on real-world argumentation datasets to validate their practical utility and computational efficiency.
2. Extend the principle-based analysis to include additional argumentation semantics and alternative principle sets to assess the generalizability of the findings.
3. Perform sensitivity analysis on the gradient-based contribution function to evaluate its robustness under varying conditions and potential numerical instabilities in non-smooth semantics.