---
ver: rpa2
title: Three Dogmas of Reinforcement Learning
arxiv_id: '2407.10583'
source_url: https://arxiv.org/abs/2407.10583
tags:
- learning
- agents
- reinforcement
- reward
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies three implicit assumptions, or "dogmas,"
  in modern reinforcement learning: (1) focusing on modeling environments rather than
  agents, (2) treating learning as finding a solution to a task, and (3) the reward
  hypothesis, which states that all goals can be well thought of as maximization of
  a reward signal. The authors argue that these dogmas have shaped the field but also
  limited its potential.'
---

# Three Dogmas of Reinforcement Learning

## Quick Facts
- arXiv ID: 2407.10583
- Source URL: https://arxiv.org/abs/2407.10583
- Authors: David Abel; Mark K. Ho; Anna Harutyunyan
- Reference count: 23
- Key outcome: Identifies three implicit assumptions shaping modern RL and proposes alternative research directions

## Executive Summary
This paper identifies three implicit assumptions, or "dogmas," that have shaped modern reinforcement learning: focusing on modeling environments rather than agents, treating learning as finding solutions to tasks, and the reward hypothesis that all goals can be expressed as reward maximization. The authors argue that these dogmas have both enabled progress and created blind spots that limit the field's potential. They propose alternative approaches including agent-centric modeling, learning as continual adaptation, and recognizing the limitations of reward-based goal specification.

## Method Summary
The paper presents a conceptual framework through theoretical analysis rather than experimental work. It systematically identifies and critiques three dogmas in reinforcement learning, then proposes alternative research directions. The authors draw on existing literature to support their critique, particularly work on the limitations of the reward hypothesis and the need for better agent models. No specific datasets, experiments, or quantitative metrics are provided.

## Key Results
- Three dogmas identified: environment focus, solution-finding mindset, and reward hypothesis overapplication
- Proposed shift toward agent-centric modeling as a complement to environment modeling
- Recognition that many real-world goals cannot be properly captured by reward functions
- Framework for future research directions in RL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Identifying and questioning implicit assumptions in a field can catalyze paradigm shifts by opening new research directions.
- Mechanism: By explicitly naming the three "dogmas," the authors create a cognitive lever that allows researchers to reconsider foundational commitments without discarding the entire framework.
- Core assumption: The field has developed around these implicit assumptions without explicit examination, and bringing them to the surface enables conscious choice about their continued relevance.
- Evidence anchors: Abstract statement about dogmas shaping RL science; section on subtle departures from implicit assumptions.

### Mechanism 2
- Claim: Shifting emphasis from environment-centric to agent-centric modeling can unlock new theoretical frameworks and research questions.
- Mechanism: The authors argue that the dominance of MDP-based environmental modeling has created a blind spot around agent modeling, limiting the field's ability to develop general principles of agency.
- Core assumption: A canonical mathematical model of agents is both possible and valuable for advancing the field.
- Evidence anchors: Discussion of lacking canonical agent models and struggle to define agent families.

### Mechanism 3
- Claim: Recognizing the limitations of reward-based goal specification can lead to more expressive and appropriate objective formulations.
- Mechanism: By fully characterizing the conditions under which the reward hypothesis holds (via von Neumann-Morgenstern axioms), the authors reveal that many real-world goals cannot be properly captured by reward functions.
- Core assumption: The reward hypothesis, while useful, has been over-applied and its limitations should be explicitly acknowledged.
- Evidence anchors: Reference to Bowling et al.'s analysis of reward hypothesis conditions and characterization of goal encoding limitations.

## Foundational Learning

- Concept: Paradigms and scientific revolutions
  - Why needed here: The paper explicitly references Kuhn's framework to position its critique within the context of paradigm shifts in science
  - Quick check question: What distinguishes "normal science" from "revolutionary" phases according to Kuhn?

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: MDPs are presented as the canonical environment model that has shaped RL thinking, and understanding their structure is crucial for grasping what's being challenged
  - Quick check question: What are the key assumptions embedded in the MDP formalism that the authors suggest have become limiting?

- Concept: von Neumann-Morgenstern utility axioms
  - Why needed here: The paper uses these axioms to characterize when the reward hypothesis is valid, so understanding them is essential for evaluating the critique
  - Quick check question: What are the five axioms that must be satisfied for a preference relation to be representable by a reward function?

## Architecture Onboarding

- Component map: Three dogmas critique -> Alternative proposals -> Supporting evidence -> Future directions

- Critical path: Understand current RL paradigm -> Identify dogma limitations -> Propose alternative framings -> Connect to existing literature

- Design tradeoffs: Abstraction vs. specificity (conceptual rather than algorithmic); critique vs. construction (balanced); field history vs. future vision (anchored in development)

- Failure signatures: Misinterpretation as dismissing all of RL; vague alternatives seeming impractical; unclear connection to existing work

- First 3 experiments:
  1. Develop a canonical agent model that captures key properties across different agent types
  2. Design a learning framework that evaluates progress without reference to optimal solutions
  3. Create a testbed for comparing reward-based vs. alternative goal specification methods on tasks with incommensurable objectives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the canonical mathematical model of an agent in reinforcement learning, and what are its key components?
- Basis in paper: [explicit] The authors argue that while environments have canonical models like MDPs, there is no clear canonical model of an agent. They suggest that defining such a model is crucial for advancing the field.
- Why unresolved: The paper does not provide a specific model but highlights the lack of one and the need for its development. Different researchers have proposed various agent models, but none have been universally accepted.
- What evidence would resolve it: A formal mathematical framework that captures the essential properties of agents across different domains, with clear axioms and theorems, would provide a canonical model.

### Open Question 2
- Question: How can we define and measure learning progress in agents that engage in continual adaptation rather than finding a solution?
- Basis in paper: [explicit] The authors propose shifting focus from agents that find solutions to those that continually adapt and improve. They question how to evaluate such agents without relying on optimality or task-specific benchmarks.
- Why unresolved: Current evaluation methods in RL are designed for agents that converge to optimal policies. There is no established framework for assessing agents that are designed to keep learning and adapting indefinitely.
- What evidence would resolve it: New metrics and benchmarks that capture the ability of agents to adapt to changing environments, transfer knowledge across tasks, and improve over time without a fixed endpoint.

### Open Question 3
- Question: What are the alternative languages for describing an agent's goals beyond reward maximization, and how can they be integrated into reinforcement learning frameworks?
- Basis in paper: [explicit] The authors discuss the limitations of the reward hypothesis and suggest exploring other ways to specify goals, such as preferences, logical languages, or open-ended objectives.
- Why unresolved: While there are some proposals for alternative goal-specification methods, they are not yet widely adopted or integrated into mainstream RL frameworks. The field lacks a unified approach to handling diverse goal types.
- What evidence would resolve it: Successful integration of alternative goal-specification methods into RL algorithms, with empirical demonstrations of their effectiveness in complex, real-world scenarios. This would involve developing new algorithms, benchmarks, and evaluation criteria.

## Limitations
- Conceptual framework without empirical validation of proposed alternatives
- Theoretical analysis of reward hypothesis limitations lacks practical demonstration
- Proposed agent-centric modeling approach remains speculative without implementation details

## Confidence
- High confidence: Identification and critique of the three dogmas as implicit assumptions shaping RL research
- Medium confidence: Theoretical analysis of reward hypothesis limitations under von Neumann-Morgenstern axioms
- Low confidence: Practical utility and implementation details of proposed agent-centric modeling approach

## Next Checks
1. Implement a minimal canonical agent model and test whether it captures meaningful distinctions between agent families across multiple benchmark tasks
2. Design a benchmark suite for evaluating learning progress without reference to optimal solutions, testing whether alternative metrics better capture real-world learning scenarios
3. Create controlled experiments comparing reward-based vs. alternative goal specification methods on tasks with known incommensurable objectives to quantify performance differences