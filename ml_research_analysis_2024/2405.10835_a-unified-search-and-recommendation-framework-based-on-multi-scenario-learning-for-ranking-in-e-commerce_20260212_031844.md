---
ver: rpa2
title: A Unified Search and Recommendation Framework Based on Multi-Scenario Learning
  for Ranking in E-commerce
arxiv_id: '2405.10835'
source_url: https://arxiv.org/abs/2405.10835
tags:
- learning
- user
- search
- views
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of joint modeling search and
  recommendation (S&R) in e-commerce, where traditional multi-scenario models fail
  to effectively capture the differences between S&R scenarios and do not fully exploit
  global label information. To overcome these limitations, the authors propose a Unified
  Search and Recommendation (USR) framework based on multi-scenario learning.
---

# A Unified Search and Recommendation Framework Based on Multi-Scenario Learning for Ranking in E-commerce

## Quick Facts
- arXiv ID: 2405.10835
- Source URL: https://arxiv.org/abs/2405.10835
- Reference count: 34
- One-line primary result: Proposed USR framework improves search and recommendation performance by capturing scenario differences through fine-grained modeling

## Executive Summary
This paper addresses the challenge of joint modeling search and recommendation (S&R) in e-commerce, where traditional multi-scenario models fail to effectively capture the differences between S&R scenarios and do not fully exploit global label information. To overcome these limitations, the authors propose a Unified Search and Recommendation (USR) framework based on multi-scenario learning. The USR framework includes three key components: S&R Views User Interest Extractor Layer (IE) and S&R Views Feature Generator Layer (FG) to generate scenario-specific user interests and feature representations, and a Global Label Space Multi-Task Layer (GLMT) that jointly models the main task and auxiliary tasks using global labels. Extensive offline experiments demonstrate that USR significantly improves the performance of various multi-scenario models, and online A/B testing shows substantial performance gains across multiple metrics. The proposed USR framework has been successfully deployed in the 7Fresh App.

## Method Summary
The paper proposes a Unified Search and Recommendation (USR) framework to jointly model search and recommendation scenarios in e-commerce. USR uses three key components: (1) S&R Views User Interest Extractor Layer (IE) that extracts scenario-specific user interests through adaptive parameter generation, (2) S&R Views Feature Generator Layer (FG) that scales domain-agnostic features differently for each scenario, and (3) Global Label Space Multi-Task Layer (GLMT) that leverages global label information as auxiliary supervision. The framework is trained with contrastive learning to better capture differences between search and recommendation scenarios, and has been deployed in the 7Fresh App with demonstrated performance improvements.

## Key Results
- USR framework significantly improves AUC for CTR and CTCVR tasks in both search and recommendation scenarios
- Online A/B testing shows substantial improvements in UCVR and UCTR metrics after deployment in 7Fresh App
- USR outperforms traditional multi-scenario models including SharedBottom, MMoE, PLE, STAR, M2M, and APG across various evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: USR improves S&R performance by extracting fine-grained user interests and scenario-agnostic feature representations for each scenario view.
- Mechanism: The S&R Views User Interest Extractor Layer (IE) and S&R Views Feature Generator Layer (FG) use a scenario trigger to adaptively generate scenario-specific parameters and scale features, capturing scenario differences.
- Core assumption: User interests and feature importance differ meaningfully between search and recommendation scenarios.
- Evidence anchors:
  - [abstract] "designed with S&R Views User Interest Extractor Layer (IE) and S&R Views Feature Generator Layer (FG) to separately generate user interests and scenario-agnostic feature representations for S&R."
  - [section 3.2] "we propose a layer to extract user interests for S&R views based on the re-parameterization method to adaptively generate parameters depending on the given condition."
  - [corpus] Weak or missing. No direct corpus evidence found for IE/FG specific mechanism.
- Break condition: If user interests and feature importance are actually similar across S&R, the scenario-specific extraction becomes redundant and may hurt performance.

### Mechanism 2
- Claim: USR improves performance by leveraging global label information through auxiliary tasks.
- Mechanism: The Global Label Space Multi-Task Layer (GLMT) uses global labels as supervised signals for auxiliary tasks and jointly models main and auxiliary tasks using conditional probability.
- Core assumption: Global label information (union of S&R labels) contains useful information for improving individual S&R task performance.
- Evidence anchors:
  - [abstract] "we introduce a Global Label Space Multi-Task Layer (GLMT) that uses global labels as supervised signals of auxiliary tasks and jointly models the main task and auxiliary tasks using conditional probability."
  - [section 3.4] "we propose a novel method that jointly models the main task and auxiliary tasks using conditional probability... The main task and auxiliary tasks are scenario-CTR/CTCVR prediction and global-CTR/CTCVR prediction, respectively."
  - [corpus] Weak or missing. No direct corpus evidence found for GLMT specific mechanism.
- Break condition: If global label information is noisy or uninformative, using it as auxiliary supervision could degrade model performance.

### Mechanism 3
- Claim: USR improves performance by using contrastive learning to better capture differences between S&R scenarios.
- Mechanism: Contrastive loss is applied to both user interest representations and scenario-agnostic feature representations to push representations from different scenarios apart while pulling similar ones together.
- Core assumption: S&R scenarios have distinct characteristics that can be better learned through contrastive learning.
- Evidence anchors:
  - [section 3.2] "we draw inspiration from classic contrastive learning... and design a novel contrastive loss... which is formulated as follows..."
  - [section 3.3] "Just as we learned the representations for ùíäùëü and ùíäùë†, we also apply the contrastive loss to ùíá ùëü and ùíá ùë†, which is formulated as follows..."
  - [corpus] Weak or missing. No direct corpus evidence found for contrastive learning specific mechanism.
- Break condition: If S&R scenarios are not sufficiently distinct, contrastive learning may over-separate representations and hurt transfer learning benefits.

## Foundational Learning

- Concept: Multi-Scenario Learning (MSL)
  - Why needed here: The paper addresses the challenge of joint modeling search and recommendation scenarios, which requires learning from multiple related but distinct tasks.
  - Quick check question: What is the difference between multi-task learning and multi-scenario learning in the context of recommendation systems?

- Concept: Contrastive Learning
  - Why needed here: The paper uses contrastive learning to better capture differences between S&R scenarios by pushing representations from different scenarios apart.
  - Quick check question: How does contrastive learning differ from traditional supervised learning in terms of learning objectives?

- Concept: Conditional Probability Modeling
  - Why needed here: The GLMT layer uses conditional probability to jointly model main and auxiliary tasks, leveraging global label information.
  - Quick check question: What is the relationship between conditional probability and multi-task learning in the context of recommendation systems?

## Architecture Onboarding

- Component map: Input Network -> S&R Views User Interest Extractor Layer (IE) -> S&R Views Feature Generator Layer (FG) -> Global Label Space Multi-Task Layer (GLMT) -> Output
- Critical path: Input Network ‚Üí IE ‚Üí FG ‚Üí GLMT ‚Üí Output
- Design tradeoffs:
  - Fine-grained vs. coarse-grained modeling: USR uses fine-grained modeling to capture scenario differences, while traditional MSL uses coarse-grained shared parameters
  - Global label utilization: USR leverages global label information as auxiliary supervision, while traditional MSL may not use it effectively
  - Contrastive learning: USR applies contrastive learning to better capture scenario differences, which may increase computational complexity

- Failure signatures:
  - Poor performance on individual S&R tasks: May indicate issues with scenario-specific extraction or global label utilization
  - Overfitting: May indicate that the model is too complex for the available data
  - Slow convergence: May indicate issues with the training process or hyperparameters

- First 3 experiments:
  1. Ablation study: Remove each key component (IE, FG, GLMT) and measure performance impact
  2. Hyperparameter tuning: Vary the dimension of embeddings (ùëë) and measure performance impact
  3. Contrastive learning analysis: Visualize learned representations with and without contrastive learning to assess its effectiveness

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implications arise from the work:

- The framework's effectiveness for scenarios beyond search and recommendation (e.g., social media feeds, news recommendations) remains unexplored
- The impact of varying the temperature coefficient (œÑ) in the contrastive loss on model performance is not analyzed
- The framework's handling of cold-start problems with limited user interaction data is not addressed

## Limitations

- Limited ablation studies to isolate the individual contribution of each proposed component (IE, FG, GLMT) to overall performance gains
- No comparison with more recent unified S&R approaches that may have emerged after the 2024 submission deadline
- Potential overfitting concerns given the model's complexity with multiple specialized components

## Confidence

- High confidence in the framework's ability to improve overall S&R performance based on both offline and online experiments
- Medium confidence in the specific mechanisms (IE, FG, GLMT) driving the improvements due to limited ablation evidence
- Medium confidence in the generalization of results to other e-commerce platforms beyond 7Fresh

## Next Checks

1. Conduct comprehensive ablation studies by systematically removing each key component (IE, FG, GLMT) and retraining to measure individual contributions to performance gains

2. Implement a simplified baseline that only applies global label supervision without the specialized IE/FG components to isolate the impact of global label utilization

3. Perform stress testing with synthetic data to evaluate model robustness when S&R scenarios have varying degrees of similarity and when global label quality degrades