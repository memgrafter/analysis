---
ver: rpa2
title: Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation
  and Self-Check
arxiv_id: '2403.18243'
source_url: https://arxiv.org/abs/2403.18243
tags:
- question
- conversational
- which
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ConvRAG, a conversation-level retrieval-augmented
  generation approach for conversational question answering. The method addresses
  the challenge of integrating large language models with retrieval augmentation in
  conversational settings where questions are interdependent on context.
---

# Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check

## Quick Facts
- arXiv ID: 2403.18243
- Source URL: https://arxiv.org/abs/2403.18243
- Authors: Linhao Ye; Zhikai Lei; Jianghao Yin; Qin Chen; Jie Zhou; Liang He
- Reference count: 40
- Primary result: Introduces ConvRAG, a conversation-level retrieval-augmented generation approach for conversational question answering

## Executive Summary
This paper introduces ConvRAG, a novel approach for conversational question answering that addresses the challenge of integrating large language models with retrieval augmentation in conversational settings where questions are interdependent on context. The method consists of three components working collaboratively: a conversational question refiner that reformulates questions and extracts keywords, a fine-grained retriever that obtains relevant information from the web, and a self-check based response generator that filters unhelpful information. The approach is evaluated on both English (QReCC, TOPIOCQA) and Chinese datasets, showing improved performance over baseline methods.

## Method Summary
ConvRAG is a conversation-level retrieval-augmented generation approach for conversational question answering that addresses the challenge of integrating LLMs with retrieval augmentation in conversational settings. The method consists of three components: a conversational question refiner that reformulates questions and extracts keywords, a fine-grained retriever that obtains relevant information from the web through document-level search, paragraph-level recall, and paragraph-level reranking, and a self-check based response generator that filters unhelpful retrieved information before generation. The approach uses Baichuan 2-7B LLM for multiple components and is trained with batch size 16, learning rate 2e-4, and cosine decay with 200 warm-up steps.

## Key Results
- ConvRAG achieves improved performance on conversational question answering tasks compared to baseline methods
- The approach demonstrates effectiveness on both English (QReCC, TOPIOCQA) and Chinese datasets
- Performance is evaluated using BLEU, METEOR, ROUGE scores and GPT-4 pairwise comparison

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conversational question refiner improves question understanding by reformulating questions and extracting keywords based on conversation context.
- Mechanism: Reformulates the original question into a more explicit form by incorporating context, then extracts core keywords from both the reformulated question and context to focus retrieval.
- Core assumption: Anaphora and ellipsis in conversational questions can be resolved through context-aware reformulation and keyword extraction, leading to more effective retrieval.
- Evidence anchors:
  - [abstract] "In particular, our approach consists of three components, namely conversational question refiner, fine-grained retriever and self-check based response generator, which work collaboratively for question understanding and relevant information acquisition in conversational settings."
  - [section 4.1] "we need to refine the question to make the questioner's intent more explicit... To obtain the reformulation, we initialize a pre-trained LLM as M_Q, and fine-tune it for new question generation"
  - [corpus] Weak - no direct citations or strong anchor for the refiner's effectiveness
- Break condition: If reformulation doesn't capture the true intent or keywords miss critical context elements, retrieval will return irrelevant passages and generation quality will degrade.

### Mechanism 2
- Claim: Fine-grained retriever obtains more relevant information by combining document-level search with paragraph-level recall and reranking.
- Mechanism: First retrieves relevant web documents using keywords as query, then decomposes documents into paragraphs and scores them based on relevance to reformulated question and keywords using embeddings, finally reranks top paragraphs using a specialized relevance scoring model.
- Core assumption: The combination of document search with paragraph-level filtering and reranking can identify the most relevant information from the web for conversational questions.
- Evidence anchors:
  - [section 4.2] "Specifically, the retriever includes three stages. Document-Level Retrieval... Paragraph-Level Recall... Paragraph-Level Rerank"
  - [section 4.2] "Unlike conventional embedding models, the reranker uses the reformulated question and paragraph as input and directly outputs the relevance score instead of embeddings, which shows great potential in retrieval."
  - [corpus] Weak - no direct citations or strong anchor for the effectiveness of this fine-grained approach
- Break condition: If paragraph relevance scoring is inaccurate or documents retrieved don't contain relevant information, the retriever will fail to provide useful evidence for generation.

### Mechanism 3
- Claim: Self-check based response generator improves accuracy by filtering unhelpful retrieved information before generation.
- Mechanism: Trains a model to assess the helpfulness of each retrieved paragraph and generates a description of which paragraphs are useful, then uses this assessment along with the retrieved information and model's own knowledge to generate the final response.
- Core assumption: LLMs can be trained to accurately judge the helpfulness of retrieved information and filter out noise that would otherwise mislead the generation process.
- Evidence anchors:
  - [abstract] "In case some noise or irrelevant information still exists in the retrieved results, we present a self-check mechanism, which urges LLMs to rethink the helpfulness of the retrieved information, and selectively utilize the helpful information for more accurate response generation."
  - [section 4.3] "To address this issue, we present a self-check mechanism, which urges LLMs to rethink the helpfulness of the retrieved information and filter the irrelevant noise for response generation."
  - [corpus] Weak - no direct citations or strong anchor for the effectiveness of this self-check mechanism
- Break condition: If the self-check model incorrectly judges paragraph helpfulness or the generator doesn't properly utilize the filtered information, generation quality will suffer from either noise or missing relevant context.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG is the foundational framework that combines retrieval of external information with generation to improve accuracy and reduce hallucination in LLMs, which is essential for open-domain CQA.
  - Quick check question: How does RAG differ from standard LLM generation in handling knowledge-intensive questions?

- Concept: Conversational Question Answering (CQA)
  - Why needed here: CQA involves questions that depend on conversation context through anaphora and ellipsis, requiring specialized handling beyond single-turn QA, which is the core problem this work addresses.
  - Quick check question: What makes conversational questions more challenging than single-turn questions in terms of question understanding?

- Concept: Question Reformulation and Keyword Extraction
  - Why needed here: These techniques resolve the core challenge of understanding conversational questions by making implicit references explicit and focusing retrieval on essential concepts, which are key components of the proposed approach.
  - Quick check question: Why is question reformulation particularly important for conversational settings compared to single-turn QA?

## Architecture Onboarding

- Component map: Conversational Question Refiner -> Fine-Grained Retriever -> Self-Check based Response Generator
- Critical path: Question reformulation → Keyword extraction → Document retrieval → Paragraph recall → Paragraph reranking → Helpfulness checking → Response generation
- Design tradeoffs: The system trades increased complexity (three-stage retrieval, additional checking step) for improved accuracy and reduced hallucination, while relying on LLMs for multiple components which may impact efficiency.
- Failure signatures: Poor reformulation leads to irrelevant retrieval; weak paragraph scoring results in missing relevant information; inaccurate helpfulness checking introduces noise or removes useful context; overall system fails when any component significantly degrades.
- First 3 experiments:
  1. Test question reformulation quality by comparing reformulated questions to human-rewritten versions on a validation set
  2. Evaluate paragraph retrieval effectiveness by measuring recall of relevant paragraphs for reformulated questions
  3. Assess self-check accuracy by comparing model's helpfulness judgments to human annotations on retrieved paragraphs

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- The effectiveness of the conversational question refiner is asserted but not validated against human-annotated ground truth for reformulation quality
- The paper lacks ablation studies showing the individual contributions of each component
- Claims about specific mechanisms rely primarily on downstream task performance rather than direct evaluation of component effectiveness

## Confidence
- **High Confidence**: The general framework of combining conversational question understanding with retrieval augmentation is well-established in the literature, and the paper correctly identifies the core challenges of CQA (anaphora resolution, ellipsis handling, context dependency).
- **Medium Confidence**: The three-component architecture (refiner, retriever, generator) is logically sound and the implementation details are sufficiently specified for reproduction, though the effectiveness of individual components remains uncertain.
- **Low Confidence**: Claims about the specific mechanisms (reformulation quality, fine-grained retrieval effectiveness, self-check accuracy) lack direct empirical validation and rely primarily on downstream task performance as evidence.

## Next Checks
1. **Reformulation Quality Validation**: Conduct a human evaluation study where annotators rate the quality of reformulated questions against original questions, measuring accuracy in capturing intent and completeness of context incorporation.
2. **Component Ablation Study**: Systematically remove each component (refiner, fine-grained retriever stages, self-check) to quantify their individual contributions to overall performance, providing clearer evidence for which mechanisms drive improvements.
3. **Retrieval-Specific Metrics Evaluation**: Measure traditional retrieval metrics (recall@k, mean average precision, nDCG) for the fine-grained retriever to directly assess its effectiveness in finding relevant information, rather than inferring this from generation quality alone.