---
ver: rpa2
title: 'Downstream Transfer Attack: Adversarial Attacks on Downstream Models with
  Pre-trained Vision Transformers'
arxiv_id: '2408.01705'
source_url: https://arxiv.org/abs/2408.01705
tags:
- downstream
- attack
- adversarial
- pre-trained
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of generating transferable adversarial
  attacks from pre-trained vision transformers (ViTs) to downstream models fine-tuned
  on different datasets. The authors propose a new method called Downstream Transfer
  Attack (DTA) that leverages the pre-trained model to craft adversarial examples
  by minimizing the average cosine similarity between clean and adversarial tokens
  at the most vulnerable layers.
---

# Downstream Transfer Attack: Adversarial Attacks on Downstream Models with Pre-trained Vision Transformers

## Quick Facts
- arXiv ID: 2408.01705
- Source URL: https://arxiv.org/abs/2408.01705
- Reference count: 20
- Primary result: New method DTA achieves >90% attack success rate against fine-tuned vision models across diverse settings

## Executive Summary
This paper introduces Downstream Transfer Attack (DTA), a novel adversarial attack method that exploits pre-trained Vision Transformers (ViTs) to craft adversarial examples transferable to downstream models fine-tuned on different datasets. The key insight is leveraging the pre-trained model's attention mechanisms and token representations to identify vulnerable layers and minimize the cosine similarity between clean and adversarial tokens. Through extensive experiments across multiple pre-training methods, fine-tuning schemes, and 10 downstream datasets, DTA demonstrates unprecedented attack success rates exceeding 90%, significantly outperforming existing transferability attack methods.

## Method Summary
DTA operates by first identifying the most vulnerable layers in a pre-trained ViT through analysis of token cosine similarity changes. The attack then crafts adversarial examples by minimizing the average cosine similarity between clean and adversarial tokens at these critical layers while preserving the semantic content of the image. This is achieved through an iterative optimization process that leverages the attention weights and token representations from the pre-trained model. The method is designed to be agnostic to the specific downstream fine-tuning approach, whether full fine-tuning or parameter-efficient methods like LoRA, making it broadly applicable across different transfer learning scenarios.

## Key Results
- DTA achieves average attack success rates exceeding 90% across various pre-training methods, fine-tuning schemes, and 10 downstream datasets
- Outperforms existing transferable attack methods by significant margins in all tested scenarios
- Demonstrates that parameter-efficient transfer learning methods like LoRA increase vulnerability to such attacks

## Why This Works (Mechanism)
The effectiveness of DTA stems from exploiting the rich hierarchical representations and attention mechanisms learned during pre-training. By targeting token similarity at vulnerable layers, the attack creates perturbations that remain effective even after the downstream model has been fine-tuned on different data. The pre-trained model serves as a universal feature extractor that captures fundamental visual patterns, making the crafted adversarial examples transferable across diverse fine-tuning scenarios. The method's success is particularly pronounced against parameter-efficient fine-tuning approaches, suggesting these methods retain more of the pre-trained model's vulnerability patterns.

## Foundational Learning

**Vision Transformer (ViT) Architecture**
- *Why needed*: Understanding ViT's self-attention mechanism and token-based processing is crucial for implementing DTA
- *Quick check*: Can you explain how ViT differs from CNN in processing image patches?

**Transfer Learning in Computer Vision**
- *Why needed*: DTA specifically targets models fine-tuned from pre-trained weights
- *Quick check*: What are the key differences between full fine-tuning and parameter-efficient methods like LoRA?

**Adversarial Attacks and Transferability**
- *Why needed*: Understanding how adversarial examples can transfer between models is fundamental to DTA's approach
- *Quick check*: What makes an adversarial example transferable between different models?

## Architecture Onboarding

**Component Map**
Pre-trained ViT -> Layer Vulnerability Analysis -> Token Similarity Optimization -> Adversarial Example Generation -> Downstream Model

**Critical Path**
The attack's effectiveness depends on accurately identifying vulnerable layers and optimizing token similarity while maintaining image quality. The pre-trained model serves as the attack source, with the optimization process being the computational bottleneck.

**Design Tradeoffs**
The method trades computational cost during attack generation for higher success rates. Using parameter-efficient fine-tuning methods as targets provides higher attack success but may not represent all real-world deployment scenarios.

**Failure Signatures**
Attacks fail when downstream models have been trained with strong adversarial defenses or when the pre-trained model's features are substantially altered during fine-tuning. Success rates drop significantly for models with architectural differences beyond standard ViT variants.

**First Experiments**
1. Test DTA transferability from a pre-trained ViT to a fully fine-tuned ResNet on CIFAR-10
2. Evaluate attack success rate when using different pre-trained model initializations (e.g., ImageNet-21k vs ImageNet-1k)
3. Measure the impact of varying the number of optimization iterations on attack success rate

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Evaluation relies on specific pre-trained models and fine-tuning protocols, limiting generalizability to truly unseen architectures
- Focuses on white-box attacks using known pre-trained models, not addressing realistic black-box scenarios
- Only evaluates image classification tasks, not extending to object detection or segmentation where transfer learning is also common

## Confidence

- **DTA achieves >90% attack success rate across diverse settings**: Medium confidence - supported by extensive experiments but limited to specific model families and tasks
- **Parameter-efficient methods like LoRA increase vulnerability**: Medium confidence - demonstrated empirically but requires more theoretical justification
- **Token similarity minimization is effective**: High confidence - consistent results across multiple datasets and pre-training methods

## Next Checks

1. Test DTA transferability on completely unseen ViT architectures not included in the pre-trained model pool, such as hybrid CNN-ViT models or vision transformers with different attention mechanisms
2. Evaluate the attack's effectiveness in black-box settings where the source pre-trained model is unknown to the attacker
3. Assess DTA performance on downstream vision tasks beyond classification, particularly object detection and semantic segmentation models fine-tuned from the same pre-trained weights