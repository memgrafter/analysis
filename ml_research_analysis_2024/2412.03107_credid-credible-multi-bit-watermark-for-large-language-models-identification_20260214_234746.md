---
ver: rpa2
title: 'CredID: Credible Multi-Bit Watermark for Large Language Models Identification'
arxiv_id: '2412.03107'
source_url: https://arxiv.org/abs/2412.03107
tags:
- text
- watermark
- watermarking
- message
- seed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CredID, a multi-party credible watermarking
  framework for LLM identification that involves a trusted third party (TTP) and multiple
  LLM vendors. The framework addresses the challenge of credible watermarking by having
  the TTP assign unique identity identifiers to vendors and generate watermark seeds,
  while vendors use these seeds to embed watermarks during text generation.
---

# CredID: Credible Multi-Bit Watermark for Large Language Models Identification

## Quick Facts
- arXiv ID: 2412.03107
- Source URL: https://arxiv.org/abs/2412.03107
- Reference count: 26
- Primary result: CredID achieves over 95% success rate in watermark extraction across different models and datasets while maintaining high text quality with perplexity increases under 10%.

## Executive Summary
CredID introduces a multi-party credible watermarking framework for LLM identification that involves a trusted third party (TTP) and multiple LLM vendors. The framework addresses the challenge of credible watermarking by having the TTP assign unique identity identifiers to vendors and generate watermark seeds, while vendors use these seeds to embed watermarks during text generation. For extraction, multiple vendors collaboratively verify the watermark under TTP coordination. The paper also proposes a novel multi-bit watermarking algorithm that improves information capacity and text quality through holistic message encoding and spike entropy thresholds.

## Method Summary
The CredID framework implements a trusted third party (TTP) that coordinates watermarking across multiple LLM vendors. The TTP assigns unique identity identifiers to each vendor, generates watermark seeds using these identifiers, and coordinates vendors to verify watermarks collaboratively during extraction. The framework uses logit modification-based watermarking where the TTP sends watermark seeds to vendors who embed watermarks by adding watermark logits to the model's pure logits. The novel multi-bit watermarking algorithm encodes messages holistically rather than bit-by-bit and uses spike entropy thresholds to bypass low-entropy words while maintaining text quality. The extraction process involves the TTP sending the same seed table to multiple vendors and conducting a voting process based on their confidence levels to determine the text origin.

## Key Results
- Achieves over 95% success rate in watermark extraction across different models and datasets
- Maintains high text quality with perplexity increases under 10%
- Demonstrates robustness against various attacks while enabling accurate identification among multiple LLM vendors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CredID uses a trusted third party (TTP) to coordinate multi-party watermark extraction, which enhances credibility and prevents forgery.
- Mechanism: The TTP assigns unique identity identifiers to each vendor, generates watermark seeds using these identifiers, and coordinates vendors to verify watermarks collaboratively. During extraction, the TTP sends the same seed table to multiple vendors and uses their confidence levels to conduct a voting process.
- Core assumption: The TTP is truly trusted and cannot be compromised by any vendor.
- Evidence anchors:
  - [abstract]: "This framework ensures the credibility of the watermark without compromising the privacy of LLM vendors and users."
  - [section]: "Our CredID not only provides verifiable watermarks but also improves the accuracy of LLM identity recognition through a joint verification mechanism."
  - [corpus]: Weak evidence - the corpus mentions "multi-bit watermarking" but doesn't specifically address the TTP coordination mechanism.
- Break condition: If the TTP is compromised or colludes with a vendor, the entire credibility guarantee fails.

### Mechanism 2
- Claim: CredID's multi-bit watermarking algorithm improves information capacity and text quality through holistic message encoding and spike entropy thresholds.
- Mechanism: The algorithm encodes messages holistically rather than bit-by-bit, which reduces the impact of text perturbations. It also uses spike entropy thresholds to bypass low-entropy words and maintain text quality.
- Core assumption: The spike entropy threshold effectively identifies low-entropy words without affecting the watermark extraction.
- Evidence anchors:
  - [abstract]: "Furthermore, current watermarking algorithms struggle with text quality, information capacity, and robustness, making it challenging to meet the diverse identification needs of LLMs. Thus, we propose a novel multi-bit watermarking algorithm..."
  - [section]: "By encoding messages holistically, we significantly increase the information capacity of the watermark. To improve performance in low-entropy texts, we bypass low-entropy words..."
  - [corpus]: Weak evidence - the corpus mentions "multi-bit watermarking" but doesn't specifically address the holistic encoding or entropy threshold mechanisms.
- Break condition: If the entropy threshold is too high, it may remove too many words, degrading text quality. If too low, it may not effectively filter low-entropy words.

### Mechanism 3
- Claim: CredID's use of logit modification-based watermarking preserves text naturalness while embedding watermarks.
- Mechanism: The algorithm adds watermark logits to the pure logits of the next token generated by the LLM, ensuring that high model logits correspond to high watermark logits. This alignment guarantees that the logits, after adding the bias, remain equivalent to the original model logits.
- Core assumption: The logit modification preserves the statistical properties of the original text distribution.
- Evidence anchors:
  - [section]: "Our improvements to the multi-bit watermarking enhance information capacity through holistic message encoding and leverage the original model's logits and spike entropy thresholds to maintain high text quality."
  - [section]: "The aim of watermarking method is to design an optimal Pw that not only alters the token selection probabilities during LLM inference but also adheres to the Maximum A Posteriori (MAP) decoding principle."
  - [corpus]: Weak evidence - the corpus mentions "multi-bit watermarking" but doesn't specifically address the logit modification mechanism.
- Break condition: If the watermark bias is too strong, it may distort the text distribution and reduce text quality.

## Foundational Learning

- Concept: Trusted Third Party (TTP) in cryptographic protocols
  - Why needed here: The TTP ensures credibility and prevents forgery in the watermarking process by coordinating multi-party verification.
  - Quick check question: What properties must a TTP have to ensure the security of the watermarking protocol?

- Concept: Multi-bit watermarking vs. one-bit watermarking
  - Why needed here: Multi-bit watermarking allows for more complex identification information to be embedded, which is necessary for distinguishing between multiple LLM vendors.
  - Quick check question: How does the information capacity of multi-bit watermarking compare to one-bit watermarking?

- Concept: Logit modification in language model watermarking
  - Why needed here: Logit modification is the mechanism by which the watermark is embedded into the LLM's output without significantly affecting text quality.
  - Quick check question: How does logit modification affect the probability distribution of the next token in a language model?

## Architecture Onboarding

- Component map: TTP -> LLM Vendors -> Users -> TTP (extraction)
- Critical path:
  1. User requests text generation from LLM vendor
  2. Vendor requests watermark seed from TTP
  3. TTP generates and sends seed to vendor
  4. Vendor embeds watermark and generates text
  5. User receives watermarked text
  6. TTP receives text for extraction
  7. TTP coordinates vendors to verify watermark
  8. TTP determines text origin based on verification results

- Design tradeoffs:
  - Credibility vs. Privacy: The TTP must be trusted but should not access user prompts or model logits
  - Information Capacity vs. Text Quality: Higher capacity may reduce text quality
  - Robustness vs. Success Rate: More robust watermarks may have lower success rates

- Failure signatures:
  - TTP compromise: Watermark credibility is lost
  - Vendor collusion: Watermark extraction accuracy is reduced
  - Entropy threshold misconfiguration: Text quality is degraded
  - Logit modification too strong: Text naturalness is compromised

- First 3 experiments:
  1. Test watermark extraction success rate with different TTP configurations
  2. Evaluate text quality impact of different entropy thresholds
  3. Measure the effect of logit modification strength on watermark robustness

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but based on the analysis, several unresolved issues emerge from the framework's design and limitations.

## Limitations

- The credibility guarantees fundamentally depend on TTP trustworthiness without addressing compromise scenarios
- Multi-bit watermarking performance across diverse text domains beyond tested datasets is not rigorously validated
- Robustness claims against paraphrasing and rewriting attacks lack theoretical guarantees about attack resistance boundaries

## Confidence

**High Confidence**: The framework's basic architecture and the general approach to multi-party watermark extraction are well-founded. The empirical results showing >95% extraction success rate and <10% perplexity increase are methodologically sound and reproducible.

**Medium Confidence**: The specific mechanisms for holistic message encoding and spike entropy threshold selection are described but lack sufficient implementation details for full verification. The claim that these mechanisms significantly improve information capacity while maintaining text quality needs more rigorous validation across diverse text types.

**Low Confidence**: The security guarantees against TTP compromise and vendor collusion are asserted but not formally proven. The paper doesn't provide quantitative analysis of how much TTP or vendor compromise would degrade the system's credibility or accuracy.

## Next Checks

1. **TTP Compromise Analysis**: Conduct experiments where the TTP is partially compromised (e.g., controlled by one vendor) to measure how this affects watermark credibility and extraction accuracy. This would quantify the actual security margin provided by the multi-party verification mechanism.

2. **Cross-Domain Text Quality Evaluation**: Test the multi-bit watermarking algorithm on text domains not included in the original evaluation (e.g., code, medical text, legal documents) to verify that the spike entropy thresholds and holistic encoding maintain text quality across diverse content types.

3. **Attack Resistance Boundary Testing**: Systematically vary attack strengths for paraphrasing and rewriting attacks to map out the relationship between attack intensity and watermark extraction success rate, providing clearer boundaries for the claimed robustness guarantees.