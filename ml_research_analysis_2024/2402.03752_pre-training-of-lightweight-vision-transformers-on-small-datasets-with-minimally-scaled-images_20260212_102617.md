---
ver: rpa2
title: Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally
  Scaled Images
arxiv_id: '2402.03752'
source_url: https://arxiv.org/abs/2402.03752
tags:
- vision
- transformer
- layer
- image
- patches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether lightweight Vision Transformers (ViTs)
  can outperform Convolutional Neural Networks (CNNs) like ResNet on small datasets
  like CIFAR-10 and CIFAR-100, without significantly scaling up image resolutions.
  The authors propose pre-training ViTs using a Masked Auto-Encoder (MAE) with minimal
  image scaling.
---

# Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images

## Quick Facts
- arXiv ID: 2402.03752
- Source URL: https://arxiv.org/abs/2402.03752
- Authors: Jen Hong Tan
- Reference count: 0
- Lightweight ViTs achieve 96.41% accuracy on CIFAR-10 and 78.27% on CIFAR-100 using MAE pre-training

## Executive Summary
This paper investigates whether lightweight Vision Transformers can outperform CNNs on small datasets without requiring large image resolutions. The authors propose using Masked Auto-Encoder (MAE) pre-training with minimally scaled images for lightweight ViT models under 3.65 million parameters. Their experiments demonstrate that pre-trained ViTs achieve state-of-the-art performance among similar lightweight transformer-based architectures on CIFAR-10 and CIFAR-100 datasets, reaching 96.41% and 78.27% accuracy respectively.

## Method Summary
The paper proposes pre-training lightweight Vision Transformers using a Masked Auto-Encoder approach with minimal image scaling on small datasets. The methodology focuses on ViT models with under 3.65 million parameters and MAC counts below 0.27G, qualifying them as lightweight architectures. The MAE pre-training is applied directly to the original-scale images without additional convolutional layers or significant resolution increases. The pre-trained models are then fine-tuned on CIFAR-10 and CIFAR-100 datasets to evaluate their performance compared to other lightweight transformer-based architectures.

## Key Results
- Achieved 96.41% accuracy on CIFAR-10, outperforming other lightweight transformer-based architectures
- Reached 78.27% accuracy on CIFAR-100 with the same approach
- Demonstrated effectiveness of MAE pre-training on minimally scaled images for small datasets
- Validated the efficiency of lightweight ViTs in handling small datasets without convolutional layers

## Why This Works (Mechanism)
The effectiveness stems from the Masked Auto-Encoder's ability to learn robust visual representations from incomplete data, which is particularly beneficial when working with limited dataset sizes. By pre-training on minimally scaled images, the model learns to extract meaningful features without being overwhelmed by high-resolution details that could lead to overfitting on small datasets. The transformer architecture's inherent ability to capture long-range dependencies in images complements the MAE's feature learning, resulting in improved generalization on small datasets compared to traditional CNNs.

## Foundational Learning

**Vision Transformers (ViTs)**: Deep learning models that apply self-attention mechanisms to image patches rather than relying on convolutions. Needed for understanding the core architecture being optimized; quick check: can process images as sequences of patches with positional embeddings.

**Masked Auto-Encoders (MAE)**: Self-supervised learning approach where random patches of an image are masked and the model learns to reconstruct them. Needed for understanding the pre-training strategy; quick check: trains by predicting masked image patches from visible ones.

**Lightweight Model Constraints**: Models with under 3.65M parameters and MAC counts below 0.27G, making them suitable for resource-constrained applications. Needed for understanding the efficiency requirements; quick check: can be deployed on edge devices with limited computational resources.

**Small Dataset Challenges**: Limited training examples that can lead to overfitting and poor generalization. Needed for context on why specialized approaches are required; quick check: CIFAR-10 has only 50,000 training images.

## Architecture Onboarding

**Component Map**: Image -> Patch Extraction -> Positional Embeddings -> Transformer Encoder -> MAE Pre-training -> Fine-tuning on Target Dataset

**Critical Path**: The sequence from patch extraction through transformer layers to the final classification head, where MAE pre-training enhances the feature learning capabilities of the transformer encoder.

**Design Tradeoffs**: The paper trades potential accuracy gains from larger models and higher resolutions for efficiency and practical deployment on resource-constrained devices. This limits absolute performance but enables broader applicability.

**Failure Signatures**: Poor performance would likely manifest as overfitting to training data, inability to generalize to test sets, or failure to converge during pre-training due to insufficient data diversity.

**Three First Experiments**:
1. Test the pre-trained ViT on CIFAR-10 without any fine-tuning to evaluate the quality of learned representations
2. Compare the performance of MAE pre-training versus random initialization on the same lightweight architecture
3. Evaluate the model's performance on a held-out validation set during pre-training to monitor overfitting

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to CIFAR-10 and CIFAR-100 datasets without testing on more diverse or larger-scale datasets
- Comparison restricted to similar parameter-count models without exploring alternative pre-training strategies
- Lack of ablation studies to isolate the contribution of MAE pre-training versus other architectural choices

## Confidence

**Performance Claims**: Medium confidence - The specific accuracy metrics are provided but the narrow dataset selection and limited comparisons create uncertainty about general applicability.

**Efficiency Claims**: Medium confidence - The parameter and MAC counts support the lightweight designation, but real-world computational benefits remain unverified.

**State-of-the-art Claims**: Medium confidence - Based on comparisons with a limited set of similar models and specific architectural constraints.

## Next Checks

1. Test the proposed approach on additional small to medium-sized datasets beyond CIFAR to assess generalization across different image domains and characteristics.

2. Conduct ablation studies to isolate the contribution of MAE pre-training versus other architectural choices in achieving the reported performance gains.

3. Evaluate the computational efficiency and accuracy trade-offs on resource-constrained devices to validate the practical benefits of the lightweight design for edge deployment scenarios.