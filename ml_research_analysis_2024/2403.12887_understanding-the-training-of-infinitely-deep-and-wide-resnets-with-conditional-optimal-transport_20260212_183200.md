---
ver: rpa2
title: Understanding the training of infinitely deep and wide ResNets with Conditional
  Optimal Transport
arxiv_id: '2403.12887'
source_url: https://arxiv.org/abs/2403.12887
tags:
- gradient
- flow
- then
- have
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the training dynamics of infinitely deep and
  arbitrarily wide residual neural networks (ResNets) through a mean-field approach.
  The key innovation is modeling the infinitely deep ResNet as a Neural Ordinary Differential
  Equation (NODE) parameterized by probability measures over layers and parameters,
  with a conditional Optimal Transport (COT) metric enforcing layer-wise structure.
---

# Understanding the training of infinitely deep and wide ResNets with Conditional Optimal Transport

## Quick Facts
- arXiv ID: 2403.12887
- Source URL: https://arxiv.org/abs/2403.12887
- Authors: Raphaël Barboni; Gabriel Peyré; François-Xavier Vialard
- Reference count: 14
- Key outcome: First rigorous convergence proof for infinitely deep and arbitrarily wide ResNets with unregularized risk, using Conditional Optimal Transport metric and local Polyak-Łojasiewicz analysis

## Executive Summary
This work provides a rigorous theoretical framework for understanding the training dynamics of infinitely deep and arbitrarily wide Residual Neural Networks (ResNets) through a mean-field approach. The key innovation is modeling the infinitely deep ResNet as a Neural Ordinary Differential Equation (NODE) parameterized by probability measures over layers and parameters, with a conditional Optimal Transport (COT) metric enforcing layer-wise structure. This COT metric more accurately models practical layer-wise training of ResNets compared to standard Wasserstein metrics. The authors establish well-posedness of gradient flow equations, prove equivalence with curves of maximal slope, and demonstrate convergence to global minima for well-chosen initializations using a local Polyak-Łojasiewicz analysis.

## Method Summary
The method models infinitely deep ResNets as NODEs parameterized by probability measures over layers and parameters, equipped with a Conditional Optimal Transport (COT) metric that enforces layer-wise structure. The gradient flow is defined through adjoint sensitivity analysis and shown to be equivalent to curves of maximal slope in metric spaces. Convergence is established through a local Polyak-Łojasiewicz (P-Ł) analysis, showing that for well-chosen initializations with sufficiently many features and low initial risk, the gradient flow converges to a global minimizer of the training risk. The analysis leverages kernel matrix conditioning associated with the feature distribution and provides explicit convergence conditions for Single Hidden Layer (SHL) residuals.

## Key Results
- First rigorous convergence proof for infinitely deep and arbitrarily wide ResNets with unregularized risk
- Definition and characterization of absolutely continuous curves in COT metric space
- Equivalence between adjoint gradient flow and curves of maximal slope
- Existence, uniqueness, and stability of gradient flow curves for compactly supported initializations
- Local P-Ł analysis proving convergence for well-chosen initializations with sufficient features and low initial risk

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Conditional Optimal Transport (COT) metric enforces layer-wise structure during training, preventing gradient overlap and ensuring stable optimization in infinitely deep ResNets.
- Mechanism: COT restricts transport plans to be identity on the first variable (layer index), modeling the practical layer-wise L2 metric used in automatic differentiation. This structure ensures that gradients are computed independently per layer, avoiding interference between layers that would occur with standard Wasserstein metrics.
- Core assumption: The marginal constraint on [0,1] (layer index) accurately represents how gradients flow in practical ResNet implementations using automatic differentiation.
- Evidence anchors:
  - [abstract] "modeling the practical layer-wise training of ResNets more accurately than standard Wasserstein metrics"
  - [section 2.1] "it is natural to define a 'layer-wise-L2' Wasserstein distance, that is an L2-distance over the set of families of probability measures in P2(Ω) indexed over s ∈ [0, 1]"
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If the marginal constraint does not accurately represent practical gradient computation, or if automatic differentiation implementations deviate significantly from this model.

### Mechanism 2
- Claim: The local Polyak-Łojasiewicz (P-Ł) analysis ensures gradient flow converges to global minima for well-chosen initializations with sufficient features and low initial risk.
- Mechanism: The P-Ł inequality provides a lower bound on the ratio between the square gradient norm and the risk. This prevents spurious critical points (saddles, local minima) and guarantees the risk decreases at a constant rate along the gradient flow, ensuring convergence to global minima.
- Core assumption: The risk satisfies the P-Ł inequality in neighborhoods of well-chosen initializations where the number of features is finite but sufficiently large and the initial risk is sufficiently small.
- Evidence anchors:
  - [abstract] "Performing a local Polyak-Łojasiewicz (P-Ł) analysis, we then show convergence of the gradient flow for well-chosen initializations"
  - [section 4.1] "This assumption is in particular satisfied by the quadratic loss ℓ(x, y) = 1/2 ||x − y||^2 in regression problems"
  - [section 4.3] "The above quantity could for example be computed numerically during training"
- Break condition: If the P-Ł inequality fails to hold in any neighborhood of the initialization, or if the initialization has too few features or too high initial risk.

### Mechanism 3
- Claim: The equivalence between gradient flows (Definition 3.2) and curves of maximal slope (Definition 3.4) provides a rigorous foundation for analyzing ResNet training dynamics.
- Mechanism: By showing that the gradient flow defined via adjoint sensitivity analysis coincides with the curve of maximal slope in metric spaces, the work leverages well-established theory of gradient flows in metric spaces to prove existence, uniqueness, and stability of solutions.
- Core assumption: The risk L is sufficiently regular (locally Lipschitz) and the gradient norm ||∇L||L2(µ) is an upper gradient for L.
- Evidence anchors:
  - [abstract] "we first show the well-posedness of the gradient flow equation and its consistency with the training of ResNets at finite width"
  - [section 3.3.2] "A direct consequence of the previous result is the differentiability of the flow map and consequently of the risk along absolutely continuous curves"
  - [section 3.4] "This result follows from the successive application of [AGS08, Thm.2.2.3 and Thm.2.3.3]"
- Break condition: If the risk L is not locally Lipschitz or if ||∇L||L2(µ) fails to be an upper gradient, breaking the equivalence between the two gradient flow definitions.

## Foundational Learning

- Concept: Metric spaces and gradient flows in non-Euclidean settings
  - Why needed here: The parameter space PLeb2([0,1]×Ω) with the COT metric is not a Hilbert space, requiring the theory of gradient flows in metric spaces to properly define and analyze the optimization dynamics.
  - Quick check question: What is the key difference between gradient flows in Euclidean spaces versus metric spaces, and how does the notion of "curve of maximal slope" generalize the classical gradient flow?

- Concept: Optimal transport and Wasserstein distances
  - Why needed here: The COT metric is a restriction of the Wasserstein distance, and understanding its properties (like absolutely continuous curves satisfying continuity equations) is crucial for defining the gradient flow and proving convergence.
- Quick check question: How does the COT distance differ from the standard Wasserstein distance, and what is the significance of the marginal constraint on [0,1]?

- Concept: Reproducing kernel Hilbert spaces (RKHS) and kernel methods
  - Why needed here: The kernel matrix associated with the feature distribution plays a crucial role in the P-Ł analysis, and understanding its conditioning is essential for proving convergence conditions for specific architectures like SHL residuals.
  - Quick check question: How does the feature distribution µ2 influence the kernel matrix K[µ], and why is its conditioning important for the P-Ł inequality?

## Architecture Onboarding

- Component map:
  - Parameter space: PLeb2([0,1]×Ω) - measures on layer-parameter pairs with Lebesgue marginal on layers
  - Metric structure: Conditional Optimal Transport (COT) distance - layer-wise L2 Wasserstein distance
  - Optimization: Gradient flow for risk L w.r.t. COT metric
  - Key objects: Feature map ϕ, adjoint variable p, kernel matrix K[µ]
  - Convergence analysis: Local Polyak-Łojasiewicz inequality

- Critical path:
  1. Define feature map ϕ and parameter space PLeb2([0,1]×Ω)
  2. Equip parameter space with COT metric
  3. Show equivalence between adjoint gradient flow and curve of maximal slope
  4. Prove existence, uniqueness, and stability of gradient flow
  5. Analyze local P-Ł property and derive convergence conditions
  6. Apply to specific architectures (e.g., SHL residuals) and verify conditions

- Design tradeoffs:
  - COT vs. standard Wasserstein: COT better models layer-wise training but has stronger topology
  - P-Ł vs. other convergence analysis: P-Ł provides explicit convergence rates but requires specific initialization conditions
  - General vs. specific architectures: General analysis applies to many architectures but convergence conditions are easier to verify for specific cases like SHL

- Failure signatures:
  - Gradient flow not converging: Check if initialization satisfies P-Ł conditions (sufficient features, low initial risk)
  - Kernel matrix ill-conditioned: Verify feature distribution has dense support or satisfies strict positivity
  - Non-unique solutions: Ensure feature map satisfies regularity assumptions (e.g., avoid ReLU activation)

- First 3 experiments:
  1. Implement COT metric computation for simple toy example (e.g., 1D parameter space) and verify layer-wise structure
  2. Verify local Lipschitz continuity of risk L for simple SHL architecture with smooth activation (e.g., tanh)
  3. Compute kernel matrix K[µ] for empirical data distribution and check conditioning for different feature distributions µ2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence behavior change when using ReLU activation instead of smooth activations like tanh or Swish?
- Basis in paper: [explicit] The paper discusses that ReLU activation violates Assumptions A and B, potentially creating issues with singularities in the continuity equation and non-uniqueness of gradient flow solutions.
- Why unresolved: The paper states that while existence of solutions might hold for ReLU, uniqueness and correspondence with curves of maximal slope are not guaranteed.
- What evidence would resolve it: A rigorous analysis showing either (a) existence and uniqueness of gradient flow curves for ReLU activations under modified assumptions, or (b) empirical and theoretical evidence of convergence to global minima despite the potential lack of uniqueness.

### Open Question 2
- Question: Can the convergence conditions be extended to data distributions with density rather than just empirical distributions?
- Basis in paper: [inferred] The paper explicitly states that the Polyak-Łojasiewicz constant scales as N^-1, making it degenerate for large N, and mentions this as an exciting perspective for future work.
- Why unresolved: The analysis heavily relies on the empirical risk formulation and kernel matrix conditioning, which become problematic when moving to continuous data distributions.
- What evidence would resolve it: A theoretical framework extending the P-Ł analysis to continuous data distributions, potentially using different techniques than kernel matrix conditioning.

### Open Question 3
- Question: What is the behavior of the feature distribution during training, and can it be leveraged to ensure escape from the "kernel regime"?
- Basis in paper: [explicit] The paper mentions that leveraging information about the feature dynamic to improve convergence proof is an exciting perspective, and that their analysis cannot ensure escape from the kernel regime.
- Why unresolved: The current analysis only considers the gradient with respect to outer weights, ignoring the evolution of the feature distribution.
- What evidence would resolve it: A theoretical framework incorporating the evolution of the feature distribution into the convergence analysis, potentially showing conditions under which the model escapes the kernel regime.

### Open Question 4
- Question: How do practical optimization algorithms like gradient descent compare to the idealized gradient flow in terms of convergence?
- Basis in paper: [explicit] The paper states that an extension to gradient descent should probably hold due to the local P-Ł property but might require stronger regularity assumptions.
- Why unresolved: The paper only analyzes gradient flow, which is an idealized version of gradient descent.
- What evidence would resolve it: A rigorous analysis showing convergence of gradient descent for the mean-field ResNet model, either by establishing stability between gradient flow and gradient descent or by directly analyzing the discrete algorithm.

## Limitations
- Limited empirical validation of theoretical convergence conditions
- Local P-Ł analysis requires specific initialization conditions that may be difficult to satisfy in practice
- Analysis focuses on gradient flow rather than practical gradient descent algorithms

## Confidence

- **High confidence**: The mathematical foundations of COT metric and gradient flow theory; existence and uniqueness results for the gradient flow (Section 3)
- **Medium confidence**: The equivalence between adjoint gradient flow and curves of maximal slope; the local P-Ł analysis framework
- **Low confidence**: Practical applicability of convergence conditions for specific architectures; empirical verification of the layer-wise training model

## Next Checks
1. **Numerical verification of P-Ł conditions**: Implement the SHL residual architecture and empirically compute the kernel matrix conditioning λmin(K[μ, x^μ]) for various initializations to verify when the P-Ł inequality holds.

2. **Comparison of COT vs. standard metrics**: Run gradient descent experiments on simple ResNet architectures using both COT-inspired layer-wise training and standard global training to measure the practical impact of the layer-wise structure.

3. **Robustness to initialization scale**: Systematically vary the initialization scale and feature count in SHL residuals to empirically map out the region of convergence and test the boundaries predicted by the P-Ł analysis.