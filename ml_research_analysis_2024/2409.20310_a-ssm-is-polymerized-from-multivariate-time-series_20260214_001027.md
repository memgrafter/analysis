---
ver: rpa2
title: A SSM is Polymerized from Multivariate Time Series
arxiv_id: '2409.20310'
source_url: https://arxiv.org/abs/2409.20310
tags:
- mopa
- channels
- poly-mamba
- multivariate
- orthogonal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Poly-Mamba is a novel SSM-based method for multivariate time series
  forecasting that explicitly models channel dependency variations with time (CDT).
  It introduces three key operations: MOPA, which projects multivariate hidden states
  onto a multivariate orthogonal polynomial basis space to capture complex inter-channel
  dependencies; LCM, which models simple linear correlations between channels; and
  Order Combining, which adaptively generates CDT patterns while retaining trend information.'
---

# A SSM is Polymerized from Multivariate Time Series

## Quick Facts
- arXiv ID: 2409.20310
- Source URL: https://arxiv.org/abs/2409.20310
- Reference count: 24
- Poly-Mamba achieves state-of-the-art performance on multivariate time series forecasting, particularly excelling in scenarios with many channels and complex correlations

## Executive Summary
Poly-Mamba introduces a novel State Space Model (SSM) approach for multivariate time series forecasting that explicitly models Channel Dependency variations with Time (CDT). The method expands univariate orthogonal polynomial basis into multivariate space through its Multivariate Orthogonal Polynomial Approximation (MOPA) operation, combined with Linear Channel Mixing (LCM) and Order Combining mechanisms. Experimental results on six real-world datasets demonstrate superior performance, especially on high-dimensional data with complex channel relationships, while maintaining efficient memory usage.

## Method Summary
Poly-Mamba is an SSM-based method for multivariate time series forecasting that explicitly models channel dependency variations with time. It introduces three key operations: MOPA projects multivariate hidden states onto a multivariate orthogonal polynomial basis space to capture complex inter-channel dependencies; LCM handles simple linear correlations between channels using a learnable parameter matrix; and Order Combining uses gating to adaptively generate CDT patterns while retaining trend information. The method builds upon Mamba's architecture, adding these operations to the hidden state processing pipeline to achieve state-of-the-art performance on datasets with varying channel counts and correlation complexities.

## Key Results
- Achieves MSE of 0.138 and MAE of 0.238 on ECL dataset with 321 channels, outperforming competing methods
- Maintains efficient memory usage of 1.43 GiB on high-dimensional datasets compared to 29.37 GiB for Crossformer
- Demonstrates state-of-the-art performance across six real-world datasets (ETTm1, ETTm2, ETTh1, ETTh2, ECL, Exchange, Traffic, Weather, Solar-Energy) with varying channel counts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Poly-Mamba explicitly models Channel Dependency variations with Time (CDT) by expanding univariate orthogonal polynomial basis into multivariate space.
- Mechanism: The method projects multivariate hidden states onto a multivariate orthogonal polynomial basis space containing variable mixing terms, allowing the model to capture complex inter-channel dependencies through weighted coefficients.
- Core assumption: Channel dependencies in multivariate time series vary with time and can be effectively represented by multivariate orthogonal polynomial basis functions.
- Evidence anchors:
  - [abstract]: "Its core concept is to expand the original orthogonal function basis space into a multivariate orthogonal function space containing variable mixing terms, and make a projection on this space so as to explicitly describe the CDT by weighted coefficients."
  - [section 3.1]: "The idea of MOPA is to explicitly describe the inter-channel dependency pattern by using the mapping weighted coefficients of the extended multivariate polynomial function basis, which contains variable mixing terms."
  - [corpus]: Weak evidence. Corpus shows related work on channel influence and interdependency but no direct mention of orthogonal polynomial basis expansion for CDT modeling.
- Break condition: When channel dependencies are static or when the number of channels is too small for meaningful multivariate polynomial expansion.

### Mechanism 2
- Claim: Linear Channel Mixing (LCM) handles simple linear correlations between channels efficiently.
- Mechanism: LCM uses a learnable parameter matrix to linearly mix channel information, capturing straightforward relationships without the complexity of higher-order terms.
- Core assumption: Not all channel relationships are complex; many can be adequately represented by linear combinations.
- Evidence anchors:
  - [abstract]: "For the simple linear relationship between channels, we propose Linear Channel Mixing (LCM) and generate CDT patterns adaptively for different channels through a proposed Order Combining method."
  - [section 3.2]: "LCM characterizes the linear relationship between channels with a learnable parameter matrix L of size C × C, and multiplies L with the coefficient matrix of size C × N."
  - [corpus]: Weak evidence. Related papers discuss channel influence and interdependency but lack specific mention of linear mixing mechanisms for time series.
- Break condition: When channel relationships are predominantly nonlinear, making linear mixing insufficient for capturing dependencies.

### Mechanism 3
- Claim: Order Combining adaptively generates CDT patterns by using gating to select between linear and complex dependencies.
- Mechanism: A gating mechanism weights the outputs of LCM and MOPA, allowing the model to adaptively choose between linear relationships and higher-order complex dependencies for each channel.
- Core assumption: Different channels and time points may require different types of dependency modeling (linear vs. complex).
- Evidence anchors:
  - [abstract]: "generate CDT patterns adaptively for different channels through a proposed Order Combining method."
  - [section 3.3]: "Order Combining adaptively generates the CDT pattern by applying a Gate operation [14] on the outputs of MOPA and LCM."
  - [corpus]: Weak evidence. While gating mechanisms are common in neural networks, specific application to adaptively selecting between linear and polynomial dependencies in MTS is not evident in corpus.
- Break condition: When the gating mechanism fails to learn meaningful distinctions between linear and complex dependencies, or when the overhead of gating outweighs its benefits.

## Foundational Learning

- Concept: Orthogonal polynomials and their properties
  - Why needed here: The method relies on expanding univariate orthogonal polynomials into multivariate space to capture channel dependencies.
  - Quick check question: Why are orthogonal polynomials particularly suitable for online function approximation in SSMs?

- Concept: State Space Models (SSMs) and their dynamical equations
  - Why needed here: Poly-Mamba builds upon SSM foundations, modifying the dynamical equations to incorporate multivariate polynomial approximations.
  - Quick check question: How does the HIPPO matrix in SSMs enable efficient online function approximation?

- Concept: Multivariate time series forecasting challenges
  - Why needed here: Understanding the specific challenges of MTS (high dimensionality, complex inter-channel dependencies) motivates the design choices in Poly-Mamba.
  - Quick check question: What are the key differences between modeling univariate and multivariate time series that make traditional methods less effective?

## Architecture Onboarding

- Component map: Input tokens -> Embedding -> SSM layer (MOPA, LCM, Order Combining) -> Fully connected layer -> Predictions

- Critical path:
  1. Input tokens processed through embedding
  2. SSM layer with MOPA, LCM, and Order Combining applied to hidden states
  3. Output from SSM layer fed to fully connected layer
  4. Predictions generated

- Design tradeoffs:
  - Complexity vs. performance: Adding MOPA, LCM, and Order Combining increases model complexity but significantly improves performance on datasets with many channels and complex correlations.
  - Memory usage: The additional operations increase memory requirements, particularly for datasets with many channels (e.g., 1.43 GiB for ECL with 321 channels).
  - Computational overhead: While more complex than standard Mamba, the method remains efficient due to the simplified implementation of MOPA.

- Failure signatures:
  - Poor performance on datasets with few channels or weak inter-channel dependencies
  - Increased memory usage leading to out-of-memory errors on large datasets
  - Gating mechanism failing to learn meaningful distinctions between linear and complex dependencies

- First 3 experiments:
  1. Compare Poly-Mamba with and without MOPA on a dataset with known complex inter-channel dependencies (e.g., Weather dataset)
  2. Test the effectiveness of LCM by comparing performance on datasets with predominantly linear relationships
  3. Evaluate the impact of Order Combining by comparing with a variant that uses fixed (non-adaptive) dependency modeling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Poly-Mamba scale with the number of channels beyond the datasets tested, and what are the theoretical limits of its effectiveness for extremely high-dimensional multivariate time series?
- Basis in paper: [inferred] The paper shows excellent performance on the ECL dataset with 321 channels and mentions scalability advantages over Crossformer (29.37 GiB vs 1.43 GiB), but does not test with thousands of channels or provide theoretical scaling analysis.
- Why unresolved: The paper demonstrates effectiveness on moderate channel counts but lacks systematic evaluation of scaling behavior or theoretical analysis of performance degradation with increasing channel dimensionality.
- What evidence would resolve it: Experimental results on datasets with thousands of channels, computational complexity analysis as a function of channel count, and theoretical analysis of how MOPA and LCM operations scale with dimensionality.

### Open Question 2
- Question: Can the MOPA operation be further optimized to reduce computational complexity while maintaining or improving its ability to capture complex channel dependencies?
- Basis in paper: [explicit] The paper acknowledges that MOPA simplifies complete polynomial expansion and mapping operations, and mentions that the two-step calculation plus linear layer after dimensional power expansion "will greatly increase the complexity," suggesting potential for further optimization.
- Why unresolved: The paper presents MOPA as a simplified implementation but does not explore alternative optimization strategies or compare different levels of simplification.
- What evidence would resolve it: Comparative experiments testing different MOPA simplifications, computational efficiency analysis of alternative implementations, and ablation studies showing the trade-off between complexity reduction and performance.

### Open Question 3
- Question: How does Poly-Mamba's performance compare to hybrid models that combine SSM-based approaches with attention mechanisms for multivariate time series forecasting?
- Basis in paper: [inferred] The paper focuses on SSM-based approaches and demonstrates superiority over pure Transformer-based methods, but does not test hybrid architectures that might combine the strengths of both approaches.
- Why unresolved: The paper establishes Poly-Mamba's effectiveness against pure SSM and pure Transformer approaches but leaves open the question of whether hybrid models could achieve better performance.
- What evidence would resolve it: Experimental comparisons with hybrid models combining SSM and attention mechanisms, ablation studies showing the contribution of each component in hybrid architectures, and analysis of when hybrid approaches might be preferable.

## Limitations

- The theoretical foundation for why multivariate orthogonal polynomial basis functions are particularly suited for CDT modeling is underdeveloped.
- The paper lacks detailed ablation studies showing how different polynomial orders or basis functions affect performance.
- Memory efficiency claims need direct comparison with competing methods' memory usage on the same hardware.

## Confidence

**High Confidence**: Claims about achieving state-of-the-art performance on the tested datasets are well-supported by quantitative metrics (MSE and MAE scores) across multiple benchmarks and forecast horizons.

**Medium Confidence**: Claims about the effectiveness of MOPA in capturing complex inter-channel dependencies are supported by results but lack detailed ablation studies showing how different polynomial orders or basis functions affect performance. The adaptive nature of Order Combining is demonstrated but not thoroughly analyzed.

**Low Confidence**: The theoretical foundation for why multivariate orthogonal polynomial basis functions are particularly suited for CDT modeling is underdeveloped. The paper presents this as a novel contribution but doesn't provide rigorous mathematical proof or extensive comparative analysis with other basis function approaches.

## Next Checks

1. **Ablation Study on Polynomial Order**: Conduct experiments varying the order of the multivariate polynomial basis in MOPA (e.g., testing orders 2, 3, and 4) to quantify how polynomial complexity affects performance and determine if the current order selection is optimal.

2. **Memory Usage Comparison**: Measure and compare the actual memory footprint of Poly-Mamba against competing methods (Mamba, S5, etc.) on the same hardware setup, particularly for high-channel datasets like ECL, to validate the claimed memory efficiency.

3. **Dependency Pattern Analysis**: Perform a detailed analysis of what types of channel dependency patterns (linear, periodic, exponential, etc.) are best captured by MOPA versus LCM, and under what conditions the Order Combining mechanism successfully selects between them.