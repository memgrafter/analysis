---
ver: rpa2
title: Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors
arxiv_id: '2402.12626'
source_url: https://arxiv.org/abs/2402.12626
tags:
- feature
- attacks
- space
- learning
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies indiscriminate data poisoning attacks on pre-trained
  feature extractors, focusing on fine-tuning and transfer learning scenarios. The
  authors propose two types of attacks: input space attacks (direct optimization of
  poisoned data in input space) and feature targeted attacks (three-stage approach:
  acquire target parameters, find poisoned features, invert back to input space).'
---

# Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors

## Quick Facts
- arXiv ID: 2402.12626
- Source URL: https://arxiv.org/abs/2402.12626
- Authors: Yiwei Lu; Matthew Y. R. Yang; Gautam Kamath; Yaoliang Yu
- Reference count: 12
- Key outcome: Transfer learning is more vulnerable to indiscriminate data poisoning attacks than fine-tuning, with feature matching attacks showing superior effectiveness compared to input space attacks

## Executive Summary
This paper studies indiscriminate data poisoning attacks on pre-trained feature extractors used in fine-tuning and transfer learning scenarios. The authors propose two types of attacks: input space attacks that directly optimize poisoned data in the input space, and feature targeted attacks that employ a three-stage approach (acquiring target parameters, finding poisoned features, inverting back to input space). Experiments on CIFAR-10 and ImageNet demonstrate that transfer learning is more susceptible to these attacks than fine-tuning. The feature targeted attacks, particularly feature matching, significantly outperform input space attacks by a margin of 2%-10% accuracy drop. The study also examines unlearnable examples, finding them less effective against fixed feature extractors.

## Method Summary
The paper investigates two categories of indiscriminate data poisoning attacks on pre-trained feature extractors. Input space attacks directly optimize poisoned samples in the input space using methods like TGDA, GC, and UE. Feature targeted attacks employ a staged approach: first acquiring target parameters for the linear head, then finding poisoned features by treating learned feature representations as a dataset, and finally inverting these poisoned features back to the input space. The experiments use CIFAR-10 and ImageNet datasets with poisoning fractions ranging from 0.03 to 1.0, evaluating test accuracy drop compared to clean models. The study compares attack effectiveness under different constraints and examines both fine-tuning and transfer learning scenarios.

## Key Results
- Transfer learning is more vulnerable to indiscriminate data poisoning attacks than fine-tuning, with 2%-10% higher accuracy drops
- Feature matching attacks outperform input space attacks by a significant margin in constrained settings
- Unlearnable examples are less effective against fixed feature extractors compared to input space attacks
- Feature targeted attacks mitigate optimization difficulties under constraints through staged feature space manipulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Indiscriminate data poisoning attacks can degrade model performance by injecting poisoned samples into training data.
- Mechanism: The attacker constructs a poisoned distribution that maximizes the loss on a clean validation set when the model is retrained on the mixed distribution of clean and poisoned data.
- Core assumption: The poisoned samples can be optimized to effectively increase the loss without being easily detected.
- Evidence anchors:
  - [abstract]: "indiscriminate data poisoning attacks, which aim to decrease model utility by injecting a small number of poisoned data into the training set"
  - [section 3.1]: "an adversary aims to construct a poisoned set to augment the clean training set, such that by training on the mixed dataset, the performance of the downstream task on the test set is reduced."
- Break condition: If the poisoned samples are detected and removed by data sanitization methods.

### Mechanism 2
- Claim: Feature-targeted attacks are more effective than input space attacks because they mitigate optimization difficulties under constraints.
- Mechanism: The attack is broken down into three stages: acquiring target parameters, finding poisoned features, and inverting them back to the input space. This staged approach allows for more effective optimization under constraints.
- Core assumption: Treating learned feature representations as a dataset allows for more effective poisoning in the feature space.
- Evidence anchors:
  - [abstract]: "feature targeted attacks... mitigate the challenge with three stages, firstly acquiring target parameters for the linear head; secondly finding poisoned features by treating the learned feature representations as a dataset; and thirdly inverting the poisoned features back to the input space."
  - [section 4]: "Motivated by the optimization challenge of GC input space attack, we propose a staged strategy to mitigate the difficulty of the constrained problem."
- Break condition: If the feature inversion step fails to produce poisoned samples that are both effective and visually similar to clean data.

### Mechanism 3
- Claim: Transfer learning is more vulnerable to data poisoning attacks than fine-tuning.
- Mechanism: The domain shift in transfer learning makes the linear evaluation process more susceptible to poisoning attacks, as the feature extractor is adapted to a different dataset than the one used for pre-training.
- Core assumption: The effectiveness of the attack is influenced by the similarity between the pre-training and downstream datasets.
- Evidence anchors:
  - [abstract]: "Empirical results reveal that transfer learning is more vulnerable to our attacks."
  - [section 5.3.1]: "In general, input space attacks are more effective (an increased accuracy drop≈ 2%-10%) on transfer learning than on fine-tuning tasks."
- Break condition: If the downstream task dataset is very similar to the pre-training dataset, reducing the domain shift.

## Foundational Learning

- Concept: Data poisoning attacks
  - Why needed here: Understanding how malicious data can degrade model performance is fundamental to this paper.
  - Quick check question: What is the difference between indiscriminate and targeted data poisoning attacks?

- Concept: Self-supervised learning and contrastive learning
  - Why needed here: The paper focuses on attacks on pre-trained feature extractors obtained through self-supervised learning methods.
  - Quick check question: How does contrastive learning differ from supervised learning in terms of the data it uses?

- Concept: Feature space vs. input space
  - Why needed here: The paper proposes attacks in both the input space and feature space, requiring an understanding of the distinction.
  - Quick check question: Why might attacking in the feature space be more effective than attacking in the input space?

## Architecture Onboarding

- Component map: Feature extractor (fixed) -> Linear head (trainable) -> Training dataset (clean + poisoned) -> Validation dataset (clean)

- Critical path:
  1. Pre-train feature extractor using contrastive learning
  2. Perform downstream task (fine-tuning or transfer learning)
  3. Inject poisoned samples into training data
  4. Retrain linear head on mixed dataset
  5. Evaluate performance on clean validation set

- Design tradeoffs:
  - Effectiveness vs. detectability of poisoned samples
  - Optimization difficulty under constraints
  - Transfer learning vs. fine-tuning vulnerability

- Failure signatures:
  - Poisoned samples are easily detected and removed
  - Optimization fails to produce effective poisoned features
  - Attack effectiveness is significantly reduced when constraints are applied

- First 3 experiments:
  1. Implement GC input space attack without constraints and observe effectiveness.
  2. Apply constraints to GC input space attack and measure the reduction in effectiveness.
  3. Implement feature matching attack and compare its effectiveness to GC input space attack.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of feature matching attacks vary with different choices of β values?
- Basis in paper: [explicit] The paper discusses the trade-off between attack effectiveness and the legitimacy of poisoned samples by controlling the hyperparameter β.
- Why unresolved: The paper only provides results for β = 0.25, 0.1, and 0.05, and does not explore the full range of possible β values or their impact on attack effectiveness.
- What evidence would resolve it: Conducting experiments with a wider range of β values and analyzing the resulting attack effectiveness and poisoned sample characteristics.

### Open Question 2
- Question: How do indiscriminate data poisoning attacks perform on pre-trained feature extractors with different architectures (e.g., ResNet vs. Vision Transformer)?
- Basis in paper: [explicit] The paper presents results for ResNet-18 and ResNet-50 architectures but does not compare their vulnerability to attacks.
- Why unresolved: The paper does not provide a direct comparison between different architectures, and the vulnerability of each to indiscriminate attacks remains unclear.
- What evidence would resolve it: Conducting experiments with various pre-trained feature extractor architectures and comparing their susceptibility to indiscriminate attacks.

### Open Question 3
- Question: How do indiscriminate data poisoning attacks perform when transferring to datasets with different characteristics (e.g., number of classes, domain shift)?
- Basis in paper: [explicit] The paper discusses transfer learning scenarios with CIFAR-10 and CIFAR-100, as well as a more challenging dataset (PatchCamelyon), but does not provide a comprehensive analysis of how dataset characteristics affect attack effectiveness.
- Why unresolved: The paper only presents results for a limited number of datasets and does not explore the impact of various dataset characteristics on attack performance.
- What evidence would resolve it: Conducting experiments with a diverse set of datasets, varying in characteristics such as number of classes, domain shift, and data distribution, and analyzing the impact on attack effectiveness.

## Limitations
- The effectiveness of feature-targeted attacks relies heavily on the quality of feature inversion, which can produce anomalies requiring filtering
- The study focuses primarily on image classification tasks, leaving questions about attack effectiveness on other data modalities
- While unlearnable examples are less effective against fixed feature extractors, adaptive defenses that detect and remove poisoned samples are not explored

## Confidence
- **High Confidence**: The core finding that transfer learning is more vulnerable to indiscriminate data poisoning attacks than fine-tuning is well-supported by extensive experiments across multiple datasets and attack methods.
- **Medium Confidence**: The proposed feature-targeted attack methodology shows promise, but its practical effectiveness depends on the reliability of feature inversion algorithms and the ability to avoid producing detectable anomalies.
- **Low Confidence**: The comparative effectiveness of different attack strategies under various constraints would benefit from additional ablation studies and real-world deployment considerations.

## Next Checks
1. **Defense Implementation**: Implement and evaluate simple data sanitization methods (e.g., outlier detection, statistical analysis of poisoned samples) to assess the practical robustness of the proposed attacks.
2. **Cross-Domain Transfer**: Test attack effectiveness when the downstream dataset differs significantly from the pre-training dataset (e.g., CIFAR-10 features used for medical image classification).
3. **Constraint Impact Analysis**: Systematically vary constraint parameters (ℓ∞ bound, perceptibility metrics) to quantify the tradeoff between attack effectiveness and stealth across all proposed attack methods.