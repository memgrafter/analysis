---
ver: rpa2
title: 'W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering'
arxiv_id: '2408.08444'
source_url: https://arxiv.org/abs/2408.08444
tags:
- passages
- passage
- question
- retrieval
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: W-RAG introduces a weakly supervised approach to improve dense
  retrieval in RAG systems for open-domain question answering. The method uses an
  LLM to generate weak relevance labels by assessing the likelihood that retrieved
  passages enable the correct answer generation, addressing the scarcity of human-annotated
  training data.
---

# W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering

## Quick Facts
- arXiv ID: 2408.08444
- Source URL: https://arxiv.org/abs/2408.08444
- Authors: Jinming Nian; Zhiyuan Peng; Qifan Wang; Yi Fang
- Reference count: 40
- Key outcome: W-RAG uses LLM-generated answer likelihoods to create weak relevance labels for training dense retrievers, achieving performance close to ground-truth supervision without human annotation.

## Executive Summary
W-RAG introduces a weakly supervised approach to improve dense retrieval in RAG systems for open-domain question answering. The method uses an LLM to generate weak relevance labels by assessing the likelihood that retrieved passages enable the correct answer generation, addressing the scarcity of human-annotated training data. Experiments on four datasets show that retrievers fine-tuned with W-RAG data outperform unsupervised baselines and approach the performance of models trained on ground-truth labels, with statistically significant improvements in both retrieval and OpenQA metrics. The approach is generalizable across different LLMs and dense retriever architectures, offering a scalable solution for enhancing RAG systems without extensive human annotation.

## Method Summary
The method retrieves top-100 passages using BM25 for each question, then uses an LLM (Llama3-8B-Instruct) to rerank these passages based on the probability of generating the ground-truth answer when conditioned on each question-passage pair. The top-ranked passage becomes the positive example for training dense retrievers (DPR or ColBERT) using in-batch negative sampling. The fine-tuned retrievers are then evaluated in a RAG pipeline with Llama3.1-8B-Instruct. The approach addresses the scarcity of human-annotated training data by leveraging LLM capabilities to generate weak relevance labels that align retrieval with answer generation quality.

## Key Results
- Retrievers fine-tuned with W-RAG data outperform unsupervised baselines across all four datasets (MSMARCO, NQ, SQuAD, WebQ)
- W-RAG-tuned retrievers achieve performance approaching models trained on ground-truth labels
- The method shows consistent improvements across different dense retriever architectures (DPR and ColBERT)
- W-RAG demonstrates statistical significance in both retrieval metrics (Recall, MRR) and end-to-end OpenQA performance (F1, EM, BLEU-1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weak relevance labels from LLM-generated answer likelihoods align retrieval with answer generation quality
- Mechanism: The system uses an LLM to rerank top-k passages retrieved by BM25 based on the probability of generating the ground-truth answer when conditioned on each question-passage pair. This probability is computed as the average log-likelihood of generating each token in the ground-truth answer. The top-ranked passage becomes the positive example for training the dense retriever.
- Core assumption: A passage that increases the LLM's likelihood of generating the correct answer is more relevant for the OpenQA task than passages ranked by traditional semantic similarity measures.
- Evidence anchors: [abstract], [section 3.1]
- Break condition: If the LLM cannot reliably generate the correct answer even when given the true evidence passage, or if the ground-truth answer is too ambiguous for consistent generation

### Mechanism 2
- Claim: In-batch negative sampling from the reranked lists provides effective training signal for dense retrievers
- Mechanism: After generating weak labels through answer likelihood ranking, the system uses in-batch negative sampling where the top-ranked passage serves as the positive example and other passages in the same batch serve as negatives. This approach maximizes the log-likelihood of the positive passage relative to the negative passages.
- Core assumption: Passages ranked lower in the LLM's answer likelihood ranking are less likely to help answer the question, making them suitable as negative examples for training.
- Evidence anchors: [section 3.2.1], [section 4.1.2]
- Break condition: If the reranked lists have poor quality (e.g., the true evidence passage is not ranked highly), the negative examples may not be truly negative, leading to poor training signal

### Mechanism 3
- Claim: Weak supervision with LLM-generated labels can achieve performance comparable to ground-truth supervised training
- Mechanism: The system demonstrates that retrievers trained with weak labels generated by LLM answer likelihood ranking achieve OpenQA performance close to retrievers trained with human-labeled ground-truth data, while requiring no manual annotation.
- Core assumption: The ranking based on answer likelihood captures the same relevance information as human annotations, at least sufficiently well for effective training.
- Evidence anchors: [abstract], [section 5.1]
- Break condition: If the LLM's answer likelihood ranking systematically differs from human judgment of relevance, the weak supervision may fail to capture the true relevance signal needed for effective retrieval

## Foundational Learning

- Concept: Dense Retrieval and Embedding Spaces
  - Why needed here: The method relies on training dense retrievers that encode questions and passages into the same embedding space and measure relevance through vector similarity. Understanding how DPR and ColBERT work is essential for implementing the training procedure.
  - Quick check question: What is the difference between DPR's dual-encoder approach and ColBERT's late interaction mechanism?

- Concept: RAG Pipeline and Retriever Importance
  - Why needed here: The method fits into the standard RAG pipeline where the retriever is a critical component that affects final answer quality. Understanding how retrievers impact RAG performance is crucial for appreciating why improving retrieval matters.
  - Quick check question: Why does the paper argue that improving the retriever is more feasible than fine-tuning LLMs in practice?

- Concept: Weak Supervision and Learning from Noisy Labels
  - Why needed here: The core innovation is using LLM-generated weak labels instead of human annotations. Understanding the principles and challenges of learning from weak supervision is essential for implementing and troubleshooting the approach.
  - Quick check question: What are the potential risks of using weak supervision, and how does the paper address them?

## Architecture Onboarding

- Component map: Question → BM25 retrieval → LLM reranking → Dense retriever training → RAG inference
- Critical path: The bottleneck is the LLM inference during reranking, which scales linearly with the number of passages
- Design tradeoffs: The method trades computational cost (LLM inference for reranking) for reduced annotation effort. Using more passages for reranking improves recall but increases latency. The choice between DPR and ColBERT involves tradeoffs between training complexity and retrieval quality.
- Failure signatures: Poor retrieval performance despite training could indicate (1) LLM reranking quality issues, (2) inadequate negative sampling in training, (3) poor initialization of the dense retriever, or (4) mismatch between the reranking task and the retrieval task.
- First 3 experiments:
  1. Verify the LLM reranking quality by comparing Recall@1 before and after reranking on a small validation set
  2. Test the training pipeline by training a dense retriever on synthetic weak labels and measuring retrieval performance
  3. Evaluate the complete RAG pipeline with the trained retriever on a small subset of the test set to verify the end-to-end improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of W-RAG scale with increasing amounts of weakly labeled training data compared to traditional human-labeled datasets?
- Basis in paper: [inferred] The paper mentions resource constraints limited the amount of weakly labeled data generated and suggests that more data could benefit generalizability, but does not empirically test this scaling relationship.
- Why unresolved: The study used only 2,000 training samples for fine-tuning, making it unclear whether W-RAG's effectiveness would improve proportionally with larger training sets or eventually plateau.
- What evidence would resolve it: Systematic experiments varying the size of W-RAG training data (e.g., 2k, 10k, 50k, 100k samples) while measuring both retrieval performance and final OpenQA accuracy, compared against ground-truth labeled datasets of equivalent size.

### Open Question 2
- Question: Which types of passages are most beneficial for RAG performance in OpenQA, and can this insight guide more effective dense retrieval methods?
- Basis in paper: [explicit] The authors cite Cuconasu et al. [6] showing that retrieved passages should answer the question and note that "even randomly sampled tokens were beneficial in some cases," suggesting current relevance definitions may be insufficient.
- Why unresolved: The paper acknowledges this gap but does not investigate which passage characteristics (e.g., length, specificity, lexical overlap) correlate with improved LLM answer generation beyond simple likelihood scores.
- What evidence would resolve it: Controlled experiments analyzing passage features (semantic density, answer specificity, noise levels) that maximize LLM answer accuracy, potentially leading to new training objectives or retrieval architectures.

### Open Question 3
- Question: Does W-RAG's performance advantage hold when using larger, more capable LLMs (e.g., 70B+ parameter models) compared to the 8B models tested?
- Basis in paper: [explicit] The authors note they only tested with 8B parameter LLMs and leave evaluation with larger models for future work, despite observing consistent trends across different 8B models.
- Why unresolved: The paper demonstrates W-RAG works well with 8B models but does not establish whether the method's effectiveness scales with model capacity or if diminishing returns occur with larger LLMs.
- What evidence would resolve it: Direct comparisons of W-RAG-trained retrievers using identical training data but different LLM labelers (e.g., Llama 3.8B vs. Llama 3.70B vs. GPT-4) measuring both retrieval performance and downstream OpenQA accuracy.

## Limitations

- The computational overhead of LLM-based reranking scales linearly with the number of passages, making it potentially impractical for very large corpora.
- The quality of weak supervision depends heavily on the LLM's ability to accurately assess answer likelihood, with no thorough analysis of failure cases where LLM judgment diverges from human relevance assessment.
- The method assumes ground-truth answers are available during training, limiting its applicability to completely unsupervised scenarios.

## Confidence

- **High Confidence**: The experimental results showing W-RAG-tuned retrievers outperforming unsupervised baselines and approaching ground-truth performance are well-supported by the four dataset evaluations.
- **Medium Confidence**: The claim that W-RAG is generalizable across different LLMs and retriever architectures is supported by experiments with multiple configurations, but the paper does not explore the full range of possible LLM and retriever combinations.
- **Medium Confidence**: The mechanism by which answer likelihood ranking captures relevance information is theoretically sound, but the paper does not provide extensive ablation studies or error analysis to fully validate that the LLM's ranking aligns with human judgment of relevance.

## Next Checks

1. **Cross-LLM Consistency**: Validate the weak supervision quality by running the W-RAG pipeline with different LLM models (e.g., GPT-4, Claude) and measuring the consistency of retrieval performance across models.

2. **Annotation Efficiency Analysis**: Conduct a cost-benefit analysis comparing the computational cost of LLM reranking versus the time/cost of human annotation for the same number of training examples.

3. **Failure Mode Investigation**: Systematically analyze cases where W-RAG-tuned retrievers fail to improve over unsupervised baselines, identifying patterns in question types or answer characteristics that challenge the weak supervision approach.