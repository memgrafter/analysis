---
ver: rpa2
title: 'IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models'
arxiv_id: '2403.15952'
source_url: https://arxiv.org/abs/2403.15952
tags:
- illusions
- vlms
- optical
- illusion
- shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IllusionVQA, a dataset of optical illusions
  designed to evaluate Vision-Language Models (VLMs) on visual comprehension and soft
  localization tasks. The dataset includes 435 multiple-choice comprehension questions
  and 1000 soft localization instances, covering 12 distinct illusion categories.
---

# IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models

## Quick Facts
- arXiv ID: 2403.15952
- Source URL: https://arxiv.org/abs/2403.15952
- Reference count: 35
- Primary result: VLMs achieve 62.99% comprehension accuracy and 49.7% localization accuracy on optical illusions, significantly below human performance

## Executive Summary
This paper introduces IllusionVQA, a dataset designed to evaluate Vision-Language Models (VLMs) on optical illusion comprehension and localization tasks. The dataset includes 435 multiple-choice comprehension questions and 1000 soft localization instances across 12 illusion categories. Evaluations show that GPT4V, the best-performing VLM, achieves only 62.99% accuracy on comprehension and 49.7% on localization, significantly lagging behind human performance (91.03% and 100%, respectively). The study reveals that smaller open-source models perform even worse, often near random chance, and that In-Context Learning and Chain-of-Thought reasoning do not consistently improve performance.

## Method Summary
The IllusionVQA dataset was created by collecting optical illusion images from the internet, filtering out those detectable by GPT4V, and creating multiple-choice question-answer pairs. VLMs were evaluated using 0-shot and 4-shot settings with Chain-of-Thought reasoning for localization tasks. Human evaluation was conducted with a subset of participants to establish baseline performance. The study compared VLM performance across different model sizes and investigated the impact of In-Context Learning and Chain-of-Thought reasoning on task performance.

## Key Results
- GPT4V achieves 62.99% accuracy on optical illusion comprehension versus human performance of 91.03%
- VLMs show 49.7% localization accuracy compared to perfect human performance (100%)
- Smaller open-source models perform near random chance on both comprehension and localization tasks
- In-Context Learning and Chain-of-Thought reasoning do not consistently improve performance and can degrade results, particularly for Gemini-Pro

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs fail to comprehend optical illusions due to the mismatch between their language priors and visual input.
- Mechanism: VLMs rely heavily on language priors and textual descriptions to understand visual content. When presented with an optical illusion, the language priors may conflict with the actual visual input, leading to incorrect interpretations. This is especially problematic for smaller VLMs that have less sophisticated visual understanding capabilities.
- Core assumption: VLMs primarily use language priors to interpret visual information, and these priors are not well-calibrated for optical illusions.
- Evidence anchors:
  - [abstract] Evaluations show that GPT4V, the best-performing VLM, achieves 62.99% accuracy on comprehension and 49.7% on localization, significantly lagging behind human performance (91.03% and 100%, respectively).
  - [section] VLMs are prone to ignoring visual input (Goyal et al., 2017; Jabri et al., 2016).
  - [corpus] The corpus evidence is weak, as the related papers focus on different aspects of VLMs and do not directly address the issue of language priors and optical illusions.
- Break condition: If VLMs are trained on a dataset that specifically addresses optical illusions and their unique characteristics, or if they are fine-tuned to better handle the mismatch between language priors and visual input.

### Mechanism 2
- Claim: VLMs struggle with spatial reasoning, which is crucial for understanding and localizing optical illusions.
- Mechanism: Optical illusions often rely on geometric impossibilities or spatial manipulations to create their effect. VLMs, especially smaller ones, have difficulty with tasks that require precise spatial reasoning and geometric understanding. This leads to poor performance on tasks like soft localization of impossible objects.
- Core assumption: Spatial reasoning is a key component of understanding optical illusions, and VLMs have limited capabilities in this area.
- Evidence anchors:
  - [abstract] Additionally, VLMs fail to locate optical illusions even when the correct answer is provided in the context.
  - [section] Soft localization of geometrically impossible objects is challenging for VLMs because it requires precise detection of contours and advanced geometric and spatial reasoning.
  - [corpus] The corpus evidence is weak, as the related papers do not directly address the issue of spatial reasoning in VLMs and optical illusions.
- Break condition: If VLMs are trained with a focus on spatial reasoning tasks or if they are augmented with modules specifically designed for geometric understanding.

### Mechanism 3
- Claim: In-Context Learning (ICL) and Chain-of-Thought (CoT) reasoning do not consistently improve VLM performance on optical illusions.
- Mechanism: ICL and CoT are designed to improve VLM performance by providing relevant examples or step-by-step reasoning. However, for optical illusions, these techniques may introduce additional language priors that conflict with the visual input, leading to worse performance. This is particularly evident in the case of Gemini-Pro, where ICL and CoT substantially degrade localization performance.
- Core assumption: ICL and CoT are not universally effective for all types of tasks, and their effectiveness depends on the specific characteristics of the task at hand.
- Evidence anchors:
  - [abstract] We discover that In-Context Learning (ICL) and Chain-of-Thought reasoning do not consistently improve performance and can even degrade results, particularly for Gemini-Pro.
  - [section] Table 3 shows that Gemini-Pro performs better than GPT4V (+3.5%) when evaluated without ICL or CoT. However, GPT4V shows the highest accuracy overall when evaluated with 4-shot+CoT (+9.7% gain) while Gemini-Pro’s performance degrades substantially (-9.6%).
  - [corpus] The corpus evidence is weak, as the related papers focus on different aspects of VLMs and do not directly address the issue of ICL and CoT performance on optical illusions.
- Break condition: If ICL and CoT examples are carefully selected to align with the unique characteristics of optical illusions, or if the techniques are adapted to better handle the specific challenges posed by these illusions.

## Foundational Learning

- Concept: Optical Illusions
  - Why needed here: Understanding the nature and characteristics of optical illusions is crucial for designing effective evaluation methods and interpreting VLM performance on these tasks.
  - Quick check question: What are the key features that distinguish optical illusions from ordinary images, and how do these features impact VLM perception?

- Concept: Vision-Language Models (VLMs)
  - Why needed here: VLMs are the primary focus of this study, and understanding their architecture, capabilities, and limitations is essential for interpreting the results and drawing meaningful conclusions.
  - Quick check question: What are the main components of a VLM, and how do they contribute to its overall performance on visual understanding tasks?

- Concept: In-Context Learning (ICL) and Chain-of-Thought (CoT) Reasoning
  - Why needed here: ICL and CoT are techniques used to improve VLM performance, and understanding their mechanisms and limitations is crucial for interpreting the results of the experiments.
  - Quick check question: How do ICL and CoT work, and under what conditions do they tend to improve or degrade VLM performance?

## Architecture Onboarding

- Component map: Image Encoder -> Fusion Module -> Language Model -> Output Module
- Critical path: Image input → Image Encoder → Fusion Module → Language Model → Output Module
- Design tradeoffs: Balancing visual and textual understanding capabilities, tradeoff between model size and performance, choosing appropriate evaluation metrics for optical illusions
- Failure signatures: Over-reliance on language priors leading to incorrect interpretations, poor spatial reasoning resulting in failure to localize impossible objects, inconsistent performance with ICL and CoT techniques
- First 3 experiments:
  1. Evaluate VLMs on a diverse set of optical illusions to assess their comprehension capabilities
  2. Test VLMs' ability to localize impossible objects within a scene using soft localization tasks
  3. Investigate the impact of ICL and CoT on VLM performance for optical illusion tasks

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset creation process involved filtering illusions detectable by GPT4V, potentially introducing bias
- The dataset size (435 questions, 1000 localization instances) may limit generalizability
- The study focuses primarily on multiple-choice comprehension and soft localization, leaving open questions about VLM performance on other optical illusion tasks

## Confidence
- Language priors hypothesis: High confidence
- Spatial reasoning limitations: Medium confidence
- ICL/CoT effectiveness: High confidence

## Next Checks
1. Replicate experiments with a larger, independently curated optical illusion dataset to verify results aren't influenced by the GPT4V filtering step
2. Compare VLM performance on identical geometric localization tasks with both ordinary objects and optical illusions to isolate illusion-specific effects
3. Test additional VLM architectures with explicit spatial reasoning modules to evaluate whether specialized components improve optical illusion performance