---
ver: rpa2
title: Model Selection with Model Zoo via Graph Learning
arxiv_id: '2404.03988'
source_url: https://arxiv.org/abs/2404.03988
tags:
- graph
- dataset
- datasets
- learning
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting the best pre-trained
  model for fine-tuning from a large model zoo. The authors propose TransferGraph,
  a novel framework that reformulates model selection as a graph learning problem.
---

# Model Selection with Model Zoo via Graph Learning

## Quick Facts
- arXiv ID: 2404.03988
- Source URL: https://arxiv.org/abs/2404.03988
- Reference count: 40
- Up to 32% improvement in correlation between predicted and actual fine-tuning results

## Executive Summary
This paper introduces TransferGraph, a novel framework for selecting the best pre-trained model for fine-tuning from a large model zoo. The authors reformulate model selection as a graph learning problem, constructing a heterogeneous graph that captures relationships between models and datasets through metadata, performance history, and transferability scores. Through comprehensive experiments across 16 real datasets (12 images, 8 text), TransferGraph demonstrates significant improvements in predicting fine-tuning performance, achieving up to 32% better correlation than state-of-the-art methods. The approach is particularly effective when using Node2Vec-based graph learners and can handle scenarios with limited training history by leveraging transferability scores.

## Method Summary
TransferGraph constructs a heterogeneous graph where nodes represent models and datasets, with edges encoding dataset-dataset similarity, model-dataset performance history, and transferability scores. The framework uses graph learning algorithms (Node2Vec, GAT, GraphSAGE) to extract embeddings that capture structural and relational patterns, then trains prediction models (Linear Regression, Random Forest, XGBoost) using metadata, dataset representations, and graph features. The approach is evaluated through leave-one-out cross-validation on 16 target datasets, predicting which pre-trained model will perform best when fine-tuned on a target dataset.

## Key Results
- TransferGraph achieves up to 32% improvement in correlation between predicted scores and actual fine-tuning accuracy compared to state-of-the-art methods
- Graph features significantly outperform metadata alone, with the most effective strategy combining all features (metadata, dataset similarity, and graph features)
- Node2Vec-based graph learners perform particularly well on the relatively small graph (265 nodes, 20.1 average degree) used in experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating model selection as graph link prediction allows TransferGraph to exploit intrinsic relationships between models and datasets
- Mechanism: The framework constructs a heterogeneous graph where nodes represent models and datasets, and edges encode either dataset-dataset similarity or model-dataset performance/transferability scores. Graph learners then capture these relationships to predict performance
- Core assumption: The relationships between models and datasets (similarity, performance history, transferability) contain predictive signal for fine-tuning outcomes
- Evidence anchors:
  - [abstract] "TransferGraph constructs a graph using extensive metadata extracted from models and datasets, while capturing their inherent relationships."
  - [section III.B] "We reformulate the challenge of model selection as a graph link prediction problem."
  - [corpus] Weak evidence - related works focus on model/architecture selection, not graph-based model selection

### Mechanism 2
- Claim: Graph learning algorithms can extract more informative features than metadata alone for predicting model performance
- Mechanism: Node embeddings from graph learners (Node2Vec, GAT, GraphSAGE) capture structural and relational patterns that complement basic metadata features, improving prediction accuracy
- Core assumption: The neighborhood structure in the graph encodes information beyond what individual metadata features capture
- Evidence anchors:
  - [section VII.C] "Our approach using graph features significantly outperform the baselines."
  - [section V.B.2] "GraphSAGE employs a sampling and aggregation method to perform inductive node embedding."
  - [corpus] Weak evidence - related works use graph methods for instance selection or domain adaptation, not for predicting fine-tuning performance

### Mechanism 3
- Claim: Incorporating both dataset representations and metadata provides complementary information for model selection
- Mechanism: Dataset representations (Task2Vec/Domain Similarity) capture semantic similarities between datasets, while metadata (number of samples, classes, model parameters) provides coarse-grained model and dataset characteristics
- Core assumption: Semantic dataset similarity and metadata characteristics are both predictive of transfer learning success
- Evidence anchors:
  - [section IV.B.1] "We adopt Domain Similarity to extract the embeddings for dataset representations."
  - [section IV.A.2] "The metadata of models reveals their learning capability from a certain perspective."
  - [section VII.C] "Among all, the most effective strategy is to include all the features, i.e., metadata features, dataset similarity, and graph features."

## Foundational Learning

- Concept: Graph representation of model-dataset relationships
  - Why needed here: Provides a structured way to encode and learn from complex relationships between models and datasets
  - Quick check question: What types of edges exist in the TransferGraph construction and what do they represent?

- Concept: Graph learning algorithms (Node2Vec, GAT, GraphSAGE)
  - Why needed here: Extract meaningful node embeddings that capture structural and relational information from the graph
  - Quick check question: How do Node2Vec and GAT differ in the information they capture from the graph?

- Concept: Transfer learning and fine-tuning performance prediction
  - Why needed here: The ultimate goal is to predict which pre-trained model will perform best when fine-tuned on a target dataset
  - Quick check question: What metrics are used to evaluate the effectiveness of model selection strategies?

## Architecture Onboarding

- Component map:
  - Metadata and feature collection → Graph construction → Graph learning → Prediction model training → Model recommendation
  - Key components: Dataset representations, metadata extraction, graph construction heuristics, graph learners, prediction models

- Critical path:
  1. Collect dataset representations and metadata
  2. Construct graph with appropriate edge weights and pruning
  3. Train graph learner to extract node embeddings
  4. Train prediction model using metadata, dataset representations, and graph features
  5. Predict performance for target dataset and recommend top models

- Design tradeoffs:
  - Graph size vs. computational efficiency: Larger graphs capture more relationships but are slower to process
  - Edge pruning threshold: Affects graph density and the information available to graph learners
  - Feature inclusion: More features may improve accuracy but increase complexity and risk of overfitting

- Failure signatures:
  - Low correlation between predicted and actual performance indicates model selection strategy is ineffective
  - Unstable or poor performance across different datasets suggests issues with feature selection or graph construction
  - High variance in predictions may indicate overfitting or insufficient training data

- First 3 experiments:
  1. Evaluate baseline performance using only metadata features (LR) on a small subset of datasets
  2. Add dataset similarity features to metadata and compare performance improvement
  3. Incorporate graph features using Node2Vec and measure the additional improvement in correlation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different graph learning algorithms perform on various model zoo configurations (size, heterogeneity, edge density)?
- Basis in paper: [explicit] The paper compares Node2Vec, Node2Vec+, GraphSAGE, and GAT, finding Node2Vec-based methods perform best on their relatively small graph, but doesn't fully explore hyperparameter tuning or different zoo configurations
- Why unresolved: The evaluation uses a fixed graph structure with specific properties (265 nodes, 20.1 average degree). The impact of graph size, density, and heterogeneity on different graph learners remains unexplored
- What evidence would resolve it: Systematic experiments varying graph size, edge density, and model zoo heterogeneity while testing multiple graph learners with tuned hyperparameters

### Open Question 2
- Question: Which features in the graph construction (dataset similarity, training history, transferability scores) contribute most to model selection performance?
- Basis in paper: [explicit] The paper uses multiple edge types and mentions that dataset representations have high dimensionality that may affect certain graph learners, but doesn't quantify individual feature contributions
- Why unresolved: While ablation studies show that combining all features works best, the relative importance of different edge types and feature combinations remains unclear
- What evidence would resolve it: Feature ablation studies systematically removing or modifying specific edge types, plus feature importance analysis from the prediction models

### Open Question 3
- Question: How does TransferGraph perform when new models or datasets are continuously added to the model zoo?
- Basis in paper: [inferred] The paper mentions that the current approach requires retraining when new nodes/edges are added, suggesting this is a limitation
- Why unresolved: The evaluation uses a leave-one-out approach but doesn't examine scenarios where the graph grows incrementally over time with new models/datasets
- What evidence would resolve it: Experiments with incremental graph updates, comparing retraining vs. dynamic graph learning approaches, and measuring performance degradation over time

## Limitations

- Performance may degrade when applied to domains with limited training history or when historical data exhibits high variance
- The framework requires retraining when new models or datasets are added to the model zoo
- Multiple hyperparameters (edge pruning thresholds, graph learner parameters) significantly impact results but are not fully explored

## Confidence

- **High Confidence**: The core mechanism of reformulating model selection as a graph link prediction problem is technically sound and well-supported by experimental results
- **Medium Confidence**: The claim that graph features consistently outperform metadata alone is supported by experiments, but the magnitude of improvement varies significantly across datasets
- **Low Confidence**: The assertion that Node2Vec-based learners are "particularly effective" compared to other graph learning methods lacks comprehensive comparative analysis

## Next Checks

1. **Sensitivity Analysis**: Systematically vary edge pruning thresholds and graph learner hyperparameters to understand their impact on prediction accuracy and identify optimal configurations for different dataset types

2. **Cross-Domain Transferability**: Evaluate TransferGraph's performance when training on image datasets and testing on text datasets (and vice versa) to assess how well the approach generalizes across modalities

3. **Data Efficiency Study**: Gradually reduce the amount of historical training data and measure how performance degrades, identifying the minimum viable training history needed for effective model selection