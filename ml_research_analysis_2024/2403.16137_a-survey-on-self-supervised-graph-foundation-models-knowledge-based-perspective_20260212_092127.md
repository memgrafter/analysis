---
ver: rpa2
title: 'A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective'
arxiv_id: '2403.16137'
source_url: https://arxiv.org/abs/2403.16137
tags:
- graph
- node
- classification
- prediction
- discrimination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey provides the first comprehensive knowledge-based taxonomy\
  \ of self-supervised graph foundation models, categorizing 300+ references by the\
  \ graph knowledge they exploit: microscopic (nodes, links), mesoscopic (context,\
  \ clusters), and macroscopic (global structure, manifolds). It addresses the shortcomings\
  \ of prior surveys\u2014lack of comprehensiveness, unclear categorization, and architecture-limited\
  \ perspectives\u2014by unifying graph neural networks, graph transformers, and graph\
  \ language models under a single framework."
---

# A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective

## Quick Facts
- arXiv ID: 2403.16137
- Source URL: https://arxiv.org/abs/2403.16137
- Reference count: 40
- Primary result: First comprehensive knowledge-based taxonomy of self-supervised graph foundation models, categorizing 300+ references by the graph knowledge they exploit

## Executive Summary
This survey provides the first comprehensive knowledge-based taxonomy of self-supervised graph foundation models, categorizing 300+ references by the graph knowledge they exploit: microscopic (nodes, links), mesoscopic (context, clusters), and macroscopic (global structure, manifolds). It addresses the shortcomings of prior surveys—lack of comprehensiveness, unclear categorization, and architecture-limited perspectives—by unifying graph neural networks, graph transformers, and graph language models under a single framework. The survey covers 9 knowledge categories, 25+ pretext tasks, and diverse downstream tuning strategies (fine-tuning, prompting), providing systematic insights into how different knowledge patterns contribute to graph foundation model performance.

## Method Summary
The survey conducts a comprehensive literature review of 300+ papers on self-supervised graph foundation models, organizing them according to a knowledge-based taxonomy that categorizes models by the types of graph knowledge they exploit. The methodology involves systematic classification of pretext tasks into microscopic (nodes, links), mesoscopic (context, clusters), and macroscopic (global structure, manifolds) knowledge categories, followed by analysis of how these knowledge patterns relate to downstream tuning strategies and architectural choices. The survey evaluates the taxonomy's comprehensiveness by examining coverage across different graph types (heterogeneous, dynamic, hypergraphs) and architectural approaches (GNNs, GTs, GLMs).

## Key Results
- First comprehensive knowledge-based taxonomy that unifies GNNs, GTs, and GLMs under a single framework
- Coverage of 9 knowledge categories and 25+ pretext tasks across microscopic, mesoscopic, and macroscopic levels
- Systematic analysis of how different knowledge patterns contribute to transfer learning performance
- Identification of key challenges including cross-domain knowledge adaptation, dynamic graph handling, and bias mitigation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The knowledge-based taxonomy enables clearer architectural insights than architecture-limited perspectives.
- Mechanism: By categorizing graph models based on the knowledge patterns they exploit (microscopic, mesoscopic, macroscopic) rather than their architectural components, the survey provides a unified framework that can accommodate both traditional GNNs/GTs and emerging graph language models under a single analytical lens.
- Core assumption: Different architectures can be meaningfully compared when evaluated on their ability to capture specific types of graph knowledge rather than their structural differences.
- Evidence anchors: [abstract] states that the taxonomy "provides clearer architectural insights" and is "architecture-agnostic"; [section] explains how the knowledge-based approach allows "re-examining potential GFM architectures" including LLMs; [corpus] shows related work focuses on architectural perspectives but lacks knowledge-based categorization.
- Break Condition: If certain architectures cannot be meaningfully mapped to the knowledge categories, or if knowledge patterns are not sufficient to distinguish architectural capabilities.

### Mechanism 2
- Claim: The three-level hierarchical structure (microscopic → mesoscopic → macroscopic) captures the full spectrum of graph knowledge patterns.
- Mechanism: This hierarchical organization mirrors the natural organization of graph data from individual elements (nodes, links) to local relationships (context, clusters) to global structures (manifolds), enabling systematic coverage of all knowledge patterns that GFMs might exploit.
- Core assumption: Graph knowledge naturally organizes into these three hierarchical levels, and this organization is both comprehensive and mutually exclusive.
- Evidence anchors: [abstract] explicitly defines the three knowledge categories and states they cover "a total of 9 knowledge categories"; [section] provides detailed breakdown of tasks within each category; [corpus] does not mention hierarchical knowledge organization in related surveys.
- Break Condition: If certain knowledge patterns don't fit cleanly into this hierarchy, or if important knowledge types are missing from the three categories.

### Mechanism 3
- Claim: The knowledge-based perspective enables better cross-domain knowledge adaptation for future GFMs.
- Mechanism: By explicitly categorizing the types of graph knowledge that different models exploit, the taxonomy provides a roadmap for identifying which knowledge patterns are transferable across different graph types (heterogeneous, dynamic, hypergraphs) and which require domain-specific adaptation.
- Core assumption: Knowledge patterns have properties that determine their transferability across different graph domains, and these properties can be systematically identified.
- Evidence anchors: [abstract] discusses future directions including "knowledge adaptation across graph types" and "cross-domain knowledge adaptation"; [section] addresses this in Section 8.2, showing how different graph types share commonalities with the knowledge taxonomy; [corpus] shows related work focuses on GFMs but doesn't address knowledge transferability systematically.
- Break Condition: If knowledge patterns prove to be too domain-specific to enable meaningful cross-domain adaptation, or if the taxonomy categories don't align with transfer learning requirements.

## Foundational Learning

- Concept: Self-supervised learning on graphs
  - Why needed here: The survey focuses on self-supervised graph foundation models, so understanding the fundamentals of how self-supervision works on graph data is essential for interpreting the taxonomy and methodology.
  - Quick check question: What are the two key conditions that a pretext task must meet in self-supervised graph learning?

- Concept: Graph knowledge patterns and their hierarchy
  - Why needed here: The entire survey is built around categorizing graph knowledge into microscopic, mesoscopic, and macroscopic levels, so understanding what constitutes each level and how they differ is fundamental to using the taxonomy.
  - Quick check question: How does the knowledge required for predicting node centralities differ from that required for predicting graph similarities?

- Concept: Pretext tasks and downstream tuning strategies
  - Why needed here: The survey covers both pre-training pretext tasks and downstream tuning approaches, and understanding how these two phases interact is crucial for grasping the full GFM development pipeline.
  - Quick check question: What is the key difference between graph fine-tuning and graph prompting in terms of which model parameters are updated?

## Architecture Onboarding

- Component map:
  - Knowledge Taxonomy: 3 main levels (microscopic, mesoscopic, macroscopic) → 9 specific knowledge categories → 25+ pretext tasks
  - Pre-training Phase: Selection of pretext tasks based on target knowledge patterns
  - Downstream Tuning: Fine-tuning (full vs. parameter-efficient) or prompting strategies
  - Model Architectures: GNNs, GTs, GLMs all mapped to knowledge categories rather than architectural components

- Critical path:
  1. Identify target graph knowledge patterns needed for downstream tasks
  2. Select appropriate pretext tasks from the taxonomy that capture those knowledge patterns
  3. Implement pre-training using chosen pretext tasks
  4. Choose downstream tuning strategy (fine-tuning vs. prompting)
  5. Evaluate transfer learning performance across domains

- Design tradeoffs:
  - Granularity vs. Comprehensiveness: More specific knowledge categories provide better guidance but may be harder to map to real tasks
  - Architecture-specific vs. knowledge-based: Architecture-based approaches may capture implementation details but miss transferable patterns
  - Pre-training complexity vs. downstream performance: More diverse pretext tasks improve generalization but increase training cost

- Failure signatures:
  - Poor transfer learning performance across domains suggests mismatch between pre-training knowledge and target task requirements
  - Computational inefficiency indicates overly complex pretext task combinations
  - Limited architectural coverage suggests taxonomy categories don't capture all relevant knowledge patterns

- First 3 experiments:
  1. Map existing graph models to the knowledge taxonomy to validate its comprehensiveness and identify gaps
  2. Implement a simple GNN using pretext tasks from different knowledge categories (e.g., one from microscopic, one from mesoscopic, one from macroscopic) to test the taxonomy's practical utility
  3. Compare transfer learning performance of models pre-trained with architecture-based vs. knowledge-based pretext task selection on a cross-domain benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively combine multiple graph knowledge patterns during pre-training to improve downstream generalization across diverse tasks?
- Basis in paper: [explicit] Section 8.1 discusses the challenge of combining different graph knowledge patterns and mentions multi-task pre-training approaches like AutoSSL and ParetoGNN that dynamically find optimal coefficients for combining pretexts.
- Why unresolved: The paper notes that traditional methods simply assign hyperparameters to weigh each pretext task, leading to suboptimal performance, but does not provide definitive evidence on which combination strategies work best across different graph types and downstream tasks.
- What evidence would resolve it: Empirical studies comparing different multi-task pre-training strategies (dynamic coefficient search, knowledge distillation, multi-task prompting) across multiple graph types (heterogeneous, dynamic, hypergraphs) and downstream tasks, showing consistent improvements in generalization.

### Open Question 2
- Question: Can graph language models effectively leverage billion-parameter LLMs' emergent abilities for graph understanding, or do architectural limitations prevent this?
- Basis in paper: [explicit] Section 7.2.2 discusses the rise of open-source LLMs and notes that "it remains underexplored whether GLMs have adequately tapped the potential in LLMs with billion-scale parameters" and that "some excellent properties of LLMs, e.g., the emergent ability [265], are yet to be discovered on graph model architectures."
- Why unresolved: The paper identifies this as an open research direction but provides no empirical evidence about whether current GLM architectures can access emergent abilities or what architectural changes might be needed.
- What evidence would resolve it: Systematic evaluation of GLMs with varying parameter scales on tasks requiring emergent abilities (few-shot learning, complex reasoning) compared to traditional GFMs, along with architectural modifications designed to better leverage LLM capabilities.

### Open Question 3
- Question: What are the most effective strategies for making graph foundation models robust against structural attacks while maintaining their generalization capabilities?
- Basis in paper: [explicit] Section 8.3 discusses vulnerability to attacks and mentions methods like RES that use random edge dropping and LLM4RGNN that fine-tunes LLMs to identify/remove malicious links, but notes this remains an open challenge.
- Why unresolved: The paper acknowledges structural attacks are particularly effective against graph models and that current defenses have limitations, but does not provide conclusive evidence about which approaches best balance robustness and performance.
- What evidence would resolve it: Comprehensive benchmarking of graph models against various structural attack types showing which defense mechanisms (random edge dropping, adversarial training, LLM-assisted filtering) provide the best trade-off between robustness and downstream task performance across different graph types and domains.

## Limitations
- The knowledge-based taxonomy may not fully capture all nuances of model performance, particularly for complex heterogeneous or dynamic graph structures
- The taxonomy's effectiveness for cross-domain knowledge adaptation requires further validation, especially for hypergraphs and temporal graphs
- Treatment of potential biases in GFMs is preliminary, requiring more systematic investigation of fairness and explainability issues

## Confidence
- High confidence in the comprehensive coverage of existing graph foundation model literature and the internal consistency of the knowledge-based taxonomy
- Medium confidence in claims about the taxonomy's ability to guide future architectural development without extensive empirical validation
- Low confidence in the taxonomy's immediate practical utility for cross-domain knowledge adaptation, given limited exploration of specialized graph types

## Next Checks
1. Conduct empirical validation by mapping 50+ additional graph models to the knowledge taxonomy to test its comprehensiveness and identify potential gaps in the classification framework
2. Implement controlled experiments comparing architecture-based vs. knowledge-based pretext task selection on cross-domain graph benchmarks to quantify the taxonomy's practical utility for transfer learning
3. Extend the taxonomy to explicitly cover heterogeneous, dynamic, and hypergraph structures through case studies of models designed for these specialized graph types