---
ver: rpa2
title: 'All or None: Identifiable Linear Properties of Next-token Predictors in Language
  Modeling'
arxiv_id: '2410.23501'
source_url: https://arxiv.org/abs/2410.23501
tags:
- linear
- rank
- equation
- definition
- properties
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies identifiability in language models, asking whether
  observed linear properties (like parallel vector differences between related words)
  are shared by all models that produce the same next-token distribution. The authors
  prove that for certain linear properties defined through a refinement of relational
  linearity, these properties either hold in all or none of the distribution-equivalent
  models.
---

# All or None: Identifiable Linear Properties of Next-token Predictors in Language Modeling

## Quick Facts
- **arXiv ID**: 2410.23501
- **Source URL**: https://arxiv.org/abs/2410.23501
- **Reference count**: 40
- **Key outcome**: Proves that certain linear properties in language models are shared across all models producing the same next-token distribution under specific conditions.

## Executive Summary
This paper investigates whether linear properties observed in language models (such as parallel vector differences between related words) are universal across all models that produce identical next-token distributions. The authors introduce the concept of effective complexity to characterize distribution-equivalent next-token predictors and prove that under suitable conditions, certain linear properties are either shared by all or none of the equivalent models. This work provides a mathematical framework explaining the ubiquity of linear properties in language models and identifies which properties are truly universal versus those that may vary across distribution-equivalent implementations.

## Method Summary
The paper develops a theoretical framework for analyzing identifiability in next-token predictors of the form p(y|x) ∝ exp(f(x)ᵀg(y)). It defines an equivalence relation based on models producing identical conditional distributions and introduces relational linear properties that can be expressed solely through embedding and unembedding functions. The main theoretical contribution is proving that under specific subspace conditions, these linear properties are preserved across all distribution-equivalent models. The framework extends previous identifiability results by considering models with different representation dimensionalities and provides conditions under which linear properties become universal.

## Key Results
- Proves that certain linear properties are either shared by all or none of the distribution-equivalent models under specific subspace conditions
- Introduces relational linear properties (parallelism, linear probing, linear steering) that can be expressed through embedding and unembedding functions
- Establishes a one-to-one correspondence between conditional probability distributions and equivalence classes of models
- Provides theoretical explanation for the ubiquity of linear properties in language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ubiquity of linear properties in language models can be explained by identifiability theory.
- Mechanism: If a linear property holds in one model with a given next-token distribution, it must hold in all models that produce the same distribution (under suitable conditions). This is proven by showing a one-to-one correspondence between conditional probability distributions and equivalence classes of models.
- Core assumption: The linear property is defined solely in terms of embedding and unembedding functions, and certain subspace conditions hold.
- Evidence anchors:
  - [abstract] "We analyze identifiability as a possible explanation for the ubiquity of linear properties across language models"
  - [section] "Our main result is a mathematical proof that, under suitable conditions, certain linear properties hold for either all or none of the equivalent models generating a given next-token distribution."
  - [corpus] Found 25 related papers on identifiability in various contexts, supporting the relevance of this theoretical framework.
- Break condition: If the linear property depends on components outside the relevant subspaces, or if the subspace conditions are not met, the property may not be shared across all equivalent models.

### Mechanism 2
- Claim: Vector parallelism is preserved within specific subspaces across all distribution-equivalent models.
- Mechanism: Two vectors are parallel in a subspace if one is a scalar multiple of the other within that subspace. The paper proves that if vectors are parallel in the subspace N of one model, their corresponding vectors in the subspace N of any distribution-equivalent model are also parallel.
- Core assumption: The equivalence relation between models preserves the structure of the subspace N.
- Evidence anchors:
  - [section] "Theorem 15 shows that vector parallelism is only preserved within a linear subspace N of the unembedding space."
  - [section] "Two parallel vectors in one model (f, g) may not be parallel in another model (f', g') with the same conditional distribution. They remain parallel only within the subspaces N and N', respectively."
  - [corpus] The corpus contains papers on identifiability and tensor decompositions, which often involve subspace analysis.
- Break condition: If the subspace N is not preserved or the scalar multiple relationship is not maintained, the parallelism may not be preserved.

### Mechanism 3
- Claim: Relational linearity is preserved across all distribution-equivalent models under specific conditions.
- Mechanism: A model linearly represents a query on a subspace if the embedding of a context-query sequence can be expressed as an affine transformation of the context embedding within that subspace. The paper proves that if a model linearly represents a query on a subspace Γ, and Γq is contained within another subspace M, then all distribution-equivalent models also linearly represent the query on corresponding subspaces.
- Core assumption: The subspaces Γ and Γq satisfy the containment conditions relative to the model's subspaces.
- Evidence anchors:
  - [section] "Theorem 14 is an example of a property that all distribution-equivalent next-token predictors, as characterized by our identifiability result (Theorem 5), must share."
  - [section] "Under the the condition that Γ ⊆ N and Γq ⊆ M, relational linearity (Γ lr) is a property of all or none next-token predictors modeling the same conditional distribution."
  - [corpus] Papers on nonlinear ICA and tensor decompositions in the corpus suggest the importance of subspace conditions in identifiability results.
- Break condition: If Γ is not contained within N, or Γq is not contained within M, the relational linearity may not be preserved.

## Foundational Learning

- Concept: Identifiability in statistical models
  - Why needed here: The paper's main contribution is proving that certain linear properties are shared across all models that produce the same next-token distribution, which is an identifiability result.
  - Quick check question: What does it mean for a statistical model to be identifiable up to an equivalence relation?

- Concept: Subspaces and projectors in linear algebra
  - Why needed here: The proofs rely heavily on the properties of subspaces spanned by embeddings and unembeddings, and orthogonal projectors onto these subspaces.
  - Quick check question: What is the relationship between a subspace and its orthogonal projector?

- Concept: Conditional probability distributions and next-token prediction
  - Why needed here: The paper studies next-token predictors, which model the conditional distribution of the next token given a context.
  - Quick check question: How is the conditional probability of the next token typically modeled in language models?

## Architecture Onboarding

- Component map:
  - Embeddings (f): Maps sequences to high-dimensional vector representations
  - Unembeddings (g): Maps tokens to high-dimensional vector representations
  - Next-token distribution: Computed as the softmax of the dot product between embeddings and unembeddings
  - Equivalence relation (EL): Characterizes models that produce the same next-token distribution
  - Linear properties: Defined in terms of the embedding and unembedding functions, such as parallelism and relational linearity

- Critical path:
  1. Define the equivalence relation between models based on their next-token distributions
  2. Prove that certain linear properties are preserved across all models in an equivalence class under specific conditions
  3. Show how these results explain the ubiquity of linear properties in language models

- Design tradeoffs:
  - The paper considers models with different representation dimensionalities, relaxing assumptions from previous work
  - The results hold under specific subspace conditions, which may not always be satisfied in practice

- Failure signatures:
  - If the linear property depends on components outside the relevant subspaces, it may not be preserved
  - If the subspace conditions are not met, the results may not hold

- First 3 experiments:
  1. Verify that the proposed equivalence relation correctly characterizes models with the same next-token distribution
  2. Check that the linear properties defined in the paper are indeed observed in language models
  3. Test whether the subspace conditions are satisfied in practice for various language models and linear properties

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do certain linear properties that are not shared across all ~EL-equivalent models (like general vector parallelism beyond N) appear consistently in empirical observations?
- Basis in paper: [explicit] The paper discusses that PCA-based visualizations of parallelism in language models often reveal structures that, according to their theory, should not be preserved across all ~EL-equivalent models. They note this as a puzzling empirical finding that contradicts their theoretical predictions.
- Why unresolved: The authors hypothesize that additional constraints or inductive biases not captured by their identifiability framework might explain why only certain models are observed in practice, but they don't identify what these constraints are.
- What evidence would resolve it: Systematic empirical studies comparing learned models across different architectures and training regimes to identify which additional constraints consistently emerge, combined with theoretical work extending the identifiability framework to incorporate these constraints.

### Open Question 2
- Question: What specific inductive biases in training algorithms or architectures cause models to converge to a subset of ~EL-equivalent models rather than the full equivalence class?
- Basis in paper: [inferred] The authors discuss that their theory predicts a rich space of ~EL-equivalent models, yet empirical observations show consistent patterns. They suggest that training algorithms or architectural constraints might act as inductive biases steering models toward specific equivalence classes.
- Why unresolved: The paper establishes the mathematical framework for ~EL-equivalence but doesn't analyze the optimization landscape or training dynamics that would explain model selection within this space.
- What evidence would resolve it: Empirical analysis of optimization trajectories across different initialization schemes and architectures, combined with theoretical work on the geometry of the training objective within ~EL-equivalence classes.

### Open Question 3
- Question: How can the theoretical framework be extended to handle models with different token vocabularies (A vs. ~A) while preserving linear property analysis?
- Basis in paper: [explicit] The authors note their analysis is restricted to models sharing the same token vocabulary and hypothesize that results might extend to shared tokens under suitable conditions, but don't formalize this extension.
- Why unresolved: The paper's ~EL-equivalence relation is defined for models with identical vocabularies, and extending it requires careful treatment of how different vocabularies relate through shared tokens.
- What evidence would resolve it: Formal definition of a cross-vocabulary equivalence relation and proof that linear properties are preserved for shared tokens, validated through empirical studies on multilingual models with different tokenization schemes.

### Open Question 4
- Question: Under what precise conditions do non-linear properties (like circular representations) emerge in next-token predictors, and are these properties shared across ~EL-equivalent models?
- Basis in paper: [explicit] The authors acknowledge that non-linear properties have been observed in language models and suggest formalizing these properties within their framework as an open direction.
- Why unresolved: The paper focuses exclusively on linear properties and doesn't provide tools for analyzing non-linear geometric structures in representations.
- What evidence would resolve it: Mathematical characterization of non-linear relational properties analogous to their linear relational framework, combined with empirical validation showing which non-linear properties are preserved across ~EL-equivalence classes.

## Limitations

- The results apply specifically to next-token predictors of the form p(y|x) ∝ exp(f(x)ᵀg(y)), limiting generalizability to other model architectures
- The proofs rely on specific subspace conditions that may not hold in practical implementations, creating a gap between theoretical guarantees and empirical observations
- The paper lacks empirical validation of the theoretical results on actual language models

## Confidence

- **High Confidence**: The core mathematical framework for characterizing distribution-equivalent models (Theorem 5) is well-established and follows logically from the definitions provided. The basic definitions of linear properties (parallelism, linear probing, linear steering) are clearly specified and internally consistent.
- **Medium Confidence**: The preservation of linear properties across equivalent models under specific subspace conditions (Theorems 14 and 15) follows from the mathematical framework, but the practical relevance depends on whether these conditions are satisfied in real models.
- **Low Confidence**: The claim that these theoretical results explain the "ubiquity of linear properties" in language models lacks empirical support and remains speculative without validation on actual models.

## Next Checks

1. **Empirical Validation**: Test whether the subspace conditions required for preserving linear properties (Γ ⊆ N and Γq ⊆ M) are satisfied in practice for popular language models like BERT, GPT, or RoBERTa, and whether observed linear properties align with the theoretical predictions.

2. **Generalization Testing**: Examine whether the results extend to other model architectures beyond next-token predictors, such as masked language models or encoder-decoder models, to assess the broader applicability of the identifiability framework.

3. **Property Preservation Verification**: Systematically verify that specific linear properties (parallelism, linear probing, linear steering) observed in one distribution-equivalent model are indeed preserved across multiple models trained with different random seeds but producing the same next-token distribution.