---
ver: rpa2
title: 'TV-SAM: Increasing Zero-Shot Segmentation Performance on Multimodal Medical
  Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation'
arxiv_id: '2402.15759'
source_url: https://arxiv.org/abs/2402.15759
tags:
- segmentation
- prompts
- medical
- images
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TV-SAM, a zero-shot segmentation algorithm
  for multimodal medical images that integrates GPT-4, GLIP, and SAM without requiring
  manual annotations. The method autonomously generates descriptive text prompts via
  GPT-4 and visual prompts via GLIP, then uses these to guide SAM for segmentation.
---

# TV-SAM: Increasing Zero-Shot Segmentation Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation

## Quick Facts
- arXiv ID: 2402.15759
- Source URL: https://arxiv.org/abs/2402.15759
- Authors: Zekun Jiang, Dongjie Cheng, Ziyuan Qin, Jun Gao, Qicheng Lao, Abdullaev Bakhrom Ismoilovich, Urazboev Gayrat, Yuldashov Elyorbek, Bekchanov Habibullo, Defu Tang, LinJing Wei, Kang Li, Le Zhang
- Reference count: 40
- Key outcome: TV-SAM achieves zero-shot segmentation of multimodal medical images without manual annotations, outperforming SAM AUTO and GSAM baselines while closely matching SAM BBOX with gold-standard prompts

## Executive Summary
TV-SAM introduces a novel zero-shot segmentation approach for multimodal medical images that leverages GPT-4, GLIP, and SAM without requiring any manual annotations. The method autonomously generates descriptive text prompts via GPT-4 and visual bounding box prompts via GLIP, which are then used to guide SAM's segmentation process. Evaluated across seven public datasets spanning eight imaging modalities, TV-SAM demonstrates superior performance compared to existing zero-shot methods, particularly excelling on datasets like ISIC and WBC. The approach effectively segments unseen targets across modalities without additional training, though it faces limitations with radiological images due to domain gaps.

## Method Summary
TV-SAM is a zero-shot segmentation algorithm that integrates GPT-4, GLIP, and SAM to segment multimodal medical images without manual annotations. The method uses GPT-4 to generate descriptive text prompts for medical concepts, GLIP to generate visual bounding box prompts from these text descriptions, and SAM in BBOX mode to produce segmentation masks. The approach is evaluated on seven public medical image datasets across eight imaging modalities, using Dice coefficient as the primary metric and statistical significance testing (P<0.05) for comparisons. No training is required as the method relies entirely on zero-shot learning through prompt engineering.

## Key Results
- TV-SAM outperforms SAM AUTO and GSAM baselines across all seven datasets with statistically significant improvements (P<0.05)
- The method closely matches SAM BBOX performance with gold-standard prompts (P=0.07), demonstrating near-optimal zero-shot capability
- GPT-4's descriptive prompts contribute significantly to performance, with TV-SAM showing superior results compared to using simple object names
- TV-SAM surpasses state-of-the-art methods on specific datasets including ISIC and WBC segmentation tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPT-4 generates descriptive text prompts that are richer than object name alone, enabling GLIP to localize medical objects more accurately.
- **Mechanism**: GPT-4 uses its general knowledge to produce multi-attribute prompts (color, shape, location) for medical concepts, which GLIP then uses to detect bounding boxes.
- **Core assumption**: GPT-4 can accurately describe medical objects it has not been explicitly trained on.
- **Evidence anchors**:
  - [abstract] "autonomously generate descriptive text prompts via GPT-4"
  - [section] "GPT-4 as an external knowledge source to generate detailed text prompts for medical concepts"
  - [corpus] Weak evidence - corpus lacks specific results on GPT-4 prompt quality in medical context.
- **Break condition**: GPT-4 refuses to describe medical images or produces generic, low-detail prompts.

### Mechanism 2
- **Claim**: GLIP leverages the rich text prompts from GPT-4 to improve zero-shot object detection in medical images.
- **Mechanism**: GLIP's vision-language alignment is enhanced by detailed prompts, leading to higher confidence bounding boxes.
- **Core assumption**: GLIP's detection performance scales with prompt specificity.
- **Evidence anchors**:
  - [abstract] "vision language model GLIP ... to autonomously generate ... visual bounding box prompts"
  - [section] "we employ VLMs to generate visual prompts automatically"
  - [corpus] Weak evidence - corpus mentions MedCLIP-SAM variants but no direct comparison of prompt specificity impact.
- **Break condition**: GLIP fails to detect objects even with detailed prompts due to domain gap.

### Mechanism 3
- **Claim**: SAM BBOX mode with GLIP-generated boxes achieves segmentation performance close to gold-standard boxes.
- **Mechanism**: SAM's mask decoder refines GLIP's coarse boxes into precise segmentations using learned visual features.
- **Core assumption**: SAM's mask decoder can correct localization errors from GLIP.
- **Evidence anchors**:
  - [abstract] "closely matching the performance of SAM BBOX with gold standard bounding box prompts (P=0.07)"
  - [section] "box prompts... achieves optimal performance across a range of medical image datasets"
  - [corpus] Weak evidence - corpus lacks quantitative comparison of SAM BBOX with automated vs. manual boxes.
- **Break condition**: SAM's mask decoder consistently underperforms when starting from GLIP boxes.

## Foundational Learning

- **Concept**: Zero-shot segmentation using vision-language models
  - **Why needed here**: Enables segmentation without manual annotations, critical for medical domains with limited labeled data
  - **Quick check question**: What is the difference between zero-shot and few-shot learning in the context of medical image segmentation?

- **Concept**: Prompt engineering for vision-language models
  - **Why needed here**: GPT-4's prompts directly impact GLIP's detection accuracy, which cascades to SAM's segmentation
  - **Quick check question**: How do descriptive prompts differ from simple object name prompts in VLM performance?

- **Concept**: Bounding box selection strategies for segmentation
  - **Why needed here**: GLIP can return multiple boxes; choosing the right ones affects SAM's final output quality
  - **Quick check question**: Why might using multiple GLIP boxes