---
ver: rpa2
title: 'BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric
  CAD Designs'
arxiv_id: '2402.05301'
source_url: https://arxiv.org/abs/2402.05301
tags:
- design
- dataset
- designs
- parametric
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BIKED++, a multimodal dataset of 1.4 million
  procedurally-generated bicycle designs represented parametrically, as JSON files,
  and as rasterized images. The dataset is created through a rendering engine that
  harnesses BikeCAD software to generate vector graphics from parametric designs.
---

# BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs

## Quick Facts
- arXiv ID: 2402.05301
- Source URL: https://arxiv.org/abs/2402.05301
- Reference count: 12
- 1.4 million procedurally-generated bicycle designs with parametric representations, rasterized images, and CLIP embeddings

## Executive Summary
This paper introduces BIKED++, a multimodal dataset containing 1.4 million procedurally-generated bicycle designs represented in three formats: parametric vectors, JSON files, and rasterized images. The dataset is created using a systematic pipeline that samples designs within valid bounds, renders them through BikeCAD, and calculates CLIP embeddings across multiple views. The authors demonstrate that a predictive model can accurately estimate CLIP embeddings from parametric representations, enabling cross-modal comparisons between parametric designs and text or image prompts. This capability allows optimization in the compact parametric space while targeting complex cross-modal objectives.

## Method Summary
The study employs a deep residual network to predict 512-dimensional CLIP embeddings from 96-dimensional parametric bicycle design vectors. The dataset is generated by sampling within design space boundaries defined by the 1st and 99th percentiles of the BIKED dataset, filtering out invalid designs, and rendering valid designs through BikeCAD. Five augmented views are used to calculate each CLIP embedding to reduce noise. The model is trained on 90% of the data with MSE loss, validated on 5%, and tested on 5%, stopping if validation loss doesn't decrease for 5 consecutive epochs.

## Key Results
- Predictive model achieves test set MSE of 0.00235 and R² of 0.810
- 99.99% accuracy in ranking similarity relations between predicted and target embeddings
- Successful demonstration of cross-modal optimization, transforming a red road bike design to resemble "a yellow mountain bike"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The predictive model can accurately estimate CLIP embeddings from parametric representations, enabling cross-modal comparisons.
- Mechanism: By training a supervised model on 1.4 million parametric-CLIP embedding pairs, the model learns a mapping function that approximates the relationship between parametric features and their corresponding image embeddings.
- Core assumption: There exists a learnable function that can approximate the mapping from parametric design space to CLIP embedding space.
- Evidence anchors:
  - [abstract] "Trained models achieve a test set mean squared error of 0.00235 and R² of 0.810"
  - [section] "We evaluate the predictive model on the test set of 70k designs... The final test set mean squared error (MSE) and coefficient of determination (R²) scores are: 0.00235 and 0.810."
  - [corpus] Weak evidence - corpus papers focus on related CAD/parametric design but don't directly address CLIP embedding prediction from parametric data.
- Break condition: The parametric space becomes too sparse or the relationship between parametric features and image embeddings becomes too non-linear for the model to approximate effectively.

### Mechanism 2
- Claim: The dataset enables optimization in parametric space using text/image objectives.
- Mechanism: By predicting CLIP embeddings from parametric designs, similarity can be calculated between parametric designs and text/image prompts, allowing optimization algorithms to work in the compact parametric space while optimizing for complex cross-modal objectives.
- Core assumption: The predicted CLIP embeddings maintain sufficient similarity ranking with true embeddings for optimization purposes.
- Evidence anchors:
  - [abstract] "we demonstrate that a predictive model can be trained to accurately estimate Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric representation directly"
  - [section] "We showcase a simple optimization run in which a red road bike design... is optimized to look more like 'a yellow mountain bike.'"
  - [corpus] Weak evidence - corpus papers discuss CAD optimization but not cross-modal optimization using CLIP embeddings.
- Break condition: The optimization landscape becomes too noisy or the predicted embeddings lose critical information needed for accurate similarity comparisons.

### Mechanism 3
- Claim: The dataset creation pipeline ensures high-quality multimodal data through systematic generation and validation.
- Mechanism: The pipeline uses Sobol sequence sampling, constraint checking, rendering through BikeCAD, and view averaging for CLIP embeddings to generate diverse, valid, and high-quality multimodal data.
- Core assumption: Each step in the pipeline contributes to the overall quality and utility of the final dataset.
- Evidence anchors:
  - [section] "Roughly 92% of the randomly sampled designs are rejected, leaving roughly 1.4 million remaining" and "To reduce noise, CLIP embeddings are calculated over five augmented images"
  - [section] "All optimization is performed in the parametric space and only the final design... is rendered using the renderer and rasterizer"
  - [corpus] No direct evidence - corpus papers don't discuss multimodal dataset creation pipelines.
- Break condition: If constraint checking becomes too restrictive or rendering failures increase significantly, dataset quality and size would degrade.

## Foundational Learning

- Concept: Parametric design representation
  - Why needed here: Understanding how bicycle designs are encoded as numerical and categorical features is crucial for working with the dataset and training predictive models.
  - Quick check question: What is the dimensionality of the parametric vectors in this dataset, and why is this representation useful for optimization?

- Concept: Contrastive Language-Image Pretraining (CLIP)
  - Why needed here: CLIP embeddings are the bridge between parametric designs and text/image prompts, enabling cross-modal comparisons and optimization.
  - Quick check question: How does CLIP establish similarity between text and images, and why is this useful for the applications described in the paper?

- Concept: Multi-output regression
  - Why needed here: The problem of predicting 512-dimensional CLIP embeddings from 96-dimensional parametric vectors is a multi-output regression problem, requiring understanding of appropriate loss functions and model architectures.
  - Quick check question: Why is mean squared error an appropriate loss function for this prediction task, and what does the R² score tell us about model performance?

## Architecture Onboarding

- Component map: Sobol sequence sampling -> Constraint checking -> BikeCAD file generation -> Rendering -> Rasterization -> CLIP embedding calculation -> Deep residual network (parametric vectors -> predicted CLIP embeddings)
- Critical path: Parametric vector -> Predictive model -> CLIP embedding prediction -> Similarity calculation -> Optimization objective
- Design tradeoffs:
  - Dataset size vs. storage constraints (1.4M designs but limited availability of full dataset)
  - Model complexity vs. prediction accuracy (simple residual network vs. more complex architectures)
  - View augmentation vs. computational cost (5 views for noise reduction vs. single view)
- Failure signatures:
  - High MSE/R² scores indicating poor model performance
  - Low similarity ranking accuracy (Equations 1 and 2 failing)
  - Optimization results that don't match expected cross-modal relationships
- First 3 experiments:
  1. Train a simple linear regression model on a small subset to establish baseline performance
  2. Train the proposed residual network on 10% of the data and evaluate on validation set
  3. Test the trained model on cross-modal optimization with simple text prompts before scaling to complex objectives

## Open Questions the Paper Calls Out
No explicit open questions were identified in the provided paper content.

## Limitations
- Model performance is demonstrated only on bicycle-specific parametric designs, limiting generalizability to other domains
- Optimization demonstrations use simple single-objective text prompts rather than complex multi-objective scenarios
- Dataset creation relies heavily on the BikeCAD ecosystem, potentially limiting applicability to other CAD platforms

## Confidence

**High Confidence**: The predictive model architecture and training methodology are sound, with strong empirical validation on held-out test data.

**Medium Confidence**: Cross-modal optimization capabilities are demonstrated but only for simple, single-objective cases.

**Low Confidence**: Generalization to other parametric domains and more complex optimization scenarios remains speculative.

## Next Checks

1. Test the trained model on parametric designs from different domains (e.g., furniture, vehicles) to assess cross-domain generalization.
2. Implement multi-objective optimization scenarios combining text and reference image objectives to evaluate practical utility.
3. Compare the residual network architecture against alternative models (transformer-based, attention mechanisms) to verify architectural choices.