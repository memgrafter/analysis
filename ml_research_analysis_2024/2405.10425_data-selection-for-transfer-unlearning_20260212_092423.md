---
ver: rpa2
title: Data Selection for Transfer Unlearning
arxiv_id: '2405.10425'
source_url: https://arxiv.org/abs/2405.10425
tags:
- unlearning
- data
- static
- target
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for transfer unlearning where a pretrained
  model is adapted to a target task containing data that may later need to be unlearned.
  The approach uses an auxiliary static dataset to select relevant examples that replace
  non-static target data, enabling all potential unlearning requests to be handled
  efficiently in advance.
---

# Data Selection for Transfer Unlearning

## Quick Facts
- arXiv ID: 2405.10425
- Source URL: https://arxiv.org/abs/2405.10425
- Reference count: 15
- Primary result: Method selects relevant auxiliary examples to replace target data that may need unlearning, often outperforming exact unlearning baselines

## Executive Summary
This paper addresses transfer unlearning, where a pretrained model is adapted to a target task but some target data may later need to be unlearned. The proposed approach uses an auxiliary static dataset to select relevant examples that replace non-static target data, enabling efficient handling of potential unlearning requests. The method achieves exact unlearning under a relaxed definition adapted for transfer learning scenarios, and experiments show it often outperforms exact unlearning baselines, particularly when the static data portion is small.

## Method Summary
The approach computes similarity between each auxiliary example and all non-static target examples of the same class, then selects the top M most similar examples per class to replace the non-static data. These selected auxiliary examples are relabeled and combined with the static target data for finetuning. The method maps to a parameterized family of learning algorithms where the selection mechanism defines which subset of auxiliary data to use, enabling exact unlearning by never training on removable data.

## Key Results
- The method often outperforms the gold standard exact unlearning baseline (finetuning only on static data), especially when the static portion is small
- Performance improvements are enhanced when non-static data constitutes the majority of the target dataset
- The approach achieves exact relative transfer unlearning by construction through its parameterized algorithm family design

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Data selection replaces non-static target data with similar examples from an auxiliary dataset, achieving exact unlearning by never training on removable data.
- **Mechanism**: The algorithm computes average similarity between each auxiliary example and all non-static target examples of the same class. It then selects the top M most similar examples per class, relabels them, and finetunes on the union of static target data and these selected auxiliary examples.
- **Core assumption**: Similarity in the pretrained embedding space correlates with usefulness for transfer learning performance on the target task.
- **Evidence anchors**:
  - [abstract]: "uses a mechanism for selecting relevant examples from an auxiliary 'static' dataset, and finetunes on the selected data instead of 'non-static' target data"
  - [section]: "For each class label c in Snon−static, we perform the following steps: 1. Compute the similarity (using dot-product in the embedding space of wsrc) between each example in Saux and each non-static example of class c"
- **Break condition**: When the auxiliary dataset has low domain affinity with the target dataset, similarity-based selection fails to find useful replacements.

### Mechanism 2
- **Claim**: The method achieves exact relative transfer unlearning by constructing a learning algorithm family where the instantiated algorithm operates on data excluding non-static examples.
- **Mechanism**: The approach maps to a parameterized family of learning algorithms {Aθ}θ∈Θ where each θ represents a subset of the auxiliary dataset. The selection mechanism g* defines θ* = g*(wsrc, Snon−static, Saux), and the algorithm uses Aθ* which operates on Sstatic ∪ θ*.
- **Core assumption**: The selection mechanism can be expressed as a deterministic mapping from data to a subset of auxiliary examples.
- **Evidence anchors**:
  - [section]: "Let Θ be the set of all subsets of Saux and define θ̂(wsrc, S) = g*(wsrc, Snon−static, Saux)"
  - [section]: "To see that our transfer learning algorithm fits this definition, pick S′ ⊆ Snon−static. Then Aθ̂(wsrc,S)(wsrc,S\S′) = Aft(wsrc,S static∪θ̂(wsrc,S )) = wtg = U(wtg,S′)"
- **Break condition**: If the selection mechanism is non-deterministic or depends on random factors not captured in θ.

### Mechanism 3
- **Claim**: Performance improvement occurs when the ratio of static to non-static target data is small, as the method can replace most non-static data with relevant auxiliary examples.
- **Mechanism**: When Sstatic is small relative to Stg, the method's ability to select M examples per class from Saux provides a significant training set expansion, while exact unlearning baselines are limited to only the small Sstatic portion.
- **Core assumption**: The auxiliary dataset contains sufficient relevant examples to replace the non-static target data meaningfully.
- **Evidence anchors**:
  - [section]: "We find that our method can, in many datasets, outperform the gold standard 'exact unlearning' (finetuning on only the 'static' portion of the target dataset), especially when the size of that portion is small"
  - [section]: "The improvement is specially enhanced when the ratio is smaller, i.e. when the non-static set constitutes the majority of the Stg"
- **Break condition**: When Sstatic is already large relative to Stg, the benefit of adding S'aux diminishes.

## Foundational Learning

- **Concept**: Transfer learning fundamentals (pretrained model adaptation)
  - Why needed here: The entire approach builds on transferring a pretrained source model to a target task, using either full finetuning or data selection strategies
  - Quick check question: What is the difference between linear probing and full finetuning in transfer learning?

- **Concept**: Machine unlearning definitions and guarantees
  - Why needed here: The work explicitly contrasts exact vs approximate unlearning and introduces a relaxed definition suitable for non-privacy applications
  - Quick check question: How does relative unlearning differ from traditional differential privacy-based unlearning definitions?

- **Concept**: Data selection and domain affinity
  - Why needed here: The method's effectiveness depends on selecting relevant auxiliary data, and the paper analyzes how domain affinity between source and target affects performance
  - Quick check question: What metric does the paper use to quantify the relationship between auxiliary and target datasets?

## Architecture Onboarding

- **Component map**: Pretrained ResNet-18 → Similarity computation module → Data selection module → Finetuning module → Evaluation module
- **Critical path**: Pretrained model weights → similarity computation → top-M selection → finetuning → test accuracy evaluation
- **Design tradeoffs**: 
  - Simplicity vs. sophistication: Uses dot-product similarity in embedding space rather than complex selection mechanisms
  - Efficiency vs. optimality: Amortizes unlearning by handling all requests upfront, potentially suboptimal for specific deletion patterns
  - Static auxiliary requirement: Needs relevant auxiliary data available, limiting applicability
- **Failure signatures**: 
  - Low test accuracy despite selection suggests poor domain affinity
  - No improvement over exact unlearning baseline suggests auxiliary data irrelevance
  - High variance across runs suggests sensitivity to random partitioning
- **First 3 experiments**:
  1. Run with empty Sstatic (ratio=0) on Pets dataset to verify upper bound approach
  2. Run with small Sstatic (ratio=0.1) on SVHN dataset to test exact unlearning baseline comparison
  3. Run domain affinity analysis on Caltech101 dataset to verify correlation with performance boost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical bounds on test accuracy improvement when using data selection for transfer unlearning, as a function of the domain affinity between source and target datasets?
- Basis in paper: [inferred] The paper shows empirical correlation between domain affinity and performance gains, but doesn't provide theoretical analysis or bounds.
- Why unresolved: The paper only demonstrates empirical correlation without deriving theoretical guarantees or bounds on performance improvements.
- What evidence would resolve it: Mathematical analysis proving upper/lower bounds on test accuracy improvements based on domain affinity metrics.

### Open Question 2
- Question: How does the proposed data selection mechanism perform under severe class imbalance in target datasets?
- Basis in paper: [explicit] The paper mentions in Appendix A.3 that Retinopathy dataset results were excluded due to severe class imbalance, but doesn't provide a solution.
- Why unresolved: The paper acknowledges the difficulty but doesn't investigate methods to handle class imbalance or evaluate the selection mechanism's performance in such scenarios.
- What evidence would resolve it: Experimental results showing performance on severely imbalanced datasets with proposed modifications to handle imbalance.

### Open Question 3
- Question: Can unsupervised pretraining improve the quality of selected auxiliary examples compared to supervised pretraining?
- Basis in paper: [inferred] The paper suggests in Appendix A.6 that using unsupervised pretraining could be a direction for future work, implying current supervised pretraining may have limitations.
- Why unresolved: The paper uses supervised pretraining for computing similarities but doesn't compare it against unsupervised alternatives.
- What evidence would resolve it: Experimental comparison of data selection performance using both supervised and unsupervised pretraining approaches.

## Limitations
- The method requires access to a relevant auxiliary dataset in advance, which may not always be available or sufficiently aligned with the target task
- Performance is heavily dependent on domain affinity between the auxiliary and target datasets, with no clear guidance on minimum affinity thresholds
- The approach assumes all data might need to be unlearned, potentially leading to suboptimal performance when only specific subsets require deletion

## Confidence
- **High confidence**: The theoretical framework for exact relative transfer unlearning is sound and well-defined (Mechanism 2)
- **Medium confidence**: Empirical results showing performance improvements, particularly when static data ratios are low (Mechanism 3)
- **Low confidence**: The claim that dot-product similarity in embedding space reliably selects relevant auxiliary examples (Mechanism 1) - limited corpus support for this specific approach

## Next Checks
1. Test the method on datasets where auxiliary data has clearly different domains (e.g., using CIFAR-100 as auxiliary for Pets classification) to establish domain affinity failure boundaries
2. Compare against random selection baselines to quantify whether the similarity-based selection provides meaningful improvement over simpler approaches
3. Evaluate performance when only specific, non-random subsets of target data need unlearning (rather than all non-static data) to assess the efficiency of the amortization approach