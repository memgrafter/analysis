---
ver: rpa2
title: 'The Power of Question Translation Training in Multilingual Reasoning: Broadened
  Scope and Deepened Insights'
arxiv_id: '2405.01345'
source_url: https://arxiv.org/abs/2405.01345
tags:
- reasoning
- question
- language
- training
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving multilingual reasoning
  performance in large language models (LLMs), where a significant performance gap
  exists between English and non-English languages. The authors propose a method called
  "question alignment" that leverages the model's English reasoning capabilities to
  enhance multilingual performance.
---

# The Power of Question Translation Training in Multilingual Reasoning: Broadened Scope and Deepened Insights

## Quick Facts
- arXiv ID: 2405.01345
- Source URL: https://arxiv.org/abs/2405.01345
- Reference count: 40
- Key outcome: Achieves 12.2% average accuracy improvement on mGSM multilingual reasoning benchmark through question alignment method

## Executive Summary
This paper addresses the significant performance gap in multilingual reasoning between English and non-English languages in large language models. The authors propose a two-stage training framework called "question alignment" that leverages the model's English reasoning capabilities to enhance multilingual performance. By aligning multilingual representations within the same semantic space as English through question translation training, the method achieves substantial improvements across various reasoning scenarios including mathematical and common sense reasoning. The approach is also shown to scale effectively to extremely large language models like LLaMA2-70B through an efficient proxy-tuning mechanism.

## Method Summary
The method employs a two-stage training framework: first, question alignment uses translated questions to align the model's internal language representations with English, creating a unified semantic space; second, response alignment uses English instruction data to teach reasoning capabilities. The approach works across multiple reasoning scenarios including mathematical reasoning with chain-of-thought and program-of-thought, as well as common sense reasoning. For extremely large models, proxy-tuning is used to approximate the behavior of truly fine-tuned models without parameter updates, using small fine-tuned models as expert and anti-expert guides. Training is performed for 3 epochs with a learning rate of 2e-5 and batch size 128 on 8×A100 GPUs.

## Key Results
- Achieves 12.2% average accuracy improvement on mGSM multilingual reasoning benchmark
- Shows consistent performance gains across mathematical reasoning (chain-of-thought and program-of-thought) and common sense reasoning tasks
- Effectively scales to extremely large models (LLaMA2-70B) through proxy-tuning without updating parameters
- Demonstrates improved question-response language consistency (52.3% vs 9.7% for baseline)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Question translation training aligns multilingual representations within the same semantic space as English, enabling the model to leverage English reasoning expertise in non-English contexts.
- Mechanism: Training with translated questions creates more compact and unified multilingual representations in middle-to-top layers, facilitating shared reasoning capabilities across languages.
- Core assumption: English reasoning capabilities are superior and can be effectively transferred to non-English contexts through representation alignment.
- Evidence anchors: Abstract statement on unified semantic space, section finding on representation distribution effects, t-SNE visualization showing alignment.
- Break condition: If multilingual representations remain distinct after alignment or if English reasoning expertise is not superior.

### Mechanism 2
- Claim: Incorporating En-X translation data during response alignment biases the model to generate non-English responses, improving question-response language consistency.
- Mechanism: Multi-task training with En-X translation data implicitly encourages the model to produce responses in the same language as input questions.
- Core assumption: The model can learn language consistency through exposure to translation data during fine-tuning.
- Evidence anchors: Discovery that X-En translation data achieves similar goals, table showing significant increase in language consistency from 9.7% to 52.3%.
- Break condition: If the model continues predominantly generating English responses despite translation data exposure.

### Mechanism 3
- Claim: The two-stage training framework scales effectively to extremely large language models through proxy-tuning, achieving near-full fine-tuning performance without parameter updates.
- Mechanism: Proxy-tuning uses small fine-tuned models as expert and anti-expert models to guide large pre-trained models, approximating truly fine-tuned behavior.
- Core assumption: The logit difference between small tuned and untuned models approximates the difference between large pre-trained and fine-tuned models.
- Evidence anchors: Exploration of question alignment on LLaMA2-70B, finding that carefully selected small models can achieve near-full fine-tuning performance, mention of effectiveness for both dense and MoE models.
- Break condition: If proxy-tuning fails to capture truly fine-tuned large model behavior or computational constraints prevent effective implementation.

## Foundational Learning

- **Representation alignment in neural networks**: Understanding how question translation training affects multilingual representation distribution is crucial for grasping the method's effectiveness. Quick check: How does representation alignment differ from simple parameter fine-tuning in terms of affecting model behavior across languages?

- **Chain-of-thought reasoning in language models**: The paper examines effectiveness across different reasoning scenarios including math reasoning with chain-of-thought. Quick check: What distinguishes chain-of-thought reasoning from direct answer generation in terms of model architecture and training data requirements?

- **Mixture-of-Experts (MoE) architecture**: The paper mentions proxy-tuning effectiveness for both dense models and sparse MoE models. Quick check: How does MoE architecture differ from dense models in terms of parameter efficiency and training complexity?

## Architecture Onboarding

- **Component map**: Base pre-trained LLM -> Question alignment stage (X-En translation data) -> Response alignment stage (English instruction data) -> Proxy-tuning component (small expert and anti-expert models) -> Evaluation benchmarks (mGSM, mSVAMP, XCSQA, XNLI)

- **Critical path**: 1) Load pre-trained base model, 2) Perform question alignment training, 3) Perform response alignment training, 4) (For large models) Apply proxy-tuning, 5) Evaluate on multilingual reasoning benchmarks

- **Design tradeoffs**: Translation data quality vs. quantity (higher quality may be more effective but harder to obtain), English vs. non-English response generation (English yields better accuracy but lower consistency), Full fine-tuning vs. proxy-tuning (full fine-tuning may achieve better results but at prohibitive computational cost)

- **Failure signatures**: Low language consistency despite question alignment, degradation in English performance after multilingual fine-tuning, proxy-tuning approximation failure in extremely large models, overfitting to translation data at expense of reasoning capability

- **First 3 experiments**: 1) Baseline comparison: RAlign vs. QAlign→RAlign on mGSM to measure impact of question alignment, 2) Language consistency test: Measure question-response language consistency across different training configurations, 3) Proxy-tuning effectiveness: Compare proxy-tuned large model performance against fully fine-tuned baseline

## Open Questions the Paper Calls Out

The paper identifies several unresolved questions about optimal balance between language consistency and reasoning accuracy, effectiveness across diverse reasoning tasks beyond the three explored scenarios, and the long-term impact of question alignment on internal representation space evolution as models continue to be fine-tuned on additional tasks.

## Limitations

- The representation alignment mechanism relies heavily on qualitative observations from t-SNE visualizations without quantitative validation of semantic space unification
- Language consistency improvements show significant gains but practical impact on end-user experience is not fully characterized
- Proxy-tuning demonstrates strong results but the approximation error between proxy-tuned and truly fine-tuned models lacks systematic measurement

## Confidence

- **High**: Overall framework design and empirical results showing accuracy improvements across multiple benchmarks and reasoning types
- **Medium**: Mechanism explanations, particularly representation alignment claims relying on visualization rather than quantitative metrics
- **Medium**: Proxy-tuning approximation claims, which show strong empirical results but lack systematic error analysis

## Next Checks

1. Conduct quantitative analysis of representation space unification using metrics like mutual information or alignment scores between English and non-English representations, rather than relying solely on t-SNE visualizations

2. Measure practical impact of language consistency improvements through human evaluation of response quality and user satisfaction across different language pairs

3. Systematically evaluate proxy-tuning approximation error by comparing proxy-tuned models against fully fine-tuned models across a range of model sizes and tasks, quantifying the performance gap and its relationship to model scale