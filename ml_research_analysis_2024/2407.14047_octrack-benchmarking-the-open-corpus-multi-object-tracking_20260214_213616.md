---
ver: rpa2
title: 'OCTrack: Benchmarking the Open-Corpus Multi-Object Tracking'
arxiv_id: '2407.14047'
source_url: https://arxiv.org/abs/2407.14047
tags:
- tracking
- object
- recognition
- novel
- octrackb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OCTrackB, a comprehensive benchmark for open-corpus
  multi-object tracking (OCMOT), which extends traditional MOT by recognizing both
  seen and unseen object categories without requiring a predefined category list.
  The benchmark addresses limitations of previous datasets by providing more diverse
  and balanced base/novel classes and abundant samples.
---

# OCTrack: Benchmarking the Open-Corpus Multi-Object Tracking

## Quick Facts
- arXiv ID: 2407.14047
- Source URL: https://arxiv.org/abs/2407.14047
- Reference count: 40
- Introduces OCTrackB, a comprehensive benchmark for open-corpus multi-object tracking (OCMOT)

## Executive Summary
This paper introduces OCTrackB, a comprehensive benchmark for open-corpus multi-object tracking (OCMOT), which extends traditional MOT by recognizing both seen and unseen object categories without requiring a predefined category list. The benchmark addresses limitations of previous datasets by providing more diverse and balanced base/novel classes and abundant samples. A new multi-granularity recognition metric is proposed to handle semantic ambiguity in object recognition. The authors also present OCTracker, a baseline method for OCMOT, and conduct extensive evaluations comparing it with state-of-the-art approaches. Results demonstrate the rationale of OCMOT and the advantages of OCTrackB in advancing open-world tracking research.

## Method Summary
The OCTrackB benchmark is constructed by combining videos from TAO and LV-VIS datasets, using greedy algorithms to ensure diverse and balanced base/novel classes with abundant samples. The proposed OCTracker baseline method consists of three components: (1) Localization using Deformable DETR as a class-agnostic detector, (2) Recognition using a generative language model (FlanT5-base) to generate object category names, and (3) Association using a two-stage training strategy combining static images and raw videos for similarity learning. The evaluation uses a new comprehensive metric called TRETA, which includes LocA, mgReA, and AssocA.

## Key Results
- OCTrackB offers more diverse and balanced base/novel classes (892 total categories across 1,635 videos) compared to existing OCMOT benchmarks
- The proposed multi-granularity recognition metric (mgReA) addresses semantic ambiguity in object recognition by considering hierarchical category relationships
- OCTracker achieves competitive performance on OCTrackB, demonstrating the feasibility of the proposed OCMOT task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed OCMOT problem is more practical than OVMOT because it eliminates the need for a predefined category list during testing.
- Mechanism: By treating object recognition as a generative task rather than a classification task, the model can directly generate category names without requiring a fixed vocabulary. This allows the system to handle truly novel categories that were not seen during training.
- Core assumption: A generative language model (FlanT5-base) can accurately produce category names for unseen objects when conditioned on visual features.
- Evidence anchors:
  - [abstract] "OCMOT... treats the object recognition task as a generative problem, rather than the classification problem in OVMOT"
  - [section 1] "our OCMOT does not require predefined category names as in the OVMOT task. Instead, it directly generates target category names using a generative model"
- Break condition: The generative model fails to produce meaningful or accurate category names for novel objects, or the generated names are too ambiguous to be useful.

### Mechanism 2
- Claim: The multi-granularity recognition metric (mgReA) provides a more compatible evaluation by addressing semantic ambiguity in object recognition.
- Mechanism: By expanding ground truth categories to include their super-categories in the WordNet hierarchy, the metric allows predictions that match at a coarser granularity to be considered correct. This accounts for the natural variation in how specific category names can be generated.
- Core assumption: Semantic hierarchy relationships in WordNet can effectively capture the "correctness" of generated category names at different levels of specificity.
- Evidence anchors:
  - [section 3.2] "we aggregate the categories in LVIS according to WordNet [60] as a hierarchy structure"
  - [section 3.5] "mgReA traces back to the expanded label 'dog' from the ground-truth label 'dalmatian'"
- Break condition: The WordNet hierarchy does not adequately capture the semantic relationships between categories in the dataset, or the expanded categories are too broad to be meaningful.

### Mechanism 3
- Claim: The OCTrackB benchmark provides a more comprehensive and balanced evaluation platform for OCMOT compared to previous datasets.
- Mechanism: By combining videos from TAO and LV-VIS datasets, and using greedy algorithms to ensure diverse and balanced base/novel classes with abundant samples, OCTrackB addresses the limitations of previous benchmarks that had limited categories and samples, especially for novel classes.
- Core assumption: The combination of TAO's longer videos with LV-VIS's larger number of categories provides a representative sample of real-world tracking scenarios with both base and novel categories.
- Evidence anchors:
  - [section 3.3] "we select two large ones with various object categories, i.e., TAO [11] and LV-VIS [58], as the basis for constructing OCTrackB"
  - [section 3.4] "OCTrackB offers more diverse and balanced base/novel classes, along with abundant videos for evaluation with less bias"
- Break condition: The selected videos do not adequately represent the diversity of real-world scenarios, or the greedy algorithms fail to produce a truly balanced dataset.

## Foundational Learning

- Concept: Multi-object tracking (MOT) fundamentals
  - Why needed here: Understanding the core MOT problem of localizing, associating, and tracking objects across video frames is essential before extending it to open-corpus scenarios.
  - Quick check question: What are the three main components of a typical tracking-by-detection MOT system?

- Concept: Open-vocabulary learning and CLIP model
  - Why needed here: The evaluation of generated category names uses CLIP to encode and compare text descriptions, so understanding how CLIP works is crucial for interpreting results.
  - Quick check question: How does CLIP enable zero-shot classification by encoding text and images into a shared embedding space?

- Concept: Generative language models (e.g., FlanT5)
  - Why needed here: The recognition head uses a generative language model to produce category names, so understanding the basics of how these models generate text is important.
  - Quick check question: What is the difference between a generative model and a discriminative model in the context of object classification?

## Architecture Onboarding

- Component map: Localization head (Deformable DETR) -> Recognition head (FlanT5-base) -> Association head (Similarity learning) -> CLIP encoder
- Critical path:
  1. Deformable DETR detects objects (bounding boxes) in each frame
  2. Visual features extracted from detected objects are fed to the generative language model
  3. The language model generates category names for each object
  4. CLIP encodes generated names and ground truth categories for evaluation
  5. Association head learns to match objects across frames for tracking
- Design tradeoffs:
  - Using a generative model for recognition vs. a fixed classification head: More flexible but potentially less accurate for known categories
  - Combining TAO and LV-VIS datasets: More diverse categories but potential domain shift issues
  - Multi-granularity evaluation: More forgiving but may mask poor fine-grained recognition
- Failure signatures:
  - Poor localization: Many false positives/negatives from the detector
  - Incorrect recognition: Generated category names don't match ground truth or are nonsensical
  - Poor association: IDs switch between frames or objects are lost
- First 3 experiments:
  1. Evaluate the localization head on a standard MOT benchmark to ensure it detects objects accurately
  2. Test the recognition head on a held-out set of known categories to see if it can generate correct names
  3. Assess the association head on a video with simple motion to check if it can maintain consistent IDs

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of OCTracker compare to other methods when using a larger beam size for the generative language model?
  - Basis in paper: [explicit] The paper mentions that the beam size is set to 2, but does not explore the impact of different beam sizes on performance.
  - Why unresolved: The paper only reports results for a beam size of 2, leaving the question of how different beam sizes might affect the generative model's performance.
  - What evidence would resolve it: Experiments comparing the performance of OCTracker with different beam sizes (e.g., 1, 2, 4, 8) on the OCTrackB benchmark.

- Open Question 2: How does the proposed multi-granularity recognition metric (mgReA) perform compared to other metrics that account for semantic similarity between generated and ground truth categories?
  - Basis in paper: [explicit] The paper proposes mgReA to address semantic ambiguity but does not compare it to other potential metrics.
  - Why unresolved: While mgReA is designed to handle semantic similarity, the paper does not benchmark it against alternative metrics or ablation studies.
  - What evidence would resolve it: Comparative analysis of mgReA against other semantic similarity metrics (e.g., cosine similarity in embedding space, word embedding-based metrics) on the OCTrackB dataset.

- Open Question 3: What is the impact of using different visual features for the recognition head in OCTracker?
  - Basis in paper: [inferred] The paper uses visual features from Deformable DETR for the recognition head but does not explore the impact of using different feature extractors.
  - Why unresolved: The choice of visual features for the recognition head is not explored, leaving the question of whether other feature extractors might improve performance.
  - What evidence would resolve it: Experiments replacing Deformable DETR features with features from other models (e.g., CLIP, ResNet) in the recognition head of OCTracker and comparing the results on OCTrackB.

## Limitations
- The generative recognition approach relies heavily on the quality and diversity of the training data for the language model, which may limit performance on highly specialized or rare object categories
- The multi-granularity metric, while addressing semantic ambiguity, may mask performance issues in fine-grained recognition by accepting coarser matches
- The dataset construction process involves subjective decisions in category selection and balancing that may introduce unintended biases

## Confidence
High Confidence Claims:
- The OCTrackB benchmark provides a more comprehensive evaluation platform than previous OCMOT datasets, as evidenced by the detailed dataset statistics and comparison with existing benchmarks
- The mgReA metric effectively addresses semantic ambiguity in object recognition through its hierarchical evaluation approach
- OCTracker serves as a reasonable baseline for OCMOT, demonstrating the feasibility of the proposed task

Medium Confidence Claims:
- The elimination of predefined category lists in OCMOT represents a significant practical advantage over OVMOT
- The combination of TAO and LV-VIS datasets creates a representative sample of real-world tracking scenarios
- The two-stage training strategy for association improves performance in open-world scenarios

Low Confidence Claims:
- The generative recognition approach will consistently outperform classification-based methods in real-world applications
- The OCTrackB dataset achieves perfect balance across all base and novel categories
- The TRETA metric provides a complete evaluation of OCMOT system performance

## Next Checks
1. **Robustness Analysis**: Evaluate OCTracker on subsets of OCTrackB with varying degrees of category overlap between base and novel classes to assess performance degradation patterns
2. **Cross-Domain Generalization**: Test the recognition head on images from completely different domains (e.g., medical imaging, satellite imagery) to verify its ability to generate meaningful category names for truly unseen objects
3. **Ablation Study**: Systematically remove components of OCTracker (e.g., generative recognition, multi-granularity metric) to quantify their individual contributions to overall performance and identify potential bottlenecks