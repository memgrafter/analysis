---
ver: rpa2
title: 'PRIMUS: Pretraining IMU Encoders with Multimodal Self-Supervision'
arxiv_id: '2411.15127'
source_url: https://arxiv.org/abs/2411.15127
tags:
- data
- learning
- primus
- pretraining
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PRIMUS, a method for pretraining IMU encoders
  that combines self-supervision, multimodal, and nearest-neighbor supervision to
  improve downstream performance on IMU-based activity recognition tasks. The approach
  addresses the challenge of scarce labeled IMU data by leveraging large amounts of
  unlabeled or weakly labeled IMU data along with multimodal data (video and text).
---

# PRIMUS: Pretraining IMU Encoders with Multimodal Self-Supervision

## Quick Facts
- arXiv ID: 2411.15127
- Source URL: https://arxiv.org/abs/2411.15127
- Authors: Arnav M. Das; Chi Ian Tang; Fahim Kawsar; Mohammad Malekzadeh
- Reference count: 37
- Primary result: PRIMUS achieves up to 15% improvement in test accuracy on IMU-based activity recognition using fewer than 500 labeled samples per class

## Executive Summary
PRIMUS introduces a novel pretraining framework for IMU encoders that leverages multimodal self-supervision combining video, text, and IMU data. The method addresses the challenge of scarce labeled IMU data by utilizing large amounts of unlabeled or weakly labeled IMU data alongside multimodal information. By integrating self-supervision, multimodal learning, and nearest-neighbor supervision, PRIMUS significantly improves downstream performance on activity recognition tasks while requiring substantially less labeled data than previous approaches.

## Method Summary
PRIMUS employs a three-pronged approach to pretraining IMU encoders: self-supervision through contrastive learning between IMU data and corresponding video frames, multimodal learning that aligns IMU representations with textual descriptions, and nearest-neighbor supervision that leverages unlabeled IMU data by matching similar activity patterns. The framework trains an encoder that maps IMU sensor readings to a shared embedding space where semantically similar activities are close together, regardless of the modality. During fine-tuning, the pretrained encoder is adapted to specific activity recognition tasks using minimal labeled data, demonstrating strong data efficiency and generalization across different datasets and sensor placements.

## Key Results
- Achieves up to 15% improvement in test accuracy compared to state-of-the-art baselines on in-domain and out-of-domain IMU datasets
- Requires fewer than 500 labeled samples per class to match or exceed baseline performance
- Demonstrates 10x data efficiency by matching prior performance using only 10% of the multimodal pretraining data
- Shows strong generalization across variations in sensor placement, user demographics, and activity types

## Why This Works (Mechanism)
PRIMUS works by creating rich, semantically meaningful representations through multimodal alignment and self-supervised learning. The contrastive learning between IMU data and video frames captures temporal and spatial patterns in human activities, while text alignment provides semantic grounding of the representations. The nearest-neighbor supervision component helps the model discover structure in unlabeled IMU data, improving its ability to generalize to new activities and sensor configurations. This combination of modalities provides complementary information that enhances the quality of learned representations beyond what single-modality approaches can achieve.

## Foundational Learning
- **Contrastive learning for IMU-video alignment**: Needed to capture temporal and spatial patterns in human activities; Quick check: Verify that similar activities produce closer embeddings across IMU and video modalities
- **Multimodal representation learning**: Required to ground IMU representations in semantic concepts from text; Quick check: Test whether text descriptions of activities align with their IMU embeddings
- **Nearest-neighbor supervision**: Essential for leveraging unlabeled IMU data to improve generalization; Quick check: Evaluate performance improvement when increasing the amount of unlabeled data
- **Few-shot learning adaptation**: Critical for fine-tuning with minimal labeled data; Quick check: Measure performance degradation as labeled data decreases
- **Cross-dataset generalization**: Important for real-world deployment across different sensor configurations; Quick check: Test performance on held-out datasets with different sensor placements
- **Multimodal fusion strategies**: Necessary for combining information from IMU, video, and text effectively; Quick check: Compare different fusion approaches (early vs. late fusion)

## Architecture Onboarding

**Component Map:**
IMU Sensor Data -> Encoder Network -> Multimodal Projector -> Contrastive Loss (IMU-Video)
IMU Sensor Data -> Encoder Network -> Text Projector -> Alignment Loss (IMU-Text)
IMU Sensor Data -> Encoder Network -> Nearest-Neighbor Module -> Self-Supervision Loss
Fine-tuned Encoder -> Classifier -> Activity Recognition

**Critical Path:**
The critical path involves the encoder network processing IMU data, followed by parallel projections to video and text spaces for multimodal alignment, while simultaneously computing nearest-neighbor relationships for self-supervision. During fine-tuning, the pretrained encoder feeds into a classifier for the specific activity recognition task.

**Design Tradeoffs:**
- Multimodal pretraining vs. computational overhead: Adding video and text modalities increases pretraining time but improves representation quality
- Nearest-neighbor supervision parameter selection: Requires careful tuning of similarity thresholds and neighbor counts
- Encoder architecture complexity: Deeper networks may capture more complex patterns but risk overfitting with limited labeled data
- Fine-tuning strategy: Full fine-tuning vs. feature extraction impacts final performance and computational requirements

**Failure Signatures:**
- Poor alignment between IMU and video embeddings indicates issues with temporal synchronization or contrastive learning parameters
- Degraded performance on out-of-domain datasets suggests overfitting to pretraining data distribution
- Instability during pretraining may result from improper temperature scaling in contrastive loss
- Suboptimal nearest-neighbor supervision effectiveness could indicate poor embedding quality or inappropriate similarity metrics

**First Experiments:**
1. Ablation study removing each supervision component (self-supervision, multimodal, nearest-neighbor) to quantify individual contributions
2. Sensitivity analysis of key hyperparameters including temperature in contrastive loss and neighbor count in nearest-neighbor supervision
3. Cross-modal transfer test applying pretrained encoder to different sensor types (e.g., EMG, PPG) to assess generalization beyond IMU

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Performance generalization to entirely different sensor types beyond IMU (e.g., physiological signals, environmental sensors) remains untested
- Scalability of nearest-neighbor supervision to significantly larger unlabeled datasets is unclear
- Computational overhead during both pretraining and fine-tuning phases needs comprehensive evaluation
- Claims of data efficiency are based on comparisons to specific baselines and may not hold against emerging few-shot learning methods

## Confidence
- **High confidence**: In-domain and cross-dataset performance improvements over established baselines, data efficiency claims with provided baselines
- **Medium confidence**: Multimodal self-supervision benefits, nearest-neighbor supervision effectiveness
- **Low confidence**: Cross-modal generalization to non-IMU sensors, scalability to orders-of-magnitude larger datasets

## Next Checks
1. Test PRIMUS transfer learning capabilities on non-IMU sensor modalities such as EMG or PPG signals to assess true cross-modal generalization
2. Evaluate computational overhead and scalability by running pretraining on datasets 10x larger than those tested, measuring both training time and memory requirements
3. Compare PRIMUS against recent few-shot learning approaches and foundation models from other domains using identical data budgets and evaluation protocols