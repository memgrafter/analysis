---
ver: rpa2
title: Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with
  Recurrent Message Passing
arxiv_id: '2402.05027'
source_url: https://arxiv.org/abs/2402.05027
tags:
- graph
- node
- graphs
- learning
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles generalizability challenges in multi-agent reinforcement
  learning on graphs, specifically the trade-off between local observability and communication
  overhead. The authors propose a novel recurrent message-passing approach where nodes
  iteratively exchange information via message passing to build a global graph representation.
---

# Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing

## Quick Facts
- arXiv ID: 2402.05027
- Source URL: https://arxiv.org/abs/2402.05027
- Authors: Jannis Weil; Zhenghua Bao; Osama Abboud; Tobias Meuser
- Reference count: 40
- One-line primary result: Novel recurrent message-passing approach achieves comparable throughput to graph-specific agents on 1000 diverse graphs when combined with action masking

## Executive Summary
This paper addresses the challenge of generalizing multi-agent reinforcement learning across diverse graph structures by decoupling graph representation learning from decision-making. The proposed method uses recurrent message passing where nodes iteratively exchange information to build a global graph representation, which is then used to generate learned observations for agents based on their position. Evaluated on a routing environment with 1000 diverse graphs, the approach achieves performance comparable to agents specialized on single graphs while enabling real-time adaptation to graph changes without retraining.

## Method Summary
The method combines recurrent graph neural networks with multi-agent reinforcement learning by having nodes iteratively exchange information through message passing over multiple environment steps. Each node maintains a hidden state that is updated via LSTM-based recurrent updates using information from neighbors. The graph neural network learns to aggregate this information into a global representation, from which agents receive graph observations based on their location. This decoupled approach separates graph representation learning from the agent's decision-making policy, enabling generalization across different graph structures.

## Key Results
- Achieved comparable throughput to agents specialized on single graphs when combined with action masking
- Demonstrated ability to adapt to graph changes in real-time without retraining
- Showed that recurrent approaches converge faster than non-recurrent ones in shortest paths regression tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Recurrent message passing enables agents to build a global view of the graph without full observability.
- **Mechanism:** Nodes iteratively exchange local information via message passing over multiple environment steps. This information flows through the graph edges, allowing distant nodes to indirectly share information. Agents receive learned graph observations based on their location, providing a richer view than their immediate neighborhood.
- **Core assumption:** The graph structure allows information to propagate through edges during the available message passing iterations.
- **Evidence anchors:** [abstract] "nodes to create a global representation of the graph by exchanging messages with their neighbors"; [section 3.1] "nodes send their state â„Žð‘£ to all direct neighbors ð‘¤ âˆˆ ð‘ (ð‘£)"

### Mechanism 2
- **Claim:** Decoupling graph representation learning from decision-making improves generalizability across different graphs.
- **Mechanism:** The graph neural network learns to aggregate and represent graph structure independently of the reinforcement learning agent's policy. This separation allows the agent to focus on decision-making while leveraging pre-learned graph representations.
- **Core assumption:** Graph representations can be learned that are useful across diverse graph structures.
- **Evidence anchors:** [abstract] "decouple learning graph-specific representations and control by separating node and agent observations"; [section 3.1] "nodes exchange local information via message passing to improve their understanding of the global state"

### Mechanism 3
- **Claim:** Recurrent graph neural networks with backpropagation through time learn more stable update functions than feed-forward approaches.
- **Mechanism:** By unrolling the message passing over multiple environment steps and applying backpropagation through time, the model learns stable node state update functions that maintain information across steps.
- **Core assumption:** Backpropagation through time provides better gradients than single-step updates for learning recurrent message passing.
- **Evidence anchors:** [section 3.3] "as node states are updated over multiple environment steps, we have to unroll the node state update over a sequence of steps and apply backpropagation through time"; [section 5.1] "recurrent approaches converge faster to a low loss value than the non-recurrent ones"

## Foundational Learning

- **Graph Neural Networks**
  - Why needed here: GNNs provide the mechanism for nodes to exchange information and build graph representations. The message passing framework is essential for distributing local information throughout the graph.
  - Quick check question: How does the number of message passing iterations relate to the graph diameter and information propagation?

- **Multi-Agent Reinforcement Learning**
  - Why needed here: MARL provides the framework for agents to learn policies based on their observations. The partial observability and decentralized execution are core challenges this work addresses.
  - Quick check question: What are the key differences between centralized and decentralized MARL approaches in terms of scalability and reactivity?

- **Recurrent Neural Networks**
  - Why needed here: RNNs enable the model to maintain state across environment steps, allowing information to persist and propagate through the graph over time.
  - Quick check question: How does backpropagation through time work in the context of unrolled message passing iterations?

## Architecture Onboarding

- **Component map:** Node observations -> Node State Encoder (FCN+LSTM) -> Message Passing Layer (iterative neighbor aggregation) -> Readout Function (graph observations) -> Agent Policy (DQN)

- **Critical path:**
  1. Environment provides node observations to each node
  2. Nodes encode observations and update states via recurrent message passing
  3. Readout function generates graph observations for agents
  4. Agents select actions based on observations
  5. Environment steps forward, new observations are generated

- **Design tradeoffs:**
  - Number of message passing iterations vs. communication overhead
  - Unroll depth for BPTT vs. training stability and computational cost
  - Graph neural network architecture complexity vs. generalization capability
  - Agent architecture complexity vs. learning efficiency

- **Failure signatures:**
  - Poor generalization: Test performance significantly worse than training performance
  - Routing loops: Packets never reach destinations within episode limits
  - Communication bottlenecks: High variance in node state updates indicating information flow issues
  - Unstable training: High variance in validation loss across seeds

- **First 3 experiments:**
  1. Implement the simplest version with GraphSAGE (non-recurrent) and DQN agent on a single fixed graph to verify basic functionality
  2. Add recurrent message passing with GCRN-LSTM and compare performance on the same fixed graph
  3. Train on randomly generated graphs and evaluate generalization on held-out test graphs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do recurrent graph neural networks (GNNs) like GCRN-LSTM and the proposed architecture converge faster than non-recurrent approaches like GraphSAGE and A-DGN in the shortest paths regression task?
- Basis in paper: [explicit] The paper states that recurrent approaches converge faster and achieve similar validation losses, though the high standard deviation of GCRN-LSTM in the beginning is caused by one of the three runs where the validation loss does not decrease initially.
- Why unresolved: The paper hypothesizes that better gradients from computing separate losses for each message passing iteration when unrolling the network might be the cause, but this is not confirmed. Further experiments in different environments would be required to verify this.
- What evidence would resolve it: Controlled experiments varying the number of message passing iterations and unroll depths across different tasks and datasets to isolate the effect of recurrent computation on convergence speed.

### Open Question 2
- Question: Why do non-recurrent GNN architectures like GraphSAGE and A-DGN perform poorly in the reinforcement learning setting with graph observations, despite achieving reasonable performance in the supervised shortest paths regression task?
- Basis in paper: [explicit] The paper notes that both non-recurrent approaches perform poorly in the RL setting, with GraphSAGE failing to learn even with larger batch sizes and jumping knowledge networks. The authors hypothesize that backpropagation through time from targets provided by the RL algorithm might facilitate learning for recurrent approaches.
- Why unresolved: The paper does not provide conclusive evidence for why non-recurrent approaches fail in the RL setting, only speculation about the role of backpropagation through time.
- What evidence would resolve it: Comparative experiments training non-recurrent GNNs with and without backpropagation through time in both supervised and RL settings to determine if the temporal credit assignment is the key factor.

### Open Question 3
- Question: How does the number of message passing iterations (K) and unroll depth (J) affect the performance and communication overhead of the proposed recurrent message passing approach in different graph structures and sizes?
- Basis in paper: [explicit] The paper shows that K=8 message passing iterations allow information to traverse over 99% of node pairs in the test graphs, and that higher K values improve predictions at the cost of increased communication overhead. For recurrent approaches, K=1 and J=8 are chosen as a compromise between performance, stability, and overhead.
- Why unresolved: While the paper provides some empirical results for specific graph sizes and structures, it does not systematically explore the trade-off between K, J, performance, and communication overhead across a wide range of graph topologies and sizes.
- What evidence would resolve it: Comprehensive experiments varying K and J on graphs with different diameters, node degrees, and structures, measuring both task performance and communication overhead to identify optimal configurations for different graph characteristics.

## Limitations
- Evaluation relies on a single synthetic routing environment with controlled graph generation parameters (20 nodes, degree 3), limiting generalizability to real-world graph structures
- Paper does not address computational complexity scaling with larger graphs or higher-degree nodes
- Action masking mechanism to prevent routing loops is mentioned but not fully specified

## Confidence
- **High confidence:** The effectiveness of recurrent message passing for building graph representations that improve agent observations
- **Medium confidence:** The claim of achieving comparable performance to graph-specific agents
- **Low confidence:** The scalability and real-time adaptation claims

## Next Checks
1. Evaluate the approach on diverse real-world graph datasets (e.g., social networks, road networks) with varying diameters and degree distributions to test true generalizability beyond the synthetic routing environment
2. Conduct a scalability analysis measuring training and inference time as graph size increases from 20 to 100+ nodes, and assess memory usage for storing node states across multiple message passing iterations
3. Implement and test the system in a dynamic graph environment where edge capacities and node availability change during episodes, measuring both adaptation speed and performance degradation compared to static retraining approaches