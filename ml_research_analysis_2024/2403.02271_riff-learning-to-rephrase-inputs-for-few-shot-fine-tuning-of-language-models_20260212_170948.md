---
ver: rpa2
title: 'RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models'
arxiv_id: '2403.02271'
source_url: https://arxiv.org/abs/2403.02271
tags:
- paraphrase
- language
- paraphrases
- learning
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RIFF, a method for enhancing few-shot fine-tuning
  of language models by incorporating paraphrased input text. The approach involves
  training a smaller paraphrase generator using a Maximum Marginal Likelihood (MML)
  objective to generate diverse paraphrases of the original input.
---

# RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models

## Quick Facts
- arXiv ID: 2403.02271
- Source URL: https://arxiv.org/abs/2403.02271
- Authors: Saeed Najafi; Alona Fyshe
- Reference count: 33
- One-line primary result: RIFF significantly improves few-shot fine-tuning accuracy by augmenting training data with diverse, semantically faithful paraphrases.

## Executive Summary
RIFF is a method for enhancing few-shot fine-tuning of language models by incorporating paraphrased input text. The approach involves training a smaller paraphrase generator using a Maximum Marginal Likelihood (MML) objective to generate diverse paraphrases of the original input. These paraphrases are then used to augment the training data and improve the performance of various parameter-efficient fine-tuning techniques like prompt optimization and LoRA. Experiments on six text classification datasets demonstrate that RIFF significantly improves the accuracy of these techniques compared to using the original input alone. The method also enhances the robustness of the model to different phrasings of the same input.

## Method Summary
RIFF trains a paraphrase generator (T5-base) using Maximum Marginal Likelihood (MML) with KL-penalized on-policy learning to produce diverse paraphrases that maintain semantic fidelity. During few-shot fine-tuning, these paraphrases augment the training data for downstream classifiers (RoBERTa-large) using parameter-efficient methods like LoRA or prompt tuning. At inference, predictions are averaged across the original input and its paraphrases to improve robustness. The method is evaluated across six text classification datasets with 16-128 shot settings, showing consistent improvements over baseline PEFT techniques.

## Key Results
- RIFF improves few-shot classification accuracy across six text classification datasets compared to standard PEFT methods.
- The paraphrase augmentation increases semantic similarity between generated paraphrases and original inputs while reducing hallucination.
- Ensemble inference using original and paraphrased inputs enhances model robustness to different phrasings of the same input.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Paraphrasing during training increases the effective diversity of training examples, allowing the model to learn more robust representations.
- **Mechanism:** By generating multiple paraphrases for each input, the training set is augmented with semantically equivalent but lexically diverse examples. This exposes the model to multiple surface forms of the same underlying meaning, improving its ability to generalize to unseen phrasings during inference.
- **Core assumption:** Paraphrases preserve the semantic label while introducing syntactic/lexical variation. The model can learn to map different surface forms to the same class label.
- **Evidence anchors:**
  - [abstract]: "enriching data with paraphrases at train and test time enhances the performance beyond what can be achieved with parameter-efficient fine-tuning alone"
  - [section 3.3]: "RIFF has contributed to higher semantic similarity compared to the original input" and "a higher F C score... verifies that the RIFF objective has reduced hallucination in the generated paraphrases"
  - [corpus]: Weak - corpus neighbors focus on related PEFT and rephrasing methods but do not directly validate semantic preservation.
- **Break condition:** If paraphrases introduce noise or change the meaning, the model may learn incorrect mappings, degrading performance.

### Mechanism 2
- **Claim:** The RIFF training objective reduces hallucination in generated paraphrases, leading to more reliable augmentations.
- **Mechanism:** The Marginal Maximum Likelihood (MML) objective with KL-penalized on-policy learning encourages paraphrases that maintain semantic fidelity to the original input. The KL penalty between the current and pre-trained paraphrase distributions discourages generation of ungrammatical or factually inconsistent text.
- **Core assumption:** The pre-trained paraphrase model provides a reasonable prior, and the MML + KL objective can steer fine-tuning toward preserving original meaning while allowing lexical variation.
- **Evidence anchors:**
  - [section 3.3]: "The perplexity of the generated paraphrases after fine-tuning with our objective is still low, demonstrating the grammatical accuracy" and "A higher F C score... verifies that the RIFF objective has reduced hallucination"
  - [section 2.1]: "The entropy regularization aids in the diverse exploration of the search space, while the grammar reward discourages the learning of ungrammatical samples"
  - [corpus]: Weak - no direct corpus evidence on hallucination reduction.
- **Break condition:** If the KL penalty is too strong, it may prevent useful lexical variation; if too weak, hallucinations may persist.

### Mechanism 3
- **Claim:** Ensemble inference using original and paraphrased inputs improves robustness to input phrasing variations.
- **Mechanism:** At test time, predictions are averaged across the original input and its paraphrases. This smooths out errors that might occur if the model is sensitive to specific phrasings, as different paraphrases may be easier for the model to classify correctly.
- **Core assumption:** Some paraphrases of an input are easier for the model to classify than others, and averaging predictions reduces variance in classification accuracy.
- **Evidence anchors:**
  - [abstract]: "incorporating ensemble predictions based on input paraphrases... in concert with prompt optimization and efficient tuning techniques"
  - [section 3.5]: "Our paraphrase augmentation during training increases paraphrase robustness" and "SPTune observes more significant improvement in robustness from paraphrase augmentation during training"
  - [corpus]: Weak - corpus neighbors do not address ensemble inference or robustness.
- **Break condition:** If paraphrases are of low quality or the model is not sensitive to phrasing, ensemble averaging may not improve and could even degrade performance.

## Foundational Learning

- **Concept:** Reinforcement Learning for Text Generation
  - Why needed here: RIFF uses RL-style objectives (MML and PG) to fine-tune the paraphrase generator with feedback from the downstream classifier.
  - Quick check question: What is the difference between MML and PG gradient estimation in the context of text generation?
- **Concept:** Parameter-Efficient Fine-Tuning (PEFT) Methods
  - Why needed here: RIFF is evaluated in combination with various PEFT techniques (LoRA, prompt tuning, etc.) to show its compatibility and benefits.
  - Quick check question: How does LoRA differ from full fine-tuning in terms of parameters updated?
- **Concept:** Data Augmentation via Paraphrasing
  - Why needed here: The core idea is to use paraphrases as a form of data augmentation to improve few-shot learning performance.
  - Quick check question: Why might paraphrasing be more effective than other forms of data augmentation for few-shot learning?

## Architecture Onboarding

- **Component map:** Pre-trained T5-base paraphrase generator -> Paraphrase fine-tuning with RIFF -> Downstream RoBERTa-large classifier -> Ensemble prediction
- **Critical path:** Paraphrase generation -> Data augmentation -> Model training -> Ensemble prediction
- **Design tradeoffs:**
  - MML vs PG: MML is more robust but may be slower to train; PG can diverge if not carefully controlled.
  - On-policy vs Off-policy: On-policy is simpler but risks generating degenerate text; off-policy is more stable but requires maintaining a fixed sampler.
  - KL penalty weight: Too high prevents useful variation; too low allows hallucinations.
- **Failure signatures:**
  - Degenerate paraphrases (ungrammatical, nonsensical) -> check KL penalty and reward normalization
  - No performance gain over baseline -> verify paraphrase quality and semantic preservation
  - Training instability -> check reward normalization and gradient estimation method
- **First 3 experiments:**
  1. Fine-tune paraphrase generator with MML + KL penalty on a small dataset; evaluate paraphrase quality (grammar, semantic similarity, factual consistency).
  2. Train downstream classifier with PEFT (e.g., LoRA) using augmented data from step 1; compare to baseline without augmentation.
  3. Implement ensemble inference; evaluate robustness to phrasing variations by testing on original and paraphrased inputs.

## Open Questions the Paper Calls Out
- None explicitly called out in the paper.

## Limitations
- The paper only experiments with English text classification datasets, so there's no empirical evidence of RIFF's performance on non-English languages or languages with significantly different syntactic structures.
- The computational overhead of generating paraphrases for each input during both training and inference is not thoroughly analyzed, which could be a limiting factor for deployment in resource-constrained settings.
- The reliance on a pre-trained paraphrase generator (T5-base) raises questions about the generalizability of the approach to domains or languages where such models may not perform well.

## Confidence

- **Mechanism 1 (Paraphrasing increases training diversity):** Medium confidence. The paper provides empirical evidence of improved accuracy with paraphrase augmentation, but the ablation studies are limited, and the exact contribution of diversity versus other factors (e.g., semantic preservation) is not fully disentangled.
- **Mechanism 2 (RIFF objective reduces hallucination):** Low confidence. While the paper reports metrics like perplexity and factual consistency scores, these are measured on generated paraphrases rather than on downstream task performance, and the direct impact on classification accuracy is not clearly established.
- **Mechanism 3 (Ensemble inference improves robustness):** Medium confidence. The paper demonstrates improved robustness to phrasing variations, but the ensemble method assumes that at least some paraphrases will be easier for the model to classify, which may not hold for all inputs or models.

## Next Checks

1. **Ablation Study on Paraphrase Quality:** Conduct a controlled experiment where paraphrases are filtered by quality (e.g., only using those with high semantic similarity or low hallucination scores) to isolate the effect of paraphrase quality on downstream performance.
2. **Generalization to Other Tasks:** Apply RIFF to a non-classification task (e.g., sentiment analysis or natural language inference) to assess whether the benefits of paraphrase augmentation generalize beyond the tested text classification datasets.
3. **Computational Overhead Analysis:** Measure and report the wall-clock time and memory usage for generating paraphrases and performing ensemble inference, comparing it to the baseline PEFT methods to quantify the practical cost of RIFF.