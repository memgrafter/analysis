---
ver: rpa2
title: Hierarchical Meta-Reinforcement Learning via Automated Macro-Action Discovery
arxiv_id: '2412.11930'
source_url: https://arxiv.org/abs/2412.11930
tags:
- learning
- tasks
- state
- policy
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a hierarchical meta-reinforcement learning
  architecture that automatically discovers task-agnostic macro-actions to guide low-level
  policies. The method addresses the challenge of learning performant policies across
  multiple complex tasks by introducing three hierarchical layers: task representation
  learning, macro-action discovery, and primitive action learning.'
---

# Hierarchical Meta-Reinforcement Learning via Automated Macro-Action Discovery

## Quick Facts
- arXiv ID: 2412.11930
- Source URL: https://arxiv.org/abs/2412.11930
- Reference count: 37
- Primary result: Achieves state-of-the-art performance on MetaWorld ML10 benchmark with average training success rates of 0.64±0.08

## Executive Summary
This paper introduces a hierarchical meta-reinforcement learning architecture that automatically discovers task-agnostic macro-actions to guide low-level policies. The method addresses the challenge of learning performant policies across multiple complex tasks by introducing three hierarchical layers: task representation learning, macro-action discovery, and primitive action learning. The approach prevents forgetting previously learned behaviors while acquiring new tasks and achieves state-of-the-art performance on the MetaWorld ML10 benchmark.

## Method Summary
The proposed method employs a tri-level hierarchical architecture with independently tailored training schemes for each layer. The high-level layer learns task representations using recurrent units and categorical networks. The intermediate layer discovers task-agnostic macro-actions through a modified variational autoencoder with imputation techniques. The low-level layer executes primitive actions conditioned on these macro-actions using PPO. The state space is decomposed into ego-state (self-state elements) and other-state (task-specific elements), enabling task-agnostic macro-action discovery that connects current states to goal states.

## Key Results
- Achieves average training success rates of 0.64±0.08 compared to 0.42±0.09 for previous best method
- Demonstrates similar improvements in test performance and cumulative returns
- Shows faster convergence and better adaptation to new tasks than baseline methods

## Why This Works (Mechanism)

### Mechanism 1
Macro-actions guide low-level policies by providing directional action vectors that prevent forgetting previously learned behaviors while learning new tasks. The intermediate layer uses a modified VAE with imputations to discover task-agnostic macro-actions that connect current states to goal states. These macro-actions are then conditioned on the low-level policy, allowing it to fill in detailed low-level controls along the given direction.

### Mechanism 2
The tri-level hierarchical architecture mitigates prospective instability by having independently tailored training schemes for each layer. Each hierarchical layer has an independent role in decomposition, preventing gradient overflow between layers and allowing each layer to focus on its specific task.

### Mechanism 3
The ego-state concept enables task-agnostic macro-action discovery by focusing on self-state elements while removing task-specific components. The state space is decomposed into ego-state and other-state, with the macro-action discovery process using only the ego-state, making the macro-actions task-agnostic and re-composable across different tasks.

## Foundational Learning

- Concept: Variational Autoencoder (VAE) and its modifications
  - Why needed here: The modified VAE with imputations is crucial for discovering task-agnostic macro-actions that connect current states to goal states
  - Quick check question: How does the modified VAE with imputations differ from a standard VAE in terms of its objective function and input/output?

- Concept: Hierarchical Reinforcement Learning (HRL)
  - Why needed here: The tri-level hierarchical architecture (task representation, macro-action discovery, and primitive action learning) is the core of the proposed method
  - Quick check question: What are the three hierarchical levels in this architecture, and what is the specific role of each level?

- Concept: Meta-Reinforcement Learning (Meta-RL)
  - Why needed here: The method aims to enable fast adaptation to new tasks by leveraging previously acquired knowledge across multiple tasks
  - Quick check question: How does Meta-RL differ from standard multi-task RL in terms of the objective function and training process?

## Architecture Onboarding

- Component map: State → Task Representation → Macro-Action → Primitive Action → Environment
- Critical path: Current state flows through task representation to generate macro-actions, which guide the low-level policy to execute primitive actions
- Design tradeoffs:
  - Complexity vs. performance: The tri-level hierarchy adds complexity but provides better performance and faster adaptation
  - Task-specific vs. task-agnostic: Balancing between learning task-specific information and maintaining task-agnostic macro-actions
- Failure signatures:
  - If macro-actions are not task-agnostic, the policy may struggle to adapt to new tasks
  - If layers are not independently trained, the "curse of hierarchy" may cause instability
  - If ego-state decomposition is incorrect, macro-actions may lose essential information
- First 3 experiments:
  1. Test macro-action discovery with a simple environment (e.g., grid world) to verify task-agnostic properties
  2. Compare performance with and without ego-state decomposition on a multi-task benchmark
  3. Validate the effectiveness of independent training schemes by testing with and without gradient overflow between layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the macro-action discovery layer perform when scaled to tasks with significantly longer horizons or more complex state-action spaces?
- Basis in paper: [inferred] The paper mentions that the method is particularly effective for "complex, sparse, long-horizon problems" but does not explore performance on tasks with significantly longer horizons or more complex state-action spaces than MetaWorld ML10
- Why unresolved: The experimental validation is limited to the MetaWorld ML10 benchmark, which may not fully capture the scalability of the approach to more complex environments
- What evidence would resolve it: Experiments on benchmarks with longer horizons or more complex state-action spaces (e.g., MetaWorld ML45 or other robotic control tasks) demonstrating consistent performance improvements

### Open Question 2
- Question: What is the impact of different goal state generation strategies (CD, CM, ST) on the learning efficiency and final performance in environments with varying levels of task complexity?
- Basis in paper: [explicit] The paper introduces three goal state generation strategies (Constant Discretization, Constant Margins, Adaptive at Sub-Task) and notes that they do not exhibit significant benefits over each other, but does not explore their impact on learning efficiency or performance in environments with varying task complexity
- Why unresolved: The paper only evaluates these strategies within the MetaWorld ML10 benchmark, which may not fully capture their potential advantages or disadvantages in different task complexities
- What evidence would resolve it: Comparative experiments on benchmarks with varying task complexities, analyzing the learning curves and final performance for each goal state generation strategy

### Open Question 3
- Question: How does the proposed method handle tasks with non-stationary dynamics or changing reward structures over time?
- Basis in paper: [inferred] The paper focuses on multi-task learning within a fixed distribution of tasks but does not address the scenario where task dynamics or reward structures change over time
- Why unresolved: The experimental setup assumes a fixed distribution of tasks, and the paper does not discuss the method's adaptability to non-stationary environments
- What evidence would resolve it: Experiments in environments with non-stationary dynamics or changing reward structures, demonstrating the method's ability to adapt and maintain performance over time

## Limitations
- The ego-state decomposition requires careful engineering and may not generalize well to domains where task-specific and task-agnostic components are not clearly separable
- The method's performance on more diverse and complex environments remains untested
- The scalability to more than three hierarchical layers or very large state spaces is uncertain

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Macro-action discovery mechanism | Medium |
| Hierarchical architecture effectiveness | High |
| Preventing forgetting through macro-actions | Medium |
| Scalability to complex environments | Low |

## Next Checks

1. Conduct experiments with varying levels of state space noise and missing information to test the robustness of the ego-state decomposition and imputation techniques
2. Perform systematic ablation studies on the number of hierarchical layers to determine the optimal depth and identify the point of diminishing returns
3. Evaluate performance on heterogeneous task distributions where the proportion of task-specific vs task-agnostic components varies significantly across tasks