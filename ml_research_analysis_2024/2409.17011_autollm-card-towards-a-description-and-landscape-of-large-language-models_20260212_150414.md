---
ver: rpa2
title: 'AutoLLM-CARD: Towards a Description and Landscape of Large Language Models'
arxiv_id: '2409.17011'
source_url: https://arxiv.org/abs/2409.17011
tags:
- data
- text
- language
- relation
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents AUTO LLM-CARD, a method for automatically generating
  large language model (LLM) cards from academic papers. The approach uses Named Entity
  Recognition (NER) and Relation Extraction (RE) to extract key information including
  model names, licenses, and applications from 106 papers.
---

# AutoLLM-CARD: Towards a Description and Landscape of Large Language Models

## Quick Facts
- arXiv ID: 2409.17011
- Source URL: https://arxiv.org/abs/2409.17011
- Authors: Shengwei Tian; Lifeng Han; Goran Nenadic
- Reference count: 34
- Primary result: Achieved 92% accuracy, 88% recall, 90% precision, and 89% F1-score in automatically extracting LLM metadata from academic papers

## Executive Summary
This paper presents AUTO LLM-CARD, a method for automatically generating large language model (LLM) cards from academic papers using Named Entity Recognition (NER) and Relation Extraction (RE). The approach processes 106 academic papers to extract key information including model names, licenses, and applications through dependency parsing and dictionary-based keyword matching. The extracted information is visualized using knowledge graphs, enabling researchers to efficiently access LLM metadata. The method demonstrates high accuracy in identifying and extracting relationships related to LLMs, addressing the challenge of information overload in the rapidly growing field of LLMs.

## Method Summary
AUTO LLM-CARD uses a multi-step process to extract LLM metadata from academic papers. First, text is extracted from PDF papers and preprocessed using spaCy for tokenization and POS tagging. Three dictionaries (LLM names, licenses, applications) are used to identify candidate sentences containing relevant information. Dependency parsing then identifies syntactic relationships between entities in these sentences, with predefined rules extracting relational triples. Manual review validates the extracted relationships, creating a high-quality dataset for training relation extraction models. The final step constructs and visualizes knowledge graphs from the validated relationships using NetworkX and PyVis.

## Key Results
- Achieved 92% accuracy, 88% recall, 90% precision, and 89% F1-score on test dataset
- Successfully extracted model names, licenses, and applications from 106 academic papers
- Processed 11,051 sentences through dictionary lookup, with 129 sentences containing license information and 106 sentences containing application information manually validated
- Generated knowledge graphs that enable efficient access to LLM metadata for researchers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dependency parsing accurately extracts relational triples (subject-predicate-object) from sentences describing LLM metadata.
- Mechanism: Dependency parsing identifies grammatical roles (nsubj, dobj, prep, etc.) and uses predefined rules to construct triples linking model names to licenses and applications.
- Core assumption: Sentences describing LLM metadata follow consistent syntactic patterns that dependency parsing can reliably capture.
- Evidence anchors:
  - [abstract] "We use Named Entity Recognition (NER) and Relation Extraction (RE) methods that automatically extract key information about LLMs from the papers"
  - [section] "Dependency parsing reveals the syntactic relationships between words in a sentence, allowing us to extract explicit relational triples"
  - [corpus] Weak evidence - corpus contains related papers on LLM information extraction but no direct validation of dependency parsing accuracy
- Break condition: If sentence structures become highly complex, nested, or use non-standard phrasing that dependency rules cannot handle.

### Mechanism 2
- Claim: Dictionary-based keyword matching combined with dependency parsing efficiently identifies relevant sentences from large corpora.
- Mechanism: Predefined dictionaries of LLM names, licenses, and applications guide initial sentence selection, then dependency parsing extracts precise relationships from these candidate sentences.
- Core assumption: Most LLM-related information appears in sentences containing at least one dictionary term.
- Evidence anchors:
  - [section] "We processed 106 academic papers by defining three dictionaries â€“ LLM's name, licence, and application"
  - [section] "11,051 sentences were extracted through dictionary lookup"
  - [corpus] Weak evidence - corpus shows related work on keyword-based information extraction but no specific validation of this approach
- Break condition: If important information appears in sentences without dictionary terms or if dictionary terms appear in irrelevant contexts.

### Mechanism 3
- Claim: Manual review of automatically extracted sentences ensures high-quality training data for relation extraction models.
- Mechanism: Human reviewers validate the accuracy of automatically extracted sentences containing license-application relationships, creating a high-quality dataset for model training.
- Core assumption: Manual review can effectively filter out false positives and improve dataset quality.
- Evidence anchors:
  - [section] "dataset was constructed through manual review of the final selection of 129 sentences with a link between the name and the licence, and 106 sentences with a link between the model name and the application"
  - [section] "evaluation results indicate that the method has a high degree of accuracy and consistency in identifying and extracting relationships related to LLMs"
  - [corpus] Weak evidence - corpus contains related papers on dataset curation but no direct validation of manual review effectiveness
- Break condition: If manual review becomes too time-consuming or inconsistent, reducing dataset quality and scalability.

## Foundational Learning

- Concept: Dependency parsing and syntactic dependencies
  - Why needed here: Forms the core mechanism for extracting structured relationships from unstructured text
  - Quick check question: What dependency relation would you expect between a model name and its application in a sentence like "BERT excels at text classification"?

- Concept: Named Entity Recognition (NER)
  - Why needed here: Identifies and classifies specific entities (model names, licenses, applications) in text
  - Quick check question: How would NER handle the phrase "Apache 2.0 license" - would it recognize "Apache 2.0" as a single license entity?

- Concept: Knowledge graph construction and visualization
  - Why needed here: Transforms extracted relationships into visual, queryable structures for research discovery
  - Quick check question: What type of graph structure would best represent the relationships between models, their licenses, and applications?

## Architecture Onboarding

- Component map:
  PDF text extraction -> Multi-source text processing -> Keyword dictionary lookup -> Dependency parsing -> Manual review -> Knowledge graph construction -> Visualization

- Critical path:
  1. Extract text from PDF papers
  2. Apply dictionary lookup to find candidate sentences
  3. Run dependency parsing on candidate sentences
  4. Manual review to validate extracted relationships
  5. Construct knowledge graph from validated relationships

- Design tradeoffs:
  - Accuracy vs scalability: Manual review ensures high accuracy but limits processing speed
  - Coverage vs precision: Broad dictionary terms increase coverage but may introduce noise
  - Complexity vs interpretability: Simple dependency rules are interpretable but may miss complex relationships

- Failure signatures:
  - High false positive rate in keyword matching
  - Inconsistent dependency parsing results across different sentence structures
  - Manual review backlog indicating processing bottlenecks
  - Visualization that doesn't reflect actual relationships due to extraction errors

- First 3 experiments:
  1. Test dictionary lookup on a small set of papers to measure recall of relevant sentences
  2. Validate dependency parsing accuracy on manually annotated sentences
  3. Evaluate knowledge graph construction by querying known relationships

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the AUTO LLM-CARD methodology be adapted to handle emerging licensing frameworks and new LLM applications that are not covered in the current dictionaries?
- Basis in paper: [explicit] The paper mentions defining three dictionaries for LLM names, licenses, and applications, but notes that future work should explore adapting the method to handle new and emerging frameworks.
- Why unresolved: The current dictionaries may become outdated as new models and applications emerge, and the methodology does not specify how to dynamically update these dictionaries.
- What evidence would resolve it: Demonstrating the methodology's ability to automatically update dictionaries with new LLM names, licenses, and applications without manual intervention.

### Open Question 2
- Question: What are the limitations of dependency parsing in accurately extracting relationships between LLM names and their applications or licenses in complex sentence structures?
- Basis in paper: [inferred] The paper discusses the use of dependency parsing and its performance metrics but does not delve into its limitations in handling complex or ambiguous sentence structures.
- Why unresolved: Dependency parsing may struggle with sentences that have nested or implicit relationships, which could affect the accuracy of extracted information.
- What evidence would resolve it: A detailed analysis of dependency parsing errors in various sentence structures and the development of enhanced parsing rules or models to address these issues.

### Open Question 3
- Question: How can the AUTO LLM-CARD methodology be scaled to process a larger corpus of academic papers beyond the initial 106 papers?
- Basis in paper: [explicit] The paper processes 106 academic papers and mentions the need for scaling the methodology for larger datasets in future work.
- Why unresolved: The current methodology's scalability is not fully tested, and it is unclear how it will perform with a significantly larger volume of data.
- What evidence would resolve it: Successful implementation of the methodology on a dataset of at least 1,000 academic papers, demonstrating consistent performance and efficiency.

## Limitations

- The methodology lacks detailed documentation about dictionary contents and specific rule definitions, making exact reproduction difficult
- Manual review process introduces potential subjectivity and scalability concerns while limiting processing speed
- Evaluation metrics are based on a relatively small test dataset that may not represent the diversity of sentence structures across broader literature

## Confidence

- Medium confidence in the core claims about method effectiveness, as results are promising but methodology lacks sufficient detail for complete verification
- Low confidence in the generalizability of dependency parsing approach to papers with varying writing styles or non-standard phrasings
- Medium confidence in knowledge graph visualization utility, though quantitative measures of relationship completeness are lacking

## Next Checks

1. Test the dictionary lookup approach on a held-out set of papers to measure recall of relevant sentences and evaluate false positive rates
2. Validate dependency parsing accuracy by comparing extracted relationships against manually annotated gold standard sentences from diverse paper sources
3. Assess knowledge graph construction by querying known relationships and measuring completeness against comprehensive LLM databases