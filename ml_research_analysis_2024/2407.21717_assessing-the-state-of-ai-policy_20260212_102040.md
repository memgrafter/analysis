---
ver: rpa2
title: Assessing the State of AI Policy
arxiv_id: '2407.21717'
source_url: https://arxiv.org/abs/2407.21717
tags:
- policy
- assurance
- intelligence
- artificial
- technology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a taxonomy and landscape analysis of AI policy
  initiatives at the international, federal, state, and city levels, as well as from
  industry and professional societies. The analysis identified overlapping themes
  including responsibility, governance, coordination, assurance, ethics, development/use
  guidance, security/privacy, fairness/non-bias, education, workforce, and societal
  benefit.
---

# Assessing the State of AI Policy

## Quick Facts
- arXiv ID: 2407.21717
- Source URL: https://arxiv.org/abs/2407.21717
- Reference count: 32
- Primary result: Taxonomy and landscape analysis of AI policy initiatives at international, federal, state, city, industry, and professional society levels, identifying eleven overlapping themes and recommending education on AI assurance tenets for policymakers.

## Executive Summary
This paper provides a comprehensive taxonomy and landscape analysis of AI policy initiatives across multiple governance levels and sectors. The authors identify eleven overlapping themes including responsibility, governance, coordination, assurance, ethics, development/use guidance, security/privacy, fairness/non-bias, education, workforce, and societal benefit. While considerable progress has been made in AI policy development, the analysis reveals that policymakers face significant risks in harmonizing requirements, keeping pace with rapid technological change, and addressing technical knowledge gaps. The study concludes that policymakers need targeted education on core AI assurance concepts to facilitate U.S. competitiveness while ensuring public safety.

## Method Summary
The study employs a grey literature review methodology using open-source methods to examine AI assurance initiatives across six categories: federal, state, city, international, industry, and professional societies. The authors collected 32 references covering AI legislation, directives, standards, and initiatives, then performed qualitative coding to extract themes and identify overlaps. The analysis resulted in a consolidated taxonomy of eleven categories that provides a reference framework for understanding existing AI policies and identifying gaps for future policymaking.

## Key Results
- Identified eleven overlapping themes across AI policy initiatives at all governance levels
- Found significant overlap between federal, state, city, international, industry, and professional society initiatives
- Highlighted risks in harmonizing requirements, keeping pace with change, and addressing technical knowledge gaps
- Recommended education on AI assurance tenets as critical for effective policymaking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policymakers with minimal technical background can effectively guide AI governance if they understand the core tenets of AI assurance (explainability, safety, trustworthiness, ethics, fairness).
- Mechanism: The paper maps overlapping themes across federal, state, city, international, industry, and professional society initiatives into eleven consolidated categories (responsibility, governance, coordination/communication, assurance, ethics, development/use guidance, security/privacy, fairness/non-bias, education, workforce, benefit as a whole). This taxonomy gives non-technical legislators a common reference framework to anchor decisions.
- Core assumption: A high-level, cross-sector taxonomy is sufficient for informed policy action without deep technical expertise.
- Evidence anchors:
  - [abstract] "policymakers need education on key AI assurance tenets to facilitate U.S. competitiveness while safeguarding public safety."
  - [section] "policymakers can rely on experts, but they should have some minimal understanding of AI."
- Break condition: If AI technologies evolve faster than the taxonomy can be updated, or if core assurance concepts diverge too widely across jurisdictions, the framework loses coherence.

### Mechanism 2
- Claim: Coordinated, multi-level governance structures reduce the risk of regulatory gaps and duplication.
- Mechanism: By aligning federal strategies (e.g., EO 13859, NIST AI RMF), state initiatives (e.g., Alabama SB 78), city plans (NYC AI Strategy), international frameworks (OECD AI Policy Observatory), and industry guidelines (Google AI Principles), the paper creates a consistent vocabulary and set of objectives. This alignment enables policymakers to trace policy decisions through each level and harmonize requirements.
- Core assumption: Policy actors at each level share sufficient incentives to adopt common terminology and objectives.
- Evidence anchors:
  - [section] "Overlapping themes... include responsibility, governance, coordination, assurance, ethics, development/use guidance..."
  - [section] "policymakers face the following risks: harmonizing the requirements of existing legislation..."
- Break condition: If political incentives diverge or stakeholder priorities become irreconcilable, coordination fails and fragmentation increases.

### Mechanism 3
- Claim: Embedding professional engineers (PEs) in legislative committees ensures technical accuracy without overburdening legislators.
- Mechanism: The paper recommends that committees balance technical and non-technical legislators with external expertise, specifically highlighting the role of PEs to "take some of the responsibility." This creates a direct feedback loop from engineering practice into lawmaking, grounding abstract principles in implementable standards.
- Core assumption: PEs have sufficient practical AI assurance experience to translate complex technical issues into legislative language.
- Evidence anchors:
  - [section] "Engage the expertise of professional engineers (PE) to take some of the responsibility."
  - [section] "policymakers can rely on experts, but they should have some minimal understanding of AI."
- Break condition: If PEs lack up-to-date AI assurance training or if communication channels between engineers and policymakers break down, technical accuracy suffers.

## Foundational Learning

- Concept: AI assurance terminology (explainability, trustworthiness, fairness, robustness, transparency)
  - Why needed here: These terms recur across all six initiative categories and are the bridge between technical detail and policy language.
  - Quick check question: What is the difference between "explainability" and "transparency" in AI assurance contexts?
- Concept: Multi-level governance architecture (federal, state, city, international, industry, professional society)
  - Why needed here: The paper's overlap analysis depends on understanding how each level contributes unique themes and overlaps.
  - Quick check question: Which governance level typically focuses on global competitiveness versus local workforce development?
- Concept: Taxonomy construction and gap analysis
  - Why needed here: The eleven consolidated themes arise from systematic coding of diverse sources; understanding this method clarifies how to maintain the taxonomy over time.
  - Quick check question: How would you identify a new emerging theme that does not fit existing categories?

## Architecture Onboarding

- Component map:
  - Data layer: Raw policy texts, reports, and standards from six initiative categories
  - Processing layer: Qualitative coding to extract themes, overlap analysis
  - Taxonomy layer: Eleven consolidated categories plus sub-themes
  - Output layer: Reference guide with recommendations for future policymaking
- Critical path:
  1. Collect policy artifacts across all six categories
  2. Code for emergent themes and overlaps
  3. Consolidate into unified taxonomy
  4. Validate against real-world policy gaps
  5. Produce actionable guidance for policymakers
- Design tradeoffs:
  - Breadth vs. depth: Including more sources increases coverage but risks diluting focus
  - Technical vs. non-technical language: More technical detail improves accuracy but reduces accessibility
  - Static vs. dynamic taxonomy: A fixed taxonomy is easier to use but may become outdated
- Failure signatures:
  - Inconsistent theme definitions across categories
  - Overlaps that are not clearly distinguished
  - Missing critical themes that emerge post-publication
- First 3 experiments:
  1. Replicate the overlap analysis on a subset of recent AI policy documents to verify reproducibility
  2. Test the taxonomy with a sample of policymakers to assess clarity and usability
  3. Simulate a policy gap analysis by applying the taxonomy to a new jurisdiction's AI strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms and policies could be implemented to harmonize the diverse AI assurance requirements across different jurisdictions (international, federal, state, city) to reduce regulatory complexity and ensure consistent safety and ethical standards?
- Basis in paper: [explicit] The paper discusses the overlap and gap analysis of AI assurance initiatives across various levels of governance, highlighting the challenge of harmonizing requirements and keeping pace with technological change.
- Why unresolved: The paper identifies the challenge of harmonizing AI assurance requirements but does not provide specific mechanisms or policies to address this issue. It emphasizes the need for education and understanding but lacks concrete solutions for standardization and coordination.
- What evidence would resolve it: Specific policy proposals or case studies demonstrating successful harmonization of AI assurance requirements across different jurisdictions, including examples of standardized frameworks, collaborative governance models, or regulatory sandboxes that facilitate consistent implementation.

### Open Question 2
- Question: How can policymakers effectively bridge the technical knowledge gap to make informed decisions about AI regulation and assurance, particularly in rapidly evolving areas like frontier AI technologies?
- Basis in paper: [explicit] The paper emphasizes the need for policymakers to understand key tenets of AI assurance and highlights the risk of technical knowledge gaps limiting AI technology understanding.
- Why unresolved: While the paper stresses the importance of education for policymakers, it does not provide a detailed roadmap for how to achieve this, especially for complex and rapidly evolving AI technologies. The challenge of translating technical concepts into actionable policy guidance remains unaddressed.
- What evidence would resolve it: Development and evaluation of targeted education programs for policymakers, including curriculum design, assessment of knowledge retention, and analysis of policy outcomes based on varying levels of technical understanding. Case studies of successful policymaker engagement with technical experts in AI regulation could also provide insights.

### Open Question 3
- Question: What are the most effective strategies for ensuring accountability and transparency in AI systems, particularly in addressing issues of bias, fairness, and ethical use across different sectors and applications?
- Basis in paper: [explicit] The paper discusses the importance of accountability, fairness, and ethical use of AI, highlighting the need for guidelines and frameworks to manage these issues. It mentions the role of AI assurance in promoting trustworthy and ethical AI systems.
- Why unresolved: The paper identifies the need for accountability and transparency but does not provide specific strategies or frameworks for achieving these goals across diverse AI applications. The challenge of implementing consistent accountability measures in complex and varied AI systems remains open.
- What evidence would resolve it: Development and implementation of comprehensive accountability frameworks that address bias, fairness, and ethical use across different AI applications. Evaluation of the effectiveness of these frameworks in real-world scenarios, including metrics for measuring transparency and accountability, and case studies of successful implementation in various sectors.

## Limitations
- The grey literature review method introduces potential bias toward publicly available documents, possibly missing unpublished or proprietary AI policy initiatives
- The qualitative overlap analysis depends on subjective interpretation of theme boundaries, making reproducibility challenging
- Rapid evolution of AI technologies may quickly render the taxonomy obsolete if not updated regularly

## Confidence

- **High Confidence**: The identification of eleven overlapping themes across policy levels is supported by direct textual evidence from multiple sources.
- **Medium Confidence**: The mechanism by which non-technical policymakers can effectively use the taxonomy without deep technical expertise, as this depends on untested assumptions about knowledge transfer.
- **Low Confidence**: The assumption that professional engineers can reliably translate complex AI assurance concepts into legislative language without documented evidence of successful precedents.

## Next Checks
1. Replicate the overlap analysis on a new set of recent AI policy documents to test reproducibility and stability of the eleven themes.
2. Conduct usability testing with a sample of policymakers to evaluate whether the taxonomy improves their understanding and decision-making capacity.
3. Track the emergence of new AI policy initiatives over the next 12 months to measure how quickly the taxonomy requires updates to remain relevant.