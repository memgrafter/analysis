---
ver: rpa2
title: 'Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics
  to Reasoning-Memorization Correlations'
arxiv_id: '2403.14112'
source_url: https://arxiv.org/abs/2403.14112
tags:
- reasoning
- llms
- chinese
- commonsense
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CHARM is the first benchmark for comprehensive evaluation of LLMs'
  commonsense reasoning in Chinese, covering both global and Chinese-specific domains
  across 7 aspects. It employs 5 representative prompt strategies and 19 LLMs (7 English,
  12 Chinese-oriented).
---

# Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations

## Quick Facts
- arXiv ID: 2403.14112
- Source URL: https://arxiv.org/abs/2403.14112
- Reference count: 31
- Key outcome: CHARM is the first benchmark for comprehensive evaluation of LLMs' commonsense reasoning in Chinese, covering both global and Chinese-specific domains across 7 aspects.

## Executive Summary
CHARM is the first comprehensive benchmark for evaluating LLMs' commonsense reasoning capabilities in Chinese, spanning both global and Chinese-specific domains across seven distinct aspects. The benchmark employs five representative prompt strategies and evaluates 19 LLMs (7 English, 12 Chinese-oriented) to systematically analyze performance variations. Key findings reveal that LLM performance depends on both language orientation and task domain, with XLT strategy excelling for English LLMs and ZH-CoT for Chinese-oriented ones. The benchmark's innovative interconnected reasoning-memorization task design enables precise identification of memorization-independent reasoning differences among similarly performing models.

## Method Summary
The CHARM benchmark consists of 1800 reasoning questions across 7 task types and 759 memorization questions linked to 4 reasoning tasks. The evaluation covers 19 LLMs (7 English, 12 Chinese-oriented) using 5 prompt strategies (Direct, ZH-CoT, EN-CoT, Translate-EN, XLT) with 3-shot settings. Memorization tasks are constructed by extracting knowledge pieces from reasoning questions, enabling analysis of memorization-reasoning correlations. Performance is measured through accuracy on multiple-choice reasoning tasks and free-form QA for memorization tasks, with GPT-3.5 used as the judge for memorization evaluation.

## Key Results
- LLM performance depends on language orientation and task domain, with XLT best for English LLMs and ZH-CoT for Chinese-oriented ones
- Strong memorization is foundational to reasoning, but factors beyond memorization cause reasoning differences among similarly performing models
- The benchmark precisely identifies LLM strengths and weaknesses, guiding targeted optimization strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CHARM reveals memorization-independent reasoning differences among similarly performing LLMs by building closely-interconnected reasoning and memorization tasks.
- Mechanism: By extracting commonsense knowledge pieces from reasoning questions and constructing related memorization questions, CHARM ensures that failure in reasoning cannot be solely attributed to poor memorization, isolating reasoning-specific deficiencies.
- Core assumption: Commonsense knowledge pieces can be reliably extracted from reasoning questions and mapped to independent memorization questions without introducing semantic drift.
- Evidence anchors:
  - [abstract] "We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance."
  - [section] "From the 7 reasoning tasks, we chose 4 that can be readily associated in this manner, AJ, TU, MMR, SpU, referred as the Memorization-Reasoning-Interconnected (MRI) tasks, and built the related memorization questions."
  - [corpus] Weak evidence; no prior work cited for the exact interconnected task design.
- Break condition: If memorization questions cannot be constructed without introducing ambiguity or if reasoning questions depend on tacit knowledge not captured in memorization questions, the isolation breaks.

### Mechanism 2
- Claim: The effectiveness of prompt strategies for Chinese commonsense reasoning depends on both the LLM's language orientation and the task's domain.
- Mechanism: English-oriented LLMs benefit more from English-based reasoning strategies (XLT, Translate-EN) on global commonsense tasks, while Chinese-oriented LLMs perform better with native-language strategies (ZH-CoT) on Chinese-specific tasks.
- Core assumption: LLMs' training corpus composition and architectural biases influence their cross-lingual reasoning performance.
- Evidence anchors:
  - [abstract] "Our findings indicated that the LLM’s language orientation and the task’s domain influence the effectiveness of the prompt strategy..."
  - [section] "XLT consistently excels for English LLMs among the 5 strategies, while for Chinese-oriented LLMs, despite some complexity, ZH-CoT generally performs best."
  - [corpus] Moderate evidence; similar conclusions in Shi et al. (2022) and Huang et al. (2023a), but CHARM extends to Chinese-oriented LLMs.
- Break condition: If LLMs are fine-tuned extensively on bilingual data or if prompt strategies evolve to neutralize language orientation effects, the dependency weakens.

### Mechanism 3
- Claim: Strong memorization is foundational to integrated reasoning, but differences in reasoning ability persist among LLMs with similar memorization performance.
- Mechanism: Memorization provides the knowledge base required for reasoning; however, reasoning involves additional cognitive steps (understanding, logical inference, answer selection) where models diverge.
- Evidence anchors:
  - [abstract] "We found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance."
  - [section] "The results clearly indicate that strong memorization is the foundation of integrated reasoning. Weak memorization leads to poor reasoning... Also, factors other than memorization can cause significant differences in reasoning abilities among LLMs with similar memorization."
  - [corpus] Moderate evidence; KoLA (Yu et al., 2023) and SeaEval (Wang et al., 2023a) also assess memorization-reasoning correlations but lack intrinsic task linkage.
- Break condition: If reasoning tasks can be solved via pattern matching without deep knowledge integration, memorization's foundational role diminishes.

## Foundational Learning

- Concept: Cross-lingual prompt strategy effectiveness
  - Why needed here: To select the optimal prompt strategy for Chinese commonsense reasoning based on LLM language orientation and task domain.
  - Quick check question: If an English LLM is evaluated on a Chinese-specific commonsense task, which prompt strategy is likely to yield the best performance: ZH-CoT, EN-CoT, or XLT?

- Concept: Memorization-reasoning correlation analysis
  - Why needed here: To understand whether reasoning failures are due to knowledge gaps or reasoning deficits, guiding model improvement priorities.
  - Quick check question: If an LLM scores high on memorization tasks but low on reasoning tasks for the same knowledge domain, what type of error is most likely responsible?

- Concept: Benchmark construction with intrinsic task linkage
  - Why needed here: To ensure that memorization and reasoning evaluations are tightly coupled, avoiding the limitations of using disparate datasets.
  - Quick check question: Why is it problematic to evaluate memorization and reasoning using unrelated datasets from different domains?

## Architecture Onboarding

- Component map:
  - Benchmark Core: CHARM reasoning tasks (7 types) + memorization tasks (4 MRI tasks)
  - Evaluation Engine: 19 LLMs × 5 prompt strategies
  - Analysis Pipeline: Performance aggregation, correlation analysis, memorization-independent reasoning evaluation (FRMM, MIB)
  - Quality Assurance: Human annotation review for bias and accuracy

- Critical path:
  1. Construct reasoning tasks spanning global and Chinese-specific domains
  2. Build closely-interconnected memorization tasks for selected reasoning tasks
  3. Evaluate 19 LLMs with 5 prompt strategies on CHARM
  4. Analyze performance by language orientation and domain
  5. Perform memorization-reasoning correlation analysis
  6. Evaluate memorization-independent reasoning (FRMM, MIB)
  7. Conduct error type analysis for reasoning failures

- Design tradeoffs:
  - Multiple-choice vs. free-form QA: Multiple-choice enables automated evaluation but may introduce guessing; free-form avoids guessing but requires manual or model-based judgment.
  - Task linkage granularity: Finer-grained knowledge extraction improves memorization-independence but increases construction complexity.
  - Prompt strategy diversity: More strategies provide better coverage but increase evaluation cost.

- Failure signatures:
  - Low memorization, low reasoning: Model lacks foundational knowledge (Type I)
  - High memorization, medium reasoning: Model struggles with knowledge application (Type II)
  - High memorization, high reasoning: Model excels at both knowledge retention and application (Type III)
  - Random performance fluctuations: Insufficient task diversity or small sample size

- First 3 experiments:
  1. Run a small-scale pilot with 3 LLMs (1 English, 1 Chinese-oriented, 1 bilingual) × 3 prompt strategies on a subset of CHARM tasks to validate prompt strategy effectiveness assumptions.
  2. Test the memorization-reasoning task linkage by manually verifying a sample of reasoning questions and their associated memorization questions for semantic consistency.
  3. Perform a correlation analysis between memorization and reasoning scores on the pilot data to check if the expected patterns (Type I/II/III) emerge.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can CHARM's construction process be automated to reduce manual effort and expand the range of knowledge covered?
- Basis in paper: [inferred] The paper mentions that most questions in CHARM's Chinese domain are manually constructed, limiting the number of questions and knowledge coverage.
- Why unresolved: Manual construction is time-consuming and may introduce human bias. The paper acknowledges this limitation but doesn't provide a solution.
- What evidence would resolve it: A study demonstrating an automated question generation system for CHARM that maintains quality while significantly increasing the number and diversity of questions.

### Open Question 2
- Question: How can the memorization-independent reasoning error classification be further automated and validated?
- Basis in paper: [inferred] The paper mentions using manual review to classify reasoning errors into four categories, but suggests using robust LLMs like GPT-4 for automated classification in future research.
- Why unresolved: Manual classification is subjective and may not be scalable. The paper acknowledges the need for automation but doesn't provide a concrete approach.
- What evidence would resolve it: A study comparing automated error classification using LLMs with human expert classifications, demonstrating high accuracy and consistency.

### Open Question 3
- Question: How do the effectiveness and preference of prompt strategies evolve with the advancement of LLM technology and training corpora?
- Basis in paper: [explicit] The paper mentions that the best prompt strategy for commonsense reasoning tasks is not static and should progress with LLM technology, influenced by new prompt strategies, new LLMs, and evolving training corpora.
- Why unresolved: The paper provides empirical findings for current LLMs and prompt strategies but acknowledges the need for ongoing updates as technology advances.
- What evidence would resolve it: A longitudinal study tracking the performance of various prompt strategies across multiple generations of LLMs, identifying trends and shifts in effectiveness over time.

## Limitations

- Task linkage validity is uncertain due to potential semantic drift in knowledge extraction from reasoning to memorization questions
- Language orientation effects may reflect training corpus biases rather than inherent language capabilities
- Prompt strategy effectiveness findings may not generalize to other commonsense domains or more complex reasoning tasks

## Confidence

**High Confidence**:
- The benchmark construction methodology and task coverage (7 reasoning aspects across global and Chinese domains)
- The basic observation that memorization performance correlates with reasoning performance
- The identification of different error types in reasoning failures

**Medium Confidence**:
- The superiority of specific prompt strategies for different LLM types
- The claim that memorization is foundational but not sufficient for reasoning
- The relative performance rankings of evaluated LLMs

**Low Confidence**:
- The precise quantification of memorization-independent reasoning differences
- The generalizability of findings to other commonsense domains or more complex reasoning tasks
- The robustness of the interconnected task design across different knowledge domains

## Next Checks

1. **Cross-validation with alternative knowledge extraction**: Reconstruct the interconnected memorization tasks using a different knowledge extraction methodology (e.g., human experts vs. automated extraction) and compare the resulting memorization-reasoning correlations to validate the task linkage reliability.

2. **Bilingual fine-tuning analysis**: Fine-tune a subset of Chinese-oriented LLMs on additional bilingual data and re-evaluate their performance with different prompt strategies to determine if language orientation effects persist after balancing training corpora.

3. **Multi-hop reasoning extension**: Add a set of multi-hop reasoning tasks to CHARM where success requires integrating knowledge from multiple sources, then analyze whether the memorization-reasoning correlation patterns observed in single-hop tasks hold for more complex reasoning scenarios.