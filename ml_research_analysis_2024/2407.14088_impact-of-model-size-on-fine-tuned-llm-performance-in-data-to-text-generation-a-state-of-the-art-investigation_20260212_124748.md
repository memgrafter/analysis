---
ver: rpa2
title: 'Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
  A State-of-the-Art Investigation'
arxiv_id: '2407.14088'
source_url: https://arxiv.org/abs/2407.14088
tags:
- llms
- text
- generation
- https
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how model size impacts fine-tuned large
  language models (LLMs) for data-to-text (D2T) generation. It evaluates 12 LLMs from
  five families (BART, T5, BLOOM, OPT, Llama 2) across five datasets covering three
  D2T types.
---

# Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation

## Quick Facts
- arXiv ID: 2407.14088
- Source URL: https://arxiv.org/abs/2407.14088
- Reference count: 40
- Key outcome: Larger LLMs improve readability and informativeness but reduce faithfulness in D2T tasks; smaller models show more resilience when source-reference divergence is high.

## Executive Summary
This study investigates how model size impacts fine-tuned large language models for data-to-text generation across three key qualities: readability, informativeness, and faithfulness. The authors evaluate 12 LLMs from five families across five diverse datasets using six automatic metrics. Results show that increasing model size enhances readability and informativeness but often reduces faithfulness, with no consistent advantage for larger model families. Notably, smaller LLMs demonstrate greater resilience to source-reference divergence, particularly when divergence is high.

## Method Summary
The study fine-tunes 12 LLMs (BART, T5, BLOOM, OPT, Llama 2) from five families at varying sizes on five D2T datasets. Models are fine-tuned for 3 epochs using AdamW with learning rates of 1e-05 for T5/BART and 1e-04 for larger models (using QLoRA). Evaluation uses six automatic metrics (BLEU, METEOR, BERTScore, MoverScore, Parent, BARTScore) with beam search decoding (beam size 5). Statistical significance is tested using Welch's t-test (p < 0.05).

## Key Results
- Increasing model size improves readability and informativeness but reduces faithfulness in D2T tasks
- Larger model families do not consistently outperform smaller ones
- Smaller LLMs show greater resilience to source-reference divergence, especially at high divergence levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger LLM parameter counts improve readability and informativeness in D2T tasks, but reduce faithfulness.
- Mechanism: Larger models capture richer contextual and semantic representations, enabling more fluent and content-complete outputs, but their generative priors can override source constraints, introducing irrelevant facts.
- Core assumption: The evaluation metrics (Bleu, Meteor, BERTScore, MoverScore) are reliable proxies for the stated qualities and are not biased by model size.
- Evidence anchors:
  - [abstract]: "increasing LLM size enhances readability and informativeness in D2T tasks, but larger (in terms of size) LLMs may sacrifice faithfulness"
  - [section 6.1]: "Across all three categories of D2T tasks, it becomes apparent from Table 2 and Table 3 that augmenting the model parameters substantially enhances the readability of D2T tasks"
  - [corpus]: Weak; only 0 citations in nearest neighbors; no direct mechanism described in related papers.

### Mechanism 2
- Claim: Model size within a family correlates positively with D2T performance across most qualities except faithfulness.
- Mechanism: Within-family parameter scaling provides a monotonic increase in representation capacity and training signal absorption, but does not proportionally improve the alignment between generated content and source constraints.
- Core assumption: All models in a family share similar architecture and training regime, so parameter scaling is the primary differentiating factor.
- Evidence anchors:
  - [section 6.4]: "Across all six metrics' tables... we can effectively address the second question raised in Section 2... there is no significant evidence to conclusively state that larger LLM families... outperform smaller LLM families"
  - [section 6.3]: "No, an increase in the number of parameters (i.e., model size) will not necessarily improve the faithfulness of LLMs inside of a LLM family for D2T tasks"
  - [corpus]: Weak; related papers focus on benchmarking rather than mechanism.

### Mechanism 3
- Claim: In the presence of source-reference divergence, smaller LLMs are more resilient than larger ones.
- Mechanism: Larger LLMs rely more heavily on their generative priors and broader training distribution, making them prone to deviate from source content when reference and source diverge; smaller LLMs have less expressive capacity, forcing tighter adherence to the source.
- Core assumption: Source-reference divergence is common and cannot be entirely eliminated, and models must balance fidelity to source vs. fluency to reference.
- Evidence anchors:
  - [abstract]: "small-sized LLMs show more resilience than larger ones when source-reference divergence is present"
  - [section 7.2]: "when the source-reference divergence becomes high, T5-base often either outperforms the larger LLMs... or significantly narrows the performance gap with them"
  - [corpus]: Weak; no explicit divergence mechanism described in nearest neighbors.

## Foundational Learning

- Concept: Data-to-text (D2T) generation and its three evaluation dimensions (readability, informativeness, faithfulness).
  - Why needed here: The paper's contributions hinge on performance trade-offs across these three dimensions; misunderstanding them leads to misinterpretation of results.
  - Quick check question: What is the difference between informativeness and faithfulness in D2T evaluation?

- Concept: Language model scaling laws and the effect of parameter count on performance.
  - Why needed here: The central hypothesis is that model size impacts performance in predictable ways; understanding scaling is essential to interpreting results.
  - Quick check question: Why might increasing parameters degrade faithfulness even as they improve fluency?

- Concept: Source-reference divergence and its measurement.
  - Why needed here: Divergence is a key confounding factor; understanding its measurement (unigram overlap) is critical for interpreting the resilience findings.
  - Quick check question: How does unigram-based divergence differ from higher-order n-gram divergence in D2T tasks?

## Architecture Onboarding

- Component map: 12 LLMs (BART, T5, BLOOM, OPT, Llama 2) -> 5 datasets (E2E, ViGGo, WikiTableText, DART, WebNLG) -> 6 metrics (BLEU, METEOR, BERTScore, MoverScore, Parent, BARTScore) -> statistical significance testing

- Critical path: 1. Fine-tune each LLM on each dataset. 2. Generate outputs with beam search. 3. Compute all six metrics per output. 4. Aggregate results by model size and family. 5. Perform Welch's t-test for significance.

- Design tradeoffs:
  - Full fine-tuning for smaller models vs. QLoRA for larger ones (parameter efficiency vs. fidelity)
  - Unigram divergence vs. n-gram divergence (simplicity vs. nuance)
  - Automatic metrics vs. human evaluation (scalability vs. ground truth)

- Failure signatures:
  - Degraded BLEU/METEOR but high BERTScore indicates potential fluency without factual alignment
  - Low Parent scores with high MoverScore suggests informativeness misalignment with source
  - Inconsistent trends across metrics hint at metric reliability issues

- First 3 experiments:
  1. Replicate the main result: compare BLEU across model sizes within T5 family
  2. Test divergence resilience: run same experiment with high-divergence subset only
  3. Validate faithfulness: manually inspect BARTScore failures for largest vs. smallest models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing model size consistently improve faithfulness across all types of D2T tasks and datasets?
- Basis in paper: [explicit] The paper shows that increasing model size often reduces faithfulness, but does not systematically examine whether this effect varies by task type or dataset characteristics.
- Why unresolved: The study examines faithfulness across datasets but does not conduct a detailed analysis of whether the negative impact of model size on faithfulness is consistent across different D2T task types (table-to-text, graph-to-text, MR-to-text) or dataset characteristics.
- What evidence would resolve it: A systematic comparison of faithfulness performance across different D2T task types and datasets, controlling for other variables, would clarify whether the negative impact of model size on faithfulness is universal or context-dependent.

### Open Question 2
- Question: How do different fine-tuning approaches (full fine-tuning vs parameter-efficient fine-tuning) interact with model size effects on D2T performance?
- Basis in paper: [explicit] The paper uses different fine-tuning methods for smaller models (full fine-tuning) versus larger models (QLoRA), but does not systematically compare how these approaches affect the relationship between model size and performance.
- Why unresolved: The study employs different fine-tuning strategies based on model size but does not isolate and compare the effects of fine-tuning approach versus model size on D2T performance.
- What evidence would resolve it: A controlled experiment where the same fine-tuning approach is applied across different model sizes would help determine whether the observed performance differences are due to model size or fine-tuning methodology.

### Open Question 3
- Question: What is the optimal model size for balancing readability, informativeness, and faithfulness in D2T tasks?
- Basis in paper: [inferred] The paper shows that larger models improve readability and informativeness but reduce faithfulness, suggesting there may be an optimal size that balances these competing objectives.
- Why unresolved: While the study identifies the trade-offs between model size and different quality metrics, it does not attempt to find the model size that optimally balances all three key qualities of D2T generation.
- What evidence would resolve it: A multi-objective optimization analysis that identifies the model size achieving the best balance across readability, informativeness, and faithfulness metrics would address this question.

## Limitations
- Reliance on automatic metrics without human validation, particularly for faithfulness evaluation
- Limited exploration of whether 3 epochs is sufficient for model convergence across all configurations
- Use of unigram overlap for source-reference divergence may miss more nuanced semantic differences

## Confidence
- **High confidence**: The finding that larger models improve readability and informativeness but often reduce faithfulness is well-supported by the data and consistent across multiple metrics and datasets. The observation that larger model families do not consistently outperform smaller ones is also strongly evidenced.
- **Medium confidence**: The resilience of smaller models to source-reference divergence is supported but could benefit from more granular analysis of divergence types. The statistical significance testing provides some support, but the practical significance of these differences is not fully explored.
- **Low confidence**: The exact mechanisms by which model size affects faithfulness are not thoroughly investigated. The paper suggests that larger models' generative priors override source constraints, but does not empirically validate this mechanism or explore alternative explanations such as optimization dynamics during fine-tuning.

## Next Checks
1. **Human evaluation validation**: Conduct a human evaluation study focusing on the faithfulness dimension to validate whether the automated BARTScore measurements align with human judgments, particularly for the largest models where the most significant faithfulness degradation is observed.
2. **Convergence analysis**: Extend the fine-tuning duration beyond 3 epochs for a subset of models and datasets to determine whether the observed trade-offs between readability/informativeness and faithfulness persist or change as models converge, addressing the assumption that 3 epochs is sufficient.
3. **Divergence granularity study**: Repeat the source-reference divergence analysis using higher-order n-gram overlap (bigram, trigram) and semantic divergence measures to determine whether the resilience of smaller models holds across different types and granularities of divergence, and to better understand the nature of the divergences being studied.