---
ver: rpa2
title: Injecting Explainability and Lightweight Design into Weakly Supervised Video
  Anomaly Detection Systems
arxiv_id: '2412.20201'
source_url: https://arxiv.org/abs/2412.20201
tags:
- detection
- tcvads
- video
- anomaly
- module
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TCVADS, a Two-stage Cross-modal Video Anomaly
  Detection System designed for edge devices. The system addresses the challenge of
  real-time and interpretable weakly supervised video anomaly detection (WSMAD) by
  combining knowledge distillation and cross-modal contrastive learning.
---

# Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems

## Quick Facts
- arXiv ID: 2412.20201
- Source URL: https://arxiv.org/abs/2412.20201
- Authors: Wen-Dong Jiang; Chih-Yung Chang; Hsiang-Chuan Chang; Ji-Yuan Chen; Diptendu Sinha Roy
- Reference count: 40
- One-line primary result: TCVADS achieves 85.58% AP and 88.58% AUC on UCF-Crime, significantly outperforming existing methods while maintaining low computational complexity suitable for edge devices.

## Executive Summary
This paper presents TCVADS, a Two-stage Cross-modal Video Anomaly Detection System designed for edge devices. The system addresses the challenge of real-time and interpretable weakly supervised video anomaly detection by combining knowledge distillation and cross-modal contrastive learning. TCVADS achieves state-of-the-art performance on UCF-Crime and XD-Violence datasets while maintaining low computational complexity, making it suitable for deployment on resource-constrained edge devices.

## Method Summary
TCVADS employs a two-stage approach for weakly supervised video anomaly detection. In the first stage, it uses an enhanced MobileNet for feature extraction followed by an improved RWKV module for temporal analysis, with knowledge distillation transferring insights to a simplified CNN for binary classification. Upon detecting an anomaly, the second stage employs CLIP for fine-grained multi-class classification using a ternary textual relationship strategy. The system is designed to balance accuracy with computational efficiency, making it suitable for edge device deployment.

## Key Results
- TCVADS achieves 85.58% AP and 88.58% AUC on UCF-Crime dataset
- TCVADS achieves 85.58% AP on XD-Violence dataset
- The system significantly outperforms existing methods while maintaining low computational complexity suitable for edge devices

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** TCVADS uses knowledge distillation to transfer the temporal feature extraction capabilities of a complex RWKV-based model to a lightweight CNN, enabling fast binary classification suitable for edge devices.
- **Mechanism:** In the first stage, the AFED module (teacher model) is trained on labeled videos to extract interpretable features and perform temporal analysis using the enhanced RWKV module. The knowledge is then distilled to the QACM module (student model), which uses a simplified CNN for rapid binary classification.
- **Core assumption:** The temporal feature extraction capabilities of the RWKV model can be effectively transferred to a CNN through knowledge distillation without significant loss in performance.
- **Evidence anchors:**
  - [abstract] "Knowledge distillation transfers insights to a simplified CNN for binary classification."
  - [section] "To achieve this, the proposed approach first trains a complete model, the Anomalous Feature Extraction and Detection (AFED) module... Subsequently, the next step is to perform knowledge distillation on the AFED model, allowing the QACM module to quickly learn to extract temporal features and rapidly determine whether a video is anomalous."
- **Break condition:** If the performance gap between AFED and QACM is too large after distillation, the knowledge transfer is ineffective and the system cannot meet real-time constraints.

### Mechanism 2
- **Claim:** The two-stage design allows TCVADS to first quickly identify potential anomalies with coarse-grained classification, then apply fine-grained multi-class classification only when needed, optimizing computational efficiency.
- **Mechanism:** The first stage performs rapid binary classification to detect anomalies. Upon detecting an anomaly, the second stage is triggered, employing CLIP for cross-modal contrastive learning with text and images to achieve refined classification.
- **Core assumption:** Most video segments are normal, so coarse-grained classification can filter out the majority of data before expensive fine-grained analysis is applied.
- **Evidence anchors:**
  - [abstract] "In the first stage, TCVADS extracts features from video frames and inputs them into a time series analysis module, which acts as the teacher model... Upon detecting an anomaly, the second stage is triggered, employing a fine-grained multi-class classification model."
  - [section] "The primary goal of the first stage is to construct and train a binary classification model that extracts features from abnormal videos and allows for quick binary classification... If the input video segment is potentially abnormal, it then proceeds to the second stage of fine-grained judgment to identify the specific type of abnormal situation."
- **Break condition:** If anomaly prevalence is too high, the coarse-grained filter provides insufficient benefit and the system becomes bottlenecked by frequent fine-grained analysis.

### Mechanism 3
- **Claim:** The ternary input strategy for the CLIP text encoder (combining original CLIP text, dataset labels, and learnable prompts) enhances fine-grained detection accuracy by providing richer semantic context.
- **Mechanism:** For each video frame, CLIP generates a basic textual description. This is combined with dataset category labels and learnable prompts, which are optimized during training to capture subtle differences in abnormal events.
- **Core assumption:** Learnable prompts can effectively capture task-specific semantic nuances that generic CLIP text descriptions and fixed labels cannot represent.
- **Evidence anchors:**
  - [section] "Firstly, for each frame ùêº‡Øç of the video, a basic textual description ùëá‡Øñ‡Øü‡Øú‡Ø£ is obtained through the pre-trained CLIP model... Secondly, the corresponding original category labels... are directly encoded as tokens... Lastly, a set of learnable tokens ùëá‡Ø£‡Ø•‡Ø¢‡Ø†‡Ø£‡Øß is introduced."
  - [corpus] "Weakly Supervised Video Anomaly Detection and Localization with Spatio-Temporal Prompts" - suggests that prompt-based approaches are relevant in this domain, though specific effectiveness claims are not detailed in the corpus.
- **Break condition:** If learnable prompts do not converge or provide negligible improvement over simpler prompting strategies, the added complexity is not justified.

## Foundational Learning

- **Concept:** Knowledge distillation
  - Why needed here: Enables transfer of complex temporal feature extraction capabilities to a lightweight model suitable for edge deployment
  - Quick check question: What is the difference between hard loss and soft loss in knowledge distillation?

- **Concept:** Cross-modal contrastive learning
  - Why needed here: Allows the system to leverage both visual and textual information for more accurate anomaly classification
  - Quick check question: How does CLIP's contrastive learning objective work to align image and text embeddings?

- **Concept:** Convolutional neural networks (CNNs)
  - Why needed here: Forms the basis of the lightweight QACM module for fast binary classification
  - Quick check question: What is the primary advantage of using a CNN for feature extraction compared to other architectures?

## Architecture Onboarding

- **Component map:** Enhanced MobileNet ‚Üí Enhanced RWKV ‚Üí QACM (for binary classification) ‚Üí CLIP-based fine-grained classification (if anomaly detected)
- **Critical path:** Video frames ‚Üí Enhanced MobileNet ‚Üí Enhanced RWKV ‚Üí QACM (for binary classification) ‚Üí CLIP-based fine-grained classification (if anomaly detected)
- **Design tradeoffs:**
  - Accuracy vs. computational efficiency: Two-stage design trades some potential accuracy for significant efficiency gains
  - Model complexity vs. interpretability: Enhanced MobileNet with deconvolution layers adds complexity but improves interpretability
  - Prompt complexity vs. performance: Ternary input strategy adds complexity but improves fine-grained detection accuracy
- **Failure signatures:**
  - High false positive rate in coarse-grained detection ‚Üí Frequent unnecessary fine-grained analysis
  - Poor knowledge distillation performance ‚Üí Large gap between AFED and QACM accuracy
  - Unstable learnable prompts ‚Üí Inconsistent fine-grained classification performance
- **First 3 experiments:**
  1. Evaluate knowledge distillation performance by comparing AFED and QACM accuracy on a validation set
  2. Test the two-stage design efficiency by measuring average processing time per video with varying anomaly prevalence
  3. Assess the impact of learnable prompts by comparing fine-grained classification accuracy with and without them

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the enhanced RWKV model compare to traditional Transformer and GCN architectures in terms of both computational efficiency and anomaly detection performance when processing long video sequences?
- Basis in paper: [explicit] The paper states that the enhanced RWKV model maintains computational complexity at O(n¬∑d) and outperforms traditional Transformer and GCN combinations in both AP and AVG metrics on the XD-Violence dataset.
- Why unresolved: While the paper provides experimental results comparing the enhanced RWKV to Transformer and GCN on specific datasets, it does not explore how these models scale with different sequence lengths or video types. The paper also does not investigate the impact of varying the number of layers or attention heads on performance.
- What evidence would resolve it: Conducting experiments on diverse video datasets with varying sequence lengths and comparing the performance of the enhanced RWKV, Transformer, and GCN models with different architectural configurations would provide insights into their relative strengths and weaknesses.

### Open Question 2
- Question: How does the ternary input strategy in the text encoder, which combines CLIP's native text, labels, and learnable prompts, impact the interpretability and performance of the fine-grained anomaly detection module compared to other prompting methods?
- Basis in paper: [explicit] The paper highlights the effectiveness of the ternary input strategy in improving both interpretability and performance, but does not provide a detailed analysis of how each component contributes to these improvements.
- Why unresolved: The paper does not investigate the individual contributions of CLIP's native text, labels, and learnable prompts to the overall performance. It also does not explore alternative prompting strategies or the impact of different prompt lengths or structures.
- What evidence would resolve it: Conducting ablation studies to evaluate the impact of each component in the ternary input strategy and comparing the results with other prompting methods would provide insights into the effectiveness of this approach.

### Open Question 3
- Question: How does the knowledge distillation process from the AFED module to the QACM module affect the interpretability and real-time performance of the coarse-grained detection stage?
- Basis in paper: [explicit] The paper mentions the use of knowledge distillation to transfer knowledge from the AFED module to the QACM module, but does not provide a detailed analysis of its impact on interpretability and real-time performance.
- Why unresolved: The paper does not investigate the interpretability of the QACM module before and after knowledge distillation or quantify the improvement in real-time performance achieved through this process.
- What evidence would resolve it: Conducting experiments to compare the interpretability and real-time performance of the QACM module with and without knowledge distillation would provide insights into the effectiveness of this approach.

## Limitations

- The knowledge distillation mechanism's effectiveness is somewhat under-specified, with exact architectural details and hyperparameters not fully disclosed
- Evaluation focuses primarily on detection performance metrics without extensive analysis of computational efficiency on actual edge devices
- Confidence in the two-stage design's efficiency benefits is medium, as it relies on the assumption that anomalies are rare enough for coarse filtering to be beneficial

## Confidence

- **High:** The two-stage architectural framework and its conceptual benefits
- **Medium:** Knowledge distillation effectiveness and computational efficiency claims
- **Low:** Exact architectural specifications and reproducibility details

## Next Checks

1. Conduct ablation studies comparing the two-stage approach against a single-stage baseline to quantify efficiency gains under varying anomaly prevalence rates
2. Implement the knowledge distillation pipeline with intermediate performance monitoring to verify that the student model (QACM) achieves comparable accuracy to the teacher model (AFED)
3. Evaluate the ternary prompt strategy against simpler alternatives (e.g., standard CLIP prompts or fixed prompts) to validate the necessity of learnable prompts for fine-grained detection performance