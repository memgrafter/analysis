---
ver: rpa2
title: 'Information Extraction from Clinical Notes: Are We Ready to Switch to Large
  Language Models?'
arxiv_id: '2411.10020'
source_url: https://arxiv.org/abs/2411.10020
tags:
- clinical
- bert
- available
- span
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically compared large language models (LLaMA-2/3)
  with BERT for clinical named entity recognition and relation extraction across multiple
  datasets. LLaMA models outperformed BERT, especially with limited training data
  and on unseen datasets, with up to 7% F1 score gains for NER and 4% for RE.
---

# Information Extraction from Clinical Notes: Are We Ready to Switch to Large Language Models?

## Quick Facts
- arXiv ID: 2411.10020
- Source URL: https://arxiv.org/abs/2411.10020
- Reference count: 40
- Primary result: LLaMA models outperform BERT for clinical NER and RE, especially with limited training data, but require up to 28× more computational resources.

## Executive Summary
This study systematically compares large language models (LLaMA-2/3) with BERT for clinical named entity recognition (NER) and relation extraction (RE) across multiple datasets. LLaMA models demonstrate superior performance, particularly with limited training data and on unseen datasets, achieving up to 7% higher F1 scores for NER and 4% for RE. However, LLaMA models require significantly more computational resources and run up to 28 times slower than BERT. The results suggest that LLaMA models are more effective for clinical information extraction in low-resource and cross-institutional settings, while BERT remains competitive when ample training data is available. The study also introduces a new clinical IE tool, "Kiwi," to facilitate model use.

## Method Summary
The study fine-tunes LLaMA-2 and LLaMA-3 models using Parameter-Efficient Fine Tuning (PEFT) with 4-bit quantized models and QLoRA, while BERT models are fine-tuned using standard methods. The models are evaluated on NER and RE tasks using four clinical datasets (UTP, MTSamples, MIMIC-III, and i2b2) with annotations for 4 main clinical entities and 16 modifiers. Performance is measured using Precision, Recall, and F1 scores under exact and relaxed match criteria, and computational costs (GPU hours, memory usage, energy consumption) and throughput are assessed.

## Key Results
- LLaMA models outperformed BERT by up to 7% F1 score on NER and 4% on RE, especially with limited training data.
- LLaMA-3-70B showed superior performance on the unseen i2b2 dataset compared to BERT.
- LLaMA models required up to 28 times more computational resources and were significantly slower than BERT during both training and inference.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLaMA models outperform BERT on clinical NER and RE, especially with limited training data.
- Mechanism: Instruction-tuned LLaMA models leverage large-scale pretraining and fine-tuning to better capture contextual nuances and generalize across institutions.
- Core assumption: The pretraining of LLaMA models provides a stronger foundation for domain adaptation compared to BERT's biomedical pretraining.
- Evidence anchors:
  - [abstract] "With sufficient training data (e.g., UTP), LLaMA showed marginal improvements (1% on NER, 1.5-3.7% on RE) over BERT; for limited training data (e.g., MTSamples, MIMIC-III), the improvements were greater."
  - [section] "On the unseen i2b2 dataset, LLaMA-3-70B outperformed BERT by over 7% (F1) on NER and 4% (F1) on RE."
- Break condition: If the pretraining corpus of LLaMA models lacks domain-specific clinical knowledge, the advantage may diminish.

### Mechanism 2
- Claim: LLaMA models require significantly more computational resources and are slower than BERT.
- Mechanism: The larger parameter size and architecture of LLaMA models lead to increased memory usage and processing time during both training and inference.
- Core assumption: The computational cost scales with model size and architecture complexity.
- Evidence anchors:
  - [abstract] "However, LLaMA models required significantly more computing resources and ran up to 28 times slower than BERT."
  - [section] "Memory requirements increase substantially with model size. The 70B versions of LLaMA-2 and LLaMA-3 required 58-66 GB for training and 146 GB for inference, while BERT used only 21.3 GB and 65.7 GB, respectively."
- Break condition: If model optimization techniques (e.g., quantization, pruning) are applied, the resource gap may narrow.

### Mechanism 3
- Claim: The choice between LLaMA and BERT models depends on the specific task, data availability, and computational resources.
- Mechanism: LLaMA models excel in low-resource and cross-institutional settings, while BERT is competitive with ample training data and is more efficient.
- Core assumption: The trade-off between performance and computational efficiency is task-specific and context-dependent.
- Evidence anchors:
  - [abstract] "These findings highlight that choosing between LLMs and traditional deep learning methods for clinical IE applications should remain task-specific, taking into account both performance metrics and practical considerations such as available computing resources and the intended use case scenarios."
  - [section] "BERT remains viable, requiring less resources while maintaining competitive performance on in-domain datasets."
- Break condition: If computational resources are severely limited or real-time processing is required, BERT may be the only feasible option.

## Foundational Learning

- Concept: Information Extraction (IE) in clinical NLP
  - Why needed here: The study focuses on IE tasks (NER and RE) in clinical notes, which are critical for structuring unstructured data.
  - Quick check question: What are the main differences between NER and RE in clinical IE?

- Concept: Named Entity Recognition (NER) and Relation Extraction (RE)
  - Why needed here: These are the two fundamental IE tasks evaluated in the study, involving identifying entities and their relationships in clinical text.
  - Quick check question: How do NER and RE contribute to structuring clinical notes?

- Concept: Large Language Models (LLMs) and their application to IE tasks
  - Why needed here: The study compares LLMs (LLaMA-2/3) with traditional models (BERT) for IE tasks, highlighting their strengths and limitations.
  - Quick check question: What are the key advantages and disadvantages of using LLMs for clinical IE tasks?

## Architecture Onboarding

- Component map: Clinical notes -> Annotation -> Model fine-tuning (LLaMA-2/3, BERT) -> Performance evaluation (NER, RE) -> Kiwi package
- Critical path: Data annotation → Model fine-tuning → Performance evaluation → Computational resource assessment → Kiwi package implementation
- Design tradeoffs: LLaMA models offer better performance but require more resources, while BERT is efficient but may underperform in low-resource settings
- Failure signatures: Poor performance on unseen data, high computational costs, slow processing throughput
- First 3 experiments:
  1. Compare LLaMA-3-8B and BERT on NER and RE tasks using the UTP dataset.
  2. Evaluate the cross-institutional generalizability of LLaMA-3-70B on the i2b2 dataset.
  3. Assess the computational resource requirements of LLaMA-3-8B and BERT during inference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of instruction-tuned LLMs compare to domain-specific pre-trained LLMs like Me-LLaMA when fine-tuned on clinical IE tasks?
- Basis in paper: [inferred] The paper mentions that future work could investigate instruction-tuning models based on domain-specific foundational LLMs, such as Me-LLaMA, which may yield better performance due to their specialized medical knowledge.
- Why unresolved: The study used LLaMA-2/3 as base models, which were not specifically pre-trained on medical texts. The potential benefits of using domain-specific pre-trained models were not explored.
- What evidence would resolve it: Conducting a study that compares the performance of instruction-tuned LLaMA-2/3 models with instruction-tuned domain-specific pre-trained models like Me-LLaMA on the same clinical IE tasks and datasets.

### Open Question 2
- Question: How does the performance of LLMs vary across different medical specialties and clinical note types?
- Basis in paper: [inferred] The study used a diverse corpus from multiple institutions, but the performance of LLMs across different medical specialties and clinical note types was not explicitly analyzed.
- Why unresolved: The paper does not provide a detailed breakdown of LLM performance across various medical specialties and clinical note types, which could reveal potential biases or limitations in their generalizability.
- What evidence would resolve it: Analyzing LLM performance on clinical notes from specific medical specialties and note types, and comparing their performance across these different categories.

### Open Question 3
- Question: What are the potential benefits and limitations of using LLMs for inter-sentence relationship extraction in clinical notes?
- Basis in paper: [inferred] The paper mentions that the current information extraction approach operates at the sentence level, which may not fully leverage the ability of LLMs to handle longer sequences. Future research could explore IE at the section or document level.
- Why unresolved: The study focused on sentence-level information extraction, and the potential of LLMs for capturing inter-sentence relationships was not explored.
- What evidence would resolve it: Developing and evaluating an LLM-based system for section or document-level information extraction, and comparing its performance with sentence-level extraction on tasks that require inter-sentence relationship understanding.

## Limitations

- Generalizability across clinical domains: All datasets share similar annotation schemas; performance on radically different clinical IE tasks remains uncertain.
- Computational cost vs. performance trade-off: The practical implications of LLaMA's computational costs in real-world settings are not fully explored.
- Model size selection guidance: The optimal model size for different clinical IE tasks and resource constraints is not clearly established.

## Confidence

- **High Confidence:**
  - LLaMA models consistently outperform BERT on clinical NER and RE tasks across all tested datasets
  - LLaMA models demonstrate superior performance on limited training data and cross-institutional generalization
  - LLaMA models require significantly more computational resources than BERT (up to 28× slower)

- **Medium Confidence:**
  - The superiority of LLaMA models is most pronounced in low-resource settings (though the exact threshold where BERT becomes competitive needs further exploration)
  - LLaMA-3 models show better performance than LLaMA-2 models, but the incremental benefit varies by task

## Next Checks

1. **Resource-Constrained Performance Analysis**: Conduct a detailed study comparing LLaMA-8B and BERT performance under progressively constrained computational resources to identify the practical threshold where BERT becomes the preferred choice.

2. **Cross-Domain Generalizability Test**: Evaluate the instruction-tuned LLaMA models on clinical IE tasks with different annotation schemas and entity types to assess the limits of their generalizability.

3. **Efficiency Optimization Evaluation**: Test whether optimization techniques (quantization, pruning, distillation) can narrow the performance gap between LLaMA models and BERT while reducing computational requirements, and quantify the trade-offs involved.