---
ver: rpa2
title: Empirical investigation of multi-source cross-validation in clinical ECG classification
arxiv_id: '2403.15012'
source_url: https://arxiv.org/abs/2403.15012
tags:
- data
- source
- sources
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates cross-validation methods for ECG-based cardiovascular
  disease classification across multiple data sources. The authors combined PhysioNet/CinC
  Challenge 2021 and Shandong Provincial Hospital datasets, creating a harmonized
  multi-source resource of 103,438 ECG recordings.
---

# Empirical investigation of multi-source cross-validation in clinical ECG classification

## Quick Facts
- arXiv ID: 2403.15012
- Source URL: https://arxiv.org/abs/2403.15012
- Authors: Tuija Leinonen; David Wong; Antti Vasankari; Ali Wahab; Ramesh Nadarajah; Matti Kaisti; Antti Airola
- Reference count: 40
- Primary result: K-fold cross-validation systematically overestimates model performance when generalizing to new data sources

## Executive Summary
This study investigates cross-validation methods for ECG-based cardiovascular disease classification across multiple data sources. The authors combined PhysioNet/CinC Challenge 2021 and Shandong Provincial Hospital datasets, creating a harmonized multi-source resource of 103,438 ECG recordings. They compared standard K-fold cross-validation with leave-source-out cross-validation across four classification methods including deep learning models. The results demonstrate that K-fold cross-validation systematically overestimates model performance when generalizing to new data sources, while leave-source-out cross-validation provides more reliable estimates with minimal bias but higher variability.

## Method Summary
The study combined two ECG datasets (PhysioNet/CinC Challenge 2021 and Shandong Provincial Hospital) into a harmonized multi-source resource of 103,438 recordings. Four classification methods were evaluated: ResNet, Transformer, Logistic Regression, and XGBoost. The authors implemented both standard stratified K-fold cross-validation and leave-source-out cross-validation, comparing their performance in single-source and multi-source settings. A source prediction experiment was conducted to quantify systematic differences between data sources. Performance was evaluated using macro and micro AUC scores, with bias-variance analysis comparing the two cross-validation approaches.

## Key Results
- K-fold cross-validation systematically overestimates prediction performance when the end goal is to generalize to new data sources
- Leave-source-out cross-validation provides more reliable performance estimates with close to zero bias though larger variability
- Source prediction experiment confirmed systematic differences between data sources can be detected with high accuracy (96.9-97.3%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Leave-source-out cross-validation (LSO CV) provides unbiased performance estimates for models intended to generalize to new data sources.
- Mechanism: LSO CV systematically excludes one source during training, forcing the model to learn patterns that generalize beyond the excluded source. This contrasts with K-fold CV where data from the same source appears in both training and validation folds.
- Core assumption: There are systematic differences between data sources that can be detected and leveraged for evaluation.
- Evidence anchors:
  - [abstract] "Leave-source-out cross-validation provides more reliable performance estimates, having close to zero bias though larger variability."
  - [section] "We further evaluate whether leave-source-out cross-validation provides more reliable estimates than the standard K-fold cross-validation regarding the generalization of the model to a new source, in settings where data from multiple sources are available for both model training and validation."
  - [corpus] Found 25 related papers, average neighbor FMR=0.373, indicating reasonable relevance to multi-source ECG classification.
- Break Condition: LSO CV becomes unreliable when the number of sources is very small (e.g., 2-3 sources), as removing one source significantly reduces training data diversity.

### Mechanism 2
- Claim: K-fold cross-validation systematically overestimates model performance when the goal is generalization to new data sources.
- Mechanism: K-fold CV randomly splits data from all sources into folds, meaning data from the same source appears in both training and validation sets. This creates an overly optimistic performance estimate because the model learns source-specific patterns rather than generalizable features.
- Core assumption: Data sources have distinct characteristics that make them non-interchangeable for evaluation purposes.
- Evidence anchors:
  - [abstract] "K-fold cross-validation, both on single-source and multi-source data, systemically overestimates prediction performance when the end goal is to generalize to new sources."
  - [section] "The increasing availability of multi-source medical datasets provides new opportunities for obtaining more comprehensive and realistic evaluations of expected accuracy through source-level cross-validation designs."
  - [corpus] Average neighbor citations=0.0 suggests this is a relatively novel area of investigation.
- Break Condition: K-fold CV becomes more reliable when all test data comes from sources already represented in the training data.

### Mechanism 3
- Claim: Systematic differences between data sources can be detected with high accuracy using source prediction experiments.
- Mechanism: By training a classifier to predict which source a given ECG comes from, we can quantify the degree of source-specific patterns in the data. High classification accuracy indicates strong source differences.
- Core assumption: Source-specific patterns exist and are learnable by machine learning models.
- Evidence anchors:
  - [abstract] "A source prediction experiment confirmed systematic differences between data sources can be detected with high accuracy (96.9-97.3%)."
  - [section] "We introduce and apply a classification based heuristic for detecting systematic differences between the source distributions."
  - [corpus] Corpus contains 5 neighbor papers, but none provide direct evidence about source prediction accuracy.
- Break Condition: Source prediction accuracy approaches random chance when data sources are truly homogeneous.

## Foundational Learning

- Concept: Cross-validation methodology and its assumptions
  - Why needed here: The paper compares two cross-validation approaches (K-fold vs LSO) and their reliability for multi-source data. Understanding CV assumptions is critical to interpreting the results.
  - Quick check question: What is the fundamental assumption behind standard K-fold cross-validation that may not hold for multi-source medical data?

- Concept: Multi-source data characteristics and batch effects
  - Why needed here: The study demonstrates that different data sources have systematic differences. Understanding what creates these differences (demographics, recording conditions, labeling standards) is essential for proper evaluation.
  - Quick check question: Why might ECG recordings from different hospitals show systematic differences even when measuring the same physiological phenomenon?

- Concept: Bias-variance tradeoff in performance estimation
  - Why needed here: The paper shows LSO CV has lower bias but higher variance compared to K-fold CV. Understanding this tradeoff is crucial for interpreting which method provides more reliable estimates.
  - Quick check question: When comparing two performance estimation methods, why might a method with higher variance but lower bias be preferable?

## Architecture Onboarding

- Component map: Data harmonization pipeline -> Classification models -> Cross-validation modules -> Evaluation framework -> Source detection experiment
- Critical path:
  1. Load and harmonize multi-source ECG data
  2. Select relevant labels based on occurrence criteria
  3. Implement LSO CV and K-fold CV methods
  4. Train multiple classification models
  5. Evaluate performance with bias-variance analysis
  6. Run source detection experiment to validate source differences
- Design tradeoffs:
  - K-fold CV provides lower variance but higher bias for out-of-source generalization
  - LSO CV provides lower bias but higher variance, especially with few sources
  - More sources improve LSO CV reliability but increase computational cost
  - Simple models (LR, XGBoost) provide baseline comparisons but may underperform deep learning models
- Failure signatures:
  - High correlation between K-fold CV and LSO CV results may indicate insufficient source differences
  - Extremely high source detection accuracy (>95%) suggests strong batch effects
  - Large discrepancy between K-fold CV and LSO CV results indicates source-specific learning in K-fold
- First 3 experiments:
  1. Implement LSO CV on a small subset of data to verify it correctly excludes one source per fold
  2. Compare K-fold CV vs LSO CV performance on a single classification model to observe bias differences
  3. Run source detection experiment with varying input features to identify which characteristics drive source differences

## Open Questions the Paper Calls Out

- Question: How does incorporating data augmentation techniques affect the generalization performance of ECG classification models across multiple sources?
  - Basis in paper: [inferred] The authors note that data augmentation in ECG classification has shown potential to tackle label imbalance and suggest it as a factor to consider in future studies to further develop ECG classifiers.
  - Why unresolved: The paper does not empirically evaluate the impact of data augmentation on multi-source cross-validation performance.
  - What evidence would resolve it: Experimental results comparing model performance with and without data augmentation across multiple data sources using both K-fold and leave-source-out cross-validation.

- Question: To what extent do domain adaptation and transfer learning techniques mitigate the performance degradation when deploying models trained on one source to new, unseen sources?
  - Basis in paper: [inferred] The authors suggest that domain adaptation and transfer learning could be considered in future studies to further understand the effect when a model trained in one context performs poorly in another context.
  - Why unresolved: The paper does not investigate or evaluate the effectiveness of domain adaptation or transfer learning methods for improving cross-source generalization.
  - What evidence would resolve it: Comparative studies showing performance improvements when applying domain adaptation or transfer learning techniques to multi-source ECG data, particularly when evaluating on leave-source-out cross-validation.

- Question: How do different network architectures beyond convolutional neural networks, such as long short-term memory networks, perform in ECG classification tasks when evaluated using multi-source cross-validation?
  - Basis in paper: [inferred] The authors mention that while convolutional neural networks like ResNets have shown success for time series classification, other network architectures such as LSTM networks have shown their potential and should be considered in future studies.
  - Why unresolved: The paper focuses on evaluating ResNet and Transformer architectures but does not systematically compare their performance against other network types like LSTMs using multi-source cross-validation.
  - What evidence would resolve it: Systematic comparison of multiple network architectures (ResNet, Transformer, LSTM, etc.) evaluated on the same multi-source ECG dataset using both K-fold and leave-source-out cross-validation methods.

## Limitations
- The study uses only 2 data sources, which may not fully represent the diversity of clinical environments
- The harmonization process involved imputing sinus rhythm labels for 53,603 ECGs, potentially introducing systematic biases
- Findings are specific to 12-lead ECG data with 250Hz sampling rate and 4096 samples per recording

## Confidence
- **High Confidence**: K-fold CV systematically overestimates performance for out-of-source generalization
- **Medium Confidence**: LSO CV provides more reliable estimates with minimal bias but higher variability
- **Low Confidence**: Methodology can be applied to other clinical datasets without modification

## Next Checks
1. Replication with Additional Data Sources: Validate the cross-validation methodology using at least 5-10 independent hospital datasets
2. Impact of Data Source Characteristics: Conduct controlled experiments varying the number of data sources, sample sizes per source, and degree of source similarity
3. Generalization to Other Clinical Data Types: Apply the same cross-validation comparison framework to other clinical data modalities (e.g., medical imaging, laboratory results)