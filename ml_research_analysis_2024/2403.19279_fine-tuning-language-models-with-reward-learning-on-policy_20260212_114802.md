---
ver: rpa2
title: Fine-Tuning Language Models with Reward Learning on Policy
arxiv_id: '2403.19279'
source_url: https://arxiv.org/abs/2403.19279
tags:
- reward
- policy
- learning
- preference
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reward model accuracy degradation
  during reinforcement learning from human feedback (RLHF) for large language models.
  The proposed method, reward learning on policy (RLP), is an unsupervised framework
  that refines reward models using policy samples to maintain on-distribution accuracy.
---

# Fine-Tuning Language Models with Reward Learning on Policy

## Quick Facts
- arXiv ID: 2403.19279
- Source URL: https://arxiv.org/abs/2403.19279
- Authors: Hao Lang; Fei Huang; Yongbin Li
- Reference count: 10
- Primary result: RLP achieves 50.2% simulated win-rate vs 46.8% for PPO on AlpacaFarm

## Executive Summary
This paper addresses reward model accuracy degradation during RLHF for large language models by introducing Reward Learning on Policy (RLP), an unsupervised framework that refines reward models using policy samples. The method employs two approaches: unsupervised multi-view learning (UML) to learn robust representations of policy data through information bottleneck optimization, and synthetic preference generation (SPG) to simulate high-quality preference data from policy outputs. Experimental results show RLP consistently outperforms state-of-the-art baselines across three benchmark datasets, with RLP-SPG achieving 50.2% simulated win-rate compared to 46.8% for PPO on AlpacaFarm.

## Method Summary
RLP is an unsupervised framework that refines reward models during RLHF using policy samples to maintain on-distribution accuracy. The method collects preference data and unlabeled instructions, trains initial reward model and policy, then retrains the reward model using either UML or SPG approaches with policy outputs. UML learns robust representations through multi-view information bottleneck loss, while SPG generates synthetic pairwise preferences by clustering policy outputs semantically. The refined reward model is then used to retrain the policy, addressing the distributional shift problem where reward models degrade as policies evolve.

## Key Results
- RLP-SPG achieves 50.2% simulated win-rate vs 46.8% for PPO on AlpacaFarm benchmark
- RLP-SPG achieves 50.5% simulated win-rate vs 47.5% for PPO on LLMBar benchmark
- RLP demonstrates strong performance on knowledge-intensive benchmarks, achieving 37.6% MMLU score vs 36.9% for PPO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLP maintains reward model accuracy during policy optimization by refining the reward model on the policy's evolving data distribution.
- Mechanism: Unsupervised multi-view learning (UML) and synthetic preference generation (SPG) use policy samples to retrain the reward model, keeping it on-distribution.
- Core assumption: Policy samples from the latest policy reflect the target distribution that the reward model needs to generalize to.
- Evidence anchors:
  - [abstract]: "refines a reward model using policy samples to keep it on-distribution"
  - [section 4.2]: "We finally retrain the reward model ˆrϕ using outputs of policy πθ" and "This training objective follows the information bottleneck principle (Tishby et al., 2000) and helps learn robust representations of the policy's data distribution."
  - [corpus]: Weak - no direct mention of RLP's UML or SPG approaches in neighbor papers.
- Break condition: If policy samples become too dissimilar from the original preference data distribution, the unsupervised refinement may introduce bias rather than improve generalization.

### Mechanism 2
- Claim: Synthetic preference generation creates high-quality pairwise preference data from policy outputs, supplementing human preference data.
- Mechanism: SPG generates multiple outputs for each instruction, clusters them semantically, and creates pairwise preferences from the largest and other clusters based on confidence scores.
- Core assumption: The most frequent output in a cluster represents the correct prediction, and confidence can be measured by cluster size relative to total outputs.
- Evidence anchors:
  - [abstract]: "a synthetic preference generation approach is developed to simulate high-quality preference data with policy outputs"
  - [section 4.2]: "Rather than producing and scoring two outputs with LLMs as in Reinforcement Learning from AI Feedback (RLAIF)... RLP-SPG generates a set of outputs for an instruction... This sampling scheme also conforms to the self-consistency assumption"
  - [corpus]: Weak - neighbor papers mention preference generation but not RLP's specific clustering and selective generation approach.
- Break condition: If the semantic clustering fails to capture true equivalence or if the self-consistency assumption breaks down for complex instructions, synthetic preferences may be misleading.

### Mechanism 3
- Claim: The multi-view information bottleneck loss learns robust representations that remove superficial information while preserving task-relevant information.
- Mechanism: UML constructs two views of each policy output, then optimizes an MIB loss that maximizes mutual information between views while minimizing KL divergence from their joint distribution.
- Core assumption: Two semantically invariant views of the same input contain the same task-relevant information but different superficial details.
- Evidence anchors:
  - [section 4.2]: "a multi-view information bottleneck (MIB) loss (Federici et al., 2020) is optimised for unsupervised representation learning, following the information bottleneck principle (Tishby et al., 2000). This optimization process retains task-relevant information in the representations while discarding superficial information."
  - [corpus]: Weak - neighbor papers mention representation learning but not RLP's specific multi-view information bottleneck approach.
- Break condition: If the view construction fails to create truly invariant representations or if the information bottleneck optimization doesn't effectively filter superficial information, the learned representations may not generalize.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF) pipeline
  - Why needed here: RLP builds on standard RLHF by adding unsupervised reward model refinement steps
  - Quick check question: What are the three main steps of standard RLHF and how does RLP modify them?

- Concept: Information Bottleneck Principle
  - Why needed here: UML uses this principle to learn representations that retain task-relevant information while discarding superficial details
  - Quick check question: How does the information bottleneck principle balance compression and prediction in representation learning?

- Concept: Semantic Clustering and Equivalence
  - Why needed here: SPG uses bidirectional entailment clustering to group semantically equivalent outputs for preference generation
  - Quick check question: What algorithm does RLP use to determine semantic equivalence between generated outputs?

## Architecture Onboarding

- Component map: SFT model → Reward model → Policy model → UML/SPG refinement → Retrained reward model → Retrained policy model
- Critical path: SFT → Reward learning → Policy optimization → RLP refinement (UML/SPG) → Final policy
- Design tradeoffs: RLP trades computational overhead for improved reward model generalization; UML focuses on representation learning while SPG focuses on data augmentation
- Failure signatures: Reward model overfitting to policy samples, synthetic preferences being too noisy, computational cost outweighing benefits
- First 3 experiments:
  1. Run standard RLHF pipeline and measure reward model performance degradation on policy samples
  2. Implement UML component and evaluate its impact on reward model generalization
  3. Implement SPG component and measure synthetic preference quality and impact on policy performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RLP compare to other state-of-the-art methods when applied to larger pretrained language models beyond LLaMA-7B?
- Basis in paper: [inferred] The paper mentions that all experiments are conducted on LLaMA-7B and suggests testing on larger models like LLaMA 2 as a future direction.
- Why unresolved: The paper does not provide experimental results for larger language models, which limits the generalizability of the findings to the broader landscape of large language models.
- What evidence would resolve it: Empirical results demonstrating the performance of RLP on larger language models, such as LLaMA 2, would provide insights into its scalability and effectiveness across different model sizes.

### Open Question 2
- Question: What is the impact of RLP on the performance of language models in languages other than English, especially in low-resource languages?
- Basis in paper: [inferred] The paper states that all training and evaluation data are in English and suggests that performance may degenerate in low-resource languages.
- Why unresolved: The lack of multilingual experiments leaves uncertainty about the applicability and effectiveness of RLP in diverse linguistic contexts.
- What evidence would resolve it: Experiments evaluating RLP's performance on multilingual datasets, particularly in low-resource languages, would clarify its cross-lingual capabilities and limitations.

### Open Question 3
- Question: How does the choice of the threshold γ for selective synthetic preference generation in RLP-SPG affect the quality and quantity of generated preference data?
- Basis in paper: [explicit] The paper mentions setting γ = 0.5 for selective generation in RLP-SPG and suggests studying its impact on preference data quality.
- Why unresolved: The paper does not explore the sensitivity of RLP-SPG's performance to different values of γ, which could affect the trade-off between data quality and quantity.
- What evidence would resolve it: A sensitivity analysis varying γ would provide insights into the optimal threshold for balancing preference data quality and quantity, potentially improving RLP-SPG's performance.

### Open Question 4
- Question: What are the computational and resource implications of implementing RLP compared to traditional RLHF methods?
- Basis in paper: [inferred] The paper introduces RLP as an unsupervised framework that refines reward models using policy samples, which may imply additional computational complexity.
- Why unresolved: The paper does not discuss the computational overhead or resource requirements of RLP compared to existing methods, which is crucial for practical deployment.
- What evidence would resolve it: A comparative analysis of the computational cost, memory usage, and training time of RLP versus traditional RLHF methods would provide a clearer understanding of its practical feasibility.

## Limitations
- The paper lacks detailed implementation specifications for UML and SPG components, making faithful reproduction difficult
- Weak connections to existing literature on these specific approaches raise questions about reproducibility and novelty verification
- Claims about maintaining reward model accuracy are supported by win-rate comparisons but lack direct measurements of reward model performance on out-of-distribution samples

## Confidence
- High confidence: The general framework of using policy samples to refine reward models is well-founded and the empirical results on benchmark datasets are robust
- Medium confidence: The specific mechanisms of UML and SPG are innovative but lack detailed validation of their individual contributions to overall performance
- Low confidence: The paper doesn't provide sufficient detail on implementation specifics that would enable faithful reproduction, particularly around the multi-view information bottleneck loss and synthetic preference generation thresholds

## Next Checks
1. Measure reward model accuracy degradation on policy samples over RLHF training iterations to quantify the problem RLP addresses
2. Perform ablation studies isolating UML and SPG contributions to determine which component drives performance improvements
3. Test RLP's robustness when policy samples diverge significantly from original preference data distribution to identify break conditions