---
ver: rpa2
title: 'In-Context Learning on a Budget: A Case Study in Token Classification'
arxiv_id: '2406.13274'
source_url: https://arxiv.org/abs/2406.13274
tags:
- pool
- samples
- selection
- examples
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of in-context learning (ICL)
  with limited annotation budgets, particularly for token classification tasks like
  NER, dependency parsing, and POS tagging. The authors propose a realistic paradigm
  where only a small subset of samples can be annotated from a large corpus, aiming
  to maximize downstream ICL performance.
---

# In-Context Learning on a Budget: A Case Study in Token Classification

## Quick Facts
- arXiv ID: 2406.13274
- Source URL: https://arxiv.org/abs/2406.13274
- Authors: Uri Berger; Tal Baumel; Gabriel Stanovsky
- Reference count: 9
- Small annotated pools (~200 samples) can achieve performance close to full training sets

## Executive Summary
This paper addresses the challenge of in-context learning (ICL) with limited annotation budgets, particularly for token classification tasks like NER, dependency parsing, and POS tagging. The authors propose a realistic paradigm where only a small subset of samples can be annotated from a large corpus, aiming to maximize downstream ICL performance. They evaluate four pool selection strategies—Central, Cluster, Vote-k, and Random—across various LLMs and find that no method significantly outperforms others, with Random surprisingly performing comparably in some scenarios. A small annotated pool (~200 samples) can achieve performance close to using the full training set. However, ICL on a budget still lags behind state-of-the-art fine-tuned methods, except in POS tagging, where it reaches 96% of SOTA performance. The study highlights the importance of demonstration example selection in ICL and advocates for more realistic evaluation setups.

## Method Summary
The authors propose a framework for maximizing ICL performance with limited annotation budgets on token classification tasks. They collect a large corpus of unlabeled text and use various pool selection strategies (Central, Cluster, Vote-k, and Random) to select small subsets for annotation. These annotated pools serve as demonstration examples for ICL prompts. The methods are evaluated across three token classification tasks (NER, dependency parsing, POS tagging) using different LLMs including GPT-4, Code Llama, and Mistral. Performance is measured using task-specific metrics (F1 for NER, LAS for dependency parsing, accuracy for POS tagging) and compared against oracle performance using full training sets and state-of-the-art fine-tuned models.

## Key Results
- No pool selection method significantly outperforms others; Random selection performs comparably to sophisticated strategies
- Very small pools (~200 samples) can achieve over 88% of oracle performance
- ICL on a budget lags behind SOTA fine-tuned methods, except for POS tagging (96% of SOTA)
- Label diversity in demonstration examples correlates significantly with ICL performance (Pearson correlations of 0.64-0.80)

## Why This Works (Mechanism)

### Mechanism 1
Small demonstration pools can approximate full training performance because token classification tasks have relatively stable label distributions across the corpus. The pool selection methods capture sufficient diversity of entity types, dependency relations, and POS tags in small samples, maintaining performance close to using the entire training set. This assumes label distributions in the corpus are representative and stable enough that sampling 0.1-10% of data captures most of the task's complexity. Evidence shows a relatively small annotated sample pool can achieve performance comparable to using the entire training set, with very small pool sizes (220 samples for Ontonotes, 138 samples for UD) achieving over 88% of oracle performance. Break condition: If the corpus contains rare or domain-specific label distributions not captured in small samples, performance would degrade significantly below 88% of oracle performance.

### Mechanism 2
Random sample selection performs comparably to sophisticated selection methods because in-context learning effectiveness depends more on demonstration quality than strategic selection. The model's ability to generalize from demonstrations depends on the inherent quality of examples rather than their strategic selection, making random selection nearly as effective. This assumes demonstration quality (clear labeling, representative examples) matters more than strategic selection for ICL performance. Evidence shows no method consistently outperforms others, with random performing similarly to other methods. Break condition: If strategic selection methods were optimized for ICL-specific metrics rather than general diversity, random selection would perform significantly worse.

### Mechanism 3
Label diversity in demonstration examples correlates with ICL performance because models need exposure to varied examples to generalize effectively. Higher entropy in label distributions across demonstration examples leads to better performance by providing the model with broader contextual understanding. This assumes model generalization improves with exposure to diverse label examples in demonstrations. Evidence shows correlation is high (> 0.5) for all models in dependency parsing and POS tagging tasks, with Pearson correlations of 0.64-0.80. Break condition: If model performance depends more on demonstration ordering or specific example quality rather than label diversity, correlation would weaken or disappear.

## Foundational Learning

- **Token classification task mechanics** (NER, dependency parsing, POS tagging): Understanding specific requirements and evaluation metrics for each task is crucial for implementing correct prompts and interpreting results. Quick check: What's the difference between strict matching for NER and labeled attachment score for dependency parsing?

- **In-context learning paradigm and demonstration example selection**: The entire study revolves around how demonstration examples affect ICL performance and different pool selection strategies. Quick check: Why does sorting demonstrations by similarity to the inference sample improve performance?

- **Pool selection strategies (Central, Cluster, Vote-k, Random)**: The study evaluates these specific strategies for maximizing ICL performance under budget constraints. Quick check: How does the Vote-k method use LLM confidence to select diverse samples?

## Architecture Onboarding

- **Component map**: Corpus → Pool selection strategy → Annotation collection → Prompt generation (with demonstrations) → LLM inference → Performance evaluation
- **Critical path**: Raw corpus → Pool selection → Annotation → Prompt creation → LLM inference → Metric calculation
- **Design tradeoffs**: Budget constraints vs. performance, annotation cost vs. sample diversity, random vs. strategic selection
- **Failure signatures**: Performance plateauing below 88% of oracle, random selection significantly outperforming strategic methods, label diversity correlation weakening
- **First 3 experiments**:
  1. Implement random pool selection and verify baseline performance across all tasks
  2. Add Central method and compare performance to random selection
  3. Implement Cluster method and analyze label diversity correlation with performance

## Open Questions the Paper Calls Out

Open Question 1
- Question: How does the effectiveness of random sample selection for annotation pools compare to more sophisticated methods across different token classification tasks and model types?
- Basis in paper: The paper states that "none of the four methods consistently outperform the others" and "surprisingly, we note that random performs similarly to other methods."
- Why unresolved: While the paper shows random selection performs comparably to other methods in their experiments, it's unclear if this holds across a broader range of tasks, datasets, and model architectures. The study focused on three specific token classification tasks and a limited set of models.
- What evidence would resolve it: A comprehensive study testing random selection against other methods across diverse NLP tasks (e.g., text classification, question answering, summarization), various datasets, and a wider range of model architectures and sizes.

Open Question 2
- Question: What is the optimal pool size for maximizing ICL performance given a fixed annotation budget, and how does this vary across different tasks and models?
- Basis in paper: The paper states that "a relatively small pool (~200 samples) allows LLMs to perform over 88% as when demonstrations are selected from the full training set."
- Why unresolved: The paper provides results for pool sizes ranging from 0.1% to 10% of the maximum pool size, but doesn't determine an optimal size. The relationship between pool size and performance may not be linear, and the optimal size likely varies by task complexity and model capacity.
- What evidence would resolve it: Systematic experiments varying pool sizes more granularly (e.g., 50, 100, 150, 200 samples) across a diverse set of tasks and models, potentially using techniques like learning curves or breakpoint analysis to identify where additional annotations yield diminishing returns.

Open Question 3
- Question: How does the diversity of demonstration examples in the annotation pool impact ICL performance, and can diversity metrics be used to improve sample selection strategies?
- Basis in paper: The paper mentions that "the choice of few-shot examples can lead to improved results" and discusses methods like clustering for maximizing coverage.
- Why unresolved: While the paper tests methods that aim to maximize coverage, it doesn't directly measure or optimize for diversity. The relationship between label space coverage, diversity, and downstream performance is not fully explored.
- What evidence would resolve it: Experiments measuring various diversity metrics (e.g., label entropy, example dissimilarity) for different sample selection strategies and correlating these with performance. Additionally, developing and testing new selection strategies that explicitly optimize for diversity metrics.

## Limitations
- Findings may not generalize beyond token classification tasks to other NLP applications
- Random selection performing comparably to sophisticated methods challenges conventional wisdom and may indicate suboptimal strategy design
- Performance gap remains substantial between ICL on a budget and state-of-the-art fine-tuned methods

## Confidence

**High Confidence:**
- Small annotated pools (~200 samples) can achieve performance close to full training sets for token classification tasks
- ICL on a budget still lags behind state-of-the-art fine-tuned methods in most tasks
- Label diversity in demonstration examples shows significant positive correlation with ICL performance

**Medium Confidence:**
- Random sample selection performs comparably to sophisticated pool selection strategies
- The 88% of oracle performance threshold is achievable with relatively small sample sizes
- Performance scaling behavior is consistent across different LLMs and tasks

**Low Confidence:**
- The mechanisms explaining why random selection works as well as strategic methods
- Generalization of findings to non-token classification tasks or different domains
- The specific contribution of each pool selection strategy to overall performance

## Next Checks
1. Test Additional Tasks and Domains: Evaluate the same pool selection strategies on different NLP tasks (e.g., text classification, question answering) and domains (e.g., biomedical, legal) to determine if the random selection equivalence holds across broader applications.

2. Optimize Selection Strategies for ICL: Design and test pool selection methods specifically optimized for ICL performance metrics rather than general diversity or coverage measures to see if sophisticated strategies can outperform random selection.

3. Analyze Demonstration Ordering Effects: Systematically vary the order of demonstration examples in prompts while keeping pool selection constant to quantify the impact of ordering versus selection strategy on ICL performance.