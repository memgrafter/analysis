---
ver: rpa2
title: 'ELFS: Label-Free Coreset Selection with Proxy Training Dynamics'
arxiv_id: '2406.04273'
source_url: https://arxiv.org/abs/2406.04273
tags:
- coreset
- selection
- pruning
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting representative
  subsets of data for efficient model training, focusing on label-free coreset selection
  to reduce human annotation costs. The authors propose ELFS (Effective Label-Free
  Coreset Selection), a method that leverages deep clustering to generate pseudo-labels
  and calculate training dynamics-based data difficulty scores without requiring ground-truth
  labels.
---

# ELFS: Label-Free Coreset Selection with Proxy Training Dynamics

## Quick Facts
- arXiv ID: 2406.04273
- Source URL: https://arxiv.org/abs/2406.04273
- Reference count: 40
- Outperforms existing label-free baselines, achieving up to 10.2% higher accuracy than D2 on ImageNet-1K at 90% pruning rate

## Executive Summary
ELFS addresses the challenge of selecting representative data subsets for efficient model training without requiring ground truth labels. The method leverages deep clustering to generate pseudo-labels and calculate training dynamics-based difficulty scores, then applies double-end pruning to mitigate bias in these scores. Evaluated across five vision benchmarks, ELFS consistently outperforms existing label-free baselines and achieves comparable performance to supervised methods at lower pruning rates, marking a significant advancement in label-free coreset selection.

## Method Summary
ELFS is a label-free coreset selection method that uses TEMI deep clustering to generate pseudo-labels from unlabeled data. These pseudo-labels enable calculation of training dynamics-based difficulty scores (AUM, forgetting score) without ground truth labels. The method then applies a double-end pruning strategy to select coresets, first pruning hard examples then continuing to prune easy examples until the budget is met. β selection is guided by pseudo-label validation accuracy, and the final coreset is used for model training with reduced annotation costs.

## Key Results
- ELFS achieves up to 10.2% higher accuracy than D2 on ImageNet-1K at 90% pruning rate
- At 30% and 50% pruning rates, ELFS matches or exceeds supervised coreset selection methods on CIFAR10 and ImageNet-1K
- Consistently outperforms existing label-free baselines across all five evaluated vision benchmarks (CIFAR10, CIFAR100, CINIC10, STL10, ImageNet-1K)

## Why This Works (Mechanism)

### Mechanism 1
Deep clustering can generate pseudo-labels that provide sufficiently accurate data difficulty scores for coreset selection, even without ground truth labels. TEMI deep clustering uses ensemble teacher-student heads and mutual information maximization to assign pseudo-labels to unlabeled data. These pseudo-labels enable calculation of training dynamics-based difficulty scores like AUM and forgetting scores, which capture how models learn from each example during training. Core assumption: Training dynamics scores derived from pseudo-labels are still meaningful proxies for data difficulty, despite potential label noise. Break condition: If pseudo-label misclassification rate becomes too high (>50%), training dynamics scores lose correlation with true data difficulty.

### Mechanism 2
Double-end pruning mitigates bias in pseudo-label-based difficulty scores by reducing the number of easy examples selected in the coreset. After selecting data based on pseudo-label difficulty scores, the method first prunes β hard examples, then continues pruning easy examples until budget is met. This balances the coreset composition by ensuring difficult examples are not underrepresented. Core assumption: Pseudo-label difficulty scores tend to overestimate the difficulty of some examples and underestimate others, creating bias that double-end pruning can correct. Break condition: If β is set too high, the coreset may become too difficult and lose diversity; if too low, easy examples may still dominate.

### Mechanism 3
Pseudo-labels enable hyperparameter tuning for β in label-free settings through validation set performance. The method splits pseudo-labeled data into training (90%) and validation (10%) sets, uses validation set pseudo-label accuracy to select optimal β, then applies this β to select coresets from the full dataset. Core assumption: Pseudo-label validation accuracy correlates with ground-truth validation accuracy when selecting optimal β for coreset selection. Break condition: If pseudo-label validation accuracy poorly correlates with ground-truth accuracy (e.g., due to class imbalance or domain shift), the selected β may not generalize.

## Foundational Learning

- **Training dynamics and difficulty scores**: Understanding how data difficulty can be quantified through metrics like Area Under the Margin (AUM) and forgetting scores, which track model behavior during training. Quick check: What does a high AUM value indicate about a data point's difficulty during training?

- **Deep clustering and pseudo-label generation**: The method uses TEMI deep clustering to generate pseudo-labels without ground truth, which is fundamental to the entire label-free approach. Quick check: How does TEMI's ensemble teacher-student architecture improve pseudo-label quality compared to single-head clustering?

- **Coreset selection optimization**: Understanding the optimization problem of selecting k representative samples from N data points to maximize model performance is crucial for grasping the method's objective. Quick check: In the coreset selection formulation, what does the objective function aim to minimize?

## Architecture Onboarding

- **Component map**: Pretrained vision model (SwAV, CLIP, DINO) → K-NN search → TEMI deep clustering → Training dynamics collection → Difficulty score calculation (AUM, forgetting) → Double-end pruning → Final coreset selection

- **Critical path**: Pretrained model → K-NN → TEMI clustering → Training dynamics collection → Difficulty scoring → Double-end pruning → Coreset selection

- **Design tradeoffs**: 
  - Choice of pretrained model affects pseudo-label quality vs computational cost
  - K-NN parameter affects clustering granularity vs memory usage
  - β selection impacts coreset difficulty balance vs coverage
  - Double-end pruning adds complexity but improves performance

- **Failure signatures**:
  - Low pseudo-label accuracy (>30% misclassification) leads to poor difficulty scores
  - Incorrect β selection results in coresets dominated by easy or hard examples
  - Poor pretrained model choice causes clustering collapse
  - Insufficient training dynamics collection leads to unreliable difficulty estimates

- **First 3 experiments**:
  1. Run TEMI clustering on CIFAR10 with DINO embeddings and verify pseudo-label accuracy
  2. Calculate AUM and forgetting scores using pseudo-labels and compare distribution to ground truth
  3. Test double-end pruning with different β values on CIFAR100 to find optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of the number of nearest neighbors (k-NN) in the embedding space affect the quality of pseudo-labels and subsequent coreset selection performance? The paper mentions using 25-NN for ImageNet-1K and 50-NN for other datasets, but does not explore the impact of varying this parameter. Conducting experiments with different k-NN values (e.g., 10, 25, 50, 100) and comparing the resulting pseudo-label quality and coreset selection performance would provide insights into the optimal k-NN value for different datasets and tasks.

### Open Question 2
Can the double-end pruning method be further improved by incorporating additional information beyond the pseudo-label-based difficulty scores? The paper proposes double-end pruning to address the distribution shift in pseudo-label-based difficulty scores, but does not explore incorporating other information to enhance the pruning process. Experimenting with variations of the double-end pruning method that incorporate additional information, such as embedding distances or model confidence scores, and comparing their performance to the standard double-end pruning approach would provide insights into potential improvements.

### Open Question 3
How does the performance of ELFS vary with different levels of class imbalance in the dataset? The paper mentions that some pretrained models may produce pseudo-labels with imbalanced class assignments, but does not explore how this affects the coreset selection performance. Conducting experiments on datasets with varying degrees of class imbalance and comparing the coreset selection performance of ELFS to other methods would provide insights into its robustness to class imbalance.

## Limitations
- Performance heavily depends on pseudo-label quality, with no explicit error bounds for low-accuracy scenarios
- Double-end pruning mechanism lacks theoretical justification for why β selection based on pseudo-label validation accuracy correlates with ground-truth performance
- Computational overhead of the full pipeline is not explicitly quantified against simpler baselines

## Confidence
- **High confidence**: Empirical results showing ELFS outperforming label-free baselines on multiple benchmarks
- **Medium confidence**: Mechanism claims about double-end pruning effectiveness, primarily supported by ablation studies
- **Medium confidence**: Pseudo-label validation accuracy correlation with ground-truth performance for β selection, demonstrated empirically but not rigorously proven

## Next Checks
1. Systematically vary pseudo-label accuracy and measure correlation between pseudo-label accuracy and coreset selection performance to identify critical accuracy threshold
2. Conduct controlled experiments removing double-end pruning while keeping all other factors constant to isolate its specific contribution
3. Measure and compare wall-clock time and memory requirements of ELFS against simpler label-free baselines across different dataset sizes to quantify practical trade-offs