---
ver: rpa2
title: 'Large Language Models as Urban Residents: An LLM Agent Framework for Personal
  Mobility Generation'
arxiv_id: '2402.14744'
source_url: https://arxiv.org/abs/2402.14744
tags:
- activity
- your
- data
- daily
- pattern
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an LLM agent framework for personal mobility
  generation, leveraging LLMs' semantic interpretability and versatility to generate
  realistic urban activity trajectories. The method extracts habitual activity patterns
  from historical data using a self-consistency evaluation, then retrieves motivations
  (either evolving-based or learning-based) to guide LLM agents in generating daily
  activity plans.
---

# Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation

## Quick Facts
- **arXiv ID**: 2402.14744
- **Source URL**: https://arxiv.org/abs/2402.14744
- **Reference count**: 40
- **Primary result**: LLM-based framework outperforms DeepMove, TrajGAIL, and DiffTraj on personal mobility generation with DARD scores of 0.125-0.141 JSD

## Executive Summary
This paper introduces an LLM agent framework for generating realistic personal mobility trajectories in urban environments. The method combines habitual activity pattern extraction with motivation-driven activity generation, leveraging LLMs' semantic interpretability to simulate how urban residents make daily activity decisions. Evaluated on Tokyo personal activity data, the framework demonstrates superior performance compared to state-of-the-art methods in generating temporally and semantically consistent activity patterns, while also showing adaptability to external factors like pandemic scenarios.

## Method Summary
The framework operates in two phases: first, it extracts activity patterns from historical data using a self-consistency evaluation that scores candidate patterns based on how well they align with target user trajectories versus others. Second, it generates daily activity plans by retrieving motivations through either evolving-based (considering past activities) or learning-based (learning from similar historical dates) strategies, then combining these with the identified patterns to guide LLM agents. The approach is evaluated across four metrics (step distance, step interval, daily activity routine distribution, and spatial-temporal visits distribution) using Jensen-Shannon divergence against real data.

## Key Results
- Outperforms state-of-the-art methods (DeepMove, TrajGAIL, DiffTraj) with DARD scores of 0.125-0.141 JSD versus 0.218-0.693 for baselines
- Demonstrates adaptability to pandemic scenarios by reducing activity frequencies in line with real-world behavioral shifts
- Shows consistent performance across different LLM models (GPT-3.5-turbo, GPT-4o-mini, Llama 3-8B) and cities (Tokyo, Osaka)
- Ablation studies confirm the importance of patterns, self-consistency evaluation, and motivations for performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework generates realistic activity trajectories by combining habitual patterns with situational motivations, leveraging LLMs' semantic interpretability
- Mechanism: Activity patterns are first extracted from historical data using a self-consistency evaluation, then motivations (evolving-based or learning-based) are retrieved and combined with these patterns to guide LLM agents in generating daily activity plans
- Core assumption: Individual activities are primarily influenced by habitual patterns and current motivations
- Evidence anchors:
  - [abstract] "Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility."
  - [section] "LLM agents to infer activity patterns and motivation for personal activity generation tasks."
  - [corpus] Found 25 related papers; average neighbor FMR=0.469, suggesting moderate relatedness
- Break condition: If the self-consistency evaluation fails to identify reliable patterns, or if motivations are poorly aligned with actual user behavior

### Mechanism 2
- Claim: Self-consistency evaluation ensures LLM agents align closely with real-world activity data by comparing generated patterns against both target and non-target user trajectories
- Mechanism: Candidate patterns are scored by having the LLM rate how likely a target user would follow sample trajectories based on each pattern. Higher scores are given for target user trajectories and lower scores for others
- Core assumption: Activity patterns derived from target user data should be more consistent with that user's own trajectories than with others'
- Evidence anchors:
  - [section] "We implement a scoring mechanism to evaluate the consistency of the candidate patterns to identify the most plausible one."
  - [section] "This function evaluates cp against two distinct sets of activity trajectories: the specific activity trajectories Ti of a targeted resident i and the sampled activity trajectories from other residents T∼i."
  - [corpus] Weak evidence; only 25 related papers found, suggesting limited prior work on this specific evaluation method
- Break condition: If the LLM's ratings are inconsistent or do not distinguish well between target and non-target trajectories

### Mechanism 3
- Claim: The framework's adaptability to external factors (e.g., pandemic) demonstrates its potential for simulating real-world urban mobility scenarios
- Mechanism: By prompting LLM agents with context-specific scenarios (e.g., "pandemic" prompt), the framework can generate activity trajectories that reflect behavioral shifts under those conditions
- Core assumption: LLMs can simulate realistic responses to external factors when provided with appropriate contextual prompts
- Evidence anchors:
  - [abstract] "It also adapts to external factors like pandemic scenarios, reducing activity frequencies in line with real-world behavioral shifts."
  - [section] "By integrating the above prompt, we can observe the impact of external elements, such as the pandemic and the government's measures, on urban mobility."
  - [corpus] Moderate evidence; 25 related papers suggest some prior work on LLM applications in urban mobility
- Break condition: If the LLM fails to generate plausible activity changes in response to external prompts, or if the changes do not align with real-world data

## Foundational Learning

- **Activity trajectory generation and its importance in urban mobility analysis**
  - Why needed here: Understanding how individual activity patterns contribute to urban mobility is crucial for effective transportation planning and urban development
  - Quick check question: How do individual activity trajectories impact overall urban mobility patterns?

- **Self-consistency evaluation and its role in ensuring data alignment**
  - Why needed here: Ensures that generated patterns are representative of the target user's behavior and not just generic patterns
  - Quick check question: Why is it important to compare generated patterns against both target and non-target user trajectories?

- **Motivation retrieval strategies and their influence on activity generation**
  - Why needed here: Captures the dynamic and situational elements that influence individual choices, allowing for more realistic and context-aware activity generation
  - Quick check question: What are the differences between evolving-based and learning-based motivation retrieval strategies?

## Architecture Onboarding

- **Component map**: Preprocessing -> Pattern Extraction -> Self-Consistency Evaluation -> Motivation Retrieval -> Activity Generation
- **Critical path**: Preprocessing → Pattern Extraction → Self-Consistency Evaluation → Motivation Retrieval → Activity Generation
- **Design tradeoffs**:
  - Using LLM for pattern evaluation vs. simpler rule-based methods: LLMs offer more nuanced and semantic understanding but are computationally expensive
  - Evolving-based vs. learning-based motivation retrieval: Evolving-based is more intuitive but requires more historical data; learning-based is more efficient but relies on training a similarity model
  - Number of candidate personas: More personas increase diversity but also computational cost
- **Failure signatures**:
  - Poor self-consistency scores: Indicates that candidate patterns are not well-aligned with target user behavior
  - Low motivation relevance: Suggests that retrieved motivations do not accurately reflect the target user's daily needs or interests
  - Unrealistic activity trajectories: Implies issues in the combination of patterns and motivations or in the LLM's activity generation capabilities
- **First 3 experiments**:
  1. Evaluate self-consistency evaluation by comparing scores for target vs. non-target trajectories
  2. Compare evolving-based and learning-based motivation retrieval strategies in terms of their impact on activity generation quality
  3. Test the framework's adaptability by generating activity trajectories under different external scenarios (e.g., normal vs. pandemic)

## Open Questions the Paper Calls Out
The paper identifies three key limitations:
1. The framework models individual agents without considering interactions between them, missing potential social dynamics in urban mobility
2. Cost-efficiency considerations limited evaluation to GPT-3.5, with only partial results for other models
3. The framework relies on relatively complete historical data, which may not capture all relevant behavioral patterns

## Limitations
- Performance heavily dependent on quality and comprehensiveness of historical activity data
- Reliance on LLMs introduces potential biases and computational costs that scale with data volume
- Current implementation focuses on daily activity generation and may not capture longer-term behavioral changes

## Confidence
- **High confidence** in the framework's ability to generate realistic activity trajectories when sufficient historical data is available, supported by strong quantitative results across multiple metrics
- **Medium confidence** in the self-consistency evaluation mechanism's ability to distinguish between target and non-target user patterns, as the evaluation method is novel but relies on LLM judgment quality
- **Medium confidence** in the framework's adaptability to external factors, based on pandemic scenario testing, though real-world validation across diverse conditions would strengthen this claim

## Next Checks
1. Conduct cross-validation across different user segments to test the framework's generalizability and identify potential demographic or behavioral biases in pattern extraction
2. Perform ablation studies removing the self-consistency evaluation step to quantify its contribution to performance improvements over baseline methods
3. Test the framework's responsiveness to multiple external scenarios (not just pandemic) including seasonal changes, special events, and policy interventions to validate robustness claims