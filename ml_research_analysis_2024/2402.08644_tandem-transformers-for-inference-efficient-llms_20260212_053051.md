---
ver: rpa2
title: Tandem Transformers for Inference Efficient LLMs
arxiv_id: '2402.08644'
source_url: https://arxiv.org/abs/2402.08644
tags:
- tandem
- speed
- secondary
- block
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Tandem Transformers, a novel architecture
  that improves inference efficiency of large language models (LLMs) by decoupling
  prompt processing capacity from response generation capacity. The architecture combines
  a small autoregressive model (MS) with a large model operating in block mode (ML),
  where MS attends to ML's richer representations during autoregressive generation.
---

# Tandem Transformers for Inference Efficient LLMs

## Quick Facts
- arXiv ID: 2402.08644
- Source URL: https://arxiv.org/abs/2402.08644
- Authors: Aishwarya P S; Pranav Ajit Nair; Yashas Samaga; Toby Boyd; Sanjiv Kumar; Prateek Jain; Praneeth Netrapalli
- Reference count: 23
- Primary result: 3.3% accuracy improvement over standalone PaLM2-Gecko with 1.16x speedup

## Executive Summary
This paper introduces Tandem Transformers, an architecture that improves inference efficiency by decoupling prompt processing from response generation. The key innovation is a small autoregressive model that attends to representations from a larger block-mode model, enabling high-quality response generation at reduced computational cost. The Tandem of PaLM2-Bison and PaLM2-Gecko achieved 3.3% better next-token prediction accuracy than standalone PaLM2-Gecko while being 1.16x faster than PaLM2-Otter.

## Method Summary
Tandem Transformers combine a small autoregressive model (MS) with a large block-mode model (ML), where MS attends to ML's representations during generation. The architecture supports both frozen ML and joint training approaches, with cross entropy loss and optional distillation loss. Training uses fixed block length γ=2, but inference supports arbitrary γ values. When integrated with speculative decoding (SPEED), Tandem achieves approximately 1.14x speedup over vanilla PaLM2-Gecko while maintaining identical downstream accuracy through higher-quality draft generation.

## Key Results
- 3.3% improvement in next-token prediction accuracy over standalone PaLM2-Gecko
- 1.16x speedup compared to PaLM2-Otter with comparable downstream performance
- 1.14x speedup with Tandem + SPEED while maintaining identical downstream accuracy
- Adaptive block length reduces latency by 1.04x to 1.09x on multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Tandem architecture achieves inference speedup by processing the query with a large model (ML) while generating responses with a small model (MS) that attends to ML's representations.
- Mechanism: ML processes the prompt in block mode (non-autoregressive) to compute rich representations. MS generates response tokens autoregressively but attends to ML's representations, allowing it to leverage ML's understanding while maintaining low computational cost for generation.
- Core assumption: High-quality response generation can be maintained when the small model has access to the large model's prompt representations, even though MS has less capacity than ML.
- Evidence anchors:
  - [abstract]: "The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations."
  - [section 1]: "ML processes the prompt/query...MS generates the first γ tokens (called a block) autoregressively, while attending to the prompt/query representations generated by ML."
  - [corpus]: Weak evidence - no direct corpus citations found for this specific mechanism.
- Break condition: If the small model cannot effectively utilize the large model's representations, or if the attention mechanism fails to transfer meaningful information from ML to MS.

### Mechanism 2
- Claim: Tandem + SPEED achieves additional speedup by using MS as a high-quality drafter that generates draft tokens verified by ML.
- Mechanism: MS generates draft tokens autoregressively while attending to ML's representations, producing higher-quality drafts than standard speculative decoding. ML then verifies these drafts, ensuring output quality matches the primary model while reducing verification overhead.
- Core assumption: The enhanced draft quality from MS attending to ML's representations reduces verification failures and improves acceptance rates.
- Evidence anchors:
  - [abstract]: "This ensures that the Tandem of PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14× faster than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream task accuracy."
  - [section 1]: "The speculative decoding (SPEED) framework leverages the small model MS in Tandem to generate draft tokens, which are then verified by the large model ML."
  - [section 4]: "On the Reddit Posts dataset, using the MS in Tandem as the drafter model in SPEED leads to about 11.24% higher per-block acceptance rate compared to a vanilla secondary model."
- Break condition: If the verification overhead becomes too high due to draft quality issues, or if the acceptance rate doesn't improve sufficiently to justify the architecture.

### Mechanism 3
- Claim: Adaptive block length in SPEED further reduces latency by dynamically adjusting the number of tokens drafted before verification.
- Mechanism: A router MLP predicts whether current draft tokens are likely to be accepted by ML, allowing MS to draft more tokens when confidence is high and verifying sooner when confidence is low.
- Core assumption: The router MLP can accurately predict verification outcomes based on MS's entropy, top-k probabilities, and model embeddings.
- Evidence anchors:
  - [section 4]: "We train a relatively small 2-layer multi-layer perceptron – router MLP – model to predict whether the current draft token from MS is likely to be accepted by the primary model ML."
  - [section 1]: "Adaptive Block Length: Enhances Tandem + SPEED by dynamically adjusting drafted token count."
  - [corpus]: Weak evidence - no direct corpus citations found for this specific adaptive mechanism.
- Break condition: If the router MLP makes poor predictions, leading to suboptimal block lengths that don't improve overall latency.

## Foundational Learning

- Concept: Autoregressive vs. non-autoregressive processing
  - Why needed here: Understanding the fundamental difference between sequential token generation (autoregressive) and parallel processing (non-autoregressive) is crucial for grasping why Tandem can achieve speedup.
  - Quick check question: What is the main computational advantage of processing the prompt in block mode versus processing tokens one by one?

- Concept: Attention mechanisms and representation transfer
  - Why needed here: The core innovation relies on the small model attending to the large model's representations, which requires understanding how attention works across different model components.
  - Quick check question: How does the small model in Tandem access the large model's representations during autoregressive generation?

- Concept: Speculative decoding framework
  - Why needed here: Tandem + SPEED builds on speculative decoding, so understanding how draft-then-verify works is essential for comprehending the full architecture.
  - Quick check question: In speculative decoding, what role does the smaller drafter model play compared to the larger verifier model?

## Architecture Onboarding

- Component map: Query → ML prompt processing → ML representations → MS generation (attending to ML) → MS response tokens → (in SPEED) ML verification
- Critical path: Query → ML prompt processing → ML representations → MS generation (attending to ML) → MS response tokens → (in SPEED) ML verification
- Design tradeoffs:
  - Model size ratio: Larger MS improves draft quality but reduces speedup; smaller MS increases speedup but may hurt quality
  - Block length γ: Larger γ improves ML utilization but may increase latency if drafts are rejected
  - Training approach: Frozen ML vs. joint training affects quality-latency tradeoff
- Failure signatures:
  - High verification rejection rates indicate MS drafts are poor quality
  - Memory bottlenecks suggest block size γ is too large for available resources
  - Degraded accuracy indicates insufficient transfer of ML's understanding to MS
- First 3 experiments:
  1. Implement Tandem with frozen ML and measure accuracy vs standalone MS to verify representation transfer
  2. Test different block lengths γ to find optimal tradeoff between ML utilization and draft quality
  3. Compare Tandem + SPEED acceptance rates against standard speculative decoding to validate improved draft quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Tandem Transformer architecture perform when using smaller or larger block lengths (γ) than those tested in the experiments?
- Basis in paper: [explicit] The paper states that "While we train Tandem Transformers with a fixed block length γ, the architecture supports arbitrary γ values during inference" and mentions that "Larger γ values generally improve efficiency by maximizing the primary model's (ML) utilization of accelerator hardware."
- Why unresolved: The paper primarily tests Tandem with γ = 2 for training and uses different γ values for inference, but does not provide a comprehensive study on how varying γ affects performance across a wider range of values.
- What evidence would resolve it: Experiments testing Tandem with a broader range of γ values during both training and inference, measuring the impact on accuracy, efficiency, and latency.

### Open Question 2
- Question: Can the Tandem Transformer architecture be effectively applied to encoder-decoder models, or is it limited to decoder-only architectures?
- Basis in paper: [inferred] The paper mentions that "Tandem is able to generate only block-size γ tokens through the secondary model MS and then refresh the entire prefill representations using primary model ML which is critical to maintaining high accuracy" and states that "by setting γ = 0, Tandem can mimic decoder-only ML model while setting γ → ∞ leads to decoder-only MS model."
- Why unresolved: The paper does not explore the application of Tandem to encoder-decoder models, leaving uncertainty about its effectiveness in that context.
- What evidence would resolve it: Experiments applying the Tandem architecture to encoder-decoder models, comparing performance with standard encoder-decoder and Tandem decoder-only models.

### Open Question 3
- Question: How does the Tandem Transformer architecture compare to other speculative decoding techniques, such as MEDUSA, in terms of efficiency and accuracy?
- Basis in paper: [explicit] The paper states that "Tandem + SPEED is able to use representations of ML while still generating tokens autoregressively, which is able to provide overall much better tradeoff in terms of token quality vs model latency for the drafter."
- Why unresolved: While the paper compares Tandem + SPEED to standard speculative decoding, it does not directly compare Tandem to other advanced speculative decoding techniques like MEDUSA.
- What evidence would resolve it: Experiments comparing Tandem + SPEED to other speculative decoding techniques, measuring token quality, model latency, and overall efficiency.

## Limitations
- Results are primarily validated on PaLM2 models, limiting generalizability to other model families
- The adaptive block length mechanism relies on a router MLP trained on a limited dataset
- Claims of "identical" accuracy with Tandem + SPEED need more rigorous validation across diverse evaluation sets

## Confidence
- High Confidence: The core architectural innovation of decoupling prompt processing from response generation through cross-model attention is technically sound and well-demonstrated. The 3.3% accuracy improvement over standalone PaLM2-Gecko is robust and repeatable.
- Medium Confidence: The speedup claims (1.16x over PaLM2-Otter, 1.14x with SPEED) are valid for the specific PaLM2 model family tested, but may not generalize to other model architectures or scales. The adaptive block length mechanism shows promise but needs validation on broader workloads.
- Low Confidence: The claim that Tandem + SPEED maintains "identical" accuracy to vanilla models while achieving substantial speedup needs more rigorous validation across diverse evaluation sets and model families beyond PaLM2.

## Next Checks
1. **Cross-Model Generalization Test**: Evaluate Tandem architecture with different model families (e.g., LLaMA, Mistral) and varying size ratios to determine if the accuracy-speedup tradeoff holds across architectures, not just PaLM2 models.

2. **Real-World Workload Analysis**: Test adaptive block length on production workloads with diverse prompt lengths and response patterns to validate whether the router MLP generalizes beyond the synthetic training dataset used.

3. **Comparison Against State-of-the-Art**: Benchmark Tandem + SPEED against the latest speculative decoding implementations and other inference optimization techniques on standard LLM leaderboards to establish its position in the current optimization landscape.