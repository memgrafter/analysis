---
ver: rpa2
title: 'I Know How: Combining Prior Policies to Solve New Tasks'
arxiv_id: '2406.09835'
source_url: https://arxiv.org/abs/2406.09835
tags:
- policies
- different
- agents
- learning
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes I Know How (IKH), a framework for combining
  prior policies to solve new tasks in reinforcement learning. IKH leverages a set
  of pre-trained policies and a master policy that predicts weights to combine their
  behaviors, enabling efficient adaptation to new scenarios.
---

# I Know How: Combining Prior Policies to Solve New Tasks

## Quick Facts
- arXiv ID: 2406.09835
- Source URL: https://arxiv.org/abs/2406.09835
- Reference count: 20
- Primary result: IKH framework combines pre-trained policies to solve new tasks in RL, outperforming baselines on a complex racetrack environment

## Executive Summary
This paper introduces I Know How (IKH), a framework that combines pre-trained policies to solve new reinforcement learning tasks efficiently. IKH uses a master policy to predict weights that linearly combine the outputs of specialized pre-trained policies, enabling agents to adapt to new scenarios without catastrophic forgetting. The method is evaluated on a simulated driving environment, demonstrating superior performance compared to standard SAC and PNN baselines in completing sectors of a complex racetrack. IKH also offers explainability by revealing which pre-trained policies are most relevant in different scenarios.

## Method Summary
IKH employs a set of pre-trained policies specialized on auxiliary tasks and a master policy that predicts weights to combine their behaviors for new tasks. The framework operates by freezing pre-trained policies and training the master policy to select appropriate weightings based on the current state. At each step, the master policy observes the state, predicts weights for each pre-trained policy, and combines their actions through weighted averaging. The method uses SAC for training and evaluates performance on a complex racetrack environment divided into nine sectors, comparing against SAC from scratch and PNN baselines.

## Key Results
- IKH outperforms both SAC and PNN baselines in completing racetrack sectors, with higher average sector completion and more consistent performance
- The framework demonstrates explainability by revealing which pre-trained policies are most relevant in different scenarios through weight analysis
- IKH maintains performance while using fewer total training steps by leveraging pre-trained policies rather than learning from scratch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The master policy πθ_M P learns to assign appropriate weights to pre-trained policies based on the current state, enabling effective task adaptation.
- Mechanism: The master policy observes the current state and predicts a weight vector w, where each element wi corresponds to the contribution of policy πi. These weights are used to compute a weighted average of the pre-trained policies' actions, allowing the agent to dynamically combine their behaviors for the new task.
- Core assumption: The pre-trained policies' actions are valid for the new task and share the same state and action space.
- Evidence anchors:
  - [abstract]: "IKH leverages a set of pre-trained policies and a master policy that predicts weights to combine their behaviors, enabling efficient adaptation to new scenarios."
  - [section]: "At each step, it computes the outputs of the models based on the current state - lines 4, 5 - and in line 6 determine the actual action."
  - [corpus]: Weak. Corpus neighbors focus on related RL topics but do not directly support this specific mechanism.

### Mechanism 2
- Claim: IKH achieves explainability by analyzing how different pre-trained policies are exploited in different scenarios.
- Mechanism: By examining the weights predicted by the master policy for each pre-trained policy in various situations, we can understand which policies are more relevant for specific scenarios. This provides insights into the decision-making process of the agent.
- Core assumption: The weights assigned by the master policy reflect the importance of each pre-trained policy in a given situation.
- Evidence anchors:
  - [abstract]: "IKH also offers explainability by analyzing how different pre-trained policies are exploited in different scenarios."
  - [section]: "In Fig.4 we report three frames of the M.I.R.IN. configuration in different parts of the track. As additional information below is reported the four different pre-trained policies used in this setting, where they are colored based on the weight assigned by the master policy."
  - [corpus]: Weak. The corpus does not provide direct evidence for this mechanism.

### Mechanism 3
- Claim: IKH maintains scalability by allowing the integration of new policies and adaptation to new scenarios without forgetting previously learned knowledge.
- Mechanism: The IKH framework is designed to be modular and compositional. It allows for the incremental addition of new pre-trained policies as the agent faces new tasks. The master policy learns to leverage the existing policies and incorporate new ones, enabling the agent to adapt to new scenarios while retaining prior knowledge.
- Core assumption: The framework can accommodate new policies without interfering with the learning of existing ones.
- Evidence anchors:
  - [abstract]: "The IKH framework offers scalability, facilitating the integration of new policies and adaptation to new scenarios."
  - [section]: "In this work, we consider a simple and initial setting of the IKH framework (Fig.1). In particular, we assume a variable set of pre-trained policies specialized on simple scenarios."
  - [corpus]: Weak. The corpus does not provide direct evidence for this mechanism.

## Foundational Learning

- Concept: Multi-Task Reinforcement Learning
  - Why needed here: IKH is designed to solve new tasks by combining prior policies, which is a key aspect of multi-task reinforcement learning.
  - Quick check question: What is the main challenge addressed by multi-task reinforcement learning, and how does IKH aim to overcome it?

- Concept: Catastrophic Forgetting
  - Why needed here: IKH aims to avoid catastrophic forgetting by leveraging pre-trained policies and learning to combine their behaviors, rather than learning from scratch for each new task.
  - Quick check question: How does IKH's approach of using pre-trained policies and a master policy help mitigate the problem of catastrophic forgetting?

- Concept: Policy Composition
  - Why needed here: IKH relies on the composition of pre-trained policies' behaviors through a weighted average, guided by the master policy's predictions.
  - Quick check question: What is the role of the combination module in IKH, and how does it enable the composition of pre-trained policies' behaviors?

## Architecture Onboarding

- Component map:
  - Pre-trained Policies (Φ) -> Master Policy (πθ_M P) -> Combination Module (C) -> Environment

- Critical path:
  1. Load and freeze the pre-trained policies (Φ).
  2. Observe the current state (st) from the environment.
  3. Compute the outputs of the pre-trained policies (Φ(st)) and the master policy (πθ_M P(st)).
  4. Combine the pre-trained policies' outputs using the weights predicted by the master policy to determine the actual action (at).
  5. Execute the action in the environment and observe the next state (st+1) and reward (rt).
  6. Store the transition tuple and update the master policy using the chosen RL algorithm.

- Design tradeoffs:
  - Flexibility vs. Complexity: Allowing the combination module to be a parameterized neural network increases flexibility but also adds complexity to the learning process.
  - Scalability vs. Performance: Incrementally adding new pre-trained policies improves scalability but may require the master policy to learn more complex weight assignments, potentially impacting performance.

- Failure signatures:
  - Poor performance on the new task: Indicates that the master policy is not effectively combining the pre-trained policies' behaviors or that the pre-trained policies are not suitable for the new task.
  - Catastrophic forgetting: Suggests that the framework is not properly retaining the knowledge from pre-trained policies when adapting to new tasks.
  - Lack of explainability: Implies that the weights predicted by the master policy do not provide meaningful insights into the decision-making process.

- First 3 experiments:
  1. Implement a simple IKH framework with a small set of pre-trained policies and evaluate its performance on a new task, comparing it to a baseline method (e.g., SAC).
  2. Analyze the weights predicted by the master policy in different scenarios to assess the explainability of the framework.
  3. Gradually increase the number of pre-trained policies and evaluate the scalability and performance of the framework on more complex tasks.

## Open Questions the Paper Calls Out
- How does the performance of IKH scale with the number and diversity of pre-trained policies?
- How does IKH handle catastrophic forgetting when new tasks are introduced sequentially?
- What are the limitations of IKH in real-world environments with higher complexity and variability?

## Limitations
- The paper doesn't provide detailed implementation specifications for the combination module and reward function, making exact reproduction challenging
- Limited empirical evidence for the framework's ability to handle catastrophic forgetting and scalability claims
- Performance evaluation is limited to a simulated driving environment, with unclear generalization to more complex real-world scenarios

## Confidence
- **High Confidence:** The core concept of combining pre-trained policies through a master policy is well-supported by the experimental results and ablation studies showing superior performance on the racetrack environment.
- **Medium Confidence:** The explainability claims are reasonably supported by the weight visualization analysis, though the corpus lacks direct evidence for this mechanism.
- **Low Confidence:** The scalability claims regarding adding new policies without interference are weakly supported, with no empirical evidence beyond the single complex task evaluation.

## Next Checks
1. Test the framework's ability to incorporate additional pre-trained policies incrementally while maintaining performance on previously learned tasks.
2. Evaluate the robustness of the combination module by testing with different normalization strategies and edge case handling methods.
3. Compare the weight assignments predicted by the master policy against human-interpretable baselines to validate the explainability claims across diverse scenarios.