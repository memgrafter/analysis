---
ver: rpa2
title: 'S2D: Sorted Speculative Decoding For More Efficient Deployment of Nested Large
  Language Models'
arxiv_id: '2407.01955'
source_url: https://arxiv.org/abs/2407.01955
tags:
- draft
- target
- soft
- decoding
- speculative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces sorted speculative decoding (S2D), a novel
  method for accelerating large language model (LLM) inference across multiple target
  models. S2D employs a single draft model trained via sorted fine-tuning, which contains
  multiple sub-models of varying sizes.
---

# S2D: Sorted Speculative Decoding For More Efficient Deployment of Nested Large Language Models

## Quick Facts
- arXiv ID: 2407.01955
- Source URL: https://arxiv.org/abs/2407.01955
- Reference count: 20
- Primary result: 1.55× average speedup on Spec-Bench using adaptive draft selection with a single multi-sub-model draft architecture

## Executive Summary
Sorted Speculative Decoding (S2D) introduces a novel method for accelerating large language model inference across multiple target models. The approach uses a single draft model trained via sorted fine-tuning, containing multiple sub-models of varying sizes. During inference, S2D adaptively selects the appropriate sub-model based on confidence thresholds, optimizing both draft latency and token acceptance rates. Evaluated on Spec-Bench with Vicuna 7B, 13B, and Llama Chat 70B as targets, S2D achieved an average speedup of 1.55× compared to standard autoregressive decoding, outperforming baseline speculative decoding methods in most settings while maintaining accuracy.

## Method Summary
S2D employs a single draft model containing three sub-models (layers 6, 9, and 12) of Vicuna 7B, trained jointly using sorted fine-tuning (SoFT) on the ShareGPT dataset. During inference, the method uses confidence-based early exiting: for each token position, it iterates through sub-models in ascending order, sampling tokens and associated confidence scores. If the confidence exceeds the sub-model's threshold, that token is accepted and draft generation stops for that position; otherwise, it proceeds to the next larger sub-model. The drafted tokens are then verified in parallel by the target model. This adaptive selection allows S2D to serve multiple target models simultaneously without maintaining separate draft models for each, reducing deployment complexity while maintaining or improving performance.

## Key Results
- S2D achieved an average speedup of 1.55× compared to standard autoregressive decoding on Spec-Bench
- Outperformed baseline speculative decoding methods in most settings while maintaining accuracy
- Successfully served multiple target models (Vicuna 7B, 13B, and Llama Chat 70B) using a single draft model
- Maintained or improved token acceptance rates compared to standard speculative decoding approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: S2D uses confidence-based early exiting within a single draft model containing multiple sub-models to adaptively choose draft capacity per token
- Mechanism: During draft generation, the method iterates over sub-models (layers 6, 9, 12) in ascending order. Each sub-model samples a token and associated confidence. If the confidence exceeds a sub-model-specific threshold, that token is accepted and the process stops for that position; otherwise, it proceeds to the next larger sub-model
- Core assumption: Draft token accuracy correlates positively with sub-model size, and confidence scores reliably indicate whether a smaller sub-model's prediction is sufficient
- Evidence anchors: [abstract] "S2D trains multiple draft models in one model to be able to serve more than just one target model at the same time"; [section] "We accept the token t as the final draft token if c ≥ τi"
- Break condition: If confidence thresholds are poorly calibrated, the method may frequently over-select small sub-models (slower acceptance) or under-select them (more draft passes), degrading speedup

### Mechanism 2
- Claim: Sorted fine-tuning (SoFT) enables efficient joint training of multiple draft sub-models without increasing model count
- Mechanism: Starting from a base draft model (first Nd layers of Vicuna 7B), SoFT adds sub-models at layers Nds and Ndm, all sharing the same LM head. The total loss is the average of losses from each sub-model, allowing simultaneous fine-tuning on the same dataset
- Core assumption: Shared head and layer-wise parameter updates do not interfere destructively; fine-tuning can improve all sub-models in parallel
- Evidence anchors: [abstract] "Sorted refers to the sorted-training approach in which a model and its selected sub-models can be trained on single or multiple tasks at the same time"; [section] "We utilize the sorted fine-tuning approach to fine-tune the whole draft on the downstream dataset"
- Break condition: If sub-models compete for representational capacity or gradient interference occurs, performance may degrade for some sub-models

### Mechanism 3
- Claim: Multi-target draft capability eliminates the need to train separate draft models for each target LLM, reducing deployment complexity and cost
- Mechanism: By training a single SoFT draft model on a shared dataset (e.g., ShareGPT), the draft can serve multiple target models (Vicuna 7B, 13B, Llama Chat 70B) with different thresholds, adapting draft capacity to each target's inference characteristics
- Core assumption: The shared dataset sufficiently represents the distributions of all target models' inference outputs
- Evidence anchors: [abstract] "we present a novel more efficient sorted speculative decoding mechanism that outperforms regular baselines in multi-target setting"; [section] "our S2D method outperforms or preserves the performance of the normal speculative decoding almost in all the target sizes"
- Break condition: If target models diverge significantly in task or style, a single draft may become suboptimal for some targets

## Foundational Learning

- Concept: Speculative decoding workflow (draft-then-verify)
  - Why needed here: S2D builds directly on speculative decoding; understanding the two-phase process is essential to grasp how adaptive draft selection improves efficiency
  - Quick check question: In speculative decoding, what happens if a drafted token fails verification by the target model?

- Concept: Confidence-based early exiting in neural networks
  - Why needed here: S2D's adaptive draft selection relies on confidence scores to decide when to stop generating with smaller sub-models
  - Quick check question: How might you compute a confidence score from a language model's output distribution?

- Concept: Sorted fine-tuning (SoFT) and multi-headed training
  - Why needed here: S2D's ability to train multiple sub-models within one architecture depends on SoFT principles
  - Quick check question: In SoFT, how is the total loss computed across sub-models?

## Architecture Onboarding

- Component map: Input sequence -> Sub-model 1 (layer 6) -> Sub-model 2 (layer 9) -> Sub-model 3 (layer 12) -> Shared LM head -> Verification function -> Output
- Critical path:
  1. Input sequence S is passed to the smallest sub-model (layer 6)
  2. Token and confidence are sampled
  3. If confidence ≥ τds, token is accepted; else, pass to next sub-model
  4. Repeat until acceptance or final sub-model (layer 12)
  5. Accepted tokens are verified in parallel by target model
  6. If verification passes, tokens are appended to output
- Design tradeoffs:
  - Larger sub-models increase token acceptance but also draft latency
  - Smaller thresholds allow more use of fast sub-models but risk rejection
  - More sub-models increase adaptability but also training and memory cost
- Failure signatures:
  - Consistently low acceptance ratio → thresholds too low or sub-models underfit
  - High draft latency with minimal speedup → thresholds too high or sub-models too large
  - Target-dependent performance variance → draft training data not representative
- First 3 experiments:
  1. Run S2D with default thresholds on Vicuna 7B; measure speedup and acceptance ratio
  2. Sweep confidence thresholds on Vicuna 13B; identify optimal thresholds for each sub-model
  3. Compare S2D against speculative decoding with single draft model on Llama Chat 70B; record latency and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of S2D scale when using more than three sub-models within the draft architecture?
- Basis in paper: [inferred] The paper discusses using three sub-models (layers 6, 9, and 12) and mentions exploring a different number of sub-models as potential future work
- Why unresolved: The paper focuses on a specific setting with three sub-models and does not explore the impact of using more or fewer sub-models
- What evidence would resolve it: Conducting experiments with varying numbers of sub-models (e.g., 2, 4, or 5) and comparing their performance to the three-sub-model setup would provide insights into the scalability and optimal number of sub-models

### Open Question 2
- Question: What is the impact of using different layer combinations for the sub-models on the overall performance of S2D?
- Basis in paper: [inferred] The paper uses specific layers (6, 9, and 12) for the sub-models but does not explore other combinations
- Why unresolved: The choice of layers for the sub-models could significantly affect the performance, and the paper does not investigate alternative combinations
- What evidence would resolve it: Experimenting with different layer combinations (e.g., layers 4, 8, and 10) and evaluating their impact on speedup and accuracy would help determine the optimal layer configuration

### Open Question 3
- Question: How does the performance of S2D compare to other speculative decoding methods when applied to larger target models (e.g., models larger than 70B parameters)?
- Basis in paper: [inferred] The paper evaluates S2D on target models up to 70B parameters and mentions the need for further investigation on larger models
- Why unresolved: The paper does not provide data on the performance of S2D with target models larger than 70B parameters
- What evidence would resolve it: Conducting experiments with target models larger than 70B parameters and comparing the performance of S2D to other speculative decoding methods would provide insights into its scalability and effectiveness for larger models

## Limitations

- Confidence threshold calibration remains heuristic with no systematic tuning methodology reported
- Multi-target generalization untested beyond three specific models, raising questions about scalability to larger collections of diverse targets
- SoFT training details underspecified, particularly critical hyperparameters like learning rate schedules and batch sizes

## Confidence

**High confidence** in the core mechanism: The adaptive draft selection based on confidence thresholds and the use of SoFT for joint training of multiple sub-models are clearly specified and technically sound.

**Medium confidence** in empirical results: The reported 1.55× average speedup is promising, but evaluation is limited to a single benchmark and three target models without statistical significance testing.

**Low confidence** in deployment claims: While the paper claims S2D reduces deployment complexity, it lacks quantitative evidence (memory usage, cost comparison) to substantiate practical benefits in real-world scenarios.

## Next Checks

1. **Threshold sensitivity analysis**: Systematically sweep confidence thresholds (τds, τdm, τd) across a wider range and measure the impact on speedup, acceptance ratio, and token accuracy for each target model. Report optimal thresholds and their robustness to task variations.

2. **Cross-task generalization study**: Evaluate S2D on additional benchmarks beyond Spec-Bench (e.g., HELM, BIG-bench) and with target models from different families (e.g., Mistral, CodeLlama). Measure whether the single draft model maintains performance advantages across diverse tasks and model architectures.

3. **Ablation of sub-model count and size**: Compare S2D variants with different numbers of sub-models (e.g., 2 vs. 3) and varying layer counts (e.g., 3, 6, 9, 12). Quantify the tradeoff between draft model capacity, training cost, and inference speedup to identify the most efficient configuration.