---
ver: rpa2
title: On Learning Action Costs from Input Plans
arxiv_id: '2408.10889'
source_url: https://arxiv.org/abs/2408.10889
tags:
- plans
- cost
- optimal
- function
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a new problem: learning action costs from
  input plans such that the resulting cost function makes the input plans optimal
  in the planning model. The authors formalize this as the CFL task and show that,
  in general, no cost function can guarantee all input plans are optimal.'
---

# On Learning Action Costs from Input Plans

## Quick Facts
- arXiv ID: 2408.10889
- Source URL: https://arxiv.org/abs/2408.10889
- Reference count: 33
- One-line primary result: LACFIP k outperforms baseline in aligning learned costs with observed plan preferences, with performance improving as k increases.

## Executive Summary
This paper introduces the Cost Function Learning from Input Plans (CFL) task, which aims to learn action costs from observed optimal plans such that the learned costs make those plans optimal in the planning model. The authors show that, in general, no cost function can guarantee all input plans are optimal, and they relax the problem to maximize the number of optimal input plans. They propose LACFIP k, a mixed-integer linear programming approach that iteratively maximizes the number of optimal plans and minimizes total cost. Experiments in GRID, BARMAN, OPENSTACKS, and TRANSPORT domains demonstrate that LACFIP k outperforms a baseline method in aligning learned costs with observed plan preferences.

## Method Summary
The paper presents LACFIP k, a mixed-integer linear programming approach to learn action costs from input plans. The algorithm generates MILPs to assign costs, iteratively maximizing the number of optimal plans (using parameter k to determine alternative plans considered) and then minimizing total cost. For CFL_C tasks that refine existing approximate cost functions, the approach minimizes the difference between learned and approximate costs while maintaining plan optimality. The method uses SYMK to compute alternative plans, FAST DOWNWARD for grounding actions, and CBC as the MILP solver. The algorithm handles three problem variants: CFL (learning from scratch), CFL_C (refining existing costs), and strict variants SCF/SCF_C.

## Key Results
- LACFIP k outperforms baseline in making input plans optimal across all tested domains
- Performance improves with increasing k values, but scalability degrades with larger CFL sizes
- CFL_C variant successfully refines approximate cost functions to better align with observed plans
- Strict Cost Functions (SCF) are more restrictive but harder to achieve than Maximal Cost Functions (MCF)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning action costs from observed plans can align the planning model with implicit user preferences.
- Mechanism: By assigning lower costs to actions that occur more frequently in observed optimal plans, the learned cost function will favor those actions when generating new plans, making them optimal under the learned model.
- Core assumption: The input plans represent optimal behavior according to some cost function that the learner wants to approximate.
- Evidence anchors:
  - [abstract] "By observing these plans we cannot only get more accurate driving times, but also understand which routes users prefer and adjust the model accordingly."
  - [section] "The underlying motivation is that by aligning the action's costs to the input plans, the new model will be able to generate new plans that better reflect the observed behavior, i.e., the user preferences."
- Break condition: If the input plans are suboptimal or inconsistent with each other, the learned cost function may not accurately reflect user preferences.

### Mechanism 2
- Claim: Maximizing the number of optimal input plans (MCF solution) can be achieved by formulating the problem as a Mixed-Integer Linear Program (MILP).
- Mechanism: The MILP uses binary decision variables to track which input plans are optimal and assigns costs to actions such that each input plan has a lower or equal cost compared to its alternative plans.
- Core assumption: The set of alternative plans considered (determined by parameter k) includes the most relevant ones that could affect the optimality of the input plans.
- Evidence anchors:
  - [section] "We aim to optimize the objective function described in Equation (12), where we have two objectives... The first objective, weighted by ω1, aims to maximize the number of optimal plans."
  - [section] "LACFIP k will first try to maximize the number of plans that can be turned optimal by setting the weights to ω1 = 1 and ω2 = 0."
- Break condition: If k is too small, the MILP may not consider all relevant alternative plans, leading to suboptimal solutions.

### Mechanism 3
- Claim: Refining an existing approximate cost function (CFL ¯C task) can be achieved by minimizing the difference between the learned and approximate costs.
- Mechanism: The MILP includes an additional constraint that enforces the learned cost function to be as close as possible to the approximate cost function, while still making the input plans optimal.
- Core assumption: The approximate cost function provides a reasonable starting point, and the input plans can be used to make small adjustments that improve alignment with user preferences.
- Evidence anchors:
  - [section] "In practice, we may have an approximate cost function ¯C that we would like to refine with observed plans... We then introduce a new task with initial costs as follows: Definition 7."
  - [section] "The quality of MCF ¯C and SCF ¯C solutions... differs in the secondary objective of minimizing the sum of actions' costs to minimizing the difference between the solution cost function C and the approximate cost function received as input ¯C."
- Break condition: If the approximate cost function is very far from the optimal one, significant adjustments may be needed, potentially leading to a learned cost function that is quite different from the original.

## Foundational Learning

- Concept: Classical Planning (STRIPS formalism)
  - Why needed here: The paper is about learning action costs in the context of classical planning, so understanding the basic planning framework is essential.
  - Quick check question: What are the components of a STRIPS planning task?

- Concept: Optimization and Constraint Satisfaction
  - Why needed here: The paper uses MILP to solve the cost function learning problem, which requires understanding how to formulate and solve optimization problems with constraints.
  - Quick check question: What is the difference between a linear program and a mixed-integer linear program?

- Concept: Inverse Reinforcement Learning (IRL)
  - Why needed here: The paper draws a connection between learning action costs and IRL, so understanding the basics of IRL is helpful for contextualizing the work.
  - Quick check question: How does IRL differ from reinforcement learning?

## Architecture Onboarding

- Component map:
  CFL task -> SYMK -> Alternative plans -> LACFIP k -> MILP -> CBC solver -> Learned cost function

- Critical path:
  1. Receive CFL task as input
  2. Compute relevant actions (AM)
  3. Generate MILP
  4. Solve MILP to find Q (number of optimal plans)
  5. Add constraint to enforce Q optimal plans
  6. Solve MILP again to minimize total cost
  7. Update cost function with minimum costs for non-relevant actions
  8. Return learned cost function

- Design tradeoffs:
  - Larger k values improve solution quality but increase computation time
  - CFL task size affects scalability (more plans and larger planning tasks increase complexity)
  - MCF vs. SCF: SCF is more restrictive but may be harder to achieve

- Failure signatures:
  - If k is too small, the learned cost function may not make enough input plans optimal
  - If the CFL task contains conflicting plans, no cost function may be able to make all of them optimal
  - If the approximate cost function in CFL ¯C is very inaccurate, significant adjustments may be needed

- First 3 experiments:
  1. Run LACFIP k on a small CFL task (e.g., 2x2 GRID) with k=∞ to verify it can find the optimal cost function
  2. Run LACFIP k on a larger CFL task (e.g., 5x5 GRID) with varying k values to observe the tradeoff between k and solution quality
  3. Run LACFIP k on a CFL ¯C task with a known approximate cost function to verify it can refine the costs to make input plans optimal

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the optimality guarantees of LACFIP k be extended to handle bounded suboptimal plans as inputs, and if so, how would this affect the MILP formulation?
- Basis in paper: The paper explicitly mentions in the conclusions that extending definitions and algorithms to handle bounded suboptimal plans is a future work direction.
- Why unresolved: The current MILP formulation assumes all input plans are optimal and does not account for bounded suboptimality, requiring new constraints and objective functions.
- What evidence would resolve it: Theoretical proofs showing modified MILP formulations that guarantee optimality for bounded suboptimal inputs, along with empirical validation showing improved performance on such cases.

### Open Question 2
- Question: What is the theoretical relationship between the value of k and the suboptimality gap of the learned cost function, and can this relationship be formally characterized?
- Basis in paper: The paper empirically shows that increasing k improves results but does not provide a formal characterization of how k relates to the optimality gap.
- Why unresolved: The paper demonstrates that higher k values lead to better solutions in practice but lacks a theoretical framework to quantify the relationship between k and solution quality.
- What evidence would resolve it: Mathematical proofs establishing bounds on the suboptimality gap as a function of k, along with empirical studies verifying these bounds across different domains.

### Open Question 3
- Question: How does the presence of redundant actions in input plans affect the theoretical guarantees of LACFIP k, and can the algorithm be modified to handle such cases?
- Basis in paper: Example 2 explicitly shows that input plans with redundant actions cannot be turned optimal under any cost function, indicating a limitation of the current approach.
- Why unresolved: The paper identifies this limitation but does not provide a solution or theoretical framework for handling redundant actions in input plans.
- What evidence would resolve it: Formal proofs showing how redundant actions impact solution guarantees, along with modified algorithms or preprocessing steps that can detect and handle redundant actions.

### Open Question 4
- Question: Can the scalability of LACFIP k be improved through alternative search strategies or pruning techniques without sacrificing optimality guarantees?
- Basis in paper: The evaluation section shows that LACFIP k scales poorly with larger CFL tasks and higher k values, suggesting a need for more efficient algorithms.
- Why unresolved: The paper prioritizes theoretical guarantees over empirical performance, leaving open the question of whether scalability can be improved while maintaining these guarantees.
- What evidence would resolve it: Comparative studies of LACFIP k against alternative algorithms using different search strategies or pruning techniques, demonstrating improved scalability while maintaining or proving similar optimality guarantees.

## Limitations
- Computational complexity scales poorly with CFL size and k values, limiting practical applicability
- Assumes input plans are optimal, but real-world plans may contain errors or suboptimal choices
- MILP formulations for SCF and CFL_C variants are not fully detailed, requiring additional derivation work

## Confidence
**High Confidence**: The core mechanism of learning action costs through MILP formulation and the theoretical framework for CFL tasks. The empirical results showing improvement over baseline in controlled domains are convincing.

**Medium Confidence**: The scalability claims and performance degradation patterns with increasing k and CFL size, as these depend heavily on implementation details and specific domain characteristics.

**Low Confidence**: The practical applicability to real-world planning scenarios with noisy or inconsistent input plans, as this was not thoroughly evaluated in the experiments.

## Next Checks
1. **Scalability Validation**: Test LACFIP k on progressively larger CFL tasks (beyond 5x5 GRID) to empirically verify the claimed computational complexity and identify practical limits on k values.

2. **Robustness Testing**: Evaluate the algorithm on CFL tasks containing known suboptimal or conflicting plans to assess how well it handles noisy real-world data and whether it produces meaningful cost functions.

3. **MILP Formulation Verification**: Complete the MILP formulations for SCF and CFL_C variants and verify they correctly implement the stated optimization objectives through small-scale test cases.