---
ver: rpa2
title: Word Alignment as Preference for Machine Translation
arxiv_id: '2405.09223'
source_url: https://arxiv.org/abs/2405.09223
tags:
- omission
- translation
- hallucination
- preference
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses hallucination and omission problems in LLM-based
  machine translation by proposing a novel approach that utilizes word alignment as
  preference signals. The method constructs preference data by collecting translations
  from multiple MT tools, selecting preferred and rejected translations based on word
  alignment coverage scores, and optimizing the LLM-based MT model using direct preference
  optimization.
---

# Word Alignment as Preference for Machine Translation

## Quick Facts
- arXiv ID: 2405.09223
- Source URL: https://arxiv.org/abs/2405.09223
- Authors: Qiyu Wu; Masaaki Nagata; Zhongtao Miao; Yoshimasa Tsuruoka
- Reference count: 31
- Primary result: Word Alignment Preference (WAP) method reduces hallucination and omission in LLM-based MT while maintaining competitive translation quality

## Executive Summary
This paper addresses hallucination and omission problems in LLM-based machine translation by proposing a novel approach that utilizes word alignment as preference signals. The method constructs preference data by collecting translations from multiple MT tools, selecting preferred and rejected translations based on word alignment coverage scores, and optimizing the LLM-based MT model using direct preference optimization. The primary results show that the proposed Word Alignment Preference (WAP) method consistently outperforms the baseline model in hard instances (prone to hallucination and omission) across most translation directions, with average improvements of 4.96, 1.63, and 1.24 in coverage scores when selecting 100, 200, and 500 hard instances respectively. Human evaluation confirms that WAP reduces both hallucination and omission in translations while maintaining competitive overall translation quality.

## Method Summary
The proposed approach constructs preference data by first collecting translations from multiple machine translation tools for the same source sentences. These translations are then scored using word alignment coverage metrics that measure how well each translation aligns with the source text. Translations with higher coverage scores are designated as "preferred" while those with lower scores are marked as "rejected." This preference data is then used to fine-tune LLM-based translation models through direct preference optimization, where the model learns to generate translations that better preserve source content and reduce hallucination and omission. The method specifically targets hard translation instances that are prone to these errors, using word alignment as an explicit signal to guide the model toward more faithful translations.

## Key Results
- WAP consistently outperforms baseline models on hard translation instances across Zh→En, En→Fr, and En→Ja directions
- Average coverage score improvements of 4.96, 1.63, and 1.24 when selecting 100, 200, and 500 hard instances respectively
- Human evaluation confirms reduction in both hallucination and omission while maintaining translation quality
- Method shows effectiveness in reducing translation errors without significant degradation in overall translation quality

## Why This Works (Mechanism)
The mechanism works by leveraging word alignment as an explicit preference signal to guide LLM-based translation models toward more faithful outputs. By constructing preference datasets where translations are ranked based on their word alignment coverage with source texts, the model learns to prioritize translations that preserve source content more accurately. The direct preference optimization framework then reinforces this behavior by training the model to prefer outputs that align better with source content, effectively reducing the tendency to hallucinate or omit information. This approach provides a more structured way to incorporate translation fidelity into the optimization process compared to traditional fine-tuning methods.

## Foundational Learning
- **Word Alignment**: The process of mapping words or phrases between source and target languages; needed to measure translation fidelity and create preference signals; quick check: verify alignment accuracy using standard metrics like AER
- **Direct Preference Optimization (DPO)**: A fine-tuning method that uses preference data to optimize model behavior; needed to incorporate word alignment preferences into LLM training; quick check: validate convergence and stability during optimization
- **Translation Coverage Metrics**: Measures of how well translations preserve source content; needed to quantify alignment quality and select preferred translations; quick check: compare coverage scores across different translation tools
- **Hallucination Detection**: Identifying when translations contain information not present in source text; needed to evaluate model performance on hard instances; quick check: validate detection accuracy using both automatic and human evaluation
- **Omission Detection**: Identifying when translations fail to include information from source text; needed to comprehensively evaluate translation quality; quick check: measure omission rates across different translation directions
- **Preference Data Construction**: The process of creating training data with preferred and rejected examples; needed to provide supervision for DPO; quick check: analyze diversity and quality of constructed preference pairs

## Architecture Onboarding

**Component Map**
Source Text -> Multiple MT Tools -> Word Alignment Scoring -> Preference Data Construction -> DPO Fine-tuning -> Optimized LLM-based MT

**Critical Path**
The critical path involves generating multiple translations for each source sentence, computing word alignment coverage scores for each translation, selecting preferred and rejected pairs based on these scores, and using this preference data to fine-tune the LLM through DPO. The word alignment scoring step is crucial as it directly determines which translations are considered preferred or rejected.

**Design Tradeoffs**
The approach trades off the computational cost of generating multiple translations and computing word alignments against the benefit of more accurate preference signals. Using multiple MT tools increases the diversity of preference data but also increases computational overhead. The selection of hard instances for preference optimization focuses on problematic cases but may not address easier translation scenarios. The reliance on word alignment as a proxy for translation quality assumes that better alignment correlates with better translations, which may not always hold true.

**Failure Signatures**
Potential failure modes include: (1) word alignment errors leading to incorrect preference signals, (2) overfitting to specific types of hard instances at the expense of general translation quality, (3) preference data bias if certain MT tools consistently produce translations that score well on alignment metrics but poorly on other quality measures, and (4) the method may not generalize well to language pairs with very different syntactic structures where word alignment is less reliable.

**3 First Experiments**
1. Compare word alignment coverage scores across different MT tools to establish baseline performance and identify which tools produce translations that align best with source content
2. Test the impact of different numbers of hard instances (100, 200, 500) on optimization effectiveness to find the optimal balance between training data quantity and quality
3. Evaluate the contribution of word alignment preference signals by comparing WAP performance against baseline models fine-tuned without preference data

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Improvements primarily demonstrated on hard translation instances, with unclear generalization to easier cases or real-world deployment
- Evaluation relies heavily on automatic metrics that may not fully capture translation quality nuances
- Preference dataset construction criteria and potential biases are not thoroughly examined
- Experiments limited to three specific translation directions, limiting generalizability across different language pairs

## Confidence
- High confidence in technical implementation of word alignment preference framework and observation that WAP improves performance on hard instances
- Medium confidence in comparative advantages over baseline models due to improvements measured against specific configurations
- Low confidence in long-term effectiveness and robustness across diverse translation scenarios given limited experimental scope

## Next Checks
1. Conduct human evaluation studies across multiple translation domains to validate whether reduction in hallucination and omission rates translates to meaningful quality improvements from user perspectives
2. Test the WAP approach on additional language pairs beyond the three directions studied, particularly low-resource languages, to assess scalability
3. Perform ablation studies to isolate the contribution of word alignment preference signals from other factors in the optimization process, and test whether the method maintains its effectiveness when applied to different LLM architectures or when integrated with other MT systems