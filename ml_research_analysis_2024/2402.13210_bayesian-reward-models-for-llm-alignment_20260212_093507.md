---
ver: rpa2
title: Bayesian Reward Models for LLM Alignment
arxiv_id: '2402.13210'
source_url: https://arxiv.org/abs/2402.13210
tags:
- reward
- ensure
- proxy
- quality
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reward overoptimization in
  LLM alignment, where reward models trained on limited human preference data can
  produce high scores for poor responses, particularly for out-of-distribution prompts.
  The core method is to train Bayesian reward models using Laplace approximation on
  LoRA weights, which provides uncertainty estimates that increase for out-of-distribution
  inputs.
---

# Bayesian Reward Models for LLM Alignment

## Quick Facts
- arXiv ID: 2402.13210
- Source URL: https://arxiv.org/abs/2402.13210
- Authors: Adam X. Yang; Maxime Robeyns; Thomas Coste; Zhengyan Shi; Jun Wang; Haitham Bou-Ammar; Laurence Aitchison
- Reference count: 40
- Primary result: Bayesian reward models using Laplace-LoRA uncertainty estimates effectively mitigate reward overoptimization in best-of-n sampling, outperforming standard MAP reward models and ensembles

## Executive Summary
This paper addresses reward overoptimization in LLM alignment, where reward models trained on limited human preference data can produce high scores for poor responses, particularly for out-of-distribution prompts. The authors propose Bayesian reward models trained using Laplace approximation on LoRA weights, which provide uncertainty estimates that increase for OOD inputs. They demonstrate that these uncertainty-penalized rewards effectively mitigate overoptimization in best-of-n sampling, outperforming both standard MAP reward models and reward model ensembles when evaluated against a gold-standard reward model.

## Method Summary
The method trains Bayesian reward models by applying Laplace approximation to LoRA weights, providing epistemic uncertainty estimates. Two uncertainty-penalized reward formulations are proposed: standard deviation-based and variance-based penalties. The approach is evaluated using best-of-n sampling with a gold-standard reward model (LLaMA 7B) and compared against MAP reward models and ensembles. The evaluation uses proxy reward models (70M Pythia) trained on synthetic preference data from a base LLM (1.4B Pythia) fine-tuned on the AlpacaFarm dataset.

## Key Results
- Bayesian reward models with uncertainty penalties outperform MAP and ensemble baselines in best-of-n sampling
- Variance-based uncertainty penalty shows slight benefits at lower KL divergence compared to standard deviation-based penalty
- Laplace-LoRA can be combined with reward ensembles for additional performance improvements
- The method achieves highest gold reward in RLHF without KL penalty, though details are limited

## Why This Works (Mechanism)

### Mechanism 1
Laplace-LoRA provides epistemic uncertainty estimates that increase for out-of-distribution (OOD) inputs, helping mitigate reward overoptimization. By applying Laplace approximation to LoRA weights, the method produces a Gaussian posterior over reward model outputs with higher variance for inputs far from training distribution. This uncertainty can penalize high-reward predictions for uncertain OOD responses.

### Mechanism 2
Penalizing rewards based on uncertainty estimates prevents selection of high-reward but potentially poor responses in BoN sampling. Two penalty formulations (standard deviation and variance) reduce effective reward for responses with high uncertainty estimates, making them less likely to be selected as "best" in BoN sampling.

### Mechanism 3
Combining Laplace-LoRA uncertainty with reward model ensembles provides additional robustness against overoptimization. The paper shows that applying Laplace-LoRA to each ensemble member and combining their uncertainty-penalized predictions leads to better performance than either approach alone.

## Foundational Learning

- Laplace Approximation
  - Why needed here: Provides tractable way to approximate posterior distribution over LoRA weights, enabling uncertainty quantification without expensive sampling
  - Quick check question: What are the key requirements for Laplace approximation to work well on neural network weights?

- Bradley-Terry Preference Model
  - Why needed here: Provides likelihood function for reward model training, modeling probability that one response is preferred over another
  - Quick check question: How does the Bradley-Terry model relate reward model outputs to preference probabilities?

- LoRA (Low-Rank Adaptation)
  - Why needed here: Provides parameter-efficient fine-tuning method that makes Bayesian inference computationally feasible by reducing parameters to approximate
  - Quick check question: What is the mathematical relationship between original weights W0 and adapted weights in LoRA?

## Architecture Onboarding

- Component map:
  Base LLM (1.4B Pythia) → SFT fine-tuning → LLM policy
  Reward model (70M Pythia) → LoRA fine-tuning → MAP training → Laplace-LoRA uncertainty
  Gold standard reward model (LLaMA 7B) → Human preference training → Synthetic labeling
  BoN sampling module → Response generation → Uncertainty-penalized ranking

- Critical path:
  1. Train proxy reward model via LoRA on synthetic preference data
  2. Apply Laplace-LoRA post-hoc to obtain uncertainty estimates
  3. Generate responses via LLM
  4. Score responses using uncertainty-penalized rewards
  5. Select best response based on penalized scores

- Design tradeoffs:
  - Uncertainty penalty strength (k hyperparameter) vs. reward accuracy
  - Number of ensemble members vs. computational cost
  - LoRA rank vs. approximation quality of posterior

- Failure signatures:
  - High variance in uncertainty estimates across similar inputs
  - Degradation in proxy reward performance when applying penalties
  - Over-penalization leading to selection of mediocre responses

- First 3 experiments:
  1. Train MAP reward model and evaluate BoN performance vs gold reward
  2. Apply Laplace-LoRA with various k values and compare performance curves
  3. Compare single model with ensemble and LA-ensemble configurations

## Open Questions the Paper Calls Out

### Open Question 1
How do different uncertainty penalties (standard deviation vs variance) perform across different ranges of KL divergence? The paper compares both penalties but only provides results for k values up to 10 without systematic analysis across different KL divergence ranges.

### Open Question 2
How does Laplace-LoRA perform in RLHF compared to other uncertainty-aware methods? The paper mentions Laplace-LoRA achieves highest gold reward in RLHF without KL penalty but provides limited details and focuses primarily on BoN sampling results.

### Open Question 3
What is the impact of ensemble size on Laplace-LoRA performance? The paper combines Laplace-LoRA with reward ensembles but doesn't explore how ensemble size affects performance or determine optimal ensemble size.

## Limitations
- Reliance on Laplace approximation validity for accurate uncertainty estimates
- Limited validation that high uncertainty correlates with poor response quality
- Evaluation uses KL divergence as proxy for optimization strength without demonstrating relationship to actual alignment failures

## Confidence
- High confidence: Basic mechanism of using uncertainty to penalize reward predictions is theoretically sound
- Medium confidence: Empirical results showing improvement over baselines, though improvements are modest
- Low confidence: Claims about why uncertainty helps with reward overoptimization specifically, due to limited ablation studies

## Next Checks
1. Systematically evaluate whether high uncertainty estimates from Laplace-LoRA actually correlate with poor response quality across different input distributions
2. Conduct more thorough ablation studies on k hyperparameter and different uncertainty penalty formulations
3. Test Bayesian reward model approach on different preference datasets to verify generalization beyond AlpacaFarm dataset