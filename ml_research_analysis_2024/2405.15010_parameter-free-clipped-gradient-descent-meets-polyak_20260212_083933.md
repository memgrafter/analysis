---
ver: rpa2
title: Parameter-free Clipped Gradient Descent Meets Polyak
arxiv_id: '2405.15010'
source_url: https://arxiv.org/abs/2405.15010
tags:
- stepsize
- gradient
- polyak
- descent
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of tuning hyperparameters for
  clipped gradient descent, specifically the stepsize and gradient clipping threshold.
  The authors propose Inexact Polyak Stepsize, a parameter-free method that converges
  to the optimal solution without any problem-specific parameters.
---

# Parameter-free Clipped Gradient Descent Meets Polyak

## Quick Facts
- arXiv ID: 2405.15010
- Source URL: https://arxiv.org/abs/2405.15010
- Authors: Yuki Takezawa; Han Bao; Ryoma Sato; Kenta Niwa; Makoto Yamada
- Reference count: 40
- Key outcome: Proposes Inexact Polyak Stepsize, a parameter-free method that converges to optimal solutions without tuning hyperparameters, achieving rates independent of Lipschitz constant L under (L0, L1)-smoothness

## Executive Summary
This paper addresses the challenge of tuning hyperparameters for clipped gradient descent, specifically the stepsize and gradient clipping threshold. The authors propose Inexact Polyak Stepsize, a parameter-free method that converges to the optimal solution without any problem-specific parameters. The method achieves a convergence rate that is asymptotically independent of the global Lipschitz constant L under (L0, L1)-smoothness assumptions, similar to clipped gradient descent with well-tuned hyperparameters. The authors validate their theoretical results using a synthetic function and demonstrate the effectiveness of their method on LSTM, Nano-GPT, and T5 models, showing that Inexact Polyak Stepsize consistently outperforms other parameter-free methods like DecSPS and AdaSPS.

## Method Summary
The proposed Inexact Polyak Stepsize method modifies the traditional Polyak stepsize by removing its dependency on the minimum loss value f*. Instead of using f*, the method employs a lower bound l* (typically 0 for non-negative losses) and scales the stepsize by 1/√T to compensate for potential excessive stepsizes. The algorithm tracks the parameter with the lowest loss achieved during training rather than returning the final parameter. This modification maintains the asymptotic independence from L while making the method parameter-free, and its convergence rate depends only on the initial distance to the optimal solution rather than the maximum distance achieved during training.

## Key Results
- Achieves convergence rate asymptotically independent of L under (L0, L1)-smoothness, matching clipped gradient descent with well-tuned hyperparameters
- Removes the need for hyperparameter tuning while maintaining competitive performance on LSTM, Nano-GPT, and T5 models
- Convergence rate depends only on ∥x0 - x*∥ rather than DT = maxt∥xt - x*∥, guaranteeing convergence to optimal solution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Inexact Polyak Stepsize inherits the favorable property of Polyak stepsize under (L0, L1)-smoothness - convergence rate asymptotically independent of L.
- Mechanism: Under (L0, L1)-smoothness, the lower bound of Polyak stepsize matches the stepsize of clipped gradient descent with well-tuned hyperparameters. By using this relationship, Inexact Polyak Stepsize achieves the same convergence rate as clipped gradient descent without needing to tune the gradient clipping threshold.
- Core assumption: The loss function is convex, L-smooth, and (L0, L1)-smooth.
- Evidence anchors:
  - [abstract] "its convergence rate is asymptotically independent of $L$ under $L$-smooth and $(L_0, L_1)$-smooth assumptions of the loss function, similar to that of clipped gradient descent with well-tuned hyperparameters"
  - [section] "By comparing Theorem 4 with Theorem 2, the convergence rate of Polyak stepsize is the same as that of clipped gradient descent."
- Break condition: If the (L0, L1)-smoothness assumption doesn't hold or if L0 is not significantly smaller than L, the convergence rate will depend on L and the advantage disappears.

### Mechanism 2
- Claim: Inexact Polyak Stepsize removes the problem-specific parameter f* from Polyak stepsize without losing asymptotic independence from L.
- Mechanism: Instead of using the exact minimum loss value f*, Inexact Polyak Stepsize uses a lower bound l* and sets the stepsize as (f(x) - l*)/(√T||∇f(x)||²). To compensate for potentially excessive stepsizes, it returns the parameter with the lowest loss achieved during training rather than the final parameter.
- Core assumption: The loss function is non-negative, providing a trivial lower bound l* = 0.
- Evidence anchors:
  - [abstract] "we propose Inexact Polyak Stepsize, which converges to the optimal solution without any hyperparameters tuning"
  - [section] "To make Polyak stepsize parameter-free, several prior studies have proposed the use of lower bound of f* instead of f*"
- Break condition: If the lower bound l* is significantly different from f*, or if √T is too small/large, the stepsize might be improperly scaled, affecting convergence.

### Mechanism 3
- Claim: The convergence rate of Inexact Polyak Stepsize depends on ∥x0 - x*∥ rather than DT = maxt∥xt - x*∥.
- Mechanism: Unlike DecSPS and AdaSPS whose convergence rates depend on DT (which may increase with iterations), Inexact Polyak Stepsize's convergence rate only depends on the initial distance ∥x0 - x*∥. This guarantees convergence to the optimal solution.
- Core assumption: The initial parameter x0 is reasonably close to the optimal solution x*.
- Evidence anchors:
  - [section] "In contrast, the convergence rate in Eq. (15) depends on only ∥x0 − x⋆∥. Theorem 5 indicates that Inexact Polyak Stepsize converges to the optimal solution."
  - [corpus] "Limited direct evidence found in corpus - need to verify this property holds across different problem classes."
- Break condition: If the initial parameter is very far from the optimal solution, the convergence rate might degrade significantly, or the method might get stuck in poor local minima.

## Foundational Learning

- Concept: Convex optimization and gradient descent
  - Why needed here: The paper builds on fundamental properties of convex functions and gradient descent convergence
  - Quick check question: What is the convergence rate of gradient descent with stepsize 1/L under L-smoothness?

- Concept: (L0, L1)-smoothness assumption
  - Why needed here: This is the key assumption that enables clipped gradient descent to achieve faster convergence rates
  - Quick check question: How does (L0, L1)-smoothness differ from standard L-smoothness, and why is it useful for training neural networks?

- Concept: Polyak stepsize and its convergence properties
  - Why needed here: The proposed method builds on Polyak stepsize by removing its dependency on the minimum loss value
  - Quick check question: Under what conditions does Polyak stepsize achieve the optimal convergence rate?

## Architecture Onboarding

- Component map: Inexact Polyak Stepsize -> Stepsize calculation -> Parameter update -> Best parameter tracking -> Final output
- Critical path: Stepsize calculation → Parameter update → Best parameter tracking → Final output
- Design tradeoffs: The √T scaling in stepsize balances between large initial steps and stability near optimum; tracking best parameter compensates for potential instability
- Failure signatures: Loss divergence, oscillation around optimum, convergence dependent on L when it shouldn't be
- First 3 experiments:
  1. Implement Inexact Polyak Stepsize on the synthetic function f(x) = L0L1²/72·x⁴ + L0/4·x² + f* with varying L1 to verify independence from L
  2. Compare convergence rates of Inexact Polyak Stepsize vs DecSPS/AdaSPS on a convex quadratic function with known minimum
  3. Test on a simple neural network (e.g., one hidden layer) to verify practical effectiveness and stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Inexact Polyak Stepsize be extended to the stochastic optimization setting?
- Basis in paper: [explicit] The paper mentions that prior works analyzed DecSPS and AdaSPS in stochastic settings but does not explore this for Inexact Polyak Stepsize.
- Why unresolved: The paper focuses on deterministic optimization and does not provide theoretical or empirical analysis for the stochastic case.
- What evidence would resolve it: Convergence rate analysis and empirical results for Inexact Polyak Stepsize under stochastic gradient settings.

### Open Question 2
- Question: How does Inexact Polyak Stepsize perform in non-convex optimization settings?
- Basis in paper: [inferred] The paper focuses on convex optimization problems, but most deep learning applications involve non-convex loss functions.
- Why unresolved: The theoretical analysis and experiments are limited to convex problems, leaving performance in non-convex settings unexplored.
- What evidence would resolve it: Convergence analysis and empirical results on non-convex problems like deep neural networks with multiple local minima.

### Open Question 3
- Question: Can the convergence rate with respect to T be improved for Inexact Polyak Stepsize?
- Basis in paper: [explicit] The paper acknowledges that Inexact Polyak Stepsize slows down convergence with respect to T compared to clipped gradient descent with optimal hyperparameters.
- Why unresolved: The authors suggest that adaptive methods from Hazan and Kakade (2019) might help but do not explore this direction.
- What evidence would resolve it: Analysis and experiments demonstrating improved convergence rates with respect to T using alternative adaptive methods.

## Limitations

- Theoretical analysis relies heavily on the (L0, L1)-smoothness assumption, which may not hold for all real-world loss landscapes
- √T scaling in stepsize calculation appears heuristic and may require more rigorous justification
- Limited empirical validation across diverse deep learning architectures and datasets

## Confidence

- **High Confidence:** The theoretical framework under (L0, L1)-smoothness assumptions and the comparison with Polyak stepsize convergence rates
- **Medium Confidence:** The practical effectiveness on the tested deep learning models and the removal of hyperparameter tuning burden
- **Low Confidence:** The universality of the √T scaling heuristic and the method's robustness in highly non-convex or noisy optimization scenarios

## Next Checks

1. Test Inexact Polyak Stepsize on a broader range of deep learning architectures and datasets to assess generalizability beyond the current scope
2. Conduct ablation studies on the √T scaling factor to determine its optimal value or whether it should be adaptive
3. Evaluate the method's performance under different noise levels and in the presence of stochastic gradients to verify robustness in realistic training scenarios