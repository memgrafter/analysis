---
ver: rpa2
title: Span-Based Optimal Sample Complexity for Weakly Communicating and General Average
  Reward MDPs
arxiv_id: '2403.11477'
source_url: https://arxiv.org/abs/2403.11477
tags:
- lemma
- bound
- span
- have
- mdps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the sample complexity of learning near-optimal
  policies in average-reward Markov decision processes (MDPs). For weakly communicating
  MDPs, the authors establish a sample complexity bound of $\widetilde{O}(SAH/\varepsilon^2)$,
  where $H$ is the span of the optimal bias function, which is optimal up to logarithmic
  factors.
---

# Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs

## Quick Facts
- arXiv ID: 2403.11477
- Source URL: https://arxiv.org/abs/2403.11477
- Authors: Matthew Zurek; Yudong Chen
- Reference count: 40
- Primary result: Achieves optimal sample complexity O(SAH/ε²) for weakly communicating average-reward MDPs using span-based analysis

## Executive Summary
This paper establishes optimal sample complexity bounds for learning near-optimal policies in average-reward Markov decision processes (MDPs). The authors show that for weakly communicating MDPs, the sample complexity is O(SAH/ε²), where H is the span of the optimal bias function, matching a known lower bound up to logarithmic factors. They introduce a novel reduction from average-reward to discounted MDPs combined with improved variance analysis that achieves this bound. For general (multichain) MDPs, they introduce a new transient time parameter B and establish a sample complexity of O(SA(B+H)/ε²), proving this is also optimal up to log factors.

## Method Summary
The method reduces the average-reward MDP problem to a discounted MDP with carefully chosen discount factor γ = 1 - ε/12(B+H). It constructs an empirical transition kernel using n samples per state-action pair, solves the resulting discounted MDP, and analyzes variance parameters to bound the error. For weakly communicating MDPs, the span H of the optimal bias function bounds the variance, while for general MDPs, both the transient time B and span H contribute to the complexity. The key technical innovation is improving the analysis of variance parameters from cubic to quadratic dependence on the effective horizon.

## Key Results
- Establishes O(SAH/ε²) sample complexity for weakly communicating MDPs, optimal up to logarithmic factors
- Introduces transient time parameter B for general MDPs with bound O(SA(B+H)/ε²)
- Proves matching lower bounds for both weakly communicating and general MDPs
- Achieves quadratic (not cubic) dependence on effective horizon through improved variance analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing average-reward MDPs to discounted MDPs with discount factor γ = 1 - ε/H allows the span-based complexity bound O(SAH/ε²) to be achieved.
- Mechanism: The optimal bias function h* in weakly communicating MDPs has bounded span H. By choosing γ close to 1, the reduction preserves the structure of the optimal policy while allowing the use of discounted MDP algorithms. The span H bounds the variance of the cumulative discounted reward, leading to a quadratic dependence on the effective horizon 1/(1-γ) instead of cubic.
- Core assumption: The MDP is weakly communicating and H is finite.
- Evidence anchors:
  - [abstract] "For weakly communicating MDPs, the authors establish a sample complexity bound of O(SAH/ε²), where H is the span of the optimal bias function, which is optimal up to logarithmic factors."
  - [section] "Our approach is based on reducing the average-reward problem to a discounted problem."
- Break condition: If the MDP is not weakly communicating or if H is very large compared to 1/(1-γ), the reduction fails.

### Mechanism 2
- Claim: Introducing a transient time parameter B captures the sample complexity of general (multichain) MDPs as O(SA(B+H)/ε²).
- Mechanism: In general MDPs, the optimal policy may be multichain, leading to multiple closed recurrent classes with different long-run rewards. The parameter B measures the expected time spent in transient states before reaching a recurrent class. By bounding the variance contribution from transient states using law-of-total-variance ideas, the total sample complexity becomes O(SA(B+H)/ε²).
- Core assumption: The MDP satisfies the bounded transient time property with parameter B.
- Evidence anchors:
  - [abstract] "We also initiate the study of sample complexity in general (multichain) average-reward MDPs. We argue a new transient time parameter B is necessary, establish an O(SA(B+H)/ε²) complexity bound, and prove a matching minimax lower bound."
  - [section] "In general MDPs, the complexity cannot be captured solely by ∥h*∥span. We first argue this point informally using the simple example in Figure 1..."
- Break condition: If B is very large or the bounded transient time assumption is violated, the complexity bound degrades.

### Mechanism 3
- Claim: Improving the analysis of variance parameters in discounted MDPs from 1/(1-γ)³ to H/(1-γ)² leads to better sample complexity bounds.
- Mechanism: By decomposing the variance of the cumulative discounted reward recursively and leveraging the low span of the optimal policy, the variance is bounded by O(H/(1-γ)) instead of O(1/(1-γ)²). This improvement is then propagated through the reduction from average-reward to discounted MDPs.
- Core assumption: The MDP is weakly communicating and H is finite.
- Evidence anchors:
  - [section] "The key to obtaining this improved complexity is a careful analysis of certain instance-specific variance parameters. It suffices to bound ||V*γ - V*γ, p||_∞ and ||Vπ*γ, p - Vπ*γ, p||_∞ by O(ε)."
  - [section] "We instead seek to bound ||Vπ*γ [∑∞t=0 γtRt]||∞ ≤ O(H/(1-γ))."
- Break condition: If the MDP is not weakly communicating or if H is very large compared to 1/(1-γ), the variance bound degrades.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The paper studies sample complexity in MDPs under different reward criteria.
  - Quick check question: What is the difference between finite-horizon, discounted, and average-reward MDPs?

- Concept: Bias function and span
  - Why needed here: The span of the optimal bias function H is a key complexity parameter in weakly communicating MDPs.
  - Quick check question: How is the span of a function defined, and why is it relevant for average-reward MDPs?

- Concept: Reduction techniques
  - Why needed here: The paper reduces average-reward MDPs to discounted MDPs to leverage existing algorithms and analysis.
  - Quick check question: What are the key challenges in reducing average-reward MDPs to discounted MDPs?

## Architecture Onboarding

- Component map: MDP simulator -> Discounted MDP solver (Algorithm 1) -> Average-to-discount reduction (Algorithm 2) -> Variance analysis module -> Lower bound construction module

- Critical path:
  1. Generate n samples per state-action pair using the MDP simulator.
  2. Construct empirical transition kernel and perturbed reward.
  3. Solve the resulting discounted MDP using Algorithm 1.
  4. Analyze the variance parameters to bound the error.
  5. Reduce the average-reward MDP to a discounted MDP and apply the bounds.

- Design tradeoffs:
  - Choosing the discount factor γ: Closer to 1 gives better approximation but requires more samples.
  - Perturbation level ξ: Larger perturbation ensures separation but may increase error.
  - Sample size n: Larger n gives better estimates but increases computational cost.

- Failure signatures:
  - High variance in the empirical transition kernel estimates.
  - Large error in the value function estimates.
  - Poor approximation of the optimal policy due to insufficient samples.

- First 3 experiments:
  1. Test the reduction from average-reward to discounted MDPs on a simple weakly communicating MDP with known H.
  2. Verify the variance bounds in the discounted MDP setting by comparing empirical and theoretical variances.
  3. Construct a general MDP with known B and H to test the full algorithm and bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sample complexity bounds for average-reward MDPs be achieved without knowledge of the optimal bias span H or the transient time parameter B?
- Basis in paper: [explicit] The authors state that "it is unclear if H-based sample complexities are possible without knowing H" and all previous algorithms require knowledge of their respective complexity parameters.
- Why unresolved: This is a fundamental question about whether the proposed sample complexity bounds can be achieved in a more practical setting where these parameters are unknown.
- What evidence would resolve it: A new algorithm that achieves the same sample complexity bounds without requiring prior knowledge of H or B, or a lower bound proving that such knowledge is necessary.

### Open Question 2
- Question: Is the relationship B ≤ 4τunif tight for uniformly mixing MDPs, or can it be improved?
- Basis in paper: [explicit] The authors provide Lemma 27 showing B ≤ 4τunif but do not investigate if this bound is tight.
- Why unresolved: Understanding the tightness of this relationship could provide insights into the structure of general MDPs and potentially lead to improved sample complexity bounds.
- What evidence would resolve it: Constructing an MDP where B = 4τunif or proving that B ≤ cτunif for some c < 4, along with a matching lower bound if possible.

### Open Question 3
- Question: Can the sample complexity bounds for general MDPs be further improved beyond the current O(SA(B+H)/ε²) bound?
- Basis in paper: [inferred] The authors establish this bound as optimal up to logarithmic factors, but do not rule out the possibility of a tighter dependence on other parameters.
- Why unresolved: While the current bounds are optimal up to log factors, there may be room for improvement in the dependence on other parameters or in removing logarithmic factors.
- What evidence would resolve it: A new algorithm achieving a better sample complexity bound, or a matching lower bound proving that the current bound is indeed tight up to constants.

## Limitations
- Assumes access to a generative model (simulator) that can obtain independent samples from P(·|s,a) for any state-action pair
- Complexity bounds depend on problem-dependent parameters H and B which may be difficult to estimate in practice
- The reduction from average-reward to discounted MDPs introduces additional error that needs careful control

## Confidence
- High confidence: The sample complexity bound of O(SAH/ε²) for weakly communicating MDPs is well-established and the reduction approach is sound
- Medium confidence: The introduction of the transient time parameter B and the bound O(SA(B+H)/ε²) for general MDPs are novel contributions, but the necessity of B is argued informally through examples
- Low confidence: The improvement in variance analysis from 1/(1-γ)³ to H/(1-γ)² is stated but the technical details are not fully verified

## Next Checks
1. Reproduce the variance analysis in the discounted MDP setting by computing empirical variances on synthetic MDPs and comparing with the theoretical bounds of O(H/(1-γ))
2. Construct a multichain MDP with known transient time B and verify that the sample complexity scales as O(SA(B+H)/ε²)
3. Test the average-to-discount reduction algorithm on a weakly communicating MDP with known H and verify that the policy obtained is ε-optimal with the claimed sample complexity