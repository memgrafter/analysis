---
ver: rpa2
title: Classification of Radiological Text in Small and Imbalanced Datasets in a Non-English
  Language
arxiv_id: '2409.20147'
source_url: https://arxiv.org/abs/2409.20147
tags: []
core_contribution: This study evaluated natural language processing models for classifying
  radiology reports in Danish, a low-resource language, using three datasets with
  small, imbalanced samples related to epilepsy patients. The models tested included
  BERT-like transformers (with and without domain-specific pretraining), few-shot
  learning with sentence transformers (SetFit), and prompted large language models
  (LLMs).
---

# Classification of Radiological Text in Small and Imbalanced Datasets in a Non-English Language

## Quick Facts
- arXiv ID: 2409.20147
- Source URL: https://arxiv.org/abs/2409.20147
- Reference count: 14
- Key outcome: BERT-like models with domain-specific pretraining achieved highest performance for classifying Danish radiology reports, while SetFit and LLM models underperformed

## Executive Summary
This study evaluates natural language processing models for classifying radiology reports in Danish, a low-resource language, using three small, imbalanced datasets related to epilepsy patients. The models tested include BERT-like transformers (with and without domain-specific pretraining), few-shot learning with sentence transformers (SetFit), and prompted large language models (LLMs). Results show that BERT-like models, particularly those pretrained on radiology-specific text, achieved the highest classification performance. While none of the models reached the accuracy level required for fully automated classification, their high recall for the majority class suggests potential for data filtering to reduce manual labeling effort.

## Method Summary
The study evaluated three types of models on Danish radiology reports for epilepsy patients: BERT-like transformers (roberta-base-danish, DanskBERT, xlm-roberta-base) with and without pretraining on a corpus of 1.2 million radiology reports, few-shot learning with sentence transformers (SetFit), and prompted LLMs (munin-neuralbeagle-7b, Meta-Llama-3-70B-Instruct, BioMistral-7B). Data preprocessing involved filtering sentences using regular expressions related to target labels (focal cortical dysplasia, mesial temporal sclerosis, and hippocampal abnormalities). Model performance was evaluated using F1-score (macro) and balanced accuracy across three labeled datasets with class imbalances.

## Key Results
- BERT-like models with domain-specific pretraining achieved the highest classification performance
- SetFit and LLM models underperformed compared to BERT-like models, with LLMs performing the worst
- None of the models reached the accuracy level required for fully automated classification
- Classifiers showed potential for data filtering due to high recall for the majority class

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERT-like models pretrained on domain-specific radiology text achieve higher performance in low-resource languages than general multilingual models.
- Mechanism: Domain-specific pretraining exposes the model to specialized vocabulary, terminology, and report structures, improving its ability to encode medically relevant information in the target language.
- Core assumption: The radiology-specific corpus captures sufficient linguistic and semantic patterns to meaningfully improve classification performance.
- Evidence anchors:
  - [abstract] "BERT-like models pretrained in the target domain of radiology reports currently offer the optimal performances"
  - [section 4] "Expectedly, in almost all cases pretraining the BERT-like models on the corpus of 1,2 million radiology reports improved the predictive performances of the models"
  - [corpus] Weak evidence: no direct comparison between radiology-pretrained and non-radiology-pretrained models in the corpus, but this is a general observation from the paper's results.

### Mechanism 2
- Claim: SetFit and LLM models underperform compared to BERT-like models in this low-resource, imbalanced, non-English classification task.
- Mechanism: Sentence transformers (SetFit) may lose critical context by encoding sentences individually, and LLMs may struggle with translation quality and few-shot prompt formatting in low-resource languages.
- Core assumption: The classification task requires understanding of inter-sentence context and domain-specific nuances that are lost in sentence-level embeddings or poorly translated inputs.
- Evidence anchors:
  - [abstract] "Notably, the SetFit and LLM models underperformed compared to BERT-like models, with LLM performing the worst"
  - [section 5] "The SetFit approach, which optimizes the embeddings of sentence transformers... has been introduced as a competitive approach... Contrary to our expectations, this approach rarely outperformed the BERT-like transformers"
  - [corpus] Weak evidence: no direct comparison of SetFit vs BERT-like models in the corpus, but this is a general observation from the paper's results.

### Mechanism 3
- Claim: Data filtering using classifiers with high recall for the majority class can reduce manual labeling effort without sacrificing classification accuracy.
- Mechanism: By first classifying reports and only manually reviewing those predicted as minority class, the amount of data requiring expert review is reduced while maintaining high recall for the majority class.
- Core assumption: The classifiers have sufficiently high recall for the majority class and acceptable precision to make data filtering practical.
- Evidence anchors:
  - [abstract] "However, they show potential for data filtering, which could reduce the amount of manual labeling required"
  - [section 5] "a closer look at the confusion matrices reveals that some of the classifiers have an almost perfect recall for the most numerous class... Therefore, when manually labeling large datasets a substantial amount of work could potentially be avoided"
  - [corpus] No direct evidence in the corpus, but this is a practical implication drawn from the results.

## Foundational Learning

- Concept: Natural language processing (NLP) in medical domain
  - Why needed here: The paper focuses on applying NLP models to classify radiology reports in a medical context.
  - Quick check question: What are some common challenges when applying NLP to medical text data?

- Concept: Imbalanced datasets and their impact on model performance
  - Why needed here: The datasets used in the study are highly imbalanced, which affects the choice of evaluation metrics and model training strategies.
  - Quick check question: How does class imbalance affect the performance of classification models, and what metrics are commonly used to evaluate them?

- Concept: Pretraining and fine-tuning in transformer models
  - Why needed here: The study evaluates the impact of pretraining BERT-like models on a domain-specific corpus before fine-tuning them for the classification task.
  - Quick check question: What is the difference between pretraining and fine-tuning in transformer models, and how do they contribute to improved performance on downstream tasks?

## Architecture Onboarding

- Component map:
  Data extraction and preprocessing -> Model training and evaluation -> Performance analysis

- Critical path:
  1. Extract and preprocess radiology reports
  2. Train and evaluate BERT-like models with and without domain-specific pretraining
  3. Train and evaluate SetFit and LLM models
  4. Analyze performance and identify potential for data filtering

- Design tradeoffs:
  - Using domain-specific pretraining vs. general multilingual models
  - Sentence-level embeddings (SetFit) vs. document-level context (BERT-like models)
  - Few-shot prompting for LLMs vs. full fine-tuning of smaller models

- Failure signatures:
  - Low recall for the majority class in data filtering scenarios
  - High false positive rates leading to unnecessary manual review
  - Underperformance of SetFit and LLM models compared to BERT-like models

- First 3 experiments:
  1. Compare the performance of BERT-like models with and without domain-specific pretraining on a small subset of the data
  2. Evaluate the impact of sentence-level preprocessing on SetFit model performance
  3. Test the effectiveness of data filtering by manually reviewing a sample of reports predicted as minority class by the best-performing classifier

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BERT-like models compare to human experts in classifying radiological text in non-English languages?
- Basis in paper: [inferred] The paper states that none of the models achieved the accuracy level required for fully automated classification, but human experts are often desired to have accuracy above 90%.
- Why unresolved: The paper does not provide a direct comparison between model performance and human expert accuracy in non-English languages.
- What evidence would resolve it: A study comparing the classification accuracy of human experts and BERT-like models on the same dataset of radiological reports in a non-English language.

### Open Question 2
- Question: What specific aspects of the translation from Danish to English affect the performance of large language models in classifying radiological text?
- Basis in paper: [explicit] The paper mentions that translation from Danish to English was sometimes suboptimal, potentially negatively impacting LLM predictions.
- Why unresolved: The paper does not provide detailed analysis on which aspects of translation (e.g., medical terminology, sentence structure) are most problematic for LLM performance.
- What evidence would resolve it: An analysis of translation quality and its correlation with LLM classification accuracy, including identification of specific translation errors that most impact model performance.

### Open Question 3
- Question: How does the performance of sentence transformers (SetFit) compare to BERT-like models when applied to more complex radiological texts with longer reports?
- Basis in paper: [explicit] The paper notes that SetFit models rarely outperformed BERT-like transformers, possibly due to the complexity of radiology reports containing many details not directly relevant to classification.
- Why unresolved: The paper does not test these models on more complex or longer radiological reports to determine if performance differences persist.
- What evidence would resolve it: A comparative study using SetFit and BERT-like models on a dataset of longer, more complex radiological reports to evaluate performance differences.

### Open Question 4
- Question: What is the potential impact of domain-specific pretraining on the performance of multilingual models (e.g., XLM-RoBERTa) in classifying radiological text in low-resource languages?
- Basis in paper: [explicit] The paper shows that pretraining BERT-like models on a domain-specific corpus of radiology reports improved performance in almost all cases.
- Why unresolved: The paper does not specifically investigate the impact of domain-specific pretraining on multilingual models for low-resource languages.
- What evidence would resolve it: A study comparing the performance of multilingual models with and without domain-specific pretraining on radiological text in a low-resource language.

## Limitations

- Results are limited to a single non-English language (Danish), making generalizability to other low-resource languages uncertain
- Evaluation focuses exclusively on epilepsy-related radiology reports, which may not represent the broader domain of medical text classification
- The impact of translation quality on LLM performance is acknowledged but not thoroughly investigated

## Confidence

**High Confidence**: The finding that BERT-like models with domain-specific pretraining outperform general models in this specific setting is well-supported by the experimental results and aligns with established principles of transfer learning.

**Medium Confidence**: The claim that SetFit and LLM models underperform compared to BERT-like models is supported by the results, but the reasons for this underperformance (particularly for LLMs) are speculative and would benefit from further investigation.

**Low Confidence**: The practical recommendation for data filtering based on classifier recall is promising but lacks direct experimental validation in the paper, relying instead on post-hoc analysis of confusion matrices.

## Next Checks

1. **Translation Quality Assessment**: Conduct a controlled experiment comparing LLM performance on translated versus natively Danish prompts to isolate the impact of translation quality on classification accuracy.

2. **Cross-Lingual Generalization Test**: Replicate the experiment with radiology reports in another low-resource language (e.g., Swedish or Finnish) to determine whether the superiority of BERT-like models with domain pretraining extends beyond Danish.

3. **Ablation Study on Pretraining Data**: Compare models pretrained on the radiology corpus against models pretrained on general Danish text of similar size to quantify the specific contribution of domain-specific pretraining to performance improvements.