---
ver: rpa2
title: Explaining Text Similarity in Transformer Models
arxiv_id: '2405.06604'
source_url: https://arxiv.org/abs/2405.06604
tags:
- similarity
- interactions
- explanations
- relevance
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BiLRP, a second-order explainable AI method
  for analyzing semantic similarity in Transformer-based models. The method extends
  layer-wise relevance propagation (LRP) to compute interaction scores between token
  pairs that drive similarity predictions.
---

# Explaining Text Similarity in Transformer Models

## Quick Facts
- arXiv ID: 2405.06604
- Source URL: https://arxiv.org/abs/2405.06604
- Authors: Alexandros Vasileiou; Oliver Eberle
- Reference count: 23
- Primary result: BiLRP accurately identifies relevant feature interactions driving similarity predictions in Transformers

## Executive Summary
This paper introduces BiLRP, a second-order explainable AI method for analyzing semantic similarity in Transformer-based models. The method extends layer-wise relevance propagation (LRP) to compute interaction scores between token pairs that drive similarity predictions. Experiments demonstrate that BiLRP more accurately identifies relevant feature interactions compared to baseline methods like token embeddings and Hessian × Product. The authors validate explanations through a synthetic task with ground truth interactions and perturbation analysis on real-world datasets (STSb, SICK, BIOSSES).

## Method Summary
BiLRP computes second-order relevance scores for token pair interactions in Transformer models by factoring LRP relevance propagation across embedding dimensions. The method applies modified propagation rules for Transformer-specific layers (attention heads, layer normalization, GeLU) to maintain relevance conservation. For each token pair, BiLRP produces interaction scores by summing tensor products of relevance matrices. The approach is evaluated through synthetic tasks with ground truth interactions and corpus-level analysis of semantic similarity across multiple languages and domains.

## Key Results
- BiLRP achieves higher cosine similarity (0.81) with ground truth interactions compared to H×P (0.62) and embedding baselines (0.67) on synthetic tasks
- Semantic similarity can be predicted using small subsets of part-of-speech interactions, primarily nouns, verbs, and their combinations
- Multilingual models show weaker cross-lingual matching of nouns and verbs compared to monolingual models
- SGPT better captures domain-specific terminology in biomedical texts compared to general-purpose models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BiLRP computes second-order relevance scores for token pair interactions that drive similarity predictions.
- Mechanism: By factoring the LRP relevance propagation across each embedding dimension, BiLRP sums tensor products of relevance matrices, producing interaction scores for each token pair.
- Core assumption: Relevance is conserved across the backward pass when proper propagation rules (AH, LN, GA) are applied.
- Evidence anchors:
  - [abstract] "Using BiLRP, an extension developed for computing second-order explanations in bilinear similarity models, we investigate which feature interactions drive similarity in NLP models."
  - [section 3.1] "For deep neural networks, these derivatives have been found to be noisy (Balduzzi et al., 2017; Montavon et al., 2018), which motivates the formulation of robustified propagation rules within the LRP framework, resulting in the BiLRP method."
  - [corpus] Weak: no direct experimental evidence of conservation in corpus results; only theoretical derivation.
- Break condition: If any propagation rule is omitted or incorrectly implemented, relevance is no longer conserved, causing explanation degradation.

### Mechanism 2
- Claim: Transformer-specific propagation rules (attention head, layer norm, GeLU) are necessary for faithful explanations.
- Mechanism: Each rule modifies the gradient computation so that the backward pass preserves relevance flow through non-linearities and normalization layers.
- Core assumption: The bilinear structure of QKV self-attention creates a relevance "break" that must be patched by detaching the attention score variable.
- Evidence anchors:
  - [section 3.2] "To compute better explanations for Transformers, leveraging gradient information has proven to be effective. Yet, the non-linear structure of Transformers motivates specific gradient propagation rules to reflect the model prediction more reliably."
  - [section 3.2] "For the attention head, the forward pass can be formulated as yj = Σ_i h_i[p_ij].detach(), viewing the attention scores p_ij as a weighting matrix for the current residual stream representation h_i and detaching the associated variable p_ij from the computation graph."
  - [corpus] Weak: conservation plots in appendix C show theoretical improvement, but corpus experiments do not isolate the effect of individual rules.
- Break condition: Removing the detach or reattaching the variable breaks conservation, causing BiLRP to collapse to first-order behavior.

### Mechanism 3
- Claim: Second-order explanations outperform first-order methods (embeddings, H×P) on tasks with ground-truth interactions.
- Mechanism: By explicitly modeling pairwise interactions rather than marginal feature importance, BiLRP captures compositional cues that first-order methods miss.
- Core assumption: The similarity model's output is well-approximated by a bilinear form over token embeddings.
- Evidence anchors:
  - [section 4.3] "In Figure 1, we observe that computing interactions directly from token embeddings results in pairwise attributions mainly between same tokens that are not selective with respect to their assigned part-of-speech (POS) tag. For H ×P, the interactions are much more selective with regard to nouns and proper nouns, and we observe considerable token interactions that are assigned negative relevance."
  - [section 4.3] "BiLRP is able to select the relevant tokens in comparison to the other baselines with higher accuracy, which is supported by the highest average cosine similarity (ACS) between true interactions and BiLRP of 0.81 in comparison to 0.62 for H×P, and 0.67 for the embedding baseline."
  - [corpus] Moderate: perturbation curves show BiLRP consistently yields lower AUPC across models and datasets, but differences are sometimes small.
- Break condition: If the model is not approximately bilinear (e.g., highly non-linear interactions), the second-order assumption fails and first-order methods may be preferable.

## Foundational Learning

- Concept: Layer-wise Relevance Propagation (LRP)
  - Why needed here: Provides a principled way to propagate relevance backward through deep networks while conserving the total relevance score.
  - Quick check question: If an LRP pass starts with relevance 1.0 at the output, what must be true about the sum of relevance at the input?

- Concept: Second-order derivatives and interaction attribution
  - Why needed here: Similarity scores are bilinear in token embeddings, so first-order gradients miss pairwise feature interactions that are crucial for understanding the prediction.
  - Quick check question: In a bilinear model y = ⟨ϕ(x), ϕ(x')⟩, what term in the Taylor expansion captures the interaction between features i and i'?

- Concept: Transformer propagation rules (attention head, layer norm, GeLU)
  - Why needed here: These layers introduce non-linearities that break relevance conservation unless special handling is applied.
  - Quick check question: Why does detaching the attention score variable in the backward pass help maintain conservation?

## Architecture Onboarding

- Component map: Tokenize -> Load model + rules -> Compute BiLRP -> Aggregate -> Visualize
- Critical path: Tokenize → Load model + rules → Compute BiLRP → Aggregate → Visualize
- Design tradeoffs: BiLRP requires 2 × h backward passes (h = embedding dim); can be batched if GPU memory allows; factorization enables reuse across sentence pairs
- Failure signatures: Relevance not conserved (sum ≠ prediction), explanations dominated by [CLS]/[SEP] tokens, AUPC not lower than baseline, second-order relevance too sparse
- First 3 experiments:
  1. Run BiLRP on a small synthetic similarity dataset with ground-truth noun interactions; verify that top interactions match ground truth
  2. Apply BiLRP to a single STSb pair; plot relevance heatmap and confirm POS patterns align with Table 3 intuition
  3. Perform conservation check on SBERT: plot sum(relevance) vs. model output for 100 samples to confirm linear relationship

## Open Questions the Paper Calls Out

- Question: What specific types of structured rationales or annotations would be most useful for evaluating second-order explanations in complex NLP tasks like semantic similarity?
  - Basis in paper: [explicit] The authors state "Considering the fast-growing number of applications based on foundation models, we believe that explainable AI has a critical role in ensuring their safe, robust and compliant use. For complex tasks like semantic similarity, datasets that contain detailed structured rationales on the level of interactions are needed to improve model alignment with human expectations."
  - Why unresolved: While the authors emphasize the need for structured rationales, they don't specify what form these should take or what level of detail would be most beneficial for evaluating interaction-based explanations.
  - What evidence would resolve it: A framework or proposal for designing datasets with structured rationales that capture feature interactions, along with experiments showing how different types of rationales affect the evaluation of explanation methods.

- Question: How do second-order explanations perform on other types of bilinear models beyond semantic similarity, such as cross-modal retrieval or multi-task learning frameworks?
  - Basis in paper: [inferred] The paper focuses on BiLRP for explaining semantic similarity in Transformers, but mentions that "these structures are also commonly used as part of internal computations, such as matching key and value representations, which could also be analyzed using interactions."
  - Why unresolved: The authors demonstrate BiLRP's effectiveness on semantic similarity but don't explore its applicability to other bilinear model architectures or tasks that might benefit from interaction-based explanations.
  - What evidence would resolve it: Experiments applying BiLRP or similar second-order methods to cross-modal retrieval tasks, multi-task learning models, or other bilinear architectures, comparing performance to first-order explanations.

- Question: What is the computational overhead of computing second-order explanations at scale, and are there efficient approximation methods that maintain faithfulness while reducing computation time?
  - Basis in paper: [explicit] The authors note "Computing second-order explanations requires to compute as many backward passes as there are sentence embedding dimensions for each sentence in a pair. For the here considered embedding dimension of 768, this required around two minutes computation time on a 12GB P100 GPU."
  - Why unresolved: While the authors acknowledge the computational cost, they don't explore potential optimizations, approximation techniques, or hardware acceleration methods that could make second-order explanations more practical for large-scale applications.
  - What evidence would resolve it: Benchmarks comparing different approximation methods (e.g., sampling, dimensionality reduction, or model compression techniques) against exact computation, showing trade-offs between computational efficiency and explanation quality.

## Limitations

- BiLRP implementation complexity due to reliance on detailed Transformer propagation rules that are only briefly described
- Second-order assumption validity may not hold for all model architectures or pooling strategies
- Corpus-level aggregation methodology may mask important individual variations and language-specific phenomena

## Confidence

- High Confidence - Claims about BiLRP outperforming baseline methods on synthetic tasks with ground truth interactions
- Medium Confidence - Claims about multilingual models struggling with cross-lingual noun and verb matching
- Low Confidence - Claims about SGPT better capturing domain-specific terminology in biomedical texts

## Next Checks

1. **Conservation verification experiment** - Implement BiLRP with the three modified propagation rules and run conservation checks on SBERT across 100 random STSb pairs. Plot the sum of propagated relevance versus model output to verify the linear relationship and quantify the error.

2. **Cross-lingual interaction analysis** - Extend the multilingual analysis beyond POS-based aggregation to examine specific token pair interactions between languages. Test whether mBERT consistently fails to match English nouns with German nouns across multiple language pairs, controlling for semantic similarity and word frequency effects.

3. **Domain adaptation comparison** - Compare SGPT's biomedical term interactions against both the base BERT model and a biomedical-specific BERT variant (like BioBERT) on the BIOSSES dataset. Use perturbation analysis to quantify whether SGPT's advantages are due to general fine-tuning versus domain-specific adaptation.