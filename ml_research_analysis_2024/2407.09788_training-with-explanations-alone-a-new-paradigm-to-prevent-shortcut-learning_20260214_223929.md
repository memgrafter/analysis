---
ver: rpa2
title: 'Training with Explanations Alone: A New Paradigm to Prevent Shortcut Learning'
arxiv_id: '2407.09788'
source_url: https://arxiv.org/abs/2407.09788
tags: []
core_contribution: This paper introduces Training with Explanations Alone (TEA), a
  novel paradigm for preventing shortcut learning in deep neural networks. The core
  idea is to train a student model to match the explanation heatmaps of an unbiased
  teacher model, rather than focusing on matching outputs directly.
---

# Training with Explanations Alone: A New Paradigm to Prevent Shortcut Learning

## Quick Facts
- arXiv ID: 2407.09788
- Source URL: https://arxiv.org/abs/2407.09788
- Reference count: 40
- Outperforms 14 SOTA methods on 5 datasets with strong background/foreground bias

## Executive Summary
This paper introduces Training with Explanations Alone (TEA), a novel paradigm for preventing shortcut learning in deep neural networks. The core idea is to train a student model to match the explanation heatmaps of an unbiased teacher model, rather than focusing on matching outputs directly. By learning from the teacher's explanations, the student learns to focus on the same image features as the teacher, effectively ignoring spurious correlations and background biases. The authors demonstrate that TEA outperforms 14 state-of-the-art methods across 5 datasets with strong background or foreground bias, including Waterbirds and an X-Ray dataset for COVID-19 and pneumonia classification.

## Method Summary
TEA is a distillation-based approach where a student model learns to match the explanation heatmaps of an unbiased teacher rather than the raw outputs. During training, the student's explanation heatmaps (computed using methods like LRP) are optimized to match the teacher's heatmaps using an L1 loss. This forces the student to focus on the same input features as the teacher, ignoring spurious correlations like background artifacts. The method can use teachers trained on debiased data or large pre-trained models like CLIP. Optionally, a separate output distillation loss can be applied to the last layer only to improve convergence.

## Key Results
- TEA outperforms 14 SOTA methods across 5 datasets with strong background or foreground bias
- LRP distillation achieved 98.2% OOD accuracy in COLOURED MNIST, surpassing deep feature distillation and IRM (92.1% and 60.2% respectively)
- Shows promise for improving generalization and robustness to bias in critical domains like medical imaging
- Demonstrates effectiveness on Waterbirds and X-Ray datasets for COVID-19 and pneumonia classification

## Why This Works (Mechanism)

### Mechanism 1
Matching explanation heatmaps rather than raw outputs prevents shortcut learning by forcing the student to focus on the same input features as the unbiased teacher. During training, the student's explanation heatmaps are optimized to match the teacher's heatmaps using an L1 loss. This ensures the student learns to attend to the same spatial regions as the teacher, ignoring spurious correlations like background artifacts or foreground biases. Core assumption: The explanation heatmap produced by the teacher accurately reflects the true causal features for classification, not spurious correlations.

### Mechanism 2
Training with explanations alone avoids competition between classification and debiasing losses, improving convergence and stability. By using only the explanation distillation loss (and optionally a separate output distillation loss on the last layer only), the student avoids balancing competing objectives. This prevents the student from optimizing for accuracy at the cost of ignoring bias. Core assumption: The explanation loss alone is sufficient to guide the student toward accurate classification without a standard cross-entropy loss on intermediate layers.

### Mechanism 3
Distilling from debiased data allows transfer of bias resistance without requiring unbiased data during training. The teacher is trained on debiased images (e.g., backgrounds removed or replaced with noise), while the student receives the original biased images. The student learns to mimic the teacher's explanations, effectively learning to ignore the spurious correlations present in the input. Core assumption: The debiasing process does not remove important foreground features and maintains spatial alignment between debiased and original images.

## Foundational Learning

- **Concept**: Explanation heatmaps (e.g., LRP, Grad-CAM)
  - Why needed here: The core mechanism relies on producing and matching explanation heatmaps between teacher and student. Understanding how different explanation methods work is critical.
  - Quick check question: What is the difference between input-level explanations (like LRP) and feature-level explanations (like Grad-CAM) in terms of what they represent?

- **Concept**: Distillation techniques (knowledge transfer)
  - Why needed here: TEA is a form of distillation, but instead of transferring logits or features, it transfers explanations. Understanding standard distillation helps contextualize TEA's novelty.
  - Quick check question: How does explanation distillation differ from standard output or feature distillation in terms of what is being matched?

- **Concept**: Spurious correlations and shortcut learning
  - Why needed here: The motivation and evaluation of TEA are based on its ability to prevent shortcut learning caused by spurious correlations. Understanding this concept is essential.
  - Quick check question: Why is shortcut learning particularly problematic in medical imaging applications?

## Architecture Onboarding

- **Component map**: Original biased data -> Student model -> Student explanation heatmap -> L1 loss -> Teacher explanation heatmap <- Teacher model <- Debiased data

- **Critical path**:
  1. Load teacher model and freeze parameters
  2. For each training batch: Compute teacher explanation heatmap, compute student explanation heatmap, calculate L1 loss between heatmaps, backpropagate loss to update student parameters
  3. Optionally apply output distillation loss to last layer only

- **Design tradeoffs**:
  - Using LRP vs. Grad-CAM: LRP provides input-level explanations without alignment assumptions but is computationally heavier
  - Explanation-only vs. DL-DL: Explanation-only is simpler but may struggle to converge; DL-DL helps convergence but adds complexity
  - Teacher choice: Large pre-trained models (like CLIP) are unbiased but expensive; smaller debiased teachers are cheaper but may have lower accuracy

- **Failure signatures**:
  - Student attention maps look random or focus on irrelevant regions
  - Student accuracy is much lower than teacher accuracy
  - Training loss plateaus early or fluctuates wildly
  - Student overfits to training bias (high IID accuracy, low OOD accuracy)

- **First 3 experiments**:
  1. Reproduce COLOURED MNIST results: Train student with LRP distillation from unbiased teacher, evaluate IID vs. OOD accuracy
  2. Compare explanation methods: Train student with LRP, Grad-CAM, and attention distillation; compare OOD accuracy and generalization gaps
  3. Test DL-DL vs. explanation-only: Train with both settings on DogsWithTies; compare convergence speed and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the TEA approach generalize beyond image-based tasks to other domains like natural language processing or tabular data where shortcut learning is prevalent?
- Basis in paper: [inferred] The paper demonstrates TEA's effectiveness on image datasets with background and foreground biases but does not explore its applicability to other data modalities or domains.
- Why unresolved: The authors only evaluate TEA on image datasets (COLOURED MNIST, DogsWithTies, COCO-on-Places), leaving its performance on non-visual data unexplored.
- What evidence would resolve it: Empirical validation of TEA on NLP tasks (e.g., text classification with spurious correlations in word usage) or tabular datasets (e.g., medical records with demographic biases) showing similar improvements in OOD generalization.

### Open Question 2
- Question: What is the computational overhead of TEA compared to standard training, and how does it scale with model size and dataset complexity?
- Basis in paper: [explicit] The authors mention that TEA requires careful tuning of loss functions and hyper-parameters, but do not provide detailed analysis of computational costs relative to baseline methods.
- Why unresolved: While the paper discusses convergence benefits, it lacks quantitative comparisons of training time, memory usage, and scalability across different model architectures and dataset sizes.
- What evidence would resolve it: Systematic benchmarks comparing training time, GPU memory requirements, and convergence speed of TEA versus standard distillation and other bias mitigation techniques across models ranging from ResNet18 to large vision-language models.

### Open Question 3
- Question: How sensitive is TEA to the choice of teacher model, and what are the minimal requirements for an effective teacher?
- Basis in paper: [explicit] The authors assume the existence of a pre-trained unbiased teacher but do not investigate how different teacher architectures or training strategies affect student performance.
- Why unresolved: The paper uses specific teachers (ResNet34 for COLOURED MNIST, zero-shot CLIP for DogsWithTies) without exploring the impact of teacher quality, architecture differences, or whether smaller/weak teachers suffice.
- What evidence would resolve it: Ablation studies varying teacher model size, architecture (CNN vs transformer), and training data diversity to determine the relationship between teacher quality and student OOD performance.

## Limitations
- Method's reliance on explanation heatmaps assumes these heatmaps accurately reflect causal features rather than spurious correlations
- Computational cost of LRP calculations is substantial, limiting practical deployment
- Scalability to larger, more complex datasets beyond the five tested domains remains uncertain

## Confidence

- **High confidence**: The core mechanism of explanation distillation and its theoretical justification for preventing shortcut learning
- **Medium confidence**: The empirical superiority over 14 state-of-the-art methods, as results depend heavily on dataset construction and teacher model selection
- **Low confidence**: The scalability of the approach to larger, more complex datasets beyond the five tested domains

## Next Checks

1. Validate explanation heatmaps by comparing against ground truth feature importance in controlled synthetic datasets where true causal features are known
2. Test computational efficiency and convergence speed across different model architectures and input resolutions
3. Evaluate robustness when teacher models themselves have imperfect explanations or contain hidden shortcuts