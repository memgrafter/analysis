---
ver: rpa2
title: Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual
  Similarity
arxiv_id: '2402.13130'
source_url: https://arxiv.org/abs/2402.13130
tags:
- layer
- spearman
- correlation
- discriminator
- electra
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates ELECTRA's poor performance in semantic
  textual similarity (STS) tasks, where BERT typically excels. The authors observe
  a significant drop in performance for ELECTRA's discriminator in the final layers
  compared to earlier ones, suggesting over-specialization to the replaced token detection
  (RTD) pre-training task.
---

# Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity

## Quick Facts
- arXiv ID: 2402.13130
- Source URL: https://arxiv.org/abs/2402.13130
- Reference count: 40
- Key outcome: TMFT improves ELECTRA's STS performance by 8+ points while using fewer parameters

## Executive Summary
This paper investigates ELECTRA's surprisingly poor performance on semantic textual similarity (STS) tasks, where BERT typically excels. The authors discover that ELECTRA's discriminator model experiences a significant performance drop in its final layers, suggesting over-specialization to the replaced token detection (RTD) pre-training task. To address this, they propose truncated model fine-tuning (TMFT), which fine-tunes only up to a specific layer followed by mean pooling. TMFT improves Spearman correlation coefficients by over 8 points on the STS benchmark while using fewer parameters. Surprisingly, ELECTRA's generator model performs on par with BERT despite having significantly fewer parameters and a smaller embedding size.

## Method Summary
The paper proposes Truncated Model Fine-Tuning (TMFT) to repair ELECTRA's STS embeddings. TMFT involves fine-tuning only up to layer l of the transformer, followed by mean pooling of the layer output. The authors systematically evaluate this approach across different model sizes (small, base, large), languages (English, Korean, German, Spanish), and tasks (STS, paraphrase identification, textual entailment). They also explore enhancements through word similarity fine-tuning prior to TMFT and domain-adaptive pre-training using MLM. The evaluation uses Spearman correlation coefficient for STS tasks and F1 score for classification tasks, with comprehensive testing on STSB, MRPC, SICK, and word similarity datasets.

## Key Results
- ELECTRA's discriminator shows significant performance drop in final layers for STS tasks compared to earlier layers
- TMFT improves Spearman correlation by over 8 points on STS benchmark while using fewer parameters than full fine-tuning
- ELECTRA's generator performs on par with BERT on STS despite having significantly fewer parameters and smaller embedding size
- Improvements are consistent across model sizes, languages, and multiple tasks including paraphrase identification and textual entailment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ELECTRA's discriminator loses semantic generalization in final layers due to over-specialization to the replaced token detection (RTD) pre-training task.
- Mechanism: The discriminator's final layers become too finely tuned to distinguishing replaced tokens, at the expense of broader semantic understanding required for STS tasks.
- Core assumption: The RTD task creates a training signal that biases the discriminator toward token-level discrimination rather than sentence-level semantic representation.
- Evidence anchors:
  - [abstract] "We notice a significant drop in performance for the ELECTRA discriminator's last layer in comparison to prior layers."
  - [section 4.1] "ELECTRA base discriminator with 33.31M parameters maintains comparable performance to BERT base... The discriminator shows a different trend, gradually increasing until the ninth layer, after which performance drops sharply."
  - [corpus] No direct evidence; corpus mentions related work on layer pruning and embeddings but not the specific over-specialization mechanism.

### Mechanism 2
- Claim: Truncated Model Fine-Tuning (TMFT) repairs ELECTRA embeddings by fine-tuning only up to an earlier layer, avoiding the over-specialized final layers.
- Mechanism: By freezing later layers and fine-tuning only up to layer l, TMFT prevents the model from further specializing to token discrimination and allows the earlier layers to adapt to semantic similarity tasks.
- Core assumption: The earlier layers of ELECTRA retain sufficient semantic capacity that can be reactivated through task-specific fine-tuning.
- Evidence anchors:
  - [abstract] "We explore this drop and propose a way to repair the embeddings using a novel truncated model fine-tuning (TMFT) method. TMFT improves the Spearman correlation coefficient by over 8 points."
  - [section 3] "The TMFT method we propose for repairing ELECTRA's embeddings reduces to taking the l-th layer output followed by pooling and fine-tuning on the target task."
  - [corpus] No direct evidence; corpus does not mention TMFT specifically.

### Mechanism 3
- Claim: ELECTRA's generator model performs surprisingly well on STS tasks despite having fewer parameters and smaller embeddings because it uses a masked language modeling objective similar to BERT.
- Mechanism: The generator's MLM pre-training objective preserves the semantic generalization capabilities that BERT has, while the discriminator's RTD objective sacrifices these capabilities for token discrimination accuracy.
- Core assumption: The pre-training objective fundamentally shapes the types of representations learned, with MLM favoring semantic generalization and RTD favoring discriminative accuracy.
- Evidence anchors:
  - [abstract] "Further, we discover the surprising efficacy of ELECTRA's generator model, which performs on par with BERT, using significantly fewer parameters and a substantially smaller embedding size."
  - [section 4.1] "The generator is similar to BERT, except the generator's input embeddings are tied to the discriminator in pre-training."
  - [corpus] No direct evidence; corpus mentions related work on embeddings but not the specific generator vs discriminator comparison.

## Foundational Learning

- Concept: Semantic Textual Similarity (STS) task and evaluation metrics
  - Why needed here: The paper's core contribution is improving STS performance, so understanding what STS measures and how it's evaluated is fundamental
  - Quick check question: What does a Spearman correlation coefficient of 0.8 mean in the context of STS evaluation?

- Concept: Transformer architecture and layer-wise representations
  - Why needed here: The paper analyzes performance across different transformer layers and proposes layer-wise fine-tuning, requiring understanding of how information flows through transformer layers
  - Quick check question: How do self-attention mechanisms in transformer layers contribute to semantic understanding?

- Concept: Pre-training objectives and their impact on downstream task performance
  - Why needed here: The paper contrasts MLM (BERT, generator) vs RTD (discriminator) objectives and their effects on STS performance
  - Quick check question: How might a token discrimination task (RTD) differ from a masked token prediction task (MLM) in terms of learned representations?

## Architecture Onboarding

- Component map:
  Input layer → Transformer layers (0 to L) → Mean pooling → Cosine similarity → Loss function
  TMFT adds a truncation point at layer l where fine-tuning stops
  Generator vs Discriminator models with different pre-training objectives
  Optional components: Word similarity fine-tuning, Domain Adaptive Pre-training (DAPT)

- Critical path:
  1. Load pre-trained ELECTRA model (generator or discriminator)
  2. Apply mean pooling at layer l
  3. Fine-tune up to layer l for STS task
  4. Evaluate Spearman correlation on validation set
  5. Select best-performing layer l for test evaluation

- Design tradeoffs:
  - Layer selection: Earlier layers preserve more general representations but may lack task-specific adaptation; later layers provide task-specific features but risk over-specialization
  - Parameter efficiency vs performance: TMFT uses fewer parameters than full fine-tuning but may sacrifice some performance
  - Generator vs Discriminator: Generator offers parameter efficiency and better STS performance but may lack some discriminative capabilities

- Failure signatures:
  - Performance drop in final layers indicates over-specialization to RTD task
  - No improvement from TMFT suggests early layers are also corrupted
  - Generator underperforming BERT suggests MLM objective insufficient for the task

- First 3 experiments:
  1. Layer-wise analysis: Run TMFT with truncation at each layer l from 0 to L and plot Spearman correlation to identify performance trends
  2. Generator comparison: Compare ELECTRA generator with BERT on STS using full fine-tuning to validate generator's surprising efficacy
  3. Over-specialization test: Apply TMFT to a randomly initialized ELECTRA model to determine if architecture alone causes the performance pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does ELECTRA's discriminator experience a significant performance drop in the final layers for semantic textual similarity tasks?
- Basis in paper: [explicit] The paper explicitly observes a significant drop in performance for ELECTRA's discriminator in the last layers compared to earlier ones, suggesting over-specialization to the replaced token detection pre-training task.
- Why unresolved: While the paper hypothesizes that the drop is due to over-specialization to the RTD pre-training task, it does not definitively prove this causal relationship. The analysis using CKA values shows a drop in similarity between the discriminator and MLM models in final layers, but this is a correlation, not causation.
- What evidence would resolve it: Systematic experiments varying pre-training objectives, model architectures, and fine-tuning procedures to isolate the specific factors causing the performance drop would provide stronger evidence.

### Open Question 2
- Question: Can the truncated model fine-tuning (TMFT) approach be generalized to other language understanding tasks beyond semantic textual similarity, paraphrase identification, and textual entailment?
- Basis in paper: [inferred] The paper demonstrates TMFT's effectiveness on STS, paraphrase identification, and textual entailment tasks, but does not explore its applicability to other tasks like named entity recognition, question answering, or sentiment analysis.
- Why unresolved: The paper's scope is limited to specific tasks, leaving open the question of whether TMFT's benefits extend to other NLP tasks or if it is particularly suited to sentence-level similarity tasks.
- What evidence would resolve it: Applying TMFT to a diverse set of NLP tasks and comparing its performance against standard fine-tuning would determine its generalizability.

### Open Question 3
- Question: How does the performance of ELECTRA's generator model compare to other efficient alternatives like MobileBERT or DistilBERT on semantic textual similarity tasks?
- Basis in paper: [explicit] The paper discovers that ELECTRA's generator performs on par with BERT while using significantly fewer parameters and a smaller embedding size, but does not compare it to other efficient transformer variants.
- Why unresolved: While the paper establishes the generator's efficiency compared to BERT, it does not benchmark against other lightweight models designed for efficiency, leaving open the question of whether ELECTRA's generator is the most parameter-efficient option.
- What evidence would resolve it: A comprehensive comparison of ELECTRA's generator against other efficient models like MobileBERT, DistilBERT, and TinyBERT on STS and other tasks would clarify its relative efficiency and performance.

## Limitations

- Layer Selection Ambiguity: The paper doesn't provide clear guidance on selecting the optimal truncation layer l for TMFT, requiring practitioners to rely on grid search.
- Generalizability Beyond STS: The effectiveness of TMFT on STS tasks doesn't guarantee similar benefits for other NLP tasks where ELECTRA originally excelled.
- Pre-training Objective Causality: The paper hypothesizes that RTD pre-training causes over-specialization but doesn't definitively prove this causal relationship.

## Confidence

- High: TMFT improves STS performance by 8+ points on benchmark datasets
- Medium: ELECTRA generator performs on par with BERT despite fewer parameters
- Low: Causal relationship between RTD pre-training and performance drop is not definitively proven

## Next Checks

1. Verify CKA analysis shows similar layer-wise representation patterns between discriminator and MLM models in final layers
2. Implement TMFT on a randomly initialized ELECTRA model to test if architecture alone causes the performance pattern
3. Apply TMFT to a diverse set of NLP tasks beyond STS to test generalizability