---
ver: rpa2
title: Feature learning in finite-width Bayesian deep linear networks with multiple
  outputs and convolutional layers
arxiv_id: '2406.03260'
source_url: https://arxiv.org/abs/2406.03260
tags:
- networks
- learning
- deep
- https
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work provides rigorous results for the statistics of functions
  implemented by finite-width Bayesian deep linear networks with multiple outputs
  and convolutional layers. The key contributions include: (i) an exact non-asymptotic
  integral representation for the joint prior distribution over outputs, expressed
  as a mixture of Gaussians; (ii) an analytical formula for the posterior distribution
  in the case of squared error loss function; and (iii) a quantitative description
  of the feature learning infinite-width regime using large deviation theory.'
---

# Feature learning in finite-width Bayesian deep linear networks with multiple outputs and convolutional layers

## Quick Facts
- arXiv ID: 2406.03260
- Source URL: https://arxiv.org/abs/2406.03260
- Reference count: 40
- Primary result: Exact non-asymptotic integral representations for Bayesian deep linear networks showing output prior is mixture of Gaussians with Wishart matrices

## Executive Summary
This work provides rigorous results for the statistics of functions implemented by finite-width Bayesian deep linear networks with multiple outputs and convolutional layers. The authors derive exact non-asymptotic integral representations for the joint prior distribution over outputs, expressed as a mixture of Gaussians, and provide an analytical formula for the posterior distribution under squared error loss. Using large deviation theory, they quantitatively describe the feature learning infinite-width regime, demonstrating non-trivial explicit dependence on training inputs and labels. These results demonstrate that at finite width, the output prior is an exactly computable mixture of Gaussians with Wishart matrices, providing dimensional reduction, while in the infinite-width limit, the large deviation asymptotics characterize feature learning behavior.

## Method Summary
The paper analyzes Bayesian deep linear networks with independent normally distributed weights at each layer. For fully-connected networks with multiple outputs, the authors derive a non-asymptotic integral representation expressing the prior over outputs as a mixture of Gaussians with Wishart-distributed covariance matrices. The posterior under squared error loss is shown to remain a mixture of Gaussians with closed-form mixing distribution. For convolutional networks with single output, analogous representations are provided using tensor contractions to handle local kernel renormalization. The feature learning infinite-width regime is analyzed using large deviation theory, showing concentration of Wishart measures around data-dependent solutions that characterize feature learning as opposed to lazy training.

## Key Results
- Exact non-asymptotic integral representation showing finite-width output prior is a mixture of Gaussians with Wishart matrices
- Analytical formula for posterior distribution under squared error loss that remains a mixture of Gaussians with closed-form mixing distribution
- Large deviation analysis of feature learning infinite-width limit demonstrating explicit dependence on training inputs and labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The output prior of finite-width Bayesian deep linear networks is an exactly computable mixture of Gaussians with Wishart-distributed covariance matrices.
- Mechanism: The network output can be expressed as a product of independent Wishart matrices U1...UL and a standard normal matrix Z, scaled by layer widths and prior variances. This provides a non-asymptotic representation where all hidden layer sizes appear parametrically, enabling dimensional reduction.
- Core assumption: The weights are independent normally distributed with zero mean and variance 1/λℓ at each layer.
- Evidence anchors:
  - [abstract] "the output prior is an exactly computable mixture of Gaussians with Wishart matrices, providing dimensional reduction"
  - [section] "the prior over the outputs at finite-width in the linear case, showing that it is a mixture of Gaussians with an explicit mixing distribution"
  - [corpus] No direct corpus evidence found for this specific Wishart mixture claim.
- Break condition: If the weight independence assumption fails or if the layer widths approach infinity in a way that breaks the finite-width regime.

### Mechanism 2
- Claim: The posterior predictive distribution remains a mixture of Gaussians with closed-form mixing distribution under squared error loss.
- Mechanism: The Gaussian likelihood combined with the Gaussian mixture prior structure ensures the posterior also takes the form of a mixture of Gaussians, where the mixing measure is updated via Bayesian conditioning on the training data.
- Core assumption: The likelihood function is Gaussian (corresponding to quadratic loss).
- Evidence anchors:
  - [abstract] "the posterior distribution in the case of squared error loss function (Gaussian likelihood)"
  - [section] "the posterior is again a mixture of Gaussians and this leads to the rather standard equations for the bias and variance of a Gaussian Process"
  - [corpus] No direct corpus evidence found for this specific closed-form mixing distribution claim.
- Break condition: If the likelihood is non-Gaussian or if the prior structure deviates significantly from the mixture form.

### Mechanism 3
- Claim: In the feature learning infinite-width limit, large deviation theory shows non-trivial explicit dependence on training inputs and labels.
- Mechanism: The mean-field parametrization rescales the loss and outputs, leading to a concentration of the Wishart measure around data-dependent solutions that characterize feature learning, as opposed to the identity matrix concentration in lazy training.
- Core assumption: The mean-field parametrization is valid and provides the correct scaling for feature learning.
- Evidence anchors:
  - [abstract] "the so-called feature learning infinite-width limit... large deviation asymptotics show non-trivial explicit dependence on training inputs and labels"
  - [section] "Using the simple parametric dependence of our formulas from the layer widths, we provide an asymptotic analysis in the limit of large width, using the language of Large Deviation Theory"
  - [corpus] No direct corpus evidence found for this specific large deviation feature learning claim.
- Break condition: If the mean-field scaling is inappropriate or if the training data does not satisfy the necessary conditions for the large deviation analysis.

## Foundational Learning

- Concept: Wishart distribution and its properties
  - Why needed here: The core of the theoretical results relies on expressing the network output as a mixture involving Wishart-distributed matrices, which requires understanding their density, moments, and integration properties.
  - Quick check question: What is the density of a Wishart distribution with N degrees of freedom and scale matrix V?

- Concept: Matrix normal distribution
  - Why needed here: The network output and intermediate layers can be expressed using matrix normal distributions, which is essential for deriving the Gaussian mixture representations.
  - Quick check question: How does the matrix normal distribution relate to the multivariate normal distribution via vectorization?

- Concept: Large deviation theory
  - Why needed here: The feature learning infinite-width limit is analyzed using large deviation principles to characterize the concentration of measure over Wishart ensembles.
  - Quick check question: What is the rate function for a sequence of measures satisfying a large deviation principle?

## Architecture Onboarding

- Component map: FC-DLN architecture -> Independent weight priors -> Wishart mixture representation -> Posterior computation -> Large deviation analysis for feature learning
- Critical path: Define network architecture and prior -> Derive output prior as Gaussian mixture -> Compute posterior under Gaussian likelihood -> Analyze large deviation behavior in feature learning limit
- Design tradeoffs: The choice between fully-connected and convolutional architectures affects the structure of Wishart matrices and resulting covariance. The mean-field parametrization enables feature learning but introduces potential pathologies in the Bayesian setting.
- Failure signatures: If weight independence assumption is violated, the mixture representation breaks down. If layer widths are not properly scaled in feature learning limit, large deviation analysis may not apply. If training data is ill-conditioned, posterior computation may become unstable.
- First 3 experiments:
  1. Verify the output prior representation by sampling from the network and comparing the empirical distribution to the theoretical mixture of Gaussians.
  2. Compute the posterior predictive distribution for a simple dataset and compare it to standard Gaussian process regression.
  3. Analyze the large deviation behavior in the feature learning limit for a small network with synthetic data to observe the concentration around data-dependent solutions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the non-Gaussian prior over outputs in finite-width Bayesian deep linear networks affect generalization performance compared to the infinite-width Gaussian process limit?
- Basis in paper: [explicit] The paper provides an exact non-asymptotic integral representation showing the prior is a mixture of Gaussians with Wishart matrices, demonstrating explicit departure from Gaussian behavior at finite width.
- Why unresolved: The paper characterizes the prior and posterior analytically but does not investigate how this non-Gaussianity impacts generalization bounds or empirical performance.
- What evidence would resolve it: Empirical studies comparing generalization error on benchmark datasets between finite-width networks with the derived non-Gaussian priors versus infinite-width GP approximations, or theoretical bounds quantifying the generalization gap.

### Open Question 2
- Question: What is the precise relationship between the kernel renormalization effects observed in the proportional limit and the feature learning behavior characterized in the mean-field infinite-width limit?
- Basis in paper: [explicit] The paper notes qualitative similarities between these two limits where Wishart measures concentrate around data-dependent solutions, but states they arise for different reasons.
- Why unresolved: The paper establishes formal results for both limits separately but does not provide a unified framework connecting them or explaining how one transitions to the other.
- What evidence would resolve it: A rigorous derivation showing how the saddle-point equations from the proportional limit emerge as a special case of the rate function in the mean-field limit, or numerical demonstrations of the interpolation between these regimes.

### Open Question 3
- Question: How can the Meijer G-function representation for D=1 fully-connected networks be extended to derive exact expressions for the partition function in multi-output networks?
- Basis in paper: [explicit] The paper shows how the D=1 case maps to Meijer G-functions and provides an alternative integral representation, but only discusses the scalar output case.
- Why unresolved: The paper establishes the connection between the Wishart integral representation and Meijer G-functions for single-output networks but does not extend this analysis to the multiple-output setting where the mixing distribution involves D×D Wishart matrices.
- What evidence would resolve it: Derivation of multi-variable generalizations of Meijer G-functions that can express the integral over D-dimensional Wishart matrices, or explicit computation of the partition function for small fixed D using the proposed representation.

## Limitations

- Results are restricted to linear networks, excluding rich nonlinear phenomena observed in deep learning
- Weight independence assumption is crucial for derivations but may not hold in practical implementations
- Large deviation analysis requires careful mean-field scaling that may not capture realistic training dynamics

## Confidence

- High confidence: The finite-width prior representation as a mixture of Gaussians with Wishart covariance is mathematically rigorous and directly follows from stated assumptions
- Medium confidence: The posterior distribution under Gaussian likelihood is well-established but practical computation may face numerical challenges for large networks
- Medium confidence: The feature learning infinite-width limit provides valuable theoretical insights but mean-field scaling assumptions may not fully capture realistic training dynamics

## Next Checks

1. **Numerical verification of Wishart mixture representation**: Implement the prior sampling procedure for a small FC-DLN and empirically verify that the output distribution matches the theoretical mixture of Gaussians with Wishart covariance matrices.

2. **Posterior predictive comparison**: Compute the posterior predictive distribution for a simple regression task and compare it against standard Gaussian process regression results to validate the closed-form mixing distribution.

3. **Large deviation concentration analysis**: For a C-DLN with synthetic data, numerically compute the rate function in the feature learning limit and verify the concentration around data-dependent solutions predicted by Proposition 10.