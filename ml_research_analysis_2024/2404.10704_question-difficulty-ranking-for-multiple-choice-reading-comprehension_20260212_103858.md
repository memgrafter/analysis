---
ver: rpa2
title: Question Difficulty Ranking for Multiple-Choice Reading Comprehension
arxiv_id: '2404.10704'
source_url: https://arxiv.org/abs/2404.10704
tags:
- difficulty
- question
- reading
- arxiv
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores task transfer and zero-shot approaches for ranking
  MC questions by difficulty on CMCQRD that has continuous difficulty scores. A RACE++
  trained level classification system transfers better to the task of difficulty ranking
  than a reading comprehension system.
---

# Question Difficulty Ranking for Multiple-Choice Reading Comprehension

## Quick Facts
- arXiv ID: 2404.10704
- Source URL: https://arxiv.org/abs/2404.10704
- Authors: Vatsal Raina; Mark Gales
- Reference count: 13
- Key outcome: Combining task transfer (level classification) and zero-shot comparative approaches achieves 43.7% Spearman correlation on CMCQRD dataset

## Executive Summary
This work addresses the challenge of ranking multiple-choice reading comprehension questions by difficulty when only continuous difficulty scores are available in limited datasets. The authors explore task transfer learning by training difficulty classification systems on RACE++ data and applying them to rank questions by continuous difficulty. They also investigate zero-shot approaches using instruction-tuned language models for both absolute and comparative difficulty assessment. The study finds that combining a RACE++ trained level classification system with a zero-shot comparative assessment approach yields the best performance, achieving 43.7% Spearman correlation with human difficulty judgments.

## Method Summary
The paper employs two main approaches: task transfer and zero-shot learning. For task transfer, they train two systems on RACE++ dataset - a level classification system using ELECTRA-base to predict difficulty tiers (easy/medium/hard) and a reading comprehension system using Llama-2 to predict correct answers. These are then adapted to estimate continuous difficulty scores on CMCQRD. For zero-shot approaches, they use instruction-tuned language models (ChatGPT3.5) to estimate question difficulty through absolute scoring and comparative pairwise ranking. The absolute approach prompts the LLM to assign a difficulty score, while the comparative approach asks it to compare two questions and determine which is more difficult. The final system combines the level classification and comparative approaches by averaging their rankings.

## Key Results
- RACE++ trained level classification system transfers better to difficulty ranking than reading comprehension system
- Zero-shot comparative assessment outperforms zero-shot absolute assessment for difficulty ranking
- Combining level classification and comparative systems achieves 43.7% Spearman correlation, outperforming individual approaches (38.7% for level classification, 40.4% for comparative)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Level classification systems trained on labeled difficulty tiers can transfer to continuous difficulty scoring.
- Mechanism: The classification model learns hierarchical representations of text complexity that correlate with human difficulty judgments. These learned features can be mapped to a continuous scale via weighted averaging of class probabilities.
- Core assumption: The categorical difficulty levels in training data (easy/medium/hard) preserve ordinal relationships that map meaningfully to continuous difficulty scores.
- Evidence anchors:
  - [abstract] "A RACE++ trained level classification system transfers better to the task of difficulty ranking than a reading comprehension system."
  - [section 3.1] "Both of these approaches rely on supervised data for training."
- Break condition: If the categorical levels in training data do not align with the continuous scale used in target data, or if the mapping function poorly preserves ordinal relationships.

### Mechanism 2
- Claim: Zero-shot comparative assessment with LLMs outperforms absolute difficulty scoring for ranking tasks.
- Mechanism: Pairwise comparison reduces the cognitive load on the model by focusing on relative judgments rather than absolute scoring. This sidesteps the model's potential miscalibration in absolute scoring while preserving ranking quality.
- Core assumption: LLMs can make reliable relative difficulty judgments when comparing two questions directly, even if their absolute difficulty estimates are biased.
- Evidence anchors:
  - [abstract] "zero-shot comparative assessment is more effective at difficulty ranking than the absolute assessment"
  - [section 3.2] "Both strategies only rely on black-box API access to instruction-tuned language models."
- Break condition: If the LLM has systematic biases in pairwise comparisons (e.g., always favoring first item) that outweigh the benefits of relative assessment.

### Mechanism 3
- Claim: Combining task transfer and zero-shot approaches yields better performance than either approach alone.
- Mechanism: The level classification system provides stable, learned features from training data, while the zero-shot comparative approach captures nuanced judgments from the LLM. Their combination leverages complementary strengths.
- Core assumption: The two systems make errors on different types of questions, so their combination through rank averaging reduces overall error.
- Evidence anchors:
  - [abstract] "Combining the systems is observed to further boost the correlation" with results showing 43.7% correlation vs 38.7% for level classification alone and 40.4% for comparative.
  - [section 5] "We observe a significant performance jump to 43.7, suggesting the two approaches have complementary behaviour"
- Break condition: If both systems share similar failure modes or if their combination introduces conflicting signals that degrade performance.

## Foundational Learning

- Concept: Task transfer learning
  - Why needed here: Limited data exists with continuous difficulty scores, so models must leverage related tasks with more abundant labeled data.
  - Quick check question: Can a model trained to classify difficulty levels be adapted to produce continuous difficulty scores?

- Concept: Zero-shot prompting with LLMs
  - Why needed here: No task-specific training data is required, enabling immediate application to difficulty ranking without expensive data collection.
  - Quick check question: Can an instruction-tuned LLM accurately rank question difficulty through pairwise comparison without any fine-tuning?

- Concept: Spearman's rank correlation
  - Why needed here: The task focuses on ranking questions by difficulty, not predicting exact difficulty values, making rank correlation the appropriate evaluation metric.
  - Quick check question: Does your evaluation metric measure the ordering of predictions rather than their absolute values?

## Architecture Onboarding

- Component map: Input preprocessor -> Level classifier (ELECTRA-base) -> Probability mapping -> Continuous score; Input preprocessor -> Zero-shot LLM (ChatGPT3.5) -> Difficulty score (absolute or comparative); Rank aggregator -> Spearman correlation calculation and rank averaging

- Critical path: Input (context, question, options) → Level classifier → Probability mapping → Continuous score (for task transfer); Input → Zero-shot LLM → Difficulty score (for zero-shot)

- Design tradeoffs: Task transfer requires training time and computational resources but produces stable results; zero-shot requires no training but depends on API access and may have variable performance.

- Failure signatures: Poor correlation with human difficulty scores; systematic bias in pairwise comparisons; performance degradation when combining systems.

- First 3 experiments:
  1. Train level classifier on RACE++ and evaluate Spearman correlation on CMCQRD
  2. Implement zero-shot comparative assessment with ChatGPT and measure correlation
  3. Combine level classification and comparative approaches and compare performance gain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the difficulty ranking performance of level classification systems vary across different pre-trained transformer models (e.g., BERT, RoBERTa, ELECTRA)?
- Basis in paper: [inferred] The paper uses ELECTRA-base for the level classification system but does not explore other pre-trained models.
- Why unresolved: The paper only reports results for ELECTRA-base and does not compare its performance with other popular transformer architectures.
- What evidence would resolve it: Training and evaluating level classification systems using different pre-trained transformer models on the RACE++ dataset and comparing their Spearman's correlation scores on the CMCQRD test set.

### Open Question 2
- Question: What is the impact of different prompting strategies on the performance of zero-shot comparative difficulty ranking using instruction-finetuned LLMs?
- Basis in paper: [explicit] The paper uses a specific prompt format for comparative assessment but does not explore alternative prompting strategies.
- Why unresolved: The paper only reports results for one prompting strategy and does not investigate how variations in the prompt might affect the performance.
- What evidence would resolve it: Experimenting with different prompt formats, lengths, and instructions for the comparative assessment task and measuring their impact on the Spearman's correlation score.

### Open Question 3
- Question: How does the performance of the combined level classification and comparative approaches vary with the weighting of each component's contribution?
- Basis in paper: [inferred] The paper combines the two approaches by averaging ranks but does not explore alternative weighting schemes.
- Why unresolved: The paper only reports results for an equal weighting of the two approaches and does not investigate how different weightings might affect the overall performance.
- What evidence would resolve it: Experimenting with different weighting schemes for the level classification and comparative approaches and measuring their impact on the Spearman's correlation score.

## Limitations
- Performance gap remains between best approach (43.7%) and perfect correlation, indicating room for improvement
- Reliance on proprietary APIs for zero-shot approaches limits reproducibility and raises cost concerns
- Evaluation focuses solely on Spearman's rank correlation, potentially missing other important aspects of difficulty prediction quality

## Confidence
- High Confidence: Task transfer using level classification outperforms reading comprehension systems for difficulty ranking; combining level classification and zero-shot comparative approaches yields better results than either alone.
- Medium Confidence: Zero-shot comparative assessment is more effective than absolute assessment for difficulty ranking; the mechanisms explaining why task transfer and zero-shot approaches work complementarily are plausible but not definitively proven.
- Low Confidence: The specific weighted averaging method used to map categorical probabilities to continuous scores is optimal; no systematic biases exist in the zero-shot comparative approach.

## Next Checks
1. **Cross-dataset validation**: Test the combined approach on an independent multiple-choice reading comprehension dataset with continuous difficulty scores to verify generalizability beyond CMCQRD.

2. **Prompt sensitivity analysis**: Systematically vary the zero-shot comparison prompts and measure how performance changes, identifying optimal prompt formulations and potential failure modes.

3. **Human baseline comparison**: Have human experts rank question difficulty and compare their Spearman correlation to the model predictions, establishing whether the computational approach reaches human-level performance.