---
ver: rpa2
title: 'QGFN: Controllable Greediness with Action Values'
arxiv_id: '2402.05234'
source_url: https://arxiv.org/abs/2402.05234
tags:
- qgfn
- reward
- training
- should
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces QGFN, a method that combines generative flow
  networks (GFlowNets) with action-value functions from reinforcement learning to
  improve the generation of high-reward and diverse samples. The core idea is to mix
  the forward policy of a GFlowNet with an action-value estimate, controlled by a
  mixing parameter, to create greedier sampling policies.
---

# QGFN: Controllable Greediness with Action Values

## Quick Facts
- arXiv ID: 2402.05234
- Source URL: https://arxiv.org/abs/2402.05234
- Reference count: 40
- Primary result: Combines GFlowNets with action-value functions to create controllable greedier sampling policies that improve both average reward and mode discovery in molecular design tasks

## Executive Summary
This work introduces QGFN, a method that combines generative flow networks (GFlowNets) with action-value functions from reinforcement learning to improve the generation of high-reward and diverse samples. The core idea is to mix the forward policy of a GFlowNet with an action-value estimate, controlled by a mixing parameter, to create greedier sampling policies. The method is evaluated on tasks like molecular design and RNA binding, showing improved performance in terms of higher average rewards and increased discovery of diverse high-reward objects compared to strong baselines. The approach allows for controllable greediness at inference time without sacrificing diversity.

## Method Summary
QGFN combines a GFlowNet forward policy PF with an action-value function Q(s,a) to create controllable greedier sampling policies. The method jointly trains both components using trajectory balance for the GFlowNet and n-step temporal difference learning for the Q-network. At inference, a mixing parameter p controls the interpolation between PF and Q, with variants including p-greedy (linear combination), p-quantile (pruning actions below a Q threshold), and p-of-max (masking non-maximal Q actions). The n-step returns in Q-learning provide better credit assignment than 1-step, and the pruning variants help avoid low-reward regions while maintaining diversity.

## Key Results
- QGFN variants achieve higher average rewards and discover more unique high-reward modes than baseline GFlowNets and RL methods on fragment-based molecule and RNA binding tasks
- Higher n-step values (close to max trajectory length) in Q-learning significantly improve performance by providing better credit assignment
- The p-quantile and p-of-max variants effectively prune low-reward actions while preserving high-reward diversity, with pruning particularly beneficial in large action spaces
- The mixing parameter p controls greediness at inference time without retraining, with lower p values maintaining diversity and higher p values maximizing reward

## Why This Works (Mechanism)

### Mechanism 1
QGFN improves sample diversity by guiding exploration with action-value estimates while retaining coverage from GFlowNets. The method combines PF (GFlowNet forward policy) with Q(s,a) estimates to create a behavior policy that prioritizes high-reward branches but still explores the full state space. Core assumption: Q(s,a) provides useful lower bounds on expected returns even when evaluated on off-policy actions.

### Mechanism 2
Pruning actions based on Q-values avoids low-reward regions while preserving high-reward diversity. The p-quantile and p-of-max variants mask actions below certain Q-value thresholds, effectively pruning unpromising branches while maintaining coverage of high-reward regions. Core assumption: Actions with low Q-values consistently lead to lower rewards across multiple trajectories.

### Mechanism 3
The mixing parameter p enables controllable greediness without retraining. By adjusting p, the behavior policy smoothly interpolates between PF-only (exploration) and Q-greedy (exploitation) modes. Core assumption: The trained Q generalizes to provide reasonable estimates for actions not taken during training.

## Foundational Learning

- Concept: Trajectory Balance and Flow Conservation in GFlowNets
  - Why needed here: Understanding how GFlowNets learn to sample proportionally to rewards is essential for grasping why combining with Q-values is beneficial
  - Quick check question: What happens to terminal state sampling probabilities when trajectory balance conditions are satisfied?

- Concept: n-step Temporal Difference Learning
  - Why needed here: The paper shows that using more than 1-step returns is crucial for learning useful Q-values in this context
  - Quick check question: How does increasing n from 1 to the maximum trajectory length affect the bias-variance tradeoff in Q-value estimates?

- Concept: Mode Discovery vs. Average Reward Trade-off
  - Why needed here: The paper explicitly measures both metrics, showing that QGFN can improve both simultaneously despite the typical exploration-exploitation tension
  - Quick check question: Why might a purely greedy policy discover fewer distinct modes than an exploration-biased policy, even with perfect Q-value estimates?

## Architecture Onboarding

- Component map:
  - GFlowNet forward policy (PF) - learns flow distributions
  - Q-network - learns action-value estimates using n-step returns
  - Mixing controller - combines PF and Q based on parameter p
  - Training loop - samples trajectories using current behavior policy and updates both models

- Critical path:
  1. Sample trajectory using current behavior policy (mixture of PF and Q)
  2. Compute trajectory balance loss for PF update
  3. Compute n-step TD loss for Q update
  4. Update both networks simultaneously

- Design tradeoffs:
  - Using n-step returns vs. 1-step: n-step provides better credit assignment but requires longer trajectories and more memory
  - Weight sharing between PF and Q: Could reduce parameters but may cause interference; paper tried without success
  - Pruning aggressiveness: p-quantile vs. p-of-max - aggressive pruning can help in large action spaces but may harm in small ones

- Failure signatures:
  - High average reward but low mode count: Q estimates may be overconfident, causing premature convergence to few modes
  - Low average reward but high mode count: Q estimates may be too conservative, providing insufficient guidance
  - Training instability: Likely from improper n-step return calculation or Q-value clipping issues

- First 3 experiments:
  1. Fragment-based molecule task with p-greedy variant, varying p and n to observe reward/diversity tradeoff
  2. RNA binding task comparing all three QGFN variants to baseline GFNs
  3. Bit sequence task with expanded action space to test pruning strategy robustness

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of QGFN scale with increasing complexity of the reward landscape, such as with more local optima or non-convex regions? The paper mentions that using high Î² values can lead to mode collapse and reduced diversity, suggesting sensitivity to reward landscape structure. Experiments focus on specific tasks with relatively well-defined reward landscapes; no systematic analysis of varying landscape complexity is provided.

### Open Question 2
What is the impact of using different n-step return values on QGFN's ability to generalize to out-of-distribution states or unseen regions of the state space? The paper shows that higher n-step values improve performance, but focuses on training dynamics rather than generalization. Experiments primarily measure in-distribution performance during training, not generalization to unseen states.

### Open Question 3
Can the QGFN framework be extended to continuous action spaces, and if so, what modifications would be necessary? The paper notes that the method could theoretically be extended to continuous cases but does not explore this. All experiments are conducted in discrete action spaces; no analysis of continuous extensions is provided.

## Limitations
- Empirical evaluation relies on synthetic and chemical domains where ground truth reward functions are known, limiting generalizability to real-world applications with noisy or incomplete reward signals
- The paper does not provide systematic analysis of how QGFN performance degrades when Q-value estimates are poor or when the mixing parameter p is poorly chosen
- While the method shows improved mode discovery and average reward simultaneously, the long-term exploration-exploitation tradeoff and potential for premature convergence are not thoroughly investigated

## Confidence

- **High Confidence**: The core algorithmic framework of combining GFlowNet policies with action-value estimates for controllable greediness, and the basic training procedure using trajectory balance and n-step TD learning
- **Medium Confidence**: The empirical claims about improved diversity and reward on the tested molecular and RNA binding tasks, given the controlled experimental conditions and reasonable baselines
- **Low Confidence**: The robustness claims across all three QGFN variants (p-greedy, p-quantile, p-of-max) and the generalizability to domains with significantly different reward landscapes or action space sizes

## Next Checks
1. Test QGFN on a real-world application with noisy reward signals (e.g., protein-ligand binding with experimental data) to assess robustness to imperfect Q-value estimates
2. Conduct an ablation study systematically varying the mixing parameter p during both training and inference to quantify the tradeoff between exploration and exploitation
3. Evaluate the long-term performance of QGFN by running extended training sessions and measuring mode coverage decay over time to assess potential premature convergence issues