---
ver: rpa2
title: 'OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering
  with open-source large language models'
arxiv_id: '2402.19371'
source_url: https://arxiv.org/abs/2402.19371
tags:
- prompting
- performance
- medical
- prompt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents OpenMedLM, a prompting platform that achieves
  state-of-the-art results for open-source large language models (LLMs) on medical
  benchmarks without fine-tuning. The authors evaluated various open-source foundation
  models (7B-70B parameters) on four medical benchmarks: MedQA, MedMCQA, PubMedQA,
  and MMLU medical subset.'
---

# OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models

## Quick Facts
- arXiv ID: 2402.19371
- Source URL: https://arxiv.org/abs/2402.19371
- Reference count: 0
- Key outcome: Prompt engineering achieves state-of-the-art results on medical benchmarks without fine-tuning open-source LLMs

## Executive Summary
This study demonstrates that advanced prompt engineering techniques can achieve superior performance on medical question-answering benchmarks compared to computationally expensive fine-tuning of open-source large language models. Using a comprehensive suite of prompting strategies including zero-shot, few-shot, chain-of-thought reasoning with kNN-selected examples, and ensemble voting, the researchers optimized the Yi 34B foundation model to achieve 72.6% accuracy on MedQA and 81.7% accuracy on the MMLU medical subset. These results surpass previous state-of-the-art open-source models that relied on fine-tuning, suggesting that prompt engineering alone can unlock latent medical knowledge in foundation models and provide a cost-effective alternative for developing medical AI applications.

## Method Summary
The researchers evaluated various open-source foundation models (7B-70B parameters) on four medical benchmarks using a comprehensive prompt engineering pipeline. They employed zero-shot, few-shot, and chain-of-thought prompting strategies, with few-shot examples selected using k-nearest neighbors for contextual relevance. For chain-of-thought prompting, they generated explanations using GPT-4 and implemented ensemble voting by running each prompt five times with shuffled answer options. The Yi 34B foundation model was optimized using these techniques and achieved state-of-the-art results on three common medical benchmarks, surpassing previous models that used computationally costly fine-tuning approaches.

## Key Results
- Yi 34B achieved 72.6% accuracy on MedQA and 81.7% accuracy on MMLU medical subset
- Prompt engineering outperformed previous open-source models that used fine-tuning
- Chain-of-thought prompting with kNN-selected examples showed significant improvement over random selection
- Ensemble voting with shuffled options reduced randomness and increased answer confidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Robust prompt engineering can elicit medical-specific emergent properties in open-source foundation models without fine-tuning
- Mechanism: By combining multiple prompt engineering techniques (zero-shot, few-shot, chain-of-thought with kNN-selected examples, and ensemble/self-consistency voting), the model's inherent capabilities are unlocked for medical question-answering tasks
- Core assumption: Open-source foundation models contain latent medical knowledge that can be accessed through appropriate prompting strategies
- Evidence anchors:
  - [abstract]: "Our results highlight medical-specific emergent properties in OSLLMs which have not yet been documented to date elsewhere"
  - [section]: "We found that OpenMedLM delivers OS SOTA results on three common medical LLM benchmarks, surpassing the previous best performing OS models that leveraged computationally costly extensive fine-tuning"
  - [corpus]: Weak evidence - no direct corpus support for emergent properties in OS models specifically
- Break condition: If the foundation model lacks sufficient pre-training on medical domain data, prompt engineering alone cannot create medical knowledge

### Mechanism 2
- Claim: Chain-of-thought prompting with k-nearest neighbor example selection improves accuracy more than random example selection
- Mechanism: kNN selects contextually relevant training examples that guide the model's reasoning process, making the chain-of-thought more applicable to the test question
- Core assumption: The similarity between training and test questions correlates with the effectiveness of the chain-of-thought reasoning
- Evidence anchors:
  - [abstract]: "We employed a series of prompting strategies, including zero-shot, few-shot, chain-of-thought (random selection and kNN selection)"
  - [section]: "For the kNNstrategy described above in the context of the CoTprompting, we also employ the ensemble or self-consistency strategy where we run the prompt multiple times through the model and implement a majority voting strategy to get the answer"
  - [corpus]: Weak evidence - corpus neighbors discuss CoT prompting but don't specifically address kNN selection effectiveness
- Break condition: If the training dataset lacks diversity or the test questions are too dissimilar from training examples, kNN selection provides minimal benefit

### Mechanism 3
- Claim: Ensemble/self-consistency voting with shuffled options reduces randomness and increases answer confidence
- Mechanism: Running the same prompt multiple times with randomly shuffled answer options and selecting the majority answer reduces the impact of stochastic generation and increases reliability
- Core assumption: The model's answers to the same question (with options shuffled) should be consistent when given the same reasoning context
- Evidence anchors:
  - [abstract]: "For everyrun, weshuffletheoptionsofthetest question"
  - [section]: "Thisapproach ensures the model selects the same answer in multiple runs and increases confidence in the model's output"
  - [corpus]: Weak evidence - corpus neighbors discuss ensemble methods but not specifically with shuffled options for medical QA
- Break condition: If the model is highly sensitive to answer option order or the question has ambiguous correct answers, ensemble voting may not improve accuracy

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Enables the model to show intermediate reasoning steps before arriving at an answer, mimicking how humans solve complex problems
  - Quick check question: What is the difference between standard prompting and chain-of-thought prompting in how the model processes information?

- Concept: k-Nearest Neighbors (kNN) algorithm
  - Why needed here: Used to select the most contextually similar training examples for each test question to improve the relevance of few-shot examples
  - Quick check question: How does kNN selection of few-shot examples differ from random selection in terms of potential impact on model performance?

- Concept: Self-consistency/ensemble methods
  - Why needed here: Reduces the impact of random generation variations by having the model answer the same question multiple times and selecting the majority answer
  - Quick check question: Why might running the same prompt multiple times with shuffled answer options lead to more reliable results than a single run?

## Architecture Onboarding

- Component map:
  Foundation model (Yi 34B) -> Prompt template generator -> kNN example selector -> GPT-4 CoT explanation generator -> Ensemble voter -> Evaluation pipeline

- Critical path:
  1. Load foundation model
  2. For each test question, generate kNN-based few-shot examples
  3. Generate CoT explanations using GPT-4
  4. Create prompt with instruction, examples, and test question
  5. Run prompt through model 5 times with shuffled options
  6. Collect answers and apply majority voting
  7. Record accuracy for the benchmark

- Design tradeoffs:
  - Computational cost vs. accuracy: More inference runs improve reliability but increase computation time
  - Prompt complexity vs. performance: More sophisticated prompts may improve accuracy but risk confusing the model
  - GPT-4 dependency vs. automation: Using GPT-4 for CoT generation adds an external dependency but ensures high-quality explanations

- Failure signatures:
  - Low accuracy across all prompting strategies suggests the foundation model lacks sufficient medical knowledge
  - High variance between runs indicates the model is highly sensitive to input variations
  - kNN selection providing minimal improvement suggests poor similarity between training and test questions
  - CoT explanations not improving performance may indicate the model struggles with reasoning chains

- First 3 experiments:
  1. Compare zero-shot performance on all four benchmarks to establish baseline
  2. Test random few-shot vs. kNN few-shot prompting on a single benchmark to validate kNN benefit
  3. Compare ensemble voting (5 runs) vs. single run on a benchmark to measure self-consistency impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can combining fine-tuning with advanced prompting techniques achieve better performance than either approach alone on medical benchmarks?
- Basis in paper: [inferred] The authors suggest this as a direction for future studies, noting that "further studies should be performed to analyze the combination of fine-tuning in combination with advanced prompting techniques."
- Why unresolved: This study only evaluated prompt engineering without fine-tuning. The potential synergistic effects of combining both approaches remain unexplored.
- What evidence would resolve it: Comparative experiments testing medical LLMs with different combinations of fine-tuning and prompt engineering techniques on the same medical benchmarks.

### Open Question 2
- Question: How well can open-source LLMs perform on open-ended medical questions compared to multiple-choice formats?
- Basis in paper: [explicit] The authors acknowledge that "all of the questions evaluated by each benchmark are multiple choice questions" and note that "most scenarios would still require open-ended responses."
- Why unresolved: The study only evaluated multiple-choice question answering, which represents a limited subset of real-world medical Q&A scenarios.
- What evidence would resolve it: Testing open-source LLMs on medical benchmarks with open-ended questions requiring free-form responses, including clinical reasoning and evidence-based explanations.

### Open Question 3
- Question: What is the full extent of emergent medical-specific properties in open-source LLMs of varying sizes?
- Basis in paper: [explicit] The authors highlight "medical-specific emergent properties" and note these "have not yet been documented to date elsewhere."
- Why unresolved: This study only tested one OS foundation model (Yi 34B). The presence and extent of emergent medical properties across different model sizes and architectures remains unknown.
- What evidence would resolve it: Systematic evaluation of medical-specific emergent properties across a range of OS foundation models (7B-70B) using diverse medical tasks beyond Q&A.

## Limitations

- Results heavily dependent on Yi 34B foundation model; unclear if same prompting strategies work equally well on other open-source models
- Computational cost of multiple inference passes (5x with ensemble voting) may offset advantages of avoiding fine-tuning
- Study does not address potential biases in medical benchmarks or evaluate performance on real-world clinical scenarios beyond standardized test questions

## Confidence

- Prompt engineering eliciting emergent medical properties: Medium confidence
- Chain-of-thought with kNN examples improving accuracy: Medium confidence
- Ensemble voting reducing randomness: High confidence

## Next Checks

1. Test the same prompt engineering pipeline across multiple open-source models (Llama, Mistral, Qwen) to determine generalizability
2. Evaluate models on actual clinical decision support scenarios with incomplete information to assess practical medical utility
3. Compare total computational cost of prompt engineering approach against fine-tuning baselines across different deployment scales