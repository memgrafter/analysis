---
ver: rpa2
title: 'Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data
  in Misaligned Languages Suffice?'
arxiv_id: '2404.14122'
source_url: https://arxiv.org/abs/2404.14122
tags:
- translation
- training
- data
- directions
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to efficiently fine-tune large language
  models (LLMs) for machine translation. The authors find that LLMs can be aligned
  to translate effectively with as few as 32 parallel sentences, challenging the assumption
  that large-scale data is necessary.
---

# Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?

## Quick Facts
- **arXiv ID**: 2404.14122
- **Source URL**: https://arxiv.org/abs/2404.14122
- **Reference count**: 40
- **Primary result**: LLMs can be effectively aligned for translation with as few as 32 parallel sentences, challenging assumptions about data requirements.

## Executive Summary
This paper investigates the efficiency of fine-tuning large language models (LLMs) for machine translation tasks. The authors demonstrate that LLMs can achieve competitive translation quality with minimal parallel data—as few as 32 sentences—and that fine-tuning on a single translation direction can enable multilingual translation across multiple language pairs. They also explore how the placement of noisy synthetic data affects translation performance, finding that target-side noise has a greater impact than source-side noise, except for under-represented languages. These findings refine the "superficial alignment hypothesis" by showing that while minimal data can suffice, careful attention to training direction and data quality is essential to avoid unintended biases and performance degradation.

## Method Summary
The authors fine-tune Llama-2 7B on parallel data from WMT17-WMT20 test sets across 11 translation directions, using supervised learning with instruction templates. They experiment with training set sizes ranging from 16 to 74,623 examples, applying random instruction prompts to source sentences during training. Models are evaluated using COMET22 and BLEU metrics on WMT22 test sets, with beam search (beam size 4, max length 256) for inference. Synthetic data experiments involve injecting noise via machine translation to test sensitivity to data quality.

## Key Results
- LLMs achieve translation quality comparable to or exceeding established baselines after fine-tuning on as few as 32 parallel sentences.
- Fine-tuning on a single translation direction enables translation across multiple language pairs, though English placement in the target side can cause task misinterpretation.
- Noisy synthetic data on the target side degrades performance more than on the source side, except for under-represented languages where LLMs show greater robustness.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs can perform multilingual translation after fine-tuning on as few as 32 parallel sentences.
- **Mechanism**: LLMs already possess multilingual understanding and generation capabilities from pre-training, so minimal fine-tuning is sufficient to align them with the translation task format.
- **Core assumption**: Pre-trained LLMs have absorbed bilingual signals and translation patterns during their extensive training, enabling them to generalize from very small task-specific datasets.
- **Evidence anchors**:
  - [abstract]: "LLMs display strong translation capability after being fine-tuned on as few as 32 parallel sentences"
  - [section 3.2]: "With further increases in the training size to 32 samples, Llama-2 performs on par with or surpasses all three IT-LLM baselines"
  - [corpus]: FMR evidence from related works supports that small data fine-tuning can unlock translation capabilities
- **Break condition**: If the pre-training corpus lacks sufficient bilingual data or the languages are too distant, the model may fail to translate effectively with minimal data.

### Mechanism 2
- **Claim**: Fine-tuning on a single translation direction enables translation in multiple directions.
- **Mechanism**: The LLM generalizes translation capability across languages once it recognizes the translation task format, even if trained on one direction.
- **Core assumption**: The model's pre-trained multilingual knowledge allows it to transfer translation skills across language pairs when the task format is aligned.
- **Evidence anchors**:
  - [abstract]: "fine-tuning on a single translation direction enables translation in multiple directions"
  - [section 3.3]: "training with just one direction enables Llama-2 to translate between multiple languages"
  - [corpus]: FMR evidence from related works shows cross-lingual transfer in instruction tuning
- **Break condition**: If the training direction places English on the target side, the model may misinterpret the task and fail to translate into non-English languages.

### Mechanism 3
- **Claim**: Noisy synthetic data degrades performance more when placed on the target side, except for under-represented languages.
- **Mechanism**: LLMs are more sensitive to imperfections in the output (target) side because they try to imitate the target language patterns, but are more robust to noise in less familiar languages.
- **Core assumption**: The model's familiarity with a language influences its sensitivity to data quality issues during fine-tuning.
- **Evidence anchors**:
  - [abstract]: "noisy synthetic data on the target side degrades performance more than on the source side, except in under-represented languages where LLMs are more robust"
  - [section 3.5]: "when noise is introduced to the target side, models fine-tuned on en→de′ and en→ha′ translations exhibit a sharp decline in performance"
  - [corpus]: FMR evidence from related works on the impact of synthetic data quality
- **Break condition**: If the synthetic data is too noisy or the language is well-represented in pre-training, the model may overfit to the noise and degrade translation quality.

## Foundational Learning

- **Concept**: Pre-training and fine-tuning in LLMs
  - **Why needed here**: Understanding how pre-training provides multilingual capabilities and how fine-tuning aligns the model to specific tasks is crucial for grasping why minimal data can suffice.
  - **Quick check question**: What is the difference between pre-training and fine-tuning, and how do they contribute to an LLM's ability to translate?
- **Concept**: Cross-lingual transfer
  - **Why needed here**: Recognizing that skills learned in one language can be applied to others helps explain why fine-tuning on a single direction can enable multilingual translation.
  - **Quick check question**: How does cross-lingual transfer work in the context of fine-tuning LLMs for translation?
- **Concept**: Data quality and synthetic data
  - **Why needed here**: Understanding the impact of data quality, especially when using synthetic data, is essential for knowing how to effectively fine-tune LLMs without degrading performance.
  - **Quick check question**: Why is noisy synthetic data more harmful on the target side, and how can under-represented languages be less affected?

## Architecture Onboarding

- **Component map**:
  Pre-trained LLM (e.g., Llama-2 7B) -> Supervised fine-tuning pipeline -> Parallel training data (source and target sentences) -> Evaluation metrics (COMET, BLEU)
- **Critical path**:
  1. Prepare parallel training data
  2. Fine-tune the LLM using supervised learning
  3. Evaluate translation quality on test sets
- **Design tradeoffs**:
  - Using minimal data vs. comprehensive datasets
  - Single direction vs. multiple translation directions
  - Clean data vs. synthetic data with potential noise
- **Failure signatures**:
  - Poor translation quality indicating insufficient pre-training coverage
  - Task misinterpretation when English is on the target side
  - Overfitting to noise when synthetic data is too imperfect
- **First 3 experiments**:
  1. Fine-tune on 32 parallel sentences and evaluate translation quality
  2. Fine-tune on a single translation direction and test on multiple directions
  3. Use synthetic data on source and target sides to observe performance differences

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the precise mechanism by which LLMs achieve translation with minimal fine-tuning data?
- **Basis in paper**: [inferred] The paper demonstrates that LLMs can translate effectively after fine-tuning on as few as 32 parallel sentences, but does not explain the underlying mechanism.
- **Why unresolved**: The paper focuses on empirical findings rather than theoretical explanations of how LLMs leverage pre-trained knowledge for translation.
- **What evidence would resolve it**: Detailed analysis of attention patterns, activation maps, or probe studies during fine-tuning could reveal how LLMs utilize pre-existing multilingual knowledge.

### Open Question 2
- **Question**: How does the effectiveness of fine-tuning on minimal data generalize to other languages beyond those tested?
- **Basis in paper**: [explicit] The paper tests on 11 language pairs and extends to two unseen languages (Icelandic and Hausa), but acknowledges limitations in model diversity and language coverage.
- **Why unresolved**: The experiments are limited to specific language pairs, and the paper notes that findings may not generalize to all languages or models.
- **What evidence would resolve it**: Systematic fine-tuning experiments across a broader range of languages, including low-resource and non-English-centric pairs, would clarify generalizability.

### Open Question 3
- **Question**: What are the long-term effects of fine-tuning on noisy synthetic data for translation tasks?
- **Basis in paper**: [explicit] The paper shows that noisy synthetic data on the target side degrades performance more than on the source side, except in under-represented languages.
- **Why unresolved**: The study focuses on immediate performance impacts, not on how models adapt or degrade over time with continued exposure to noisy data.
- **What evidence would resolve it**: Longitudinal studies tracking model performance and behavior over multiple fine-tuning cycles with noisy data would provide insights into long-term effects.

## Limitations

- The findings are constrained by the specific languages and model architecture tested, primarily focusing on English-centric pairs.
- The "superficial alignment hypothesis" refinement may not capture all real-world complexities, such as domain shifts or stylistic variations.
- Synthetic data experiments use a specific type of noise injection, which may not generalize to all forms of data quality degradation.

## Confidence

**High Confidence:**
- The empirical finding that Llama-2 can achieve competitive translation quality with as few as 32 parallel sentences
- The observation that fine-tuning on one direction enables translation across multiple directions
- The evidence that target-side noise has greater impact than source-side noise, with exceptions for under-represented languages

**Medium Confidence:**
- The explanation that pre-training provides sufficient bilingual signals for this generalization
- The claim that English placement in training direction causes task misinterpretation
- The robustness of LLMs to noise in under-represented languages

**Low Confidence:**
- The precise boundaries of the "superficial alignment hypothesis" refinement
- Generalizability to other LLM architectures beyond Llama-2
- Performance with truly resource-poor languages not represented in WMT datasets

## Next Checks

1. **Cross-Architecture Validation**: Replicate the minimal-data findings with different LLM architectures (e.g., OPT, BLOOM, or other transformer-based models) to assess whether the observed capabilities are specific to Llama-2 or more general across LLMs.

2. **True Low-Resource Language Testing**: Design experiments with genuinely low-resource language pairs (e.g., languages with minimal parallel data in pre-training corpora) to validate whether the 32-sentence threshold holds and to better understand the limits of pre-trained multilingual knowledge.

3. **Alternative Noise Patterns**: Test the noise sensitivity findings using different types of synthetic data corruption (e.g., grammatical errors, semantic distortions, or domain-specific jargon) to determine if the word-level vs sentence-level distinction is the critical factor or if other noise characteristics affect target-side sensitivity differently.