---
ver: rpa2
title: Can Kernel Methods Explain How the Data Affects Neural Collapse?
arxiv_id: '2406.02105'
source_url: https://arxiv.org/abs/2406.02105
tags:
- kernel
- neural
- data
- relu
- nngp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of unconstrained features models
  (UFMs) in analyzing Neural Collapse (NC), which masks the effect of data on the
  extent of collapse. The authors propose a kernel-based analysis that depends on
  the data structure.
---

# Can Kernel Methods Explain How the Data Affects Neural Collapse?

## Quick Facts
- arXiv ID: 2406.02105
- Source URL: https://arxiv.org/abs/2406.02105
- Authors: Vignesh Kothapalli; Tom Tirer
- Reference count: 40
- This paper addresses the limitation of unconstrained features models (UFMs) in analyzing Neural Collapse (NC), proposing a kernel-based analysis that depends on the data structure.

## Executive Summary
This paper addresses the limitation of unconstrained features models (UFMs) in analyzing Neural Collapse (NC), which masks the effect of data on the extent of collapse. The authors propose a kernel-based analysis that depends on the data structure. They derive an NC1 metric based on traces of within- and between-class covariance matrices of the samples' features using an arbitrary kernel function. The metric is then specialized to kernels associated with shallow neural networks, specifically the NN Gaussian Process kernel (NNGP) and the Neural Tangent Kernel (NTK). Surprisingly, the authors show that the NTK does not represent more collapsed features than the NNGP for prototypical data models, highlighting the limitations of data-independent kernels in approximating NC behavior. As an alternative, they empirically explore a recently proposed data-aware Gaussian Process kernel, which generalizes NNGP to model feature learning. This kernel yields lower NC1 than NNGP but may not follow the trends of the shallow NN. The study demonstrates that adaptivity to data may allow kernel-based analysis of NC, though further advancements are needed. Additionally, the authors show both theoretically and empirically that the choice of nonlinear activation function affects NC1, with Erf yielding lower values than ReLU.

## Method Summary
The authors propose a kernel-based approach to analyze Neural Collapse (NC) by deriving a metric (NC1) based on the traces of within-class and between-class covariance matrices using an arbitrary kernel function. They specialize this metric to kernels associated with shallow neural networks, specifically the NN Gaussian Process kernel (NNGP) and the Neural Tangent Kernel (NTK). The paper also explores a data-aware Gaussian Process kernel that generalizes NNGP to model feature learning. The method involves computing kernel Gram matrices for different kernel types, calculating NC1 values using trace-based formulations, and comparing results across kernels and activation functions.

## Key Results
- The NTK does not represent more collapsed features than the NNGP for prototypical data models, highlighting limitations of data-independent kernels.
- The choice of nonlinear activation function affects NC1, with Erf yielding lower values than ReLU.
- Data-aware adaptive kernels can capture feature learning beyond NNGP and NTK, potentially yielding lower NC1 values.
- The kernel-based NC1 metric provides a data-dependent analysis of Neural Collapse, addressing limitations of unconstrained features models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The NTK does not represent more collapsed features than the NNGP for prototypical data models
- Mechanism: NTK is derived from gradients of network outputs, while NNGP is a fixed kernel at initialization. For Gaussian data with symmetric class means, both kernels yield the same expected NC1 metric because the gradient structure in the lazy regime does not induce additional collapse beyond what the initial kernel already encodes.
- Core assumption: Data follows a mixture of Gaussians with sufficiently separated class means (|µc| ≫ σc), and the network operates in the lazy regime (infinite width, small learning rate).
- Evidence anchors:
  - [abstract]: "we show that the NTK does not represent more collapsed features than the NNGP for prototypical data models"
  - [section 5.3]: "we show similar values of the expected NC1 metric for NNGP and NTK even for the Erf activation, which are smaller than the ReLU case"
  - [corpus]: Weak (no direct kernel comparison studies found in neighbors)
- Break condition: If data distribution deviates significantly from Gaussian mixture assumptions, or if the network operates outside the lazy regime, NTK may exhibit different NC1 behavior.

### Mechanism 2
- Claim: Activation function choice affects NC1, with Erf yielding lower values than ReLU
- Mechanism: Erf activation introduces smoother gradients and different higher-order interactions in the kernel compared to ReLU. These interactions lead to a more pronounced collapse of within-class variability in the feature space, as captured by the NC1 metric.
- Core assumption: The kernel-based NC1 metric accurately reflects the variability collapse in the actual network features.
- Evidence anchors:
  - [abstract]: "we show both theoretically and empirically that the choice of nonlinear activation function affects NC1, with ERF yielding lower values than ReLU"
  - [section 5.3]: "the Erf-based kernels reflect a larger extent of 'variability collapse' (NC1) of the hidden layer post-activations"
  - [corpus]: Weak (no direct activation function comparison studies found in neighbors)
- Break condition: If the network width is finite or if the data distribution is highly non-Gaussian, the activation-induced differences in NC1 may be less pronounced or reversed.

### Mechanism 3
- Claim: Data-aware adaptive kernels can capture feature learning beyond NNGP and NTK
- Mechanism: Adaptive kernels generalize NNGP by incorporating data-dependent corrections through Equations of State (EoS). This allows the kernel to evolve with the data structure, potentially yielding lower NC1 values than data-independent kernels like NNGP and NTK.
- Core assumption: The EoS accurately models the finite-width corrections to the kernel behavior during feature learning.
- Evidence anchors:
  - [abstract]: "we then empirically explore a recently proposed data-aware Gaussian Process kernel, which generalizes NNGP to model feature learning"
  - [section 6.1]: "EoS provides a mechanism for transitioning from NNGP kernels to finite-width-based kernels that adapt to the data"
  - [corpus]: Weak (no direct adaptive kernel studies found in neighbors)
- Break condition: If the EoS assumptions about weight covariance and layer interactions break down, or if the annealing schedule is not well-tuned, the adaptive kernel may not accurately capture feature learning.

## Foundational Learning

- Concept: Neural Collapse (NC) phenomenon
  - Why needed here: NC is the core phenomenon being studied, specifically the within-class variability collapse (NC1).
  - Quick check question: What are the key components of Neural Collapse, and how is NC1 defined?

- Concept: Neural Tangent Kernel (NTK) and Neural Network Gaussian Process (NNGP) kernels
  - Why needed here: These kernels are the main comparison points for understanding how data affects NC1.
  - Quick check question: How do NTK and NNGP kernels differ in their derivation and what regime do they model?

- Concept: Kernel-based analysis of neural networks
  - Why needed here: The paper uses kernel methods to analyze NC1 without relying on unconstrained features models.
  - Quick check question: How does the NC1 metric depend on the kernel function, and what is the relationship between kernel Gram matrices and feature covariance?

## Architecture Onboarding

- Component map:
  Data generation and preprocessing -> Kernel computation (NNGP, NTK, adaptive) -> NC1 metric calculation -> Comparison and analysis scripts

- Critical path:
  1. Generate Gaussian mixture data with specified class means and variances
  2. Compute kernel Gram matrices for NNGP, NTK, and adaptive kernels
  3. Calculate NC1 metric using the trace-based formulation
  4. Compare NC1 values across kernels and activation functions

- Design tradeoffs:
  - Using limiting kernels (NNGP, NTK) simplifies analysis but may not capture finite-width effects
  - Adaptive kernels can model feature learning but require solving complex EoS equations
  - Erf activation may yield lower NC1 but can be more computationally intensive than ReLU

- Failure signatures:
  - NC1 values not converging or showing high variance across runs
  - EoS solver failing to converge or producing unrealistic kernel values
  - Unexpected trends in NC1 with respect to data dimension or class imbalance

- First 3 experiments:
  1. Reproduce Figure 2: Compare NC1 values for NNGP and NTK with Erf and ReLU activations on balanced Gaussian data
  2. Implement the adaptive kernel (EoS) solver and verify it reproduces NNGP behavior at initialization
  3. Test the effect of class imbalance on NC1 values for different kernels and activation functions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the NTK always represent more collapsed features than the NNGP for any data distribution, or are there specific data properties where this relationship breaks down?
- Basis in paper: [explicit] The authors state that "the NTK does not represent more collapsed features than the NNGP for prototypical data models" but acknowledge this may not hold for all data distributions.
- Why unresolved: The paper only analyzes Gaussian data distributions and does not explore other data types like non-Gaussian or structured data.
- What evidence would resolve it: Testing NTK vs NNGP NC1 metrics on diverse data distributions (e.g., non-Gaussian, imbalanced, structured) would determine if the relationship is universal or data-dependent.

### Open Question 2
- Question: Can the adaptive kernel (EoS) approach be extended to deeper networks beyond 2-layer FCNs while maintaining its ability to model feature learning?
- Basis in paper: [explicit] The authors note that "analyzing the EoS for multi-layer FCN and convolutional networks can provide further insights into the depthwise reduction of NC1 for deeper networks" but do not implement this.
- Why unresolved: The current EoS formulation is specialized for 2-layer networks, and extending it to deeper architectures would require significant theoretical and computational work.
- What evidence would resolve it: Deriving and solving the EoS for deeper networks, then comparing their NC1 metrics to trained deep networks, would validate the approach's scalability.

### Open Question 3
- Question: How does the choice of activation function (e.g., ReLU vs. Erf) interact with data properties to influence the extent of NC1?
- Basis in paper: [explicit] The authors show that "the choice of nonlinear activation function affects NC1, with Erf yielding lower values than ReLU" but do not explore the interaction with data structure.
- Why unresolved: The analysis focuses on activation function effects in isolation, without examining how these effects vary across different data distributions or complexities.
- What evidence would resolve it: Systematic experiments varying activation functions across diverse data types (e.g., low vs. high dimensional, balanced vs. imbalanced) would reveal interaction effects.

### Open Question 4
- Question: Is there a fundamental limit to how much NC1 can be reduced by feature learning, or can it approach zero for certain data configurations?
- Basis in paper: [inferred] The authors observe that NC1 values depend on data properties like class separation and dimensionality, suggesting a potential upper bound on collapse.
- Why unresolved: The paper does not theoretically derive or empirically establish a maximum possible reduction in NC1.
- What evidence would resolve it: Proving theoretical bounds on NC1 reduction or identifying data configurations that minimize NC1 would clarify the limits of feature learning.

## Limitations

- The kernel-based NC1 metric assumes the feature space structure is adequately captured by the kernel, which may not hold for all data distributions or network architectures.
- The analysis is limited to the lazy training regime, and it's unclear how these results extend to feature learning regimes or finite-width networks.
- The adaptive kernel approach using EoS shows promise but requires solving complex nonlinear equations whose convergence and stability are not fully characterized.

## Confidence

- NTK vs NNGP collapse comparison: Medium (conditional on data assumptions)
- Activation function effects: Medium (empirical validation limited)
- Adaptive kernel efficacy: Low-Medium (theoretical foundation established but empirical validation incomplete)

## Next Checks

1. Test NTK/NC1 behavior on non-Gaussian data distributions (e.g., mixture of asymmetric Gaussians, heavy-tailed distributions)
2. Implement finite-width corrections to NTK and verify if the collapse gap with NNGP emerges
3. Conduct systematic study of activation functions beyond Erf and ReLU, including Swish and GELU, to map the full landscape of NC1 values