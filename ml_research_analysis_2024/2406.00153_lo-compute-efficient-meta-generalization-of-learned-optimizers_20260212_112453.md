---
ver: rpa2
title: "$\u03BC$LO: Compute-Efficient Meta-Generalization of Learned Optimizers"
arxiv_id: '2406.00153'
source_url: https://arxiv.org/abs/2406.00153
tags:
- training
- meta-training
- tasks
- learned
- width
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses poor meta-generalization of learned optimizers\
  \ (LOs) to wider, deeper, or longer training tasks than seen during meta-training.\
  \ The authors theoretically derive the maximal update parametrization (\xB5P) for\
  \ two state-of-the-art LO architectures\u2014smallfclopt and VeLO\u2014and propose\
  \ a simple meta-training recipe for \xB5-parameterized LOs (\xB5LOs)."
---

# $μ$LO: Compute-Efficient Meta-Generalization of Learned Optimizers

## Quick Facts
- arXiv ID: 2406.00153
- Source URL: https://arxiv.org/abs/2406.00153
- Reference count: 40
- The paper proposes maximal update parametrization (μP) for learned optimizers to improve meta-generalization to wider, deeper, and longer training tasks.

## Executive Summary
This paper addresses the poor meta-generalization of learned optimizers (LOs) to tasks that are wider, deeper, or longer than those seen during meta-training. The authors theoretically derive the maximal update parametrization (μP) for two state-of-the-art LO architectures—small_fc_lopt and VeLO—and propose a simple meta-training recipe for μ-parameterized LOs (μLOs). Empirically, μLOs significantly outperform standard parameterization (SP) LOs and strong hand-designed baselines on wide out-of-distribution tasks, with smooth loss curves up to 8192-width MLPs. Remarkably, μLOs also generalize better to deeper networks (5× meta-training depth) and much longer training horizons (25× meta-training unroll length), achieving these improvements at zero extra computational cost compared to SP LOs.

## Method Summary
The method involves deriving μP for learned optimizer architectures and meta-training them using a multi-width approach. The μP parameterization rescales both optimizer updates and model pre-activations based on width-dependent factors, preventing activation explosion seen in SP. The meta-training uses evolution strategies with AdamW optimizer, linear warmup, and cosine annealing schedule. The approach is validated on MLP tasks with varying widths (128-8192), depths (3-16), and training steps (1000-25000), comparing against SP LOs, μAdam, and AdamW baselines.

## Key Results
- μLOs significantly outperform SP LOs and strong hand-designed baselines on wide out-of-distribution tasks, with smooth loss curves up to 8192-width MLPs
- μLOs generalize better to deeper networks (5× meta-training depth) and much longer training horizons (25× meta-training unroll length)
- These improvements come at zero extra computational cost compared to SP LOs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: μLOs maintain stable pre-activation scales across varying widths, enabling smooth optimization.
- Mechanism: The maximal update parametrization (μP) rescales both optimizer updates and model pre-activations based on width-dependent factors, preventing the activation explosion seen in standard parameterization.
- Core assumption: Weight matrices scale with width such that FAN_IN grows proportionally; Law of Large Numbers (LLN) scaling applies during training.
- Evidence anchors:
  - [abstract] "μLOs significantly outperform SP LOs and strong hand-designed baselines on wide out-of-distribution tasks"
  - [section] "all models parameterized in μP enjoy stable coordinates across widths, while the pre-activations of larger-width models in SP blow up"
  - [corpus] Weak evidence for learned optimizer generalization; most related works focus on scaling compute or training data.
- Break condition: If LLN scaling breaks down (e.g., highly sparse gradients, pathological data distributions) or width scaling is non-uniform.

### Mechanism 2
- Claim: μLOs enable maximal parameter updates without divergence, allowing effective training beyond meta-training unroll lengths.
- Mechanism: By normalizing feature inputs and scaling optimizer updates as Θ(1/n), the update magnitude remains appropriate for infinite-width limit, ensuring each parameter updates maximally without destabilizing training.
- Core assumption: Optimizer inputs (momentum, gradient, etc.) scale to Θ(1) after RMS normalization; meta-training captures sufficient diversity in task distribution.
- Evidence anchors:
  - [abstract] "surprising generalization to much longer training horizons (25× meta-training)"
  - [section] "our μLOs are capable of smoothly decreasing the loss for the largest out-of-distribution tasks... despite training for 5× longer than the maximum meta-training unroll length"
  - [corpus] Limited direct evidence; corpus neighbors focus on learned optimizer scaling but not μP theory.
- Break condition: If input feature scaling fails (e.g., improper normalization) or meta-training task diversity is insufficient.

### Mechanism 3
- Claim: Meta-training with multiple widths improves μLO generalization to unseen widths and longer horizons.
- Mechanism: Multi-width meta-training exposes the optimizer to a range of scale behaviors, allowing it to learn adaptive update rules that generalize across width regimes.
- Core assumption: The optimizer architecture can effectively interpolate between widths seen during meta-training.
- Evidence anchors:
  - [abstract] "meta-trained with our recipe substantially improve meta-generalization to wider unseen tasks"
  - [section] "meta-training with multiple tasks of different widths has benefits for generalization to longer unrolls in addition to improved generalization to larger optimizees"
  - [corpus] Related works (Celo, PyLO) emphasize multi-task meta-training but without μP focus.
- Break condition: If meta-training width range is too narrow or task diversity insufficient for effective interpolation.

## Foundational Learning

- Concept: Tensor Programs theory and maximal update parametrization (μP).
  - Why needed here: μP provides the mathematical foundation for width-invariant optimization behavior; without it, learned optimizers fail on wider tasks.
  - Quick check question: What are the three key modifications required to apply μP to a neural network?

- Concept: Law of Large Numbers (LLN) scaling in wide neural networks.
  - Why needed here: Ensures that pre-activations and gradients remain well-behaved as width increases, enabling stable optimization.
  - Quick check question: Why does LLN scaling break down in narrow networks, and how does μP address this?

- Concept: Meta-learning objective and evolution strategies for gradient estimation.
  - Why needed here: Learned optimizers are trained via meta-learning; understanding the objective and gradient estimation method is crucial for implementation.
  - Quick check question: How do evolution strategies differ from backpropagation-through-time for meta-gradient estimation?

## Architecture Onboarding

- Component map:
  - Optimizee -> Learned Optimizer -> Accumulators -> Input features -> Meta-training loop

- Critical path:
  1. Initialize optimizee in μP.
  2. Run inner optimization loop (gradient descent with learned optimizer).
  3. Compute loss on meta-training tasks.
  4. Estimate meta-gradients via evolution strategies.
  5. Update learned optimizer parameters.
  6. Repeat until convergence.

- Design tradeoffs:
  - Single-width vs. multi-width meta-training: Multi-width improves generalization but increases compute.
  - SP vs. μP: μP enables width generalization at zero extra compute; SP may diverge on wider tasks.
  - Evolution strategies vs. backpropagation: ES is more stable for long unroll lengths but noisier.

- Failure signatures:
  - Pre-activations blow up across widths → likely missing μP scaling.
  - Optimizer diverges on wider tasks → meta-training task diversity insufficient.
  - Poor performance on longer horizons → meta-training unroll length too short or ES truncation too aggressive.

- First 3 experiments:
  1. Verify pre-activation stability across widths using μP vs. SP on a simple MLP.
  2. Compare single-width vs. multi-width meta-training on 1000-step tasks, evaluate on 2000+ steps.
  3. Test generalization to depth by training on 3-layer MLPs, evaluating on 16-layer MLPs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can μLO meta-training on transformer architectures (not just MLPs) enable zero-shot width scaling for transformers and other non-residual networks?
- Basis in paper: [explicit] Authors note that while they do not meta-train on transformers, they suspect that transformer-specific guidelines from Yang et al. (2022) may be useful. They also observe improved generalization to transformers despite only meta-training on MLPs.
- Why unresolved: Authors did not conduct experiments with transformer architectures during meta-training, only during evaluation.
- What evidence would resolve it: Meta-training μLOs on transformer tasks and evaluating their performance on unseen transformer tasks of varying width.

### Open Question 2
- Question: What is the precise relationship between pre-activation stability in μP and improved meta-generalization to deeper networks and longer training horizons?
- Basis in paper: [inferred] Authors observe empirically that μLOs maintain stable pre-activations when training deeper models and for longer horizons, and hypothesize this may be the cause of improved generalization, but cannot confirm causality.
- Why unresolved: The experiments show correlation but not causation between activation stability and improved generalization.
- What evidence would resolve it: Controlled experiments comparing μLOs with modified activation stability to determine if stability is necessary and sufficient for improved generalization.

### Open Question 3
- Question: What is the optimal meta-training distribution (task diversity, width range, depth range) for maximizing μLO generalization across all axes of meta-generalization?
- Basis in paper: [explicit] Authors conduct ablation studies on meta-training width diversity and find multiple-width meta-training improves generalization, but do not explore depth diversity or other dimensions.
- Why unresolved: The study only varies width during meta-training and does not explore the full space of possible meta-training distributions.
- What evidence would resolve it: Systematic study varying multiple dimensions (width, depth, unroll length, architecture type) during meta-training to identify optimal configurations.

### Open Question 4
- Question: Can μP be extended to create a Depth-μP parameterization that formally enables zero-shot depth scaling for learned optimizers?
- Basis in paper: [explicit] Authors note that Depth-μP exists but is only valid for residual networks with block depth of 1, making it unusable for most practical architectures. They observe empirical depth generalization benefits but no theoretical justification.
- Why unresolved: Current Depth-μP theory does not apply to the architectures studied, and authors do not provide theoretical analysis for depth generalization.
- What evidence would resolve it: Mathematical derivation of Depth-μP for general architectures or formal proof that such a parameterization cannot exist.

## Limitations

- The paper's empirical validation focuses primarily on MLP tasks, with limited ablation studies on the necessity of each μP modification or the impact of different meta-training task distributions.
- Several implementation-specific details remain underspecified, including exact clipping thresholds and normalization constants for VeLO's LSTM input features.
- Claims about generalization to deeper networks (5× meta-training depth) and very long training horizons (25× meta-training steps) are based on limited experiments and warrant further validation across diverse architectures and tasks.

## Confidence

**High Confidence**: The claim that μP parameterization prevents pre-activation explosion across widths is well-supported by theoretical analysis and empirical evidence. The mechanism that maximal parameter updates enable effective training beyond meta-training unroll lengths is logically sound given the normalization properties of μP.

**Medium Confidence**: The assertion that multi-width meta-training improves generalization to both wider tasks and longer horizons is supported by results but lacks extensive ablation studies. The computational efficiency claim (zero extra cost) holds for the LO architecture but requires careful implementation of μP scaling.

**Low Confidence**: The paper's claims about generalization to deeper networks (5× meta-training depth) and very long training horizons (25× meta-training steps) are based on limited experiments and warrant further validation across diverse architectures and tasks.

## Next Checks

1. **Pre-activation stability verification**: Implement both SP and μP parameterizations for small_fc_lopt, train on 1000-step MLP tasks, and systematically measure coordinate-wise pre-activation standard deviation across widths 128-8192 to confirm the claimed stability improvement.

2. **Multi-width vs. single-width meta-training ablation**: Train μLOs with both single-width (1024) and multi-width (128, 512, 1024) meta-training, then evaluate meta-generalization on tasks with widths 2048-8192 and unroll lengths 1000-25000 to quantify the benefit of width diversity.

3. **Depth generalization stress test**: Extend the depth generalization experiments beyond the limited 3→16 layer comparison by training μLOs on depths 3-5 and evaluating on depths 8-32, measuring performance degradation to establish the true limits of depth generalization.