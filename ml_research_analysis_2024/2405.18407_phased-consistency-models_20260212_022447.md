---
ver: rpa2
title: Phased Consistency Models
arxiv_id: '2405.18407'
source_url: https://arxiv.org/abs/2405.18407
tags:
- consistency
- generation
- diffusion
- arxiv
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Phased Consistency Models (PCMs), a novel
  approach to improve the efficiency and quality of text-conditioned image and video
  generation. The authors identify three key limitations in Latent Consistency Models
  (LCMs): inconsistency across inference steps, poor controllability, and inefficiency
  at few-step settings.'
---

# Phased Consistency Models

## Quick Facts
- arXiv ID: 2405.18407
- Source URL: https://arxiv.org/abs/2405.18407
- Reference count: 40
- Key outcome: Introduces Phased Consistency Models (PCMs) that outperform Latent Consistency Models (LCMs) across 1-16 step generation settings, achieving state-of-the-art performance on widely recognized benchmarks for both image and video generation.

## Executive Summary
This paper introduces Phased Consistency Models (PCMs), a novel approach to improve the efficiency and quality of text-conditioned image and video generation. The authors identify three key limitations in Latent Consistency Models (LCMs): inconsistency across inference steps, poor controllability, and inefficiency at few-step settings. To address these issues, PCMs phase the ODE trajectory into sub-trajectories, enabling deterministic multi-step sampling without error accumulation. Additionally, PCMs optionally remove classifier-free guidance (CFG) during distillation, improving controllability and sensitivity to negative prompts. An adversarial consistency loss is introduced to enhance sample quality in few-step settings.

## Method Summary
PCMs address three key limitations in LCMs by phasing the ODE trajectory into sub-trajectories for deterministic multi-step sampling, optionally removing CFG during distillation to improve controllability, and introducing an adversarial consistency loss for better few-step quality. The method trains a U-Net with multiple consistency heads (one per sub-trajectory) and optional adversarial discriminator heads, using a DDIM ODE solver to compute target solutions from pre-trained diffusion models. Training is performed on datasets like CC3M with hyperparameters including LoRA rank 64, learning rate 5e-6, and batch sizes of 160 (Stable Diffusion) or 80 (SDXL).

## Key Results
- PCMs achieve state-of-the-art performance on widely recognized benchmarks for both image and video generation
- Outperforms LCMs across 1-16 step generation settings in terms of FID, FID-CLIP, and CLIP scores
- Successfully addresses three key limitations of LCMs: inconsistency across inference steps, poor controllability, and inefficiency at few-step settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phasing the ODE trajectory into sub-trajectories enables deterministic multi-step sampling without error accumulation.
- Mechanism: By splitting the continuous probability flow ODE trajectory into M sub-trajectories with defined edge timesteps, PCMs enforce self-consistency within each sub-trajectory independently. This allows sampling along different solution points deterministically rather than relying on stochastic error accumulation across all steps.
- Core assumption: The pre-trained diffusion model's PF-ODE is well-approximated by discrete steps, and the sub-trajectory boundaries are chosen such that each segment can be learned as an independent consistency model.
- Evidence anchors:
  - [abstract] "PCMs phase the ODE trajectory into sub-trajectories, enabling deterministic multi-step sampling without error accumulation."
  - [section 3.1] "Instead of mapping all points along the ODE trajectory to the same solution, PCMs phase the ODE trajectory into several sub-trajectories and only enforce the self-consistency property on each sub-trajectory."
  - [corpus] Weak evidence - corpus neighbors focus on unrelated LCM topics (code models, audio compression) rather than consistency model phasing mechanisms.
- Break condition: If sub-trajectories are poorly defined or too few, error bounds become large; if diffusion model's ODE is poorly approximated, phasing cannot guarantee consistency.

### Mechanism 2
- Claim: Optionally removing CFG during distillation improves controllability and sensitivity to negative prompts.
- Mechanism: LCMs use CFG-augmented ODE solvers during distillation, which dilutes negative prompt influence and restricts CFG values. PCMs can optionally train without CFG-augmented solvers when using phased sub-trajectories, preserving negative prompt sensitivity and allowing larger CFG values during inference.
- Core assumption: The distribution distance between solution points of sub-trajectories and real data distribution is smaller than the distribution gap between CFG-augmented predictions and real data, making unguided distillation viable for PCMs.
- Evidence anchors:
  - [abstract] "PCMs optionally remove classifier-free guidance (CFG) during distillation, improving controllability and sensitivity to negative prompts."
  - [section 3.2] "we propose to optionally remove the CFG for consistency distillation" and analysis showing dilution of negative prompt effects in LCMs.
  - [corpus] Weak evidence - corpus neighbors don't discuss CFG in consistency model training.
- Break condition: If phasing is insufficient (too few sub-trajectories), distribution gaps remain too large for unguided distillation to work effectively.

### Mechanism 3
- Claim: Adversarial consistency loss enforces distribution consistency, improving sample quality in few-step settings.
- Mechanism: An adversarial loss is added to the consistency training objective, comparing generated samples from the PCM with those from the pre-trained diffusion model after noise injection. This ensures the PCM's intermediate distributions match the diffusion model's distributions at each sub-trajectory boundary.
- Core assumption: The adversarial loss, when combined with consistency training, will converge to zero when self-consistency is achieved, without corrupting the consistency learning objective.
- Evidence anchors:
  - [abstract] "An adversarial consistency loss is introduced to enhance sample quality in few-step settings."
  - [section 3.3] "we propose an adversarial loss in the latent space for more fine-grained supervision" with theoretical analysis showing convergence properties.
  - [corpus] Weak evidence - corpus neighbors don't discuss adversarial consistency losses in diffusion models.
- Break condition: If the adversarial loss is improperly weighted or the discriminator is poorly designed, it may conflict with consistency learning rather than complement it.

## Foundational Learning

- Concept: Probability Flow ODE (PF-ODE) and its relationship to diffusion models
  - Why needed here: Understanding PF-ODE is essential because PCMs build upon and modify the trajectory consistency framework that relies on reversing the forward diffusion process.
  - Quick check question: How does the PF-ODE relate to the forward SDE in diffusion models, and why is this relationship important for consistency models?

- Concept: Consistency models and their self-consistency property
  - Why needed here: PCMs are an extension of consistency models, so understanding how standard consistency models enforce that all points on an ODE trajectory map to the same solution is fundamental to grasping the phasing innovation.
  - Quick check question: What is the key difference between how consistency models and PCMs enforce consistency along the trajectory?

- Concept: Classifier-free guidance (CFG) in text-conditioned diffusion models
  - Why needed here: The paper's analysis of CFG in LCMs versus PCMs requires understanding how CFG works during both training (with null prompts) and inference (with guidance scaling).
  - Quick check question: How does CFG augmentation during ODE solving affect the consistency distillation process, and why does this create limitations for LCMs?

## Architecture Onboarding

- Component map:
  - U-Net backbone (from pre-trained diffusion model) -> Multiple consistency heads (one per sub-trajectory) -> Adversarial discriminator heads (optional) -> ODE solver (DDIM) -> Text encoder (CLIP)

- Critical path:
  1. Sample noisy latents from forward process
  2. Compute target solutions using pre-trained diffusion model with ODE solver
  3. Apply PCM consistency heads to predict phased solutions
  4. Optionally apply adversarial loss comparing PCM and diffusion model outputs
  5. Backpropagate through consistency loss and adversarial loss

- Design tradeoffs:
  - More sub-trajectories → better deterministic sampling but higher training complexity
  - Using latent discriminator → faster training but potentially less detailed than pixel discriminators
  - Phasing vs. full trajectory consistency → reduced error bounds but requires careful boundary selection

- Failure signatures:
  - Inconsistent generations across different step counts → phasing boundaries poorly chosen
  - Loss of negative prompt sensitivity → CFG-augmented ODE solver used when it shouldn't be
  - Poor quality in few-step settings → adversarial loss weight too low or discriminator poorly trained
  - Artifacts in high-resolution outputs → numerical issues in parameterization (alpha clipping needed)

- First 3 experiments:
  1. Train PCM with 2 sub-trajectories on CC3M, compare FID across 1-16 steps vs LCM baseline
  2. Train PCM with and without CFG-augmented ODE solver, test negative prompt sensitivity
  3. Train PCM with and without adversarial consistency loss, measure FID improvement in 1-4 step regime

## Open Questions the Paper Calls Out
None

## Limitations

- Sub-trajectory boundary selection is not specified, making it unclear how to optimally phase the ODE trajectory
- Key implementation details of the adversarial discriminator (latent vs pixel space, architecture, weighting) are unspecified
- Claims about improved negative prompt sensitivity lack comprehensive empirical validation across diverse scenarios

## Confidence

**High Confidence** in the core architectural contribution: Phasing ODE trajectories into sub-trajectories for deterministic multi-step sampling has clear theoretical grounding in ODE discretization error analysis. The mathematical framework for error bounds in phased sampling is well-established.

**Medium Confidence** in the adversarial consistency loss effectiveness: While the theoretical convergence analysis is sound, the practical impact on generation quality depends heavily on implementation details that aren't fully specified. The ablation showing FID improvements is encouraging but limited to one model scale.

**Low Confidence** in the CFG removal claims: The paper provides theoretical arguments about negative prompt dilution in LCMs, but lacks comprehensive empirical validation. The claim that PCMs "optionally remove CFG" doesn't clarify when this option should be used or its limitations.

## Next Checks

1. **Boundary Sensitivity Analysis**: Systematically vary the number of sub-trajectories (M) and measure the trade-off between error accumulation reduction and training stability. Test boundary selection strategies (uniform vs adaptive) to identify optimal phasing configurations.

2. **Adversarial Loss Ablation**: Conduct controlled experiments varying the adversarial loss weight and discriminator architecture (latent vs pixel). Measure not just FID but also training stability metrics to quantify the impact of different configurations.

3. **CFG Removal Behavioral Study**: Test PCM controllability across diverse negative prompt scenarios (multiple negatives, complex constraints). Compare against LCMs with standard CFG tuning to empirically validate the claimed improvements in negative prompt sensitivity.