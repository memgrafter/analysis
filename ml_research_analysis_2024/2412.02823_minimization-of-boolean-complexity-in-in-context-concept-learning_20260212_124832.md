---
ver: rpa2
title: Minimization of Boolean Complexity in In-Context Concept Learning
arxiv_id: '2412.02823'
source_url: https://arxiv.org/abs/2412.02823
tags:
- concept
- learning
- concepts
- complexity
- alice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates in-context concept learning in large language
  models (LLMs) by testing their ability to learn numerical concepts from labeled
  examples. The authors designed carefully controlled concept learning tasks using
  a simple logical language with basic operators, then measured how well different
  LLMs could learn these concepts.
---

# Minimization of Boolean Complexity in In-Context Concept Learning

## Quick Facts
- **arXiv ID**: 2412.02823
- **Source URL**: https://arxiv.org/abs/2412.02823
- **Reference count**: 19
- **Primary result**: LLM performance strongly correlates with Boolean complexity - simpler concepts (with shorter logical descriptions) are significantly easier for models to learn.

## Executive Summary
This study investigates in-context concept learning in large language models by testing their ability to learn numerical concepts from labeled examples. The authors designed carefully controlled concept learning tasks using a simple logical language with basic operators, then measured how well different LLMs could learn these concepts. The key finding is that LLM performance strongly correlates with Boolean complexity - simpler concepts (with shorter logical descriptions) are significantly easier for models to learn. Across multiple model families including Qwen2 and Gemma 2, accuracy consistently decreased as concept complexity increased. For example, Gemma 2-9B showed a 17 percentage point drop in accuracy (from 83% to 66%) as complexity increased from 1 to 5.

## Method Summary
The study tests in-context concept learning by providing LLMs with prompts containing labeled examples of numerical concepts expressed in a logical language. Each prompt follows a template structure with subjects, predicates, and objects, containing 10 positive and 10 negative examples. The concepts are generated using a logical grammar with operators for equality, inequality, multiplication, and logical connectives (AND, OR). Models are tested on their ability to classify a new example after seeing the examples. The study measures accuracy across different Boolean complexity classes (based on the number of operators in the minimal logical description) and computes correlation coefficients between complexity and accuracy. Experiments were run on Qwen2 and Gemma 2 models of different sizes using instruction-tuned versions with greedy decoding.

## Key Results
- LLM accuracy decreases as Boolean complexity increases, with Gemma 2-9B showing a 17 percentage point drop (83% to 66%) from complexity 1 to 5
- The correlation between complexity and accuracy is statistically significant across multiple model families (Qwen2 and Gemma 2)
- Simpler concepts are consistently learned more easily than complex ones in the in-context learning setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit a simplicity bias in in-context learning, preferring simpler logical concepts over complex ones.
- Mechanism: When presented with examples of a new concept, LLMs use the labeled examples to infer the underlying logical rule. Simpler concepts (with shorter logical descriptions) require fewer parameters or less complex internal representations to capture, making them easier to learn in the limited context window.
- Core assumption: The LLM's internal representation space can efficiently encode simpler logical expressions, and the model uses a bias toward simpler explanations when multiple concepts could explain the data.
- Evidence anchors:
  - [abstract] "show that task performance highly correlates with the Boolean complexity of the concept"
  - [section] "LLM performance strongly correlates with Boolean complexity - simpler concepts (with shorter logical descriptions) are significantly easier for models to learn"
  - [corpus] "Average neighbor FMR=0.465" - weak evidence, suggesting the concept learning approach may be novel in this context
- Break condition: If the model's attention mechanism or representation capacity changes significantly, or if the prompt format biases the model toward more complex explanations, the simplicity bias could diminish or reverse.

### Mechanism 2
- Claim: The concept learning task structure (examples + question) enables effective in-context learning by providing sufficient signal for concept induction.
- Mechanism: The prompt format provides multiple positive and negative examples that establish the boundaries of the concept. The model can use these examples to build an internal representation of the concept's logical structure, then apply this to the final test case.
- Core assumption: The number and diversity of examples (10 positive, 10 negative) is sufficient for the model to infer the concept reliably, and the model can generalize from these examples to novel test cases.
- Evidence anchors:
  - [section] "For each complexity class, we randomly sample 18 concepts... Each data point on the plot... represents the model's accuracy on a specific concept"
  - [section] "We use the template 'Let us define a new word, bnik.', followed by labeled examples and a question at the end, for all prompts"
  - [corpus] Weak evidence - the corpus doesn't provide strong related work on this specific prompt structure
- Break condition: If the number of examples is reduced, if the examples are not sufficiently diverse, or if the prompt template changes significantly, the in-context learning effectiveness may degrade.

### Mechanism 3
- Claim: The Boolean complexity metric (number of logical operators) effectively captures the cognitive difficulty of concepts for LLMs.
- Mechanism: Concepts with more logical operators require more complex reasoning chains and more sophisticated internal representations. The model must learn to compose multiple operations (like AND, OR, comparisons) to correctly classify inputs.
- Core assumption: The minimal description length in the logical grammar is a valid proxy for the computational difficulty of learning the concept, and that this aligns with the model's internal learning processes.
- Evidence anchors:
  - [section] "The complexity of a concept is determined by the number of operators in its minimal description"
  - [section] "Gemma 2-9B showed a 17 percentage point drop in accuracy (from 83% to 66%) as complexity increased from 1 to 5"
  - [corpus] "Average neighbor FMR=0.465" - weak evidence for this specific complexity metric
- Break condition: If the logical grammar is incomplete or doesn't capture the true complexity of concepts, or if the model uses a different internal representation that doesn't align with operator count, the correlation may break.

## Foundational Learning

- Concept: Boolean logic and logical operators (AND, OR, NOT, comparison operators)
  - Why needed here: The entire study is based on concepts expressed in a logical language with these operators. Understanding how these combine to form complex expressions is essential to grasp why complexity affects learning.
  - Quick check question: What is the minimal description of the concept "numbers between 5 and 10" using the operators from Table 1?

- Concept: In-context learning (few-shot prompting)
  - Why needed here: The study tests whether LLMs can learn new concepts from examples provided in the prompt without parameter updates. This is the core mechanism being evaluated.
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and what are its limitations?

- Concept: Kolmogorov complexity / minimal description length
  - Why needed here: The study uses the number of operators in the minimal logical description as a measure of concept complexity, which is directly related to the idea of minimal description length in information theory.
  - Quick check question: Why might the minimal description length be a good proxy for cognitive complexity in both humans and LLMs?

## Architecture Onboarding

- Component map: Concept generation → Prompt creation → Model inference → Accuracy calculation → Correlation analysis
- Critical path: Concept generation → Prompt creation → Model inference → Accuracy calculation → Correlation analysis
- Design tradeoffs:
  - Limited fraction set in grammar for efficiency vs. potential underrepresentation of some concepts
  - Single prompt template for consistency vs. potential bias introduction
  - Fixed number of examples (10 positive, 10 negative) for balance vs. potential underfitting for complex concepts
- Failure signatures:
  - Accuracy doesn't decrease with complexity (suggests simplicity bias is absent or prompt format is ineffective)
  - High variance in accuracy within complexity classes (suggests concepts aren't properly deduplicated)
  - Correlation coefficient near zero (suggests complexity metric doesn't align with learning difficulty)
- First 3 experiments:
  1. Run a single concept from complexity class 1 through all models to verify basic functionality
  2. Test with only 5 examples instead of 10 to see if the number of examples affects learning
  3. Modify the prompt template to use a different nonce word or structure to test robustness to prompt variations

## Open Questions the Paper Calls Out

- Open Question 1: Does the simplicity bias observed in in-context learning extend to non-numerical conceptual domains like natural language predicates or visual concepts?
- Open Question 2: How does the size of the model affect the strength of the simplicity bias in in-context learning?
- Open Question 3: What specific factors beyond minimal description length contribute to the ease of in-context concept learning?
- Open Question 4: How does the choice of prompt template affect the observed simplicity bias in in-context learning?

## Limitations

- The study only tested numerical concepts with a specific logical grammar, limiting generalizability to other domains
- Only one prompt template was used, which may introduce implicit biases in the data and affect experiment results
- The concept generation algorithm and deduplication process are referenced but not fully specified in the paper

## Confidence

**High Confidence**: The core finding that LLM performance correlates with Boolean complexity (simpler concepts are learned more easily) is well-supported by the experimental results across multiple model families. The correlation between accuracy and complexity (with Gemma 2-9B showing a 17 percentage point drop from complexity 1 to 5) is consistently observed and statistically significant.

**Medium Confidence**: The mechanism explaining why simpler concepts are easier to learn (fewer parameters/complexity needed for internal representation) is plausible but not directly tested. The paper assumes the minimal description length in the logical grammar captures computational difficulty, but this connection is not empirically validated.

**Low Confidence**: The generalizability of the findings to other types of concepts beyond the logical language used, or to other prompt formats, is uncertain. The study's controlled setting with a specific logical grammar may not fully represent real-world in-context learning scenarios.

## Next Checks

1. **Template Sensitivity Test**: Systematically vary the prompt template structure (e.g., different nonce words, example ordering, question phrasing) while keeping the concept constant to determine how sensitive the learning performance is to prompt format changes.

2. **Cross-Concept Validation**: Test the same models on concepts from different logical grammars or domains (e.g., string manipulation, spatial reasoning) to assess whether the simplicity bias generalizes beyond numerical concepts.

3. **Internal Representation Analysis**: Use probing techniques or attention visualization to examine how the models represent concepts of different complexities internally, providing evidence for or against the proposed mechanism of simpler concepts requiring less complex representations.