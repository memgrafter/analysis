---
ver: rpa2
title: 'Bias and Volatility: A Statistical Framework for Evaluating Large Language
  Model''s Stereotypes and the Associated Generation Inconsistency'
arxiv_id: '2402.15481'
source_url: https://arxiv.org/abs/2402.15481
tags:
- risk
- discrimination
- llms
- bias
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a statistical framework for analyzing stereotypes
  in large language models (LLMs) by estimating both bias and variation in their generation.
  The proposed Bias-Volatility Framework (BVF) addresses the limitation of current
  alignment evaluation metrics that overlook the randomness of stereotypes caused
  by LLMs' inconsistent generative behavior.
---

# Bias and Volatility: A Statistical Framework for Evaluating Large Language Model's Stereotypes and the Associated Generation Inconsistency

## Quick Facts
- arXiv ID: 2402.15481
- Source URL: https://arxiv.org/abs/2402.15481
- Authors: Yiran Liu; Ke Yang; Zehan Qi; Xiao Liu; Yang Yu; ChengXiang Zhai
- Reference count: 30
- Primary result: Bias risk dominates over volatility risk in LLM discrimination, with most models exhibiting pro-male stereotypes across professions

## Executive Summary
This paper introduces the Bias-Volatility Framework (BVF) to measure and analyze social discrimination in large language models by estimating both bias and variation in their generation. Unlike current alignment evaluation metrics that overlook the randomness of stereotypes caused by LLMs' inconsistent generative behavior, BVF captures the variation in generative behavior to assess the likelihood and degree of negative impact on vulnerable groups. The framework decomposes discrimination risk into bias risk (from the mean of the stereotype distribution) and volatility risk (from its variation), providing a more comprehensive evaluation of model fairness.

The study applies BVF to 12 widely used LLMs and finds that bias risk is the dominant contributor to discrimination risk, most LLMs exhibit substantial pro-male stereotypes across nearly all professions, reinforcement learning from human feedback reduces bias but increases volatility, and discrimination risk correlates with socio-economic factors such as professional salaries. The framework uses heterogeneous contexts mined from real text rather than synthetic or crowdsourced contexts to frame the assessment of discrimination, addressing limitations in existing evaluation approaches.

## Method Summary
The Bias-Volatility Framework estimates the probability distribution of stereotypes in LLM outputs by collecting heterogeneous context templates mined from Wikipedia that contain occupation terms and gender-specific words with coreference. These context templates are parsed and counted to approximate the distribution of contexts where LLMs operate. The framework then applies a cost function J to transform predicted probabilities into discrimination risk values, which are decomposed into bias risk (from the mean of the stereotype distribution) and volatility risk (from its variation). This decomposition relies on the convexity of the discrimination risk function J to ensure clean separation between mean effects and variance effects.

## Key Results
- Bias risk is the dominant contributor to discrimination risk across all evaluated LLMs
- Most LLMs exhibit substantial pro-male stereotypes across nearly all professions
- Reinforcement learning from human feedback reduces bias but increases volatility risk
- Discrimination risk correlates with socio-economic factors such as professional salaries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing discrimination risk into bias risk and volatility risk enables separate tracking of persistent stereotypes vs. context-dependent fluctuations.
- Mechanism: The framework defines conditional prejudice risk as the discrimination risk from the mean prediction across contexts, and conditional caprice risk as the difference between total risk and prejudice risk, effectively isolating variance effects.
- Core assumption: The discrimination risk function J is convex, ensuring that variance contributes positively to overall risk and can be cleanly separated from mean effects.
- Break condition: If J is not convex, the decomposition becomes mathematically invalid and the two risk components cannot be cleanly separated.

### Mechanism 2
- Claim: Heterogeneous context collection captures model inconsistencies that homogeneous contexts would miss.
- Mechanism: By mining diverse sentence skeletons from real text rather than using synthetic or crowdsourced contexts, the framework approximates the true distribution of contexts where LLMs operate, revealing prediction variation.
- Core assumption: The distribution of mined contexts approximates the operational context distribution of real LLM applications.
- Break condition: If the mined context distribution significantly differs from actual application contexts, the framework will fail to capture true model inconsistencies.

### Mechanism 3
- Claim: Mapping word probabilities to stereotypes and then to discrimination risk creates a measurable scale for comparing models.
- Mechanism: The framework uses stereotype projection direction dy to transform probability distributions into directional stereotypes Sy, then applies cost function J to convert stereotypes into discrimination risk values that can be aggregated and compared.
- Core assumption: The stereotype projection direction dy is appropriately defined to capture meaningful discrimination dimensions.
- Break condition: If the stereotype projection direction dy is poorly chosen or the cost function J is inappropriate, the resulting discrimination risk values will not meaningfully represent actual discrimination.

## Foundational Learning

- Concept: Probability distributions and conditional probability
  - Why needed here: The framework operates on LLMs' predicted word probabilities conditioned on context templates, requiring understanding of how probabilities represent model preferences
  - Quick check question: If an LLM predicts "he" with 0.8 probability and "she" with 0.2 probability for a doctor, what is the probability distribution over gender terms?

- Concept: Statistical moments (mean and variance)
  - Why needed here: Risk decomposition requires calculating both the mean of risk across contexts (prejudice risk) and the variance in risk (caprice risk)
  - Quick check question: If risk values across contexts are [0.1, 0.2, 0.15, 0.25], what are the mean and variance of these risk values?

- Concept: Convex functions and Jensen's inequality
  - Why needed here: The framework relies on convex risk functions to ensure that variance contributes positively to overall risk and enables clean decomposition
  - Quick check question: Is the function f(x) = x² convex, and if so, does Jensen's inequality apply to it?

## Architecture Onboarding

- Component map: Data mining → Context template collection → LLM prediction generation → Probability transformation → Risk calculation → Aggregation → Risk decomposition → Analysis
- Critical path: Context template collection → LLM prediction generation → Risk calculation → Aggregation
- Design tradeoffs: More diverse contexts improve detection of inconsistencies but increase computational cost; simpler risk functions are faster but may miss nuanced discrimination
- Failure signatures: High caprice risk with low prejudice risk suggests model is inconsistent rather than systematically biased; high variance in risk across contexts indicates context sensitivity
- First 3 experiments:
  1. Test framework on a simple toy model with known biases to verify decomposition works correctly
  2. Apply framework to GPT-2 and compare results with existing bias metrics to validate findings
  3. Modify context diversity and observe how caprice risk changes to understand sensitivity to context collection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of context template mining approach affect the measurement of stereotype risk in LLMs?
- Basis in paper: [explicit] The paper describes using a data-mining approach to collect context templates, avoiding human or AI-generated templates to prevent introducing prior biases.
- Why unresolved: The impact of different context template collection methods on the measurement of stereotype risk is not fully explored.
- What evidence would resolve it: Comparative analysis of stereotype risk measurements using different context template collection methods, such as human-generated vs. data-mined templates.

### Open Question 2
- Question: What is the impact of using different definitions of the criterion function J on the measurement of stereotype risk and volatility?
- Basis in paper: [explicit] The paper mentions that J can be defined in various ways, and its convexity governs the decomposition outcomes.
- Why unresolved: The paper does not explore how different definitions of J affect the measurement of stereotype risk and volatility.
- What evidence would resolve it: Experiments comparing the measurements of stereotype risk and volatility using different definitions of J, such as the l∞ norm vs. the lk norm.

### Open Question 3
- Question: How do social and economic factors correlate with the discrimination risk exhibited by LLMs?
- Basis in paper: [explicit] The paper mentions a correlation between discrimination risk and factors such as professional salaries.
- Why unresolved: The paper does not provide a detailed analysis of how different social and economic factors correlate with discrimination risk.
- What evidence would resolve it: Detailed statistical analysis of the correlation between various social and economic factors and discrimination risk, including education, recruitment ratios, and word frequency.

## Limitations
- The framework's decomposition of risk into bias and volatility components fundamentally depends on the convexity of the discrimination risk function J, which is not empirically validated across all operating regimes.
- The approach relies on mined Wikipedia contexts to approximate real-world LLM usage patterns, but Wikipedia text may not represent the full diversity of contexts where LLMs actually operate.
- The framework requires defining stereotype projection directions (dy) to map word probabilities to meaningful discrimination dimensions, but the paper does not specify how these directions are chosen or validated.

## Confidence
- **High Confidence**: The core mathematical framework for decomposing discrimination risk into bias and volatility components is sound, assuming convexity holds. The general approach of using heterogeneous contexts to detect model inconsistencies is methodologically valid.
- **Medium Confidence**: The empirical findings about bias dominance over volatility, pro-male stereotypes, and RLHF effects are likely correct but may be sensitive to the specific context distribution and cost function choices. The correlation with socio-economic factors requires careful interpretation given potential confounding variables.
- **Low Confidence**: The absolute discrimination risk values depend heavily on the specific implementation of the cost function J and stereotype projection directions, which are not fully specified in the paper.

## Next Checks
1. Test whether the chosen discrimination risk function J is convex across the full range of probability distributions observed in LLM outputs by plotting J against input probabilities and verifying that Jensen's inequality holds for edge cases.
2. Systematically vary the diversity and composition of context templates and measure how discrimination risk, bias risk, and volatility risk change to reveal whether the framework's conclusions are robust to context collection methodology.
3. Test alternative stereotype projection directions and verify that the framework's key findings (bias dominance, pro-male stereotypes, RLHF effects) remain consistent across different but reasonable choices of dy.