---
ver: rpa2
title: Does GPT Really Get It? A Hierarchical Scale to Quantify Human vs AI's Understanding
  of Algorithms
arxiv_id: '2406.14722'
source_url: https://arxiv.org/abs/2406.14722
tags:
- algorithm
- understanding
- language
- each
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hierarchical scale to quantify understanding
  of algorithms in both humans and AI systems. The authors define six levels of understanding,
  ranging from basic execution to advanced reasoning and explanation, based on the
  ability to manipulate internal representations of algorithms.
---

# Does GPT Really Get It? A Hierarchical Scale to Quantify Human vs AI's Understanding of Algorithms

## Quick Facts
- arXiv ID: 2406.14722
- Source URL: https://arxiv.org/abs/2406.14722
- Reference count: 40
- Primary result: Introduces hierarchical scale to quantify algorithmic understanding in humans and LLMs, showing GPT-4/4o comparable to grad students

## Executive Summary
This paper presents a novel hierarchical scale for measuring algorithmic understanding that spans from basic execution to advanced reasoning capabilities. The scale consists of six levels of understanding, validated through human participants and applied to evaluate three versions of GPT models on two classic algorithms: Euclidean and Ford-Fulkerson. The study reveals that while GPT-4 and GPT-4o perform comparably to graduate students on most tasks, all versions excel at code generation but struggle with mathematical reasoning compared to language-based reasoning tasks. This work provides a framework for monitoring AI progress in algorithmic domains and demonstrates that the proposed hierarchy effectively captures depth of understanding across both human and artificial intelligence systems.

## Method Summary
The researchers developed a 5-level understanding hierarchy to evaluate algorithmic comprehension in both humans and LLMs. They conducted human surveys with undergraduate and graduate students using Euclidean and Ford-Fulkerson algorithms, with randomized inputs for reproducibility. The same survey was administered to GPT-3.5, GPT-4, and GPT-4o via API with system prompts. Responses were manually graded on a 0-2 scale for correctness, depth, and reasoning. The study compared performance across understanding levels, human groups, and LLM versions using statistical analysis to validate the scale's effectiveness in capturing algorithmic understanding depth.

## Key Results
- GPT-4 and GPT-4o achieve performance comparable to graduate students on most understanding tasks
- All GPT versions excel at code generation but significantly underperform on mathematical reasoning tasks
- GPT-3.5 lags substantially behind GPT-4/4o and human participants across all levels
- The hierarchical scale effectively distinguishes between surface-level pattern matching and genuine algorithmic comprehension

## Why This Works (Mechanism)
The hierarchical scale works by decomposing algorithmic understanding into distinct cognitive components that can be independently measured and evaluated. By focusing on the ability to manipulate internal representations rather than just surface-level execution, the scale captures the depth of comprehension that separates genuine understanding from pattern matching. The use of both code generation and mathematical reasoning tasks reveals complementary strengths and weaknesses in current AI systems, showing that proficiency in one domain doesn't guarantee competence in the other.

## Foundational Learning
- Algorithmic complexity theory: Understanding time and space complexity is essential for evaluating algorithmic efficiency claims
  - Why needed: Many evaluation questions require reasoning about algorithm performance
  - Quick check: Can explain Big-O notation and its application to the tested algorithms
- Graph theory fundamentals: Core concepts like paths, flows, and connectivity are crucial for understanding Ford-Fulkerson
  - Why needed: The algorithm's operation depends on graph structure properties
  - Quick check: Can identify augmenting paths and residual networks in flow problems
- Number theory basics: GCD properties and Euclidean algorithm mechanics form the foundation for level 0-2 tasks
  - Why needed: Direct evaluation of basic algorithm execution and correctness
  - Quick check: Can trace through the Euclidean algorithm step-by-step

## Architecture Onboarding

**Component Map:** Survey design -> Human administration -> LLM API execution -> Manual grading -> Statistical analysis

**Critical Path:** Algorithm selection → Question design → Randomization → Response collection → Manual evaluation → Comparison

**Design Tradeoffs:** Manual grading provides nuanced assessment but introduces subjectivity; randomization ensures reproducibility but may miss edge cases; focusing on two algorithms allows depth but limits generalizability

**Failure Signatures:** GPT responses that mimic format without consistent logic suggest pattern matching; incorrect hedging on objectively true statements indicates comprehension gaps; performance drops on mathematical vs. language tasks reveal domain-specific limitations

**First Experiments:**
1. Test the hierarchical scale on a third algorithm (e.g., Dijkstra's) to validate generalizability
2. Compare manual grading consistency across multiple evaluators to assess subjectivity
3. Run ablation study removing randomization to measure its impact on result reproducibility

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Small sample sizes for human participants (32 total across algorithms) limit statistical power
- Focus on only two algorithms from a single textbook source restricts generalizability
- Manual grading introduces potential subjectivity despite structured scoring rubric
- Does not explore performance on algorithms beyond the tested set

## Confidence

**High Confidence Claims:**
- The hierarchical scale effectively captures different levels of algorithmic understanding
- GPT-4/4o performance approaches graduate student level on most tasks
- Code generation remains a strength while mathematical reasoning is a weakness for all GPT versions

**Medium Confidence Claims:**
- The scale generalizes to other algorithms beyond Euclidean and Ford-Fulkerson
- Performance gaps reflect fundamental architectural limitations rather than training data biases
- Manual grading reliability is sufficient for meaningful comparisons

**Low Confidence Claims:**
- The observed performance differences will persist as models scale further
- The specific ranking of understanding levels will remain stable across different algorithmic domains

## Next Checks
1. Replicate the study with a larger, more diverse set of algorithms and human participants to verify the hierarchical scale's robustness across different computational domains
2. Implement automated evaluation metrics alongside human grading to assess inter-rater reliability and reduce subjectivity in response scoring
3. Test whether performance improvements on the hierarchical scale correlate with measurable improvements in code execution and correctness, distinguishing between surface-level pattern matching and genuine algorithmic understanding