---
ver: rpa2
title: 'KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing'
arxiv_id: '2410.18517'
source_url: https://arxiv.org/abs/2410.18517
tags:
- cache
- arxiv
- layer
- sharing
- kvsharer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KVSharer addresses GPU memory pressure during LLM inference by
  sharing dissimilar KV caches across layers. Rather than intuitively sharing similar
  caches, it empirically discovers that sharing dissimilar ones better preserves performance.
---

# KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing

## Quick Facts
- arXiv ID: 2410.18517
- Source URL: https://arxiv.org/abs/2410.18517
- Reference count: 38
- Primary result: Reduces KV cache computation by 30% while maintaining >90% model performance

## Executive Summary
KVSharer introduces an innovative approach to reduce GPU memory pressure during large language model (LLM) inference by sharing key-value (KV) caches across layers. Unlike conventional methods that share similar caches, KVSharer discovers that sharing dissimilar caches actually preserves performance better. The method employs an empirical search on calibration datasets to determine optimal layer-wise sharing strategies, then applies these strategies during inference. This approach achieves significant computational savings while maintaining model quality.

## Method Summary
KVSharer addresses the memory bottleneck in LLM inference by implementing layer-wise KV cache sharing. The key innovation lies in recognizing that sharing dissimilar KV caches between layers leads to better performance preservation than sharing similar ones. The method involves an offline calibration phase where it searches for optimal sharing configurations on representative datasets, then applies these configurations during inference. The approach is designed to be compatible with existing intra-layer compression techniques, allowing for stacked optimizations.

## Key Results
- Reduces KV cache computation by 30% during inference
- Maintains over 90% of original model performance
- Achieves at least 1.3× generation speed-up compared to baseline

## Why This Works (Mechanism)
KVSharer works by strategically sharing KV caches across different layers rather than within the same layer. The counterintuitive insight is that sharing dissimilar caches (those with different statistical properties or from different semantic contexts) preserves model performance better than sharing similar ones. This is because dissimilar caches provide complementary information that helps maintain the model's ability to capture diverse patterns. The empirical search process identifies which layers should share caches based on their dissimilarity metrics, optimizing the trade-off between memory savings and performance preservation.

## Foundational Learning
- **KV Cache Compression**: Technique to reduce memory footprint of attention mechanisms; needed to understand the baseline optimization space.
- **Layer-Wise Sharing**: Distributing computational resources across layers; quick check: verify which layers benefit most from sharing.
- **Dissimilarity Metrics**: Measures to quantify differences between KV caches; quick check: ensure metrics capture meaningful semantic differences.
- **Calibration Datasets**: Representative data used for optimization; quick check: validate dataset diversity matches real-world usage.
- **Memory-Computation Trade-off**: Balancing resource usage against performance; quick check: quantify degradation at different sharing levels.

## Architecture Onboarding
- **Component Map**: KV cache → Dissimilarity analyzer → Sharing optimizer → Inference engine
- **Critical Path**: Calibration → Cache sharing strategy → Inference optimization
- **Design Tradeoffs**: Memory savings vs. performance degradation; offline computation vs. runtime efficiency
- **Failure Signatures**: Performance degradation when sharing overly similar caches; memory inefficiency when sharing overly dissimilar caches
- **First Experiments**: 1) Test sharing similar vs. dissimilar caches on small models; 2) Validate calibration dataset representativeness; 3) Measure performance impact across different sharing ratios

## Open Questions the Paper Calls Out
None

## Limitations
- The counterintuitive finding that dissimilar cache sharing outperforms similar sharing lacks theoretical justification and may not generalize across different model architectures.
- Performance degradation thresholds (maintaining over 90% performance) are not contextualized with respect to specific quality metrics or user tolerance levels.
- The empirical search for optimal sharing strategies on calibration data raises concerns about how representative these datasets are of real-world usage patterns.

## Confidence
- **High Confidence**: Technical feasibility of layer-wise KV cache sharing and using calibration datasets for optimization
- **Medium Confidence**: Empirical results showing 30% reduction in KV cache computation and 1.3× speed-up, as these are dataset-dependent findings
- **Low Confidence**: Theoretical explanation for why dissimilar cache sharing outperforms similar cache sharing, and universal applicability across different LLM architectures

## Next Checks
1. Test KVSharer's performance on multiple diverse datasets beyond the calibration set to verify generalization of the dissimilar sharing advantage.
2. Conduct ablation studies comparing KVSharer with traditional similar-cache sharing approaches across different model sizes and architectures.
3. Evaluate the end-to-end quality impact using multiple metrics (perplexity, task-specific accuracy, human evaluation) to better understand the 10% performance gap.