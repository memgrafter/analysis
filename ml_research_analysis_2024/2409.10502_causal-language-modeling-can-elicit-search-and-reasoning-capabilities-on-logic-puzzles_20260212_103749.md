---
ver: rpa2
title: Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic
  Puzzles
arxiv_id: '2409.10502'
source_url: https://arxiv.org/abs/2409.10502
tags:
- puzzle
- cell
- puzzles
- cells
- sudoku
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether causal language models can learn
  complex reasoning tasks like solving Sudoku and Zebra puzzles. The authors train
  Transformer models using next-token prediction, experimenting with different input
  orderings of puzzle cells.
---

# Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles

## Quick Facts
- arXiv ID: 2409.10502
- Source URL: https://arxiv.org/abs/2409.10502
- Authors: Kulin Shah; Nishanth Dikkala; Xin Wang; Rina Panigrahy
- Reference count: 31
- One-line primary result: Transformers can learn to solve complex logic puzzles when trained with solver-decomposed reasoning order

## Executive Summary
This paper investigates whether causal language models can learn complex reasoning tasks like solving Sudoku and Zebra puzzles. The authors train Transformer models using next-token prediction with different input orderings of puzzle cells. They find that training with a "solver-decomposed reasoning order" - the order a Sudoku solver would fill cells in - dramatically improves performance compared to fixed or random orderings. The model achieves 87.18% accuracy on full Sudoku puzzles, which improves to 94.21% with beam search decoding. Through probing analysis, they show the model implicitly learns to track candidate value sets for each cell, similar to how human solvers and algorithmic solvers approach these puzzles.

## Method Summary
The authors train Transformer-based GPT-2 models on synthetic Sudoku and Zebra puzzle datasets using next-token prediction. They experiment with three different cell orderings: fixed order (left-to-right, top-to-bottom), random order, and solver-decomposed reasoning order based on the order a Sudoku solver would fill cells. The solver-decomposed approach uses a Sudoku solver with 7 strategies to determine cell filling order based on difficulty. During inference, they employ beam search decoding with widths of 3 and 5 to improve performance. They also use linear probing to analyze the model's internal representations and verify that it learns to track candidate value sets for each cell.

## Key Results
- Solver-decomposed reasoning order achieves 87.18% complete puzzle accuracy vs 7.2% for fixed order and 1% for random order
- Beam search decoding with k=3 improves complete puzzle accuracy to 94.21%
- Linear probing shows 93%+ overlap between model's internal candidate sets and the solver's candidate sets
- Model performance is robust across different difficulty levels of Sudoku puzzles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training with solver-decomposed reasoning order enables the model to learn to search for easy-to-fill cells before applying strategies.
- Mechanism: The solver provides an adaptive order of cells based on difficulty, allowing the model to learn the two-step process: (1) search for cells where progress can be made, (2) apply the correct strategy to fill that cell. This decomposition breaks down the complex task into manageable sub-tasks.
- Core assumption: The model can learn the decomposition when provided with the correct order during training, rather than having to discover it independently.
- Evidence anchors:
  - [abstract] "We observe that Transformer models trained on this synthetic task can indeed learn to solve Sudokus (our model solves 94.21% of the puzzles fully correctly) when trained on a logical sequence of steps taken by a solver."
  - [section 3.2] "To provide decomposed reasoning order of cells during the training, we arrange the cells according to how easy to fill they are."
  - [corpus] Weak - no direct corpus evidence about this specific mechanism.
- Break condition: If the solver order is not adaptive or if the strategies are not well-defined, the model may not learn the decomposition effectively.

### Mechanism 2
- Claim: The model implicitly learns to track candidate value sets for each cell, similar to how human solvers and algorithmic solvers approach these puzzles.
- Mechanism: Through linear probing of the model's internal representations, the authors can decode information about the set of possible values in any given cell, indicating that the model has learned to maintain candidate sets internally.
- Core assumption: The model's internal representations contain sufficient information to reconstruct the candidate sets without explicit training on this task.
- Evidence anchors:
  - [abstract] "we study the internal representations of the trained Transformer and find that through linear probing, we can decode information about the set of possible values in any given cell from them"
  - [section 4.4] "We see that for all positions the average overlap between the solver's and the model's candidate set is above 93%"
  - [corpus] Weak - no direct corpus evidence about this specific mechanism.
- Break condition: If the model's internal representations do not encode candidate set information, or if the probing method is insufficient, this mechanism would fail.

### Mechanism 3
- Claim: Beam search decoding improves performance by allowing the model to explore multiple potential cells when it has confusion about which cell to fill next.
- Mechanism: The model makes more first mistakes at the start of puzzles when there are more empty cells because it's harder to predict the correct cell to fill. Beam search allows the model to explore multiple options and select the most probable solution.
- Core assumption: The model's confusion about which cell to fill next is the primary source of errors, and beam search can effectively resolve this confusion.
- Evidence anchors:
  - [section 3.5] "To understand the failure modes of the model, we measure the hinted cell accuracy by providing information about easy-to-decode cells to the model during inference."
  - [section 3.5] "We see that beam search with k = 3 improves the cell accuracy by around 2% and complete puzzle accuracy by around 4%"
  - [corpus] Weak - no direct corpus evidence about this specific mechanism.
- Break condition: If the model's errors are not primarily due to cell selection confusion, or if beam search does not effectively resolve this confusion, this mechanism would fail.

## Foundational Learning

- Concept: Constraint Satisfaction Problems (CSPs)
  - Why needed here: Sudoku and Zebra puzzles are both CSPs where the goal is to find values for variables that satisfy a set of constraints. Understanding CSPs is fundamental to understanding how the model approaches these puzzles.
  - Quick check question: What are the constraints in a Sudoku puzzle, and how do they differ from the constraints in a Zebra puzzle?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: The paper draws inspiration from CoT prompting to provide intermediate steps that help the model solve complex reasoning tasks. Understanding CoT is important for understanding the training methodology.
  - Quick check question: How does providing intermediate steps during training help the model learn to solve complex reasoning tasks?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The paper uses a Transformer-based model for solving logic puzzles. Understanding how Transformers work and how attention mechanisms enable them to process sequential data is crucial for understanding the model's capabilities.
  - Quick check question: How does the attention mechanism in Transformers allow the model to focus on relevant parts of the input when solving logic puzzles?

## Architecture Onboarding

- Component map: Input sequence -> Transformer encoder-decoder -> Output sequence
- Critical path: The forward pass through the Transformer layers, where the model processes the input sequence and generates the output sequence
- Design tradeoffs: The authors chose a relatively small Transformer model (42M parameters) compared to large language models, which may limit its capabilities but also makes it more efficient to train and deploy
- Failure signatures: The model may fail to solve puzzles if it cannot effectively search for easy-to-fill cells, if it cannot apply the correct strategies to fill those cells, or if it cannot maintain accurate candidate sets internally
- First 3 experiments:
  1. Train the model with fixed order of cells and evaluate its performance
  2. Train the model with random order of cells and evaluate its performance
  3. Train the model with solver-decomposed reasoning order and evaluate its performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance change if the model was trained with the entire search trace of the solver, rather than just the final sequence of moves?
- Basis in paper: [explicit] The authors mention that they trained their model using only the final sequence of moves from the solver, without access to the entire search trace. They contrast this with recent work by [LSM+24] which posits including the entire search trace as part of the training Chain-of-Thought data to help a Transformer learn tasks involving search and planning dynamics.
- Why unresolved: The paper focuses on training with just the final sequence of moves and does not explore the impact of providing the full search trace during training.
- What evidence would resolve it: Training a model with the full search trace and comparing its performance to the current model on Sudoku and Zebra puzzles.

### Open Question 2
- Question: How does the performance of the model vary with the size of the Sudoku puzzles? Would the model still be able to solve larger Sudoku puzzles effectively?
- Basis in paper: [inferred] The paper only considers 9x9 Sudoku puzzles and does not explore the performance of the model on larger Sudoku puzzles. It is mentioned that the generalized version of Sudoku with board size nxn is NP-complete.
- Why unresolved: The paper does not provide any results or analysis on the performance of the model for larger Sudoku puzzles.
- What evidence would resolve it: Training and evaluating the model on Sudoku puzzles of varying sizes (e.g., 16x16, 25x25) and comparing the performance.

### Open Question 3
- Question: How would the performance of the model change if the candidate set information was explicitly provided during training, rather than having the model implicitly learn it?
- Basis in paper: [explicit] The authors show that the model implicitly learns to track the candidate set information through probing analysis. They mention that during training, no direct information about the candidate set is provided to the model.
- Why unresolved: The paper does not explore the impact of explicitly providing candidate set information during training on the model's performance.
- What evidence would resolve it: Training a model with explicit candidate set information and comparing its performance to the current model on Sudoku and Zebra puzzles.

## Limitations

- Limited puzzle domain: The study focuses exclusively on Sudoku and Zebra puzzles, which are well-defined constraint satisfaction problems with clear rules. While this controlled setting allows for rigorous evaluation, it's unclear how well the approach generalizes to other reasoning tasks or real-world problem-solving scenarios.
- Solver dependency: The method relies heavily on a pre-defined Sudoku solver to generate the "solver-decomposed reasoning order." This raises questions about whether the model is truly learning reasoning capabilities or simply learning to follow the solver's prescribed steps.
- Data requirements: The approach requires generating large synthetic datasets (1.8M Sudoku puzzles for training) with carefully constructed training sequences. This data generation process is non-trivial and may not be feasible for all reasoning tasks.

## Confidence

- High confidence: The claim that training with solver-decomposed reasoning order significantly outperforms fixed or random orderings is well-supported by the empirical results (87.18% vs 7.2% and 1% complete puzzle accuracy).
- Medium confidence: The claim about the model implicitly learning to track candidate value sets is supported by probing experiments showing 93%+ overlap with the solver's candidate sets, but the probing methodology could be more rigorously validated.
- Medium confidence: The mechanism explaining beam search improvement (resolving confusion about which cell to fill) is plausible given the hinted cell accuracy results, but alternative explanations haven't been ruled out.

## Next Checks

- Check 1: Ablation on solver strategies: Systematically remove individual Sudoku solving strategies from the solver used to generate training sequences and measure the impact on model performance. This would test whether the model truly learns the decomposition or simply memorizes the specific solver's approach.
- Check 2: Cross-puzzle generalization: Evaluate the trained Sudoku model on Zebra puzzles (without fine-tuning) to assess whether the learned reasoning capabilities transfer to different constraint satisfaction problems. Similarly, test on partially-defined or corrupted puzzles.
- Check 3: Internal representation analysis: Beyond linear probing of candidate sets, perform ablation studies on attention heads and layer activations to identify which components of the Transformer are most critical for the search and reasoning capabilities. This would validate the mechanism claims about how the model operates internally.