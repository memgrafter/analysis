---
ver: rpa2
title: Do Large Language Models Exhibit Cognitive Dissonance? Studying the Difference
  Between Revealed Beliefs and Stated Answers
arxiv_id: '2406.14986'
source_url: https://arxiv.org/abs/2406.14986
tags:
- probability
- scenario
- reasoning
- llms
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether Large Language Models (LLMs) can
  correctly integrate probabilistic information into their next-token predictions,
  contrasting their performance on multiple-choice questions (MCQs) with their implicit
  reasoning ability during text completion. While LLMs perform well on MCQs requiring
  probabilistic reasoning, the authors find that their implicit reasoning is often
  flawed.
---

# Do Large Language Models Exhibit Cognitive Dissonance? Studying the Difference Between Revealed Beliefs and Stated Answers

## Quick Facts
- arXiv ID: 2406.14986
- Source URL: https://arxiv.org/abs/2406.14986
- Reference count: 30
- Primary result: LLMs show systematic biases in implicit probabilistic reasoning despite strong explicit MCQ performance

## Executive Summary
This paper investigates whether Large Language Models can correctly integrate probabilistic information into their next-token predictions, revealing a significant gap between their explicit performance on multiple-choice questions and their implicit reasoning during text completion. While LLMs perform well on MCQs requiring probabilistic reasoning, the authors find that their implicit reasoning is often flawed, frequently favoring unlikely outcomes and being inappropriately influenced by prior independent events. The evaluation framework introduced—comparing next-token distributions to ground truth probabilities—reveals systematic biases and inconsistencies that conventional MCQ-based evaluations fail to detect.

## Method Summary
The authors develop a novel evaluation framework that compares LLMs' next-token probability distributions against ground truth probabilities across 1300+ prompts spanning five scenario families (Dice, Coins, Preference, Choice, Statistics). They evaluate 10 different LLMs using both explicit MCQ-style questions and implicit text completion tasks where the same probabilistic information must be integrated. The framework measures discrepancies using Chebyshev distance, Manhattan distance, and KL divergence between predicted and ground truth distributions, revealing systematic biases in how models handle probabilistic reasoning during text generation.

## Key Results
- LLMs show significantly better performance on explicit MCQ questions than on implicit text completion tasks requiring the same probabilistic reasoning
- Models frequently assign disproportionate probability mass to unlikely outcomes despite correct MCQ answers
- LLM predictions are inappropriately influenced by prior independent events and partial observations, leading to erroneous results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large Language Models (LLMs) can correctly integrate probabilistic information into their next-token predictions when answering explicit multiple-choice questions (MCQs).
- Mechanism: When prompted with a MCQ that requires probabilistic reasoning, LLMs are able to select the correct answer by leveraging their understanding of the context and applying appropriate probabilistic reasoning. This is evidenced by their high accuracy in MCQ settings, where they often select the correct answer with a probability close to 1.
- Core assumption: The model has been trained on a diverse dataset that includes examples of probabilistic reasoning, allowing it to learn the necessary patterns and associations.
- Evidence anchors:
  - [abstract] "While LLMs perform well on MCQs requiring probabilistic reasoning..."
  - [section] "In our experiments, the model is able to use the information provided in the system prompt to answer the question correctly (choice [[A]]) with a probability of 99.96%."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.419, average citations=0.0. Top related titles: Quantifying and Mitigating Selection Bias in LLMs: A Transferable LoRA Fine-Tuning and Efficient Majority Voting Approach, AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology, Towards Understanding the Cognitive Habits of Large Reasoning Models.
- Break condition: If the model has not been exposed to sufficient examples of probabilistic reasoning during training, or if the MCQ is particularly complex and requires advanced reasoning beyond the model's capabilities.

### Mechanism 2
- Claim: LLMs' implicit probabilistic reasoning during text completion is often flawed, leading to inconsistent probability mass allocation and systematic biases.
- Mechanism: When completing a text scenario that requires probabilistic reasoning, LLMs frequently fail to integrate the provided probabilistic information into their next-token predictions. This results in probability distributions over outcomes that do not align with the true likelihood of the events, and can lead to the model favoring unlikely outcomes.
- Core assumption: The model's training data does not adequately represent the complexities of probabilistic reasoning in natural language contexts, leading to gaps in its understanding and application of probabilistic concepts.
- Evidence anchors:
  - [abstract] "However, during text completion (i.e., implicit probabilistic reasoning), where the same information must be taken into account to generate text, the models' predictions often significantly diverge from the known ground truth."
  - [section] "In the implicit reasoning setting, the model is also given the population statistics. The beginning of an answer is then provided to the LLM, ending on the last token before the diagnosis (see Figure 3b, text in light red). Letting the model generate the rest of the answer will disproportionally favor the 'anxiety' diagnosis: indeed, the model places a99.08%probability mass on 'anxiety' (and totaling less than1%on 'burnout' and 'depression')."
  - [corpus] Weak or missing evidence. No direct evidence found in the corpus related to this specific mechanism.
- Break condition: If the model is fine-tuned on a dataset that specifically addresses probabilistic reasoning in text completion scenarios, or if the text completion prompts are carefully designed to guide the model towards correct probabilistic reasoning.

### Mechanism 3
- Claim: LLMs' probabilistic predictions are inappropriately influenced by prior independent events and partial observations, leading to erroneous results in text generation.
- Mechanism: When presented with prior independent events or partial observations in a text scenario, LLMs often fail to update their predictions appropriately. This can lead to the model's probability distribution being skewed towards or against certain outcomes, even when the prior events or observations are unrelated to the outcome of interest.
- Core assumption: The model's training data does not adequately represent the nuances of handling prior information and partial observations in probabilistic reasoning, leading to over-reliance on certain patterns or heuristics.
- Evidence anchors:
  - [abstract] "For instance, our evaluation method reveals that implicit probabilistic reasoning is improperly influenced by many factors, such as independent prior events, partial observations about a result, or statistical background information."
  - [section] "Result 2: Model predictions about the next outcome are affected by prior independent events. The ability to handle uncertainty and partial information also includes the ability to discard irrelevant information, i.e., not updating predictions about the outcome of a scenario when presented with unrelated evidence. Studying the predicted next-token distribution in a scenario with repeated (independent) events allows us to measure this. Our results (illustrated withLlama-3.3-70B2 in Figure 7) exhibit a fundamental misalignment between the models' predictions about the outcome of a scenario and the true probability distribution."
  - [corpus] Weak or missing evidence. No direct evidence found in the corpus related to this specific mechanism.
- Break condition: If the model is trained on a dataset that explicitly addresses the handling of prior information and partial observations in probabilistic reasoning, or if the text completion prompts are carefully designed to minimize the impact of irrelevant prior events or observations.

## Foundational Learning

- Concept: Probability theory and Bayesian inference
  - Why needed here: Understanding the principles of probability theory and Bayesian inference is crucial for interpreting the results of the experiments and for designing appropriate evaluation scenarios. It allows for a deeper understanding of how LLMs should ideally handle probabilistic information and how their performance can be measured.
  - Quick check question: What is the difference between prior probability and posterior probability in Bayesian inference?

- Concept: Text generation and autoregressive models
  - Why needed here: Knowledge of how LLMs generate text through autoregressive modeling is essential for understanding the mechanism behind their implicit probabilistic reasoning. It helps in interpreting the next-token probability distributions and in designing evaluation scenarios that accurately reflect the model's text generation process.
  - Quick check question: How does the autoregressive nature of LLMs influence their ability to handle probabilistic information in text completion tasks?

- Concept: Multiple-choice question (MCQ) evaluation and biases
  - Why needed here: Familiarity with MCQ evaluation methods and their potential biases is important for understanding the limitations of explicit probabilistic reasoning assessment. It helps in interpreting the results of MCQ-based evaluations and in designing more robust evaluation frameworks.
  - Quick check question: What are some common biases that can affect the performance of LLMs on MCQ tasks, and how can they be mitigated?

## Architecture Onboarding

- Component map: Evaluation framework consists of explicit MCQ assessment -> implicit text completion assessment -> next-token probability extraction -> ground truth comparison -> metric computation
- Critical path: Generate text completion prompts → extract next-token probabilities → normalize distributions → compute distance metrics → compare to ground truth
- Design tradeoffs: MCQ assessments offer simplicity and interpretability but may not capture full probabilistic reasoning; text completion provides realism but is more complex to design and evaluate
- Failure signatures: Disproportionate probability mass to unlikely outcomes, influence from prior independent events, inconsistent handling of partial observations
- First 3 experiments:
  1. Evaluate model on simple MCQ scenario (e.g., die roll) requiring basic probabilistic reasoning
  2. Design matching text completion prompt and compare next-token distribution to ground truth
  3. Introduce variations like prior independent events and assess model's prediction updates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Large Language Models handle probabilistic reasoning when faced with conflicting evidence from multiple sources?
- Basis in paper: [inferred] The paper demonstrates that LLMs struggle with integrating new evidence into their predictions, as seen in the Dice scenario with partial observations.
- Why unresolved: The paper does not explore scenarios where evidence from multiple sources conflicts, which could further reveal the limitations of LLMs' probabilistic reasoning.
- What evidence would resolve it: Experiments showing how LLMs handle conflicting probabilistic evidence from multiple sources, such as different medical studies with varying prevalence rates.

### Open Question 2
- Question: To what extent do Large Language Models exhibit systematic biases in their probabilistic reasoning, and how can these biases be mitigated?
- Basis in paper: [explicit] The paper highlights that LLMs often favor unlikely outcomes and are influenced by prior independent events, indicating systematic biases.
- Why unresolved: While the paper identifies biases, it does not explore potential methods to mitigate these biases or reduce their impact on LLM performance.
- What evidence would resolve it: Research into techniques for reducing biases in LLM probabilistic reasoning, such as fine-tuning or post-processing methods.

### Open Question 3
- Question: How does the performance of Large Language Models on probabilistic reasoning tasks vary across different model architectures and training datasets?
- Basis in paper: [explicit] The paper evaluates a diverse set of LLMs with varying architectures and sizes, but does not explore how performance varies based on training data or architecture.
- Why unresolved: The paper does not provide insights into the relationship between model architecture, training data, and probabilistic reasoning performance.
- What evidence would resolve it: Comparative studies of LLM performance on probabilistic reasoning tasks across different architectures and training datasets.

## Limitations

- The evaluation framework relies on controlled synthetic scenarios that may not fully capture the complexity of real-world probabilistic reasoning in natural language
- The analysis focuses primarily on next-token distributions without examining longer-range generation patterns that could reveal additional failure modes
- The connection between identified biases and cognitive dissonance theory remains weak, with limited corpus evidence supporting this specific interpretation

## Confidence

- High confidence: The systematic gap between MCQ and text completion performance
- Medium confidence: The specific mechanisms of bias (ordering effects, prior event influence)
- Low confidence: The direct connection to cognitive dissonance theory

## Next Checks

1. **Cross-architecture validation**: Test whether the observed performance gap persists across different model families (e.g., transformers vs alternative architectures) and training paradigms (supervised fine-tuning vs reinforcement learning) to determine if the limitations are fundamental or architecture-specific.

2. **Real-world scenario extension**: Develop evaluation scenarios based on actual domains where probabilistic reasoning is critical (medical diagnosis, financial forecasting) to assess whether the identified limitations manifest in practical applications and measure their downstream impact.

3. **Intervention testing**: Implement targeted fine-tuning or prompt engineering interventions designed to address specific biases (ordering effects, prior event influence) and measure whether these improve implicit reasoning while maintaining explicit MCQ performance.