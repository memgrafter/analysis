---
ver: rpa2
title: 'BurstM: Deep Burst Multi-scale SR using Fourier Space with Optical Flow'
arxiv_id: '2409.15384'
source_url: https://arxiv.org/abs/2409.15384
tags:
- burstm
- optical
- image
- flow
- scale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BurstM addresses the problem of multi-frame super-resolution by
  overcoming limitations in existing methods that use deformable convolution networks,
  which suffer from small receptive fields and fixed kernel numbers. The core idea
  is to use optical flow for precise alignment between reference and source frames,
  combined with neural warping in Fourier space to represent high-frequency textures.
---

# BurstM: Deep Burst Multi-scale SR using Fourier Space with Optical Flow

## Quick Facts
- **arXiv ID:** 2409.15384
- **Source URL:** https://arxiv.org/abs/2409.15384
- **Reference count:** 40
- **Key outcome:** Achieves state-of-the-art PSNR gains up to 0.17 dB on BurstSR dataset using optical flow for alignment and Fourier space representation for high-frequency textures

## Executive Summary
BurstM addresses the limitations of deformable convolution networks in multi-frame super-resolution by introducing optical flow-based alignment combined with neural warping in Fourier space. This approach overcomes the small receptive fields and fixed kernel numbers of DCNs, enabling precise alignment and flexible multi-scale super-resolution (×2, ×3, ×4) without retraining. The method demonstrates superior performance on both synthetic and real-world datasets, achieving up to 0.17 dB PSNR improvement on BurstSR while maintaining faster inference times despite higher parameter count.

## Method Summary
BurstM uses optical flow estimation (FNet) to align reference and source frames, followed by Fourier space neural warping to capture high-frequency textures. The architecture includes an encoder for latent feature extraction, amplitude/frequency/phase estimators in Fourier space, and a reconstruction module with blending capabilities. The method supports multi-scale SR through continuous coordinate-based modeling in Fourier space, eliminating the need for fixed-scale upsampling modules. Training uses L1 loss for image reconstruction and L2 loss for optical flow alignment, with AdamW optimizer and cosine annealing scheduler.

## Key Results
- Achieves up to 0.17 dB PSNR improvement on BurstSR dataset compared to state-of-the-art methods
- Demonstrates faster inference times despite having 14M parameters
- Supports flexible multi-scale super-resolution (×2, ×3, ×4) without retraining
- Outperforms existing methods on both synthetic and real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optical flow provides more precise alignment than deformable convolution networks (DCNs)
- **Mechanism:** Optical flow estimates continuous pixel-level correspondence between reference and source frames, capturing global motion patterns that DCNs with small receptive fields (3×3 kernels) miss. This results in more accurate warping and fewer boundary artifacts.
- **Core assumption:** Optical flow can be computed efficiently enough to integrate into a deep learning pipeline without prohibitive computational cost
- **Evidence anchors:**
  - [abstract]: "The proposed method estimates the optical flow offset for accurate alignment"
  - [section]: "We adopt FNet (optical flow estimator) of FRVSR [36]. FNet predicts optical flow on low-resolution grid"
  - [corpus]: Weak evidence - related papers focus on different optical flow applications (medical, SAR-optical, anime)
- **Break condition:** Optical flow fails to estimate correspondences in occlusion regions or with large object displacements

### Mechanism 2
- **Claim:** Fourier space representation captures high-frequency textures more effectively than spatial domain methods
- **Mechanism:** The neural warping in Fourier space estimates continuous Fourier coefficients (amplitude, frequency, phase) from burst frames, allowing the network to represent high-frequency details without spectral bias that plagues standard implicit neural representations
- **Core assumption:** Fourier coefficients can be learned from burst frames to accurately reconstruct high-frequency information
- **Evidence anchors:**
  - [abstract]: "predicts the continuous Fourier coefficient of each frame for representing high-frequency textures"
  - [section]: "To represent high-frequency textures using well-aligned information, our network estimates Fourier information"
  - [corpus]: Weak evidence - related papers focus on different Fourier applications (B-splines, multimodal fusion)
- **Break condition:** Insufficient burst frames or severe noise prevents accurate Fourier coefficient estimation

### Mechanism 3
- **Claim:** Unimodal architecture enables multi-scale super-resolution without retraining
- **Mechanism:** By representing images in Fourier space with continuous coordinates, the network can directly predict different scale factors (×2, ×3, ×4) using the same architecture, avoiding the need for fixed-scale upsampling modules like pixel shuffle
- **Core assumption:** Continuous Fourier representation generalizes across different scale factors
- **Evidence anchors:**
  - [abstract]: "enhanced the network's flexibility by supporting various super-resolution (SR) scale factors with the unimodel"
  - [section]: "Our model adds offsetsδL i into regular coordinates xL i ∈ [−1, 1]H×W ×2 and applies s scale upsampling"
  - [corpus]: Weak evidence - no direct comparison with multi-scale SR methods
- **Break condition:** Performance degrades significantly on scales not seen during training

## Foundational Learning

- **Concept:** Optical flow estimation and its applications in video processing
  - **Why needed here:** Understanding how optical flow provides pixel-level correspondence is crucial for grasping why BurstM outperforms DCN-based alignment
  - **Quick check question:** What are the main limitations of conventional optical flow methods, and how do learning-based approaches address them?

- **Concept:** Fourier analysis and spectral representation of signals
  - **Why needed here:** The method relies on representing images in Fourier space to capture high-frequency details, so understanding Fourier transforms and spectral bias is essential
  - **Quick check question:** How does spectral bias affect neural networks, and why does positional encoding help mitigate this issue?

- **Concept:** Implicit Neural Representations (INR) and continuous coordinate-based modeling
  - **Why needed here:** BurstM uses INR concepts with Fourier features to represent images as continuous functions, which is fundamental to its multi-scale capability
  - **Quick check question:** What distinguishes INR from traditional discrete representations, and how does this enable scale flexibility?

## Architecture Onboarding

- **Component map:** Low-resolution frames → FNet (Optical Flow Estimator) → Encoder Eψ → Neural Warping Tν (amplitude, frequency, phase estimators) → Blender Bη → Decoder GΘ → Skip connection with upsampling → Output

- **Critical path:** Low-resolution frames → FNet → Encoder → Neural Warping (Fourier space) → Blender → Decoder → Skip connection → Output

- **Design tradeoffs:**
  - Higher parameter count (14M) for better performance vs. computational efficiency
  - Single-scale training vs. multi-scale flexibility
  - Fourier space computation vs. spatial domain simplicity

- **Failure signatures:**
  - Boundary artifacts when alignment fails
  - Loss of high-frequency details when Fourier estimation is inaccurate
  - Scale-specific performance degradation when trained on limited scale factors

- **First 3 experiments:**
  1. Validate optical flow accuracy on synthetic data with known ground truth motion
  2. Test Fourier coefficient estimation on simple patterns to verify high-frequency capture
  3. Compare single-scale vs. multi-scale training performance on ×4 SR task

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the proposed BurstM method perform when dealing with more extreme occlusion scenarios or significant motion between frames?
  - **Basis in paper:** The paper mentions that BurstM has an occlusion issue and shows an example in Figure 10, but states that it can still utilize the inaccurate optical flow to reconstruct high-quality images.
  - **Why unresolved:** The paper only provides one example of a corner case with occlusion, and it's unclear how the method would perform in more challenging scenarios with multiple occlusions or larger object displacements.
  - **What evidence would resolve it:** Additional experiments and analysis showing the performance of BurstM on datasets with varying degrees of occlusion and motion between frames, comparing it to other state-of-the-art methods in these challenging scenarios.

- **Open Question 2:** Can the proposed BurstM method be extended to handle videos with longer temporal sequences or higher frame rates?
  - **Basis in paper:** The paper focuses on burst super-resolution using a fixed number of frames (14 in the case of the datasets used), but it doesn't explore the possibility of extending the method to longer video sequences.
  - **Why unresolved:** The paper doesn't provide any analysis or experiments on the scalability of the method to handle videos with more frames or higher frame rates.
  - **What evidence would resolve it:** Experiments and analysis demonstrating the performance of BurstM on video datasets with varying numbers of frames and frame rates, comparing it to other video super-resolution methods.

- **Open Question 3:** How does the computational complexity of BurstM scale with increasing image resolution or number of frames?
  - **Basis in paper:** The paper mentions that BurstM has a large number of parameters and provides inference times for a specific input size, but it doesn't explore how the computational complexity scales with different input sizes or number of frames.
  - **Why unresolved:** The paper doesn't provide any analysis on the scalability of the method's computational complexity.
  - **What evidence would resolve it:** Experiments and analysis showing the inference times and memory usage of BurstM for different input sizes and numbers of frames, comparing it to other methods in terms of computational efficiency.

## Limitations

- **Generalization to extreme occlusion and large motion scenarios** remains unclear from limited experimental evidence
- **Computational complexity scaling** with higher resolutions and frame counts is not thoroughly analyzed
- **Real-time application feasibility** is uncertain given the 14M parameter count and inference requirements

## Confidence

- **High confidence:** PSNR/SSIM improvements on BurstSR and SyntheticBurst datasets, optical flow providing better alignment than DCNs
- **Medium confidence:** Fourier space superiority for high-frequency texture representation, multi-scale flexibility without retraining
- **Low confidence:** Computational efficiency claims given higher parameter count, performance in real-time applications

## Next Checks

1. Conduct ablation study comparing optical flow alignment vs. deformable convolution performance on challenging motion scenarios with occlusions
2. Test multi-scale capability on scale factors (×1.5, ×5) not present in training data to validate generalization claims
3. Measure actual inference latency and memory usage across different GPU architectures to verify computational efficiency claims