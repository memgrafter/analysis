---
ver: rpa2
title: 'RPMixer: Shaking Up Time Series Forecasting with Random Projections for Large
  Spatial-Temporal Data'
arxiv_id: '2402.10487'
source_url: https://arxiv.org/abs/2402.10487
tags:
- time
- series
- forecasting
- random
- rpmixer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses spatial-temporal forecasting problems, which
  involve predicting future values in large-scale datasets where nodes have both spatial
  and temporal relationships. The authors propose RPMixer, an all-Multi-Layer Perceptron
  (all-MLP) architecture that does not rely on input graphs.
---

# RPMixer: Shaking Up Time Series Forecasting with Random Projections for Large Spatial-Temporal Data

## Quick Facts
- **arXiv ID**: 2402.10487
- **Source URL**: https://arxiv.org/abs/2402.10487
- **Reference count**: 40
- **Primary result**: Outperforms spatial-temporal graph models and general forecasting models on LargeST dataset using random projections and frequency-domain processing

## Executive Summary
This paper addresses the challenge of spatial-temporal forecasting in large-scale datasets where nodes have both spatial and temporal relationships. The authors propose RPMixer, an all-Multi-Layer Perceptron architecture that achieves state-of-the-art performance without relying on input graphs. By integrating random projection layers to increase diversity among mixer block outputs and processing time series in the frequency domain using complex linear layers, RPMixer demonstrates superior performance on the LargeST dataset, the largest spatial-temporal forecasting benchmark available.

## Method Summary
RPMixer is an all-MLP architecture that processes spatial-temporal data through a series of mixer blocks. Each block contains temporal and spatial mixers, with the temporal mixer processing time series in the frequency domain using complex linear layers (FFT → linear(real/imag) → iFFT) to capture periodic patterns. Random projection layers are integrated to increase diversity among mixer block outputs, inspired by ensemble learning theory. The model uses pre-activation design to preserve identity mapping connections, enabling ensemble-like behavior where each mixer block acts as an independent base learner. The architecture is trained using AdamW optimizer with MAE loss and evaluated on MAE, RMSE, and MAPE metrics.

## Key Results
- Achieves lower MAE, RMSE, and MAPE across all LargeST datasets compared to spatial-temporal graph models and general forecasting models
- Demonstrates superior scalability to large graphs without requiring adjacency matrices
- Shows state-of-the-art performance on long-term time series forecasting tasks
- Effectively captures daily and weekly periodic patterns in traffic data through frequency-domain processing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Random projection layers increase diversity among the outputs of different mixer blocks, improving ensemble-like behavior.
- **Mechanism**: Random projection layers force each mixer block to focus on different sets of nodes, making their outputs less correlated and thus improving the ensemble-like diversity.
- **Core assumption**: Identity mapping residual connections allow mixer blocks to behave like base learners in an ensemble model.
- **Evidence anchors**: [abstract], [section], [corpus]
- **Break condition**: If identity mapping connections are removed, the ensemble-like behavior disappears and random projections no longer improve diversity.

### Mechanism 2
- **Claim**: Processing time series in the frequency domain using complex linear layers better captures periodic patterns inherent in spatial-temporal data.
- **Mechanism**: Complex linear layers leverage FFT and inverse FFT to model periodic signals, preserving daily and weekly patterns in traffic data.
- **Core assumption**: Spatial-temporal datasets exhibit periodicity due to human activity.
- **Evidence anchors**: [section], [section], [corpus]
- **Break condition**: If the time series lacks periodicity, complex linear layers offer no advantage over standard linear layers.

### Mechanism 3
- **Claim**: Pre-activation design preserves identity mapping connections, enabling shorter paths that act as base learners.
- **Mechanism**: By placing ReLU before linear layers, the forward pass includes explicit identity terms that create shorter paths, allowing each mixer block to contribute independently.
- **Core assumption**: Residual connections with identity mapping create ensemble-like behavior as per [51].
- **Evidence anchors**: [section], [section], [corpus]
- **Break condition**: If post-activation is used instead, identity mapping is lost and ensemble-like behavior diminishes.

## Foundational Learning

- **Concept**: Ensemble learning theory and diversity among base learners
  - **Why needed here**: The paper frames deep MLP layers as an ensemble where each block is a base learner; diversity among them is crucial for performance.
  - **Quick check question**: What happens to ensemble performance if all base learners are highly correlated?

- **Concept**: Fast Fourier Transform (FFT) and complex number arithmetic
  - **Why needed here**: Complex linear layers rely on FFT to transform time series into frequency domain and process real/imaginary parts separately.
  - **Quick check question**: How does FFT decompose a periodic signal into its frequency components?

- **Concept**: Johnson-Lindenstrauss lemma and random projection
  - **Why needed here**: Random projections preserve pairwise distances between nodes while reducing dimensionality, enabling scalable spatial modeling without adjacency matrices.
  - **Quick check question**: What is the probability that random projection preserves pairwise distances up to (1±ε)?

## Architecture Onboarding

- **Component map**: Input → (Temporal mixer + Spatial mixer) → Mixer block → Repeat → Output linear
- **Critical path**: Input → (Temporal mixer + Spatial mixer) → Mixer block → Repeat → Output linear
- **Design tradeoffs**:
  - Random projection vs. learned adjacency: Random projection avoids O(n²) memory but may lose precise node relationships.
  - Complex vs. real linear layers: Complex layers better model periodicity but add computational overhead.
  - Pre-activation vs. post-activation: Pre-activation preserves identity mapping but may complicate gradient flow.
- **Failure signatures**:
  - If MAE/RMSE improves but MAPE worsens: Model may overfit to absolute errors, not relative errors.
  - If performance plateaus early: Insufficient diversity among mixer blocks; consider increasing random projection dimension.
  - If memory explodes on large n: Random projection dimension may be too high; reduce m_neuron.
- **First 3 experiments**:
  1. Train baseline TSMixer on SD dataset, record MAE/MAPE/RMSE across horizons.
  2. Add one random projection layer, compare diversity via correlation-error diagram.
  3. Replace complex linear layer with real linear, measure impact on periodic vs. non-periodic datasets.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the random projection concept be integrated into transformer-based spatial-temporal forecasting models like PDFormer?
  - **Basis in paper**: [explicit] The paper discusses the potential integration of random projection layers into PDFormer to diversify the intermediate representation output.
  - **Why unresolved**: The paper does not provide experimental results or implementation details for this integration.
  - **What evidence would resolve it**: Experimental results comparing the performance of PDFormer with and without random projection layers on large-scale spatial-temporal datasets.

- **Open Question 2**: What is the impact of incorporating learnable spatial and temporal embeddings from STID into RPMixer?
  - **Basis in paper**: [explicit] The paper presents an ablation study on RPMixer with STID components, showing improved performance in terms of RMSE when integrating STID.
  - **Why unresolved**: The paper does not provide a comprehensive analysis of the trade-offs between performance gains and increased model complexity.
  - **What evidence would resolve it**: A detailed analysis comparing the performance, runtime, and memory usage of RPMixer with and without STID components on various datasets.

- **Open Question 3**: How does the frequency-domain MLP proposed in [68] compare to RPMixer's complex linear layer design?
  - **Basis in paper**: [inferred] The paper mentions the frequency-domain MLP as a potential alternative design for the temporal mixer in RPMixer.
  - **Why unresolved**: The paper does not provide a direct comparison between the two approaches.
  - **What evidence would resolve it**: Experimental results comparing the performance of RPMixer with its current complex linear layer design versus a frequency-domain MLP on spatial-temporal datasets.

## Limitations
- The claim that random projections create meaningful diversity among mixer blocks lacks corpus validation.
- The performance benefits on non-periodic datasets haven't been thoroughly evaluated.
- The pre-activation design's role in preserving ensemble-like behavior requires more empirical evidence.

## Confidence
- **High Confidence**: The overall performance improvements on LargeST dataset (MAE, RMSE, MAPE metrics) are well-documented with proper baselines.
- **Medium Confidence**: The mechanism that random projections increase diversity among mixer blocks is theoretically sound but lacks corpus validation.
- **Low Confidence**: The specific claim that pre-activation design preserves identity mapping for ensemble behavior requires more empirical evidence, as corpus neighbors don't discuss this connection.

## Next Checks
1. **Diversity Validation**: Measure pairwise correlation between outputs of different mixer blocks with and without random projections across multiple runs. Compare these correlations to ensemble diversity metrics.
2. **Periodicity Ablation**: Train RPMixer on both periodic (traffic) and non-periodic (stock prices) datasets, replacing complex linear layers with standard real layers. Quantify performance differences.
3. **Identity Mapping Test**: Implement post-activation variant of RPMixer and measure whether MAE/MAPE/RMSE degrade, specifically testing if ensemble-like behavior is preserved without pre-activation design.