---
ver: rpa2
title: 'KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based
  False Premise Questions'
arxiv_id: '2407.05868'
source_url: https://arxiv.org/abs/2407.05868
tags:
- fpqs
- llms
- task
- knowledge
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KG-FPQ, an automated and scalable pipeline
  for constructing False Premise Questions (FPQs) to evaluate factuality hallucination
  in LLMs. The method involves editing true triplets from knowledge graphs into false
  triplets and using GPTs to generate semantically rich FPQs across six levels of
  confusability and two task formats.
---

# KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions

## Quick Facts
- arXiv ID: 2407.05868
- Source URL: https://arxiv.org/abs/2407.05868
- Reference count: 17
- This paper introduces KG-FPQ, an automated and scalable pipeline for constructing False Premise Questions (FPQs) to evaluate factuality hallucination in LLMs.

## Executive Summary
This paper presents KG-FPQ, a comprehensive framework for evaluating factuality hallucination in large language models (LLMs) using automatically generated False Premise Questions (FPQs) derived from knowledge graphs. The method edits true triplets into false ones using six different approaches based on distance and semantic association, then generates questions using GPT models across two task formats. The resulting benchmark contains approximately 178k FPQs across three knowledge domains, revealing that LLMs are more prone to hallucination when false premises involve semantically related entities or are structurally closer in the knowledge graph.

## Method Summary
The KG-FPQ pipeline extracts true triplets from knowledge graphs and systematically edits them into false triplets using six methods: neighbor subject-concept (NSC), neighbor distance-concept (NDC), non-neighbor subject-concept (NNSC), non-neighbor distance-concept (NNDC), neighbor subject-relation (NNSR), and non-neighbor subject-relation (NNDR). GPT-3.5 generates Yes-No questions while GPT-4 creates WH-questions, with manual verification for WH-questions. Two LLMs evaluate the FPQs in discriminative (Yes-No) and generative (WH) formats, while an automated evaluator (FPQ-Judge, a LoRA-tuned Llama3-8B) assesses generative responses.

## Key Results
- LLMs perform worse at generating factual statements than at distinguishing them when faced with FPQs
- FPQs with closer distances or stronger associations between edited and original objects are more confusing to LLMs
- The average accuracy for NSC is consistently lower than for NNSC across all domains, validating the distance effect

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs experience increased factuality hallucination when the edited object in false triplets is a neighbor of the subject within the knowledge graph.
- Mechanism: The closer the edited object is to the subject in the KG, the more likely the LLM's internal knowledge graph representation conflates the false premise with correct facts, leading to hallucination.
- Core assumption: LLMs store and retrieve knowledge in a structure resembling knowledge graphs, where proximity indicates semantic relatedness.
- Evidence anchors:
  - "when the edited object in the false triplets is a neighbor of the subject are more confusing to LLMs, resulting in a higher probability of factuality hallucination"
  - "the average accuracy for NSC is consistently lower than for NNSC across all domains"
- Break condition: If LLMs do not use graph-like knowledge representations or if distance metrics do not correlate with semantic relatedness.

### Mechanism 2
- Claim: FPQs with stronger associations between edited and original objects (same concept or same relation) induce more hallucination than those with weaker associations.
- Mechanism: When the edited object shares semantic properties with the original object, the LLM's confidence in the false premise increases, making it more likely to accept and generate hallucinated content.
- Core assumption: LLM knowledge retrieval is sensitive to semantic similarity and conceptual relationships encoded in KGs.
- Evidence anchors:
  - "when the edited object and the original object belong to the same concept in the KG, the FPQs generated are more confusing for LLMs"
  - "FPQs generated from false triplets where the edited object and original object share the same relation are more likely to induce factuality hallucination"
- Break condition: If semantic similarity does not influence LLM confidence or if LLM knowledge retrieval is not sensitive to KG relations.

### Mechanism 3
- Claim: LLMs perform worse at generating factual statements than at distinguishing them when faced with FPQs.
- Mechanism: Discriminative tasks require binary classification, which is computationally simpler than generative tasks that require constructing coherent factual statements while avoiding hallucination.
- Core assumption: The cognitive load of generating factual content is higher than that of evaluating its truth.
- Evidence anchors:
  - "LLMs perform worse at generating factual statements than at distinguishing them when faced with FPQs"
  - "the overall accuracy in generative task is lower than in the discriminative task"
- Break condition: If the computational complexity difference between tasks is not significant or if LLM architectures handle both tasks equally well.

## Foundational Learning

- Concept: Knowledge Graph Structure and Triplet Representation
  - Why needed here: Understanding how KGs represent facts as <subject, relation, object> triplets is essential for grasping how FPQs are constructed by editing these triplets.
  - Quick check question: What are the three components of a KG triplet and how do they relate to each other?

- Concept: False Premise Questions (FPQs) and Factuality Hallucination
  - Why needed here: FPQs contain incorrect facts that mislead LLMs into generating hallucinated content, which is the core phenomenon being evaluated.
  - Quick check question: How does a false premise question differ from a true premise question in terms of its effect on LLM responses?

- Concept: Distance and Association Metrics in Knowledge Graphs
  - Why needed here: The paper's main findings rely on understanding how distance (hops between entities) and association (same concept/relation) affect LLM performance on FPQs.
  - Quick check question: What is the difference between distance-based and association-based editing methods for creating FPQs?

## Architecture Onboarding

- Component map: Triplet extraction -> Triplet editing (6 methods) -> FPQ generation (GPT-3.5 for Yes-No, GPT-4 for WH) -> LLM evaluation -> FPQ-Judge evaluation for generative task
- Critical path: Triplet editing -> FPQ generation -> LLM evaluation (discriminative and generative) -> Analysis of confusability effects
- Design tradeoffs: Automated generation vs. quality control (manual review needed), scalability vs. semantic richness, discriminative vs. generative evaluation difficulty
- Failure signatures: Low accuracy on TPQs indicates knowledge gaps; consistent bias in Yes-No answers suggests model-specific tendencies; high variance across domains indicates knowledge domain sensitivity
- First 3 experiments:
  1. Test a small sample of FPQs on a single LLM to verify the pipeline generates coherent questions
  2. Compare accuracy on NSC vs NNSC FPQs to validate distance effect
  3. Run both discriminative and generative tasks on the same LLM to confirm format difficulty difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the internal knowledge storage structure of LLMs compare to knowledge graphs, and can this be leveraged to improve factuality hallucination detection?
- Basis in paper: The paper speculates that LLMs' internal knowledge storage may resemble knowledge graphs based on their analysis of confusability.
- Why unresolved: The paper only speculates about this similarity without providing concrete evidence or analysis.
- What evidence would resolve it: Empirical studies comparing the structure of LLM internal representations to knowledge graph structures, or experiments testing if knowledge graph-based techniques can improve LLM factuality detection.

### Open Question 2
- Question: Can the KG-FPQ methodology be extended to create more diverse and challenging false premise questions beyond the six editing methods used in this study?
- Basis in paper: The paper mentions that FPQs can be exploited as prompt injection attacks and suggests expanding the variety of FPQs for red teaming.
- Why unresolved: The paper only explores six editing methods and doesn't investigate other potential ways to create confusing false premises.
- What evidence would resolve it: Experiments testing alternative editing methods or prompt engineering techniques to create more challenging FPQs, along with evaluations of their impact on LLM factuality.

### Open Question 3
- Question: How can the FPQ-Judge evaluator be improved to detect all forms of hallucination in LLM responses, not just those induced by false premises?
- Basis in paper: The paper acknowledges that FPQ-Judge cannot ensure responses are completely non-hallucinated and its generalization to other tasks remains unexplored.
- Why unresolved: The current evaluator is specifically trained for FPQ evaluation and may not capture all types of factual errors.
- What evidence would resolve it: Development and testing of more comprehensive hallucination detection methods, including evaluations on diverse LLM outputs beyond FPQ responses.

## Limitations
- The automated evaluation using FPQ-Judge cannot comprehensively detect all hallucinated content in generative responses, leaving potential gaps in accuracy measurement
- The analysis focuses predominantly on model accuracy without exploring underlying reasoning patterns or failure modes in detail
- The study evaluates only three knowledge domains, raising questions about whether findings generalize across broader knowledge landscapes

## Confidence
- **High Confidence**: The finding that LLM performance decreases as edited objects become closer neighbors to subjects in the KG is well-supported by consistent accuracy differences between NSC and NNSC categories across all domains. The discriminative vs. generative task difficulty comparison also shows clear and reproducible patterns.

- **Medium Confidence**: The semantic association effects (same concept/same relation) are demonstrated but with less statistical power. The paper shows directional trends but could benefit from more granular analysis of when and why these associations matter.

- **Low Confidence**: The automated evaluator's ability to accurately detect hallucinated content in generative responses remains uncertain. Without ground truth validation of FPQ-Judge's assessments, the reported accuracy scores for generative tasks carry significant uncertainty.

## Next Checks
1. **Ground Truth Validation**: Manually validate a statistically significant sample of FPQ-Judge's evaluations on generative responses to establish the correlation between automated and human assessments of hallucination detection.

2. **Domain Generalization Test**: Apply the KG-FPQ pipeline to at least two additional knowledge domains (e.g., medical knowledge, scientific facts) to test whether distance and association effects hold across different knowledge structures.

3. **Cross-Model Consistency**: Evaluate the same FPQ sets across a broader range of LLMs including open-source models (Llama, Mistral) to determine whether the observed patterns are model-agnostic or specific to the evaluated architectures.