---
ver: rpa2
title: Fine-grained Hallucination Detection and Editing for Language Models
arxiv_id: '2401.06855'
source_url: https://arxiv.org/abs/2401.06855
tags:
- detection
- error
- fine-grained
- errors
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a comprehensive taxonomy of hallucinations
  and proposes a novel task of automatic fine-grained hallucination detection. The
  authors construct a new evaluation benchmark, FavaBench, with about one thousand
  fine-grained human judgments on three LM outputs across various domains.
---

# Fine-grained Hallucination Detection and Editing for Language Models

## Quick Facts
- arXiv ID: 2401.06855
- Source URL: https://arxiv.org/abs/2401.06855
- Authors: Abhika Mishra; Akari Asai; Vidhisha Balachandran; Yizhong Wang; Graham Neubig; Yulia Tsvetkov; Hannaneh Hajishirzi
- Reference count: 40
- One-line primary result: Introduces a comprehensive taxonomy of hallucinations and proposes a novel task of automatic fine-grained hallucination detection

## Executive Summary
This paper addresses the critical problem of hallucination detection and correction in language model outputs through a novel fine-grained approach. The authors introduce a hierarchical taxonomy that categorizes hallucinations into six types with varying editability requirements, moving beyond binary detection to nuanced classification. They construct FavaBench, a new evaluation benchmark with about one thousand fine-grained human judgments, and train FAVA, a retrieval-augmented LM, using carefully created synthetic data to detect and correct these hallucinations.

The system demonstrates significant improvements over existing approaches, with FAVA outperforming ChatGPT and GPT-4 on fine-grained hallucination detection while achieving 5-10% FActScore improvements in factuality. The work provides a comprehensive framework for understanding, detecting, and correcting different types of hallucinations, addressing a critical challenge in ensuring the reliability of language model outputs across various domains.

## Method Summary
The authors introduce FAVA (Fine-grained Automatic Hallucination Detection and Editing), a retrieval-augmented LM designed to detect and correct fine-grained hallucinations in language model outputs. The system employs a hierarchical taxonomy categorizing hallucinations into six types: Entity, Relation, Modification, Addition, Invented, and Unverifiable. FAVA uses a two-stage approach where a retriever finds relevant documents from Wikipedia, and an editor analyzes the LM output against this context to identify and correct hallucinations. The model is trained on synthetic data generated by systematically inserting diverse error types into reference passages using GPT-4/ChatGPT, creating realistic erroneous examples paired with gold edits for training.

## Key Results
- FAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination detection on the FavaBench benchmark
- FAVA's suggested edits improve factuality of LM-generated text by 5-10% FActScore
- Retrieval of top five documents significantly enhances performance compared to top one document by 4.5%
- Synthetic data generation produces valid training examples with average quality scores of 1.66 for validity and 1.36 for quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hierarchical taxonomy enables fine-grained hallucination detection by categorizing errors into six types with different editability.
- Mechanism: The taxonomy distinguishes between entity-level, relation-level, and more complex errors like invented or unverifiable statements. This allows the detection system to identify not just that an error exists, but what type it is and how it should be corrected.
- Core assumption: Different hallucination types require different detection and correction strategies, and the taxonomy captures these distinctions meaningfully.
- Evidence anchors:
  - [abstract] "We introduce a comprehensive taxonomy of hallucinations and argue that hallucinations manifest in diverse forms, each requiring varying degrees of careful assessments to verify factuality."
  - [section 3.2] "Our taxonomy hierarchically classifies factual errors in LM generations into six categories."
  - [corpus] Weak evidence - related work focuses on task-specific taxonomies rather than open-ended generation
- Break condition: If the taxonomy fails to capture important error types or if the distinctions between categories prove not to matter for detection/editing quality.

### Mechanism 2
- Claim: Synthetic data generation creates sufficient training examples for fine-grained detection by systematically inserting diverse error types into reference passages.
- Mechanism: The system generates diverse text from Wikipedia passages, then uses GPT-4/ChatGPT to insert each error type one at a time with special tags. This creates realistic erroneous examples paired with gold edits for training.
- Core assumption: Large language models can generate realistic hallucinations when prompted appropriately, and the resulting data is representative enough for training.
- Evidence anchors:
  - [section 5.2] "Inspired by prior work that leverages LMs to generate synthetic training data... we introduce a new data creation method grounded on our fine-grained hallucination taxonomies."
  - [section 7.2] "Our analysis revealed that the data generated by our system with iterative insertion obtained an average score of 1.66 for validity assessment and 1.36 for quality assessment."
  - [corpus] Related work uses similar synthetic generation approaches, though typically for different tasks
- Break condition: If the synthetic errors don't generalize to real LLM outputs or if the insertion process creates unrealistic patterns.

### Mechanism 3
- Claim: Retrieval-augmented editing improves factuality by providing relevant context for verification and correction.
- Mechanism: The system retrieves top Wikipedia documents relevant to the input/output, then uses this context during editing to verify claims and suggest corrections. Different retrieval strategies (top-1 vs top-5, entity matching, reranking) yield different performance.
- Core assumption: Relevant external knowledge improves the model's ability to detect and correct hallucinations, and retrieval provides this knowledge effectively.
- Evidence anchors:
  - [section 5.1] "Our base model is Llama2-Chat 7b trained using 4xA40 GPUs... At inference time, we retrieve the top five documents from Wikipedia."
  - [section 6.3] "Table 5 shows the experimental results. Retrieving the top five documents significantly enhances performance compared to retrieval of only the top one document by 4.5%."
  - [corpus] Retrieval-augmented generation is a well-established approach, though typically for different tasks
- Break condition: If retrieval fails to find relevant documents or if the editing model cannot effectively use the retrieved context.

## Foundational Learning

- Concept: Hierarchical classification of errors
  - Why needed here: The taxonomy creates a structured way to categorize hallucinations at different levels of specificity, enabling both broad categorization and fine-grained analysis
  - Quick check question: Can you explain why the taxonomy separates "Entity" and "Relation" errors as distinct categories rather than treating all factual errors the same?

- Concept: Synthetic data generation for training
  - Why needed here: Manual annotation of hallucination data is expensive, so synthetic generation allows scaling up training data while maintaining quality
  - Quick check question: What are the key steps in the synthetic data generation pipeline, and why is error insertion done one type at a time?

- Concept: Retrieval-augmented generation
  - Why needed here: LLMs need external knowledge to verify claims against real-world facts, and retrieval provides this knowledge in a computationally efficient way
  - Quick check question: How does the retrieval-augmented editing process differ from standard RAG approaches?

## Architecture Onboarding

- Component map:
  - Retriever (Mret) -> Editor (Medit) -> Taxonomy classifier
  - Data generator: Synthetic data creation pipeline using GPT-4/ChatGPT

- Critical path:
  1. User provides input query
  2. LM generates output (external to system)
  3. Retriever finds relevant documents
  4. Editor analyzes output against context
  5. Editor identifies and tags hallucinations
  6. Editor suggests corrections
  7. System returns edited output with annotations

- Design tradeoffs:
  - Smaller model (7B) vs larger proprietary models: Tradeoff between cost/reproducibility and performance
  - Synthetic vs human-annotated data: Tradeoff between scale and authenticity
  - Fixed vs adaptive retrieval: Tradeoff between simplicity and optimal context selection

- Failure signatures:
  - High precision but low recall: System is conservative in detecting hallucinations
  - Low precision but high recall: System over-detects hallucinations (many false positives)
  - Poor performance on invented errors: Retrieval strategy may not capture novel concepts
  - Performance degradation on rare entities: Retrieval system may struggle with uncommon topics

- First 3 experiments:
  1. Evaluate detection performance on each error type separately to identify which types are most challenging
  2. Test different retrieval strategies (top-1, top-5, entity-matching) to optimize context selection
  3. Compare synthetic training data quality against human-annotated data on a small validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of hallucination type distribution on downstream task performance?
- Basis in paper: [inferred] The paper analyzes hallucination distributions across different datasets and models, showing that certain types like "Invented" and "Unverifiable" are more prevalent in some cases.
- Why unresolved: While the paper identifies these distributions, it doesn't explore how they specifically affect the performance of downstream tasks like question answering or summarization.
- What evidence would resolve it: Controlled experiments measuring downstream task performance with different hallucination type distributions would clarify this relationship.

### Open Question 2
- Question: How does the quality of retrieved documents affect hallucination detection and correction?
- Basis in paper: [explicit] The paper mentions that FAVA retrieves top documents and shows that including an entity-matching document improves performance. It also notes that neural retrievers can struggle with rare entities.
- Why unresolved: The paper doesn't thoroughly investigate the relationship between retrieval quality and detection/correction performance, or explore alternative retrieval strategies.
- What evidence would resolve it: Ablation studies varying retrieval quality and methods, and analyzing their impact on FAVA's performance would address this.

### Open Question 3
- Question: Can FAVA's hallucination detection and correction capabilities generalize to languages other than English?
- Basis in paper: [inferred] The paper focuses on English Wikipedia data and English language models. There's no mention of multilingual capabilities.
- Why unresolved: The paper doesn't explore FAVA's performance on non-English text, leaving its generalizability to other languages unknown.
- What evidence would resolve it: Training and evaluating FAVA on multilingual data and comparing its performance across languages would determine its generalizability.

### Open Question 4
- Question: How does FAVA's performance scale with model size and training data?
- Basis in paper: [explicit] The paper shows that FAVA outperforms larger models like ChatGPT and Llama2-Chat despite being smaller. It also presents ablation studies on training data size.
- Why unresolved: While the paper demonstrates FAVA's effectiveness, it doesn't explore the limits of its performance scaling with increased model size or training data.
- What evidence would resolve it: Experiments scaling FAVA's model size and training data, and analyzing the corresponding performance changes would clarify this.

### Open Question 5
- Question: What are the limitations of FAVA's synthetic data generation approach?
- Basis in paper: [explicit] The paper introduces a synthetic data generation pipeline using GPT-4 and ChatGPT to insert hallucinations. It mentions human evaluation of the generated data quality.
- Why unresolved: The paper doesn't thoroughly investigate potential biases or limitations in the synthetic data generation process that could affect FAVA's real-world performance.
- What evidence would resolve it: Analyzing the diversity and representativeness of the synthetic data, and testing FAVA's performance on real-world data with different characteristics would address this.

## Limitations

- The evaluation is conducted primarily on Wikipedia-based knowledge-intensive queries, which may not generalize to other domains or more subjective tasks
- The taxonomy, while comprehensive, may not capture all types of hallucinations that occur in practice, particularly more complex error types like "invented" and "unverifiable" statements
- The use of a 7B parameter model, while practical for reproducibility, achieves lower performance than larger proprietary models like GPT-4

## Confidence

- High Confidence: The claim that FAVA outperforms ChatGPT and GPT-4 on the FavaBench evaluation is well-supported by experimental results. The synthetic data generation approach is technically sound and produces valid training examples. The taxonomy provides a useful framework for categorizing hallucinations, and the retrieval-augmented editing mechanism demonstrably improves factuality when relevant documents are available.

- Medium Confidence: Claims about FAVA's ability to handle diverse hallucination types and achieve significant FActScore improvements are supported by results but may not generalize beyond the evaluated domains. The assertion that the hierarchical taxonomy meaningfully improves detection and editing quality is plausible but requires further validation across diverse use cases.

- Low Confidence: The claim that the six-category taxonomy captures all meaningful types of hallucinations is difficult to verify and may overlook important error categories. The assertion that synthetic data generation fully represents real-world hallucination patterns is also uncertain, as the data depends heavily on the capabilities and biases of the generation models used.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate FAVA on a diverse set of domains beyond Wikipedia-based knowledge queries, including creative writing, code generation, and subjective opinion tasks. Measure performance using domain-specific factuality metrics and qualitative human evaluation to assess whether the taxonomy and detection mechanisms generalize effectively.

2. **Synthetic vs. Real Data Comparison**: Create a small benchmark of human-annotated hallucinations from real LLM outputs across various domains. Compare FAVA's performance when trained on synthetic data versus a hybrid dataset containing both synthetic and human-annotated examples. This will validate whether the synthetic data generation approach captures the full complexity of real-world hallucinations.

3. **Retrieval Effectiveness Analysis**: Conduct ablation studies varying the number of retrieved documents (1, 3, 5, 10) and different retrieval strategies (BM25, dense retrieval, entity matching). Analyze which error types benefit most from additional context and identify scenarios where retrieval fails to provide useful information for hallucination detection and correction.