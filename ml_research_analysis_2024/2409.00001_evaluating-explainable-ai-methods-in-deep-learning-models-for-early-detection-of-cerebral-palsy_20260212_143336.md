---
ver: rpa2
title: Evaluating Explainable AI Methods in Deep Learning Models for Early Detection
  of Cerebral Palsy
arxiv_id: '2409.00001'
source_url: https://arxiv.org/abs/2409.00001
tags:
- ensemble
- grad-cam
- data
- metrics
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the reliability of two Explainable AI (XAI)\
  \ methods\u2014Class Activation Mapping (CAM) and Gradient-weighted Class Activation\
  \ Mapping (Grad-CAM)\u2014in predicting cerebral palsy (CP) from infant movement\
  \ data using deep learning models. The research tests faithfulness and stability\
  \ metrics to assess how well these methods identify important body points and remain\
  \ robust against minor data perturbations."
---

# Evaluating Explainable AI Methods in Deep Learning Models for Early Detection of Cerebral Palsy

## Quick Facts
- arXiv ID: 2409.00001
- Source URL: https://arxiv.org/abs/2409.00001
- Reference count: 0
- Both CAM and Grad-CAM effectively identify key body points influencing CP predictions, with Grad-CAM significantly outperforming CAM in velocity stability and CAM excelling in bone stability and internal representation robustness

## Executive Summary
This study evaluates the reliability of Class Activation Mapping (CAM) and Gradient-weighted Class Activation Mapping (Grad-CAM) for explaining deep learning predictions of cerebral palsy from infant movement data. The research uses an ensemble of 70 Graph Convolutional Network models and assesses both faithfulness (how well explanations identify important features) and stability (how robust explanations are to data perturbations). Results show both methods effectively identify key body points influencing CP predictions, with Grad-CAM significantly outperforming CAM in velocity stability while CAM excels in bone stability and internal representation robustness.

## Method Summary
The study uses an ensemble approach of 70 GCN instances across 10 unique architectures to predict cerebral palsy from infant movement videos. Skeleton tracker data from 557 infants at elevated risk of perinatal brain injury is processed, with 24 random 5-second windows selected from CP data and 136 from No CP data in the test set. CAM and Grad-CAM methods are applied to identify important body points influencing CP predictions, and XAI evaluation metrics are used to assess reliability and robustness against minor data perturbations.

## Key Results
- Both CAM and Grad-CAM effectively identify key body points influencing CP predictions, outperforming random attribution
- Grad-CAM significantly outperforms CAM in the RISv metric (velocity stability) with p < 8.507 × 10^-7
- CAM excels in bone stability (RISb) and internal representation robustness compared to Grad-CAM
- The ensemble approach provides a comprehensive representation of outcomes from its constituent models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Both CAM and Grad-CAM effectively identify important body points influencing CP predictions
- Mechanism: Both methods use activation-based attribution to highlight regions in the input skeleton data that most strongly correlate with the model's CP risk prediction. CAM uses fixed weights from the classification layer, while Grad-CAM uses gradient information to capture which features the model considers important.
- Core assumption: The attribution scores generated by CAM and Grad-CAM accurately reflect the model's internal decision-making process
- Evidence anchors:
  - [abstract] "both XAI methods effectively identify key body points influencing CP predictions"
  - [section] "PGI results indicate that the ensemble's predictions are influenced by key features identified by the two XAI methods"
  - [corpus] No direct evidence in corpus papers about CP prediction, but similar attribution methods work for melanoma and breast cancer classification

### Mechanism 2
- Claim: Grad-CAM outperforms CAM in velocity stability (RISv metric)
- Mechanism: Grad-CAM uses gradient information which captures dynamic relationships between body points and velocity features, making it more stable when velocity components of the skeleton data are perturbed. CAM uses fixed weights that may not adapt well to velocity perturbations.
- Core assumption: Velocity information in skeleton data is crucial for CP prediction and is captured differently by CAM vs Grad-CAM
- Evidence anchors:
  - [abstract] "Grad-CAM significantly outperforms CAM in the RISv metric, which measures stability in terms of velocity"
  - [section] "Grad-CAM offers a highly significant improvement (with p < 8.507 × 10−7) of input stability compared to CAM for RISv of the ensemble model"
  - [corpus] No direct evidence about velocity stability, but corpus shows Grad-CAM used for various medical applications

### Mechanism 3
- Claim: Ensemble approach provides more robust explanations than individual models
- Mechanism: The ensemble aggregates predictions and explanations from multiple model architectures, reducing the impact of individual model biases and providing a more stable, generalizable attribution. The median aggregation smooths out extreme attribution values.
- Core assumption: Individual models in the ensemble have complementary strengths and weaknesses that average out when combined
- Evidence anchors:
  - [abstract] "the ensemble approach providing a representation of outcomes from its constituent models"
  - [section] "Individual models in the ensemble show varied results in metrics tests, while the ensemble provides a combined outcome"

## Foundational Learning

- Concept: Graph Convolutional Networks for skeleton data processing
  - Why needed here: The CP prediction model processes 3D skeleton data extracted from infant movement videos, requiring spatial and temporal relationships between body points to be captured
  - Quick check question: How does a GCN differ from a standard CNN when processing skeleton data, and why is this difference important for CP prediction?

- Concept: XAI evaluation metrics (faithfulness and stability)
  - Why needed here: The study needs quantitative ways to assess whether XAI methods actually identify important features and remain consistent under data perturbations, which is critical for medical applications
  - Quick check question: What is the difference between PGI and PGU metrics, and why are both needed to assess faithfulness?

- Concept: Skeleton data perturbation techniques
  - Why needed here: Stability metrics require controlled perturbations to test whether explanations change when input data is slightly modified, which tests the robustness of the XAI methods
  - Quick check question: Why does the study use spherical coordinate perturbation instead of simple Cartesian coordinate perturbation for skeleton joints?

## Architecture Onboarding

- Component map: Data preprocessing -> 70 GCN models (ensemble) -> Median aggregation -> XAI attribution (CAM/Grad-CAM) -> Metric evaluation
- Critical path: Skeleton data extraction -> CP prediction -> XAI explanation generation -> Metric computation (faithfulness + stability)
- Design tradeoffs: Ensemble provides robustness but increases computation time; CAM is faster but less stable for velocity features; Grad-CAM is more computationally expensive but provides better velocity stability
- Failure signatures: Poor PGI scores indicate XAI methods not identifying important features; high RISv scores for random attribution indicate explanations not stable to velocity perturbations; inconsistent results across individual models suggest ensemble may not be beneficial
- First 3 experiments:
  1. Run CAM and Grad-CAM on a single 5-second window from test set, compare attribution heatmaps visually
  2. Perturb the same window with 1% magnitude, compute PGI and PGU for both methods
  3. Compute RISp, RISv, and RISb for the perturbed window to compare velocity stability specifically

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can domain-specific knowledge be systematically integrated into XAI evaluation metrics to enhance their validity for medical applications beyond CP prediction?
- Basis in paper: [explicit] The authors explicitly state that future work should "introduce more XAI metrics that incorporate domain-specific knowledge to further validate the relevance and accuracy of the explanations provided."
- Why unresolved: The paper only suggests the need for domain-specific metrics but does not propose specific methodologies or frameworks for their integration, leaving a gap in practical implementation guidance.

### Open Question 2
- Question: How do the XAI explanations correlate with specific movement patterns identified by clinical experts using Prechtl's GMA, and can these explanations be used to enhance clinical decision-making?
- Basis in paper: [explicit] The authors suggest that XAI methods "could potentially supplement GMA observers with specialized domain knowledge" and mention the need for "clinical interpretations on the explanations to enhance the practical utility of the models."
- Why unresolved: The study does not perform a direct comparison between XAI explanations and clinical expert assessments, nor does it evaluate the practical impact of XAI explanations on clinical decision-making processes.

### Open Question 3
- Question: What is the optimal strategy for selecting between CAM and Grad-CAM based on the specific requirements of different medical applications, considering their respective strengths in various stability metrics?
- Basis in paper: [explicit] The authors note that "neither CAM nor Grad-CAM consistently outperform the other" and suggest that the choice depends on "the specific application and key metrics," with CAM excelling in bone stability and Grad-CAM in velocity stability.
- Why unresolved: The paper does not provide a decision framework or criteria for choosing between CAM and Grad-CAM in different medical contexts, nor does it explore the implications of these differences in practical scenarios.

## Limitations

- Lack of detailed descriptions of the 70 GCN ensemble architectures, making exact replication challenging
- Stability metric implementation details (perturbation magnitude, distribution) are not fully specified
- The relationship between attribution scores and clinical significance of identified body points remains unvalidated

## Confidence

- **High**: CAM and Grad-CAM can identify body points influencing CP predictions (supported by PGI results)
- **Medium**: Grad-CAM outperforms CAM in velocity stability (statistically significant but requires deeper mechanistic understanding)
- **Medium**: Ensemble provides more robust explanations than individual models (supported by stability metrics but architectural details unclear)

## Next Checks

1. **Mechanistic validation**: Test whether the identified important body points align with known CP biomarkers and clinical understanding of movement disorders
2. **Perturbation sensitivity analysis**: Systematically vary perturbation magnitude and distribution to determine thresholds where XAI method performance degrades
3. **Cross-dataset generalization**: Apply the same XAI evaluation framework to a different infant movement dataset to test method robustness beyond the original study data