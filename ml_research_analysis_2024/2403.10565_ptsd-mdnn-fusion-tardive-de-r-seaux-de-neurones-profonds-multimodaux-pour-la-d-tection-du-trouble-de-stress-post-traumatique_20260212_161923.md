---
ver: rpa2
title: "PTSD-MDNN : Fusion tardive de r\xE9seaux de neurones profonds multimodaux\
  \ pour la d\xE9tection du trouble de stress post-traumatique"
arxiv_id: '2403.10565'
source_url: https://arxiv.org/abs/2403.10565
tags:
- pour
- tspt
- nous
- donn
- stress
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents PTSD-MDNN, a multimodal deep neural network
  that uses video and audio data to detect post-traumatic stress disorder (PTSD) objectively
  and quickly. The method fuses two unimodal convolutional neural networks - a (2+1)D
  ResNet18 for video and an adapted convolutional network for audio MFCC features
  - through late fusion.
---

# PTSD-MDNN : Fusion tardive de réseaux de neurones profonds multimodaux pour la détection du trouble de stress post-traumatique

## Quick Facts
- arXiv ID: 2403.10565
- Source URL: https://arxiv.org/abs/2403.10565
- Authors: Long Nguyen-Phuoc; Renald Gaboriau; Dimitri Delacroix; Laurent Navarro
- Reference count: 0
- One-line primary result: PTSD-MDNN achieves 0.92 accuracy, 0.88 precision, and 0.97 recall using late fusion of video and audio modalities for PTSD detection

## Executive Summary
This paper presents PTSD-MDNN, a multimodal deep neural network that combines video and audio data for objective PTSD detection. The approach uses late fusion of two independently trained unimodal networks: a (2+1)D ResNet18 for video analysis and a CNN for audio MFCC features. By avoiding early fusion, the model mitigates overfitting while capturing PTSD indicators across different modalities. The system demonstrates superior performance compared to unimodal approaches, achieving high accuracy on the PTSD in-the-wild dataset.

## Method Summary
PTSD-MDNN employs a two-stage approach where video and audio data are processed independently before being combined. The video stream uses a (2+1)D ResNet18 architecture to capture spatiotemporal patterns, while the audio stream extracts 13 MFCC coefficients from preprocessed audio using STFT with 64ms windows and 75% overlap. Each unimodal model is trained separately with L2 regularization on the audio network, then their predictions are fused through a two-layer neural network using binary cross-entropy loss. The model is trained on an 80/10/10% train/validation/test split with Adam optimizer and 0.001 learning rate.

## Key Results
- Achieves 0.92 accuracy, 0.88 precision, and 0.97 recall on the PTSD in-the-wild dataset
- Outperforms unimodal models by avoiding overfitting issues through independent modality training
- Successfully detects PTSD using multimodal data without requiring clinical interviews

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Late fusion of video and audio modalities improves PTSD detection accuracy compared to unimodal approaches.
- Mechanism: By training video and audio classifiers independently and fusing their predictions at a late stage, the model avoids overfitting issues that can occur when modalities are fused early. This allows each modality to specialize and capture modality-specific PTSD indicators without being constrained by the other.
- Core assumption: Different modalities exhibit different rates of overfitting and generalization, making independent training more optimal than joint training.
- Evidence anchors:
  - [abstract] "This approach improves over unimodal models, achieving an accuracy of 0.92, precision of 0.88, and recall of 0.97"
  - [section] "We have chosen for our model a late fusion, called 'decision-oriented fusion'. Indeed, [20] has shown that: (1) a better unimodal network can outperform a multimodal network against the problem of overfitting; (2) different modalities overfit and generalize at different rates, so training them jointly with a single optimization strategy is not optimal."
  - [corpus] Weak evidence - the corpus contains unrelated papers on various topics without direct relevance to late fusion in PTSD detection.

### Mechanism 2
- Claim: The (2+1)D ResNet18 architecture effectively captures spatiotemporal features in video data for PTSD detection.
- Mechanism: The (2+1)D convolution factorizes the 3D convolution into separate spatial and temporal components, reducing the number of parameters while maintaining the ability to capture both spatial and temporal patterns in video data that may indicate PTSD symptoms.
- Core assumption: PTSD symptoms manifest through both spatial (facial expressions, body language) and temporal (changes over time, speech patterns) patterns that can be captured by convolutional networks.
- Evidence anchors:
  - [section] "The video classification sub-model uses a convolutional neural network (2+1)D [19] with residual connections 18 layers deep (ResNet18). The (2+1)D convolution allows the decomposition of spatial and temporal dimensions, creating two distinct steps. An advantage of this approach is that factorizing convolutions into spatial and temporal dimensions allows for a reduction in the number of parameters compared to full 3D convolution."
  - [section] "The spatial convolution takes data in the form (1, width, height), while the temporal convolution takes data in the form (time, 1, 1)"
  - [corpus] No direct evidence in corpus - the related papers focus on different domains and architectures.

### Mechanism 3
- Claim: MFCC features extracted from audio data capture PTSD-relevant vocal characteristics.
- Mechanism: The audio preprocessing pipeline converts raw audio to MFCC features using STFT with longer windows (64ms) and 75% overlap, followed by Mel-scale conversion and DCT. This process extracts vocal characteristics that may correlate with PTSD symptoms such as voice modulation, speech patterns, and emotional tone.
- Core assumption: Vocal characteristics and speech patterns contain detectable indicators of PTSD that can be captured through MFCC analysis.
- Evidence anchors:
  - [section] "We adapted the [9] model based on short-term Fourier transform (STFT) with longer windows (64 ms) overlapping at 75%, which gives better frequency resolution for human voice. We then converted the frequencies of the spectrogram to the logarithmic Mel scale which approximates human perception of sound. Finally, the discrete cosine transform of type II (DCT) gives the Mel-Frequency Cepstral Coefficients (MFCC) by 80 triangular filters created to cover the Mel frequency range. We select only the first 13 which are useful for speech recognition [3]."
  - [section] "The audio classification is trained from these MFCCs using a convolutional neural network illustrated in Fig 1."
  - [corpus] No direct evidence in corpus - the related papers do not address audio feature extraction for PTSD detection.

## Foundational Learning

- Concept: Convolutional Neural Networks for spatiotemporal feature extraction
  - Why needed here: The model needs to extract meaningful features from both video (spatial + temporal) and audio (spectral) data to detect PTSD indicators
  - Quick check question: How does a (2+1)D convolution differ from a standard 3D convolution, and why might this be advantageous for video analysis?

- Concept: Late vs. early fusion in multimodal learning
  - Why needed here: Understanding when to combine information from different modalities is crucial for optimizing model performance and avoiding overfitting
  - Quick check question: What are the key differences between late fusion and early fusion approaches, and in what scenarios might each be preferable?

- Concept: Feature extraction from audio signals (MFCC, STFT)
  - Why needed here: Converting raw audio into meaningful features that capture vocal characteristics relevant to PTSD detection
  - Quick check question: What information is captured by MFCC features that might be relevant for detecting emotional or psychological states?

## Architecture Onboarding

- Component map: Video Input → (2+1)D ResNet18 → Video Features → Concatenation; Audio Input → MFCC Extraction → 2D CNN → Audio Features → Concatenation; Concatenated Features → Dense Layers → PTSD Classification Output

- Critical path: Raw video/audio → Preprocessing → Independent unimodal training → Late fusion → Classification
- Design tradeoffs:
  - Independent training allows parallelization but requires careful synchronization of model updates
  - Late fusion provides flexibility in choosing modality-specific architectures but may lose some cross-modal interactions
  - MFCC extraction simplifies audio representation but may lose fine-grained temporal information

- Failure signatures:
  - Poor unimodal performance indicates issues with feature extraction or modality-specific patterns
  - High variance between training and validation suggests overfitting, requiring regularization or data augmentation
  - Low performance in one modality may indicate need for different feature extraction or model architecture

- First 3 experiments:
  1. Train unimodal video classifier only and evaluate performance to establish baseline
  2. Train unimodal audio classifier only and compare with video baseline to identify stronger modality
  3. Implement late fusion with both modalities and compare against unimodal baselines to verify improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of PTSD-MDNN change if high-level behavioral features like body movements and facial expressions were extracted and incorporated alongside the current low-level pixel and MFCC features?
- Basis in paper: [explicit] The authors mention in the conclusion that future work could involve extracting high-level behavioral features from body movements, facial expressions, and speech prosody.
- Why unresolved: The current model only uses raw video and audio data at a low level, without explicitly extracting or analyzing behavioral cues that might be more directly indicative of PTSD symptoms.
- What evidence would resolve it: A comparative study showing performance metrics (accuracy, precision, recall) of PTSD-MDNN with and without the inclusion of explicitly extracted behavioral features would determine the impact of such features on detection performance.

### Open Question 2
- Question: How would the model's performance be affected if a different fusion strategy, such as early fusion or hybrid fusion, were employed instead of the current late fusion approach?
- Basis in paper: [inferred] The authors chose late fusion and mention that different modalities overfit and generalize at different rates, suggesting potential benefits from alternative fusion strategies.
- Why unresolved: The paper only evaluates late fusion and does not explore how other fusion methods might impact the model's ability to detect PTSD, leaving open the question of whether a different strategy could yield better results.
- What evidence would resolve it: Experiments comparing the performance of PTSD-MDNN using late fusion, early fusion, and hybrid fusion strategies on the same dataset would provide insights into the optimal fusion method for this application.

### Open Question 3
- Question: What is the impact of the significant variability in video file lengths (ranging from 35 seconds to 44 minutes) on the model's ability to accurately detect PTSD?
- Basis in paper: [explicit] The authors note that the difference in file lengths in the PTSD in-the-wild dataset can create difficulties for feature extraction in the convolutional model.
- Why unresolved: The paper does not address how the model handles or is affected by the wide range of video lengths, which could influence the consistency and reliability of PTSD detection.
- What evidence would resolve it: An analysis showing how PTSD-MDNN's performance metrics vary with different video lengths would clarify the impact of file length variability on detection accuracy and reliability.

## Limitations

- The exact video preprocessing parameters (resolution, frame rate) are not specified, limiting faithful reproduction
- Audio CNN architecture details beyond basic filter counts are unclear, including dropout rates and pooling configurations
- The PTSD in-the-wild dataset description lacks details about video duration, frame extraction methods, and audio sampling rates

## Confidence

- **High confidence**: The late fusion approach and its theoretical justification based on modality-specific overfitting patterns
- **Medium confidence**: The MFCC extraction parameters and overall architecture combining (2+1)D ResNet18 for video and MFCC-based CNN for audio
- **Low confidence**: Complete reproducibility without access to exact preprocessing pipeline and audio CNN architecture details

## Next Checks

1. Reproduce unimodal baselines by implementing and evaluating the video-only and audio-only models independently to establish baseline performance

2. Validate MFCC extraction pipeline by testing audio preprocessing with different STFT window sizes and overlaps to confirm optimal frequency resolution for PTSD-related vocal characteristics

3. Analyze modality contribution through ablation studies by training with video-only, audio-only, and fused modalities to quantify the exact performance gain from multimodal fusion