---
ver: rpa2
title: On the Effect of (Near) Duplicate Subwords in Language Modelling
arxiv_id: '2404.06508'
source_url: https://arxiv.org/abs/2404.06508
tags:
- subwords
- duplicates
- near
- which
- duplicated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigates how near-duplicate subwords in language\
  \ model vocabularies affect training efficiency and performance. By duplicating\
  \ subwords in a controlled experiment, researchers establish that models can generalize\
  \ across perfect duplicates but incur a performance cost\u2014about 17% less data\
  \ efficiency when vocabulary is fully duplicated."
---

# On the Effect of (Near) Duplicate Subwords in Language Modelling

## Quick Facts
- arXiv ID: 2404.06508
- Source URL: https://arxiv.org/abs/2404.06508
- Authors: Anton Schäfer; Thomas Hofmann; Imanol Schlag; Tiago Pimentel
- Reference count: 10
- Key outcome: Models generalize across perfect duplicate subwords through high embedding cosine similarity, but incur ~17% data efficiency cost; naturally occurring near-duplicates carry meaningful semantic distinctions that merging hurts performance.

## Executive Summary
This paper investigates how near-duplicate subwords in language model vocabularies affect training efficiency and performance. Through controlled experiments with synthetic vocabulary duplication, the authors establish that models can generalize across perfect duplicates via high cosine similarity in embeddings, particularly for frequent subwords. However, this generalization comes at a performance cost of approximately 17% less data efficiency. When tested on naturally occurring near-duplicates like "Now" vs "now", merging them hurts model performance, suggesting these carry meaningful semantic distinctions. The study reveals that while synthetic duplication provides insights into generalization mechanisms, real near-duplicates are less interchangeable than their synthetic counterparts.

## Method Summary
The authors create synthetic vocabularies by duplicating each subword in the original BPE vocabulary (e.g., 16k → 32k) and train GPT-style transformer models on these duplicated vocabularies. They evaluate models using projected perplexity and analyze embedding cosine similarities for duplicate pairs. The study also tests on naturally occurring near-duplicates identified through a deduplication mapping (Sall), comparing performance when these are merged versus kept separate. Models range from 100M parameters, with experiments conducted on filtered books3 data from The Pile and evaluated on a 11M token Languini test set.

## Key Results
- Perfect duplicate subwords achieve average cosine similarity of ~0.8 in embeddings, enabling generalization across duplicates
- Naturally occurring near-duplicates show much lower similarity (~0.4), indicating meaningful semantic distinctions
- Merging natural near-duplicates (e.g., "Now"→"now") hurts performance by 3.0% on average
- A shared learned embedding mitigates but doesn't fully recover performance losses from deduplication
- Infrequent subwords suffer more from duplication due to fewer gradient updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models generalize across perfect duplicate subwords through high cosine similarity in embeddings, especially for frequent subwords.
- Core assumption: Contexts of duplicate subwords follow similar distributions, causing similar gradient signals.
- Evidence: Average cosine similarity of ~0.8 among duplicate pairs, higher for frequent subwords.
- Break condition: If duplicate subwords appear in significantly different contexts, gradient signals diverge.

### Mechanism 2
- Claim: Merging naturally occurring near-duplicates hurts performance because they carry meaningful semantic distinctions.
- Core assumption: Semantic differences between near-duplicates are significant enough to impact predictions.
- Evidence: Natural duplicates exhibit average cosine similarity of ~0.4, much lower than perfect duplicates (0.8).
- Break condition: If near-duplicates are truly semantically equivalent, merging wouldn't hurt performance.

### Mechanism 3
- Claim: Infrequent subwords are predicted worse when duplicated due to fewer gradient updates.
- Core assumption: Number of gradient updates directly affects embedding quality.
- Evidence: Frequent subwords show stronger embedding alignment, implying more effective updates.
- Break condition: If all subwords receive sufficient training regardless of frequency, duplication penalty disappears.

## Foundational Learning

- Cross-entropy loss and perplexity: Understanding how cross-entropy relates to model accuracy is crucial for interpreting results. Quick check: If cross-entropy decreases from 3.0 to 2.5, perplexity decreases from exp(3.0) to exp(2.5).
- Subword tokenization and BPE: The study investigates duplicate subwords, requiring understanding of how text is split into subword units. Quick check: "unbelievable" tokenized as ["un", "believ", "able"] contains 3 subword tokens.
- Cosine similarity: The paper uses cosine similarity to explain generalization across duplicates. Quick check: Cosine similarity ranges from -1 (opposite) to 1 (identical), with 0 indicating orthogonality.

## Architecture Onboarding

- Component map: Tokenizer -> Embedding layer -> Transformer decoder -> Output projection -> Loss function
- Critical path: Tokenize input → Look up embeddings → Process through transformer layers → Project to vocabulary space → Compute softmax probabilities → Calculate cross-entropy loss
- Design tradeoffs: Vocabulary size vs. model capacity; perfect duplicates vs. natural near-duplicates; character-level vs. subword-level modeling
- Failure signatures: High perplexity on duplicated tokens suggests embedding misalignment; performance degradation proportional to duplication rate indicates generalization failure; infrequent tokens suffering more points to update frequency issues
- First 3 experiments: 1) Duplicate 10% vocabulary and measure performance impact; 2) Vary frequency of duplicated subwords and measure cosine similarity changes; 3) Implement shared embedding for near-duplicates and measure performance improvement

## Open Questions the Paper Calls Out

### Open Question 1
How do findings about subword duplication generalize to models significantly larger than 100M parameters? The authors acknowledge uncertainty about scalability to modern LLMs with billions or trillions of parameters.

### Open Question 2
What is the impact of subword duplication on model performance when trained on languages other than English? The experiments are limited to English text, and linguistic properties may vary across languages.

### Open Question 3
Can more sophisticated methods than a simple shared embedding be developed to account for semantic differences between naturally occurring near-duplicates? The study shows a shared embedding mitigates but doesn't fully recover performance losses.

## Limitations
- Limited real-world applicability due to synthetic duplication experiments
- Vocabulary size constraints may not scale to modern LLM sizes
- Single model architecture (GPT-style transformers) may not generalize to other architectures
- Training data limitations from filtered books3 subset

## Confidence

- **High confidence**: Core finding that perfect duplicates can be generalized across through embedding alignment (supported by direct cosine similarity measurements)
- **Medium confidence**: Mechanism that frequent subwords generalize better due to more gradient updates (inferred from correlation rather than direct measurement)
- **Medium confidence**: Claim that merging natural near-duplicates hurts performance (demonstrated on specific examples but may not generalize)

## Next Checks

1. Replicate embedding alignment analysis with larger vocabularies (50k-100k subwords) to verify if generalization mechanism scales to modern LLM sizes.

2. Test merging strategy on broader range of natural near-duplicates including morphological variants, semantic near-synonyms, and cross-linguistic duplicates to establish generalizability.

3. Implement and evaluate alternative deduplication strategies such as frequency-based merging thresholds or context-aware merging to determine if performance can be improved beyond simple shared embedding approach.