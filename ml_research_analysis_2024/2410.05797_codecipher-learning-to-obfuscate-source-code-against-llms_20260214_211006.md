---
ver: rpa2
title: 'CodeCipher: Learning to Obfuscate Source Code Against LLMs'
arxiv_id: '2410.05797'
source_url: https://arxiv.org/abs/2410.05797
tags:
- code
- obfuscation
- embedding
- codecipher
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CodeCipher is a novel method that perturbs privacy from source\
  \ code while preserving the original response from LLMs. It transforms the LLM\u2019\
  s embedding matrix so that each row corresponds to a different word in the original\
  \ matrix, forming a token-to-token confusion mapping for obfuscating source code."
---

# CodeCipher: Learning to Obfuscate Source Code Against LLMs

## Quick Facts
- arXiv ID: 2410.05797
- Source URL: https://arxiv.org/abs/2410.05797
- Reference count: 15
- Key outcome: CodeCipher achieves 47.56% Pass@1 in code completion while increasing perplexity by 71%

## Executive Summary
CodeCipher introduces a novel approach to protect source code privacy against large language models by learning to obfuscate code while preserving the original LLM performance. The method transforms the LLM's embedding matrix through permutation to create a token-to-token confusion mapping, then optimizes this mapping using a discrete gradient search algorithm that projects updates to the nearest valid tokens. Experiments on three AI-assisted coding tasks demonstrate that CodeCipher successfully confuses privacy in source code while maintaining task performance with only minimal degradation.

## Method Summary
CodeCipher operates by first permuting the embedding matrix of a target LLM to create a confusion mapping between tokens, then optimizing this mapping through discrete gradient descent. The optimization minimizes a task-specific loss function while ensuring that updated vectors remain aligned with valid tokens in the vocabulary. This approach addresses the discrete and sparse nature of word embedding spaces by projecting each gradient update to the nearest valid token. The method is trained on white-box LLMs where embedding layers and task gradients are accessible, but the resulting obfuscated code can be transferred to different model sizes and even black-box models.

## Key Results
- Achieves 47.56% Pass@1 in code completion, only 3% lower than original performance
- Increases perplexity by 71% while preserving task performance
- Successfully transfers obfuscated code to different LLM architectures including black-box models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CodeCipher transforms the LLM's embedding matrix so that each row corresponds to a different word in the original matrix, creating a token-to-token confusion mapping for obfuscating source code.
- Mechanism: By permuting the embedding matrix E to create E', where each row in E' represents the perturbed word vector corresponding to the original word vector in the same row of E, CodeCipher establishes a confusion mapping between tokens. When a token is encountered, its embedding is perturbed using this mapping and then decoded to the nearest valid token in the vocabulary.
- Core assumption: The semantic relationships captured in the embedding space remain meaningful even when the matrix is permuted row-wise, allowing the transformed embeddings to still produce valid tokens after decoding.
- Evidence anchors:
  - [abstract] "CodeCipher transforms the LLM's embedding matrix so that each row corresponds to a different word in the original matrix, forming a token-to-token confusion mapping for obfuscating source code."
  - [section] "For any code snippet x = (w1, ..., wn), the embedding layer of an LLM converts the tokens into a sequence of continuous vectors: e1, ..., eT = E(w1), ..., E(wT) where E ∈ R|V|×d represents the embedding matrix... To perturb the code tokens, we create a learnable confusion mapping within their word embedding space... The matrix E' is a permutation of E, meaning that ∀e' ∈ E', e' ∈ E."
- Break condition: If the permutation disrupts the semantic relationships in the embedding space to the point where decoding no longer produces meaningful or valid tokens, the obfuscation would fail to preserve LLM performance.

### Mechanism 2
- Claim: CodeCipher uses a discrete optimization strategy with gradient updates followed by projection to the nearest valid token, which addresses the discrete and sparse nature of word vector spaces.
- Mechanism: Instead of direct gradient descent, CodeCipher performs iterative updates where each gradient step is followed by projecting the updated vector onto the nearest valid token in the vocabulary. This ensures that the perturbation remains within the discrete manifold of valid token embeddings.
- Core assumption: The discrete gradient search algorithm can find a trajectory through the embedding space that minimizes the task-specific loss while keeping perturbations aligned with valid tokens.
- Evidence anchors:
  - [abstract] "To tackle the challenge of the discrete and sparse nature of word vector spaces, CodeCipher adopts a discrete optimization strategy that aligns the updated vector to the nearest valid token in the vocabulary before each gradient update."
  - [section] "Due to the discrete and sparse nature of word embedding space, straightforward gradient descent can not pinpoint the embedding of a valid token... Inspired from Yuan et al. (2021), we develop a discrete gradient search algorithm... At each mini step, the embedding vector is incrementally perturbed and then projected onto the nearest valid token vector in the vocabulary."
- Break condition: If the projection step frequently maps to tokens that are semantically unrelated to the original, the obfuscation may become too aggressive and degrade LLM performance significantly.

### Mechanism 3
- Claim: CodeCipher's obfuscated code can be transferred to different LLM architectures and even black-box models while maintaining performance.
- Mechanism: The token-to-token confusion mapping learned by CodeCipher captures common patterns in token relationships that are portable across different LLMs. When the obfuscated code is input to a new model, the same token substitutions apply, and the new model's embedding space can still decode these to meaningful tokens.
- Core assumption: Different LLMs share enough similarity in their token embeddings and semantic relationships that a confusion mapping learned on one model will generalize to others.
- Evidence anchors:
  - [abstract] "We will show in Section 5.6 that the obfuscated code generated by CodeLlama-7B can be transferred to a wide range of model specifications including different sizes and black-box LLMs."
  - [section] "Our approach is developed on white-box LLMs where the embedding layers and task gradients are accessible. In practice, there is always a need to apply the model in a new LLM... We used the obfuscated code generated by CodeLlama-7B with a perturbed embedding layer to evaluate the performance on other models."
- Break condition: If the embedding spaces of different LLMs are too dissimilar, the transferred obfuscation may result in tokens that are decoded to completely different meanings, breaking functionality.

## Foundational Learning

- Concept: Word embeddings and their semantic properties
  - Why needed here: CodeCipher operates directly on the embedding matrix of LLMs, so understanding how embeddings capture semantic relationships between tokens is crucial for grasping how the permutation and perturbation work.
  - Quick check question: Why does permuting rows in an embedding matrix create a "confusion mapping" between tokens?

- Concept: Gradient-based optimization in discrete spaces
  - Why needed here: CodeCipher uses a discrete optimization strategy to navigate the sparse embedding space, which differs from standard continuous optimization techniques.
  - Quick check question: What challenge does discrete optimization address that standard gradient descent cannot handle in the context of word embeddings?

- Concept: Transferability in machine learning models
  - Why needed here: The paper demonstrates that CodeCipher's obfuscation transfers across different LLM architectures, which requires understanding when and why learned patterns generalize across models.
  - Quick check question: What property of embedding spaces might explain why obfuscation learned on one LLM works on another?

## Architecture Onboarding

- Component map: Embedding layer (frozen) -> Learnable perturbed embedding matrix E' -> Discrete gradient search algorithm -> Decoding function -> Task-specific loss function

- Critical path:
  1. Initialize E' as a permutation of original embedding matrix E
  2. For each training sample, obfuscate code using current E'
  3. Compute task-specific loss on obfuscated code
  4. Update E' using discrete gradient search with projection to nearest valid tokens
  5. Repeat until convergence or maximum iterations reached

- Design tradeoffs:
  - Performance preservation vs. obfuscation degree: Higher obfuscation typically degrades performance
  - Discrete vs. continuous optimization: Discrete is more complex but necessary for valid tokens
  - White-box vs. black-box training: White-box allows gradient optimization but limits practical deployment

- Failure signatures:
  - Rapid degradation in task-specific metrics (Pass@1, BLEU scores)
  - Code becoming uncompilable after obfuscation
  - Edit distance between original and obfuscated code becoming too large
  - Perplexity increasing beyond acceptable thresholds

- First 3 experiments:
  1. Implement CodeCipher on a small toy embedding matrix and verify that permutation creates confusion mapping
  2. Test discrete gradient search on a simple 2D embedding space to visualize projection behavior
  3. Evaluate transferability by obfuscating code with one small LLM and testing on another model variant

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several important questions emerge regarding the generalizability and robustness of the approach.

## Limitations
- Model dependency constraints: Performance relies heavily on similarity between embedding spaces of different LLMs
- Discrete optimization challenges: Computational overhead and potential local minima issues
- Limited evaluation scope: Focuses on three coding tasks without testing against adaptive attacks

## Confidence
**High Confidence**: The core mechanism of permuting embedding matrices to create token confusion mappings is well-grounded in established NLP principles. The experimental results showing preservation of LLM performance (3% drop in Pass@1 for code completion) while increasing perplexity (71% increase) are reproducible and align with the theoretical framework.

**Medium Confidence**: The transferability claims across different model sizes (7B, 13B, 34B parameters) are supported by experiments, but the underlying reasons for why certain mappings transfer better than others remain unclear. The relationship between model architecture, vocabulary size, and transferability success rates needs further investigation.

**Low Confidence**: The practical effectiveness against real-world adversaries is not thoroughly evaluated. The paper does not address potential countermeasures such as fine-tuning on obfuscated code, semantic analysis to detect patterns, or attacks that exploit the statistical properties of token substitutions.

## Next Checks
1. **Cross-Architecture Transferability Test**: Implement CodeCipher on a non-transformer LLM (such as an RNN-based model) and evaluate whether the obfuscation mappings transfer successfully. Measure performance degradation and perplexity changes to quantify architecture-specific limitations.

2. **Adversarial Robustness Evaluation**: Design an adaptive attack that leverages frequency analysis of token substitutions or semantic clustering to partially deobfuscate CodeCipher-protected code. Measure the attack's success rate and compare against baseline obfuscation methods.

3. **Scalability Analysis**: Systematically vary the embedding dimension size (e.g., 128, 256, 512, 1024) and measure how optimization time, convergence quality, and transferability performance change. Identify at what dimensionality the discrete optimization becomes computationally prohibitive.