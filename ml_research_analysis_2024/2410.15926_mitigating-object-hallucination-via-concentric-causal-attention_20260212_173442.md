---
ver: rpa2
title: Mitigating Object Hallucination via Concentric Causal Attention
arxiv_id: '2410.15926'
source_url: https://arxiv.org/abs/2410.15926
tags:
- arxiv
- visual
- tokens
- object
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large vision-language models suffer from object hallucination,
  generating responses not aligned with image inputs. The paper identifies that rotary
  position encoding (RoPE), used for modeling positional dependencies in existing
  LVLMs, contributes to this issue through long-term decay, weakening visual-instruction
  interactions at long distances.
---

# Mitigating Object Hallucination via Concentric Causal Attention

## Quick Facts
- arXiv ID: 2410.15926
- Source URL: https://arxiv.org/abs/2410.15926
- Authors: Yun Xing; Yiheng Li; Ivan Laptev; Shijian Lu
- Reference count: 40
- Key outcome: CCA achieves +4.24% accuracy and +2.73% F1 score on POPE hallucination benchmark

## Executive Summary
Large vision-language models suffer from object hallucination, generating responses not aligned with image inputs. This paper identifies that rotary position encoding (RoPE), used for modeling positional dependencies in existing LVLMs, contributes to this issue through long-term decay, weakening visual-instruction interactions at long distances. To address this, the authors propose Concentric Causal Attention (CCA), a positional alignment strategy that reorganizes visual tokens concentrically (from periphery to center) to reduce relative distances to instruction tokens, and modifies causal masking to model 2D spatial locality. This mitigates the impact of RoPE's long-term decay. CCA achieves significant improvements over state-of-the-art methods on hallucination benchmarks and enhances general perception capability across six multimodal benchmarks.

## Method Summary
The method proposes Concentric Causal Attention (CCA) to mitigate object hallucination in LVLMs caused by RoPE long-term decay. The approach uses a two-stage training process: pre-training on CC-558K dataset followed by instruction tuning on a 665k multi-turn conversation dataset. CCA consists of two modules: a position reorganization module that arranges visual tokens concentrically from periphery to center, and a concentric causal masking module that models 2D spatial locality instead of 1D sequential causality. The method uses CLIP ViT-L/14 as vision encoder and Vicuna-7B as LLM, with evaluation on hallucination benchmarks (POPE, CHAIR) and general perception capability (MME, LLaVA-Bench).

## Key Results
- +4.24% accuracy and +2.73% F1 score improvement on POPE hallucination benchmark
- Significant improvements across six multimodal benchmarks including MME and LLaVA-Bench
- Enhanced general perception capability while reducing object hallucination

## Why This Works (Mechanism)

### Mechanism 1
RoPE long-term decay weakens visual-instruction interactions at long distances, causing object hallucination. RoPE encodes position information via rotation matrices that introduce relative position dependency. As the relative distance j-i increases, the attention weight ai,j decays exponentially, attenuating information flow from distant visual tokens to instruction tokens. Core assumption: The decay is detrimental for multimodal alignment where visual tokens should maintain strong connections with instruction tokens regardless of their relative distance.

### Mechanism 2
Concentric positional reorganization reduces relative distances between visual and instruction tokens. Instead of raster-scan ordering, visual tokens are arranged concentrically from periphery to center. This geometric reorganization ensures that visual tokens are closer (in sequence distance) to instruction tokens, reducing the impact of RoPE decay. Core assumption: Reducing sequence distance between visual and instruction tokens maintains stronger attention weights despite RoPE's decay properties.

### Mechanism 3
Concentric causal masking models 2D spatial locality instead of 1D sequential causality. Modified causal attention mask ensures that visual tokens attend to preceding tokens in 2D space (center attends to periphery) rather than in 1D raster order. This maintains spatial coherence in visual feature processing. Core assumption: 2D spatial continuity in visual features is more important than 1D sequential causality for image understanding tasks.

## Foundational Learning

- Concept: Rotary Position Encoding (RoPE) and its decay properties
  - Why needed here: Understanding why RoPE causes problems is essential before implementing the fix. The decay mechanism is the root cause being addressed.
  - Quick check question: How does RoPE encode position information and why does it cause long-term decay in attention weights?

- Concept: Causal attention masking in Transformers
  - Why needed here: CCA modifies the standard causal mask to work with 2D spatial organization. Understanding the default behavior is necessary to implement the modification correctly.
  - Quick check question: What is the difference between standard causal attention and the concentric causal attention proposed in CCA?

- Concept: Visual token reorganization strategies
  - Why needed here: The concentric reorganization is the core innovation. Understanding alternative strategies (like raster-scan vs reverse raster-scan) helps appreciate why concentric is superior.
  - Quick check question: How does concentric reorganization differ from raster-scan ordering in terms of relative distances to instruction tokens?

## Architecture Onboarding

- Component map: Vision Encoder (CLIP ViT-L/14) → Projector (2-layer MLP) → LLM (Vicuna-7B) → CCA modifications
- Critical path: Image → Vision Encoder → Visual features (V tokens) → Projector → Visual tokens in LLM space → CCA concentric reorganization → CCA concentric causal mask application → Concatenate with instruction tokens → LLM processing with modified attention
- Design tradeoffs: Spatial locality preservation vs. computational overhead of reorganization; 2D masking complexity vs. 1D masking simplicity; Concentric ordering may be less intuitive for certain visual tasks
- Failure signatures: Increased hallucination on tasks requiring strict 1D processing; Degraded performance when visual content is uniformly distributed; Computational overhead during projection layer
- First 3 experiments: 1) Compare baseline vs. CCA on POPE benchmark to verify hallucination reduction; 2) Test spatial locality preservation by evaluating on tasks requiring 2D understanding; 3) Benchmark computational overhead vs. accuracy gains on representative datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CCA perform when applied to non-image modalities like audio or video inputs?
- Basis in paper: [inferred] 
- Why unresolved: The paper explicitly states this as a limitation in the conclusion, noting that their focus is only on image-text inputs and suggesting this as future work.
- What evidence would resolve it: Experiments applying CCA to audio-visual or video-language models with performance comparisons to baseline models.

### Open Question 2
- Question: What is the computational overhead of CCA during both training and inference compared to standard LVLM architectures?
- Basis in paper: [inferred]
- Why unresolved: While the paper mentions they used 4 NVIDIA RTX A6000s for training, they don't provide detailed computational complexity analysis or inference-time overhead measurements.
- What evidence would resolve it: Detailed FLOPs analysis, memory usage comparisons, and wall-clock time measurements for both training and inference phases.

### Open Question 3
- Question: How does the concentric positional encoding strategy affect model performance on tasks requiring precise spatial localization versus those requiring global scene understanding?
- Basis in paper: [explicit]
- Why unresolved: The paper shows improved general perception capability but doesn't analyze the trade-offs between local spatial accuracy and global understanding that might arise from the concentric reordering.
- What evidence would resolve it: Ablation studies comparing performance on spatial localization tasks (like object detection) versus global reasoning tasks (like visual question answering) with and without CCA.

## Limitations

- The paper focuses only on image-text inputs and doesn't explore applicability to other modalities like audio or video
- Computational overhead analysis is incomplete, lacking detailed measurements of training and inference costs
- The universal applicability of 2D spatial locality over 1D sequential causality is assumed but not thoroughly validated across diverse task types

## Confidence

- **High Confidence**: Experimental results demonstrating CCA's effectiveness on hallucination benchmarks (POPE accuracy improvement of 4.24%, F1 score improvement of 2.73%)
- **Medium Confidence**: Core hypothesis that RoPE long-term decay causes object hallucination through weakened visual-instruction interactions
- **Medium Confidence**: Concentric reorganization strategy's effectiveness in reducing relative distances
- **Low Confidence**: Universal applicability of 2D spatial locality modeling over 1D sequential causality

## Next Checks

1. **Ablation Study on RoPE Decay Thresholds**: Conduct experiments varying the relative distance thresholds at which RoPE decay becomes significant, and measure corresponding hallucination rates. This would establish the precise quantitative relationship between decay magnitude and hallucination severity.

2. **Cross-Domain Performance Analysis**: Test CCA on diverse image types including text-heavy documents, medical images, and architectural plans where spatial relationships differ significantly from natural images. This would validate whether concentric ordering consistently improves performance or introduces domain-specific artifacts.

3. **Computational Overhead Benchmarking**: Measure the actual computational overhead introduced by concentric reorganization and 2D masking during inference, comparing it against the accuracy gains across different batch sizes and sequence lengths. This would quantify the practical tradeoffs of implementing CCA.