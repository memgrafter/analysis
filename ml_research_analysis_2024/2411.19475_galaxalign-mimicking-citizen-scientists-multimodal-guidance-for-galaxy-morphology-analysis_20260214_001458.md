---
ver: rpa2
title: 'GalaxAlign: Mimicking Citizen Scientists'' Multimodal Guidance for Galaxy
  Morphology Analysis'
arxiv_id: '2411.19475'
source_url: https://arxiv.org/abs/2411.19475
tags:
- galaxy
- images
- astronomical
- galaxalign
- morphology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'GalaxAlign is a tri-modal framework that adapts general pre-trained
  vision models for galaxy morphology analysis by aligning galaxy images, schematic
  symbols, and textual descriptions. Inspired by citizen scientists'' annotation strategies,
  it uses a two-stage fine-tuning approach: first learning shared visual-symbolic
  representations, then specializing encoders for each modality.'
---

# GalaxAlign: Mimicking Citizen Scientists' Multimodal Guidance for Galaxy Morphology Analysis

## Quick Facts
- arXiv ID: 2411.19475
- Source URL: https://arxiv.org/abs/2411.19475
- Reference count: 40
- GalaxAlign achieves state-of-the-art classification accuracy (97.32% on Galaxy10) and mean average precision (0.9645) using tri-modal alignment

## Executive Summary
GalaxAlign is a tri-modal framework that adapts general pre-trained vision models for galaxy morphology analysis by aligning galaxy images, schematic symbols, and textual descriptions. Inspired by citizen scientists' annotation strategies, it uses a two-stage fine-tuning approach: first learning shared visual-symbolic representations, then specializing encoders for each modality. Experiments on Galaxy10 and GalaxyMNIST datasets show GalaxAlign outperforms models pretrained on large astronomical datasets, achieving classification accuracy up to 97.32% on Galaxy10 and mean average precision up to 0.9645 for similarity search.

## Method Summary
GalaxAlign employs a two-stage fine-tuning process that bridges general vision models with astronomical tasks. In Stage 1, galaxy images and schematic symbols are processed by a shared image encoder while textual descriptions are handled by a separate text encoder, learning common morphological patterns. Stage 2 copies these learned parameters to initialize modality-specific encoders, allowing each to specialize while maintaining foundational understanding. The framework uses contrastive loss functions to align visual, symbolic, and textual representations into a unified embedding space, eliminating the need for expensive astronomical pretraining.

## Key Results
- Achieves 97.32% classification accuracy on Galaxy10 dataset
- Reaches mean average precision of 0.9645 for similarity search
- Outperforms models pretrained on large astronomical datasets including MAE, DINOv2, CLIP, and Zoobot

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage fine-tuning process effectively bridges general vision models and astronomical tasks by first learning shared representations and then specializing them.
- Mechanism: Stage 1 uses a shared encoder to learn common patterns between schematic symbols and galaxy images, creating a foundation of galaxy morphology understanding. Stage 2 then copies these learned parameters to initialize modality-specific encoders, allowing each to specialize while maintaining the foundational understanding.
- Core assumption: Schematic symbols capture essential morphological features that overlap significantly with actual galaxy images, making shared representation learning possible.
- Evidence anchors:
  - [abstract]: "Specifically, GalaxAlign employs a tri-modal alignment framework to align three types of data during fine-tuning: (1) schematic symbols representing galaxy shapes and structures, (2) textual labels for these symbols, and (3) galaxy images."
  - [section]: "In the first stage, galaxy images and schematic symbols are input into a shared image encoder, while textual descriptions are processed by a separate text encoder. This stage enables the image encoder to learn a shared representation of galaxy features from both symbolic and photographic images."
- Break condition: If schematic symbols do not capture essential morphological features that overlap with actual galaxy images, the shared representation learning would fail to provide meaningful foundation for the specialized encoders.

### Mechanism 2
- Claim: Multi-modal contrastive learning aligns visual, symbolic, and textual representations to create a unified embedding space for galaxy morphology tasks.
- Mechanism: The model uses contrastive loss functions to maximize similarity between matching image-text pairs, symbol-text pairs, and image-symbol pairs, while minimizing similarity between non-matching pairs. This creates a shared embedding space where semantically similar items across modalities are close together.
- Core assumption: Cosine similarity in the embedding space effectively captures morphological similarity between galaxies across different modalities.
- Evidence anchors:
  - [abstract]: "By incorporating multimodal instructions, GalaxAlign eliminates the need for expensive pretraining and enhances the effectiveness of fine-tuning."
  - [section]: "To align image/symbol and text embeddings, we use a contrastive loss function that maximizes the cosine similarity between positive (matching) pairs and minimizes it for non-matching pairs."
- Break condition: If the contrastive loss does not properly align the three modalities, the unified embedding space would fail to capture meaningful morphological relationships.

### Mechanism 3
- Claim: Mimicking citizen scientists' annotation strategies by incorporating schematic symbols and textual descriptions helps bridge the domain gap between natural and astronomical images.
- Mechanism: The model learns from the same multimodal guidance that citizen scientists use - schematic symbols paired with textual descriptions - to understand galaxy morphology. This allows general vision models pretrained on natural images to adapt to astronomical data without extensive astronomical pretraining.
- Core assumption: Citizen scientists' annotation strategies effectively capture the key morphological features needed for galaxy classification.
- Evidence anchors:
  - [abstract]: "Inspired by citizen scientists' annotation strategies, it uses a two-stage fine-tuning approach: first learning shared visual-symbolic representations, then specializing encoders for each modality."
  - [section]: "Since human amateur volunteers can label astronomical images based on their knowledge learned from textual descriptions and schematic diagrams, we believe that these modalities can also assist models pre-trained on general datasets in performing classification tasks on astronomical images."
- Break condition: If citizen scientists' annotation strategies do not effectively capture the essential morphological features needed for accurate galaxy classification, the multimodal approach would fail to bridge the domain gap.

## Foundational Learning

- Concept: Contrastive learning and embedding space alignment
  - Why needed here: The method relies on aligning representations across three modalities using contrastive loss functions. Understanding how contrastive learning works and how embedding spaces are constructed is crucial for implementing and debugging this approach.
  - Quick check question: How does the contrastive loss function ensure that semantically similar items across different modalities are positioned close together in the embedding space?

- Concept: Vision transformer architectures and fine-tuning strategies
  - Why needed here: The method uses vision transformers (ViT) and convolutional networks (ConvNeXT) as backbones. Understanding these architectures and how fine-tuning differs from training from scratch is essential for implementation.
  - Quick check question: What is the difference between full fine-tuning and adapter-based fine-tuning, and why might full fine-tuning be preferred in this astronomical domain adaptation scenario?

- Concept: Multi-modal data alignment and representation learning
  - Why needed here: The core innovation involves aligning three different types of data (images, symbols, text) in a shared representation space. Understanding techniques for multi-modal alignment is crucial for extending or modifying this approach.
  - Quick check question: How can we measure whether the three modalities are properly aligned in the embedding space, and what metrics would indicate successful alignment?

## Architecture Onboarding

- Component map: Text Encoder -> Shared Image Encoder (Stage 1) -> Symbol Encoder (Stage 2) -> Image Encoder (Stage 2) -> Contrastive Loss Function -> Two-Stage Pipeline

- Critical path: 
  1. Load pretrained CLIP model weights
  2. Initialize shared encoder and text encoder
  3. Stage 1 training: train shared encoder with image-symbol-text pairs
  4. Copy shared encoder parameters to initialize symbol encoder
  5. Stage 2 training: train all three encoders with full tri-modal contrastive loss
  6. Evaluate on downstream tasks (classification, similarity search)

- Design tradeoffs:
  - Two-stage vs. single-stage training: Two-stage allows for better initialization and specialization but requires more training steps
  - Shared vs. separate encoders: Shared encoders in Stage 1 enable cross-modal learning but may limit specialization
  - Contrastive vs. other alignment methods: Contrastive learning provides effective alignment but may be sensitive to batch size and temperature parameters

- Failure signatures:
  - Poor classification accuracy: May indicate insufficient alignment between modalities or inadequate fine-tuning
  - High intra-class variance in t-SNE visualizations: Suggests the model is not capturing consistent morphological features
  - Loss plateauing early: Could indicate learning rate issues or insufficient model capacity
  - Symbol encoder not improving from initialization: May indicate the two-stage approach is not providing benefit

- First 3 experiments:
  1. Implement Stage 1 only with shared encoder and evaluate on classification task to verify basic functionality
  2. Add Stage 2 with separate encoders but only image-text contrastive loss to isolate the effect of specialization
  3. Implement full tri-modal contrastive loss and compare performance against the two previous stages to validate the complete architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GalaxAlign perform on astronomical datasets beyond Galaxy10 and GalaxyMNIST, such as datasets with more classes or different types of astronomical objects?
- Basis in paper: [inferred] The paper only tests GalaxAlign on two galaxy datasets and mentions potential applications to other natural sciences but does not provide empirical evidence for other astronomical datasets or object types.
- Why unresolved: The authors do not provide experiments or results on other astronomical datasets, leaving uncertainty about the model's generalizability to more complex or varied astronomical data.
- What evidence would resolve it: Testing GalaxAlign on additional astronomical datasets with more classes, different object types (e.g., stars, nebulae), or different imaging conditions, and comparing performance to existing methods.

### Open Question 2
- Question: What is the impact of varying the temperature parameter ùúè in the contrastive loss function on GalaxAlign's performance across different tasks and datasets?
- Basis in paper: [explicit] The paper mentions that ùúè is a learnable temperature parameter in the loss function but does not explore how its value affects performance or discuss sensitivity analysis.
- Why unresolved: The authors do not provide experiments varying ùúè or analyzing its impact on model performance, leaving uncertainty about its optimal value and sensitivity.
- What evidence would resolve it: Conducting experiments with different ùúè values and analyzing their effect on classification accuracy, similarity search, and few-shot performance across datasets.

### Open Question 3
- Question: How does GalaxAlign's performance compare to astronomical foundation models when trained on smaller-scale astronomical datasets (e.g., datasets smaller than GZD-5)?
- Basis in paper: [explicit] The paper mentions that astronomical foundation models like Zoobot are pretrained on large-scale datasets like GZD-5, but does not compare GalaxAlign's performance when both models are trained on datasets of similar small sizes.
- Why unresolved: The authors only compare GalaxAlign to astronomical models trained on large datasets, leaving uncertainty about its advantage when both models have access to limited data.
- What evidence would resolve it: Training both GalaxAlign and astronomical foundation models (e.g., Zoobot) on datasets of comparable small sizes and comparing their performance across tasks.

## Limitations
- The two-stage fine-tuning approach requires more computational resources and training time compared to single-stage methods, which may limit scalability
- Performance heavily depends on the quality and coverage of schematic symbols from Galaxy Zoo 2, which may not capture all morphological variations
- The contrastive loss formulation assumes linear separability in the embedding space, which may not hold for complex galaxy morphology relationships

## Confidence
- **High confidence**: Galaxy classification accuracy improvements (97.32% on Galaxy10) and mAP metrics for similarity search
- **Medium confidence**: The claim that schematic symbols effectively bridge the domain gap between natural and astronomical images
- **Medium confidence**: The necessity of the two-stage training procedure over single-stage alternatives

## Next Checks
1. Conduct ablation studies removing each modality (symbols, text, or one image encoder) to quantify individual contributions to overall performance
2. Test the model's generalization on additional astronomical datasets not seen during training to validate domain adaptation claims
3. Perform t-SNE visualizations of embeddings to verify that the three modalities are properly aligned in the shared representation space and that semantically similar galaxies cluster together regardless of modality