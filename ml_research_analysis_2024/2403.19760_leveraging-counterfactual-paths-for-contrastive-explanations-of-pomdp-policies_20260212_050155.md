---
ver: rpa2
title: Leveraging Counterfactual Paths for Contrastive Explanations of POMDP Policies
arxiv_id: '2403.19760'
source_url: https://arxiv.org/abs/2403.19760
tags:
- policy
- user
- optimal
- explanations
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explaining POMDP policies
  to human users in search and rescue scenarios. The authors propose using counterfactual
  user-provided paths to generate contrastive explanations of optimal policies.
---

# Leveraging Counterfactual Paths for Contrastive Explanations of POMDP Policies

## Quick Facts
- arXiv ID: 2403.19760
- Source URL: https://arxiv.org/abs/2403.19760
- Authors: Benjamin Kraske; Zakariya Laouar; Zachary Sunberg
- Reference count: 23
- One-line primary result: This paper addresses the challenge of explaining POMDP policies to human users in search and rescue scenarios.

## Executive Summary
This paper addresses the challenge of explaining POMDP policies to human users in search and rescue scenarios. The authors propose using counterfactual user-provided paths to generate contrastive explanations of optimal policies. They leverage feature expectations to compare the performance of optimal and user-proposed policies, providing insight into why the optimal policy outperforms the alternative. Through two case studies in a search and rescue POMDP domain, the authors demonstrate that feature expectations can effectively explain why optimal policies differ from user expectations, particularly in terms of balancing observable and unobservable objectives and respecting resource constraints.

## Method Summary
The authors propose a method to generate contrastive explanations for POMDP policies by comparing the optimal policy (computed using SARSOP) with user-provided counterfactual paths. These counterfactual paths are translated into open-loop policies and their performance is compared using feature expectations. The approach involves computing feature expectations for both the optimal and user-proposed policies using the Bellman equation, then generating explanations based on the differences in feature frequencies and values. The method is demonstrated through two case studies in a search and rescue POMDP domain.

## Key Results
- Feature expectations can effectively explain why optimal policies differ from user expectations in search and rescue POMDPs
- The approach highlights differences in balancing observable and unobservable objectives between optimal and user-proposed policies
- Feature expectations can capture how optimal policies respect resource constraints that user paths may violate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature expectations provide a quantitative basis for contrastive explanations by capturing the expected frequency of feature occurrences under different policies.
- Mechanism: By defining a feature indicator function that maps state-action pairs to feature vectors, the expected feature occupancy can be calculated recursively using the Bellman equation. This allows comparison of optimal and user-proposed policies in terms of feature frequencies rather than just raw rewards.
- Core assumption: Feature expectations can be accurately computed for both the optimal policy (using methods like SARSOP) and the user-proposed open-loop policy.
- Evidence anchors:
  - [abstract]: "Feature expectations are used as a means of contrasting the performance of these policies."
  - [section 2.1]: "Let ðœ‡ðœ‹ (ð‘0) = E[âˆ‘âˆžð‘¡=0 ð›¾ð‘¡ ðœ™ (ð‘ð‘¡ , ð‘Žð‘¡ ) | ðœ‹, ð‘0], where ð‘ is a belief distribution over states."
  - [corpus]: Weak evidence - no directly related work on feature expectations for POMDP explanations found.
- Break condition: If feature expectations cannot be computed accurately (e.g., due to computational limitations or poor approximation in large state spaces).

### Mechanism 2
- Claim: Counterfactual user paths provide a natural basis for contrastive explanations by highlighting differences between user expectations and optimal policy behavior.
- Mechanism: Users draw alternative paths on a visual interface, which are translated into open-loop policies. The performance of these counterfactual policies is compared to the optimal policy using feature expectations, generating explanations for why the optimal path differs from the user's expectations.
- Core assumption: Users can intuitively express counterfactual paths through a visual interface that maps to valid open-loop policies.
- Evidence anchors:
  - [abstract]: "We investigate the use of user-provided counterfactuals to generate contrastive explanations of POMDP policies."
  - [section 2.2]: "The user would then be prompted to provide an alternative path, represented as an open-loop action sequence in this work."
  - [corpus]: Weak evidence - related work on counterfactual explanations exists but not specifically for POMDP policies.
- Break condition: If users cannot effectively express counterfactual paths (e.g., due to interface limitations or cognitive load).

### Mechanism 3
- Claim: Separating feature values from feature frequencies enhances comprehensibility by showing both what objectives are pursued and how often they are achieved.
- Mechanism: By using weighted features where weights represent the value of each feature and feature expectations represent the frequency of occurrence, explanations can convey both the importance and prevalence of different objectives under each policy.
- Core assumption: Users can understand explanations that combine feature weights (values) and feature expectations (frequencies).
- Evidence anchors:
  - [section 2.1]: "This approach gives users insight into both the frequency of visited features and the value assigned to them."
  - [section 3]: "These features relate back to Example 2.1, representing user-specified objectives... a central objective... and a constraint on the problem."
  - [corpus]: Weak evidence - related work on feature expectations exists but not specifically for this combined approach in POMDP explanations.
- Break condition: If users cannot effectively interpret explanations that combine values and frequencies.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The work explains POMDP policies, so understanding POMDP structure is fundamental.
  - Quick check question: What are the three key components that differentiate POMDPs from regular MDPs?

- Concept: Feature expectations and their calculation
  - Why needed here: Feature expectations are the core mechanism for generating explanations.
  - Quick check question: How do you calculate feature expectations recursively using the Bellman equation?

- Concept: Counterfactual reasoning in decision making
  - Why needed here: The approach uses user-provided counterfactual paths as the basis for explanations.
  - Quick check question: What is the difference between a counterfactual path and the executed optimal path in this context?

## Architecture Onboarding

- Component map:
  - User Interface -> POMDP Solver -> Feature Expectation Calculator -> Explanation Generator -> Domain Model

- Critical path:
  1. User executes optimal policy
  2. User provides counterfactual path via interface
  3. Counterfactual path translated to open-loop policy
  4. Feature expectations calculated for both policies
  5. Explanation generated from feature expectation differences
  6. Explanation presented to user

- Design tradeoffs:
  - Open-loop vs. closed-loop counterfactual policies: Open-loop is simpler but may not capture user reasoning as well
  - Feature selection: More features provide richer explanations but increase complexity
  - Explanation detail: More detailed explanations are more informative but may be harder to understand

- Failure signatures:
  - User confusion about why certain features are included/excluded
  - Explanation generator producing contradictory or unclear explanations
  - Feature expectation calculations diverging significantly between methods

- First 3 experiments:
  1. Implement feature expectation calculation for a simple 2x2 grid world POMDP
  2. Create a basic visual interface for drawing counterfactual paths in the grid world
  3. Generate and test explanations for a case where the optimal policy differs from an intuitive user path

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the approach be extended to handle closed-loop user feedback where the user's proposed policy adapts based on new observations?
- Basis in paper: [explicit] The paper mentions that a substantial assumption is that the user maintains an open-loop policy and suggests that accounting for policy changes due to new information would better capture user reasoning.
- Why unresolved: The paper does not provide a methodology for handling closed-loop user feedback and only suggests that this would be valuable for future work.
- What evidence would resolve it: A demonstration of the approach applied to a scenario where the user's policy adapts based on new observations, showing improved explanation accuracy compared to the open-loop approach.

### Open Question 2
- Question: How can proactive explanations be generated automatically to anticipate user confusion without requiring specific user input?
- Basis in paper: [explicit] The paper states that providing proactive explanations which can be provided automatically in anticipation of user confusion will be valuable in reducing user workload and reducing dependence on domain-specific means of user feedback.
- Why unresolved: The paper does not provide a methodology for generating proactive explanations and only suggests that this would be valuable for future work.
- What evidence would resolve it: A system that can automatically generate explanations for executed paths based on the domain and user context, validated by user studies showing reduced confusion and improved trust.

### Open Question 3
- Question: What is the most effective way to translate feature expectations into plain language explanations that are both interpretable and comprehensible to users?
- Basis in paper: [explicit] The paper mentions that translating feature expectations into plain language is necessary for explanations but leaves the specific translation approach to future work.
- Why unresolved: The paper does not provide a methodology for translating feature expectations into plain language and only suggests that this would be valuable for future work.
- What evidence would resolve it: A comparison of different translation approaches (e.g., rule-based, machine learning-based) in terms of their ability to produce explanations that are both interpretable and comprehensible to users, validated by user studies.

## Limitations
- The paper relies heavily on feature expectations as the primary mechanism for explanation, but provides limited empirical validation of whether this actually improves user understanding of POMDP policies
- Confidence in the explanation generation approach is limited by the lack of user studies or human subject experiments to verify that the explanations are comprehensible and useful
- The computational complexity of feature expectation calculations for larger POMDPs is not addressed, which could limit scalability to more complex domains

## Confidence
- **High confidence**: The mathematical framework for calculating feature expectations and using them to compare policies is sound and well-defined
- **Medium confidence**: The case studies demonstrate that feature expectations can capture meaningful differences between optimal and user-proposed policies in the specific SAR domain
- **Low confidence**: The overall approach's effectiveness in improving human understanding of POMDP policies lacks empirical validation through user studies

## Next Checks
1. Implement a user study to evaluate whether feature expectation-based explanations actually improve users' understanding of why optimal policies differ from their expectations, compared to baseline explanations
2. Test the scalability of the approach by applying it to larger POMDPs with more complex state spaces and compare computational requirements with simpler explanation methods
3. Conduct ablation studies to determine which components of the explanation (feature values vs. frequencies vs. both) contribute most to user understanding and trust in the system