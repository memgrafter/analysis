---
ver: rpa2
title: 'EARBench: Towards Evaluating Physical Risk Awareness for Task Planning of
  Foundation Model-based Embodied AI Agents'
arxiv_id: '2408.04449'
source_url: https://arxiv.org/abs/2408.04449
tags:
- safety
- risk
- task
- scene
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EARBench, the first automated framework for
  assessing physical risk awareness in embodied AI agents powered by foundation models.
  EARBench uses a multi-agent system to generate safety guidelines, create risky scenarios,
  perform task planning, and evaluate safety.
---

# EARBench: Towards Evaluating Physical Risk Awareness for Task Planning of Foundation Model-based Embodied AI Agents

## Quick Facts
- arXiv ID: 2408.04449
- Source URL: https://arxiv.org/abs/2408.04449
- Reference count: 37
- Key outcome: EARBench reveals foundation models achieve high task effectiveness (80-95%) but maintain high task risk rates (95.75% average), highlighting critical safety gaps in embodied AI.

## Executive Summary
This paper introduces EARBench, the first automated framework for evaluating physical risk awareness in foundation model-based embodied AI agents. The framework uses a multi-agent system to generate safety guidelines, create risky scenarios, perform task planning, and evaluate safety. Testing on 2,636 scenarios across seven domains reveals that all state-of-the-art models exhibit high task risk rates (average 95.75%), with even the best model (GPT-4o) showing 94.03%. The study proposes two prompting-based risk mitigation strategies that achieve modest improvements (4-14% reduction in risk rates), but substantial safety concerns remain, highlighting the critical need for enhanced safety measures in embodied AI systems.

## Method Summary
EARBench employs a multi-agent cooperative system with four specialized modules: Safety Guidelines Generation (GPT-4o), Risky Scene Generation (LLM + text-to-image model), Embodied Task Planning (foundation models), and Plan Assessment (GPT-4o). The framework uses EARDataset with 2,636 test cases across seven domains to evaluate physical risk awareness. Task Risk Rate (TRR) measures the proportion of plans containing potential risks, while Task Effectiveness Rate (TER) measures executable plans. The system leverages explicit safety guidelines as evaluation criteria, drawing inspiration from Constitutional AI principles to improve consistency in safety assessment.

## Key Results
- All evaluated models show high task risk rates averaging 95.75%, with GPT-4o achieving 94.03% TRR
- Visual scenarios provide only marginal safety improvements compared to text scenarios
- Risk mitigation strategies reduce TRR by 4-14% but still leave substantial safety concerns
- High task effectiveness (80-95%) is achieved alongside high risk rates, revealing a fundamental trade-off

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent cooperative system reduces hallucination risk in safety guideline generation
- Mechanism: EARBench uses different foundation models for distinct roles instead of relying on a single model for all tasks
- Core assumption: Specialized models perform their specific tasks with higher accuracy than generalist approaches
- Evidence anchors:
  - "EARBench employs a multi-agent cooperative system that leverages various foundation models to generate safety guidelines, create risk-prone scenarios, make task planning, and evaluate safety systematically"
  - "Drawing inspiration from real-world safety practices and 'Constitutional AI', we design this module to generate EAI-centric safety guidelines"

### Mechanism 2
- Claim: Safety tips as constitutional principles improve risk evaluation consistency
- Mechanism: Generated safety guidelines serve as explicit criteria for both test case generation and evaluation
- Core assumption: Explicit safety criteria reduce subjective variation in safety judgments
- Evidence anchors:
  - "The safety guidelines serve as evaluation criteria during assessments, enhancing the accuracy of safety assessment"
  - "The results demonstrate a significant improvement in assessment consistency when safety tips are included"

### Mechanism 3
- Claim: Risk mitigation prompting strategies improve safety through explicit constraints
- Mechanism: Two prompting approaches (implicit general safety reminders vs explicit specific safety tips) guide models to consider safety during planning
- Core assumption: Models can incorporate safety constraints when explicitly prompted without compromising task effectiveness
- Evidence anchors:
  - "Drawing inspiration from Constitutional AI, we propose two fundamental prompt-based risk mitigation strategies"
  - "Both strategies reduce TRR, with the explicit strategy consistently outperforming the implicit one"

## Foundational Learning

- Concept: Constitutional AI principles
  - Why needed here: EARBench applies Constitutional AI ideas to embodied AI safety by using explicit safety guidelines as "constitutions" for evaluation
  - Quick check question: How does EARBench's use of safety tips as evaluation criteria relate to the Constitutional AI concept of using explicit principles for AI behavior?

- Concept: Multimodal foundation model evaluation
  - Why needed here: EARBench evaluates both text-only LLMs and multimodal VLMs to assess whether visual information improves risk awareness
  - Quick check question: What does EARBench's finding about marginal improvements in visual scenarios suggest about the role of multimodal input in safety assessment?

- Concept: Risk-aware planning vs task completion trade-off
  - Why needed here: EARBench reveals high task effectiveness rates alongside high risk rates, highlighting the tension between completing tasks and avoiding risks
  - Quick check question: Why might models achieve high task effectiveness while maintaining high risk rates, and what does this imply for real-world deployment?

## Architecture Onboarding

- Component map: Safety Guidelines Generation -> Risky Scene Generation -> Embodied Task Planning -> Plan Assessment
- Critical path: Safety Guidelines → Risky Scene Generation → Task Planning → Assessment
- Design tradeoffs:
  - Specialized vs generalist models: EARBench uses specialized models for different tasks but this adds coordination complexity
  - Automated vs human evaluation: Automated evaluation enables scalability but requires careful validation against human judgments
  - Textual vs visual scenarios: Visual scenarios add complexity and computational cost but provide marginal safety improvements
- Failure signatures:
  - High TRR across all models indicates fundamental limitations in risk awareness
  - Inconsistent results between automated and human evaluation suggest evaluator reliability issues
  - Low improvement from risk mitigation strategies indicates limitations of prompting-based approaches
- First 3 experiments:
  1. Run EARBench with a single model handling all tasks vs the multi-agent approach to quantify hallucination reduction
  2. Test automated evaluation consistency by comparing results with 5 human annotators on the same test cases
  3. Evaluate different risk mitigation strategies (implicit vs explicit) on a subset of high-risk scenarios to measure relative effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing model parameter size beyond current benchmarks (e.g., 236B+ parameters) lead to significant improvements in physical risk awareness for EAI agents?
- Basis in paper: The paper explicitly states that "increasing the model parameter scale did not lead to a significant improvement in risk avoidance capabilities" and notes that even DeepSeek-V2 with 236B parameters showed high TRR (96.7%).
- Why unresolved: The study only tested models up to 236B parameters, leaving uncertainty about whether extremely large models (500B+ parameters) might show different behavior in risk awareness.
- What evidence would resolve it: Testing models with 500B+ parameters on EARBench would determine if there's a threshold where model size begins to significantly impact risk awareness.

### Open Question 2
- Question: Can specialized pre-training techniques on safety-oriented datasets (like EARDataset) substantially improve foundation models' risk awareness compared to general pre-training?
- Basis in paper: The paper suggests "developing specialized foundation model pre-training techniques and preference alignment methods tailored for embodied intelligence scenarios" as a future research direction.
- Why unresolved: The study only tested existing pre-trained models without specialized safety training, so the potential impact of targeted pre-training on risk awareness remains unknown.
- What evidence would resolve it: Fine-tuning various foundation models on EARDataset and re-evaluating them on EARBench would show if specialized safety training leads to meaningful improvements in TRR.

### Open Question 3
- Question: Would incorporating real-time environmental feedback mechanisms into EAI agents significantly reduce risk rates compared to pre-planning approaches?
- Basis in paper: The study focuses on high-level task planning without considering dynamic feedback during execution, and notes that "even the best-performing model (GPT-4o) still maintains a TRR above 40% with the explicit risk mitigation strategy."
- Why unresolved: EARBench evaluates static planning capabilities without simulating real-time environmental interaction and adaptation during task execution.
- What evidence would resolve it: Implementing EARBench with real-time feedback loops and comparing TRR between static planners and adaptive agents would demonstrate the value of dynamic risk mitigation.

## Limitations
- The evaluation framework relies heavily on automated assessment by GPT-4o, which may introduce systematic biases in safety judgment
- Multi-agent coordination introduces potential inconsistencies between modules, particularly when safety guidelines are ambiguous
- The focus on foundation models may not fully capture safety challenges that arise when these models are deployed in real-world embodied systems

## Confidence
- High confidence: The reported high task risk rates across all evaluated models (95.75% average) and the modest improvements from risk mitigation strategies (4-14% reduction) are well-supported by the experimental methodology
- Medium confidence: The automated evaluation consistency results (90.7% automated vs 92.1% human for text scenarios) suggest reasonable reliability, but the small gap and domain-specific variations indicate potential limitations
- Medium confidence: The claim that visual information provides marginal safety improvements requires further validation across more diverse scenarios and model architectures

## Next Checks
1. Conduct a systematic comparison between automated EARBench evaluation and independent human expert assessment across all seven domains to quantify systematic biases
2. Test the framework with a broader range of foundation models including open-source VLMs to assess whether the high risk rates are model-specific or represent a fundamental challenge in EAI safety
3. Evaluate the impact of different robot skill set configurations on task effectiveness and risk rates to determine whether skill limitations artificially inflate risk assessments