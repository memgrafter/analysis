---
ver: rpa2
title: 'Views Are My Own, but Also Yours: Benchmarking Theory of Mind Using Common
  Ground'
arxiv_id: '2403.02451'
source_url: https://arxiv.org/abs/2403.02451
tags:
- common
- order
- belief
- beliefs
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces COMMON-TOM, a new benchmark dataset for evaluating
  the Theory of Mind (ToM) capabilities of language models using naturally occurring
  spoken dialogues. The dataset is based on the concept of common ground, which refers
  to mutually shared beliefs among conversation participants.
---

# Views Are My Own, but Also Yours: Benchmarking Theory of Mind Using Common Ground

## Quick Facts
- arXiv ID: 2403.02451
- Source URL: https://arxiv.org/abs/2403.02451
- Reference count: 23
- Primary result: Introduces COMMON-TOM, a new benchmark dataset for evaluating Theory of Mind capabilities of language models using naturally occurring spoken dialogues based on common ground

## Executive Summary
This paper introduces COMMON-TOM, a novel benchmark dataset designed to evaluate the Theory of Mind (ToM) capabilities of language models using naturally occurring spoken dialogues. The benchmark leverages the concept of common ground - mutually shared beliefs among conversation participants - to create queries testing first, second, and third-order beliefs about propositions in the dialogues. Experiments demonstrate that large language models struggle significantly with this benchmark, performing worse than human baselines. However, a neuro-symbolic approach that explicitly represents beliefs and common ground outperforms pure language models, suggesting that explicit representation of mental states and shared beliefs is crucial for Theory of Mind reasoning.

## Method Summary
The authors construct COMMON-TOM by analyzing naturally occurring spoken dialogues to identify propositions and extract common ground information about shared beliefs between participants. They create queries designed to test different orders of belief attribution (first, second, and third-order) by asking models to reason about what speakers know, believe, or think about various propositions and each other's mental states. The benchmark is evaluated across multiple large language models, with performance compared against human baselines and a neuro-symbolic approach that combines symbolic reasoning about beliefs with neural language understanding. The neuro-symbolic method explicitly represents beliefs and common ground information, allowing it to reason about mental states in a more structured way than pure language models.

## Key Results
- Large language models perform significantly worse than human baselines on the COMMON-TOM benchmark
- Models show reasonable performance on first-order beliefs but struggle with higher-order beliefs
- The neuro-symbolic approach that explicitly represents beliefs and common ground outperforms both pure language models and human baselines
- Performance degradation increases with belief order complexity, indicating fundamental limitations in current LMs' ability to model others' mental states

## Why This Works (Mechanism)
The benchmark works by operationalizing Theory of Mind through the lens of common ground, which captures the shared understanding that emerges during natural conversation. By testing models' ability to reason about what speakers know, believe, and think about various propositions and each other's mental states, the benchmark exposes the gap between language models' ability to process text and their capacity to understand the social and mental dimensions of communication. The neuro-symbolic approach succeeds because it explicitly encodes belief representations and common ground information, providing structural support for reasoning about mental states that pure language models lack.

## Foundational Learning
**Common Ground Theory**: Understanding how shared beliefs emerge and evolve in conversation is essential for interpreting the benchmark's design and purpose. *Why needed*: The entire benchmark is built around this concept as the foundation for Theory of Mind evaluation. *Quick check*: Verify understanding of how common ground differs from individual beliefs and why it's crucial for effective communication.

**Theory of Mind Hierarchy**: Familiarity with first, second, and third-order belief attribution is necessary to interpret the benchmark's structure and results. *Why needed*: The benchmark systematically tests different orders of belief reasoning, with performance varying by complexity. *Quick check*: Ensure ability to distinguish between "X believes P", "X believes Y believes P", and "X believes Y believes Z believes P".

**Neuro-symbolic AI**: Understanding the integration of symbolic reasoning with neural approaches is key to grasping why the neuro-symbolic method outperforms pure LMs. *Why needed*: The paper's key contribution includes demonstrating the effectiveness of this hybrid approach for ToM tasks. *Quick check*: Confirm understanding of how symbolic belief representations complement neural language understanding.

## Architecture Onboarding

**Component Map**: Dialogue Data -> Proposition Extraction -> Common Ground Annotation -> Query Generation -> Benchmark -> LM Evaluation -> Neuro-symbolic Evaluation

**Critical Path**: The most critical pathway is Dialogue Data -> Common Ground Annotation -> Query Generation -> Evaluation, as the quality and accuracy of common ground annotation directly determines the benchmark's validity and the meaningfulness of evaluation results.

**Design Tradeoffs**: The choice to use naturally occurring dialogues provides ecological validity but introduces complexity in annotation and potential inconsistencies. The neuro-symbolic approach trades model flexibility for explicit representational power, which helps with structured reasoning but may limit scalability.

**Failure Signatures**: Models failing on higher-order beliefs while succeeding on first-order beliefs indicates a fundamental limitation in modeling nested mental states. Poor performance on common ground reasoning suggests inability to track shared versus individual knowledge. The neuro-symbolic approach's success reveals that pure LMs lack the structural representations needed for Theory of Mind reasoning.

**3 First Experiments**:
1. Test the neuro-symbolic approach on a subset of queries with varying belief orders to isolate which components drive performance improvements
2. Conduct ablation studies removing the common ground representation to measure its specific contribution to performance
3. Evaluate the benchmark on language models specifically trained with social reasoning objectives to test generalizability of findings

## Open Questions the Paper Calls Out
The paper raises questions about whether the neuro-symbolic approach's superior performance truly indicates conceptual understanding or simply benefits from explicit representations. It also questions the scalability of the neuro-symbolic method and whether the benchmark's focus on naturally occurring dialogues introduces biases in how beliefs are identified and categorized.

## Limitations
- The neuro-symbolic approach's superior performance raises questions about scalability and whether it truly captures conceptual understanding
- The benchmark's focus on naturally occurring dialogues may introduce biases in how beliefs are identified and categorized
- The human baseline methodology lacks sufficient detail, making it difficult to assess whether the performance gap is meaningful or influenced by experimental design choices

## Confidence

**High confidence**: The benchmark construction methodology and dataset creation process
**Medium confidence**: The claim that LMs struggle with higher-order beliefs relative to first-order beliefs
**Medium confidence**: The neuro-symbolic approach's effectiveness relative to pure LMs
**Low confidence**: The broader claim about LMs lacking conceptual understanding of mental states

## Next Checks

1. Conduct ablation studies on the neuro-symbolic approach to determine which components (explicit belief representation vs. common ground modeling) drive performance improvements

2. Test the benchmark on additional language models, including those specifically trained with social reasoning objectives, to better understand the generalizability of the findings

3. Implement a more rigorous human baseline protocol with multiple annotators and inter-rater reliability measures to strengthen the comparative analysis between human and model performance