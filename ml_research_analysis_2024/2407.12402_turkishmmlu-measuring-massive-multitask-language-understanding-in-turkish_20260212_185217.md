---
ver: rpa2
title: 'TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish'
arxiv_id: '2407.12402'
source_url: https://arxiv.org/abs/2407.12402
tags:
- open
- turkish
- questions
- language
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TurkishMMLU, the first large-scale multitask,
  multiple-choice Turkish benchmark for evaluating Large Language Models (LLMs) on
  the Turkish language. The dataset consists of 10,032 questions across nine high
  school subjects, developed from official Turkish educational resources.
---

# TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish

## Quick Facts
- **arXiv ID:** 2407.12402
- **Source URL:** https://arxiv.org/abs/2407.12402
- **Reference count:** 13
- **Primary result:** First large-scale multitask, multiple-choice Turkish benchmark for evaluating LLMs, showing GPT-4o achieved 83.1% accuracy.

## Executive Summary
This paper introduces TurkishMMLU, the first large-scale multitask, multiple-choice Turkish benchmark for evaluating Large Language Models (LLMs) on the Turkish language. The dataset consists of 10,032 questions across nine high school subjects, developed from official Turkish educational resources. Over 40 models were evaluated, including multilingual and Turkish-adapted models, in various setups such as zero-shot, few-shot, and chain-of-thought reasoning. Results showed that closed-source models like GPT-4o and Claude-3 Opus outperformed others, with GPT-4o achieving 83.1% accuracy. Mathematics was the most challenging subject for models, while social sciences and humanities were easier. A difficulty analysis confirmed that model performance decreases as question difficulty increases.

## Method Summary
TurkishMMLU is developed from official Turkish educational resources, specifically the Ministry of Education's EBA platform, containing 10,032 questions across nine high school subjects. Questions were manually reviewed for formatting and accuracy, with difficulty calibrated using student performance data (correctness ratio). The benchmark evaluates models in zero-shot, few-shot (5-shot), and chain-of-thought reasoning setups, measuring accuracy across subjects and difficulty levels. Over 40 models were tested, including both open-source and closed-source variants, with performance compared across different evaluation conditions.

## Key Results
- GPT-4o achieved the highest accuracy at 83.1% on the full TurkishMMLU benchmark
- Mathematics was consistently the most challenging subject across all model types
- Performance decreases systematically as question difficulty increases, validating the correctness ratio calibration
- Open-source models showed significant performance gaps compared to closed-source models like GPT-4o and Claude-3 Opus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TurkishMMLU provides a culturally grounded evaluation resource by using curriculum-expert-authored questions from official Turkish educational materials.
- Mechanism: By sourcing questions directly from the Turkish Ministry of Education's EBA platform, the benchmark avoids the linguistic and cultural errors that arise in automatic translation-based multilingual benchmarks.
- Core assumption: Questions crafted by Turkish curriculum experts reflect the linguistic and cultural realities needed for valid evaluation of Turkish language understanding.
- Evidence anchors:
  - [abstract] "These questions are written by curriculum experts, suitable for the high-school curricula in Turkey, covering subjects ranging from natural sciences and math questions to more culturally representative topics such as Turkish Literature and the history of the Turkish Republic."
  - [section] "We conducted repeated manual reviews, randomly selecting 30 questions from the complete dataset during each review session to assess their formatting and accuracy."
  - [corpus] Weak: corpus shows similar Turkish MMLU benchmarks, but no direct evidence that this method is superior.
- Break condition: If the questions become outdated or fail to represent evolving Turkish educational standards, the benchmark's validity would degrade.

### Mechanism 2
- Claim: The inclusion of a correctness ratio for each question provides a reliable difficulty measure that correlates with actual student performance.
- Mechanism: Correctness ratio is calculated as the percentage of correct responses from students who took the test, providing a data-grounded difficulty score rather than an arbitrary scale.
- Core assumption: Student performance on these official exam questions is representative of the intrinsic difficulty of the questions for Turkish language understanding.
- Evidence anchors:
  - [section] "Each question's difficulty is denoted by a Correctness Ratio (black boxes in Figure 2), calculated as the percentage of correct user responses."
  - [section] "Questions are categorized as Easy (top 30%), Medium (middle 40%), or Hard (bottom 30%), with percentile thresholds at 41 and 28, respectively."
  - [corpus] Missing: no corpus evidence about validity of this difficulty calibration.
- Break condition: If the student population taking these tests differs significantly from the general Turkish-speaking population, the difficulty measure may not generalize.

### Mechanism 3
- Claim: TurkishMMLU's large scale (10,032 questions) and coverage of nine subjects enables robust, multitask evaluation of language models.
- Mechanism: A large, diverse question set reduces variance in model performance estimates and allows for subject-specific and difficulty-based analyses that smaller benchmarks cannot support.
- Core assumption: A broad subject coverage (Natural Sciences, Mathematics, Turkish Language & Literature, Social Sciences & Humanities) captures the full spectrum of language understanding capabilities needed for Turkish.
- Evidence anchors:
  - [abstract] "TurkishMMLU includes over 10,000 questions, covering 9 different subjects from Turkish high-school education curricula."
  - [section] "The dataset includes nine high school subjects across four domains: Math (Mathematics); Natural Sciences (Biology, Chemistry, Physics); Language (Turkish Language and Literature); and Humanities and Social Sciences (History, Geography, Philosophy, Religion and Ethics)."
  - [corpus] Weak: corpus shows related benchmarks but no evidence this scale is necessary or sufficient.
- Break condition: If models overfit to specific question patterns or subjects, the large scale may not prevent biased evaluation.

## Foundational Learning

- Concept: High school curriculum structure in Turkey
  - Why needed here: Understanding the source of questions and the educational context they represent
  - Quick check question: What are the four main subject domains covered in Turkish high school education according to TurkishMMLU?

- Concept: Question difficulty calibration using student performance data
  - Why needed here: The correctness ratio provides the primary difficulty measure for the benchmark
  - Quick check question: How is question difficulty determined in TurkishMMLU?

- Concept: Multitask language understanding evaluation
  - Why needed here: TurkishMMLU is designed to evaluate multiple aspects of language understanding across different subjects
  - Quick check question: Why does TurkishMMLU cover nine different subjects instead of focusing on a single domain?

## Architecture Onboarding

- Component map: Dataset ingestion → Question parsing → Difficulty categorization → Model evaluation → Results analysis → Leaderboard generation
- Critical path: Question extraction from EBA platform → Manual quality verification → Difficulty calculation → Model evaluation setup → Performance reporting
- Design tradeoffs: Large-scale comprehensive evaluation vs. computational cost; expert-authored questions vs. automatic translation; single-choice format vs. open-ended assessment
- Failure signatures: Low correlation between TurkishMMLUsub and full set; parsing failures due to irregular question formatting; models exploiting pattern recognition rather than understanding
- First 3 experiments:
  1. Verify question parsing accuracy by randomly sampling 30 questions and checking formatting
  2. Test correlation between TurkishMMLUsub and full dataset using 5-shot accuracy across 32 open-source models
  3. Evaluate zero-shot performance of a baseline model (e.g., Llama-3 70B-IT) to establish performance floor

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TurkishMMLU models vary when multimodal questions involving images or audio are included?
- Basis in paper: [inferred] The paper mentions that TurkishMMLU is focused solely on text-based assessment and suggests exploring multimodal questions for future work.
- Why unresolved: The current dataset and evaluation are limited to text-based multiple-choice questions, excluding image or audio components.
- What evidence would resolve it: Creating a new version of TurkishMMLU that includes multimodal questions and evaluating model performance on this expanded dataset.

### Open Question 2
- Question: How would the performance of TurkishMMLU models change if the benchmark included more open-ended questions and assessments of generative abilities?
- Basis in paper: [inferred] The paper notes that TurkishMMLU covers multiple-choice questions from high school curriculum and university entrance exams, but suggests expanding to include generative abilities and more open-ended questions in future efforts.
- Why unresolved: The current benchmark is limited to multiple-choice format, which may not fully capture the generative capabilities of LLMs.
- What evidence would resolve it: Developing a new benchmark with open-ended questions and evaluating model performance on both the original TurkishMMLU and the new generative-focused benchmark.

### Open Question 3
- Question: To what extent does knowledge leakage from pre-training data affect the performance of models on TurkishMMLU?
- Basis in paper: [explicit] The paper mentions the potential risk of knowledge leakage, as some LLMs may have been pre-trained on datasets that overlap with or are sourced from similar data used in the benchmarks.
- Why unresolved: The paper acknowledges this risk but does not provide a method to quantify or mitigate its impact on model performance.
- What evidence would resolve it: Conducting a study to identify and remove overlapping data between pre-training corpora and TurkishMMLU, then re-evaluating model performance to measure the impact of knowledge leakage.

## Limitations

- The benchmark's cultural grounding mechanism assumes Turkish curriculum experts' questions are universally valid, but effectiveness for non-Turkish speakers remains unclear
- Difficulty calibration relies on student performance data that may not represent the general Turkish-speaking population or the types of language models being evaluated
- While the benchmark includes 10,032 questions, there's no evidence this scale is necessary or sufficient to prevent model overfitting or pattern exploitation

## Confidence

- **Claim: TurkishMMLU is the first large-scale multitask Turkish benchmark** - High confidence
- **Claim: GPT-4o and Claude-3 Opus significantly outperform other models** - Medium confidence
- **Claim: Mathematics is the most challenging subject** - Medium confidence

## Next Checks

1. Cross-validate the correctness ratio-based difficulty calibration against alternative methods such as expert ratings or statistical item analysis
2. Test TurkishMMLU performance on models trained with different data distributions to assess cultural grounding consistency
3. Conduct controlled experiments to determine whether models exploit specific question patterns rather than demonstrating genuine language understanding