---
ver: rpa2
title: 'Reading Subtext: Evaluating Large Language Models on Short Story Summarization
  with Writers'
arxiv_id: '2403.01061'
source_url: https://arxiv.org/abs/2403.01061
tags:
- stories
- story
- writers
- summary
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel evaluation framework for assessing
  large language models' (LLMs) understanding of short stories, focusing on narrative
  comprehension and thematic analysis. The authors collaborate directly with experienced
  writers to obtain unpublished stories and informed evaluations, ensuring the data
  remains unseen by the models during training.
---

# Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers

## Quick Facts
- arXiv ID: 2403.01061
- Source URL: https://arxiv.org/abs/2403.01061
- Authors: Melanie Subbiah; Sean Zhang; Lydia B. Chilton; Kathleen McKeown
- Reference count: 10
- Key outcome: This paper presents a novel evaluation framework for assessing large language models' (LLMs) understanding of short stories, focusing on narrative comprehension and thematic analysis. The authors collaborate directly with experienced writers to obtain unpublished stories and informed evaluations, ensuring the data remains unseen by the models during training. They evaluate three state-of-the-art LLMs (GPT-4, Claude-2.1, and Llama-2-70B) on 25 short stories across four key attributes: coverage, faithfulness, coherence, and analysis. The results reveal that while all models make faithfulness mistakes in over 50% of summaries and struggle with specificity and interpretation of difficult subtext, they demonstrate notable capabilities in thematic analysis when performing at their best. The study also demonstrates that LLM-based evaluations do not correlate well with writer judgments, highlighting the importance of human expertise in assessing narrative understanding. This work provides valuable insights into LLMs' strengths and limitations in processing complex narrative content and emphasizes the need for expert human evaluation in this domain.

## Executive Summary
This paper introduces a novel evaluation framework for assessing LLMs' understanding of short stories through direct collaboration with writers. The authors obtain 25 unpublished stories from 9 writers and evaluate three state-of-the-art LLMs (GPT-4, Claude-2.1, and Llama-2-70B) on their ability to summarize these narratives. The study focuses on four key attributes: coverage, faithfulness, coherence, and analysis, using writer evaluations as the gold standard. The results reveal that while LLMs demonstrate notable capabilities in thematic analysis, they struggle significantly with faithfulness, making errors in over 50% of summaries, particularly in interpreting character feelings and reactions. The study also highlights that LLM-based evaluations do not correlate well with human judgments, emphasizing the importance of expert human evaluation in assessing narrative understanding.

## Method Summary
The study involves recruiting 9 skilled writers to provide 25 unpublished short stories, ensuring the data remains unseen by models during training. Summaries are generated using GPT-4, Claude-2.1, and Llama-2-70B with specific prompts. For longer stories, Llama uses a chunk-then-summarize approach. Writers evaluate the summaries on four attributes: coverage, faithfulness, coherence, and analysis. The study also compares human evaluations with LLM-based evaluations and analyzes faithfulness errors using narrative theory categories (feeling, causation, action, character, setting). The methodology emphasizes ethical considerations, including informed consent and data protection protocols.

## Key Results
- All three models (GPT-4, Claude-2.1, Llama-2-70B) made faithfulness errors in over 50% of summaries.
- LLMs struggled most with interpreting character feelings and reactions, which accounted for the majority of faithfulness errors.
- LLM-based evaluations did not correlate well with writer judgments, highlighting the limitations of automatic metrics in narrative comprehension.
- When performing at their best, models demonstrated notable capabilities in thematic analysis and interpretation of subtext.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct collaboration with writers ensures access to unpublished stories that models have not seen during training.
- Mechanism: Writers submit stories that are not available online, and the study obtains informed consent while protecting the stories through secure storage and data deletion protocols.
- Core assumption: Writers are willing to share unpublished work under appropriate ethical and compensation conditions.
- Evidence anchors:
  - [abstract] "work directly with authors to ensure that the stories have not been shared online"
  - [section 3] "Each writer is given the opportunity to submit short stories that they have written and not published anywhere online"
  - [corpus] Weak - corpus contains related work but not direct evidence of this mechanism
- Break condition: If writers are unwilling to share unpublished work or if ethical requirements cannot be met, this mechanism fails.

### Mechanism 2
- Claim: Human writer evaluations provide more reliable assessment of narrative comprehension than automatic metrics.
- Mechanism: Writers rate summaries on four narrative attributes (coverage, faithfulness, coherence, analysis) using their expert understanding of their own stories.
- Core assumption: Writers have deep familiarity with their stories and can quickly and accurately judge summary quality.
- Evidence anchors:
  - [abstract] "obtain informed evaluations of summary quality using judgments from the authors themselves"
  - [section 5.1] "Each writer is shown the summaries of their own stories to evaluate as they are deeply familiar with the contents"
  - [section 6.3.4] "GPT-4 and Claude have moderate to substantial agreement with our human labelers"
- Break condition: If writers' judgments are inconsistent or if automatic metrics improve significantly, this mechanism's advantage diminishes.

### Mechanism 3
- Claim: Narrative theory categorization reveals specific types of faithfulness errors LLMs make.
- Mechanism: Faithfulness errors are categorized using narrative elements (feeling, causation, action, character, setting) based on established narrative theory frameworks.
- Core assumption: Narrative theory provides a useful framework for understanding LLM errors in narrative comprehension.
- Evidence anchors:
  - [section 5.2] "we ask annotators to categorize the instances of faithfulness errors from this list using elements from narrative theory"
  - [section 6.2] "we find that most faithfulness errors are in Feeling when averaged across the models"
  - [corpus] Weak - corpus contains related work but not direct evidence of this specific categorization approach
- Break condition: If categorization doesn't align with observed errors or if different frameworks prove more useful, this mechanism needs revision.

## Foundational Learning

- Concept: Narrative theory and its elements
  - Why needed here: The study uses narrative theory to categorize faithfulness errors and understand LLM limitations in narrative comprehension
  - Quick check question: Can you list the five narrative elements used to categorize faithfulness errors in this study?

- Concept: Human evaluation methodology for text summarization
  - Why needed here: The study relies on writer judgments rather than automatic metrics to evaluate summary quality
  - Quick check question: What four attributes did writers use to evaluate summaries, and why are these important for narrative understanding?

- Concept: LLM evaluation best practices and limitations
  - Why needed here: The study demonstrates that LLM-based evaluation doesn't correlate well with human judgment
  - Quick check question: According to the study, what correlation (if any) exists between LLM-assigned scores and writer-assigned scores for summary quality?

## Architecture Onboarding

- Component map:
  - Writer recruitment and consent management
  - Secure story collection and storage system
  - LLM API integration (GPT-4, Claude, Llama)
  - Human evaluation interface for writers
  - Error categorization pipeline with annotator interface
  - Data analysis and visualization components

- Critical path: Story collection → Summary generation → Writer evaluation → Error analysis → Results interpretation

- Design tradeoffs:
  - Using unpublished stories ensures no training data contamination but limits dataset size
  - Human evaluation provides expert judgment but is time-consuming and expensive
  - Multiple LLM evaluation methods provide comparison but increase complexity

- Failure signatures:
  - Writer recruitment issues → Insufficient stories for evaluation
  - LLM API failures → Incomplete summary generation
  - Evaluation interface problems → Incomplete or inaccurate writer feedback
  - Data protection failures → Ethical violations and loss of trust

- First 3 experiments:
  1. Test writer recruitment process with small group to validate compensation and consent procedures
  2. Run summary generation on sample stories to verify API integration and chunking strategy
  3. Pilot evaluation interface with writers to ensure clarity of instructions and usability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the models' faithfulness errors vary across different story styles (e.g., unreliable narrators, detailed subplots, nonlinear timelines)?
- Basis in paper: Inferred from the methodology section where the authors mention analyzing how writing style affects model summarization by comparing stories with reliable vs. unreliable narrators, detailed subplots vs. commonplace settings, and linear vs. nonlinear timelines.
- Why unresolved: While the paper mentions analyzing these aspects, the results section does not provide specific details on how faithfulness errors vary across these different story styles. This could provide valuable insights into which types of narrative elements are most challenging for LLMs to summarize accurately.
- What evidence would resolve it: Detailed error analysis results showing the distribution of faithfulness errors across different story styles, with examples of specific errors for each style.

### Open Question 2
- Question: How do the models' faithfulness errors correlate with the reading level of the stories?
- Basis in paper: Inferred from the methodology section where the authors mention using the Flesch-Kincaid readability test to assess the complexity of story wording and exploring whether the reading level affects summary quality.
- Why unresolved: The results section states that there is no significant correlation between story reading level and writer-assigned summary scores, but it does not provide information on whether there is a correlation between reading level and faithfulness errors specifically. This could reveal if more complex language leads to more faithfulness mistakes.
- What evidence would resolve it: Correlation analysis between story reading level and the number or severity of faithfulness errors in the summaries.

### Open Question 3
- Question: How do the models' faithfulness errors vary across different narrative elements (feeling, causation, action, character, setting)?
- Basis in paper: Explicit from the methodology section where the authors define categories for faithfulness errors based on narrative elements (feeling, causation, action, character, setting) and mention categorizing errors in each category.
- Why unresolved: The results section provides the percentage of faithfulness errors in each category but does not explore if these distributions vary across the different models or if certain categories are more prone to errors in specific models.
- What evidence would resolve it: Detailed analysis of faithfulness error distributions across models and narrative elements, with examples of specific errors in each category for each model.

## Limitations
- Small sample size of 25 stories from 9 writers limits generalizability of findings
- Unpublished stories may not represent the full diversity of published short fiction
- Human evaluation process is time-consuming and expensive, limiting scalability

## Confidence
- Faithfulness error analysis: High confidence
- Human vs. LLM evaluation comparison: High confidence
- Generalizability of thematic analysis capabilities: Medium confidence
- Correlation between reading level and summary quality: Medium confidence

## Next Checks
1. Validate writer recruitment process with a small pilot group to ensure ethical compliance and data protection protocols
2. Test LLM API integration with sample stories to verify summary generation and chunking strategies
3. Pilot human evaluation interface with writers to confirm clarity of instructions and usability