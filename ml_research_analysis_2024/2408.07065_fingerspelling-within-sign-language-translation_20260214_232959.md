---
ver: rpa2
title: Fingerspelling within Sign Language Translation
arxiv_id: '2408.07065'
source_url: https://arxiv.org/abs/2408.07065
tags:
- have
- were
- which
- they
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the role of fingerspelling within sign\
  \ language translation by annotating fingerspelling instances in FLEURS-ASL and\
  \ evaluating two approaches to improve recognition. The first approach uses ByT5,\
  \ a character-level tokenization model, instead of T5, a subword-level model, which\
  \ significantly improves both overall translation quality (BLEURT 38.8 \u2192 45.4)\
  \ and fingerspelling accuracy (CER 76.6% \u2192 41.6%)."
---

# Fingerspelling within Sign Language Translation

## Quick Facts
- arXiv ID: 2408.07065
- Source URL: https://arxiv.org/abs/2408.07065
- Reference count: 40
- Primary result: Character-level tokenization (ByT5) substantially improves fingerspelling recognition (CER 76.6% → 41.6%) and overall translation quality (BLEURT 38.8 → 45.4) in ASL translation

## Executive Summary
This paper investigates how to improve fingerspelling recognition within sign language translation by addressing the challenges of high-frequency motion and out-of-vocabulary terms. The author compares T5 (subword-level tokenization) with ByT5 (character-level tokenization) and finds that character-level tokenization dramatically improves both overall translation quality and fingerspelling accuracy. The paper also explores mixing fingerspelling recognition data into translation training, finding mixed results that suggest domain mismatch issues. The work highlights the importance of tokenization strategy for handling fingerspelling in sign language translation.

## Method Summary
The study compares two approaches to improve fingerspelling recognition in sign language translation: (1) using ByT5 with character-level tokenization instead of T5 with subword-level tokenization, and (2) mixing fingerspelling recognition data (FSboard) into the translation training mixture. The experiments use the FLEURS-ASL dataset with manually annotated fingerspelling instances (FLEURS-ASL-FS), pretraining on noisy YouTube-ASL data before finetuning on mixed clean YouTube-ASL data. Models are evaluated using BLEURT for overall translation quality and Character Error Rate (CER) for fingerspelling accuracy within translations.

## Key Results
- Character-level tokenization (ByT5) improves zero-shot sentence-level translation BLEURT from 38.8 to 45.4
- ByT5 reduces fingerspelling transcription CER from 76.6% to 41.6%
- Mixing FSboard fingerspelling recognition data into translation training yields mixed results (45.4 → 45.1 BLEURT, 41.6% → 42.1% CER)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Character-level tokenization in ByT5 improves fingerspelling recognition by making token boundaries match the actual spelling of fingerspelled words.
- Mechanism: ByT5 uses byte-level tokenization instead of subword tokenization, where each character in a fingerspelled word is its own token, allowing direct learning of the correspondence between video frames and character sequences.
- Core assumption: Fingerspelled words are more directly represented by individual characters than by subword tokens.
- Evidence anchors: Abstract states character-level tokenization "substantially improves understanding of fingerspelling (and therefore translation quality overall)"; section confirms "substantially improves understanding of fingerspelling."
- Break condition: If fingerspelled words are frequently shortened or abbreviated in ways that don't match their full spelling.

### Mechanism 2
- Claim: Character-level tokenization improves translation quality overall by providing more granular word structure information.
- Mechanism: ByT5's character-level tokenization exposes the model to individual characters within words, enabling better learning of spelling patterns and word composition, particularly helpful for out-of-vocabulary terms and proper nouns.
- Core assumption: The model can leverage additional character-level information to improve understanding of word structure and composition.
- Evidence anchors: Abstract links character-level tokenization to improved translation quality overall; section shows major improvements in both BLEURT and CER scores.
- Break condition: If the dataset contains high proportions of common words well-represented in subword vocabulary.

### Mechanism 3
- Claim: Mixing fingerspelling recognition data yields mixed results due to domain mismatch between tasks.
- Mechanism: Fingerspelling recognition data (FSboard) is collected in different contexts (mobile text entry) than fingerspelling within full sign language sentences, making knowledge transfer difficult.
- Core assumption: Differences in context and task outweigh potential benefits of additional training data.
- Evidence anchors: Abstract states "effect of 2) is mixed" regarding mixing fingerspelling recognition data; section reports regression in both BLEURT and CER scores.
- Break condition: If domain mismatch is reduced by collecting fingerspelling recognition data in more natural signing contexts.

## Foundational Learning

- Concept: Fingerspelling
  - Why needed here: Fingerspelling represents words from spoken languages that don't have native signs in sign language, making it crucial for translation models.
  - Quick check question: What is the primary purpose of fingerspelling in sign language?

- Concept: Tokenization
  - Why needed here: Tokenization strategy significantly impacts model performance on fingerspelling tasks, with character-level vs subword-level approaches having different strengths.
  - Quick check question: What is the difference between character-level and subword-level tokenization?

- Concept: BLEURT and CER
  - Why needed here: These evaluation metrics assess translation quality (BLEURT) and fingerspelling accuracy (CER) in sign language translation.
  - Quick check question: What do BLEURT and CER measure in the context of sign language translation?

## Architecture Onboarding

- Component map: Sign language video (MediaPipe Holistic landmarks) -> T5/ByT5 model -> English translation
- Critical path: 1) Preprocess video into MediaPipe Holistic landmarks, 2) Feed landmarks into T5/ByT5 model, 3) Generate English translation, 4) Evaluate using BLEURT and CER
- Design tradeoffs: Character-level tokenization provides more granular word information but increases sequence length and computational cost; mixing fingerspelling data provides additional training data but may introduce domain mismatch.
- Failure signatures: Low BLEURT scores indicate poor overall translation quality; high CER scores indicate poor fingerspelling recognition; slow convergence suggests model architecture or training data issues.
- First 3 experiments: 1) Train baseline T5 model on noisy YouTube-ASL, 2) Train ByT5 model on same dataset and compare performance, 3) Mix FSboard data into training mixture and evaluate impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does character-level tokenization significantly improve translation quality for sign languages other than ASL with less frequent fingerspelling?
- Basis in paper: The paper shows ByT5 substantially improves ASL translation but explicitly states character-level modeling would have reduced effect on overall translation quality where fingerspelling is less frequent.
- Why unresolved: Only tested on ASL data; authors expect reduced effect on overall translation quality for sign languages with less fingerspelling.
- What evidence would resolve it: Direct experiments comparing T5 vs ByT5 on translation datasets for other sign languages (BSL, LSQ) with different fingerspelling frequencies.

### Open Question 2
- Question: Does mixing fingerspelling recognition data provide benefits beyond what character-level tokenization achieves?
- Basis in paper: Mixing FSboard yields mixed/negative results (45.4 → 45.1 BLEURT, 41.6% → 42.1% CER), unlike substantial ByT5 improvements; authors speculate domain mismatch issues.
- Why unresolved: Experiment shows cotraining doesn't help, but authors don't conclusively determine whether this is due to character-level tokenization already solving the problem or the specific dataset used.
- What evidence would resolve it: Experiments using fingerspelling recognition data from natural signing contexts versus ByT5 with same training mixture minus fingerspelling data.

### Open Question 3
- Question: How does LLM-based autorater quality affect measured fingerspelling performance compared to human annotation?
- Basis in paper: Paper acknowledges autorater may overestimate character-level error but states estimates should be unbiased for comparing conditions; validates quality in Appendix A but doesn't quantify bias magnitude.
- Why unresolved: Uses autorater for scalability but acknowledges it's not unambiguously superior to human raters and may introduce systematic errors.
- What evidence would resolve it: Direct comparison between LLM autorater and human annotators on same test examples, measuring correlation and systematic differences in extracted spans and CER scores.

## Limitations

- Small test set size (24 fingerspelling instances) makes CER improvements potentially unreliable and may introduce noise in evaluation of mixed training approach.
- The study only tests on ASL data, limiting generalizability to other sign languages with different fingerspelling frequencies and patterns.
- Limited exploration of alternative mixing strategies for fingerspelling recognition data, making it unclear whether negative results are due to data itself or mixing approach.

## Confidence

**High Confidence:** The claim that character-level tokenization (ByT5) substantially improves fingerspelling recognition accuracy is well-supported by the reported CER improvement from 76.6% to 41.6% and BLEURT improvement from 38.8 to 45.4, based on proper evaluation protocols with clear baselines.

**Medium Confidence:** The mechanism explanation that character-level tokenization works because it "matches the actual spelling of fingerspelled words" is plausible but not definitively proven, as the paper doesn't test alternative explanations or conduct ablation studies.

**Low Confidence:** The claim that mixing FSboard data yields "mixed or negative results" is based on limited evidence due to the small test set size, and the paper doesn't explore whether different mixing ratios or training strategies might yield better results.

## Next Checks

1. **Expand evaluation with larger test sets:** Collect or annotate additional fingerspelling instances to create a larger test set (at least 100 instances) to reduce variance in CER measurements and provide more reliable evaluation.

2. **Conduct ablation studies on tokenization effects:** Train models with different tokenization granularities (character, byte, subword) on the same data to isolate whether improvements are specifically due to better fingerspelling handling or general improvements in handling out-of-vocabulary terms.

3. **Explore alternative mixing strategies for FSboard data:** Test different mixing ratios, curriculum learning approaches, or domain adaptation techniques when incorporating FSboard data to determine if negative results are due to mixing strategy rather than the data itself.