---
ver: rpa2
title: Memorization in Self-Supervised Learning Improves Downstream Generalization
arxiv_id: '2401.12233'
source_url: https://arxiv.org/abs/2401.12233
tags:
- memorization
- data
- training
- points
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SSLMem, a method to define and measure memorization
  in self-supervised learning (SSL) encoders. It addresses the gap in existing memorization
  definitions that rely on labels, which are absent in SSL.
---

# Memorization in Self-Supervised Learning Improves Downstream Generalization

## Quick Facts
- arXiv ID: 2401.12233
- Source URL: https://arxiv.org/abs/2401.12233
- Authors: Wenhao Wang; Muhammad Ahmad Kaleem; Adam Dziedzic; Michael Backes; Nicolas Papernot; Franziska Boenisch
- Reference count: 40
- Primary result: Memorization is essential for SSL encoders to achieve higher generalization performance on downstream tasks.

## Executive Summary
This paper introduces SSLMem, a method to define and measure memorization in self-supervised learning (SSL) encoders, addressing the gap in existing memorization definitions that rely on labels. The core idea is to compare the alignment of representations for data points and their augmentations between encoders trained with and without those data points. The authors find that even with large datasets and strong augmentations, significant memorization occurs, especially for atypical data points, and that this memorization improves downstream generalization across various tasks and datasets. Notably, limiting memorization through regularization harms downstream accuracy, highlighting memorization as a key property of SSL crucial for its success on diverse downstream tasks.

## Method Summary
The authors propose SSLMem, a method to measure memorization in SSL encoders by comparing the alignment of representations for data points and their augmentations between two encoder families: one trained on the full dataset and another trained without the candidate data points. This leverages the common property of alignment in SSL frameworks. The method defines a data point as memorized if its alignment is significantly higher on the encoder trained with it than on the encoder trained without it. The authors evaluate memorization's impact on downstream generalization by removing memorized points and measuring accuracy drops, and by adding alignment penalties to the loss to limit memorization.

## Key Results
- Even with large datasets and strong augmentations, significant memorization occurs, especially for atypical data points.
- Memorization improves downstream generalization across various tasks and datasets.
- Limiting memorization through regularization harms downstream accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memorization in SSL encoders is detectable by measuring changes in representation alignment for augmented views between encoders trained with and without specific data points.
- Mechanism: SSL methods all rely on data augmentations and alignment as a common property. When a data point is removed from training, the alignment of its augmented views degrades more than for points never seen, indicating memorization. This is formalized by comparing alignment losses across two encoder families (trained with vs. without the candidate point).
- Core assumption: Alignment loss is a reliable proxy for the influence of individual data points in the absence of labels.
- Evidence anchors:
  - [abstract]: "Our definition compares the difference in alignment of representations for data points and their augmented views returned by both encoders that were trained on these data points and encoders that were not."
  - [section 3.2]: "we consider a data point as having a high level of memorization by an encoder f if its alignment is significantly higher on f than on encoder g that was not trained with the considered data point."
- Break condition: If augmentation sets do not overlap between points (e.g., for outliers), alignment loss differences may not reflect memorization reliably.

### Mechanism 2
- Claim: Higher memorization scores correlate with improved downstream generalization, even across different data distributions and task types.
- Mechanism: Memorized atypical or outlier data points expand the effective coverage of the representation space. When encoders generalize, they can map novel inputs into learned manifolds anchored by these memorized points, improving performance. Removing memorized points harms accuracy more than removing random points.
- Core assumption: Representation space benefits from memorizing rare or atypical data points because they act as anchors for better interpolation.
- Evidence anchors:
  - [abstract]: "Through our empirical results, we show that this memorization is essential for encoders to achieve higher generalization performance on different downstream tasks."
  - [section 4.4]: "removing the memorized data points harms downstream accuracy stronger than the removal of random data points."
- Break condition: If regularization forces alignment too strongly, memorization may drop without harming accuracy, suggesting the link is not always monotonic.

### Mechanism 3
- Claim: Different SSL methods memorize similar types of data points (especially atypical ones), but supervised learning memorizes different points than SSL.
- Mechanism: SSL encoders trained via alignment-based objectives naturally memorize points whose augmentations differ strongly from others. Supervised learning, by contrast, memorizes based on label-specific signals. SSLMem reveals this divergence by measuring alignment changes rather than label-based influence.
- Core assumption: The type of memorization signal (alignment vs. label) determines which data points are memorized.
- Evidence anchors:
  - [section 4.2]: "we still observe that atypical data points experience higher levels of memorization than typical ones, a result similar to the supervised setting."
  - [section 4.4]: "we also find that while different SSL methods and encoder architectures exhibit high memorization on a similar set of data points, the data points that are memorized in supervised learning differ substantially."
- Break condition: If augmentation strategies or encoder architectures differ drastically, memorization overlap between SSL methods may break down.

## Foundational Learning

- Concept: Alignment loss as a measure of representation consistency across augmentations.
  - Why needed here: Core to detecting memorization in SSL where labels are absent.
  - Quick check question: If an encoder returns very different representations for two augmentations of the same image, is its alignment loss high or low?

- Concept: Leave-one-out analysis in model training.
  - Why needed here: Allows isolating the effect of individual data points on encoder behavior without labels.
  - Quick check question: What happens to alignment loss on a data point when that point is removed from training?

- Concept: Linear probing as a proxy for downstream generalization.
  - Why needed here: Standard way to evaluate SSL encoders without retraining the full model.
  - Quick check question: Why is a linear layer trained on frozen encoder representations a good proxy for encoder quality?

## Architecture Onboarding

- Component map: Encoder (ViT or ResNet) -> Augmentation pipeline -> Representation space -> Alignment loss calculator -> Memorization scorer (SSLMem) -> Downstream evaluator (linear probe / fine-tuning)
- Critical path: Data augmentation -> Encoder forward pass -> â„“2 distance computation -> Memorization score aggregation -> Interpretation
- Design tradeoffs: Larger candidate sets give more stable scores but cost more compute; smaller overlap between f and g reduces confounding but weakens signal
- Failure signatures: Memorization scores near zero for all points; high variance across runs; memorization increasing with training time but accuracy plateauing
- First 3 experiments:
  1. Train two MAE encoders on disjoint 80%/10%/10% splits of CIFAR10; compute SSLMem scores for the 10% candidate set.
  2. Remove top 5% memorized vs. random points; evaluate linear probing accuracy drop.
  3. Add alignment penalty to loss; measure effect on both memorization and downstream accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- The assumption that alignment loss is a universally reliable proxy for memorization in SSL encoders may be less effective for data points with minimal augmentation overlap or for SSL frameworks that do not explicitly optimize alignment.
- The leave-one-out analysis may not fully capture complex dependencies between data points in large-scale training.
- The generalizability of results across diverse SSL architectures and downstream tasks remains to be fully established.

## Confidence

- **High Confidence**: The empirical demonstration that memorization correlates with downstream accuracy and that removing memorized points harms performance. The SSLMem method's operationalization is clearly defined and validated.
- **Medium Confidence**: The mechanism by which atypical data points improve representation space coverage through memorization. While supported by evidence, alternative explanations for the observed generalization benefits cannot be fully ruled out.
- **Medium Confidence**: The claim that different SSL methods memorize similar atypical points, while supervised learning memorizes different points. The comparison is insightful but based on limited direct comparisons.

## Next Checks
1. Apply SSLMem to additional SSL methods (e.g., DINO, BYOL) and diverse encoder architectures to test the robustness of memorization patterns across the SSL landscape.
2. Systematically vary augmentation strength and diversity to determine how these factors influence memorization scores and downstream generalization, isolating the effect of augmentation design.
3. Implement targeted regularization to reduce memorization of specific atypical points and measure the precise impact on downstream task performance, testing the causal link between memorization and generalization.