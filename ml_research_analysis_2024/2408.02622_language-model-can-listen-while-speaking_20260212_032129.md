---
ver: rpa2
title: Language Model Can Listen While Speaking
arxiv_id: '2408.02622'
source_url: https://arxiv.org/abs/2408.02622
tags:
- speech
- lslm
- language
- arxiv
- listening
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LSLM, a speech language model that can listen
  while speaking, addressing the challenge of real-time turn-taking in spoken dialogue
  systems. The core idea is to combine a token-based decoder-only TTS model for speech
  generation with a streaming self-supervised learning encoder for real-time audio
  input, fusing both channels for autoregressive generation and turn-taking detection.
---

# Language Model Can Listen While Speaking

## Quick Facts
- **arXiv ID**: 2408.02622
- **Source URL**: https://arxiv.org/abs/2408.02622
- **Reference count**: 40
- **Primary result**: LSLM achieves 4.05% WER and 98.00% F1 in command-based, and 5.33% WER and 95.50% F1 in voice-based full duplex evaluation.

## Executive Summary
This paper introduces LSLM, a speech language model that can listen while speaking, addressing the challenge of real-time turn-taking in spoken dialogue systems. The core idea is to combine a token-based decoder-only TTS model for speech generation with a streaming self-supervised learning encoder for real-time audio input, fusing both channels for autoregressive generation and turn-taking detection. Three fusion strategies—early, middle, and late fusion—are explored, with middle fusion achieving the best balance between speech generation and real-time interaction. The model is evaluated in two settings: command-based and voice-based full duplex modeling. Results show that LSLM achieves a word error rate of 4.05% and an F1 score of 98.00% in the command-based setting, and a word error rate of 5.33% and an F1 score of 95.50% in the voice-based setting, demonstrating robustness to noise and sensitivity to diverse instructions.

## Method Summary
LSLM combines a token-based decoder-only TTS model with a streaming self-supervised learning encoder to enable real-time speech generation and turn-taking. The system fuses audio input and generated tokens via early, middle, or late fusion strategies, allowing the model to listen while speaking. Three key experiments validate LSLM's performance in command-based and voice-based full duplex settings, showing low word error rates and high F1 scores. The model's robustness to noise and sensitivity to instructions are also demonstrated.

## Key Results
- LSLM achieves a word error rate of 4.05% and an F1 score of 98.00% in the command-based setting.
- LSLM achieves a word error rate of 5.33% and an F1 score of 95.50% in the voice-based setting.
- The model demonstrates robustness to noise and sensitivity to diverse instructions.

## Why This Works (Mechanism)
LSLM's effectiveness stems from its ability to fuse real-time audio input with generated speech tokens using streaming encoders and decoder-only TTS models. The middle fusion strategy allows the model to balance speech generation and turn-taking by integrating audio and token information at an intermediate stage. This design enables LSLM to detect turn-taking cues and generate appropriate responses while speaking, facilitating natural, real-time dialogue.

## Foundational Learning
- **Streaming self-supervised learning encoder**: Needed for real-time audio input processing; quick check: ensure low latency and high accuracy in speech recognition.
- **Token-based decoder-only TTS**: Needed for efficient speech generation; quick check: verify naturalness and clarity of generated speech.
- **Fusion strategies (early, middle, late)**: Needed to combine audio and token information; quick check: compare performance and latency across strategies.
- **Turn-taking detection**: Needed for real-time dialogue management; quick check: assess accuracy in identifying turn boundaries.
- **Full duplex modeling**: Needed for bidirectional communication; quick check: evaluate responsiveness and coherence in dialogue.
- **Robustness to noise**: Needed for real-world applicability; quick check: test performance under varying noise conditions.

## Architecture Onboarding
- **Component map**: Streaming encoder -> Fusion module -> Decoder-only TTS -> Speech output; Audio input -> Streaming encoder -> Fusion module
- **Critical path**: Audio input -> Streaming encoder -> Fusion module -> Decoder-only TTS -> Speech output
- **Design tradeoffs**: Early fusion offers low latency but may compromise speech quality; late fusion preserves speech quality but increases latency; middle fusion balances both.
- **Failure signatures**: High latency in turn-taking, degraded speech quality, or missed turn boundaries.
- **First experiments**: (1) Measure end-to-end latency in streaming conditions with varying speaking speeds; (2) Test noise robustness across multiple noise types and SNR levels; (3) Validate in an open-domain, multi-turn dialogue benchmark with human evaluators.

## Open Questions the Paper Calls Out
None

## Limitations
- True real-time, bidirectional streaming efficacy remains uncertain due to limited evaluation of end-to-end latency and synchronization overhead.
- Fusion strategies are compared only in offline, fixed-length conditions, leaving open questions about their performance under varying speaking rates and background noise.
- ASR integration for command-based evaluation may introduce an additional error layer not fully characterized.

## Confidence
- **High confidence**: Core architectural contribution and controlled evaluation results.
- **Medium confidence**: Real-world turn-taking efficacy.
- **Low confidence**: Noise robustness and scalability to open-domain dialogue.

## Next Checks
1. Measure full end-to-end latency in streaming conditions with varying speaking speeds.
2. Test noise robustness across multiple noise types and SNR levels.
3. Validate in an open-domain, multi-turn dialogue benchmark with human evaluators.