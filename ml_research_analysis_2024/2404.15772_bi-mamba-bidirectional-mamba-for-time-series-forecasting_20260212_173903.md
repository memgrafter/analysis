---
ver: rpa2
title: 'Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting'
arxiv_id: '2404.15772'
source_url: https://arxiv.org/abs/2404.15772
tags:
- series
- mamba
- time
- bi-mamba
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses long-term time series forecasting by proposing
  Bi-Mamba+, an improved state-space model that combines bidirectional Mamba blocks
  with a series-relation-aware tokenization strategy. The key innovation is a forget
  gate in Mamba+ that selectively preserves historical information, and an SRA decider
  that automatically chooses between channel-independent or channel-mixing tokenization
  based on dataset characteristics.
---

# Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting

## Quick Facts
- arXiv ID: 2404.15772
- Source URL: https://arxiv.org/abs/2404.15772
- Reference count: 40
- Primary result: Outperforms state-of-the-art methods on 8 real-world datasets, achieving average MSE and MAE reductions of 4.72% and 2.60% compared to iTransformer

## Executive Summary
Bi-Mamba+ addresses long-term time series forecasting by extending Mamba with bidirectional processing and a series-relation-aware tokenization strategy. The method introduces a forget gate in Mamba+ that selectively combines new and historical features, and applies Mamba+ in both forward and backward directions to capture richer temporal patterns. A key innovation is the SRA decider, which automatically selects between channel-independent and channel-mixing tokenization strategies based on dataset characteristics. Extensive experiments on 8 real-world datasets demonstrate superior performance over state-of-the-art methods while maintaining linear computational complexity.

## Method Summary
Bi-Mamba+ is a state-space model that combines bidirectional Mamba+ blocks with adaptive tokenization. The core Mamba+ block adds a forget gate that preserves historical information through complementary combination with new features. Two Mamba+ blocks process sequences in opposite directions, with their outputs combined additively. The SRA decider analyzes pairwise Spearman correlations between time series to automatically choose between channel-independent (for loosely correlated series) and channel-mixing (for highly correlated series) tokenization strategies. Time series are divided into patches to create semantically richer tokens, and the entire architecture maintains linear computational complexity through the state-space model foundation.

## Key Results
- Achieves average MSE reduction of 4.72% and MAE reduction of 2.60% compared to iTransformer across 8 datasets
- Maintains linear computational complexity versus quadratic complexity of attention-based methods
- Demonstrates effectiveness across diverse domains including weather, traffic, electricity, and solar datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The forget gate in Mamba+ selectively preserves historical information by combining new features with historical features in a complementary manner.
- Mechanism: The forget gate (1 - σ(z)) is multiplied with the 1-D convolutional output (x′), allowing new features to be added while simultaneously retaining relevant historical information through the SSM branch.
- Core assumption: Preserving historical information through complementary combination is more effective than direct filtering for long-term forecasting.
- Evidence anchors:
  - [abstract]: "we design a novel Mamba+ block by adding a forget gate inside Mamba to selectively combine the new features with the historical features in a complementary manner"
  - [section]: "Although the HIPPO matrix(Gu et al. 2021b) embedded in the SSM block can retain a fairly long-term historical information, the obtained result is filtered directly through the gate of another branch, resulting in the tendency to prioritize proximal information"
- Break condition: If the complementary combination introduces too much noise from irrelevant historical features, degrading forecasting accuracy.

### Mechanism 2
- Claim: Bidirectional processing captures richer temporal patterns by modeling sequences in both forward and backward directions.
- Mechanism: Two Mamba+ blocks process the same input sequence in opposite directions, with their outputs combined through addition, allowing the model to capture temporal dependencies that might be missed in a single direction.
- Core assumption: Time series data contains meaningful patterns in both temporal directions that can improve forecasting when combined.
- Evidence anchors:
  - [abstract]: "we apply Mamba+ both forward and backward and propose Bi-Mamba+, aiming to promote the model's ability to capture interactions among time series elements"
  - [section]: "Considering the rich evolutionary patterns of time series in different directions as well as the complexity of inter-series dependencies, we design a bidirectional Mamba+ structure"
- Break condition: If backward modeling introduces artifacts or the combined representation becomes less coherent than single-direction processing.

### Mechanism 3
- Claim: SRA decider automatically selects optimal tokenization strategy based on dataset characteristics.
- Mechanism: The decider calculates Spearman correlation coefficients between time series pairs, counts highly correlated pairs (ρλ_max) and loosely correlated pairs (ρ0_max), then chooses channel-mixing when r ≥ 1 - λ, otherwise channel-independent.
- Core assumption: Datasets with different variable counts and correlation structures benefit from different tokenization strategies.
- Evidence anchors:
  - [abstract]: "we propose a series-relation-aware decider that controls the utilization of channel-independent or channel-mixing tokenization strategy for specific datasets"
  - [section]: "We then use threshold λ and 0 to filter out series pairs with positive correlation. Finally, we count the maximum number of relevant series ρλ_max and ρ0_max in the training set and calculate the relation ratio r = ρλ_max/ρ0_max"
- Break condition: If the Spearman-based threshold λ is poorly chosen, leading to suboptimal tokenization strategy selection.

## Foundational Learning

- Concept: State Space Models (SSM) fundamentals
  - Why needed here: Bi-Mamba+ builds on Mamba, which is an SSM variant. Understanding SSM discretization, recurrence, and parallel computation is essential for implementing the Mamba+ block.
  - Quick check question: What is the difference between continuous and discretized SSM formulations, and why does discretization enable parallel training?

- Concept: Spearman correlation coefficient
  - Why needed here: The SRA decider uses Spearman correlation to measure monotonic relationships between time series. Understanding rank-based correlation is crucial for implementing the decider logic.
  - Quick check question: How does Spearman correlation differ from Pearson correlation, and why is it more suitable for non-linear relationships in time series?

- Concept: Patch-based tokenization for time series
  - Why needed here: Bi-Mamba+ divides time series into patches to create semantically richer tokens. Understanding the trade-off between patch size and temporal resolution is important for hyperparameter tuning.
  - Quick check question: How does patch length affect the balance between capturing local patterns and maintaining global context in time series forecasting?

## Architecture Onboarding

- Component map:
  - Input normalization (RevIN) → Patch division → SRA decider → Tokenization (channel-independent/mixing) → Bi-Mamba+ encoders (forward/backward Mamba+ blocks) → Residual connections → Linear projection → Output
  - Key modules: Mamba+ block (with forget gate), Bi-Mamba+ encoder (bidirectional structure), SRA decider (correlation-based strategy selection)

- Critical path: Input → Patch division → Tokenization → Bi-Mamba+ encoder → Output
  - The Bi-Mamba+ encoder is the core computational bottleneck, containing two Mamba+ blocks per layer
  - Memory usage scales linearly with sequence length due to SSM architecture

- Design tradeoffs:
  - Patch length vs. granularity: Smaller patches capture finer patterns but increase computational cost
  - Hidden state dimension vs. capacity: Larger dimensions improve modeling but increase memory usage
  - λ threshold in SRA decider vs. strategy selection: Different thresholds lead to different tokenization strategies, affecting performance on different datasets

- Failure signatures:
  - Performance degradation with increasing look-back window suggests insufficient long-term dependency modeling
  - High variance across runs indicates sensitivity to initialization or insufficient regularization
  - Memory errors during training suggest hidden state dimension is too large for available GPU memory

- First 3 experiments:
  1. Validate SRA decider: Run with λ = 0.2, 0.6, 0.8 on a small dataset and verify that strategy selection changes appropriately with correlation threshold
  2. Test forget gate effectiveness: Compare Bi-Mamba+ with original Mamba on a dataset with known long-term dependencies, measuring improvement in forecasting accuracy
  3. Evaluate bidirectional benefit: Run Bi-Mamba+ with forward-only and bidirectional modes on the same dataset, quantifying performance difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SRA decider's threshold λ affect the model's performance across different datasets with varying numbers of variables?
- Basis in paper: [explicit] The paper mentions that λ is set to 0.6 and shows decision ratios for different datasets, but does not provide a systematic sensitivity analysis of λ's impact on forecasting accuracy.
- Why unresolved: The paper only shows how λ affects the tokenization strategy choice but does not demonstrate how different λ values impact the actual forecasting performance across various datasets.
- What evidence would resolve it: A comprehensive study showing MSE/MAE performance for multiple λ values (e.g., 0.2, 0.4, 0.6, 0.8) across all datasets, with statistical significance testing.

### Open Question 2
- Question: What is the optimal patch length P for different types of time series datasets, and how does it vary with dataset characteristics?
- Basis in paper: [explicit] The paper conducts experiments with different patch lengths (1/2L, 1/4L, 1/8L, 1/12L, 1/24L) but only provides results for a subset of datasets and does not establish clear guidelines for choosing P based on dataset properties.
- Why unresolved: While the paper shows that patch length affects performance, it does not provide a systematic framework for determining optimal P based on dataset characteristics such as the number of variables, stationarity, or periodicity.
- What evidence would resolve it: A comprehensive analysis mapping dataset characteristics to optimal patch lengths, including guidelines for selecting P based on statistical properties of the time series.

### Open Question 3
- Question: How does the forget gate in Mamba+ compare to other gating mechanisms in terms of preserving long-term dependencies?
- Basis in paper: [explicit] The paper introduces a forget gate in Mamba+ and claims it preserves historical information better than the original Mamba, but does not compare it to other gating mechanisms like LSTM forget gates or GRU update gates.
- Why unresolved: The paper only compares Mamba+ to the original Mamba and does not provide a comprehensive comparison with other gating mechanisms used in sequence modeling.
- What evidence would resolve it: A comparative study of Mamba+, original Mamba, LSTM, and GRU on the same LTSF tasks, measuring their ability to capture long-term dependencies using metrics like gradient flow or attention span.

## Limitations
- Reproducibility challenges due to unspecified hardware-aware parallel computing algorithm and CUDA modifications
- Limited ablation studies on component contributions to overall performance gains
- Evaluation focused on 8 specific datasets without thorough validation across diverse time series domains

## Confidence

**High confidence claims**:
- Bi-Mamba+ achieves state-of-the-art performance on the tested datasets, with average MSE and MAE improvements of 4.72% and 2.60% over iTransformer
- The Mamba+ block with forget gate effectively preserves historical information while incorporating new features
- Linear computational complexity compared to quadratic complexity of attention-based methods

**Medium confidence claims**:
- The bidirectional structure consistently improves performance across all datasets
- The SRA decider appropriately selects between channel-independent and channel-mixing tokenization strategies based on dataset characteristics
- The forget gate mechanism is essential for long-term forecasting accuracy

**Low confidence claims**:
- The specific value λ = 0.8 is optimal for all datasets and use cases
- The patch-based tokenization strategy with length 4 is universally effective across different time series characteristics

## Next Checks

1. **SRA decider sensitivity analysis**: Systematically vary the threshold λ from 0.2 to 0.8 on multiple datasets and measure how tokenization strategy selection affects forecasting accuracy. This will validate whether λ = 0.8 is truly optimal or dataset-dependent.

2. **Component ablation study**: Train Bi-Mamba+ with individual components disabled (no forget gate, unidirectional only, fixed tokenization strategy) and compare performance degradation. This will quantify the contribution of each innovation to overall performance gains.

3. **Cross-dataset generalization**: Evaluate Bi-Mamba+ on additional time series datasets with different characteristics (e.g., financial, sensor, medical) to assess whether the SRA decider and bidirectional structure generalize beyond the 8 tested datasets.