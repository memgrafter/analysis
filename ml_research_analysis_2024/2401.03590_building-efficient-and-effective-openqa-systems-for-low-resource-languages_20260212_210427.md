---
ver: rpa2
title: Building Efficient and Effective OpenQA Systems for Low-Resource Languages
arxiv_id: '2401.03590'
source_url: https://arxiv.org/abs/2401.03590
tags:
- openqa
- question
- retriever
- dataset
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that high-performing OpenQA systems can be built
  for low-resource languages like Turkish without requiring a gold training dataset.
  The key idea is to leverage machine-translated training data and large-scale unlabeled
  knowledge sources, using weakly supervised learning to train effective retrievers
  and readers.
---

# Building Efficient and Effective OpenQA Systems for Low-Resource Languages

## Quick Facts
- arXiv ID: 2401.03590
- Source URL: https://arxiv.org/abs/2401.03590
- Reference count: 40
- Primary result: High-performing OpenQA systems can be built for low-resource languages without gold training data using machine-translated datasets and weakly supervised learning

## Executive Summary
This paper demonstrates that efficient and effective OpenQA systems can be built for low-resource languages like Turkish without requiring expensive gold training datasets. The authors leverage machine-translated SQuAD2.0 data to create SQuAD-TR, and use weak supervision to train retriever and reader models. Their ColBERT-QA-based system achieves 24-32% improvement in Exact Match and 22-29% improvement in F1 score over BM25 and DPR baselines. The study also shows that only a few hundred gold evaluation examples are needed to reliably benchmark OpenQA systems.

## Method Summary
The authors build an OpenQA system for Turkish using machine-translated SQuAD2.0 data (SQuAD-TR-TRAIN) and Turkish Wikipedia dumps as knowledge sources. They employ weak supervision by first training BM25 retrievers, then using these to bootstrap training for DPR and ColBERT-QA models. XLM-RoBERTa readers are trained on retriever-specific datasets, and the system is evaluated on XQuAD-TR, a human-translated Turkish evaluation set. The approach eliminates the need for gold passage annotations during training.

## Key Results
- ColBERT-QA achieves 24-32% improvement in Exact Match over BM25 and DPR baselines
- F1 score improvements of 22-29% for ColBERT-QA compared to traditional retrievers
- Only 200-500 gold evaluation examples needed for reliable benchmarking of OpenQA systems
- Machine-translated training data sufficient for effective retriever and reader training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine-translated training data can effectively substitute for gold training data in OpenQA systems for low-resource languages
- Mechanism: Weak supervision from automatically translated QA datasets provides sufficient training signal for retriever models, which then enable effective reader models without requiring gold passages
- Core assumption: Translation noise does not critically impair the semantic alignment between questions and answers needed for training
- Evidence anchors:
  - [abstract]: "effective, low-cost OpenQA systems can be developed for low-resource contexts... weak supervision using machine-translated labeled datasets"
  - [section 3.1.1]: Describes successful post-processing to recover 61K out of 81K answer spans after automatic translation
- Break condition: If translation quality is so poor that semantic relationships between questions and answers are lost

### Mechanism 2
- Claim: A few hundred gold evaluation examples are sufficient to reliably benchmark OpenQA systems
- Mechanism: Subsampling experiments show that even small test sets (100-200 examples) can differentiate system performance with acceptable variance
- Core assumption: Performance differences between models are large enough to be detectable with small sample sizes
- Evidence anchors:
  - [section 4.2.3]: "with only 100 examples, we can already pretty clearly differentiate our BM25-based and DPR-based models from our ColBERT-QA-based models"
  - [abstract]: "we show that only a few hundred gold assessment examples are needed to reliably evaluate these systems"
- Break condition: If model performance differences are subtle or if the evaluation set doesn't capture the diversity of the test distribution

### Mechanism 3
- Claim: Expanding knowledge sources can improve or degrade OpenQA performance depending on retriever capability to handle noise
- Mechanism: ColBERT-QA's token-level interaction and semantic matching better suppresses noise from irrelevant passages compared to BM25 and DPR as knowledge sources grow
- Core assumption: The retriever's ability to distinguish relevant from irrelevant passages determines whether knowledge expansion helps or hurts
- Evidence anchors:
  - [section 4.2.1]: "ColBERT-QA seems to be better able to suppress these interfering factors and benefit from the additional relevant data"
  - [abstract]: "increasing the size of unstructured knowledge sources can have varying effects on the performance of OpenQA systems, depending on the ability of the retriever systems to manage noise effectively"
- Break condition: If the knowledge source expansion introduces noise faster than the retriever can learn to suppress it

## Foundational Learning

- Concept: Weak supervision in machine learning
  - Why needed here: The paper relies on automatically translated data without gold passage annotations, requiring understanding of how models can learn from noisy labels
  - Quick check question: How does weak supervision differ from supervised learning, and what are its main advantages and limitations?

- Concept: Token-level interactions in neural retrieval
  - Why needed here: ColBERT-QA's late interaction mechanism is central to its superior performance, requiring understanding of how token-level similarity differs from document-level similarity
  - Quick check question: What is the key difference between ColBERT's late interaction approach and traditional dense retrievers like DPR?

- Concept: Cross-lingual transfer learning
  - Why needed here: The paper transfers QA capabilities from English to Turkish using translation, requiring understanding of how language models handle multilingual contexts
  - Quick check question: What are the main challenges in cross-lingual transfer, and how do multilingual models like mBERT address them?

## Architecture Onboarding

- Component map: Retriever (BM25, DPR, ColBERT-QA) → Knowledge Source (Wikipedia chunks) → Reader (XLM-RoBERTa) → Answer extraction
- Critical path: Retriever indexing → Retriever training with weak supervision → Reader training with retrieved passages → Evaluation
- Design tradeoffs: BM25 is fast and simple but semantically limited; DPR is dense but computationally expensive; ColBERT-QA balances semantic richness with efficiency
- Failure signatures: Poor retriever performance (low S@K scores) indicates knowledge source or retrieval issues; poor reader performance despite good retrieval suggests reader model or training data issues
- First 3 experiments:
  1. Verify BM25 baseline on Turkish Wikipedia with morphological stemming
  2. Train DPR model using BM25-retrieved passages and evaluate S@K scores
  3. Implement ColBERT-QA with BERTurk initialization and compare retrieval performance against baselines

## Open Questions the Paper Calls Out
The paper identifies several open questions but does not provide specific details in the provided text about what these questions are.

## Limitations
- Study focuses exclusively on Turkish, limiting generalizability to other low-resource languages
- Machine translation introduces potential quality issues despite post-processing mitigation
- Evaluation relies on a relatively small human-translated dataset (XQuAD-TR) that may not capture full language diversity

## Confidence
- High: Effectiveness of ColBERT-QA in low-resource settings, demonstrated through extensive ablation studies and comparison with strong baselines
- Medium: Claim that only hundreds of gold examples are needed for reliable evaluation, based on subsampling experiments needing cross-linguistic validation
- Low: Generalizability of the weak supervision approach to other low-resource languages, as Turkish-specific preprocessing may not translate directly

## Next Checks
1. **Cross-linguistic validation**: Replicate the study with at least two additional low-resource languages (e.g., Vietnamese and Swahili) to test the generalizability of the weak supervision approach and the claim about minimal gold evaluation examples

2. **Translation quality analysis**: Systematically evaluate how different levels of machine translation quality affect retriever and reader performance by conducting experiments with varying translation quality thresholds and measuring the correlation between translation quality scores and downstream QA performance

3. **Knowledge source expansion stress test**: Conduct controlled experiments that gradually increase the knowledge source size while measuring retriever performance on Success@k metrics, to quantify the exact point at which additional knowledge becomes detrimental for each retriever type (BM25, DPR, ColBERT-QA)