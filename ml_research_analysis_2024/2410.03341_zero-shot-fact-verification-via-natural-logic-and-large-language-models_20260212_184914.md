---
ver: rpa2
title: Zero-Shot Fact Verification via Natural Logic and Large Language Models
arxiv_id: '2410.03341'
source_url: https://arxiv.org/abs/2410.03341
tags:
- natural
- claims
- arxiv
- evidence
- logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Zero-NatVer, a zero-shot fact verification
  system based on natural logic that generates faithful, explainable proofs without
  requiring training data annotated with natural logic. The method uses instruction-tuned
  large language models to chunk claims, align them with evidence, assign natural
  logic operators via question-answering ensembles, and execute proofs on a finite
  state automaton.
---

# Zero-Shot Fact Verification via Natural Logic and Large Language Models

## Quick Facts
- arXiv ID: 2410.03341
- Source URL: https://arxiv.org/abs/2410.03341
- Reference count: 27
- Outperforms existing natural logic-based baselines by 8.96 accuracy points in zero-shot generalization

## Executive Summary
Zero-NatVer is a zero-shot fact verification system that leverages natural logic operators and large language models to verify claims without requiring training data. The system chunks claims into atomic pieces, aligns them with evidence, assigns natural logic operators via question-answering ensembles, and executes proofs on a finite state automaton. Evaluated across eight datasets including multilingual and real-world claims, Zero-NatVer consistently outperforms existing natural logic-based baselines and trained systems in zero-shot transfer while providing explainable proofs.

## Method Summary
Zero-NatVer uses a single instruction-tuned large language model to perform four key stages: chunking claims into smaller units, aligning these chunks with evidence while explaining the relationship, assigning natural logic operators (NatOps) through weighted question-answering ensembles, and executing proofs on a deterministic finite automaton to determine the final verdict. The system employs constrained decoding during chunking and alignment to prevent hallucinations, and uses multiple question templates per NatOp to improve prediction stability and calibration.

## Key Results
- Outperforms existing natural logic-based baselines by an average of 8.96 accuracy points in zero-shot generalization
- Consistently outperforms trained systems in zero-shot transfer across eight datasets
- Achieves competitive results compared to direct QA approaches while providing explainable proofs
- Shows strong generalization to unseen domains and languages without fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Zero-NatVer achieves strong zero-shot generalization by leveraging alignment signals and NatOp candidate filtering. The system uses alignment explanations (support/refute signals) to constrain the set of possible NatOps for each claim-evidence pair, then applies a weighted ensemble of questions to select the final operator. This process allows for transferring some global information from the aligner, which has access to the full claim and evidence texts, to the NatOp assigner, which only sees chunks.

### Mechanism 2
Constrained decoding prevents hallucinations during chunking and alignment. During chunking, the model can only output tokens from the claim or special newline tokens. During alignment, it can only output tokens from the evidence or claim, not arbitrary text. This restriction prevents the model from generating text not present in the source documents.

### Mechanism 3
Question-answering ensembles reduce prediction variability and improve calibration. For each NatOp candidate, the system generates 10 question templates, aggregates their answers using weighted averaging, and only considers operators above a confidence threshold. Multiple questions per operator capture different aspects of the semantic relationship, and weighted averaging produces better-calibrated scores than single-question approaches.

## Foundational Learning

- **Concept:** Natural logic operators and their set-theoretic definitions
  - Why needed here: Zero-NatVer needs to assign NatOps (≡, ⊑, ⊒, ¬, ⇃ ↾) to claim-evidence pairs to construct proofs
  - Quick check question: Can you explain the difference between forward entailment (⊑) and reverse entailment (⊒) using set theory?

- **Concept:** Deterministic finite automaton execution for verdict determination
  - Why needed here: After generating NatOps, Zero-NatVer executes them on a DFA to determine the final verdict (Supported, Refuted, NEI)
  - Quick check question: What are the initial and final states in the DFA used by Zero-NatVer, and how do NatOps transition between states?

- **Concept:** Question-answering frameworks for semantic relation detection
  - Why needed here: Zero-NatVer uses Yes/No questions to determine whether specific NatOps hold between claim and evidence chunks
  - Quick check question: How would you design a question template to determine if expression X is a paraphrase of expression Y?

## Architecture Onboarding

- **Component map:** Claim → Chunking → Alignment → NatOp Assignment → DFA Execution → Verdict
- **Critical path:** Claim → Chunking → Alignment → NatOp Assignment → DFA Execution → Verdict
  - Each stage depends on the previous one; failures propagate downstream
- **Design tradeoffs:**
  - Multiple small models vs single large model: Zero-NatVer uses one LLM for all stages vs traditional approaches with separate chunkers, aligners, etc.
  - Constrained vs unconstrained generation: Prevents hallucinations but may limit expressiveness
  - Ensemble size: Larger ensembles improve stability but increase computational cost
- **Failure signatures:**
  - Incorrect chunking: Claim split into illogical pieces, leading to poor alignment
  - Alignment hallucinations: Evidence contains text not present in source documents
  - NatOp misclassification: Wrong operator selected, leading to incorrect DFA traversal
  - DFA errors: Incorrect final state determination despite correct proof sequence
- **First 3 experiments:**
  1. Test chunking with simple claims to verify proper segmentation and no hallucinations
  2. Test alignment with claims where evidence contains exact paraphrases to verify proper matching
  3. Test NatOp assignment with clear entailment examples to verify operator selection accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does Zero-NatVer handle claims that require multi-hop reasoning across multiple pieces of evidence? The paper mentions that Hover is a multi-hop dataset but doesn't explicitly discuss how Zero-NatVer handles multi-hop reasoning. The paper focuses on single-hop reasoning and doesn't provide details on how the system would handle claims requiring multiple evidence pieces.

### Open Question 2
What is the impact of alignment signal quality on NatOp assignment accuracy, and how could this be improved? While the paper shows the importance of alignment signals and demonstrates an ablation study where removing alignment signals drops accuracy by 6.78 points, it doesn't explore how signal quality affects performance or discuss methods to improve signal extraction.

### Open Question 3
How does Zero-NatVer's performance compare to other zero-shot fact verification methods on long-form claims and evidence? The paper evaluates on various datasets but doesn't specifically analyze performance on long-form claims or evidence, which could be challenging for LLM-based systems due to context window limitations.

## Limitations
- Performance on multilingual datasets (RU22Fact, CHEF) is notably weaker than on English datasets, with accuracy drops of 5-15 points
- Reliance on post-hoc filtering to remove hallucinated tokens indicates constrained decoding mechanism is not fully reliable
- Ensemble approach introduces computational overhead that scales linearly with ensemble size

## Confidence
- **High Confidence:** The core mechanism of using constrained decoding to prevent hallucinations and the DFA-based proof execution are well-supported by the methodology and ablation studies
- **Medium Confidence:** The claim of superior zero-shot generalization is supported but the comparison is primarily against natural logic-based baselines rather than modern LLM approaches
- **Low Confidence:** The multilingual performance claims are based on limited datasets and the evaluation doesn't account for potential translation artifacts or cultural context differences

## Next Checks
1. **Ablation Study on Alignment Signals:** Systematically evaluate the impact of alignment signals on NatOp prediction accuracy by comparing performance with and without alignment explanations in the NatOp assignment stage.

2. **Cross-Lingual Robustness Test:** Test the system on additional low-resource languages and evaluate performance when translating claims vs. translating evidence to determine the source of multilingual performance degradation.

3. **Hallucination Detection Benchmark:** Implement automated hallucination detection to measure the actual percentage of chunks containing hallucinated content, comparing the effectiveness of constrained decoding vs. post-hoc filtering across different claim types.