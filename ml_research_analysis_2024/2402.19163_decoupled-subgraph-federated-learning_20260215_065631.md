---
ver: rpa2
title: Decoupled Subgraph Federated Learning
arxiv_id: '2402.19163'
source_url: https://arxiv.org/abs/2402.19163
tags:
- fedstruct
- node
- clients
- client
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces FedStruct, a novel federated learning framework
  for graph-structured data that eliminates the need to share node features or embeddings
  across clients. Unlike prior approaches, FedStruct leverages explicit global graph
  structure information to capture inter-node dependencies while preserving privacy.
---

# Decoupled Subgraph Federated Learning

## Quick Facts
- arXiv ID: 2402.19163
- Source URL: https://arxiv.org/abs/2402.19163
- Reference count: 33
- One-line primary result: FedStruct achieves near-centralized performance on six graph datasets for federated node classification without sharing node features

## Executive Summary
FedStruct introduces a novel federated learning framework for graph-structured data that eliminates the need to share sensitive node features across clients. The approach decouples node features from graph structure, using explicit global graph structure information to capture inter-node dependencies while preserving privacy. FedStruct operates under two realistic scenarios - with and without knowledge of global graph connections - and achieves performance close to centralized approaches on six datasets for semi-supervised node classification.

## Method Summary
FedStruct addresses federated graph learning by separating node features (held locally) from structure features (shared globally). The framework uses a decoupled GCN architecture that separates propagation and transformation steps, preventing oversmoothing while enabling efficient computation. Node structure features (NSFs) can be generated using one-hot degree vectors, graphlet degree vectors, node2vec, or task-dependent HOP2VEC. The method operates in two variants: FedStruct-A (server knows global graph) and FedStruct-B (no global knowledge). Clients compute local gradients and share NSFs and adjacency matrices, while the server aggregates gradients and updates the model.

## Key Results
- FedStruct achieves performance close to centralized approaches on Cora, Citeseer, Pubmed, Chameleon, Amazon Photo, and Amazon Ratings datasets
- Outperforms existing methods by up to 10% in heavily semi-supervised settings
- Demonstrates robustness across varying label availability, number of clients, and data partitioning methods
- Successfully handles heterophilic graphs where traditional GNNs struggle

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling node features from structure enables privacy-preserving federated learning on graph data.
- Mechanism: By separating node features (held locally by clients) from structure features (shared globally), FedStruct eliminates the need to share sensitive node features across clients. Structure features capture inter-node dependencies using only graph topology information.
- Core assumption: Node structure features contain sufficient information to capture inter-node dependencies without requiring raw node features.
- Evidence anchors:
  - [abstract] "FedStruct eliminates the necessity of sharing or generating sensitive node features or embeddings among clients. Instead, it leverages explicit global graph structure information to capture inter-node dependencies."
  - [section 4.1] "Our focus is on node classification, where the goal is to classify each node based on the underlying graph data. More precisely, we assume that each client employs a GNN model that generates embeddings for each node using the node features and subgraph connections."
- Break condition: If structure features cannot adequately represent inter-node dependencies, performance degrades significantly compared to approaches using raw node features.

### Mechanism 2
- Claim: Task-dependent node structure features (HOP2VEC) outperform task-agnostic alternatives in federated settings.
- Mechanism: HOP2VEC learns node structure features during training to minimize misclassification, making them inherently task-dependent. Unlike NODE2VEC which requires global graph knowledge, HOP2VEC operates locally and adapts to the specific classification task.
- Core assumption: Learning structure features during training improves classification performance compared to pre-computed, task-agnostic structure features.
- Evidence anchors:
  - [section 5.1] "HOP2VEC constructs NSFs as follows. First, NSFs are initialized with a random vector, after which they are learned from the graph structure during the training of FedStruct. As a result, the NSFs are inherently task-dependentâ€”formed to minimize misclassification."
  - [section 4.2] "HOP2VEC does not require knowledge of the complete adjacency matrix. Therefore, it cannot be used for the scenario with no knowledge of the global graph."
- Break condition: If task-agnostic methods like NODE2VEC could be adapted to work without global graph knowledge, the advantage of HOP2VEC might diminish.

### Mechanism 3
- Claim: Decoupled GCN addresses oversmoothing while enabling efficient federated learning.
- Mechanism: Decoupled GCN separates propagation and transformation steps, allowing computation of all propagation layers before training. This enables clients to limit information exchange to specific requirements, reducing privacy risks while preventing oversmoothing.
- Core assumption: Separating propagation and transformation steps in GCN layers prevents oversmoothing while maintaining expressive power.
- Evidence anchors:
  - [section 3] "A significant limitation of GNNs is over-smoothing (Li et al., 2018; Liu et al., 2020; Dong et al., 2021), which results in performance degradation when multiple layers are applied."
  - [section 6] "Decoupled GCNs separate the propagation and transformation steps. The propagation step does not involve learning, enabling the computation of all propagation layers before training."
- Break condition: If oversmoothing is not a significant issue for the specific datasets or graph structures being used, the benefits of decoupled GCN may be minimal.

## Foundational Learning

- Graph Neural Networks (GNNs)
  - Why needed here: GNNs are the fundamental building blocks for processing graph-structured data in federated settings.
  - Quick check question: What is the key difference between GNNs and traditional neural networks when processing graph data?

- Federated Learning (FL)
  - Why needed here: FL provides the framework for training models across decentralized clients without sharing raw data.
  - Quick check question: How does federated learning differ from centralized learning in terms of data privacy and communication?

- Graph Structure Features
  - Why needed here: Structure features capture inter-node dependencies without requiring raw node features, enabling privacy preservation.
  - Quick check question: What types of information can graph structure features capture that node features cannot?

## Architecture Onboarding

- Component map:
  - Clients: Hold local subgraphs, compute local gradients, and train local models
  - Server: Coordinates training, shares structure features, aggregates gradients
  - Structure Feature Generator: Creates node structure features (DEG, GDV, NODE2VEC, or HOP2VEC)
  - Decoupled GCN: Processes structure features to generate structure embeddings
  - Node Feature Processor: Generates node feature embeddings using local node features

- Critical path:
  1. Server generates or receives structure features
  2. Server computes structure embeddings using decoupled GCN
  3. Server sends structure embeddings to clients
  4. Clients compute node feature embeddings and predictions
  5. Clients compute local gradients and send to server
  6. Server aggregates gradients and updates model

- Design tradeoffs:
  - Privacy vs. Performance: More information sharing improves performance but reduces privacy
  - Computation vs. Communication: Local computation reduces communication but may increase computational load
  - Structure Feature Quality vs. Computation: Higher-quality structure features improve performance but require more computation

- Failure signatures:
  - Performance degradation when using structure features alone (without node features)
  - Communication bottlenecks if structure feature generation or sharing is inefficient
  - Privacy breaches if structure features inadvertently reveal node feature information

- First 3 experiments:
  1. Compare FedStruct performance with and without structure features on a small dataset
  2. Evaluate different structure feature generation methods (DEG, GDV, NODE2VEC, HOP2VEC) on a benchmark dataset
  3. Test FedStruct with different numbers of clients and data partitioning methods on a standard graph dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FedStruct's performance scale with the size and density of the global graph, particularly for very large-scale networks?
- Basis in paper: [inferred] The paper discusses the communication complexity of FedStruct, mentioning that the offline complexity scales with n^2, which may be impractical for large networks. However, it doesn't provide empirical results on performance scaling for very large graphs.
- Why unresolved: The paper focuses on six datasets and doesn't explore performance trends as graph size and density increase significantly.
- What evidence would resolve it: Experiments demonstrating FedStruct's accuracy and communication efficiency on graphs with orders of magnitude more nodes and edges than the current datasets.

### Open Question 2
- Question: What is the impact of varying the number of layers (L_s) in the decoupled GCN on FedStruct's performance for different types of graph structures?
- Basis in paper: [explicit] The paper mentions that the performance of FedStruct with HOP2VEC is low for small L_s and degrades for large L_s, achieving optimal performance between 10 and 20 layers. However, it doesn't explore the impact of varying L_s for different graph structures.
- Why unresolved: The paper only provides results for a fixed number of layers in the decoupled GCN and doesn't investigate how the optimal number of layers might change for different graph structures (e.g., homophilic vs. heterophilic graphs).
- What evidence would resolve it: Experiments varying L_s for different graph structures and analyzing the resulting performance trends.

### Open Question 3
- Question: How does FedStruct handle dynamic graphs where the structure changes over time, and what is the overhead of updating the model in such scenarios?
- Basis in paper: [inferred] The paper focuses on static graph scenarios and doesn't address the challenge of dynamic graphs. The communication complexity analysis suggests that significant updates might be required if the graph structure changes frequently.
- Why unresolved: The paper doesn't consider the temporal aspect of graph data, which is common in many real-world applications.
- What evidence would resolve it: Experiments on dynamic graph datasets and analysis of the model update frequency and associated communication overhead required to maintain performance.

## Limitations

- Limited theoretical guarantees about when structure features alone are sufficient for good performance
- Potential privacy leakage through structure information or gradient patterns not fully addressed
- Communication complexity scales poorly with graph size (O(n^2)), limiting scalability

## Confidence

- Privacy claims: Low - analysis focuses on feature privacy but doesn't address potential leakage through structure information or gradients
- Performance claims: High - strong empirical results across six datasets with statistical significance
- Scalability claims: Medium - theoretical analysis suggests limitations, but empirical validation on large graphs is missing

## Next Checks

1. Test FedStruct on graphs with minimal node features to verify structure features alone are sufficient
2. Evaluate privacy leakage through structure feature reconstruction attacks
3. Measure communication overhead and compare with alternative federated approaches on large-scale graphs