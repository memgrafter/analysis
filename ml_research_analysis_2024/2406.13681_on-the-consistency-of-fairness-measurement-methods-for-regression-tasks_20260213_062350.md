---
ver: rpa2
title: On the Consistency of Fairness Measurement Methods for Regression Tasks
arxiv_id: '2406.13681'
source_url: https://arxiv.org/abs/2406.13681
tags:
- fairness
- methods
- regression
- various
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines consistency across fairness measurement methods
  for regression tasks, where continuous outcomes make standard fairness metrics computationally
  intractable. It evaluates four methods approximating parity-based fairness and two
  for confusion matrix-based fairness using Pearson and Spearman correlations across
  three regression datasets (Law School, Communities and Crime, Insurance).
---

# On the Consistency of Fairness Measurement Methods for Regression Tasks

## Quick Facts
- arXiv ID: 2406.13681
- Source URL: https://arxiv.org/abs/2406.13681
- Authors: Abdalwahab Almajed; Maryam Tabar; Peyman Najafirad
- Reference count: 9
- Primary result: Method choice significantly affects fairness assessments in regression tasks, with substantial variability in consistency across different approximation approaches

## Executive Summary
This paper examines the consistency of fairness measurement methods for regression tasks, where continuous outcomes make standard fairness metrics computationally intractable. The study evaluates four methods for approximating parity-based fairness and two for confusion matrix-based fairness across three regression datasets. Results reveal significant variability in method consistency, with some method pairs showing strong correlation (>0.9) while others demonstrate poor agreement, particularly on the Insurance dataset. The findings indicate that method choice can substantially affect fairness assessments and the evaluation of bias mitigation approaches.

## Method Summary
The study evaluates six fairness measurement methods across three regression datasets: Law School (predicting GPA), Communities and Crime (predicting violent crime percentage), and Insurance (predicting insurance cost). For each dataset, 22 ML models are trained including XGBoost, Random Forest, SVM, TabNet, and MLP. Four methods approximate parity-based fairness (P1: Demographic Parity through Reduction to Classification, P2: Demographic Parity with Wasserstein Barycenters, P3: Demographic Parity via RÃ©nyi Correlation, P4: Independence via Probabilistic Classifier-based Density Ratio Estimation) and two methods approximate confusion matrix-based fairness (C1: Separation via Probabilistic Classifier-based Density Ratio Estimation, C2: Equalized Odds). Pearson and Spearman correlations are computed between all method pairs to assess consistency across models and datasets.

## Key Results
- Demographic Parity through Reduction to Classification (P1) and Demographic Parity with Wasserstein Barycenters (P2) show strong correlation (>0.9) across all datasets
- Independence via Probabilistic Classifier-based Density Ratio Estimation (P4) shows poor consistency with other methods on the Insurance dataset (average Pearson and Spearman correlations of about 0.36)
- Visual analysis reveals anomalies where different methods suggest opposite fairness trends for the same models
- Method choice significantly affects fairness assessments, indicating that using multiple fairness measures is necessary for reliable evaluation

## Why This Works (Mechanism)

### Mechanism 1
Density ratio estimation methods approximate fairness metrics by treating fairness as a probabilistic classification problem. Fairness metrics require comparing distributions of predictions across protected groups. In regression, targets are continuous, so exact computation is intractable. Density ratio estimation methods reframe this by constructing a probabilistic classifier that distinguishes between protected groups, then using the classifier's output to estimate the density ratio between group distributions. This allows computation of independence measures like demographic parity. The core assumption is that the density ratio between protected groups can be accurately estimated via a probabilistic classifier, and this ratio is sufficient to approximate the fairness metric. Break condition: If the classifier cannot accurately distinguish protected groups, the density ratio estimate becomes biased, leading to incorrect fairness measurements.

### Mechanism 2
Classification-reduction methods discretize continuous regression outputs into classes to enable standard fairness metrics. Continuous target variables are partitioned into discrete bins or classes, transforming the regression task into a classification problem. Once discretized, standard fairness metrics (like demographic parity or equalized odds) can be directly computed using classification fairness measures. This approach leverages existing classification fairness literature while adapting it to regression contexts. The core assumption is that discretizing continuous targets preserves the essential fairness relationships between groups and does not introduce artificial bias through arbitrary binning choices. Break condition: If the discretization scheme is poorly chosen (too coarse or too fine), it may either lose important distributional information or create artificial class boundaries that distort fairness relationships.

### Mechanism 3
Wasserstein barycenter methods measure fairness by comparing cumulative distribution functions across protected groups. Instead of computing exact independence measures, these methods use optimal transport theory to compare the cumulative distribution functions (CDFs) of predictions across protected groups. The Wasserstein distance provides a principled way to measure distributional differences, and barycenters provide a way to summarize these differences into a fairness metric. The core assumption is that the Wasserstein distance between CDFs provides a meaningful and stable approximation of the independence relationship required for fairness metrics. Break condition: If the CDF comparison is sensitive to sample size or distributional outliers, the fairness measure may become unstable across different datasets or model predictions.

## Foundational Learning

- **Concept**: Pearson and Spearman correlation coefficients
  - Why needed here: The paper uses these correlations to assess consistency between different fairness measurement methods. Pearson measures linear relationships while Spearman measures monotonic relationships.
  - Quick check question: What is the key difference between Pearson and Spearman correlation coefficients?

- **Concept**: Density ratio estimation
  - Why needed here: Understanding how density ratio estimation works is crucial for grasping why methods like P4 use probabilistic classifiers to approximate fairness metrics.
  - Quick check question: In the context of fairness measurement, what does the density ratio between protected groups represent?

- **Concept**: Wasserstein distance and optimal transport
  - Why needed here: Method P2 uses Wasserstein barycenters, so understanding how Wasserstein distance measures distributional differences is important.
  - Quick check question: How does Wasserstein distance differ from other distributional distance measures like KL divergence?

## Architecture Onboarding

- **Component map**: Trained ML models -> Fairness measurement methods (P1-P4, C1-C2) -> Correlation analysis between method outputs
- **Critical path**: 1) Train regression models on dataset, 2) Generate predictions for all models, 3) Apply each fairness approximation method to the predictions, 4) Compute Pearson and Spearman correlations between method outputs, 5) Analyze consistency patterns
- **Design tradeoffs**: The choice between approximation methods involves tradeoffs between computational efficiency (classification reduction is often faster) and theoretical soundness (Wasserstein methods may be more robust). Density ratio methods may be more sensitive to classifier quality, while CDF comparison methods may be more sensitive to sample size.
- **Failure signatures**: Inconsistent correlation results across datasets suggest method instability. High correlations between some method pairs but not others indicate that method choice matters. Anomalies in scatter plots where methods suggest opposite fairness trends reveal fundamental measurement disagreements.
- **First 3 experiments**:
  1. Reproduce the correlation analysis on a simple synthetic dataset where ground truth fairness is known, to verify that methods behave as expected.
  2. Test the sensitivity of each method to different discretization schemes (for classification-reduction methods) or different classifier architectures (for density ratio methods).
  3. Compare method consistency across datasets with different characteristics (balanced vs. imbalanced protected groups, different target distributions) to identify when methods are most likely to disagree.

## Open Questions the Paper Calls Out

- **Open Question 1**: Why do certain fairness measurement methods (like Independence via Probabilistic Classifier-based Density Ratio Estimation) show poor consistency with others on specific datasets like Insurance? The paper identifies this inconsistency but does not investigate the underlying causes or data characteristics that lead to these discrepancies.

- **Open Question 2**: What specific anomalies in the scatter plots (e.g., in Figure 1 and Figure 2) indicate opposite fairness trends, and what causes these patterns? The paper notes "anomalies that can be seen in distinct regions" where one method suggests increasing disparity while another suggests decreasing disparity, but does not explain the cause.

- **Open Question 3**: How can we develop a principled approach for measuring fairness in regression that addresses the inconsistency issues identified? The paper concludes that "further research needs to be conducted to provide reliable fairness measurement methods for regression tasks" and calls for "fundamental research to develop more reliable methods."

## Limitations

- The evaluation relies on only three datasets with different characteristics, which may not generalize to all regression scenarios
- The choice of protected attributes (race and gender) restricts the breadth of fairness contexts examined
- The paper focuses on specific approximation methods without exploring alternative approaches or variations in implementation details

## Confidence

- **Finding that method choice affects fairness assessments**: High confidence, consistently supported across all datasets and correlation analyses
- **Specific methodological recommendations**: Medium confidence, given limited dataset diversity and potential implementation variations
- **Observations about method consistency patterns**: High confidence, as patterns are consistently observed across datasets
- **Implications for bias mitigation approaches**: Medium confidence, due to need for broader validation

## Next Checks

1. **Dataset Diversity Test**: Evaluate method consistency across a broader range of datasets varying in protected attribute distributions, target variable characteristics, and sample sizes to assess generalizability of the findings.

2. **Implementation Sensitivity Analysis**: Systematically vary key implementation parameters (discretization granularity for classification-reduction methods, classifier architectures for density ratio methods) to quantify their impact on method consistency.

3. **Ground Truth Validation**: Apply methods to synthetic regression datasets where ground truth fairness relationships are known, to verify that methods correctly identify fair vs. unfair scenarios and that inconsistencies reflect genuine measurement differences rather than artifacts.