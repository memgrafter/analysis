---
ver: rpa2
title: 'The Potential of LLMs in Medical Education: Generating Questions and Answers
  for Qualification Exams'
arxiv_id: '2410.23769'
source_url: https://arxiv.org/abs/2410.23769
tags:
- medical
- llms
- answers
- questions
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the potential of large language models
  (LLMs) to generate medical qualification exam questions and answers using real-world
  electronic health record data. The research employs few-shot prompting techniques
  with 8 different LLMs on a Chinese elderly chronic disease database to produce open-ended
  questions and answers.
---

# The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams

## Quick Facts
- arXiv ID: 2410.23769
- Source URL: https://arxiv.org/abs/2410.23769
- Reference count: 40
- Key outcome: LLMs can generate medical qualification exam questions and answers at levels close to human clinicians using few-shot prompting with EHR data

## Executive Summary
This study investigates whether large language models can generate medical qualification exam questions and answers using real-world electronic health record data. The research employs few-shot prompting techniques with 8 different LLMs on a Chinese elderly chronic disease database, producing open-ended questions and answers that are evaluated by human medical experts. Results show that while LLMs can generate questions at levels approaching human clinicians, answer quality shows more substantial gaps. The study establishes a novel paradigm for AI-assisted medical education while highlighting both the potential and limitations of LLMs in this domain.

## Method Summary
The study uses few-shot prompting with 8 LLMs (ERNIE 4, ChatGLM 4, Doubao, Hunyuan, Spark 4, Qwen, Llama 3, Mistral) to generate medical qualification exam questions and answers from admission reports in a Chinese elderly chronic disease database. Medical experts create 4 reference question-answer pairs for few-shot learning, then evaluate AI-generated content using 5-point Likert scales across multiple criteria including coherence, sufficiency of key information, factual consistency, and professionalism. The approach combines EHR data processing with prompt engineering to enable LLMs to generate contextually appropriate medical examination content.

## Key Results
- ERNIE 4 achieved the highest scores in question generation among the tested LLMs
- Human experts outperformed all AI models in answer generation, particularly for sufficiency of key information
- LLMs can generate questions with coherence and professionalism levels close to human clinicians
- Answer generation quality shows more substantial gaps compared to question generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting enables LLMs to generate medical qualification questions that match clinician-level coherence and professionalism
- Mechanism: Prompt templates with reference Q&A pairs guide the model to emulate the question style and format, allowing the model to leverage its pre-trained knowledge in medical domain
- Core assumption: The few-shot examples provide sufficient signal for the model to adapt its generation to the target domain and format
- Evidence anchors:
  - [abstract] "we leverage LLMs to produce medical qualification exam questions and the corresponding answers through few-shot prompts"
  - [section] "this study adopts the method of prompt engineering, enabling LLMs to emulate a limited number of reference medical qualification exam questions and answers"
  - [corpus] Weak evidence - corpus neighbors don't provide direct evidence for this mechanism
- Break condition: If reference examples are too dissimilar from target domain or too few to provide meaningful signal, the model will fail to adapt properly

### Mechanism 2
- Claim: LLMs can extract and abstract key information from electronic health records to generate coherent medical questions
- Mechanism: The model processes structured EHR data and identifies relevant clinical elements to formulate questions that test diagnostic reasoning
- Core assumption: The model's pre-trained knowledge includes sufficient medical domain understanding to identify relevant information patterns in EHRs
- Evidence anchors:
  - [abstract] "Utilizing a multicenter bidirectional anonymized database... we tasked LLMs with generating open-ended questions and answers based on a subset of sampled admission reports"
  - [section] "this task assesses the model's capability to educate by creating exam content"
  - [corpus] Weak evidence - corpus neighbors don't provide direct evidence for this mechanism
- Break condition: If EHR data is too complex or the model lacks sufficient medical knowledge, it may fail to identify relevant information or generate coherent questions

### Mechanism 3
- Claim: Human expert evaluation using multi-dimensional criteria provides reliable assessment of AI-generated medical content quality
- Mechanism: Independent experts score AI-generated questions and answers across multiple dimensions (coherence, sufficiency of key information, information correctness, professionalism) using a 5-point Likert scale
- Core assumption: Human experts can reliably distinguish between high and low quality medical questions/answers across the specified dimensions
- Evidence anchors:
  - [abstract] "human experts evaluate these open-ended questions and answers across multi-dimensional criteria with a 5-point Likert Scale method"
  - [section] "we involved medical experts in the evaluation of the AI-generated content with a 5-point Likert Scale"
  - [corpus] Weak evidence - corpus neighbors don't provide direct evidence for this mechanism
- Break condition: If evaluators lack domain expertise or evaluation criteria are ambiguous, the assessment reliability will be compromised

## Foundational Learning

- Concept: Prompt engineering techniques for few-shot learning
  - Why needed here: This study relies on few-shot prompting to guide LLMs in generating medical qualification exam content
  - Quick check question: How do you design effective prompt templates that provide sufficient examples for the model to learn the target task format?

- Concept: Electronic health record data structure and medical terminology
  - Why needed here: The study uses EHR data as input for generating medical questions and answers
  - Quick check question: What are the key components of an admission report in elderly chronic disease databases, and how do they map to clinical reasoning?

- Concept: Medical education assessment design principles
  - Why needed here: The study evaluates AI-generated content for medical qualification exams
  - Quick check question: What makes a medical examination question "professionally appropriate" versus simply "factually correct"?

## Architecture Onboarding

- Component map:
  EHR database → sampled admission reports → AI processing → few-shot prompting → 8 LLMs → question/answer generation → human expert evaluation

- Critical path: EHR data → reference Q&A creation → few-shot prompting → AI generation → human evaluation → iteration

- Design tradeoffs:
  - Model selection: Using multiple LLMs vs. focusing on single best-performing model
  - Reference quantity: 4 reference Q&A pairs vs. more examples for better adaptation
  - Evaluation scope: Medical experts only vs. including automated evaluation metrics

- Failure signatures:
  - Low coherence scores indicate generation quality issues
  - Low sufficiency of key information suggests poor information extraction from EHRs
  - Large standard deviations in evaluation indicate inconsistent model performance

- First 3 experiments:
  1. Test each LLM with the same reference Q&A pairs to establish baseline performance
  2. Vary the number of reference examples (1, 2, 4) to find optimal few-shot quantity
  3. Compare direct AI generation vs. AI-revised answers to measure improvement from human feedback

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal prompt engineering strategy to improve LLM-generated medical question quality while maintaining factual accuracy?
- Basis in paper: [explicit] The paper states "efforts are needed to mitigate these biases through subsequent human supervision for the AI-generated content intended for medical education" and notes limitations in prompt engineering effectiveness.
- Why unresolved: Current prompt engineering methods cannot guarantee output quality, and the paper acknowledges that LLMs may omit critical medical procedures or bypass diagnostic reasoning stages.
- What evidence would resolve it: Comparative studies testing different prompt engineering techniques on medical question generation quality metrics, with human expert evaluation across multiple medical domains.

### Open Question 2
- Question: How do different medical specialties affect LLM performance in generating exam questions and answers?
- Basis in paper: [inferred] The paper mentions "extending the scope of medical examination to other clinical departments such as oncology, gynaecology, and orthopedics could comprehensively assess the performance of LLMs" as a future research direction.
- Why unresolved: The study was limited to elderly chronic diseases, and different medical specialties may have varying complexity and knowledge structures that affect LLM performance.
- What evidence would resolve it: Cross-specialty evaluation of LLM-generated medical exam content across diverse medical domains with standardized quality metrics.

### Open Question 3
- Question: What is the optimal balance between automated LLM-generated content and human expert oversight in medical education?
- Basis in paper: [explicit] The paper discusses "the necessity for further research and development to refine LLMs' comprehension and application of medical knowledge" and notes that "human experts scored higher than all AI models in terms of sufficiency of key information."
- Why unresolved: While LLMs can generate questions close to human clinicians, answer quality shows substantial gaps, and the optimal integration of AI and human expertise remains unclear.
- What evidence would resolve it: Empirical studies comparing different ratios of AI-generated to human-vetted medical exam content on student learning outcomes and knowledge retention.

## Limitations

- Single database dependency: The study uses only a Chinese elderly chronic disease database, limiting generalizability to other medical domains or populations
- Subjective evaluation methodology: Human expert assessment using 5-point Likert scales introduces subjectivity and potential bias
- Insufficient reference examples: The 4 reference Q&A pairs for few-shot prompting may be inadequate for optimal model adaptation given the complexity of medical reasoning

## Confidence

**High Confidence:** The finding that ERNIE 4 achieves highest scores in question generation is well-supported by the experimental data and human evaluation results. The observation that human experts outperform LLMs in answer generation is also strongly evidenced.

**Medium Confidence:** The claim that LLMs can generate questions at levels "close to human clinicians" requires cautious interpretation. While the evaluation scores are promising, the comparison is based on limited reference samples and subjective expert judgment rather than standardized medical assessment metrics.

**Low Confidence:** The assertion that this establishes a "novel paradigm for using AI in medical education" is somewhat overstated. The study demonstrates feasibility but doesn't provide evidence of actual educational effectiveness or long-term learning outcomes.

## Next Checks

1. **External Dataset Validation:** Test the same prompting methodology across multiple medical domains (pediatrics, surgery, emergency medicine) using different EHR databases to assess generalizability. This addresses the limitation of single-database dependency and would validate whether the few-shot prompting approach transfers across medical specialties.

2. **Automated Quality Metrics Integration:** Implement automated evaluation metrics (BLEU, ROUGE, medical domain-specific similarity measures) alongside human expert assessment to provide more objective quality measurements. This would help quantify the subjective elements of the evaluation and enable larger-scale testing.

3. **Longitudinal Educational Impact Study:** Conduct a controlled study measuring learning outcomes when students use LLM-generated questions versus traditional materials. This addresses the gap between content generation capability and actual educational effectiveness, providing evidence for whether AI-generated questions translate to improved medical knowledge and clinical reasoning skills.