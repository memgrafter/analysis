---
ver: rpa2
title: 'Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal
  Large Language Models'
arxiv_id: '2409.10197'
source_url: https://arxiv.org/abs/2409.10197
tags:
- pruning
- fitprune
- uni00000013
- layer
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FitPrune is a training-free visual token pruning method for MLLMs
  that achieves significant computational reduction while maintaining high performance.
  The core idea is to treat token pruning as a statistical distribution fitting problem,
  using attention statistics from a small batch of examples to determine optimal pruning
  strategies that minimize divergence between attention distributions before and after
  pruning.
---

# Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models

## Quick Facts
- arXiv ID: 2409.10197
- Source URL: https://arxiv.org/abs/2409.10197
- Reference count: 33
- Primary result: Training-free visual token pruning achieving up to 54.9% FLOPs reduction with only 0.5% accuracy drop

## Executive Summary
FitPrune is a novel training-free method for visual token pruning in multi-modal large language models (MLLMs) that significantly reduces computational complexity while maintaining high performance. The method treats token pruning as a statistical distribution fitting problem, using attention statistics from a small batch of examples to determine optimal pruning strategies. Applied to LLaVA-1.5, LLaVA-HR, and LLaVA-NEXT on 10 benchmarks, FitPrune achieves up to 54.9% FLOPs reduction with minimal accuracy degradation.

## Method Summary
FitPrune works by collecting attention statistics from a small batch of examples, then fitting both self-attention and cross-attention distributions before and after pruning. It uses binary search to find the optimal divergence threshold that meets a specified FLOPs budget, generating layer-wise pruning ratios. During inference, tokens are ranked by a combined importance metric (product of self and cross-attention scores) and pruned accordingly. The entire recipe generation process takes approximately 5 minutes and requires no additional training.

## Key Results
- Up to 54.9% FLOPs reduction on LLaVA-NEXT with only 0.5% accuracy drop
- Pruning recipes generated in approximately 5 minutes from small batches (655 samples)
- Effective across multiple MLLM variants including LLaVA-1.5, LLaVA-HR, and LLaVA-NEXT
- Maintains performance across 10 different benchmarks while reducing computational load

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token pruning works because MLLMs exhibit visual token redundancy, especially in deeper layers, where many tokens have minimal impact on attention distributions.
- Mechanism: By fitting attention distributions (both self-attention and cross-attention) before and after pruning, FitPrune identifies tokens whose removal causes minimal divergence, preserving model performance while reducing FLOPs.
- Core assumption: The attention distributions of visual tokens remain stable enough after pruning that performance degradation is minimal if divergence is controlled.
- Evidence anchors:
  - [abstract] "The experimental results show that our FitPrune can not only reduce the computational complexity to a large extent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT with only 0.5% accuracy drop."
  - [section] "Concretely, it aims to minimize the divergence of attention distributions before and after pruning, thereby reducing the negative impact on performance."
  - [corpus] Weak; no direct mention of distribution fitting in neighboring papers.
- Break condition: If attention distributions are too sensitive to token removal, divergence increases sharply and performance drops significantly.

### Mechanism 2
- Claim: The combination of self-attention and cross-attention fitting is necessary because each captures different aspects of token importance.
- Mechanism: Self-attention measures how much a token attends to other visual tokens, while cross-attention measures how much text tokens attend to it. Pruning tokens with low combined importance preserves both visual and multimodal reasoning.
- Core assumption: Both self-attention and cross-attention distributions are predictive of token importance for downstream performance.
- Evidence anchors:
  - [section] "FitPrune also considers the distributions of both cross- and intra-attentions of visual tokens, i.e., the two distributions in Fig.2."
  - [section] "During inference, tokens are ranked by the combined importance metric ai,j u = ai,j s · ai,j c."
  - [corpus] Weak; neighboring papers focus on single-attention pruning.
- Break condition: If one attention type is irrelevant for the task, the combined metric may over-prune important tokens.

### Mechanism 3
- Claim: Binary search on divergence threshold efficiently finds the optimal pruning ratio for a given FLOPs budget.
- Mechanism: By iteratively adjusting the divergence upper bound α and greedily pruning least important tokens, FitPrune converges to a pruning recipe that meets the computation constraint while minimizing performance loss.
- Core assumption: The relationship between divergence threshold and FLOPs reduction is monotonic and can be navigated with binary search.
- Evidence anchors:
  - [section] "To accomplish the objective of Eq.7, we adopt the principle of binary search to obtain the optimal pruning recipe."
  - [section] "In each iteration of the binary search, the midpoint α = (αL+αR)/2 is evaluated."
  - [corpus] Weak; no explicit binary search mention in neighbors.
- Break condition: If the pruning ratio-FLOPs relationship is non-monotonic or noisy, binary search may fail to converge.

## Foundational Learning

- Concept: Attention mechanisms in Transformers
  - Why needed here: FitPrune relies on analyzing attention distributions (self and cross) to determine token importance.
  - Quick check question: What is the difference between self-attention and cross-attention in a multimodal setting?

- Concept: Statistical distribution fitting
  - Why needed here: The core idea is to minimize divergence between attention distributions before and after pruning.
  - Quick check question: How is KL-divergence or other metrics used to measure distribution similarity?

- Concept: Binary search algorithm
  - Why needed here: FitPrune uses binary search to find the smallest divergence threshold that meets the FLOPs budget.
  - Quick check question: What are the termination conditions for binary search in this context?

## Architecture Onboarding

- Component map: Input data -> Attention statistics collection -> Attention distribution fitting -> Binary search for pruning recipe -> Output layer-wise pruning ratios

- Critical path:
  1. Collect attention statistics from small batch
  2. Initialize binary search bounds (αL=0, αR=1)
  3. For each α, compute pruning ratios per layer
  4. Check FLOPs constraint, adjust bounds
  5. Output pruning recipe
  6. At inference, rank and prune tokens per layer

- Design tradeoffs:
  - Small batch size → faster recipe generation but less accurate statistics
  - Strict divergence threshold → better performance but less FLOPs reduction
  - Greedy per-layer pruning → simple but may miss global optimum

- Failure signatures:
  - Performance drops sharply despite small FLOPs reduction → divergence threshold too low
  - FLOPs reduction much smaller than target → pruning ratios too conservative
  - Recipe generation fails to converge → non-monotonic pruning-FLOPs relationship

- First 3 experiments:
  1. Validate attention distribution stability: Prune tokens at different layers and measure performance impact.
  2. Test binary search convergence: Run with different FLOPs budgets and check if recipe meets target.
  3. Compare single vs combined attention fitting: Use only self or cross attention and measure performance/FLOPs trade-off.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Statistical distribution assumptions may not hold for highly diverse datasets or out-of-distribution samples
- Small batch representativeness may lead to suboptimal pruning for edge cases or uncommon visual patterns
- Binary search convergence assumes monotonic relationship between divergence threshold and FLOPs reduction, which may not hold across different architectures

## Confidence
- High Confidence: Computational reduction claims (54.9% FLOPs) and general framework are well-supported
- Medium Confidence: Performance retention claims (0.5% accuracy drop) are credible but small batch size introduces uncertainty
- Low Confidence: Binary search efficiency claims and equal attention weighting are less substantiated with minimal discussion of edge cases

## Next Checks
1. **Distribution Stability Validation**: Run FitPrune on progressively larger batches (e.g., 655, 2000, 5000 samples) and measure how pruning recipe quality and performance change. This tests whether the small batch assumption is valid and quantifies the trade-off between recipe generation speed and quality.

2. **Binary Search Robustness Test**: Apply FitPrune across different MLLM architectures (e.g., BLIP-2, LLaVA-Next variants) and datasets with varying visual complexity. Monitor whether binary search consistently converges and whether the FLOPs-divergence relationship remains monotonic across these variations.

3. **Attention Type Ablation Study**: Create variants of FitPrune that use only self-attention or only cross-attention for pruning decisions, then compare performance and FLOPs reduction across multiple benchmarks. This quantifies the actual contribution of each attention type and tests whether the combined metric is optimal.