---
ver: rpa2
title: 'Graph Mamba: Towards Learning on Graphs with State Space Models'
arxiv_id: '2402.08678'
source_url: https://arxiv.org/abs/2402.08678
tags:
- graph
- mamba
- node
- gmns
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Graph Mamba Networks (GMNs), a new class of
  graph neural networks based on State Space Models (SSMs), specifically the Mamba
  architecture. The key contributions are: (1) a recipe for designing GMNs with four
  required and one optional steps, including neighborhood tokenization, token ordering,
  bidirectional selective SSM encoder, and local encoding; (2) a new neighborhood
  sampling approach that bridges node- and subgraph-level tokenization methods; (3)
  a new bidirectional SSM architecture for graphs that scans input sequence in two
  directions, making the model more robust to permutation; and (4) theoretical justification
  for the power of GMNs, showing they are universal approximators of any function
  on graphs and more expressive than any Weisfeiler-Lehman isomorphism test with proper
  positional/structural encoding.'
---

# Graph Mamba: Towards Learning on Graphs with State Space Models

## Quick Facts
- arXiv ID: 2402.08678
- Source URL: https://arxiv.org/abs/2402.08678
- Reference count: 40
- Key outcome: Graph Mamba Networks achieve outstanding performance on long-range, small-scale, large-scale, and heterophilic datasets while consuming less GPU memory than existing methods

## Executive Summary
This paper introduces Graph Mamba Networks (GMNs), a novel graph neural network architecture based on State Space Models (SSMs), specifically the Mamba architecture. GMNs address key limitations of traditional graph neural networks including over-smoothing, over-squashing, and poor long-range dependency learning. The architecture combines neighborhood tokenization with bidirectional selective SSM encoders, achieving superior performance across diverse graph learning tasks while maintaining linear memory complexity.

## Method Summary
GMNs implement a four-step recipe for graph learning: neighborhood tokenization using random walks of varying lengths, token ordering based on hierarchy or degree, bidirectional selective SSM encoding that processes sequences in both directions, and local encoding of subgraph tokens. The method bridges node-level and subgraph-level tokenization approaches through a novel random-walk-based sampling strategy. A bidirectional Mamba encoder processes the token sequences, with two separate modules scanning in forward and reverse directions to ensure permutation robustness. The architecture optionally incorporates positional/structural encoding and MPNN augmentation for additional inductive bias.

## Key Results
- GMNs achieve state-of-the-art performance on long-range graph benchmark datasets
- The method demonstrates superior efficiency on small-scale datasets with limited computational resources
- GMNs outperform existing methods on large-scale datasets with millions of nodes while consuming less GPU memory
- The architecture shows robustness to heterophilic graph structures where traditional GNNs typically fail

## Why This Works (Mechanism)

### Mechanism 1
- Bidirectional Mamba scanning in two directions compensates for Mamba's sequential limitation on graphs, enabling permutation-equivariant behavior
- The architecture uses two separate Mamba modules - one processes forward, one backward - with outputs combined to ensure each token has access to context from both directions
- Core assumption: Token ordering provides meaningful sequence information that bidirectional scan can leverage
- Evidence anchors: Abstract mentions bidirectional SSM makes model robust to permutation; section 4.2 describes using two recurrent scan modules in different directions
- Break condition: Poor or random token ordering prevents bidirectional scan from extracting meaningful context

### Mechanism 2
- Neighborhood tokenization with random walks captures hierarchical structure while avoiding over-smoothing and over-squashing
- Method samples M random walks of varying lengths (0 to m) for each node, capturing neighborhood hierarchy without fixed-radius aggregation
- Core assumption: Random walks with sufficient samples and length range can approximate full neighborhood structure without fixed-radius drawbacks
- Evidence anchors: Abstract describes bridging node- and subgraph-level tokenization methods; section 4.1 explains addressing limitations of fixed-length random walks and k-hop neighborhoods
- Break condition: Too few samples (M too small) or excessive walk lengths (m too large) result in insufficient neighborhood structure capture

### Mechanism 3
- Long sequences of subgraph tokens enable long-range dependency learning without quadratic complexity
- Method samples s sets of subgraphs per node, creating long token sequences that Mamba's selection mechanism filters efficiently
- Core assumption: Mamba selection mechanism can effectively filter irrelevant information from long sequences
- Evidence anchors: Abstract motivates using Mamba's performance in language modeling; section 4.1 notes performance improves monotonically with sequence length
- Break condition: Selection mechanism fails to filter effectively, introducing noise rather than useful context in long sequences

## Foundational Learning

- State Space Models (SSMs) and discrete-time formulation: Essential for understanding Mamba-based architecture; quick check: What are key parameters in discrete-time SSM formulation and how relate to continuous-time system?
- Graph Neural Networks and message-passing limitations: Critical context for GMN design; quick check: How do message-passing GNNs aggregate neighbor information and why does this lead to over-smoothing?
- Random walks and graph representation learning: Fundamental to neighborhood tokenization approach; quick check: What is relationship between random walk length and neighborhood coverage in graphs?

## Architecture Onboarding

- Component map: Input graph with node features -> Tokenization (random walk-based neighborhood sampling) -> Optional PE/SE -> Encoding (ϕ function, MPNN or RWF) -> Ordering (hierarchy or degree) -> Bidirectional Mamba blocks -> Optional MPNN augmentation -> Output node representations
- Critical path: Tokenization → Encoding → Bidirectional Mamba → Output
- Design tradeoffs: Node vs subgraph tokenization (long-range vs local information); random walk parameters (M, m, s) balancing structure capture vs computation; bidirectional vs unidirectional Mamba (permutation robustness vs computation)
- Failure signatures: Poor heterophilic performance suggests selection mechanism filtering issues; degraded performance with longer sequences indicates selection mechanism overwhelmed; memory issues suggest m or s parameters too large
- First 3 experiments: 1) Compare GMN with m=0 vs m>0 on heterophilic dataset to validate inductive bias tradeoff; 2) Vary M parameter to find optimal sampling size for specific graph type; 3) Test bidirectional vs unidirectional Mamba on permutation-sensitive dataset

## Open Questions the Paper Calls Out

### Open Question 1
- How does tokenization strategy choice (node vs. subgraph) affect GMN performance across different graph learning tasks?
- Basis: Paper discusses trade-offs between node and subgraph tokenization but lacks comprehensive empirical comparison
- Resolution: Extensive experiments comparing node and subgraph tokenization across diverse tasks and datasets

### Open Question 2
- How does bidirectional selective SSM encoder compare to other sequential encoders like Transformers in expressive power and efficiency?
- Basis: Paper demonstrates bidirectional selective SSM effectiveness but lacks direct comparison with Transformers
- Resolution: Thorough comparison of bidirectional selective SSM with Transformers on various graph learning tasks

### Open Question 3
- How does random-walk-based neighborhood sampling affect model's ability to capture graph hierarchical structure?
- Basis: Paper introduces random-walk sampling but doesn't analyze its impact on capturing hierarchical structure
- Resolution: Experiments analyzing random-walk sampling impact on hierarchical structure capture through performance comparison

## Limitations

- Theoretical universal approximation claims depend on proper positional/structural encoding implementation, which is not fully specified
- Memory efficiency claims relative to Transformers lack empirical validation across different graph sizes
- Theoretical expressiveness guarantees assume ideal conditions that may not hold in practice

## Confidence

- Bidirectional Mamba mechanism: Medium - clearly specified but critical assumption about token ordering lacks empirical validation
- Neighborhood tokenization: Low - weak evidence anchor about avoiding over-smoothing relies on sampling diversity rather than rigorous analysis
- Long-range dependency claims: Medium - align with Mamba's established properties but lack specific graph adaptation validation

## Next Checks

1. **Token Ordering Sensitivity Test:** Systematically vary token ordering strategy (degree-based, random, hierarchical) and measure performance degradation to quantify bidirectional mechanism's robustness

2. **Selection Mechanism Ablation:** Compare GMN performance with and without Mamba selection mechanism on long sequences to isolate whether filtering actually improves results versus processing more tokens

3. **Memory Scaling Analysis:** Measure actual GPU memory consumption across graphs of increasing size and plot memory vs graph size to verify claimed linear scaling and compare with Transformer baselines