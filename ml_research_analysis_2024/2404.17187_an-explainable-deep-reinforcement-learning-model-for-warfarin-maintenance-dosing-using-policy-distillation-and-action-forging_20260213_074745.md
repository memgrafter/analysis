---
ver: rpa2
title: An Explainable Deep Reinforcement Learning Model for Warfarin Maintenance Dosing
  Using Policy Distillation and Action Forging
arxiv_id: '2404.17187'
source_url: https://arxiv.org/abs/2404.17187
tags:
- dosing
- action
- dose
- protocol
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops an explainable deep reinforcement learning
  (DRL) model for warfarin maintenance dosing. The approach combines Proximal Policy
  Optimization (PPO) with policy distillation and action forging to create an interpretable
  dosing protocol.
---

# An Explainable Deep Reinforcement Learning Model for Warfarin Maintenance Dosing Using Policy Distillation and Action Forging

## Quick Facts
- arXiv ID: 2404.17187
- Source URL: https://arxiv.org/abs/2404.17187
- Reference count: 26
- Primary result: 84.5% PTTR using PPO with policy distillation and action forging, outperforming baseline protocols (76.4-68.6% PTTR)

## Executive Summary
This paper develops an explainable deep reinforcement learning (DRL) model for warfarin maintenance dosing that combines Proximal Policy Optimization (PPO) with policy distillation and action forging techniques. The approach creates an interpretable dosing protocol that achieves 84.5% Percent Time in Therapeutic Range (PTTR), significantly outperforming existing baseline protocols. The final protocol is distilled into a simple table with three INR ranges and corresponding dose changes, making it easy for practitioners to understand and deploy while maintaining high performance.

## Method Summary
The method trains a PPO-based DRL model on virtual patient data, then applies action forging techniques (action regularizer and action focus) to reduce the action space and promote simpler dosing decisions. Policy distillation converts the trained model into a decision tree, which is then translated into a simple table format. The action forging techniques eliminate less-used dose changes and promote "no dose change" actions, balancing model performance with interpretability. The approach focuses on INR as the sole input for the final protocol, creating an easily deployable decision table.

## Key Results
- Achieved 84.5% PTTR, significantly outperforming baseline protocols (76.4% Aurora, 68.6% Intermountain)
- Final protocol distilled into a simple table with only three INR ranges and corresponding dose changes
- Action forging reduced action space from 11 possible dose changes to 4 optimal actions while maintaining high performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The combination of policy distillation and action forging produces an interpretable warfarin dosing protocol that outperforms baseline methods.
- Mechanism: The PPO-trained model explores optimal dosing strategy, then action forging techniques reduce the action space and promote "no dose change" actions, making the policy simpler. Policy distillation converts this into a decision tree, then a table format.
- Core assumption: The distilled policy retains most of the PPO model's performance while being more interpretable.
- Evidence anchors:
  - [abstract] "The final protocol is distilled into a simple table with three INR ranges and corresponding dose changes, making it easy for practitioners to understand and deploy."
  - [section IV] "We applied Python's 'DecisionTreeClassifier' class to the maintenance portion of dosing trajectories in the test set with INR as the only input variable and the percent dose change as the label."

### Mechanism 2
- Claim: Action forging techniques effectively balance model performance with interpretability by reducing the action space and promoting simpler dosing decisions.
- Mechanism: The action regularizer eliminates less-used dose change percentages by forcing their weights to zero, while the action focus mechanism increases the probability of "no dose change" actions, leading to fewer dose changes overall.
- Core assumption: A smaller action space with more emphasis on maintaining current doses improves both interpretability and patient outcomes.
- Evidence anchors:
  - [section IV] "The action forging techniques have their hyper-parameters, and they can interact with each other and impact the training... The minimum happens at 0.1, where there are four actions left."
  - [section IV] "As a result, this action becomes preferred over its close neighbors and will be the dominant action during the inference."

### Mechanism 3
- Claim: Using a decision tree for policy distillation effectively translates the complex PPO model into a simple, interpretable table format.
- Mechanism: The decision tree algorithm identifies the most important features (in this case, INR values) and creates a tree structure that can be easily converted into a table with INR ranges and corresponding dose changes.
- Core assumption: A decision tree can accurately represent the PPO model's policy using only INR values as input, without significant loss of performance.
- Evidence anchors:
  - [section IV] "Like baseline maintenance dosing protocols, we focus on the INR as the only covariate to decide the dose. So, the final protocol will be a table with INR ranges on the left side and the percentage of dose change on the right side."
  - [section V] "The final tree can fit in a simple table. Table VI only has three decisions."

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The problem is formulated as an MDP to model the sequential decision-making process of warfarin dosing, where the state (patient information) and action (dose change) determine the next state and reward.
  - Quick check question: What are the key components of an MDP, and how do they relate to the warfarin dosing problem?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO is used to train the deep neural network that learns the optimal dosing policy, as it is a policy gradient method that directly learns the action probabilities.
  - Quick check question: How does PPO differ from value function approximation methods like Q-learning, and why is it preferred in this context?

- Concept: Policy Distillation
  - Why needed here: Policy distillation is used to convert the trained PPO model into an interpretable decision tree, which is then translated into a simple table format.
  - Quick check question: What is the purpose of policy distillation, and how does it help in creating an explainable dosing protocol?

## Architecture Onboarding

- Component map:
  - Virtual patient data generator -> PPO actor and critic networks -> Action forging techniques (regularizer and focus) -> Decision tree for policy distillation -> Final dosing protocol table

- Critical path:
  1. Generate virtual patient data
  2. Train PPO model with action forging
  3. Test PPO model performance
  4. Apply decision tree to distill policy
  5. Create final dosing protocol table

- Design tradeoffs:
  - Larger action space vs. smaller action space (more flexibility vs. easier interpretation)
  - More complex state features vs. simpler state features (potentially better performance vs. easier interpretation)
  - More training iterations vs. fewer training iterations (potentially better performance vs. faster development)

- Failure signatures:
  - Significant drop in PTTR when distilling policy to decision tree
  - Decision tree requires complex features or interactions, making it difficult to translate into a simple table
  - Action forging techniques overly constrain the model, leading to suboptimal dosing decisions

- First 3 experiments:
  1. Train PPO model without action forging techniques and compare performance to baseline protocols.
  2. Apply action forging techniques with varying regularizer coefficients and evaluate the resulting action space size and model performance.
  3. Use decision tree to distill the policy from the PPO model and create the final dosing protocol table, then test its performance against the baseline protocols.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed explainable dosing protocol perform on actual patient data compared to virtual patient simulations?
- Basis in paper: [explicit] The paper uses virtual patients generated from a PK/PD model for training and testing, explicitly noting that clinical trials of the required magnitude are not feasible.
- Why unresolved: The model was validated only on simulated data, which may not capture all the complexities and variabilities of real-world patient populations.
- What evidence would resolve it: Clinical trials applying the protocol to real patients and comparing outcomes with current dosing protocols.

### Open Question 2
- Question: How sensitive is the final dosing protocol to variations in the reward function parameters, particularly the quadratic penalty for out-of-range INR values?
- Basis in paper: [inferred] The authors note that the reward function's quadratic penalty might make the model more conservative in defining cut-off values inside the therapeutic range, suggesting sensitivity to this parameter.
- Why unresolved: The paper does not explore how different reward function designs might affect the resulting protocol's cut-off values and overall performance.
- What evidence would resolve it: Experiments testing the model with alternative reward function designs and analyzing how cut-off values and performance metrics change.

### Open Question 3
- Question: Would incorporating duration as part of the action space (beyond just dose percentage changes) significantly improve the protocol's performance?
- Basis in paper: [explicit] The authors explicitly state that duration was excluded from the decision space and acknowledge this as a limitation, noting that adding this dimension would increase the complexity and data requirements.
- Why unresolved: The current model only optimizes dose changes without considering the timing of measurements, which could be important for optimizing patient outcomes.
- What evidence would resolve it: Training and testing an expanded model that includes duration as part of the action space and comparing its performance against the current model.

## Limitations

- The model was validated only on virtual patient data generated by a PK/PD model, not on real clinical data
- Action forging techniques may overly constrain the model's ability to find optimal solutions in certain clinical scenarios
- The final protocol focuses only on INR values as input, potentially missing important patient characteristics

## Confidence

- **High Confidence**: The core methodology of combining PPO with policy distillation and action forging is sound and well-implemented
- **Medium Confidence**: The quantitative improvements in PTTR are robust within the simulation environment
- **Low Confidence**: The clinical applicability and safety of the distilled protocol in real-world settings

## Next Checks

1. **Clinical Validation**: Test the distilled protocol on real patient data from electronic health records to assess its performance and safety in actual clinical practice

2. **Sensitivity Analysis**: Evaluate how the protocol performs across different patient subgroups (e.g., different genotypes, comorbidities) to ensure equitable effectiveness

3. **Ablation Study**: Systematically remove each component (PPO, action forging, policy distillation) to quantify their individual contributions to the final performance