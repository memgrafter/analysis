---
ver: rpa2
title: Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large
  Language Models
arxiv_id: '2407.21659'
source_url: https://arxiv.org/abs/2407.21659
tags:
- cider
- mllms
- adversarial
- images
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the vulnerability of Multimodal Large Language
  Models (MLLMs) to jailbreak attacks, particularly those exploiting visual inputs.
  To mitigate this threat, the authors propose CIDER, a plug-and-play jailbreaking
  detector that identifies adversarially perturbed images by analyzing the semantic
  similarity between harmful queries and images before and after denoising.
---

# Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2407.21659
- Source URL: https://arxiv.org/abs/2407.21659
- Authors: Yue Xu; Xiuyuan Qi; Zhan Qin; Wenjie Wang
- Reference count: 13
- One-line primary result: CIDER achieves ~80% detection success rate and significantly reduces attack success rate across multiple MLLMs

## Executive Summary
This paper addresses the vulnerability of Multimodal Large Language Models (MLLMs) to jailbreak attacks that exploit visual inputs. The authors propose CIDER, a plug-and-play jailbreaking detector that identifies adversarially perturbed images by analyzing the semantic similarity between harmful queries and images before and after denoising. CIDER leverages cross-modal information to distinguish adversarial images from clean ones, achieving high detection success rates while maintaining reasonable computational overhead. The method effectively balances robustness and utility, though it may slightly impact MLLM performance on normal tasks.

## Method Summary
CIDER is a detection mechanism that uses cross-modal semantic similarity analysis to identify jailbreak attacks in MLLMs. The method works by measuring the cosine similarity between harmful text queries and image embeddings, then comparing this similarity before and after applying a diffusion-based denoising process. The key insight is that adversarial images carry harmful semantic content that is closer to malicious queries than clean images in embedding space, and this semantic proximity decreases more significantly after denoising. A threshold-based classifier flags images as adversarial when the drop in cosine similarity exceeds a predefined value, effectively blocking harmful queries from reaching the MLLM.

## Key Results
- CIDER achieves approximately 80% detection success rate across multiple MLLM architectures
- Attack Success Rate (ASR) is reduced by up to 71.3% when CIDER is deployed
- The method maintains low computational overhead while providing robust protection against various jailbreak attack methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial images carry harmful semantic content that is closer to malicious queries than clean images in embedding space.
- Mechanism: Optimization-based jailbreak attacks perturb images so their embeddings align more closely with embeddings of harmful queries, reducing cosine distance between them.
- Core assumption: The perturbation process explicitly optimizes to increase the likelihood that the MLLM outputs harmful content when presented with the query-image pair.
- Evidence anchors:
  - [abstract] "the semantic similarity between harmful queries and adversarial images before and after denoising"
  - [section] "the semantic distance between a harmful query and an adversarially perturbed image is significantly smaller than that between a harmful query and a clean image"
  - [corpus] No direct evidence; related papers focus on jailbreak attack methods, not semantic distance measurements.
- Break condition: If the attack optimization no longer focuses on semantic proximity, or if the image encoder fails to capture harmful semantics in the perturbation.

### Mechanism 2
- Claim: Denoising reduces harmful information in adversarial images but does not eliminate it, causing a relative drop in semantic similarity with harmful queries.
- Mechanism: Diffusion-based denoising progressively removes high-frequency adversarial noise, reducing the embedded harmful content's influence on the image embedding.
- Core assumption: Adversarial perturbations are more sensitive to denoising than natural image features, so their semantic contribution diminishes more rapidly during denoising.
- Evidence anchors:
  - [section] "Figure 3c illustrates how cosine similarity between harmful query and adversarial images decreases as denoising progresses"
  - [section] "denoising can reduce harmful information but cannot eliminate"
  - [corpus] No direct evidence; denoising effects on adversarial images are not discussed in neighboring papers.
- Break condition: If denoising removes useful image content along with adversarial noise, or if attacks use robust perturbations less sensitive to denoising.

### Mechanism 3
- Claim: The relative shift in semantic distance before and after denoising (Î”cosine similarity) is a reliable signal to distinguish adversarial from clean images.
- Mechanism: Clean images show stable or minimal change in cosine similarity after denoising, while adversarial images show a significant drop, enabling threshold-based detection.
- Core assumption: Natural images maintain semantic consistency under denoising, whereas adversarial images do not.
- Evidence anchors:
  - [abstract] "using the relative shift in the semantic distance before and after denoising to reflect the difference between clean and adversarial images"
  - [section] "Figure 3d...the change of cosine similarity before and after denoising"
  - [corpus] No direct evidence; detection via semantic shift is unique to this paper.
- Break condition: If adversarial images are constructed to preserve semantic similarity under denoising, or if clean images have unstable embeddings.

## Foundational Learning

- Concept: Cosine similarity as a measure of semantic distance in embedding space
  - Why needed here: CIDER relies on comparing cosine similarities between query and image embeddings to detect adversarial perturbations
  - Quick check question: If two embeddings have cosine similarity of 0.9, are they more or less semantically similar than embeddings with similarity 0.3?

- Concept: Diffusion-based denoising and its effect on image embeddings
  - Why needed here: CIDER uses denoising to reveal the difference between clean and adversarial images by observing embedding changes
  - Quick check question: What happens to an image's embedding when high-frequency adversarial noise is progressively removed by denoising?

- Concept: Threshold-based binary classification from continuous similarity scores
  - Why needed here: CIDER classifies images as adversarial or clean based on whether the drop in cosine similarity exceeds a predefined threshold
  - Quick check question: If 95% of clean images pass a threshold, what does that imply about the false positive rate?

## Architecture Onboarding

- Component map:
  Text encoder -> Image encoder -> Denoiser -> Cosine similarity calculator -> Threshold comparator -> MLLM interface

- Critical path:
  1. Encode text and original image
  2. Iteratively denoise image and encode denoised versions
  3. Calculate cosine similarity drop
  4. Compare against threshold
  5. Block or forward to MLLM

- Design tradeoffs:
  - Higher thresholds reduce false positives but increase false negatives
  - More denoising iterations improve detection but increase latency
  - Using different encoders than the target MLLM may reduce detection accuracy

- Failure signatures:
  - High false positive rate: threshold too low or clean images have unstable embeddings
  - High false negative rate: threshold too high or denoising insufficient to reveal adversarial nature
  - Increased latency: too many denoising iterations or inefficient encoder

- First 3 experiments:
  1. Measure cosine similarity between harmful queries and clean vs. adversarial images to confirm semantic proximity difference
  2. Apply denoising to both clean and adversarial images and track similarity changes to identify detection signal
  3. Vary denoising iteration count and threshold values to optimize detection accuracy and efficiency trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would CIDER be against jailbreak attacks that use multimodal inputs beyond images, such as audio or video?
- Basis in paper: [inferred] The paper focuses on image-based jailbreak attacks and does not explore other modalities.
- Why unresolved: The authors did not test CIDER on attacks involving other types of inputs, such as audio or video.
- What evidence would resolve it: Testing CIDER on multimodal jailbreak attacks involving audio or video inputs would determine its effectiveness across different data types.

### Open Question 2
- Question: Can CIDER's threshold be dynamically adjusted to improve the balance between robustness and utility without significantly degrading either?
- Basis in paper: [explicit] The paper mentions the robustness-utility trade-off and suggests exploring multi-level thresholds in future work.
- Why unresolved: The current implementation uses a fixed threshold, which may not optimally balance robustness and utility for all tasks.
- What evidence would resolve it: Developing and testing a dynamic threshold adjustment mechanism could show whether it improves the trade-off between robustness and utility.

### Open Question 3
- Question: How does CIDER perform in real-world scenarios where the input data may be more diverse and less controlled than in the experimental setup?
- Basis in paper: [inferred] The experiments are conducted in a controlled environment, which may not fully represent real-world conditions.
- Why unresolved: The paper does not provide evidence of CIDER's performance in real-world applications with diverse and uncontrolled data.
- What evidence would resolve it: Deploying CIDER in real-world applications and evaluating its performance on diverse and uncontrolled input data would provide insights into its practical effectiveness.

## Limitations

- The detection mechanism relies on specific assumptions about how adversarial images behave under denoising that may not hold for all attack strategies
- The method may slightly impact MLLM performance on normal tasks, creating a robustness-utility tradeoff that requires careful calibration
- The generalizability to unseen attack methods and datasets requires further validation beyond the controlled experimental setup

## Confidence

**High Confidence**: The detection mechanism works against the specific attack methods evaluated in the paper, as evidenced by substantial ASR reduction (up to 71.3%) and DSR around 80% across multiple MLLM architectures.

**Medium Confidence**: The generalizability to unseen attack methods and datasets, while suggested by experiments, requires further validation given the rapidly evolving landscape of jailbreak attacks.

**Medium Confidence**: The utility-robustness tradeoff, where detection slightly impacts normal task performance, is well-documented but the practical impact threshold needs more real-world testing.

## Next Checks

1. Test CIDER's detection capability against a broader range of attack methods beyond the optimization-based approach used in the paper, including black-box attacks and those specifically designed to evade denoising.

2. Evaluate the method's performance across diverse image domains beyond ImageNet, particularly in specialized domains like medical imaging or technical documentation where semantic interpretations may differ.

3. Conduct a systematic analysis of false positive rates across different threshold values and image categories to optimize the utility-robustness tradeoff for specific deployment scenarios.