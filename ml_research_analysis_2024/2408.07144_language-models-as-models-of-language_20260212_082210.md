---
ver: rpa2
title: Language Models as Models of Language
arxiv_id: '2408.07144'
source_url: https://arxiv.org/abs/2408.07144
tags:
- language
- linguistic
- linguistics
- computational
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This chapter examines the potential of modern language models to
  inform theoretical linguistics, particularly regarding syntax and language acquisition.
  It reviews empirical evidence suggesting that language models can learn hierarchical
  syntactic structure and exhibit sensitivity to various linguistic phenomena, even
  when trained on developmentally plausible amounts of data.
---

# Language Models as Models of Language

## Quick Facts
- arXiv ID: 2408.07144
- Source URL: https://arxiv.org/abs/2408.07144
- Authors: Raphaël Millière
- Reference count: 40
- Primary result: Language models can learn hierarchical syntactic structure from developmentally plausible data, providing insights into linguistic nativism debates.

## Executive Summary
This chapter examines whether modern language models can inform theoretical linguistics, particularly regarding syntax and language acquisition. The author reviews empirical evidence showing that language models can learn hierarchical syntactic structure and exhibit sensitivity to various linguistic phenomena, even when trained on developmentally plausible amounts of data. While the competence/performance distinction has been used to dismiss their relevance, the chapter argues that controlled experiments with language models can constrain hypotheses about language acquisition and competence. The work concludes that closer collaboration between theoretical linguists and computational researchers could yield valuable insights, especially in advancing debates about linguistic nativism.

## Method Summary
The paper synthesizes experimental approaches from computational linguistics to evaluate language models as scientific models of human language acquisition. Methods include training language models on developmentally appropriate corpora (like CHILDES), evaluating performance on targeted syntactic tasks using minimal pairs and behavioral experiments, conducting probing studies to investigate internal representations of linguistic features, and applying causal intervention techniques to establish representational claims. The approach involves systematic variation of model architecture, training data size and quality, and evaluation protocols to understand which factors enable or constrain syntactic learning.

## Key Results
- Language models can learn hierarchical syntactic structure from developmentally plausible amounts of data, challenging strong nativist claims
- Causal intervention methods demonstrate that models genuinely represent and causally use linguistic features like subject-verb agreement
- The performance-competence gap may be bridgeable through controlled experiments with realistic training conditions

## Why This Works (Mechanism)

### Mechanism 1
Language models can serve as scientific models of human language acquisition when trained on developmentally plausible data. By controlling model training data quantity and quality to match child-directed speech, models can learn linguistic structures without built-in syntactic knowledge, providing existence proofs against strong nativist claims. The core assumption is that the performance-competence gap is not insurmountable; surface patterns in language data encode sufficient information for learning hierarchical syntax.

### Mechanism 2
Causal intervention methods can reveal whether language models internally represent linguistic features in a way that causally influences behavior. By perturbing model activations and measuring behavioral changes, researchers can demonstrate that specific linguistic features (like subject-verb agreement) are both encoded and causally used, supporting claims about representational competence. The core assumption is that the ability to decode a feature from model activations combined with causal intervention showing behavioral impact constitutes evidence for genuine representation, not just correlation.

### Mechanism 3
Language models can model linguistic performance even if they don't capture full competence, providing insights into human language processing. Language models trained on next-word prediction learn probabilistic patterns in human language production that correlate with human reading times and processing difficulty, serving as models of performance constraints. The core assumption is that performance constraints like memory limitations and processing difficulty manifest in predictable patterns that models can capture through statistical learning.

## Foundational Learning

- Concept: Developmental plausibility in model training
  - Why needed here: To establish whether language models can genuinely inform theories of human language acquisition rather than just engineering artifacts
  - Quick check question: What distinguishes developmentally plausible training data from standard web-scale corpora?

- Concept: Causal intervention methodology
  - Why needed here: To move beyond correlational probing and demonstrate genuine internal representation of linguistic features
  - Quick check question: How does causal intervention differ from standard diagnostic probing in computational linguistics?

- Concept: Performance-competence distinction
  - Why needed here: To understand the theoretical framework within which language models can contribute to linguistic theory
  - Quick check question: What are the key differences between modeling linguistic performance versus competence?

## Architecture Onboarding

- Component map: Data preprocessing -> Model architecture (Transformer-based) -> Training pipeline (next-word prediction) -> Evaluation framework (behavioral experiments, probing, causal interventions) -> Analysis tools (interpretability methods, generalization benchmarks)

- Critical path: Data selection → Model training → Behavioral evaluation → Causal probing → Theoretical interpretation

- Design tradeoffs:
  - Model size vs. developmental plausibility (smaller models on realistic data vs. larger models on web-scale data)
  - Interpretability vs. performance (simpler architectures easier to analyze but may underperform)
  - Control vs. ecological validity (highly controlled experiments vs. naturalistic language use)

- Failure signatures:
  - Poor generalization to unseen linguistic constructions
  - Reliance on shallow heuristics rather than hierarchical structure
  - Disconnection between decodable features and behavioral impact
  - Inability to learn from developmentally plausible data quantities

- First 3 experiments:
  1. Train GPT-2 on child-directed speech corpus (10-100M words) and evaluate on BLiMP benchmark
  2. Apply causal intervention to erase number information and measure impact on subject-verb agreement accuracy
  3. Compare reading time predictions between small models (realistic data) and large models (excessive data)

## Open Questions the Paper Calls Out

### Open Question 1
Can language models learn to handle recursive syntactic structures effectively when trained on developmentally plausible amounts of data? This remains unresolved because while some studies suggest Transformers can handle recursion well under certain conditions, there's still uncertainty about whether they can do so robustly across all cases when trained on limited data. Experiments training language models on realistic linguistic input sizes and evaluating their ability to process increasingly complex nested structures, comparing performance to human children learning the same structures, would help resolve this question.

### Open Question 2
What are the necessary and sufficient conditions for language models to acquire human-like syntactic knowledge without built-in linguistic biases? This remains unresolved because while researchers have identified various factors that influence learning outcomes, there's no clear consensus on which combination of architectural choices, data characteristics, and training procedures are truly necessary and sufficient for acquiring human-like syntactic competence. Systematic ablation studies varying different aspects of model architecture, training data, and learning procedures while measuring syntactic competence on standardized benchmarks would help resolve this question.

### Open Question 3
To what extent can language models' syntactic representations be considered truly compositional rather than relying on pattern matching or memorization? This remains unresolved because despite evidence that models can handle novel combinations, distinguishing between genuine compositional understanding and sophisticated pattern matching remains challenging. Experiments testing models' ability to handle extreme compositional generalization cases that require combining known elements in completely novel ways, and mechanistic analysis to understand how models represent and process compositional structures, would help resolve this question.

## Limitations

- Evidence anchors are largely drawn from the abstract and a single section, with sparse direct citations from the main body of work
- Corpus neighbor analysis shows weak relevance, suggesting limited empirical grounding in the broader literature
- The mechanism descriptions rely heavily on theoretical argumentation rather than detailed experimental results

## Confidence

- Medium: Claims about language models learning hierarchical structure from developmentally plausible data
- Medium: Causal intervention methodology as a tool for establishing representational claims
- Low: Claims about performance modeling of human language processing

## Next Checks

1. Conduct a detailed literature review of specific studies where language models were trained on child-directed speech corpora and systematically evaluated on syntactic acquisition benchmarks
2. Replicate the causal intervention methodology on a standard language model to verify that decodable linguistic features can be causally manipulated to affect model behavior
3. Compare model predictions against human reading time data for a controlled set of syntactic constructions to assess the validity of performance modeling claims