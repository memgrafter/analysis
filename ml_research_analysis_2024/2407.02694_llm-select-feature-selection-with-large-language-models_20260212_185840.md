---
ver: rpa2
title: 'LLM-Select: Feature Selection with Large Language Models'
arxiv_id: '2407.02694'
source_url: https://arxiv.org/abs/2407.02694
tags:
- feature
- score
- features
- selected
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LLM-Select shows that large language models can effectively perform\
  \ feature selection using only feature names and task descriptions, achieving performance\
  \ competitive with traditional data-driven methods like LASSO. Three approaches\u2014\
  LLM-Score (importance scoring), LLM-Rank (ranking), and LLM-Seq (sequential selection)\u2014\
  all yield strong results, with GPT-4-based LLM-Score performing best."
---

# LLM-Select: Feature Selection with Large Language Models
## Quick Facts
- arXiv ID: 2407.02694
- Source URL: https://arxiv.org/abs/2407.02694
- Authors: Daniel P. Jeong; Zachary C. Lipton; Pradeep Ravikumar
- Reference count: 40
- Key outcome: LLM-Select shows LLMs can perform feature selection using only feature names and task descriptions, achieving performance competitive with traditional methods like LASSO

## Executive Summary
LLM-Select demonstrates that large language models can effectively perform feature selection without access to the actual data, using only feature names and task descriptions. The paper introduces three LLM-based approaches (LLM-Score, LLM-Rank, and LLM-Seq) that achieve competitive performance with traditional feature selection methods like LASSO. Remarkably, zero-shot prompting often suffices, and as model scale increases, LLM-generated scores better align with standard feature importance metrics. This approach is particularly valuable for domains where data collection is expensive, such as healthcare and social sciences.

## Method Summary
LLM-Select leverages large language models to perform feature selection through three distinct approaches. LLM-Score prompts the LLM to generate numerical importance scores for each feature individually, LLM-Rank asks the LLM to rank all features by importance, and LLM-Seq uses sequential selection by repeatedly prompting the LLM to choose the most important remaining feature. These methods require only feature names and task descriptions as input, without any dataset-specific context. The selected features are then used to train downstream models (typically L2-penalized logistic or linear regression), with performance evaluated across different feature proportions.

## Key Results
- LLM-Score with GPT-4 performs best among the three approaches and achieves comparable results to LASSO
- Zero-shot prompting without dataset-specific context often yields strong feature selection performance
- Correlation between LLM-generated scores and standard feature importance metrics (SHAP, Fisher score, mutual information) increases with model scale
- Concept-level selection is more efficient than feature-level selection while maintaining comparable performance

## Why This Works (Mechanism)
### Mechanism 1
- Claim: LLMs can rank features by their importance to a prediction task using only feature names and task descriptions, without seeing the data.
- Mechanism: LLMs leverage their pretraining on vast text corpora to encode real-world relationships between concepts. When prompted with feature names and a target outcome, they use this encoded knowledge to assign relative importance scores.
- Core assumption: The LLM's pretraining data includes sufficient real-world context to establish meaningful relationships between the given feature names and the target outcome.
- Evidence anchors:
  - [abstract] "given only input feature names and a description of a prediction task, they are capable of selecting the most predictive features"
  - [section 4.1] "LLM-generated feature importance scores are correlated with several commonly used feature importance metrics"
  - [corpus] Weak correlation with corpus papers on feature selection, but distinct in zero-shot approach without data
- Break Condition: If feature names are ambiguous, lack real-world semantics, or the task domain is too specialized for the LLM's pretraining corpus.

### Mechanism 2
- Claim: LLM-generated feature importance scores correlate with standard statistical metrics like SHAP, Fisher score, and mutual information, with correlation increasing as model scale increases.
- Mechanism: Larger models encode richer real-world relationships in their parameters, which translates to more statistically meaningful importance scores that align with established feature importance metrics.
- Core assumption: The LLM's parameter space captures sufficient real-world statistical relationships that can be mapped to standard feature importance metrics.
- Evidence anchors:
  - [abstract] "as model scale increases, LLM-generated scores better align with standard feature importance metrics"
  - [section 4.1] "LLM-Score generally exhibits higher rank correlation with standard feature importance metrics as model scale increases"
  - [corpus] No direct evidence in corpus, but general principle of larger models capturing more complex patterns
- Break Condition: If the LLM's pretraining data lacks sufficient statistical context for the domain, or if the feature importance metrics measure fundamentally different relationships than what the LLM encodes.

### Mechanism 3
- Claim: Different prompting strategies (zero-shot, few-shot, CoT) have varying impacts on feature selection performance, with zero-shot often performing competitively.
- Mechanism: LLMs can extract relevant information from minimal context when prompted correctly, and additional context (examples, explanations) can either help or hinder performance depending on the model and task.
- Core assumption: The LLM can infer task requirements from minimal prompt context without explicit examples or explanations.
- Evidence anchors:
  - [abstract] "even zero-shot prompting the LLM to generate a numerical feature importance score one-feature-at-a-time can lead to strong feature selection performance"
  - [section 4.1] "zero-shot prompting with no dataset-specific context and greedy decoding results in strong feature selection performance"
  - [corpus] Related work on prompt sensitivity, but LLM-Select shows zero-shot can be sufficient
- Break Condition: If the task requires complex reasoning or domain-specific knowledge not captured in the pretraining corpus, or if the prompt format is critical for correct interpretation.

## Foundational Learning
- Concept: Feature importance metrics (SHAP, Fisher score, mutual information)
  - Why needed here: To evaluate whether LLM-generated scores align with established statistical measures of feature importance
  - Quick check question: What does a high SHAP value for a feature indicate about its relationship to the target variable?

- Concept: Prompt engineering and few-shot learning
  - Why needed here: To understand how different prompting strategies affect LLM performance on feature selection tasks
  - Quick check question: What is the difference between zero-shot and few-shot prompting, and when might each be more effective?

- Concept: Feature selection methods (filter, wrapper, embedded)
  - Why needed here: To compare LLM-based approaches against traditional feature selection baselines
  - Quick check question: What distinguishes filter methods from wrapper methods in feature selection?

## Architecture Onboarding
- Component map: Prompt templates -> LLM interaction layer -> Feature selection output -> Downstream model training -> Evaluation
- Critical path:
  1. Generate feature importance scores/rankings via LLM prompts
  2. Select top-k features based on LLM output
  3. Train downstream model on selected features
  4. Evaluate performance on test set
- Design tradeoffs:
  - Model scale vs. computational cost: Larger models perform better but are more expensive
  - Prompt complexity vs. performance: More detailed prompts may help or hurt depending on the model
  - Feature-level vs. concept-level selection: Concept-level selection is more efficient but may miss important feature interactions
- Failure signatures:
  - Poor performance when feature names are ambiguous or lack real-world semantics
  - Inconsistent results across different LLMs or prompting strategies
  - Performance degradation when dataset-specific context is missing for specialized domains
- First 3 experiments:
  1. Compare LLM-Score performance across different LLMs (GPT-4, GPT-3.5, Llama-2) on a simple dataset
  2. Test impact of different prompting strategies (zero-shot vs. few-shot with examples) on feature selection performance
  3. Evaluate correlation between LLM-generated scores and standard feature importance metrics (SHAP, Fisher score)

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can LLM-based feature selection methods be made robust to datasets with irrelevant or meaningless feature names?
- Basis in paper: [inferred] The paper demonstrates that replacing meaningful feature descriptions with irrelevant names (e.g., "feature_1", "feature_2") significantly degrades LLM-based feature selection performance, even worse than random selection.
- Why unresolved: The paper does not propose solutions to mitigate this limitation, leaving open the question of how to adapt LLMs for feature selection when rich text semantics are unavailable.
- What evidence would resolve it: Developing and evaluating methods that enhance LLM feature selection performance on datasets with meaningless feature names, such as incorporating auxiliary data or prompting strategies that elicit more robust semantic reasoning.

### Open Question 2
- Question: Can prompting strategies be designed to increase the alignment of LLM-Score with desired notions of feature importance (e.g., SHAP, Fisher score)?
- Basis in paper: [explicit] The paper finds that LLM-Score generally exhibits higher rank correlation with standard feature importance metrics as model scale increases, but does not uniquely align to a specific notion of importance. The paper suggests investigating prompting strategies to increase alignment.
- Why unresolved: While the paper explores the semantics of LLM-Score and observes varying degrees of correlation with different metrics, it does not investigate specific prompting strategies to control or align the LLM's notion of importance.
- What evidence would resolve it: Conducting experiments that test various prompting techniques (e.g., few-shot examples, chain-of-thought reasoning, specific instructions about the desired notion of importance) and measuring their impact on the rank correlation between LLM-Score and target metrics.

### Open Question 3
- Question: How can group fairness be incorporated into LLM-based feature selection methods to mitigate potential biases?
- Basis in paper: [inferred] The paper acknowledges that LLMs may exhibit undesirable biases inherited from their pretraining data, which could lead to performance disparities across data subpopulations. It notes the challenge of incorporating group fairness into LLM-based methods, especially for closed-source LLMs.
- Why unresolved: The paper identifies the limitation of potential bias in LLM-based feature selection but does not propose concrete methods for mitigating these biases through group fairness considerations.
- What evidence would resolve it: Developing and evaluating methods that adapt LLM-based feature selection to account for group fairness, such as prompting strategies that encourage the LLM to consider fairness across subgroups or hybrid approaches that combine LLM-driven selection with data-driven fairness constraints.

## Limitations
- Performance degrades significantly when feature names are ambiguous or lack real-world semantics
- Limited evaluation on highly specialized domains where feature semantics are less standardized
- No proposed solutions for making the approach robust to meaningless feature names

## Confidence
- High Confidence: LLM-Score and LLM-Rank achieving competitive performance with LASSO on standard benchmarks
- Medium Confidence: Zero-shot prompting being sufficient for strong performance across diverse datasets
- Medium Confidence: Correlation between LLM-generated scores and standard feature importance metrics increasing with model scale
- Low Confidence: Generalizability to specialized domains with highly technical or ambiguous feature names

## Next Checks
1. Test LLM-Select performance on datasets with intentionally ambiguous or domain-specific feature names to quantify the semantic dependency of the approach
2. Evaluate feature selection performance when multiple highly correlated features exist, testing whether LLM-based methods capture feature redundancy
3. Compare LLM-Select against state-of-the-art feature selection methods (like recursive feature elimination) on high-dimensional biomedical datasets where feature semantics are less standardized