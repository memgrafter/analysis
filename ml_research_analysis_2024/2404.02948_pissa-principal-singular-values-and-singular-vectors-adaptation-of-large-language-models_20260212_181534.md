---
ver: rpa2
title: 'PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large
  Language Models'
arxiv_id: '2404.02948'
source_url: https://arxiv.org/abs/2404.02948
tags:
- pissa
- lora
- training
- steps
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PiSSA improves large language model fine-tuning by initializing
  low-rank adapters with principal singular values and vectors from the original model's
  weight matrices, rather than random noise. This approach enables faster convergence
  and better performance compared to LoRA across 12 different models (184M-70B parameters)
  on 13 tasks.
---

# PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models

## Quick Facts
- arXiv ID: 2404.02948
- Source URL: https://arxiv.org/abs/2404.02948
- Authors: Fanxu Meng; Zhaohui Wang; Muhan Zhang
- Reference count: 40
- Primary result: PiSSA achieves 77.7% accuracy on GSM8K versus 74.53% for LoRA when fine-tuning Gemma-7B

## Executive Summary
PiSSA introduces a novel approach to fine-tuning large language models by initializing adapter matrices with principal singular values and vectors from the original weight matrices, rather than random noise. This method improves convergence speed and performance across 12 different models (184M-70B parameters) on 13 diverse tasks. The approach is compatible with existing LoRA pipelines and can be combined with 4-bit quantization to reduce quantization error by approximately 20% while maintaining high performance.

## Method Summary
PiSSA adapts large language models by decomposing weight matrices using SVD and initializing adapter matrices A and B with the principal components. The original weight matrix W is decomposed into U S V^T, with the principal components (U[:,:r], S[:r,:r], V[:,:r]) used to initialize A = U[:,:r] S^1/2[:r,:r] and B = S^1/2[:r,:r] V[:,:r]⊤, while the residual components are frozen. The adapter matrices are then fine-tuned using standard optimizers, inheriting the low-rank structure from LoRA for computational efficiency.

## Key Results
- PiSSA outperforms LoRA across 12 models (184M-70B parameters) on 13 tasks
- On GSM8K, Gemma-7B fine-tuned with PiSSA achieves 77.7% accuracy versus 74.53% for LoRA
- QPiSSA reduces quantization error by ~20% compared to QLoRA while maintaining fast convergence
- Faster initial convergence with significantly higher gradient norms in early training stages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PiSSA improves convergence speed by initializing adapters with principal singular values/vectors from the original model
- Mechanism: Instead of random initialization (LoRA), PiSSA decomposes the original weight matrix W into principal components and residual components. The principal components initialize the adapter matrices A and B, while the residual components are frozen
- Core assumption: The principal singular values/vectors of the pre-trained weight matrix capture the most important directions for the model's behavior
- Evidence anchors:
  - [abstract]: "PiSSA shares the same architecture as LoRA, but initializes the adaptor matrices A and B with the principal components of the original matrix W"
  - [section 3]: "A and B are initialized based on the principal singular values and singular vectors and are trainable"
- Break condition: If the principal components of W are not actually the most important for fine-tuning

### Mechanism 2
- Claim: PiSSA reduces quantization error by preserving principal components in full precision
- Mechanism: When combining PiSSA with quantization, the residual matrix W_res is quantized while the adapter matrices A and B remain in full precision
- Core assumption: The residual matrix W_res after removing principal components has a more favorable distribution for quantization than the original matrix W
- Evidence anchors:
  - [section 4]: "Because the principal components W pri are preserved in the adapter at full precision, an additional benefit of PiSSA is that when applying quantization to the frozen part W res, we can significantly reduce the quantization error compared to QLoRA"
  - [section 4]: "Since the residual model has removed the large-singular-value components, W res has a narrower distribution than that of W"
- Break condition: If the principal components don't actually contain the most important information

### Mechanism 3
- Claim: PiSSA maintains faster convergence while being compatible with LoRA improvements
- Mechanism: PiSSA uses the same low-rank architecture as LoRA, making it compatible with improvements like AdaLoRA and DoRA
- Core assumption: The low-rank structure shared between PiSSA and LoRA is sufficient for compatibility with LoRA improvements
- Evidence anchors:
  - [section 6]: "Since PiSSA shares the identical architecture with LoRA, it inherits most of LoRA's benefits"
  - [section A]: "PiSSA and AdaLoRA represent different improvements to LoRA, making them combinable"
- Break condition: If the principal component initialization interferes with LoRA improvements

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: PiSSA fundamentally relies on SVD to decompose weight matrices into principal and residual components
  - Quick check question: What mathematical operation decomposes a matrix into U, S, and V components where S contains singular values?

- Concept: Low-rank approximation
  - Why needed here: Both LoRA and PiSSA use low-rank matrices to approximate weight updates efficiently
  - Quick check question: Why is using rank-r matrices (where r << min(m,n)) computationally advantageous for large models?

- Concept: Quantization error measurement
  - Why needed here: Understanding how quantization error is measured (nuclear norm) is crucial for evaluating QPiSSA's benefits
  - Quick check question: How is quantization error typically measured for matrices, and why does the nuclear norm matter?

## Architecture Onboarding

- Component map: Original weight matrix W → SVD decomposition → Principal components → Adapter matrices A and B → Residual matrix W_res → Forward pass Y = X(W_res + AB)

- Critical path: SVD decomposition → Adapter initialization → Fine-tuning with gradient updates to A and B only

- Design tradeoffs:
  - SVD computation cost vs. initialization benefit
  - Rank selection (r) vs. performance/memory tradeoff
  - Full precision for adapters vs. quantized residuals

- Failure signatures:
  - Slow convergence (indicates poor initialization or rank too small)
  - High quantization error (indicates rank too small or poor SVD decomposition)
  - Memory issues (indicates rank too large)

- First 3 experiments:
  1. Compare convergence speed of PiSSA vs LoRA on a small model (e.g., LLaMA-2-7B) with rank=8
  2. Measure quantization error reduction when applying QPiSSA vs QLoRA on the same model
  3. Test compatibility by combining PiSSA with AdaLoRA-style orthogonal regularization

## Open Questions the Paper Calls Out

- Can PiSSA be effectively adapted to convolutional layers and enhance the performance of vision tasks?
- Can PiSSA benefit from adaptive rank adjustment methods like AdaLoRA and DyLoRA to dynamically optimize its performance?
- What is the theoretical explanation for PiSSA's superior performance over LoRA in terms of convergence and generalization?

## Limitations
- Rank selection impact is not thoroughly explored across different model sizes and tasks
- SVD computation efficiency details are not provided
- Long-term fine-tuning behavior beyond 1-3 epochs is not evaluated

## Confidence

### High confidence:
- PiSSA's core mechanism of initializing adapters with principal singular values/vectors is mathematically sound
- Empirical improvements over LoRA on tested tasks are clearly demonstrated

### Medium confidence:
- The claim about reduced quantization error (~20% improvement) is supported but practical impact could vary
- Compatibility with LoRA improvements is theoretically sound but limited empirical validation

## Next Checks
1. Systematically test PiSSA performance across a wider range of rank values (e.g., 4, 8, 16, 32, 64, 128) on the same tasks to identify optimal rank ranges and sensitivity
2. Run PiSSA and LoRA for 10+ epochs on multiple tasks to evaluate long-term stability and convergence behavior
3. Test PiSSA initialization strategy on a model architecture not included in the original study (e.g., Falcon or Mistral) to validate generalizability beyond LLaMA-family models