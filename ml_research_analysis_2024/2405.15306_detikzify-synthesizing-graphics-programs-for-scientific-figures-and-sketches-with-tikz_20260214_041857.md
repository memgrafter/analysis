---
ver: rpa2
title: 'DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches
  with TikZ'
arxiv_id: '2405.15306'
source_url: https://arxiv.org/abs/2405.15306
tags:
- node
- figures
- sketches
- tikz
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DeTikZify, a multimodal language model that
  automatically synthesizes scientific figures as semantics-preserving TikZ graphics
  programs from sketches and existing figures. The authors create three new datasets:
  DaTikZv2 (over 360k human-created TikZ graphics), SketchFig (hand-drawn sketches
  paired with scientific figures), and MetaFig (diverse scientific figures with metadata).'
---

# DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ

## Quick Facts
- arXiv ID: 2405.15306
- Source URL: https://arxiv.org/abs/2405.15306
- Authors: Jonas Belouadi; Simone Paolo Ponzetto; Steffen Eger
- Reference count: 40
- One-line primary result: DeTikZify achieves SelfSim scores of 0.965 on reference figures and sketches, outperforming GPT-4V (0.612 and 0.15 respectively)

## Executive Summary
This paper introduces DeTikZify, a multimodal language model that automatically synthesizes scientific figures as semantics-preserving TikZ graphics programs from sketches and existing figures. The authors create three new datasets: DaTikZv2 (over 360k human-created TikZ graphics), SketchFig (hand-drawn sketches paired with scientific figures), and MetaFig (diverse scientific figures with metadata). DeTikZify combines a SigLIP vision encoder with a LLaMA-based language model and employs an MCTS-based inference algorithm for iterative refinement without additional training. Through both automatic and human evaluation, DeTikZify outperforms commercial models Claude 3 and GPT-4V in synthesizing TikZ programs, achieving significantly higher SelfSim scores on both reference figures and sketches.

## Method Summary
DeTikZify is a multimodal language model that generates TikZ code from visual inputs by combining a SigLIP vision encoder with a LLaMA-based language model. The model is trained on three newly created datasets: DaTikZv2 (over 360k human-created TikZ graphics), SketchFig (sketches paired with scientific figures), and MetaFig (diverse scientific figures with metadata). The model uses an MCTS-based inference algorithm that enables iterative refinement of outputs through a self-assessed perceptual similarity reward (SelfSim) computed by the model's own vision encoder. This approach allows the model to improve its generated TikZ programs without additional training by exploring different code variations and selecting those that best match the input image.

## Key Results
- DeTikZify achieves SelfSim scores of 0.965 on reference figures and 0.965 on sketches, compared to 0.612 and 0.15 for GPT-4V respectively
- The MCTS-based inference algorithm effectively boosts performance without additional training
- Through human evaluation, DeTikZify outperforms commercial models Claude 3 and GPT-4V in synthesizing TikZ programs
- The best model demonstrates strong generalization across diverse scientific figures and sketches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model can iteratively refine its output without additional training by integrating Monte Carlo Tree Search (MCTS) with its own outputs.
- Mechanism: MCTS builds a search tree where each node represents a partial TikZ program. The algorithm selects promising continuations based on an Upper Confidence Trees (UCT) policy, evaluates them through rollouts (generating more code), and backpropagates rewards based on compilation success or perceptual similarity.
- Core assumption: DeTikZify can assess the quality of its own generated TikZ programs either through compiler diagnostics or by computing SelfSim using its own vision encoder.
- Evidence anchors:
  - [abstract] "We also introduce an MCTS-based inference algorithm that enables DeTikZify to iteratively refine its outputs without the need for additional training."
  - [section] "We also present an inference algorithm based on Monte Carlo Tree Search (MCTS) that is tailored to graphics programs and allows DeTikZify to iteratively refine its own outputs for a given computational budget, further improving performance without additional training."
  - [corpus] Weak evidence; the corpus mentions related works on MCTS and graphics program synthesis but lacks direct evidence of DeTikZify's specific MCTS integration.
- Break condition: If the model cannot reliably evaluate its own outputs or if the search space becomes too large for practical computation.

### Mechanism 2
- Claim: The model outperforms commercial LLMs like GPT-4V and Claude 3 in synthesizing TikZ programs.
- Mechanism: DeTikZify is trained on specialized datasets (DaTikZv2, SketchFig, MetaFig) that provide extensive exposure to TikZ graphics and paired sketches/figures, enabling it to generate more accurate and semantically preserving TikZ code.
- Core assumption: The training data captures the diversity and complexity of scientific figures and sketches needed for the model to generalize well.
- Evidence anchors:
  - [abstract] "Through both automatic and human evaluation, we demonstrate that DeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ programs."
  - [section] "We train DeTikZify on MetaFig and DaTikZv2, augmented with synthetic sketches that mimic SketchFig. We demonstrate that DeTikZify can effectively synthesize TikZ programs for both existing scientific figures and sketches, outperforming the commercial large language models (LLMs) GPT-4V and Claude 3."
  - [corpus] Moderate evidence; related works on image-to-LaTeX conversion and graphics program synthesis suggest the importance of specialized training data.
- Break condition: If the training data is insufficient or unrepresentative of the target domain.

### Mechanism 3
- Claim: SelfSim (self-assessed perceptual similarity) is an effective reward signal for guiding the model's search process.
- Mechanism: The model uses its own vision encoder to encode both the input image and the compiled output figure, computing their cosine similarity as a reward. This allows the model to guide its own search without external evaluators.
- Core assumption: The model's vision encoder can accurately assess perceptual similarity between the input and output images.
- Evidence anchors:
  - [section] "We also present an inference algorithm based on Monte Carlo Tree Search (MCTS) that is tailored to graphics programs and allows DeTikZify to iteratively refine its own outputs for a given computational budget, further improving performance without additional training."
  - [section] "For time-budgeted inference (TI), we use the more fine-grained SelfSim-based reward and continue from OI until a computational budget of 10 minutes is exhausted."
  - [corpus] Weak evidence; the corpus mentions related works on visual instruction tuning but lacks direct evidence of SelfSim's effectiveness.
- Break condition: If the vision encoder's similarity assessments do not correlate with human judgments or if they lead the search astray.

## Foundational Learning

- Concept: Multimodal learning
  - Why needed here: The model must process both visual inputs (sketches and figures) and generate textual outputs (TikZ code).
  - Quick check question: Can the model encode images into a format that can be processed by a language model to generate code?

- Concept: Code generation
  - Why needed here: The ultimate goal is to generate syntactically and semantically correct TikZ programs from visual inputs.
  - Quick check question: Does the model generate code that compiles without errors and accurately represents the input figure or sketch?

- Concept: Reinforcement learning / iterative refinement
  - Why needed here: The model improves its outputs through iterative refinement guided by reward signals, without additional training.
  - Quick check question: Does the model's performance improve over multiple iterations of MCTS-guided refinement?

## Architecture Onboarding

- Component map: Input image -> SigLIP vision encoder -> Connector -> LLaMA-based language model -> TikZ code -> LaTeX engine -> SelfSim reward -> MCTS refinement

- Critical path: Input image → Vision encoder → Connector → Language model → TikZ code → LaTeX engine → Reward computation → MCTS refinement

- Design tradeoffs:
  - Model size vs. inference speed: Larger models may generate better code but are slower
  - Reward signal choice: Compiler diagnostics are binary, while SelfSim provides a continuous signal but may be less reliable
  - MCTS budget: More iterations can improve quality but increase computation time

- Failure signatures:
  - Model generates code that does not compile: Indicates issues with the language model or reward signal
  - Generated figures do not match the input: Suggests problems with the vision encoder or reward signal
  - MCTS does not improve outputs: May indicate issues with the search algorithm or reward signal

- First 3 experiments:
  1. Verify that the model can generate compilable TikZ code from a simple input figure without MCTS
  2. Test the SelfSim reward signal by comparing its assessments to human judgments on a small set of examples
  3. Run MCTS with a small budget on a simple input to see if it improves the output compared to direct generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DeTikZify vary when trained on synthetic sketches versus human-created sketches?
- Basis in paper: [explicit] The paper mentions that DeTikZify is trained on synthetic sketches learned from SketchFig, but does not directly compare performance on synthetic vs. human-created sketches.
- Why unresolved: The paper only provides overall performance metrics, not a breakdown comparing the two sketch types.
- What evidence would resolve it: A direct comparison of DeTikZify's performance metrics (e.g., SelfSim, cBLEU) when trained and evaluated on synthetic sketches versus human-created sketches from SketchFig.

### Open Question 2
- Question: How does the size of the training dataset (DaTikZv2) impact the performance of DeTikZify?
- Basis in paper: [inferred] The paper introduces DaTikZv2 as the largest TikZ dataset to date, but does not explore the relationship between dataset size and model performance.
- Why unresolved: The paper does not provide any experiments or analysis on how varying the size of the training dataset affects the model's ability to generate TikZ programs.
- What evidence would resolve it: Training DeTikZify on subsets of different sizes from DaTikZv2 and comparing the performance metrics to determine the impact of dataset size.

### Open Question 3
- Question: How does the computational budget for MCTS affect the trade-off between the quality of generated TikZ programs and inference time?
- Basis in paper: [explicit] The paper introduces an MCTS-based inference algorithm and mentions a computational budget of 10 minutes, but does not explore how varying this budget impacts the quality-time trade-off.
- Why unresolved: The paper only reports results for a fixed computational budget, leaving the relationship between budget, quality, and inference time unexplored.
- What evidence would resolve it: Running experiments with different computational budgets for MCTS and measuring the resulting changes in performance metrics (e.g., SelfSim, cBLEU) and inference times to quantify the quality-time trade-off.

## Limitations
- The SelfSim metric may introduce self-reinforcing bias as it relies on the model's own vision encoder for evaluation
- The MCTS algorithm details (exploration coefficient, rollout policy) are not fully specified, making exact replication difficult
- Performance comparisons with commercial models may be influenced by differences in prompting strategies rather than inherent model capability

## Confidence

- High confidence: The model architecture combining SigLIP and LLaMA-based components is technically sound for multimodal code generation
- Medium confidence: The MCTS-based refinement algorithm is novel and theoretically justified, but implementation details are sparse
- Low confidence: The SelfSim metric as a standalone evaluation measure may not fully capture semantic fidelity to human standards

## Next Checks

1. Implement a blind human evaluation comparing DeTikZify outputs with GPT-4V and Claude 3 on identical inputs to validate automated metrics
2. Test the MCTS algorithm with varying computational budgets on a standardized benchmark to measure the relationship between iteration count and output quality
3. Evaluate the model's robustness by testing it on out-of-distribution scientific figures not present in any training set