---
ver: rpa2
title: Mind the Cost of Scaffold! Benign Clients May Even Become Accomplices of Backdoor
  Attack
arxiv_id: '2411.16167'
source_url: https://arxiv.org/abs/2411.16167
tags:
- backdoor
- learning
- global
- local
- attacker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Scaffold improves federated learning in non-IID settings but increases
  vulnerability to backdoor attacks. This paper introduces BadSFL, the first backdoor
  attack targeting Scaffold, which exploits the control variate to turn benign clients
  into accomplices.
---

# Mind the Cost of Scaffold! Benign Clients May Even Become Accomplices of Backdoor Attack

## Quick Facts
- arXiv ID: 2411.16167
- Source URL: https://arxiv.org/abs/2411.16167
- Reference count: 11
- Primary result: Scaffold's control variate can be exploited to create backdoor attacks where benign clients become accomplices, achieving 60%+ primary task accuracy and backdoor persistence lasting 3x longer than baselines

## Executive Summary
This paper presents BadSFL, the first backdoor attack targeting Scaffold Federated Learning (SFL). The attack exploits the control variate mechanism in Scaffold to turn benign clients into accomplices, amplifying backdoor persistence and effectiveness. By using GAN-based data supplementation to simulate full dataset knowledge and optimizing local updates with the control variate, BadSFL achieves high attack success rates while maintaining primary task accuracy above 60%. The attack is particularly stealthy and durable, persisting for over 60 rounds and lasting up to three times longer than existing methods after malicious injections stop.

## Method Summary
BadSFL targets Scaffold Federated Learning by exploiting the control variate mechanism. The attacker uses a GAN to generate synthetic samples that supplement their local non-IID dataset, effectively bridging the data gap between clients. During the attack, the attacker downloads the global model and control variate from the server, then trains a backdoor model using the supplemented dataset. The backdoor is optimized with the control variate to predict the global model's convergence direction, ensuring persistence. By tampering with the control variate (∆cp), the attacker subtly steers benign clients' local gradient updates towards the poisoned direction, amplifying the backdoor's effectiveness.

## Key Results
- BadSFL achieves high Backdoor Task Accuracy (BTA) while maintaining Primary Task Accuracy (PTA) above 60% on MNIST, CIFAR-10, and CIFAR-100
- The backdoor persists for over 60 rounds and lasts up to three times longer than existing methods after stopping malicious injections
- Feature-based backdoor triggers perform best among different trigger types due to their stealthiness
- BadSFL remains effective under multiple defense strategies including differential privacy, model pruning, and anomaly detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BadSFL exploits the control variate to subtly steer benign clients' local gradient updates towards the attacker's poisoned direction
- Mechanism: By tampering with the control variate (∆cp), the attacker manipulates the correction term that normally mitigates client drift, causing benign clients to inadvertently amplify the backdoor's persistence in their updates
- Core assumption: The global control variate is known to participating clients and can be used as a reference for the global model's convergence direction
- Evidence anchors:
  - [abstract] "This paper presents BadSFL, the first backdoor attack targeting Scaffold, which turns benign clients into accomplices to amplify the attack effect. The core idea of BadSFL is to uniquely tamper with the control variate to subtly steer benign clients' local gradient updates towards the attacker's poisoned direction"
  - [section] "In SFL, control variate (denoted as ci) is used to correct the client drift and align local models with the global model. If attackers strictly follow protocols and use the ci to correct their malicious models during the triggering planting process, the effectiveness of the attack can be reduced"
- Break condition: If the server implements strict validation of control variate values or if the manipulation is detected through anomaly detection

### Mechanism 2
- Claim: GAN-based data supplementation enables the attacker to simulate full dataset knowledge and maintain high accuracy on both backdoor and benign samples
- Mechanism: The attacker trains a GAN using their local non-IID data to generate synthetic samples that resemble data from other clients, effectively bridging the dataset gap and creating a more representative dataset (Dc) for backdoor training
- Core assumption: The GAN can generate realistic fake samples that closely resemble data from other clients' datasets
- Evidence anchors:
  - [abstract] "BadSFL leverages a Generative Adversarial Network (GAN) based on the global model to complement the training set, achieving high accuracy on both backdoor and benign samples"
  - [section] "To mitigate this, inspired by Zhang et al. (Zhang et al. 2019), the attacker can employ GAN to generate synthetic samples that resemble the data held by other clients"
- Break condition: If the GAN fails to generate realistic samples or if the synthetic data is detected as artificial by the system

### Mechanism 3
- Claim: The backdoor function persists in the global model for over 60 rounds and lasts up to three times longer than existing methods after stopping malicious injections
- Mechanism: By optimizing the local updates with the control variate and training the backdoor model to be closer to the global model, the attacker ensures the backdoor remains effective even after malicious updates cease
- Core assumption: The control variate optimization effectively prevents catastrophic forgetting of the backdoor function
- Evidence anchors:
  - [abstract] "By optimizing local updates with the control variate, BadSFL ensures backdoor persistence, maintaining effectiveness for over 60 rounds and lasting up to three times longer than existing methods after stopping malicious injections"
  - [section] "This constraint, derived from c, functions similarly to using the future global model for optimization, as suggested by Wen et al. (Wen et al. 2022). This constraint can be integrated into the loss function to enhance the backdoor's effectiveness and persistence in the global model"
- Break condition: If benign clients' updates overwhelm the backdoor updates or if the system implements effective backdoor erasure mechanisms

## Foundational Learning

- Concept: Federated Learning and Scaffold Algorithm
  - Why needed here: Understanding how Scaffold works is crucial to comprehending how BadSFL exploits it
  - Quick check question: How does Scaffold's control variate differ from standard federated learning aggregation methods?

- Concept: Backdoor Attacks in Machine Learning
  - Why needed here: The paper builds on existing backdoor attack methodologies but adapts them for Scaffold
  - Quick check question: What distinguishes data poisoning from model replacement in backdoor attacks?

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: GANs are used to supplement the attacker's dataset with synthetic samples
  - Quick check question: How does a GAN's generator and discriminator work together to create realistic synthetic data?

## Architecture Onboarding

- Component map: Attacker client with GAN -> Global model on server -> Control variate mechanism -> Data supplementation pipeline -> Backdoor trigger injection system
- Critical path: GAN training -> Data supplementation -> Backdoor training with control variate optimization -> Upload poisoned updates -> Backdoor persistence
- Design tradeoffs: The paper balances between maintaining primary task accuracy and ensuring backdoor effectiveness, while also considering stealth and durability
- Failure signatures: Significant drop in primary task accuracy, detection of unusual control variate values, failure of backdoor to persist after attack cessation
- First 3 experiments:
  1. Test GAN's ability to generate realistic samples that match the global model's distribution
  2. Verify that backdoor training maintains primary task accuracy above 60% while achieving high backdoor task accuracy
  3. Measure backdoor persistence duration compared to baseline attacks after stopping malicious injections

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors make feature-based backdoor triggers more durable than label-flipping or pattern triggers in Scaffold Federated Learning?
- Basis in paper: [explicit] The paper states "feature-based trigger performs the best among them, benefiting from its stealthiness without directly manipulating the images, thereby making its updates less likely to conflict with those from benign updates."
- Why unresolved: The paper doesn't provide detailed analysis of why feature-based triggers are more durable, only stating the observed result.
- What evidence would resolve it: Detailed comparison of how different trigger types affect the alignment with global model updates, including analysis of parameter sensitivity and robustness to benign client updates.

### Open Question 2
- Question: Why does the Neurotoxin approach fail to provide expected durability in Scaffold Federated Learning despite theoretical advantages?
- Basis in paper: [explicit] The paper states "our experimental results shown above do not demonstrate the anticipated effectiveness of Neurotoxin" and notes "This observation is also present in our BadSFL experiments and needs further investigation."
- Why unresolved: The paper acknowledges the failure but doesn't provide analysis of why Neurotoxin doesn't work as expected in Scaffold FL.
- What evidence would resolve it: Investigation of how control variates in Scaffold affect the stability of parameters targeted by Neurotoxin, and analysis of the interaction between control variates and backdoor persistence.

### Open Question 3
- Question: What is the exact mechanism by which control variates in Scaffold can be exploited to enhance backdoor durability?
- Basis in paper: [explicit] The paper states "We exploit the global control variate... as a reference to predict the global model's convergence direction, ensuring the backdoor's persistence" and describes using it in the loss function.
- Why unresolved: While the paper describes using control variates, it doesn't fully explain the theoretical basis for why this improves durability or provide a complete mathematical model.
- What evidence would resolve it: Mathematical analysis of how control variate optimization affects parameter drift over time, including convergence analysis showing why the backdoor persists longer.

## Limitations
- The paper lacks specific implementation details for the GAN architecture, particularly exact layer configurations for ResNet discriminators
- The assumption that GAN can successfully generate realistic synthetic samples bridging the dataset gap is not extensively validated
- The paper doesn't address scenarios where benign clients might detect anomalous behavior or where systems might implement stricter validation mechanisms

## Confidence
- High Confidence: The fundamental mechanism of exploiting the control variate for backdoor persistence is well-grounded in the Scaffold algorithm's design
- Medium Confidence: Experimental results showing high PTA and BTA are compelling but lack detailed implementation specifications
- Low Confidence: The claim about backdoor persistence lasting "up to three times longer than existing methods" is difficult to verify without access to baseline implementations

## Next Checks
1. **GAN Sample Quality Validation**: Implement the GAN architecture with different configurations and quantitatively measure sample quality using established metrics (FID, Inception Score) to verify that synthetic samples are sufficiently realistic and diverse.

2. **Control Variate Manipulation Detection**: Test whether the proposed control variate manipulation can be detected by standard anomaly detection mechanisms in federated learning systems, measuring false positive and false negative rates.

3. **Defense Strategy Effectiveness**: Implement and test the proposed defense strategies (differential privacy, model pruning, anomaly detection) against BadSFL to verify the claimed robustness and identify potential weaknesses or blind spots in the attack.