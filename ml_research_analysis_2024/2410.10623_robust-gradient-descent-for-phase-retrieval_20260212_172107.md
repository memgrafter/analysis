---
ver: rpa2
title: Robust Gradient Descent for Phase Retrieval
arxiv_id: '2410.10623'
source_url: https://arxiv.org/abs/2410.10623
tags:
- robust
- phase
- mean
- noise
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a framework for robust phase retrieval in the
  presence of heavy-tailed noise and adversarial contamination. The authors propose
  a two-stage approach combining spectral initialization with robust gradient descent.
---

# Robust Gradient Descent for Phase Retrieval

## Quick Facts
- arXiv ID: 2410.10623
- Source URL: https://arxiv.org/abs/2410.10623
- Reference count: 0
- One-line primary result: A framework for robust phase retrieval handling both heavy-tailed noise and adversarial contamination with spectral initialization and robust gradient descent

## Executive Summary
This paper develops a framework for robust phase retrieval in the presence of heavy-tailed noise and adversarial contamination. The authors propose a two-stage approach combining spectral initialization with robust gradient descent. For the initialization step, they present two alternatives: a computationally efficient method based on mean estimation with sample complexity O(n² log n) and contamination level O(1/n), or a statistically optimal method based on covariance estimation with sample complexity O(n) and constant contamination. The iterative procedure uses robust gradient estimators at each step with sample complexity O(n log n) per iteration. The method achieves a statistical error of O(σ∥x*∥²(√ε + √(n log n)/m)) and optimization error that decays exponentially with the number of iterations. The framework works for both known zero-mean noise and unknown mean noise (via preprocessing). The approach is notable for simultaneously handling both heavy-tailed noise and adversarial contamination in both covariates and responses, a setting not previously addressed in the phase retrieval literature.

## Method Summary
The method uses a two-stage approach: spectral initialization followed by robust gradient descent. For initialization, two variants are available - mean estimation (computationally efficient, O(n²log n) samples, O(1/n) contamination) or covariance estimation (statistically optimal, O(n) samples, constant contamination). The iterative procedure uses robust gradient estimators with fresh samples at each iteration, achieving exponential convergence. For unknown mean noise, a preprocessing step converts the problem to zero-mean noise format. The framework simultaneously handles heavy-tailed noise and adversarial contamination in both covariates and responses, achieving statistical error O(σ∥x*∥²(√ε + √(n log n)/m)) with O(n log n) samples per iteration.

## Key Results
- Achieves statistical error O(σ∥x*∥²(√ε + √(n log n)/m)) with sample complexity O(n log n) per iteration
- Provides two initialization methods: computationally efficient mean estimation (O(n²log n) samples, O(1/n) contamination) or statistically optimal covariance estimation (O(n) samples, constant contamination)
- Handles both heavy-tailed noise and adversarial contamination simultaneously in both covariates and responses
- Preprocessing step enables handling of unknown mean noise by converting to zero-mean noise format
- Optimization error decays exponentially with number of iterations T

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The spectral initialization leverages the special eigenstructure of $E[ya a^T]$ to find a starting point within the region of strong convexity and smoothness.
- Mechanism: The matrix $E[ya a^T]$ has eigenvalues $\{3\|x^*\|^2, \|x^*\|^2, \ldots, \|x^*\|^2\}$ with eigenvectors $\{x^*, v_2, \ldots, v_n\}$ where $v_i$ are orthogonal to $x^*$. Robust mean estimation recovers this matrix, and the leading eigenvector is $\pm x^*$.
- Core assumption: The noise has mean zero and the covariance structure is preserved under robust estimation.
- Evidence anchors:
  - [abstract]: "For the initialization step, they present two alternatives: a computationally efficient method based on mean estimation"
  - [section]: "the leading eigenpair of $E[ya a^T] = \|x^*\|^2 I_n + 2x^*(x^*)^T$ is $(\pm x^*, 3\|x^*\|^2)$"
  - [corpus]: Weak evidence - related work on spectral methods but not directly on this specific eigenstructure
- Break condition: If noise has non-zero mean or if robust estimation fails to preserve the eigenstructure due to heavy-tailed noise or contamination.

### Mechanism 2
- Claim: Robust gradient descent with fresh samples at each iteration maintains descent directions that are accurate enough to ensure linear convergence within the region of strong convexity.
- Mechanism: Each gradient estimate $\hat{g}(x_t; T, \delta, \varepsilon)$ satisfies $\| \hat{g} - \nabla r(x_t) \| \leq A(m, \delta, \varepsilon)\|x_t - x^*\| + B(m, \delta, \varepsilon)$ with high probability. This error bound allows controlling the contraction factor in the descent step.
- Core assumption: The population risk is strongly convex and smooth in a ball around $x^*$, and the gradient estimator error is sufficiently small.
- Evidence anchors:
  - [abstract]: "The iterative procedure uses robust gradient estimators at each step with sample complexity $O(n \log n)$ per iteration"
  - [section]: "Lemma 2.2 can be then applied inductively as long as the right-hand side of (7) is at most R"
  - [corpus]: Strong evidence - multiple papers on robust gradient descent cite similar mechanisms
- Break condition: If contamination level $\varepsilon$ is too high relative to signal-to-noise ratio, or if sample size per iteration is insufficient for the error bound.

### Mechanism 3
- Claim: The preprocessing step for unknown mean noise converts the problem to one with zero-mean noise while preserving the essential geometry for recovery.
- Mechanism: By forming $b = (a + a')/\sqrt{2}$, $c = (a - a')/\sqrt{2}$, and $\upsilon = (y - y')/2$, the new model $\upsilon = b^T x^*(x^*)^T c + \zeta$ has noise $\zeta$ with mean zero and reduced variance, allowing application of the zero-mean noise algorithm.
- Core assumption: The original noise is independent of the covariates and has finite variance.
- Evidence anchors:
  - [abstract]: "For the latter, we propose a preprocessing step that alters the problem into a new format"
  - [section]: "the noise $\zeta$ has (known) mean zero and variance $\sigma^2/2$ and is independent of $b$ and $c$"
  - [corpus]: Moderate evidence - preprocessing for unknown mean appears in related linear regression work
- Break condition: If noise has heavy tails that aren't preserved under the differencing operation, or if the preprocessing destroys the essential geometry.

## Foundational Learning

- Concept: Non-convex optimization and local geometry
  - Why needed here: Phase retrieval is inherently non-convex, and success depends on understanding when gradient descent can find global minima from good initializations
  - Quick check question: What conditions on a non-convex function guarantee that gradient descent from a good initialization converges to a global minimum?

- Concept: Robust statistics and mean estimation under contamination
  - Why needed here: The algorithm must handle both heavy-tailed noise and adversarial contamination in both covariates and responses
  - Quick check question: How does the sample complexity of robust mean estimation scale with contamination level $\varepsilon$ and dimension $n$?

- Concept: Spectral methods and eigenvalue perturbation
  - Why needed here: The initialization relies on finding the leading eigenvector of a noisy covariance matrix
  - Quick check question: Under what conditions does Davis-Kahan theorem guarantee that the leading eigenvector of a perturbed matrix is close to the original?

## Architecture Onboarding

- Component map: Data → Preprocessing → Spectral Initialization → Iterative Descent → Output
- Critical path: Data → Preprocessing → Spectral Initialization → Iterative Descent → Output
- Design tradeoffs:
  - Mean estimation vs covariance estimation: computational efficiency vs sample complexity
  - Fresh samples vs reusing samples: statistical accuracy vs computational cost
  - Step size choice: depends on unknown $\|x^*\|$ vs using $\|x_0\|$ approximation
- Failure signatures:
  - Poor initialization: iterates drift away from $x^*$
  - Gradient estimation error too large: descent direction points away from minimum
  - Contamination too high: error bounds fail to guarantee convergence
- First 3 experiments:
  1. Test spectral initialization on clean data with known $x^*$ to verify eigenstructure recovery
  2. Run gradient descent with perfect gradients (oracle) from initialization to establish baseline convergence
  3. Add contamination and test gradient estimation accuracy at various sample sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the spectral initialization step be made computationally efficient while maintaining statistical optimality?
- Basis in paper: Explicit - The paper discusses two initialization methods: mean estimation (computationally efficient but suboptimal sample complexity) and covariance estimation (statistically optimal but not efficiently computable)
- Why unresolved: The paper acknowledges that covariance estimation provides better statistical guarantees but notes "to the best of our knowledge, there are currently no efficient methods for covariance estimation"
- What evidence would resolve it: A polynomial-time algorithm for robust covariance estimation with the same statistical guarantees as the non-efficient methods

### Open Question 2
- Question: Is the sample complexity of O(n log n) per iteration for the iterative procedure optimal?
- Basis in paper: Explicit - The paper states "Although the sample complexity (total number of samples used) of the iterative procedure is O(T n log(n)), a tuning of T = ˜O(1) reduces the sample complexity to O(n log(n))"
- Why unresolved: The paper notes this is comparable to previous work but doesn't establish lower bounds for the sample complexity in this robust setting
- What evidence would resolve it: Information-theoretic lower bounds showing that O(n log n) is necessary for this problem with adversarial contamination

### Open Question 3
- Question: Can the contamination tolerance be improved beyond the current bounds?
- Basis in paper: Explicit - The paper achieves ε = O(1/n) for mean estimation-based initialization and ε = O(1) for covariance-based initialization
- Why unresolved: The paper doesn't explore whether these bounds are tight or if they can be improved with different techniques
- What evidence would resolve it: Lower bounds on the maximum tolerable contamination level, or algorithms that achieve better contamination tolerance under the same assumptions

## Limitations

- The initialization methods rely on specific eigenstructure assumptions that may not hold under extreme contamination
- Computational efficiency claims for the mean estimation approach depend on access to efficient robust mean estimators that may not be practical for high dimensions
- Theoretical bounds contain unspecified constants that could significantly impact practical performance
- Method's behavior with structured noise or very low signal-to-noise ratio is not fully characterized

## Confidence

**High Confidence**: The core mechanism of combining spectral initialization with robust gradient descent is well-established in the literature. The theoretical framework for analyzing convergence under these conditions is sound and follows standard non-convex optimization analysis.

**Medium Confidence**: The specific error bounds and sample complexity guarantees depend on unspecified constants and assumptions about the noise structure. While the asymptotic behavior is correctly characterized, practical performance may vary significantly.

**Low Confidence**: The preprocessing step for unknown mean noise and its effect on the overall statistical guarantees requires more empirical validation. The paper provides theoretical justification but limited experimental verification of this component.

## Next Checks

1. **Eigenstructure Sensitivity Test**: Systematically vary the contamination level ε and noise distribution to quantify how the spectral initialization degrades. Measure the distance between the estimated leading eigenvector and x* as a function of ε and noise parameters.

2. **Gradient Estimation Accuracy**: Implement the robust gradient estimator and measure its accuracy at various sample sizes m and contamination levels ε. Compare against oracle gradients to establish the practical error bounds and their impact on convergence speed.

3. **Scaling Behavior**: Run comprehensive experiments varying n, σ, and ε across multiple orders of magnitude. Measure both statistical error and computational time to validate the claimed O(n log n) sample complexity per iteration and O(n² log n) initialization complexity.