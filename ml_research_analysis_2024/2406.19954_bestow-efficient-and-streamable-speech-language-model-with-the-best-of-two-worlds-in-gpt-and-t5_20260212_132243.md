---
ver: rpa2
title: 'BESTOW: Efficient and Streamable Speech Language Model with the Best of Two
  Worlds in GPT and T5'
arxiv_id: '2406.19954'
source_url: https://arxiv.org/abs/2406.19954
tags:
- speech
- arxiv
- streaming
- speechllm
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BESTOW, a SpeechLLM architecture that enables
  both streaming and multitask capabilities at scale, which previous SpeechLLM approaches
  could not achieve simultaneously. The core innovation is a cross-attention feature
  extractor that allows speech and text to interact efficiently, combined with a read-write
  policy formulation that unifies offline and streaming modes.
---

# BESTOW: Efficient and Streamable Speech Language Model with the Best of Two Worlds in GPT and T5

## Quick Facts
- arXiv ID: 2406.19954
- Source URL: https://arxiv.org/abs/2406.19954
- Authors: Zhehuai Chen; He Huang; Oleksii Hrinchuk; Krishna C. Puvvada; Nithin Rao Koluguri; Piotr Żelasko; Jagadeesh Balam; Boris Ginsburg
- Reference count: 0
- Primary result: Introduces BESTOW, a SpeechLLM achieving state-of-the-art performance on multiple speech tasks while enabling streaming with tunable latency-quality tradeoffs

## Executive Summary
This paper introduces BESTOW, a SpeechLLM architecture that enables both streaming and multitask capabilities at scale, which previous SpeechLLM approaches could not achieve simultaneously. The core innovation is a cross-attention feature extractor that allows speech and text to interact efficiently, combined with a read-write policy formulation that unifies offline and streaming modes. BESTOW achieves state-of-the-art performance on multiple speech tasks including ASR (3.2 WER on Librispeech), AST (BLEU scores up to 41.9), SQA (59.8 ROUGE-L), and DynamicSuperb (accuracy 59.8), while supporting streaming with tunable latency-quality tradeoffs. The architecture is more computationally efficient than GPT-style SpeechLLMs and requires lower training/inference cost.

## Method Summary
BESTOW combines a speech encoder, cross-attention feature extractor, LLM backbone, and text decoder into a unified architecture. The key innovation is the cross-attention feature extractor that uses speech features as keys/values and text prompts as queries, reducing computational complexity. A read-write policy formulation enables streaming by decoupling READ (waiting for more speech) and WRITE (predicting next token) operations. The architecture uses two layers of transformer-like self-attention and cross-attention before the LLM input, with residual connections to preserve original LLM capabilities.

## Key Results
- Achieves 3.2 WER on Librispeech, setting new state-of-the-art for streaming ASR
- Reaches BLEU scores up to 41.9 on AST tasks, demonstrating strong translation capabilities
- Achieves 59.8 ROUGE-L on SQA and 59.8 accuracy on DynamicSuperb, showing multitask competence
- Provides tunable latency-quality tradeoffs for streaming applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention feature extractor enables efficient speech-text interaction with lower computational complexity than GPT-style models
- Mechanism: Speech features serve as keys/values while text prompts serve as queries in cross-attention layers, reducing complexity from O(Lt+La)^2 to O(Lt*La + Lt^2) where Lt is text length and La is speech length
- Core assumption: Cross-attention can effectively extract task-relevant features from speech using text queries
- Evidence anchors:
  - [abstract]: "We propose BESTOW architecture to bring the BESt features from TwO Worlds into a single model that is highly efficient"
  - [section 4.4]: "With our cross-attention between text and speech prompts, we are able to reduce the complexity to LtLa + L2t"
  - [corpus]: Weak evidence - no corpus papers directly test this computational complexity claim
- Break condition: If cross-attention cannot extract meaningful features, performance degrades significantly

### Mechanism 2
- Claim: Read-write policy formulation enables unified offline and streaming modes with LLM knowledge transfer
- Mechanism: Speech encoder provides streaming features, cross-attention module decides whether to READ (wait for more speech) or WRITE (predict next token) at each LLM step
- Core assumption: Decoupling READ/WRITE operations from LLM backbone allows streaming while preserving pretrained LLM capabilities
- Evidence anchors:
  - [abstract]: "We reformulate streamable SpeechLLM as a read-write policy problem and unifies the offline and streaming research"
  - [section 4.1]: "We propose to formulate the streaming problem of SpeechLLM as the read-write policy problem previously defined in simultaneous speech translation"
  - [corpus]: Weak evidence - corpus contains papers on streaming but not using this specific read-write policy approach
- Break condition: If policy decisions are poor, streaming quality suffers with either too much latency or insufficient context

### Mechanism 3
- Claim: Two-layer cross-attention architecture achieves state-of-the-art performance with lower training/inference cost
- Mechanism: Two layers of transformer-like self-attention and cross-attention before LLM input, with residual connection to preserve original LLM capabilities
- Core assumption: Two layers are sufficient to capture necessary speech-text interactions without excessive parameters
- Evidence anchors:
  - [section 4.2]: "We empirically found in the ablation study section that X = 2 is sufficient"
  - [section 5.3]: "We show with only two layers of cross-attention before feeding into LLMs, we can achieve state-of-the-art performance"
  - [corpus]: No direct corpus evidence comparing two-layer vs deeper architectures
- Break condition: If task complexity increases, two layers may become insufficient

## Foundational Learning

- Concept: Transformer self-attention and cross-attention mechanisms
  - Why needed here: Core to understanding how speech and text interact in BESTOW architecture
  - Quick check question: How does cross-attention differ from self-attention in terms of query, key, and value sources?

- Concept: Speech feature extraction and temporal resolution
  - Why needed here: Understanding how speech is encoded and processed before being fed to cross-attention layers
  - Quick check question: What is the relationship between speech frame rate and computational complexity in cross-attention?

- Concept: Read-write policy formulation for streaming
  - Why needed here: Key to understanding how BESTOW achieves streaming capability
  - Quick check question: How does the read-write policy balance between latency and quality in streaming applications?

## Architecture Onboarding

- Component map: Speech encoder → Cross-attention feature extractor (2 layers) → LLM backbone → Text decoder
- Critical path: Speech features must be processed through cross-attention before reaching LLM for each prediction step
- Design tradeoffs: Two-layer cross-attention vs deeper architecture (efficiency vs potential performance), bidirectional vs unidirectional encoder (accuracy vs true streaming)
- Failure signatures: Poor streaming quality indicates read-write policy issues; degraded accuracy suggests cross-attention layer problems
- First 3 experiments:
  1. Compare BESTOW with SALM on Librispeech to verify architectural efficiency claims
  2. Test different numbers of cross-attention layers (1, 2, 3) on ASR task to validate "2 is sufficient" claim
  3. Implement streaming inference with different K values to verify latency-quality tradeoff curves

## Open Questions the Paper Calls Out
None

## Limitations

**Computational Complexity Claims**: While the paper claims O(Lt*La + Lt^2) complexity reduction compared to O((Lt+La)^2), this analysis is theoretical and not empirically validated across different sequence lengths. The practical impact depends heavily on actual implementation details and hardware characteristics.

**Cross-attention Layer Sufficiency**: The claim that two cross-attention layers are "sufficient" is based on ablation studies within this specific implementation but lacks comparison with deeper architectures on more diverse tasks. Complex speech tasks might require more sophisticated feature extraction.

**Read-write Policy Optimization**: The policy mechanism that decides between READ and WRITE operations in streaming mode is not fully explained in terms of how it's trained or optimized. The quality of these decisions directly impacts the latency-quality tradeoff curve.

## Confidence

**High Confidence**: State-of-the-art performance metrics on established benchmarks (Librispeech, SQA, DynamicSuperb) - these are directly measurable and comparable to existing work.

**Medium Confidence**: Computational efficiency claims - while theoretically sound, practical implementation details could affect actual performance gains.

**Low Confidence**: Claims about generalizability to new tasks beyond those tested, and the sufficiency of the two-layer architecture for more complex speech understanding tasks.

## Next Checks

1. **Complexity Validation**: Measure actual runtime and memory usage across varying sequence lengths (both speech and text) to verify the claimed computational complexity reduction holds in practice, not just theory.

2. **Architecture Scaling**: Test BESTOW with 1, 2, 3, and 4 cross-attention layers on a challenging speech task (e.g., speech summarization or dialogue understanding) to determine if the "two layers are sufficient" claim generalizes beyond ASR and translation tasks.

3. **Streaming Robustness**: Evaluate streaming performance on spontaneous speech with disfluencies and background noise to test whether the read-write policy maintains quality under realistic conditions where speech patterns deviate from clean, prepared speech.