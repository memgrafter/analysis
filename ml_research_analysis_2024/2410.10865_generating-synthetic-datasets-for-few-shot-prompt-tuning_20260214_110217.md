---
ver: rpa2
title: Generating Synthetic Datasets for Few-shot Prompt Tuning
arxiv_id: '2410.10865'
source_url: https://arxiv.org/abs/2410.10865
tags:
- data
- real
- prompt
- synthetic
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the data-efficiency problem in prompt tuning
  by generating task-specific synthetic datasets using LLMs. It introduces DawGen,
  which adapts a generator LLM to produce high-quality synthetic samples aligned with
  real few-shot data, and combines these with real data using gradient surgery to
  avoid conflicts.
---

# Generating Synthetic Datasets for Few-shot Prompt Tuning

## Quick Facts
- arXiv ID: 2410.10865
- Source URL: https://arxiv.org/abs/2410.10865
- Authors: Xu Guo; Zilin Du; Boyang Li; Chunyan Miao
- Reference count: 28
- One-line primary result: Synthetic data generation with DawGen and gradient surgery improves few-shot prompt tuning accuracy by ~18% on sentence-pair classification tasks.

## Executive Summary
This paper addresses the data-efficiency challenge in prompt tuning by introducing DawGen, a framework that generates high-quality synthetic datasets using a generator LLM adapted to match the target task domain. The approach combines distribution-aligned generator tuning with gradient surgery to train soft prompts on both real and synthetic data while avoiding gradient conflicts. Evaluated across seven sentence-pair classification tasks, the method achieves significant improvements over standard prompt tuning and matches or exceeds full fine-tuning and transfer learning baselines on multiple datasets.

## Method Summary
DawGen trains a generator LLM to produce task-specific synthetic data by adapting it with label-conditional soft prompts on few-shot real data, using distribution alignment to ensure generated samples match the target domain. The synthetic dataset is then used alongside real data to train soft prompts on the target model, with gradient surgery employed to eliminate conflicting gradients between the two data sources. The framework uses T5-large or Flan-T5-large as the target model and CTRL as the generator, with evaluation on seven sentence-pair classification tasks under few-shot learning settings.

## Key Results
- DawGen improves average accuracy by ~18% over naive prompt tuning across seven sentence-pair classification tasks
- Achieves comparable or better performance than full fine-tuning and transfer learning baselines on QQP, MRPC, and SICK
- Flan-T5-large outperforms T5-large significantly, confirming instruction-tuned models as better few-shot learners for prompt tuning
- Gradient surgery effectively resolves conflicts between real and synthetic data, enabling improved learning from both sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient surgery removes conflicting gradients from synthetic data, allowing soft prompts to learn from both real and synthetic sources without interference.
- Mechanism: When computing gradients on paired batches from real and synthetic data, if the synthetic gradient opposes the real gradient (dot product < 0), the projection of the synthetic gradient onto the real gradient direction is subtracted, effectively removing the conflicting component.
- Core assumption: Synthetic data contains useful signal but also conflicting directions that harm learning when naively combined with few-shot real data.
- Evidence anchors:
  - [abstract] "Then, we train soft prompts on both synthetic and real datasets using a gradient surgery approach, which eliminates the conflicting gradients from different data sources."
  - [section 3.3] "To better exploit the synthetic data while making the best use of few-shot real data, we employ gradient surgery (Yu et al., 2020) to modify the gradients computed on the synthetic data by subtracting the components that oppose the direction of gradients from the real data."
- Break condition: If synthetic and real data are already well-aligned or if the synthetic data is too noisy, gradient surgery might remove useful learning signals.

### Mechanism 2
- Claim: Distribution-Aligned Weighted Generator Tuning (DawGen) produces synthetic data that better matches the task domain by encouraging label-relevant text generation.
- Mechanism: DawGen trains soft prompts for the generator LLM using a distribution constraint that pulls generated samples from the same class closer together while pushing different classes apart, and uses weighted generation to emphasize label-discriminative tokens.
- Core assumption: Without domain adaptation, LLMs tend to generate texts following their pretraining distribution, which may differ from the target task domain.
- Evidence anchors:
  - [abstract] "We first introduce a distribution-aligned weighted generator tuning (DawGen) method to encourage generating in-distribution data that aligns with the few-shot real data."
  - [section 3.2] "To encourage the data generator network to generate label-discriminative texts, FewGen (Meng et al., 2023) propose to train a weight net... We propose to regularize the generator tuning objective by adding a sentence-level distribution constraint to encourage the generated sentence to align with the in-distribution data."
- Break condition: If the few-shot real data is too limited or unrepresentative, the distribution alignment may reinforce biases or fail to capture task complexity.

### Mechanism 3
- Claim: Instruction-tuned models (Flan-T5-large) serve as better few-shot learners for soft prompt tuning than standard T5 models.
- Mechanism: Instruction tuning improves the model's ability to follow task instructions encoded in soft prompts, making them more responsive to prompt-based learning with limited data.
- Core assumption: Standard T5 models lack the instruction-following capability that makes prompt tuning effective in few-shot settings.
- Evidence anchors:
  - [section 4.6] "Across the tables, Flan-T5-large excels T5-large on all experiments by a large margin... instruction-tuned models can follow soft prompts better... Prompt Tuning on top of instruction-tuned models are less prone to overfitting the few-shot data."
- Break condition: If the task instructions are poorly designed or the model is already well-tuned for the task, the instruction tuning advantage may diminish.

## Foundational Learning

- Concept: Gradient computation and projection in multi-task learning
  - Why needed here: Understanding how gradient surgery works requires knowledge of vector operations on gradients and how conflicting gradients can be identified and resolved.
  - Quick check question: What condition indicates that two tasks have interfering gradients in gradient surgery?

- Concept: Domain adaptation and distribution alignment
  - Why needed here: DawGen's effectiveness relies on adapting a general-purpose LLM to generate domain-specific data that matches the target task distribution.
  - Quick check question: How does the distribution alignment term in DawGen encourage in-distribution generation?

- Concept: Soft prompt learning and parameter-efficient fine-tuning
  - Why needed here: The entire approach builds on prompt tuning as a parameter-efficient alternative to full fine-tuning, requiring understanding of how soft prompts are learned and used.
  - Quick check question: What distinguishes prompt tuning from full-model fine-tuning in terms of parameter updates?

## Architecture Onboarding

- Component map:
  - Generator LLM (gϕ) with prefix tuning (Q) → produces synthetic data
  - Target LLM (fθ) with soft prompts (P) → performs classification
  - Real data (Dreal) and synthetic data (Dsyn) → training sources
  - Gradient surgery module → resolves conflicts between real and synthetic gradients

- Critical path:
  1. Adapt generator using few-shot real data via DawGen
  2. Generate synthetic dataset using adapted generator
  3. Train soft prompts on both real and synthetic data using gradient surgery
  4. Deploy soft prompts with target LLM for inference

- Design tradeoffs:
  - Generator quality vs. generation cost: More adaptation steps improve synthetic data quality but increase computation time
  - Synthetic data volume vs. quality: Larger synthetic datasets provide more examples but may include more noise
  - Gradient surgery strength vs. learning signal: Stronger gradient surgery prevents interference but may remove useful learning signals

- Failure signatures:
  - Poor performance despite large synthetic datasets: May indicate generator misalignment or ineffective gradient surgery
  - Overfitting to few-shot real data: May suggest insufficient synthetic data regularization or inappropriate gradient surgery strength
  - Inconsistent results across runs: May indicate sensitivity to synthetic data quality or gradient surgery implementation

- First 3 experiments:
  1. Run DawGen generator adaptation on a small subset of few-shot data and inspect generated samples for label relevance and domain alignment
  2. Test gradient surgery with synthetic data alone (no real data) to verify it can learn from synthetic sources
  3. Compare soft prompt performance with and without gradient surgery on a single dataset to measure interference effects directly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of synthetic data generated by DawGen compare to human-labeled data across diverse domains?
- Basis in paper: [inferred] The paper suggests that DawGen's synthetic data can match or surpass full fine-tuning and transfer learning baselines, but does not directly compare the quality of synthetic data to human-labeled data.
- Why unresolved: The paper focuses on comparing the performance of prompt tuning with synthetic data to other methods, but does not explicitly compare the quality of the synthetic data itself to human-labeled data.
- What evidence would resolve it: A direct comparison of the quality of DawGen-generated synthetic data to human-labeled data across a variety of domains and tasks would be needed to answer this question.

### Open Question 2
- Question: What is the impact of the order in which real and synthetic data are presented during training on the performance of prompt tuning?
- Basis in paper: [explicit] The paper discusses the importance of the order in which real and synthetic data are presented, noting that T5-large prefers "Real → Syn" while Flan-T5-large prefers "Syn → Real".
- Why unresolved: While the paper identifies the impact of data order on different model backbones, it does not provide a comprehensive explanation for why this difference exists or how to determine the optimal order for a given task and model.
- What evidence would resolve it: Further research investigating the reasons behind the observed differences in data order preferences across model backbones and tasks would be needed to answer this question.

### Open Question 3
- Question: How does the computational cost of DawGen compare to other methods for generating synthetic data for prompt tuning?
- Basis in paper: [inferred] The paper mentions that synthetic data generation cost can be a concern due to the large size of the language models used, but does not provide a direct comparison of the computational cost of DawGen to other methods.
- Why unresolved: The paper does not provide a detailed analysis of the computational cost of DawGen compared to other synthetic data generation methods, making it difficult to assess its efficiency.
- What evidence would resolve it: A comprehensive comparison of the computational cost of DawGen to other synthetic data generation methods, considering factors such as model size, generation time, and memory usage, would be needed to answer this question.

## Limitations

- Evaluation scope limited to seven sentence-pair classification tasks with similar data formats, which may not generalize to other task types or data modalities
- Few-shot regime uses only 16 samples per class, potentially limiting applicability to tasks requiring more diverse examples
- Generator adaptation implementation details underspecified, particularly regarding distribution alignment term and weight net architecture
- Gradient surgery mechanism assumes conflicts exist but lacks analysis of gradient similarity distributions or when this assumption breaks down

## Confidence

**High confidence** in the core observation that synthetic data improves few-shot prompt tuning performance, as this is directly measured across multiple datasets with statistical validation (five random seeds).

**Medium confidence** in the DawGen mechanism's effectiveness, since the paper describes the approach but lacks ablation studies showing the specific contribution of the distribution alignment term versus standard FewGen.

**Medium confidence** in gradient surgery's necessity, as the paper demonstrates improved performance but doesn't quantify how much of the gain comes from removing conflicting gradients versus simply increasing effective training data.

## Next Checks

1. **Gradient analysis experiment**: Compute and visualize the dot product distribution between real and synthetic gradients during training to verify that conflicting gradients (negative dot products) are common and that gradient surgery is effectively removing them. This would validate the core assumption of the gradient surgery mechanism.

2. **Synthetic data quality audit**: Generate a large sample of synthetic data for each task and have human annotators rate the relevance, diversity, and quality of the synthetic pairs. Correlate these quality scores with downstream performance to determine if generator alignment is the limiting factor.

3. **Break condition testing**: Systematically vary the amount of real data (e.g., test with 8, 32, 64 samples per class) and synthetic data (e.g., test with 2x, 4x, 8x the real data) to identify when the synthetic data approach stops providing benefits or when gradient surgery becomes unnecessary.