---
ver: rpa2
title: 'S$^3$Attention: Improving Long Sequence Attention with Smoothed Skeleton Sketching'
arxiv_id: '2408.08567'
source_url: https://arxiv.org/abs/2408.08567
tags:
- attention
- self
- matrix
- information
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces S3Attention, a new attention mechanism designed
  for efficient long sequence processing. The core innovation is a smoothed skeleton
  sketching approach that combines two mechanisms: a smoothing block to reduce noise
  and improve token representativeness via Fourier convolution, and a matrix sketching
  technique for efficient row and column selection.'
---

# S$^3$Attention: Improving Long Sequence Attention with Smoothed Skeleton Sketching

## Quick Facts
- arXiv ID: 2408.08567
- Source URL: https://arxiv.org/abs/2408.08567
- Authors: Xue Wang; Tian Zhou; Jianqing Zhu; Jialin Liu; Kun Yuan; Tao Yao; Wotao Yin; Rong Jin; HanQin Cai
- Reference count: 40
- One-line primary result: S3Attention achieves 66.08% average accuracy on Long Range Arena, outperforming vanilla attention (62.03%) and state-of-the-art variants

## Executive Summary
S3Attention introduces a novel attention mechanism for efficient long sequence processing by combining a smoothing block with matrix sketching. The method uses Fourier convolution to reduce noise and improve token representativeness, followed by skeleton sketching that simultaneously selects rows and columns for efficient approximation. This approach achieves linear computational complexity while preserving global information, demonstrated through extensive experiments on Long Range Arena benchmarks, time-series forecasting tasks, and GLUE transfer learning.

## Method Summary
S3Attention addresses the quadratic complexity of standard attention by introducing a two-stage mechanism: a smoother that applies Fourier convolution to mix information over long sequences, and a skeleton sketching method that approximates the input matrix by selecting a smaller number of rows and columns. The smoother uses learnable Fourier convolution to reduce the incoherence parameter, making the token matrix more uniform and requiring fewer samples for accurate approximation. The skeleton attention then performs row and column attention in parallel, with outputs combined to preserve global information while achieving linear computational complexity.

## Key Results
- Achieves 66.08% average accuracy on Long Range Arena benchmarks, outperforming vanilla attention (62.03%) and state-of-the-art variants
- Demonstrates comparable performance to leading long-term forecasting models on six time-series benchmarks
- Shows strong transfer learning ability with consistent improvements across GLUE tasks
- Maintains linear computational complexity while preserving global information over long sequences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fourier convolution smooths the input token matrix, reducing the incoherence parameter and improving sampling efficiency.
- Mechanism: The Fourier convolution layer applies a learnable filter in the frequency domain that smooths the input sequence by reducing differences between adjacent elements. This makes the token matrix more uniform, allowing fewer landmark rows/columns to capture the same amount of information.
- Core assumption: The token matrix has high variance between adjacent elements, and smoothing preserves global information while reducing local noise.
- Evidence anchors:
  - [abstract] - "a smoothing block to mix information over long sequences and a matrix sketching method that simultaneously selects columns and rows from the input matrix"
  - [section] - "Decreasing in μ leads to a smaller sampling size. Furthermore, the μ-incoherence condition implies that the 'energy' of the matrix is evenly distributed over its entries, i.e., the matrix is 'smooth'"
  - [corpus] - Weak evidence; no direct corpus support for Fourier smoothing effect on incoherence parameters
- Break condition: If the learnable matrix L becomes too aggressive and over-smooths, eliminating important local features and reducing model expressiveness.

### Mechanism 2
- Claim: Skeleton sketching with simultaneous row and column selection provides better matrix approximation than column-only sampling.
- Mechanism: By randomly sampling both rows and columns from the smoothed token matrix, the method captures the matrix's structure more efficiently than standard attention (which only uses column selection). The row attention provides a complementary view of the information.
- Core assumption: The input matrix has structure that can be approximated with fewer samples when using both dimensions simultaneously.
- Evidence anchors:
  - [abstract] - "a matrix sketching technique to approximate the input matrix by a smaller number of rows and columns"
  - [section] - "From the view of matrix sketching, solely using either row or column selection could reduce the cost-effectiveness, compared to using both column and row selections"
  - [corpus] - Weak evidence; limited corpus support for dual-dimension sampling superiority in attention mechanisms
- Break condition: If the sampled rows and columns don't capture the essential structure of the matrix, leading to information loss.

### Mechanism 3
- Claim: The combination of smoothing and skeleton sketching achieves linear computational complexity while maintaining or improving accuracy.
- Mechanism: Smoothing reduces the number of samples needed by making the matrix more uniform, while skeleton sketching ensures efficient representation through dual-dimension sampling. This combination avoids the quadratic complexity of standard attention.
- Core assumption: The computational savings from reduced sampling size and dual-dimension selection outweigh any potential accuracy loss from approximation.
- Evidence anchors:
  - [abstract] - "linear computational complexity while preserving global information"
  - [section] - "By combining these mechanisms, we found, both theoretically and empirically, that S3Attention is able to preserve global information over long sequences and reduce the impact of noise simultaneously"
  - [corpus] - Weak evidence; no direct corpus support for the combined mechanism's complexity-accuracy tradeoff
- Break condition: If the approximation error from skeleton sketching becomes too large relative to the computational savings.

## Foundational Learning

- Concept: Fourier Transformation
  - Why needed here: The Fourier convolution uses Fast Fourier Transformation to efficiently apply smoothing in the frequency domain, reducing computational complexity from O(n²) to O(n log n).
  - Quick check question: How does the convolution theorem enable efficient smoothing operations in the frequency domain?

- Concept: Matrix Sketching
  - Why needed here: Skeleton sketching approximates large matrices with smaller representative subsets, enabling linear complexity attention mechanisms while preserving essential information.
  - Quick check question: What is the difference between column-only sampling and dual-dimension (row+column) sampling in matrix approximation?

- Concept: Incoherence Parameter
  - Why needed here: The incoherence parameter μ measures how uniformly a matrix's energy is distributed, determining how many samples are needed for accurate approximation.
  - Quick check question: How does the incoherence parameter affect the number of rows/columns needed for skeleton sketching?

## Architecture Onboarding

- Component map: Input → Smoother (Fourier Convolution + Convolution Stem) → Skeleton Attention (Row Attention + Column Attention) → Output
- Critical path: The smoother component must be applied before skeleton attention to reduce incoherence; the row and column attention blocks operate in parallel and their outputs are combined.
- Design tradeoffs: Smoothing reduces sampling needs but may lose local detail; row attention adds computational overhead but provides complementary information; random sampling introduces variance but ensures simplicity.
- Failure signatures: Poor performance on tasks requiring fine-grained local detail (over-smoothing); accuracy degradation when sequence length exceeds effective sampling capacity; instability during early training phases.
- First 3 experiments:
  1. Test smoothing effect: Compare incoherence parameters with and without Fourier convolution on a small dataset.
  2. Test sampling efficiency: Vary the number of sampled rows/columns and measure accuracy vs. computational cost.
  3. Test component ablation: Remove each component (Fourier convolution, convolution stem, row attention, column attention) and measure impact on LRA benchmark tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Fourier convolution matrix L behave in later stages of training, and does its learned form still provide the smoothness benefits observed in the early training stage?
- Basis in paper: [explicit] The paper discusses the behavior of L during early training, noting that it starts as sub-Gaussian with variance O(n⁻¹) and converges to a data-driven form. It mentions that while the variance may not remain very low, good memorization ability can occur.
- Why unresolved: The paper does not provide empirical data or theoretical analysis on the behavior of L in later training stages, nor does it verify whether the smoothness benefits persist throughout training.
- What evidence would resolve it: Experiments tracking the distribution of L's elements and the incoherence parameter μ throughout training, along with performance comparisons between early and late-stage training, would provide clarity.

### Open Question 2
- Question: Can the computational efficiency of S³Attention be further improved by replacing the FFT/IFFT operations with random Fourier features, and what would be the trade-off in terms of accuracy?
- Basis in paper: [inferred] The paper mentions that one limitation is the need to use both FFT and IFFT sequentially, which could be slower than existing Fourier-based attention architectures that only involve FFT. It suggests that random Fourier transformation could be used to modify S³Attention with only FFT.
- Why unresolved: The paper does not explore or implement this potential modification, nor does it provide any analysis of the trade-offs involved.
- What evidence would resolve it: Implementing S³Attention with random Fourier features, benchmarking its speed against the current version, and comparing the accuracy on standard datasets would provide insights into the trade-offs.

### Open Question 3
- Question: How does S³Attention's performance scale with extremely long sequences (e.g., millions of tokens), and what are the bottlenecks in such scenarios?
- Basis in paper: [explicit] The paper focuses on improving attention for long sequences but does not explicitly test or analyze performance on extremely long sequences. It mentions the linear complexity but doesn't discuss practical limitations at scale.
- Why unresolved: The experiments and analysis in the paper are conducted on datasets with sequence lengths up to a few thousand tokens, not on extremely long sequences.
- What evidence would resolve it: Testing S³Attention on datasets with sequence lengths in the millions, profiling memory usage and computational time, and identifying specific bottlenecks would clarify its scalability limitations.

### Open Question 4
- Question: How does the optimal number of sampled rows and columns (s₁ and s₂) in Skeleton Attention vary with different types of data distributions, and can this be learned adaptively rather than set as a fixed hyperparameter?
- Basis in paper: [explicit] The paper discusses the impact of varying s₁ and s₂ on performance across different tasks, showing that the optimal values differ among tasks. It mentions that Pathfinder requires more samples due to long-range dependencies, while other tasks need fewer.
- Why unresolved: The paper does not explore adaptive methods for determining s₁ and s₂ based on data characteristics, nor does it provide a theoretical framework for predicting optimal values based on data distribution.
- What evidence would resolve it: Developing an adaptive method to estimate s₁ and s₂ based on data statistics, testing it across diverse datasets, and comparing performance to fixed hyperparameter settings would address this question.

## Limitations
- Limited empirical validation for core theoretical claims about Fourier convolution reducing incoherence parameters
- Matrix sketching component lacks ablation studies showing specific contribution of row selection versus column selection alone
- Time-series experiments conducted on standard benchmarks without comparison to specialized time-series architectures

## Confidence
- **High Confidence**: Experimental results showing S3Attention outperforming vanilla attention and achieving competitive performance with state-of-the-art methods on LRA benchmarks (66.08% average accuracy) and GLUE transfer learning tasks
- **Medium Confidence**: Claim that S3Attention achieves linear computational complexity while preserving global information (theoretical framework supports this but lacks detailed complexity analysis)
- **Low Confidence**: Specific mechanisms by which Fourier convolution reduces incoherence parameters and how skeleton sketching with dual-dimension sampling provides superior approximation compared to existing methods (claims supported by theoretical references but lack direct empirical validation)

## Next Checks
1. **Incoherence Parameter Validation**: Conduct an experiment measuring the incoherence parameter μ of token matrices before and after Fourier convolution across different sequence types (text, images, time series) to directly verify the claim that smoothing reduces incoherence and enables more efficient sampling.

2. **Ablation Study for Skeleton Sketching**: Perform systematic ablation experiments comparing S3Attention with variants that use only column selection, only row selection, and random sampling without smoothing to quantify the specific contribution of each component to overall performance.

3. **Computational Complexity Analysis**: Implement timing benchmarks measuring actual wall-clock time and memory usage of S3Attention versus vanilla attention and other efficient attention mechanisms across varying sequence lengths (from 1K to 16K tokens) to validate the claimed linear complexity advantage.