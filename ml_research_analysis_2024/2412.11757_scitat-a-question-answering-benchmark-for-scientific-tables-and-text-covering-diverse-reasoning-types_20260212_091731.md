---
ver: rpa2
title: 'SCITAT: A Question Answering Benchmark for Scientific Tables and Text Covering
  Diverse Reasoning Types'
arxiv_id: '2412.11757'
source_url: https://arxiv.org/abs/2412.11757
tags:
- reasoning
- tables
- types
- text
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCITAT, a new benchmark for scientific question
  answering that requires reasoning over both tables and text with diverse reasoning
  types. The dataset includes 953 questions derived from 871 scientific papers, covering
  four reasoning types (Look Up, Numerical Reasoning, Data Analysis, and Tabulation)
  and 13 subtypes.
---

# SCITAT: A Question Answering Benchmark for Scientific Tables and Text Covering Diverse Reasoning Types

## Quick Facts
- arXiv ID: 2412.11757
- Source URL: https://arxiv.org/abs/2412.11757
- Authors: Xuanliang Zhang; Dingzirui Wang; Baoxin Wang; Longxu Dou; Xinyuan Lu; Keyan Xu; Dayong Wu; Qingfu Zhu; Wanxiang Che
- Reference count: 22
- Primary result: Introduces SCITAT benchmark with 953 questions from 871 scientific papers, evaluating CAR baseline that combines Calculator and Reasoner modules

## Executive Summary
This paper introduces SCITAT, a new benchmark for scientific question answering that requires reasoning over both tables and text with diverse reasoning types. The dataset includes 953 questions derived from 871 scientific papers, covering four reasoning types (Look Up, Numerical Reasoning, Data Analysis, and Tabulation) and 13 subtypes. To address the challenges of SCITAT, the authors propose CAR, a strong baseline that combines a Calculator module for numerical extraction and a Reasoner module for complex reasoning. CAR significantly outperforms other baselines with an average improvement of 12.9% across different models and settings. Despite this improvement, CAR using gpt-4o still achieves less than 50% Exact Match, indicating SCITAT's challenging nature. Error analysis reveals key difficulties including context grounding, complex numerical calculations, and domain knowledge requirements.

## Method Summary
The SCITAT benchmark is constructed from 871 scientific papers in AI, Computation and Language, and Machine Learning subfields from arXiv.org, yielding 953 questions covering four reasoning types and 13 subtypes. The proposed CAR baseline combines a Calculator module that extracts numerical information from tables and text, and a Reasoner module that performs complex reasoning based on this information. CAR is compared against Direct QA, Chain-of-Thought (CoT), and Program-of-Thought (PoT) baselines using Llama3.1-8B/70B and gpt-4o models. The evaluation uses Exact Match for short-form answers and F1/BERTScore for free-form answers, with experiments conducted in both long-context (32k tokens) and short-context (4k tokens) settings.

## Key Results
- CAR baseline significantly outperforms Direct QA, CoT, and PoT baselines with an average improvement of 12.9% across different models and settings
- CAR using gpt-4o achieves less than 50% Exact Match, demonstrating SCITAT's challenging nature for current models
- CAR shows larger performance gaps in short-context settings, highlighting the importance of long-context models for this task
- Error analysis identifies context grounding, complex numerical calculations, and domain knowledge requirements as key difficulty factors

## Why This Works (Mechanism)
SCITAT works by creating a benchmark that requires integrated reasoning over both tables and text, which more closely reflects real scientific literature analysis than previous datasets that focused on only one modality. The CAR baseline succeeds because it explicitly separates numerical extraction (Calculator) from reasoning (Reasoner), allowing each component to specialize in its respective task. This modular approach enables better handling of complex numerical calculations and scientific reasoning that requires both quantitative and qualitative analysis.

## Foundational Learning
1. **Exact Match (EM) metric** - Used to evaluate short-form answers by checking if generated answers exactly match gold answers. Needed to provide precise evaluation for factual questions. Quick check: Compare EM scores between models on validation set.

2. **Chain-of-Thought (CoT) prompting** - A reasoning technique where models are prompted to show intermediate reasoning steps before answering. Needed to improve model reasoning capabilities on complex tasks. Quick check: Evaluate CoT performance vs Direct QA on reasoning-intensive questions.

3. **Program-of-Thought (PoT) prompting** - An extension of CoT where models generate executable programs for reasoning. Needed for structured numerical reasoning and calculations. Quick check: Compare PoT program execution accuracy on numerical reasoning questions.

## Architecture Onboarding
**Component Map:** Scientific Papers -> Data Extraction -> Question Generation -> CAR (Calculator -> Reasoner) -> Answer Generation

**Critical Path:** Input context (table+text) → Calculator extracts numerical data → Reasoner performs reasoning → Final answer generation

**Design Tradeoffs:** The modular CAR design trades potential end-to-end optimization for better handling of specialized tasks (numerical extraction vs reasoning). This separation allows more targeted improvements but may miss synergies from joint optimization.

**Failure Signatures:** Direct QA baseline generates overly long responses for short-form answers; PoT baseline struggles with free-form answers due to program limitations; CAR may fail on questions requiring extensive domain knowledge not captured in the context.

**First Experiments:** 1) Evaluate CAR components individually to assess their separate contributions, 2) Test CAR on out-of-domain scientific papers to measure generalizability, 3) Compare CAR performance across different reasoning subtypes to identify specific weaknesses.

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset focuses primarily on AI-related scientific literature, potentially limiting generalizability to other scientific domains
- The dataset size (953 questions) may be insufficient for training large models exclusively on this task
- Automated metrics for free-form answers may not fully capture answer quality, missing nuances in scientific reasoning and explanation quality

## Confidence
- High confidence in CAR's effectiveness based on systematic evaluation showing consistent improvements across multiple models and settings
- Medium-High confidence in SCITAT's challenging nature, supported by gpt-4o achieving less than 50% EM, though specific difficulty breakdown could be more granular
- Medium confidence in error analysis findings, though additional human evaluation would strengthen these conclusions

## Next Checks
1. Conduct ablation studies to isolate the contribution of Calculator versus Reasoner components within CAR
2. Test CAR on out-of-domain scientific papers to evaluate generalizability beyond AI, CL, and ML subfields
3. Implement human evaluation to assess quality of free-form answers beyond automated metrics like F1 and BERTScore