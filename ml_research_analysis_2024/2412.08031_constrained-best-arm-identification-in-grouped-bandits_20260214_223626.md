---
ver: rpa2
title: Constrained Best Arm Identification in Grouped Bandits
arxiv_id: '2412.08031'
source_url: https://arxiv.org/abs/2412.08031
tags:
- feasible
- arms
- attr
- best
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies a constrained best arm identification problem
  in a grouped bandit setting, where each arm consists of multiple independent attributes
  and an arm is feasible only if all its attribute means exceed a threshold. The goal
  is to identify the feasible arm with the highest average reward.
---

# Constrained Best Arm Identification in Grouped Bandits

## Quick Facts
- arXiv ID: 2412.08031
- Source URL: https://arxiv.org/abs/2412.08031
- Reference count: 40
- One-line primary result: Near-optimal algorithm for identifying best feasible arm in grouped bandit setting with feasibility constraints

## Executive Summary
This paper studies the problem of constrained best arm identification in a grouped bandit setting, where each arm consists of multiple independent attributes and an arm is feasible only if all its attribute means exceed a threshold. The goal is to identify the feasible arm with the highest average reward. The authors propose CSS-LUCB, a confidence-bound based algorithm that adaptively samples arm-attribute pairs and terminates when the best feasible arm is identified with high confidence. Analytical guarantees show the algorithm is near-optimal up to logarithmic factors, and simulation results demonstrate its superiority over adapted action elimination baselines.

## Method Summary
The CSS-LUCB algorithm maintains confidence intervals for both arm means and individual attribute means. It initializes by uniformly sampling each attribute of each arm once, then iteratively samples the least explored attribute of each arm. The algorithm divides arm-attribute pairs into perfectly feasible, almost feasible, and infeasible sets based on confidence bounds, and samples strategically from these sets. It terminates when the feasible set and potential set are disjoint, indicating the best feasible arm is identified with high confidence. The algorithm's sample complexity is shown to be near-optimal with respect to a hardness index.

## Key Results
- Proposed CSS-LUCB algorithm achieves near-optimal sample complexity up to logarithmic factors
- Algorithm outperforms suitably adapted variants of action elimination in simulations
- Sample complexity scales with hardness index, which captures the intrinsic difficulty of the problem instance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The CSS-LUCB algorithm identifies the best feasible arm by maintaining confidence intervals for both arm means and individual attribute means, allowing adaptive exploration of arm-attribute pairs.
- Mechanism: The algorithm samples the least explored attribute of each arm initially, then iteratively pulls arm-attribute pairs that are in the "almost feasible" set or between the current best feasible arm and its most competitive rival.
- Core assumption: Attribute rewards are independent and bounded, allowing Hoeffding-style confidence bounds to be applied uniformly.
- Evidence anchors:
  - [abstract] states the goal is to find the feasible arm with the highest average reward among attributes, and the algorithm uses a confidence-bound based approach.
  - [section] defines confidence intervals for both individual attributes and arms, and explains how the algorithm updates sets based on these intervals.
  - [corpus] does not directly support this mechanism; the related papers focus on different variants of best arm identification without grouped attributes.
- Break condition: If attribute rewards are correlated or unbounded, the Hoeffding bounds used for confidence intervals may no longer hold, invalidating the algorithm's theoretical guarantees.

### Mechanism 2
- Claim: The algorithm terminates when the best feasible arm is confirmed with high confidence, by ensuring no competitor arms remain in the "potential set."
- Mechanism: At each round, the algorithm identifies the best arm from the perfectly feasible set and the most competitive arm from the feasible set. It stops when there are no arms in the feasible set that could potentially have a higher mean than the best feasible arm.
- Core assumption: The "potential set" correctly captures all arms that could potentially be better than the current best feasible arm, based on confidence bounds.
- Evidence anchors:
  - [section] defines the potential set as arms with upper confidence bound higher than the lower confidence bound of the empirically best arm, and explains the stopping condition when this set intersects the feasible set is empty.
  - [abstract] mentions the algorithm terminates when the best feasible arm is identified with high confidence.
  - [corpus] does not provide direct evidence for this specific stopping criterion mechanism.
- Break condition: If the confidence bounds are too loose or the sampling strategy fails to explore critical arm-attribute pairs, the potential set may incorrectly exclude or include arms, leading to premature termination or unnecessary exploration.

### Mechanism 3
- Claim: The algorithm achieves near-optimal sample complexity by focusing exploration on arms and attributes that are most informative for determining feasibility and optimality.
- Mechanism: By dividing arm-attribute pairs into perfectly feasible, almost feasible, and infeasible sets, and sampling strategically from these sets, the algorithm minimizes unnecessary exploration of clearly suboptimal or infeasible options.
- Core assumption: The hardness index (Hid) accurately captures the intrinsic difficulty of the problem instance, and the algorithm's sample complexity is proportional to this index.
- Evidence anchors:
  - [section] defines the hardness index based on the minimum gaps between arm means and the threshold, and between attribute means and the threshold, and states the algorithm's sample complexity is O(Hid ln(Hid/δ)).
  - [abstract] claims the algorithm is near-optimal with respect to the hardness index.
  - [corpus] does not directly address the hardness index or sample complexity in the context of grouped bandits with feasibility constraints.
- Break condition: If the hardness index does not accurately reflect the true difficulty of the problem, or if the algorithm's exploration strategy does not align well with the problem structure, the sample complexity may be higher than claimed.

## Foundational Learning

- Concept: Stochastic multi-armed bandits and best arm identification
  - Why needed here: The paper builds on classic bandit theory to solve a constrained best arm identification problem, so understanding the basic setting and algorithms is essential.
  - Quick check question: What is the difference between the fixed confidence and fixed budget settings in best arm identification?

- Concept: Confidence bounds and concentration inequalities
  - Why needed here: The algorithm relies on Hoeffding-style confidence intervals to determine when an arm or attribute is likely to be feasible or optimal, so familiarity with these tools is crucial.
  - Quick check question: How do Hoeffding bounds relate to the number of samples and the desired confidence level?

- Concept: Grouped or structured bandit problems
  - Why needed here: The paper considers a specific variant where each arm consists of multiple independent attributes, and feasibility depends on all attributes exceeding a threshold, so understanding this structure is key to following the algorithm and analysis.
  - Quick check question: How does the grouped structure affect the exploration-exploitation tradeoff compared to a standard bandit problem?

## Architecture Onboarding

- Component map: Initialization -> Confidence bound calculation -> Set updates -> Sampling strategy -> Termination check
- Critical path: The most critical sequence is confidence bound calculation -> set updates -> sampling strategy -> termination check, as errors in any of these steps can lead to incorrect identification or unnecessary exploration.
- Design tradeoffs: The algorithm trades off between exploring to reduce uncertainty about feasibility and optimality, and exploiting current knowledge to minimize samples. The choice of confidence radius and sampling strategy balances these competing goals.
- Failure signatures: If the algorithm consistently requires many more samples than predicted, it may indicate that the confidence bounds are too loose or the sampling strategy is not effectively targeting informative arm-attribute pairs. If the algorithm frequently misidentifies the best feasible arm, it may suggest issues with the confidence bound calculations or the set update logic.
- First 3 experiments:
  1. Run the algorithm on a simple instance with 2-3 arms and 2-3 attributes each, with known means and a moderate threshold, to verify basic functionality and compare to theoretical predictions.
  2. Vary the threshold and attribute means to create instances of different hardness levels, and measure how the sample complexity scales with the hardness index.
  3. Compare the algorithm's performance to a naive approach that samples uniformly from all arm-attribute pairs, to quantify the benefits of the adaptive sampling strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CSS-LUCB scale when the number of attributes M grows very large, especially in comparison to the action elimination baselines?
- Basis in paper: [explicit] The paper varies M from 2 to 4 in experiments and notes that sample complexity increases with M, but does not explore very large M.
- Why unresolved: Experiments only test small M; no theoretical analysis of scaling with large M.
- What evidence would resolve it: Numerical experiments with large M (e.g., M > 100) and/or theoretical bounds showing how sample complexity depends on M in the worst case.

### Open Question 2
- Question: Can the CSS-LUCB algorithm be adapted to handle non-independent attributes or correlated rewards within an arm?
- Basis in paper: [inferred] The paper assumes independence of attributes within an arm, which is a key modeling assumption for deriving the lower and upper bounds.
- Why unresolved: No discussion or analysis of correlated attribute settings; the algorithm relies on independence for confidence interval calculations.
- What evidence would resolve it: Extension of CSS-LUCB to a correlated attribute model and corresponding theoretical guarantees.

### Open Question 3
- Question: Is there a more efficient way to sample arm-attribute pairs in CSS-LUCB to reduce the constant factors in the sample complexity, beyond the current LUCB-style approach?
- Basis in paper: [explicit] The paper notes that CSS-LUCB is near-optimal up to logarithmic factors and outperforms adapted action elimination baselines, but the constants matter in practice.
- Why unresolved: The analysis focuses on asymptotic optimality, not on minimizing constants; no comparison of different sampling strategies.
- What evidence would resolve it: Empirical comparison of alternative sampling strategies (e.g., different prioritization rules) and/or tighter analysis reducing constants in the upper bound.

### Open Question 4
- Question: How robust is CSS-LUCB to misspecification of the confidence parameter δ or to reward distributions that deviate from the assumed bounded [0,1] support?
- Basis in paper: [inferred] The algorithm and analysis assume bounded rewards and a fixed δ; no discussion of robustness or adaptive confidence intervals.
- Why unresolved: No experiments or theory addressing robustness to parameter or distributional mismatches.
- What evidence would resolve it: Numerical experiments with rewards outside [0,1] or adaptive δ schemes, and/or theoretical analysis of robustness.

## Limitations

- The theoretical guarantees rely on independence of attribute rewards and boundedness within [0,1], which may not hold in all applications.
- While the hardness index provides a lower bound, the gap between this bound and the algorithm's sample complexity is not fully characterized.
- The practical performance on high-dimensional problems (large N and M) is not thoroughly explored.

## Confidence

- Mechanism 1 (confidence interval maintenance and adaptive sampling): High confidence - well-supported by theoretical analysis and experimental results
- Mechanism 2 (termination based on feasible vs potential set intersection): Medium confidence - the concept is clear but implementation details are sparse
- Mechanism 3 (near-optimal sample complexity via hardness index): Medium confidence - theoretical claims are made but empirical validation is limited

## Next Checks

1. Implement the CSS-LUCB algorithm and reproduce the core experimental results, varying the number of arms (N) and attributes (M) to assess scalability
2. Conduct ablation studies to quantify the contribution of each algorithmic component (confidence intervals, set updates, sampling strategy) to overall performance
3. Test the algorithm's robustness to attribute reward correlation by modifying the simulation environment and comparing results to the independent case