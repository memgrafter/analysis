---
ver: rpa2
title: Automated Filtering of Human Feedback Data for Aligning Text-to-Image Diffusion
  Models
arxiv_id: '2410.10166'
source_url: https://arxiv.org/abs/2410.10166
tags:
- text
- fifa
- data
- dataset
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FiFA, an automated data filtering algorithm
  for efficiently fine-tuning text-to-image diffusion models using human feedback.
  The core idea is to frame data selection as an optimization problem that maximizes
  three components: preference margin (calculated via a proxy reward model), text
  quality (assessed by an LLM to filter harmful content), and text diversity (measured
  using k-NN entropy estimation).'
---

# Automated Filtering of Human Feedback Data for Aligning Text-to-Image Diffusion Models

## Quick Facts
- arXiv ID: 2410.10166
- Source URL: https://arxiv.org/abs/2410.10166
- Authors: Yongjin Yang; Sihyeon Kim; Hojung Jung; Sangmin Bae; SangMook Kim; Se-Young Yun; Kimin Lee
- Reference count: 40
- Primary result: 17% higher human preference scores using <0.5% of full dataset and 1% GPU hours

## Executive Summary
This paper introduces FiFA, an automated data filtering algorithm that significantly improves the efficiency of fine-tuning text-to-image diffusion models using human feedback. The method frames data selection as an optimization problem that maximizes preference margin (via proxy reward model), text quality (via LLM assessment), and text diversity (via k-NN entropy estimation). FiFA achieves remarkable results, improving human preference scores by 17% while using less than 0.5% of the full dataset and only 1% of GPU hours, while also reducing harmful content generation by over 50%.

## Method Summary
FiFA optimizes data selection by combining three components: preference margin calculated using a proxy reward model to identify informative samples, text quality assessed by LLM to filter harmful content, and text diversity measured through k-NN entropy estimation. The algorithm selects the most important data pairs based on these criteria and uses the filtered dataset to fine-tune diffusion models via Diffusion-DPO. This approach addresses the inefficiency of using large, noisy human feedback datasets for alignment training.

## Key Results
- 17% higher human preference scores compared to using full dataset
- >50% reduction in harmful content generation
- <0.5% of full dataset size and 1% of GPU hours required

## Why This Works (Mechanism)

### Mechanism 1
High preference margin pairs accelerate convergence by reducing noisy and ambiguous preference signals. The proxy reward model calculates preference margin as the absolute difference between scores for winning and losing images. High-margin pairs have clear, consistent preference distinctions, while low-margin pairs may flip based on annotator subjectivity.

### Mechanism 2
LLM-based text quality assessment prevents harmful content generation during fine-tuning. Prompts with low LLM scores are filtered out, reducing model exposure to harmful concepts. The LLM evaluates prompt clarity, formatting, and harmfulness.

### Mechanism 3
Text diversity maximization through k-NN entropy estimation prevents overfitting to specific prompt families. Higher entropy in the prompt embedding space correlates with better generalization across diverse text prompts, encouraging selection of prompts from different semantic regions.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO) for diffusion models
  - Why needed: FiFA uses Diffusion-DPO as the fine-tuning objective
  - Quick check: How does Diffusion-DPO loss approximate standard DPO when dealing with denoising trajectories?

- Concept: Proxy reward model training
  - Why needed: FiFA relies on proxy reward model for preference margin calculation
  - Quick check: What architecture and training procedure would you use to create a proxy reward model for text-to-image pairs?

- Concept: k-NN entropy estimation
  - Why needed: Text diversity is measured using k-NN entropy
  - Quick check: How does k-NN entropy estimator approximate true entropy of continuous distribution?

## Architecture Onboarding

- Component map: Proxy reward model → Preference margin calculation → LLM scoring → Text quality assessment → Text embedding + k-NN entropy estimator → Text diversity measurement → FiFA optimization layer → Data importance scores → Filtered dataset → Diffusion-DPO training pipeline

- Critical path: Data → Proxy reward model → Margin scores → LLM quality scores → Diversity scores → FiFA importance scores → Filtered dataset → Diffusion-DPO training

- Design tradeoffs:
  - Accuracy vs efficiency: Proxy reward model saves computation but may be less accurate than online reward learning
  - Quality vs quantity: Stricter text quality filtering reduces harmful content but may remove useful data
  - Diversity vs margin: Increasing diversity may require selecting some lower-margin pairs, potentially slowing convergence

- Failure signatures:
  - Performance plateaus or degrades: Check if proxy reward model is poorly trained or diversity component is too aggressive
  - Harmful content still appears: Verify LLM quality filtering thresholds are appropriate
  - Overfitting to specific prompts: Check if diversity component is insufficient or k-NN entropy estimation is broken

- First 3 experiments:
  1. Verify proxy reward model can distinguish clear preference pairs by testing margin calculation on known good/bad examples
  2. Test LLM quality scoring on small prompt set to ensure it correctly identifies harmful vs clean content
  3. Validate k-NN entropy estimation by checking if it produces higher values for diverse prompt sets versus clustered ones

## Open Questions the Paper Calls Out

### Open Question 1
How would FiFA perform when applied to online DPO or other RLHF methods that require iterative data selection? The current FiFA framework is designed for offline datasets and static selection, while online DPO requires iterative selection during training.

### Open Question 2
What is the optimal trade-off between preference margin, text quality, and text diversity when using different reward models or datasets? The paper uses fixed values but does not systematically explore optimal configurations across different scenarios.

### Open Question 3
How does FiFA compare to model-centric efficiency methods (like pruning or timestep reduction) when combined with data filtering? The paper focuses solely on data filtering efficiency and does not investigate potential synergies with model compression or architectural optimizations.

## Limitations

- The proxy reward model's ability to reliably estimate preference margins across diverse prompt domains without extensive calibration is not thoroughly validated.
- The LLM-based text quality filtering system lacks detailed analysis of false positive/negative rates and may inadvertently filter out valuable content.
- The paper does not empirically compare k-NN entropy diversity metric against simpler diversity measures to demonstrate its added value.

## Confidence

- **High confidence**: Experimental results showing 17% higher human preference scores and >50% reduction in harmful content using <0.5% of dataset
- **Medium confidence**: Theoretical framework connecting preference margin, text quality, and diversity to training efficiency
- **Low confidence**: Assumption that proxy reward model can reliably estimate preference margins across diverse domains without extensive calibration

## Next Checks

1. Conduct ablation studies varying preference margin threshold to empirically determine optimal balance between filtering noise and retaining informative samples, measuring correlation between margin values and actual preference informativeness.

2. Implement detailed error analysis of LLM-based text quality filtering system, categorizing false positives/negatives and assessing whether filtering criteria might be too conservative, potentially removing valuable training data.

3. Compare k-NN entropy diversity metric against simpler measures (cosine distance clustering, frequency-based filtering) in controlled experiments to determine whether additional computational complexity provides measurable performance benefits.