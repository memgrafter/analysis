---
ver: rpa2
title: 'When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning
  Method'
arxiv_id: '2402.17193'
source_url: https://arxiv.org/abs/2402.17193
tags:
- finetuning
- scaling
- size
- data
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how scaling factors affect LLM finetuning, focusing
  on the data-limited regime. The authors propose a multiplicative joint scaling law
  that relates finetuning data size to LLM model size, pretraining data size, and
  PET parameter size.
---

# When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method

## Quick Facts
- **arXiv ID**: 2402.17193
- **Source URL**: https://arxiv.org/abs/2402.17193
- **Authors**: Biao Zhang; Zhongtao Liu; Colin Cherry; Orhan Firat
- **Reference count**: 40
- **Primary result**: Proposes a multiplicative joint scaling law for LLM finetuning performance based on data size, model size, pretraining data size, and PET parameter size

## Executive Summary
This paper investigates how scaling factors affect LLM finetuning performance, particularly in data-limited regimes. The authors propose a multiplicative joint scaling law that relates finetuning data size to LLM model size, pretraining data size, and PET parameter size. Through extensive experiments on translation and summarization tasks, they find that scaling LLM model size provides more benefits than scaling pretraining data size, and that scaling PET parameters is generally ineffective. The study reveals that optimal finetuning methods are highly task- and data-dependent.

## Method Summary
The authors conduct systematic scaling experiments using bilingual LLMs (1B-16B parameters) for English-German and English-Chinese translation tasks, along with multilingual summarization. They evaluate three finetuning methods: full-model tuning (FMT), prompt tuning (Prompt), and LoRA. The scaling experiments vary one factor at a time while controlling others, measuring performance with token-level perplexity, BLEURT, and RougeL. The multiplicative joint scaling law is then fitted to the experimental data to quantify how each scaling factor affects finetuning performance.

## Key Results
- LLM finetuning follows a power-based multiplicative joint scaling law between finetuning data size and each scaling factor
- Scaling LLM model size benefits finetuning more than scaling pretraining data size
- Scaling PET parameters (prompt length or rank) is generally ineffective for both LoRA and Prompt methods
- The proposed scaling law captures the relationship between different factors and finetuning data size with small fitting and extrapolation errors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM finetuning performance scales with a multiplicative joint scaling law between finetuning data size and each of the other scaling factors (LLM model size, pretraining data size, PET parameter size).
- **Mechanism**: The performance loss L follows a power law relationship: L(X, Df) = A * 1/X^α * 1/Df^β + E, where X represents each scaling factor and Df is the finetuning data size. This captures how performance improves as any of these factors increases, with the effect being multiplicative rather than additive.
- **Core assumption**: The finetuning performance can be decomposed into independent contributions from each scaling factor, and these contributions combine multiplicatively.
- **Evidence anchors**: 
  - [abstract] "LLM finetuning follows a power-based multiplicative joint scaling law between finetuning data size and each other scaling factor"
  - [section 3] "Figure 1 shows that the above formulation well describes LLM finetuning data scaling with small predictive errors across model sizes and methods"
  - [corpus] Weak evidence - corpus neighbors focus on parameter-efficient methods but don't directly address multiplicative scaling laws
- **Break condition**: If the scaling factors interact in non-multiplicative ways, or if there are fundamental limits to scaling that create plateaus or reversals in the power law relationship.

### Mechanism 2
- **Claim**: Scaling LLM model size benefits finetuning more than scaling pretraining data size.
- **Mechanism**: The scaling exponent for LLM model size (αm) is often larger than the scaling exponent for pretraining data size (αp), indicating that increasing the model size has a stronger impact on finetuning performance than increasing the amount of pretraining data.
- **Core assumption**: The knowledge encoded in larger models is more valuable for finetuning than the knowledge from more pretraining data, or that larger models can better leverage the same pretraining data.
- **Evidence anchors**:
  - [abstract] "LLM finetuning benefits more from LLM model scaling than pretraining data scaling"
  - [section 4] "Results in Figures 2, 3 and Table 4 show that the scaling exponent for LLM model size αm often outnumbers that for pretraining data size αp across finetuning methods and tasks"
  - [corpus] Weak evidence - corpus focuses on parameter-efficient methods but doesn't directly compare model vs pretraining data scaling
- **Break condition**: If the pretraining data becomes extremely diverse or high-quality, it might overcome the advantage of model size scaling.

### Mechanism 3
- **Claim**: Scaling PET parameters is generally ineffective for both LoRA and Prompt methods.
- **Mechanism**: Increasing the number of trainable parameters in PET methods (prompt length for Prompt, rank for LoRA) has minimal impact on finetuning performance, as indicated by very small scaling exponents (|αt| ≪ 1e-2).
- **Core assumption**: The expressivity bottleneck in PET methods is not primarily determined by the number of trainable parameters, or that the base LLM knowledge is sufficient and additional parameters don't add significant value.
- **Evidence anchors**:
  - [abstract] "PET parameter scaling is generally ineffective"
  - [section 4] "Figure 4 and Table 4 show that increasing PET parameter sizes...affects finetuning performance marginally"
  - [corpus] Moderate evidence - corpus includes papers on parameter-efficient methods and LoRA, supporting the focus on PET parameter scaling
- **Break condition**: If the base LLM knowledge is insufficient for the task, or if the PET method implementation is suboptimal, scaling parameters might become more effective.

## Foundational Learning

- **Concept**: Power law scaling relationships
  - **Why needed here**: The paper relies on power law formulations to describe how finetuning performance scales with various factors. Understanding power laws is crucial for interpreting the results and the proposed multiplicative joint scaling law.
  - **Quick check question**: What is the general form of a power law relationship, and how does it differ from linear scaling?

- **Concept**: Parameter-efficient tuning methods (PET)
  - **Why needed here**: The study compares full-model tuning (FMT) with two PET methods (Prompt and LoRA). Understanding how these methods work and their key differences is essential for interpreting the scaling results.
  - **Quick check question**: How do Prompt tuning and LoRA differ in terms of which parameters they update and how they modify the LLM?

- **Concept**: Transfer learning and knowledge transfer
  - **Why needed here**: Finetuning is a form of transfer learning, and the paper discusses how different factors affect the transfer of knowledge from the pretrained LLM to downstream tasks. Understanding transfer learning concepts helps in interpreting the results.
  - **Quick check question**: What are the key factors that influence the effectiveness of knowledge transfer in finetuning?

## Architecture Onboarding

- **Component map**: Pretrained LLMs (1B-16B) -> Finetuning datasets (WMT, summarization) -> Finetuning methods (FMT, Prompt, LoRA) -> Scaling factors (model size, pretraining data, finetuning data, PET parameters) -> Evaluation metrics (PPL, BLEURT, RougeL)

- **Critical path**: 
  1. Pretrain bilingual LLMs (En-De, En-Zh) with varying model sizes
  2. For each scaling factor, conduct finetuning experiments with different values
  3. Measure performance using appropriate metrics
  4. Fit the multiplicative joint scaling law to the data
  5. Analyze the scaling exponents and their implications

- **Design tradeoffs**:
  - Model size vs. computational cost: Larger models provide better performance but are more expensive to train and finetune
  - Finetuning data size vs. overfitting: More data generally improves performance but increases the risk of overfitting, especially with smaller models
  - PET method choice: Different PET methods have different strengths and weaknesses in terms of performance, stability, and parameter efficiency

- **Failure signatures**:
  - Poor performance on held-out data despite good fit on training data (overfitting)
  - Unstable training or convergence issues, especially with larger prompt lengths or ranks
  - Unexpected reversals in scaling trends (e.g., performance degradation with increased model size)

- **First 3 experiments**:
  1. Finetuning data scaling: Fix LLM model size at 1B, vary finetuning data size, and measure performance for FMT, Prompt, and LoRA
  2. LLM model size scaling: Fix finetuning data size, vary LLM model size (1B-16B), and measure performance for FMT, Prompt, and LoRA
  3. PET parameter scaling: Fix LLM model size at 1B, vary PET parameter size (prompt length or rank), and measure performance for Prompt and LoRA

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the proposed multiplicative joint scaling law generalize to multi-modal LLMs or more creative generation tasks beyond translation and summarization?
  - **Basis in paper**: [inferred] The authors acknowledge their study focuses on closed generation tasks (translation/summarization) and express interest in extending to multi-modal LLMs and open/creative generation tasks in the conclusion.
  - **Why unresolved**: The current experiments are limited to specific types of tasks and LLM architectures. Scaling behavior may differ significantly for multi-modal models or more open-ended generation.
  - **What evidence would resolve it**: Conducting the same systematic scaling experiments with multi-modal LLMs and open-ended generation tasks, comparing the resulting scaling exponents and law formulation.

- **Open Question 2**: How does the quality and diversity of finetuning data impact the scaling laws and optimal finetuning method selection?
  - **Basis in paper**: [inferred] The authors mention data quality as a factor they would like to explore in future work, but do not investigate it in the current study.
  - **Why unresolved**: The scaling experiments use fixed datasets without varying data quality or diversity. It's unclear how noisy or limited data affects the scaling relationships.
  - **What evidence would resolve it**: Systematically varying the quality and diversity of finetuning data while keeping other factors constant, and measuring the impact on scaling exponents and optimal method selection.

- **Open Question 3**: Can theoretical foundations be established for the proposed multiplicative joint scaling law?
  - **Basis in paper**: [explicit] The authors state their scaling law is "mostly based on empirical results" without theoretical grounding.
  - **Why unresolved**: The law is derived empirically without mathematical derivation or theoretical justification. Its general applicability is uncertain.
  - **What evidence would resolve it**: Developing a theoretical model that predicts the multiplicative relationship between finetuning data size and other scaling factors, and validating it against empirical results.

- **Open Question 4**: How does finetuning affect the zero-shot capability of LLMs on entirely unrelated tasks or domains?
  - **Basis in paper**: [explicit] The authors examine zero-shot generalization to related tasks (shared target language) but not to unrelated tasks.
  - **Why unresolved**: The study focuses on generalization to closely related tasks, but finetuning may have different effects on unrelated domains.
  - **What evidence would resolve it**: Finetuning LLMs on specific tasks and evaluating zero-shot performance on a diverse set of unrelated tasks across multiple domains, comparing results across different finetuning methods.

## Limitations

- The multiplicative joint scaling law assumes independent multiplicative effects between scaling factors, which may not hold in all scenarios
- The study focuses on bilingual translation and summarization tasks, limiting generalizability to other domains
- The analysis covers model sizes up to 16B parameters, leaving uncertainty about scaling behavior for much larger LLMs
- The effectiveness of scaling laws may vary depending on the quality and diversity of pretraining data, which is not systematically controlled

## Confidence

**High Confidence**: The empirical observation that LLM model scaling benefits finetuning more than pretraining data scaling (Mechanism 2). This is supported by direct experimental evidence across multiple tasks and finetuning methods, with clear quantitative differences in scaling exponents.

**Medium Confidence**: The overall multiplicative joint scaling law formulation (Mechanism 1). While the law fits the observed data well with small errors, the assumption of independent multiplicative effects between scaling factors requires further validation, especially for extrapolation beyond tested ranges.

**Medium Confidence**: The ineffectiveness of PET parameter scaling (Mechanism 3). The results show small scaling exponents, but this could be method-specific rather than a fundamental limitation. Different PET implementations or tasks might yield different results.

## Next Checks

1. **Cross-domain validation**: Test the scaling law on non-translation tasks (e.g., code generation, reasoning) to assess generalizability beyond the studied domains. This would reveal if the multiplicative relationships hold across different types of knowledge and task structures.

2. **Extreme scaling extrapolation**: Validate the scaling law predictions for model sizes beyond 16B parameters (e.g., 30B-70B) and for very small finetuning datasets (few-shot learning scenarios). This would test the law's predictive power at the boundaries of the current experimental range.

3. **Pretraining data quality sensitivity**: Systematically vary the quality and diversity of pretraining data while keeping model size constant to determine if the relative importance of model scaling vs. pretraining data scaling changes under different data regimes.