---
ver: rpa2
title: On the Capacity of Citation Generation by Large Language Models
arxiv_id: '2410.11217'
source_url: https://arxiv.org/abs/2410.11217
tags:
- citation
- citations
- arxiv
- llms
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a systematic analysis of large language models'
  (LLMs) ability to generate citations within responses, focusing on improving citation
  quality in retrieval-augmented generation (RAG). The authors identify that while
  RAG can help reduce hallucination by incorporating external resources, existing
  works largely overlook the critical aspect of accurate source attribution.
---

# On the Capacity of Citation Generation by Large Language Models

## Quick Facts
- arXiv ID: 2410.11217
- Source URL: https://arxiv.org/abs/2410.11217
- Reference count: 36
- One-line primary result: Novel Generate-then-Reﬁne approach improves citation recall and precision across multiple LLMs, with F1 score improvements exceeding 20%

## Executive Summary
This paper systematically analyzes large language models' ability to generate citations in retrieval-augmented generation contexts, identifying that while RAG reduces hallucination, accurate source attribution remains a critical gap. The authors introduce new evaluation metrics that eliminate over-penalization of unnecessary citations and propose a Generate-then-Reﬁne approach that enhances citation quality without altering response text. Experiments across three datasets demonstrate significant improvements in citation recall and precision, with over 20% gains in F1 scores for multiple LLMs including Llama-2-13B and Mistral-7B.

## Method Summary
The paper proposes a Generate-then-Reﬁne approach where LLMs first generate responses with citations, then a fine-tuned model analyzes and refines these citations by adding relevant ones that were missed and removing irrelevant ones without changing the response text. The method uses a fine-tuned Mistral-7B model trained on WebGLM-QA dataset, along with new citation evaluation metrics that address over-penalization issues in existing metrics. The approach is tested on three datasets (WebGLM-QA, ASQA, and ELI5) using both few-shot and fine-tuning strategies, with evaluation combining response correctness metrics (BLEU-4, ROUGE-L) and citation quality metrics (recall, precision).

## Key Results
- Generate-then-Reﬁne approach achieves over 20% improvement in citation F1 scores across multiple LLMs
- Llama-2-13B surpasses GPT-3.5-turbo in citation generation capabilities after fine-tuning
- Fine-tuned models show substantial improvements on WebGLM-QA but exhibit poor generalization to ASQA and ELI5 datasets
- The proposed evaluation metrics successfully eliminate over-penalization of unnecessary citations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generate-then-Reﬁne approach improves citation quality without altering response text
- Mechanism: The approach first generates a response without citations, then uses a fine-tuned model to identify and insert relevant citations while removing irrelevant ones
- Core assumption: A fine-tuned model can effectively distinguish relevant from irrelevant citations based on semantic matching rather than just text overlap
- Evidence anchors: [abstract] "we propose a Generate-then-Reﬁne method that completes relevant citations and removes irrelevant ones without altering the response text"; [section 4.1] "We aim for the reﬁner to have three capabilities: (1) keep relevant citations within the response; (2) add necessary citations that are missing; (3) remove any irrelevant citations that are present"
- Break condition: If the fine-tuned reﬁner model cannot accurately assess the semantic relationship between statements and references, the refinement process will degrade citation quality

### Mechanism 2
- Claim: New citation evaluation metrics reduce over-penalization of responses
- Mechanism: The proposed metrics exclude statements that don't require citations from recall calculations and redefine "relevant" citations to allow for comprehensive citation sets
- Core assumption: Not all statements in responses require citations, and having multiple citations with the same information should not be penalized
- Evidence anchors: [abstract] "we introduce new citation evaluation metrics to eliminate the over-penalization of unnecessary and excessive citations in existing metrics"; [section 3.2] "If a statement si doesn't have any citation and φ(concat(Call), si) = 0, then it will not require computation of its recall score"; [section 3.2] "For a citation cij, if it can support statement si independently or if it can support statement si after combining with a subset of remaining citations"
- Break condition: If the distinction between statements requiring and not requiring citations is unclear, the metric may still over-penalize certain responses

### Mechanism 3
- Claim: Fine-tuning significantly improves LLMs' citation generation capabilities
- Mechanism: Models trained on WebGLM-QA dataset learn to generate responses with appropriate citations through supervised learning
- Core assumption: The training data contains sufficient examples of high-quality citation-generation pairs
- Evidence anchors: [section 3.4] "After fine-tuning, all open-source models demonstrate substantial improvements on WebGLM-QA, both in response correctness and citation quality"; [section 3.4] "Notably, Llama-2-13b even surpassed GPT-3.5-turbo in citation generation capabilities"; [section 3.4] "The previously underperforming Llama-2 series models reached performance levels comparable to other models through fine-tuning"
- Break condition: If the training data quality is poor or doesn't represent the target domain, fine-tuning may not improve performance or could even degrade it

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Understanding how RAG works is crucial to grasp why citation generation is important for addressing hallucination issues
  - Quick check question: What is the primary purpose of incorporating retrieval in RAG systems, and how does this relate to citation generation?

- Concept: Citation evaluation metrics
  - Why needed here: The paper introduces new metrics to evaluate citation quality, which is central to the research contribution
  - Quick check question: How do the proposed citation recall and precision metrics differ from traditional information retrieval metrics?

- Concept: Few-shot vs. fine-tuning approaches
  - Why needed here: The paper uses both methods to guide LLMs in generating citations, and understanding their differences is key to interpreting results
  - Quick check question: What are the advantages and disadvantages of few-shot learning compared to fine-tuning for citation generation tasks?

## Architecture Onboarding

- Component map: Question + Retrieved passages -> LLM (generates response with citations) -> Reﬁner (fine-tuned model) -> Evaluation metrics
- Critical path: 1. Question and retrieved passages are provided as input; 2. LLM generates initial response (with or without citations); 3. Reﬁner processes the response to add/remove citations; 4. Evaluation metrics assess both response correctness and citation quality; 5. NLI model is used to verify citation-statement relationships
- Design tradeoffs: Pre-hoc vs. Post-hoc citation generation (pre-hoc offers better consistency but is more challenging; post-hoc is easier but may result in less coherent citations); Comprehensive vs. concise citations (allowing multiple citations for the same information increases recall but may reduce precision); Generalization vs. specialization (models fine-tuned on specific datasets may perform better on those datasets but worse on others)
- Failure signatures: Low citation recall with high precision (model is too conservative in adding citations); High citation recall with low precision (model is adding too many irrelevant citations); Poor generalization (model performs well on training data but poorly on other datasets); NLI model bias (evaluation results are skewed due to the NLI model's preferences)
- First 3 experiments: 1. Compare few-shot vs. fine-tuning approaches on WebGLM-QA dataset to establish baseline performance; 2. Apply Generate-then-Reﬁne method to few-shot results to measure improvement in citation quality; 3. Test generalization of fine-tuned models on ASQA and ELI5 datasets to identify domain adaptation issues

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the research reveals several important areas for future investigation:

## Limitations
- The evaluation methodology relies heavily on an NLI model for determining citation relevance, which introduces potential bias that is not fully explored
- Generalization performance across domains shows significant variance, with the Generate-then-Reﬁne approach degrading on ASQA compared to few-shot results
- The effectiveness of the reﬁner model on real-world, noisy data versus curated benchmark datasets remains untested

## Confidence
- High confidence: The proposed evaluation metrics successfully address over-penalization issues in citation assessment
- Medium confidence: The Generate-then-Reﬁne approach improves citation quality on average, but shows inconsistent results across datasets
- Medium confidence: Fine-tuning significantly improves citation generation capabilities, though the gains are dataset-dependent

## Next Checks
1. Test the NLI model's sensitivity to different prompt formulations and citation styles to quantify evaluation bias
2. Evaluate the reﬁner model's performance on out-of-domain questions and noisy retrieval results to assess real-world robustness
3. Conduct ablation studies removing the reﬁner's different capabilities (adding vs. removing citations) to identify the most impactful components