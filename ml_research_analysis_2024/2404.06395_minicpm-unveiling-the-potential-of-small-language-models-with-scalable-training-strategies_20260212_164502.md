---
ver: rpa2
title: 'MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training
  Strategies'
arxiv_id: '2404.06395'
source_url: https://arxiv.org/abs/2404.06395
tags:
- training
- arxiv
- data
- loss
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MiniCPM, a family of Small Language Models
  (SLMs) designed to address the challenges of resource efficiency and practical deployment
  faced by large language models (LLMs). The authors propose a scalable training strategy
  that includes model wind tunnel experiments for hyperparameter optimization, a Warmup-Stable-Decay
  (WSD) learning rate scheduler to facilitate continuous training and domain adaptation,
  and a two-stage pre-training strategy that incorporates high-quality data during
  the decay stage.
---

# MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies

## Quick Facts
- arXiv ID: 2404.06395
- Source URL: https://arxiv.org/abs/2404.06395
- Authors: Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun
- Reference count: 40
- Primary result: MiniCPM models (1.2B, 2.4B) achieve capabilities on par with 7B-13B LLMs through scalable training strategies

## Executive Summary
This paper introduces MiniCPM, a family of Small Language Models (SLMs) designed to address the challenges of resource efficiency and practical deployment faced by large language models (LLMs). The authors propose a scalable training strategy that includes model wind tunnel experiments for hyperparameter optimization, a Warmup-Stable-Decay (WSD) learning rate scheduler to facilitate continuous training and domain adaptation, and a two-stage pre-training strategy that incorporates high-quality data during the decay stage. The resulting models, MiniCPM-1.2B and MiniCPM-2.4B, demonstrate superior performance compared to their larger counterparts, achieving capabilities on par with 7B-13B LLMs. Additionally, the authors introduce the MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE, and MiniCPM-128K, further expanding the capabilities of SLMs in various applications.

## Method Summary
MiniCPM employs a scalable training strategy combining model wind tunnel experiments for hyperparameter optimization, a Warmup-Stable-Decay (WSD) learning rate scheduler, and a two-stage pre-training strategy. The WSD scheduler optimizes training efficiency by dividing the learning rate schedule into warmup, stable, and decay stages. Model wind tunnel experiments identify optimal hyperparameters through small-scale testing before scaling to larger models. The two-stage pre-training strategy uses coarse-quality data initially, then introduces high-quality, domain-specific data during the decay stage to enhance specialized capabilities.

## Key Results
- MiniCPM-1.2B and MiniCPM-2.4B achieve performance comparable to 7B-13B LLMs on benchmarks like C-Eval, CMMLU, and MMLU
- The WSD learning rate scheduler enables more efficient training compared to standard approaches
- The two-stage pre-training strategy with high-quality data introduction during decay improves specialized capabilities
- The MiniCPM family includes variants for different applications, including MiniCPM-DPO, MiniCPM-MoE, and MiniCPM-128K for long-context tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Warmup-Stable-Decay (WSD) learning rate scheduler improves training efficiency by splitting training into three distinct stages, each optimized for different learning dynamics.
- Mechanism: During warmup, the learning rate increases to stabilize early training. In the stable stage, a constant high learning rate enables rapid convergence. During decay, a gradually decreasing learning rate allows fine-tuning and avoids overshooting local minima.
- Core assumption: Training dynamics can be effectively partitioned into stages with distinct optimal learning rate profiles.
- Evidence anchors:
  - [abstract] "we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation."
  - [section 4.1] "The current commonly used learning rate strategy is the Cosine LRS... which gradually decreases the learning rate following a cosine curve after it reaches its maximum after the warmup stage."
  - [corpus] Weak evidence; corpus provides no direct comparison of scheduler performance.
- Break condition: If the stable stage duration is too short, the model may not fully explore the parameter space, leading to suboptimal convergence.

### Mechanism 2
- Claim: The model wind tunnel experiments enable efficient hyperparameter optimization across model scales by identifying stable configurations transferable to larger models.
- Mechanism: By systematically testing hyperparameters on smaller models and leveraging scaling laws, optimal configurations can be extrapolated to larger models without exhaustive retraining.
- Core assumption: Hyperparameter effects scale predictably with model size, allowing small-scale experiments to inform large-scale training.
- Evidence anchors:
  - [abstract] "Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling."
  - [section 3.1] "Extensive experiments should be conducted through an SLM to explore the limit of SLM before transferring the experience into LLMs."
  - [corpus] No direct corpus evidence on hyperparameter scaling transferability.
- Break condition: If scaling relationships deviate significantly for larger models, small-scale experiments may yield suboptimal configurations.

### Mechanism 3
- Claim: The two-stage pre-training strategy, incorporating high-quality data during the decay stage, enhances model performance by focusing on specialized capabilities at the optimal training point.
- Mechanism: Introducing high-quality, domain-specific data during the decay stage, when the model is near convergence, allows for more effective learning of specialized tasks without disrupting general pre-training.
- Core assumption: Introducing specialized data at the decay stage provides more effective learning than distributing it throughout pre-training or only in fine-tuning.
- Evidence anchors:
  - [section 5] "Based on these two hypotheses, we propose the following training strategy: during the pre-training phase, only use large-scale coarse-quality pre-training data... During the annealing phase, we use diverse and high-quality knowledge and ability-oriented SFT data, mixed into the pre-training data."
  - [corpus] No corpus evidence on the comparative effectiveness of data introduction timing.
- Break condition: If the decay stage is too short or the specialized data is not representative, the benefits of this strategy may not materialize.

## Foundational Learning

- Concept: Learning Rate Scheduling
  - Why needed here: The WSD scheduler is a novel approach that requires understanding of how learning rate affects model training dynamics.
  - Quick check question: How does the WSD scheduler differ from the commonly used Cosine LRS, and what are the benefits of this difference?

- Concept: Scaling Laws
  - Why needed here: The paper leverages scaling laws to optimize model and data size for efficient training.
  - Quick check question: How do the scaling laws proposed in this paper differ from the Chinchilla optimal scaling law, and what implications does this have for model training?

- Concept: Hyperparameter Optimization
  - Why needed here: The model wind tunnel experiments rely on efficient hyperparameter optimization techniques.
  - Quick check question: What are the key hyperparameters that need to be optimized for transformer models, and how can small-scale experiments inform large-scale training?

## Architecture Onboarding

- Component map: Embedding layer -> Transformer blocks with attention mechanisms -> Language modeling head
- Critical path: Train base model using WSD scheduler -> Domain adaptation with two-stage pre-training strategy -> Optimize hyperparameters through model wind tunnel experiments
- Design tradeoffs: WSD scheduler balances training efficiency vs. model performance; two-stage pre-training balances general vs. specialized capabilities; model wind tunnel experiments trade small-scale accuracy for large-scale efficiency
- Failure signatures: Short stable stage prevents full parameter exploration; short decay stage limits specialized capability learning; ineffective hyperparameter optimization reduces scaling performance
- First 3 experiments:
  1. Replicate model wind tunnel experiments on small-scale model to verify hyperparameter optimization approach
  2. Implement WSD scheduler and compare performance to Cosine LRS on standard benchmark
  3. Test two-stage pre-training strategy by introducing high-quality data during decay stage and measuring impact on model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise functional form of the optimal loss envelope during continuous training with WSD LRS, and how does it compare to the power-law form suggested in the paper?
- Basis in paper: [explicit] The paper mentions that the optimal loss envelope during continuous training with WSD LRS can be fitted using either an exponential function or a power-law function, with the power-law form being a better fit.
- Why unresolved: While the paper suggests that the power-law form is a better fit, it does not provide the exact parameters of the fitted function or a detailed comparison between the two forms.
- What evidence would resolve it: A rigorous mathematical derivation of the optimal loss envelope, along with a comprehensive comparison of the fitted parameters for both the exponential and power-law forms, would resolve this question.

### Open Question 2
- Question: How does the performance of MiniCPM models on long-context tasks scale with context length, and what is the impact of different long-context strategies (e.g., ABF, NTK-Aware RoPE Scaling) on this scaling?
- Basis in paper: [inferred] The paper introduces MiniCPM-128K, a long-context variant of MiniCPM, and mentions the use of strategies like ABF and NTK-Aware RoPE Scaling. However, it does not provide a detailed analysis of how the model's performance scales with context length or the effectiveness of these strategies.
- Why unresolved: The paper lacks a comprehensive evaluation of MiniCPM-128K's performance on long-context tasks across varying context lengths and a detailed analysis of the impact of different long-context strategies.
- What evidence would resolve it: Extensive benchmarking of MiniCPM-128K on a variety of long-context tasks, across different context lengths, and a comparative analysis of the performance with and without different long-context strategies would resolve this question.

### Open Question 3
- Question: What are the specific architectural modifications and training techniques that contribute to the superior performance of MiniCPM models compared to their larger counterparts?
- Basis in paper: [explicit] The paper mentions several architectural modifications and training techniques used in MiniCPM, such as shared input-output layers, deep-and-thin networks, group query attention, and the two-stage pre-training strategy. However, it does not provide a detailed ablation study to isolate the impact of each modification.
- Why unresolved: The paper does not provide a comprehensive analysis of the individual contributions of each architectural modification and training technique to the overall performance of MiniCPM models.
- What evidence would resolve it: A rigorous ablation study that systematically evaluates the impact of each architectural modification and training technique on the performance of MiniCPM models would resolve this question.

## Limitations
- Lack of direct comparative evidence for WSD learning rate scheduler's effectiveness versus standard approaches
- Model wind tunnel experiments lack detailed documentation of scaling relationships between small and large models
- Proprietary datasets used for fine-tuning create uncertainty about reproducibility of reported performance gains

## Confidence
- High Confidence: The architectural choices (transformer-based models with standard attention mechanisms) are well-established and the general framework of small model scaling is theoretically sound.
- Medium Confidence: The conceptual approach of using model wind tunnel experiments for hyperparameter optimization and the two-stage pre-training strategy are reasonable, but lack direct empirical validation.
- Low Confidence: The specific claims about the WSD learning rate scheduler's superiority and the exact scaling relationships between model sizes are not sufficiently supported by comparative evidence.

## Next Checks
1. **Scheduler Comparison**: Implement a direct ablation study comparing the WSD learning rate scheduler against the standard Cosine LRS across identical training runs, measuring both convergence speed and final performance on standardized benchmarks.

2. **Scaling Law Verification**: Conduct systematic experiments varying model sizes while holding data composition constant, measuring whether the proposed scaling relationships hold across at least three distinct model scales with statistical significance testing.

3. **Data Timing Impact**: Design a controlled experiment testing the two-stage pre-training strategy by introducing high-quality data at different training stages (early, middle, late) and comparing the resulting model capabilities to quantify the claimed benefits of decay-stage introduction.