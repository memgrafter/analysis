---
ver: rpa2
title: 'DeepCSHAP: Utilizing Shapley Values to Explain Deep Complex-Valued Neural
  Networks'
arxiv_id: '2403.08428'
source_url: https://arxiv.org/abs/2403.08428
tags:
- complex-valued
- neural
- methods
- explanation
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of explanation methods for complex-valued
  neural networks (CVNNs), which are gaining popularity for processing complex-valued
  data such as MRI or PolSAR images. The authors develop DeepCSHAP, a complex-valued
  adaptation of the widely-used DeepSHAP algorithm, along with complex-valued versions
  of four gradient-based explanation methods.
---

# DeepCSHAP: Utilizing Shapley Values to Explain Deep Complex-Valued Neural Networks

## Quick Facts
- arXiv ID: 2403.08428
- Source URL: https://arxiv.org/abs/2403.08428
- Authors: Florian Eilers; Xiaoyi Jiang
- Reference count: 40
- Primary result: DeepCSHAP extends SHAP values to complex-valued neural networks and outperforms gradient-based methods on both real and complex datasets

## Executive Summary
This paper addresses the lack of explanation methods for complex-valued neural networks (CVNNs), which are gaining popularity for processing complex-valued data such as MRI or PolSAR images. The authors develop DeepCSHAP, a complex-valued adaptation of the widely-used DeepSHAP algorithm, along with complex-valued versions of four gradient-based explanation methods. DeepCSHAP extends SHAP values to the complex domain and introduces a complex-valued chain rule for propagating contributions through CVNN layers. The method is evaluated on both real-valued (MNIST) and complex-valued (PolSAR) datasets, outperforming gradient-based baselines in terms of explanation quality. The authors also validate theoretical properties of their method, demonstrating that it satisfies SHAP axioms like local accuracy and missingness. DeepCSHAP provides a significant contribution to the field of explainable AI for CVNNs, enabling better understanding and interpretation of these models in safety-critical applications.

## Method Summary
DeepCSHAP extends the SHAP framework to complex-valued neural networks by defining partial contributions for real and imaginary parts separately, then combining them using Wirtinger derivatives to form complex multipliers. These multipliers propagate contributions through CVNN layers using a complex-valued chain rule analogous to gradient backpropagation. The method is implemented for a complex-valued CNN architecture with three blocks of convolutions, CReLU activations, and max pooling, followed by a 2-layer MLP for MNIST. For the PolSAR dataset, a complex-valued ResNet18 is used. The authors also adapt four gradient-based explanation methods (vanilla gradient, integrated gradients, guided backpropagation, and smooth grad) to the complex domain using Wirtinger calculus and complex ReLU variants.

## Key Results
- DeepCSHAP outperforms gradient-based methods on both MNIST and PolSAR datasets in terms of explanation quality, measured by change in log-odds after masking important pixels
- The method satisfies SHAP axioms (local accuracy and missingness) with validation showing local accuracy error ~10^-6 on MNIST
- DeepCSHAP is computationally more expensive than gradient-based methods, explaining approximately 8 images per second on MNIST and 3 images per second on the PolSAR dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeepCSHAP extends SHAP values to the complex domain by defining complex-valued chain rule multipliers.
- Mechanism: The authors define partial contributions ϕR(j) and ϕI(j) for real and imaginary parts separately, then combine them using Wirtinger derivatives to form complex multipliers mj and mj̄. These multipliers propagate contributions through CVNN layers using a chain rule analogous to gradient backpropagation.
- Core assumption: Complex-valued neural networks can be decomposed into real and imaginary contributions that behave linearly under certain conditions (Equations 18-19 in the paper).
- Evidence anchors:
  - [abstract] "DeepCSHAP extends SHAP values to the complex domain and introduces a complex-valued chain rule for propagating contributions through CVNN layers"
  - [section 5.2] Formal definition of partial multipliers and complex-valued chain rule
  - [corpus] Weak - no direct citations discussing complex SHAP extensions
- Break condition: If the CVNN architecture uses non-holomorphic activations or outputs that cannot be cleanly separated into real/imaginary contributions, the partial contribution decomposition fails.

### Mechanism 2
- Claim: Gradient-based methods can be adapted to complex domains using Wirtinger calculus.
- Mechanism: The authors replace real-valued gradients with Wirtinger derivatives (Equations 6-7), which maintain the property of pointing in the direction of steepest ascent. They then adapt ReLU interpretations to complex domains (first quadrant mapping vs. projection), creating z-ReLU and C-ReLU variants.
- Core assumption: Wirtinger gradients preserve the directional and magnitude properties needed for saliency map generation, even when the result is complex-valued.
- Evidence anchors:
  - [section 4] Detailed explanation of Wirtinger derivative adaptation and complex ReLU variants
  - [abstract] "adaptation of four gradient based explanation methods to the complex domain"
  - [corpus] Missing - no neighboring papers discussing complex gradient adaptation for XAI
- Break condition: When real and imaginary parts cancel out in the sum R+I variant, the method produces zero contribution despite non-zero gradients, misleading the explanation.

### Mechanism 3
- Claim: DeepCSHAP outperforms gradient-based methods on both real and complex datasets due to its game-theoretic foundation.
- Mechanism: By satisfying SHAP axioms (local accuracy, missingness, consistency), DeepCSHAP provides theoretically grounded feature attributions that are more reliable than heuristic gradient-based methods. The quantitative results show better log-odds changes after masking important features.
- Core assumption: SHAP axioms provide meaningful guarantees about explanation quality that translate across real and complex domains.
- Evidence anchors:
  - [section 6] Quantitative results showing DeepCSHAP superiority on MNIST and PolSAR datasets
  - [section 5.3] Validation of SHAP axioms (local accuracy error ~10^-6 on MNIST)
  - [corpus] Weak - neighboring papers discuss Shapley values but not in complex-valued contexts
- Break condition: If the model's decision boundary is highly non-linear in the complex domain, the linearization assumptions underlying SHAP may break down.

## Foundational Learning

- Wirtinger Calculus
  - Why needed here: Enables gradient-based methods to work in complex domains where standard complex derivatives fail
  - Quick check question: What is the relationship between Wirtinger derivatives and the standard complex derivative?

- SHAP Framework
  - Why needed here: Provides the theoretical foundation (local accuracy, missingness, consistency) that DeepCSHAP builds upon
  - Quick check question: How do SHAP values differ from simple gradient-based attributions?

- Complex-Valued Neural Networks
  - Why needed here: The target architecture requires understanding how real/imaginary parts interact in CVNN building blocks
  - Quick check question: What makes a CVNN non-holomorphic, and why does this matter for explanation methods?

## Architecture Onboarding

- Component map:
  - Input preprocessing: Complex number handling (real + imaginary channels)
  - CVNN backbone: Complex convolutions, complex activations, complex pooling
  - DeepCSHAP module: Partial contribution calculation, chain rule propagation, multiplier computation
  - Output aggregation: Combining real and imaginary contributions into final attributions

- Critical path:
  1. Compute real and imaginary partial contributions for each layer (precomputed)
  2. Calculate complex multipliers using Wirtinger chain rule
  3. Propagate multipliers backward through network
  4. Reconstruct final contributions from complex multipliers

- Design tradeoffs:
  - Speed vs. accuracy: DeepCSHAP is slower than gradient methods but more theoretically sound
  - Complexity vs. generality: Complex ReLU variants handle different interpretations but require careful choice
  - Memory vs. precision: Precomputing contributions increases memory usage but enables efficient chain rule application

- Failure signatures:
  - Numerical instability when denominators approach zero (handled by Wirtinger derivative fallback)
  - Poor performance on highly non-linear decision boundaries where SHAP assumptions break
  - Misleading attributions when real and imaginary parts cancel in R+I gradient variants

- First 3 experiments:
  1. Verify DeepCSHAP on simple linear CVNN to confirm chain rule correctness
  2. Compare gradient variants on synthetic complex data with known ground truth
  3. Benchmark DeepCSHAP vs. gradient methods on MNIST to replicate paper results

## Open Questions the Paper Calls Out

- Q1: Can DeepCSHAP be extended to other complex-valued explanation methods, such as LIME or kernelSHAP, and how would their performance compare to DeepCSHAP?
  - Basis: The authors mention that future work could involve adapting more explanation methods, such as LIME and kernelSHAP, to the complex domain.
  - Why unresolved: The paper focuses solely on DeepCSHAP and does not explore other complex-valued adaptations of explanation methods.
  - What evidence would resolve it: Implementing and evaluating complex-valued versions of LIME and kernelSHAP on the same datasets and models used in this paper, comparing their performance to DeepCSHAP in terms of explanation quality and computational efficiency.

- Q2: How do the SHAP properties (Local Accuracy and Missingness) hold in the complex-valued case, and are there any theoretical limitations or edge cases where they might fail?
  - Basis: The paper validates Local Accuracy and Missingness on the MNIST and PolSAR datasets, showing that these properties are satisfied. However, the validation is empirical and limited to specific datasets and models.
  - Why unresolved: The paper does not provide a formal proof that these properties hold universally for all complex-valued functions or models. Edge cases, such as non-differentiable functions or highly non-linear architectures, are not explored.
  - What evidence would resolve it: A formal mathematical proof of the SHAP properties for complex-valued functions, along with empirical testing on edge cases (e.g., non-differentiable activation functions or highly non-linear models).

## Limitations

- DeepCSHAP is computationally more expensive than gradient-based methods, which may limit its practical adoption in real-time applications
- The method's performance on complex-valued architectures beyond CNNs (e.g., RNNs or transformers) remains untested
- The choice of complex ReLU variants (z-ReLU vs C-ReLU) can significantly impact explanation quality without clear guidance on optimal selection

## Confidence

- Confidence is Medium for the explanation quality improvements, as the results show consistent gains over gradient methods but are evaluated on only two datasets
- Confidence is Low regarding the generalizability to other complex-valued architectures beyond CNNs, as the method's performance on recurrent or transformer-based CVNNs remains untested
- Confidence is High in the theoretical validation of SHAP axioms (local accuracy and missingness) due to the mathematical rigor and empirical verification

## Next Checks

1. Test DeepCSHAP on a broader range of complex-valued architectures (e.g., complex RNNs or transformers) to assess generalizability beyond CNNs
2. Benchmark computational efficiency against gradient methods across different CVNN sizes and depths
3. Validate SHAP axiom satisfaction on the PolSAR dataset to ensure theoretical guarantees hold for complex-valued data