---
ver: rpa2
title: Towards Low-Resource Harmful Meme Detection with LMM Agents
arxiv_id: '2411.05383'
source_url: https://arxiv.org/abs/2411.05383
tags:
- meme
- harmful
- memes
- agent
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of detecting harmful memes in low-resource
  settings where only a few labeled examples are available. It proposes an agentic
  approach that uses a Large Multimodal Model (LMM) as an agent, enhancing it with
  two strategies: outward analysis by retrieving similar labeled memes for context,
  and inward analysis by enabling the agent to learn from its past misclassifications
  through a knowledge-revising mechanism.'
---

# Towards Low-Resource Harmful Meme Detection with LMM Agents

## Quick Facts
- **arXiv ID**: 2411.05383
- **Source URL**: https://arxiv.org/abs/2411.05383
- **Reference count**: 38
- **Primary result**: LOREHM improves macro-averaged F1 scores by 2.75-4.40% on three harmful meme datasets

## Executive Summary
This paper addresses harmful meme detection in low-resource settings where few labeled examples are available. The proposed approach uses a Large Multimodal Model (LMM) as an agent enhanced with two key strategies: outward analysis through retrieving similar labeled memes for context, and inward analysis through knowledge revision from past misclassifications. Experiments on three datasets (HarM, FHM, MAMI) demonstrate that this agentic approach achieves superior performance compared to state-of-the-art baselines, with improvements of 2.75%, 4.40%, and 2.46% in macro-averaged F1 scores respectively.

## Method Summary
The approach employs an LMM agent with two enhancement strategies. First, the outward analysis (Relative Sample Augmentation) retrieves top-K similar memes from the reference set based on multimodal embeddings, then uses majority voting to provide auxiliary harmfulness signals. Second, the inward analysis (Meme Insight Augmentation) enables the agent to learn from its past misclassifications through iterative knowledge revision, generating generalizable insights about implicit harm patterns. These strategies are combined to enable dialectical reasoning, where preliminary predictions are refined using both explicit similarity signals and learned reasoning patterns.

## Key Results
- LOREHM achieves macro-averaged F1 scores of 0.7868, 0.5969, and 0.7023 on HarM, FHM, and MAMI datasets respectively
- Performance improves by 2.75%, 4.40%, and 2.46% over state-of-the-art baselines on the three datasets
- Combining RSA and MIA strategies yields better results than either approach alone
- The approach demonstrates effectiveness particularly in detecting implicit harmful content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented voting with similarity-based selection provides effective auxiliary harmfulness signals in low-resource regimes.
- Mechanism: Top-K retrieved memes are selected based on multimodal similarity embeddings, and their labels are aggregated through majority voting to infer a preliminary harmfulness prediction for the target meme.
- Core assumption: Memes that are highly similar in multimodal embedding space share common harmfulness patterns, so their labels are reliable signals for the target.
- Evidence anchors:
  - [abstract] "we first retrieve relative memes with annotations to leverage label information as auxiliary signals for the LMM agent"
  - [section 3.2.1] describes multimodal embedding fusion (α · VE(I) + β · TE(T)) and cosine similarity-based retrieval
  - [corpus] F1 scores of ~79%, ~59%, and ~70% on HarM, FHM, and MAMI respectively for voting mechanism alone
- Break condition: If the retrieval returns memes that are similar visually but semantically unrelated in harmfulness, voting signals will mislead the agent.

### Mechanism 2
- Claim: Experience-based knowledge revision through self-reflection improves detection of implicit harmfulness.
- Mechanism: LMM agent evaluates reference memes, collects failed predictions, and iteratively refines an insight set by analyzing these failures to derive generalizable rules for harmful meme detection.
- Core assumption: Misjudged memes reveal patterns of implicit harmfulness that the agent can generalize into rules for future detection.
- Evidence anchors:
  - [abstract] "we elicit knowledge-revising behavior within the LMM agent to derive well-generalized insights into meme harmfulness"
  - [section 3.3.2] describes iterative operations (ADD, EDIT, UPVOTE, DOWNVOTE) on insight set based on failed trajectories
  - [corpus] Performance improvement when combining RSA and MIA vs either alone in Table 2
- Break condition: If the reference set is too small or unrepresentative, insights derived may not generalize and could reinforce biases.

### Mechanism 3
- Claim: Integration of outward and inward analysis enables dialectical reasoning over intricate and implicit harm-indicative patterns.
- Mechanism: Preliminary prediction from RSA is combined with insights from MIA to guide final harmfulness judgment, balancing explicit similarity signals with learned reasoning patterns.
- Core assumption: Combining explicit label signals from similar memes with internalized reasoning rules leads to more accurate harmfulness inference than either method alone.
- Evidence anchors:
  - [abstract] "By combining these strategies, our approach enables dialectical reasoning over intricate and implicit harm-indicative patterns"
  - [section 3.4] describes final inference using preliminary prediction P under guidance of insight set En
  - [corpus] LOREHM outperforms both RSA and MIA alone in Table 2
- Break condition: If either component provides misleading signals (e.g., irrelevant retrieved memes or incorrect insights), the integration may degrade performance.

## Foundational Learning

- Concept: Multimodal similarity embedding and retrieval
  - Why needed here: To find relevant reference memes that share visual and textual characteristics with the target, providing contextual harmfulness signals
  - Quick check question: What embedding strategy combines visual and textual features for meme similarity search?

- Concept: Chain-of-thought reasoning in multimodal context
  - Why needed here: To enable the LMM agent to perform structured analysis of memes by explicitly reasoning through harmfulness indicators
  - Quick check question: How does the CoT prompt template guide the agent to analyze memes for harmfulness?

- Concept: Knowledge revision through failure analysis
  - Why needed here: To extract generalizable insights from past mistakes and improve the agent's ability to detect implicit harmful patterns
  - Quick check question: What operations can the agent perform on the insight set during the revision process?

## Architecture Onboarding

- Component map: Meme encoder (visual+text) → Similarity retriever → Voting mechanism → LMM agent (CoT reasoning) → Failure analyzer → Insight set → Final judgment
- Critical path: Input meme → Embedding → Retrieve similar memes → Voting → LMM evaluation with insights → Output prediction
- Design tradeoffs: Voting mechanism vs direct input of retrieved memes (chose voting for explicit label signals), frozen encoders vs fine-tuning (chose frozen for gradient-free approach), number of retrieved memes K (odd number < N to avoid ties)
- Failure signatures: Poor retrieval quality (irrelevant memes returned), voting deadlock (equal harmful/harmless labels), insight set saturation (capacity limit reached), LMM misinterpretation of combined prompt
- First 3 experiments:
  1. Test retrieval and voting with K=3 on a small labeled set to verify similarity measures and voting logic
  2. Run CoT reasoning on reference set to collect failure cases and validate insight extraction mechanism
  3. Perform end-to-end inference on test set with RSA+MIA combination to measure performance gain over baseline LMM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the effectiveness of the knowledge-revising mechanism in the Meme Insight Augmentation strategy be quantitatively measured and validated?
- Basis in paper: [inferred] The paper describes the Meme Insight Augmentation strategy as enabling the LMM agent to capture implicit harmfulness by mimicking human knowledge-revising behavior. However, it does not provide a clear method for quantitatively measuring the effectiveness of this mechanism.
- Why unresolved: While the paper shows that the full paradigm 'w/ LOREHM' yields better performance than using either 'w/ RSA' or 'w/ MIA' alone, it does not provide a direct comparison or quantitative measure of the effectiveness of the Meme Insight Augmentation strategy specifically.
- What evidence would resolve it: A controlled experiment comparing the performance of the LMM agent with and without the Meme Insight Augmentation strategy, while keeping all other factors constant, would provide quantitative evidence of its effectiveness.

### Open Question 2
- Question: How does the performance of the LOREHM framework scale with the size of the reference set (Sref) and the number of retrieved memes (K)?
- Basis in paper: [explicit] The paper mentions that the number N of memes in the reference set Sref is set as 50, and the number K of memes in the retrieved set H is set as 5. It also provides a figure showing the effect of TopK and N-shot on performance.
- Why unresolved: While the paper provides some insights into the effect of K and N on performance, it does not explore the full range of possible values or provide a clear understanding of how performance scales with these parameters.
- What evidence would resolve it: A systematic study varying the values of N and K over a wide range, and measuring the corresponding performance of the LOREHM framework, would provide insights into how performance scales with these parameters.

### Open Question 3
- Question: How does the LOREHM framework handle ambiguous or borderline cases where the harmfulness of a meme is not clear-cut?
- Basis in paper: [inferred] The paper discusses the challenge of detecting implicit harmful information in memes and proposes the Meme Insight Augmentation strategy to address this. However, it does not explicitly discuss how the framework handles ambiguous or borderline cases.
- Why unresolved: The paper does not provide a clear discussion or examples of how the LOREHM framework handles cases where the harmfulness of a meme is not clear-cut, such as memes that contain both harmful and harmless elements, or memes that could be interpreted differently by different audiences.
- What evidence would resolve it: A detailed analysis of the LOREHM framework's performance on a dataset containing ambiguous or borderline cases, along with a discussion of how the framework handles these cases, would provide insights into its robustness and limitations in handling such scenarios.

## Limitations
- Performance improvements are modest (2.75-4.40% F1 gains) which may not justify the complexity of the two-stage agent architecture
- The knowledge revision mechanism relies heavily on LMM self-reflection, which may introduce hallucinated insights
- The approach assumes multimodal similarity correlates with harmfulness patterns, which may not hold for culturally-specific or context-dependent harmful content

## Confidence
- **High confidence**: The retrieval-augmented voting mechanism and its implementation details (embedding fusion, similarity search)
- **Medium confidence**: The effectiveness of combining outward and inward analysis strategies, as evidenced by improved performance over baselines
- **Low confidence**: Generalization of knowledge revision insights across diverse harmful meme categories, particularly for implicit harm patterns

## Next Checks
1. Ablation study isolating the impact of knowledge revision vs. retrieval augmentation on performance to quantify each component's contribution
2. Cross-dataset evaluation to test generalization of the insight set when trained on one dataset and evaluated on another
3. Human evaluation of LMM-generated insights to assess their validity and identify potential hallucinated patterns