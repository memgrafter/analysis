---
ver: rpa2
title: 'Towards a Research Community in Interpretable Reinforcement Learning: the
  InterpPol Workshop'
arxiv_id: '2404.10906'
source_url: https://arxiv.org/abs/2404.10906
tags:
- interpretable
- learning
- policies
- reinforcement
- explainable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors identify a gap in the reinforcement learning research
  community: the lack of venues dedicated to interpretable and explainable RL, despite
  its growing importance and publications. They propose the InterpPol Workshop as
  the first venue focused on interpretable RL policies.'
---

# Towards a Research Community in Interpretable Reinforcement Learning: the InterpPol Workshop

## Quick Facts
- arXiv ID: 2404.10906
- Source URL: https://arxiv.org/abs/2404.10906
- Authors: Hector Kohler; Quentin Delfosse; Paul Festor; Philippe Preux
- Reference count: 40
- The paper proposes the InterpPol Workshop as the first dedicated venue for interpretable RL research, addressing a gap in the community and establishing research directions through discussions and paper submissions.

## Executive Summary
The paper identifies a significant gap in the reinforcement learning research community: the lack of dedicated venues for interpretable and explainable RL research, despite growing interest and publications. To address this, the authors propose the InterpPol Workshop, the first venue focused specifically on interpretable RL policies. The workshop aims to tackle key challenges in the field including defining interpretability, comparing different policy classes, and developing methods for learning interpretable policies. Scheduled for August 9 at the Reinforcement Learning Conference in Amherst, MA, the workshop seeks to create a community, formalize the problem of learning interpretable policies, and establish research directions through paper submissions and discussions.

## Method Summary
The authors propose creating a dedicated workshop venue as the primary method to address the lack of infrastructure in interpretable RL research. The approach involves organizing a focused event with specific research questions about defining interpretability, comparing policy classes, and learning methods. The workshop structure includes paper submissions, presentations, and discussions designed to systematically advance the field. To ensure sustainability, the organizers plan to establish ongoing community infrastructure including a Google Group and seminar series, along with producing a position paper to formalize research directions. The method relies on bringing together researchers to share knowledge, define common terminology, and develop comparison frameworks for interpretable policies.

## Key Results
- The paper identifies 25 related papers in interpretable RL with average citations of 0.0, suggesting this is an emerging field with limited established literature.
- The workshop proposes specific research questions including the distinction between explainability and interpretability, comparison of policy classes like trees versus programs, and advantages of different learning paradigms.
- The organizers plan to create lasting community infrastructure through a Google Group, seminar series, and position paper to sustain progress beyond the single workshop event.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The InterpPol Workshop addresses a structural gap in the RL research community by creating the first dedicated venue for interpretable RL research.
- Mechanism: By providing a focused forum for researchers working on interpretable RL, the workshop enables knowledge sharing, formalizes research problems, and builds community around shared challenges.
- Core assumption: The lack of dedicated venues has been a significant barrier to progress in interpretable RL research.
- Evidence anchors:
  - [abstract] The authors identify "the lack of venues dedicated to interpretable and explainable RL" and propose InterpPol as "the first venue focused on interpretable RL policies."
  - [section] "There is a need for crystallizing both a community and a set of research directions. As shown in Figure 1, there is a lack dedicated venues for people interested in explainable and interpretable RL"
  - [corpus] Weak evidence - only mentions 25 related papers with average citations of 0.0, suggesting this is an emerging field with limited established literature.
- Break condition: If the workshop fails to attract sufficient submissions or participation, indicating the community gap may be smaller than anticipated.

### Mechanism 2
- Claim: The workshop structure directly addresses key definitional and methodological challenges in interpretable RL.
- Mechanism: By organizing around specific research questions about defining interpretability, comparing policy classes, and learning methods, the workshop provides a framework for advancing the field systematically.
- Core assumption: Current lack of definitions and formal research paradigms is a primary obstacle to interpretable RL progress.
- Evidence anchors:
  - [abstract] "Embracing the pursuit of intrinsically explainable reinforcement learning raises crucial questions: what distinguishes explainability from interpretability?"
  - [section] "The biggest challenge in interpretable RL research remains the lack of definitions and a common paradigm."
  - [corpus] Weak evidence - corpus contains related XAI workshops but lacks specific RL interpretability content, supporting the claim of a gap.
- Break condition: If post-workshop surveys show researchers still struggle with fundamental definitional issues despite the workshop discussions.

### Mechanism 3
- Claim: The workshop creates sustainable infrastructure for ongoing interpretable RL research beyond the single event.
- Mechanism: By establishing a Google Group, planning future seminars, and creating a position paper, the workshop organizers build lasting community infrastructure.
- Core assumption: Sustainable research communities require ongoing communication channels and formal outputs beyond single events.
- Evidence anchors:
  - [section] "We aim to create an open community on Interpretable RL... We will establish an open Google Group on Interpretable RL... we hope to organize a series of online seminars"
  - [abstract] "we propose the first venue dedicated to Interpretable RL: the InterpPol Workshop"
  - [corpus] Weak evidence - related workshops exist but no evidence of long-term community infrastructure in the corpus.
- Break condition: If the Google Group and seminar series fail to gain traction after the workshop concludes.

## Foundational Learning

- Concept: Distinction between explainability and interpretability in RL systems
  - Why needed here: The workshop explicitly addresses this distinction as a core question, indicating it's fundamental to the field
  - Quick check question: Can you articulate the difference between a post-hoc explanation system and an intrinsically interpretable policy?

- Concept: MDPs and state representation learning
  - Why needed here: The workshop questions "Can Markov Decision Processes integrate interpretable state representations?" suggesting this is crucial for the field
  - Quick check question: How does the choice of state representation affect the interpretability of learned policies?

- Concept: Policy classes and their interpretability properties
  - Why needed here: The workshop asks "How to compare different classes of interpretable policies such as trees or programs?" indicating this comparison is central
  - Quick check question: What are the interpretability trade-offs between decision trees, programs, and neural networks as policy representations?

## Architecture Onboarding

- Component map:
  - Workshop infrastructure: submission system (OpenReview), scheduling, speaker coordination
  - Research framework: defined topics/questions, submission guidelines, review process
  - Community platform: Google Group, seminar series planning, position paper drafting
  - Evaluation mechanisms: metrics for interpretability, comparison frameworks, benchmark identification

- Critical path: Submission deadline (April 26) → Review period (3 weeks) → Decision notification (May 20) → Workshop preparation (2 months) → Workshop execution (August 9)

- Design tradeoffs: Academic focus vs practical applications, theoretical rigor vs accessibility to practitioners, single-discipline vs interdisciplinary approaches

- Failure signatures: Low submission numbers (<10 papers), lack of diverse participation, failure to produce position paper or establish Google Group

- First 3 experiments:
  1. Submit a position paper draft to the workshop outlining the current state of interpretable RL research
  2. Implement a comparison framework for two interpretable policy classes (e.g., decision trees vs programs) on a simple benchmark
  3. Design a simulated user study to evaluate interpretability metrics proposed in the literature

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental differences between explainability and interpretability in reinforcement learning, and how do these distinctions impact the design of interpretable policies?
- Basis in paper: [explicit] The paper explicitly raises this question: "what distinguishes explainability from interpretability?"
- Why unresolved: The paper acknowledges this as a crucial question but does not provide a definitive answer, leaving it open for further research.
- What evidence would resolve it: A clear theoretical framework or empirical study comparing explainability and interpretability in RL, potentially through case studies or experiments demonstrating the practical implications of each approach.

### Open Question 2
- Question: How can interpretability be rigorously defined and measured in reinforcement learning policies without relying on user studies?
- Basis in paper: [explicit] The paper asks: "How can we rigorously define and measure interpretability in policies, without user studies?"
- Why unresolved: Existing metrics like simulatability are not specifically developed for RL tasks, and there is a lack of tools for comparing different classes of interpretable policies.
- What evidence would resolve it: Development of domain-specific interpretability metrics or benchmarks for RL, validated through comparison with user study results and practical applicability in various RL scenarios.

### Open Question 3
- Question: What are the advantages and limitations of different reinforcement learning paradigms (e.g., imitation learning, direct RL, evolutionary methods) for learning interpretable policies?
- Basis in paper: [explicit] The paper discusses this: "What advantages does each learning paradigms (e.g imitation learning, direct reinforcement learning, or evolutionary methods) incorporate?"
- Why unresolved: While the paper mentions these approaches, it does not provide a comparative analysis of their effectiveness in producing interpretable policies.
- What evidence would resolve it: Empirical studies comparing the performance, interpretability, and stability of policies learned through different RL paradigms across various benchmarks and real-world applications.

## Limitations

- Limited empirical evidence supporting the claim of a significant community gap, with only 25 related papers cited having zero average citations.
- No concrete metrics or benchmarks provided for evaluating interpretability in RL policies, despite acknowledging this as a key challenge.
- Unclear whether the proposed community infrastructure (Google Group, seminars) will achieve sustainability beyond the initial workshop event.

## Confidence

- Confidence Level: Low on the claim that interpretable RL lacks sufficient venues and community infrastructure. The paper provides limited evidence beyond citing 25 related papers with zero average citations and mentioning related XAI workshops.
- Confidence Level: Medium on the workshop's potential to address definitional challenges. The workshop explicitly targets key questions about interpretability definitions and comparison frameworks, but success depends on participant engagement and follow-through.
- Confidence Level: Medium on the workshop creating sustainable infrastructure. While plans include a Google Group and seminar series, there's no evidence these initiatives will persist beyond the initial event.

## Next Checks

1. **Community Size Validation**: Review submission numbers and attendance statistics from the InterpPol Workshop to assess whether the perceived community gap matches reality.

2. **Definitional Progress Assessment**: Conduct a follow-up survey of workshop participants 3-6 months after the event to determine if fundamental definitional issues have been resolved or clarified.

3. **Infrastructure Sustainability Audit**: Track the activity levels and member growth of the proposed Google Group and seminar series over a 6-month period following the workshop to evaluate long-term community building success.