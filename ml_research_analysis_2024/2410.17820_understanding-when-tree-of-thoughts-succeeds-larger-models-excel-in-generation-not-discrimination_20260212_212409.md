---
ver: rpa2
title: 'Understanding When Tree of Thoughts Succeeds: Larger Models Excel in Generation,
  Not Discrimination'
arxiv_id: '2410.17820'
source_url: https://arxiv.org/abs/2410.17820
tags:
- steps
- generator
- discriminator
- left
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tree of Thoughts (ToT) extends Chain-of-Thought reasoning by combining
  a generator to propose reasoning steps with a discriminator to select the best ones.
  While ToT often outperforms simpler methods like IO prompting and Chain-of-Thought
  reasoning, it is not universally beneficial across all model scales, leaving open
  questions about when it should be applied.
---

# Understanding When Tree of Thoughts Succeeds: Larger Models Excel in Generation, Not Discrimination

## Quick Facts
- arXiv ID: 2410.17820
- Source URL: https://arxiv.org/abs/2410.17820
- Reference count: 40
- Key outcome: Tree of Thoughts success depends primarily on generator quality rather than discriminator quality

## Executive Summary
This paper investigates when Tree of Thoughts (ToT) reasoning succeeds by analyzing the roles of generator and discriminator components separately. The key finding is that generator quality is the primary driver of ToT performance, with larger models producing significantly better intermediate reasoning steps. Surprisingly, scaling the discriminator provides only marginal improvements when the generator is fixed. Models of different scales show similar discrimination capabilities but vary dramatically in their generative performance, suggesting that ToT is most effective when paired with large generators even if using smaller discriminators for computational efficiency.

## Method Summary
The study compares ToT against baseline methods (IO prompting and Chain-of-Thought reasoning) across two reasoning tasks: Game of 24 and Knights and Knaves. Three model scales are tested: smaller (<10B), medium (10B-50B), and larger (>50B) models. The research uses controlled-accuracy oracle discriminators to isolate the effects of generator and discriminator quality. Average success rate serves as the primary metric. Experiments systematically vary generator and discriminator capabilities to determine their relative impact on ToT performance.

## Key Results
- Scaling the generator leads to notable improvements in ToT performance, even when using smaller models as discriminators
- Models across different scales exhibit comparable discrimination capabilities but differ significantly in generative performance
- ToT provides greater benefits for tasks with larger search spaces where baseline methods fail

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling the generator has a more significant impact on ToT success than scaling the discriminator
- Mechanism: Larger generator models produce higher-quality intermediate reasoning steps, which statistically increases the likelihood of selecting viable steps and thus leads to higher success rates
- Core assumption: The generator's ability to produce viable intermediate steps is the primary bottleneck in ToT's effectiveness
- Evidence anchors:
  - [abstract] "Scaling the generator leads to notable improvements in ToT performance, even when using a smaller model as the discriminator"
  - [section 4.1] "While models of different scales exhibit similar discrimination capabilities, yet differ significantly in their generative performance for ToT"
  - [corpus] Weak - corpus lacks direct evidence about generator vs discriminator scaling effects
- Break condition: If the generator can already produce high-quality steps, further scaling provides diminishing returns

### Mechanism 2
- Claim: ToT provides greater benefits for tasks with larger search spaces
- Mechanism: In complex tasks with many possible reasoning paths, ToT's exploration capabilities (via the generator) and selection mechanisms (via the discriminator) can outperform simpler methods like IO and CoT
- Core assumption: Tasks with high decision complexity benefit more from ToT's search-based approach
- Evidence anchors:
  - [section 4.1] "Regarding ToT, for all three LLMs and both datasets, as the performance of the oracle discriminator increases, the overall performance of ToT increases, and substantially outperforms the baseline methods for the Game of 24"
  - [section 4.3] "For the baseline methods, they generally fail at such games with a high complexity level"
  - [corpus] Weak - corpus doesn't provide specific comparisons of ToT effectiveness across tasks of varying complexity
- Break condition: If the task has low decision complexity, simpler methods like IO and CoT may be sufficient

### Mechanism 3
- Claim: The generator's step-wise quality varies significantly across model scales, while the discriminator's performance remains relatively consistent
- Mechanism: Larger models produce more viable intermediate steps at each reasoning step, while models of different sizes show comparable abilities to discriminate between good and bad steps
- Core assumption: Model scale primarily affects generation quality, not discrimination quality
- Evidence anchors:
  - [section 4.1] "the stronger the model, the higher the proportion of viable steps it generates, statistically increasing the likelihood of selecting viable steps"
  - [section 4.2] "the performance differences between models serving as discriminators do not significantly widen with the increasing accuracy of the oracle generator; instead, they remain relatively stable and consistent"
  - [corpus] Weak - corpus lacks detailed analysis of step-wise generation quality across model scales
- Break condition: If the discriminator task becomes more complex, larger models might show superior discrimination abilities

## Foundational Learning

- Concept: Tree Search and Search Space Complexity
  - Why needed here: Understanding how ToT explores the reasoning space is crucial for interpreting the results about generator and discriminator roles
  - Quick check question: What is the branching factor in the Game of 24, and how does it compare to Knights and Knaves?

- Concept: Prompt Engineering and In-Context Learning
  - Why needed here: The paper relies heavily on carefully crafted prompts for both generators and discriminators, which affects the quality of generated and evaluated steps
  - Quick check question: How do the generation and value prompts differ between the Game of 24 and Knights and Knaves tasks?

- Concept: Evaluation Metrics for Reasoning Tasks
  - Why needed here: Understanding average success rate as the primary metric helps interpret the comparative performance of different methods
  - Quick check question: How does average success rate eliminate the advantage of multiple attempts for the same problem?

## Architecture Onboarding

- Component map:
  - Generator -> Discriminator -> Candidate selection -> Next reasoning state

- Critical path:
  1. Generate candidate steps from the current reasoning state
  2. Evaluate all candidate steps
  3. Select top candidates to proceed
  4. Repeat until solution or failure

- Design tradeoffs:
  - Generator scale vs. computational cost: Larger generators improve performance but increase resource requirements
  - Number of candidates vs. evaluation efficiency: More candidates improve exploration but increase computation
  - Search algorithm choice: BFS vs DFS affects exploration strategy and resource usage

- Failure signatures:
  - Poor generation quality: Low proportion of viable steps, especially in early reasoning steps
  - Ineffective discrimination: Random selection among candidates performs similarly to the discriminator
  - Baseline methods outperforming ToT: Indicates task complexity is too low for ToT to provide benefits

- First 3 experiments:
  1. Compare ToT performance with oracle generators of varying accuracy (20%, 40%, 60%, 80%, 100%) to establish the upper bound
  2. Test different model scales as discriminators with fixed oracle generators to isolate discriminator effects
  3. Implement real LLM generators and discriminators to validate oracle-controlled results in practical settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of ToT vary significantly across different task domains beyond mathematical and logical reasoning, such as creative writing or code generation?
- Basis in paper: [inferred] The paper focuses on mathematical reasoning (Game of 24) and natural language-based logical reasoning (Knights and Knaves), but does not explore other domains where ToT might be applied.
- Why unresolved: The study is limited to two specific task types, leaving open whether the observed patterns of generator importance and discriminator irrelevance hold in other domains.
- What evidence would resolve it: Systematic experiments applying ToT to a diverse set of tasks including creative generation, code synthesis, and multi-modal reasoning, comparing performance across different model scales and task complexities.

### Open Question 2
- Question: What is the precise relationship between the complexity of the search space and the effectiveness of ToT compared to simpler methods like IO and CoT?
- Basis in paper: [explicit] The authors note that ToT provides more benefits in highly complex tasks where IO and CoT show poor performance, but do not quantify this relationship or establish a threshold for when ToT becomes beneficial.
- Why unresolved: While the paper demonstrates that ToT outperforms baselines in high-complexity tasks, it does not establish a clear metric or formula for predicting when the search space complexity justifies using ToT over simpler methods.
- What evidence would resolve it: Empirical studies measuring success rates across tasks with systematically varied search space sizes, identifying the complexity threshold where ToT transitions from being detrimental to beneficial.

### Open Question 3
- Question: How do different search algorithms (e.g., BFS vs DFS) and their parameters impact ToT performance across different model scales and task types?
- Basis in paper: [explicit] The paper mentions that ToT can use search algorithms like Breadth-First Search and Depth-First Search but does not systematically compare their effectiveness or explore how parameters like beam width affect performance.
- Why unresolved: The study uses default search settings without exploring how algorithmic choices or parameter tuning might influence the relative importance of the generator versus discriminator components.
- What evidence would resolve it: Controlled experiments comparing ToT performance using different search algorithms and parameter settings across multiple task types and model scales, measuring how these choices interact with generator and discriminator capabilities.

### Open Question 4
- Question: What are the computational efficiency trade-offs when using smaller discriminators with larger generators in ToT compared to using uniform model sizes?
- Basis in paper: [explicit] The authors recommend using a state-of-the-art large model as the generator alongside a smaller model as the discriminator to reduce computational costs, but do not provide empirical measurements of the actual efficiency gains.
- Why unresolved: While the theoretical benefit of using smaller discriminators is proposed, the paper lacks concrete measurements of inference time, memory usage, or overall computational overhead when implementing this asymmetric approach.
- What evidence would resolve it: Detailed benchmarking comparing wall-clock time, GPU memory consumption, and total computational cost between ToT setups with symmetric model sizes versus asymmetric configurations across different hardware platforms.

### Open Question 5
- Question: How does the quality of few-shot examples in prompts affect ToT performance differently for generators versus discriminators across model scales?
- Basis in paper: [inferred] The paper uses few-shot prompts for Game of 24 but does not investigate how the quality or quantity of these examples impacts the relative performance of generators and discriminators, particularly for smaller models that may overfit to examples.
- Why unresolved: The study uses fixed prompt templates without exploring how prompt engineering choices might differentially affect the two components of ToT or how smaller models might be more sensitive to prompt quality than larger ones.
- What evidence would resolve it: Systematic experiments varying the number, diversity, and quality of few-shot examples in ToT prompts, measuring how these changes affect generator versus discriminator performance across different model scales and task types.

## Limitations
- Conclusions based on only three model scales may not capture full scaling behavior
- Analysis limited to two specific reasoning tasks may not generalize to other domains
- Controlled accuracy oracle discriminators may not perfectly represent real-world discriminator performance

## Confidence
- High Confidence: The finding that larger generators produce higher-quality intermediate steps is well-supported by the experimental results across both tasks and all three model scales
- Medium Confidence: The claim that scaling the discriminator provides only marginal gains is supported by the data but could benefit from testing with a wider range of model scales and task complexities
- Low Confidence: The assertion that models across different scales have comparable discrimination capabilities is based on limited experimental evidence and may not hold for more complex discrimination tasks

## Next Checks
1. Cross-Domain Validation: Test the generator vs discriminator scaling hypothesis on a diverse set of reasoning tasks beyond mathematical and logical puzzles to verify generalizability
2. Scaling Law Analysis: Conduct experiments with additional model scales (both smaller and larger than the three tested) to better understand the scaling relationships and identify potential breakpoints
3. Discrimination Task Complexity: Design experiments with varying levels of discrimination complexity to test whether larger models show superior discrimination abilities in more challenging scenarios