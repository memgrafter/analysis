---
ver: rpa2
title: Refining Skewed Perceptions in Vision-Language Contrastive Models through Visual
  Representations
arxiv_id: '2405.14030'
source_url: https://arxiv.org/abs/2405.14030
tags:
- clip
- background
- image
- text
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates spurious correlations in vision-language
  contrastive models (VLCMs) like CLIP. Using Waterbirds and CelebA datasets, the
  authors show that CLIP text embeddings contain non-causal contextual features that
  impair robustness.
---

# Refining Skewed Perceptions in Vision-Language Contrastive Models through Visual Representations

## Quick Facts
- arXiv ID: 2405.14030
- Source URL: https://arxiv.org/abs/2405.14030
- Authors: Haocheng Dai; Sarang Joshi
- Reference count: 33
- This study demonstrates that visual embeddings from CLIP outperform text embeddings for debiasing tasks, with the VisualDistiller framework achieving 82.40% worst-group accuracy on Waterbirds and 83.88% on CelebA without requiring group annotations.

## Executive Summary
This study investigates spurious correlations in vision-language contrastive models (VLCMs) like CLIP, showing that CLIP text embeddings contain non-causal contextual features that impair robustness. Using Waterbirds and CelebA datasets, the authors demonstrate that linear probing with CLIP's visual embeddings can achieve optimal task performance, often matching or exceeding supervised methods. Their VisualDistiller framework uses background images to remove spurious features from visual representations, achieving strong worst-group accuracy without requiring group annotations.

## Method Summary
The paper proposes using CLIP's visual embeddings with linear probing for classification tasks, finding this approach often matches or exceeds supervised methods. The VisualDistiller framework removes spurious features by projecting target image features onto the orthogonal complement of a background feature subspace, using ERM training to learn a projection head. The method requires only background images (no group annotations) and operates by identifying and removing non-causal features associated with contextual information like backgrounds or attributes.

## Key Results
- Visual embeddings from CLIP achieve superior debiasing performance compared to text embeddings
- Linear probing with visual embeddings matches or exceeds supervised methods on benchmark datasets
- VisualDistiller achieves 82.40% worst-group accuracy on Waterbirds and 83.88% on CelebA without requiring group annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP text embeddings encode spurious contextual features, making them unreliable for debiasing.
- Mechanism: Text representations are trained to align images with captions, inheriting contextual associations from the training data (e.g., "cow" paired with "grass"). These non-causal features are then present in the embeddings and can mislead zero-shot classification.
- Core assumption: The pre-training corpus contains spurious correlations that are not filtered out during contrastive training.
- Evidence anchors:
  - [abstract] "Empirical evidence suggests that relying on visual representations from CLIP, as opposed to text embedding, is more effective to refine the skewed perceptions in VLCMs"
  - [section] "CLIP text representations are often tainted by spurious correlations, inherited in the biased pre-training dataset"
  - [corpus] Weak: no direct corpus studies on CLIP's text bias; relies on synthetic experiments.
- Break condition: If training data were perfectly balanced or if the model had explicit de-correlation during training, text embeddings would be less biased.

### Mechanism 2
- Claim: A simple linear probe on CLIP visual embeddings is sufficient to capture task-specific core features.
- Mechanism: The pre-trained visual encoder already contains rich, generalizable features; a linear layer can adapt these to a new classification without retraining the full model.
- Core assumption: CLIP's visual representations are sufficiently expressive to encode the relevant discriminative information for downstream tasks.
- Evidence anchors:
  - [abstract] "linear probing with CLIP's visual embeddings can achieve optimal task performance, often matching or exceeding supervised methods"
  - [section] "we expect the VLCMs to capture a broad spectrum of nuanced visual information and remove the spurious feature by specialized modules"
  - [corpus] Weak: no external corpus evidence that linear probes are optimal for all VLCMs; specific to CLIP here.
- Break condition:

## Foundational Learning
- CLIP model architecture: Vision-language contrastive model that aligns image and text embeddings through contrastive learning; needed to understand the source of representations being used.
- Spurious correlations: Statistical associations between features that are not causally related to the target label; critical for understanding the bias problem being addressed.
- Linear probing: A transfer learning technique where a linear classifier is trained on frozen feature representations; represents the minimal adaptation needed to use pre-trained features.
- Background images in debiasing: Using images with similar contextual features to help identify and remove spurious correlations; key to the VisualDistiller approach.
- Worst-group accuracy: A metric that measures performance on the minority or most challenging subgroups; essential for evaluating bias mitigation effectiveness.
- ERM (Empirical Risk Minimization): A standard training objective that minimizes average loss; used to train the projection head in VisualDistiller.

## Architecture Onboarding

### Component Map
CLIP Visual Encoder -> Linear Probing Module -> Classification Output
VisualDistiller Background Processing -> Projection Head -> Spurious Feature Removal

### Critical Path
Image input → CLIP visual encoder → VisualDistiller projection → Linear classifier → Final prediction
Background images → Feature extraction → Projection head training → Spurious feature subspace removal

### Design Tradeoffs
- Using visual vs. text embeddings: Visual embeddings avoid inherited textual biases but may miss linguistic context
- Linear probing vs. fine-tuning: Linear probing is parameter-efficient but may limit adaptation capacity
- Background image selection: Similar backgrounds help identify spurious features but may introduce new biases if too specific

### Failure Signatures
- Poor worst-group accuracy: Indicates failure to effectively remove spurious features
- Degradation in majority group performance: Suggests over-aggressive feature removal
- Sensitivity to background image selection: Reveals brittleness in the spurious feature identification process

### 3 First Experiments to Run
1. Compare linear probing vs. fine-tuning on visual embeddings for the Waterbirds dataset
2. Evaluate VisualDistiller with different background image selection strategies
3. Test worst-group accuracy sensitivity to the number of background images used

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on specific datasets (Waterbirds and CelebA) with particular types of spurious correlations, limiting generalizability to other bias patterns.
- The analysis relies on synthetic datasets and controlled experiments rather than extensive corpus analysis to demonstrate the presence of spurious correlations in CLIP's training data.
- Critical implementation details are underspecified, including background image selection criteria and exact hyperparameter settings.

## Confidence

**High confidence**: The core finding that linear probing on CLIP visual embeddings can achieve competitive or superior performance compared to supervised methods on the tested datasets. The empirical results are consistent and well-documented.

**Medium confidence**: The claim that text embeddings are more susceptible to spurious correlations than visual embeddings. While supported by the experimental results, this conclusion is based on a limited set of tasks and datasets.

**Low confidence**: The generalizability of the VisualDistiller framework to arbitrary bias patterns beyond the specific spurious correlations present in Waterbirds and CelebA.

## Next Checks
1. Test the VisualDistiller framework on datasets with different types of spurious correlations (e.g., gender bias in medical imaging, age bias in facial recognition) to assess generalizability.
2. Systematically vary the criteria for selecting background images in the VisualDistiller framework to determine how background-image similarity affects performance and identify optimal selection strategies.
3. Conduct a detailed analysis of CLIP's text embeddings on the same tasks to identify specific spurious correlations and compare their nature to those in visual embeddings, providing more direct evidence for the claim about text representation bias.