---
ver: rpa2
title: What is Sentiment Meant to Mean to Language Models?
arxiv_id: '2405.02454'
source_url: https://arxiv.org/abs/2405.02454
tags:
- sentiment
- valence
- classification
- stance
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how large language models (LLMs) interpret
  "sentiment" for text classification. Sentiment is shown to be a confounded measurement
  construct that conflates emotional valence and opinion/stance.
---

# What is Sentiment Meant to Mean to Language Models?

## Quick Facts
- arXiv ID: 2405.02454
- Source URL: https://arxiv.org/abs/2405.02454
- Authors: Michael Burnham
- Reference count: 5
- Key outcome: Sentiment classification in LLMs conflates emotional valence and opinion/stance, with precision in measurement constructs improving accuracy.

## Executive Summary
This study investigates how large language models interpret "sentiment" for text classification, revealing that sentiment is a confounded measurement construct conflating emotional valence and opinion/stance. Through experiments with three LLMs (GPT-4, Claude-3, Llama-3) on two datasets, the author demonstrates that LLMs primarily interpret sentiment as emotional valence rather than opinion. The research concludes that researchers should use precise measurement constructs (valence or stance) instead of the ambiguous "sentiment" label to improve classification accuracy.

## Method Summary
The study tests three LLMs (GPT-4 Turbo, Claude-3 Opus, Llama-3 8B) on two datasets using zero-shot classification prompts for sentiment, valence, and stance classification. The datasets include 2,390 stance-labeled tweets about politicians and 2,000 sentiment-labeled tweets from SemEval-2017. Performance is measured using Matthews Correlation Coefficient (MCC) across all prompt-dataset combinations, with results compared through correlation matrices and confusion analysis.

## Key Results
- LLMs interpret "sentiment" primarily as emotional valence rather than opinion/stance
- Valence classification performs better than sentiment classification for measuring emotional tone
- Stance classification outperforms sentiment classification for opinion mining tasks
- Classification accuracy improves when using precise measurement constructs instead of ambiguous "sentiment" labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs interpret "sentiment" primarily as emotional valence, not opinion/stance.
- Mechanism: The model's internal representations of "sentiment" align more closely with emotional valence features than with stance features due to training data patterns and common usage.
- Core assumption: LLMs learn associations from training data that reflect how "sentiment" is commonly used in text corpora.
- Evidence anchors:
  - [abstract] "I find that sentiment labels most strongly correlate with valence labels."
  - [section] "Results show that LLMs interpret sentiment primarily as emotional valence, not opinion."
  - [corpus] Weak evidence - corpus neighbors are tangentially related but don't directly support the mechanism.

### Mechanism 2
- Claim: Using precise measurement constructs (valence or stance) improves classification accuracy over the ambiguous "sentiment" label.
- Mechanism: Disambiguation reduces the confounded nature of "sentiment" by directing the model's attention to specific features, reducing classification error.
- Core assumption: The ambiguity in "sentiment" causes the model to average across multiple concepts, reducing performance on any single concept.
- Evidence anchors:
  - [abstract] "I further find that classification improves when researchers more precisely specify their dimension of interest rather than using the less well-defined concept of sentiment."
  - [section] "Prompting for stance classification yields the best results in recovering stance labels while prompting for valence produces the worst."
  - [corpus] Weak evidence - corpus neighbors discuss various sentiment analysis approaches but don't directly address the disambiguation effect.

### Mechanism 3
- Claim: Sentiment and valence classification don't produce identical results because LLMs have conflated sentiment with various measurement constructs beyond emotional valence.
- Mechanism: Training data patterns have led LLMs to associate "sentiment" with multiple concepts, creating residual variance even when focusing on valence.
- Core assumption: LLMs learn from human-generated text where "sentiment" is used inconsistently across contexts.
- Evidence anchors:
  - [abstract] "Notably, across all three models we see that instances of positive sentiment are the most likely to disagree with the valence class."
  - [section] "While LLMs generally understand sentiment in terms of emotional valence, the two are not perfectly correlated."
  - [corpus] Weak evidence - corpus neighbors don't provide direct evidence for this specific mechanism.

## Foundational Learning

- Concept: Confounded measurement constructs
  - Why needed here: Understanding that "sentiment" conflates multiple concepts (emotion, opinion, etc.) is fundamental to why disambiguation improves performance
  - Quick check question: What makes "sentiment" a confounded measurement construct, and why does this confound affect LLM performance?

- Concept: Zero-shot classification
  - Why needed here: The paper tests LLMs using prompts without supervised training, so understanding this technique is essential
  - Quick check question: How does zero-shot classification work in LLMs, and why is it particularly relevant to this study's methodology?

- Concept: Matthews Correlation Coefficient (MCC)
  - Why needed here: MCC is the primary performance metric used, and understanding its properties is crucial for interpreting results
  - Quick check question: Why is MCC preferred over F1 or ROC AUC for evaluating classification performance in this context?

## Architecture Onboarding

- Component map:
  Text documents -> Prompt templates -> LLM models -> Classification labels -> MCC evaluation

- Critical path:
  1. Prepare datasets with ground truth labels
  2. Design three distinct prompt templates for each measurement construct
  3. Run each prompt-template combination on each LLM model
  4. Calculate MCC scores for each configuration
  5. Analyze correlation patterns between different classification approaches

- Design tradeoffs:
  - Using multiple LLM models provides robustness but increases computational cost
  - Sampling from SemEval-2017 ensures balanced classes but reduces dataset size
  - Single prompt set per model simplifies comparison but may miss optimal prompts
  - MCC as metric handles class imbalance well but is less intuitive than accuracy

- Failure signatures:
  - Low MCC scores across all models suggest dataset quality issues or prompt design problems
  - Large variance between models indicates task may require larger model capacity
  - Inconsistent correlations between sentiment and valence/stance suggest model-specific interpretations
  - Unexpectedly high performance on one measurement construct may indicate overfitting to that specific prompt

- First 3 experiments:
  1. Run baseline sentiment prompt on all models with both datasets to establish performance floor
  2. Test valence and stance prompts separately on each dataset to measure disambiguation effect
  3. Analyze correlation matrices between different classification approaches to quantify semantic overlap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of sentiment analysis vary across different LLM architectures (e.g., encoder-only vs. decoder-only) and model sizes?
- Basis in paper: [inferred] The paper tests only generative LLMs (GPT-4, Claude-3, Llama-3) but mentions BERT as an alternative in the discussion.
- Why unresolved: The paper does not compare generative LLMs with encoder-only models or explore how model architecture impacts sentiment classification accuracy.
- What evidence would resolve it: Direct comparison of sentiment classification performance between encoder-only models (e.g., BERT) and decoder-only/generative models across multiple datasets and prompt variations.

### Open Question 2
- Question: To what extent does domain-specific pretraining affect LLM performance in distinguishing between valence and stance?
- Basis in paper: [inferred] The paper uses general-purpose LLMs without examining how domain-specific pretraining (e.g., financial or political text) impacts their ability to disambiguate sentiment dimensions.
- Why unresolved: The paper does not test domain-adapted models or explore whether pretraining on specific text domains improves classification accuracy for valence vs. stance.
- What evidence would resolve it: Experiments comparing general-purpose LLMs with domain-specific variants (e.g., FinBERT for financial text) on valence and stance classification tasks.

### Open Question 3
- Question: How does prompt engineering (e.g., few-shot examples, explicit definitions) affect LLM performance in sentiment, valence, and stance classification?
- Basis in paper: [explicit] The paper uses simple prompts without exploring variations in prompt design, though it mentions that researchers should be more precise in specifying their measurement constructs.
- Why unresolved: The paper tests only one prompt format per task and does not investigate how different prompt engineering techniques impact classification accuracy.
- What evidence would resolve it: Systematic testing of different prompt formats (e.g., with/without examples, explicit definitions, different phrasing) across multiple LLMs and datasets.

### Open Question 4
- Question: How do cultural and linguistic differences impact LLM performance in distinguishing between sentiment, valence, and stance across different languages?
- Basis in paper: [inferred] The paper uses English datasets and does not explore multilingual performance or cultural variations in sentiment interpretation.
- Why unresolved: The paper does not test non-English datasets or examine how cultural differences in expressing emotions and opinions affect LLM classification accuracy.
- What evidence would resolve it: Cross-linguistic experiments testing the same classification tasks across multiple languages and cultural contexts, comparing performance across different language models.

## Limitations
- Results rely on three specific LLM models tested with single prompt sets, limiting generalizability
- Only two datasets from specific domains (political tweets, general Twitter sentiment) were used
- No exploration of prompt engineering variations or fine-tuned model comparisons
- Limited to English language datasets without cross-linguistic validation

## Confidence
- High confidence: That sentiment is semantically confounded between emotion and opinion/stance concepts in language models
- Medium confidence: That valence classification performs better than sentiment for emotional tone detection, and stance classification outperforms sentiment for opinion mining
- Medium confidence: That using precise measurement constructs improves classification accuracy, though the magnitude may vary with prompt quality and model architecture

## Next Checks
1. **Prompt robustness testing**: Systematically vary prompt wording, temperature settings, and few-shot examples across all three models to determine if results are robust to prompt engineering or specific to the tested prompts.

2. **Cross-domain validation**: Test the same experimental design on additional datasets from different domains (product reviews, news articles, scientific discourse) and languages to assess generalizability of the sentiment conflation phenomenon.

3. **Fine-tuned model comparison**: Compare zero-shot results with models fine-tuned on sentiment datasets to determine whether training mitigates the conflation effect or simply encodes different assumptions about what "sentiment" means.