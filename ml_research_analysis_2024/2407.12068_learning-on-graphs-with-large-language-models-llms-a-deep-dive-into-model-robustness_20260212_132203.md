---
ver: rpa2
title: 'Learning on Graphs with Large Language Models(LLMs): A Deep Dive into Model
  Robustness'
arxiv_id: '2407.12068'
source_url: https://arxiv.org/abs/2407.12068
tags:
- attacks
- graph
- attack
- robustness
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of Large Language Models
  (LLMs) for graph learning under adversarial attacks, addressing a gap in existing
  evaluations. The authors conduct a comprehensive benchmark that explores structural
  and textual perturbations on LLMs-as-Enhancers and LLMs-as-Predictors for node classification.
---

# Learning on Graphs with Large Language Models(LLMs): A Deep Dive into Model Robustness

## Quick Facts
- arXiv ID: 2407.12068
- Source URL: https://arxiv.org/abs/2407.12068
- Authors: Kai Guo; Zewen Liu; Zhikai Chen; Hongzhi Wen; Wei Jin; Jiliang Tang; Yi Chang
- Reference count: 40
- One-line primary result: LLM-generated features show superior robustness against adversarial attacks compared to shallow features in graph learning tasks

## Executive Summary
This paper investigates the robustness of Large Language Models (LLMs) for graph learning under adversarial attacks, addressing a gap in existing evaluations. The authors conduct a comprehensive benchmark that explores structural and textual perturbations on LLMs-as-Enhancers and LLMs-as-Predictors for node classification. Experiments on multiple datasets reveal that LLM features exhibit superior robustness compared to shallow features, especially under high attack rates. Additionally, GCN-based models demonstrate greater resilience against textual attacks than MLP-based ones. The study also provides insights into the underlying factors contributing to robustness, such as feature distinguishability and node centrality. The benchmark library is made publicly available to support future research.

## Method Summary
The authors evaluate LLM robustness in graph learning by creating a benchmark that tests both structural and textual attacks on LLMs-as-Enhancers and LLMs-as-Predictors for node classification. They use multiple text-attributed graph datasets and apply attacks using PGD/PRBCD for structural perturbations and SemAttack for textual modifications. The evaluation employs two metrics: performance degradation percentage (GAP) and attack success rate (ASR). Models are fine-tuned with consistent hyperparameters across all datasets, and feature quality is assessed using distinguishability metrics like the Davies-Bouldin Index.

## Key Results
- LLM-generated features (SBert, E5, LLaMA) demonstrate significantly better robustness than shallow features (BOW, TF-IDF) under both structural and textual attacks
- GCN-based architectures show greater resilience against textual attacks compared to MLP models
- Fine-tuned LLaMA achieves the lowest attack success rates (2.98% to 6.91%) among all tested models
- Feature distinguishability, measured by lower Davies-Bouldin Index scores, correlates strongly with improved robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated features provide higher feature distinguishability, leading to improved robustness against structural attacks
- Mechanism: Pre-trained LLMs like SBert, E5, and LLaMA encode text attributes into dense vector representations that preserve semantic relationships and class separability. This high-quality feature representation reduces the model's reliance on graph structure, making it less vulnerable to structural perturbations.
- Core assumption: The semantic information captured by LLMs is more robust to structural noise than shallow features like BOW or TF-IDF
- Evidence anchors:
  - [abstract]: "both LLMs-as-Enhancers and LLMs-as-Predictors offer superior robustness against structural and textual attacks"
  - [section 4.5]: "we find that robustness is strongly positively correlated with the quality of features, indicating that higher distinguishability of features leads to stronger robustness"
  - [corpus]: Weak - corpus focuses on adversarial robustness in general but doesn't specifically address feature distinguishability

### Mechanism 2
- Claim: GCN-based architectures demonstrate greater robustness against textual attacks compared to MLP
- Mechanism: GCN aggregates information from neighboring nodes, creating a smoothing effect that helps filter out noise from individual textual perturbations. The graph structure acts as a regularizer, reducing the impact of adversarial text modifications on classification decisions.
- Core assumption: The neighborhood aggregation process in GCN provides natural defense against localized textual perturbations
- Evidence anchors:
  - [abstract]: "LLMs-as-Enhancers demonstrate excellent robustness against textual attacks, with GCN being significantly more robust than MLP"
  - [section 4.2]: "For textual attack, GCN as the victim model is more robust compared to MLP as the victim model"
  - [corpus]: Weak - corpus contains general adversarial attack literature but lacks specific GCN vs MLP comparisons for textual attacks

### Mechanism 3
- Claim: Fine-tuning LLMs on graph-specific data improves robustness against textual attacks
- Mechanism: Fine-tuning adapts the pre-trained language model to the specific distribution and characteristics of graph text attributes, creating features that are more discriminative and less susceptible to adversarial perturbations in the target domain.
- Core assumption: Domain adaptation through fine-tuning creates more robust feature representations for the specific graph learning task
- Evidence anchors:
  - [abstract]: "LLaMA fine-tuned (LLaMA-FT) with LoRa"
  - [section 4.2]: "fine-tuned LLaMA achieves the lowest ASR among all datasets, ranging from 2.98% to 6.91%"
  - [corpus]: Weak - corpus focuses on LLM robustness evaluation but doesn't specifically address fine-tuning for adversarial defense

## Foundational Learning

- Concept: Graph Neural Networks and their aggregation mechanisms
  - Why needed here: Understanding how GCN aggregates neighbor information is crucial for analyzing why it's more robust than MLP against textual attacks
  - Quick check question: How does the aggregation function in GCN differ from a standard MLP, and why might this difference contribute to robustness?

- Concept: Adversarial attack strategies on graph data
  - Why needed here: Knowledge of how structural and textual attacks work is essential for understanding the benchmark design and interpreting results
  - Quick check question: What are the key differences between attacking graph structures versus attacking text attributes, and how do these differences affect the choice of attack methods?

- Concept: Feature representation quality and distinguishability
  - Why needed here: The correlation between feature quality (measured by DBI) and robustness is a key finding that requires understanding of feature space analysis
  - Quick check question: How does the Davies-Bouldin Index (DBI) measure feature distinguishability, and why would lower DBI scores indicate better robustness?

## Architecture Onboarding

- Component map: Text-attributed graph datasets -> Feature extraction (LLM encoders + shallow methods) -> Model training (GCN/MLP) -> Attack application (PGD/PRBCD/SemAttack) -> Performance evaluation (GAP/ASR) -> Analysis (t-SNE/DBI/centrality)
- Critical path: Feature extraction → Model training → Attack application → Performance evaluation → Robustness analysis
- Design tradeoffs:
  - Feature quality vs computational cost: LLM features provide better robustness but require more resources
  - Attack strength vs realism: Higher perturbation rates show clearer robustness patterns but may be less realistic
  - Model complexity vs interpretability: GCN offers better robustness but is more complex to analyze than MLP
- Failure signatures:
  - Sudden accuracy drops indicate successful attacks
  - High standard deviations suggest unstable model behavior
  - Low feature distinguishability (high DBI) correlates with vulnerability
- First 3 experiments:
  1. Compare GCN accuracy using BOW vs SBert features on clean Cora dataset
  2. Apply 5% PGD structural attack on GCN with SBert features and measure GAP
  3. Perform textual attack on GCN with LLaMA features and calculate ASR for different node centrality groups

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can textual and structural attacks be optimally combined to maximize the effectiveness of adversarial attacks on Graph-LLMs?
- Basis in paper: [explicit] The authors mention that combining textual and structural attacks could enhance attack capabilities, but they only provide preliminary results with simple strategies like prioritizing small-degree nodes.
- Why unresolved: The paper acknowledges the need for a combined framework but does not explore advanced strategies or the theoretical underpinnings of how these attacks interact.
- What evidence would resolve it: Experiments comparing various combination strategies (e.g., clustering-based attacks, adaptive sampling) and theoretical analysis of attack synergies.

### Open Question 2
- Question: What specific architectural modifications to LLMs can improve their robustness against adversarial attacks on graph data?
- Basis in paper: [explicit] The authors note that angle-optimized LLMs like Angle-LLaMA show better robustness, suggesting that architecture plays a role in resilience.
- Why unresolved: While the paper identifies that certain LLMs are more robust, it does not investigate why or propose architectural changes to enhance robustness.
- What evidence would resolve it: Comparative studies of different LLM architectures under identical attack scenarios, followed by ablation studies to isolate effective components.

### Open Question 3
- Question: How does the robustness of Graph-LLMs vary across different graph learning tasks such as link prediction and graph classification?
- Basis in paper: [inferred] The authors focus solely on node classification tasks and explicitly state that other tasks like link prediction and graph classification remain unexplored.
- Why unresolved: The paper's benchmark is limited to node classification, leaving a gap in understanding how robustness generalizes to other tasks.
- What evidence would resolve it: Extending the benchmark to include link prediction and graph classification tasks, then analyzing robustness trends across these tasks.

## Limitations
- The study focuses exclusively on node classification, limiting generalizability to other graph learning tasks
- Attack budgets and scenarios may not reflect all real-world threat models
- Analysis of why specific LLM architectures perform better lacks depth in explaining underlying mechanisms

## Confidence

High confidence in the core finding that LLM-generated features demonstrate superior robustness compared to shallow features across both structural and textual attack scenarios.

Medium confidence in the claim that GCN-based models show greater resilience against textual attacks than MLP-based models, as corpus support is limited.

Medium confidence in the mechanism explanations regarding feature distinguishability and its correlation with robustness, which could benefit from more extensive ablation studies.

## Next Checks

1. **Reproduce the correlation analysis**: Independently verify the relationship between feature distinguishability (DBI scores) and robustness metrics across different attack types and model architectures.

2. **Test attack transferability**: Apply adversarial examples crafted for one model architecture (e.g., GCN) to other architectures (e.g., MLP) to understand if the observed robustness differences are transferable across models.

3. **Extend to different graph tasks**: Evaluate the robustness findings on graph-level tasks like graph classification or link prediction to determine if the observed patterns generalize beyond node classification.