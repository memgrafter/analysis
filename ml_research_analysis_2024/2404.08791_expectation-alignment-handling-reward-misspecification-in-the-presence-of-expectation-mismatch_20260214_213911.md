---
ver: rpa2
title: 'Expectation Alignment: Handling Reward Misspecification in the Presence of
  Expectation Mismatch'
arxiv_id: '2404.08791'
source_url: https://arxiv.org/abs/2404.08791
tags:
- reward
- human
- function
- will
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward misspecification in AI agents by introducing
  the Expectation Alignment (EAL) framework. EAL formalizes how human expectations
  about agent behavior translate into reward specifications, identifying misspecification
  when the agent's optimal policy fails to meet these expectations.
---

# Expectation Alignment: Handling Reward Misspecification in the Presence of Expectation Mismatch

## Quick Facts
- arXiv ID: 2404.08791
- Source URL: https://arxiv.org/abs/2404.08791
- Authors: Malek Mechergui; Sarath Sreedharan
- Reference count: 12
- One-line primary result: Introduces the Expectation Alignment (EAL) framework to identify and address reward misspecification when human expectations about agent behavior are not met.

## Executive Summary
This paper addresses reward misspecification in AI agents by introducing the Expectation Alignment (EAL) framework. EAL formalizes how human expectations about agent behavior translate into reward specifications, identifying misspecification when the agent's optimal policy fails to meet these expectations. The framework reveals that there may not exist a single reward function that satisfies expectations in both human and robot task domains. To address this, the authors propose an interactive algorithm that uses the specified reward to infer potential user expectations about system behavior. The algorithm is implemented using linear programming and queries the user when necessary to refine its understanding. Evaluation on standard MDP benchmarks shows the method is computationally efficient, requiring few queries while guaranteeing satisfaction of user expectations, outperforming baseline methods like Inverse Reward Design which often violate expectations.

## Method Summary
The Expectation Alignment (EAL) framework addresses reward misspecification by formalizing human expectations as constraints on state visitation frequencies. The core insight is that humans specify rewards based on their beliefs about agent capabilities, which may differ from reality. The framework uses linear programming to identify potential violations of expectations and iteratively queries users about specific states to refine the understanding of expectations. The algorithm calculates supersets of forbidden and goal states, solves a constrained optimization problem, and updates constraints based on user feedback until an expectation-aligned policy is found or no solution exists. The approach is evaluated on standard MDP benchmarks, showing computational efficiency and superior performance compared to baseline methods.

## Key Results
- Introduces Expectation Alignment (EAL) framework to formalize how human expectations translate into reward specifications
- Proves that no single reward function may satisfy expectations in both human and robot domains (Theorem 1)
- Evaluates on standard MDP benchmarks (Walkway, Obstacles, Four Rooms, Puddle, Maze) with grid sizes 4x4 to 11x11
- Shows computational efficiency with few queries required while guaranteeing satisfaction of user expectations
- Outperforms baseline methods like Inverse Reward Design which often violate user expectations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward misspecification arises from mismatch between user expectations (occupancy frequencies) and the robot's optimal policy under the specified reward.
- Mechanism: The Expectation Alignment (EAL) framework formalizes expectations as constraints on occupancy frequencies and identifies when a policy is "aligned" (satisfies expectations in the robot's domain). It leverages the theory of mind to recognize that humans specify rewards based on beliefs about agent capabilities and expected outcomes.
- Core assumption: Human expectations can be represented as constraints on state visitation frequencies, and the human's model of the task (DH) differs from the robot's true model (DR).
- Evidence anchors:
  - [abstract] "EAL formalizes how human expectations about agent behavior translate into reward specifications, identifying misspecification when the agent's optimal policy fails to meet these expectations."
  - [section] "We will deem a reward function to be misspecified if there is at least one robot optimal policy where at least one expectation set element is unsatisfied."
  - [corpus] Weak evidence - no directly relevant citations found in the corpus.
- Break condition: If the human's expectation set cannot be expressed as constraints on occupancy frequencies, or if the human and robot domains are identical (no mismatch).

### Mechanism 2
- Claim: There may not exist a single reward function that satisfies expectations in both human and robot domains, invalidating approaches that seek a "true" human reward function.
- Mechanism: Theorem 1 constructs an example where states that need to be rewarded in the human model are different from those in the robot model due to domain differences (e.g., different transition dynamics). This proves that no single reward function can satisfy both models simultaneously.
- Core assumption: The human and robot domains differ in ways that affect which states should be rewarded to achieve the desired behavior.
- Evidence anchors:
  - [section] "Theorem 1. There may be human and robot domains, DH and DR, and an expectation set EH, such that one can never come up with a human-sufficient reward function R that is not misspecified with respect to DR, even if one allows the human planning function P H to correspond to some subset of optimal policies in the human model."
  - [corpus] Weak evidence - no directly relevant citations found in the corpus.
- Break condition: If the human and robot domains are identical, or if expectations are so simple that a single reward function happens to satisfy both models.

### Mechanism 3
- Claim: An efficient algorithm can identify expectation-aligned policies by iteratively querying the user about states that might be forbidden or required, using linear programming to calculate supersets of these states.
- Mechanism: The algorithm calculates sets bSF (states never reachable in any optimal policy under human model) and bSG (states always reachable). It then solves an LP that softens constraints on these states, using bookkeeping variables to track violations. Positive bookkeeping variables indicate states to query. After user feedback, states move to hard constraint sets SF* and SG*, and the process repeats.
- Core assumption: The user's expectations can be discovered through queries about specific states, and the human model (DH) is known or can be learned.
- Evidence anchors:
  - [section] "We will claim that a policy satisfies an expectation element for a domain if the corresponding occupancy frequency relation holds for that policy in that domain."
  - [section] "Algorithm 1... guarantees policies that will never result in violation of user policies."
  - [corpus] Weak evidence - no directly relevant citations found in the corpus.
- Break condition: If the user model (DH) is unknown and cannot be learned, or if the expectation set is too complex to be discovered through state-level queries.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and occupancy frequencies
  - Why needed here: The framework is built on MDP theory, and expectations are formalized as constraints on occupancy frequencies. Understanding MDPs is essential to grasp how policies, rewards, and state visitation relate.
  - Quick check question: What is the relationship between a policy's value and its occupancy frequency in an MDP?

- Concept: Linear programming for MDP planning
  - Why needed here: The algorithm uses LP formulations to identify optimal policies and to calculate supersets of forbidden/goal states. Knowing how to express MDP planning as an LP is crucial for understanding the implementation.
  - Quick check question: How can you reformulate the MDP value function in terms of occupancy frequencies and rewards using an LP?

- Concept: Theory of mind and human-AI interaction
  - Why needed here: The framework assumes that humans specify rewards based on their beliefs about the agent (theory of mind). Understanding this concept helps explain why misspecification occurs and how to address it.
  - Quick check question: How might a human's incorrect beliefs about an agent's capabilities lead to reward misspecification?

## Architecture Onboarding

- Component map:
  - Human model (DH) -> Robot model (DR) -> Expectation set (EH) -> Algorithm (LP calculations + user queries) -> Expectation-aligned policy

- Critical path:
  1. Receive human-specified reward RH and human model DH.
  2. Calculate supersets bSF and bSG using LP formulations (Equations 2 and 3).
  3. Solve LP (Equation 4) with soft constraints on bSF and bSG.
  4. If bookkeeping variables are positive, query user about corresponding states.
  5. Update hard constraint sets SF* and SG* based on user responses.
  6. Repeat from step 3 until a policy is found or no more states to query.
  7. Return the expectation-aligned policy or indicate no solution exists.

- Design tradeoffs:
  - Query efficiency vs. completeness: The algorithm aims to minimize queries while guaranteeing satisfaction of expectations. Fewer queries mean less user burden but potentially more computation to identify the right states to query.
  - Computational cost of LP vs. query cost: Solving LPs is computationally expensive, but querying the user is time-consuming and may be impractical. The design balances these costs by using LPs to identify promising states to query.
  - Generality of expectation representation vs. algorithmic tractability: The framework allows for general expectation sets, but the presented algorithm only handles a specific form (forbidden states and goal states). Handling more general expectations may require different algorithms.

- Failure signatures:
  - Algorithm gets stuck in an infinite loop: This could happen if the user provides inconsistent responses to queries, or if the expectation set is unsatisfiable in the robot domain.
  - LP solver fails to find a solution: This could indicate numerical issues with the LP formulation, or that the expectation set is indeed unsatisfiable.
  - Algorithm requires too many queries: This could suggest that the human model (DH) is very different from the robot model (DR), or that the expectation set is complex and hard to discover through state-level queries.

- First 3 experiments:
  1. Implement the LP formulations (Equations 2 and 3) to calculate bSF and bSG for a simple grid world with known forbidden and goal states. Verify that the calculated sets match expectations.
  2. Implement the main LP (Equation 4) and test it on a grid world where the human model differs from the robot model (e.g., different transition dynamics). Verify that the algorithm identifies the correct states to query.
  3. Implement the full algorithm (Algorithm 1) and test it on a grid world with randomly generated forbidden and goal states. Measure the number of queries required and verify that the returned policy satisfies the expectations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Expectation Alignment framework be extended to handle non-Markovian expectations, such as sequential constraints on state visitation?
- Basis in paper: [explicit] The authors mention that non-Markovian expectations could be captured by creating augmented states to track visitation sequences, but do not explore this extension in detail.
- Why unresolved: The paper focuses on Markovian expectations using occupancy frequencies, leaving the treatment of non-Markovian constraints as future work.
- What evidence would resolve it: Empirical results demonstrating the framework's effectiveness on tasks with sequential or temporal dependencies, along with formal proofs of correctness.

### Open Question 2
- Question: What are the limitations of the current query-based algorithm in terms of scalability and performance in large or continuous state spaces?
- Basis in paper: [inferred] The evaluation focuses on grid-world domains, and the authors note that the method requires querying the user, which may become impractical in high-dimensional or continuous domains.
- Why unresolved: The paper does not test the algorithm on domains with large or continuous state spaces, nor does it analyze the computational complexity in such settings.
- What evidence would resolve it: Experiments on continuous or high-dimensional MDPs, along with theoretical analysis of query complexity and runtime scalability.

### Open Question 3
- Question: How can the framework be adapted to handle cases where the human's theory of mind about the robot's planning capabilities is incorrect or overly pessimistic?
- Basis in paper: [explicit] The authors acknowledge that the human's beliefs about the robot's capabilities (captured by DH and P H) may differ from reality, but do not address how to infer or correct these beliefs.
- Why unresolved: The paper assumes access to the human's model (DH) but does not explore methods for learning or validating these beliefs from observations.
- What evidence would resolve it: Empirical results showing the framework's performance when the human's model is incorrect, along with methods for inferring or updating the human's theory of mind.

### Open Question 4
- Question: Can the Expectation Alignment framework be integrated with reinforcement learning approaches to handle reward misspecification in dynamic or uncertain environments?
- Basis in paper: [inferred] The paper focuses on planning in known MDPs, but the authors mention connections to works on minimizing side effects in reinforcement learning, suggesting potential for integration.
- Why unresolved: The paper does not explore how the framework could be adapted for learning in unknown or stochastic environments.
- What evidence would resolve it: Experiments combining the framework with reinforcement learning algorithms, along with theoretical guarantees for convergence and safety in uncertain settings.

## Limitations

- The framework assumes the human model (DH) is known or can be learned, which may not hold in practice
- Algorithm's query efficiency depends heavily on similarity between human and robot domains - large mismatches could lead to prohibitive query numbers
- Does not address non-linear reward functions or continuous state spaces, limiting applicability to complex real-world scenarios

## Confidence

- **High**: Core theoretical claims with formal proofs and clear definitions
- **Medium**: Practical algorithm effectiveness limited to synthetic benchmark domains
- **Low**: Not applicable (no claims rated as low confidence)

## Next Checks

1. Implement the framework on a continuous control task (like a simulated robot arm) where the human model differs from the robot model, and measure query efficiency and policy performance compared to inverse reward design methods.

2. Test the algorithm's robustness when the human model is imperfectly learned (e.g., with noisy state transition estimates) to understand how model uncertainty affects query generation and policy alignment.

3. Extend the framework to handle non-linear reward functions by incorporating function approximation methods, and evaluate whether the theoretical guarantees about expectation satisfaction still hold.