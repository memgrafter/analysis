---
ver: rpa2
title: 'Visualization Literacy of Multimodal Large Language Models: A Comparative
  Study'
arxiv_id: '2407.10996'
source_url: https://arxiv.org/abs/2407.10996
tags:
- visualization
- mllms
- performance
- literacy
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the visualization literacy of multimodal
  large language models (MLLMs) using the VLAT and Mini-VLAT datasets. It compares
  state-of-the-art models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro against human
  baselines.
---

# Visualization Literacy of Multimodal Large Language Models: A Comparative Study

## Quick Facts
- arXiv ID: 2407.10996
- Source URL: https://arxiv.org/abs/2407.10996
- Reference count: 40
- Primary result: MLLMs perform competitively in visualization literacy, excelling in correlation identification and clustering while showing limitations in value retrieval and pie chart interpretation

## Executive Summary
This study evaluates the visualization literacy of state-of-the-art multimodal large language models (MLLMs) using standardized datasets (VLAT and Mini-VLAT). The research compares models including GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro against human baselines on various visualization tasks. Results show that MLLMs achieve competitive performance in many visualization literacy tasks, particularly excelling at identifying correlations and clusters while showing specific limitations in value retrieval and interpreting pie charts. The study provides insights into the current capabilities and limitations of MLLMs in visual data interpretation.

## Method Summary
The study employs a comparative evaluation approach using two standardized visualization literacy datasets (VLAT and Mini-VLAT). The researchers tested three leading MLLMs (GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro) across multiple visualization tasks including correlation identification, clustering, and hierarchical structure recognition. Human performance baselines were established using Amazon Mechanical Turk workers. The evaluation methodology includes systematic error analysis to identify specific failure modes and limitations of MLLMs in visualization interpretation tasks.

## Key Results
- MLLMs outperform humans in identifying correlations and clusters within visualizations
- Humans demonstrate more stable performance across different task types
- MLLMs show significant limitations in value retrieval and pie chart interpretation
- Color-semantic association challenges contribute to interpretation errors

## Why This Works (Mechanism)
Not specified in the paper.

## Foundational Learning
- Multimodal Large Language Models (MLLMs): Why needed - Foundation for understanding the models being evaluated; Quick check - Can identify the models tested (GPT-4o, Claude 3 Opus, Gemini 1.5 Pro)
- Visualization Literacy: Why needed - Core concept being measured; Quick check - Understanding of correlation identification, clustering, and hierarchical structure recognition
- VLAT/Mini-VLAT datasets: Why needed - Standardized evaluation benchmarks; Quick check - Familiarity with the specific visualization tasks used in evaluation

## Architecture Onboarding

**Component Map:** MLLMs -> Visualization Input Processing -> Task Execution -> Output Generation

**Critical Path:** Visual input reception → Feature extraction → Task-specific processing → Response generation

**Design Tradeoffs:** Proprietary architectures limit transparency but enable sophisticated visual processing; balance between visual understanding and textual reasoning capabilities

**Failure Signatures:** 
- Value retrieval failures
- Pie chart interpretation difficulties
- Color-semantic association confusion
- Task-specific performance inconsistencies

**3 First Experiments:**
1. Correlation identification test with scatter plots
2. Clustering task with bar charts
3. Hierarchical structure recognition with tree diagrams

## Open Questions the Paper Calls Out
Not specified in the paper.

## Limitations
- Evaluation relies on proprietary models with opaque architectures
- Human baseline expertise level not clearly specified
- Focus on specific visualization tasks may limit generalizability
- Performance metrics don't account for response time or resource consumption

## Confidence

| Major Claim | Confidence Level |
|-------------|------------------|
| MLLMs' competitive performance in visualization literacy | Medium |
| Human superiority in stability | Medium |
| Specific limitations in value retrieval and pie chart interpretation | High |
| Color-semantic association challenges | Medium |

## Next Checks
1. Replicate the study using open-source MLLM variants to verify if performance patterns hold across different model architectures and training approaches
2. Conduct a controlled study with domain experts as human baselines to establish more rigorous performance comparisons
3. Test model performance across a broader range of visualization types, including time series, geographic maps, and network diagrams, to assess generalizability of the findings