---
ver: rpa2
title: Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms
arxiv_id: '2406.02900'
source_url: https://arxiv.org/abs/2406.02900
tags:
- reward
- training
- should
- learning
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes reward over-optimization in Direct Alignment
  Algorithms (DAAs) for aligning large language models. The authors conduct extensive
  experiments across different model scales (1B, 2.8B, 6.9B) and DAA objectives (DPO,
  IPO, SLiC) using the TL;DR summarization dataset.
---

# Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms

## Quick Facts
- arXiv ID: 2406.02900
- Source URL: https://arxiv.org/abs/2406.02900
- Reference count: 40
- Key outcome: DAAs exhibit over-optimization patterns similar to RLHF, with performance degrading at higher KL budgets and often occurring within the first quarter of training

## Executive Summary
This paper analyzes reward over-optimization in Direct Alignment Algorithms (DAAs) for aligning large language models. The authors conduct extensive experiments across different model scales (1B, 2.8B, 6.9B) and DAA objectives (DPO, IPO, SLiC) using the TL;DR summarization dataset. They find that DAAs exhibit over-optimization patterns similar to classical RLHF methods, with performance degrading at higher KL budgets. Notably, over-optimization often occurs within the first quarter of training, suggesting the problem is more severe than in RLHF. The authors provide theoretical analysis showing that DAAs are under-constrained optimization problems that can place probability mass on out-of-distribution responses.

## Method Summary
The authors train Direct Alignment Algorithms using DPO, IPO, and SLiC objectives on Pythia family models (1B, 2.8B, 6.9B scales) with the TL;DR summarization dataset. They use supervised fine-tuning as the reference policy and optimize for one epoch with varying KL budgets (β values). Training uses RMSProp optimizer with learning rate 0.5×10^-6 and batch size 128. Evaluation includes GPT-4 win rates on held-out prompts, KL divergence metrics, length correlations, and reward model accuracy.

## Key Results
- DAAs exhibit over-optimization patterns similar to RLHF with hump-shaped performance curves
- Over-optimization typically occurs within the first quarter of training for higher β values
- Model size and KL budget affect length extrapolation behavior
- Little correlation between DAA reward accuracy and downstream performance

## Why This Works (Mechanism)

### Mechanism 1
DAAs suffer over-optimization due to an under-constrained reward modeling objective that allows placing probability mass on OOD responses. The DAA loss function can have multiple global optima because the query matrix Q has a non-trivial null space, allowing the learned policy to assign high likelihood to out-of-distribution responses not present in the preference dataset.

### Mechanism 2
Smaller models and lower KL budgets lead to stronger length extrapolation, exacerbating over-optimization. Under limited capacity (either from model size or KL budget), the model overfits to simpler features like response length that are easier to optimize but don't correlate with true quality.

### Mechanism 3
Decreasing likelihoods of both preferred and dis-preferred responses during training is expected and necessary but exhibits non-linear over-optimization dynamics. The DAA implicit reward represents a forward KL divergence between reference and optimized policies, causing both response types' likelihoods to decrease while OOD responses gain probability mass.

## Foundational Learning

- Concept: Bradley-Terry model for pairwise preference modeling
  - Why needed here: The DAA objectives are derived assuming preferences follow the Bradley-Terry model
  - Quick check question: What probability distribution does the Bradley-Terry model assume for pairwise comparisons between responses y1 and y2 given prompt x?

- Concept: KL divergence regularization in language model alignment
  - Why needed here: DAAs use KL divergence to constrain how far the optimized policy can deviate from the reference policy
  - Quick check question: How does the β parameter in DAA objectives control the trade-off between exploiting preferences and staying close to the reference distribution?

- Concept: Over-optimization in reward learning
  - Why needed here: Understanding why learned reward models can lead to degraded performance despite increasing reward scores
  - Quick check question: What is the difference between optimizing a proxy reward model versus the true underlying reward function in alignment settings?

## Architecture Onboarding

- Component map: Preference dataset -> Reference policy (πref) -> DAA objective -> Evaluation (GPT-4 win rates, KL divergence) -> Scaling analysis
- Critical path: 1) Prepare preference dataset and reference model, 2) Choose DAA objective and set β, 3) Train policy using DAA objective for one epoch, 4) Evaluate win rates and KL divergence at checkpoints, 5) Analyze over-optimization patterns across configurations
- Design tradeoffs: Stronger KL constraints prevent over-optimization but may limit preference satisfaction; weaker KL constraints allow better preference satisfaction but increase over-optimization risk
- Failure signatures: Hump-shaped performance curves where win rates initially increase then decrease with higher β; best performance achieved early in training for high β settings; strong length correlation without corresponding quality improvement
- First 3 experiments: 1) Train DPO with β = 0.1 on 2.8B model, monitor win rates and KL at 25%, 50%, 75%, and 100% of epoch; 2) Compare DPO, IPO, and SLiC performance at same β = 0.1 on 6.9B model; 3) Train 1B model with various β values (0.01, 0.1, 0.5) to observe scaling effects

## Open Questions the Paper Calls Out

### Open Question 1
How does the over-optimization problem scale with model size beyond 6.9B parameters? The authors note computational limitations prevented testing larger models. Additional experiments with larger models would show whether over-optimization trends continue, plateau, or reverse.

### Open Question 2
What is the relationship between KL divergence and over-optimization across different DAA objectives? While empirical differences are observed, the theoretical reasons for why certain objectives exhibit stronger over-optimization are not fully developed.

### Open Question 3
Can online data collection mitigate the over-optimization problem in DAAs? The authors mention online data has proven critical for improving performance but don't explore this in their study.

## Limitations
- Theoretical framework relies on assumptions about preference data sparsity and Bradley-Terry model validity
- Experimental scope limited to TL;DR summarization dataset with unknown human annotation quality
- Scaling law generalization beyond tested 1B-6.9B range remains uncertain
- Implementation details for some metrics and regularization techniques not fully specified

## Confidence

**High Confidence:**
- DAAs exhibit over-optimization patterns similar to RLHF with hump-shaped performance curves
- Over-optimization typically occurs within the first quarter of training for higher β values
- Model size and KL budget affect length extrapolation behavior
- Little correlation between DAA reward accuracy and downstream performance

**Medium Confidence:**
- The under-constrained optimization problem explanation for over-optimization
- Forward KL divergence interpretation of expected implicit rewards
- Length extrapolation as a primary mechanism for over-optimization
- Theoretical scaling laws connecting model size and KL budget effects

**Low Confidence:**
- Claims about fundamental flaws in DAAs requiring careful β tuning
- Generalization of scaling laws to models beyond 6.9B parameters
- Effectiveness of proposed length regularization techniques

## Next Checks

**Validation Check 1:** Train DPO and IPO objectives on TL;DR dataset with 1B, 2.8B, and 6.9B models across β values (0.01, 0.1, 0.5) for one epoch. Measure win-rates and KL divergence at 25%, 50%, 75%, and 100% completion to verify hump-shaped curves and early over-optimization patterns.

**Validation Check 2:** Apply the same DAA training and evaluation pipeline to a different summarization dataset (e.g., CNN/DailyMail or XSum) to test whether over-optimization patterns and scaling laws hold across different data distributions and quality levels.

**Validation Check 3:** Test the scaling law predictions by training DAAs on models in the 10B-30B parameter range (if available) with the same β values and monitoring length correlations and win-rates to validate whether scaling trends continue or change at larger scales.