---
ver: rpa2
title: Not All Votes Count! Programs as Verifiers Improve Self-Consistency of Language
  Models for Math Reasoning
arxiv_id: '2410.12608'
source_url: https://arxiv.org/abs/2410.12608
tags:
- prove
- zero-shot
- wang
- reasoning
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PROVE, a framework that leverages translated
  Python programs as verifiers to improve mathematical reasoning in language models.
  Unlike simple majority voting, PROVE filters out solutions whose program-generated
  outputs are inconsistent with the original solutions before aggregating answers.
---

# Not All Votes Count! Programs as Verifiers Improve Self-Consistency of Language Models for Math Reasoning

## Quick Facts
- arXiv ID: 2410.12608
- Source URL: https://arxiv.org/abs/2410.12608
- Reference count: 13
- Primary result: PROVE improves mathematical reasoning accuracy by filtering solutions through program verification before majority voting, with up to 18% gains on GSM8K and 8% on MATH-500.

## Executive Summary
This paper introduces PROVE, a framework that enhances mathematical reasoning in language models by using translated Python programs as verifiers. Unlike simple majority voting, PROVE filters out solutions whose program-generated outputs are inconsistent with the original solutions before aggregation. The framework was evaluated across 13 models ranging from 0.5B to 13B parameters on eight mathematical benchmarks. Results show PROVE consistently outperforms vanilla majority voting, achieving significant accuracy improvements, particularly for smaller models. The approach demonstrates effectiveness in filtering out calculation and semantic errors, leading to more accurate reasoning outcomes.

## Method Summary
PROVE operates by first generating multiple plans and solutions for a math problem using Chain-of-Thought prompting. Each solution is then translated into a Python program by a separate translation model (typically Phi-3-mini-4k-instruct or GPT-4o for harder tasks). The Python programs are executed, and their outputs are compared against the original solutions using answer extraction prompts. Only solutions that pass this verification step undergo majority voting to determine the final answer. The framework was tested on 13 LLMs across eight mathematical reasoning datasets, comparing PROVE against vanilla self-consistency methods.

## Key Results
- PROVE achieves up to 18% improvement on GSM8K and 8% on MATH-500 compared to vanilla majority voting
- Smaller models (0.5B-3B parameters) show the largest performance gains, benefiting more from verification filtering
- PROVE demonstrates consistent improvements across all eight mathematical reasoning datasets tested
- The framework shows effectiveness in filtering out calculation and semantic errors in solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PROVE filters out solutions with calculation or semantic errors before aggregation, improving accuracy.
- Mechanism: For each natural language solution, a Python program is generated and executed. If the program output matches the solution, it passes verification; otherwise, it is filtered out. Only verified solutions undergo majority voting.
- Core assumption: The generated Python program accurately reflects the reasoning in the natural language solution, and any mismatch indicates an error in the solution.
- Evidence anchors:
  - [abstract] "PROVE filters out solutions whose corresponding program output is inconsistent with the generated solution, aggregating only those that pass verification."
  - [section] "We execute the Python program pi to obtain the output ˆpi, which is then used to verify the answer ˆsi from the generated solution si. The answer ˆsi is extracted using the answer extraction prompting method proposed by Kojima et al. (2022): 'Therefore, the answer (arabic numerals) is'. We consider the answer ˆsi valid if it matches the program output ˆpi."
- Break condition: If the Python program generation is unreliable or fails to capture the solution's logic, valid solutions might be incorrectly filtered out.

### Mechanism 2
- Claim: Smaller models benefit more from PROVE because they are more prone to errors that the verification can catch.
- Mechanism: The verification step removes incorrect solutions generated by smaller, less accurate models, increasing the proportion of correct solutions in the final aggregation.
- Core assumption: Smaller models generate more errors that the program verification can detect, and these errors would otherwise negatively impact the majority voting outcome.
- Evidence anchors:
  - [section] "PROVE provides larger performance gains for smaller models. In our comparison of various model families including Qwen-2, Gemma-2, Llama-2, and Llama-3.2, we observed that smaller models tend to show greater performance improvements than their larger counterparts."
- Break condition: If the program verification step is imperfect and filters out some correct solutions, the relative benefit for smaller models could be reduced or even become negative.

### Mechanism 3
- Claim: Using a separate, more capable model for program generation improves verification quality without needing to fine-tune the main reasoning model.
- Mechanism: A dedicated, often larger, model (e.g., Phi-3-mini-4k-instruct or GPT-4o for harder tasks) translates natural language solutions into Python programs, leveraging its code generation strength for verification.
- Core assumption: The translation model is sufficiently capable of accurately converting the reasoning steps into executable Python code, even if the main reasoning model is smaller or less capable.
- Evidence anchors:
  - [section] "To extract the answer from the generated solutions, we rely on Phi-3-mini-4k-instruct. We also standardize the use of Phi-3-mini-4k-instruct to translate the plan and solution into Python programs for verification."
  - [section] "For the more challenging MATH dataset, we use GPT-4o (2024-02-15-preview) to translate the plan and solution into Python code."
- Break condition: If the translation model introduces errors in the program generation, the verification becomes unreliable.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Understanding how LLMs generate step-by-step reasoning is crucial for appreciating why verification is needed.
  - Quick check question: What is the primary benefit of using Chain-of-Thought prompting in mathematical reasoning tasks?

- Concept: Program synthesis from natural language
  - Why needed here: The core of PROVE relies on translating reasoning steps into executable code; understanding this process is key to the mechanism.
  - Quick check question: Why is translating a natural language solution into a Python program an effective way to verify its correctness?

- Concept: Majority voting and self-consistency
  - Why needed here: PROVE builds on self-consistency by adding a verification step before aggregation; knowing how self-consistency works is foundational.
  - Quick check question: How does self-consistency improve LLM performance on reasoning tasks compared to single sampling?

## Architecture Onboarding

- Component map: Problem → Reasoning Model → Translation Model → Verifier → Aggregator → Final Answer
- Critical path: Problem → Reasoning Model → Translation Model → Verifier → Aggregator → Final Answer
- Design tradeoffs:
  - Using a separate translation model adds complexity but improves verification accuracy.
  - Sampling more solutions increases computational cost but improves the chances of finding correct answers.
  - Relying on Python execution assumes the problem can be solved programmatically.
- Failure signatures:
  - If translation model fails: Many valid solutions get filtered out, performance drops.
  - If program execution fails (e.g., runtime error): Solution is marked invalid, even if reasoning was correct.
  - If majority voting over invalid solutions is triggered too often: Overall accuracy decreases.
- First 3 experiments:
  1. Test PROVE with a small model (e.g., 1B) on a simple dataset (e.g., SingleArith) to verify the core mechanism works.
  2. Compare PROVE vs. vanilla self-consistency on a mid-sized model (e.g., 7B) across multiple datasets to measure performance gain.
  3. Evaluate the impact of the translation model by swapping Phi-3-mini-4k-instruct with a larger model on a challenging dataset like MATH.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the authors acknowledge the need for more capable translation models for challenging datasets like MATH and suggest potential improvements in the verification framework.

## Limitations
- The framework's effectiveness depends heavily on the quality of program translation, which is not extensively validated
- Using Phi-3-mini-4k-instruct as both the translation model and a tested reasoning model creates potential circular dependencies
- Performance gains for larger models (13B parameters) are relatively modest compared to smaller models, suggesting diminishing returns
- The evaluation focuses primarily on accuracy metrics without examining computational efficiency or cost implications

## Confidence
- High Confidence: PROVE consistently improves performance over vanilla majority voting across diverse model sizes and datasets, with particularly strong gains for smaller models.
- Medium Confidence: PROVE's larger performance gains on smaller models stem from their higher error rates that verification can catch.
- Low Confidence: The claim that PROVE eliminates the need for fine-tuning is overstated.

## Next Checks
1. **Translation Quality Validation:** Conduct a detailed analysis of the translation model's performance by manually evaluating a sample of translated programs to measure true positive and false positive rates.
2. **Cross-Model Translation Testing:** Evaluate PROVE's performance when using different translation models for different reasoning models, testing whether using GPT-4o for all translations improves or degrades results.
3. **Error Type Analysis Across Datasets:** Perform a comprehensive error analysis categorizing incorrect predictions by type (calculation errors, missing steps, semantic misunderstandings) across all eight datasets to understand which error types PROVE most effectively addresses.