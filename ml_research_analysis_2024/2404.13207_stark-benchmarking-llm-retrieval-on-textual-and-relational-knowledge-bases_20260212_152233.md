---
ver: rpa2
title: 'STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases'
arxiv_id: '2404.13207'
source_url: https://arxiv.org/abs/2404.13207
tags:
- queries
- query
- information
- textual
- product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STARK, a large-scale benchmark for retrieval
  tasks on semi-structured knowledge bases (SKBs) that integrate both textual and
  relational information. The authors design a novel pipeline to synthesize realistic
  user queries and automatically generate ground-truth answers by disentangling relational
  and textual requirements.
---

# STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases

## Quick Facts
- arXiv ID: 2404.13207
- Source URL: https://arxiv.org/abs/2404.13207
- Reference count: 40
- Current LLM retrieval systems achieve Hit@1 scores of only 12.75%-42.08% on semi-structured knowledge base queries

## Executive Summary
This paper introduces STARK, a large-scale benchmark for retrieval tasks on semi-structured knowledge bases (SKBs) that integrate both textual and relational information. The authors design a novel pipeline to synthesize realistic user queries and automatically generate ground-truth answers by disentangling relational and textual requirements. STARK spans three domains—product search, academic paper search, and precision medicine—and includes both synthesized and human-generated queries validated through rigorous human evaluation. Experiments on current retrieval systems show significant performance gaps, with Hit@1 scores ranging from 12.75% to 42.08% across datasets, highlighting the challenges in handling complex semi-structured queries and the need for more capable retrieval systems.

## Method Summary
The benchmark constructs three SKBs (STARK-AMAZON, STARK-MAG, STARK-PRIME) from public data sources, then synthesizes queries by entangling relational and textual information during generation and disentangling them during answer filtering. The pipeline samples relational templates, grounds them with specific entities, extracts textual properties, and uses multiple LLMs to generate queries and verify answers. The benchmark evaluates five classes of retrieval models (sparse, small dense, LLM-based dense, multivector, and LLM rerankers) using standard metrics like Hit@k, Recall@k, and MRR across synthesized and human-generated query sets.

## Key Results
- BM25 and dense retrievers (ada-002, voyage-l2-instruct, LLM2Vec) achieve Hit@1 scores of 12.75%-32.42% across datasets
- LLM rerankers (Claude3, GPT-4) significantly outperform retrievers with Hit@1 scores of 28.21%-42.08%
- Multi-ada-002 and ColBERTv2 show improved performance through fine-grained document representations
- STARK-PRIME presents the greatest challenge with Hit@1 scores of only 12.75%-29.56% even for rerankers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthesizing queries by entangling relational and textual information during generation, then disentangling them during answer filtering, allows accurate ground truth construction from millions of candidates.
- Mechanism: The pipeline samples a relational template (e.g., "(product) belongs to <brand>"), grounds it with a specific entity (e.g., "belongs to Radio Flyer"), then extracts textual properties from the resulting candidate entities. Two LLMs generate queries from these combined inputs, and multiple LLMs verify candidate satisfaction of textual requirements.
- Core assumption: LLMs can accurately verify whether candidate entities satisfy complex textual requirements extracted from documents.
- Evidence anchors:
  - [abstract] "We develop a novel pipeline that synthesizes the SKB queries and automatically generates the ground truth answers. The key idea is to entangle relational and textual information during synthesis and disentangle them during answer filtering."
  - [section] "Finally, we employ multiple LLMs to verify if the candidates from the first step meet the extracted textual properties. Only candidates passing all LLM verifications are included in the final ground truth set."
  - [corpus] Weak evidence - the corpus mentions related retrieval approaches but not this specific disentanglement mechanism.
- Break condition: If LLMs cannot accurately verify textual satisfaction, the ground truth filtering will produce incorrect answers.

### Mechanism 2
- Claim: Using multiple different LLM models for different steps reduces bias and improves benchmark quality.
- Mechanism: Different LLM versions are used for extracting textual requirements (Step 2), combining requirements (Step 3), and filtering answers (Step 4). This prevents overfitting to a single model's biases.
- Core assumption: Different LLM models have complementary strengths that improve overall quality when combined.
- Evidence anchors:
  - [abstract] "We use two LLMs to synthesize queries from textual properties and relational requirements, enhancing diversity and reducing bias arise from relying on a single LLM."
  - [section] "We employ multiple LLMs to verify if the candidates from the first step meet the extracted textual properties."
  - [corpus] Weak evidence - the corpus shows related retrieval work but doesn't discuss multi-model approaches.
- Break condition: If models have correlated biases, using multiple models may not reduce overall bias.

### Mechanism 3
- Claim: Multivector retrievers and LLM rerankers significantly improve retrieval accuracy by capturing finer-grained semantic relationships.
- Mechanism: Multi-ada-002 splits documents into overlapping chunks and embeds them, while ColBERTv2 uses token-level embeddings. Rerankers like Claude3 and GPT-4 re-evaluate top candidates using deeper contextual understanding.
- Core assumption: Fine-grained document representations and contextual reranking can capture semantic nuances that dense embeddings miss.
- Evidence anchors:
  - [section] "For multivector retrievers, we found that multi-ada-002 generally outperforms ada-002, indicating that using multiple vectors per document enhances retrieval effectiveness. Similarly, fine-grained representation allows ColBERTv2 to capture subtle semantic nuances between queries and documents."
  - [section] "However, both GritLM-7b and ColBERTv2 generally underperform compared to the rerankers on the random split, especially in terms of Hit@k metrics. This suggests that while these dense retriever models effectively capture semantic information, they may not fully grasp the nuanced relevance judgments required for top-tier retrieval performance."
  - [corpus] Moderate evidence - the corpus mentions related retrieval approaches but doesn't specifically discuss multivector or reranking improvements.
- Break condition: If computational costs of fine-grained representations outweigh accuracy gains, simpler approaches may be preferred.

## Foundational Learning

- Concept: Semi-structured knowledge bases (SKBs) combine unstructured text documents with structured relational graphs
  - Why needed here: Understanding how textual and relational information integrate is fundamental to designing retrieval systems for STARK
  - Quick check question: What are the two main components of an SKB, and how do they differ in representation?

- Concept: Multi-hop reasoning in knowledge graphs
  - Why needed here: Many STARK queries require traversing multiple relations to find answers (e.g., institution → author → paper)
  - Quick check question: How many relation hops are typically required for academic paper queries in STARK-MAG?

- Concept: Dense vs sparse retrieval methods
  - Why needed here: The benchmark compares traditional BM25 with dense retrievers like DPR and ANCE, requiring understanding of their tradeoffs
  - Quick check question: What is the key difference between how BM25 and dense retrievers like DPR represent documents?

## Architecture Onboarding

- Component map: Public source data → SKB construction → Entity/relation extraction → Document integration → Query synthesis → Answer filtering → Benchmark evaluation → System comparison
- Critical path: Query synthesis → Answer filtering → Benchmark evaluation → System comparison
- Design tradeoffs: Multi-hop queries provide realistic complexity but increase computational cost; using multiple LLMs improves quality but adds latency
- Failure signatures: Low Hit@1 scores indicate retriever inability to find correct answers; high MRR but low Hit@1 suggests reranking issues; slow latency indicates computational bottlenecks
- First 3 experiments:
  1. Compare BM25 vs ada-002 on STARK-AMAZON to establish baseline performance gap
  2. Test multi-ada-002 vs single ada-002 to verify multivector benefits
  3. Evaluate Claude3 reranker on top-20 ada-002 results to measure reranking impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the current LLM retrieval systems handle complex queries involving multiple relational hops in STARK-MAG and STARK-PRIME?
- Basis in paper: [explicit] The paper mentions that STARK-MAG and STARK-PRIME involve multi-hop relational queries, and the experiments show significant performance gaps for current retrieval systems.
- Why unresolved: The paper provides overall performance metrics but does not delve into the specific challenges or failure modes of handling multi-hop relational queries. It remains unclear how the systems process and reason about these complex relational structures.
- What evidence would resolve it: Detailed analysis of system performance on individual query templates with varying numbers of hops, case studies of specific multi-hop queries, and comparison of performance across different types of relational structures.

### Open Question 2
- Question: What is the impact of combining textual and relational information on retrieval accuracy in STARK?
- Basis in paper: [explicit] The paper emphasizes that STARK queries involve both textual and relational information, and the retrieval systems must handle their interdependence.
- Why unresolved: While the paper presents overall performance metrics, it does not isolate the contribution of textual vs. relational information to retrieval accuracy. It is unclear how the systems perform when only textual or only relational information is provided.
- What evidence would resolve it: Ablation studies comparing retrieval accuracy with different combinations of textual and relational information, analysis of the relative importance of textual and relational features in the retrieval process.

### Open Question 3
- Question: How do the synthesized queries in STARK compare to human-generated queries in terms of difficulty and diversity?
- Basis in paper: [explicit] The paper mentions that synthesized and human-generated queries are included in STARK, and human evaluation was conducted to validate the quality of synthesized queries.
- Why unresolved: The paper provides some statistics on query length and answer length, but does not directly compare the difficulty and diversity of synthesized vs. human-generated queries. It is unclear if one type of query is more challenging for retrieval systems.
- What evidence would resolve it: Direct comparison of retrieval system performance on synthesized vs. human-generated queries, analysis of query complexity and diversity metrics for both types of queries, case studies of specific queries that highlight differences between the two types.

## Limitations

- Benchmark's reliance on LLM-based ground truth generation introduces potential brittleness if LLM behavior changes across versions
- Significant performance gap between retrievers and rerankers suggests the benchmark may be pushing current technology limits
- Evaluation focuses primarily on retrieval accuracy metrics without addressing fairness or bias considerations

## Confidence

- **High confidence**: The benchmark construction methodology and dataset characteristics are well-documented with clear validation procedures
- **Medium confidence**: The retrieval performance results are reproducible but may not generalize to different LLM versions or domain-specific contexts
- **Low confidence**: Claims about specific model superiority require independent verification given the rapid evolution of LLM capabilities

## Next Checks

1. **Replication with alternate LLM versions**: Rerun the query synthesis and answer filtering pipeline using different LLM versions to assess robustness of ground truth generation
2. **Cross-domain generalization test**: Apply the best-performing models from STARK-AMAZON to STARK-PRIME to verify whether retrieval patterns transfer across domains
3. **Bias and fairness audit**: Analyze the SKBs for representational bias and evaluate whether retrieval performance disparities exist across different entity types or demographics