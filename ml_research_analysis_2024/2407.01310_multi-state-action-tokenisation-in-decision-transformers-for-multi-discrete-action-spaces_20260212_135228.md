---
ver: rpa2
title: Multi-State-Action Tokenisation in Decision Transformers for Multi-Discrete
  Action Spaces
arxiv_id: '2407.01310'
source_url: https://arxiv.org/abs/2407.01310
tags:
- action
- transformer
- actions
- decision
- m-sat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Decision Transformers struggle with multi-discrete action spaces
  in image-based environments due to sub-optimal representation of action relationships.
  This paper proposes Multi-State Action Tokenisation (M-SAT), which tokenises multi-discrete
  actions at the individual action level and incorporates auxiliary state embeddings.
---

# Multi-State-Action Tokenisation in Decision Transformers for Multi-Discrete Action Spaces

## Quick Facts
- arXiv ID: 2407.01310
- Source URL: https://arxiv.org/abs/2407.01310
- Reference count: 35
- Primary result: M-SAT outperforms baseline Decision Transformer on ViZDoom environments by better representing multi-discrete action relationships

## Executive Summary
Decision Transformers struggle with multi-discrete action spaces in image-based environments due to sub-optimal representation of action relationships. This paper proposes Multi-State Action Tokenisation (M-SAT), which tokenises multi-discrete actions at the individual action level and incorporates auxiliary state embeddings. M-SAT improves performance on challenging ViZDoom environments (Deadly Corridor and My Way Home) by enabling better action-level interpretability and visibility within attention layers. Results show M-SAT outperforms the baseline Decision Transformer in both environments, achieving higher rewards with lower variance. Notably, M-SAT solves My Way Home while the baseline fails, and demonstrates sample efficiency with shorter context lengths. Removing positional encoding further improves M-SAT's performance in some cases, highlighting the importance of state-action associations for goal-based tasks.

## Method Summary
The paper proposes Multi-State Action Tokenisation (M-SAT) as a modification to Decision Transformers for handling multi-discrete action spaces. M-SAT works by expanding multi-discrete actions into individual action tokens and pairing each with the preceding state embedding before tokenization. The approach uses a CNN to process image states into embeddings, then concatenates these with one-hot encoded individual actions before passing through an MLP to generate action tokens. These tokens, along with state and return-to-go tokens, are fed into a standard transformer architecture. The model is trained on expert data generated using Asynchronous-PPO for 5 epochs with batch size 128 and learning rate 0.005.

## Key Results
- M-SAT achieves higher rewards than baseline Decision Transformer on both Deadly Corridor and My Way Home environments
- M-SAT solves My Way Home task while baseline fails, demonstrating superior sample efficiency with shorter context lengths
- Removing positional encoding improves M-SAT performance in some cases, particularly in environments with high state similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding multi-discrete actions into individual action tokens increases their visibility and interaction within transformer attention layers, enabling the model to better learn action relationships.
- Mechanism: By tokenizing each component of a multi-discrete action separately (e.g., turning left, moving forward, shooting) and pairing it with the preceding state embedding, the transformer attention heads can form more granular state-action and action-action relationships. This contrasts with baseline approaches that compress the entire multi-discrete action into a single token before attention layers process it.
- Core assumption: The transformer's attention mechanism is sufficiently powerful to learn meaningful relationships between individual action tokens when given the opportunity, and that this granularity improves learning over compressed representations.
- Evidence anchors:
  - [abstract] "Our approach involves two key changes: disentangling actions to the individual action level and tokenising the actions with auxiliary state information."
  - [section 4.1] "The tokenisation process effectively encodes state or environment features with each action... The causal nature of the underlying GPT model means current timesteps refer to historical tokens or context to learn which tokens to attend to when generating a new action."
  - [corpus] Weak - related papers focus on different aspects (behavior generation, attention for combinatorial spaces) but don't directly test this specific multi-token mechanism.
- Break condition: If the attention mechanism cannot effectively learn from the increased token granularity, or if the state-action pairing doesn't provide useful context, performance may degrade or remain similar to baseline.

### Mechanism 2
- Claim: Injecting auxiliary state information into action tokens provides richer context that helps the model understand state-action associations, particularly important for goal-based tasks in image-based environments.
- Mechanism: Each individual action token is concatenated with the state embedding from the preceding timestep before tokenization. This creates state-action pairs that help the transformer understand how specific actions relate to particular visual states, which is crucial for navigation and goal-reaching tasks.
- Core assumption: The state information encoded in the state-action token is relevant and helpful for predicting the next action, and that this context improves learning over action-only tokens.
- Evidence anchors:
  - [abstract] "Our approach involves two key changes: disentangling actions to the individual action level and tokenising the actions with auxiliary state information."
  - [section 4.1] "Actions are tokenised with the preceding state embedding to encourage state-action associations and provide richer context to the action token."
  - [section 5.3] "M-SAT outperforms MAT, suggesting the state embedding is contributing useful information to the action tokenisation."
- Break condition: If the state information becomes noisy or irrelevant for certain tasks, or if the model can learn state-action associations effectively without explicit pairing, this mechanism may provide diminishing returns.

### Mechanism 3
- Claim: Removing positional encoding (PE) can improve performance in certain environments by allowing the model to learn more suitable position representations for RL tasks, particularly when states have high visual similarity across timesteps.
- Mechanism: In standard transformers, PE provides absolute position information to tokens. By removing PE, M-SAT forces the model to learn relative position information or rely more heavily on the state-action associations, which may be more suitable for certain RL environments where visual states repeat or have high similarity.
- Core assumption: The learned position representation without explicit PE is more effective for the specific RL task than the fixed sinusoidal PE, particularly in environments with repetitive or similar states.
- Evidence anchors:
  - [abstract] "Additionally, we find that removing positional encoding does not adversely affect M-SAT's performance and, in some cases, even improves it."
  - [section 5.3] "In DC there is more similarity between corridor states which would benefit from removing PE... There is less similarity within the MWH task by comparison."
  - [section 5.2.1] "We investigate the impact of PE by removing all PE tying states, actions, and RTGs to a timestep, to improve the visibility of the newly proposed individual action tokens in attention layers."
- Break condition: If the task requires strong absolute position information (e.g., tasks where exact timing matters), or if the model cannot learn effective position representations without explicit PE, performance may suffer.

## Foundational Learning

- Concept: Transformer attention mechanisms and their ability to form relationships between tokens
  - Why needed here: Understanding how M-SAT leverages transformer attention is crucial for grasping why expanding action tokens and adding state context improves performance
  - Quick check question: How does the attention mechanism in a transformer differ from the token mixing in FNET, and why is this difference important for M-SAT's approach?

- Concept: Multi-discrete action spaces in reinforcement learning
  - Why needed here: The paper addresses a specific problem in RL where agents must select multiple discrete actions simultaneously, which is common in complex environments like ViZDoom
  - Quick check question: In the Deadly Corridor scenario, what are the four discrete actions the agent must select at each timestep, and why might their relationships matter for performance?

- Concept: Offline reinforcement learning and the limitations of Decision Transformers in multi-discrete action spaces
  - Why needed here: M-SAT is specifically designed to address limitations in Decision Transformers when handling multi-discrete actions in image-based environments
  - Quick check question: Why do standard Decision Transformers struggle with multi-discrete action spaces in image-based environments, and how does M-SAT's approach address this limitation?

## Architecture Onboarding

- Component map:
  Image states -> CNN -> State embeddings
  Multi-discrete actions -> Split into components -> One-hot encoding
  State embeddings + One-hot actions -> Concatenation -> MLP -> Action tokens
  All tokens (RTG, states, actions) -> Transformer -> Output layer

- Critical path:
  1. Image states are processed by CNN to generate state embeddings
  2. Multi-discrete actions are split into individual components
  3. Each action component is one-hot encoded and concatenated with the state embedding
  4. The concatenated state-action pairs are processed by MLP to generate individual action tokens
  5. All tokens (RTG, states, individual actions) are fed into the transformer
  6. The transformer generates the next action through the output layer

- Design tradeoffs:
  - Increased token count vs. richer information: M-SAT increases the number of tokens per timestep (4x for Deadly Corridor, 2x for My Way Home) but provides more granular information
  - Context length management: Longer context lengths are needed to accommodate more tokens, which increases computational cost
  - State-action pairing vs. simplicity: Adding state context to action tokens increases complexity but provides richer information

- Failure signatures:
  - No performance improvement over baseline: Indicates the attention mechanism isn't effectively utilizing the additional token granularity
  - Degraded performance with state-action pairing: Suggests the state information is noisy or irrelevant for the task
  - Sensitivity to context length: If performance varies significantly with context length changes, it may indicate suboptimal token utilization

- First 3 experiments:
  1. Compare baseline single-action-token model vs. M-SAT multi-action-token model on Deadly Corridor
  2. Ablation study: Compare M-SAT with and without state embeddings (MAT variant) to isolate the impact of state-action associations
  3. Positional encoding ablation: Test M-SAT with and without positional encoding to understand its impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of M-SAT scale with increasingly complex multi-discrete action spaces beyond the 4-action Deadly Corridor and 2-action My Way Home environments?
- Basis in paper: [explicit] The paper tests M-SAT only on ViZDoom environments with 4 and 2 discrete actions respectively
- Why unresolved: The paper doesn't explore environments with more complex multi-discrete action spaces to understand performance limits
- What evidence would resolve it: Testing M-SAT on environments with 5+ discrete actions or higher-dimensional action spaces

### Open Question 2
- Question: What is the theoretical explanation for why removing positional encoding improves M-SAT performance in some cases but not others?
- Basis in paper: [explicit] The paper observes mixed results when removing PE in different environments but doesn't explain the mechanism
- Why unresolved: The paper notes the phenomenon but doesn't provide theoretical justification for when PE helps vs hurts
- What evidence would resolve it: Mathematical analysis or controlled experiments showing how PE affects state-action associations in different environment types

### Open Question 3
- Question: How does M-SAT's performance compare to other transformer architectures designed for multi-modal inputs when applied to multi-discrete action spaces?
- Basis in paper: [inferred] The paper compares only to vanilla Decision Transformer, not to other transformer-based approaches
- Why unresolved: The paper establishes M-SAT's superiority over baseline but doesn't benchmark against alternative transformer architectures
- What evidence would resolve it: Head-to-head comparison of M-SAT with Perceiver, Gato, or other transformer architectures on the same tasks

## Limitations
- Limited evaluation to only two ViZDoom environments with specific multi-discrete action space configurations
- No analysis of computational overhead implications of increased token count
- Positional encoding ablation lacks theoretical framework for understanding when removal helps vs hurts

## Confidence
- High confidence: M-SAT improves performance over baseline Decision Transformers in tested ViZDoom environments
- Medium confidence: Proposed mechanisms explaining why M-SAT works are partially validated by ablation studies
- Low confidence: Claims about generalizability and computational efficiency due to limited evaluation scope

## Next Checks
1. Test M-SAT on a diverse set of environments including non-image-based tasks, continuous control environments, and environments with different action space configurations to validate broader applicability
2. Measure and compare training time, inference speed, and memory usage of M-SAT versus baseline Decision Transformers across different context lengths and batch sizes
3. Conduct comprehensive ablation study testing M-SAT with and without state embeddings across multiple environments with varying characteristics to understand when state-action associations provide the most benefit