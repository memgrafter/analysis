---
ver: rpa2
title: 'CriticBench: Benchmarking LLMs for Critique-Correct Reasoning'
arxiv_id: '2402.14809'
source_url: https://arxiv.org/abs/2402.14809
tags:
- answer
- critique
- generation
- correction
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces CriticBench, a comprehensive benchmark designed
  to assess LLMs'' abilities to critique and correct their reasoning across five domains:
  mathematical, commonsense, symbolic, coding, and algorithmic. Using 15 datasets
  and responses from three LLM families, the authors evaluate 17 LLMs on generation,
  critique, and correction (GQC) reasoning.'
---

# CriticBench: Benchmarking LLMs for Critique-Correct Reasoning

## Quick Facts
- **arXiv ID**: 2402.14809
- **Source URL**: https://arxiv.org/abs/2402.14809
- **Reference count**: 24
- **Primary result**: Introduces CriticBench benchmark to assess LLMs' generation, critique, and correction reasoning across 5 domains with 15 datasets and 17 models

## Executive Summary
CriticBench is a comprehensive benchmark designed to evaluate Large Language Models' (LLMs) abilities to critique and correct their reasoning across five domains: mathematical, commonsense, symbolic, coding, and algorithmic. Using 15 datasets and responses from three LLM families, the authors assess 17 LLMs on generation, critique, and correction (GQC) reasoning tasks. The benchmark reveals important patterns including linear relationships in GQC capabilities, task-dependent correction effectiveness, and intriguing inter-model critiquing dynamics where stronger models excel at critiquing weaker ones while weaker models can outperform stronger ones in self-critique.

## Method Summary
The authors compiled 15 datasets across five reasoning domains and generated responses using multiple LLM families (LLaMA, Vicuna, GPT). Response correctness was annotated through a three-tier approach: rule-based matching, GPT-4 evaluation, and manual review. The benchmark evaluates 17 LLMs on generation, critique, and correction tasks using fixed prompts. Models are assessed on accuracy for generation and correction, and F1 score for critique. The evaluation compares performance across base models, training strategies, prompt strategies, and oracle feedback to understand GQC capabilities comprehensively.

## Key Results
- Linear relationship exists between generation, critique, and correction capabilities, with critique-focused training enhancing performance
- Correction effectiveness varies by task type, with logic-oriented tasks being more amenable to correction than detail-oriented tasks
- GQC knowledge inconsistencies decrease as model size increases
- Inter-model critiquing shows stronger models better critique weaker ones, while weaker models can surpass stronger ones in self-critique

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear relationship between generation, critique, and correction abilities
- Mechanism: Models excelling at generation also perform well in critique and correction, suggesting shared underlying reasoning capabilities
- Core assumption: Reasoning skills transfer across generation, critique, and correction tasks
- Evidence anchors: Positive linear relationship observed between generating and judging abilities; critique-focused training enhances performance

### Mechanism 2
- Claim: Task type determines correction effectiveness
- Mechanism: Detail-oriented tasks are harder to correct than logic-oriented tasks because errors are more subtle and harder to detect
- Core assumption: Models more easily identify and correct logical errors than detail errors
- Evidence anchors: Weaker critique performance in detail-oriented algorithmic tasks compared to generation abilities

### Mechanism 3
- Claim: Stronger models better critique weaker models, but weaker models can outperform stronger ones in self-critique
- Mechanism: Stronger models have better overall reasoning capabilities for identifying errors, while weaker models may have more critical self-assessment
- Core assumption: Self-critique influenced by factors beyond raw reasoning ability, such as self-awareness
- Evidence anchors: Inter-model critiquing dynamic where stronger models critique weaker ones better, but weaker models sometimes surpass stronger ones in self-critique

## Foundational Learning

- **Concept: Linear relationships in model capabilities**
  - Why needed here: Understanding linear relationships helps predict model performance and design effective training strategies
  - Quick check question: If a model has high generation accuracy but low critique accuracy, what does this imply about its correction ability?

- **Concept: Task-specific reasoning challenges**
  - Why needed here: Different task types pose different correction challenges, requiring tailored approaches
  - Quick check question: Why might a model struggle more with correcting errors in algorithmic tasks compared to code generation tasks?

- **Concept: Inter-model critiquing dynamics**
  - Why needed here: Understanding how models critique each other is crucial for collaborative AI systems and self-improvement
  - Quick check question: What factors might contribute to weaker models sometimes outperforming stronger models in self-critique?

## Architecture Onboarding

- **Component map**: Question Collection → Response Collection → Response Annotation → Evaluation
- **Critical path**: Question Collection → Response Collection → Response Annotation → Evaluation
- **Design tradeoffs**: Multiple LLM families increase diversity but may introduce inconsistencies; rule-based matching is efficient but requires GPT-4 evaluation and manual review for accuracy
- **Failure signatures**: Low inter-model critique accuracy may indicate response quality or evaluation metric issues; inconsistent GQC knowledge suggests model training or evaluation limitations
- **First 3 experiments**:
  1. Evaluate a new LLM on CriticBench to assess GQC capabilities across different task types
  2. Compare critique accuracy of different LLM families on the same responses to identify strengths and weaknesses
  3. Analyze relationship between model size and GQC knowledge consistency to understand scale impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does language model size affect consistency of knowledge across generation, critique, and correction tasks?
- Basis in paper: The paper discusses that knowledge coherence across GQC decreases as model size increases
- Why unresolved: Does not provide detailed analysis of how different model sizes impact knowledge consistency across these tasks
- What evidence would resolve it: Comprehensive study comparing consistency of knowledge across different model sizes using knowledge overlap or consistency scores

### Open Question 2
- Question: What are underlying reasons for task-dependent variation in correction effectiveness between logic-oriented and detail-oriented tasks?
- Basis in paper: Models perform better in Q and C for logic-focused tasks compared to detail-focused tasks
- Why unresolved: Does not explore specific reasons why logic-oriented tasks are more amenable to correction
- What evidence would resolve it: In-depth analysis of error types in logic-oriented versus detail-oriented tasks and how models identify and correct these errors

### Open Question 3
- Question: How does inter-model critiquing pattern manifest across different task types?
- Basis in paper: Discusses inter-model critiquing dynamic where stronger models critique weaker ones better, but weaker models can surpass stronger ones in self-critique
- Why unresolved: Does not explore how this pattern varies across mathematical, commonsense, symbolic, coding, and algorithmic tasks
- What evidence would resolve it: Study analyzing inter-model critiquing pattern across various task types to reveal consistency or variation

## Limitations
- Linear relationship findings may be influenced by shared evaluation methodologies rather than fundamental reasoning abilities
- Task-dependent correction effectiveness based on limited task types with potentially oversimplified classification
- Inter-model critiquing patterns based on pairwise comparisons that may not generalize to broader model populations

## Confidence
- High confidence in benchmark construction and evaluation methodology
- Medium confidence in generalizability of linear relationship finding
- Medium confidence in task-dependent correction effectiveness claims
- Low confidence in broader implications of inter-model critiquing patterns due to limited model comparisons

## Next Checks
1. Test linear relationship hypothesis across additional reasoning domains and with different evaluation methodologies
2. Expand task classification beyond logic-oriented and detail-oriented to include additional dimensions and re-evaluate correction effectiveness
3. Conduct broader inter-model critique study with additional LLM families and different model sizes to validate observed patterns and identify boundary conditions