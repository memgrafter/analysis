---
ver: rpa2
title: Can Graph Neural Networks Expose Training Data Properties? An Efficient Risk
  Assessment Approach
arxiv_id: '2411.03663'
source_url: https://arxiv.org/abs/2411.03663
tags:
- graph
- graphs
- attack
- data
- property
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of graph property inference attacks
  on graph neural networks (GNNs), where an attacker aims to infer sensitive properties
  of a target graph from a shared GNN model. The authors propose an efficient attack
  method that leverages model approximation techniques to avoid the computational
  cost of training numerous shadow models.
---

# Can Graph Neural Networks Expose Training Data Properties? An Efficient Risk Assessment Approach

## Quick Facts
- arXiv ID: 2411.03663
- Source URL: https://arxiv.org/abs/2411.03663
- Reference count: 40
- Key outcome: Achieves 2.7% higher attack accuracy and 4.1% higher ROC-AUC while being 6.5× faster than baseline methods

## Executive Summary
This paper addresses graph property inference attacks on GNNs, where attackers aim to infer sensitive properties of a target graph from a shared model. The authors propose an efficient attack method that leverages model approximation techniques to avoid the computational cost of training numerous shadow models. By training a small set of reference models and generating approximated shadow models through graph perturbations, they achieve high diversity and low error in the attack models. The method demonstrates substantial improvements in both attack performance and computational efficiency compared to baseline approaches.

## Method Summary
The proposed method trains a small set of reference models on sampled graphs from an auxiliary graph dataset, then generates a large number of approximated shadow models by perturbing these reference graphs and applying model approximation techniques based on influence functions. The authors use graph edit distance to quantify diversity among the approximated models and derive a theoretical error bound to ensure approximation quality. A quadratic programming approach selects a diverse subset of low-error models, which are then used to train an attack classifier that infers sensitive properties of the target graph. The method is designed to work in both white-box and black-box scenarios, using model parameters or posterior probabilities respectively.

## Key Results
- Achieves 2.7% average increase in attack accuracy compared to baseline methods
- Improves ROC-AUC by 4.1% on average across tested datasets
- Reduces computational time by 6.5× through efficient model approximation
- Demonstrates effectiveness on real-world datasets (Facebook, PubMed, Pokec) with both node and link property inference

## Why This Works (Mechanism)

### Mechanism 1: Model Approximation via Influence Functions
- Claim: Model approximation allows generating multiple shadow models from a small set of reference models, reducing computational cost
- Mechanism: By perturbing reference graphs and applying model approximation techniques based on influence functions, the authors generate a diverse set of approximated shadow models without retraining from scratch
- Core assumption: The Hessian of the loss function is invertible or can be made invertible with a damping term
- Evidence anchors: [abstract] "Our method only requires training a small set of models on graphs, while generating a sufficient number of approximated shadow models for attacks."
- Break condition: If the Hessian becomes non-invertible and the damping term fails to provide a stable solution, the approximation would break down

### Mechanism 2: Diversity Quantification via Edit Distance
- Claim: Edit distance can effectively quantify diversity among approximated models
- Mechanism: The authors use graph edit distance as a metric to measure diversity among a set of approximated models, enabling them to select a diverse subset that maximizes diversity while minimizing approximation error
- Core assumption: Graph edit distance is a suitable metric for quantifying diversity in this context
- Evidence anchors: [section] "We apply edit distance to quantify the diversity within a group of approximated models."
- Break condition: If graph edit distance fails to capture meaningful differences between models (e.g., when models are very similar structurally), the diversity metric would be ineffective

### Mechanism 3: Approximation Error Bounds
- Claim: A theoretically guaranteed criterion can assess approximation errors for each model
- Mechanism: The authors derive a bound on approximation error based on the number of removed and influenced nodes, providing a criterion to evaluate and select models with low error
- Core assumption: The loss function is twice-differentiable and convex
- Evidence anchors: [section] "We establish that different graph perturbations can lead to varying approximation errors, which offers a theoretical-guaranteed criterion for assessing the errors of each approximated model."
- Break condition: If the loss function violates the convexity assumption (common in deep learning), the error bound would not hold

## Foundational Learning

- Concept: Graph neural networks (GNNs) and their message-passing mechanism
  - Why needed here: Understanding how GNNs process graph data is essential for grasping why they're vulnerable to property inference attacks and how model approximation techniques work
  - Quick check question: How does the neighborhood aggregation in GNNs potentially amplify distribution bias in the training data?

- Concept: Influence functions and model approximation
  - Why needed here: The attack relies on efficiently approximating model parameters after graph perturbations without full retraining, which requires understanding influence functions
  - Quick check question: What is the key insight behind using influence functions for model approximation in the context of graph data?

- Concept: Edit distance and graph similarity metrics
  - Why needed here: The authors use edit distance to quantify diversity among approximated models, which is crucial for selecting a diverse yet accurate set of models for the attack
  - Quick check question: How does graph edit distance differ from other graph similarity metrics, and why might it be suitable for this application?

## Architecture Onboarding

- Component map: Reference graph sampler -> Model trainer -> Graph perturbation generator -> Approximator -> Diversity selector -> Attack model trainer
- Critical path:
  1. Sample reference graphs from auxiliary graph
  2. Train reference models
  3. Generate augmented graphs via perturbations
  4. Apply model approximation to create approximated models
  5. Select diverse, low-error approximated models
  6. Train attack model on selected models
  7. Infer target graph properties
- Design tradeoffs:
  - Number of reference graphs vs. diversity of approximated models
  - Number of augmented graphs vs. computational cost
  - Approximation error tolerance vs. model diversity
  - White-box vs. black-box knowledge scenarios
- Failure signatures:
  - High approximation error leading to poor attack performance
  - Low diversity in selected models resulting in overfitting
  - Computational cost becoming prohibitive for large graphs
  - Sensitivity to hyper-parameter choices (number of reference/augmented graphs)
- First 3 experiments:
  1. Test approximation error on a small synthetic graph with known properties
  2. Evaluate diversity selection on a reference graph with varied perturbations
  3. Compare attack performance with different numbers of reference and augmented graphs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different perturbation strategies (e.g., node vs. edge removal) affect the diversity and approximation error of the generated models?
- Basis in paper: [inferred] The paper discusses the use of model approximation techniques and the importance of ensuring diversity while minimizing approximation errors, but does not explicitly compare different perturbation strategies
- Why unresolved: The paper does not provide empirical results comparing the impact of different perturbation strategies on model diversity and approximation error
- What evidence would resolve it: Empirical results showing the effects of various perturbation strategies (node removal, edge removal, or combinations) on model diversity and approximation error

### Open Question 2
- Question: Can the proposed method be extended to handle more complex graph properties, such as community structures or temporal dynamics?
- Basis in paper: [explicit] The paper focuses on inferring simple node and link properties, and mentions that graph properties can be either discrete or continuous, but does not explore more complex properties
- Why unresolved: The paper does not investigate the applicability of the method to more complex graph properties beyond the basic node and link properties
- What evidence would resolve it: Experiments demonstrating the method's effectiveness in inferring complex graph properties, such as community structures or temporal dynamics, on real-world datasets

### Open Question 3
- Question: How does the choice of graph metric (e.g., edit distance) influence the diversity and approximation error of the generated models?
- Basis in paper: [explicit] The paper mentions the use of graph edit distance as a metric for measuring diversity but does not explore the impact of different graph metrics on the method's performance
- Why unresolved: The paper does not provide empirical results comparing the effects of different graph metrics on model diversity and approximation error
- What evidence would resolve it: Empirical results showing the effects of various graph metrics (e.g., graph edit distance, graph similarity, or other metrics) on the diversity and approximation error of the generated models

## Limitations
- The method relies on the invertibility of the Hessian matrix, which may not hold for non-convex loss functions common in deep learning
- The edit distance metric for diversity assessment may not capture all relevant aspects of model behavior for complex graph structures
- The quadratic programming approach for selecting diverse models may become computationally expensive for very large graph sets

## Confidence
- High confidence: The overall framework design and experimental methodology are sound and well-documented
- Medium confidence: The approximation technique using influence functions is theoretically grounded but relies on assumptions that may not hold in practice
- Low confidence: The diversity metric using graph edit distance and its effectiveness in capturing meaningful model differences

## Next Checks
1. **Hessian invertibility test**: Implement a diagnostic check to verify the stability of the Hessian inversion during model approximation, particularly for non-convex loss functions
2. **Alternative diversity metrics**: Compare graph edit distance with other graph similarity metrics (e.g., graph kernels, spectral distances) to validate the choice of diversity metric
3. **Approximation error sensitivity**: Systematically vary the damping parameter λ and error threshold ϵ to understand their impact on approximation quality and attack performance