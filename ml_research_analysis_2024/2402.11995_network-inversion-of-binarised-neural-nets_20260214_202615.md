---
ver: rpa2
title: Network Inversion of Binarised Neural Nets
arxiv_id: '2402.11995'
source_url: https://arxiv.org/abs/2402.11995
tags:
- output
- input
- network
- variables
- inversion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to invert a trained Binarised
  Neural Network (BNN) by encoding it into a CNF formula, allowing for both inference
  and inversion. The CNF formula precisely captures the network's structure, enabling
  deterministic and fine-grained control over inverted samples.
---

# Network Inversion of Binarised Neural Nets

## Quick Facts
- arXiv ID: 2402.11995
- Source URL: https://arxiv.org/abs/2402.11995
- Authors: Pirzada Suhail; Supratik Chakraborty; Amit Sethi
- Reference count: 4
- Proposes encoding Binarised Neural Networks (BNNs) into CNF formulas for inversion

## Executive Summary
This paper introduces a novel approach to invert trained Binarised Neural Networks (BNNs) by encoding them into Conjunctive Normal Form (CNF) formulas. This encoding enables both inference and inversion operations while providing deterministic control over inverted samples. The method is demonstrated on a 100-20-10 BNN trained on MNIST, successfully uncovering erroneous classifications and showing potential for improving BNN trustworthiness in safety-critical applications.

## Method Summary
The approach encodes a trained BNN into a CNF formula that precisely captures the network's structure and logic. This encoding allows the BNN to be solved using standard SAT solvers, enabling both forward inference and backward inversion. The CNF representation provides fine-grained control over the inversion process, allowing researchers to generate out-of-distribution inputs that can be used to identify model weaknesses and retrain the network iteratively.

## Key Results
- Successfully encoded a 100-20-10 BNN trained on MNIST into CNF formula
- Demonstrated ability to uncover erroneous classifications through inversion
- Showed potential for iterative retraining using out-of-distribution inputs generated during inversion

## Why This Works (Mechanism)
The approach works by leveraging the logical structure of BNNs, where all weights and activations are binary. This binary nature allows the network's computation to be expressed as logical constraints, which can be systematically converted into CNF form. The SAT solver can then efficiently explore the solution space to find inputs that satisfy specific output conditions, effectively inverting the network.

## Foundational Learning

Boolean Satisfiability (SAT): Theory of determining if a Boolean formula can be satisfied - needed to understand the core inversion mechanism, check by verifying CNF formula structure.

Binary Neural Networks: Neural networks with binary weights and activations - essential for understanding why CNF encoding is possible, check by confirming all parameters are {0,1} or {-1,1}.

CNF Encoding: Process of converting logical expressions to Conjunctive Normal Form - crucial for the technical implementation, check by examining the encoding steps.

## Architecture Onboarding

Component Map: BNN layers -> CNF constraints -> SAT solver -> Inverted samples

Critical Path: Training -> CNF Encoding -> SAT Solving -> Inversion Analysis

Design Tradeoffs: Binary representation enables exact logical encoding but limits expressiveness compared to full-precision networks

Failure Signatures: Unsatisfiable CNF formulas indicate no possible input for target output; multiple solutions suggest ambiguous mappings

First Experiments:
1. Verify CNF formula correctly reproduces forward inference results
2. Test inversion on simple linear BNN to validate basic functionality
3. Compare inversion results with traditional gradient-based methods

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- CNF encoding may become computationally intractable for larger networks
- Experimental evaluation limited to small 100-20-10 BNN on MNIST
- Iterative retraining process lacks detailed validation and effectiveness analysis
- No discussion of limitations for different BNN structures or training objectives

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Encoding BNNs into CNF formulas is technically sound | High |
| Experimental results demonstrate approach potential | Medium |
| Iterative retraining improves model generalization | Low |

## Next Checks

1. Evaluate scalability on larger BNNs (>100 layers) and complex datasets (CIFAR-10, ImageNet)
2. Conduct ablation studies to quantify iterative retraining impact on accuracy and input space refinement
3. Test applicability to BNNs with different structures (residual connections, recurrent architectures) and training objectives (adversarial training, multi-task learning)