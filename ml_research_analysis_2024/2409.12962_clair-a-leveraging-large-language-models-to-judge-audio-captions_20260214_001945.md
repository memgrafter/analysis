---
ver: rpa2
title: 'CLAIR-A: Leveraging Large Language Models to Judge Audio Captions'
arxiv_id: '2409.12962'
source_url: https://arxiv.org/abs/2409.12962
tags:
- audio
- claira
- captions
- score
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLAIRA, a simple and interpretable measure
  for evaluating audio captions using large language models (LLMs). The key innovation
  is directly asking LLMs to produce a semantic distance score between candidate and
  reference captions, leveraging in-context learning and efficient guided generation
  to ensure valid JSON outputs.
---

# CLAIR-A: Leveraging Large Language Models to Judge Audio Captions

## Quick Facts
- **arXiv ID**: 2409.12962
- **Source URL**: https://arxiv.org/abs/2409.12962
- **Reference count**: 39
- **Primary result**: CLAIRA achieves up to 5.8% relative accuracy improvement over FENSE and up to 11% over general-purpose metrics on Clotho-Eval

## Executive Summary
CLAIRA introduces a novel LLM-based metric for evaluating audio captions by directly asking language models to score semantic similarity between candidate and reference captions. The method uses efficient guided generation with context-free grammars to ensure valid JSON outputs and includes a tie-breaking mechanism for improved granularity. CLAIRA demonstrates superior correlation with human judgments compared to both domain-specific and general-purpose evaluation metrics, while also providing interpretable justifications for its scores.

## Method Summary
CLAIRA evaluates audio captions by prompting a large language model to score semantic similarity between candidate and reference captions, producing both a numeric score and justification. The system uses context-free grammars to constrain generation and ensure valid JSON outputs, then applies a tie-breaking mechanism (FENSE or Sentence-BERT) when scores are identical or too close. The final score combines the LLM judgment with the tie-breaker weighted by ε = 0.25.

## Key Results
- CLAIRA achieves up to 5.8% relative accuracy improvement over FENSE on Clotho-Eval
- Outperforms best general-purpose metrics by up to 11% on the same dataset
- Human evaluators rated CLAIRA's explanations up to 30% higher quality than baseline methods

## Why This Works (Mechanism)

### Mechanism 1
LLMs can directly assess semantic similarity between audio captions by leveraging in-context learning without explicit parsing. The LLM is given a prompt with candidate and reference captions and asked to produce a numeric score and justification, bypassing the need for intermediate graph representations or N-gram matching.

### Mechanism 2
Efficient guided generation using context-free grammars ensures deterministic, valid JSON outputs from the LLM. The system uses a CFG-based parser to constrain LLM token sampling, guaranteeing valid JSON and eliminating invalid generations without expensive re-sampling.

### Mechanism 3
Adding a tie-breaking mechanism (FENSE or Sentence-BERT) improves evaluation granularity and human correlation. When LLM scores are identical or too close, an additional semantic similarity measure is added with a small weight (ε = 0.25) to break ties and provide finer distinctions.

## Foundational Learning

- **Large language model in-context learning**
  - Why needed here: CLAIRA relies on prompting the LLM with candidate and reference captions to generate a score, rather than fine-tuning or training a new model.
  - Quick check question: Can you explain how in-context learning differs from fine-tuning, and why it is suitable for this zero-shot evaluation task?

- **Context-free grammars and constrained generation**
  - Why needed here: The system uses CFGs to guarantee valid JSON outputs from the LLM, avoiding invalid generations and re-sampling.
  - Quick check question: What is a context-free grammar, and how does it ensure valid JSON output during LLM generation?

- **Semantic similarity measures (e.g., Sentence-BERT, FENSE)**
  - Why needed here: These measures are used as tie-breakers to provide additional granularity when LLM scores are identical or too close.
  - Quick check question: How do Sentence-BERT and FENSE differ in their approach to computing semantic similarity between text?

## Architecture Onboarding

- **Component map**: Prompt template -> LLM inference engine -> Efficient guided generation module -> Tie-breaker module -> JSON parser and score normalizer -> Optional ensemble module

- **Critical path**: 
  1. Input candidate and reference captions
  2. Construct prompt with scoring rubric
  3. Generate JSON score via LLM with CFG-guided generation
  4. Parse and normalize LLM score (0-100)
  5. If needed, compute tie-breaker score
  6. Combine LLM and tie-breaker scores
  7. Output final CLAIRA score and justification

- **Design tradeoffs**:
  - LLM choice: Proprietary models (GPT-4o) are faster and cheaper per call but may have access/cost limits; open models (Phi-3) are more flexible but slower
  - Tie-breaker choice: FENSE is domain-specific and better for audio; Sentence-BERT is multilingual but less specialized
  - CFG complexity: More complex CFGs may better match natural LLM output but are harder to implement and maintain

- **Failure signatures**:
  - Invalid JSON output: Likely CFG or generation constraint issue
  - Identical or tied scores across diverse inputs: Tie-breaker not working or ε too small
  - Poor correlation with human judgment: LLM's semantic understanding misaligned, or prompt not clear

- **First 3 experiments**:
  1. Test CLAIRA on a small set of Clotho-Eval pairs with GPT-4o; verify JSON output and score range
  2. Replace LLM with an open model (e.g., Phi-3) and compare speed, cost, and score distribution
  3. Disable tie-breaker and measure how often scores are tied; then re-enable with different ε values to optimize granularity

## Open Questions the Paper Calls Out

### Open Question 1
How does CLAIRA perform on audio captioning datasets beyond Clotho and Audio-Caps, particularly on languages other than English and Chinese? The paper's multilingual evaluation is limited to Chinese and a translated version of Clotho, leaving questions about performance on other languages and datasets.

### Open Question 2
What is the impact of different LLM choices and configurations on CLAIRA's performance and computational efficiency? The paper only briefly mentions a few LLM choices without a systematic comparison of their impact on performance, cost, and efficiency.

### Open Question 3
How does CLAIRA handle audio captions with varying levels of complexity and ambiguity, and what are its limitations in these cases? The paper focuses on overall performance metrics but does not delve into specific cases of complex or ambiguous audio captions.

## Limitations

- Exact prompt template and JSON schema used for CLAIRA are not specified, making exact reproduction difficult
- Multilingual evaluation is limited to Chinese translation of Clotho, not tested on other languages
- Resource constraints and access limits for proprietary models may affect real-world scalability

## Confidence

- **High Confidence**: CLAIRA's core mechanism (LLM-based semantic scoring with JSON-constrained output) is clearly described and grounded in recent work on efficient guided generation
- **Medium Confidence**: The claim that CLAIRA offers better interpretability and fairness is supported by human ratings, but these are relative to unspecified baseline methods
- **Low Confidence**: The multilingual transfer results and open-source model performance are based on limited experiments without systematic exploration

## Next Checks

1. **Prompt Template and Parameter Sensitivity**: Reconstruct the CLAIRA prompt and JSON schema as closely as possible from the paper. Systematically vary the tie-breaking weight (ε) and measure its impact on score distribution, tie rate, and correlation with human judgments on Clotho-Eval.

2. **Cross-Dataset and Cross-Domain Robustness**: Evaluate CLAIRA on at least two additional audio captioning datasets (e.g., Clotho-CRO or AudioCaps-2) and one non-audio captioning dataset (e.g., MS COCO image captions). Compare performance with FENSE and general metrics to assess domain generalization.

3. **Open-Source Model Scalability**: Replace the proprietary LLM with several open-source models (e.g., Llama-3, Phi-3-mini, Qwen-2) and measure latency, cost, and accuracy on a subset of Clotho-Eval. Identify the performance-cost tradeoff and determine the minimum viable model for practical deployment.