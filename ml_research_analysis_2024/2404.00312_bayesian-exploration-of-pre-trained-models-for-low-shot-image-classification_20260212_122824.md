---
ver: rpa2
title: Bayesian Exploration of Pre-trained Models for Low-shot Image Classification
arxiv_id: '2404.00312'
source_url: https://arxiv.org/abs/2404.00312
tags:
- uni00000013
- uni00000011
- uni00000048
- uni00000044
- uni00000055
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Bayesian framework for low-shot image classification
  that combines CLIP with other pre-trained models using Gaussian processes. The approach
  integrates prior knowledge by specifying the mean function with CLIP and the kernel
  function with an ensemble of deep kernels built on various pre-trained models.
---

# Bayesian Exploration of Pre-trained Models for Low-shot Image Classification

## Quick Facts
- arXiv ID: 2404.00312
- Source URL: https://arxiv.org/abs/2404.00312
- Authors: Yibo Miao; Yu Lei; Feng Zhou; Zhijie Deng
- Reference count: 40
- Primary result: Bayesian GP ensemble with CLIP zero-shot prior outperforms CLIP-based and ensemble baselines on ImageNet 1-shot/5-shot and miniImageNet.

## Executive Summary
This paper proposes a Bayesian framework for low-shot image classification that integrates CLIP with other pre-trained models using Gaussian processes. The approach leverages CLIP's zero-shot classifier as a prior mean and combines deep kernels from multiple pre-trained models for flexible similarity measures. Experiments show consistent improvements over ensemble baselines in accuracy, calibration, and out-of-distribution robustness across standard benchmarks.

## Method Summary
The method employs Gaussian Process regression where the mean function is defined by CLIP's zero-shot linear classifier and the kernel is an ensemble of deep kernels built from pre-trained vision models. Hyper-parameters are optimized using predictive likelihood on validation data when available, otherwise marginal likelihood. The framework enables analytical inference, uncertainty quantification, and principled hyper-parameter tuning for low-shot image classification tasks.

## Key Results
- Outperforms ensemble baselines in predictive performance on ImageNet 1-shot/5-shot and miniImageNet
- Demonstrates strong robustness and model calibration on out-of-distribution datasets
- Predictive likelihood hyper-parameter optimization yields better generalization than marginal likelihood in low-shot regimes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The zero-shot CLIP classifier serves as an effective prior mean, enabling good predictions even before seeing any training data.
- Mechanism: The prior mean function m(x) is defined as the output of CLIP's zero-shot linear classifier. This injects strong inductive bias derived from CLIP's large-scale vision-language training into the GP model, improving generalization under few-shot conditions.
- Core assumption: The zero-shot CLIP classifier captures general image semantics that are relevant across most downstream tasks.
- Evidence anchors:
  - [abstract] "We achieve the integration of prior knowledge by specifying the mean function with CLIP"
  - [section 4.2] "We set the mean function m(·) to the zero-shot linear classifier in CLIP, which has demonstrated strong performance."
  - [corpus] Weak evidence: No corpus neighbor directly addresses this specific GP-mean design choice.
- Break condition: If the downstream task involves images or classes that are highly dissimilar to CLIP's pretraining distribution, the zero-shot classifier may be uninformative or misleading.

### Mechanism 2
- Claim: Deep kernels built from pre-trained vision models provide flexible, data-adapted similarity measures for the GP.
- Mechanism: Each pre-trained model g(i) is used to transform input images into feature space; a learnable scaling vector l(i) and base kernel ˜k then define a kernel k(i). Summing across models yields a composite kernel that adaptively integrates diverse visual priors.
- Core assumption: Pre-trained models capture complementary and complementaryable knowledge about image structure and semantics.
- Evidence anchors:
  - [abstract] "the kernel function with an ensemble of deep kernels built upon various pre-trained models"
  - [section 4.1] "We propose to alternatively use pre-trained models to define deep kernels and then perform an adaptive combination."
  - [corpus] Weak evidence: No direct corpus support for kernel ensemble with pre-trained models; related papers discuss CLIP fine-tuning or PEFT.
- Break condition: If the pre-trained models are too similar in the features they extract, the ensemble adds little benefit and may overfit.

### Mechanism 3
- Claim: Using predictive likelihood for hyper-parameter tuning yields better generalization than marginal likelihood in low-shot regimes.
- Mechanism: The GP hyper-parameters α are optimized by maximizing log p(Yval|Xval, X, Y, α), the likelihood of held-out validation data, rather than the marginal likelihood of the training set.
- Core assumption: Marginal likelihood is sensitive to prior assumptions and can underfit or overfit when data is scarce.
- Evidence anchors:
  - [section 4.3] "Given this, a more proper objective can be log p(Yval|Xval, X, Y, α), i.e., the predictive likelihood on extra validation data"
  - [section 5.5] "The results clearly echo such an argument and support the use of predictive likelihood for hyper-parameter optimization."
  - [corpus] No corpus neighbor directly addresses this objective choice; related works focus on PEFT or CLIP fine-tuning.
- Break condition: If no validation split is available (e.g., 1-shot ImageNet), the method falls back to marginal likelihood, potentially reducing performance.

## Foundational Learning

- Concept: Gaussian Process regression and kernel methods
  - Why needed here: The method builds a GP model with custom mean and kernel; understanding GP priors, posteriors, and kernel design is essential.
  - Quick check question: What is the role of the kernel function in a GP, and how does it relate to similarity between data points?

- Concept: Vision-language pretraining and zero-shot classification
  - Why needed here: CLIP's zero-shot classifier is used as the GP mean; knowing how CLIP projects images and text into a joint space is key.
  - Quick check question: How does CLIP's zero-shot classifier work, and what is the intuition behind using it as a prior mean?

- Concept: Hyper-parameter optimization in probabilistic models
  - Why needed here: The method tunes GP hyper-parameters via marginal or predictive likelihood; understanding the difference and their implications is important.
  - Quick check question: What is the difference between marginal and predictive likelihood, and why might predictive likelihood generalize better?

## Architecture Onboarding

- Component map:
  Pre-trained models (CLIP image encoder, DINO, MoCo) -> GP mean function (CLIP zero-shot classifier) -> GP kernel function (ensemble of deep kernels) -> Hyper-parameter optimizer (Adam, 100 steps) -> Validation set (for predictive likelihood) -> Prediction pipeline (analytical GP inference)

- Critical path:
  1. Load pre-trained models and extract features.
  2. Define GP mean with CLIP classifier.
  3. Define GP kernel as sum of deep kernels from each pre-trained model.
  4. Optimize hyper-parameters using predictive likelihood (or marginal if no validation split).
  5. Perform GP inference to obtain predictions and uncertainties.

- Design tradeoffs:
  - Using CLIP as mean vs. a learned mean: CLIP provides strong inductive bias but may be suboptimal for highly specialized tasks.
  - Ensemble of kernels vs. single deep kernel: Ensemble integrates diverse knowledge but increases hyper-parameter count.
  - Predictive vs. marginal likelihood: Predictive is more robust but requires a validation split, which may not exist in extreme low-shot cases.

- Failure signatures:
  - Poor performance if pre-trained models' features are irrelevant to the task.
  - Overfitting if too many hyper-parameters are tuned on tiny datasets.
  - High uncertainty on out-of-distribution data (desired behavior).

- First 3 experiments:
  1. Reproduce 1-shot ImageNet results using only CLIP as mean and kernel, no ensemble.
  2. Add DINO and MoCo to the kernel ensemble, measure improvement.
  3. Switch from marginal to predictive likelihood tuning, compare validation set performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed GP-based ensemble method perform when incorporating more than three pre-trained models, particularly models trained on datasets distinct from ImageNet?
- Basis in paper: [explicit] The paper mentions that adding models like SimCLR does not significantly improve performance and suggests that including models trained on different datasets could bring further benefits.
- Why unresolved: The paper only experiments with up to three pre-trained models and does not explore the impact of including models trained on very different datasets.
- What evidence would resolve it: Experiments showing performance comparisons of the GP-based ensemble method with varying numbers of pre-trained models, especially those trained on datasets with different characteristics from ImageNet.

### Open Question 2
- Question: What is the impact of using alternative base kernels (e.g., Laplacian or Matérn kernels) on the performance of the GP-based ensemble method?
- Basis in paper: [explicit] The paper mentions that the RBF kernel performs best among the tested base kernels (RBF, Laplacian, and Matérn) but does not provide a detailed comparison or analysis of why the RBF kernel is superior.
- Why unresolved: The paper only briefly mentions the performance of different base kernels without exploring the underlying reasons for the RBF kernel's superiority or investigating other potential base kernels.
- What evidence would resolve it: A comprehensive study comparing the performance of various base kernels (including but not limited to RBF, Laplacian, and Matérn) on different datasets, along with an analysis of the factors contributing to their relative performance.

### Open Question 3
- Question: How does the GP-based ensemble method's performance and uncertainty estimates compare to other Bayesian approaches for low-shot image classification, such as Bayesian neural networks or other probabilistic ensemble methods?
- Basis in paper: [inferred] The paper focuses on the GP-based approach and demonstrates its superiority over deterministic ensemble baselines and leading CLIP-based methods. However, it does not compare its performance and uncertainty estimates to other Bayesian approaches.
- Why unresolved: The paper does not provide a direct comparison between the GP-based ensemble method and other Bayesian approaches for low-shot image classification.
- What evidence would resolve it: Experiments comparing the GP-based ensemble method's performance, uncertainty estimates, and computational efficiency to other Bayesian approaches, such as Bayesian neural networks or other probabilistic ensemble methods, on standard low-shot image classification benchmarks.

## Limitations

- The paper's claims rely on untested assumptions about CLIP's zero-shot classifier being universally informative across tasks
- Lack of corpus support for key methodological choices (kernel ensemble, predictive likelihood tuning)
- No ablation studies to isolate the contribution of each component (CLIP mean, kernel ensemble, hyper-parameter tuning)

## Confidence

- **High**: The GP framework with custom mean/kernel is mathematically sound and reproducible
- **Medium**: Claims about strong OOD robustness and calibration are supported by reported ECE/TACE scores but lack error analysis or failure case studies
- **Low**: The assertion that predictive likelihood is universally superior to marginal likelihood is weakly supported, as the paper only demonstrates this in settings where validation splits exist

## Next Checks

1. **Ablation Study**: Run experiments with CLIP mean but no kernel ensemble, and with kernel ensemble but no CLIP mean, to isolate each component's contribution
2. **Cross-Domain Robustness**: Test on datasets far from CLIP's pretraining distribution (e.g., medical imaging) to verify if the zero-shot prior remains informative
3. **Hyper-parameter Sensitivity**: Vary the number of optimization steps and learning rate for hyper-parameter tuning to assess stability and reproducibility