---
ver: rpa2
title: 'Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language
  Models'
arxiv_id: '2402.13887'
source_url: https://arxiv.org/abs/2402.13887
tags:
- llms
- high
- school
- evaluation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the alignment between probability-based\
  \ evaluation methods and actual generative performance of large language models\
  \ (LLMs) on multiple-choice question (MCQ) benchmarks. The authors compare three\
  \ prediction methods\u2014label-based, sequence-based, and generation-based\u2014\
  across three datasets: MMLU, TruthfulQA, and Belebele, using ten different LLMs."
---

# Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models

## Quick Facts
- arXiv ID: 2402.13887
- Source URL: https://arxiv.org/abs/2402.13887
- Reference count: 11
- This paper investigates the alignment between probability-based evaluation methods and actual generative performance of large language models on multiple-choice question benchmarks.

## Executive Summary
This study investigates the critical misalignment between probability-based evaluation methods and actual generative performance of large language models on multiple-choice question benchmarks. Through comprehensive experiments across three datasets (MMLU, TruthfulQA, and Belebele) using ten different LLMs, the authors reveal that current evaluation frameworks based on output probabilities often fail to capture how LLMs perform in real-world applications where users interact with free-text generation. The findings show that agreement between probability-based predictions and generation-based outputs frequently falls below 50%, with even lower consistency when predictions are correct. The study also demonstrates that while instruction-tuned models show improved alignment with generation-based predictions, the overall correlation with human preferences remains weak, particularly in natural sciences.

## Method Summary
The study compares three prediction methods—label-based, sequence-based, and generation-based—across three datasets using ten LLMs including Mistral-7B, LLaMA-1-7B, LLaMA-2-7B variants, and Vicuna-7B. The authors implement these methods by providing standardized prompts to each model and measuring agreement between predictions, accuracy scores, overlap of correctly predicted options, and correlation with human preferences from Chatbot Arena. The experimental setup involves running each model through the three prediction methods and comparing outputs using various metrics to quantify the misalignment between probability-based and generation-based evaluation approaches.

## Key Results
- Agreement scores between probability-based methods and generation-based predictions frequently fall below 50% across all datasets
- Even when predictions are correct, consistency remains low, especially for smaller base models
- Instruction-tuned models show better alignment with generation-based methods compared to base models, but overall correlation with human preferences remains weak
- LLMs exhibit stronger correlations with human preferences in social science subjects while displaying notably lower consistency with human judgments in natural science subjects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Probability-based evaluation methods fail to capture the generative nature of LLM usage in real-world applications.
- **Mechanism**: LLM evaluation typically uses output probabilities to select answers, but this approach diverges from how users interact with LLMs—by generating responses. This misalignment leads to evaluations that do not accurately reflect LLM capabilities.
- **Core assumption**: The way LLMs are evaluated in benchmarks (via probabilities) should align with how they are used in practice (via generation).
- **Evidence anchors**:
  - [abstract]: "current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses"
  - [section 2.3]: "We argue that MCQ-proxy might not always reflect the actual performance of LLM under user-facing free-text generation"
- **Break condition**: If probability-based methods show high agreement with generation-based predictions, the misalignment claim would weaken.

### Mechanism 2
- **Claim**: Instruction-tuned models show better alignment with generation-based predictions compared to base models.
- **Mechanism**: Instruction tuning enhances LLMs' ability to follow prompts and generate coherent responses, leading to better consistency between probability-based predictions and generation-based outputs.
- **Core assumption**: Instruction tuning improves the alignment between evaluation methods and real-world LLM usage.
- **Evidence anchors**:
  - [section 3.2]: "instruction-tuned LLMs typically exhibit better alignment with the generation-based methods across both probability-based methods"
  - [section 3.2]: "instruction-tuned (chat) LLMs is always better than the vanilla LLMs, potentially demonstrating the importance of instruction tuning"
- **Break condition**: If base models show equal or better alignment than instruction-tuned models, the instruction tuning hypothesis would weaken.

### Mechanism 3
- **Claim**: Evaluation benchmarks have low correlation with human preferences, especially in natural sciences.
- **Mechanism**: Benchmarks like MMLU are designed to test knowledge and problem-solving, but human preferences (as measured by Chatbot Arena) may prioritize different qualities like fluency and helpfulness. This leads to discrepancies between benchmark scores and human judgments.
- **Core assumption**: Human preferences for LLM outputs are not fully captured by traditional benchmarks.
- **Evidence anchors**:
  - [section 3.2]: "many of these multiple-choice NLP benchmark rankings do not agree with human preference for free-text generation output"
  - [section 3.2]: "LLMs exhibit stronger correlations with human preferences in social science subjects... while displaying notably lower consistency with human judgments in natural science subjects"
- **Break condition**: If benchmarks show high correlation with human preferences across all categories, the discrepancy claim would weaken.

## Foundational Learning

- **Concept**: Probability-based evaluation
  - **Why needed here**: Understanding how probability-based methods work is crucial to analyzing their misalignment with generation-based predictions.
  - **Quick check question**: How do probability-based methods select answers from multiple-choice options?

- **Concept**: Generation-based prediction
  - **Why needed here**: This method mirrors real-world LLM usage and serves as a baseline for comparing other evaluation methods.
  - **Quick check question**: What is the key difference between generation-based and probability-based evaluation methods?

- **Concept**: Agreement metrics
  - **Why needed here**: Agreement metrics quantify the consistency between different evaluation methods, which is central to the paper's analysis.
  - **Quick check question**: How is agreement between two prediction methods defined in this study?

## Architecture Onboarding

- **Component map**: Datasets (MMLU, TruthfulQA, Belebele) -> Models (Mistral, LLaMA variants) -> Prediction Methods (label-based, sequence-based, generation-based) -> Evaluation Metrics (accuracy, agreement, correlation with human preferences) -> Experimental setup and analysis pipeline

- **Critical path**: Dataset → Model → Prediction Method → Evaluation Metric → Analysis → Comparison with human preferences

- **Design tradeoffs**:
  - Computational efficiency vs. evaluation accuracy (probability-based vs. generation-based)
  - Benchmark comprehensiveness vs. real-world applicability
  - Instruction tuning benefits vs. base model performance

- **Failure signatures**:
  - Low agreement between probability-based and generation-based methods
  - Poor correlation between benchmark scores and human preferences
  - Inconsistent predictions across different evaluation methods

- **First 3 experiments**:
  1. Compare label-based and sequence-based predictions with generation-based predictions on MMLU dataset
  2. Analyze correlation between MMLU sub-category scores and Chatbot Arena Elo scores
  3. Evaluate few-shot in-context learning effects on prediction method agreement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop evaluation frameworks that better align with real-world LLM usage and human preferences?
- Basis in paper: [explicit] The authors highlight the misalignment between probability-based evaluation methods and generation-based predictions, and the weak correlation with human preferences from Chatbot Arena.
- Why unresolved: Current benchmarks like MMLU, TruthfulQA, and Belebele show significant discrepancies between probability-based and generation-based predictions, and do not consistently align with human preferences.
- What evidence would resolve it: Development and validation of new evaluation protocols that incorporate human feedback, focus on free-text generation quality, and demonstrate improved alignment with real-world LLM usage and human preferences.

### Open Question 2
- Question: What are the underlying factors contributing to the instability of LLM predictions across different evaluation methods?
- Basis in paper: [inferred] The study reveals low agreement between probability-based methods and generation-based predictions, even for instruction-tuned models, suggesting inherent instability in LLM predictions.
- Why unresolved: The paper identifies the problem but does not deeply investigate the root causes of this instability, such as model architecture, training data, or prompt sensitivity.
- What evidence would resolve it: Detailed analysis of LLM architectures, training procedures, and sensitivity to prompt variations to identify specific factors contributing to prediction instability.

### Open Question 3
- Question: How does few-shot in-context learning affect the reliability of LLM evaluations, and can we develop methods to account for this?
- Basis in paper: [explicit] The authors find that agreement between probability-based and generation-based predictions decreases with more in-context examples, complicating few-shot LLM evaluation.
- Why unresolved: Few-shot learning is a key capability of LLMs, but current evaluation methods may not accurately capture their performance in these settings, leading to potentially misleading results.
- What evidence would resolve it: Investigation of how different numbers and types of in-context examples affect LLM predictions, and development of evaluation methods that can account for few-shot learning effects.

## Limitations
- The study focuses on three specific MCQ benchmarks which may not represent the full spectrum of LLM evaluation scenarios
- The ten models studied exclude other major architectures (GPT models, Claude, etc.) and findings may be specific to examined model families
- Chatbot Arena Elo scores serve as human preference benchmark but represent preferences for conversational quality rather than pure factual accuracy

## Confidence

**High Confidence**: The experimental methodology for comparing probability-based and generation-based prediction methods is technically sound. The finding that agreement scores frequently fall below 50% across multiple datasets is robust and reproducible.

**Medium Confidence**: The claim that instruction-tuned models show better alignment with generation-based predictions is supported but may be partially attributable to other factors like model scale or training data differences.

**Medium Confidence**: The correlation analysis with human preferences reveals meaningful discrepancies, but the interpretation that benchmarks fail to capture human preferences requires more nuanced analysis of what aspects of performance each metric captures.

## Next Checks

1. **Cross-Architecture Validation**: Replicate the agreement analysis using models from different architectural families (GPT, Claude, etc.) to determine if the misalignment pattern holds across the broader LLM landscape.

2. **Prompt Ablation Study**: Systematically vary prompt formulations for the generation-based method to quantify how sensitive the agreement metrics are to prompt design choices.

3. **Domain-Specific Correlation**: Break down the human preference correlation analysis by specific sub-categories within each dataset to identify whether certain types of questions (causal reasoning, factual recall, etc.) show stronger or weaker alignment with human judgments.