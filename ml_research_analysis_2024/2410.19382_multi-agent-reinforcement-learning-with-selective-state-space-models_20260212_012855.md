---
ver: rpa2
title: Multi-Agent Reinforcement Learning with Selective State-Space Models
arxiv_id: '2410.19382'
source_url: https://arxiv.org/abs/2410.19382
tags:
- mamba
- performance
- mappo
- mean
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Multi-Agent Mamba (MAM), a novel architecture
  for Multi-Agent Reinforcement Learning (MARL) that replaces the Transformer-based
  MAT with Mamba blocks. The key innovation is the development of a "cross-attention"
  Mamba block, along with the use of standard and bi-directional Mamba blocks, to
  handle the multi-agent sequence modeling problem.
---

# Multi-Agent Reinforcement Learning with Selective State-Space Models

## Quick Facts
- arXiv ID: 2410.19382
- Source URL: https://arxiv.org/abs/2410.19382
- Authors: Jemma Daniel; Ruan de Kock; Louay Ben Nessir; Sasha Abramowitz; Omayma Mahjoub; Wiem Khlifi; Claude Formanek; Arnu Pretorius
- Reference count: 11
- Primary result: MAM achieves performance comparable to MAT while offering superior computational efficiency and linear scaling with agent count

## Executive Summary
This paper introduces Multi-Agent Mamba (MAM), a novel architecture for Multi-Agent Reinforcement Learning that replaces the Transformer-based MAT architecture with Mamba blocks. The key innovation is the development of a "cross-attention" Mamba block, along with standard and bi-directional Mamba blocks, to handle the multi-agent sequence modeling problem. MAM demonstrates performance comparable to MAT across multiple MARL environments while offering superior computational efficiency and linear scaling with the number of agents, unlike MAT's quadratic scaling.

## Method Summary
MAM is implemented as a JAX-based Mava architecture that replaces MAT's attention mechanisms with Mamba blocks. The architecture consists of bi-directional Mamba blocks for encoding observations, causal Mamba blocks for action generation, and CrossMamba blocks for cross-attention between observations and actions. The method uses a 20M timestep training budget across 10 random seeds with Bayesian hyperparameter tuning.

## Key Results
- MAM matches or outperforms MAT in 17 out of 21 MARL tasks tested
- MAM demonstrates linear scaling with agent count while MAT scales quadratically
- MAM achieves comparable sample efficiency to MAT with faster inference times

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAM achieves comparable performance to MAT while offering superior computational efficiency.
- Mechanism: MAM replaces attention mechanisms with Mamba blocks, which have linear scaling with sequence length compared to MAT's quadratic scaling.
- Core assumption: Mamba blocks can effectively capture the necessary interactions between agents without the full attention mechanism.
- Evidence anchors:
  - [abstract]: "MAM achieves performance comparable to MAT across multiple MARL environments while offering superior computational efficiency and linear scaling with the number of agents"
  - [section 3.2]: "This makes causal self-attention replacement straightforward: simply use Mamba as a drop-in replacement"
  - [corpus]: Weak evidence - no directly comparable papers found in corpus
- Break condition: If the selective state space model cannot adequately model the multi-agent interactions, performance would degrade compared to MAT.

### Mechanism 2
- Claim: MAM can scale to environments with more agents without the computational bottleneck faced by MAT.
- Mechanism: The linear scaling of Mamba blocks allows MAM to handle longer sequences (more agents) without quadratic increase in computational cost.
- Core assumption: The linear scaling of Mamba blocks translates directly to linear scaling in multi-agent scenarios.
- Evidence anchors:
  - [abstract]: "offering superior scalability to larger agent scenarios"
  - [section 4.2]: "Figure 2: Mean time (seconds) per evaluation step in smacv2 tasks with increasing numbers of agents for MAM, MAT, and MAPPO. The mean time per evaluation step for MAT increases approximately quadratically, while MAM and MAPPO scale linearly in the number of agents"
  - [corpus]: Weak evidence - no directly comparable papers found in corpus
- Break condition: If the linear scaling assumption doesn't hold in practice, MAM would face similar computational challenges as MAT with many agents.

### Mechanism 3
- Claim: The novel CrossMamba block effectively replaces MAT's cross-attention mechanism.
- Mechanism: CrossMamba adapts Mamba to process information from two input sequences (observations and actions), allowing it to attend between them.
- Core assumption: The modified Mamba architecture can handle the cross-attention functionality needed for multi-agent action selection.
- Evidence anchors:
  - [section 3.2]: "We adapt Mamba to process information from two inputs, creating a 'cross-attentional' Mamba block which we name CrossMamba"
  - [section 3.2]: "Given 6 and 8, we can rewrite 11 as... which shows that a vanilla Mamba block possesses a form of causal self-attention similar to 4 (Ali et al., 2024). CrossMamba, in contrast to a vanilla Mamba block, allows the selective parameter C to depend on a second input sequence"
  - [corpus]: Weak evidence - no directly comparable papers found in corpus
- Break condition: If CrossMamba cannot adequately model the dependencies between observations and actions, MAM's performance would suffer.

## Foundational Learning

- Concept: State-Space Models (SSMs)
  - Why needed here: Understanding SSMs is crucial as MAM is built on Mamba, a selective SSM
  - Quick check question: What is the key difference between traditional SSMs and selective SSMs like Mamba?

- Concept: Multi-Agent Advantage Decomposition Theorem
  - Why needed here: MAM is based on MAT, which uses this theorem to reframe joint policy optimization
  - Quick check question: How does the multi-agent advantage decomposition theorem simplify the search for optimal policies across multiple agents?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding what MAM is replacing (attention) helps in grasping the design choices
  - Quick check question: Why does the attention mechanism in Transformers scale quadratically with sequence length?

## Architecture Onboarding

- Component map: Observations -> Bi-directional Mamba blocks -> Encoder output -> CrossMamba blocks -> Decoder (Causal Mamba blocks) -> Actions

- Critical path:
  1. Observations are encoded using bi-directional Mamba blocks
  2. Actions are generated sequentially using causal Mamba blocks
  3. CrossMamba blocks attend between observations and actions during action selection

- Design tradeoffs:
  - Efficiency vs. expressivity: MAM trades some of the fine-grained control of attention for linear scaling
  - Complexity: MAM introduces new components (CrossMamba) that need to be carefully implemented

- Failure signatures:
  - Degraded performance compared to MAT on tasks with fewer agents
  - Training instability due to improper implementation of CrossMamba
  - Unexpected scaling behavior if Mamba blocks don't scale linearly in practice

- First 3 experiments:
  1. Implement and test the bi-directional Mamba block on a simple sequence task
  2. Implement and validate the CrossMamba block on a toy multi-agent problem
  3. Compare MAM's performance and scaling against MAT on a small MARL benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MAM's performance scale with significantly larger numbers of agents beyond those tested in current MARL benchmarks?
- Basis in paper: [inferred] The paper mentions limitations in testing with very large numbers of agents due to current benchmark constraints and hypothesizes that performance benefits may become more apparent in significantly many-agent environments.
- Why unresolved: Current MARL benchmarks lack tasks with very large numbers of agents, making it difficult to evaluate MAM's true scaling potential.
- What evidence would resolve it: Empirical results from MAM in environments with significantly larger numbers of agents (e.g., 100+ agents) compared to current benchmarks, showing performance and efficiency gains.

### Open Question 2
- Question: What is the optimal balance between information compression and performance for the Mamba hyperparameters (∆ projection dimension and hidden state dimension) in many-agent MARL tasks?
- Basis in paper: [explicit] The ablation study shows that increasing the dimension size for ∆ and N is not always advantageous, and the best performance was achieved with the smallest hyperparameter values tested.
- Why unresolved: The ablation study was conducted on tasks with a limited number of agents, and the optimal balance may differ in significantly many-agent environments.
- What evidence would resolve it: Further ablation studies on tasks with larger numbers of agents, exploring a wider range of hyperparameter values to determine the optimal balance between information compression and performance.

### Open Question 3
- Question: How does MAM's sample efficiency compare to MAT's in tasks with significantly larger numbers of agents?
- Basis in paper: [inferred] The paper mentions that MAM's sample efficiency closely matches MAT's on current benchmarks, but suggests that on tasks with more agents, MAM may have superior sample-efficiency.
- Why unresolved: Current benchmarks do not include tasks with large enough numbers of agents to fully test this hypothesis.
- What evidence would resolve it: Empirical results comparing the sample efficiency of MAM and MAT on tasks with significantly larger numbers of agents, showing whether MAM maintains or improves its relative sample efficiency advantage.

## Limitations
- Performance evaluation lacks statistical significance testing across the 21 tasks
- Computational efficiency claims rely on synthetic benchmarks rather than full training scenarios
- Limited exploration of edge cases where attention mechanisms might outperform Mamba blocks
- No investigation of potential training instability from replacing attention with SSMs

## Confidence

**High confidence** in the core architectural contribution - the MAM framework with its bi-directional, causal, and CrossMamba blocks is clearly specified and implementable. The theoretical motivation for replacing attention with SSMs is sound.

**Medium confidence** in the performance claims - while 17/21 tasks show comparable or better results, the lack of statistical significance testing and potential cherry-picking of hyperparameters reduces confidence. The computational efficiency claims are supported by synthetic benchmarks but not validated in full training scenarios.

**Low confidence** in the generalizability of results - the evaluation covers specific MARL environments but lacks exploration of edge cases, such as scenarios with highly correlated agent states or tasks requiring complex temporal reasoning that attention mechanisms might handle better.

## Next Checks

1. **Statistical validation**: Perform statistical significance testing (e.g., paired t-tests with multiple comparison corrections) across all 21 tasks to quantify whether MAM's performance differences versus MAT are meaningful rather than due to random variation.

2. **Training efficiency validation**: Compare full training wall-clock time for MAM versus MAT across increasing agent numbers (2, 4, 8, 16 agents) on representative tasks to validate the claimed computational advantages beyond synthetic evaluation benchmarks.

3. **Architectural ablation**: Conduct systematic ablation studies removing CrossMamba or using alternative SSM variants to quantify the specific contribution of each architectural component to overall performance.