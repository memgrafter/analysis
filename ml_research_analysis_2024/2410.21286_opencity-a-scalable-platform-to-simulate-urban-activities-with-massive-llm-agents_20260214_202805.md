---
ver: rpa2
title: 'OpenCity: A Scalable Platform to Simulate Urban Activities with Massive LLM
  Agents'
arxiv_id: '2410.21286'
source_url: https://arxiv.org/abs/2410.21286
tags:
- agents
- simulation
- agent
- urban
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces OpenCity, a scalable simulation platform
  designed to overcome the computational challenges of deploying large-scale LLM agents
  for urban activity simulation. OpenCity employs two key optimizations: an LLM request
  scheduler that parallelizes requests using I/O multiplexing to reduce communication
  overhead, and a "group-and-distill" prompt strategy that clusters agents with similar
  static attributes to minimize token usage and LLM requests.'
---

# OpenCity: A Scalable Platform to Simulate Urban Activities with Massive LLM Agents

## Quick Facts
- **arXiv ID:** 2410.21286
- **Source URL:** https://arxiv.org/abs/2410.21286
- **Reference count:** 39
- **Primary result:** Enables simulation of 10,000 agents' daily activities within one hour on commodity hardware

## Executive Summary
OpenCity addresses the computational bottleneck of deploying large-scale LLM agents for urban activity simulation. The platform introduces two key optimizations: an LLM request scheduler that parallelizes requests using I/O multiplexing to reduce communication overhead, and a "group-and-distill" prompt strategy that clusters agents with similar static attributes to minimize token usage and LLM requests. Evaluated across six global cities, OpenCity achieves a 600-fold acceleration in simulation time per agent, a 70% reduction in LLM requests, and a 50% reduction in token usage. These improvements enable the simulation of 10,000 agents' daily activities within one hour on commodity hardware, and allow the establishment of a benchmark for LLM agents in urban simulation.

## Method Summary
OpenCity employs two complementary optimizations to scale LLM-based urban simulations. First, an LLM request scheduler uses I/O multiplexing to parallelize agent requests, significantly reducing communication overhead with LLM APIs. Second, the "group-and-distill" prompt strategy clusters agents based on static attributes (demographics, preferences) to generate shared context, minimizing redundant token generation and reducing the total number of LLM requests. The platform was evaluated across six global cities, demonstrating substantial improvements in computational efficiency while maintaining high behavioral fidelity.

## Key Results
- Achieves 600-fold acceleration in simulation time per agent
- Reduces LLM requests by 70% and token usage by 50%
- Maintains high behavioral faithfulness with up to 96% top-1 hit rate when using GPT-4o

## Why This Works (Mechanism)
The two optimizations work synergistically: the request scheduler addresses the I/O bottleneck that limits parallelization, while the group-and-distill strategy reduces the computational load by eliminating redundancy. By clustering agents with similar static attributes, the platform avoids generating identical context for each agent, significantly reducing token usage. The I/O multiplexing allows multiple requests to be processed concurrently, overcoming the sequential nature of API calls. Together, these approaches enable massive scale while maintaining simulation quality.

## Foundational Learning
- **I/O Multiplexing**: Parallel processing of multiple I/O operations simultaneously; needed to overcome sequential API call limitations; quick check: measure request queue depth vs. concurrent connections
- **Token Optimization**: Reducing redundant text generation through context sharing; needed to minimize LLM costs and latency; quick check: compare token counts before/after clustering
- **Agent Clustering**: Grouping similar agents based on static attributes; needed to identify opportunities for shared context generation; quick check: evaluate cluster purity vs. behavioral diversity
- **Benchmark Establishment**: Creating standardized evaluation metrics for urban simulations; needed to enable comparative analysis across cities and platforms; quick check: validate consistency across different urban contexts

## Architecture Onboarding

**Component Map:** Request Scheduler -> Agent Clustering -> Context Generator -> LLM API -> Response Aggregator

**Critical Path:** User request → Request Scheduler (parallelization) → Agent Clustering (group formation) → Context Generator (shared prompt creation) → LLM API (concurrent calls) → Response Aggregator (individual agent results)

**Design Tradeoffs:** The platform trades some behavioral granularity for computational efficiency by clustering agents. More aggressive clustering reduces costs but may oversimplify individual variations. The choice of clustering granularity represents a key parameter that balances accuracy against scalability.

**Failure Signatures:** Performance degradation when agent diversity exceeds clustering capacity, leading to insufficient context sharing. API rate limiting becomes a bottleneck if parallelization exceeds provider limits. Quality degradation occurs when static attributes poorly predict behavioral patterns, resulting in inappropriate context sharing.

**3 First Experiments:**
1. Benchmark baseline performance without optimizations on a small agent set (100 agents)
2. Vary clustering granularity to identify the optimal balance between efficiency and accuracy
3. Test different LLM providers/models to evaluate cost-quality trade-offs

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains may not generalize to smaller or differently optimized LLM models
- Clustering approach could miss important behavioral variations between similar agents
- Benchmark establishment limited to six cities raises questions about broader generalizability

## Confidence

**High Confidence:** Computational optimization claims (600x acceleration, 70% reduction in requests, 50% reduction in tokens)

**Medium Confidence:** Behavioral faithfulness metrics (96% top-1 hit rate)

**Low Confidence:** Cross-city generalizability - limited to six cities without systematic variation analysis

## Next Checks
1. Test performance degradation when scaling to 50,000+ agents and verify if optimizations maintain their relative improvements
2. Evaluate the platform using smaller LLM variants (e.g., GPT-3.5, Claude 3 Haiku) to assess cost-effectiveness trade-offs
3. Conduct systematic sensitivity analysis on clustering parameters to determine the impact on simulation fidelity across diverse urban scenarios