---
ver: rpa2
title: Pretraining Decision Transformers with Reward Prediction for In-Context Multi-task
  Structured Bandit Learning
arxiv_id: '2406.05064'
source_url: https://arxiv.org/abs/2406.05064
tags:
- predetor
- action
- learning
- setting
- bandit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PreDeToR, a transformer-based method for
  in-context learning in structured multi-task bandit problems. The key innovation
  is training transformers to predict expected rewards for all actions rather than
  requiring optimal actions or access to arm features.
---

# Pretraining Decision Transformers with Reward Prediction for In-Context Multi-task Structured Bandit Learning

## Quick Facts
- arXiv ID: 2406.05064
- Source URL: https://arxiv.org/abs/2406.05064
- Authors: Subhojyoti Mukherjee; Josiah P. Hanna; Qiaomin Xie; Robert Nowak
- Reference count: 40
- Primary result: PreDeToR outperforms baselines in multi-task structured bandit learning by predicting rewards for all actions

## Executive Summary
This paper introduces PreDeToR, a transformer-based method for in-context learning in structured multi-task bandit problems. The key innovation is training transformers to predict expected rewards for all actions rather than requiring optimal actions or access to arm features. The approach exploits shared latent structure across tasks to minimize cumulative regret without privileged information. Experiments across linear, non-linear, bilinear, and latent bandit settings show PreDeToR outperforms baselines including DPT and AD while matching specialized algorithms. Theoretical analysis provides generalization bounds showing low excess risk as the number of source tasks increases.

## Method Summary
PreDeToR trains a transformer (GPT-2 causal model) to predict expected rewards for all actions in a given context dataset. The method uses a pretraining phase where trajectories collected by weak demonstrators are used to train the transformer with least-squares loss on reward predictions. At test time, the trained transformer is used in-context to predict rewards for unseen tasks, selecting actions based on highest predicted reward. The approach requires shared actions across tasks to learn latent structure, and performs a two-phase exploration implicitly - first with strong prior from training data, then switching to task-specific exploration after observing few examples.

## Key Results
- PreDeToR outperforms baselines (DPT, AD) across linear, non-linear, bilinear, and latent bandit settings
- Method matches specialized algorithms while requiring no privileged information about arm features
- Performance degrades when number of new actions exceeds shared actions across tasks
- Theoretical analysis shows low excess risk as number of source tasks increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PreDeToR learns shared latent structure across tasks by predicting expected rewards for all actions
- Mechanism: Transformer trained to minimize mean squared error between predicted and observed rewards for each action in context, enabling discovery of probabilistic dependencies across tasks
- Core assumption: Actions are shared across tasks (invariant actions exist) allowing the transformer to learn reward correlations
- Evidence anchors:
  - [abstract] "training transformers to predict expected rewards for all actions rather than requiring optimal actions or access to arm features"
  - [section] "the key idea is that transformers have the capacity to discover and exploit complex dependencies in order to predict the rewards of all possible actions"
  - [corpus] Weak evidence: only one relevant neighbor paper mentions transformers and bandits, but doesn't discuss reward prediction mechanism
- Break condition: When number of new actions exceeds shared actions, the transformer fails to learn latent structure as observed in experiments

### Mechanism 2
- Claim: PreDeToR conducts two-phase exploration implicitly
- Mechanism: First phase explores with strong prior from training data, second phase switches to task-specific exploration after observing few examples
- Core assumption: Transformer can maintain and update probability distributions over actions based on context
- Evidence anchors:
  - [abstract] "conducts a two-phase exploration to minimize regret"
  - [section] "learns to adapt, in-context, to novel actions and new tasks as long as the number of new actions is small compared to shared actions"
  - [corpus] Missing: no corpus papers specifically discuss transformer exploration mechanisms in bandit settings
- Break condition: When horizon is too short for sufficient in-context examples, exploration fails to switch phases

### Mechanism 3
- Claim: PreDeToR achieves low excess risk through algorithmic stability and covering number bounds
- Mechanism: Multi-task learning risk decreases as number of source tasks increases due to error stability and finite covering number of transformer hypothesis space
- Core assumption: Transformer outputs are stable under small input perturbations and the hypothesis space has finite covering number
- Evidence anchors:
  - [abstract] "Theoretical analysis provides generalization bounds showing low excess risk as the number of source tasks increases"
  - [section] "Suppose error stability Theorem 8.1 holds and assume loss function ℓ(·,·) is C-Lipschitz"
  - [corpus] Missing: no corpus papers provide theoretical analysis of transformer generalization in bandit settings
- Break condition: When number of source tasks is small, generalization bounds become loose and performance degrades

## Foundational Learning

- Concept: Multi-task structured bandit learning
  - Why needed here: The entire problem setup requires understanding how tasks share structure and how algorithms can exploit this
  - Quick check question: What distinguishes a structured bandit from a regular multi-armed bandit?

- Concept: In-context learning with transformers
  - Why needed here: PreDeToR relies on transformer's ability to learn from context without parameter updates
  - Quick check question: How does in-context learning differ from traditional supervised learning?

- Concept: Algorithmic stability and generalization bounds
  - Why needed here: Theoretical analysis of PreDeToR's performance depends on these concepts
  - Quick check question: What is error stability and why is it important for generalization?

## Architecture Onboarding

- Component map: Input dataset Ht containing action-reward pairs -> GPT-2 causal transformer -> Linear output layer mapping to reward predictions -> Greedy/softmax action selection

- Critical path:
  1. Collect pretraining dataset from demonstrator
  2. Train transformer to predict rewards for all actions
  3. At test time, use trained transformer to predict rewards for unseen task
  4. Select action with highest predicted reward (greedy) or sample from softmax (exploratory)

- Design tradeoffs:
  - Shared vs new actions: More shared actions enable better structure learning but less task-specific adaptation
  - Greedy vs softmax: Greedy selection maximizes immediate reward but may miss exploration benefits
  - Number of training tasks: More tasks improve generalization but increase computational cost

- Failure signatures:
  - High prediction error on optimal actions indicates poor structure learning
  - Regret similar to demonstrator suggests failure to exploit structure
  - Performance drops with many new actions indicates over-reliance on shared structure

- First 3 experiments:
  1. Linear bandit setting with known structure to validate basic functionality
  2. Non-linear bandit setting to test structure discovery capability
  3. New action setting to test robustness to task variations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between PreDeToR's reward prediction and posterior sampling in structured bandit settings?
- Basis in paper: [explicit] The paper mentions connections between PreDeToR and posterior sampling, stating "Both of these methods can also be linked to posterior sampling. Such connections between sequence modeling with transformers and posterior sampling have also been made in Chen et al. (2021); Müller et al. (2021); Lee et al. (2023); Yang et al. (2023)."
- Why unresolved: While the paper suggests a connection, it does not formally establish or characterize how PreDeToR's reward prediction relates to posterior sampling distributions, particularly in terms of uncertainty quantification and exploration-exploitation trade-offs.
- What evidence would resolve it: Formal theoretical analysis showing how PreDeToR's predicted rewards relate to posterior distributions over rewards, and experimental comparison showing PreDeToR's exploration behavior against true posterior sampling methods like Thompson Sampling.

### Open Question 2
- Question: How does PreDeToR's performance scale with the number of shared actions when the shared structure is unknown or partially corrupted?
- Basis in paper: [explicit] The paper states "We also show that the shared actions across the tasks are vital for PreDeToR to exploit the latent structure. We show that PreDeToR learns to adapt, in-context, to novel actions and new tasks as long as the number of new actions is small compared to shared actions across the tasks."
- Why unresolved: The paper demonstrates PreDeToR's dependence on shared actions but doesn't fully characterize the threshold at which performance degrades or how partial corruption of shared structure affects learning.
- What evidence would resolve it: Systematic experiments varying both the number of shared actions and the degree of corruption in the shared structure, along with theoretical analysis of how these factors affect generalization bounds.

### Open Question 3
- Question: What is the computational complexity of PreDeToR compared to traditional bandit algorithms when scaling to high-dimensional action spaces?
- Basis in paper: [explicit] The paper mentions "Finally, note that PreDeToR needs to forward-pass|A| times to select the best arm during evaluation, and hence suffers from more computational overhead with a large action space, compared to AD or DPT."
- Why unresolved: While the paper acknowledges the computational overhead, it doesn't provide detailed complexity analysis or empirical measurements of how PreDeToR's runtime scales with action space size compared to traditional methods.
- What evidence would resolve it: Detailed computational complexity analysis and runtime experiments comparing PreDeToR against traditional bandit algorithms (UCB, Thompson Sampling) across varying action space sizes and dimensionalities.

## Limitations
- Performance critically depends on the number of shared actions across tasks, degrading when new actions exceed shared actions
- Computational overhead increases linearly with action space size due to needing to forward-pass for each action during selection
- Theoretical generalization bounds assume error stability and finite covering number properties that may not hold for complex transformer architectures

## Confidence
- **High confidence**: The basic mechanism of using transformers to predict rewards for all actions is well-supported by experimental results
- **Medium confidence**: The two-phase exploration mechanism is described but not explicitly validated through ablation studies
- **Medium confidence**: The theoretical generalization bounds are mathematically derived but their practical tightness requires further investigation

## Next Checks
1. Ablation study on action sharing: Systematically vary the ratio of shared to new actions across tasks and measure the resulting performance degradation to quantify the limits of structure exploitation
2. Exploration mechanism analysis: Compare PreDeToR's regret trajectory against explicit exploration algorithms to validate the claimed two-phase exploration behavior
3. Generalization bound validation: Test the theoretical excess risk bounds by measuring actual performance degradation as the number of training tasks decreases below the asymptotic regime assumed in the analysis