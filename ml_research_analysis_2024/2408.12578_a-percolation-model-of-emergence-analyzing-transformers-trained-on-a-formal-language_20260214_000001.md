---
ver: rpa2
title: 'A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal
  Language'
arxiv_id: '2408.12578'
source_url: https://arxiv.org/abs/2408.12578
tags:
- properties
- iterations
- number
- learning
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a phenomenological definition for emergence
  in neural networks, linking it to the acquisition of general structures underlying
  the data-generating process that drive sudden improvements in narrower tasks. The
  authors introduce a formal language learning task with grammar and type constraints
  as the underlying structures, and train Transformers to perform reasoning tasks
  on strings from this language.
---

# A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language

## Quick Facts
- arXiv ID: 2408.12578
- Source URL: https://arxiv.org/abs/2408.12578
- Reference count: 40
- Primary result: Observes phases in Transformer learning dynamics corresponding to emergent acquisition of grammar and type constraints in a formal language, with percolation model predicting scaling of emergence points

## Executive Summary
This paper proposes a phenomenological definition for emergence in neural networks, linking it to the acquisition of general structures underlying the data-generating process. Through experiments on a formal language learning task with grammar and type constraints, the authors observe phases in Transformer learning dynamics that correspond to sudden improvements in narrower tasks. A percolation model on bipartite graphs is proposed to explain the scaling of when capabilities emerge, predicting a square-root dependence on the number of descriptive properties. The experimental results show strong qualitative agreement with this hypothesis, demonstrating a step towards better defining, characterizing, and predicting emergence in neural networks.

## Method Summary
The authors design a formal language learning task using probabilistic context-free grammars (PCFG) and type constraints, training a two-block Transformer on next-token prediction. The model is evaluated on grammaticality, type constraints satisfaction, and task-specific metrics (unscrambling, conditional generation) over training iterations. A percolation model on bipartite graphs is proposed to explain the scaling of emergence points, predicting that transition thresholds scale with the square root of the product of entities and properties in the language.

## Key Results
- Observed phases in Transformer learning dynamics corresponding to emergent acquisition of grammar and type constraints
- Percolation model predicts square-root scaling of emergence points with number of descriptive properties
- Learning curves show log-log linear growth after transition, consistent across different language complexities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning underlying grammar and type constraints creates shared structural representation enabling sudden improvements across downstream tasks
- Mechanism: Grammar provides syntactic structure while type constraints enforce semantic coherence; when both are learned, model can generalize to novel entity-property compositions via percolation-like process on bipartite graph
- Core assumption: Model's internal representations capture bipartite graph structure of type constraints
- Evidence anchors: [abstract] "once the language's underlying grammar and context-sensitivity inducing structures are learned by the model, performance on narrower tasks suddenly begins to improve"; [section] "formation of systematic structures drives phase changes"; [corpus] 25 related papers, average FMR=0.38
- Break condition: If model's representations don't capture bipartite graph structure, percolation-based generalization won't occur

### Mechanism 2
- Claim: Percolation transition threshold scales with square root of product of entities and properties, explaining how transition points shift with language complexity
- Mechanism: As more entity-property pairs are seen during training, bipartite graph becomes increasingly connected; once edge density crosses percolation threshold, macroscopic connected component emerges, enabling generalization to unseen compositions
- Core assumption: Model's learning dynamics mirror bond percolation on bipartite graph
- Evidence anchors: [abstract] "percolation model on bipartite graphs...predicts shift in point of emergence"; [section] "percolation threshold is obtained as pc ≃ √(1/|E||K|) for large |E| and |K|"; [corpus] "Manifold Percolation: from generative model to Reinforce learning"
- Break condition: If model learns through memorization rather than structural generalization, percolation scaling won't apply

### Mechanism 3
- Claim: Geometry of learning curves (log-log linear growth after transition) is consistent across different language complexities, indicating universal learning mechanism
- Mechanism: After percolation transition, model's ability to infer valid entity-property compositions grows linearly with log data, reflecting combinatorial explosion in connected component size
- Core assumption: Post-transition learning dynamics follow predictable scaling law
- Evidence anchors: [abstract] "geometry of these performance curves are extremely similar to geometry we observed for base setting"; [section] "scaling of point where ability to unscramble descriptive sentences starts to emerge, but with exponent of 1.5"; [corpus] "Predicting Emergent Capabilities by Finetuning"
- Break condition: If learning dynamics change fundamentally with language complexity, scaling law won't hold

## Foundational Learning

- Concept: Phase transitions and order parameters
  - Why needed here: Paper models emergence as phase transition where learning underlying structures drives sudden capability improvements
  - Quick check question: What distinguishes a first-order from second-order phase transition in terms of order parameter behavior?

- Concept: Percolation theory and critical thresholds
  - Why needed here: Proposed mechanism for emergence relies on percolation transitions in bipartite graphs
  - Quick check question: How does percolation threshold scale with system size in random graphs?

- Concept: Probabilistic context-free grammars and formal language theory
  - Why needed here: Experimental system based on learning formal language with grammatical and type constraints
  - Quick check question: What distinguishes a context-free from context-sensitive language?

## Architecture Onboarding

- Component map: PCFG-based grammar sampling -> Type constraints bipartite graph -> Transformer next-token prediction -> Grammaticality/type check evaluation -> Performance metric tracking

- Critical path: 1) Sample symbolic sentence from PCFG, 2) Populate with tokens respecting type constraints, 3) Feed to Transformer for next-token prediction, 4) Evaluate outputs for grammaticality and type constraint satisfaction, 5) Track performance metrics over training iterations

- Design tradeoffs: Online vs batch learning (online chosen to study emergence under data scaling); Simple vs complex architecture (simple two-block Transformer to isolate emergence mechanisms); Symbolic vs natural language (symbolic chosen for precise control over underlying structures)

- Failure signatures: Performance plateaus without sudden jumps (model may be memorizing rather than learning structures); Grammar learned but type constraints not (issues with type constraint implementation or model capacity); Inconsistent scaling across runs (random seed sensitivity or insufficient training duration)

- First 3 experiments: 1) Reproduce base results: Train on language with 900 entities, 18000 properties, track learning curves for grammaticality and type constraints, 2) Vary number of properties: Sweep from 14800 to 38800 in increments of 1600, observe transition point scaling, 3) Change number of classes: Reduce from 10 to 2 classes, assess impact on emergence patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the scaling exponent for the transition point in the unscrambling task be explained theoretically?
- Basis in paper: [explicit] The paper observes a scaling exponent of 1.5 for the transition point in the unscrambling task, which differs from the expected 0.5 scaling based on the percolation model
- Why unresolved: Paper does not provide theoretical explanation for this discrepancy; authors suggest it might be due to interactions with learning other mechanisms, but this is left as open question
- What evidence would resolve it: Theoretical analysis of interactions between percolation model and mechanisms involved in unscrambling could explain observed scaling exponent; alternatively, experimental results showing how scaling exponent changes with different language parameters could provide insights

### Open Question 2
- Question: Can the proposed phenomenological definition of emergence be applied to more complex and naturalistic settings?
- Basis in paper: [explicit] Paper proposes definition of emergence based on acquisition of general structures underlying data-generating process; demonstrates this definition in toy task of learning formal languages
- Why unresolved: Paper acknowledges definition is relatively informal and leaves open question of whether it can be applied to more complex and naturalistic settings
- What evidence would resolve it: Applying proposed definition to analyze emergence in large language models or other complex neural networks could demonstrate generalizability; identifying structures underlying emergent capabilities in these settings and showing their acquisition correlates with sudden performance improvements would provide strong evidence

### Open Question 3
- Question: How can rich literature on phase transitions in physics be leveraged to develop predictive theories for emergence in neural networks?
- Basis in paper: [explicit] Paper draws inspiration from theory of phase transitions in physics and proposes percolation model to explain scaling of when capabilities emerge in formal language learning task
- Why unresolved: Paper suggests theory of phase transitions could be used to develop predictive theories for emergence in neural networks, but does not provide concrete examples or further exploration of this idea
- What evidence would resolve it: Developing predictive theories for emergence based on theory of phase transitions and testing them in various neural network tasks would demonstrate potential of this approach; identifying order parameters for different emergent capabilities and showing they can be used to predict point of emergence would be particularly compelling

## Limitations

- Experiments confined to highly controlled symbolic domain; may not generalize to natural language or other architectures
- Percolation model predictions, while showing strong agreement in experiments, need validation across broader language families and architectural variations
- Online learning setup may not reflect typical training regimes for large language models

## Confidence

- Confidence in core emergence definition: Medium
- Confidence in percolation model predictions: Medium-High for specific experimental domain, Low for generalization to natural language or other architectures
- Confidence in architectural generalizability: Low

## Next Checks

1. **Cross-architecture validation**: Repeat experiments with varying Transformer depths (1-6 blocks) and attention heads to test robustness of percolation model's predictions across architectural complexity

2. **Natural language extension**: Design simplified natural language task with clear underlying structures (e.g., subject-verb-object patterns with semantic constraints) to test whether phenomenological definition of emergence and percolation-based predictions hold beyond formal languages

3. **Theoretical connection analysis**: Conduct ablation studies on bipartite graph structure (e.g., varying edge density, introducing random edges) to empirically validate assumptions linking neural network learning dynamics to bond percolation transitions