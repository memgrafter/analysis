---
ver: rpa2
title: State-Novelty Guided Action Persistence in Deep Reinforcement Learning
arxiv_id: '2409.05433'
source_url: https://arxiv.org/abs/2409.05433
tags:
- action
- persistence
- exploration
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses sample inefficiency in deep reinforcement learning
  (DRL) by introducing a method to dynamically adjust action persistence (repeating
  actions) based on state novelty. The core idea is to use a state-novelty measure
  to guide the probability of repeating the last action, allowing for more exploration
  in early training stages and fine-grained policies in later stages.
---

# State-Novelty Guided Action Persistence in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2409.05433
- Source URL: https://arxiv.org/abs/2409.05433
- Authors: Jianshu Hu; Paul Weng; Yutong Ban
- Reference count: 40
- One-line primary result: SNAP improves sample efficiency in DRL by 10-20% on DMControl tasks

## Executive Summary
This paper addresses sample inefficiency in deep reinforcement learning by introducing a method to dynamically adjust action persistence based on state novelty. The core idea is to use a state-novelty measure to guide the probability of repeating the last action, allowing for more exploration in early training stages and fine-grained policies in later stages. This approach does not require training additional value functions or policies, making it computationally efficient. The method was tested on DeepMind Control Suite tasks and showed significant improvements in sample efficiency compared to baselines.

## Method Summary
The method introduces a state-novelty guided action persistence framework that modifies how often an agent repeats its last action based on how novel each state is. The approach uses binary hashing of image features to count state visits, with novel states receiving higher probability of action repetition. The method integrates with existing off-policy algorithms like DrQv2 without requiring additional value functions. A persistence adaptor module calculates repeat probabilities using the formula P(repeat) = α / max(1, sqrt(˜N(st))), where α is an exploration coefficient and ˜N(st) is a pseudo-count. The method transitions from high action persistence (exploration) to low action persistence (exploitation) as training progresses.

## Key Results
- Sample efficiency improved by 10-20% on average across multiple DMControl tasks
- SNAP can be seamlessly integrated with various basic exploration strategies
- Method shows significant performance gains on tasks like Acrobot-swingup, Reacher-hard, and Quadruped-run
- Ablation studies validate design choices and demonstrate robustness of the approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: State novelty measure enables smooth transition from exploration to exploitation.
- Mechanism: The method counts visited states using binary codes derived from image features, adjusting action repetition probability based on how familiar the agent is with each state. Higher probabilities are assigned to novel states, ensuring more action repetition when exploration is needed.
- Core assumption: Binary hashing of feature vectors preserves sufficient information about state novelty for effective exploration guidance.
- Evidence anchors:
  - [abstract]: "Our method enables the agent to adapt its action repetition strategy smoothly, striking a delicate balance between exploration and exploitation"
  - [section]: "We choose to use state-novelty, a measure to quantitatively evaluate how familiar the agent is with the current state"
  - [corpus]: Weak evidence - only 1 of 8 corpus papers mentions novelty-guided exploration directly
- Break condition: If state space becomes too large or image features don't capture relevant novelty, the count-based measure becomes ineffective.

### Mechanism 2
- Claim: Decoupling behavior policy from target policy enables efficient exploration without compromising training stability.
- Mechanism: The behavior policy uses state-guided action persistence for exploration, while the target policy remains focused on learning the optimal policy. This separation allows diverse exploration strategies without affecting the stability of the training process.
- Core assumption: Off-policy algorithms can effectively use different policies for behavior and target without introducing significant bias.
- Evidence anchors:
  - [section]: "By decoupling the behavior policy from the target policy, the off-policy framework allows for diverse exploration strategies without compromising the stability of the training process"
  - [abstract]: "our method can be seamlessly integrated into various basic exploration strategies to incorporate temporal persistence"
  - [corpus]: Weak evidence - corpus doesn't provide direct evidence for this specific decoupling mechanism
- Break condition: If the gap between behavior and target policies becomes too large, it may lead to unstable learning or poor policy convergence.

### Mechanism 3
- Claim: Progressive reduction in action persistence improves final policy performance.
- Mechanism: The method starts with high action persistence for broad exploration, then gradually reduces it as training progresses, allowing the agent to develop fine-grained policies for optimal performance.
- Core assumption: There exists an optimal trajectory from high to low action persistence that balances exploration benefits with exploitation quality.
- Evidence anchors:
  - [section]: "ideally, action persistence should gradually transition from large to small... action persistence is expected to gradually decrease for obtaining a fine-grained policy"
  - [section]: "the use of a smooth scheduling of the repeat probability allows a more effective balance between exploration and exploitation"
  - [corpus]: Weak evidence - corpus papers discuss action persistence but not specifically this progressive reduction mechanism
- Break condition: If the scheduling is too aggressive or too conservative, it may lead to either insufficient exploration or suboptimal final performance.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The paper builds on MDP formulation for reinforcement learning, where action persistence is a key parameter affecting state transitions
  - Quick check question: How does action persistence affect the effective horizon of an MDP?

- Concept: Count-based exploration
  - Why needed here: The novelty measure relies on counting state visits, a classic count-based exploration technique
  - Quick check question: What are the limitations of count-based methods in high-dimensional state spaces?

- Concept: Image-based reinforcement learning
  - Why needed here: The method uses image encoders to process visual inputs, requiring understanding of how visual features relate to state representation
  - Quick check question: How does using stacked frames affect the agent's ability to capture temporal information?

## Architecture Onboarding

- Component map:
  Image encoder (shared with critic) -> Quantization encoder (hash function for binary codes) -> State novelty counter (hash table) -> Action persistence adaptor (probability calculator) -> Behavior policy (uses adapted action persistence) -> Target policy (unchanged) -> Replay buffer (stores transitions)

- Critical path: Image → Feature vector → Binary code → State count → Repeat probability → Behavior policy action selection

- Design tradeoffs:
  - Hash function size vs. collision rate: Larger K reduces collisions but increases memory usage
  - State quantization granularity vs. counting accuracy: Finer quantization captures more states but may lead to sparsity
  - Exploration coefficient α vs. exploration-exploitation balance: Higher α encourages more exploration

- Failure signatures:
  - Poor exploration: Low state novelty values throughout training, suggesting the agent isn't discovering new states
  - Unstable learning: Large variance in performance, possibly due to mismatched behavior and target policies
  - Memory issues: Hash table growing too large, indicating insufficient state compression

- First 3 experiments:
  1. Test state novelty measure: Run the agent on a simple grid world and visualize state counts to verify the novelty measure captures exploration patterns
  2. Validate action persistence adaptation: Compare state coverage with and without the persistence adaptor on a controlled environment
  3. Integration test: Combine SNAP with a basic exploration strategy (epsilon-greedy) and measure sample efficiency improvements on a simple control task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SNAP compare to state-of-the-art exploration methods like curiosity-driven exploration or uncertainty-based exploration in complex, high-dimensional tasks?
- Basis in paper: [explicit] The paper mentions that SNAP can be integrated with basic exploration strategies and compares it to epsilon-zeta greedy, but does not compare it to more advanced exploration methods.
- Why unresolved: The paper focuses on comparing SNAP to simpler exploration methods and does not provide a comprehensive comparison with the latest state-of-the-art exploration techniques.
- What evidence would resolve it: Empirical results showing the performance of SNAP compared to curiosity-driven exploration, uncertainty-based exploration, and other advanced exploration methods on a variety of complex, high-dimensional tasks.

### Open Question 2
- Question: What is the impact of different state novelty measures on the performance of SNAP, and can we develop a more robust and adaptive measure of state novelty?
- Basis in paper: [explicit] The paper mentions that different measures of state novelty (count-based, k-means clustering) are compared, but does not explore the impact of more advanced or adaptive measures.
- Why unresolved: The paper only considers a limited set of state novelty measures and does not investigate the potential benefits of more sophisticated or adaptive measures.
- What evidence would resolve it: Empirical results showing the performance of SNAP with various state novelty measures, including more advanced or adaptive measures, on a range of tasks.

### Open Question 3
- Question: Can SNAP be extended to multi-agent reinforcement learning scenarios, and how would the state novelty measure be adapted for multiple agents?
- Basis in paper: [inferred] The paper focuses on single-agent reinforcement learning and does not discuss the potential application of SNAP to multi-agent scenarios.
- Why unresolved: The paper does not explore the extension of SNAP to multi-agent reinforcement learning, which is an important area of research with unique challenges.
- What evidence would resolve it: Empirical results showing the performance of SNAP in multi-agent reinforcement learning tasks, along with an analysis of how the state novelty measure can be adapted for multiple agents.

## Limitations
- Count-based novelty measures may struggle in high-dimensional state spaces common in image-based RL
- Binary hashing approach introduces quantization errors that could affect the reliability of novelty measurements
- Performance gains are environment-dependent, with some tasks showing minimal improvements
- Computational overhead of maintaining hash tables for state counting is not fully characterized

## Confidence
- High confidence in the basic mechanism of state-novelty guided action persistence and its integration with off-policy algorithms
- Medium confidence in the claimed sample efficiency improvements, as these are based on specific environments and hyperparameters
- Low confidence in the method's generalization to non-image-based environments and more complex tasks requiring longer horizons

## Next Checks
1. Test the robustness of the novelty measure by comparing performance when using different hash functions or quantization methods
2. Evaluate the method's performance on tasks with larger state spaces or more complex dynamics to assess scalability
3. Conduct a thorough ablation study varying the exploration coefficient α and the scheduling of action persistence to identify optimal configurations