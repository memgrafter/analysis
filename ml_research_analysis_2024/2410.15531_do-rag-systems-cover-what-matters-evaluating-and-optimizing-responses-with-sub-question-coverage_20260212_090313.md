---
ver: rpa2
title: Do RAG Systems Cover What Matters? Evaluating and Optimizing Responses with
  Sub-Question Coverage
arxiv_id: '2410.15531'
source_url: https://arxiv.org/abs/2410.15531
tags:
- core
- sub-questions
- answer
- sub-question
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel evaluation framework for Retrieval-Augmented
  Generation (RAG) systems based on sub-question coverage, which measures how well
  a RAG system addresses different facets of a question. The authors propose decomposing
  complex questions into sub-questions classified as core, background, or follow-up,
  reflecting their roles and importance.
---

# Do RAG Systems Cover What Matters? Evaluating and Optimizing Responses with Sub-Question Coverage

## Quick Facts
- arXiv ID: 2410.15531
- Source URL: https://arxiv.org/abs/2410.15531
- Authors: Kaige Xie; Philippe Laban; Prafulla Kumar Choubey; Caiming Xiong; Chien-Sheng Wu
- Reference count: 11
- Primary result: Novel evaluation framework using sub-question coverage achieves 82% accuracy in predicting human preference for RAG responses

## Executive Summary
This paper introduces a novel evaluation framework for Retrieval-Augmented Generation (RAG) systems based on sub-question coverage, which measures how well a RAG system addresses different facets of a question. The authors propose decomposing complex questions into sub-questions classified as core, background, or follow-up, reflecting their roles and importance. Using this categorization, they introduce a fine-grained evaluation protocol that provides insights into the retrieval and generation characteristics of RAG systems, including three commercial generative answer engines: You.com, Perplexity AI, and Bing Chat. The study finds that while all answer engines cover core sub-questions more often than background or follow-up ones, they still miss around 50% of core sub-questions, revealing clear opportunities for improvement. The sub-question coverage metrics prove effective for ranking responses, achieving 82% accuracy compared to human preference annotations. Additionally, the authors demonstrate that leveraging core sub-questions enhances both retrieval and answer generation in a RAG system, resulting in a 74% win rate over the baseline that lacks sub-questions.

## Method Summary
The paper proposes decomposing complex, open-ended questions into sub-questions and classifying them into core, background, and follow-up categories. Core sub-questions are essential for answering the main question, background sub-questions provide supporting context, and follow-up sub-questions address related but non-essential aspects. The framework uses GPT-4 to automatically generate and classify sub-questions, then measures coverage in both retrieved chunks and generated answers. The evaluation protocol includes six metrics capturing retrieval and generation characteristics separately. For optimization, the authors incorporate core sub-questions into RAG systems by using them during chunk retrieval and reranking, and prompting generation models to address them. The implementation uses LlamaIndex with OpenAI's text-embedding-ada-002 model, evaluated on 200 open-ended questions from the Researchy Questions dataset and 500 questions from the WebGPT Comparisons dataset for human preference analysis.

## Key Results
- Commercial answer engines miss approximately 50% of core sub-questions despite covering them more often than background or follow-up sub-questions
- Sub-question coverage metrics achieve 82% accuracy in ranking responses compared to human preference annotations
- Incorporating core sub-questions into RAG systems yields a 74% win rate over baseline systems
- The most effective augmentation strategy uses core sub-questions during chunk retrieval and reranking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing questions into sub-questions improves RAG system evaluation by providing fine-grained coverage metrics
- Mechanism: Complex open-ended questions are broken down into core, background, and follow-up sub-questions. The system then measures coverage of each sub-question type in both retrieved chunks and generated answers, revealing gaps in retrieval and generation separately
- Core assumption: Sub-questions accurately represent the facets of the main question that need to be addressed
- Evidence anchors:
  - [abstract] "We propose decomposing questions into sub-questions and classifying them into three types -- core, background, and follow-up -- to reflect their roles and importance"
  - [section 3.1] "We decompose complex, open-ended, non-factoid questions into a comprehensive set of sub-questions; we classify these sub-questions into core, background, and follow-up categories based on their functional roles"
  - [corpus] Weak - corpus provides related work on retrieval and evaluation but not specifically on sub-question decomposition methodology
- Break condition: If sub-question decomposition fails to capture important aspects of the original question, the coverage metrics will be misleading and fail to identify actual performance gaps

### Mechanism 2
- Claim: Core sub-question coverage correlates strongly with human perception of answer quality
- Mechanism: The framework measures how many core sub-questions are addressed in generated answers, and this metric shows strong correlation (78-82%) with human preference scores compared to traditional LLM-as-judge methods
- Core assumption: Human preference for answers is primarily determined by how well core aspects of the question are addressed
- Evidence anchors:
  - [abstract] "sub-question coverage metrics prove effective for ranking responses, achieving 82% accuracy compared to human preference annotations"
  - [section 4] "Leveraging human preference data from the WebGPT Comparisons dataset... we find that addressing core sub-questions correlates most positively with human preferences"
  - [corpus] Weak - corpus contains related work on RAG evaluation but lacks specific evidence about sub-question coverage correlation with human preferences
- Break condition: If users value background or follow-up information more than assumed, or if the weightings between sub-question types vary significantly across contexts, the correlation may weaken

### Mechanism 3
- Claim: Incorporating core sub-questions into RAG systems significantly improves response quality
- Mechanism: Core sub-questions are used to augment retrieval (by retrieving chunks for both the original query and core sub-questions separately) and generation (by prompting models to address core sub-questions), resulting in 74% win rate over baseline
- Core assumption: Retrieval and generation improvements focused on core information transfer effectively to overall answer quality
- Evidence anchors:
  - [abstract] "we also demonstrate that leveraging core sub-questions enhances both retrieval and answer generation in a RAG system, resulting in a 74% win rate over the baseline that lacks sub-questions"
  - [section 5] "We evaluate four strategies for incorporating sub-question type information... demonstrating that the most effective approach uses core sub-questions during chunk retrieval and reranking"
  - [corpus] Weak - corpus provides related work on query optimization and RAG improvements but lacks specific evidence about core sub-question incorporation effectiveness
- Break condition: If the overhead of processing multiple core sub-questions outweighs benefits, or if retrieval augmentation introduces noise that degrades generation quality

## Foundational Learning

- Concept: Question decomposition techniques
  - Why needed here: The entire framework depends on breaking down complex questions into manageable sub-components that can be individually evaluated
  - Quick check question: Given "How does climate change affect global food security?", what would be three potential sub-questions that capture different facets of this topic?

- Concept: Retrieval-augmented generation (RAG) architecture
  - Why needed here: Understanding how retrieval and generation components interact is crucial for interpreting coverage metrics and improvement strategies
  - Quick check question: In a typical RAG system, what happens if relevant documents are retrieved but the generation model fails to incorporate the information effectively?

- Concept: Evaluation metric design and correlation analysis
  - Why needed here: The framework introduces novel metrics and must demonstrate their correlation with human preferences to be valid
  - Quick check question: If a new evaluation metric achieves 90% accuracy in predicting human preferences, what statistical considerations should be examined to validate this claim?

## Architecture Onboarding

- Component map:
  - Question Decomposition Engine -> Coverage Measurement Module -> Evaluation Protocol -> RAG Enhancement Pipeline
  - Complex question -> Sub-question generation and classification -> Coverage evaluation -> Optimization implementation

- Critical path:
  1. Input question → Decomposition → Classification
  2. Decomposed sub-questions + question → Retrieval
  3. Retrieved chunks + sub-questions → Coverage measurement
  4. Generated answer + sub-questions → Coverage measurement
  5. Coverage metrics → Evaluation/Improvement decisions

- Design tradeoffs:
  - Granularity vs. computational cost: More sub-questions provide better coverage but increase processing time
  - Automation vs. accuracy: LLM-based decomposition is fast but may miss nuances that human annotation would catch
  - Complexity vs. interpretability: Multiple metrics provide detailed insights but may be harder to act upon than single scores

- Failure signatures:
  - Low coverage across all sub-question types suggests retrieval issues
  - High retrieval coverage but low answer coverage suggests generation issues
  - Uneven coverage between core and other types suggests prioritization problems
  - Inconsistent coverage across similar questions suggests decomposition instability

- First 3 experiments:
  1. Implement question decomposition on a small dataset and manually verify classification accuracy against human annotations
  2. Measure coverage metrics on existing RAG system outputs to establish baseline patterns
  3. Test core sub-question augmentation on retrieval only, comparing chunk relevance scores before and after augmentation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can sub-question coverage metrics be adapted for real-time RAG system evaluation during user interactions?
- Basis in paper: [explicit] The paper mentions computational demands may limit real-time application potential
- Why unresolved: The paper demonstrates effectiveness of sub-question coverage for offline evaluation but doesn't explore implementation strategies for dynamic, real-time assessment during actual user sessions
- What evidence would resolve it: Studies comparing sub-question coverage implementation in production RAG systems versus traditional evaluation methods, measuring latency and user experience impact

### Open Question 2
- Question: How do different user personas and use cases affect the optimal weighting coefficients for the hybrid sub-question coverage metric?
- Basis in paper: [explicit] The paper notes that human preferences can be subjective, varying across different user personas and use cases, which may affect the ratios between weighting coefficients
- Why unresolved: The paper performs a grid search on a hold-out validation set but doesn't systematically explore how different user groups (e.g., researchers vs. general public) would require different weightings
- What evidence would resolve it: User studies with diverse personas evaluating RAG responses, measuring how preference distributions change across different demographic and professional groups

### Open Question 3
- Question: What is the relationship between sub-question coverage completeness and actual user satisfaction in practical applications?
- Basis in paper: [inferred] While the paper shows sub-question coverage correlates with human preferences in controlled studies, it doesn't examine real-world user satisfaction outcomes
- Why unresolved: The paper establishes correlation between core sub-question coverage and human preference ratings but doesn't bridge this gap to actual user experience metrics like task completion rates or satisfaction scores
- What evidence would resolve it: Longitudinal studies tracking users interacting with RAG systems optimized for sub-question coverage versus baseline systems, measuring both objective task success metrics and subjective satisfaction surveys

## Limitations
- Subjective nature of sub-question classification, with human annotators achieving only 74.6% accuracy
- Limited evaluation dataset of 200 questions focused on commercial answer engines
- Computational overhead of processing multiple core sub-questions for real-time applications
- Potential for sub-question decomposition to miss important facets of complex questions

## Confidence
- Core sub-question coverage correlation with human preferences: High (82% accuracy validated against WebGPT dataset)
- Core sub-question incorporation effectiveness: Medium (74% win rate depends on implementation details not fully disclosed)
- Framework generalizability: Low (limited dataset and focus on commercial engines)

## Next Checks
1. Replicate the human preference correlation study on a different dataset with independently collected human judgments to verify the 82% accuracy claim across domains
2. Implement the core sub-question augmentation strategy on an open-source RAG system and measure performance gains across different model sizes and embedding dimensions
3. Conduct ablation studies to quantify the impact of different sub-question types by systematically removing background and follow-up sub-questions from the evaluation framework