---
ver: rpa2
title: 'WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?'
arxiv_id: '2403.07718'
source_url: https://arxiv.org/abs/2403.07718
tags:
- tasks
- workarena
- agents
- action
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces WorkArena, a benchmark of 29 enterprise-related
  web tasks on the ServiceNow platform, and BrowserGym, a flexible environment for
  evaluating web agents. The authors assess the performance of state-of-the-art LLM-based
  agents on WorkArena and analyze the impact of BrowserGym's features.
---

# WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?

## Quick Facts
- **arXiv ID**: 2403.07718
- **Source URL**: https://arxiv.org/abs/2403.07718
- **Reference count**: 40
- **Primary result**: Even the best agents achieve only a 54.8% success rate on WorkArena, with open-source models like CodeLlama failing completely.

## Executive Summary
WorkArena introduces a challenging benchmark of 29 enterprise-related web tasks on the ServiceNow platform, paired with BrowserGym, a flexible environment for evaluating web agents. The authors assess state-of-the-art LLM-based agents and find significant performance gaps, with even the best models struggling to complete tasks autonomously. The study reveals that while BrowserGym's multimodal observations and BID-based actions offer potential benefits, they can also overwhelm models with large context sizes. The results highlight the need for improved web automation capabilities and more robust evaluation frameworks.

## Method Summary
The authors created WorkArena, a benchmark of 29 tasks on ServiceNow, and BrowserGym, an OpenAI Gym wrapper for web automation using Chromium via Chrome DevTools Protocol and Playwright. They evaluated GPT-3.5, GPT-4, and CodeLlama using a simple agent design with chain-of-thought prompting. Tasks were presented as chat messages, and agents could respond or ask questions. Success rates were measured across both MiniWoB and WorkArena tasks, with additional analysis of BrowserGym's features like multimodal observations and BID-based actions.

## Key Results
- Best agents achieve only 54.8% success rate on WorkArena tasks
- CodeLlama fails to complete any WorkArena tasks, highlighting open-source limitations
- BrowserGym features like multimodal observations can improve performance but may overwhelm models with large context sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: BrowserGym's multimodal observation space improves agent performance on complex web tasks.
- **Mechanism**: The environment provides DOM, AXTree, screenshot, and error logs as observations, allowing agents to choose the most informative modality for decision-making. This addresses the challenge of large DOM trees by offering alternative representations.
- **Core assumption**: Different task types benefit from different observation modalities, and agents can learn to select the most appropriate one.
- **Evidence anchors**: [abstract]: "BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations." [section]: "BrowserGym introduces an accessibility representation of websites (Zhou et al., 2023), significantly diminishing the context size required for processing."
- **Break condition**: If all tasks in WorkArena can be solved using only BID actions, then the additional modalities provide no benefit and may even hinder performance by overwhelming the model.

### Mechanism 2
- **Claim**: The BID-based action space provides unambiguous interaction with web elements.
- **Mechanism**: BrowserGym marks every element with a unique BID identifier and coordinates, allowing agents to specify actions at the object level rather than relying on text-based selection or coordinate-based clicks.
- **Core assumption**: Unique element identifiers and coordinates enable more reliable interaction than text-based approaches, especially in dynamic UIs with non-standard DOM structures.
- **Evidence anchors**: [abstract]: "BrowserGym marks every element on the web pages with a unique identifier bid, its center screen coordinates (x,y), its bounding box (left,top,right,bottom), and a visibility flag (visible)." [section]: "These attributes provide a crude summary of the visual rendering of the UI and allow unambiguous interaction with individual elements through their identifiers."
- **Break condition**: If the BID system fails to handle dynamic content changes or elements that appear/disappear during interaction, agents may select invalid BIDs leading to failures.

### Mechanism 3
- **Claim**: The chat-based interaction modality enables information retrieval tasks and sequential workflows.
- **Mechanism**: WorkArena tasks are presented as initial chat messages, and agents can respond with answers or ask clarifying questions, supporting both one-shot answers and multi-step interactions.
- **Core assumption**: Chat interface provides a natural way to present tasks and receive responses, particularly for information retrieval where specific answers are expected.
- **Evidence anchors**: [abstract]: "Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation." [section]: "In WorkArena, the goal of each task is provided as the initial user message, to which the agent can reply at any time."
- **Break condition**: If agents cannot effectively parse natural language goals or maintain context across multiple chat turns, the chat interface provides no advantage over direct task specification.

## Foundational Learning

- **Concept**: Partially Observable Markov Decision Process (POMDP)
  - **Why needed here**: BrowserGym follows a POMDP paradigm where the agent receives partial observations (DOM, AXTree, screenshots) rather than full state information about the web page.
  - **Quick check question**: How does the POMDP framework handle situations where the agent needs to infer hidden state information from partial observations?

- **Concept**: Accessibility Tree (AXTree)
  - **Why needed here**: AXTree provides a semantic representation of the UI that is more concise than the full DOM and focuses on elements relevant to user interaction.
  - **Quick check question**: What advantages does the AXTree offer over raw HTML DOM for web automation tasks, and what information might be lost in this abstraction?

- **Concept**: Shadow DOM and iFrames
  - **Why needed here**: WorkArena tasks involve ServiceNow pages that use nested iFrames and shadow DOMs, which require special handling for web automation.
  - **Quick check question**: How do iFrames and shadow DOMs complicate web automation, and what strategies can agents use to navigate these encapsulation techniques?

## Architecture Onboarding

- **Component map**: Environment (BrowserGym) -> Browser Control (Chromium via Chrome DevTools Protocol) -> Observation Space (Chat history, URLs, error logs, DOM, AXTree, screenshots) -> Action Space (BID-based, coordinate-based, Python code, tab navigation) -> Agent (LLM-based with chain-of-thought prompting) -> Tasks (WorkArena benchmark with validation functions)

- **Critical path**: 1. Environment initializes ServiceNow instance 2. Task setup creates required database entries 3. Agent receives observation (goal + page state) 4. Agent generates action via LLM 5. Environment executes action and updates state 6. Validation function checks task completion 7. Repeat until success or max steps reached

- **Design tradeoffs**: Large observation space vs. context window limitations, Rich action space vs. safety and reliability concerns, Real ServiceNow instances vs. simulation speed and cost, Zero-shot learning vs. few-shot fine-tuning for better performance

- **Failure signatures**: Context truncation errors when DOM/AXTree exceed model limits, Invalid BID selection when elements dynamically change, Syntax errors in generated Python code, Infinite loops in sequential tasks due to poor state tracking

- **First 3 experiments**: 1. Run a simple form-filling task with GPT-4 to verify basic functionality 2. Test a list-filtering task to evaluate BID-based interaction 3. Attempt a knowledge base search to validate chat-based information retrieval

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Why does WorkArena perform significantly worse than MiniWoB despite both using similar LLM agents?
- **Basis in paper**: [explicit] Comparison of success rates between WorkArena and MiniWoB tasks.
- **Why unresolved**: The paper notes that WorkArena is more challenging due to complex UIs, large DOM trees, and non-standard HTML, but doesn't quantify the relative contribution of each factor.
- **What evidence would resolve it**: Controlled experiments isolating each complexity factor (e.g., testing on simplified ServiceNow tasks with standard HTML).

### Open Question 2
- **Question**: How does the truncation of large DOM trees affect agent performance on WorkArena tasks?
- **Basis in paper**: [explicit] Discussion of large DOM sizes (40k-500k tokens) exceeding model context windows.
- **Why unresolved**: The paper mentions truncation occurs but doesn't measure the performance impact of different truncation strategies.
- **What evidence would resolve it**: Systematic testing of agent performance with varying truncation levels and methods.

### Open Question 3
- **Question**: What specific aspects of CodeLlama's architecture make it fail on WorkArena while performing reasonably on MiniWoB?
- **Basis in paper**: [explicit] CodeLlama's 0% success rate on WorkArena versus 25.5% on MiniWoB.
- **Why unresolved**: The paper identifies failure modes but doesn't analyze architectural differences between CodeLlama and other LLMs.
- **What evidence would resolve it**: Comparative analysis of CodeLlama's attention mechanisms, context handling, and code generation capabilities versus GPT models.

## Limitations

- Evaluation conducted exclusively on ServiceNow instances, limiting generalizability to other platforms
- Narrow selection of LLMs tested, with potential performance gaps between open-source and closed-source models
- Benchmark focuses on tasks with clear success criteria, potentially missing complexity of real-world workflows

## Confidence

- **High Confidence**: WorkArena is a challenging benchmark requiring multiple actions and complex reasoning
- **Medium Confidence**: BrowserGym's multimodal observations and BID-based actions may improve agent performance
- **Low Confidence**: There remains a considerable gap towards achieving full task automation

## Next Checks

1. **Cross-platform validation**: Test WorkArena agents on at least two additional enterprise platforms (e.g., Salesforce, SAP) to assess generalizability beyond ServiceNow

2. **Open-source model scaling**: Evaluate whether fine-tuned or larger versions of open-source models (such as CodeLlama-70B or LLaMA-2-70B) can close the performance gap observed with GPT-4

3. **Context size optimization**: Systematically measure the impact of different observation truncation strategies on performance across task types