---
ver: rpa2
title: What Should Embeddings Embed? Autoregressive Models Represent Latent Generating
  Distributions
arxiv_id: '2406.03707'
source_url: https://arxiv.org/abs/2406.03707
tags:
- topic
- sufficient
- embeddings
- language
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates what embeddings in autoregressive language
  models should represent by connecting them to predictive sufficient statistics.
  The authors identify three cases where optimal embedding content can be formally
  characterized: exchangeable data (where embeddings should capture sufficient statistics),
  latent state models (where embeddings should encode posterior distributions over
  next states), and discrete hypothesis spaces (where embeddings should reflect posterior
  distributions over hypotheses).'
---

# What Should Embeddings Embed? Autoregressive Models Represent Latent Generating Distributions

## Quick Facts
- arXiv ID: 2406.03707
- Source URL: https://arxiv.org/abs/2406.03707
- Authors: Liyi Zhang, Michael Y. Li, R. Thomas McCoy, Theodore R. Sumers, Jian-Qiao Zhu, Thomas L. Griffiths
- Reference count: 40
- One-line primary result: Autoregressive transformer embeddings encode predictive sufficient statistics and posterior distributions over latent variables, with probing accuracy ranging from 43-89% on natural corpora.

## Executive Summary
This paper establishes a theoretical connection between autoregressive language model objectives and what embeddings should represent. The authors show that embeddings from autoregressive models correspond to predictive sufficient statistics - compressed representations that contain all information needed to predict the next token. Through probing experiments on synthetic and natural data, they demonstrate that transformers successfully encode sufficient statistics for exchangeable models, posterior distributions over latent states in HMMs, and topic mixtures from LDA, even in partially exchangeable sequences. The embeddings generalize to out-of-distribution cases and do not rely on token memorization, providing a principled answer to what language model embeddings should represent.

## Method Summary
The authors train transformers on synthetic datasets generated from various probabilistic models (Gaussian-Gamma, Beta-Bernoulli, Gamma-Exponential, discrete hypothesis spaces, HMMs, and LDA topic models). They then use linear probes to decode target quantities (sufficient statistics, posterior distributions) from the final-layer embeddings. Control experiments include testing on mismatched datasets, using different embedding layers, comparing autoregressive vs. masked language models, and measuring OOD generalization. The method validates whether embeddings capture predictive sufficient statistics by measuring probe accuracy and comparing with alternative measures like token-level recovery.

## Key Results
- Transformers trained on exchangeable data encode sufficient statistics, recovering mean and variance of posterior distributions with high accuracy
- Embeddings from HMM-trained models successfully encode posterior distributions over next latent states, outperforming alternative targets like most likely state
- Topic distributions from LDA are encoded in LLM embeddings when topics form a significant subcomponent (semantic proportion p ≥ 0.4)
- Encoding persists in pretrained large language models on natural corpora (20Newsgroups, WikiText-103) with accuracy scores of 43-89%
- Results generalize to out-of-distribution cases and do not rely on token memorization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Autoregressive models learn predictive sufficient statistics that capture all information needed to predict the next token.
- Mechanism: The autoregressive objective forces the model to compress sequence information into embeddings such that future predictions depend only on these embeddings, not the full sequence history. This compression must preserve predictive information, making the embeddings sufficient statistics.
- Core assumption: The model achieves near-optimal autoregressive prediction performance during training.
- Evidence anchors:
  - [abstract]: "We show that the embeddings from autoregressive models correspond to predictive sufficient statistics."
  - [section]: "if a model performs autoregressive modeling perfectly, it learns a predictive sufficient statistic."
  - [corpus]: Weak evidence - only 0 citations, suggesting limited independent validation of this specific mechanism claim.
- Break condition: If the autoregressive model fails to achieve near-optimal prediction performance, or if the data violates exchangeability assumptions where sufficient statistics don't exist.

### Mechanism 2
- Claim: Embeddings encode posterior distributions over latent variables in structured generative models.
- Mechanism: In latent state models (HMMs) and exchangeable models with latent parameters, the optimal embedding must contain the posterior distribution over the relevant latent variables because this is the predictive sufficient statistic. The autoregressive model implicitly performs Bayesian inference to achieve optimal prediction.
- Core assumption: The autoregressive model can approximate the required integrals and posterior computations through its architecture.
- Evidence anchors:
  - [abstract]: "we show that embeddings of autoregressive models encode these explainable quantities of interest" for latent state models.
  - [section]: "the embedding should encode the posterior distribution over the next latent state given the data" for latent state models.
  - [corpus]: Weak evidence - only 0 citations, suggesting limited independent validation of this specific mechanism claim.
- Break condition: When the required posterior computations are too complex for the model to approximate accurately, or when the latent structure is too intricate for the model's capacity.

### Mechanism 3
- Claim: Topic distributions from LDA are encoded in LLM embeddings because they form a predictive sufficient statistic for topic modeling tasks.
- Mechanism: Since LDA is an exchangeable generative model, the topic mixture parameters form sufficient statistics. Autoregressive models trained on such data will encode these parameters to achieve optimal prediction. This encoding persists even in partially exchangeable sequences where topics form a significant subcomponent.
- Core assumption: Topic mixtures are a significant enough component of the text to be learned as predictive sufficient statistics.
- Evidence anchors:
  - [abstract]: "we extend our analysis of exchangeable models to more realistic scenarios by generating synthetic data where only a fraction of words are generated from LDA."
  - [section]: "topic mixture θ and word distribution β form the sufficient statistic, our theory suggests that autoregressive LMs should implicitly encode the topic structure of a document."
  - [corpus]: Weak evidence - only 0 citations, suggesting limited independent validation of this specific mechanism claim.
- Break condition: When topic mixtures constitute a small enough proportion of the text that they no longer form predictive sufficient statistics for the autoregressive objective.

## Foundational Learning

- Concept: Predictive sufficiency and sufficient statistics
  - Why needed here: This is the theoretical foundation that connects autoregressive objectives to what embeddings should contain. Without understanding that embeddings must be sufficient statistics for prediction, the rest of the analysis lacks theoretical grounding.
  - Quick check question: If a statistic s(x) is sufficient for parameter θ, what does this imply about the relationship between p(x|θ) and p(s(x)|θ)?

- Concept: Exchangeability and de Finetti's theorem
  - Why needed here: These concepts establish when and how sufficient statistics can be identified in data. The paper relies on exchangeability to justify that topic mixtures and other latent parameters can be encoded as sufficient statistics.
  - Quick check question: What is the key condition required for de Finetti's theorem to apply, and what does it tell us about the structure of exchangeable data?

- Concept: Bayesian inference and posterior distributions
  - Why needed here: The paper argues that autoregressive models implicitly perform Bayesian inference by encoding posterior distributions over latent variables. Understanding Bayesian updating is crucial for grasping why these posteriors form predictive sufficient statistics.
  - Quick check question: In a conjugate Bayesian model, what is the relationship between the posterior distribution and the sufficient statistics of the observed data?

## Architecture Onboarding

- Component map:
  Transformer backbone -> Embedding layer -> Decoder layers -> Final embedding -> Output layer -> Next token prediction
  (Probe network) -> Linear classifier that maps embeddings to target quantities

- Critical path: Input tokens → Embedding layer → Decoder layers → Final embedding → Output layer → Next token prediction
  The critical path for probing: Input tokens → Embedding layer → Decoder layers → Final embedding → Probe network → Target quantity

- Design tradeoffs:
  - Model capacity vs. interpretability: Larger models may capture more complex latent structures but make probing more difficult
  - Autoregressive vs. masked objectives: Autoregressive objectives provide more expressive Bayesian inference capabilities
  - Embedding dimension: Higher dimensions may capture more information but require more data and computation

- Failure signatures:
  - Poor probe accuracy indicates insufficient information in embeddings
  - Memorization (high token-level probe accuracy) suggests failure to capture sufficient statistics
  - Poor OOD generalization suggests overfitting to specific data distributions rather than learning general principles

- First 3 experiments:
  1. Train a simple autoregressive transformer on synthetic exchangeable data (Gaussian-Gamma or Beta-Bernoulli) and probe for sufficient statistics to verify the basic mechanism
  2. Test probe performance on out-of-distribution data to validate generalization claims
  3. Compare autoregressive vs. masked language model performance on the same task to demonstrate the importance of the autoregressive objective

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do transformer-based language models capture predictive sufficient statistics in partially exchangeable sequences where only a fraction of the data comes from an exchangeable generative process?
- Basis in paper: [explicit] The authors demonstrate that language models encode topic distributions when topics form a significant subcomponent of partially exchangeable sequences in the HMM-LDA experiments, showing successful encoding when the semantic proportion p ≥ 0.4.
- Why unresolved: While the paper shows success for topic mixtures in partially exchangeable sequences, it remains unclear whether this extends to other forms of partial exchangeability beyond topic models, such as sentiment or author type, and what proportion of exchangeable data is necessary for successful encoding in different domains.
- What evidence would resolve it: Systematic experiments varying the proportion of exchangeable versus non-exchangeable components across different latent variables (sentiment, author type, etc.) while measuring encoding accuracy would clarify the generalizability of this finding.

### Open Question 2
- Question: How does the expressivity of autoregressive language models compare to masked language models in capturing latent generating distributions?
- Basis in paper: [explicit] The authors provide theoretical analysis showing that masked language models form a less expressive Bayesian inference objective than autoregressive models because each token is predicted independently from the same posterior over the latent variable, rather than a different posterior at each step.
- Why unresolved: While the theoretical analysis suggests autoregressive models should be more expressive, empirical comparisons between autoregressive and masked models on recovering latent variables across diverse generative processes are limited in the paper.
- What evidence would resolve it: Direct empirical comparisons measuring recovery accuracy of predictive sufficient statistics across multiple latent variable models (HMMs, topic models, Bayesian conjugate models) using both autoregressive and masked architectures would quantify the expressivity difference.

### Open Question 3
- Question: Do transformer embeddings encode higher-order moments of posterior distributions beyond mean and variance in Bayesian conjugate models?
- Basis in paper: [inferred] The authors successfully probe and recover first and second moments (mean and variance) of posterior distributions in Bayesian conjugate models, but do not explore higher-order moments like skewness and kurtosis.
- Why unresolved: The paper focuses on first and second moments which are most directly related to parameter estimation, but higher-order moments could provide additional information about uncertainty representation and distributional shape that might be encoded in the embeddings.
- What evidence would resolve it: Probing experiments targeting third and fourth moments of posterior distributions in Bayesian conjugate models, along with comparison of probe complexity required to recover different moment orders, would determine whether higher-order moments are encoded and accessible.

## Limitations
- The analysis relies heavily on synthetic data experiments, which may not fully capture the complexity of real-world language data
- The probing methodology assumes linear probes can adequately recover target quantities, potentially underestimating models' true representational capabilities
- The paper focuses on decoder-only autoregressive transformers, leaving open questions about other architectures

## Confidence
- High Confidence: Core theoretical claims about predictive sufficient statistics in exchangeable models are well-established and empirically validated across multiple synthetic scenarios
- Medium Confidence: Extension to topic models and natural language data is reasonable but less thoroughly validated with only two natural corpora examined
- Low Confidence: The assertion that autoregressive models perform Bayesian inference through their architecture is conceptually appealing but mechanistically underspecified

## Next Checks
1. **Cross-Architecture Validation**: Replicate the synthetic experiments (Gaussian-Gamma, Beta-Bernoulli) using different autoregressive architectures (RNNs, LSTMs, GPT-style transformers with varying depths) to test whether the sufficient statistic encoding is architecture-dependent or a general property of autoregressive training.

2. **Nonlinear Probing Analysis**: Apply nonlinear probes (small MLPs with 1-2 hidden layers) to the same synthetic tasks and compare performance with linear probes. This would determine whether the linear probe assumption is valid or whether the relationship between embeddings and sufficient statistics requires nonlinear extraction.

3. **Natural Language Corpus Expansion**: Apply the probing methodology to additional diverse natural language datasets (e.g., PubMed abstracts for scientific topics, legal documents for specialized terminology) and systematically vary the proportion of topic-like vs. non-topic-like text to map the boundary conditions where topic mixture encoding breaks down.