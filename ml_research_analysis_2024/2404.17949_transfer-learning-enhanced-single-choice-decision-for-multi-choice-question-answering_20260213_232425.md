---
ver: rpa2
title: Transfer Learning Enhanced Single-choice Decision for Multi-choice Question
  Answering
arxiv_id: '2404.17949'
source_url: https://arxiv.org/abs/2404.17949
tags:
- question
- answer
- multi-choice
- learning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel single-choice model for multi-choice
  machine reading comprehension (MMRC) that considers options separately, overcoming
  the limitations of traditional multi-choice models. The key idea is to train a binary
  classifier to determine if an answer is correct, then select the option with the
  highest confidence score.
---

# Transfer Learning Enhanced Single-choice Decision for Multi-choice Question Answering

## Quick Facts
- arXiv ID: 2404.17949
- Source URL: https://arxiv.org/abs/2404.17949
- Reference count: 15
- Key outcome: Achieves state-of-the-art accuracy on RACE (90.7%) and DREAM (93.2%) using single-choice formulation with transfer learning

## Executive Summary
This paper proposes a novel single-choice decision framework for multi-choice machine reading comprehension that reconstructs the problem as independent binary classification for each answer option. By treating each (passage, question, option) triplet separately rather than modeling all options jointly, the approach enables effective knowledge transfer from extractive MRC datasets like SQuAD. The model uses ALBERT-xxlarge with layer-wise adaptive attention to aggregate information across transformer layers, achieving state-of-the-art results on RACE and DREAM benchmarks.

## Method Summary
The method converts multi-choice questions into single-choice binary classification by training a model to predict whether each option is correct given the passage and question. It uses ALBERT-xxlarge as the encoder and introduces layer-wise adaptive attention to combine representations from all transformer layers rather than using only the final layer. The model is first trained on a mixture of MRC datasets in binary format, then fine-tuned on the target RACE dataset. This transfer learning approach allows the model to leverage knowledge from extractive QA tasks that cannot be directly used by traditional multi-choice models.

## Key Results
- Achieves 90.7% accuracy on RACE test set, state-of-the-art performance
- Achieves 93.2% accuracy on DREAM test set, state-of-the-art performance
- Transfer learning from multiple MRC datasets significantly improves performance over training on RACE alone
- Layer-wise adaptive attention provides performance gains over standard [CLS] token usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-choice reconstruction enables knowledge transfer from extractive MRC datasets like SQuAD by converting each (passage, question, answer) triplet into a binary classification instance.
- Mechanism: Instead of modeling all answer options jointly, the model learns to independently score whether each option is correct given passage and question. This removes the multi-choice constraint, allowing use of datasets with a single correct answer per question.
- Core assumption: Binary classification on single options can capture the same discriminative signal as full multi-choice comparison, and diversity in training is maintained when options are considered separately.
- Evidence anchors:
  - [abstract] "we reconstruct multi-choice to single-choice by training a binary classification to distinguish whether a certain answer is correct"
  - [section] "For each option of a given context and question, we calculate a confidence score. Then we select the one with the highest score as the final answer"
  - [corpus] Weak. No direct citations found for this binary conversion strategy.
- Break condition: If the binary classifier cannot distinguish subtle differences between closely related options, or if diversity drops due to repetitive context in training samples.

### Mechanism 2
- Claim: Layer-wise adaptive attention aggregates information across all transformer layers, capturing both early-layer interaction and late-layer answer-focus patterns.
- Mechanism: Each layer's [CLS] and pooled representation are weighted adaptively and summed to form the final sentence representation. This allows the model to leverage both early-layer question-passage interaction and late-layer answer-related focus.
- Core assumption: Different layers encode complementary information; a weighted combination improves over using only the last layer's [CLS] token.
- Evidence anchors:
  - [section] "we propose layer-wise adaptive attention to obtain the sentence representation"
  - [section] "Different layers have different concerns... Therefore, it is not efficient enough to just use the vector of last layer"
  - [corpus] No direct evidence in corpus; claim based on internal analysis.
- Break condition: If layer weights become uniform or collapse to a single layer, or if the adaptive attention adds noise instead of signal.

### Mechanism 3
- Claim: Transfer learning from multiple QA datasets improves performance by exposing the model to broader question-answering patterns and domains.
- Mechanism: The model is first trained on a large mixed corpus of QA instances from multiple datasets, then fine-tuned on the target RACE dataset. This pre-training step injects knowledge from diverse sources before specialization.
- Core assumption: The binary classification format is general enough to absorb patterns from both extractive (SQuAD, CoQA) and multi-choice (DREAM, ARC) datasets, and that these patterns are useful for RACE.
- Evidence anchors:
  - [section] "by transferring knowledge from other QA datasets... our model achieves state-of-the-art results"
  - [section] "we propose a simple yet effective strategy to transfer knowledge from other QA datasets"
  - [corpus] Weak. No cited studies on this exact transfer approach; results are empirical.
- Break condition: If transferred knowledge is domain-mismatched (e.g., casual dialogue vs. formal exam), or if fine-tuning erases pre-trained benefits.

## Foundational Learning

- Concept: Binary classification loss formulation
  - Why needed here: The single-choice model treats each (passage, question, option) as an independent instance with label 1 if correct, 0 if distractor. This differs from standard multi-choice cross-entropy.
  - Quick check question: In the loss function, what are the two classes and their labels for a correct answer instance?

- Concept: Layer aggregation and attention weighting
  - Why needed here: Layer-wise adaptive attention requires understanding how to combine hidden states across layers with learned weights, and how to interpret the [CLS] vs. pooled representations.
  - Quick check question: How does the model compute the weight αi for layer i in the layer-wise adaptive attention?

- Concept: Transfer learning via dataset preprocessing
  - Why needed here: Converting extractive and multi-choice datasets into the binary format used by the single-choice model is essential for effective knowledge transfer.
  - Quick check question: What preprocessing step converts an extractive MRC instance into a binary classification example?

## Architecture Onboarding

- Component map:
  Input sequence (passage + question + option) -> ALBERT-xxlarge encoder -> Layer-wise adaptive attention (weighted sum of [CLS] and pooled vectors across layers) -> Binary classifier -> Confidence score for each option

- Critical path:
  Input sequence → ALBERT-xxlarge → Layer-wise adaptive attention → Binary classifier → Confidence scores → Option with maximum score selected

- Design tradeoffs:
  - Single-choice vs. multi-choice: Simpler integration of external data vs. potential loss of relative option comparison
  - Layer-wise attention vs. last-layer [CLS]: Richer representation at cost of additional parameters and computation
  - Binary classification vs. multi-class: More flexible dataset usage vs. possible weaker discrimination between options

- Failure signatures:
  - Low accuracy on in-domain RACE but good on transferred datasets: Possible overfitting or domain mismatch during transfer
  - No improvement from layer-wise attention: Attention weights may be degenerate or uninformative
  - Degraded performance when adding more datasets: Negative transfer or noise from incompatible formats

- First 3 experiments:
  1. Train baseline ALBERT with multi-choice loss on RACE; record dev accuracy
  2. Train single-choice model with layer-wise adaptive attention on RACE only; compare accuracy and examine attention weight distribution
  3. Add SQuAD instances as positive samples; train single-choice model and measure accuracy gain

## Open Questions the Paper Calls Out

- Question: How does the single-choice model's performance scale with different numbers of options per question, and is there an optimal number of options for maximizing accuracy?
  - Basis in paper: [inferred] The paper mentions that the number of options varies across datasets and that multi-choice models face challenges when applied to datasets with different numbers of options.
  - Why unresolved: The paper does not provide experimental results comparing the single-choice model's performance across datasets with varying numbers of options.
  - What evidence would resolve it: Experiments comparing the single-choice model's accuracy on datasets with 3, 4, and more options per question.

- Question: How does the layer-wise adaptive attention mechanism compare to other attention mechanisms, such as self-attention or cross-attention, in terms of accuracy and computational efficiency?
  - Basis in paper: [explicit] The paper introduces a layer-wise adaptive attention mechanism and claims it improves performance over using only the [CLS] vector.
  - Why unresolved: The paper does not provide a direct comparison between the proposed attention mechanism and other established attention mechanisms.
  - What evidence would resolve it: Experiments comparing the single-choice model with layer-wise adaptive attention to versions using self-attention or cross-attention.

- Question: How does the transfer learning strategy affect the model's performance on different types of questions, such as summarization, inference, and commonsense reasoning?
  - Basis in paper: [inferred] The paper mentions that RACE contains a variety of question types and that transfer learning improves performance, but it does not analyze the impact on specific question types.
  - Why unresolved: The paper does not provide a breakdown of the model's performance on different question types.
  - What evidence would resolve it: Experiments analyzing the single-choice model's accuracy on different question types in RACE, with and without transfer learning.

## Limitations
- The paper lacks detailed ablation studies to validate the contribution of individual components like layer-wise adaptive attention
- The preprocessing steps for converting extractive datasets into binary classification format are not clearly specified
- Performance gains from transfer learning are not clearly separated from architectural improvements
- No comparison with alternative attention mechanisms or multi-choice approaches using the same experimental framework

## Confidence

- High confidence: The core single-choice formulation (treating each option independently) is clearly described and logically sound
- Medium confidence: The experimental results showing state-of-the-art performance, as they are well-documented but lack detailed ablation studies
- Low confidence: The specific implementation details of layer-wise adaptive attention and the exact preprocessing steps for transfer learning datasets

## Next Checks

1. **Ablation study**: Run experiments comparing single-choice model performance with and without layer-wise adaptive attention, and with different numbers of pre-trained layers used, to quantify the actual contribution of this component.

2. **Dataset contribution analysis**: Train the single-choice model on RACE alone, then incrementally add each transfer dataset (SQuAD, CoQA, ARC) individually, measuring performance changes to identify which datasets provide meaningful knowledge transfer versus noise.

3. **Multi-choice vs single-choice comparison**: Implement a controlled experiment where the same pre-trained ALBERT-xxlarge model is trained on RACE using both the proposed single-choice binary classification approach and a standard multi-choice cross-entropy approach, keeping all other factors constant to isolate the effect of the formulation change.