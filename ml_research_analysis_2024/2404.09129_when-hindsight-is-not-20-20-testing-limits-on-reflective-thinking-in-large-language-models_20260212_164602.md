---
ver: rpa2
title: 'When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large
  Language Models'
arxiv_id: '2404.09129'
source_url: https://arxiv.org/abs/2404.09129
tags:
- question
- self-reflection
- response
- role
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether self-reflection prompting can
  improve large language model (LLM) performance without external feedback. The authors
  use a single-round, multi-response evaluation framework to test this on ChatGPT,
  finding mixed results: self-reflection improves accuracy on TruthfulQA but reduces
  accuracy on HotpotQA.'
---

# When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models

## Quick Facts
- arXiv ID: 2404.09129
- Source URL: https://arxiv.org/abs/2404.09129
- Reference count: 18
- Self-reflection prompting shows mixed results, improving accuracy on TruthfulQA but reducing it on HotpotQA

## Executive Summary
This paper investigates whether self-reflection prompting can improve large language model performance without external feedback. The authors use a single-round, multi-response evaluation framework to test this on ChatGPT, finding that self-reflection's effectiveness depends on the interaction between question difficulty and model comprehension. The study reveals that self-reflection is most beneficial when initial response accuracy is low and question difficulty is high, but can be harmful when models are already performing well. The authors also observe that self-reflection reduces majority voting tendencies, suggesting it promotes more thoughtful responses rather than defaulting to common answers.

## Method Summary
The paper evaluates self-reflection prompting through a single-round self-reflection verification (SR2V) framework. The method involves generating K candidate responses, prompting the model to critique each response, and then producing a final answer based on both the responses and critiques. The study tests this approach across two datasets (TruthfulQA and HotpotQA) using different LLM models (ChatGPT-3.5-turbo-16k-0613, LLaMA-2, and Mixtral). The authors systematically analyze performance across different levels of response accuracy and question difficulty, including generating artificial responses to control for comprehension levels. Manual and automatic evaluation metrics (accuracy, ROUGE-1, BLEURT) are used to assess outcomes.

## Key Results
- Self-reflection improves accuracy on TruthfulQA but reduces accuracy on HotpotQA
- Self-reflection is most beneficial when initial response accuracy is low (below 50%) and question difficulty is high
- Self-reflection significantly reduces majority voting tendencies across response accuracy levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-reflection helps when initial response accuracy is low and question difficulty is high
- Mechanism: Self-reflection provides an opportunity to catch and correct errors when the model is uncertain about its initial response
- Core assumption: The model has the capability to identify and correct its own errors when prompted to reflect
- Evidence anchors:
  - [abstract] "self-reflection shows the most benefit when models are less likely to be correct initially