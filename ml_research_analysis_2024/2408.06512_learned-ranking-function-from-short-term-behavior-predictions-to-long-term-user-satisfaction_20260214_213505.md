---
ver: rpa2
title: 'Learned Ranking Function: From Short-term Behavior Predictions to Long-term
  User Satisfaction'
arxiv_id: '2408.06512'
source_url: https://arxiv.org/abs/2408.06512
tags:
- user
- optimization
- function
- ranking
- slate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Learned Ranking Function (LRF), a system
  that directly optimizes long-term user satisfaction in large-scale recommendation
  systems. Instead of using heuristic ranking functions with manually tuned hyperparameters,
  LRF formulates ranking as a slate optimization problem using reinforcement learning.
---

# Learned Ranking Function: From Short-term Behavior Predictions to Long-term User Satisfaction

## Quick Facts
- arXiv ID: 2408.06512
- Source URL: https://arxiv.org/abs/2408.06512
- Reference count: 24
- Key result: 0.66% improvement in long-term user satisfaction after adding cascade click model to LRF

## Executive Summary
This paper introduces the Learned Ranking Function (LRF), a reinforcement learning-based system that directly optimizes long-term user satisfaction in large-scale recommendation systems. Rather than using heuristic ranking functions with manually tuned hyperparameters, LRF treats ranking as a slate optimization problem. The system was deployed on YouTube's Watch, Home, and Shorts pages, demonstrating measurable improvements over baseline systems through innovations including a cascade click model, lift formulation, and dynamic constraint optimization.

## Method Summary
The LRF system takes short-term user-item behavior predictions from multitask models as input and applies a cascade click model to estimate position-aware click and abandonment probabilities. It then computes lift rewards (incremental value over baseline abandonment) using separate neural networks for abandonment reward, lift reward, click probability, and abandonment probability. The final ranking score combines these elements with dynamic weight updates for multi-objective optimization. The system is trained using on-policy reinforcement learning with exploration through random candidate promotion.

## Key Results
- Initial LRF implementation achieved 0.21% lift in long-term user satisfaction over baseline
- Adding cascade click model provided additional 0.66% improvement
- LRF outperformed two-model baseline by 0.12% while maintaining stable multi-objective trade-offs
- Constraint optimization reduced objective fluctuation from 13.15% to 1.46% for same model change

## Why This Works (Mechanism)

### Mechanism 1
The cascade click model explicitly captures both the position-based decay of engagement probability and the abandonment probability, enabling more accurate modeling of user interaction with slates. The model assumes users inspect items sequentially, with each inspection having three possible outcomes: click, skip, or abandon. This allows the system to differentiate between items that were skipped because they were unappealing versus items that were never seen because the user abandoned earlier. Core assumption: Users interact with recommendation slates in a sequential, position-aware manner, and the abandonment probability is independent of the specific item being inspected.

### Mechanism 2
Lift formulation optimizes for the incremental value of recommendations rather than absolute engagement, accounting for platform-wide effects. By explicitly modeling the baseline value of abandonment and optimizing for the difference between clicking and abandoning, the system can better allocate recommendations across the platform, recognizing that users may abandon one page and engage elsewhere. Core assumption: The baseline abandonment value is consistent across different recommendation slates and can be accurately estimated.

### Mechanism 3
Dynamic linear scalarization stabilizes multi-objective optimization by maintaining consistent trade-offs during system changes. Instead of using fixed weights for combining objectives, the system dynamically updates weights based on offline evaluation correlations, ensuring that improvements to one objective don't inadvertently degrade others. Core assumption: Offline evaluation correlations between weight-combined lift and immediate rewards can effectively predict online performance trade-offs.

## Foundational Learning

- Concept: Slate optimization in reinforcement learning
  - Why needed here: The system treats ranking as a sequential decision problem where the order of recommendations affects future rewards.
  - Quick check question: How does slate optimization differ from traditional pointwise ranking in terms of reward structure?

- Concept: Cascade click models
  - Why needed here: These models capture the position-dependent nature of user engagement and abandonment in recommendation slates.
  - Quick check question: What are the key differences between standard cascade models and the variant used in this paper?

- Concept: Multi-objective constraint optimization
  - Why needed here: Large-scale recommendation systems must balance multiple competing objectives (e.g., engagement, satisfaction, diversity) while maintaining stability.
  - Quick check question: How does dynamic linear scalarization differ from traditional fixed-weight approaches in multi-objective optimization?

## Architecture Onboarding

- Component map: Candidate generation → Multitask scoring → LRF ranking → Re-ranking → User interaction → Reward collection → Training update
- Critical path: Candidate generation → Multitask scoring → LRF ranking → Re-ranking → User interaction → Reward collection → Training update
- Design tradeoffs:
  - Model complexity vs. inference speed: Small DNNs (~10^4 parameters) chosen for low latency
  - Exploration vs. exploitation: Random promotion of candidates for exploration
  - Offline vs. online evaluation: Dynamic weight adjustment based on offline correlations
- Failure signatures:
  - Objective instability: Large fluctuations in secondary metrics during model changes
  - Poor exploration: Lack of diversity in recommendations
  - Reward misalignment: Model optimizes for wrong signals
- First 3 experiments:
  1. Simplified LRF with CTR and lift formulation, fixed weights
  2. Addition of cascade click model replacing CTR with position-aware probabilities
  3. Implementation of constraint optimization to stabilize multi-objective trade-offs

## Open Questions the Paper Calls Out

### Open Question 1
How does the LRF system perform when deployed on platforms with different user behavior patterns, such as e-commerce or news recommendation systems? The paper mentions deployment on YouTube's Watch, Home, and Shorts pages but doesn't explore other types of platforms. This remains unresolved as the evaluation is limited to YouTube without discussion of generalizability to other recommendation domains.

### Open Question 2
What is the optimal frequency for updating the constraint weights in the multi-objective optimization, and how does this frequency impact system stability and performance? The paper mentions dynamically updating weights but doesn't specify the update frequency or its impact. This is unresolved as the authors describe the constraint optimization mechanism but don't provide empirical data on update frequency optimization.

### Open Question 3
How does the LRF system handle cold-start scenarios where there is limited user behavior data available? The paper doesn't address cold-start scenarios despite discussing user-item behavior predictions. This remains unresolved as the evaluation focuses on existing users with established behavior patterns, with no discussion of handling new users or items.

## Limitations

- Evaluation relies heavily on offline metrics and YouTube-specific A/B testing, limiting external validation
- System's dependence on multitask model outputs for short-term behavior predictions introduces potential failure point
- Constraint optimization algorithm's effectiveness depends on offline evaluation correlations remaining stable over time

## Confidence

- High confidence: The cascade click model's ability to capture position effects and abandonment improves prediction accuracy over simple CTR-based models.
- Medium confidence: The lift formulation's effectiveness in accounting for platform-wide effects and baseline abandonment, as this relies on assumptions about consistent baseline behavior.
- Medium confidence: The dynamic linear scalarization approach for stabilizing multi-objective optimization, given limited external validation of this specific technique in production systems.

## Next Checks

1. **Ablation study**: Run controlled experiments removing each innovation (cascade click model, lift formulation, constraint optimization) to quantify individual contributions to the 0.66% and 0.12% improvements.

2. **Offline-online correlation validation**: Systematically test whether offline evaluation correlations used for dynamic weight adjustment accurately predict online performance across different time periods and user segments.

3. **Robustness testing**: Evaluate system performance when multitask model predictions degrade by injecting controlled noise, to assess the impact of the LRF's dependence on base model accuracy.