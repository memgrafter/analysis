---
ver: rpa2
title: 'Multimodal Variational Autoencoder: a Barycentric View'
arxiv_id: '2412.20487'
source_url: https://arxiv.org/abs/2412.20487
tags:
- modalities
- multimodal
- distributions
- barycenter
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a theoretical framework for multimodal VAEs
  based on the concept of barycenters. The authors show that existing methods like
  PoE and MoE are specific instances of barycenters derived from minimizing KL divergence.
---

# Multimodal Variational Autoencoder: a Barycentric View

## Quick Facts
- arXiv ID: 2412.20487
- Source URL: https://arxiv.org/abs/2412.20487
- Reference count: 29
- Key outcome: WB-VAE outperforms PoE and MoE on multimodal benchmarks by using Wasserstein barycenters for aggregation

## Executive Summary
This paper introduces a theoretical framework for multimodal VAEs based on the concept of barycenters. The authors show that existing methods like PoE and MoE are specific instances of barycenters derived from minimizing KL divergence. They propose WB-VAE, which uses Wasserstein barycenters to aggregate unimodal inference distributions, better preserving the geometry of unimodal distributions compared to KL-based methods. Experiments on three multimodal benchmark datasets demonstrate that WB-VAE outperforms state-of-the-art methods in terms of linear classification accuracy of latent representations and generation coherence.

## Method Summary
The proposed WB-VAE uses Wasserstein barycenters to aggregate unimodal inference distributions in multimodal VAEs. Each modality is encoded separately to obtain Gaussian inference distributions, which are then combined using the Wasserstein barycenter. For isotropic Gaussians, this barycenter has an analytic solution based on the Bures-Wasserstein distance. The method balances the zero-forcing property of PoE and the mass-covering property of MoE. An extension, MWB-VAE, uses a mixture of Wasserstein barycenters over all subsets of modalities to further balance these properties.

## Key Results
- WB-VAE achieves higher linear classification accuracy on latent representations compared to PoE, MoE, and MoPoE-VAE
- Generation coherence improves with WB-VAE, particularly as the number of input modalities increases
- The method shows an approximately linear relationship between performance metrics and the number of input modalities on PolyMNIST
- MWB-VAE provides a better balance between zero-forcing and mass-covering properties than either pure PoE or MoE

## Why This Works (Mechanism)

### Mechanism 1
The Wasserstein barycenter preserves the geometry of unimodal distributions better than KL-based aggregations because the 2-Wasserstein distance is a true metric in probability space, allowing interpolation that respects the underlying manifold structure of the data. This leads to smoother transitions and better preservation of modality-specific and modality-invariant features. Core assumption: The unimodal inference distributions are Gaussian or approximately Gaussian, enabling the analytic Bures-Wasserstein barycenter solution.

### Mechanism 2
The barycentric formulation unifies PoE and MoE under a single theoretical framework by framing aggregation as minimizing a weighted sum of divergences. PoE and MoE emerge as specific instances using reverse and forward KL divergences respectively, providing theoretical insight into their zero-forcing vs. mass-covering properties. Core assumption: The true joint posterior can be well-approximated by a barycenter of the unimodal inference distributions.

### Mechanism 3
The mixture of Wasserstein barycenters (MWB-VAE) balances zero-forcing and mass-covering properties by constructing a mixture of Wasserstein barycenters over all subsets of modalities. This combines the sharp joint distributions from PoE-like behavior with the comprehensive coverage from MoE-like behavior. Core assumption: The optimal joint posterior can be represented as a mixture of barycenters over different modality subsets.

## Foundational Learning

- Concept: Optimal Transport and Wasserstein Distance
  - Why needed here: Understanding how the Wasserstein distance defines a metric space for probability measures and enables geometry-preserving aggregation.
  - Quick check question: What is the key difference between KL divergence and 2-Wasserstein distance in terms of defining a probability metric space?

- Concept: Variational Autoencoders and ELBO
  - Why needed here: Understanding the foundation of multimodal VAEs and how the joint posterior is approximated through aggregation of unimodal inference distributions.
  - Quick check question: How does the ELBO objective relate to the approximation of the true joint posterior in multimodal VAEs?

- Concept: Barycenters in Probability Theory
  - Why needed here: Understanding the theoretical foundation of the aggregation function as a barycenter that minimizes weighted divergences.
  - Quick check question: What is the mathematical definition of a barycenter in the context of probability distributions?

## Architecture Onboarding

- Component map: M unimodal encoders -> Barycenter aggregator -> Joint decoder
- Critical path: 1) Encode each modality to obtain unimodal Gaussian distributions 2) Compute the Wasserstein barycenter (analytically for isotropic Gaussians) 3) Sample from the barycenter to obtain the aggregated latent representation 4) Decode to reconstruct all modalities 5) Optimize ELBO with KL divergence between barycenter and prior
- Design tradeoffs: Computational complexity: Wasserstein barycenter is O(M) for isotropic Gaussians vs. O(M) for PoE/MoE, but MWB-VAE is O(2^M) due to subset enumeration. Representation quality: WB-VAE balances sharpness and coverage better than pure PoE or MoE. Scalability: MWB-VAE may become intractable with many modalities due to exponential subset growth.
- Failure signatures: Poor generation quality (unimodal distributions too dissimilar), degenerate barycenter (some modalities have very different supports), training instability (improper weighting of subset barycenters in MWB-VAE)
- First 3 experiments: 1) Implement WB-VAE on PolyMNIST with 2-3 modalities, compare linear classification accuracy vs. PoE/MoE 2) Test WB-VAE on MNIST-SVHN-TEXT with different modality combinations, evaluate conditional generation coherence 3) Implement MWB-VAE on bimodal CelebA, compare attribute classification accuracy for image→text generation

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed WB-VAE perform when aggregating more than three modalities compared to other state-of-the-art methods? The paper mentions that WB-VAE showed an approximately linear relationship between performance metrics and the number of input modalities on PolyMNIST, which contains five modalities, but does not provide a comprehensive comparison with other methods.

### Open Question 2
What is the impact of different choices of weights {λ1, ..., λM} on the performance of the proposed WB-VAE? The paper mentions that different choices of weights associated with unimodal distributions may lead to a joint posterior that maintains diverse shapes and structures of unimodal distributions, but does not explore this impact.

### Open Question 3
How does the proposed WB-VAE handle missing modalities compared to other state-of-the-art methods? The paper mentions that WB-VAE can handle missing modalities by directly sampling the learned joint posterior, but does not provide a comprehensive comparison with other methods in handling missing modalities.

## Limitations
- The method assumes unimodal inference distributions can be well-approximated by Gaussian distributions, which may not hold for highly non-Gaussian data
- MWB-VAE faces scalability challenges due to exponential growth of modality subsets (2^M), potentially becoming intractable for many modalities
- The paper assumes access to pre-trained classifiers for evaluation, which may not be available in all domains

## Confidence

**High Confidence**: The theoretical framework connecting barycenters to PoE and MoE through KL divergence minimization is well-established and mathematically rigorous. The experimental results demonstrating improved performance on standard multimodal benchmarks are also robust.

**Medium Confidence**: The specific implementation details of the Wasserstein barycenter calculation and the exact neural network architectures are not fully specified in the paper, requiring reference to external sources. This introduces some uncertainty in faithful reproduction.

**Low Confidence**: The scalability analysis of MWB-VAE for large numbers of modalities is limited, and the paper does not provide concrete guidance on when the method becomes computationally prohibitive.

## Next Checks

1. **Geometric Preservation Validation**: Conduct ablation studies on PolyMNIST varying the degree of modality mismatch to quantify how well WB-VAE preserves the geometry of unimodal distributions compared to PoE and MoE across different similarity regimes.

2. **Scalability Analysis**: Implement MWB-VAE on synthetic multimodal datasets with varying numbers of modalities (M=2, 3, 4, 5) and measure computational time and memory usage to empirically determine the practical limits of the method.

3. **Distributional Robustness Test**: Replace the Gaussian assumption with other unimodal distributions (e.g., Laplace, Student's t) in controlled experiments to assess the robustness of the Bures-Wasserstein barycenter approach when the unimodal inference distributions deviate from Gaussianity.