---
ver: rpa2
title: 'BioNCERE: Non-Contrastive Enhancement For Relation Extraction In Biomedical
  Texts'
arxiv_id: '2410.23583'
source_url: https://arxiv.org/abs/2410.23583
tags:
- learning
- contrastive
- loss
- non-contrastive
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses relation extraction (RE) in biomedical texts
  using BioNCERE, a non-contrastive enhancement method. It leverages transfer learning
  and non-contrastive learning to predict relations without using named entity labels,
  reducing annotation costs.
---

# BioNCERE: Non-Contrastive Enhancement For Relation Extraction In Biomedical Texts

## Quick Facts
- arXiv ID: 2410.23583
- Source URL: https://arxiv.org/abs/2410.23583
- Authors: Farshad Noravesh
- Reference count: 40
- Primary result: BioNCERE achieves state-of-the-art performance on SemMedDB without using named entity labels, with optimal batch size around 64.

## Executive Summary
BioNCERE is a novel non-contrastive enhancement method for relation extraction in biomedical texts. It leverages transfer learning and non-contrastive learning to predict relations without requiring named entity labels, significantly reducing annotation costs. The method employs a three-stage approach: fine-tuning BioBERT, training a non-contrastive network, and final classification. Experiments on SemMedDB demonstrate that BioNCERE achieves performance similar to state-of-the-art models while avoiding the need for entity information.

## Method Summary
BioNCERE uses a three-stage transfer learning pipeline to extract relations from biomedical texts. First, BioBERT is fine-tuned for relation extraction and its weights are frozen. Next, a non-contrastive network is trained using supervised BYOL (Bootstrap Your Own Latent) to improve sentence representations without requiring negative samples. Finally, a classifier is trained on the frozen non-contrastive network to predict one of 28 predicates. The method respects the "separation of concerns" principle, making it flexible and reusable for other tasks. Experiments show that batch sizes around 64 are optimal, with larger or smaller sizes leading to underfitting or overfitting respectively.

## Key Results
- BioNCERE achieves state-of-the-art performance on SemMedDB without using named entity labels
- Batch sizes around 64 are optimal; higher or lower sizes lead to underfitting or overfitting
- The three-stage approach respects "separation of concerns" principle, making it flexible and reusable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BioNCERE uses a three-stage transfer learning pipeline to avoid overfitting and maintain model interpretability.
- Mechanism: The model first fine-tunes BioBERT for relation extraction, freezes the learned weights, then applies non-contrastive learning to improve sentence representations, and finally classifies into 28 predicates. Each stage is independent, respecting the "separation of concerns" principle.
- Core assumption: Freezing weights after each stage prevents the non-contrastive network from being adversely affected by backpropagation from the classifier, reducing overfitting risk.
- Evidence anchors:
  - [abstract]: "It resolves RE in three stages by leveraging transfer learning two times."
  - [section]: "the suggested training procedure is shown in Algorithm 1" and "These three stages make the algorithm very flexible and can be done by three different teams of developers and respect the 'separation of concern principle' in software development."
  - [corpus]: Weak evidence; only a few similar biomedical relation extraction papers are listed, none detailing a comparable three-stage pipeline.
- Break condition: If the classifier's backpropagation is allowed to affect earlier stages (e.g., via joint training), the model may suffer from overfitting or collapse to outlier dimensions.

### Mechanism 2
- Claim: Non-contrastive learning using BYOL in a supervised setting improves sentence representation without requiring negative samples.
- Mechanism: The model uses an asymmetric loss between two networks (online and target) and stop-gradient updates to minimize negative cosine similarity. Positive samples are naturally defined by having the same relation.
- Core assumption: Supervised non-contrastive learning is sufficient in this domain because of the abundance of labeled data in SemMedDB.
- Evidence anchors:
  - [abstract]: "BioNCERE uses transfer learning and non-contrastive learning to avoid full or dimensional collapse as well as bypass overfitting."
  - [section]: "Figure 2 shows the proposed model which contains two networks... Using the same naming convention in the BYOL paper, the first network is called the online network which contains the encoder fθ, projection, and the predictor pθ."
  - [corpus]: Weak evidence; no direct mentions of BYOL applied to biomedical relation extraction, though related contrastive learning methods are cited.
- Break condition: If the batch size is too small or too large, the model may underfit or overfit, respectively, as noted in the experiments.

### Mechanism 3
- Claim: Batch sizes around 64 are optimal for BioNCERE; sizes outside this range lead to underfitting or overfitting.
- Mechanism: Experiments tested batch sizes of 64, 128, and 256, showing that 64 yields the best F1 scores, while larger or smaller sizes degrade performance.
- Core assumption: The batch size is a critical hyperparameter because it determines the number of positive examples per relation in each batch.
- Evidence anchors:
  - [abstract]: "The method demonstrates that batch sizes around 64 are optimal, with higher or lower sizes leading to underfitting or overfitting, respectively."
  - [section]: "Table 2 with Table 1, it is observed that training with high batch sizes such as 128 and 256 could underfit. Any batch size below 64 could lead to overfitting since there are not enough examples for each class inside each batch."
  - [corpus]: No direct corpus evidence on batch size tuning for BioNCERE; the claim is supported only by the paper's internal experiments.
- Break condition: If the batch size is not tuned properly, the model's performance on relation extraction will degrade, as shown by lower F1 scores in the experiments.

## Foundational Learning

- Concept: Transfer learning in biomedical NLP
  - Why needed here: BioBERT is pre-trained on large biomedical corpora, providing a strong starting point for fine-tuning on specific relation extraction tasks.
  - Quick check question: Why is it beneficial to freeze BioBERT weights after the first fine-tuning stage?

- Concept: Non-contrastive learning (BYOL)
  - Why needed here: It improves sentence representation without requiring negative samples, which can be difficult to obtain in supervised settings.
  - Quick check question: How does the asymmetric loss in BYOL help avoid collapse to constant representations?

- Concept: Batch size impact on supervised learning
  - Why needed here: The paper demonstrates that batch size critically affects model performance, with 64 being optimal.
  - Quick check question: What happens to the model if the batch size is too small or too large?

## Architecture Onboarding

- Component map: BioBERT encoder (fine-tuned, then frozen) -> Non-contrastive network (online/target encoder + predictor) -> Classifier (linear layer for 28 predicates)
- Critical path: 1. Fine-tune BioBERT on RE task 2. Train non-contrastive network with BYOL loss 3. Freeze non-contrastive network weights 4. Train classifier for final predicate prediction
- Design tradeoffs:
  - Joint training vs. three-stage pipeline: Three-stage allows clearer separation of concerns and easier debugging, but may require more compute time.
  - Batch size: 64 is optimal; smaller causes overfitting, larger causes underfitting.
  - Entity information: Not used, which reduces annotation costs but may limit performance compared to models that leverage entity labels.
- Failure signatures:
  - Overfitting: Low validation loss but poor test performance; may occur with batch size < 64 or insufficient regularization.
  - Underfitting: High training and validation loss; may occur with batch size > 64 or overly aggressive weight freezing.
  - Collapse: Model outputs become constant or overly sensitive to certain dimensions; may occur if BYOL loss is not properly tuned or if the classifier backpropagation affects earlier stages.
- First 3 experiments:
  1. Vary batch size (32, 64, 128, 256) and measure F1 scores to confirm the optimal size.
  2. Test joint training vs. three-stage pipeline to verify the separation of concerns benefit.
  3. Compare performance with and without entity information to quantify the annotation cost tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of batch size impact the performance of BioNCERE in relation extraction tasks?
- Basis in paper: [explicit] The paper discusses the impact of batch sizes on model performance, noting that sizes around 64 are optimal, while higher or lower sizes lead to underfitting or overfitting, respectively.
- Why unresolved: While the paper identifies the optimal batch size, it does not explore the underlying reasons for this effect or whether different datasets or tasks might require different batch sizes.
- What evidence would resolve it: Conducting experiments with a wider range of batch sizes and different datasets to determine if the optimal batch size varies with task complexity or dataset characteristics.

### Open Question 2
- Question: Can the three-stage training approach of BioNCERE be effectively applied to other natural language processing tasks beyond relation extraction?
- Basis in paper: [explicit] The paper mentions that the three-stage approach respects the "separation of concerns" principle, making it flexible and reusable for other tasks.
- Why unresolved: The paper does not provide experimental evidence or theoretical justification for applying this approach to other NLP tasks.
- What evidence would resolve it: Applying the three-stage training approach to different NLP tasks such as named entity recognition or sentiment analysis and evaluating its effectiveness.

### Open Question 3
- Question: What are the theoretical underpinnings of why non-contrastive learning methods like BYOL can avoid collapse without negative samples?
- Basis in paper: [inferred] The paper discusses the use of BYOL for non-contrastive learning but does not delve into the theoretical reasons for its effectiveness.
- Why unresolved: The paper focuses on practical implementation rather than theoretical analysis, leaving the underlying mechanisms of non-contrastive learning unexplored.
- What evidence would resolve it: Conducting a theoretical analysis of non-contrastive learning methods to understand the conditions under which they succeed or fail, potentially supported by empirical studies.

## Limitations

- Weak empirical support for BYOL in supervised biomedical RE: The paper lacks direct comparison to contrastive baselines or other representation learning methods in the biomedical domain.
- Batch size tuning is critical but not fully explained: The exact reasoning for why 64 is optimal is not detailed, and the claim is based only on internal experiments without ablation studies.
- No entity information may limit performance ceiling: The paper does not quantify the performance gap between BioNCERE and state-of-the-art models that use entity information.

## Confidence

- High confidence: The three-stage transfer learning pipeline is clearly specified and aligns with the "separation of concerns" principle.
- Medium confidence: Non-contrastive learning using BYOL improves sentence representation without negative samples, but lacks direct empirical comparison to contrastive methods.
- Medium confidence: Batch size around 64 is optimal for BioNCERE, as supported by F1 score experiments, but underlying reasons are not fully explained.

## Next Checks

1. Compare BioNCERE to contrastive learning baselines: Run experiments using SimCLR or other contrastive methods on the same SemMedDB data to quantify the benefit (or lack thereof) of non-contrastive learning in this supervised biomedical RE setting.

2. Ablation study on batch size and relation distribution: Vary batch sizes and measure not only F1 scores but also the number of positive examples per relation class per batch to confirm the mechanism behind the optimal batch size.

3. Quantify annotation cost vs. performance tradeoff: Train a variant of BioNCERE that uses entity information and compare its performance to the original model to measure the exact performance cost of avoiding entity labels.