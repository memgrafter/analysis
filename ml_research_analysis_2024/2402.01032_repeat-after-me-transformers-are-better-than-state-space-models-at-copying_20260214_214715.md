---
ver: rpa2
title: 'Repeat After Me: Transformers are Better than State Space Models at Copying'
arxiv_id: '2402.01032'
source_url: https://arxiv.org/abs/2402.01032
tags:
- copy
- state
- length
- gssms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares transformers and generalized state space models
  (GSSMs) on copying tasks. The authors prove that transformers can copy strings exponentially
  longer than their size using n-gram hashing, while GSSMs are fundamentally limited
  by their fixed-size latent state.
---

# Repeat After Me: Transformers are Better than State Space Models at Copying

## Quick Facts
- **arXiv ID:** 2402.01032
- **Source URL:** https://arxiv.org/abs/2402.01032
- **Reference count:** 40
- **Key outcome:** Transformers can copy strings exponentially longer than their size using n-gram hashing, while GSSMs are fundamentally limited by their fixed-size latent state

## Executive Summary
This paper presents a comprehensive comparison between transformers and generalized state space models (GSSMs) on copying tasks. The authors demonstrate that transformers have a fundamental advantage over GSSMs in accessing and reproducing arbitrary parts of context. Through both theoretical analysis and empirical evaluation, they show that transformers can efficiently copy sequences exponentially longer than their size using n-gram hashing techniques, while GSSMs are constrained by their fixed-size latent state representation. The research establishes that transformers learn copying tasks faster and generalize better to longer sequences than GSSMs, with significant performance gaps observed even with pretrained models.

## Method Summary
The authors conduct a theoretical analysis of both architectures' capabilities in copying tasks, proving that transformers can use n-gram hashing to copy strings exponentially longer than their size. They empirically compare the two architectures on various copying and retrieval tasks, using both randomly initialized and pretrained models. The experiments systematically vary sequence lengths and task complexities to demonstrate the relative strengths of each architecture. The theoretical framework establishes formal bounds on copying capabilities, while the empirical work validates these findings across multiple experimental conditions.

## Key Results
- Transformers can copy strings exponentially longer than their size using n-gram hashing, while GSSMs are limited by fixed latent state size
- Empirically, transformers learn copying tasks significantly faster than GSSMs and show better generalization to longer sequences
- Pretrained transformers dramatically outperform pretrained GSSMs on copying and retrieval tasks
- The copying gap persists even when controlling for model size and parameter count

## Why This Works (Mechanism)
The fundamental mechanism underlying transformers' superiority in copying tasks stems from their attention-based architecture, which allows direct access to any part of the input sequence. Through the self-attention mechanism, transformers can effectively implement n-gram hashing, enabling them to store and retrieve information about arbitrary patterns in the input. The multi-head attention mechanism provides multiple independent pathways for information storage and retrieval, while the positional encodings preserve sequence order information. This architecture allows transformers to create exponentially many possible n-gram combinations, far exceeding their parameter count.

## Foundational Learning

1. **State Space Models (SSMs)**
   - *Why needed:* Understanding the architectural constraints that limit GSSMs' copying capabilities
   - *Quick check:* Can represent sequences through fixed-size latent states but struggle with long-range dependencies

2. **Transformer Architecture**
   - *Why needed:* Understanding the attention mechanism that enables efficient copying
   - *Quick check:* Uses self-attention to directly access any sequence position

3. **N-gram Hashing**
   - *Why needed:* Key technique enabling transformers' exponential copying capability
   - *Quick check:* Maps sequence patterns to unique identifiers for storage and retrieval

4. **Latent State Representation**
   - *Why needed:* Understanding the fundamental limitation of GSSMs
   - *Quick check:* Fixed-size representation constrains the amount of information that can be stored

5. **Attention Mechanism**
   - *Why needed:* Core component enabling transformers' copying ability
   - *Quick check:* Allows direct weighted access to any sequence position

6. **Positional Encoding**
   - *Why needed:* Essential for maintaining sequence order information
   - *Quick check:* Embeds position information into token representations

## Architecture Onboarding

**Component Map:** Input Sequence -> Transformer/GSSM Block -> Output Sequence
- Transformers: Input -> Self-Attention -> Feed-Forward -> Output
- GSSMs: Input -> State Update -> Output Mapping -> Output

**Critical Path:** For copying tasks, the critical path is the ability to store and retrieve arbitrary sequence patterns. Transformers achieve this through attention mechanisms, while GSSMs are constrained by their state update equations.

**Design Tradeoffs:** GSSMs offer computational efficiency and linear scaling, while transformers provide superior copying capabilities at the cost of quadratic complexity. The choice between architectures depends on the specific requirements of the task and available computational resources.

**Failure Signatures:** GSSMs fail to copy long sequences due to state size limitations. Transformers may struggle with extremely long sequences due to memory constraints, but this is a practical rather than fundamental limitation.

**First 3 Experiments to Run:**
1. Compare copying performance on sequences of increasing length (2x, 4x, 8x model size)
2. Test retrieval accuracy for randomly positioned subsequences
3. Evaluate performance degradation as n-gram complexity increases

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis focuses on idealized scenarios that may not fully capture practical behavior
- Empirical comparisons limited to specific copying and retrieval tasks
- Results may not generalize to all sequence modeling problems beyond copying
- Claims about fundamental limitations of GSSMs need further validation across different task domains

## Confidence
- **High Confidence:** Transformers outperform GSSMs on copying tasks in empirical evaluations
- **Medium Confidence:** Theoretical limitations of GSSMs in copying tasks are fundamental rather than practical
- **Low Confidence:** The results generalize to all sequence modeling tasks beyond copying and retrieval

## Next Checks
1. Test whether architectural modifications to GSSMs (e.g., larger latent states or different parameterization schemes) can close the performance gap on copying tasks.

2. Evaluate both architectures on non-copying sequence modeling tasks (e.g., language modeling, time series forecasting) to assess whether the observed differences are task-specific or more general.

3. Investigate the impact of different training regimes, including longer training times and curriculum learning, on GSSMs' ability to learn copying tasks.