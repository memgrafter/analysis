---
ver: rpa2
title: Bayesian Inverse Reinforcement Learning for Non-Markovian Rewards
arxiv_id: '2406.13991'
source_url: https://arxiv.org/abs/2406.13991
tags:
- reward
- learning
- expert
- function
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Bayesian inverse reinforcement learning (BIRL)
  framework for inferring non-Markovian reward functions represented as reward machines
  (RMs) directly from expert demonstrations without requiring access to reward signals.
  The key idea is to extend the BIRL framework to handle histories rather than just
  current states, adapt demonstrations to include observed label sequences, and use
  simulated annealing to maximize the posterior distribution over RMs.
---

# Bayesian Inverse Reinforcement Learning for Non-Markovian Rewards

## Quick Facts
- arXiv ID: 2406.13991
- Source URL: https://arxiv.org/abs/2406.13991
- Reference count: 22
- This paper proposes a Bayesian IRL framework for inferring non-Markovian reward functions represented as reward machines from expert demonstrations.

## Executive Summary
This paper addresses the challenge of inferring non-Markovian reward functions from expert demonstrations using a Bayesian Inverse Reinforcement Learning (BIRL) framework. The key innovation is representing non-Markovian rewards as reward machines (RMs) and adapting the BIRL framework to handle histories rather than just current states. The method projects demonstrations onto product MDP spaces and uses simulated annealing to maximize the posterior distribution over RMs. The approach is evaluated on gridworld tasks and compared against existing algorithms for learning binary non-Markovian rewards.

## Method Summary
The algorithm extends BIRL to handle non-Markovian rewards by representing them as reward machines and projecting expert demonstrations onto the state space of each hypothesis product MDP M × R. It uses a modified simulated annealing approach to maximize the posterior distribution over RMs, starting with a random hypothesis and iteratively proposing neighboring hypotheses by randomly updating transitions. The method incorporates a prior distribution that favors simple transitions to improve efficiency and accuracy. The algorithm computes optimal Q-values and posterior probabilities for each RM hypothesis to identify the most likely reward structure.

## Key Results
- Successfully infers non-Markovian reward functions represented as reward machines from expert demonstrations
- Outperforms existing algorithms for learning binary non-Markovian rewards on gridworld tasks
- Demonstrates effective optimization according to inferred rewards in recharge, coffee, and multi-coffee environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm can handle non-Markovian rewards by representing them as reward machines and projecting demonstrations onto the product MDP space.
- Mechanism: Uses reward machines (RMs) to represent non-Markovian rewards and projects expert demonstrations onto the state space of each hypothesis product MDP M × R. This allows computation of optimal Q-values and posterior probabilities for each RM hypothesis.
- Core assumption: The expert is optimizing a joint MDP formed by the product of the original MDP and the reward machine.
- Evidence anchors:
  - [abstract]: "We propose a Bayesian IRL (BIRL) framework for inferring RMs directly from expert behavior, requiring significant changes to the standard framework."
  - [section]: "In our setting, the reward is a function of histories rather than MDP states. We label our MDP states with relevant observations, and the reward is a function over label sequences."

### Mechanism 2
- Claim: The algorithm uses a modified simulated annealing approach to maximize the posterior distribution over RMs.
- Mechanism: Starts with a random RM hypothesis and iteratively proposes neighboring hypotheses by randomly updating transitions. Accepts or rejects proposals based on a modified transition rule that incorporates temperature and prior information.
- Core assumption: The posterior distribution over RMs can be maximized using simulated annealing with a suitable temperature schedule and proposal mechanism.
- Evidence anchors:
  - [abstract]: "We propose a Bayesian IRL (BIRL) framework for inferring RMs directly from expert behavior, requiring significant changes to the standard framework."
  - [section]: "Let us summarize the process... With the proper temperature schedule and choice of neighbors, we will in principle eventually converge to the peak of the distribution."

### Mechanism 3
- Claim: The algorithm uses a prior distribution over RMs that favors simple transitions to improve efficiency and accuracy.
- Mechanism: The prior distribution assigns higher probability to RMs with zero-reward and self-transitions, encouraging exploration of simpler hypotheses first.
- Core assumption: Simpler RMs are more likely to be the true underlying reward function.
- Evidence anchors:
  - [abstract]: "We define a new reward space, adapt the expert demonstration to include history, show how to compute the reward posterior, and propose a novel modification to simulated annealing to maximize this posterior."
  - [section]: "Finally, we must set the prior P(R). In early experiments, we simply used a uniform prior. However, we found it useful to place a prior in favor of 'simple' transitions."

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The algorithm operates on MDPs and their extensions (product MDPs) to represent the environment and reward structure.
  - Quick check question: What is the difference between a Markov Decision Process and a Markov Reward Process?

- Concept: Inverse Reinforcement Learning (IRL)
  - Why needed here: The algorithm is an extension of IRL techniques to handle non-Markovian rewards represented as reward machines.
  - Quick check question: What is the main difference between standard IRL and Bayesian IRL?

- Concept: Reward Machines (RMs)
  - Why needed here: RMs are used to represent non-Markovian rewards that depend on the history of states and observations.
  - Quick check question: How do reward machines differ from standard reward functions in MDPs?

## Architecture Onboarding

- Component map:
  - Expert demonstrations (state-action pairs with label sequences) -> Product MDP projection -> Q-value computation -> Posterior calculation -> Simulated annealing optimization -> Inferred reward machine

- Critical path:
  1. Parse expert demonstrations and project them onto the product MDP space for each RM hypothesis.
  2. Compute optimal Q-values and posterior probabilities for each RM hypothesis.
  3. Use simulated annealing to maximize the posterior distribution over RMs.
  4. Return the RM with the highest posterior probability as the inferred reward function.

- Design tradeoffs:
  - Accuracy vs. efficiency: Using a prior that favors simple RMs can improve efficiency but may bias the algorithm towards incorrect hypotheses if the true RM is complex.
  - Exploration vs. exploitation: The temperature schedule in simulated annealing balances exploration of the hypothesis space with exploitation of promising regions.

- Failure signatures:
  - Poor performance on the test gridworlds indicates that the algorithm failed to accurately infer the reward function.
  - Getting stuck in local optima during simulated annealing suggests that the temperature schedule or proposal mechanism needs to be adjusted.

- First 3 experiments:
  1. Test the algorithm on a simple gridworld with a known non-Markovian reward structure to verify that it can accurately infer the reward function.
  2. Vary the prior distribution over RMs and observe how it affects the accuracy and efficiency of the algorithm.
  3. Test the algorithm with different temperature schedules and proposal mechanisms in simulated annealing to find the optimal settings for balancing exploration and exploitation.

## Open Questions the Paper Calls Out
None

## Limitations

- The evaluation is limited to gridworld environments, which may not capture the complexity of real-world scenarios where non-Markovian rewards are prevalent.
- The assumption that expert behavior can be modeled as optimizing a product MDP formed by the original MDP and an RM may not hold in all cases.
- The choice of prior distribution over RMs that favors simple transitions could bias the algorithm towards incorrect hypotheses when the true RM is complex.

## Confidence

- **High Confidence**: The algorithm's ability to handle non-Markovian rewards by projecting demonstrations onto the product MDP space and computing optimal Q-values for each RM hypothesis is well-established.
- **Medium Confidence**: The use of simulated annealing to maximize the posterior distribution over RMs is theoretically sound, but the effectiveness depends on the choice of temperature schedule and proposal mechanism.
- **Low Confidence**: The assumption that simpler RMs are more likely to be the true underlying reward function and the choice of prior distribution to favor such RMs may not hold in all cases.

## Next Checks

1. Test the algorithm on more complex environments with richer non-Markovian reward structures to assess its scalability and robustness.
2. Investigate the impact of different prior distributions over RMs on the accuracy and efficiency of the algorithm, including priors that do not favor simple transitions.
3. Evaluate the algorithm's performance when the expert is not optimizing according to a product MDP formed by the original MDP and an RM, to assess its robustness to model misspecification.