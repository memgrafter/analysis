---
ver: rpa2
title: A Counterfactual Explanation Framework for Retrieval Models
arxiv_id: '2409.00860'
source_url: https://arxiv.org/abs/2409.00860
tags:
- retrieval
- document
- query
- counterfactual
- setup
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CFIR, a counterfactual explanation framework
  for information retrieval models. It addresses the underexplored problem of explaining
  why documents are not ranked highly by identifying which words, if added, could
  improve a document's rank.
---

# A Counterfactual Explanation Framework for Retrieval Models

## Quick Facts
- arXiv ID: 2409.00860
- Source URL: https://arxiv.org/abs/2409.00860
- Reference count: 14
- Primary result: CFIR achieves up to 72% fidelity in identifying words that improve document ranking, outperforming query and top-K word baselines

## Executive Summary
This paper introduces CFIR, a counterfactual explanation framework for information retrieval models that addresses why documents are not ranked highly by identifying which words, if added, could improve a document's rank. The framework frames this as a constrained optimization problem, using a binary classifier to predict whether a document will be in the top-K results and then finding minimal feature changes to flip the prediction. Experiments on MS MARCO passage and document datasets with models like BM25, DRMM, DSSM, and ColBERT show CFIR consistently outperforms baseline approaches in fidelity.

## Method Summary
CFIR builds a binary classifier to predict if a document is in top-K results for a given query and retrieval model. The framework constructs feature vectors using the top-n TF-IDF weighted words per document, creating a vocabulary from top-K documents. For counterfactual generation, CFIR applies constrained optimization to find minimal word additions that flip the classifier's prediction from non-top-K to top-K. The optimization balances three criteria: fidelity (changing the prediction), minimal distance (adding few words), and diversity (suggesting novel terms beyond query words).

## Key Results
- CFIR achieves fidelity scores up to 72%, significantly outperforming query word and top-K word baselines
- The method identifies terms that improve document ranking in approximately 60% of cases on average
- CFIR suggests diverse, non-query words with maximum query word overlap of only 63%, indicating novel term discovery
- Performance is generally better on shorter documents (MS MARCO passages) compared to longer documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CFIR improves document ranking by identifying and adding specific missing words that would flip the retrieval model's prediction from non-top-K to top-K.
- Mechanism: The framework builds a binary classifier to predict whether a document is in top-K. It then uses constrained optimization to find minimal word additions that change the classifier's prediction, which empirically improves actual retrieval rank in ~60% of cases.
- Core assumption: A locally trained binary classifier can approximate the behavior of complex retrieval models (BM25, DRMM, DSSM, ColBERT) with sufficient fidelity to guide effective counterfactual term selection.
- Evidence anchors:
  - [abstract] "CFIR outperforms query word and top-K word baselines in fidelity (up to 72%), suggesting more effective and diverse counterfactual terms."
  - [section 3.1] "The main assumptions in the above mentioned setup is that the machine learning model should be differentiable and the output of the model should not change over time."
- Break condition: If the binary classifier cannot accurately approximate the retrieval model locally, the optimization will suggest ineffective terms and fidelity will drop below baseline.

### Mechanism 2
- Claim: Using only top-n TF-IDF weighted words per document for classifier feature vectors maintains computational efficiency while preserving ranking-relevant information.
- Mechanism: By filtering to the most significant n words per document, the framework reduces dimensionality and focuses on terms that have the highest discriminative power for classification, which correlates with retrieval relevance.
- Core assumption: The top TF-IDF words per document capture sufficient information to predict top-K membership and guide effective counterfactual generation.
- Evidence anchors:
  - [section 3.1] "We use Tf-Idf mechanism to select the top n words from each document. The dimension of the feature vector required for the classifier setup is set to the size of the vocabulary set |V|."
  - [section 5] "It is clearly observed from Figure 3 (a) that with increase in the number of documents having label 0, the performance of the classifier decreases."
- Break condition: If important ranking signals are distributed across less frequent terms, the top-n filtering will miss critical counterfactual terms and reduce effectiveness.

### Mechanism 3
- Claim: Counterfactual explanations provide diverse, non-query words that improve ranking more effectively than simply adding query terms or top-K document terms.
- Mechanism: The optimization objective explicitly balances between fidelity, minimal distance to original document, and diversity of counterfactuals, encouraging the discovery of novel terms beyond obvious query matches.
- Core assumption: Diversity in counterfactual terms leads to more effective rank improvement than redundant or obvious term additions.
- Evidence anchors:
  - [abstract] "CFIR outperforms query word and top-K word baselines in fidelity (up to 72%), suggesting more effective and diverse counterfactual terms."
  - [section 5] "Thirdly, it can be also observed from Table 2 that the maximum query word overlap by our proposed approach is 63%. This implies that the counterfactual algorithm is suggesting new words that are not even present in a query."
- Break condition: If the diversity constraint is too strong, the optimization may suggest irrelevant terms that don't improve ranking, reducing fidelity below baseline.

## Foundational Learning

- Concept: Binary classification for local model approximation
  - Why needed here: CFIR needs to determine whether a document is in top-K without having direct access to the retrieval model's internal ranking logic.
  - Quick check question: How would you train a binary classifier to predict if a document will be in the top-10 results for a given query and retrieval model?

- Concept: Constrained optimization for counterfactual generation
  - Why needed here: To find minimal document modifications that change the classification from non-top-K to top-K while maintaining document coherence.
  - Quick check question: What are the three criteria in the optimization objective, and how do they trade off against each other?

- Concept: Feature vector construction from vocabulary intersection
  - Why needed here: To create a consistent feature space across documents for the binary classifier while capturing document-specific term frequencies.
  - Quick check question: How does the vocabulary set V get constructed, and what determines which words appear in a document's feature vector?

## Architecture Onboarding

- Component map: Classifier trainer → Counterfactual optimizer → Evaluation pipeline → Retrieval model interface
- Critical path: Train classifier for query-retrieval model pair → Generate counterfactuals via optimization → Evaluate rank improvement on actual retrieval model
- Design tradeoffs: Computational efficiency (top-n TF-IDF filtering) vs. completeness (all document words); Fidelity vs. diversity in counterfactuals; Local approximation vs. global model behavior
- Failure signatures: Low fidelity scores on test set; Optimization suggesting irrelevant terms; Classifier accuracy dropping with more non-top-K documents
- First 3 experiments:
  1. Train binary classifier on BM25 with top-10 TF-IDF words and verify it can distinguish top-K from non-top-K documents
  2. Run counterfactual optimization on a single query-document pair and check if suggested terms improve actual BM25 ranking
  3. Compare fidelity scores of CFIR vs. query word baseline across all four retrieval models on MS MARCO passage dataset

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations

- The framework's effectiveness depends heavily on the assumption that binary classifiers can accurately approximate complex retrieval models, which may not hold across all scenarios
- Performance is generally better on shorter documents (passages) than longer documents, suggesting limitations with document length
- The evaluation is based on a limited set of 50 query-document pairs per dataset, which may not capture full variability

## Confidence

- High confidence: The framework's general approach of using counterfactual explanations for retrieval models is well-founded and the experimental methodology is sound.
- Medium confidence: The empirical results showing CFIR outperforming baselines are credible, but the generalizability to larger, more diverse datasets remains uncertain.
- Low confidence: The assumption that binary classifiers can reliably approximate retrieval model behavior across all query-document pairs without extensive validation.

## Next Checks

1. Scale validation: Test CFIR on the full MS MARCO passage and document datasets (not just 50 pairs) to verify if fidelity scores remain consistent at scale.
2. Classifier fidelity analysis: Measure and report the accuracy of the binary classifiers on held-out test sets for each query-retrieval model combination to quantify how well they approximate the actual retrieval models.
3. Feature completeness evaluation: Compare counterfactual effectiveness when using different numbers of TF-IDF words (e.g., top-5, top-20, top-50) to determine if the top-10 cutoff is optimal or limiting.