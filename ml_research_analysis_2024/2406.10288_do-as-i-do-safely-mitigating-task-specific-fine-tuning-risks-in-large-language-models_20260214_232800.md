---
ver: rpa2
title: 'Do as I do (Safely): Mitigating Task-Specific Fine-tuning Risks in Large Language
  Models'
arxiv_id: '2406.10288'
source_url: https://arxiv.org/abs/2406.10288
tags:
- safety
- fine-tuning
- data
- benign
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the risk that fine-tuning on benign task-specific
  datasets can inadvertently increase model harmfulness, and shows that malicious
  actors can manipulate benign datasets to achieve this while evading detection. The
  authors evaluate multiple prompting strategies for fine-tuning on seven Q&A datasets
  and find that while benign users are unlikely to accidentally create harmful models,
  adversarial strategies like AOA, AutoIF, and AutoIF + AOA can increase attack success
  rates (ASR) significantly (up to 80% on Harmful Instructions).
---

# Do as I do (Safely): Mitigating Task-Specific Fine-tuning Risks in Large Language Models

## Quick Facts
- arXiv ID: 2406.10288
- Source URL: https://arxiv.org/abs/2406.10288
- Reference count: 40
- Primary result: Malicious fine-tuning on benign datasets can increase harmfulness, but Paraphrase mitigation strategy effectively reduces risks while maintaining task performance

## Executive Summary
This paper investigates how fine-tuning large language models on seemingly benign task-specific datasets can inadvertently increase model harmfulness, particularly when adversarial actors manipulate dataset structure. The authors demonstrate that malicious actors can modify Q&A datasets using prompting strategies like AutoIF and AutoIF + AOA to create harmful models while maintaining task performance. They propose Paraphrase, a novel mitigation strategy that mixes in safety data adapted to the user's prompting style, which significantly outperforms existing methods at re-establishing safety alignment while preserving task performance.

## Method Summary
The study evaluates multiple prompting strategies (Benign, AOA, AutoIF, AutoIF + AOA) for fine-tuning LLaMA-2 7B and LLaMA-3 8B models on seven Q&A datasets. Models are fine-tuned for one epoch using LoRA 8-bit training with varying proportions of safety data (1-50%). Safety is evaluated using Harmful Instructions (HarmI) and Harmful Questions (HarmQ) datasets, measuring attack success rate (ASR). Three mitigation strategies are compared: Base (no safety data), Longest (baseline mixing), and Paraphrase (safety data adapted to user data style).

## Key Results
- Benign prompting strategies rarely create harmful models, while adversarial strategies like AutoIF + AOA can achieve up to 80% ASR on HarmI
- Paraphrase mitigation consistently achieves ASR near 0% on HarmI and below baseline levels on HarmQ
- Paraphrase maintains downstream task performance close to non-mixing baselines while being more efficient than existing methods
- Safety data adaptation to user data style prevents safety alignment forgetting during fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Malicious actors can automatically adversarially modify benign task-specific datasets to increase harmfulness while maintaining benign appearance and reasonable downstream task performance.
- Mechanism: By applying prompting strategies like AutoIF and AutoIF + AOA, which convert Q&A data into instruction-following format, the fine-tuning process inadvertently prioritizes helpfulness over safety, leading to increased compliance with harmful queries.
- Core assumption: The instruction-following nature of the modified data causes the model to forget some safety alignment established during pre-training.
- Evidence anchors:
  - [abstract]: "malicious actors can subtly manipulate the structure of almost any task-specific dataset to foster significantly more dangerous model behaviors, while maintaining an appearance of innocuity and reasonable downstream task performance."
  - [section]: "we find that malicious users can modify benign datasets to increase harmfulness while avoiding detection (Q2)."
  - [corpus]: Found 25 related papers; average neighbor FMR=0.445 indicates moderate similarity, but no direct evidence of adversarial modification mechanisms in neighbors.
- Break condition: If the toxicity detection rates increase significantly or if downstream task performance drops substantially below baseline levels.

### Mechanism 2
- Claim: The Paraphrase mitigation strategy effectively reduces harmfulness in adversarial settings while maintaining comparable downstream task performance to non-mixing cases.
- Mechanism: Paraphrase modifies safety data to mimic the format and style of user data, allowing the model to learn the task structure while reinforcing safety alignment without forgetting task-specific knowledge.
- Core assumption: Safety data adapted to match user data style prevents the forgetting that occurs during fine-tuning without compromising task performance.
- Evidence anchors:
  - [abstract]: "we propose a novel mitigation strategy that mixes in safety data which mimics the task format and prompting style of the user data, showing this is significantly more effective and efficient than existing baselines at re-establishing safety alignment while maintaining similar task performance."
  - [section]: "Paraphrase consistently leads to a lower ASR on both HarmI and HarmQ compared to the baselines Base and Longest, being the only method that achieves an ASR near 0% on HarmI and lower than the baseline modelâ€™s 19% for HarmQ on all prompting strategies."
  - [corpus]: Weak evidence; neighbors discuss safety alignment but not format-mimicking strategies specifically.
- Break condition: If the model's performance on downstream tasks significantly degrades or if harmfulness rates do not decrease below baseline levels.

### Mechanism 3
- Claim: Benign users are unlikely to accidentally obtain harmful models by training on data suitable for well-defined downstream tasks.
- Mechanism: The Benign prompting strategy, which is typically recommended for Q&A datasets, does not lead to increased harmfulness and often results in the highest downstream task performance.
- Core assumption: Standard prompting strategies used by benign users align with safety objectives and do not prioritize harmful compliance.
- Evidence anchors:
  - [abstract]: "benign users are unlikely to accidentally create harmful models (Q1)"
  - [section]: "we can answer Q1... with no."
  - [corpus]: No direct evidence in neighbors regarding benign user behavior.
- Break condition: If new evidence shows that benign prompting strategies can inadvertently increase harmfulness under certain conditions.

## Foundational Learning

- Concept: Fine-tuning large language models on small, high-quality datasets can enhance performance on specific downstream tasks.
  - Why needed here: Understanding the fine-tuning process is essential to grasp how task-specific data can be manipulated for adversarial purposes.
  - Quick check question: What is the primary goal of fine-tuning in the context of task-specific datasets?

- Concept: Safety alignment in language models involves training them to refuse harmful content while being helpful.
  - Why needed here: Recognizing the importance of safety alignment helps explain why its degradation is a significant concern.
  - Quick check question: What are the two main objectives of safety alignment in language models?

- Concept: Prompting strategies influence model behavior during fine-tuning.
  - Why needed here: Different prompting strategies can lead to varying levels of harmfulness, which is central to the study's findings.
  - Quick check question: How do prompting strategies like AOA and AutoIF affect the safety alignment of fine-tuned models?

## Architecture Onboarding

- Component map: Q&A datasets -> Fine-tuning Process -> Safety Evaluation -> Mitigation Strategies
- Critical path:
  1. Prepare Q&A dataset and select prompting strategy
  2. Fine-tune the model
  3. Evaluate harmfulness on HarmI and HarmQ
  4. Apply mitigation strategy if necessary
  5. Re-evaluate to assess effectiveness
- Design tradeoffs:
  - Balancing safety alignment with task performance
  - Computational efficiency vs. safety effectiveness
  - Detection of adversarial data vs. user data privacy
- Failure signatures:
  - Increased ASR on HarmI and HarmQ without corresponding performance drop
  - High toxicity detection rates in fine-tuning data
  - Significant decrease in downstream task performance after mitigation
- First 3 experiments:
  1. Fine-tune on a Q&A dataset using the Benign strategy and evaluate harmfulness.
  2. Apply the AutoIF prompting strategy to the same dataset and compare harmfulness levels.
  3. Implement the Paraphrase mitigation on the adversarial dataset and assess changes in harmfulness and task performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the Paraphrase mitigation strategy scale with different proportions of safety data in the fine-tuning mixture?
- Basis in paper: [explicit] The paper discusses the effect of varying the percentage of safety data between 1 and 50% on the ASR on HarmI and the refusal rate on XSTest.
- Why unresolved: The paper provides some results on the scaling of the Paraphrase strategy but does not fully explore the optimal proportion of safety data that maximizes both safety and task performance.
- What evidence would resolve it: Comprehensive experiments testing a wider range of safety data proportions, including more granular steps between 1% and 50%, would help identify the optimal balance for different datasets and prompting strategies.

### Open Question 2
- Question: Can the AutoIF and AutoIF + AOA prompting strategies be further optimized to reduce harmfulness while maintaining or improving downstream task performance?
- Basis in paper: [inferred] The paper mentions that the adversarial prompting strategies are primarily demonstrative and have high potential for detection through structural analysis, suggesting room for improvement.
- Why unresolved: The paper does not explore alternative methods for optimizing the conversion of Q&A data to instruction-following format, such as using different LLMs or meta-learning task templates.
- What evidence would resolve it: Experiments comparing different conversion methods, including the use of various LLMs and meta-learning approaches, would provide insights into optimizing these strategies.

### Open Question 3
- Question: How does the Paraphrase mitigation strategy perform when applied to different types of task-specific datasets beyond Q&A?
- Basis in paper: [explicit] The paper focuses on Q&A datasets but mentions the potential for applying the strategy to other task-specific data.
- Why unresolved: The paper does not provide empirical evidence of the Paraphrase strategy's effectiveness on other types of task-specific datasets, such as text summarization or image captioning.
- What evidence would resolve it: Testing the Paraphrase strategy on a diverse set of task-specific datasets and comparing its performance to other mitigation strategies would demonstrate its generalizability and effectiveness across different domains.

## Limitations

- The study focuses specifically on Q&A datasets and instruction-following prompting strategies, which may not generalize to all fine-tuning scenarios
- Safety evaluation relies on HarmI and HarmQ datasets, which may not capture all forms of harmful outputs
- The Paraphrase mitigation requires access to safety data that may not be available in all contexts

## Confidence

- High confidence in the core finding that benign users are unlikely to accidentally create harmful models using standard prompting strategies
- High confidence in the demonstration that malicious actors can manipulate fine-tuning data to increase harmfulness while maintaining task performance
- Medium confidence in the effectiveness of the Paraphrase mitigation strategy, as it shows strong results but may have implementation-specific factors

## Next Checks

1. Test the Paraphrase mitigation strategy on additional dataset types beyond Q&A (e.g., code generation, summarization) to assess generalizability
2. Evaluate whether the ASR improvements persist when using different safety evaluation datasets or human evaluation
3. Investigate whether the adversarial prompting strategies can be detected through statistical analysis of the fine-tuning data before deployment