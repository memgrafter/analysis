---
ver: rpa2
title: What comes after transformers? -- A selective survey connecting ideas in deep
  learning
arxiv_id: '2408.00386'
source_url: https://arxiv.org/abs/2408.00386
tags:
- learning
- deep
- arxiv
- attention
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of deep learning architectures,
  focusing on transformers and their alternatives. It addresses the challenge of keeping
  up with rapid advancements in AI models by offering an overview of influential works
  and identifying key strategies for successful innovations.
---

# What comes after transformers? -- A selective survey connecting ideas in deep learning

## Quick Facts
- arXiv ID: 2408.00386
- Source URL: https://arxiv.org/abs/2408.00386
- Authors: Johannes Schneider
- Reference count: 40
- Primary result: Comprehensive survey identifying transformers' dominance and four key innovation patterns in deep learning

## Executive Summary
This survey addresses the challenge of keeping pace with rapid advancements in deep learning by providing a selective overview of influential architectures and identifying key strategies for successful innovations. The paper focuses on transformers and their alternatives, analyzing why transformers have achieved dominance across multiple domains including NLP, computer vision, and speech processing. Through examination of usage statistics, specialized surveys, and public debates, the author identifies four key patterns that characterize improvements in deep learning models.

## Method Summary
The survey employs a literature review methodology using usage statistics from platforms like Paperswithcode.com, specialized surveys in narrow focus areas, and public debates to select influential works published since 2016. The author identifies common patterns in successful innovations through qualitative analysis of these selected papers, focusing on architectural improvements, training strategies, and emerging alternatives to transformers. The approach aims to provide a coherent overview while acknowledging the rapid pace of innovation in the field.

## Key Results
- Transformers dominate machine learning benchmarks across computer vision, speech, and NLP due to parallelization advantages and self-supervised scalability
- Four key patterns characterize improvements: multi-X, higher order layers, data controlled gating, and moving average techniques
- State-space models and capsule networks show promise as transformer alternatives but face scalability challenges
- Emerging trends include neurosymbolic AI, spiking neural networks, and increased focus on efficiency and interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers dominate despite quadratic attention complexity because parallelization and self-supervised scalability outweigh inefficiencies
- Mechanism: Self-attention allows parallel processing of all tokens, avoiding sequential dependencies of RNNs. Pre-training on massive unlabeled datasets followed by fine-tuning reduces labeling costs.
- Core assumption: Parallelization speedup and scalability of self-supervised learning compensate for quadratic time/space costs
- Evidence anchors: [abstract] Transformers top ML benchmark leaderboards; [section] Transformers more parallelizable than RNNs
- Break condition: Model size or sequence length exceeds feasible memory/compute limits without further optimization

### Mechanism 2
- Claim: Multi-modal and multi-task capabilities arise from transformer's flexible, modular architecture
- Mechanism: Encoder-decoder structure with residual connections and layer normalization allows stacking of diverse layers and objectives. Multi-head attention learns different subspaces for specialization.
- Core assumption: Architectural modularity and attention-based gating enable flexible integration of multiple modalities/tasks
- Evidence anchors: [abstract] Discussion of Meta's Llama and Google's Gemini models; [section] Multi- and Grouped Query Attention description
- Break condition: Added modalities/tasks cause catastrophic interference or degrade performance

### Mechanism 3
- Claim: Self-supervised pre-training with contrastive learning improves generalization by learning rich representations without labeled data
- Mechanism: Augmentations create multiple views of same sample; model learns to maximize agreement between views while pushing apart different samples
- Core assumption: Diversity and quality of augmentations, combined with large batch sizes, enable learning discriminative features that transfer well
- Evidence anchors: [abstract] Discussion of self-supervised pre-training and fine-tuning; [section] SimCLR framework description
- Break condition: Augmentations too weak (overfitting) or too strong (destroy semantic content), or insufficient batch size

## Foundational Learning

- Concept: Self-attention and scaled dot-product attention
  - Why needed here: Core mechanism enabling transformers to weigh token relevance dynamically
  - Quick check question: What is the purpose of scaling by √dk in the attention formula?

- Concept: Residual connections and layer normalization
  - Why needed here: Stabilize training of deep networks and allow gradient flow
  - Quick check question: How do skip connections help mitigate vanishing gradients?

- Concept: Pre-training and fine-tuning paradigm
  - Why needed here: Explain why transformers are trained on large unlabeled data then adapted to specific tasks
  - Quick check question: Why is self-supervised pre-training beneficial compared to supervised training from scratch?

## Architecture Onboarding

- Component map: Input embeddings + positional encodings → Multi-head self-attention → Add & Norm → Feed-forward network → Add & Norm → Output (decoder: cross-attention step) → Linear + softmax
- Critical path: Data → Tokenizer → Embeddings + Positional Encoding → Transformer blocks → (Decoder cross-attention if seq2seq) → Output projection → Loss computation
- Design tradeoffs:
  - Model depth vs. width: Deeper models capture more abstract features but risk overfitting; wider models increase capacity but require more data
  - Attention window size vs. computational cost: Full attention captures all dependencies but is O(n²); sparse/sliding window reduces cost but may miss long-range dependencies
  - Pre-training data size vs. compute: Larger datasets improve generalization but increase training time and cost
- Failure signatures:
  - Vanishing/exploding gradients: Training loss stalls or diverges; check residual connections and normalization
  - Overfitting: Training loss low, validation loss high; increase dropout, regularization, or data augmentation
  - Attention saturation: Attention weights collapse to a few tokens; check scaling factor and initialization
- First 3 experiments:
  1. Train a small transformer (2 layers, 2 heads) on a toy sequence task (e.g., copy or addition) to verify basic functionality
  2. Compare training speed and loss curves with and without layer normalization to confirm its stabilizing effect
  3. Implement and test multi-head attention with varying numbers of heads on a simple classification task to observe specialization effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can state-space models (SSMs) like Mamba achieve state-of-the-art performance on large language models compared to transformers?
- Basis in paper: [explicit] Discussion of Mamba and other SSMs as potential alternatives to transformers
- Why unresolved: While SSMs show promise in sequence modeling and have achieved comparable performance on smaller models, their effectiveness on large-scale language models has not been conclusively demonstrated
- What evidence would resolve it: Empirical results showing SSMs outperforming or matching transformers on standard LLM benchmarks (e.g., GLUE, SuperGLUE, SQuAD) with comparable model sizes and training resources

### Open Question 2
- Question: Will capsule networks eventually achieve state-of-the-art performance on challenging computer vision tasks like ImageNet classification?
- Basis in paper: [explicit] Mention of capsule networks' potential for recognizing whole-part relationships but failure to deliver close to state-of-the-art performance
- Why unresolved: Capsule networks have shown good performance on simpler datasets but have not scaled effectively to more complex tasks
- What evidence would resolve it: Achieving top-5 accuracy on ImageNet that matches or exceeds current transformer-based vision models while maintaining capsule network architecture's theoretical benefits

### Open Question 3
- Question: Can neurosymbolic AI approaches combining neural networks with symbolic reasoning achieve superior performance on complex reasoning tasks compared to pure neural approaches?
- Basis in paper: [explicit] Discussion of neurosymbolic AI's potential for program synthesis and concept extraction
- Why unresolved: While neurosymbolic approaches show promise in specific domains like program synthesis, their general applicability and performance advantages over pure neural methods for complex reasoning tasks remain unproven
- What evidence would resolve it: Demonstrating consistent performance improvements on a range of complex reasoning benchmarks compared to state-of-the-art pure neural approaches while maintaining interpretability and sample efficiency

## Limitations
- Literature review methodology lacks systematic quantitative validation of identified patterns across the deep learning field
- Selection of "influential works" based on usage statistics may introduce publication bias toward visible or recently published works
- Qualitative analysis of four key patterns would benefit from empirical validation across diverse model families

## Confidence
- Transformers' dominance due to parallelization and self-supervised scalability: Medium
- Architectural flexibility enabling multi-modal capabilities: Medium
- Self-supervised learning mechanism effectiveness: High

## Next Checks
1. Conduct systematic citation analysis comparing survey's identified influential works against alternative selection methods to validate representativeness
2. Design controlled experiments testing four identified innovation patterns across different model architectures on standardized benchmarks
3. Implement ablation studies on multi-modal transformer variants to quantify effects of architectural modularity on cross-modal transfer learning