---
ver: rpa2
title: 'The Epochal Sawtooth Phenomenon: Unveiling Training Loss Oscillations in Adam
  and Other Optimizers'
arxiv_id: '2410.10056'
source_url: https://arxiv.org/abs/2410.10056
tags:
- loss
- epoch
- figure
- training
- adam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies and analyzes the Epochal Sawtooth Phenomenon\
  \ (ESP), a recurring training loss pattern observed with adaptive gradient-based\
  \ optimizers like Adam, characterized by a sharp loss drop at each epoch start followed\
  \ by gradual increase. Through empirical studies and theoretical analysis, the authors\
  \ show ESP arises from adaptive learning rate adjustments controlled by Adam's second\
  \ moment estimate and is influenced by factors including data shuffling, \u03B2\
  2 parameter, batch size, and model capacity."
---

# The Epochal Sawtooth Phenomenon: Unveiling Training Loss Oscillations in Adam and Other Optimizers

## Quick Facts
- arXiv ID: 2410.10056
- Source URL: https://arxiv.org/abs/2410.10056
- Authors: Qi Liu; Wanjing Ma
- Reference count: 2
- One-line primary result: This paper identifies the Epochal Sawtooth Phenomenon (ESP), a recurring training loss pattern in adaptive optimizers like Adam, caused by adaptive learning rate adjustments and influenced by β2, batch size, and model capacity.

## Executive Summary
This paper identifies and analyzes the Epochal Sawtooth Phenomenon (ESP), a recurring training loss pattern observed with adaptive gradient-based optimizers like Adam, characterized by a sharp loss drop at each epoch start followed by gradual increase. Through empirical studies and theoretical analysis, the authors show ESP arises from adaptive learning rate adjustments controlled by Adam's second moment estimate and is influenced by factors including data shuffling, β2 parameter, batch size, and model capacity. Smaller β2 values exacerbate ESP, which acts as implicit regularization. The phenomenon is replicated in controlled quadratic optimization tasks, demonstrating its generality beyond neural networks. The authors provide a quantitative model of the loss dynamics and show that while ESP is not necessarily indicative of overfitting, higher model capacity amplifies it. Code for reproducing experiments is provided.

## Method Summary
The authors train BERT-tiny and BERT-small models on Wikitext using Pytorch Adam optimizer (β1=0.9, β2=0.999, lr=1e-4, weight decay=1e-5), plotting loss every 50/500 batches. They systematically vary β1, β2, batch size, model capacity, and shuffling to study ESP behavior. To demonstrate ESP's generality, they implement incremental quadratic optimization with 10,000 random convex functions in 10,000 dimensions, using Adam with shuffled function sequences. The experiments isolate the effect of each hyperparameter on the sawtooth pattern's sharpness, curvature, and amplitude.

## Key Results
- ESP is caused by adaptive learning rate adjustments controlled by Adam's second moment estimate (β2), not by model complexity or dataset specifics
- Smaller β2 values produce more pronounced sawtooth patterns with concave loss curves, while β2 near 1 yields linear increases
- ESP is replicated in quadratic optimization tasks, demonstrating it is a general property of adaptive optimizers, not specific to neural networks
- Higher model capacity amplifies ESP, suggesting it acts as implicit regularization during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sharp drop at the start of each epoch is caused by the model encountering previously seen or easier samples after reshuffling, leading to rapid convergence on those examples.
- Mechanism: At epoch boundaries, the optimizer's momentum from the previous epoch is combined with the initial gradients from the first few batches, creating a larger-than-normal parameter update. Since the first few batches are statistically similar to those at the end of the previous epoch (due to the small sample overlap in shuffled data), the model makes quick progress, causing a sharp loss drop.
- Core assumption: Data shuffling creates partial overlap between the last batches of one epoch and the first batches of the next, and the model retains sufficient momentum to exploit this.
- Evidence anchors:
  - [abstract] "We also identify the 'immediate re-exposure to samples' effect during data shuffling, which causes the model to learn or memorize more at the beginning of each epoch."
  - [section 3.1] "Due to large momentum at the beginning, the model will take large steps towards the optimality of losses of the first few minibatchs. it means that the model will memorize the first few mini-batchs better than the rest."
  - [corpus] Weak: No direct corpus evidence for the immediate re-exposure mechanism; related works focus on loss oscillations but not this specific temporal re-exposure effect.
- Break condition: If data shuffling is disabled or if sampling is with replacement, this mechanism disappears, as confirmed experimentally in the paper.

### Mechanism 2
- Claim: The gradual loss increase within an epoch is driven by Adam's adaptive learning rate mechanism, particularly the second moment estimate controlled by β2, causing the optimizer to take progressively larger or misaligned steps as the epoch progresses.
- Mechanism: Adam's update rule divides the momentum term by the square root of the second moment estimate. When β2 is close to 1, the second moment estimate is smoothed over time, and the learning rate remains relatively stable, producing a nearly linear loss increase. When β2 is smaller, the second moment reacts faster to gradient magnitudes, causing the learning rate to fluctuate more aggressively, leading to a concave loss increase as the optimizer struggles with harder examples later in the epoch.
- Core assumption: The second moment estimate's smoothing behavior (controlled by β2) directly affects the curvature of the loss increase within an epoch.
- Evidence anchors:
  - [abstract] "Our analysis shows that this behavior stems from the adaptive learning rate controlled by the second moment estimate, with β1 playing a minimal role when β2 is large."
  - [section 3.2] "When β2 is close to 1, Adam smooths the second moment estimate over a long period, resulting in stable, linear loss behavior... when β2 is small, the optimizer reacts more quickly to changes in gradient magnitudes, resulting in concave loss patterns."
  - [corpus] Moderate: Related works on Adam discuss β1 and β2 but do not explicitly model the within-epoch loss curvature as a function of β2.
- Break condition: If β2 is set to 1 (no second moment adaptation), this mechanism vanishes, as the learning rate becomes static.

### Mechanism 3
- Claim: The sawtooth pattern is not exclusive to neural networks but is a general property of gradient-based optimization, as demonstrated by its replication in a controlled quadratic minimization task.
- Mechanism: In the quadratic minimization setup, each quadratic function acts on a single variable, and the optimizer encounters a sequence of such functions. The sawtooth pattern emerges because the optimizer's momentum and adaptive learning rates interact with the sequence of loss landscapes, even though the functions are simple and convex. This shows the pattern arises from optimizer dynamics rather than model complexity.
- Core assumption: The sawtooth pattern is a fundamental property of how adaptive optimizers interact with sequential loss landscapes, not a byproduct of deep network training.
- Evidence anchors:
  - [abstract] "We replicate this pattern using incremental quadratic optimization, demonstrating that this phenomenon is not exclusive to deep learning models but also emerges in simple optimization tasks."
  - [section 4] "By solving a sequence of quadratic optimization tasks incrementally, we demonstrate that the Epochal Sawtooth Effect can emerge even in simple optimization scenarios."
  - [corpus] Weak: No corpus evidence for sawtooth patterns in simple quadratic optimization; related works focus on neural network training.
- Break condition: If the sequence of loss functions is not reshuffled (no temporal variation), the sawtooth pattern disappears, as shown in the quadratic experiments.

## Foundational Learning

- Concept: Adaptive gradient-based optimization (e.g., Adam, RMSProp)
  - Why needed here: The phenomenon is specific to optimizers that adjust learning rates per parameter based on historical gradient statistics.
  - Quick check question: How does Adam's update rule differ from standard SGD in terms of learning rate adaptation?

- Concept: Momentum in gradient descent
  - Why needed here: Momentum accumulates gradients over time, influencing the size and direction of parameter updates, especially at epoch boundaries.
  - Quick check question: What role does the β1 parameter play in Adam's momentum term, and how does it affect the sawtooth pattern?

- Concept: Batch processing and data shuffling
  - Why needed here: The pattern depends on how data is presented to the model each epoch, particularly the overlap between consecutive epochs due to shuffling.
  - Quick check question: Why does disabling data shuffling eliminate the sawtooth pattern, according to the paper?

## Architecture Onboarding

- Component map: Data pipeline -> Adam optimizer -> Model (BERT or quadratic functions) -> Loss tracking -> Plot output
- Critical path:
  1. Load and shuffle data each epoch
  2. Process batches, updating model parameters via Adam
  3. Track and average loss over batches
  4. Plot loss curve to observe sawtooth pattern
  5. Adjust hyperparameters (β1, β2, batch size, etc.) to study their influence
- Design tradeoffs:
  - β2 vs. loss curvature: Higher β2 leads to linear loss increase; lower β2 causes concave increase
  - Batch size vs. pattern visibility: Larger batch sizes reduce the sawtooth amplitude
  - Model capacity vs. pattern strength: Higher capacity models exhibit stronger sawtooth effects
  - Shuffling vs. pattern existence: Disabling shuffling or using replacement eliminates the pattern
- Failure signatures:
  - No sawtooth pattern: Check if data shuffling is disabled or if sampling is with replacement
  - Linear loss increase only: Likely β2 is close to 1; try lowering β2 to see concave increase
  - Pattern absent in quadratic experiments: Verify that data (quadratic functions) is shuffled between epochs
- First 3 experiments:
  1. Reproduce the sawtooth pattern with BERT-tiny using default Adam settings (β1=0.9, β2=0.999) and shuffled data; plot loss averaged over 50 batches
  2. Disable data shuffling and observe the disappearance of the sawtooth pattern; compare loss curves
  3. Set β2=0.5 and observe the amplification of the sawtooth pattern and the change in loss curvature from linear to concave

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Epochal Sawtooth Effect (ESE) serve as a reliable indicator of model capacity being too high or insufficient regularization?
- Basis in paper: [explicit] The authors mention it is "an important open question that whether this ESE implies model capacity too high and the regularization strength should be increased."
- Why unresolved: The paper demonstrates ESE is influenced by model capacity and regularization, but does not establish a causal relationship between ESE and these factors.
- What evidence would resolve it: Controlled experiments varying model capacity and regularization strength while measuring ESE magnitude and generalization performance would clarify this relationship.

### Open Question 2
- Question: How does the Epochal Sawtooth Effect impact model generalization and robustness beyond training loss patterns?
- Basis in paper: [inferred] The paper focuses on explaining ESE mechanisms and replicating it in controlled settings, but does not investigate its effects on generalization or model robustness.
- Why unresolved: The authors acknowledge this as a potential direction for future work but do not provide empirical evidence linking ESE to generalization outcomes.
- What evidence would resolve it: Comparative studies of models trained with and without ESE mitigation strategies, measuring generalization gaps and robustness to data distribution shifts.

### Open Question 3
- Question: Can the theoretical model of ESE be extended to account for the convexity observed when β2 is small?
- Basis in paper: [explicit] The authors note that "the replication example and formulas could be remediated to account for the convexity observed when beta2 small."
- Why unresolved: The current quantitative analysis provides a good approximation of ESE but does not fully capture the concave loss curves that emerge with smaller β2 values.
- What evidence would resolve it: Refinement of the mathematical model incorporating additional terms or mechanisms that explain the transition from linear to concave loss patterns as β2 decreases.

## Limitations

- The extent to which ESP findings generalize to more complex, non-convex optimization landscapes beyond the controlled quadratic experiments remains unclear
- The theoretical explanation of ESP arising from adaptive learning rate adjustments is plausible but not rigorously proven for all adaptive optimizers
- The claim that ESP acts as implicit regularization is based on observed correlations but lacks direct evidence of improved generalization

## Confidence

- **High Confidence**: The empirical observation of the ESP in BERT training and its dependence on β2 and shuffling are well-supported by the presented experiments
- **Medium Confidence**: The theoretical explanation of the ESP arising from adaptive learning rate adjustments is plausible but not rigorously proven for all adaptive optimizers
- **Medium Confidence**: The claim that ESP acts as implicit regularization is based on observed correlations (e.g., higher capacity models showing stronger patterns) but lacks direct evidence of improved generalization

## Next Checks

1. **Extended Quadratic Experiments**: Test the ESP in higher-dimensional and non-convex quadratic landscapes to assess its generality beyond convex optimization
2. **Cross-Optimizer Comparison**: Replicate the ESP with other adaptive optimizers (e.g., RMSProp, Adagrad) to confirm it is a universal property of adaptive methods
3. **Generalization Impact**: Measure the effect of ESP on model generalization (e.g., test loss) across different datasets and model architectures to validate its role as implicit regularization