---
ver: rpa2
title: 'The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models
  for Conversational Multi-Doc QA'
arxiv_id: '2402.18385'
source_url: https://arxiv.org/abs/2402.18385
tags:
- language
- training
- documents
- llms
- wsdm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the winning solution to the Conversational
  Multi-Doc QA challenge in WSDM Cup 2024, which focuses on answering questions based
  on contextual conversations and retrieved documents. The core approach leverages
  Large Language Models (LLMs), specifically SOLAR-10.7B-Instruct, and introduces
  several key techniques: (1) a multi-turn conditional generation formulation with
  LLMs, (2) a hybrid training strategy incorporating pseudo-labeled eval set data,
  (3) a noisy document filter using Nomic Embed for relevance scoring, and (4) an
  ensemble method that selects the best response from 8 candidate models.'
---

# The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA

## Quick Facts
- arXiv ID: 2402.18385
- Source URL: https://arxiv.org/abs/2402.18385
- Authors: Yiming Li; Zhao Zhang
- Reference count: 20
- Primary result: First place in WSDM Cup 2024 Conversational Multi-Doc QA challenge, outperforming second place by 1.6%, 0.9%, and 2.3% on W-ROUGE-L, C-ROUGE-L, and KR metrics

## Executive Summary
This paper presents the winning solution to the Conversational Multi-Doc QA challenge in WSDM Cup 2024, which focuses on answering questions based on contextual conversations and retrieved documents. The core approach leverages Large Language Models (LLMs), specifically SOLAR-10.7B-Instruct, and introduces several key techniques including multi-turn conditional generation, hybrid training with pseudo-labeled eval set data, noisy document filtering using Nomic Embed, and an ensemble method for candidate selection. The solution achieved first place in the competition, demonstrating the effectiveness of these techniques for conversational question answering tasks.

## Method Summary
The solution adapts SOLAR-10.7B-Instruct LLM to conversational multi-document QA using a multi-turn conditional generation formulation that forces attention to conversational context across dialogue turns. The method employs a hybrid training strategy that incorporates pseudo-labeled eval set data generated by a well-trained model, effectively performing knowledge distillation on in-domain data. Document filtering uses Nomic Embed to compute relevance scores and remove noisy documents based on cosine similarity thresholds. An ensemble approach selects the best response from 8 candidate models using consensus-based selection, where candidates are evaluated using embedding similarity and ROUGE metrics. The entire pipeline is trained with LoRA fine-tuning (lora_rank=8, lora_alpha=16, lora_dropout=0.05) for 4 epochs.

## Key Results
- Achieved first place in WSDM Cup 2024 Conversational Multi-Doc QA challenge
- Outperformed second-place team by 1.6% on W-ROUGE-L metric
- Outperformed second-place team by 0.9% on C-ROUGE-L metric
- Outperformed second-place team by 2.3% on Keywords Recall (KR) metric

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-turn conditional generation formulation improves LLM performance by forcing attention to conversational context.
- Mechanism: By using a multi-turn mode where loss masks include both final answers and intermediate conversational responses, the LLM learns to better integrate contextual information across dialogue turns.
- Core assumption: The conversational context contains important information for answering the final question that would be missed with single-turn training.
- Evidence anchors:
  - [section]: "Specifically, there are two modes to determine ð‘šð‘–: 1) the single-turn mode, which means thatð‘šð‘– = 1 if and only ifð‘¢ð‘– belongs to {a}. 2) the multi-turn mode, ð‘šð‘– = 1 as long as ð‘¢ð‘– belongs to {a} or {ai}. We conduct a toy experiment to examine them with the Llama2-13B-base model [10], the results are shown in Table 1. It can be seen that the multi-turn mode results in a better performance as it forces the LLM to pay more attention to contextual information."
  - [abstract]: "We first adapt LLMs to the task, then devise a hybrid training strategy to make the most of in-domain unlabeled data."
  - [corpus]: No direct evidence in corpus about multi-turn effectiveness.
- Break condition: If conversational context is irrelevant or noisy, multi-turn training could harm performance by forcing the model to attend to unhelpful information.

### Mechanism 2
- Claim: The hybrid training strategy improves performance by incorporating pseudo-labeled eval set data.
- Mechanism: A well-trained model generates pseudo answers for the unlabeled eval set, which is then combined with the original training set to fine-tune a new model, effectively performing knowledge distillation on in-domain data.
- Core assumption: The eval set distribution is similar to the test set, making pseudo-labeled eval data valuable for training.
- Evidence anchors:
  - [section]: "Appropriate labeled texts from a similar distribution may contribute a lot to the improvement of LLMs' generation performance. During phase 2, we propose to utilize a well-trained model to produce (pseudo) answers for the eval dataset before adding them to the original training set to finetune a new model from scratch."
  - [section]: "incorporating the eval set with the corresponding pseudo targets can largely boost the generation quality, especially for the keywords recall score."
  - [corpus]: No direct evidence in corpus about hybrid training effectiveness.
- Break condition: If the pseudo labels are poor quality or the eval set distribution differs significantly from the test set, this could degrade performance.

### Mechanism 3
- Claim: The ensemble method improves final answer quality by selecting the best response from multiple candidates.
- Mechanism: For each test sample, relevance scores between candidate responses are calculated using embedding similarity and ROUGE metrics. The candidate with the highest consensus score (sum of relevance to other candidates) is selected as the final answer.
- Core assumption: High-quality answers will be more similar to each other than to poor answers, making consensus-based selection effective.
- Evidence anchors:
  - [section]: "The model ensemble has proven to be effective in discriminative tasks, however, it is rarely explored under generative settings. In this work, we propose to approximately evaluate the quality of generated answers from different models and then select the best one as the final result."
  - [section]: "As seen from Figure 2 (b), more candidates generally lead to better performance."
  - [corpus]: No direct evidence in corpus about ensemble effectiveness for generative tasks.
- Break condition: If all candidate models are poor quality or make similar errors, ensemble selection won't improve results.

## Foundational Learning

- Concept: Multi-turn dialogue understanding
  - Why needed here: The task involves conversational question answering where previous Q&A pairs provide context for the final question.
  - Quick check question: What information from previous turns in a conversation might be relevant to answering the final question?

- Concept: Text embedding and similarity measurement
  - Why needed here: Used for filtering irrelevant documents and for ensemble candidate selection through similarity scoring.
  - Quick check question: How would you compute the semantic similarity between two documents using embeddings?

- Concept: ROUGE metrics for text evaluation
  - Why needed here: The evaluation uses ROUGE-L (word and character level) and Keywords Recall to assess answer quality.
  - Quick check question: What's the difference between word-level and character-level ROUGE-L in terms of what they measure?

## Architecture Onboarding

- Component map: Input preprocessor -> Document filter -> LLM backbone -> Hybrid trainer -> Ensemble selector -> Output formatter
- Critical path: Input â†’ Document filtering â†’ LLM generation â†’ Ensemble selection â†’ Output
- Design tradeoffs:
  - Document filtering threshold: Too strict removes useful context; too loose includes noise
  - Number of ensemble candidates: More candidates improve quality but increase inference time and cost
  - Pseudo-label quality vs quantity: More pseudo-labeled data helps but poor quality hurts
- Failure signatures:
  - Poor document filtering: Model includes irrelevant information in answers
  - Weak pseudo-labels: Hybrid training degrades rather than improves performance
  - Ensemble failure: All candidates are poor quality or too similar
- First 3 experiments:
  1. Test single-turn vs multi-turn training modes with a small dataset to verify the claimed improvement
  2. Implement and test the document filtering pipeline on a sample of noisy documents to verify threshold effectiveness
  3. Create a simple ensemble with 2-3 candidate models using different metrics to validate the selection approach

## Open Questions the Paper Calls Out
None

## Limitations
- Key implementation details remain unspecified, including exact threshold values (Ï„_h and Ï„_l) for document filtering
- Hybrid training strategy implementation details are unclear, particularly how pseudo-labeled eval set data was generated and incorporated
- Specific metrics and weighting schemes used for ensemble candidate selection are not detailed
- Lack of ablation studies on individual components' contributions beyond hybrid training and ensemble approaches

## Confidence
- **High Confidence**: The overall competition win and the reported metric improvements (1.6%, 0.9%, and 2.3% gains on W-ROUGE-L, C-ROUGE-L, and KR respectively) are well-documented and verifiable through the competition leaderboard.
- **Medium Confidence**: The effectiveness of the multi-turn conditional generation formulation and the hybrid training strategy are supported by in-paper experiments and reasonable mechanisms, but lack external validation.
- **Low Confidence**: The marginal benefit of the noisy document filter (described as showing only marginal improvement) and the specific implementation details of the ensemble method's consensus calculation are less well-supported by evidence.

## Next Checks
1. **Document Filtering Validation**: Implement the Nomic Embed-based document filtering with varying threshold values and test on a sample dataset with known noisy documents to empirically determine the optimal thresholds (Ï„_h and Ï„_l) and measure the impact on answer quality.

2. **Hybrid Training Ablation**: Conduct controlled experiments comparing models trained with and without the pseudo-labeled eval set data to quantify the exact contribution of the hybrid training strategy to overall performance improvements.

3. **Ensemble Method Testing**: Create a simplified ensemble with 3-4 candidate models using different generation strategies (e.g., temperature variations, decoding algorithms) and test the consensus selection approach with different scoring metrics to validate whether ensemble selection consistently improves over individual model outputs.