---
ver: rpa2
title: 'Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource Scripts'
arxiv_id: '2412.16119'
source_url: https://arxiv.org/abs/2412.16119
tags:
- scripts
- text
- figure
- blur
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks Large Language Models (LLMs), specifically
  GPT-4o, for OCR performance across low-resource scripts (Urdu, Albanian, Tajik)
  and English. A controlled dataset of 2,520 images was created, varying text length,
  font size, background color, and blur.
---

# Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource Scripts

## Quick Facts
- arXiv ID: 2412.16119
- Source URL: https://arxiv.org/abs/2412.16119
- Authors: Muhammad Abdullah Sohail; Salaar Masood; Hamza Iqbal
- Reference count: 13
- Primary result: LLM-based OCR shows significant limitations for low-resource scripts, with Urdu WER rising from 0.20 to 0.35 as word count increases

## Executive Summary
This study benchmarks GPT-4o's OCR performance across low-resource scripts (Urdu, Albanian, Tajik) and English using a controlled dataset of 2,520 images. The research reveals significant disparities in OCR accuracy, with complex scripts like Urdu experiencing sharp performance degradation under challenging conditions such as longer texts, smaller fonts, and low-contrast backgrounds. English and Albanian, benefiting from simpler Latin-based structures, maintain near-zero error rates across all test conditions. These findings highlight the limitations of zero-shot LLM-based OCR for underrepresented languages and underscore the need for annotated datasets and fine-tuned models to improve robustness.

## Method Summary
The researchers created a controlled dataset of 2,520 images across four languages, systematically varying text length (40-60, 110-130, 180-200 words), font size (12, 18, 24pt), background color (white, slate gray, light yellow), and blur levels (0, 0.75, 1.5 Gaussian). GPT-4o performed zero-shot OCR on Base64-encoded images using a structured system prompt. Performance was evaluated using Character Error Rate (CER), Word Error Rate (WER), and BLEU scores, with temperature set to 0 for deterministic outputs.

## Key Results
- Urdu WER increased from 0.20 to 0.35 as word count grew from 40-60 to 180-200 words
- Urdu OCR accuracy dropped sharply with smaller font sizes, from WER 0.50 at size 12 to 0.21 at size 24
- Low-contrast backgrounds (slate gray) caused Urdu WER to spike to 0.52, more than double the error rate on white backgrounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4o's zero-shot OCR performance degrades predictably as word count increases due to error accumulation in complex scripts.
- Mechanism: Longer sequences increase the likelihood of cumulative recognition errors, especially in scripts with dense ligatures and diacritics like Urdu.
- Core assumption: The model's contextual understanding does not scale linearly with input length for complex scripts.
- Evidence anchors:
  - [abstract] Results emphasize the limitations of zero-shot LLM-based OCR, particularly for linguistically complex scripts, highlighting the need for annotated datasets and fine-tuned models.
  - [section] Figure 1 shows Urdu's WER rising sharply from 0.20 for shorter texts (40-60 words) to 0.35 for longer ones (180-200 words).
  - [corpus] No direct corpus evidence on error accumulation, but related work on low-resource OCR systems confirms similar patterns.
- Break condition: Performance plateaus or improves if the model's training data includes extensive examples of long-form text in complex scripts.

### Mechanism 2
- Claim: Font size reduction disproportionately impacts OCR accuracy for scripts with dense visual details.
- Mechanism: Smaller font sizes reduce visual clarity, causing character boundaries to blur, particularly in scripts with intricate ligatures and diacritics.
- Core assumption: Visual recognition relies on sufficient pixel density to distinguish character features.
- Evidence anchors:
  - [abstract] Smaller font sizes and low-contrast backgrounds (slate gray) further degraded accuracy, particularly for Urdu (WER up to 0.52).
  - [section] Figure 4 shows Urdu's WER dropping sharply from 0.50 at size 12 to 0.24 at 18 and 0.21 at 24.
  - [corpus] Weak corpus evidence; no direct mention of font size effects, but related synthetic OCR datasets address visual scaling challenges.
- Break condition: Performance stabilizes if adaptive scaling or resolution enhancement techniques are applied during preprocessing.

### Mechanism 3
- Claim: Low-contrast backgrounds (slate gray) impair OCR performance by reducing character boundary detection.
- Mechanism: Contrast-based edge detection fails when text-to-background differentiation is minimal, leading to misalignment and character-level inaccuracies.
- Core assumption: OCR systems rely on sufficient contrast to delineate character boundaries.
- Evidence anchors:
  - [abstract] Smaller font sizes and low-contrast backgrounds (slate gray) further degraded accuracy, particularly for Urdu (WER up to 0.52).
  - [section] Figure 7 shows Urdu's WER spikes to 0.52, more than double its error on white (0.24) or light yellow (0.26).
  - [corpus] No direct corpus evidence on background color effects, but synthetic dataset generators mention contrast variations.
- Break condition: Performance improves if adaptive thresholding or contrast normalization techniques are applied.

## Foundational Learning

- Concept: Character Error Rate (CER) and Word Error Rate (WER)
  - Why needed here: These metrics quantify OCR accuracy at character and word levels, essential for evaluating model performance across different scripts.
  - Quick check question: If a predicted text has 3 substitutions, 2 insertions, and 1 deletion out of 100 reference characters, what is the CER?

- Concept: Multimodal LLMs and OCR
  - Why needed here: Understanding how GPT-4o processes visual input and generates text is crucial for interpreting its OCR capabilities and limitations.
  - Quick check question: What distinguishes zero-shot inference from fine-tuned inference in the context of OCR?

- Concept: Script complexity and OCR challenges
  - Why needed here: Recognizing how structural features like ligatures, diacritics, and bidirectional text affect OCR performance explains the observed disparities.
  - Quick check question: Why would a script with fused ligatures be more challenging for OCR than a linear script?

## Architecture Onboarding

- Component map: Image encoding -> GPT-4o inference -> Text extraction -> Error rate computation
- Critical path: Base64-encoded images are processed by GPT-4o's multimodal model with a structured system prompt, producing transcribed text that is evaluated using CER, WER, and BLEU scores
- Design tradeoffs: Zero-shot approach avoids fine-tuning costs but limits performance on underrepresented scripts; controlled dataset ensures reproducibility but restricts real-world generalizability
- Failure signatures: Sharp performance drops with longer texts, smaller fonts, or low-contrast backgrounds; near-zero errors for high-resource Latin scripts under all conditions
- First 3 experiments:
  1. Test OCR performance on Urdu text with varying word counts (40-60, 110-130, 180-200) to observe error accumulation
  2. Evaluate Urdu OCR accuracy across font sizes (12, 18, 24) to assess the impact of visual clarity
  3. Measure Urdu OCR performance on different background colors (white, slate gray, light yellow) to quantify contrast effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific model architectures and training techniques would most effectively improve OCR accuracy for Urdu and Tajik scripts?
- Basis in paper: [explicit] The paper identifies limitations of zero-shot LLM-based OCR for linguistically complex scripts like Urdu and Tajik, and calls for annotated datasets and fine-tuned models.
- Why unresolved: While the paper identifies the need for fine-tuned models, it does not specify which architectures or techniques would be most effective.
- What evidence would resolve it: Experimental results comparing various model architectures (e.g., CNN-RNN, transformer-based) and training techniques (e.g., fine-tuning, transfer learning) on annotated datasets for Urdu and Tajik would provide insights into the most effective approaches.

### Open Question 2
- Question: How do LLM-based OCR systems perform on handwritten text and scene text in low-resource scripts compared to printed text?
- Basis in paper: [inferred] The paper mentions expanding datasets to include handwriting recognition and scene text scenarios in future work, suggesting this is an unexplored area.
- Why unresolved: The current study focuses on printed text images with controlled variations, leaving the performance of LLM-based OCR on handwritten and scene text in low-resource scripts unknown.
- What evidence would resolve it: Experimental results evaluating LLM-based OCR systems on datasets containing handwritten and scene text in low-resource scripts like Urdu, Tajik, and Albanian would provide insights into their performance in these real-world scenarios.

### Open Question 3
- Question: What are the computational and resource requirements for implementing LLM-based OCR systems for low-resource languages in real-world applications?
- Basis in paper: [explicit] The paper highlights the high cost of GPT-4o inference as a limitation, indicating the need for more accessible solutions.
- Why unresolved: While the paper acknowledges the cost barrier, it does not explore the specific computational and resource requirements for implementing LLM-based OCR systems in real-world applications.
- What evidence would resolve it: Detailed analysis of the computational resources (e.g., processing power, memory) and financial costs required to deploy LLM-based OCR systems for low-resource languages in practical settings would provide insights into their feasibility and scalability.

## Limitations

- The controlled synthetic dataset may not capture real-world variability in text presentation and document quality
- Zero-shot approach inherently limits performance on underrepresented scripts due to training data bias
- Absence of direct corpus evidence for certain mechanisms means some claims rely on indirect inference from related work

## Confidence

**High Confidence**: The observed degradation in OCR accuracy for Urdu with increasing word count and the near-zero error rates for English and Albanian across all conditions are well-supported by the experimental data.

**Medium Confidence**: The mechanisms explaining why complex scripts like Urdu are more susceptible to errors are plausible but not directly validated in the study. Related work supports these claims, but the specific interactions with GPT-4o's architecture remain speculative.

**Low Confidence**: Claims about the impact of font size and background color on OCR accuracy are based on experimental results but lack direct corpus evidence or ablation studies to isolate these factors from other variables.

## Next Checks

1. Conduct a detailed error analysis on Urdu OCR outputs to identify specific failure modes and correlate them with script features

2. Test OCR performance on additional low-resource scripts (e.g., Pashto, Bengali) to determine if observed patterns are consistent across linguistically diverse scripts

3. Evaluate whether adaptive preprocessing steps (e.g., contrast normalization, resolution enhancement) can mitigate performance drops under challenging conditions