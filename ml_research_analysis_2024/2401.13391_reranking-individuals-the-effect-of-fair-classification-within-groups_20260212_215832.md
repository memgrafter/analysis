---
ver: rpa2
title: 'Reranking individuals: The effect of fair classification within-groups'
arxiv_id: '2401.13391'
source_url: https://arxiv.org/abs/2401.13391
tags:
- bias
- fairness
- methods
- mitigation
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates unintended consequences of popular bias
  mitigation methods in fair classification. The authors demonstrate that these methods
  not only affect rankings between sensitive groups but also significantly alter rankings
  within each group, often in arbitrary ways.
---

# Reranking individuals: The effect of fair classification within-groups

## Quick Facts
- arXiv ID: 2401.13391
- Source URL: https://arxiv.org/abs/2401.13391
- Authors: Sofie Goethals; Marco Favier; Toon Calders
- Reference count: 15
- One-line primary result: Popular bias mitigation methods in fair classification not only affect rankings between sensitive groups but also significantly alter rankings within each group, often in arbitrary ways.

## Executive Summary
This paper investigates the unintended consequences of popular bias mitigation methods in fair classification, revealing that these methods not only affect rankings between sensitive groups but also significantly alter rankings within each group. Using five real-world datasets, the authors demonstrate that preprocessing and inprocessing methods substantially change prediction scores and rankings, while postprocessing methods only adjust labels. The study highlights the need for more nuanced fairness evaluation frameworks that consider both between-group and within-group effects, as most methods fail to satisfy fairness metrics and drastically alter positive decision rates.

## Method Summary
The authors evaluate bias mitigation methods using five real-world datasets: Adult Income, Compas, Dutch Census, Law admission, and Student Performance. They train fully connected neural networks with binary cross-entropy loss and apply various bias mitigation methods (preprocessing, inprocessing, postprocessing) from the AIF360 package. The study compares prediction scores and labels before and after applying these methods, assessing the impact on AUC scores, fairness metrics, and positive decision rates. Kendall Tau correlation is used to measure similarity between original and post-mitigation rankings.

## Key Results
- Preprocessing and inprocessing methods substantially change prediction scores and rankings within each sensitive group.
- Postprocessing methods preserve the original ranking but only adjust labels to satisfy fairness constraints.
- Most bias mitigation methods fail to satisfy fairness metrics and drastically alter positive decision rates, making direct label-based comparisons problematic.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bias mitigation methods that alter prediction scores cause arbitrary reranking within each sensitive group.
- Mechanism: These methods (e.g., LFR, ADV, MFC) change the underlying score distribution in ways that disrupt the original ordinal ranking, so individuals within a group can switch from positive to negative labels and vice versa.
- Core assumption: The original model's ranking reflects a meaningful ordering of individuals; changes to this ranking are not explainable by shifts in true underlying risk.
- Evidence anchors:
  - [abstract] "these methods can also change which specific individuals are selected within each group"
  - [section 5.1] "prediction scores and consequently the intrinsic rankings are substantially modified by every preprocessing and inprocessing bias mitigation method"
  - [corpus] weak — no corpus neighbor directly addresses within-group reranking

### Mechanism 2
- Claim: Post-processing methods preserve the original ranking but flip labels to satisfy fairness constraints.
- Mechanism: Methods like ROC, TO, and EOP only adjust thresholds or flip labels for individuals at the decision boundary, leaving all prediction scores unchanged.
- Core assumption: Fairness constraints can be met by adjusting who is labeled positive/negative without changing the model's confidence ordering.
- Evidence anchors:
  - [abstract] "postprocessing methods only adjust labels"
  - [section 5.1] "the used postprocessing methods do not alter the inherent ranking; instead, they adjust labels based on the prediction scores"
  - [corpus] weak — corpus lacks direct coverage of post-processing label flipping strategies

### Mechanism 3
- Claim: The positive decision rate (PDR) varies drastically across bias mitigation methods, making direct label-based comparison invalid.
- Mechanism: Different methods produce different numbers of positive predictions; comparing accuracy or fairness at fixed thresholds conflates changes due to threshold shifts with changes due to model quality.
- Core assumption: In real-world deployment, the number of positive decisions is constrained by external factors (quotas, business rules).
- Evidence anchors:
  - [abstract] "these methods fail to satisfy fairness metrics and drastically alter positive decision rates"
  - [section 5.2] "Table 3 reveals significant positive decision rate disparities among different strategies"
  - [corpus] weak — corpus does not directly address PDR variability across mitigation methods

## Foundational Learning

- Concept: Prediction score vs. prediction label
  - Why needed here: Understanding that models output a continuous score (0-1) and labels are derived from a threshold is essential to interpret why bias mitigation affects rankings.
  - Quick check question: What is the difference between a model's prediction score and its prediction label?

- Concept: Fairness metrics (demographic parity, equalized opportunity)
  - Why needed here: The paper compares methods on these metrics, so knowing what they measure and their limitations is key.
  - Quick check question: How does demographic parity differ from equalized opportunity in what it requires across groups?

- Concept: Kendall Tau correlation
  - Why needed here: The paper uses Kendall Tau to measure similarity between original and post-mitigation rankings.
  - Quick check question: What does a Kendall Tau value of 1, 0, or -1 indicate about two ranked lists?

## Architecture Onboarding

- Component map: Data loader → Preprocessing bias mitigation (LFR, DIR) → Model training → Inprocessing bias mitigation (ADV, MFC) → Prediction scores → Postprocessing bias mitigation (ROC, EOP, TO) → Final labels
- Critical path: For evaluating ranking changes: original scores → mitigation → new scores → Kendall Tau comparison. For evaluating fairness: scores → labels (at threshold) → fairness metrics calculation.
- Design tradeoffs:
  - Preprocessing methods: highest potential for within-group ranking disruption but can remove sensitive attribute correlation.
  - Inprocessing methods: balance between fairness and ranking preservation; may still alter rankings significantly.
  - Postprocessing methods: preserve ranking, only adjust labels; simplest to implement but limited in what fairness constraints they can satisfy.
- Failure signatures:
  - Large drop in AUC after mitigation → model ranking degraded.
  - PDR far from expected value → method unrealistic for deployment.
  - Kendall Tau near zero → ranking nearly random after mitigation.
- First 3 experiments:
  1. Run the original model and one mitigation method (e.g., LFR) on a small dataset; compute AUC, PDR, and Kendall Tau between original and mitigated scores.
  2. Compare label distributions at 0.5 threshold for original and mitigated models; calculate demographic parity difference.
  3. Apply postprocessing (e.g., TO) to the mitigated model; check if fairness metrics improve without changing AUC.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does within-group bias exist in real-world datasets, and if so, how prevalent is it?
- Basis in paper: [explicit] The paper states that the Affirmative Action assumption holds if and only if for all a ∈ A it holds that for almost all (x, a), (x′, a) ∈ X × {a} p(Y = 1|X = x, A = a) ≤ p(Y = 1|X = x′, A = a) ⇒ S(x, a) ≤ S(x′, a), and that if this assumption does not hold, there is some form of within-group bias.
- Why unresolved: The paper discusses the theory behind within-group bias and its implications, but does not provide empirical evidence on its prevalence in real-world datasets.
- What evidence would resolve it: Empirical studies analyzing the prevalence of within-group bias in various real-world datasets and its impact on the effectiveness of bias mitigation methods.

### Open Question 2
- Question: How should we evaluate bias mitigation methods when the goal is to address both between-group and within-group fairness?
- Basis in paper: [inferred] The paper argues that current evaluation methods focusing on prediction labels are insufficient due to alterations in global selection rates and the effects within each group. It suggests using AUC scores and prediction scores instead.
- Why unresolved: The paper proposes using AUC scores and prediction scores for evaluation, but does not provide a comprehensive framework for evaluating bias mitigation methods when considering both between-group and within-group fairness.
- What evidence would resolve it: Development and validation of a comprehensive evaluation framework that considers both between-group and within-group fairness, incorporating metrics such as AUC scores, prediction scores, and other relevant fairness metrics.

### Open Question 3
- Question: Is the reranking of individuals within each group, as a side effect of fair classification, always undesirable?
- Basis in paper: [explicit] The paper discusses the reranking process within each group as a consequence of bias mitigation methods and states that it is not necessarily negative, but deserves more attention.
- Why unresolved: The paper acknowledges that reranking within each group is not necessarily undesirable, but does not provide a clear answer on when it might be beneficial or detrimental.
- What evidence would resolve it: Empirical studies analyzing the impact of within-group reranking on the overall fairness and performance of bias mitigation methods, considering different scenarios and datasets.

## Limitations
- The analysis is based on a small set of real-world datasets with limited sample sizes, which may constrain the generalizability of the findings.
- The study does not thoroughly investigate the relationship between ranking changes and shifts in underlying true risk, leaving open the possibility that some reranking is justified rather than arbitrary.
- The research focuses on specific bias mitigation methods from the AIF360 library and does not explore a broader range of techniques or custom implementations.

## Confidence

- **High confidence**: The claim that postprocessing methods preserve rankings while only adjusting labels is well-supported by both the paper's analysis and the mechanism described.
- **Medium confidence**: The assertion that preprocessing and inprocessing methods substantially alter prediction scores and rankings is supported by the evidence, but the arbitrary nature of the reranking could benefit from further investigation into underlying risk shifts.
- **Low confidence**: The generalizability of the findings to other datasets, bias mitigation methods, or deployment contexts remains uncertain due to the limited scope of the study.

## Next Checks
1. Replicate the analysis on a new dataset to verify the consistency of the findings across different data distributions and sensitive attributes.
2. Investigate the relationship between ranking changes and shifts in underlying true risk by comparing the original and post-mitigation rankings with an oracle that has access to the true outcomes.
3. Explore alternative bias mitigation methods or custom implementations to assess whether the within-group reranking effects are specific to the methods used in this study or a more general phenomenon.