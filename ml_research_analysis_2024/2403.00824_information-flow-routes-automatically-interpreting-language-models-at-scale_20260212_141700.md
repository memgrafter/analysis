---
ver: rpa2
title: 'Information Flow Routes: Automatically Interpreting Language Models at Scale'
arxiv_id: '2403.00824'
source_url: https://arxiv.org/abs/2403.00824
tags:
- heads
- information
- attention
- figure
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method for automatically interpreting large
  language models by extracting information flow routes. The core idea is to view
  the model as a graph where nodes are token representations and edges are operations,
  then trace the important paths for each prediction using attribution.
---

# Information Flow Routes: Automatically Interpreting Language Models at Scale

## Quick Facts
- **arXiv ID**: 2403.00824
- **Source URL**: https://arxiv.org/abs/2403.00824
- **Authors**: Javier Ferrando; Elena Voita
- **Reference count**: 28
- **Primary result**: Proposes a method to automatically interpret LLMs by extracting information flow routes using attribution in a single forward pass, ~100x faster than activation patching

## Executive Summary
This paper introduces a method for automatically interpreting large language models by extracting information flow routes - subgraphs of the model that are most important for each prediction. The approach views the model as a graph where nodes are token representations and edges are operations, then traces important paths using an attribution method (ALTI) rather than the slower activation patching. The method recovers previously discovered circuits like Indirect Object Identification and Greater-than tasks, identifies generally important heads like previous token and subword merging heads, and reveals domain-specific model components specialized for coding or multilingual text.

## Method Summary
The method extracts information flow routes by computing edge importance scores in a single forward pass using ALTI (Aggregation of Layer-Wise Token-to-Token Interactions), which measures proximity between input vectors and their contribution to output nodes. It builds a graph representation of the model where nodes are token representations and edges are operations (attention heads, FFN blocks), then prunes edges below a threshold τ to retain only the important subgraph. The approach is significantly faster than activation patching while recovering known circuits and identifying new important components, with analysis showing that component importance varies by part of speech, domain, and prediction type.

## Key Results
- Recovers previously discovered circuits for IOI and Greater-than tasks while finding new important heads
- Identifies generally important heads like previous token and subword merging heads across predictions
- Shows some model components are specialized for domains like coding or non-English text
- Method is approximately 100 times faster than activation patching alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Information flow routes can be extracted in a single forward pass using attribution rather than activation patching
- Mechanism: Uses ALTI proximity measure to compute edge importance between input vectors and their contribution to output nodes
- Core assumption: The proximity measure reliably identifies which edges contribute most to final prediction
- Evidence: 100x faster than alternatives while recovering known circuits
- Break condition: If proximity measure fails to correlate with prediction importance

### Mechanism 2
- Claim: Information flow routes can recover previously discovered circuits like IOI and greater-than tasks
- Mechanism: Computes importance scores for all edges and retains those above threshold τ
- Core assumption: Important heads identified by attribution match those found by contrastive patching
- Evidence: Recovers known circuits for IOI and Greater-than tasks
- Break condition: If attribution scores don't align with task-specific importance from patching

### Mechanism 3
- Claim: Information flow routes can reveal domain-specific model components specialized for certain tasks or languages
- Mechanism: Computes average importance across predictions from different domains
- Core assumption: Domain specialization manifests as consistently higher importance scores
- Evidence: Identifies components specialized for coding and multilingual texts
- Break condition: If domain specialization doesn't correlate with importance score differences

## Foundational Learning

- **Transformer architecture and residual streams**: Needed to understand how information flows through attention and FFN blocks in the residual stream. Quick check: How does the residual connection interact with attention and FFN outputs in a typical transformer block?

- **Activation patching methodology**: Needed to understand the contrast with traditional patching methods and their limitations. Quick check: What are the key limitations of activation patching that the attribution method aims to address?

- **Attribution methods and proximity measures**: Needed to understand how ALTI's proximity-based attribution computes edge importance. Quick check: How does the negative L1 distance measure in ALTI capture the importance of an input vector to an output node?

## Architecture Onboarding

- **Component map**: Token embeddings -> Layer normalization -> Graph structure (nodes = token representations, edges = operations) -> Attribution module (computes edge importance using ALTI) -> Pruning module (retains edges above threshold τ) -> Analysis module (aggregates importance scores)

- **Critical path**: Forward pass → Edge importance computation → Subgraph pruning → Analysis of important components

- **Design tradeoffs**: Single forward pass (fast) vs. multiple patching interventions (slower but potentially more precise); General importance (captures all relevant components) vs. contrastive importance (isolates task-specific components); Threshold sensitivity (τ selection affects subgraph size and interpretability)

- **Failure signatures**: Important edges missed due to threshold setting; Attribution scores don't correlate with actual prediction importance; Domain-specific patterns obscured by general importance measures

- **First 3 experiments**: 
  1. Run method on GPT2-small with IOI task and compare recovered heads to Wang et al. (2023) results
  2. Vary threshold τ and observe how subgraph structure changes for a simple task
  3. Apply method to Llama 2-7B on C4 dataset and identify most consistently important attention heads

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can we develop more efficient methods to automatically discover specialized model components without relying on human-defined templates or contrastive examples?
- **Basis**: Paper acknowledges limitations of patching methods requiring human-designed templates and proposes faster attribution method, but doesn't provide comprehensive framework for systematic discovery
- **Why unresolved**: Attribution method improves efficiency but still requires threshold setting and doesn't fully automate discovery of specialized components
- **What evidence would resolve it**: Framework that can automatically identify and categorize specialized components across domains and languages without human intervention, validated on multiple models

### Open Question 2
- **Question**: What is the relationship between importance of model components for general predictions versus specific tasks, and how can we quantify this relationship?
- **Basis**: Paper shows some heads are generally important while others are specialized, and importance varies by part of speech and domain
- **Why unresolved**: Identifies patterns but lacks quantitative framework to measure or predict general vs. task-specific importance relationship
- **What evidence would resolve it**: Mathematical framework or metric that can predict general vs. task-specific importance based on architecture and training data, validated across models and tasks

### Open Question 3
- **Question**: How does the "period acting as BOS" phenomenon affect model performance and generation quality, and can we mitigate its negative effects?
- **Basis**: Paper identifies phenomenon where periods can act as BOS tokens causing unusual information flow patterns in Llama 2-7B
- **Why unresolved**: Identifies phenomenon but doesn't investigate frequency, impact on generation quality, or mitigation strategies
- **What evidence would resolve it**: Comprehensive study showing frequency and impact on generation quality with effective detection and mitigation methods, validated on multiple models

## Limitations
- Attribution method's effectiveness in capturing edge importance needs more extensive validation across diverse tasks
- Method's generalization to non-English and code-specific domains is demonstrated but with limited analysis of potential biases
- 100x speedup claim over activation patching lacks direct runtime comparisons for verification

## Confidence
- **High confidence**: Core claim that information flow routes can be extracted via single-pass attribution
- **Medium confidence**: Recovery of known circuits and domain-specific specialization findings
- **Medium confidence**: General importance patterns for specific attention heads across predictions

## Next Checks
1. Conduct ablation studies varying threshold τ systematically to quantify sensitivity across different tasks
2. Implement direct runtime comparison between attribution method and activation patching on identical hardware
3. Test method on additional model architectures and diverse tasks to assess generalizability beyond demonstrated cases