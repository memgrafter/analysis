---
ver: rpa2
title: 'MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the Hints
  from Its Router'
arxiv_id: '2410.12013'
source_url: https://arxiv.org/abs/2410.12013
tags:
- pruning
- expert
- experts
- arxiv
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoE-Pruner addresses high memory usage and redundancy in MoE models
  by pruning weights with the smallest magnitudes multiplied by input activations
  and router weights per output neuron. It is a one-shot, efficient method requiring
  no retraining.
---

# MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router

## Quick Facts
- arXiv ID: 2410.12013
- Source URL: https://arxiv.org/abs/2410.12013
- Reference count: 22
- One-line primary result: One-shot pruning method that achieves 50% sparsity while maintaining 99% of Mixtral-8x7B performance

## Executive Summary
MoE-Pruner is a one-shot pruning method designed specifically for Mixture-of-Experts (MoE) large language models that leverages router weights to identify unimportant weights for pruning. The approach multiplies weight magnitude by input activation norms and router weights per output neuron to create a pruning metric that better preserves model performance than traditional magnitude-based pruning. By combining this pruning with expert-wise knowledge distillation, MoE-Pruner achieves state-of-the-art results, maintaining 99% of original Mixtral-8x7B performance at 50% sparsity without requiring retraining.

## Method Summary
MoE-Pruner uses a novel pruning metric S = |Wij| · ∥Xj · Gatej∥ that combines weight magnitude, input activation norms, and router weights to identify unimportant weights for removal. The method performs one-shot unstructured pruning based on this metric, followed by expert-wise knowledge distillation using the pretrained model as a teacher to recover performance. The approach also extends to structured N:M sparsity patterns (e.g., 2:4) that enable hardware acceleration through tensor cores. The entire pipeline requires only a small calibration dataset and avoids the computational overhead of iterative pruning or extensive retraining.

## Key Results
- Mixtral-8x7B maintains 99% of original performance after 50% sparsity pruning with expert-wise knowledge distillation
- Outperforms state-of-the-art pruning methods (SparseGPT, Wanda) on both perplexity and zero-shot task accuracy
- Structured 2:4 sparsity enables hardware acceleration while preserving model accuracy
- No retraining required beyond the expert-wise knowledge distillation fine-tuning step

## Why This Works (Mechanism)

### Mechanism 1
Pruning unimportant weights based on a product of weight magnitude, input activation norm, and router weight yields better model retention than pure magnitude-based pruning. The pruning metric S = |Wij| · ∥Xj · Gatej∥ identifies weights that are both small in magnitude and have low router-assigned importance, ensuring that critical pathways in the MoE expert layer are preserved. Core assumption: Router weights Gatej accurately reflect the importance of expert contributions to the final output, and input activations Xj carry complementary scaling information.

### Mechanism 2
One-shot pruning without retraining can preserve MoE model performance when combined with expert-wise knowledge distillation. Pruning is performed once using a small set of calibration data; then, expert-wise knowledge distillation uses the pretrained model as a teacher to recover performance by matching expert outputs directly. Core assumption: The pruned model's experts can still benefit from distillation even after significant sparsity, because expert knowledge is preserved in the remaining weights.

### Mechanism 3
Structured N:M sparsity can be derived from the same pruning metric used for unstructured pruning, enabling hardware acceleration without retraining. Within each group of M consecutive weights per output neuron, weights are compared using the same metric S and pruned accordingly to form a dense N:M sparse pattern. Core assumption: The metric's ranking is stable enough that selecting the top N weights in each group preserves model accuracy while enabling tensor core acceleration.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) routing and gating mechanisms**
  - Why needed here: Understanding how router weights Gate(x) are computed from logits and assigned to experts is essential to interpret the pruning metric
  - Quick check question: How does the router decide which experts to activate for a given input token?

- **Concept: Second-order pruning metrics and Hessian-based importance scores**
  - Why needed here: The paper references SparseGPT and Wanda, which use Hessian approximations; knowing how diag(H^-1) or diag((X^T X)^-1) relate to weight importance clarifies why the new metric omits weight updates
  - Quick check question: Why does SparseGPT compute |W|^2 / diag(H^-1) while Wanda simplifies to |W| * ||X||?

- **Concept: Knowledge distillation, especially expert-wise distillation**
  - Why needed here: The fine-tuning step uses expert-wise MSE between teacher and student experts; understanding how to structure this loss is critical for implementation
  - Quick check question: What does "expert-wise" mean in the context of KD loss for MoE models?

## Architecture Onboarding

- **Component map:** Pretrained MoE model -> Calibration data (C4) -> Router weight collection -> Pruning metric computation (S = |Wij| · ∥Xj · Gatej∥) -> Mask generation -> Pruned model -> Expert-wise knowledge distillation -> Fine-tuned model

- **Critical path:**
  1. Load pretrained MoE model
  2. Run forward pass on calibration data to collect X and Gate
  3. Compute pruning metric S for each expert layer
  4. Generate mask M and apply to weights
  5. Save pruned model
  6. Fine-tune with expert-wise KD using pretrained model as teacher

- **Design tradeoffs:**
  - Unstructured vs. structured sparsity: unstructured allows higher sparsity but no hardware acceleration; structured N:M enables tensor core use but may require more weights to remain
  - Calibration data size vs. metric accuracy: fewer samples speed up pruning but may misestimate importance; more samples improve metric but cost time
  - KD coefficient λ vs. cross-entropy LCE: too high λ may overfit to teacher; too low may not recover performance

- **Failure signatures:**
  - High perplexity spike after pruning indicates critical weights were removed
  - Large gap between pretrained and pruned zero-shot accuracy before KD signals loss of expert knowledge
  - KD loss plateauing early suggests teacher-student expert mismatch or insufficient training samples

- **First 3 experiments:**
  1. Run pruning on Mixtral-8x7B with 50% sparsity and evaluate perplexity on WikiText without KD
  2. Apply expert-wise KD with 1000 C4 samples, learning rate 2e-5, for 3 epochs; compare zero-shot accuracy
  3. Repeat pruning with 2:4 structured sparsity and measure inference speed-up on an NVIDIA GPU with sparse tensor cores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the expert activation frequency imbalance affect the optimal pruning strategy for MoE models?
- Basis in paper: [explicit] The paper analyzes expert activation frequency and observes that different MoE expert initialization methods result in different expert activation frequencies and expert similarities, which will impact the MoE pruning strategies
- Why unresolved: While the paper identifies the relationship between activation frequency and pruning strategy, it doesn't provide a quantitative model for how this imbalance should guide pruning decisions or prove the superiority of weight pruning over expert pruning in specific scenarios
- What evidence would resolve it: A systematic study comparing different pruning strategies across MoE models with varying levels of activation frequency imbalance, measuring both performance retention and computational efficiency

### Open Question 2
- Question: Can MoE-Pruner's pruning metric be extended to handle other MoE architectures beyond SwiGLU and top-2 routing?
- Basis in paper: [inferred] The paper mentions that current LLMs mostly adopt SwiGLU architecture for FFN and MoE LLMs such as Mixtral-8x7B use top-2 to select experts, but the pruning metric formulation seems general enough to potentially handle other architectures
- Why unresolved: The paper only validates MoE-Pruner on Mixtral models using SwiGLU and top-2 routing. The effectiveness and efficiency of the pruning metric for other common MoE architectures (e.g., GEGLU, top-1 routing, shared experts) remains unexplored
- What evidence would resolve it: Applying MoE-Pruner to a diverse set of MoE models with different FFN architectures and routing policies, comparing performance against baseline pruning methods

### Open Question 3
- Question: What is the theoretical limit of sparsity that MoE-Pruner can achieve without significant performance degradation?
- Basis in paper: [explicit] The paper demonstrates that Mixtral-8x7B model with 50% sparsity maintains 99% of the performance of the original model after expert-wise knowledge distillation, but doesn't explore higher sparsity levels
- Why unresolved: While the paper shows promising results at 50% sparsity, it doesn't investigate the upper bound of sparsity that can be achieved while maintaining acceptable model performance, nor does it explore the relationship between sparsity level and the effectiveness of knowledge distillation
- What evidence would resolve it: A comprehensive study applying MoE-Pruner across a wide range of sparsity levels (e.g., 50% to 90%), measuring performance degradation and the diminishing returns of knowledge distillation at higher sparsity ratios

## Limitations

- Performance claims are based on specific Mixtral architectures and may not generalize to other MoE models with different routing mechanisms
- Structured sparsity hardware acceleration claims lack concrete timing measurements or GPU utilization data
- The calibration data size (128 sequences) may be insufficient for models trained on diverse domains or larger architectures

## Confidence

**High Confidence:** The pruning algorithm's computational mechanics and expert-wise knowledge distillation methodology are well-specified and reproducible

**Medium Confidence:** Claims that 50% sparsity maintains 99% of original performance are supported by experimental results but may be sensitive to the specific Mixtral architecture and evaluation datasets used

**Low Confidence:** Hardware acceleration claims for structured sparsity lack empirical timing data; calibration data sufficiency analysis doesn't explore generalization to other domains or model sizes

## Next Checks

1. **Router Weight Stability Analysis:** Measure router weight variance across diverse input distributions to verify that Gatej consistently identifies important expert contributions, and test whether pruning performance degrades when router weights are artificially perturbed

2. **Structured Sparsity Performance Benchmarking:** Implement the 2:4 structured sparsity pattern on GPU with tensor cores and measure actual inference speed-up compared to unstructured sparse and dense models, including memory bandwidth utilization metrics

3. **Generalization Across MoE Architectures:** Apply MoE-Pruner to MoE models with different routing mechanisms (e.g., hash-based routing, different gating networks) and evaluate whether the pruning metric maintains its effectiveness across architectures beyond the Mixtral family