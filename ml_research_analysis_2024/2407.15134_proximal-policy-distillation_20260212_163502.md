---
ver: rpa2
title: Proximal Policy Distillation
arxiv_id: '2407.15134'
source_url: https://arxiv.org/abs/2407.15134
tags:
- distillation
- teacher
- policy
- learning
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Proximal Policy Distillation (PPD) addresses the challenge of efficient
  policy transfer in reinforcement learning by combining student-driven distillation
  with PPO. The method incorporates a distillation loss into PPO's surrogate objective,
  allowing the student to benefit from both teacher guidance and environment rewards
  during training.
---

# Proximal Policy Distillation

## Quick Facts
- arXiv ID: 2407.15134
- Source URL: https://arxiv.org/abs/2407.15134
- Reference count: 40
- Proximal Policy Distillation (PPD) consistently outperforms two common distillation baselines (student-distill and teacher-distill) in sample efficiency and final performance across 19 environments.

## Executive Summary
Proximal Policy Distillation (PPD) addresses the challenge of efficient policy transfer in reinforcement learning by combining student-driven distillation with PPO. The method incorporates a distillation loss into PPO's surrogate objective, allowing the student to benefit from both teacher guidance and environment rewards during training. Evaluation across 19 environments spanning Atari, Mujoco, and Procgen shows that PPD consistently outperforms two common distillation baselines in sample efficiency and final performance. On Procgen test levels, PPD achieves 1.04x the teacher's score versus 0.96x for the best baseline.

## Method Summary
PPD enhances PPO by incorporating a distillation loss (KL divergence between teacher and student policies) into the surrogate objective, with a hyperparameter λ balancing the two terms. The student policy acts as the control policy, collecting its own trajectories and benefiting from both environment rewards and teacher guidance. The method reuses rollout buffers across epochs for improved sample efficiency, and includes clipping mechanisms for both the PPO surrogate objective and the KL-divergence term. Experiments compare PPD against student-driven and teacher-driven distillation baselines across various student network sizes (smaller, identical, larger) on 19 benchmark environments.

## Key Results
- PPD consistently outperforms student-distill and teacher-distill baselines in sample efficiency and final performance across 19 environments
- On Procgen test levels, PPD achieves 1.04x the teacher's score versus 0.96x for the best baseline
- PPD is particularly effective when distilling to larger student networks, often surpassing teacher performance
- PPD demonstrates greater robustness to imperfect teachers, recovering 71% of original performance compared to 64% for the best baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PPD achieves higher sample efficiency by combining PPO's clipped surrogate objective with a KL-divergence distillation loss, reusing rollout buffers across epochs.
- Mechanism: The student policy is updated using both environment rewards (via PPO) and teacher guidance (via KL loss). PPO's clipped surrogate prevents destructive policy updates, while the distillation term accelerates learning by biasing the student toward the teacher's behavior. Reusing data from the rollout buffer across epochs increases sample efficiency compared to one-step updates.
- Core assumption: The student policy can effectively explore and collect meaningful trajectories while being guided by the teacher, without overfitting to imperfect demonstrations.
- Evidence anchors:
  - [abstract] "PPD enhances PPO by incorporating a distillation loss to either perform traditional distillation, or to act as skills prior."
  - [section] "PPD improves sample efficiency by extending the original formulation to re-use data from a rollout buffer for several epochs..."
  - [corpus] Weak evidence - only general mentions of distillation methods, no specific support for PPO+KL hybrid.
- Break Condition: If the teacher policy is too poor, the KL loss may bias the student toward suboptimal actions, preventing effective exploration and learning from rewards.

### Mechanism 2
- Claim: Using the student policy as the control policy during distillation reduces overfitting to the teacher's limited state-visitation distribution.
- Mechanism: When the student collects its own trajectories, it explores states beyond the teacher's experience. This contrasts with teacher-driven methods where the student only sees states the teacher visited. The combination of student-driven exploration and teacher guidance allows the student to potentially outperform the teacher.
- Core assumption: The student policy, even when randomly initialized, can generate diverse enough trajectories to learn from both rewards and distillation.
- Evidence anchors:
  - [abstract] "We focus on the setting where the student itself acts as the control policy, i.e., q = πθ, since it typically yields better performance after distillation..."
  - [section] "Relying on the teacher for exploration biases the data toward its limited state-visitation distribution, leaving many states unexplored."
  - [corpus] Weak evidence - no direct comparison of student-driven vs teacher-driven exploration in the cited papers.
- Break Condition: If the student policy is too poor initially, it may fail to explore effectively, making teacher-driven distillation necessary despite overfitting risks.

### Mechanism 3
- Claim: Larger student networks generally achieve better performance after distillation, often surpassing the teacher.
- Mechanism: Larger networks have greater representational capacity, allowing them to capture both the teacher's policy and potentially improve upon it by better fitting the environment's reward structure. This supports the "reincarnating RL" concept where simpler models train faster, then transfer knowledge to more capable architectures.
- Core assumption: The distillation process effectively transfers knowledge without catastrophic forgetting, and the larger network can leverage its capacity to improve beyond the teacher.
- Evidence anchors:
  - [abstract] "We assessed the performance of the three methods with smaller, identical (self-distillation), and larger student network sizes..."
  - [section] "Furthermore, we noted that distilling into larger student networks generally results in better student performance after distillation, often surpassing that of the teacher models..."
  - [corpus] Weak evidence - no specific studies on student network size effects in the related papers.
- Break Condition: If the distillation process is too rigid or the teacher is imperfect, the larger network may simply overfit to suboptimal teacher behavior rather than improving.

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO) and its clipped surrogate objective
  - Why needed here: PPD builds directly on PPO by replacing the vanilla policy gradient term with PPO's clipped objective, adding stability to the combined PPO+distillation optimization
  - Quick check question: What is the purpose of the clipping mechanism in PPO's surrogate objective, and how does it prevent destructive policy updates?

- Concept: Knowledge Distillation in supervised learning (soft targets vs hard targets)
  - Why needed here: Understanding the difference between using teacher probabilities (soft targets) versus teacher actions (hard targets) is crucial for implementing the KL-divergence loss correctly in policy distillation
  - Quick check question: Why does using KL-divergence between teacher and student policies (soft targets) generally work better than behavior cloning with hard action targets in policy distillation?

- Concept: Importance sampling and its role in PPO
- Why needed here: PPD reuses trajectories from a rollout buffer across multiple epochs, requiring importance sampling weights to correct for the distribution shift between the behavior policy and the current policy
  - Quick check question: How do importance sampling weights ρt(θ) = πθ(at|st) / πθk(at|st) correct for the distribution mismatch when reusing old trajectories?

## Architecture Onboarding

- Component map: PPO core (Actor, Critic, PPO loss with clipping) -> Distillation module (KL-divergence loss, hyperparameter λ) -> Data pipeline (Rollout buffer, importance sampling weights) -> Control policy (Student policy)

- Critical path:
  1. Collect trajectories using current student policy
  2. Compute returns and advantage estimates
  3. For multiple epochs, shuffle and sample mini-batches
  4. Update policy and value networks using combined PPO + distillation loss
  5. Repeat until convergence

- Design tradeoffs:
  - λ hyperparameter: Higher λ prioritizes teacher guidance but may prevent learning from rewards; lower λ allows more independent learning but reduces distillation benefits
  - Network architecture: Larger students can outperform teachers but require more computation; smaller students train faster but may underfit
  - Distillation loss clipping: Prevents extreme updates but may limit responsiveness to teacher guidance

- Failure signatures:
  - Student consistently underperforms teacher: Likely λ too low or distillation loss too weak
  - Student collapses to random behavior: Likely λ too high, preventing learning from rewards
  - Slow convergence: May need to adjust PPO clipping parameter ε or increase λ
  - Overfitting to teacher: Student performs well on teacher-collected data but poorly on its own trajectories (indicative of teacher-driven distillation)

- First 3 experiments:
  1. Implement basic PPD with λ=1.0 on a simple continuous control task (e.g., Pendulum-v1) with identical student/teacher networks to verify basic functionality
  2. Test student-driven vs teacher-driven distillation on the same task to confirm the exploration benefits
  3. Vary λ across {0.5, 1.0, 2.0} to observe the trade-off between teacher guidance and independent learning

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the discussion and limitations section, several open questions emerge:

1. How does Proximal Policy Distillation perform when transferring knowledge from a single teacher to multiple student networks of different architectures simultaneously?

2. What is the optimal balance between the PPO and distillation losses in PPD across different environment types and teacher quality levels?

3. Can PPD effectively transfer knowledge from teachers trained with different RL algorithms (e.g., DQN, SAC) to PPO-based students?

4. How does PPD's performance degrade when distilling from teachers with partially observable states to fully observable students?

## Limitations
- The reported improvements depend on careful hyperparameter tuning that may not generalize across different RL domains
- The claim about student-driven distillation being universally superior to teacher-driven methods lacks direct empirical comparison
- The KL-divergence clipping mechanism and precise network architectures for larger students are not fully specified

## Confidence

**High**: The core algorithmic framework combining PPO with distillation loss is well-defined and reproducible

**Medium**: The empirical results showing improved sample efficiency and final performance are convincing but may depend on specific hyperparameter choices

**Medium**: The theoretical arguments for student-driven distillation being superior are reasonable but not conclusively proven through ablation studies

## Next Checks

1. **KL-divergence clipping sensitivity**: Systematically vary the clipping threshold for the distillation loss to determine if the reported improvements are robust to this hyperparameter, or if they critically depend on a narrow range of values.

2. **Teacher quality ablation**: Conduct experiments with intentionally imperfect teachers (reduced training steps, noisy actions) to validate the robustness claims, measuring the actual performance gap between PPD and baselines across different teacher quality levels.

3. **Network architecture transfer**: Test PPD across different network architectures beyond the IMPALA-CNN (e.g., MLPs, ResNets) to assess whether the improvements generalize to architectures not used during development, or if they are specific to the convolutional design.