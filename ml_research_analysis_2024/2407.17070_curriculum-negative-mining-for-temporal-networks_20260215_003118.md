---
ver: rpa2
title: Curriculum Negative Mining For Temporal Networks
arxiv_id: '2407.17070'
source_url: https://arxiv.org/abs/2407.17070
tags:
- negative
- networks
- temporal
- positive
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of negative sampling in temporal
  graph neural networks, where two challenges arise: positive sparsity (high ratio
  of negatives to positives) and positive shift (changing node relationships over
  time). The proposed solution, Curriculum Negative Mining (CurNM), is a model-aware
  curriculum learning framework that dynamically adjusts the difficulty of negative
  samples.'
---

# Curriculum Negative Mining For Temporal Networks

## Quick Facts
- arXiv ID: 2407.17070
- Source URL: https://arxiv.org/abs/2407.17070
- Reference count: 40
- Key outcome: CurNM achieves best results across 8 datasets in transductive settings and consistently best performance across 9 datasets in inductive settings

## Executive Summary
This paper addresses negative sampling challenges in temporal graph neural networks, specifically positive sparsity and positive shift. The proposed Curriculum Negative Mining (CurNM) framework dynamically adjusts negative sample difficulty through a curriculum learning approach. It constructs a balanced negative pool and employs temporal-aware disentanglement to select informative negatives. Extensive experiments demonstrate CurNM significantly outperforms baseline methods across 12 datasets and 3 TGNN architectures.

## Method Summary
CurNM is a model-aware curriculum learning framework that addresses negative sampling challenges in temporal graphs through three main components: (1) a dynamically updated negative pool balancing random, historical, and hard negatives to address positive sparsity; (2) a temporal-aware negative selection module that disentangles relevant and irrelevant factors and focuses on recently active edges to capture shifting preferences; and (3) an adaptive curriculum strategy that adjusts the difficulty of negative samples by modifying the sampling proportion πe based on validation performance. The method combines selected negatives with annealing random negatives for stable training.

## Key Results
- CurNM achieves best results across 8 datasets in transductive settings with an average rank of 1.72
- In inductive settings, CurNM consistently achieves best performance across 9 datasets
- The method demonstrates significant improvements over baseline methods including random sampling, recent sampling, and ENS

## Why This Works (Mechanism)

### Mechanism 1
The adaptive sampling proportion πe allows the model to start with easy negatives and gradually shift to harder ones, preventing early confusion. πe is decreased when validation performance improves, thus tightening the selection of harder negatives over epochs. Initially, a large πe includes both easy and hard negatives; as learning progresses, only the hardest negatives remain selected. Core assumption: The model can reliably detect improvements in validation performance and that early training benefits from easier negatives. Evidence anchors: [abstract] "dynamically adjusts the difficulty of negative samples" and "prevents the model from being overwhelmed by overly challenging negatives"; [section 4.1] "πe is a sampling proportion that self-adjusts based on the model's performance at e-th epoch – shrinking when performance improves and expanding otherwise". Break condition: If validation metric fluctuates, πe might shrink prematurely or not at all, leading to either too-hard or too-easy negatives throughout training.

### Mechanism 2
The temporal-aware disentanglement isolates relevant and irrelevant factors so the model can focus on differentiating positives from negatives only in the most informative dimensions. A positive gate extracts the relevant factor from the destination node, then a negative gate does the same for candidates. The difference in relevant factors is used to score negatives, with irrelevant factors serving as contrastive supervision. Core assumption: Relevant factors truly drive the interaction and can be disentangled reliably via learned gates. Evidence anchors: [section 4.3] Equations (8) and (9) define the gate mechanism, and (11) defines the contrastive loss enforcing this structure; [abstract] "temporal-aware negative selection module that focuses on learning from the disentangled factors". Break condition: If the disentanglement fails to capture the true interaction drivers, negatives may be poorly scored, leading to suboptimal training.

### Mechanism 3
The negative pool balances random, historical, and hard negatives to address positive sparsity and positive shift, with hard negatives cached only after the model stabilizes. Initially, the pool contains random and historical neighbors. Once πe falls below τ, a hard negative cache is populated with negatives that have high prediction scores and high score variance. This cache is refreshed each epoch. Core assumption: Historical neighbors and hard negatives are more informative than pure random sampling in temporal settings, and caching avoids expensive recomputation. Evidence anchors: [abstract] "establishes a dynamically updated negative pool that balances random, historical, and hard negatives"; [section 4.2] Equations (4), (6), (7) define the pool construction and caching logic. Break condition: If the hard negative cache is too small or stale, the pool may lose diversity; if too large, it may overwhelm computation.

## Foundational Learning

- Concept: Temporal graph structure and evolving node interactions
  - Why needed here: CurNM explicitly targets the positive shift problem, which arises because node relationships change over time; understanding this structure is key to designing temporal-aware selection
  - Quick check question: What is the difference between a neighbor and a historical neighbor in this context?

- Concept: Negative sampling and its role in contrastive learning
  - Why needed here: CurNM is a negative sampling strategy; grasping how negatives shape the decision boundary and how hard negatives can be both helpful and harmful is essential
  - Quick check question: Why might sampling only very hard negatives in early training epochs hurt performance?

- Concept: Curriculum learning and adaptive difficulty scheduling
  - Why needed here: The πe mechanism is a curriculum strategy; understanding why gradually increasing difficulty can outperform constant-difficulty training is critical for tuning
  - Quick check question: How does the adaptive shrinkage of πe differ from a fixed difficulty schedule?

## Architecture Onboarding

- Component map: TGNN backbone -> Negative pool manager -> Disentanglement module -> Temporal scoring function -> Curriculum controller -> Training loop
- Critical path:
  1. For each mini-batch, update πe based on validation performance
  2. Build/update negative pool for each source node
  3. Disentangle relevant/irrelevant factors for each positive-negative pair
  4. Score negatives using temporal-aware scoring
  5. Select top nb negatives and combine with random negatives
  6. Compute composite loss and update model
- Design tradeoffs:
  - Pool size M vs. computational cost: larger M increases diversity but slows training
  - Proportion γhist of historical neighbors: higher values improve temporal coverage but risk stale negatives
  - Hard negative cache size: larger caches increase difficulty but may cause overfitting to cached patterns
- Failure signatures:
  - Training loss plateaus early → πe may be shrinking too fast
  - Validation performance degrades → hard negatives too dominant or πe not shrinking when it should
  - Very slow training → negative pool construction too large or inefficient
- First 3 experiments:
  1. Baseline TGNN with random negatives vs. CurNM on a small dense dataset (e.g., WIKI) to confirm AP improvement
  2. Ablation: remove temporal-aware scoring (keep only factor scores) on a medium dataset (e.g., MOOC) to measure impact of time embeddings
  3. Ablation: remove hard negative cache (keep only random + historical) on a sparse dataset (e.g., UCI) to measure impact of hard negatives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of Curriculum Negative Mining (CurNM) vary across different temporal network sizes and densities?
- Basis in paper: [inferred] The paper mentions that the method is tested on 12 datasets with varying sizes, densities, and network features, but does not provide a detailed analysis of its performance across different network sizes and densities
- Why unresolved: The paper does not explicitly analyze the impact of network size and density on the effectiveness of CurNM, leaving this as an area for further investigation
- What evidence would resolve it: Conducting experiments on a wider range of datasets with varying sizes and densities, and analyzing the performance of CurNM across these different network characteristics

### Open Question 2
- Question: What is the impact of the initial difficulty level and shrinkage rate on the performance of CurNM?
- Basis in paper: [explicit] The paper mentions that the initial difficulty level and shrinkage rate are hyperparameters that control the adjustment steps of the sampling proportion, but does not provide a detailed analysis of their impact on performance
- Why unresolved: The paper does not explore the sensitivity of CurNM's performance to different initial difficulty levels and shrinkage rates, leaving this as an area for further investigation
- What evidence would resolve it: Conducting experiments with different initial difficulty levels and shrinkage rates, and analyzing the performance of CurNM across these different configurations

### Open Question 3
- Question: How does CurNM compare to other negative sampling methods in terms of computational efficiency?
- Basis in paper: [explicit] The paper mentions that CurNM is more efficient than ENS on large datasets, but does not provide a detailed comparison with other negative sampling methods
- Why unresolved: The paper does not provide a comprehensive comparison of CurNM's computational efficiency with other negative sampling methods, leaving this as an area for further investigation
- What evidence would resolve it: Conducting experiments to compare the computational efficiency of CurNM with other negative sampling methods, and analyzing the results across different datasets and network sizes

## Limitations

- The effectiveness of the curriculum learning strategy and temporal-aware disentanglement is primarily supported by experimental results rather than theoretical analysis
- The method's performance heavily depends on hyperparameter choices (pool size M, γhist, cache size) which may require dataset-specific tuning
- The temporal-aware disentanglement assumes that relevant and irrelevant factors can be reliably separated, but this assumption may not hold for all types of temporal interactions

## Confidence

- High confidence in experimental results showing CurNM outperforms baselines across multiple datasets and TGNN models
- Medium confidence in mechanism claims about curriculum pacing and temporal-aware selection, as these rely on empirical validation rather than theoretical guarantees
- Low confidence in generalizability of hyperparameter choices across diverse temporal graph settings without further validation

## Next Checks

1. **Theoretical Analysis**: Derive bounds or convergence guarantees for the curriculum learning strategy with adaptive πe to understand why this specific adjustment rule works better than alternatives
2. **Hyperparameter Sensitivity**: Conduct a systematic sensitivity analysis across different temporal graph types to determine if the current hyperparameter ranges are universally optimal or require dataset-specific tuning
3. **Disentanglement Robustness**: Test the temporal-aware disentanglement module on datasets where relevant and irrelevant factors may be more entangled or harder to separate to assess the robustness of the factor separation assumption