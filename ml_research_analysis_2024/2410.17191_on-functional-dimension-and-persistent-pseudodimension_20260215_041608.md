---
ver: rpa2
title: On Functional Dimension and Persistent Pseudodimension
arxiv_id: '2410.17191'
source_url: https://arxiv.org/abs/2410.17191
tags:
- rank
- parameterized
- function
- then
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the persistent pseudodimension as a new measure
  of local complexity for ReLU neural networks, aiming to understand generalization
  behavior and the double descent phenomenon. It establishes that the local functional
  dimension is a lower bound for the persistent pseudodimension.
---

# On Functional Dimension and Persistent Pseudodimension

## Quick Facts
- **arXiv ID**: 2410.17191
- **Source URL**: https://arxiv.org/abs/2410.17191
- **Reference count**: 32
- **Primary result**: Introduces persistent pseudodimension as a measure of local complexity for ReLU networks, establishing it's locally bounded by Jacobian matrix rank over polynomial rings

## Executive Summary
This paper introduces the persistent pseudodimension as a new measure of local complexity for ReLU neural networks, aiming to understand generalization behavior and the double descent phenomenon. The authors establish that the local functional dimension serves as a lower bound for the persistent pseudodimension and prove bounds relating this measure to the rank of the Jacobian matrix over the polynomial ring of formal parameters. A key structural result shows that near almost every parameter, the parameter space decomposes into a product of directions that change the function on a finite batch and directions that do not.

## Method Summary
The authors develop an algebraic framework for analyzing ReLU neural networks by representing them as piecewise polynomial functions parameterized by network weights. They construct Jacobian matrices over polynomial rings to capture how parameters affect network outputs on finite batches, then analyze the ranks of these matrices to bound the persistent pseudodimension. The approach combines algebraic geometry techniques with neural network theory, using activation patterns and formal polynomial representations to establish relationships between different complexity measures.

## Key Results
- The local functional dimension is a lower bound for the persistent pseudodimension
- The persistent pseudodimension is bounded above by the row rank of the Jacobian matrix over the polynomial ring of formal parameters
- The parameter space near almost every parameter decomposes into a product of directions affecting and not affecting function values on finite batches
- The activation matrix rank provides an upper bound on the Jacobian rank, creating a hierarchy of complexity measures

## Why This Works (Mechanism)

### Mechanism 1
The persistent pseudodimension is locally bounded by the rank of the Jacobian matrix over the polynomial ring of formal parameters. For almost all parameters θ in a ReLU network, the local functional dimension dim_fun(θ) is a lower bound for the persistent pseudodimension dim_p.VC∆(θ), which in turn is bounded above by the row rank of the Jacobian matrix JER_Z over the polynomial ring R = R[θ₁,...,θ_D]. This bound arises because the formal polynomials representing the network's piecewise-linear structure encode how parameters affect outputs on finite batches of points.

### Mechanism 2
The rank gap between R-row rank and R-row rank of the Jacobian measures the potential failure of functional dimension to agree with persistent pseudodimension. When the R-row rank of JER_Z(θ) is less than its R-row rank, this indicates that there are formal linear dependence relations among the rows that disappear when evaluating at real parameter values. This rank gap represents directions in parameter space that don't affect the function on the batch Z, creating a mismatch between functional dimension and persistent pseudodimension.

### Mechanism 3
The activation matrix rank provides an upper bound on the Jacobian rank, creating a hierarchy of complexity measures. The activation matrix α(θ₀,Z) captures which neurons are active on a batch Z for parameter θ₀. Its rank bounds the R-row rank of JER_Z from above, which in turn bounds the persistent pseudodimension. This creates a chain: dim_fun(θ) ≤ dim_p.VC∆(θ) ≤ rank(α(θ,Z)) ≤ rR(JER_Z).

## Foundational Learning

- **Piecewise polynomial structure of ReLU networks**: Understanding how ReLU networks are piecewise linear functions with polynomial pieces on each region is crucial for defining the algebraic representation and computing ranks
- **Rank over polynomial rings vs. real numbers**: The paper establishes bounds using both R-row rank and R-row rank, and understanding the difference is key to interpreting the rank gap
- **Transversality and supertransversality in ReLU networks**: These concepts ensure generic behavior of networks and are used to establish that results hold for almost all parameters

## Architecture Onboarding

- **Component map**: Parameter space Ω = R^D -> Jacobian matrix JER_Z -> Activation matrix α(θ,Z) -> Persistent pseudodimension dim_p.VC∆(θ)
- **Critical path**: Define parameterized family F:Ω×R^{n₀}→R^{n_d} → Compute algebraic representation P(θ,x) → Build Jacobian matrix JER_Z(θ) → Compute ranks rR(JER_Z) and rR(JER_Z) → Build activation matrix α(θ,Z) → Establish bounds on persistent pseudodimension
- **Design tradeoffs**: Computational complexity vs. accuracy in rank computations, genericity assumptions vs. applicability to specific networks, local vs. global complexity measures
- **Failure signatures**: Jacobian rank unexpectedly low due to special parameter configurations, activation matrix rank much smaller than expected, rank gap between R and R ranks not explained by theory
- **First 3 experiments**: Compute functional dimension and persistent pseudodimension for a simple (1,2,1) network on a small batch of points; Verify the rank gap phenomenon by constructing a network where rR(JER_Z) < rR(JER_Z); Test the activation matrix bound by comparing rank(α(θ,Z)) with rR(JER_Z) for various random parameters

## Open Questions the Paper Calls Out

### Open Question 1
Is the upper bound on persistent pseudodimension given by the row rank of the Jacobian matrix over the polynomial ring tight for generic, supertransversal parameters? The authors explicitly conjecture this, stating that for almost all parameters, the functional dimension equals the persistent pseudodimension, which would be achieved if the upper bound equals the lower bound.

### Open Question 2
What is the relationship between the rank gap (difference between R-row rank and R-row rank of the Jacobian matrix) and the generalization behavior of ReLU networks? The paper discusses the rank gap, noting it measures potential failure of functional dimension to agree with batch persistent pseudodimension, but does not explore its implications for generalization.

### Open Question 3
Does the batch fiber product structure described in Theorem 1 hold for other piecewise-polynomial parameterized families beyond ReLU networks? The theorem is stated for piecewise-polynomial parameterized families, but the proof relies heavily on properties specific to ReLU networks.

## Limitations

- The theoretical bounds assume genericity conditions (supertransversality) that may not hold in practical scenarios
- Computing ranks over polynomial rings becomes computationally intractable for larger networks
- The relationship between rank gaps and actual generalization performance requires further empirical validation

## Confidence

- **High Confidence**: The lower bound relationship between functional dimension and persistent pseudodimension
- **Medium Confidence**: The rank bounds on persistent pseudodimension and their theoretical derivation
- **Low Confidence**: The practical applicability of these bounds to real-world networks given computational complexity

## Next Checks

1. **Empirical Tightness Verification**: Test the rank bounds on randomly generated ReLU networks across various architectures and batch sizes to assess how often the conjectured tightness holds in practice
2. **Robustness to Non-Generic Parameters**: Systematically investigate how violations of supertransversality affect the bounds, identifying specific parameter configurations where the bounds fail
3. **Computational Scalability Assessment**: Benchmark the computational complexity of rank calculations for networks of increasing size, determining practical limits for applying these bounds to real-world architectures