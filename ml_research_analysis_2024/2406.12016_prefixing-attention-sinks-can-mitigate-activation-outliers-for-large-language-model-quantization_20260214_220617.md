---
ver: rpa2
title: Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language
  Model Quantization
arxiv_id: '2406.12016'
source_url: https://arxiv.org/abs/2406.12016
tags: []
core_contribution: This paper introduces CushionCache, a method to mitigate activation
  outliers in large language model quantization. Activation outliers, where a few
  activation values are significantly larger than others, hinder effective quantization.
---

# Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization
## Quick Facts
- arXiv ID: 2406.12016
- Source URL: https://arxiv.org/abs/2406.12016
- Reference count: 6
- Key outcome: Introduces CushionCache, a method to mitigate activation outliers by prefixing key-value cache tokens, significantly improving large language model quantization performance.

## Executive Summary
This paper introduces CushionCache, a method to mitigate activation outliers in large language model quantization. Activation outliers, where a few activation values are significantly larger than others, hinder effective quantization. CushionCache addresses this by finding and inserting a prefix of key-value cache tokens, called CushionCache, that reduces outlier magnitudes in subsequent tokens. The method works in two steps: (1) a greedy search for a sequence of prompt tokens that minimize the maximum activation values in subsequent tokens, and (2) fine-tuning the token cache to make activations more quantization-friendly. Experiments show that CushionCache significantly improves the performance of per-tensor activation quantization, boosting zero-shot accuracy by over 30% for LLaMA3-8B and outperforming established baselines.

## Method Summary
CushionCache is a two-step method designed to mitigate activation outliers in large language model quantization. First, it performs a greedy search to find a sequence of prompt tokens that minimize the maximum activation values in subsequent tokens. This sequence, called CushionCache, is then used as a prefix to the input. Second, the method fine-tunes the token cache to make activations more quantization-friendly. By effectively replacing attention sinks, CushionCache reduces activation magnitudes and redistributes attention, leading to improved quantization performance.

## Key Results
- CushionCache significantly improves the performance of per-tensor activation quantization, boosting zero-shot accuracy by over 30% for LLaMA3-8B.
- The method outperforms established baselines in quantization performance.
- CushionCache is effective across various models and quantization schemes, including per-tensor and per-token quantization.

## Why This Works (Mechanism)
CushionCache mitigates activation outliers by strategically inserting a prefix of key-value cache tokens, called CushionCache, which reduces the magnitudes of activation outliers in subsequent tokens. This approach effectively replaces attention sinks, redistributing attention and reducing the overall magnitude of activations. By minimizing the maximum activation values in subsequent tokens, CushionCache makes the activations more quantization-friendly, leading to improved performance in per-tensor activation quantization.

## Foundational Learning
- Activation outliers: Large activation values that significantly deviate from the majority, hindering effective quantization. Understanding this concept is crucial as it is the primary target of CushionCache.
- Key-value cache: A mechanism in transformer models that stores computed key and value vectors for efficient processing of subsequent tokens. It is central to the CushionCache method.
- Attention sinks: Tokens or positions in the input that attract a disproportionate amount of attention, leading to large activation values. CushionCache aims to replace these attention sinks.

## Architecture Onboarding
Component map: Input -> Greedy Search for CushionCache -> Fine-tuning Token Cache -> Reduced Activation Outliers
Critical path: The critical path involves the greedy search for the optimal CushionCache sequence, followed by fine-tuning the token cache to make activations more quantization-friendly.
Design tradeoffs: The method trades off computational overhead during the greedy search for improved quantization performance. It also introduces an additional step of fine-tuning the token cache.
Failure signatures: The method may fail if the greedy search does not find an effective CushionCache sequence or if the fine-tuning step does not sufficiently reduce activation outliers.
First experiments:
1. Evaluate the computational overhead of the greedy search across different model sizes and sequence lengths.
2. Test the method's effectiveness on out-of-distribution tasks and datasets not included in the original evaluation.
3. Conduct ablation studies to isolate the contributions of the greedy search and fine-tuning steps to the overall performance gain.

## Open Questions the Paper Calls Out
None

## Limitations
- The computational cost and scalability of the greedy search for larger models or longer sequences is not discussed.
- The effectiveness across diverse tasks and datasets beyond the evaluated benchmarks is uncertain.
- The long-term stability of the fine-tuned token cache and its impact on model generalization are unclear.

## Confidence
- Major claims: Medium
  - Strong experimental results but limited ablation studies and theoretical analysis.
  - Uncertainties regarding computational overhead, scalability, and generalization.

## Next Checks
1. Evaluate the computational overhead of the greedy search across different model sizes and sequence lengths.
2. Test the method's effectiveness on out-of-distribution tasks and datasets not included in the original evaluation.
3. Conduct ablation studies to isolate the contributions of the greedy search and fine-tuning steps to the overall performance gain.