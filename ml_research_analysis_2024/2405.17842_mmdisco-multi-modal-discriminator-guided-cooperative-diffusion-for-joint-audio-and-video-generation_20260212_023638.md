---
ver: rpa2
title: 'MMDisCo: Multi-Modal Discriminator-Guided Cooperative Diffusion for Joint
  Audio and Video Generation'
arxiv_id: '2405.17842'
source_url: https://arxiv.org/abs/2405.17842
tags:
- base
- discriminator
- diffusion
- video
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a method to integrate pre-trained single-modal
  audio and video diffusion models into a joint audio-video generation model by training
  a lightweight discriminator that distinguishes real paired data from fake pairs
  generated by the base models. The discriminator's gradient provides guidance to
  adjust the base models' outputs toward the joint distribution, while a denoising
  regularization loss stabilizes the gradient.
---

# MMDisCo: Multi-Modal Discriminator-Guided Cooperative Diffusion for Joint Audio and Video Generation

## Quick Facts
- arXiv ID: 2405.17842
- Source URL: https://arxiv.org/abs/2405.17842
- Authors: Akio Hayakawa; Masato Ishii; Takashi Shibuya; Yuki Mitsufuji
- Reference count: 40
- Primary result: Achieves FVD of 405 and FAD of 5.52 on Landscape dataset with 15-22% faster inference

## Executive Summary
This paper introduces MMDisCo, a method for generating aligned audio-video content by integrating pre-trained single-modal diffusion models through a lightweight discriminator. The approach trains a discriminator to distinguish real audio-video pairs from fake pairs generated by independent base models, then uses the discriminator's gradient to guide the base models toward generating content that matches the joint distribution. A denoising regularization loss stabilizes the discriminator's gradient, while the model-agnostic design allows compatibility with arbitrary pre-trained diffusion architectures.

## Method Summary
MMDisCo integrates pre-trained single-modal audio and video diffusion models into a joint generation framework by training a lightweight discriminator. The discriminator learns to distinguish real paired audio-video data from fake pairs generated by the base models. During sampling, the discriminator's gradient provides guidance to adjust the base models' outputs toward the joint distribution. A denoising regularization loss stabilizes the gradient and helps the discriminator work as a noise estimator. The method requires relatively few additional parameters and achieves faster training and inference compared to baselines.

## Key Results
- Achieves FVD of 405 and FAD of 5.52 on the Landscape dataset
- Reduces inference time by 15-22% compared to baselines
- Improves both single-modal fidelity and multimodal alignment metrics

## Why This Works (Mechanism)

### Mechanism 1
The joint guidance module approximates the joint score function by using the gradient of a discriminator trained to distinguish real audio-video pairs from fake pairs. The optimal discriminator provides a density ratio estimate that, when differentiated and scaled, yields conditional scores that approximate the joint distribution gradients.

### Mechanism 2
The denoising regularization loss stabilizes the discriminator's gradient by encouraging it to match the residual error between ground truth noise and base model predictions. This makes the gradient more stable and accurate as a noise estimator during sampling.

### Mechanism 3
The model-agnostic design allows integration of any pre-trained single-modal diffusion models by treating them as black boxes that only output predicted noise. The joint guidance module only needs to compute gradients with respect to its own inputs, not the base models' parameters.

## Foundational Learning

- **Score matching and score-based generative models**: Why needed - the method relies on estimating and manipulating score functions for both marginal and joint distributions. Quick check - can you explain why a diffusion model trained with denoising score matching implicitly learns to estimate the score function?

- **Classifier guidance and conditional generation in diffusion models**: Why needed - the joint guidance module extends classifier guidance from single-modal conditional generation to multimodal joint generation. Quick check - how does classifier guidance modify the sampling process in diffusion models, and what role does the classifier's gradient play?

- **Generative Adversarial Networks (GANs) and optimal discriminator theory**: Why needed - the joint guidance module uses a discriminator trained similarly to GANs, and understanding optimal discriminator theory is crucial for deriving the gradient-based guidance. Quick check - what is the relationship between the optimal discriminator in GANs and the density ratio between real and generated data distributions?

## Architecture Onboarding

- **Component map**: Pre-trained audio diffusion model -> Pre-trained video diffusion model -> Lightweight discriminator (audio encoder + video encoder + fusion module + output layer) -> Gradient computation -> Joint sampling

- **Critical path**: Discriminator training → Gradient computation → Joint sampling. First train the discriminator on real/fake pairs, then during sampling at each timestep: get base model predictions, compute discriminator gradient, combine for final noise estimate

- **Design tradeoffs**: Discriminator complexity vs. training stability (more complex discriminators may capture joint distribution better but risk instability); Regularization strength λ (higher values stabilize gradients but may weaken joint distribution learning); Gradient scaling (may need to scale discriminator gradient when base model and discriminator operate on different scales)

- **Failure signatures**: Discriminator collapse (generated samples show no diversity or fail to match training distribution); Gradient instability (training loss oscillates or discriminator fails to converge); Mode dropping (generated samples cluster around only a few modes of the true distribution)

- **First 3 experiments**: 1) Toy dataset validation: Implement on simple 2D Gaussian mixture to verify gradient-based guidance works as expected; 2) Ablation study: Train discriminator with only Ldisc vs. only Ldenoise vs. both to verify regularization helps; 3) Out-of-domain test: Use mismatched base models and datasets to verify model-agnostic capability

## Open Questions the Paper Calls Out

### Open Question 1
Does the MMDisCo approach generalize to other multimodal combinations beyond audio-video generation, such as text-image or text-audio generation? The paper demonstrates success in audio-video joint generation but does not test other modality pairs, leaving uncertainty about applicability to different data types.

### Open Question 2
How does the performance of MMDisCo scale with larger base models or more complex multimodal datasets, and what are the computational trade-offs? The experiments use specific base models and datasets of limited scale, without exploring scaling to larger models or more diverse, complex data.

### Open Question 3
What is the impact of dataset quality and alignment on the effectiveness of MMDisCo, and how can dataset design improve multimodal alignment? While the paper adjusts captions for better results, it does not systematically study how dataset quality, alignment strength, or specific design choices impact MMDisCo's performance.

## Limitations

- The theoretical grounding for the discriminator-based joint score approximation relies on the assumption that base models' marginals perfectly match real data, which is rarely true in practice
- The denoising regularization's effectiveness for multimodal discriminator gradients lacks direct empirical validation beyond inspiration from DLSM
- The model-agnostic design claim hasn't been stress-tested with diverse architectures beyond the specific models used in experiments

## Confidence

- Joint score approximation mechanism: Medium
- Denoising regularization effectiveness: Low
- Model-agnostic capability: Medium

## Next Checks

1. Test gradient approximation accuracy on a controlled toy dataset where true joint scores are known
2. Perform ablation studies varying the denoising regularization strength λ to identify optimal balance
3. Validate performance with base models from different families (e.g., VAEs, GANs) to test true model-agnostic capability