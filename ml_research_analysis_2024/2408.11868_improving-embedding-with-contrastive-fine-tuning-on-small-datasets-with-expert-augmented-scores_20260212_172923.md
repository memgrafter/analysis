---
ver: rpa2
title: Improving embedding with contrastive fine-tuning on small datasets with expert-augmented
  scores
arxiv_id: '2408.11868'
source_url: https://arxiv.org/abs/2408.11868
tags:
- uni0000004c
- uni00000013
- uni00000057
- uni00000011
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving text embedding
  models for retrieval tasks, particularly when labeled data is scarce. The core method
  introduces a contrastive fine-tuning approach that leverages soft labels derived
  from expert-augmented scores, allowing models to adapt to specific tasks while preserving
  versatility.
---

# Improving embedding with contrastive fine-tuning on small datasets with expert-augmented scores

## Quick Facts
- arXiv ID: 2408.11868
- Source URL: https://arxiv.org/abs/2408.11868
- Reference count: 31
- Outperforms benchmark model on MTEB retrieval tasks using soft labels from expert-augmented scores

## Executive Summary
This paper addresses the challenge of improving text embedding models for retrieval tasks when labeled data is scarce. The authors propose a contrastive fine-tuning approach that leverages soft labels derived from expert-augmented scores, allowing models to adapt to specific tasks while preserving versatility. Experiments on a Q&A dataset and evaluation using the Massive Text Embedding Benchmark (MTEB) demonstrate that the proposed method outperforms a benchmark model across multiple retrieval tasks. The approach is resource-efficient and effective in balancing task-specific performance with general utility.

## Method Summary
The method uses contrastive fine-tuning with soft labels derived from expert-augmented scores. A small Q&A dataset is augmented with questions from multiple small LLMs, then expert embedding models compute similarity scores for all pairs. These scores are used as soft labels (Soft-1, Soft-2, or Soft-3 variants) during contrastive fine-tuning. The BCE-embedding-base_v1 model is fine-tuned using this approach and evaluated on MTEB retrieval tasks and held-out sets.

## Key Results
- Soft-1 achieves higher average nDCG@10 (40.633) and mAP@10 (35.323) compared to benchmark (39.675 and 34.419)
- Soft-2 also outperforms benchmark with nDCG@10 (40.334) and mAP@10 (35.04)
- Both Soft-1 and Soft-2 show smaller standard deviations in performance metrics
- Method is resource-efficient and effective when labeled data is scarce

## Why This Works (Mechanism)

### Mechanism 1
Soft labels derived from expert-augmented scores improve model adaptation while preserving generalization. Instead of using binary hard labels, the method computes similarity scores from multiple expert embedding models and uses these as soft labels during contrastive fine-tuning. This allows the model to learn nuanced distinctions rather than strict binary boundaries. Core assumption: Expert models can differentiate relevant and irrelevant pairs to some extent, and their averaged scores provide a more stable learning signal than binary labels.

### Mechanism 2
Contrastive fine-tuning with soft labels reduces anisotropy in the embedding space, improving retrieval performance. The fine-tuning process encourages embeddings to occupy a wider cone in vector space rather than being concentrated in narrow directions. This increased diversity makes it easier to distinguish relevant from irrelevant pairs. Core assumption: Models fine-tuned with soft labels will exhibit reduced anisotropy compared to hard label fine-tuning.

### Mechanism 3
Using a small, expert-augmented dataset for fine-tuning is resource-efficient and effective. Rather than requiring large labeled datasets, the method generates training pairs from a small Q&A dataset augmented with questions from multiple small LLMs, then applies contrastive fine-tuning with soft labels from existing expert models. Core assumption: A small dataset with high-quality, diverse questions and expert score augmentation can provide sufficient signal for effective fine-tuning.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: The method relies on distinguishing relevant from irrelevant pairs using contrastive objectives, which is fundamental to the approach.
  - Quick check question: What is the primary objective function used in contrastive fine-tuning?

- Concept: Cosine similarity and vector space geometry
  - Why needed here: The method uses cosine similarity to compute expert scores and to evaluate embeddings, and discusses anisotropy in the embedding space.
  - Quick check question: How does anisotropy affect the ability to distinguish between relevant and irrelevant pairs in vector space?

- Concept: Evaluation metrics for retrieval (nDCG@10, mAP@10)
  - Why needed here: The method evaluates performance using standard retrieval metrics, which are critical for understanding its effectiveness.
  - Quick check question: What is the difference between nDCG@10 and mAP@10 in evaluating retrieval performance?

## Architecture Onboarding

- Component map:
  Expert embedding models (8 total) → Similarity computation → Soft label generation → Fine-tuning dataset → Base embedding model → Contrastive fine-tuning → Evaluation on MTEB and held-out sets
  Small LLM generators → Question augmentation → Q&A dataset → Hard label pairs → Expert score augmentation

- Critical path:
  1. Generate augmented question pairs using small LLMs
  2. Compute expert similarity scores for all pairs
  3. Generate soft labels (Soft-1, Soft-2, or Soft-3 variants)
  4. Fine-tune base embedding model with contrastive loss
  5. Evaluate on MTEB retrieval tasks and held-out set

- Design tradeoffs:
  - Soft vs hard labels: Soft labels preserve generalization but may be less discriminative; hard labels are more precise but can degrade versatility
  - Number of expert models: More experts provide better score aggregation but increase computation; fewer experts reduce noise but may miss nuances
  - Dataset size: Small datasets are efficient but may lack diversity; large datasets improve robustness but increase cost

- Failure signatures:
  - Performance drops on held-out sets while improving on MTEB → overfitting to MTEB distribution
  - High variance across MTEB tasks → poor generalization
  - Similarity distributions show minimal separation between relevant and irrelevant pairs → insufficient discriminative power

- First 3 experiments:
  1. Implement Soft-1 fine-tuning on the provided Q&A dataset and evaluate on MTEB retrieval tasks
  2. Compare Soft-1 with Soft-2 and Soft-3 variants on the same dataset to identify optimal soft label strategy
  3. Analyze the impact of varying the number of expert models (e.g., 4 vs 8) on fine-tuning effectiveness and resource requirements

## Open Questions the Paper Calls Out

### Open Question 1
Does fine-tuning with soft labels reduce the anisotropy problem in high-dimensional embedding spaces, and if so, to what extent? The paper mentions recent research showing transformer-based models are anisotropic and suggests it would be interesting to investigate whether soft label fine-tuning reduces anisotropy in high-dimensional spaces. This remains unresolved as the paper does not conduct experiments or analysis to measure anisotropy before and after soft label fine-tuning.

### Open Question 2
How do different expert model combinations affect the quality and stability of soft labels in contrastive fine-tuning? The paper mentions that "expert models" are used to generate soft labels, but does not explore how different combinations or selections of expert models impact the fine-tuning results. This remains unresolved as the paper uses a fixed set of eight expert models without investigating the impact of varying this set.

### Open Question 3
What is the optimal balance between task-specific fine-tuning and preserving general utility in embedding models when using soft labels? The paper states that the method aims to "strike a balance between task-specific performance and the general utility of the embedding models," but does not empirically investigate what this optimal balance looks like. This remains unresolved as the paper does not quantify the trade-off between task-specific gains and general-purpose performance degradation.

## Limitations

- Expert-augmented soft labels may be unreliable if expert models disagree significantly on relevance judgments
- Resource efficiency claims are not systematically validated through analysis of dataset size effects
- The method's effectiveness depends on the quality and consistency of expert models, which may not be readily available in all domains

## Confidence

- **High confidence**: Experimental results showing Soft-1 and Soft-2 outperforming the benchmark on MTEB tasks with clear statistical comparisons
- **Medium confidence**: Proposed mechanisms (reduced anisotropy, preservation of generalization through soft labels) are theoretically sound but not directly measured
- **Low confidence**: Resource efficiency claims as the paper does not provide systematic analysis of how dataset size affects performance

## Next Checks

1. **Expert Agreement Analysis**: Quantify the consistency among the eight expert models by computing inter-rater reliability metrics on a subset of question pairs to validate the soft label quality.

2. **Dataset Size Sensitivity**: Systematically evaluate model performance as a function of training dataset size to empirically validate resource efficiency claims and identify minimum effective dataset size.

3. **Ablation on Expert Models**: Compare performance when using different numbers of expert models and different aggregation strategies to determine optimal configuration and understand individual expert contributions.