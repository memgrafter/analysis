---
ver: rpa2
title: 'KG4RecEval: Does Knowledge Graph Really Matter for Recommender Systems?'
arxiv_id: '2404.03164'
source_url: https://arxiv.org/abs/2404.03164
tags:
- decreased
- ratio
- knowledge
- distortion
- degree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates the role of knowledge graphs
  (KGs) in recommender systems (RSs). It introduces KG4RecEval, a framework that quantifies
  KG contribution via KGER (KG utilization efficiency), measuring recommendation accuracy
  changes when KGs are removed, distorted, or decreased.
---

# KG4RecEval: Does Knowledge Graph Really Matter for Recommender Systems?

## Quick Facts
- arXiv ID: 2404.03164
- Source URL: https://arxiv.org/abs/2404.03164
- Reference count: 40
- One-line primary result: Reducing KG authenticity or size does not necessarily degrade recommendation accuracy, even for cold-start users.

## Executive Summary
This paper introduces KG4RecEval, a framework that systematically evaluates how knowledge graphs (KGs) contribute to recommender systems (RSs) through the KGER metric. The framework measures recommendation accuracy changes when KGs are removed, distorted, or decreased. Experiments across four real-world datasets and ten state-of-the-art KG-based RSs reveal that the relationship between KG quality/quantity and recommendation performance is not monotonic. Notably, reducing KG authenticity or size does not necessarily decrease recommendation accuracy, even for cold-start users, challenging the common assumption that KGs always improve recommendations.

## Method Summary
KG4RecEval systematically evaluates KG contribution by manipulating knowledge graphs through removal, random distortion, or random decrease of facts/entities/relations. The framework calculates KGER (KG utilization efficiency in recommendation) by comparing recommendation accuracy between original and modified KGs. Experiments use grid search hyperparameter tuning via HyperOPT, 5-fold cross-validation, and measure performance across multiple RS models including RippleNet, KGRec, CKE, CFKG, KGCN, KGNNLS, KGAT, and DiffKG on ML-1M, Amazon-Books, BX, and Last.FM datasets.

## Key Results
- Reducing KG authenticity or size does not necessarily decrease recommendation accuracy, even for cold-start users
- Knowledge authenticity matters slightly more than quantity for normal users, while the opposite holds for cold-start users
- Certain models (RippleNet, KGRec) consistently show positive KGER across datasets due to efficient attention mechanisms
- CFKG shows lower KGER when KG is small relative to user-item interaction graph

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge graph contributions to recommendation accuracy are not monotonic with respect to graph alterations (removal, distortion, or reduction).
- Mechanism: KG4RecEval evaluates KGER, a metric that captures the efficiency of KG utilization by measuring performance changes relative to the amount of altered knowledge. By systematically varying KG authenticity and size, the framework reveals that recommendation models often maintain or even improve performance when KGs are degraded, suggesting that the models do not necessarily exploit KG knowledge efficiently or that the models can compensate with other signals.
- Core assumption: The relationship between KG quality/quantity and recommendation performance is not strictly positive; models can leverage user-item interactions or other features to offset KG degradation.
- Evidence anchors:
  - [abstract] "to remove, randomly distort or decrease knowledge does not necessarily decrease recommendation accuracy, even for cold-start users."
  - [section] "MRR of most the models including CKE, CFKG, KGCN, KGNNLS, KGAT, and Diï¬€KG, does not show consistent monotonicity with the decrease of fact/entity/relation."
  - [corpus] Weak. Related papers focus on different aspects of RSs but do not directly address the non-monotonic relationship between KG quality and recommendation performance.
- Break condition: If KGER values consistently decrease with KG degradation across all models and datasets, or if the models fail to compensate for missing KG knowledge, the mechanism breaks.

### Mechanism 2
- Claim: The effectiveness of KGs in RSs depends on the interplay between dataset characteristics (e.g., sparsity) and model architecture (e.g., attention mechanisms).
- Mechanism: KG4RecEval demonstrates that KGER values vary significantly across datasets and models. For example, RippleNet and KGRec consistently show positive KGER, likely due to their attention mechanisms that efficiently filter and weight relevant KG facts. Conversely, CFKG, which combines KGs and user-item interactions into a single graph, shows lower KGER when the KG is small relative to the interaction graph.
- Core assumption: Model architecture determines how effectively a KG is utilized, and dataset characteristics influence the relative importance of KG vs. interaction data.
- Evidence anchors:
  - [section] "Among all the tested KG-based RSs, RippleNet and KGRec usually present a positive KGER across datasets and also relatively higher KGER for cold-start users."
  - [section] "CFKG combines KGs and user-item interactions to create a new graph and generate recommendation results based on it. This may result in low KGER if the KG is much smaller than the user-item interaction graph."
  - [corpus] Weak. Related papers do not explicitly discuss the interplay between dataset characteristics and model architecture in KG utilization.
- Break condition: If all models show similar KGER values across all datasets, or if dataset sparsity does not correlate with KG utilization efficiency, the mechanism breaks.

### Mechanism 3
- Claim: Knowledge authenticity matters more than quantity for normal users, while knowledge quantity matters more for cold-start users.
- Mechanism: KG4RecEval compares KGER values under random distortion (reduced authenticity) and random decrease (reduced quantity) of knowledge. For normal users, the impact of distortion is greater than the impact of decrease, suggesting that authenticity is more important. For cold-start users, the opposite is true, indicating that quantity is more important.
- Core assumption: Normal users have sufficient interaction data to compensate for false knowledge, while cold-start users rely more heavily on KG knowledge, making quantity more important.
- Evidence anchors:
  - [section] "For normal users, knowledge authenticity has a slightly stronger influence on KG utilization efficiency of KG-based RSs. For cold-start users, knowledge amount has a slightly stronger influence."
  - [section] "With more knowledge randomly distorted, the recommendation accuracy of a KG-based RS for cold-start users does not necessarily decrease."
  - [corpus] Weak. Related papers do not directly address the relative importance of knowledge authenticity vs. quantity for different user types.
- Break condition: If KGER values show the opposite trend (authenticity more important for cold-start users, quantity more important for normal users), or if the impact of distortion and decrease is similar for both user types, the mechanism breaks.

## Foundational Learning

- Concept: Knowledge Graphs (KGs) and their role in recommender systems (RSs)
  - Why needed here: Understanding the basic structure and purpose of KGs is essential for grasping how KG4RecEval manipulates and evaluates their impact on RS performance.
  - Quick check question: What is the difference between a KG and a user-item interaction graph, and how do KG-based RSs utilize KGs?

- Concept: Evaluation metrics for RSs (e.g., MRR, Hit@10, NDCG)
  - Why needed here: KG4RecEval uses various metrics to assess recommendation accuracy under different KG conditions. Understanding these metrics is crucial for interpreting the results and comparing model performance.
  - Quick check question: How do MRR, Hit@10, and NDCG differ in their assessment of recommendation quality, and when would you use each metric?

- Concept: Hyperparameter tuning and its impact on model performance
  - Why needed here: KG4RecEval involves hyperparameter tuning for different KG variants. Understanding how hyperparameters affect model performance is essential for interpreting the results and ensuring fair comparisons.
  - Quick check question: What is the purpose of hyperparameter tuning, and how can it affect the performance of KG-based RSs under different KG conditions?

## Architecture Onboarding

- Component map: KG4RecEval framework -> KG manipulation methods -> RS models -> Datasets -> Evaluation metrics
- Critical path: 1. Load dataset and construct original KG 2. Apply KG manipulation (remove, distort, or decrease knowledge) 3. Train and evaluate RS models on manipulated KG 4. Calculate KGER and other evaluation metrics 5. Repeat for different KG manipulations and user types (normal vs. cold-start)
- Design tradeoffs: Computational cost vs. experimental coverage: Running experiments on multiple datasets, models, and KG manipulations is computationally expensive but provides a comprehensive understanding of KG utilization
- Failure signatures: Inconsistent or unexpected KGER values across models or datasets; RS models failing to train or evaluate on manipulated KGs; Errors in KG manipulation methods or evaluation metric calculations
- First 3 experiments: 1. Replicate the "no knowledge" experiment (Section 4.2) on a single dataset and model to verify the basic functionality of KG4RecEval 2. Replicate the "false knowledge" experiment (Section 4.3) on a single dataset and model to verify the KG manipulation methods and KGER calculation 3. Replicate the "decreasing knowledge" experiment (Section 4.4) on a single dataset and model to verify the impact of knowledge quantity on recommendation performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does removing specific types of relations from a KG improve RS performance, and what characteristics make certain relations more detrimental than others?
- Basis in paper: [explicit] Additional experiments in Appendix C systematically remove top-5 most frequent relations and find 47.5% of cases show MRR increase after removal
- Why unresolved: The paper doesn't identify which relation types are consistently harmful or beneficial, nor does it explore why certain relations negatively impact performance
- What evidence would resolve it: Detailed analysis of relation types that consistently degrade vs. improve performance across multiple RS models and datasets, plus explanation of underlying mechanisms

### Open Question 2
- Question: Does re-tuning hyperparameters after modifying KGs substantially change the observed effects on recommendation accuracy, or are the effects robust to hyperparameter optimization?
- Basis in paper: [explicit] Appendix B conducts hyperparameter re-tuning experiments and finds minimal impact (1.5% average variation)
- Why unresolved: The paper only tests a subset of models and datasets; it's unclear if the robustness generalizes to all KG-based RSs and scenarios
- What evidence would resolve it: Comprehensive hyperparameter re-tuning experiments across all models, datasets, and KG modification scenarios to determine if the minimal impact holds universally

### Open Question 3
- Question: What mechanisms explain why certain KG-based RS models (like RippleNet and KGRec) consistently achieve positive KGER values across diverse datasets and conditions?
- Basis in paper: [explicit] Discussion section notes these models use attention mechanisms to filter useless/incorrect facts and assign higher weights to relevant entities
- Why unresolved: The paper doesn't investigate whether these mechanisms can be generalized or applied to other RS models, nor does it identify specific attention patterns that predict success
- What evidence would resolve it: Comparative analysis of attention patterns across models, identification of features that distinguish effective vs. ineffective attention mechanisms, and experiments testing transfer of successful attention strategies to other models

## Limitations

- Results are limited to four specific datasets and ten RS models tested
- KG manipulations use random distortion and decrease methods that may not capture all real-world KG quality issues
- KGER metric, while innovative, may not fully capture the nuanced ways different models utilize KG knowledge

## Confidence

- **High confidence**: Basic experimental methodology and metric definitions (KGER calculation)
- **Medium confidence**: Non-monotonic relationship between KG quality and performance; authenticity vs. quantity trade-offs for user types
- **Medium confidence**: Dataset-architecture interplay effects; model-specific KG utilization efficiency

## Next Checks

1. **Replicate with alternative KG degradation methods**: Test the robustness of findings by applying systematic (rather than random) KG distortions and targeted entity/relation removals to assess whether results hold under different degradation patterns.

2. **Cross-dataset generalization study**: Validate the authenticity vs. quantity trade-off findings on additional datasets with different sparsity levels and KG characteristics to determine if the observed patterns are generalizable.

3. **Ablation study on attention mechanisms**: Perform controlled experiments comparing attention-based models (RippleNet, KGRec) with non-attention variants to isolate the specific contribution of attention to KG utilization efficiency.