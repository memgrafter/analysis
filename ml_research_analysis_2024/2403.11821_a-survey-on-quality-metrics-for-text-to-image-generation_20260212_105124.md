---
ver: rpa2
title: A Survey on Quality Metrics for Text-to-Image Generation
arxiv_id: '2403.11821'
source_url: https://arxiv.org/abs/2403.11821
tags:
- image
- quality
- metrics
- images
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive overview of quality metrics
  for text-to-image generation, categorizing them based on whether they evaluate general
  image quality (image-only metrics) or compositional quality (text-image metrics).
  The paper introduces a taxonomy grounded in two main criteria: compositional quality,
  which measures alignment between image content and text prompts, and general quality,
  which assesses overall image quality independently of the text.'
---

# A Survey on Quality Metrics for Text-to-Image Generation

## Quick Facts
- arXiv ID: 2403.11821
- Source URL: https://arxiv.org/abs/2403.11821
- Reference count: 40
- One-line primary result: Comprehensive survey categorizing text-to-image quality metrics by compositional vs. general quality, with guidelines for selection and experimental validation using human preference metrics

## Executive Summary
This survey provides a comprehensive taxonomy of quality metrics for text-to-image generation, organizing them based on whether they evaluate general image quality (image-only metrics) or compositional quality (text-image metrics). The authors propose a dual-criteria framework distinguishing compositional quality (alignment between image content and text prompts) and general quality (overall image quality independently of text). The survey reviews embedding-based metrics using pre-trained models like CLIP and BLIP, content-based metrics analyzing semantic alignment through visual question answering, and image-only metrics assessing distribution and single-image quality. Experimental results highlight the importance of prompt length and normalization in achieving meaningful evaluations.

## Method Summary
The survey analyzes quality metrics through a structured taxonomy based on two main quality criteria: compositional quality and general quality. For reproduction, researchers need to collect text-image pairs from text-to-image models or existing datasets like DrawBench, implement various quality metrics (CLIPScore, BLIP, BLIP2, ImageReward, etc.), compute scores for the dataset, and analyze results by comparing score distributions, examining relationships between prompt characteristics and scores, and investigating metric correlations. Key challenges include variations in pre-trained model versions and preprocessing steps that may affect results, as well as the need to normalize scores across different scales for meaningful comparison.

## Key Results
- Taxonomy effectively categorizes metrics by compositional vs. general quality, operating data structure, scope, and conditions
- Embedding-based metrics using CLIP and similar models measure text-image alignment through cosine similarity between embeddings
- Content-based metrics provide more direct and explainable evaluation by analyzing semantic content through visual question answering
- Prompt length and normalization significantly impact metric scores and evaluation outcomes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The survey's taxonomy effectively categorizes text-to-image quality metrics by distinguishing between image-only and text-image metrics, enabling structured evaluation.
- **Mechanism:** The taxonomy is grounded in two main quality criteria: compositional quality (alignment between image content and text prompts) and general quality (overall image quality). This dual-criteria approach allows for a comprehensive classification of metrics based on their operating data structure (embeddings vs. content), scope (distribution vs. single image), and conditions (image-only vs. text-image).
- **Core assumption:** Compositional quality and general quality are the two primary contributors to overall image quality in text-to-image generation.
- **Evidence anchors:**
  - [abstract] The survey proposes a taxonomy grounded in the assumption that there are two main quality criteria, namely compositional quality and general quality, that contribute to the overall image quality.
  - [section] The taxonomy categorizes metrics based on their operating data structure, scope, and conditions, with a focus on compositional and general quality.
  - [corpus] Weak - the corpus evidence does not directly address the taxonomy's effectiveness, but it provides related papers on evaluation metrics.
- **Break condition:** If the taxonomy fails to capture the nuances of metric evaluation or if new metrics emerge that do not fit within the existing categories.

### Mechanism 2
- **Claim:** Embedding-based metrics leverage pre-trained models like CLIP to assess text-image alignment by measuring the cosine similarity between text and image embeddings.
- **Mechanism:** Text prompts and images are transformed into embedding vectors using text and image encoders. The cosine distance between these embeddings is computed to measure alignment, with fine-tuning on human-annotated data further improving the metric's ability to reflect human preferences.
- **Core assumption:** Pre-trained models like CLIP encode valuable information that can be used to measure the alignment between text and images.
- **Evidence anchors:**
  - [abstract] Embedding-based metrics use learned embedding representations for vision and language inputs, measuring alignment through cosine similarity.
  - [section] CLIPScore is introduced as a reference-free evaluation metric for image caption generation tasks, using the cosine similarity between text and image embeddings.
  - [corpus] Weak - the corpus evidence does not directly address the mechanism of embedding-based metrics, but it provides related papers on evaluation metrics.
- **Break condition:** If the pre-trained models fail to capture the necessary semantic information or if the fine-tuning process does not effectively incorporate human preferences.

### Mechanism 3
- **Claim:** Content-based metrics analyze the semantic content of both the generated image and the text prompt, providing a more direct and explainable evaluation of text-image alignment.
- **Mechanism:** Content-based metrics dissect the prompt and the image into multiple text-image pairs and evaluate the matching of these pairs. This involves decomposing the prompt into distinct semantic elements and comparing them to the corresponding regions in the image using visual question answering models or image captioning models.
- **Core assumption:** Analyzing the actual content of the text and image provides a more accurate and interpretable measure of alignment than relying solely on embeddings.
- **Evidence anchors:**
  - [abstract] Content-based metrics examine the content of both the generated image and the text prompt, evaluating alignment through visual question answering and segmentation.
  - [section] The Decompositional-Alignment Score (DA-Score) evaluates text-to-image alignment by breaking down image prompts and assessing individual assertions using a VQA model.
  - [corpus] Weak - the corpus evidence does not directly address the mechanism of content-based metrics, but it provides related papers on evaluation metrics.
- **Break condition:** If the content-based metrics fail to capture the nuances of complex prompts or if the visual question answering models are not sufficiently accurate.

## Foundational Learning

- **Concept:** Taxonomy and classification
  - **Why needed here:** The survey's taxonomy provides a structured framework for understanding and comparing different text-to-image quality metrics. This classification is essential for researchers and practitioners to make informed decisions about which metrics to use for their specific applications.
  - **Quick check question:** Can you explain the difference between compositional quality and general quality in the context of text-to-image generation?

- **Concept:** Embedding representations and cosine similarity
  - **Why needed here:** Embedding-based metrics rely on pre-trained models to generate embeddings for text and images, and then measure their alignment using cosine similarity. Understanding these concepts is crucial for evaluating the effectiveness of embedding-based metrics.
  - **Quick check question:** How does cosine similarity measure the alignment between text and image embeddings?

- **Concept:** Visual question answering and image captioning
  - **Why needed here:** Content-based metrics use visual question answering and image captioning models to analyze the semantic content of text and images. Familiarity with these concepts is essential for understanding how content-based metrics evaluate text-image alignment.
  - **Quick check question:** How do visual question answering models help in evaluating the alignment between text and image content?

## Architecture Onboarding

- **Component map:** Taxonomy (compositional vs. general quality) -> Metric categories (embedding-based, content-based, image-only) -> Evaluation methods (VLMs, VQA, distribution metrics)
- **Critical path:** Select appropriate metrics based on evaluation criteria → Collect relevant datasets → Implement metric computations → Normalize scores → Analyze results and correlations
- **Design tradeoffs:** Embedding-based metrics offer efficiency and scalability but may lack interpretability; content-based metrics provide accuracy and explainability but require more computational resources; distribution metrics evaluate overall model performance while single-image metrics assess individual outputs
- **Failure signatures:** Metrics insensitive to prompt length, biased toward certain image styles, or failing to reflect human preferences; inconsistent results due to different model versions; misinterpretation due to varying score scales
- **First 3 experiments:**
  1. Implement CLIPScore to evaluate alignment between text prompts and generated images from a text-to-image model
  2. Use Decompositional-Alignment Score (DA-Score) to assess compositional accuracy on a benchmark dataset
  3. Compare results of different metrics (CLIPScore, DA-Score, FID) on a common dataset to understand their strengths and weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can T2I evaluation metrics be designed to quantify the quality of compositional elements in generated images, rather than just their presence or absence?
- Basis in paper: [explicit] The paper identifies this as an open challenge, stating that "these alignment-focused metrics are targeted to sense the presence or absence of certain compositions in image space, but are unable to quantify the quality of such detected components."
- Why unresolved: Current metrics, particularly those based on VLMs, primarily provide binary or probabilistic assessments of whether specific compositional elements are present, lacking a nuanced measure of their quality or fidelity.
- What evidence would resolve it: Development and validation of new metrics that can assign quality scores to detected compositional elements, such as object accuracy, spatial relationships, or attribute bindings, and demonstrate improved correlation with human judgments.

### Open Question 2
- Question: How can the "bag-of-words" behavior in vision-language models be effectively mitigated to improve compositional understanding in T2I evaluation?
- Basis in paper: [explicit] The paper highlights this as a challenge, noting that "VLMs tend to behave like bags-of-words" and "the contrastive pre-training optimizes for image-text retrieval... which does not acknowledge compositional information."
- Why unresolved: Current VLM training objectives prioritize retrieval performance over compositional reasoning, leading to insensitivity to word order and permutations of object relations.
- What evidence would resolve it: Development and validation of training strategies or architectural modifications for VLMs that improve their ability to understand and represent compositional relationships in text, demonstrated through improved performance on benchmarks like Winoground.

### Open Question 3
- Question: What strategies can be employed to reduce hallucination in content-based T2I metrics that rely on VLMs or LLMs for image analysis?
- Basis in paper: [explicit] The paper identifies this as a challenge, stating that "VLMs or LLMs that may contain additional details fabricated by a language model rather than actually represented by the image."
- Why unresolved: VLMs and LLMs can generate plausible but inaccurate descriptions or interpretations of image content, leading to misleading evaluation results.
- What evidence would resolve it: Development and validation of techniques to improve the accuracy and faithfulness of VLM/LLM-generated image descriptions or analyses, such as incorporating image-based verification or multi-modal reasoning.

### Open Question 4
- Question: How can a comprehensive and representative benchmark dataset for T2I evaluation be created that covers a wide range of compositional aspects and visual concepts?
- Basis in paper: [explicit] The paper identifies this as a challenge, stating that "creating a comprehensive benchmark for compositionality evaluation should be targeted in the near future" and that "benchmarking image generation is lacking comparability due to evaluation on individually proposed datasets."
- Why unresolved: Existing datasets are often limited in size, scope, or complexity, making it difficult to draw generalizable conclusions about T2I model performance.
- What evidence would resolve it: Creation and validation of a large-scale, diverse, and well-annotated dataset that covers a broad range of compositional aspects, visual concepts, and prompt styles, and demonstrates improved reliability and comparability of T2I evaluation results.

## Limitations

- The survey's framework rests on two primary assumptions (compositional and general quality as only contributors, taxonomy comprehensiveness) that require validation with weak corpus evidence
- The effectiveness of the proposed taxonomy in practice is uncertain, particularly whether the two-criteria framework adequately captures all relevant aspects of text-to-image quality assessment
- The assertion that the survey provides definitive guidelines for metric selection needs more rigorous validation given the rapidly evolving nature of the field

## Confidence

- **High Confidence**: The survey's systematic organization of existing metrics into a coherent taxonomy and its comprehensive coverage of the field
- **Medium Confidence**: The effectiveness of the proposed taxonomy in practice and the mechanisms described for embedding-based and content-based metrics
- **Low Confidence**: The assertion about definitive guidelines for metric selection and the claim about prompt length normalization significantly impacting evaluation outcomes

## Next Checks

1. **Cross-metric validation**: Implement multiple metrics from different categories (embedding-based, content-based, and image-only) on the same benchmark dataset and analyze their correlation patterns to test whether the taxonomy accurately captures distinct evaluation dimensions.

2. **Taxonomy stress test**: Identify edge cases or recently proposed metrics that challenge the current taxonomy structure, particularly metrics that might simultaneously evaluate compositional and general quality, or metrics that use hybrid approaches combining embeddings and content analysis.

3. **Prompt normalization experiment**: Systematically vary prompt length and structure in controlled experiments to measure the actual impact on different metrics' scores and determine whether the proposed normalization techniques effectively address this variability.