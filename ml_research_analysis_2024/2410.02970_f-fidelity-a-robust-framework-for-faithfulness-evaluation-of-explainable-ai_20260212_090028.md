---
ver: rpa2
title: 'F-Fidelity: A Robust Framework for Faithfulness Evaluation of Explainable
  AI'
arxiv_id: '2410.02970'
source_url: https://arxiv.org/abs/2410.02970
tags:
- f-fidelity
- explanation
- explanations
- evaluation
- fidelity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces F-Fidelity, a robust evaluation framework
  for XAI that addresses the Out-of-Distribution (OOD) problem in explanation evaluation.
  The key innovation is a two-step approach: (1) fine-tuning the model with random
  masking to improve robustness to perturbations, and (2) using controlled stochastic
  removal during evaluation that ensures perturbed inputs remain in-distribution.'
---

# F-Fidelity: A Robust Framework for Faithfulness Evaluation of Explainable AI

## Quick Facts
- arXiv ID: 2410.02970
- Source URL: https://arxiv.org/abs/2410.02970
- Authors: Xu Zheng; Farhad Shirani; Zhuomin Chen; Chaohao Lin; Wei Cheng; Wenbo Guo; Dongsheng Luo
- Reference count: 40
- Primary result: F-Fidelity framework achieves near-perfect Spearman correlations (0.94-1.00) in recovering ground-truth explainer rankings across multiple data modalities

## Executive Summary
This paper introduces F-Fidelity, a robust evaluation framework for XAI that addresses the Out-of-Distribution (OOD) problem in explanation evaluation. The key innovation is a two-step approach: (1) fine-tuning the model with random masking to improve robustness to perturbations, and (2) using controlled stochastic removal during evaluation that ensures perturbed inputs remain in-distribution. Unlike prior methods like ROAR, F-Fidelity avoids information leakage by using explanation-agnostic masks during fine-tuning. Comprehensive experiments across multiple data modalities (images, time series, and natural language) demonstrate that F-Fidelity significantly outperforms existing metrics in recovering ground-truth explainer rankings.

## Method Summary
F-Fidelity employs a two-step approach to evaluate explanation faithfulness. First, it fine-tunes the model with random masking operations (up to β ∈ [0, 1] ratio of input elements) to generate augmented training samples, making the model robust to perturbations. Second, during evaluation, it uses a controlled stochastic removal process where explanation-agnostic masks ensure perturbed inputs remain within the distribution seen during fine-tuning. The framework measures faithfulness using two metrics: F F id+ (when removing important features) and F F id− (when keeping important features). This approach prevents information leakage that occurs in methods like ROAR, which retrain models based on explainer output.

## Key Results
- F-Fidelity achieves near-perfect macro and micro Spearman correlations (0.94-1.00) in recovering ground-truth explainer rankings across most scenarios
- The method significantly outperforms existing metrics (Fidelity, ROAR, R-Fidelity) with improvements of 0.1-0.2 in correlation scores
- F-Fidelity successfully recovers true explanation size (sparsity) under certain theoretical conditions when given a faithful explainer
- The framework demonstrates consistent performance across diverse data modalities: computer vision (CIFAR-100, Tiny-Imagenet), time series (PAM, Boiler), and natural language (SST2, BoolQ)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: F-Fidelity addresses the Out-of-Distribution (OOD) problem by fine-tuning the model with randomly masked inputs, improving its robustness to perturbations during evaluation.
- Mechanism: During fine-tuning, random masking operations (e.g., randomly dropping pixels in images, tokens in language, or time steps in time series data) generate augmented training samples. This augmented data is then used to fine-tune a surrogate model, making it more robust to OOD inputs. During evaluation, a controlled stochastic removal process ensures that the perturbed inputs remain within the distribution seen during fine-tuning.
- Core assumption: The random masking during fine-tuning creates a distribution of perturbed inputs that is representative of the types of perturbations used during evaluation.
- Evidence anchors:
  - [abstract]: "The fine-tuning process employs stochastic masking operations... to generate augmented training samples. This augmented data is then used to fine-tune a surrogate model."
  - [section]: "To achieve reliable predictions on partially removed inputs, we design a fine-tuning process that randomly removes up to β ∈ [0, 1] ratio of input elements."
- Break condition: If the random masking distribution during fine-tuning does not cover the types of perturbations used during evaluation, the OOD problem may persist.

### Mechanism 2
- Claim: F-Fidelity prevents information leakage by using an explanation-agnostic fine-tuning strategy, unlike prior methods like ROAR that retrain the model based on the explainer output.
- Mechanism: The fine-tuning process uses randomly generated masks that are independent of any specific explainer's output. This ensures that the fine-tuned model does not learn patterns specific to a particular explainer, thus avoiding information leakage.
- Core assumption: The random masks used during fine-tuning are truly independent of the explainer's output and do not contain class-specific information.
- Evidence anchors:
  - [abstract]: "F-Fidelity utilizes i) an explanation-agnostic fine-tuning strategy, thus mitigating the information leakage issue..."
  - [section]: "Unlike previous approaches such as ROAR (Hooker et al., 2019), our strategy effectively mitigates the risk of information leakage and label bias by using explanation-agnostic stochastic masks..."
- Break condition: If the random masks used during fine-tuning inadvertently contain information about the classes or the explainer's output, information leakage may still occur.

### Mechanism 3
- Claim: F-Fidelity can recover the true explanation size (sparsity) given a faithful explainer, by analyzing the piecewise constant function produced by the F-Fidelity metric as a function of mask size.
- Mechanism: When the explainer outputs an accurate mask function, the F-Fidelity metric output, when evaluated as a function of mask size, produces a piecewise constant function. The length of the constant pieces depends on the explanation size. By analyzing this function, the true explanation size can be inferred.
- Core assumption: The explainer's ranking of input elements is close to that of an ideal Shapley-based explainer, and the input elements can be partitioned into distinct influence tiers.
- Evidence anchors:
  - [abstract]: "Furthermore, we show both theoretically and empirically that, given a faithful explainer, the F-Fidelity metric can be used to compute the sparsity of influential input components, i.e., to extract the true explanation size."
  - [section]: "Theorem 1. For the classification task described above, and a given pre-trained classifier f (·), consider a Shapley-value-based explainer ψ(·). For α+orig ∈ [0, 1] and β ∈ [0, α+], let e(s) = EX,Y (F F id+(ψ, α+orig, β, s)), s ∈ [0, td]. Then, e(s) is monotonically increasing for s ∈ [0, c1] and monotonically decreasing for s ∈ [max(β/α+td, c1), td]."
- Break condition: If the explainer's ranking is not close to that of an ideal Shapley-based explainer, or if the input elements cannot be partitioned into distinct influence tiers, the F-Fidelity metric may not accurately recover the explanation size.

## Foundational Learning

- Concept: Out-of-Distribution (OOD) problem
  - Why needed here: Understanding the OOD problem is crucial for grasping the motivation behind F-Fidelity. The OOD problem arises when perturbed inputs during explanation evaluation fall outside the distribution the model was trained on, leading to unreliable predictions.
  - Quick check question: What is the main issue with using perturbed inputs during explanation evaluation, and why does it affect the reliability of the evaluation?

- Concept: Explanation-agnostic fine-tuning
  - Why needed here: This concept is central to F-Fidelity's approach to preventing information leakage. By using random masks that are independent of any specific explainer's output during fine-tuning, F-Fidelity ensures that the fine-tuned model does not learn patterns specific to a particular explainer.
  - Quick check question: How does F-Fidelity's explanation-agnostic fine-tuning strategy differ from prior methods like ROAR, and what problem does it address?

- Concept: Shapley values
  - Why needed here: Shapley values provide a theoretical foundation for understanding how F-Fidelity can recover the explanation size. Under certain assumptions, input elements within the same influence tier receive equal Shapley values, which is crucial for the theoretical analysis in Theorem 1.
  - Quick check question: What is the relationship between Shapley values and influence tiers, and how does this relationship enable F-Fidelity to recover the explanation size?

## Architecture Onboarding

- Component map: Fine-tuning process -> Evaluation process -> F F id+ and F F id− metrics
- Critical path:
  1. Fine-tune the model using random masking on the training data.
  2. Generate explanations using the explainer being evaluated.
  3. Evaluate the explainer using F F id+ and F F id− on the evaluation set.
- Design tradeoffs:
  - Balancing the fine-tuning ratio β: A higher β improves robustness to perturbations but may reduce the model's agreement with the original model's predictions.
  - Choosing the number of sampling iterations N: A higher N reduces the standard deviation but increases the evaluation time.
- Failure signatures:
  - Poor correlation between F-Fidelity scores and ground-truth explainer rankings.
  - Inconsistent results across different sparsity levels.
  - High standard deviation in F-Fidelity scores.
- First 3 experiments:
  1. Evaluate a simple explainer (e.g., random explainer) on a small dataset (e.g., MNIST) to verify that F-Fidelity assigns low scores to poor explainers.
  2. Evaluate a well-known explainer (e.g., GradCAM) on a small dataset to verify that F-Fidelity assigns high scores to good explainers.
  3. Compare the performance of F-Fidelity with existing metrics (e.g., Fidelity, ROAR) on a small dataset to verify that F-Fidelity outperforms them.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does F-Fidelity perform when evaluating explanation methods on out-of-distribution data that differs significantly from the training distribution?
- Basis in paper: [explicit] The paper focuses on in-distribution evaluation but acknowledges that OOD problems can occur in real-world scenarios where data may shift.
- Why unresolved: The current evaluation framework assumes data remains within the training distribution, but practical applications often involve data drift or distribution shifts.
- What evidence would resolve it: Experiments comparing F-Fidelity's performance on data with varying degrees of distribution shift, and analysis of how well it maintains reliability under such conditions.

### Open Question 2
- Question: Can F-Fidelity be extended to evaluate explanations for models that operate on multimodal inputs (e.g., text and images together)?
- Basis in paper: [inferred] The paper evaluates F-Fidelity across three data modalities (images, time series, and natural language) but doesn't address multimodal inputs.
- Why unresolved: The current framework is designed for single-modality inputs, and extending it to multimodal scenarios would require handling the interaction between different data types.
- What evidence would resolve it: Experiments demonstrating F-Fidelity's effectiveness on multimodal models, and theoretical analysis of how the fine-tuning and stochastic removal processes would adapt to such inputs.

### Open Question 3
- Question: How sensitive is F-Fidelity to the choice of fine-tuning strategy (e.g., different types of masking or regularization techniques)?
- Basis in paper: [explicit] The paper uses random masking for fine-tuning but doesn't explore alternative strategies or their impact on evaluation performance.
- Why unresolved: The choice of fine-tuning strategy could significantly affect the robustness of the evaluation and the generalizability of the results.
- What evidence would resolve it: Comparative experiments using different fine-tuning strategies (e.g., Gaussian blur, structured masking) and analysis of their impact on F-Fidelity's performance and reliability.

## Limitations

- The evaluation is limited to specific datasets (CIFAR-100, Tiny-Imagenet, PAM, Boiler, SST2, BoolQ) and may not generalize to other real-world scenarios
- The framework's performance depends on the explainer producing rankings "close to" an ideal Shapley-based explainer, but this condition is not quantified
- The fine-tuning process introduces computational overhead and requires careful hyperparameter selection, which is not fully addressed in terms of computational trade-offs

## Confidence

- Confidence: Low - Claims of "near-perfect" correlations are based on limited dataset evaluation
- Confidence: Medium - Theoretical analysis provides conditions for explanation size recovery but lacks practical quantification
- Confidence: Medium - Fine-tuning introduces computational overhead and hyperparameter sensitivity not fully addressed

## Next Checks

1. **Cross-domain robustness test**: Evaluate F-Fidelity on datasets outside the tested modalities (e.g., medical imaging, tabular data) to verify the framework's generalizability claims.

2. **Ablation study on hyperparameters**: Systematically vary the fine-tuning ratio β and sampling iterations N to establish guidelines for hyperparameter selection and quantify the impact on evaluation reliability.

3. **Baseline comparison under controlled conditions**: Replicate ROAR and Fidelity evaluations using the same surrogate models and datasets to isolate whether performance improvements stem from the OOD mitigation or other implementation details.