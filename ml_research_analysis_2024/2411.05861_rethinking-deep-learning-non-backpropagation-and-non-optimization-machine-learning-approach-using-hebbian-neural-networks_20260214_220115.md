---
ver: rpa2
title: 'Rethinking Deep Learning: Non-backpropagation and Non-optimization Machine
  Learning Approach Using Hebbian Neural Networks'
arxiv_id: '2411.05861'
source_url: https://arxiv.org/abs/2411.05861
tags:
- learning
- label
- data
- trained
- hebbian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a novel machine learning approach using Hebbian
  learning in neural networks to solve the MNIST handwritten digit classification
  problem without backpropagation or optimization. The author develops a three-stage
  method: first applying Hebbian learning to a distributed representation approach,
  then examining individually trained networks using norm-based cognition, and finally
  creating a recognition program based on vector norm comparisons.'
---

# Rethinking Deep Learning: Non-backpropagation and Non-optimization Machine Learning Approach Using Hebbian Neural Networks

## Quick Facts
- arXiv ID: 2411.05861
- Source URL: https://arxiv.org/abs/2411.05861
- Authors: Kei Itoh
- Reference count: 16
- Primary result: Achieved ~75% accuracy on MNIST using Hebbian learning without backpropagation or optimization

## Executive Summary
This paper presents a novel machine learning approach that solves the MNIST handwritten digit classification problem without using backpropagation, optimization, or large training datasets. The author develops a three-stage method based on Hebbian learning principles, where neural networks are trained individually on each digit label and classification is performed through vector norm comparisons rather than similarity calculations. The resulting system achieves approximately 75% accuracy on MNIST, demonstrating that neural networks can perform character recognition through biologically plausible Hebbian learning mechanisms without requiring objective functions or gradient-based optimization.

## Method Summary
The method uses Hebbian learning with the update rule W_i = ηa_i a_{i+1}^T, where networks are trained individually on each MNIST digit label using limited training data (5,420 samples per label). Input data undergoes L2 normalization, and networks use ReLU activation with fully connected layers. During inference, multiple individually trained networks are run in parallel, and classification is performed by comparing the L2 norms of output vectors, selecting the label with the largest norm. The approach avoids backpropagation entirely by relying on selective strengthening of connections through Hebbian learning and norm-based decision making rather than similarity calculations.

## Key Results
- Achieved approximately 75% accuracy on MNIST handwritten digit classification
- Demonstrated Hebbian learning can work without backpropagation or optimization
- Showed that vector norm comparisons can effectively replace similarity-based classification
- Validated that biologically plausible learning rules can solve practical recognition tasks

## Why This Works (Mechanism)

### Mechanism 1
Hebbian learning can achieve classification without backpropagation by selectively strengthening connections for specific input patterns. The update rule (ΔW = η * a_i * a_i+1^T) strengthens synaptic connections active together when training data for a specific label is repeatedly presented, creating neurons that respond more strongly to their trained label and enabling classification based on vector norm magnitude.

### Mechanism 2
Vector norm magnitude serves as a basis for classification instead of vector similarity calculations. Each individually trained network produces output vectors whose L2 norms reflect response strength to test inputs, with the network having the largest norm for a given input indicating the most likely label, creating a binary-like decision mechanism through magnitude comparison.

### Mechanism 3
Indirect similarity cognition emerges from combinations of norm-based decisions. By comparing norms of multiple individually trained networks in parallel, a form of similarity judgment emerges without directly calculating similarity, with the network having the largest norm acting as a winner-takes-all decision mechanism that approximates similarity through relative magnitude.

## Foundational Learning

- Concept: Hebbian learning rule
  - Why needed here: This is the core mechanism that replaces backpropagation and optimization in the proposed approach, allowing the network to learn without gradient descent.
  - Quick check question: What is the mathematical form of the Hebbian learning update rule used in this paper?

- Concept: Vector norms and magnitude comparison
  - Why needed here: Classification is performed by comparing the L2 norms of output vectors from individually trained networks, rather than using similarity measures or backpropagation-based loss functions.
  - Quick check question: How does comparing vector norms differ from comparing vector similarity in terms of computational requirements?

- Concept: Distributed vs. norm-based representations
  - Why needed here: The paper argues that biological systems likely use norm-based cognition rather than distributed representations with direct similarity calculations, which is fundamental to understanding the proposed approach.
  - Quick check question: What is the key difference between distributed representations and norm-based cognition in terms of how information is processed?

## Architecture Onboarding

- Component map: 784-input → [hidden layers] → 784-output fully connected networks with ReLU activation, where each network is trained individually on one MNIST digit label. Multiple networks run in parallel during inference.

- Critical path: 1) Load and preprocess MNIST data with L2 normalization, 2) Create individually trained networks for each label using Hebbian learning, 3) During inference, input test data into all networks, 4) Calculate output vector norms, 5) Select label with largest norm as prediction.

- Design tradeoffs: The approach trades computational efficiency (running multiple networks in parallel) for biological plausibility (Hebbian learning instead of backpropagation). It also sacrifices some accuracy (approximately 75%) compared to optimized deep learning approaches.

- Failure signatures: High learning rates cause weight divergence and output explosion; low learning rates result in insufficient learning; certain digits (like '5') show poor performance due to similarity with other digits; accuracy drops significantly as the number of hidden layers increases beyond 3.

- First 3 experiments:
  1. Train a single network on only label '0' data and test with '0' vs '1' to verify selective responsiveness develops.
  2. Compare performance with different learning rates (10^-1 to 10^-9) to identify optimal range.
  3. Test norm-based classification with 2 networks (trained on '0' and '1') before scaling to all 10 digits.

## Open Questions the Paper Calls Out

### Open Question 1
Can Hebbian learning principles be effectively adapted to handle discrete information without temporal continuity, or are there alternative learning rules that could better suit such data? The study found that simply applying Hebbian learning to discrete information did not yield satisfactory results, suggesting a need for alternative approaches or adaptations that incorporate temporal elements or other mechanisms to handle discrete data effectively.

### Open Question 2
How can norm-based cognition be effectively integrated into more complex decision-making processes beyond binary choices, to mimic advanced cognitive functions in biological systems? While norm-based cognition is effective for simple reflex actions, its application to complex cognitive tasks like character recognition remains underexplored, and demonstrating successful application in more complex AI systems would validate its capability to handle tasks requiring indirect similarity perception.

### Open Question 3
What are the limitations of using Hebbian learning in neural networks for tasks beyond character recognition, such as natural language processing or real-time decision-making? The paper does not provide evidence or exploration of Hebbian learning's effectiveness in diverse applications, leaving its broader utility uncertain, and conducting experiments to apply Hebbian learning to various tasks and domains would assess its performance and limitations compared to traditional methods.

## Limitations
- The method requires running multiple networks in parallel during inference, creating computational inefficiency compared to single-network approaches
- The reported 75% accuracy, while impressive without backpropagation, is significantly lower than state-of-the-art deep learning methods
- The approach lacks clear scalability to more complex datasets beyond simple MNIST digit recognition

## Confidence
- **High confidence**: The basic mechanism of Hebbian learning for weight updates and norm-based classification is well-established and the theoretical framework is sound.
- **Medium confidence**: The reported accuracy of ~75% on MNIST is plausible given the simplified approach, but the exact implementation details needed for exact reproduction are unclear.
- **Low confidence**: Claims about biological plausibility and norm-based cognition as fundamental to strong AI development are speculative and lack empirical validation beyond the MNIST results.

## Next Checks
1. **Reproduce the core results**: Implement the Hebbian learning rule with the exact parameters (learning rate 10^-7, 3 hidden layers, L2 normalization) and verify the ~75% accuracy on MNIST using the same train/test split.

2. **Test scaling limitations**: Evaluate the method on Fashion-MNIST or a subset of CIFAR-10 to determine whether the approach generalizes beyond simple digit recognition and identify the point at which performance degrades significantly.

3. **Compare against baseline alternatives**: Implement a simple k-NN classifier or logistic regression on the same preprocessed MNIST data to establish whether the Hebbian approach offers advantages beyond avoiding backpropagation, or if it simply trades one set of computational requirements for another.