---
ver: rpa2
title: Benchmarking Children's ASR with Supervised and Self-supervised Speech Foundation
  Models
arxiv_id: '2406.10507'
source_url: https://arxiv.org/abs/2406.10507
tags:
- speech
- finetuning
- data
- child
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks child automatic speech recognition (ASR)
  using various speech foundation models (SFMs) like Whisper, Wav2vec2.0, HuBERT,
  and WavLM. It compares supervised and self-supervised SFMs, finding that supervised
  models like Canary and Parakeet outperform Whisper despite less training data, suggesting
  data quality is crucial.
---

# Benchmarking Children's ASR with Supervised and Self-supervised Speech Foundation Models

## Quick Facts
- arXiv ID: 2406.10507
- Source URL: https://arxiv.org/abs/2406.10507
- Authors: Ruchao Fan; Natarajan Balaji Shankar; Abeer Alwan
- Reference count: 0
- Primary result: Supervised SFMs like Canary/Parakeet outperform Whisper on child speech despite less data; PEFT works better for large models

## Executive Summary
This paper presents a comprehensive benchmark of automatic speech recognition (ASR) for children using various speech foundation models (SFMs). The study compares both supervised models (Whisper, Canary, Parakeet) and self-supervised models (Wav2vec2.0, HuBERT, WavLM) on child speech datasets, finding that supervised models with higher quality training data can outperform larger models trained on more but potentially noisier data. The paper also investigates data augmentation strategies and parameter-efficient finetuning (PEFT) methods, proposing a perturbation invariant finetuning technique to stabilize training with augmented data.

## Method Summary
The study benchmarks child ASR using two datasets: My Science Tutor (MyST) with 133 hours of spontaneous speech and CSLU OGI with 50 hours of scripted read speech. Various SFMs including Whisper, Canary, Parakeet, Wav2vec2.0, HuBERT, and WavLM are evaluated through finetuning experiments. Data augmentation methods (pitch perturbation, speed perturbation, vocal tract length perturbation, SpecAugment) and PEFT techniques (LoRA, adapter tuning, prompt tuning, prefix tuning) are systematically tested. A novel perturbation invariant finetuning (PIF) loss is proposed to stabilize training when using augmented data by adding distance regularization between original and perturbed encoder outputs.

## Key Results
- Supervised SFMs (Canary, Parakeet) outperform Whisper on child speech despite using less training data, indicating data quality matters more than quantity
- Adapter tuning matches full finetuning performance for large models but performs worse for small models
- The proposed perturbation invariant finetuning (PIF) technique stabilizes training with data augmentation
- Supervised SFMs are recommended when little finetuning data is available, while self-supervised SFMs work better with more data

## Why This Works (Mechanism)

### Mechanism 1
The training data for Canary and Parakeet is likely cleaner, more curated, and better matched to child speech characteristics, enabling better generalization even with fewer hours. Weakly-supervised data in Whisper introduces noise that degrades performance on the specific domain of child speech.

### Mechanism 2
Large models have sufficient capacity that lightweight adapters can effectively learn domain-specific features without overfitting, while small models lack the representational headroom to compensate for the frozen base parameters. The representational capacity of large models allows adapters to capture task-specific information without needing to modify all parameters.

### Mechanism 3
The PIF regularization prevents the model from overfitting to augmentation artifacts while still learning robust features, addressing instability in finetuning with pitch and vocal tract length perturbations. The regularization term adds a distance loss between original and perturbed encoder outputs.

## Foundational Learning

- **Self-supervised vs supervised learning paradigms in speech models**: Understanding why supervised models like Whisper, Canary, and Parakeet behave differently from self-supervised models like Wav2vec2.0 and HuBERT is crucial for selecting appropriate model families for child ASR. Quick check: What is the fundamental difference in how supervised and self-supervised speech foundation models are trained?

- **Parameter-efficient finetuning (PEFT) techniques**: The paper extensively compares adapter tuning, LoRA, prompt tuning, and prefix tuning to understand when lightweight finetuning strategies are appropriate for child ASR. Quick check: How does adapter tuning differ from LoRA in terms of where and how parameters are modified?

- **Data augmentation for low-resource speech recognition**: Child ASR suffers from data scarcity, making data augmentation critical; understanding which methods work best with foundation models is essential. Quick check: Why might data augmentation methods that work well for training from scratch behave differently when used with pretrained foundation models?

## Architecture Onboarding

- **Component map**: Speech foundation models (supervised like Whisper/Canary/Parakeet or self-supervised like Wav2vec2.0/HuBERT/WavLM) → Data augmentation pipelines (pitch perturbation, speed perturbation, VTLP, SpecAugment) → PEFT modules (adapters, LoRA, prompts, prefixes) → Finetuning framework with CTC or encoder-decoder loss → Output transcriptions

- **Critical path**: Input speech → Feature extraction by SFM → Augmentation (if enabled) → PEFT adaptation (if using PEFT) → Finetuning loss computation → Parameter updates → Output transcriptions

- **Design tradeoffs**: Full finetuning provides best performance but requires high memory; PEFT reduces memory but may underperform on small models; supervised SFMs need less finetuning but may have domain mismatch; self-supervised SFMs are more flexible but require careful finetuning.

- **Failure signatures**: PEFT underperforming on small models (insufficient capacity); data augmentation instability (requires PIF); supervised models failing on child speech (domain mismatch); self-supervised models requiring longer finetuning.

- **First 3 experiments**:
  1. Compare zero-shot performance of Whisper-tiny through Whisper-largeV3 on child speech to establish baseline without finetuning
  2. Full finetuning of Whisper-small with and without each data augmentation method to assess augmentation impact
  3. Adapter tuning vs full finetuning on Whisper-largeV3 to verify PEFT effectiveness scales with model size

## Open Questions the Paper Calls Out

### Open Question 1
How do different model sizes impact the effectiveness of data augmentation methods in child ASR? The paper explored model size impact on PEFT but not on data augmentation methods, suggesting comparative studies across model sizes would be valuable.

### Open Question 2
What is the relative performance of Whisper-largeV3 compared to other large-scale models like SeamlessM4T and OWSM on child ASR tasks? The paper mentions future work including these models but doesn't provide direct comparisons.

### Open Question 3
How does the Perturbation Invariant Finetuning (PIF) technique affect the stability and performance of finetuning when using other data augmentation methods beyond pitch and vocal tract length perturbation? The paper only tested PIF on two perturbation methods, leaving its effectiveness on other augmentation techniques unexplored.

## Limitations
- Relatively small child speech datasets (133 hours for MyST and 50 hours for OGI) may not fully represent child speech diversity
- Limited analysis of PEFT methods without exploring the full space of parameter-efficient approaches
- PIF method lacks extensive ablation studies to validate its necessity across different augmentation combinations
- Comparison between supervised and self-supervised models doesn't account for potential domain adaptation techniques

## Confidence

- **High Confidence**: Data quality trumps quantity for supervised SFMs on child speech; PEFT matching full finetuning for large models but not small ones
- **Medium Confidence**: PIF stabilizing finetuning with augmented data; supervised SFMs recommended for limited finetuning data
- **Low Confidence**: Generalization of findings to other low-resource speech domains beyond child speech

## Next Checks

1. **PIF Ablation Study**: Systematically evaluate PIF's effectiveness by comparing finetuning stability with and without PIF across different augmentation combinations on both small and large model variants.

2. **Domain Transfer Validation**: Test whether the observed data quality advantage of supervised SFMs extends to other low-resource domains (elderly speech, accented speech, medical dictation) using the same experimental framework.

3. **Extended PEFT Exploration**: Evaluate additional PEFT methods like BitFit, sparse fine-tuning, and full-parameter tuning with learning rate schedules on the Whisper-largeV3 model to better understand scaling relationships.