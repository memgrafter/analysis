---
ver: rpa2
title: 'Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based
  Task'
arxiv_id: '2401.02909'
source_url: https://arxiv.org/abs/2401.02909
tags:
- portuguese
- language
- llama
- bode
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce Bode, a fine-tuned large language model for
  Portuguese prompt-based tasks. Bode is based on LLaMA 2 and trained using LoRA on
  a Portuguese instruction-following dataset.
---

# Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task

## Quick Facts
- arXiv ID: 2401.02909
- Source URL: https://arxiv.org/abs/2401.02909
- Reference count: 8
- Primary result: Bode achieves 82.5% accuracy on Portuguese 3-class sentiment analysis using zero-shot learning

## Executive Summary
This paper introduces Bode, a family of fine-tuned large language models specifically optimized for Portuguese prompt-based tasks. Built on LLaMA 2 and trained using LoRA on a Portuguese instruction-following dataset, Bode comes in two versions: Bode 7B and Bode 13B. The models are evaluated on Portuguese text classification tasks using zero-shot and in-context learning approaches, demonstrating strong performance on sentiment analysis, news categorization, and fake news detection. Bode aims to address the performance gap of multilingual models on Portuguese-specific tasks.

## Method Summary
The authors fine-tuned LLaMA 2 using LoRA (alpha=32, dropout=0.05) on a Portuguese instruction-following dataset derived from Alpaca. The fine-tuning process adapted the pre-trained weights to Portuguese language context while preserving the base model's knowledge. The models were evaluated on three Portuguese datasets (TweetSentBr for sentiment, AGNews for news categorization, and FakeRecogna for fake news detection) using both zero-shot learning (no task-specific examples) and in-context learning (with contextual examples provided in the prompt). Evaluation metrics included accuracy and F1-score.

## Key Results
- Bode 7B achieves 82.5% accuracy on Portuguese 3-class sentiment analysis
- Bode 7B achieves 93.2% accuracy on fake news detection
- Bode 13B achieves 78.6% accuracy on Portuguese 2-class sentiment analysis
- Bode outperforms other language models on Portuguese text classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bode outperforms base LLaMA 2 models because it is fine-tuned on a Portuguese instruction-following dataset, enabling better Portuguese language task performance.
- Mechanism: LoRA-based fine-tuning adapts the pre-trained weights to the Portuguese language context, preserving the base model's knowledge while adding language-specific capability.
- Core assumption: The Portuguese instruction-following dataset contains sufficient and representative examples to improve Portuguese performance.
- Evidence anchors:
  - [abstract] "Bode is based on LLaMA 2 and trained using LoRA on a Portuguese instruction-following dataset."
  - [section 4] "The fine-tuning procedure to create the Bode model was performed using the Low-Rank Adaptation method"
- Break condition: If the instruction-following dataset is too small or lacks diversity, the fine-tuning may not generalize well to new tasks.

### Mechanism 2
- Claim: Zero-shot and in-context learning approaches allow Bode to perform well on classification tasks without task-specific fine-tuning.
- Mechanism: By engineering effective prompts and leveraging the model's pre-trained understanding, Bode can generalize to classification tasks using only the provided context.
- Core assumption: The base LLaMA 2 model has sufficient pre-training to understand and respond to classification prompts in Portuguese.
- Evidence anchors:
  - [section 5.1.1] "In zero-shot learning, the proposed model is requested to produce an output for previously untrained data."
  - [section 5.1.2] "In-context learning approach is designed to explore the model's capability regarding tasks related to specific contextual information."
- Break condition: If the prompts are not well-engineered or the context is insufficient, the model may fail to generalize to the task.

### Mechanism 3
- Claim: The multilingual models like mBERT struggle with Portuguese tasks because they are not specifically optimized for the language.
- Mechanism: Multilingual models must balance performance across many languages, potentially leading to suboptimal performance for low-resource languages like Portuguese.
- Core assumption: The performance degradation is due to the lack of language-specific optimization rather than the model architecture itself.
- Evidence anchors:
  - [abstract] "LLMs trained on multilingual datasets normally struggle to respond to prompts in Portuguese satisfactorily, presenting, for example, code switching in their responses."
  - [section 1] "multilingual models may not capture the complex nuances and idiosyncrasies of a certain language."
- Break condition: If the multilingual model is sufficiently large and well-trained, it may overcome the performance gap with monolingual models.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA allows efficient fine-tuning of large language models by updating only a small subset of parameters, reducing computational cost and enabling quick task switching.
  - Quick check question: How does LoRA differ from traditional fine-tuning in terms of the number of trainable parameters?
- Concept: Zero-shot and in-context learning
  - Why needed here: These approaches allow the model to perform tasks without task-specific fine-tuning, reducing the need for labeled data and enabling generalization to new tasks.
  - Quick check question: What is the key difference between zero-shot and in-context learning in terms of how the model is provided with task information?
- Concept: Prompt engineering
  - Why needed here: Effective prompts are crucial for guiding the model's response and enabling it to understand the task requirements in Portuguese.
  - Quick check question: How does the structure of the prompt influence the model's output in a classification task?

## Architecture Onboarding

- Component map: LLaMA 2 (7B or 13B) -> LoRA fine-tuning -> Portuguese instruction dataset -> Custom prompts -> Classification tasks
- Critical path: Fine-tune LLaMA 2 with LoRA on Portuguese dataset → Design effective prompts → Evaluate on classification tasks using zero-shot and in-context learning
- Design tradeoffs:
  - Using a smaller LoRA rank reduces the number of trainable parameters but may limit the model's ability to adapt to new tasks.
  - Fine-tuning on a Portuguese-specific dataset improves performance but may reduce the model's ability to handle other languages.
- Failure signatures:
  - Poor performance on classification tasks may indicate issues with the fine-tuning dataset, prompt engineering, or the model's ability to generalize.
  - Catastrophic forgetting may occur if the fine-tuning process causes the model to lose previously learned knowledge.
- First 3 experiments:
  1. Evaluate the base LLaMA 2 model on Portuguese classification tasks using zero-shot learning to establish a baseline.
  2. Fine-tune the LLaMA 2 model with LoRA on the Portuguese instruction-following dataset and evaluate on the same classification tasks.
  3. Compare the performance of Bode with other language models like Cabrita and openCabrita on the Portuguese classification tasks.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation methodology lacks ablation studies to isolate contributions of LoRA fine-tuning versus prompt engineering
- Inconsistent performance results between Bode 7B and 13B versions raise questions about implementation or data leakage
- Limited comparisons with other contemporary Portuguese-specific models beyond brief mentions of Cabrita and openCabrita

## Confidence

**High Confidence**: The methodology for fine-tuning LLaMA 2 with LoRA on Portuguese instruction data is technically sound and well-documented. The approach of using zero-shot and in-context learning for evaluation is standard practice in the field.

**Medium Confidence**: The claim that Bode outperforms other large language models on Portuguese text classification tasks. While the results show improvements over mBERT and GPT-3, the evaluation lacks comprehensive comparisons with other contemporary Portuguese models.

**Low Confidence**: The specific accuracy numbers reported for Bode 13B, particularly the 56.9% on 3-class sentiment analysis, which appears anomalously low and may indicate implementation issues or data leakage problems.

## Next Checks
1. **Ablation study on model versions**: Test Bode 7B and 13B on the same tasks with identical prompts to verify the unexpected performance reversal. This should include testing both models on a held-out validation set not used during any fine-tuning.

2. **Cross-linguistic generalization test**: Evaluate Bode on English classification tasks to check for catastrophic forgetting and confirm that Portuguese fine-tuning didn't degrade English performance, which would validate the LoRA approach's effectiveness.

3. **Prompt sensitivity analysis**: Systematically vary the prompt templates and in-context examples to measure how sensitive the classification performance is to prompt engineering versus model capability, including tests with no examples (true zero-shot) versus with examples (few-shot).