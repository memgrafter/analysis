---
ver: rpa2
title: Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty
arxiv_id: '2412.06771'
source_url: https://arxiv.org/abs/2412.06771
tags:
- image
- prompt
- user
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of underspecified user prompts
  in text-to-image generation by developing proactive agents that actively engage
  users to clarify intent. The approach uses a belief graph to represent uncertainty
  about entities, attributes, and relations, and employs LLM-based strategies to ask
  informative questions and update beliefs through multi-turn interaction.
---

# Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty

## Quick Facts
- arXiv ID: 2412.06771
- Source URL: https://arxiv.org/abs/2412.06771
- Reference count: 40
- Primary result: Agents achieve 2× higher VQAScore than standard T2I generation within five turns through proactive clarification questions

## Executive Summary
This paper addresses the challenge of underspecified user prompts in text-to-image generation by developing proactive agents that engage users in multi-turn dialogue to clarify intent. The approach uses a belief graph to represent uncertainty about entities, attributes, and relations in the target image, and employs LLM-based strategies to ask informative questions and update beliefs through iterative interaction. Experiments across three datasets (COCO, ImageInWords, and DesignBench) demonstrate significant improvements in image-text alignment, with the agents achieving at least 2× higher VQAScore than standard single-turn T2I generation. Human studies indicate over 90% of participants found the agents and belief graphs helpful for their T2I workflow.

## Method Summary
The method constructs three agent prototypes (Ag1, Ag2, Ag3) that use belief graphs to represent uncertainty in user prompts and LLM-driven strategies to ask clarification questions. The belief graph contains three entity types (image, attribute, relation) with probabilities and importance scores. Agents select questions targeting the most uncertain but important attributes, then update their belief graphs based on user feedback. The approach uses frozen Gemini 1.5 and Imagen 3 APIs without fine-tuning. Evaluation is conducted through self-play simulations and human studies, measuring VQAScore, image-text similarity metrics, and user preferences across multiple interaction turns.

## Key Results
- Agents achieve at least 2× higher VQAScore than standard single-turn T2I generation within five turns
- Human studies show over 90% of participants found the agents and belief graphs helpful for T2I workflow
- Agent-generated images were preferred over baseline T2I in over 80% of cases in user studies
- VQAScore improvements are observed across all three datasets (COCO, ImageInWords, DesignBench)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Proactive question-asking reduces underspecification in prompts by targeting uncertain but important image attributes.
- Mechanism: The agent constructs a belief graph representing probabilities and importance scores for entities, attributes, and relations. It then selects questions about the most uncertain attributes of important entities to clarify user intent.
- Core assumption: The LLM can accurately parse user prompts into a structured belief graph that captures relevant uncertainty.
- Evidence anchors:
  - [abstract]: "Experiments over the three datasets demonstrate the proposed T2I agents' ability to ask informative questions and elicit crucial information to achieve successful alignment with at least 2 times higher VQAScore than the standard T2I generation."
  - [section]: "Based on both uncertainty and importance, the agent proactively asks clarification questions. It can, for instance, identify the most uncertain attribute of an important entity (e.g., cuisine type of 'breakfast')."
  - [corpus]: Weak evidence - only 1 of 5 related papers explicitly addresses question selection based on uncertainty-importance tradeoffs.

### Mechanism 2
- Claim: Belief graph transitions through user feedback allow the agent to iteratively refine its understanding of user intent.
- Mechanism: The agent merges conversation history with the current prompt to generate an updated belief graph that incorporates user responses to clarification questions.
- Core assumption: Merging conversation history with prompts via LLM produces coherent updates that reflect user intent.
- Evidence anchors:
  - [abstract]: "Experiments over the three datasets demonstrate the proposed T2I agents' ability to ask informative questions and elicit crucial information to achieve successful alignment."
  - [section]: "The agent's belief graph gets updated when the agent receives new information through user feedback... The transition process integrates information from the initial user prompt, the conversation history, interaction and the previous belief."
  - [corpus]: Weak evidence - related papers focus on general agent interaction but not specifically on belief graph transitions for T2I.

### Mechanism 3
- Claim: Multi-turn interaction with belief visualization improves user satisfaction and image quality compared to single-turn generation.
- Mechanism: The agent engages users in iterative clarification while presenting an editable belief graph that makes uncertainty visible, enabling users to directly control agent beliefs.
- Core assumption: Users can understand and effectively use belief graphs to communicate their intent.
- Evidence anchors:
  - [abstract]: "Moreover, we conducted human studies and observed that at least 90% of human subjects found these agents and their belief graphs helpful for their T2I workflow."
  - [section]: "Presenting the agent's belief graph can be a generalizable method for AI transparency... By asking clarification questions, our proposed agents may enable a more customizable and personalized content creation experience."
  - [corpus]: Weak evidence - no related papers directly test belief graph visualization in T2I contexts.

## Foundational Learning

- Concept: Belief graphs as probabilistic symbolic representations
  - Why needed here: T2I models must handle underspecified prompts by representing uncertainty about entities, attributes, and relations in the target image
  - Quick check question: What are the three types of entities in the belief graph and how do their probabilities differ?

- Concept: Active learning through uncertainty reduction
  - Why needed here: The agent must select questions that maximize information gain about user intent while minimizing interaction cost
  - Quick check question: How does the agent balance uncertainty and importance when selecting which attribute to ask about?

- Concept: Multi-modal evaluation metrics
  - Why needed here: T2I quality must be assessed across text-text, image-image, and text-image similarity dimensions
  - Quick check question: What three types of similarity metrics are used to evaluate the final generated image?

## Architecture Onboarding

- Component map: User prompt → Belief graph construction → Question selection → User feedback → Belief update → Prompt refinement → Image generation → Evaluation

- Critical path: User prompt → Belief graph construction → Question selection → User feedback → Belief update → Prompt refinement → Image generation → Evaluation

- Design tradeoffs:
  - Heuristic vs LLM-driven question selection: Ag1 uses hardcoded importance scores (predictable but rigid), Ag2 uses belief-prompted LLM (flexible but potentially verbose), Ag3 uses principle-prompted LLM (most natural but less structured)
  - Prompt merging strategy: Simple concatenation vs sophisticated LLM summarization
  - Belief graph complexity: More detailed graphs capture more uncertainty but require more computational resources

- Failure signatures:
  - Questions don't reduce uncertainty (belief graph representation issues)
  - User confusion or disengagement (interface complexity)
  - T2I model fails to follow refined prompts (model limitations)
  - Belief graph transitions lose critical information (merging failures)

- First 3 experiments:
  1. Test belief graph construction accuracy by comparing LLM-generated graphs against human-annotated ground truth for 50 diverse prompts
  2. A/B test question selection strategies (Ag1 vs Ag2 vs Ag3) on a fixed set of prompts to measure information gain per turn
  3. Measure user comprehension of belief graphs through think-aloud protocols while they interact with a prototype interface

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proactive agents handle cultural or contextual differences in user intent that may not be captured by the belief graph structure?
- Basis in paper: Explicit - The paper discusses how interpretations of prompts can vary significantly across diverse populations and cultures, but the agents do not explicitly address how they would handle such differences beyond asking clarification questions.
- Why unresolved: The paper mentions cultural competence as an important aspect of future work but does not detail how the current belief graph approach would adapt to different cultural contexts or user backgrounds.
- What evidence would resolve it: Experiments showing agent performance across different cultural contexts, user studies with diverse participants, or technical details on how the belief graph adapts to cultural variations.

### Open Question 2
- Question: What is the optimal stopping criterion for the multi-turn interaction - how does the agent know when it has gathered sufficient information to generate a satisfactory image?
- Basis in paper: Explicit - The paper discusses the trade-off between interaction cost and image quality but does not provide specific criteria for determining when to stop asking questions.
- Why unresolved: The experiments show performance improving up to about 10 turns, but the paper doesn't address how the agent should determine when to stop the interaction in practice.
- What evidence would resolve it: A formal stopping criterion based on uncertainty thresholds, a user study on preferred interaction length, or an analysis of diminishing returns in information gain.

### Open Question 3
- Question: How does the belief graph handle ambiguous or contradictory user feedback during the interaction?
- Basis in paper: Inferred - The paper describes how the belief graph gets updated with user feedback but doesn't address what happens when feedback is inconsistent or unclear.
- Why unresolved: The implementation details mention merging prompts and updating beliefs, but there's no discussion of conflict resolution mechanisms when users provide contradictory information.
- What evidence would resolve it: A formal mechanism for handling conflicting feedback, user study results on how the agents handle inconsistent responses, or technical details on belief graph conflict resolution strategies.

## Limitations

- Evaluation relies heavily on self-play with simulated users rather than real user interactions, limiting ecological validity
- The belief graph construction process depends on LLM accuracy for parsing complex prompts into structured representations, which may not generalize to all prompt types
- The three agent variants (Ag1, Ag2, Ag3) show different question styles but lack direct comparative effectiveness data beyond anecdotal examples

## Confidence

- High Confidence: VQAScore improvements (2× higher than baseline) and quantitative metrics showing iterative refinement through multi-turn interaction
- Medium Confidence: The effectiveness of proactive question-asking strategies and belief graph representation of uncertainty
- Low Confidence: Claims about user preference for belief graph visualization and the generalizability of the approach to diverse user populations

## Next Checks

1. **User Comprehension Validation**: Conduct think-aloud protocols with 20+ participants interacting with belief graphs to assess actual understanding versus claimed helpfulness (currently >90% positive but lacks depth)

2. **Real-World Deployment Test**: Deploy agents in a controlled user study with diverse participants asking open-ended prompts, measuring both image quality and interaction efficiency compared to baseline T2I

3. **Belief Graph Accuracy Benchmark**: Create a human-annotated gold standard for belief graph construction across 100 diverse prompts and measure LLM accuracy in capturing uncertainty, attribute importance, and entity relations