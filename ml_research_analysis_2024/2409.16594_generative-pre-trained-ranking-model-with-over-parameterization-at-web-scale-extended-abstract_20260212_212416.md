---
ver: rpa2
title: Generative Pre-trained Ranking Model with Over-parameterization at Web-Scale
  (Extended Abstract)
arxiv_id: '2409.16594'
source_url: https://arxiv.org/abs/2409.16594
tags:
- chen
- wang
- learning
- gs2p
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes GS2P, a generative pre-trained learning-to-rank
  model designed to address two main challenges in web search: lack of diverse query-webpage
  annotations and overfitting in traditional models. GS2P employs a semi-supervised
  approach with pseudo-label generation and over-parameterized MLPs using Random Fourier
  Features to learn generalizable representations.'
---

# Generative Pre-trained Ranking Model with Over-parameterization at Web-Scale (Extended Abstract)

## Quick Facts
- arXiv ID: 2409.16594
- Source URL: https://arxiv.org/abs/2409.16594
- Authors: Yuchen Li; Haoyi Xiong; Linghe Kong; Jiang Bian; Shuaiqiang Wang; Guihai Chen; Dawei Yin
- Reference count: 31
- Key outcome: GS2P achieves NDCG@4 and NDCG@10 improvements of up to 3.6% and 3.57% respectively over baselines, with 0.61% relative improvement in NDCG@4 using only 5% labeled data in online A/B tests.

## Executive Summary
This paper introduces GS2P, a generative pre-trained learning-to-rank model designed to address key challenges in web search: the scarcity of diverse query-webpage annotations and overfitting in traditional ranking models. GS2P employs a semi-supervised approach with pseudo-label generation through co-training of diverse LTR models, and over-parameterization using Random Fourier Features to learn generalizable representations. The model demonstrates significant performance gains in both offline experiments on Web30K and commercial datasets, and online A/B tests, particularly excelling at random and long-tail queries with minimal labeled data requirements.

## Method Summary
GS2P addresses web search ranking challenges through a three-stage approach: (1) Semi-supervised pseudo-label generation using co-training of multiple diverse LTR models with different ranking losses to create high-quality labels for unlabeled query-webpage pairs; (2) Self-attentive representation learning via a denoising autoencoder that learns generalizable query-webpage representations through Transformer blocks; (3) Over-parameterized MLP ranking using Random Fourier Features to transform representations into high-dimensional space, enabling better generalization in the interpolating regime. The model is trained jointly to minimize both generative loss (reconstruction) and discriminative loss (ranking), with hyperparameter optimization for the RFF dimension N through cross-validation on labeled data.

## Key Results
- Offline experiments show NDCG@4 and NDCG@10 improvements of up to 3.6% and 3.57% respectively over baselines on Web30K and commercial datasets
- Online A/B tests demonstrate consistent performance gains across all days with a 0.61% relative improvement in NDCG@4 using only 5% labeled data
- Significant enhancement in ranking efficacy for random and long-tail queries, with 5.50% and 6.50% improvements respectively compared to legacy systems
- Superior performance compared to SOTA ranking methods including ListNet, ListPL, SetRank, and CollabRank across multiple evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Over-parameterization with Random Fourier Features (RFF) enables the model to operate in the interpolating regime, leading to better generalization
- RFF transforms low-dimensional feature vectors into high-dimensional space, effectively increasing input features and allowing perfect fitting of training data while generalizing well
- Core assumption: Optimal RFF dimension N is determined via cross-validation on labeled data
- Evidence: Paper states over-parameterized model "operates in the interpolating regime and is projected to exhibit excellent generalization performance"

### Mechanism 2
- Semi-supervised pseudo-label generation using co-training of diverse LTR models improves label quality for unlabeled data
- Multiple LTR models with different ranking losses are trained on labeled data, then generate pseudo-labels for unlabeled query-webpage pairs
- Core assumption: Diverse LTR models capture complementary aspects of relevance, leading to more robust pseudo-labels
- Evidence: Paper describes co-training "multiple/diverse LTR models based on various ranking losses" to generate high-quality pseudo labels

### Mechanism 3
- Self-attentive representation learning via denoising autoencoding learns generalizable query-webpage representations
- Self-attentive encoder processes feature vectors through Transformer blocks to learn representations, while decoder reconstructs original features
- Core assumption: Reconstruction task combined with self-attention captures important patterns in query-webpage interactions useful for ranking
- Evidence: Paper describes jointly optimizing encoder-decoder to minimize generative loss, forcing encoder to learn robust representations

## Foundational Learning

- **Learning to Rank (LTR)**
  - Why needed: Entire paper focuses on improving LTR models for web search, addressing specific challenges in this domain
  - Quick check: What are the three main approaches to LTR (pointwise, pairwise, listwise), and how do they differ in their optimization objectives?

- **Semi-supervised Learning**
  - Why needed: GS2P uses unlabeled data to improve ranking performance, crucial given scarcity of labeled query-webpage pairs
  - Quick check: How does semi-supervised learning differ from supervised learning, and what are common techniques used in semi-supervised learning?

- **Transformer and Self-attention**
  - Why needed: Self-attentive encoder in GS2P uses Transformer blocks to learn representations
  - Quick check: What is the purpose of self-attention in Transformers, and how does it differ from traditional recurrent or convolutional neural networks?

## Architecture Onboarding

- **Component map**: Feature Extraction -> Semi-supervised Pseudo-label Generation -> Self-attentive Representation Learning -> Over-parameterized MLP Ranking
- **Critical path**: Feature Extraction → Semi-supervised Pseudo-label Generation → Self-attentive Representation Learning → Over-parameterized MLP Ranking
- **Design tradeoffs**:
  - Labeled vs. Unlabeled Data Ratio: More unlabeled data can improve performance but may introduce noise if pseudo-labels are inaccurate
  - Number of Transformer Blocks (B): More blocks can capture complex patterns but increase computational cost
  - RFF Dimension (N): Higher N enables better interpolation but may lead to overfitting if not properly regularized
- **Failure signatures**:
  - Poor performance on long-tail queries: May indicate insufficient pseudo-label quality for rare queries
  - Degradation with more unlabeled data: Could suggest poor pseudo-label quality or overfitting to noisy labels
  - High variance in NDCG scores across runs: Might indicate instability in the co-training process
- **First 3 experiments**:
  1. Compare GS2P with and without pseudo-label generation on a small labeled dataset to isolate the impact of semi-supervised learning
  2. Vary the number of Transformer blocks (B) in the self-attentive encoder to find the optimal depth for representation learning
  3. Test different values of RFF dimension (N) to identify the sweet spot for over-parameterization and generalization

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several significant areas for future research emerge from the methodology and results:

1. **Scalability of semi-supervised approach**: How does GS2P's performance scale with different proportions of labeled data beyond the 5-20% range tested, particularly at extremes like 1% or 50% labeled data?

2. **Computational efficiency trade-offs**: What is the impact of GS2P's over-parameterization approach on computational efficiency and latency in real-world search applications, and how does this affect user experience?

3. **Generalization to other content types**: How does GS2P's performance generalize to different types of web content beyond text-based webpages, such as images, videos, or interactive content that are increasingly prevalent in modern web search?

## Limitations

- The paper's reliance on commercial dataset performance metrics (GSB scores) limits reproducibility due to undisclosed dataset details
- Pseudo-label generation effectiveness depends heavily on quality of diverse LTR models, but detailed analysis of pseudo-label quality and impact is not provided
- Random Fourier Features over-parameterization claims are supported by offline experiments, but exact mechanisms of N optimization and its relationship to "double descent" phenomenon are not fully explained

## Confidence

- **High Confidence**: Offline NDCG@4 and NDCG@10 improvements (up to 3.6% and 3.57%) on Web30K dataset, as these results are verifiable with publicly available data
- **Medium Confidence**: Online A/B test results showing 0.61% relative improvement in NDCG@4, as these are reported but not independently verifiable
- **Medium Confidence**: Effectiveness of the three proposed mechanisms (pseudo-label generation, self-attention, over-parameterization), as individual components are theoretically sound but synergistic effects need further validation

## Next Checks

1. Conduct ablation studies to isolate the contribution of each component (pseudo-label generation, self-attention, over-parameterization) to overall performance
2. Test the model's robustness to different proportions of labeled vs. unlabeled data to understand the limits of the semi-supervised approach
3. Analyze the quality of pseudo-labels generated by co-training diverse LTR models and their correlation with ground truth labels to validate the semi-supervised learning component