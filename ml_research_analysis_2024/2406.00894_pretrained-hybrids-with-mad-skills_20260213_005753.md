---
ver: rpa2
title: Pretrained Hybrids with MAD Skills
arxiv_id: '2406.00894'
source_url: https://arxiv.org/abs/2406.00894
tags:
- manticore
- component
- search
- hybrids
- pretrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Manticore automates hybrid language model design by mixing pretrained
  models with projectors and convex combination weights, enabling cross-architecture
  integration without full retraining. It uses NAS-inspired search to find optimal
  mixture weights, supports both training from scratch and fine-tuning pretrained
  hybrids, and allows programming via proxy tasks like MAD.
---

# Pretrained Hybrids with MAD Skills

## Quick Facts
- arXiv ID: 2406.00894
- Source URL: https://arxiv.org/abs/2406.00894
- Reference count: 40
- Primary result: Manticore matches or exceeds existing hybrids on MAD and LRA benchmarks, improving over individual component models on natural language tasks

## Executive Summary
Manticore introduces a novel approach to hybrid language model design by leveraging pretrained models through projectors and convex combination weights. The framework automates hybrid architecture selection using NAS-inspired search techniques, enabling cross-architecture integration without full retraining. Manticore demonstrates effectiveness on both proxy tasks like MAD and real-world applications including long-range sequence modeling and multilingual benchmarks.

## Method Summary
Manticore automates hybrid language model design by mixing pretrained models with projectors and convex combination weights. The framework uses gradient-based NAS on mixture weights to select optimal component combinations without full retraining. It supports both training from scratch and fine-tuning pretrained hybrids, with the ability to program hybrids via proxy tasks like MAD. Projectors (simple linear transformations with gated residuals) align features between different architectures, enabling reuse of pretrained components while mixture weights determine their contribution to the hybrid output.

## Key Results
- Matches or exceeds existing hybrids on MAD and LRA benchmarks
- Improves over individual component models on natural language tasks (Penn Treebank, Alpaca, ELI5)
- Demonstrates transferability from proxy tasks to target tasks
- Supports both training from scratch and fine-tuning pretrained hybrids

## Why This Works (Mechanism)

### Mechanism 1
Manticore uses projectors to align features between pretrained models of different architectures without retraining from scratch. Simple linear transformations with gated residuals translate between "languages" spoken by different architectures, enabling reuse of pretrained components. The mapping between feature spaces across architectures can be learned with lightweight projectors rather than heavyweight nonlinear mappings.

### Mechanism 2
Manticore can automatically select language models without training multiple models from scratch by using gradient-based NAS on mixture weights. Mixture weights learned via DARTS-style gradient descent act as architecture parameters, allowing selection of optimal component combinations without full retraining. The search space of mixture weights is sufficiently expressive to capture good hybrid architectures, and gradient-based search can find them efficiently.

### Mechanism 3
Manticore hybrids can be programmed using external information (like MAD tasks or sequence length requirements) without full training. External task metadata or proxy task results can predict good mixture weights, which are then frozen and fine-tuned on target tasks. Proxy tasks like MAD are predictive of scaling laws and can transfer mixture weight preferences to downstream tasks.

## Foundational Learning

- **Neural Architecture Search (NAS) principles**: Manticore adapts NAS techniques to search over mixture weights rather than traditional architectural components. Why needed: Enables automated hybrid design. Quick check: What is the key difference between Manticore's search space and traditional NAS search spaces?

- **Feature alignment and projection**: Projectors must align features from different architectures to make them compatible for combination. Why needed: Enables cross-architecture integration. Quick check: Why can't we simply concatenate features from different architectures without projectors?

- **Convex combination and mixture weights**: Mixture weights determine how much each component model influences the hybrid's output. Why needed: Controls hybrid behavior and enables fallbacks. Quick check: What property of convex combinations ensures that Manticore can fall back to individual component models when beneficial?

## Architecture Onboarding

- **Component map**: Component models (pretrained) -> Projectors (linear + gated residual) -> Mixture weights (softmax of parameter vector) -> Manticore blocks (convex combination of projected features) -> New embeddings and LM head

- **Critical path**: 1. Select pretrained component models, 2. Pretrain projectors on general language data, 3. Initialize mixture weights, 4. Search for optimal mixture weights, 5. Fine-tune with frozen mixture weights

- **Design tradeoffs**: Inference cost doubles (both component models must be evaluated), training cost is dominated by component model pretraining (can be amortized), flexibility to mix arbitrary pretrained models vs. training new hybrids from scratch

- **Failure signatures**: Poor performance on target tasks, mixture weights converging to extremes (0 or 1) indicating one component dominates, projectors failing to align features (high reconstruction error)

- **First 3 experiments**: 1. Validate projectors can align features by measuring reconstruction error on held-out data, 2. Test mixture weight search on synthetic task with known optimal configuration, 3. Evaluate hybrid performance against component models on simple downstream task

## Open Questions the Paper Calls Out

1. **Optimal pretraining tokens**: What is the optimal number of projector pretraining tokens for different model architectures and tasks? The paper shows projector pretraining stabilizes around 70M tokens for their specific setup, but does not explore this systematically across architectures.

2. **Specialized hybrid search**: Can specialized hybrid search algorithms outperform off-the-shelf NAS methods like DARTS for Manticore? The paper notes that DARTS "was not always able to recover the best architecture" and suggests this is a limitation requiring future work.

3. **Scaling to multiple components**: How does Manticore performance scale when combining more than two component models? The framework theoretically supports K models but experimental validation is limited to K=2.

4. **MAD task transferability**: Under what conditions can MAD tasks reliably predict mixture weights for downstream tasks from different distributions? The paper finds MAD programming works when fine-tuning data is "fairly similar to the pretraining distribution" but fails when distributions diverge significantly.

5. **Tokenization impact**: What is the impact of tokenization choices on Manticore hybrid performance and merge conflicts? The paper identifies tokenization as a challenge but does not empirically investigate its impact.

## Limitations

- Generalization across tasks: Effectiveness on MAD and LRA benchmarks doesn't guarantee transferability to diverse downstream applications
- Computational overhead: Requires evaluating both component models during inference, doubling inference costs
- Search space expressivity: Convex combination weights may be too limited for certain optimal hybrid configurations

## Confidence

- **High confidence**: Core mechanism of projector-based feature alignment - supported by both theoretical justification and empirical results
- **Medium confidence**: Effectiveness of NAS-inspired search for mixture weights - shows promise but doesn't extensively explore failure modes
- **Medium confidence**: Proxy task transferability - MAD programming works well for demonstrated tasks but broader generalization requires validation

## Next Checks

1. **Cross-domain transfer test**: Evaluate Manticore hybrids on non-language tasks (e.g., vision-language tasks, code generation) to test generalizability of proxy task programming

2. **Ablation on projector complexity**: Systematically test whether more complex projector architectures yield better performance, particularly for highly dissimilar component model pairs

3. **Search space expansion**: Experiment with non-convex combinations and additional architectural parameters to determine if current search space captures full potential of hybrid model design