---
ver: rpa2
title: 'OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent
  AI'
arxiv_id: '2406.12753'
source_url: https://arxiv.org/abs/2406.12753
tags:
- reasoning
- answer
- problems
- arxiv
- abilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OlympicArena, a comprehensive benchmark designed
  to evaluate the cognitive reasoning abilities of large language models (LLMs) and
  large multimodal models (LMMs) using Olympic-level problems across seven scientific
  disciplines. The benchmark comprises 11,163 bilingual problems in text-only and
  interleaved text-image modalities, with rigorous data leakage detection and fine-grained
  evaluation mechanisms.
---

# OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI

## Quick Facts
- **arXiv ID**: 2406.12753
- **Source URL**: https://arxiv.org/abs/2406.12753
- **Reference count**: 40
- **Key outcome**: GPT-4o achieves only 39.97% overall accuracy on OlympicArena benchmark, revealing significant limitations in AI reasoning across scientific disciplines

## Executive Summary
OlympicArena is a comprehensive benchmark designed to evaluate the cognitive reasoning abilities of large language models (LLMs) and large multimodal models (LMMs) using Olympic-level problems across seven scientific disciplines. The benchmark comprises 11,163 bilingual problems in text-only and interleaved text-image modalities, with rigorous data leakage detection and fine-grained evaluation mechanisms. Even the most advanced model, GPT-4o, achieves only 39.97% overall accuracy (28.67% for mathematics and 29.71% for physics), highlighting current AI limitations in complex reasoning and multimodal integration. The study reveals that LMMs struggle to effectively utilize visual information and perform poorly in decompositional reasoning and spatial/geometric perception tasks.

## Method Summary
OlympicArena benchmark comprises 11,163 bilingual problems from 62 international Olympic competitions across seven scientific disciplines. The benchmark evaluates models in three settings: text-only, image-caption, and multimodal (interleaved text-image). Problems are categorized by difficulty (knowledge recall, concept application, cognitive reasoning) using GPT-4V annotation. Evaluation uses both answer-level scoring and process-level analysis where each reasoning step is scored using structured step-by-step format conversion. The benchmark includes rigorous data leakage detection and provides resources like annotation platform, evaluation tool, and leaderboard with automatic submission.

## Key Results
- GPT-4o achieves only 39.97% overall accuracy, with particularly low performance in mathematics (28.67%) and physics (29.71%)
- Current LMMs struggle significantly with leveraging interleaved visual information for complex cognitive reasoning problems
- Process-level evaluation reveals that models often perform intermediate reasoning steps correctly while failing at final answers
- Multimodal models do not show consistent improvement over text-only models when visual information is provided

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The benchmark's difficulty arises from its focus on cognitive reasoning over knowledge recall, creating a harder evaluation surface.
- **Mechanism**: By sampling problems from 62 international competitions and categorizing them into three difficulty levels, the benchmark ensures that even top models like GPT-4o only achieve ~39.97% accuracy. The categorization uses GPT-4V to label problems as knowledge recall, concept application, or cognitive reasoning, emphasizing the latter.
- **Core assumption**: Human annotators cannot reliably differentiate cognitive reasoning problems without model assistance; GPT-4V serves as an accurate proxy.
- **Evidence anchors**:
  - [abstract] "These challenges encompass a wide range of disciplines spanning seven fields and 62 international Olympic competitions, rigorously examined for data leakage."
  - [section] "We utilize GPT-4V as the annotator for categorizing different difficulty levels" (B.2)
  - [corpus] Weak: No external study validating GPT-4V's accuracy for this task; only internal cross-checks mentioned.
- **Break condition**: If GPT-4V systematically misclassifies problem difficulty, the benchmark would over- or under-represent cognitive reasoning challenges.

### Mechanism 2
- **Claim**: Multimodal evaluation reveals LMMs' limited ability to leverage visual information effectively.
- **Mechanism**: The benchmark includes interleaved text-image problems, but many LMMs do not improve over their text-only counterparts. This is attributed to models focusing on text, losing language abilities during visual training, or struggling with interleaved formats.
- **Core assumption**: Interleaved text-image format is more complex than side-by-side or single-image input; models optimized for one modality degrade in the other.
- **Evidence anchors**:
  - [abstract] "We also discover that current LMMs seem to struggle significantly in leveraging interleaved visual information for complex cognitive reasoning problems."
  - [section] "Some LMMs, while training their visual capabilities based on their text-based models, may lose some of their inherent language abilities" (§4.4)
  - [corpus] Weak: No comparative ablation studies on input format; claims are observational.
- **Break condition**: If models are retrained or fine-tuned on interleaved data, the performance gap might narrow, invalidating the current interpretation.

### Mechanism 3
- **Claim**: Process-level evaluation exposes hidden reasoning gaps that answer-only metrics miss.
- **Mechanism**: By scoring each reasoning step with GPT-4V, the benchmark distinguishes between models that arrive at correct answers through flawed reasoning versus sound reasoning. This uncovers potential for improvement even when final accuracy is low.
- **Core assumption**: Step-by-step correctness correlates with model reasoning depth and is more informative than final answer correctness alone.
- **Evidence anchors**:
  - [abstract] "We conduct detailed experiments and analyses from multiple perspectives... their outcomes in process-level evaluations, which are vital for tasks requiring complex reasoning with lengthy solutions."
  - [section] "We employ GPT-4 to convert both the references... and the model-generated solutions into a structured step-by-step format" (§4.2)
  - [corpus] Weak: No external validation of step-scoring reliability beyond internal human checks.
- **Break condition**: If step-scoring introduces high variance or bias, it may mislead model development priorities.

## Foundational Learning

- **Concept**: Multimodal input formats (text-only, interleaved text-image, image-caption)
  - **Why needed here**: The benchmark evaluates models across three settings to isolate the effect of visual information on reasoning performance.
  - **Quick check question**: What is the difference between interleaved text-image and image-caption input modes in this benchmark?
- **Concept**: Cognitive reasoning ability categorization (logical and visual)
  - **Why needed here**: Fine-grained analysis of reasoning skills (e.g., deductive, spatial, analogical) helps identify specific model weaknesses.
  - **Quick check question**: Which logical reasoning ability category shows the lowest average performance across models?
- **Concept**: Rule-based vs. model-based evaluation protocols
  - **Why needed here**: Different answer types (e.g., code, equations) require tailored evaluation methods to ensure fairness and accuracy.
  - **Quick check question**: Why does the benchmark use GPT-4V as an evaluator for certain answer types?

## Architecture Onboarding

- **Component map**: Data collection → Annotation (via Streamlit UI) → Problem categorization (difficulty, reasoning type) → Dataset split (val/test/ot) → Model evaluation (three settings) → Answer-level and process-level scoring → Leaderboard submission
- **Critical path**: Annotation quality → Categorization accuracy → Multimodal problem generation → Process-level scoring reliability → Leaderboard fairness
- **Design tradeoffs**: High-quality annotation requires human labor but ensures reliability; multimodal problems increase complexity but better reflect real-world tasks; process-level evaluation adds depth but requires more compute and careful prompt design.
- **Failure signatures**: Low inter-annotator agreement in difficulty labeling; inconsistent step scoring across problems; multimodal models not improving over text-only baselines without clear cause.
- **First 3 experiments**:
  1. Run a subset of problems through both text-only and multimodal settings to confirm the performance drop is consistent.
  2. Manually verify step-scoring on 20 sampled problems to check for systematic bias.
  3. Test a new model variant fine-tuned on interleaved text-image data to see if the multimodal gap closes.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can models effectively utilize visual information when processing complex scientific problems?
  - **Basis in paper**: [explicit] The paper states "Most LMMs are still not proficient at utilizing visual information" and "Many LMMs do not exhibit enhanced performance with image inputs"
  - **Why unresolved**: While the paper identifies this limitation, it doesn't explore specific architectural modifications or training strategies that could improve visual information utilization in LMMs.
  - **What evidence would resolve it**: Comparative experiments testing different multimodal architectures (cross-attention vs. parallel processing) or training strategies (curriculum learning with increasing visual complexity) on OlympicArena problems.

- **Open Question 2**: What is the relationship between process-level reasoning accuracy and final answer accuracy?
  - **Basis in paper**: [explicit] "The accuracy at the process-level is often higher than at the answer-level" and "even for very complex problems, the model can correctly perform some of the intermediate steps"
  - **Why unresolved**: The paper observes this phenomenon but doesn't investigate why models can perform intermediate steps correctly while failing at the final answer, or what this reveals about reasoning mechanisms.
  - **What evidence would resolve it**: Detailed error analysis categorizing mistakes at different reasoning stages, and experiments testing whether models that perform better on intermediate steps consistently produce better final answers across different problem types.

- **Open Question 3**: How does data leakage detection methodology affect benchmark validity?
  - **Basis in paper**: [explicit] "We employ a recently proposed instance-level leakage detection metric" and find "models cannot correctly answer most of the leaked instances"
  - **Why unresolved**: The paper uses one leakage detection method but doesn't evaluate whether different detection approaches yield different results, or how leakage detection sensitivity impacts benchmark reliability.
  - **What evidence would resolve it**: Comparative analysis using multiple leakage detection metrics, and experiments testing model performance on problems flagged by different detection methods to establish detection threshold reliability.

## Limitations
- Benchmark difficulty calibration relies heavily on GPT-4V for problem categorization without external validation of its accuracy in distinguishing cognitive reasoning problems.
- Multimodal evaluation reveals performance gaps but lacks controlled ablation studies to definitively attribute these gaps to format complexity versus model architecture limitations.
- Process-level evaluation introduces additional complexity through step-by-step scoring, but reliability depends on consistency of GPT-4V's evaluation criteria without quantified inter-annotator agreement.

## Confidence
- **High confidence**: The benchmark construction methodology (data collection from 62 competitions, bilingual problem generation, data leakage detection) is well-documented and technically sound. The overall architecture and evaluation framework are clearly specified.
- **Medium confidence**: Claims about LMMs struggling with visual information utilization are supported by empirical results but lack rigorous causal analysis. The performance gaps observed could have multiple explanations not fully explored in the current study.
- **Low confidence**: The interpretation that current AI limitations in complex reasoning and multimodal integration are fundamental rather than training-data or architectural issues. The paper demonstrates performance gaps but does not rule out that better training approaches could close these gaps.

## Next Checks
1. Conduct controlled experiments varying input formats (side-by-side vs. interleaved text-image) to isolate the impact of format complexity on LMM performance, and determine whether the observed performance gaps are format-specific or model-architecture specific.
2. Perform external validation of GPT-4V's problem categorization accuracy by having human experts independently classify a subset of problems into knowledge recall, concept application, and cognitive reasoning categories, then compare agreement rates.
3. Test a new model variant specifically fine-tuned on OlympicArena's interleaved text-image data to determine whether the multimodal performance gap narrows with targeted training, helping distinguish between fundamental reasoning limitations and data distribution effects.