---
ver: rpa2
title: 'Less is More: Parameter-Efficient Selection of Intermediate Tasks for Transfer
  Learning'
arxiv_id: '2410.15148'
source_url: https://arxiv.org/abs/2410.15148
tags:
- source
- task
- language
- tasks
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently selecting intermediate
  tasks for transfer learning in NLP. The authors propose Embedding Space Maps (ESMs),
  lightweight neural networks that approximate the effect of fine-tuning a language
  model, and combine them with LogME to create ESM-LogME, a method that reduces execution
  time by a factor of 10 and disk space usage by a factor of 278 compared to state-of-the-art
  methods.
---

# Less is More: Parameter-Efficient Selection of Intermediate Tasks for Transfer Learning

## Quick Facts
- **arXiv ID**: 2410.15148
- **Source URL**: https://arxiv.org/abs/2410.15148
- **Reference count**: 40
- **One-line primary result**: ESM-LogME achieves an average regret@5 score of 2.95, performing well on most target tasks while being substantially more efficient than other well-performing methods.

## Executive Summary
This paper addresses the problem of efficiently selecting intermediate tasks for transfer learning in NLP. The authors propose Embedding Space Maps (ESMs), lightweight neural networks that approximate the effect of fine-tuning a language model, and combine them with LogME to create ESM-LogME. This method significantly reduces execution time by a factor of 10 and disk space usage by a factor of 278 compared to state-of-the-art methods. In the largest study on NLP task transferability with 12k source-target pairs, ESM-LogME achieves strong performance while being substantially more efficient than other methods.

## Method Summary
The paper proposes Embedding Space Maps (ESMs), single linear transformation layers that approximate the effect of fine-tuning by transforming base model embeddings to mimic fine-tuned embeddings. ESM-LogME combines these ESMs with the LogME ranking algorithm: ESMs are pre-computed for each source task by training on pairs of base and fine-tuned embeddings, then used to transform target embeddings for ranking. This eliminates the need for forward passes through source models during ranking, achieving 10x speedup and 278x space reduction. The method was evaluated on 1553 source datasets and 8 target datasets using BERT-base-multilingual-uncased, achieving an average regret@5 score of 2.95.

## Key Results
- ESM-LogME achieves an average regret@5 score of 2.95, meaning transferring from the best of the top 5 picks leads to 97.05% of the best possible performance
- Reduces execution time by a factor of 10 compared to state-of-the-art methods
- Reduces disk space usage by a factor of 278
- Performs well on most target tasks while being substantially more efficient than other well-performing methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ESMs approximate the effect of fine-tuning by linearly transforming base model embeddings to mimic fine-tuned embeddings.
- **Mechanism**: A single linear layer (ESMs) is trained on pairs of embeddings from the base model and the fine-tuned model, enabling the ESM to approximate the embedding space transformation induced by fine-tuning.
- **Core assumption**: The transformation from base to fine-tuned embeddings can be sufficiently approximated by a linear function.
- **Evidence anchors**:
  - [abstract]: "ESMs, light-weight neural networks that approximate the effect of fine-tuning a language model"
  - [section]: "We describe this effect as a function h0→T on the embedding space and approximate this function using a neural network, which we call ESM, ϕ0→T."
- **Break condition**: If the embedding space transformation is highly non-linear, the linear approximation will be insufficient.

### Mechanism 2
- **Claim**: ESM-LogME significantly reduces computational cost compared to LogME by eliminating the need for forward passes through source models.
- **Mechanism**: ESMs transform embeddings from the base model once, then the transformed embeddings are used for ranking. This avoids repeated forward passes through each source model.
- **Core assumption**: ESMs can be pre-computed and shared, making the ranking phase computationally efficient.
- **Evidence anchors**:
  - [abstract]: "applying ESMs on a prior method reduces execution time by a factor of 10"
  - [section]: "ESM-LogME reduces execution time by a factor of 10 and disk space usage by a factor of 278"
- **Break condition**: If ESMs need to be re-computed frequently for different tasks, the efficiency gains may be reduced.

### Mechanism 3
- **Claim**: ESM-LogME retains high selection performance despite the linear approximation.
- **Mechanism**: The linear transformation of embeddings is sufficient to capture task-relevant characteristics, allowing ESM-LogME to rank source tasks effectively.
- **Core assumption**: The linear approximation of the embedding space transformation is sufficient to distinguish between different tasks.
- **Evidence anchors**:
  - [abstract]: "ESM-LogME achieves an average regret@5 score of 2.95"
  - [section]: "ESM-LogME yields R@5 of 2.95, i.e., transferring from the best of the top 5 picks leads to 97.05% of the best possible performance"
- **Break condition**: If the linear approximation fails to capture important task-specific features, the ranking performance will degrade.

## Foundational Learning

- **Concept**: Transfer learning
  - **Why needed here**: Understanding transfer learning is crucial because the paper focuses on selecting intermediate tasks for transfer learning.
  - **Quick check question**: What is the main benefit of using intermediate tasks in transfer learning?

- **Concept**: Embedding space
  - **Why needed here**: The paper relies on transforming embeddings to approximate the effect of fine-tuning.
  - **Quick check question**: How do embeddings represent input data in NLP models?

- **Concept**: Linear transformations
  - **Why needed here**: ESMs use linear transformations to approximate the effect of fine-tuning.
  - **Quick check question**: What is the difference between linear and non-linear transformations?

## Architecture Onboarding

- **Component map**: BERT-base-multilingual-uncased -> Embedding Space Maps (ESMs) -> LogME ranking algorithm -> Source tasks and target tasks

- **Critical path**:
  1. Train ESMs for each source task using base and fine-tuned model embeddings.
  2. Transform target task embeddings using each ESM.
  3. Apply LogME to rank source tasks based on transformed embeddings.

- **Design tradeoffs**:
  - ESMs are parameter-efficient but may underfit complex transformations.
  - Linear transformations are fast but may not capture all task-specific features.

- **Failure signatures**:
  - Poor ranking performance indicates that the linear approximation is insufficient.
  - High computational cost suggests that ESMs are not being effectively shared or reused.

- **First 3 experiments**:
  1. Train ESMs for a few source tasks and evaluate their ability to approximate fine-tuned embeddings.
  2. Compare the ranking performance of ESM-LogME with LogME on a small set of tasks.
  3. Measure the computational cost of ESM-LogME compared to LogME for a large set of tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do ESMs compare in performance when using non-linear architectures instead of linear layers?
- **Basis in paper**: [inferred] The authors designed ESMs as single linear layers for simplicity and efficiency, but mention that other architectures are worth exploring.
- **Why unresolved**: The paper only evaluates linear ESMs and does not test non-linear alternatives.
- **What evidence would resolve it**: Comparative experiments showing performance differences between linear and non-linear ESM architectures across various task types.

### Open Question 2
- **Question**: How does the performance of ESM-LogME vary across different base language models?
- **Basis in paper**: [inferred] The authors evaluated ESM-LogME solely on BERT and acknowledge that users might want to consider several base models.
- **Why unresolved**: The study only used BERT as the base model, leaving uncertainty about generalizability to other models.
- **What evidence would resolve it**: Systematic evaluation of ESM-LogME rankings using different base models (e.g., RoBERTa, T5, GPT) across the same source-target pairs.

### Open Question 3
- **Question**: What is the impact of target dataset size on ESM-LogME's ranking performance?
- **Basis in paper**: [inferred] The authors reduced target datasets to 1k rows for evaluation efficiency and acknowledge that prior work shows target dataset size significantly affects transfer gains.
- **Why unresolved**: The evaluation used a single fixed target dataset size without exploring the relationship between dataset size and selection accuracy.
- **What evidence would resolve it**: Experiments varying target dataset sizes (e.g., 100, 1k, 10k samples) to measure how ranking performance changes.

### Open Question 4
- **Question**: Can ESM-LogME rankings transfer effectively across different language models?
- **Basis in paper**: [explicit] The authors note that ESMs are specific to a base language model and users might need to compute target embeddings with several base models.
- **Why unresolved**: The paper does not investigate whether ESM-LogME rankings created for one model can be applied to another model.
- **What evidence would resolve it**: Experiments comparing ESM-LogME rankings from one base model (e.g., BERT) against actual transfer performance using another model (e.g., RoBERTa).

## Limitations

- The linear approximation may not capture complex, non-linear transformations needed for certain task pairs, potentially limiting ranking accuracy.
- The efficiency gains depend on pre-computing ESMs for all source tasks, which requires significant upfront computation that may not be worthwhile for smaller source pools.
- Results are based on BERT as the base model and may not generalize to other language models or task distributions not represented in the 1553 source datasets.

## Confidence

- **ESM Approximation Capability**: Medium confidence - Supported by competitive ranking performance but lacks direct analysis of approximation quality
- **Computational Efficiency Gains**: High confidence - Clear empirical evidence with specific metrics (10x speedup, 278x space reduction)
- **Overall Method Effectiveness**: Medium-High confidence - Strong results across 12k source-target pairs, though generalization remains uncertain
- **Parameter Efficiency Claims**: High confidence - The method demonstrably uses fewer parameters than alternatives requiring full fine-tuning

## Next Checks

1. **Approximation Quality Analysis**: Conduct a direct comparison between ESM-transformed embeddings and actual fine-tuned model embeddings using similarity metrics (cosine similarity, MSE) across multiple task pairs. This would validate whether the linear approximation is truly capturing the relevant transformation.

2. **Cross-Model Generalization Test**: Apply ESM-LogME using a different base model (e.g., RoBERTa or T5-small) and compare performance against the BERT-based results. This would test whether the method's effectiveness depends on the specific architecture used.

3. **Efficiency Break-Even Analysis**: Systematically vary the number of source tasks and measure when the pre-computation overhead of ESMs becomes worthwhile. Determine the minimum source pool size needed to achieve the claimed efficiency benefits in practical scenarios.