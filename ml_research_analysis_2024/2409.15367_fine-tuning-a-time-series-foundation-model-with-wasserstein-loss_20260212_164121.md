---
ver: rpa2
title: Fine-Tuning a Time Series Foundation Model with Wasserstein Loss
arxiv_id: '2409.15367'
source_url: https://arxiv.org/abs/2409.15367
tags:
- loss
- time
- series
- wasserstein
- forecast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of adapting large language model
  (LLM) architectures to time series forecasting, where traditional cross-entropy
  loss is suboptimal due to its disregard for distances between classes in continuous
  domains. The proposed solution is to replace cross-entropy loss with Wasserstein
  loss, which incorporates the topology of the space and better accounts for the distances
  between predicted tokens.
---

# Fine-Tuning a Time Series Foundation Model with Wasserstein Loss

## Quick Facts
- arXiv ID: 2409.15367
- Source URL: https://arxiv.org/abs/2409.15367
- Reference count: 40
- The paper shows Wasserstein loss significantly improves point estimation (MASE) in time series forecasting compared to cross-entropy loss, with up to 73% relative improvement, though at the cost of quantile accuracy.

## Executive Summary
This paper addresses the challenge of adapting large language model architectures to time series forecasting by replacing traditional cross-entropy loss with Wasserstein loss. The proposed approach incorporates the topology of the continuous space, leading to significantly improved point estimation while sacrificing some probabilistic accuracy. The method is validated through fine-tuning a pretrained time series foundation model (Chronos-T5 Small) on 22 zero-shot datasets, demonstrating that Wasserstein loss is better suited for continuous domains where distances between values matter.

## Method Summary
The method involves fine-tuning a pretrained Chronos-T5 Small model using Wasserstein loss instead of cross-entropy. Time series are preprocessed with mean absolute scaling and quantized into 4094 tokens with centroids ranging from -15 to 15. The model is fine-tuned for 1000 steps with learning rate 0.001 linearly decreasing to 0, using three loss functions: Wasserstein-1 (p=1), Wasserstein-2 (p=2), and cross-entropy. The Wasserstein loss is computed using a closed-form formula that accounts for distances between token centroids. Evaluation uses MASE for point estimation and WQL for probabilistic forecasting across nine quantile levels.

## Key Results
- Wasserstein-1 loss achieves the best MASE improvement (up to 73% relative improvement over cross-entropy)
- All Wasserstein variants significantly outperform cross-entropy for point estimation
- Wasserstein loss degrades WQL performance due to sharper predicted distributions
- Wasserstein-2 performs similarly to Wasserstein-1 but with slightly worse MASE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wasserstein loss improves point estimation because it penalizes large errors more heavily than cross-entropy, reflecting the continuous nature of time series.
- Mechanism: Cross-entropy treats each token as a distinct class with equal misclassification penalty, while Wasserstein loss incorporates distances between token centroids, penalizing distant misclassifications more heavily.
- Core assumption: The quantization grid is uniform with constant distance between consecutive centroids.
- Evidence anchors:
  - [abstract] "cross-entropy loss is primarily designed for classification tasks and does not account for the distance between classes"
  - [section 3.3] "We define the distance between two tokens as the distance between their centroids: D(yi, yj) = r · |i − j|"
- Break condition: Non-uniform quantization grid or non-monotonic token-to-value mapping breaks the closed-form solution.

### Mechanism 2
- Claim: Replacing cross-entropy with Wasserstein loss sharpens the predicted distribution, improving point estimates but degrading probabilistic accuracy.
- Mechanism: Wasserstein loss treats the target as a degenerate distribution, causing the model to concentrate probability mass around the true value, reducing expected error for point forecasts but producing distributions too peaked for reliable quantile estimation.
- Core assumption: The model outputs a softmax distribution over tokens, and loss is computed between this distribution and a degenerate target.
- Evidence anchors:
  - [abstract] "Wasserstein loss significantly improves point estimation... However, Wasserstein loss leads to a degradation in quantile loss (WQL)"
  - [section 3.3] "we model the target distribution as a degenerate random variable"
- Break condition: When accurate probabilistic forecasts are required for risk assessment.

### Mechanism 3
- Claim: Using Wasserstein loss enables direct application of LLM architectures to time series by preserving the geometry of the continuous domain.
- Mechanism: While tokenization allows LLM architectures to process time series with standard cross-entropy training, this loses the metric structure. Wasserstein loss re-introduces continuous geometry by weighting misclassification penalties by token distance.
- Core assumption: The quantization is fine enough that centroid distance approximates true value distance, and the token space is one-dimensional.
- Evidence anchors:
  - [section 3.1] "tokenization function... constructs a uniform grid from ymin to ymax"
  - [section 3.3] "the Wasserstein distance needed to be calculated between a degenerate distribution and a discrete distribution"
- Break condition: If quantization is too coarse or the token space is high-dimensional.

## Foundational Learning

- Concept: Wasserstein distance as an optimal transport metric
  - Why needed here: Understanding that Wasserstein loss accounts for the geometry of the underlying space is key to grasping why it outperforms cross-entropy in continuous domains.
  - Quick check question: In a uniform grid of tokens, if token 5 is the true value and the model predicts token 7 with probability 0.8 and token 3 with probability 0.2, which token contributes more to the Wasserstein loss? (Answer: Token 7, because it is farther from the true token.)

- Concept: Tokenization and quantization of time series
  - Why needed here: The model relies on discretizing continuous values into tokens; the choice of grid spacing and bounds directly affects the Wasserstein loss computation.
  - Quick check question: If the scaling factor s=2 and a predicted token has centroid value 1.5, what is the corresponding time series value? (Answer: 1.5 * 2 = 3.0)

- Concept: Autoregressive sampling in sequence models
  - Why needed here: The forecasting procedure generates future values step-by-step, feeding each prediction back as input for the next step.
  - Quick check question: If the model predicts the next token with distribution [0.1, 0.7, 0.2] over three tokens, what is the most likely next value in the time series? (Answer: The centroid of token 2, since it has the highest probability.)

## Architecture Onboarding

- Component map:
  Tokenizer (mean absolute scaling + uniform quantization into 4094 bins) -> Model (Chronos-T5 Small transformer) -> Loss (Wasserstein or cross-entropy) -> Inference (autoregressive sampling with detokenization)

- Critical path:
  1. Load and preprocess time series (scale, tokenize)
  2. Forward pass through T5 to get token probabilities
  3. Compute Wasserstein loss against ground truth token
  4. Backpropagate and update model parameters
  5. For inference, sample tokens autoregressively and detokenize

- Design tradeoffs:
  - Quantization granularity vs. model complexity: finer grids improve fidelity but increase token vocabulary size
  - Choice of p in Wasserstein loss: p=1 (MAE-like) vs. p=2 (MSE-like) affects sensitivity to outliers
  - Degenerate vs. spread target distribution: sharper targets improve point estimates but hurt probabilistic calibration

- Failure signatures:
  - Poor MASE but good WQL: likely overfitting to cross-entropy, distribution too diffuse
  - Good MASE but poor WQL: distribution too sharp, may miss uncertainty
  - Training instability: learning rate too high for Wasserstein loss, or grid too coarse causing large jumps in loss

- First 3 experiments:
  1. Fine-tune with Wasserstein-1 loss on a small dataset, compare MASE to cross-entropy baseline
  2. Visualize predicted vs. true token distributions for a few test series to check sharpness
  3. Sweep p in Wasserstein loss (p=1, p=2) and observe effect on point vs. quantile accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does training a foundational time series model from scratch using Wasserstein loss compare to fine-tuning in terms of point estimation accuracy and computational efficiency?
- Basis in paper: [explicit] The authors mention that training foundational models from scratch with Wasserstein loss is a key area for future work, noting that fine-tuning is more efficient for industrial applications but may not achieve optimal performance.
- Why unresolved: The paper only explores fine-tuning a pretrained model with Wasserstein loss, not training from scratch. The computational cost and potential performance gains of training from scratch remain unknown.
- What evidence would resolve it: Direct comparison of MASE and WQL metrics between a model trained from scratch with Wasserstein loss versus fine-tuned models on the same zero-shot datasets, along with computational resource analysis.

### Open Question 2
- Question: Can sampling techniques like Monte Carlo Dropout effectively improve quantile loss when using Wasserstein loss by capturing uncertainty in sharper predicted distributions?
- Basis in paper: [inferred] The authors observe that Wasserstein loss leads to sharper distributions that perform worse on WQL metrics, but suggest that incorporating uncertainty quantification techniques could potentially enhance distribution forecasts.
- Why unresolved: The paper does not experimentally test uncertainty quantification methods with Wasserstein-trained models, leaving open whether these techniques can mitigate the degradation in quantile loss.
- What evidence would resolve it: Comparative WQL results between Wasserstein-trained models with and without Monte Carlo Dropout or other uncertainty quantification techniques across multiple datasets.

### Open Question 3
- Question: How do absolute error and squared error losses compare to Wasserstein loss for tokenized time series forecasting in terms of point estimation accuracy and computational efficiency?
- Basis in paper: [explicit] The authors note that absolute error (AE) and squared error (SE) serve as lower bounds for Wasserstein losses and suggest exploring these losses as a promising direction for future research.
- Why unresolved: The paper does not conduct experiments using AE and SE losses, so their performance relative to Wasserstein loss on the same tasks is unknown.
- What evidence would resolve it: Direct comparison of MASE scores across datasets when using Wasserstein-1 loss, Wasserstein-2 loss, absolute error loss, and squared error loss in identical experimental conditions.

## Limitations

- The results are based on a single model (Chronos-T5 Small) and specific tokenization scheme, limiting generalizability to other architectures or quantization strategies.
- The choice of grid bounds (-15 to 15) and spacing is not derived from dataset statistics, which may limit performance on datasets with different value ranges.
- The degradation in quantile performance is acknowledged but not fully explained—it's unclear whether this is an inherent tradeoff of Wasserstein loss or a consequence of the degenerate target modeling.

## Confidence

- **High confidence**: Wasserstein loss outperforms cross-entropy for point estimation (MASE) on the tested datasets. The mechanism—that Wasserstein incorporates token distance—is mathematically sound and directly supported by the formulation.
- **Medium confidence**: The tradeoff between point accuracy and quantile calibration is real but may depend on the choice of degenerate target distribution and grid design. Sharper distributions may be preferable in some applications despite worse WQL.
- **Low confidence**: Claims about the broader applicability of Wasserstein loss to other foundation models or tokenization schemes. The analysis is limited to one model, one tokenization, and one grid design.

## Next Checks

1. **Grid sensitivity**: Repeat the experiments with different grid bounds and spacing (e.g., derived from training set percentiles) to assess robustness to the choice of quantization.
2. **Target distribution ablation**: Compare degenerate targets to smoothed (e.g., Gaussian-smoothed) targets to see if the quantile loss degradation can be mitigated without sacrificing MASE gains.
3. **Dataset generalization**: Test the method on datasets with significantly different value ranges or distributional properties (e.g., heavy-tailed or highly volatile series) to evaluate out-of-distribution robustness.