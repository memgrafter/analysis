---
ver: rpa2
title: 'RoMath: A Mathematical Reasoning Benchmark in Romanian'
arxiv_id: '2409.11074'
source_url: https://arxiv.org/abs/2409.11074
tags:
- problems
- solution
- problem
- arxiv
- romanian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RoMath, a Romanian mathematical reasoning
  benchmark suite consisting of three subsets: Baccalaureate, Competitions, and Synthetic.
  The benchmark addresses the lack of mathematical reasoning datasets in non-English
  languages by providing 76,910 problems across various mathematical domains and difficulty
  levels.'
---

# RoMath: A Mathematical Reasoning Benchmark in Romanian

## Quick Facts
- arXiv ID: 2409.11074
- Source URL: https://arxiv.org/abs/2409.11074
- Authors: Adrian Cosma; Ana-Maria Bucur; Emilian Radoi
- Reference count: 40
- Primary result: Romanian-specialized models perform competitively with English-centric models on Romanian mathematical reasoning tasks

## Executive Summary
This paper introduces RoMath, a Romanian mathematical reasoning benchmark suite addressing the lack of mathematical reasoning datasets in non-English languages. The benchmark consists of 76,910 problems across three subsets: Baccalaureate (5,777 problems), Competitions (1,133 problems), and Synthetic (63,000 problems). The authors demonstrate that Romanian-specialized models achieve competitive performance with English-centric models despite limited math-specific training data, while also showing that direct translation of mathematical problems degrades performance due to challenges in preserving precise mathematical language. The work highlights the importance of dedicated resources for underrepresented languages in mathematical reasoning.

## Method Summary
RoMath was created through a semi-automatic workflow that collected problems from Romanian baccalaureate exams, competitions, and programmatically generated synthetic problems. The authors benchmarked several open-weight language models under zero-shot, fine-tuning, and verifiable reward scenarios. A key component was using LLM-as-a-judge for solution verification, though the authors acknowledge this approach may artificially inflate performance scores. The benchmark evaluates models across various mathematical domains including algebra, geometry, and calculus, providing a comprehensive assessment of mathematical reasoning capabilities in Romanian.

## Key Results
- Romanian-specialized models perform competitively with English-centric models on Romanian mathematical reasoning tasks
- Fine-tuning improves performance only when training data matches the target problem format and style
- Direct translation of Romanian mathematical problems to English degrades performance due to loss of precise mathematical language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Romanian language models achieve competitive performance on Romanian mathematical reasoning despite limited math-specific training data
- Mechanism: Romanian-specialized models leverage broader Romanian language understanding to compensate for sparse mathematical token exposure, enabling them to process mathematical syntax and terminology effectively
- Core assumption: Mathematical reasoning ability is transferable across languages, and strong general language understanding facilitates mathematical comprehension
- Evidence anchors:
  - [abstract] "Romanian-specialized models perform competitively with English-centric models despite limited training on Romanian math tokens"
  - [section 4.2] "Surprisingly, we found that mathematics problems written in Romanian can be properly handled by English-centric models"
  - [corpus] Weak evidence - no direct corpus support for this mechanism found
- Break condition: When mathematical problems require deep integration of specialized mathematical concepts that differ significantly from general language patterns

### Mechanism 2
- Claim: Translation from Romanian to English degrades mathematical reasoning performance due to loss of precise mathematical language
- Mechanism: Mathematical expressions and precise terminology cannot be adequately preserved during translation, leading to information loss and context disruption
- Core assumption: Mathematical language has unique characteristics that are not fully captured by general-purpose translation models
- Evidence anchors:
  - [abstract] "simple translation of problem statements is not enough, as sub-par translations of precise mathematical language significantly reduces performance"
  - [section 4.5] "Translating mathematics is challenging due to the need for precise language, as even slight ambiguities can alter meaning"
  - [corpus] Weak evidence - corpus mentions translation challenges but lacks specific mathematical examples
- Break condition: When translation models are specifically trained on mathematical corpora or when mathematical expressions are kept intact during translation

### Mechanism 3
- Claim: Fine-tuning improves mathematical reasoning performance only when training data matches the target problem format and style
- Mechanism: Models learn to adapt to specific problem presentation formats and solution styles present in the training data, leading to better performance on similar test data
- Core assumption: Mathematical reasoning performance depends on familiarity with problem format rather than just mathematical knowledge
- Evidence anchors:
  - [section 4.2] "Surprisingly, we obtained that fine-tuning does not always result in improved performance" and "One possible explanation is that the solutions present in RoMath are qualitatively different than solutions present in other math datasets"
  - [section 4.3] "Fine-tuning improves performance on Baccalaureate for Qwen2-7b and Qwen2-Math-7b"
  - [corpus] Moderate evidence - corpus mentions RoMath but lacks specific performance data for different fine-tuning approaches
- Break condition: When training data format matches test data format, or when models are evaluated on problems requiring generalization beyond training format

## Foundational Learning

- Concept: Mathematical reasoning vs natural language processing
  - Why needed here: Understanding that mathematical reasoning involves different cognitive processes than general NLP tasks, requiring specialized evaluation approaches
  - Quick check question: Can you identify three key differences between mathematical reasoning and standard NLP tasks?

- Concept: Chain-of-thought reasoning and intermediate steps
  - Why needed here: Many mathematical problems require multiple reasoning steps, and the presence of intermediate steps significantly impacts model performance
  - Quick check question: Why does the presence of intermediate reasoning steps in solutions enable better model performance on mathematical problems?

- Concept: Mathematical notation and LaTeX processing
  - Why needed here: Mathematical problems often contain complex notation that requires proper parsing and understanding for correct solutions
  - Quick check question: What challenges arise when mathematical expressions are embedded in natural language text?

## Architecture Onboarding

- Component map: OCR pipeline -> LLM parsing -> JSON structuring -> Judge evaluation -> Performance analysis
- Critical path: Problem collection -> Structured formatting -> Model evaluation -> Results analysis
- Design tradeoffs: Balance between automated processing (OCR, LLM parsing) and quality control (manual inspection, validation)
- Failure signatures: Poor OCR quality leading to incorrect LaTeX parsing, LLM parsing errors creating malformed JSON, judge evaluation bias affecting results
- First 3 experiments:
  1. Test OCR pipeline on sample PDFs with varying quality to establish accuracy baseline
  2. Evaluate LLM parsing performance on different problem types and complexity levels
  3. Compare judge model performance across different problem categories and solution types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance on RoMath compare to English math benchmarks when both are translated to Romanian?
- Basis in paper: [explicit] Authors note that direct translation of problems degrades performance due to challenges in translating precise mathematical language
- Why unresolved: The paper only tests English-to-Romanian translation degradation, not Romanian-to-English translation comparison
- What evidence would resolve it: Benchmark results showing Romanian model performance on both RoMath and translated English benchmarks

### Open Question 2
- Question: What specific linguistic features of Romanian make mathematical reasoning uniquely challenging compared to other languages?
- Basis in paper: [explicit] Authors highlight Romanian's "unique linguistic features" and status as a "low-resource language with unique linguistic particularities"
- Why unresolved: Paper identifies Romanian's uniqueness but doesn't specify which linguistic features create mathematical reasoning challenges
- What evidence would resolve it: Comparative analysis of Romanian math problem structures versus other languages, identifying specific linguistic hurdles

### Open Question 3
- Question: Why does fine-tuning not reliably improve performance across all RoMath subsets and model types?
- Basis in paper: [explicit] Authors observe that "fine-tuning does not always result in improved performance" and suggest solutions may be "qualitatively different"
- Why unresolved: Paper proposes potential explanations but doesn't investigate the underlying reasons for inconsistent fine-tuning effects
- What evidence would resolve it: Detailed analysis of solution formatting differences and their impact on fine-tuning effectiveness across subsets

## Limitations
- Heavy reliance on English-centric models for evaluation may not fully capture Romanian mathematical reasoning nuances
- Challenges in establishing ground truth for mathematical solutions due to variable judge model performance
- Translation experiments reveal inherent limitations in cross-lingual mathematical reasoning benchmarks

## Confidence

**High Confidence:** Romanian-specialized models perform competitively with English-centric models on Romanian mathematical reasoning tasks; translation degrades performance due to mathematical language precision requirements.

**Medium Confidence:** Fine-tuning improves performance only when training data matches target format; dedicated resources are needed for underrepresented languages in mathematical reasoning.

**Low Confidence:** Generalizability of findings to other non-English languages; effectiveness of synthetic data generation for mathematical reasoning tasks.

## Next Checks
1. **Cross-Lingual Transfer Validation:** Conduct controlled experiments comparing mathematical reasoning performance across multiple non-English languages (e.g., Romanian, Spanish, Arabic) to validate whether the observed competitive performance of native language models generalizes beyond Romanian.

2. **Judge Model Calibration Study:** Perform systematic analysis of judge model reliability by having multiple judge models evaluate the same solutions and measuring inter-rater agreement, particularly focusing on identifying and quantifying potential inflation of performance scores.

3. **Translation Quality Impact Analysis:** Design experiments that isolate the effect of mathematical terminology translation quality by comparing model performance on problems with professionally translated mathematical content versus machine-translated content, to better understand the specific impact of translation precision on reasoning performance.