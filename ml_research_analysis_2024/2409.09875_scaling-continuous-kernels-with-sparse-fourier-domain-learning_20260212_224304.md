---
ver: rpa2
title: Scaling Continuous Kernels with Sparse Fourier Domain Learning
arxiv_id: '2409.09875'
source_url: https://arxiv.org/abs/2409.09875
tags:
- kernel
- fourier
- training
- domain
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses three key challenges in learning continuous
  kernel representations: computational efficiency, parameter efficiency, and spectral
  bias. The authors propose Continuous Fourier Convolutions (CF-Convs), a method that
  learns continuous kernels in the Fourier domain using sparse updates.'
---

# Scaling Continuous Kernels with Sparse Fourier Domain Learning

## Quick Facts
- arXiv ID: 2409.09875
- Source URL: https://arxiv.org/abs/2409.09875
- Reference count: 27
- Primary result: CF-Convs achieve 85.30% accuracy on Cats vs. Dogs using only 59K parameters, comparable to traditional 3x3 CNN baselines

## Executive Summary
This paper addresses three key challenges in learning continuous kernel representations: computational efficiency, parameter efficiency, and spectral bias. The authors propose Continuous Fourier Convolutions (CF-Convs), a method that learns continuous kernels in the Fourier domain using sparse updates. By leveraging sparse evaluations and exponential moving averages, CF-Convs significantly reduce memory consumption and training time while maintaining expressiveness. The approach mitigates spectral bias by learning in the frequency domain, enabling the capture of high-frequency details that traditional methods struggle with. Experiments on the Cats vs. Dogs dataset show that CF-Convs achieve 85.30% accuracy using only 59K parameters, comparable to traditional 3x3 CNN baselines while offering the flexibility to learn arbitrarily sized kernels.

## Method Summary
The method learns continuous kernels by parameterizing them as functions ΦΘ(H, W, Cin, Cout) that map to the Fourier domain. Instead of computing gradients for all kernel positions, CF-Convs uses sparse sampling—evaluating only a subset of positions uniformly at random during each training step. An exponential moving average (EMA) updates the kernel at these selected positions while unselected positions retain their previous values. The learned kernel is applied in the Fourier domain via pointwise multiplication after FFT, then converted back to the spatial domain using inverse FFT. This approach reduces memory consumption from storing the full kernel and mitigates spectral bias by learning in the frequency domain where low-frequency functions correspond to high-pass spatial filters.

## Key Results
- CF-Convs achieve 85.30% accuracy on Cats vs. Dogs binary classification using only 59K parameters
- Outperforms traditional 3x3 CNN baselines with equivalent parameter counts
- Successfully mitigates spectral bias through frequency domain learning
- Reduces memory consumption by evaluating only 218 kernel positions per forward pass (vs. full H×W×Cin×Cout)

## Why This Works (Mechanism)

### Mechanism 1: Spectral Bias Mitigation via Fourier Domain Learning
Learning in the Fourier domain mitigates spectral bias by allowing low-frequency functions to capture high-frequency spatial details via the Gabor limit. Spectral bias causes neural networks to underfit high-frequency components in the spatial domain. By learning in the Fourier domain, the method exploits the reciprocal relationship between domains—smooth (low-frequency) functions in the Fourier domain correspond to high-pass filters in the spatial domain.

### Mechanism 2: Memory-Efficient Sparse Updates
Sparse Fourier domain updates drastically reduce memory and computation by evaluating only a subset of kernel positions during training. Instead of computing gradients for all H×W×Cin×Cout positions, the method samples a subset of positions uniformly at randomly at each step. An exponential moving average (EMA) updates the kernel at these selected positions, while unselected positions are retained from previous states.

### Mechanism 3: Parameter-Efficient MLP Architecture
The ΦΘ(H, W, Cin, Cout) parameterization balances expressiveness and parameter efficiency by using a single MLP conditioned on all axes. Rather than using multiple MLPs (which would increase parameter count), a single MLP generates the entire kernel by querying it at H×W×Cin×Cout positions. This allows the kernel to adapt to both spatial and channel dimensions without parameter explosion.

## Foundational Learning

- Concept: Spectral bias in neural networks
  - Why needed here: Understanding why standard MLPs struggle with high-frequency components is essential to grasping the motivation for Fourier domain learning
  - Quick check question: Why do MLPs tend to learn low-frequency functions first, and how does this affect image reconstruction tasks?

- Concept: Fourier transforms and convolution theorem
  - Why needed here: The method relies on transforming between spatial and frequency domains to perform efficient convolution via pointwise multiplication
  - Quick check question: How does convolution in the spatial domain relate to multiplication in the Fourier domain, and why is this computationally advantageous?

- Concept: Continuous kernel representations
  - Why needed here: The approach generates convolutional kernels by sampling from a continuous function (MLP) rather than learning discrete weights
  - Quick check question: What are the trade-offs between continuous kernel representations and traditional discrete convolutional kernels in terms of flexibility and computational cost?

## Architecture Onboarding

- Component map: Input → FFT → CF-Conv (sparse evaluation + EMA update) → IFFT → Activation → Output
- Critical path: Input → FFT → CF-Conv (sparse evaluation + EMA update) → IFFT → Activation → Output
- Design tradeoffs:
  - Parameter efficiency vs. expressiveness: Single MLP vs. multiple specialized MLPs
  - Memory efficiency vs. training speed: Sparse updates reduce memory but may slow convergence
  - Domain choice: Learning in Fourier domain mitigates spectral bias but requires complex-valued operations
- Failure signatures:
  - Memory exhaustion: Model fails to fit in GPU memory even with sparse updates
  - Poor convergence: Training accuracy plateaus early or fluctuates wildly
  - Frequency artifacts: Output images show ringing or high-frequency noise patterns
- First 3 experiments:
  1. Baseline comparison: Run CF-Convs with sparse updates vs. traditional 3×3 CNN on Cats vs. Dogs with identical parameter counts
  2. Sparse sampling ablation: Test different numbers of selected positions (212, 215, 218, 221) to find optimal trade-off between memory and accuracy
  3. Parameterization comparison: Compare ΦΘ(H, W, Cin, Cout) with alternative parameterizations (ΦΘ(H, W), ΦΘ(H, W, Cin)) on same task

## Open Questions the Paper Calls Out

- Question: What is the optimal number of sparse evaluation points for different image resolutions and kernel sizes in CF-Convs?
  - Basis in paper: The authors experimentally tested 212, 215, and 218 selected positions and found 218 to yield the best performance
  - Why unresolved: The paper only tested these three specific values on one dataset and architecture, without exploring the full parameter space or considering how this might vary with different resolutions and kernel sizes
  - What evidence would resolve it: Systematic experiments varying image resolution, kernel size, and selected points across multiple datasets, establishing scaling laws or heuristics for optimal sparse sampling

- Question: Can complex-valued activation functions be designed that would eliminate the need for inverse FFT transforms in CF-Convs?
  - Basis in paper: The authors note that "improved complex-valued activation functions could provide an alternative solution" to their current approach of using inverse FFT
  - Why unresolved: The paper acknowledges this as a potential direction but does not explore or implement such activation functions
  - What evidence would resolve it: Development and experimental validation of complex-valued activation functions that perform as well as or better than the current spatial-domain approach

- Question: How does the performance gap between CF-Convs and traditional 3x3 CNNs change with larger models and more complex datasets?
  - Basis in paper: The authors acknowledge that CF-Convs "still lag behind 3x3 spatial CNNs" but suggest this might require "more complex MLP architectures or further optimization"
  - Why unresolved: The paper only tests a relatively small 6-layer CNN on a binary classification task, leaving open questions about scaling to larger models and more complex tasks
  - What evidence would resolve it: Comparative experiments on larger architectures (deeper networks, more filters) and complex datasets (multi-class classification, object detection) to determine if the performance gap narrows or widens

## Limitations

- The method's effectiveness on high-resolution images and complex multi-class classification tasks remains untested
- Sparse sampling introduces a trade-off between memory efficiency and training quality that lacks systematic characterization
- The Gabor limit mechanism for spectral bias mitigation relies on empirical validation that may not generalize to all kernel learning scenarios

## Confidence

- **High Confidence**: The computational efficiency improvements through sparse Fourier domain updates are well-supported by the implementation details and mathematical framework presented
- **Medium Confidence**: The parameter efficiency claims are credible given the comparisons with traditional CNN baselines, though broader validation across different network architectures would strengthen this claim
- **Low Confidence**: The spectral bias mitigation mechanism's effectiveness in diverse scenarios beyond the presented experimental setup remains uncertain

## Next Checks

1. **Scale-up validation**: Test CF-Convs on CIFAR-100 or ImageNet-10 to verify parameter efficiency and spectral bias mitigation hold for multi-class classification with larger datasets
2. **Sparse sampling sensitivity**: Systematically vary the number of selected positions (212, 215, 218, 221) and measure the trade-off between GPU memory usage, training time, and final accuracy to establish optimal configurations
3. **Cross-architecture comparison**: Implement CF-Convs in a transformer-based architecture to evaluate whether the Fourier domain learning advantages transfer beyond CNN structures