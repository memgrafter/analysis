---
ver: rpa2
title: 'BIGCity: A Universal Spatiotemporal Model for Unified Trajectory and Traffic
  State Data Analysis'
arxiv_id: '2412.00953'
source_url: https://arxiv.org/abs/2412.00953
tags:
- data
- traffic
- tasks
- trajectory
- bigcity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BIGCity, the first multi-task, multi-data
  modality (MTMD) model for spatiotemporal (ST) data analysis, capable of jointly
  handling trajectory and traffic state data across diverse tasks. The key challenges
  addressed are unifying heterogeneous ST data representations and heterogeneous analysis
  tasks.
---

# BIGCity: A Universal Spatiotemporal Model for Unified Trajectory and Traffic State Data Analysis

## Quick Facts
- arXiv ID: 2412.00953
- Source URL: https://arxiv.org/abs/2412.00953
- Reference count: 40
- BIGCity outperforms 18 baselines across 8 tasks on three real-world datasets

## Executive Summary
BIGCity introduces the first multi-task, multi-data modality (MTMD) model for spatiotemporal (ST) data analysis that jointly handles trajectory and traffic state data. The model addresses key challenges in unifying heterogeneous ST data representations and diverse analysis tasks through ST-units and task-oriented prompts. By combining self-supervised masked reconstruction with prompt-based fine-tuning, BIGCity achieves state-of-the-art performance across diverse tasks including trajectory prediction, travel time estimation, and traffic state forecasting. The model demonstrates strong generalization and efficiency, maintaining performance within 7% degradation when transferring across datasets.

## Method Summary
BIGCity introduces ST-units to unify trajectory and traffic state data by representing them as sequences of road segment-traffic state-timestamp triples. The model employs a two-stage training strategy: masked reconstruction for learning general spatiotemporal dependencies, followed by task-oriented prompt tuning to handle specific tasks. A tunable large language model with LoRA modules serves as the backbone, while task-oriented prompts provide textual instructions and expected output formats to guide task execution without fine-tuning. The model processes ST-units through an ST tokenizer, LLM backbone, and general-task heads to produce task-specific outputs.

## Key Results
- Outperforms 18 baselines across 8 spatiotemporal analysis tasks
- Achieves state-of-the-art performance on three real-world datasets (Beijing, Xi'an, Chengdu)
- Maintains cross-dataset transfer performance within 7% degradation
- Demonstrates strong efficiency and scalability compared to task-specific models

## Why This Works (Mechanism)

### Mechanism 1
- ST-units provide unified representation by encapsulating road segment, traffic state, and timestamp into triples
- Eliminates need for separate data structures enabling joint processing of trajectory and traffic state data
- Assumes triple representation captures essential information for both individual and population mobility patterns

### Mechanism 2
- Task-oriented prompts guide LLM to perform diverse tasks without fine-tuning through textual instructions
- Combines textual instructions, input data, and task placeholders to direct model behavior
- Assumes LLMs can accurately interpret textual instructions and apply them to ST data sequences

### Mechanism 3
- Two-stage training enables learning general ST dependencies then specializing for specific tasks
- Masked reconstruction stage learns general patterns, prompt tuning adapts to specific tasks
- Assumes general patterns transfer effectively to specific tasks through prompt-based specialization

## Foundational Learning

- Graph Neural Networks (GNNs) for spatial dependency modeling
  - Why needed here: Capture spatial relationships among road segments in road network
  - Quick check question: How does GAT differ from traditional graph convolution in handling road network data?

- Self-supervised learning via masked reconstruction
  - Why needed here: Learn general ST dependencies without labeled data in stage 1 training
  - Quick check question: How does masked reconstruction in ST data differ from masked language modeling in NLP?

- Prompt engineering and in-context learning
  - Why needed here: Guide LLM backbone to perform diverse tasks through textual instructions
  - Quick check question: What are key components of task-oriented prompts and how do they influence model behavior?

## Architecture Onboarding

- Component map: ST units → ST tokenizer → LLM backbone → General-task heads → Task output
- Critical path: ST units flow through tokenizer, backbone, and heads to produce task-specific results
- Design tradeoffs:
  - LoRA reduces computational cost but may limit capacity vs full fine-tuning
  - Unified ST-units simplify processing but may lose modality-specific nuances
  - Prompt-based guidance offers flexibility but relies heavily on prompt quality
- Failure signatures:
  - Poor task performance indicates inadequate prompt engineering or LoRA adaptation
  - Slow inference suggests inefficient tokenizer or excessive LoRA parameters
  - Generalization issues stem from insufficient reconstruction training or task-specific overfitting
- First 3 experiments:
  1. Test ST tokenizer with synthetic ST-unit sequences to verify unified representation
  2. Evaluate LLM backbone with task-oriented prompts on single task to validate prompt interpretation
  3. Assess two-stage training by comparing performance with and without masked reconstruction pretraining

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Impact of incorporating additional spatial elements like POIs and grids alongside road segments
- Basis in paper: Paper mentions future work could involve incorporating POIs and grids
- Why unresolved: Paper doesn't explore performance implications of extending ST-unit representation
- What evidence would resolve it: Experimental results comparing performance with and without POIs/grids

### Open Question 2
- Question: How cross-dataset generalization varies across different urban environments and traffic patterns
- Basis in paper: Paper demonstrates 7% degradation in cross-dataset transfer but doesn't analyze urban environment impact
- Why unresolved: Experiments focus on limited cities without investigating varying characteristics
- What evidence would resolve it: Experiments on diverse cities with different urban characteristics

### Open Question 3
- Question: Optimal balance between LoRA module quantity and matrix rank for different tasks and datasets
- Basis in paper: Paper conducts parameter sensitivity analysis but doesn't explore task-specific optimal configurations
- Why unresolved: Analysis provides general insights but doesn't address task-specific tuning
- What evidence would resolve it: Task-specific and dataset-specific experiments to determine optimal LoRA configurations

## Limitations

- Limited validation to three specific urban datasets, potentially restricting generalizability to non-urban or different traffic environments
- Reliance on prompt quality for task execution, where ambiguous or poorly constructed prompts could significantly impact performance
- Two-stage training strategy effectiveness depends on general patterns learned in stage 1 transferring effectively to stage 2 tasks

## Confidence

- Universal applicability: Medium - Strong performance across diverse tasks but validated only on specific urban datasets
- ST-unit representation completeness: Medium - Experimental results support effectiveness but assumes triple captures all necessary information
- Prompt-based task handling: Medium - Demonstrates effectiveness across 8 tasks but quality impacts performance
- Two-stage training strategy: Medium - Shows improved performance vs single-stage but limited hyperparameter sensitivity exploration

## Next Checks

1. Conduct cross-dataset transfer experiments using non-urban road networks to validate ST-unit representation effectiveness beyond city environments
2. Perform ablation studies varying prompt specificity and format to quantify impact on task performance
3. Test model's robustness to missing or sparse traffic state data to verify ST-unit approach maintains effectiveness under data quality variations