---
ver: rpa2
title: 'Mixture-of-Instructions: Aligning Large Language Models via Mixture Prompting'
arxiv_id: '2404.18410'
source_url: https://arxiv.org/abs/2404.18410
tags:
- arxiv
- language
- code
- system
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture-of-Instructions (MoI), a method that
  addresses overfitting to system prompts in large language models (LLMs) by employing
  diverse system prompts for different tasks and packing them with balanced sampling.
  The approach improves alignment efficiency and performance across multiple tasks
  including code generation, mathematics, and tool usage.
---

# Mixture-of-Instructions: Aligning Large Language Models via Mixture Prompting

## Quick Facts
- arXiv ID: 2404.18410
- Source URL: https://arxiv.org/abs/2404.18410
- Authors: Bowen Xu; Shaoyu Wu; Kai Liu; Lulu Hu
- Reference count: 30
- One-line primary result: Mixture-of-Instructions (MoI) improves multi-task alignment of LLMs by addressing system prompt overfitting through diverse prompts and balanced sampling

## Executive Summary
This paper introduces Mixture-of-Instructions (MoI), a method that addresses overfitting to system prompts in large language models (LLMs) by employing diverse system prompts for different tasks and packing them with balanced sampling. The approach improves alignment efficiency and performance across multiple tasks including code generation, mathematics, and tool usage. When applied to the Qwen-7B-chat model, the resulting Qwen-SFT-MoI model demonstrates significant improvements on benchmarks like HumanEval, MBPP, GSM8K, and MATH, achieving state-of-the-art results among open-source models of similar size. The MoI method effectively mitigates dataset bias in multi-task learning while preserving existing capabilities, making it a robust solution for enhancing LLM alignment and versatility.

## Method Summary
The Mixture-of-Instructions (MoI) method addresses system prompt overfitting in LLMs through three key components: (1) using diverse, domain-specific system prompts for different tasks to prevent the model from overfitting to a single prompt style, (2) implementing balanced sampling across multiple tasks during training to prevent dataset bias and ensure consistent performance across all tasks, and (3) employing chunk-based attention masking to prevent cross-contamination between different task instructions while preserving beneficial attention within task-specific chunks. The approach is applied to multi-task alignment of the Qwen-7B-chat model using open-source datasets for dialogue, code generation, mathematics, and tool usage.

## Key Results
- Qwen-SFT-MoI achieves state-of-the-art performance among open-source models of similar size on benchmarks like HumanEval, MBPP, GSM8K, and MATH
- The MoI method effectively mitigates dataset bias in multi-task learning while preserving existing capabilities
- Balanced sampling rapidly improves GSM8K accuracy and stabilizes code generation performance during training

## Why This Works (Mechanism)

### Mechanism 1: System Prompt Overfitting and Attention Realignment
- Claim: Large language models overfit to specific system prompts during training, causing them to prioritize prompt tokens over task-relevant content during inference
- Mechanism: The model develops strong attention weights to system prompt tokens during training, creating a bias that persists during inference and prevents proper task focus
- Core assumption: The model's attention mechanism can be manipulated through training data composition and system prompt diversity
- Evidence anchors:
  - [abstract] "we found that the system prompt has a significant impact on both training and inference processes of LLM"
  - [section 2.1] "we found that simply changing to a different system prompt and retraining could easily overcome this issue"
  - [corpus] Weak evidence - no direct citations found for system prompt overfitting phenomenon
- Break condition: If the model has sufficient diverse training data with varied system prompts, the overfitting effect may be naturally mitigated

### Mechanism 2: Balanced Sampling for Multi-Task Learning
- Claim: Balanced sampling across multiple tasks during training prevents dataset bias and ensures consistent performance across all tasks
- Mechanism: By proportionally sampling from different task datasets and concatenating them into balanced training batches, the model receives equal exposure to all tasks, preventing dominance by any single task
- Core assumption: Equal representation of tasks in training data leads to balanced capability development across all tasks
- Evidence anchors:
  - [section 2.2] "we ensure that the amount of data from different tasks within a single packed instruction is as balanced as possible"
  - [section 2.2] "balanced sampling rapidly improves GSM8K accuracy and stabilizes code generation performance during training"
  - [corpus] Weak evidence - no direct citations found for balanced sampling in multi-task learning
- Break condition: If task complexity varies significantly, simple proportional sampling may not be sufficient for optimal learning

### Mechanism 3: Chunk-Based Attention Masking
- Claim: Chunk-based attention masking prevents cross-contamination between different task instructions while preserving beneficial attention within task-specific chunks
- Mechanism: The model processes each task instruction in isolation by applying attention masks that block cross-task attention, while allowing full attention within each chunk
- Core assumption: Isolating task instructions during training improves the model's ability to learn distinct task-specific patterns
- Evidence anchors:
  - [section 2.3] "we block interference between different chunks while ensuring high similarity between the data in each pair of chunks"
  - [section 2.3] "By using chunk-based attention, the mathematical and coding capabilities derived from packing are preserved"
  - [corpus] Weak evidence - no direct citations found for chunk-based attention masking in multi-task learning
- Break condition: If tasks are highly interdependent, isolating them may prevent the model from learning useful cross-task relationships

## Foundational Learning

- Concept: Attention mechanisms in transformer models
  - Why needed here: Understanding how attention weights are assigned to different tokens is crucial for grasping why system prompts can bias model behavior
  - Quick check question: How does the multi-head attention mechanism in transformers assign weights to input tokens?

- Concept: Multi-task learning and dataset bias
  - Why needed here: The paper addresses the challenge of training a single model on multiple tasks without one task dominating the others
  - Quick check question: What is dataset bias in multi-task learning and how does it affect model performance?

- Concept: Supervised fine-tuning (SFT) in LLM training
  - Why needed here: The paper builds upon standard SFT methodology and introduces modifications to improve multi-task alignment
  - Quick check question: What is the difference between pre-training and supervised fine-tuning in the context of LLM development?

## Architecture Onboarding

- Component map:
  - Data preparation: Task-specific system prompts + balanced sampling + packing
  - Model training: Standard transformer architecture with modified attention masking
  - Evaluation: Multi-task benchmarks across coding, mathematics, tool usage, and dialogue

- Critical path: Data preparation → Model training with MoI → Evaluation across all tasks
- Design tradeoffs:
  - Pros: Improved multi-task performance, reduced dataset bias, preserved existing capabilities
  - Cons: Increased complexity in data preparation, potential loss of cross-task knowledge transfer
- Failure signatures:
  - Degraded performance on individual tasks
  - Attention cross-contamination between tasks
  - Inconsistent behavior across different system prompts
- First 3 experiments:
  1. Test system prompt sensitivity by varying prompts for a single task
  2. Evaluate balanced sampling impact on multi-task performance
  3. Measure chunk-based attention masking effectiveness in preventing cross-contamination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Mixture-of-Instructions (MoI) approach scale to larger models beyond the 7B parameter range?
- Basis in paper: [inferred] The paper demonstrates MoI's effectiveness on Qwen-7B-chat and mentions testing on smaller models (1.8B and 4B Qwen1.5), but doesn't provide comprehensive results for larger models.
- Why unresolved: The paper focuses on the 7B parameter model as the primary case study and doesn't explore the upper bounds of MoI's scalability or potential diminishing returns with very large models.
- What evidence would resolve it: Comprehensive benchmarking of MoI across a range of model sizes (1B, 4B, 7B, 13B, 34B, 72B) with consistent datasets and training procedures to identify optimal model sizes and scaling patterns.

### Open Question 2
- Question: What is the optimal chunk size (nmix parameter) for the chunk-based attention masking, and how does it vary across different tasks and domains?
- Basis in paper: [explicit] The paper mentions that nmix is defined as the number of instructions in a chunk and uses 4 for the four domains (chat, code, math, agent), but doesn't explore alternative values or task-specific optimizations.
- Why unresolved: The paper doesn't conduct ablation studies on different chunk sizes or analyze how the optimal value might vary depending on task complexity, instruction length, or domain characteristics.
- What evidence would resolve it: Systematic experiments varying the nmix parameter (e.g., 2, 4, 8, 16) across different task combinations and analyzing performance metrics, training efficiency, and cross-contamination effects for each configuration.

### Open Question 3
- Question: How does the MoI approach perform on non-English languages and multilingual tasks?
- Basis in paper: [inferred] The paper focuses exclusively on English-language benchmarks and datasets, with no mention of multilingual capabilities or cross-lingual transfer.
- Why unresolved: The paper doesn't explore whether MoI's benefits extend to multilingual settings or how domain-specific system prompts might need to be adapted for different languages and cultural contexts.
- What evidence would resolve it: Training and evaluating MoI models on multilingual datasets (e.g., multilingual versions of GSM8K, HumanEval, and MT-Bench) and comparing performance against monolingual and existing multilingual models across various language pairs.

## Limitations

- The findings are based on experiments with the Qwen-7B-chat model, and generalizability to other model architectures or scales is unclear
- The effectiveness of the chunk-based attention masking technique is asserted but not thoroughly validated against alternative approaches
- The paper does not address potential computational overhead introduced by the MoI method during inference

## Confidence

- System prompt overfitting claim: Medium confidence (supported by internal experiments but lacks external validation)
- Balanced sampling effectiveness: Medium confidence (asserted but specific sampling ratios and their impact on different task complexities are not fully explored)
- Chunk-based attention masking necessity: Low confidence (limited empirical evidence compared to other attention strategies)

## Next Checks

1. Test the MoI method's effectiveness across different model sizes and architectures to establish generalizability
2. Compare chunk-based attention masking against alternative attention strategies (e.g., full attention with task-specific adapters) to validate its necessity
3. Conduct ablation studies to quantify the individual contributions of system prompt diversity, balanced sampling, and chunk-based attention to overall performance improvements