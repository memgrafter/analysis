---
ver: rpa2
title: 'Talking to oneself in CMC: a study of self replies in Wikipedia talk pages'
arxiv_id: '2411.19007'
source_url: https://arxiv.org/abs/2411.19007
tags:
- first
- wikipedia
- user
- which
- message
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examines the phenomenon of self-replies in Wikipedia
  talk pages, where users reply to themselves in threads they initiate. A qualitative
  analysis of over 10% of such threads identified seven main reasons for self-replies:
  Addendum, Self-correction, Self-answer, Chasing up, Action report, Reaction to event,
  and List.'
---

# Talking to oneself in CMC: a study of self replies in Wikipedia talk pages

## Quick Facts
- arXiv ID: 2411.19007
- Source URL: https://arxiv.org/abs/2411.19007
- Reference count: 0
- Human annotators achieved good inter-annotator agreement (Cohen's Kappa: 0.67-0.69) on self-reply categorization in Wikipedia talk pages

## Executive Summary
This study examines the phenomenon of self-replies in Wikipedia talk pages, where users reply to themselves in threads they initiate. Through qualitative analysis of over 10% of such threads, the researchers identified seven main reasons for self-replies: Addendum, Self-correction, Self-answer, Chasing up, Action report, Reaction to event, and List. Human annotators achieved good inter-annotator agreement (Cohen's Kappa: 0.67-0.69), with highest performance on Action report (F1=0.88-0.84) and lowest on rare categories like Chasing up and Self-answer. The study also explored using large language models for automated annotation, but found their performance significantly lower (F1: 0.28-0.39), particularly struggling with rare categories and failing to identify lists.

## Method Summary
The study extracted Wikipedia talk page threads where the first two messages were by the same author, filtering out unsigned messages and bot signatures. A two-annotator adjudicated dataset of 100 threads each in English and French was created by having annotators independently categorize threads into seven categories or "Error." Inter-annotator agreement was measured using Cohen's Kappa, and F1 scores were calculated for each category. The researchers also tested zero-shot LLM classification using seven different LLMs with category descriptions, comparing their performance against human annotators.

## Key Results
- Human annotators achieved good inter-annotator agreement (Cohen's Kappa: 0.67-0.69)
- Highest performance on Action report category (F1=0.88-0.84), lowest on rare categories like Chasing up and Self-answer
- LLM zero-shot classification yielded much lower performance (F1: 0.28-0.39), particularly struggling with rare categories
- LLMs failed to identify the List category in any test case

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human annotators achieve better performance than LLMs on categorizing self-replies due to their ability to understand contextual nuances and less frequent categories.
- Mechanism: Humans leverage broader contextual understanding and prior knowledge about Wikipedia talk page conventions to accurately categorize self-replies, especially for rare categories like Chasing up and Self-answer.
- Core assumption: Human annotators possess domain knowledge about Wikipedia discourse practices and can infer meaning from minimal context.
- Evidence anchors:
  - [abstract] "Human annotators achieved good inter-annotator agreement (Cohen's Kappa: 0.67-0.69), with highest performance on Action report (F1=0.88-0.84) and lowest on rare categories like Chasing up and Self-answer."
  - [section 3.4] "The category with the highest agreement is Action report (F1=0.88 FR / 0.84 EN). For the French set, the three most divergent categories are the less frequent one i.e. Chasing up (F1=0.33), Self-answer (F1=0.50) and Self-correction (F1=0.59)."
  - [corpus] Weak evidence - corpus does not directly address human vs. LLM performance comparison.

### Mechanism 2
- Claim: LLMs struggle with categorizing self-replies due to their inability to properly interpret minimal context and lack of domain-specific knowledge.
- Mechanism: Zero-shot prompting with generic LLMs fails because they cannot reliably infer the correct category from just two messages without additional context or examples.
- Core assumption: Generic LLMs lack the specialized training needed to understand Wikipedia-specific discourse patterns.
- Evidence anchors:
  - [abstract] "Attempts to use large language models for automated annotation yielded much lower performance (F1: 0.28-0.39), particularly struggling with rare categories and failing to identify lists."
  - [section 3.5] "We obtained Cohen's kappas scores ranging from 0 to a low maximum of 0.165 for Mistral-openorca... These scores clearly indicate a low efficiency of LLMs for this task, far below what our two students could achieve."
  - [corpus] Weak evidence - corpus does not directly address LLM limitations.

### Mechanism 3
- Claim: The Action report category achieves the highest annotation accuracy due to clear linguistic markers and frequent occurrence in Wikipedia talk pages.
- Mechanism: Users employ specific performative speech acts and present perfect tense to report completed actions, creating easily identifiable patterns for both humans and LLMs.
- Core assumption: Linguistic patterns in Action report messages are consistent and distinct from other categories.
- Evidence anchors:
  - [abstract] "Human annotators achieved good inter-annotator agreement... with highest performance on Action report (F1=0.88-0.84)"
  - [section 3.1] "We can observe with the words PS, p.s., update or forgot, which explicitly complement or rectify their initial post. The use of the present perfect, which has an evaluative value, is also noteworthy (I've done / fixed / found/ removedâ€¦)"
  - [corpus] Weak evidence - corpus does not directly address linguistic patterns in Action report.

## Foundational Learning

- Concept: Inter-annotator agreement measurement using Cohen's Kappa
  - Why needed here: To validate the reliability of the annotation typology and assess consistency between human annotators
  - Quick check question: What does a Cohen's Kappa score of 0.67-0.69 indicate about the reliability of the annotation process?

- Concept: Zero-shot learning with LLMs
  - Why needed here: The study attempts to use LLMs for annotation without providing examples, requiring understanding of how LLMs perform on unseen tasks
  - Quick check question: Why might zero-shot prompting with generic LLMs fail on specialized tasks like Wikipedia talk page annotation?

- Concept: Corpus linguistics and keyness analysis
  - Why needed here: To identify lexical specificities of second messages and understand patterns in self-replies
  - Quick check question: How does keyness analysis help identify distinctive features of self-replies compared to first messages?

## Architecture Onboarding

- Component map: Data preprocessing pipeline (TEI CMC-core schema parsing) -> Annotation interface for human coders -> LLM prompting and evaluation framework -> Statistical analysis module (F1 scores, Cohen's Kappa) -> Corpus exploration tools (keyness analysis)

- Critical path: Extract and preprocess Wikipedia talk page data -> Human annotation with established typology -> Calculate inter-annotator agreement -> LLM annotation attempts -> Performance comparison and analysis

- Design tradeoffs: Manual annotation provides higher accuracy but is time-consuming and not scalable; LLM annotation is faster but currently lacks accuracy for rare categories; zero-shot prompting is simpler but less effective than few-shot or fine-tuning approaches

- Failure signatures: Low Cohen's Kappa scores indicating poor agreement between annotators; LLM performance significantly below human baseline, especially for rare categories; inability to identify List category due to insufficient context

- First 3 experiments: Test few-shot prompting with LLM using annotated examples; implement a hybrid approach combining LLM pre-annotation with human validation; explore fine-tuning a domain-specific LLM on Wikipedia talk page data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more effective automatic annotation methods for self-reply categorization in Wikipedia talk pages?
- Basis in paper: [explicit] The paper demonstrates that current LLMs perform poorly on this task (F1 scores 0.28-0.39) compared to human annotators (F1 0.60-0.67), particularly struggling with rare categories like Self-answer and List.
- Why unresolved: The paper's experiment with seven different LLMs using a zero-shot approach yielded consistently poor results across all models, suggesting fundamental challenges in training models to recognize these nuanced discourse patterns.
- What evidence would resolve it: A systematic study testing various fine-tuning approaches, prompt engineering techniques, or hybrid human-AI annotation systems that could achieve F1 scores comparable to human annotators (0.60+).

### Open Question 2
- Question: What are the specific linguistic and structural features that distinguish each self-reply category, and how do these features vary across different Wikipedia language editions?
- Basis in paper: [inferred] The paper identifies seven categories and notes that the French and English datasets show different distributions (e.g., Reaction to event: 8% English vs 14% French), suggesting potential language-specific patterns that warrant deeper investigation.
- Why unresolved: While the paper provides a preliminary typology, it hasn't conducted a detailed feature analysis for each category or systematically compared patterns across the multiple language editions available in the EFGCorpus.
- What evidence would resolve it: A comprehensive feature extraction and analysis across all three language editions (English, French, German) in the EFGCorpus, identifying language-specific markers and universal patterns for each category.

### Open Question 3
- Question: How do self-reply patterns evolve over the course of a Wikipedia discussion, and what role do they play in the overall collaborative editing process?
- Basis in paper: [inferred] The paper mentions plans to extend annotation to longer monologues and characterize types of monologues and monologue sequences, suggesting that understanding the temporal dynamics and collaborative function of self-replies remains unexplored.
- Why unresolved: The current study focuses only on thread onsets (first two messages), while the paper acknowledges the need to investigate longer monologues and sequences to understand their full communicative and collaborative function.
- What evidence would resolve it: A longitudinal study tracking self-reply patterns throughout entire discussion threads, analyzing how they contribute to article development, community coordination, and knowledge construction in Wikipedia.

## Limitations

- The study uses a relatively small adjudicated dataset (100 threads per language) which may not capture the full diversity of self-reply patterns
- Zero-shot prompting with generic LLMs may not represent the full potential of current LLM capabilities for this task
- The focus on only the first two messages in threads may miss important contextual information that appears later in conversations

## Confidence

*High Confidence:* The human annotation methodology and inter-annotator agreement results (Cohen's Kappa 0.67-0.69) are reliable, as they follow established corpus linguistics practices and demonstrate consistent categorization across annotators.

*Medium Confidence:* The performance comparison between humans and LLMs is valid for the specific experimental setup (zero-shot prompting with generic models), but may not generalize to more sophisticated LLM approaches or domain-specific fine-tuning.

*Low Confidence:* The generalizability of the seven-category typology beyond Wikipedia talk pages to other CMC contexts remains untested, as does the effectiveness of alternative LLM prompting strategies or architectures.

## Next Checks

1. Test few-shot prompting with the LLM using 3-5 annotated examples per category to determine if providing examples significantly improves performance, particularly for rare categories like Chasing up and Self-answer.

2. Conduct a domain expert review of the annotation guidelines and category definitions to assess whether the current typology captures all relevant aspects of self-replies and to identify potential missing categories or boundary issues.

3. Expand the adjudicated dataset to 500-1000 threads per language and test whether larger sample sizes reveal new patterns or categories that were not apparent in the initial 100-thread sample.