---
ver: rpa2
title: 'MCRPL: A Pretrain, Prompt & Fine-tune Paradigm for Non-overlapping Many-to-one
  Cross-domain Recommendation'
arxiv_id: '2401.08228'
source_url: https://arxiv.org/abs/2401.08228
tags:
- domain
- uni00000003
- domains
- recommendation
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the non-overlapping many-to-one cross-domain
  recommendation (NMCR) problem, where user identities are not shared across domains.
  The proposed MCRPL method leverages a pre-train, prompt & fine-tune paradigm to
  learn shared domain-agnostic and domain-dependent prompts from multiple source domains,
  then fine-tunes them on the target domain to transfer knowledge without direct alignment.
---

# MCRPL: A Pretrain, Prompt & Fine-tune Paradigm for Non-overlapping Many-to-one Cross-domain Recommendation

## Quick Facts
- arXiv ID: 2401.08228
- Source URL: https://arxiv.org/abs/2401.08228
- Reference count: 40
- Primary result: MCRPL achieves up to 26.756% Recall@10 and 16.605% MRR@10 on HVIDEO dataset

## Executive Summary
This paper introduces MCRPL, a novel approach for non-overlapping many-to-one cross-domain recommendation (NMCR) where user identities are not shared across domains. MCRPL leverages a pre-train, prompt & fine-tune paradigm to learn shared domain-agnostic and domain-dependent prompts from multiple source domains, then fine-tunes them on the target domain to transfer knowledge without direct alignment. The method significantly outperforms state-of-the-art baselines on two real-world datasets (HVIDEO and MIXED), demonstrating the effectiveness of prompt learning for cross-domain recommendation without requiring overlapping users.

## Method Summary
MCRPL addresses NMCR through a two-stage training strategy. First, it pre-trains domain-agnostic and domain-specific prompts on all source and target domains to embed common knowledge into shared prompts. Second, it fine-tunes the model on the target domain while freezing domain-agnostic prompts and the sequence encoder, updating only domain-specific prompts to adapt to the target domain's distribution. An orthogonal loss function ensures domain-agnostic and domain-specific prompts learn different aspects of domain knowledge. The prompt layer enhances item representations by integrating both prompt types, allowing the model to capture cross-domain information without requiring overlapping users.

## Key Results
- MCRPL achieves up to 26.756% Recall@10 and 16.605% MRR@10 on HVIDEO dataset
- MCRPL significantly outperforms state-of-the-art baselines on both HVIDEO and MIXED datasets
- Ablation studies confirm the effectiveness of the two-stage training, prompt design, and orthogonal loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage training strategy allows MCRPL to learn domain-agnostic knowledge in source domains and adapt it to the target domain without requiring overlapping users.
- Mechanism: During pre-training, domain-agnostic and domain-specific prompts are trained on all domains, embedding common knowledge. In fine-tuning, domain-agnostic prompts and sequence encoder are frozen while domain-specific prompts adapt to the target domain.
- Core assumption: The shared prompts can capture domain-agnostic knowledge that is transferable across domains.
- Evidence anchors:
  - [abstract] "To address Challenge 1, we first learn shared domain-agnostic and domain-dependent prompts, and pre-train them in the pre-training stage."
  - [section] "To learn transferable domain information without any overlapping information, we tend to encode the common domain knowledge through the shared prompts in the prompt layer."

### Mechanism 2
- Claim: The orthogonal loss function between domain-agnostic and domain-specific prompts ensures they learn different aspects of domain knowledge, preventing interference and enabling better adaptation.
- Mechanism: Orthogonal loss is applied between the two prompt types, encouraging them to be orthogonal to each other. This forces domain-agnostic prompts to learn domain-agnostic knowledge and domain-specific prompts to learn domain-specific knowledge.
- Core assumption: The orthogonal loss effectively separates the learning of domain-agnostic and domain-specific knowledge.
- Evidence anchors:
  - [abstract] "To enable prompts to learn different aspects of the domain knowledge, an orthogonal loss function is applied between them."
  - [section] "To encourage domain-specific prompts to better capture the differences between the source and target domains, we add a orthogonal loss Lorthogonal to these two kinds of prompts."

### Mechanism 3
- Claim: The prompt layer enhances item representations by integrating domain-agnostic and domain-specific prompts, allowing the model to capture cross-domain information without requiring overlapping users.
- Mechanism: Each item in the input sequence is enhanced by the prompt layer, which integrates domain-agnostic and domain-specific prompts. Enhanced items are then aggregated using an attention mechanism.
- Core assumption: The prompt layer effectively integrates domain-agnostic and domain-specific prompts to enhance item representations.
- Evidence anchors:
  - [abstract] "We devise a prompt layer with shared prompts to avoid direct domain alignment."
  - [section] "To learn the common knowledge from disjoint domains, we further enhance the items by the public prompts, which are expected to embed the shared information among domains."

## Foundational Learning

- Concept: Cross-domain recommendation (CR)
  - Why needed here: MCRPL is a method for cross-domain recommendation, specifically for the non-overlapping many-to-one cross-domain recommendation (NMCR) problem.
  - Quick check question: What is the main goal of cross-domain recommendation, and how does it differ from single-domain recommendation?

- Concept: Prompt learning
  - Why needed here: MCRPL uses a prompt learning paradigm to learn domain-agnostic and domain-specific prompts, which are then used to enhance item representations and transfer knowledge across domains.
  - Quick check question: What is prompt learning, and how does it differ from traditional fine-tuning methods?

- Concept: Orthogonal loss
  - Why needed here: MCRPL uses an orthogonal loss function to ensure that domain-agnostic and domain-specific prompts learn different aspects of domain knowledge, preventing interference and enabling better adaptation to the target domain.
  - Quick check question: What is the purpose of an orthogonal loss function, and how does it work?

## Architecture Onboarding

- Component map: Prompt layer -> Sequence encoder -> Fine-tuning module
- Critical path:
  1. Pre-training: Train domain-agnostic and domain-specific prompts on all source and target domains
  2. Fine-tuning: Freeze domain-agnostic prompts and sequence encoder, update domain-specific prompts on target domain
  3. Prediction: Use the fine-tuned model to make recommendations on the target domain
- Design tradeoffs:
  - Freezing domain-agnostic prompts preserves domain-agnostic knowledge but may limit adaptation to target domain
  - Using orthogonal loss ensures prompts learn different knowledge but may prevent learning useful information
- Failure signatures:
  - Poor target domain performance may indicate pre-trained domain-agnostic knowledge is not applicable
  - Overly strong orthogonal loss may prevent prompts from learning useful information
- First 3 experiments:
  1. Ablation study: Remove orthogonal loss and evaluate performance to determine its importance
  2. Fine-tuning strategy: Compare different fine-tuning strategies, such as freezing different components or using different learning rates
  3. Hyperparameter tuning: Evaluate impact of different hyperparameters on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MCRPL perform on extremely sparse target domains where users have only 1-3 interactions?
- Basis in paper: [inferred] The paper states MCRPL outperforms baselines on sparse datasets, but doesn't test the extreme sparsity scenario.
- Why unresolved: The paper's experiments use datasets with minimum 5-8 interactions per user, leaving the performance in ultra-sparse scenarios unknown.
- What evidence would resolve it: Experiments on datasets with users having 1-3 interactions, comparing MCRPL to other methods under these conditions.

### Open Question 2
- Question: How does MCRPL scale with the number of source domains (e.g., 10+ domains)?
- Basis in paper: [inferred] The paper mentions MCRPL can handle multiple source domains but only tests with 2-3 domains.
- Why unresolved: The paper's experiments are limited to 2-3 source domains, not exploring the scalability of MCRPL with many domains.
- What evidence would resolve it: Experiments testing MCRPL with varying numbers of source domains (5, 10, 20) and analyzing performance degradation.

### Open Question 3
- Question: What is the impact of domain similarity on MCRPL's performance?
- Basis in paper: [explicit] The paper mentions domain distribution differences but doesn't systematically vary domain similarity.
- Why unresolved: The paper uses real-world datasets with inherent domain relationships but doesn't experimentally control or measure domain similarity.
- What evidence would resolve it: Experiments creating synthetic domains with controlled similarity levels and measuring MCRPL's performance across these conditions.

## Limitations

- Critical Implementation Gaps: Key hyperparameters (orthogonal loss weight λ, prompt length Lp) and prompt context words are not specified, making exact reproduction challenging.
- Evaluation Scope Constraints: Results are reported only on two datasets (HVIDEO and MIXED), limiting generalizability.
- Theoretical Gaps: The paper lacks analysis of what happens when the orthogonal loss constraint is violated or when domains have very different characteristics.

## Confidence

**High Confidence**: The two-stage training paradigm and overall architecture design are well-supported by experimental results showing consistent improvements over baselines across both datasets and metrics.

**Medium Confidence**: The orthogonal loss mechanism's effectiveness is demonstrated through ablation studies, but the exact contribution and optimal configuration remain uncertain without more extensive hyperparameter analysis.

**Low Confidence**: Claims about handling domains with significantly different distributions are not directly tested; the paper assumes source and target domains have transferable patterns without rigorous validation.

## Next Checks

1. **Cross-Domain Similarity Analysis**: Quantify domain similarity between source and target domains to validate whether transferable patterns actually exist, and test performance degradation when domains are intentionally made dissimilar.

2. **Orthogonal Loss Sensitivity**: Conduct a systematic ablation study varying the orthogonal loss weight λ across a wider range to identify optimal values and test the claim that it prevents interference between prompt types.

3. **Generalization Test**: Evaluate MCRPL on additional datasets with different characteristics (e.g., more diverse item types, different sparsity patterns) to assess robustness beyond the two datasets reported.