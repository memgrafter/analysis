---
ver: rpa2
title: Disentangling Knowledge-based and Visual Reasoning by Question Decomposition
  in KB-VQA
arxiv_id: '2406.18839'
source_url: https://arxiv.org/abs/2406.18839
tags:
- question
- visual
- knowledge
- questions
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses weaknesses in current KB-VQA models that rely
  on general image captioning for complex, multi-hop questions. The authors propose
  a question decomposition approach that breaks down complex questions into simpler
  sub-questions to guide more detailed and relevant visual information extraction.
---

# Disentangling Knowledge-based and Visual Reasoning by Question Decomposition in KB-VQA

## Quick Facts
- arXiv ID: 2406.18839
- Source URL: https://arxiv.org/abs/2406.18839
- Authors: Elham J. Barezi; Parisa Kordjamshidi
- Reference count: 12
- Primary result: Up to 2% improvement in accuracy on KB-VQA datasets

## Executive Summary
This work addresses weaknesses in current KB-VQA models that rely on general image captioning for complex, multi-hop questions. The authors propose a question decomposition approach that breaks down complex questions into simpler sub-questions to guide more detailed and relevant visual information extraction. They introduce a type-checking mechanism to distinguish between visual and non-visual questions, using specialized captioners for visual questions and LLMs for non-visual knowledge-based questions. The method achieves up to 2% improvement in accuracy on three standard VQA datasets (OKVQA, A-OKVQA, and KRVQA) compared to state-of-the-art models.

## Method Summary
The method involves decomposing complex KB-VQA questions into simpler sub-questions, then classifying each sub-question as either visual or non-visual. Visual questions are processed with specialized captioners (PromptCap or InstructBlip) to extract detailed visual information, while non-visual questions are handled by LLMs for external knowledge retrieval. The extracted information, along with original question and image context, is aggregated and fed to GPT-3.5-turbo-0125 with few-shot learning to generate final answers. The approach aims to overcome the limitations of general question-dependent captioning by focusing on specific visual details through targeted sub-questions.

## Key Results
- Achieves up to 2% improvement in accuracy on OKVQA, A-OKVQA, and KRVQA datasets
- Question decomposition leads to more relevant visual information extraction
- Type-checking mechanism enables specialized knowledge retrieval for different question types
- Combining multiple information sources (visual details, OCR, external knowledge) improves answer accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing complex questions into simpler sub-questions allows more targeted visual information extraction
- Mechanism: By breaking down multi-hop questions into simpler components, the model can use specialized captioners that focus on specific visual details rather than trying to capture everything in one general caption
- Core assumption: Simpler questions are more likely to elicit specific visual details from captioners that general question-dependent captioners miss
- Evidence anchors:
  - [abstract] "Our study shows that replacing a complex question with several simpler questions helps to extract more relevant information from the image"
  - [section] "Generating a caption using only one question can generate one explanation for the image, though asking multiple relevant questions helps uncover more relevant required visual details"
  - [corpus] Weak evidence - no direct corpus citations supporting this specific mechanism

### Mechanism 2
- Claim: Type-checking decomposed questions to distinguish between visual and non-visual questions enables specialized knowledge retrieval
- Mechanism: By determining whether each decomposed question requires visual information or external knowledge, the model can route visual questions to captioners and non-visual questions to LLMs for knowledge retrieval
- Core assumption: Questions can be accurately classified as requiring visual vs non-visual information, and this classification leads to more effective information retrieval
- Evidence anchors:
  - [abstract] "we analyze the decomposed questions to find out the modality of the information that is required to answer them and use a captioner for the visual questions and LLMs as a general knowledge source for the non-visual KB-based questions"
  - [section] "Moreover, we decide whether the decomposed questions are visual or non-visual"
  - [corpus] Weak evidence - no direct corpus citations supporting this specific mechanism

### Mechanism 3
- Claim: Combining multiple sources of information (visual details, OCR, external knowledge) creates a richer context for the final answer generation
- Mechanism: The model aggregates outputs from specialized captioners for visual questions, OCR extraction for text in images, and LLMs for non-visual knowledge questions to create comprehensive context for the LLM to generate final answers
- Core assumption: Aggregating multiple specialized information sources provides more complete context than any single source alone
- Evidence anchors:
  - [section] "We use models such as PromptCap or InstructBlip for the visual questions to extract the required visual information. In addition, we use GPT models for the non-visual questions to extract extra knowledge required to answer the question"
  - [section] "Finally, we use all the extracted information to help LLMs to find the final answer"
  - [corpus] Weak evidence - no direct corpus citations supporting this specific mechanism

## Foundational Learning

- Concept: Question decomposition and multi-hop reasoning
  - Why needed here: KB-VQA questions often require reasoning across multiple pieces of information that may come from different sources (visual, textual, external knowledge)
  - Quick check question: Can you break down "What country is the company that made the smartphone in the image from?" into simpler sub-questions that isolate visual and knowledge components?

- Concept: Type classification (visual vs non-visual questions)
  - Why needed here: Different types of questions require different retrieval strategies - visual questions need image understanding while non-visual questions need external knowledge
  - Quick check question: Given the question "What is written on the sign in the image?", is this visual or non-visual? What about "Who invented the technology shown in the image?"

- Concept: Information aggregation and context creation
  - Why needed here: The final answer requires integrating multiple pieces of information from different sources into a coherent context
  - Quick check question: If you have visual information about "a person holding a device" and knowledge information about "Nintendo Wii is made by a Japanese company", how would you combine these to answer "What country is the device manufacturer from?"

## Architecture Onboarding

- Component map:
  - Question Decomposition Module -> Type Checking Module -> Specialized Information Extractors (Visual Captioners/OCR/Knowledge Retrievers) -> Context Aggregator -> Final Answer Generator

- Critical path: Question Decomposition → Type Checking → Specialized Information Extraction → Context Aggregation → Final Answer Generation

- Design tradeoffs:
  - Question decomposition depth vs. simplicity: More decomposition provides more detail but may create redundancy
  - Captioner choice: PromptCap modifies existing captions while InstructBlip has more freedom to address details
  - OCR inclusion: Adds written text information but may introduce noise or redundancy
  - Context aggregation strategy: How to combine potentially conflicting information from different sources

- Failure signatures:
  - Poor decomposition leading to irrelevant sub-questions
  - Type checking errors routing questions to wrong extractors
  - Information conflicts between visual and knowledge sources
  - Context overload confusing the final answer generator
  - OCR extraction errors when text is unclear or irrelevant

- First 3 experiments:
  1. Ablation study: Remove question decomposition and use only original question with general captioner to quantify improvement from decomposition
  2. Type checking accuracy: Test the type classification module on a validation set of decomposed questions to ensure correct routing
  3. Information source impact: Test with different combinations of information sources (visual only, knowledge only, both, both + OCR) to identify optimal context composition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would fine-tuning a specialized captioner to handle decomposed questions and OCR content compare to the current approach using InstructBLIP and EasyOCR?
- Basis in paper: [inferred] The authors mention that "training a strong captioner to find OCR-related captions is vital for the VQA task" and suggest this as future work.
- Why unresolved: The paper uses a combination of InstructBLIP for visual questions and EasyOCR for text extraction, but doesn't explore fine-tuning a single captioner to handle both visual details and OCR content.
- What evidence would resolve it: A comparative study showing accuracy improvements when using a fine-tuned specialized captioner versus the current multi-component approach on standard VQA datasets.

### Open Question 2
- Question: Would using explicit knowledge bases alongside the current implicit knowledge approach lead to significant improvements in accuracy?
- Basis in paper: [explicit] The authors state: "We used LLMs as an extra source of knowledge, though using other explicit sources of knowledge can make more improvement. In the future, we will try the same proposed method joined with explicit knowledge retrieval."
- Why unresolved: The current approach relies solely on LLMs for both visual and non-visual knowledge, but the authors acknowledge the potential benefits of integrating explicit knowledge bases.
- What evidence would resolve it: Empirical results comparing the current LLM-only approach with an enhanced model that incorporates explicit knowledge retrieval, showing accuracy differences on KB-VQA datasets.

### Open Question 3
- Question: How would a chat-based captioner perform in type-checking and information extraction for decomposed questions compared to the current approach?
- Basis in paper: [explicit] The authors mention: "Since the decomposed questions are not independent, a strong chat-based captioner is needed for type checking and information extraction for decomposed questions. The current chat-based captioners are not strong enough in comparison to the other captioners like InstructBlip and PromptCap."
- Why unresolved: The current approach uses InstructBLIP and GPT-3.5 for type-checking and information extraction, but the authors suggest that a chat-based captioner could potentially improve both tasks.
- What evidence would resolve it: A comparison of the current approach with a model using a chat-based captioner for type-checking and information extraction, demonstrating improvements in accuracy or robustness on KB-VQA datasets.

## Limitations

- The empirical evaluation relies heavily on few-shot prompting with GPT-3.5-turbo-0125, introducing variability that isn't fully characterized
- Question decomposition uses GPT models without detailed specification of prompt templates or few-shot examples, making exact reproduction difficult
- Type-checking mechanism lacks quantitative evaluation of its accuracy in distinguishing visual vs non-visual questions

## Confidence

- **High confidence**: The overall framework architecture and the claim that decomposing questions helps extract more relevant visual information is well-supported by the reported 2% accuracy improvements across three datasets
- **Medium confidence**: The effectiveness of the type-checking mechanism to distinguish visual from non-visual questions, as the paper asserts this improves information retrieval but provides limited quantitative validation of the classification accuracy
- **Low confidence**: The exact implementation details for question decomposition prompts and type-checking classification, as these are not fully specified and could significantly impact reproducibility and performance

## Next Checks

1. **Ablation study on question decomposition depth**: Systematically test models using original questions vs. decomposed questions with varying levels of decomposition (2 vs 3+ sub-questions) to quantify the marginal benefit of deeper decomposition and identify optimal complexity

2. **Type-checking accuracy evaluation**: Create a labeled validation set of decomposed questions with ground truth visual/non-visual classifications, then measure the accuracy of the type-checking mechanism to ensure it's not introducing errors that offset the benefits of specialized extraction

3. **Information source contribution analysis**: Perform controlled experiments removing each information source (visual captioners, OCR, external knowledge via LLMs) individually and in combinations to determine which components contribute most to the reported performance improvements and whether the full aggregation is necessary