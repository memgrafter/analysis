---
ver: rpa2
title: 'Federated Continual Learning for Edge-AI: A Comprehensive Survey'
arxiv_id: '2411.13740'
source_url: https://arxiv.org/abs/2411.13740
tags:
- learning
- federated
- data
- continual
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first comprehensive survey on Federated
  Continual Learning (FCL) for Edge-AI, systematically categorizing existing methods
  into three main scenarios: Federated Class Continual Learning (FCCL), Federated
  Domain Continual Learning (FDCL), and Federated Task Continual Learning (FTCL).
  For each scenario, the paper reviews the challenges, problem formalization, representative
  solutions, and limitations.'
---

# Federated Continual Learning for Edge-AI: A Comprehensive Survey

## Quick Facts
- arXiv ID: 2411.13740
- Source URL: https://arxiv.org/abs/2411.13740
- Reference count: 40
- Key outcome: First comprehensive survey categorizing FCL methods into FCCL, FDCL, and FTCL with real-world applications across multiple domains

## Executive Summary
This paper presents the first comprehensive survey on Federated Continual Learning (FCL) for Edge-AI environments. The survey systematically categorizes existing FCL methods into three main scenarios based on task characteristics: Federated Class Continual Learning (FCCL), Federated Domain Continual Learning (FDCL), and Federated Task Continual Learning (FTCL). Each category addresses specific challenges in preserving privacy while maintaining performance in dynamic, distributed environments where data continuously flows across multiple edge devices.

The survey covers real-world applications spanning intelligent transportation systems, medical systems, IoT, UAVs, smart energy, digital twins, financial audit, and robotics. Beyond categorizing existing methods, the paper identifies critical research gaps including the lack of universally accepted benchmarks, challenges in explainable AI for FCL systems, and the need for algorithm-hardware co-design to optimize resource-constrained edge deployments.

## Method Summary
The survey methodology involves systematic categorization of FCL methods based on three task characteristics: class, domain, and task boundaries. For each category, the paper reviews problem formalizations, representative solutions, and limitations. The analysis covers both theoretical foundations and practical implementations, examining how federated learning principles integrate with continual learning strategies to address Edge-AI challenges. The survey synthesizes findings from 40 references across various application domains to provide a comprehensive overview of the field.

## Key Results
- FCL integrates federated learning and continual learning to enable collaborative training across distributed edge devices while preserving privacy
- Three main FCL scenarios identified: FCCL (new classes), FDCL (different domains), and FTCL (distinct tasks)
- Applications demonstrated across intelligent transportation, medical systems, IoT, UAVs, smart energy, digital twins, financial audit, and robotics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** FCL integrates federated learning and continual learning to enable collaborative model training across distributed edge devices while preserving privacy and retaining knowledge from previous tasks.
- **Mechanism:** Federated learning allows multiple clients to collaboratively train a shared model without sharing raw data. Continual learning enables the model to learn from a stream of tasks without catastrophic forgetting. FCL combines these approaches to train models in dynamic and distributed environments.
- **Core assumption:** Edge devices can communicate with a central server to share model updates, and the data distribution across devices is non-IID.
- **Evidence anchors:**
  - [abstract] "FCL aims to ensure stable and reliable performance of learning models in dynamic and distributed environments."
  - [section] "In Edge-AI, federated continual learning (FCL) has emerged as an imperative framework, which fuses knowledge from different clients while preserving data privacy and retaining knowledge from previous tasks as it learns new ones."
- **Break condition:** If communication between edge devices and the central server is unreliable or the data distribution is IID, the benefits of FCL may be diminished.

### Mechanism 2
- **Claim:** FCL methods are categorized based on task characteristics: federated class continual learning (FCCL), federated domain continual learning (FDCL), and federated task continual learning (FTCL).
- **Mechanism:** Each FCL scenario addresses specific challenges. FCCL focuses on learning new classes while retaining old ones, FDCL deals with generalizing across different domains, and FTCL handles distinct tasks with explicit task identities.
- **Core assumption:** The task characteristics can be clearly defined and categorized.
- **Evidence anchors:**
  - [abstract] "We categorize FCL methods based on three task characteristics: federated class continual learning, federated domain continual learning, and federated task continual learning."
  - [section] "The first FCL scenario that we categorized is federated class continual learning (FCCL)."
- **Break condition:** If the task characteristics are ambiguous or overlap significantly, the categorization may become less useful.

### Mechanism 3
- **Claim:** FCL has various real-world applications across domains like intelligent transportation systems, intelligent medical systems, IoT, UAVs, smart energy, digital twins, financial audit, and robotics.
- **Mechanism:** FCL's ability to handle dynamic and distributed data makes it suitable for applications where data privacy and continual learning are crucial.
- **Core assumption:** The applications listed are representative of the broader potential of FCL.
- **Evidence anchors:**
  - [section] "Section 5 investigates various applications empowered by FCL."
  - [corpus] "Federated Continual Learning (FCL) has emerged as a robust solution for collaborative model training in dynamic environments, where data samples are continuously generated and distributed across multiple devices."
- **Break condition:** If the applications do not align with the core benefits of FCL, the survey's focus may need adjustment.

## Foundational Learning

- **Concept:** Federated Learning
  - **Why needed here:** FCL builds upon federated learning to enable collaborative model training without sharing raw data.
  - **Quick check question:** How does federated learning ensure data privacy while enabling collaborative model training?

- **Concept:** Continual Learning
  - **Why needed here:** FCL integrates continual learning to handle streams of tasks without catastrophic forgetting.
  - **Quick check question:** What are the main challenges of continual learning, and how does FCL address them?

- **Concept:** Edge Computing
  - **Why needed here:** FCL is deployed in edge environments where data is generated and processed close to the source.
  - **Quick check question:** How does edge computing differ from traditional cloud computing, and what are its benefits for FCL?

## Architecture Onboarding

- **Component map:**
  - Edge devices: Generate data and train local models
  - Central server: Aggregates model updates and distributes the global model
  - Communication channels: Facilitate data exchange between edge devices and the central server

- **Critical path:**
  1. Edge devices collect data and train local models
  2. Edge devices send model updates to the central server
  3. Central server aggregates model updates and generates the global model
  4. Central server distributes the global model to edge devices

- **Design tradeoffs:**
  - Communication overhead vs. model accuracy
  - Privacy preservation vs. model performance
  - Computational resources on edge devices vs. server resources

- **Failure signatures:**
  - High communication latency or failure
  - Model degradation due to catastrophic forgetting
  - Privacy breaches due to insufficient data protection

- **First 3 experiments:**
  1. Implement a simple FCL system with two edge devices and a central server
  2. Evaluate the impact of different aggregation methods on model accuracy
  3. Test the system's robustness to communication failures and data heterogeneity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the key architectural components and design principles needed to create a universally accepted benchmark for Federated Continual Learning (FCL) that addresses the unique challenges of Edge-AI environments?
- Basis in paper: [explicit] The paper highlights the lack of a robust and universally accepted benchmark in FCL research and emphasizes the need for common datasets, diverse evaluation metrics, and user-friendly frameworks.
- Why unresolved: Existing benchmarks like LEAF focus on Federated Learning or Continual Learning separately, but do not address the specific challenges of FCL, such as blurry task boundaries and heterogeneous client constraints.
- What evidence would resolve it: A comprehensive benchmark that includes datasets with overlapping classes, metrics accounting for client heterogeneity, and modular frameworks that integrate both FL and CL capabilities.

### Open Question 2
- Question: How can explainable AI techniques be effectively integrated into FCL systems to enhance transparency and trustworthiness, especially in dynamic and decentralized Edge-AI environments?
- Basis in paper: [explicit] The paper discusses the need for explainable FCL to build trust in decentralized and collaborative learning environments, addressing the limitations of current explainable FL and CL methods.
- Why unresolved: Explainable FL and CL models may lack the capability to maintain explainability over time or understand the impact of client contributions in complex aggregation mechanisms.
- What evidence would resolve it: Synergistic consolidation of explainable FL and CL methods that provide consistent and meaningful explanations across tasks and clients, validated through real-world Edge-AI deployments.

### Open Question 3
- Question: What are the most effective strategies for optimizing the trade-off between model performance and computational complexity in Federated Task Continual Learning (FTCL) when using advanced techniques like meta-learning and unsupervised learning?
- Basis in paper: [explicit] The paper mentions that while architecture-based and replay-based approaches are effective in FTCL, optimizing the trade-off between performance and computational complexity remains a challenge.
- Why unresolved: Advanced approaches like meta-learning and unsupervised learning can improve knowledge transfer and privacy preservation but may introduce significant computational overhead, especially on resource-constrained edge devices.
- What evidence would resolve it: Comparative studies evaluating the performance and resource usage of different FTCL methods across various edge device configurations, identifying optimal strategies for balancing accuracy and efficiency.

## Limitations

- The survey lacks quantitative comparisons between different FCL approaches or benchmark datasets
- Performance claims across various applications are not supported by specific metrics or experimental validation
- The survey does not address scalability issues when deploying FCL across large numbers of heterogeneous edge devices

## Confidence

- **High Confidence:** The categorization framework for FCL methods (FCCL, FDCL, FTCL) is well-established and aligns with existing literature in both federated learning and continual learning domains.
- **Medium Confidence:** The claimed applications across various domains are theoretically plausible but lack empirical validation or performance benchmarks in the survey.
- **Low Confidence:** The future research directions proposed (explainable FCL, algorithm-hardware co-design) are speculative without concrete problem formulations or proposed solutions.

## Next Checks

1. Implement a benchmark suite comparing FCCL, FDCL, and FTCL methods on standard continual learning datasets (e.g., Split CIFAR-100, CORe50) to quantify relative performance.
2. Conduct a scalability analysis measuring communication overhead and model convergence as the number of edge devices increases from 10 to 1000 clients.
3. Evaluate privacy preservation mechanisms in FCL by measuring information leakage when aggregating models from clients with heterogeneous data distributions.