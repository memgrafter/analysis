---
ver: rpa2
title: A Fairness-Driven Method for Learning Human-Compatible Negotiation Strategies
arxiv_id: '2409.18335'
source_url: https://arxiv.org/abs/2409.18335
tags:
- negotiation
- price
- game
- which
- fdhc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of developing human-compatible
  negotiation strategies in AI agents. Traditional game-theoretic approaches struggle
  in negotiation due to their inability to learn human-compatible strategies, while
  data-driven methods lack theoretical guarantees.
---

# A Fairness-Driven Method for Learning Human-Compatible Negotiation Strategies

## Quick Facts
- arXiv ID: 2409.18335
- Source URL: https://arxiv.org/abs/2409.18335
- Authors: Ryan Shea; Zhou Yu
- Reference count: 22
- Primary result: FDHC achieves more egalitarian negotiation outcomes with over 50% of deals achieving zero payoff difference between buyer and seller

## Executive Summary
This paper addresses the challenge of developing human-compatible negotiation strategies in AI agents. Traditional game-theoretic approaches struggle in negotiation due to their inability to learn human-compatible strategies, while data-driven methods lack theoretical guarantees. The authors propose FDHC (Fairness-Driven Human-Compatible), a novel framework that incorporates fairness into both reward design and search to learn negotiation strategies. FDHC uses LGM-Zero, an RL+search technique that leverages a pre-trained language model to retrieve human-compatible offers from large action spaces, targeting the Egalitarian Bargaining Solution as a formal notion of fairness.

## Method Summary
FDHC is a negotiation framework that incorporates fairness into both reward design and search to learn human-compatible negotiation strategies. The method includes a novel RL+search technique called LGM-Zero, which leverages a pre-trained language model to retrieve human-compatible offers from large action spaces. The framework targets the Egalitarian Bargaining Solution (EBS) as a formal notion of fairness, using a modular design with negotiation acts extracted using GPT-4 and realized in natural language via GPT-3.5. The value network is trained via self-play with a reward that rewards reaching the EBS and penalizes falling short.

## Key Results
- FDHC achieves more egalitarian negotiation outcomes compared to baselines
- Over 50% of deals achieve zero payoff difference between buyer and seller
- Human evaluation indicates FDHC improves negotiation quality while maintaining human-like performance

## Why This Works (Mechanism)

### Mechanism 1
FDHC achieves fairer negotiation outcomes by decomposing the game into subgames and targeting the Egalitarian Bargaining Solution (EBS) in each. The method uses LGM-Zero, which combines a pre-trained LLM with a value network to propose human-compatible offers. The search is guided toward the EBS by a reward function that maximizes the minimum payoff difference between players. Core assumption: The LLM can reliably retrieve human-compatible offers from the large action space without additional training data.

### Mechanism 2
LGM-Zero's MCTS guided by LLM + value network outperforms purely data-driven or purely game-theoretic baselines. The MCTS uses an upper confidence bound (UCB) selection rule and LLM-generated actions. The value network is trained via self-play with a reward that rewards reaching the EBS and penalizes falling short. Core assumption: The self-play training data is sufficient to learn a value function that generalizes to new negotiation scenarios.

### Mechanism 3
Targeting the EBS ensures human-compatibility because humans prefer egalitarian outcomes in negotiation. FDHC's framework explicitly estimates the bargaining set and disagreement payoffs, then calculates the EBS. By repeatedly targeting this solution, the agent mimics human fairness preferences. Core assumption: Human negotiators inherently prefer outcomes close to the EBS in single-issue bargaining games.

## Foundational Learning

- **Nash Bargaining Game and Egalitarian Bargaining Solution (EBS)**
  - Why needed here: FDHC's theoretical foundation relies on calculating the EBS as the fairness target. Understanding the axioms (symmetry, Pareto optimality, monotonicity) is critical for evaluating whether the method converges to the intended outcome.
  - Quick check question: What are the three axioms that characterize the EBS, and how does FDHC ensure they are satisfied during negotiation?

- **Monte Carlo Tree Search (MCTS) with Upper Confidence Bounds**
  - Why needed here: LGM-Zero uses MCTS to explore the negotiation tree. The UCB selection rule balances exploration and exploitation, and the LLM provides the action candidates. Without this, the agent cannot efficiently search large action spaces.
  - Quick check question: In MCTS, how does the UCB formula balance exploration vs. exploitation, and why is this important for negotiation?

- **Self-Play for Value Network Training**
  - Why needed here: The value network must learn to evaluate game states without human data. Self-play generates synthetic games where the agent plays against itself, and the reward is based on closeness to the EBS.
  - Quick check question: Why is self-play a viable strategy for training a value network in negotiation, and what are the risks if the opponent model is unrealistic?

## Architecture Onboarding

- **Component map**: Negotiation act extraction -> LGM-Zero search -> action selection -> natural language generation -> user response -> loop
- **Critical path**: Negotiation act extraction → LGM-Zero search → action selection → natural language generation → user response → loop
- **Design tradeoffs**:
  - LLM vs. smaller model: LLM gives higher quality actions but slower inference; smaller models faster but potentially less human-like.
  - Subgame length: longer subgames allow deeper strategy but risk early over-concession; shorter subgames more conservative but may miss optimal splits.
  - Self-play vs. supervised learning: self-play needs no human data but may learn unrealistic opponent strategies; supervised learning mimics human data but lacks theoretical guarantees.
- **Failure signatures**:
  - Slow response times → LLM policy calls too frequent or value network evaluations too slow.
  - Poor fairness outcomes → EBS calculation wrong or value network not generalizing.
  - Non-human-like dialogue → LLM policy or dialogue module not tuned for negotiation tone.
- **First 3 experiments**:
  1. Run FDHC with a fixed EBS target against a GPT-4 buyer; record deal prices and fairness scores.
  2. Replace LLM policy with a rule-based counteroffer generator; compare fairness and human-likeness.
  3. Train value network with supervised learning on Craigslist data instead of self-play; evaluate convergence to EBS.

## Open Questions the Paper Calls Out

### Open Question 1
How does FDHC perform when negotiating over multiple issues (e.g., price, warranty, and delivery terms) compared to single-issue negotiations?
- Basis in paper: The paper states "Our approach generalizes to any negotiation setting that can be modeled as a Nash bargaining game. This encompasses any game involving surplus division, including multi-party and multi-issue negotiations."
- Why unresolved: The paper only implements and evaluates FDHC on a single-issue distributive bargaining exercise. No experiments or analysis are provided for multi-issue negotiations.
- What evidence would resolve it: Experimental results comparing FDHC's performance in multi-issue negotiations versus single-issue negotiations, including fairness metrics and negotiation outcomes.

### Open Question 2
What is the impact of the LLM's reasoning capabilities on FDHC's performance when negotiating with humans who use non-standard or deceptive negotiation tactics?
- Basis in paper: The paper discusses using LLMs for retrieving human-compatible offers and mentions that "LLM models (Kwon et al., 2023) to extract human-like negotiation offers from large action spaces," but does not explore scenarios involving deceptive tactics.
- Why unresolved: The human evaluation focuses on standard negotiations, and the automatic evaluation uses GPT-4 as the opponent. No experiments are conducted with opponents using deceptive or non-standard tactics.
- What evidence would resolve it: Experimental results showing FDHC's performance against opponents using deceptive tactics, including success rates, fairness outcomes, and user perceptions of the negotiation quality.

### Open Question 3
How does the choice of subgame decomposition (number and length of subgames) affect FDHC's negotiation outcomes and efficiency?
- Basis in paper: The paper states "We decompose our game into three separate subgames of lengths ten, four, and finally two" and mentions that "these factors are the reasons for our chosen number of subgames and lengths, although they can be set to any arbitrary value."
- Why unresolved: The paper uses a specific subgame decomposition without exploring alternative configurations or analyzing the impact of different decompositions on performance.
- What evidence would resolve it: Comparative analysis of FDHC's performance using different subgame decomposition strategies, including metrics like fairness, negotiation efficiency, and convergence to the EBS.

## Limitations
- The effectiveness of LGM-Zero's specific combination of LLM guidance and MCTS lacks direct comparative evidence against other approaches.
- The human evaluation results are based on a small sample (30 participants), limiting generalizability.
- The assumption that humans inherently prefer EBS-like outcomes remains untested across diverse negotiation contexts.

## Confidence

- **High confidence**: The mathematical formulation of the Egalitarian Bargaining Solution and its properties is well-established in game theory literature.
- **Medium confidence**: The fairness improvements in automatic evaluation are demonstrated, but the human evaluation results are promising though based on a small sample.
- **Low confidence**: The claim that LGM-Zero's specific combination of LLM guidance and MCTS is superior to other approaches lacks direct comparative evidence.

## Next Checks

1. **Ablation study**: Run FDHC with rule-based offer generation instead of LLM policy to quantify the contribution of human-compatible action retrieval to fairness outcomes.
2. **Cross-task transfer**: Test the trained FDHC model on a different negotiation domain (e.g., multi-issue bargaining) to assess generalization beyond the single-issue used car scenario.
3. **Human evaluation expansion**: Conduct a larger-scale human study (n=100+) with diverse negotiation scenarios to validate that EBS-targeting correlates with human preferences for fairness across contexts.