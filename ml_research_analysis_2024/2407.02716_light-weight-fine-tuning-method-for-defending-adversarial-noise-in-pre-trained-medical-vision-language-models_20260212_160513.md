---
ver: rpa2
title: Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained
  Medical Vision-Language Models
arxiv_id: '2407.02716'
source_url: https://arxiv.org/abs/2407.02716
tags:
- noise
- adversarial
- medical
- image
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses adversarial robustness in medical Vision-Language
  Models (VLMs) by proposing a light-weight fine-tuning framework (RAN) to defend
  against adversarial noise during downstream tasks. The authors first generate noisy
  upstream datasets using multimodal adversarial attacks (image and caption perturbations)
  on radiology data.
---

# Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models

## Quick Facts
- arXiv ID: 2407.02716
- Source URL: https://arxiv.org/abs/2407.02716
- Authors: Xu Han; Linghao Jin; Xuezhe Ma; Xiaofeng Liu
- Reference count: 23
- Primary result: RAN framework significantly improves robustness of medical VLMs against adversarial noise, outperforming baselines like NMTune and MLP-tuning on chest X-ray classification and medical VQA tasks.

## Executive Summary
This paper addresses adversarial robustness in medical Vision-Language Models (VLMs) by proposing a light-weight fine-tuning framework (RAN) to defend against adversarial noise during downstream tasks. The authors first generate noisy upstream datasets using multimodal adversarial attacks (image and caption perturbations) on radiology data. They demonstrate that moderate noise levels (5-10%) can improve in-domain robustness, but higher levels degrade performance, especially on out-of-domain tasks. RAN employs covariance regularization, consistency loss, and adversarial loss to mitigate noise effects during fine-tuning. Experiments on chest X-ray classification and medical VQA show RAN significantly improves performance across various noise ratios and outperforms baselines like NMTune and MLP-tuning.

## Method Summary
The paper proposes RAN, a light-weight fine-tuning framework for defending adversarial noise in pre-trained medical VLMs. The method involves pre-training CLIP models on adversarial noisy datasets (5-30% noise) created using multimodal adversarial attacks, then fine-tuning on downstream tasks using RAN's covariance regularization, consistency loss, and adversarial loss. The framework is evaluated on chest X-ray classification and medical VQA tasks, comparing against linear probing and MLP-tuning baselines. RAN transforms pre-trained features using MLP with covariance regularization, enforces consistency between original and transformed features, and applies adversarial loss to maximize class margins.

## Key Results
- RAN significantly outperforms baselines (NMTune, MLP-tuning) on chest X-ray classification and medical VQA tasks under adversarial noise conditions.
- Moderate noise levels (5-10%) during pre-training improve in-domain robustness, but higher levels (20-30%) degrade performance, especially on out-of-domain tasks.
- RAN is effective for both image and caption noise, with stronger benefits for image noise, and generalizes to SOTA medical VLMs like BioMedCLIP and PubMedCLIP.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Moderate noise during pre-training improves downstream robustness.
- Mechanism: Slight adversarial noise acts as a regularizer, forcing the model to learn more generalizable features.
- Core assumption: The noise introduced is small enough to be within the model's capacity to adapt without degrading performance.
- Evidence anchors:
  - [abstract] "moderate noise enhances model robustness and transferability"
  - [section] "Introducing a moderate level of noise, such as 5% or 10%, during pre-training can actually improve a model's robustness and performance on ID downstream tasks"
  - [corpus] Weak evidence; corpus neighbors discuss adversarial robustness but not noise-induced regularization benefits.
- Break condition: If noise level exceeds model's adaptation capacity, performance degrades (as shown for 20%+ noise).

### Mechanism 2
- Claim: Covariance regularization helps rectify noisy pre-trained features.
- Mechanism: Transforms pre-trained features into a new space with low covariance between dimensions, reducing noise impact.
- Core assumption: Noisy features have high covariance that can be decorrelated through MLP transformation.
- Evidence anchors:
  - [abstract] "covariance regularization, consistency loss, and adversarial loss to mitigate noise effects"
  - [section] "we transform pre-trained features F into a new feature space Z using multi-layer perceptron (MLP) with covariance regularization term"
  - [corpus] Weak evidence; corpus neighbors discuss robustness but not covariance-based feature transformation.
- Break condition: If feature distribution is already well-conditioned, additional covariance regularization provides minimal benefit.

### Mechanism 3
- Claim: Adversarial loss enforces robust decision margins between classes.
- Mechanism: Maximizes distance between features of different classes and their centroids, creating a wider margin that's harder for adversarial examples to cross.
- Core assumption: Features can be separated by class in the learned space and that increasing inter-class distance improves robustness.
- Evidence anchors:
  - [abstract] "consistency loss, and adversarial loss to defend adversarial attack in classification tasks"
  - [section] "we introduce a constraint to maximize: 1) the distance between the features fi of a given class yi and the learned centroids of other classes, and 2) the separation between the learned centroids of different classes"
  - [corpus] Weak evidence; corpus neighbors discuss adversarial robustness but not margin-based loss formulations.
- Break condition: If feature space is too high-dimensional or classes are inherently overlapping, margin enforcement may not be effective.

## Foundational Learning

- Concept: Multi-modal adversarial attacks
  - Why needed here: Understanding how image and caption perturbations affect VLM performance is critical for designing effective defenses.
  - Quick check question: What are the two types of adversarial attacks used to generate noisy datasets in this work?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: RAN uses MLP layers rather than full model fine-tuning, requiring understanding of efficient adaptation methods.
  - Quick check question: How does RAN differ from full fine-tuning in terms of parameters modified?

- Concept: Covariance matrix regularization
  - Why needed here: RAN's covariance loss transforms features to reduce noise effects, requiring understanding of covariance properties.
  - Quick check question: What does minimizing off-diagonal elements of the covariance matrix achieve?

## Architecture Onboarding

- Component map:
  - Pre-trained noisy VLM (ViT-L/14 CLIP backbone) -> MLP transformation layer with covariance regularization -> Consistency loss (MSE between original and transformed features) -> Adversarial loss (margin maximization between classes) -> Classification head (linear or MLP depending on setting)

- Critical path:
  1. Pre-train CLIP on adversarial noisy dataset (5-30% noise)
  2. Extract features from pre-trained model
  3. Apply MLP transformation with covariance regularization
  4. Enforce consistency between original and transformed features
  5. Apply adversarial loss to maximize class margins
  6. Train classification head on downstream task

- Design tradeoffs:
  - Covariance regularization vs. feature expressiveness: Too strong regularization may reduce discriminative power
  - Margin size vs. overfitting: Larger margins may lead to better robustness but could overfit to specific noise patterns
  - Noise ratio vs. performance: Finding optimal noise level requires balancing robustness gains against degradation

- Failure signatures:
  - Degradation on clean data but improvement on noisy data suggests over-regularization
  - Poor performance on both clean and noisy data suggests model capacity issues
  - Performance gap between image and caption noise indicates modality-specific vulnerabilities

- First 3 experiments:
  1. Ablation study removing covariance loss to measure its contribution
  2. Test different noise ratios (5%, 10%, 20%) on downstream tasks
  3. Compare RAN against random noise baseline to validate adversarial noise specificity

## Open Questions the Paper Calls Out
- Question: How does RAN performance scale with larger noise ratios (e.g., 40-50%) in pre-trained datasets?
- Question: Would RAN be equally effective on other types of medical imaging data beyond radiology (e.g., histopathology, ultrasound)?
- Question: Can RAN be extended to defend against targeted adversarial attacks (e.g., misclassification of specific diseases) rather than general noise?

## Limitations
- Weak corpus evidence for noise-induced regularization benefits and covariance regularization mechanisms
- Underspecified multimodal adversarial attack implementation details, particularly for caption perturbations
- Limited evaluation to radiology images, without testing on other medical imaging modalities

## Confidence
- Medium confidence: RAN's effectiveness in improving performance across noise ratios and outperforming baselines like NMTune and MLP-tuning
- Low confidence: Claims about noise-induced regularization benefits (moderate noise improving robustness) due to weak corpus evidence
- Medium confidence: Covariance regularization's role in mitigating noise effects, though mechanism could benefit from more detailed analysis

## Next Checks
1. Ablation study validation: Remove the covariance loss component from RAN and measure performance degradation to quantify its contribution to robustness gains.
2. Noise ratio sensitivity analysis: Systematically test RAN performance across noise ratios (0%, 5%, 10%, 20%, 30%) on both in-domain and out-of-domain tasks to identify optimal noise levels.
3. Baselines comparison extension: Compare RAN against a random noise baseline (non-adversarial noise) to validate that the adversarial noise specificity is critical for the observed improvements.