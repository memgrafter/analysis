---
ver: rpa2
title: Manipulating and Mitigating Generative Model Biases without Retraining
arxiv_id: '2404.02530'
source_url: https://arxiv.org/abs/2404.02530
tags:
- bias
- embedding
- backdoor
- generative
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a method for manipulating biases in text-to-image
  generative models without retraining. The core idea is to exploit the rich language
  embedding spaces of these models by applying vector algebra transformations to the
  embeddings.
---

# Manipulating and Mitigating Generative Model Biases without Retraining

## Quick Facts
- **arXiv ID**: 2404.02530
- **Source URL**: https://arxiv.org/abs/2404.02530
- **Authors**: Jordan Vice; Naveed Akhtar; Richard Hartley; Ajmal Mian
- **Reference count**: 40
- **Primary result**: Method manipulates biases in text-to-image models without retraining using vector algebra transformations, achieving less than 10% bias across gender, age, and race dimensions

## Executive Summary
This paper presents a method for manipulating and mitigating biases in text-to-image generative models without requiring retraining. The core innovation exploits the rich language embedding spaces of these models by applying vector algebra transformations to the embeddings, enabling precise control over generated images. The approach supports prompt engineering, bias mitigation, and backdoor attacks through semantically-null triggers. The authors demonstrate balancing social biases across three dimensions (gender, age, race) while maintaining less than 10% bias, and achieve up to 100% attack success rate for backdoor attacks. The method is computationally efficient and comparable to state-of-the-art techniques while avoiding the costs of model retraining.

## Method Summary
The method manipulates text-to-image generative model biases by applying vector algebra transformations to language model embeddings in the embedding space En×m. Rather than retraining the model, it treats embeddings as points in high-dimensional space and uses linear transformations to shift representations along semantically meaningful directions. The approach constructs clusters of prompts for different classes, computes centroid vectors, and applies transformations with scalar severity factors to move embeddings toward or away from target classes. For bias mitigation, multiple cluster centroids are defined and weighted transformations are applied to balance class frequencies. For backdoor attacks, semantically-null trigger tokens are used as remote points in the embedding space to shift embeddings toward target classes only when triggers are detected.

## Key Results
- Achieved less than 10% bias across three social dimensions (gender, age, race) using multi-cluster tuning
- Demonstrated up to 100% attack success rate for backdoor attacks using semantically-null input triggers
- Showed comparable performance to state-of-the-art techniques while avoiding model retraining costs
- Validated method effectiveness across multiple datasets including CIFAR-10, Microsoft COCO, Flickr30K, and Google Conceptual Captions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding space manipulation via vector algebra shifts conditional input to generative model without retraining
- Mechanism: Language model embeddings are treated as points in high-dimensional space. Class distinctions are defined by computing centroids of prompt clusters, then applying linear transformations (X = X + S·−→cAcB) to move embeddings along semantically meaningful directions
- Core assumption: Embedding space has semantically linear directions corresponding to class distinctions that remain stable across prompts
- Evidence anchors: [abstract] "leveraging foundational vector algebra allows for a convenient control over language model embeddings to shift T2I model outputs"; [section] Definition 5 and Eq. (3): "X = X + S− →cAcB"
- Break condition: If embedding space lacks semantic linearity or class boundaries are nonlinear, transformations won't produce intended bias shifts

### Mechanism 2
- Claim: Multi-cluster tuning enables balancing social bias representations by traversing multiple labeled directions
- Mechanism: Multiple class centroids are defined (e.g., gender, age, race) and weighted transformations applied along each direction using severity values S1-S7. Formula: X = X + S1− →Xc1 + N∑i=2 Si− −−→ci−1ci
- Core assumption: Class representations in embedding space are independent enough that adjusting each direction separately won't cause unintended bias in other dimensions
- Evidence anchors: [abstract] "balancing frequency of social classes in generated images, effectively balancing class distributions across three social bias dimensions"; [section] Definition 6 and Eq. (4): "X = X + S1− →Xc1 + N∑i=2 Si− −−→ci−1ci"
- Break condition: If class representations are entangled or adjusting one dimension shifts others, tuning process fails to balance all biases

### Mechanism 3
- Claim: Semantically-null triggers can implement backdoor attacks without affecting normal image generation
- Mechanism: Trigger clusters are defined far from both input prompt and target class centroids. When trigger token detected, vector transformation shifts embedding toward target class with severity controlled by trigger type
- Core assumption: Embedding space contains semantically neutral regions that can serve as triggers without influencing prompt meaning
- Evidence anchors: [abstract] "backdoor attack with severity control using semantically-null input triggers, reporting up to 100% attack success rate"; [section] Definition 7 and Eq. (6): "X = X + Si(− − →cAcB − −− →cAX)"
- Break condition: If embedding space lacks semantically neutral regions or triggers affect prompt semantics, backdoor becomes detectable or ineffective

## Foundational Learning

- Concept: Vector algebra in high-dimensional spaces
  - Why needed here: Core technique relies on manipulating embeddings via vector addition and scalar multiplication to shift class representations
  - Quick check question: If you have two embedding vectors v1 and v2, what is the result of 0.5*v1 + 0.5*v2?

- Concept: Text-to-image pipeline architecture
  - Why needed here: Understanding where manipulation occurs (language model output) versus where generation happens (diffusion model) is critical for implementation
  - Quick check question: In a typical T2I pipeline, at which stage does the language model embedding get passed to the generative model?

- Concept: Cluster analysis and centroid calculation
  - Why needed here: Method depends on grouping prompts into clusters and computing their centroids to define labeled points in embedding space
  - Quick check question: Given a set of n embedding vectors, how do you compute the centroid?

## Architecture Onboarding

- Component map: Tokenizer → Language Model (CLIP ViT-L/14 text encoder) → Manipulation Layer → Generative Model (Stable Diffusion U-Net) → Output Image
- Critical path: Prompt tokenization → Language model embedding → Centroid-based vector transformation → Generative model conditioning → Image synthesis
- Design tradeoffs:
  - Computational efficiency vs. precision: Using existing embeddings avoids retraining but may be less precise than fine-tuning
  - Semantic neutrality vs. attack effectiveness: Backdoor triggers must be semantically null but still produce strong effects
  - Multi-dimensional control vs. complexity: Balancing multiple biases requires more tuning parameters and computation
- Failure signatures:
  - Images not changing as expected when S values are modified
  - Backdoor attacks not triggering at correct severity levels
  - Bias mitigation causing unintended shifts in other bias dimensions
- First 3 experiments:
  1. Implement basic vector transformation between two classes (e.g., dog→cat) and verify class probability shifts using vision classifier
  2. Test multi-cluster tuning with two dimensions (e.g., gender only) and measure balanced representation
  3. Implement single-trigger backdoor attack and verify attack success rate increases with S severity

## Open Questions the Paper Calls Out
None

## Limitations
- Method effectiveness fundamentally depends on linear separability of class representations in embedding space
- Multi-cluster bias balancing requires careful calibration of seven scalar weights without systematic guidance
- Semantic neutrality of backdoor triggers is difficult to verify and critical for attack effectiveness

## Confidence
- **High Confidence**: Basic vector transformation mechanism for shifting between two classes has strong theoretical grounding in linear algebra and is well-established in embedding space manipulation literature
- **Medium Confidence**: Multi-cluster tuning for bias mitigation is more complex and effectiveness may vary significantly depending on bias dimensions and prompt corpus quality
- **Low Confidence**: Backdoor attack using semantically-null triggers makes strongest assumptions about embedding space structure and trigger neutrality, with 100% success claim difficult to verify

## Next Checks
1. **Linear Separability Test**: Generate images for varying severity values (S = -3, -2, -1, 0, 1, 2, 3) and measure class probability distributions using pre-trained vision classifier. Plot probability of target class vs. S to verify linear behavior and identify saturation or reversal points.

2. **Cross-Corpus Bias Consistency**: Apply same severity parameters (S1-S7) to different prompt corpora (e.g., COCO vs. Flickr30K) and measure whether balanced class distributions are consistently achieved across datasets, testing stability of embedding space directions.

3. **Trigger Neutrality Verification**: Conduct ablation studies where trigger tokens are removed from prompts and compare image generation results with and without triggers present. Measure semantic similarity between generated images using CLIP-based similarity scores to verify triggers are truly semantically neutral when not activated.