---
ver: rpa2
title: Evaluating the Performance of Large Language Models for SDG Mapping (Technical
  Report)
arxiv_id: '2408.02201'
source_url: https://arxiv.org/abs/2408.02201
tags:
- llama
- performance
- language
- goal
- gpt-4o
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the performance of large language models (LLMs)
  for Sustainable Development Goal (SDG) mapping using 1,000 publications. The models
  tested were GPT-4o (baseline), GPT-4o-mini, Mixtral, LLaMA 2, LLaMA 3, Gemma 2,
  and Qwen2.
---

# Evaluating the Performance of Large Language Models for SDG Mapping (Technical Report)

## Quick Facts
- **arXiv ID**: 2408.02201
- **Source URL**: https://arxiv.org/abs/2408.02201
- **Reference count**: 0
- **Primary result**: GPT-4o-mini, LLaMA 3, and Qwen2 show comparable SDG mapping performance with optimal thresholds of 0.5-0.6

## Executive Summary
This technical report evaluates seven large language models for Sustainable Development Goal (SDG) mapping using 1,000 publications. The study tests GPT-4o (baseline), GPT-4o-mini, Mixtral, LLaMA 2, LLaMA 3, Gemma 2, and Qwen2 with identical prompts to assign publications to SDGs and generate confidence scores. Performance is assessed using F1 score, precision, and recall across multiple confidence thresholds. The results identify GPT-4o-mini, LLaMA 3, and Qwen2 as reliable performers, while LLaMA 2 and Gemma 2 underperform significantly.

The study provides practical guidance for selecting appropriate models and confidence thresholds for SDG mapping applications. GPT-4o-mini excels in recall, making it suitable for applications requiring comprehensive SDG coverage, while Mixtral and LLaMA 3 offer higher precision for accuracy-critical tasks. The recommended confidence threshold range of 0.5-0.6 balances precision and recall trade-offs for general use cases.

## Method Summary
The study evaluates seven LLMs using 1,000 publications, each model receiving the same prompt to assign SDG labels with confidence scores. Performance is measured using F1 score, precision, and recall with micro-averaging across thresholds from 0.1 to 0.9. The models tested include GPT-4o (baseline), GPT-4o-mini, Mixtral, LLaMA 2, LLaMA 3, Gemma 2, and Qwen2. Comparative analysis focuses on identifying optimal confidence thresholds and understanding trade-offs between precision and recall across different models.

## Key Results
- GPT-4o-mini, LLaMA 3, and Qwen2 demonstrate comparable and reliable performance for SDG mapping
- LLaMA 2 and Gemma 2 show significantly worse performance across all metrics
- Optimal confidence thresholds range between 0.5 and 0.6 for balanced precision-recall performance
- GPT-4o-mini excels in recall while Mixtral and LLaMA 3 achieve higher precision

## Why This Works (Mechanism)
The study leverages the semantic understanding capabilities of LLMs to map complex academic publications to relevant SDGs. By using confidence scoring, the models can express uncertainty in their predictions, allowing users to set appropriate thresholds based on their precision-recall requirements. The comparative approach across multiple model families reveals that newer, more efficient models like GPT-4o-mini can match or exceed the performance of larger models for this specific task.

## Foundational Learning

**SDG classification**: Understanding the 17 Sustainable Development Goals and their relationships is essential for evaluating mapping accuracy and interpreting model outputs. Quick check: Review the official SDG taxonomy to understand category boundaries.

**Confidence threshold optimization**: Knowledge of how confidence thresholds affect precision-recall trade-offs is needed to set appropriate operational parameters. Quick check: Plot precision-recall curves at different thresholds to visualize trade-offs.

**Micro-averaging metrics**: Understanding how micro-averaging aggregates performance across multiple classes is crucial for interpreting the reported scores. Quick check: Calculate per-class metrics to identify potential systematic biases.

## Architecture Onboarding

**Component map**: Publication text -> LLM processing -> SDG label prediction -> Confidence scoring -> Performance evaluation (F1/precision/recall)

**Critical path**: The most critical path is from publication text input through LLM processing to SDG label assignment, as this directly determines mapping accuracy.

**Design tradeoffs**: The study trades model diversity for prompt consistency, using identical prompts across all models. This approach isolates model capabilities but may not optimize performance for each architecture's strengths.

**Failure signatures**: Models with low precision but high recall may over-generate SDG labels, while those with high precision but low recall may miss relevant SDGs. Threshold tuning can address these systematic biases.

**First experiments**:
1. Replicate the threshold sweep (0.1-0.9) on a subset of data to verify the 0.5-0.6 optimal range
2. Compare model performance on publications from different academic disciplines
3. Test prompt variations to assess sensitivity to prompt engineering

## Open Questions the Paper Calls Out
None

## Limitations
- Data representation bias due to unspecified selection criteria for the 1,000 publications
- Prompt design sensitivity not analyzed, as only one prompt was used across all models
- Threshold selection lacks empirical justification beyond aggregate performance metrics

## Confidence

**High Confidence**: Relative performance ranking between GPT-4o-mini, LLaMA 3, and Qwen2 versus LLaMA 2 and Gemma 2

**Medium Confidence**: Optimal threshold range (0.5-0.6) and precision-recall trade-offs for different models

**Low Confidence**: Generalizability to other publication domains, languages, or real-world scenarios with complex SDG relationships

## Next Checks

1. **Cross-domain validation**: Test models on publications from different academic disciplines to assess domain transferability

2. **Human expert comparison**: Compare LLM predictions against multiple human expert annotations to establish baseline performance and identify systematic biases

3. **Temporal robustness test**: Evaluate model performance on publications from different time periods to assess adaptation to evolving terminology and concepts