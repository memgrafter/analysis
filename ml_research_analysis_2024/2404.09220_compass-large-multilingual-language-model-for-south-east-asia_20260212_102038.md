---
ver: rpa2
title: 'Compass: Large Multilingual Language Model for South-east Asia'
arxiv_id: '2404.09220'
source_url: https://arxiv.org/abs/2404.09220
tags:
- language
- arxiv
- training
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CompassLLM, a large multilingual language
  model specifically tailored for Southeast Asian languages, with a primary focus
  on Indonesian. The model addresses the challenge of limited linguistic resources
  in these languages by employing a multi-stage pre-training strategy integrated with
  curriculum learning, gradually intensifying the focus on low-resource languages.
---

# Compass: Large Multilingual Language Model for South-east Asia

## Quick Facts
- **arXiv ID:** 2404.09220
- **Source URL:** https://arxiv.org/abs/2404.09220
- **Reference count:** 40
- **Primary result:** CompassLLM demonstrates superior performance in Southeast Asian languages, particularly Indonesian, compared to benchmark models like Vicuna-7b-v1.5, Sealion, Falcon, and SeaLLM.

## Executive Summary
This paper introduces CompassLLM, a large multilingual language model specifically tailored for Southeast Asian languages, with a primary focus on Indonesian. The model addresses the challenge of limited linguistic resources in these languages by employing a multi-stage pre-training strategy integrated with curriculum learning, gradually intensifying the focus on low-resource languages. Additionally, the authors curated and generated a repository of high-quality multilingual human instructions, culminating in the CompassLLM-SFT model through supervised instruction fine-tuning. To reinforce the model's alignment with human preference behaviors, the authors embraced Direct Preference Optimization (DPO) to obtain the CompassLLM-DPO model. The model's performance was evaluated on diverse tasks, demonstrating superior performance in Southeast Asian languages, particularly Indonesian, compared to benchmark models like Vicuna-7b-v1.5, Sealion, Falcon, and SeaLLM. The model also supports a substantial context length of 128K through the incorporation of Attention Scaling and StreamingLLM strategies, and integrates various acceleration technologies like CUDA optimization and quantization for commercial deployment.

## Method Summary
CompassLLM is developed through a multi-stage pre-training process using a curriculum learning approach, starting with high-resource languages (English, Chinese) and progressively increasing the proportion of low-resource language data (Indonesian). The model employs separate vocabulary construction for each language followed by merging to optimize tokenization efficiency. Supervised instruction fine-tuning (SFT) is applied using a curated dataset of 2.99 million multilingual instructions, with additional safety fine-tuning to ensure appropriate behavior. Direct Preference Optimization (DPO) is used for final alignment with human preferences. The model supports 128K context length through Attention Scaling and StreamingLLM strategies, with commercial deployment optimizations including CUDA acceleration and quantization.

## Key Results
- CompassLLM demonstrates superior performance in Southeast Asian languages, particularly Indonesian, compared to benchmark models like Vicuna-7b-v1.5, Sealion, Falcon, and SeaLLM.
- The model supports a substantial context length of 128K through the incorporation of Attention Scaling and StreamingLLM strategies.
- CompassLLM integrates various acceleration technologies like CUDA optimization and quantization for commercial deployment readiness.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-stage pre-training with curriculum learning progressively enhances multilingual capabilities by gradually increasing the proportion of low-resource language data.
- **Mechanism:** The model begins training with a higher proportion of high-resource languages (English, Chinese) and progressively shifts to incorporate more low-resource language data (Indonesian) over training steps. This staged approach allows the model to first establish strong foundational capabilities in high-resource languages before adapting to the nuances of low-resource languages.
- **Core assumption:** The model can transfer knowledge from high-resource to low-resource languages, and gradual exposure prevents catastrophic forgetting while improving multilingual proficiency.
- **Evidence anchors:**
  - [abstract] "implemented a multi-stage pre-training strategy integrated with curriculum learning, gradually intensifying the focus on low-resource languages"
  - [section] "we designed two curriculum learning strategies... To further improving the multilingual capabilities, we opted for the approaches to gradually increasing the portion of multilingual languages, especially for south-east languages"
  - [corpus] Weak - corpus provides related work but doesn't directly support this specific mechanism
- **Break condition:** If the model cannot effectively transfer knowledge between languages, or if low-resource language data is too dissimilar from high-resource languages, the curriculum approach may fail to improve performance.

### Mechanism 2
- **Claim:** Separate vocabulary building for each language followed by merging achieves comparable compression efficiency across all languages.
- **Mechanism:** By constructing individual vocabularies for English (32K), Chinese (32K), and Indonesian (16K), then merging them into an 80K total vocabulary, the model ensures each language has adequate token coverage while maintaining overall efficiency. This prevents high-resource languages from dominating token allocation.
- **Core assumption:** Separate vocabulary construction followed by merging preserves language-specific tokenization needs while creating a unified system.
- **Evidence anchors:**
  - [abstract] "we constructed vocabularies separately for English, Chinese, and Indonesian, and subsequently merged them"
  - [section] "To improve training efficiency and optimize downstream task performance, we undertaken the retraining of tokenizers utilizing the byte pair encoding (BPE) algorithm... Vocabulary Building: A multilingual vocabulary is built to support English, Chinese and Indonesian languages"
  - [corpus] Weak - corpus provides context but no direct evidence for this specific approach
- **Break condition:** If merged vocabularies create too many ambiguous tokens or if language-specific tokenization needs conflict significantly, compression efficiency may degrade.

### Mechanism 3
- **Claim:** Direct Preference Optimization (DPO) provides stable alignment with human preferences without the instability of RLHF.
- **Mechanism:** DPO reframes alignment as a classification problem on human preference data, directly optimizing the model to produce responses that humans prefer. This elegant solution eliminates the need for reward modeling and costly LM sampling during fine-tuning.
- **Core assumption:** Human preference data can be effectively used to classify and optimize model outputs without the complexity of reinforcement learning.
- **Evidence anchors:**
  - [abstract] "to reinforce the model's alignment with human preference behaviors, we have embraced the principle of Direct Preference Optimization (DPO) to obtain CompassLLM-DPO model"
  - [section] "we adopted Direct Preference Optimization (DPO) (Rafailov et al., 2023), a novel approach that directly learns to align with human preference without complicated Reinforcement Learning process"
  - [corpus] Weak - corpus provides related work context but no direct evidence for DPO effectiveness
- **Break condition:** If human preference data is insufficient, noisy, or doesn't capture the full range of desired behaviors, DPO may fail to achieve meaningful alignment.

## Foundational Learning

- **Concept:** Curriculum learning in multi-stage pre-training
  - Why needed here: Southeast Asian languages have limited resources compared to English and Chinese, so gradual exposure helps the model adapt without being overwhelmed
  - Quick check question: What is the primary advantage of starting with high-resource languages and gradually introducing low-resource languages in curriculum learning?

- **Concept:** Vocabulary construction and tokenization for multilingual models
  - Why needed here: Different languages have different character sets and morphological structures requiring appropriate tokenization strategies
  - Quick check question: Why does the model use different vocabulary sizes for English/Chinese (32K each) versus Indonesian (16K)?

- **Concept:** Alignment techniques for instruction following
  - Why needed here: Raw language models predict text continuations but need alignment to follow human instructions effectively
  - Quick check question: What is the key difference between Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) in model alignment?

## Architecture Onboarding

- **Component map:**
  Data processing pipeline → Tokenizer training → Multi-stage curriculum learning → Model training → Instruction data collection → SFT fine-tuning → Safety pipeline → DPO fine-tuning → Context extension methods → Acceleration optimizations → Quantization

- **Critical path:** Data processing → Vocabulary construction → Multi-stage pre-training → SFT alignment → DPO alignment → Inference optimization

- **Design tradeoffs:**
  - Separate vocabularies vs. single unified vocabulary: Separate allows better language-specific coverage but increases complexity
  - Curriculum learning vs. uniform training: Curriculum improves low-resource language performance but requires careful scheduling
  - DPO vs. RLHF: DPO is more stable and efficient but may capture less nuanced preferences

- **Failure signatures:**
  - Poor multilingual performance: Indicates curriculum learning or vocabulary construction issues
  - Alignment failures: Suggests problems with instruction data quality or DPO implementation
  - Inference bottlenecks: Points to inadequate optimization or quantization strategies

- **First 3 experiments:**
  1. Test curriculum learning progression by training with different language ratios at various stages
  2. Evaluate vocabulary merging by comparing compression efficiency and tokenization quality across languages
  3. Compare DPO vs. RLHF alignment by measuring preference alignment quality and training stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CompassLLM model perform on low-resource languages other than Indonesian, such as Thai, Vietnamese, and Burmese?
- Basis in paper: [inferred] The paper mentions that CompassLLM was designed to support Southeast Asian languages, with a primary focus on Indonesian. However, it does not provide specific performance results for other Southeast Asian languages.
- Why unresolved: The paper does not include experimental results or evaluations for languages other than Indonesian, Chinese, and English. This leaves a gap in understanding the model's performance on a broader range of Southeast Asian languages.
- What evidence would resolve it: Conducting experiments and evaluations on CompassLLM using datasets and benchmarks for Thai, Vietnamese, Burmese, and other Southeast Asian languages would provide concrete evidence of the model's performance across a wider linguistic landscape.

### Open Question 2
- Question: What is the impact of the multi-stage pre-training strategy with curriculum learning on the model's ability to handle low-resource languages compared to a single-stage pre-training approach?
- Basis in paper: [explicit] The paper describes the use of a multi-stage pre-training strategy integrated with curriculum learning to gradually increase the focus on low-resource languages. However, it does not provide a direct comparison with a single-stage pre-training approach.
- Why unresolved: Without a comparative analysis, it is unclear whether the multi-stage pre-training strategy with curriculum learning significantly improves the model's performance on low-resource languages compared to a simpler pre-training approach.
- What evidence would resolve it: Conducting experiments that compare the performance of CompassLLM with a multi-stage pre-training strategy to a version trained with a single-stage pre-training approach would provide evidence of the effectiveness of the curriculum learning methodology.

### Open Question 3
- Question: How does the CompassLLM model's performance on safety and bias mitigation compare to other large language models when dealing with low-resource languages?
- Basis in paper: [explicit] The paper discusses the implementation of safety measures and bias mitigation techniques, such as the moderation API and safety fine-tuning dataset. However, it does not provide a direct comparison of the model's performance on these aspects with other large language models in the context of low-resource languages.
- Why unresolved: The paper does not include a comparative analysis of CompassLLM's safety and bias mitigation capabilities with other models, particularly in the context of low-resource languages. This leaves uncertainty about the effectiveness of the implemented safety measures.
- What evidence would resolve it: Conducting experiments that evaluate and compare the safety and bias mitigation performance of CompassLLM with other large language models, using datasets and metrics specifically designed for low-resource languages, would provide evidence of the model's relative strengths and weaknesses in these areas.

## Limitations

- The model's performance is primarily benchmarked against open-source models rather than state-of-the-art proprietary models, limiting the comprehensiveness of performance comparisons.
- The curriculum learning strategies lack detailed implementation specifications for pacing functions and transition criteria, making exact replication challenging.
- The human preference data generation through GPT-4 scoring introduces potential bias from the scoring model's own limitations and preferences.

## Confidence

**High Confidence:**
- The multi-stage pre-training with curriculum learning improves multilingual capabilities
- Separate vocabulary construction followed by merging achieves efficient tokenization
- DPO provides stable alignment compared to RLHF

**Medium Confidence:**
- Superior performance in Southeast Asian languages compared to benchmark models
- 128K context length extension with acceptable perplexity
- Commercial deployment readiness with described optimizations

**Low Confidence:**
- Absolute performance claims without comparison to proprietary models
- Safety alignment effectiveness in real-world scenarios
- Long-context task performance beyond perplexity metrics

## Next Checks

1. **Independent reproducibility test:** Reimplement the curriculum learning strategies with specified parameters and verify the claimed performance improvements on Indonesian language tasks. This should include testing different pacing functions and transition points between curriculum stages.

2. **Benchmark expansion validation:** Evaluate CompassLLM against both open-source and proprietary state-of-the-art models (GPT-4, Claude) on the same Southeast Asian language benchmarks to establish relative performance positioning and validate superiority claims.

3. **Long-context task validation:** Design and execute qualitative assessments of the 128K context model on real-world long-context tasks (document analysis, multi-turn conversation analysis) beyond perplexity measurements to verify practical utility.