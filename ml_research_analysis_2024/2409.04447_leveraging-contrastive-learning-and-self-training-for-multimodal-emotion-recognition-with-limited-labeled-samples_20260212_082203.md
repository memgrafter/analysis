---
ver: rpa2
title: Leveraging Contrastive Learning and Self-Training for Multimodal Emotion Recognition
  with Limited Labeled Samples
arxiv_id: '2409.04447'
source_url: https://arxiv.org/abs/2409.04447
tags:
- data
- learning
- recognition
- emotion
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multimodal emotion recognition under limited
  labeled data conditions. The authors propose a semi-supervised framework combining
  contrastive learning and self-training.
---

# Leveraging Contrastive Learning and Self-Training for Multimodal Emotion Recognition with Limited Labeled Samples

## Quick Facts
- arXiv ID: 2409.04447
- Source URL: https://arxiv.org/abs/2409.04447
- Authors: Qi Fan; Yutong Li; Yi Xin; Xinyu Cheng; Guanglai Gao; Miao Ma
- Reference count: 10
- Primary result: Achieved 88.25% weighted average F-score on MER2024-SEMI challenge

## Executive Summary
This paper addresses the challenge of multimodal emotion recognition when labeled data is scarce. The authors propose a semi-supervised framework that combines contrastive learning with self-training to effectively leverage both limited labeled and abundant unlabeled data. The approach introduces a modality representation combinatorial contrastive learning (MR-CCL) framework to learn robust feature representations, followed by self-training with pseudo-labels to expand the training set. The method achieves strong performance on the MER2024-SEMI dataset, ranking 6th with an 88.25% weighted average F-score.

## Method Summary
The authors propose a semi-supervised framework for multimodal emotion recognition that combines contrastive learning and self-training. The method uses a MR-CCL framework to learn robust feature representations from both labeled and unlabeled data, incorporating noise embedding enhancement for data augmentation. A self-training approach with pseudo-label generation expands the training set, while a multi-classifier weighted voting strategy improves prediction robustness. The approach specifically addresses class imbalance through oversampling and demonstrates effectiveness on the MER2024-SEMI dataset with limited labeled samples (5,030) and abundant unlabeled data (115,595).

## Key Results
- Achieved 88.25% weighted average F-score on MER2024-SEMI test set
- Ranked 6th on the official MER2024-SEMI leaderboard
- Successfully addressed class imbalance through oversampling to 1,000 samples per emotion class
- Demonstrated effective integration of contrastive learning and self-training strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MR-CCL framework improves emotion recognition by combining modality-specific and modality-invariant feature learning.
- Mechanism: The framework uses specificity encoders to extract modality-specific embeddings and an invariant encoder to learn cross-modal relationships. This dual representation allows the model to capture both individual modality characteristics and shared emotional information across modalities.
- Core assumption: Modality-specific and invariant features can be effectively separated and learned using contrastive learning techniques.
- Evidence anchors:
  - [abstract]: "we propose a modality representation combinatorial contrastive learning (MR-CCL) framework on the trimodal input data to establish robust initial models"
  - [section]: "Each specificity encoder is composed of multiple Transformer layers, while the invariant encoder is structured with linear layers"
  - [corpus]: Weak - no direct corpus evidence supporting this specific contrastive learning mechanism
- Break condition: If the contrastive learning objectives fail to properly align modality-specific and invariant features, the dual representation may not provide complementary information, leading to suboptimal performance.

### Mechanism 2
- Claim: Self-training with pseudo-labels expands the training set and improves model generalization.
- Mechanism: The model generates pseudo-labels for unlabeled data using high-confidence predictions from multiple classifiers. These pseudo-labeled samples are then added to the training set, providing additional diverse training examples.
- Core assumption: The pseudo-labels generated by the model are sufficiently accurate to improve training without introducing too much noise.
- Evidence anchors:
  - [abstract]: "we explore a self-training approach to expand the training set"
  - [section]: "We apply a confidence threshold to select predictions with high confidence. These high-confidence pseudo-labeled samples are incorporated into the training set as additional labeled data"
  - [corpus]: Weak - no direct corpus evidence supporting the specific self-training approach used in this paper
- Break condition: If the pseudo-labeling process introduces too many incorrect labels, the model may learn from noisy examples, degrading performance.

### Mechanism 3
- Claim: Multi-classifier weighted voting improves prediction robustness by combining diverse model perspectives.
- Mechanism: Four classifiers (one fusion and three unimodal) generate predictions with different strengths. A weighted voting strategy combines these predictions, leveraging the strengths of each classifier while mitigating individual weaknesses.
- Core assumption: Different classifiers have complementary strengths across emotion categories, making weighted combination beneficial.
- Evidence anchors:
  - [abstract]: "we enhance prediction robustness through a multi-classifier weighted soft voting strategy"
  - [section]: "The process begins with aggregating confidence scores from the four classifiers and assigning specific weights to each result"
  - [corpus]: Weak - no direct corpus evidence supporting this specific weighted voting approach
- Break condition: If classifier predictions are highly correlated or if weighting is poorly calibrated, the ensemble may not provide significant improvement over individual models.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Contrastive learning helps the model learn meaningful representations by pulling similar samples together and pushing dissimilar samples apart, which is crucial for learning robust emotion features from limited labeled data
  - Quick check question: What is the main objective of contrastive learning in the MR-CCL framework?

- Concept: Semi-supervised learning
  - Why needed here: Semi-supervised learning leverages both labeled and unlabeled data to improve model performance when labeled data is scarce, which is the primary challenge in this MER2024-SEMI challenge
  - Quick check question: How does self-training with pseudo-labels fit into the broader semi-supervised learning framework?

- Concept: Ensemble learning
  - Why needed here: Ensemble methods combine multiple models to reduce variance and improve generalization, which helps address the robustness concerns in emotion recognition where different modalities may have varying reliability
  - Quick check question: What is the advantage of using weighted voting over simple majority voting in this context?

## Architecture Onboarding

- Component map: Feature extraction layer (CLIP-Large for visual, Chinese-HuBert-Large for audio, Baichuan2-13B-Base for text) → MR-CCL framework (specificity encoders + invariant encoder + contrastive learning modules) → Self-training module (pseudo-label generation + confidence filtering + training set augmentation) → Voting layer (four classifiers with weighted soft voting)

- Critical path: Feature extraction → MR-CCL pretraining → Initial model training → Pseudo-label generation → Augmented training → Weighted voting

- Design tradeoffs:
  - Using large pre-trained models increases feature quality but requires significant computational resources
  - The two-step training process (without pseudo-labels first, then with) balances stability and data utilization
  - Weighted voting adds complexity but improves robustness compared to single-model approaches

- Failure signatures:
  - If MR-CCL fails: Poor performance on both labeled and unlabeled data, suggesting feature learning issues
  - If self-training fails: Performance degrades after adding pseudo-labels, indicating noisy pseudo-labels
  - If voting fails: Single classifiers perform better than ensemble, suggesting poor weight calibration or classifier correlation

- First 3 experiments:
  1. Test baseline performance without MR-CCL or self-training to establish the performance gap
  2. Validate MR-CCL pretraining by comparing with randomly initialized encoders on the validation set
  3. Test pseudo-label quality by checking agreement rates between different classifiers before adding to training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the noise embedding enhancement strategy compare to other data augmentation techniques in terms of effectiveness and computational efficiency?
- Basis in paper: [explicit] The paper mentions that various data augmentation techniques can be applied but introduces NEE as a simpler alternative, without comparing it to other methods.
- Why unresolved: The paper only presents NEE without benchmarking against other augmentation techniques like SpecAugment or CutMix.
- What evidence would resolve it: Comparative experiments showing performance and computational costs of NEE versus traditional augmentation methods.

### Open Question 2
- Question: What is the optimal confidence threshold for pseudo-label selection across different emotion classes?
- Basis in paper: [explicit] The paper uses different thresholds (0.99 for dominant classes, 0.85 for minority classes) but doesn't explore the full range of possible thresholds.
- Why unresolved: The threshold values appear to be selected empirically without systematic exploration of the parameter space.
- What evidence would resolve it: Ablation studies testing multiple threshold values and their impact on F-score across all emotion classes.

### Open Question 3
- Question: How does the performance of the modality fusion classifier compare to the weighted voting strategy?
- Basis in paper: [inferred] The paper uses a weighted voting strategy but doesn't directly compare it to using only the fusion classifier.
- Why unresolved: The contribution of each component in the ensemble approach is not isolated and evaluated independently.
- What evidence would resolve it: Experiments comparing the weighted voting ensemble to using only the fusion classifier or other combinations of classifiers.

## Limitations

- The paper lacks detailed implementation specifics for critical components like the MR-CCL framework architecture and pseudo-label generation thresholds
- No ablation studies are provided to isolate the contribution of individual components (contrastive learning vs. self-training vs. ensemble voting)
- The approach's generalization to other multimodal tasks beyond the MER2024-SEMI dataset remains untested

## Confidence

- High confidence: The general framework combining contrastive learning and self-training is sound and addresses a well-known problem in limited-data scenarios
- Medium confidence: The specific implementation details and hyperparameter choices are likely effective but cannot be fully verified without more information
- Low confidence: The exact contribution of each component to the final performance remains unclear due to lack of ablation analysis

## Next Checks

1. Implement the MR-CCL framework with varying levels of unlabeled data to test the scalability of the contrastive learning approach
2. Conduct ablation studies removing one component at a time (contrastive learning, self-training, ensemble voting) to quantify individual contributions
3. Test the model on an external multimodal emotion recognition dataset to evaluate generalization beyond the MER2024-SEMI challenge data