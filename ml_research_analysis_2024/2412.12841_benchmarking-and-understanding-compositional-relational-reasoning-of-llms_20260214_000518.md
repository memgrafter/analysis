---
ver: rpa2
title: Benchmarking and Understanding Compositional Relational Reasoning of LLMs
arxiv_id: '2412.12841'
source_url: https://arxiv.org/abs/2412.12841
tags:
- heads
- tasks
- 'true'
- 'false'
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of understanding how transformer
  large language models (LLMs) perform compositional relational reasoning (CRR). To
  enable systematic exploration, the authors introduce the Generalized Associative
  Recall (GAR) benchmark, a synthetic dataset that integrates and generalizes several
  mechanistic interpretability (MI) tasks into a unified framework with varied forms
  and difficulty levels.
---

# Benchmarking and Understanding Compositional Relational Reasoning of LLMs

## Quick Facts
- arXiv ID: 2412.12841
- Source URL: https://arxiv.org/abs/2412.12841
- Reference count: 40
- LLMs show fundamental deficiencies in compositional relational reasoning despite apparent simplicity of tasks

## Executive Summary
This work introduces the Generalized Associative Recall (GAR) benchmark to systematically study how transformer large language models perform compositional relational reasoning. The synthetic GAR dataset generalizes multiple mechanistic interpretability tasks into a unified framework that reveals LLMs' fundamental limitations in CRR. The authors evaluate mainstream models including Llama-2/3, Vicuna, and GPT variants, finding that even GPT-4 achieves only 71.5% average accuracy, with compositionality gaps increasing with model size.

Through mechanistic interpretability analysis using attribution patching, the authors identify core circuits and vital attention heads reused across GAR tasks. They discover two classes of heads encoding abstract truth values (true/false) that are crucial for CRR performance. Intervention experiments confirm these heads' importance, and their activation patterns generalize to other datasets like SNLI and GoT, validating broader relevance of the findings.

## Method Summary
The authors evaluate existing LLMs on the GAR benchmark using in-context one-shot learning format without fine-tuning. They employ attribution patching to identify core circuits and vital attention heads in Vicuna-33B, then validate these through intervention experiments. The GAR dataset is programmatically generated with varying difficulty levels controlled by non-same semantic relations. Additional validation is performed on SNLI and Game of Thrones datasets using pre-trained MLP classifiers to detect True/False head activations.

## Key Results
- GAR benchmark reveals compositionality gaps even in advanced models like GPT-4 (71.5% accuracy)
- Compositionality gaps increase with model size, indicating fundamental CRR deficiencies
- True/False heads identified in Vicuna-33B are validated through intervention experiments showing 6-17% performance improvements
- These heads' activation patterns generalize to SNLI and GoT datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: True/False heads encode abstract truth values by comparing higher-order relational patterns between statement components
- Mechanism: These heads activate when relational loop structures produce matching vs non-matching semantic relations through composed attention paths
- Core assumption: Attention mechanism can represent logical truth through pattern matching without explicit logical operations
- Evidence anchors: Identification of True/False heads in Vicuna models; intervention experiments increasing accuracy by 6-17%; weak corpus evidence

### Mechanism 2
- Claim: Higher-order induction heads bridge context across few-shot demonstrations by transmitting answer information
- Mechanism: These heads attend from answer tokens in previous demonstrations to current query positions, enabling activation of local syntactic and relating heads
- Core assumption: Few-shot demonstration information can be effectively routed through induction heads
- Evidence anchors: Role of higher-order induction heads in bridging different attention head classes; weak corpus evidence

### Mechanism 3
- Claim: Compositionality gaps increase with model size due to more complex circuits failing to generalize
- Mechanism: Larger models develop sophisticated attention patterns that become overfit to training distribution patterns
- Core assumption: Model scaling introduces complexity that reduces compositional generalization
- Evidence anchors: Increasing compositionality gap across Llama series; weak corpus evidence

## Foundational Learning

- Concept: Relational loops as fundamental reasoning motifs
  - Why needed here: GAR tasks are designed around relational loops that interleave semantic and syntactic relations - understanding this structure is crucial for interpreting circuit mechanisms
  - Quick check question: How does the relational loop structure in GAR ensure predictability of answers?

- Concept: Attention head compositionality
  - Why needed here: Circuit analysis reveals complex reasoning emerges from composition of simpler attention heads - understanding how heads compose is essential for circuit interpretation
  - Quick check question: What is the difference between basic and higher-order attention heads in the GAR circuits?

- Concept: In-context learning mechanisms
  - Why needed here: Higher-order induction heads demonstrate how few-shot demonstrations influence current predictions - understanding ICL is crucial for interpreting demonstration examples
  - Quick check question: How do induction heads enable information transfer from previous demonstrations to current task processing?

## Architecture Onboarding

- Component map: GAR benchmark consists of relational schemas (commonsense/factual), semantic variations (negate, g2c), syntactic variations (swapQA/KV), and difficulty control through non-same semantic relations. Circuits involve predicting heads, pre-predicting heads, local syntactic heads, relating heads, induction heads, and higher-order variants.

- Critical path: For affirmative generation tasks: K-V pairs → relating heads → local heads → pre-predicting heads → predicting heads → output. For classification: K-V pairs → relating heads → higher-order relating heads → MLP → Yes/No output.

- Design tradeoffs: GAR uses synthetic data for controlled difficulty but may not capture all real-world reasoning complexity. Benchmark balances tractability for MI study with sufficient challenge to reveal LLM limitations.

- Failure signatures: Performance drops on tasks with multiple non-same semantic relations (nr=2) and negation indicate compositionality gaps. Poor attention weight matching for higher-order heads reveals circuit formation issues.

- First 3 experiments:
  1. Test Vicuna-33B on GAR tasks with varying nr values to confirm compositionality gap relationship
  2. Apply weak intervention on identified vital heads to validate their importance
  3. Extract True/False head activations from classification tasks to verify truth value encoding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the identified True/False heads' attention patterns generalize to other types of relational reasoning tasks beyond GAR and tested datasets?
- Basis in paper: Authors identify True/False heads validated on SNLI and GoT but don't explore generalization to other relational reasoning tasks
- Why unresolved: Paper doesn't explore whether these heads can generalize to other relational reasoning tasks with different structures or complexities
- What evidence would resolve it: Testing on diverse relational reasoning benchmarks (MCR, LAMBADA, other NLI datasets) and analyzing attention patterns and performance

### Open Question 2
- Question: What are limitations of current mechanistic interpretability methods in discovering circuits for more complex CRR tasks?
- Basis in paper: Authors note automated circuit discovery methods struggle with model size and circuit complexity of Vicuna-33B
- Why unresolved: Paper doesn't explore alternative methods or improvements to MI techniques for larger models or complex tasks
- What evidence would resolve it: Developing and testing new automated circuit discovery methods on larger models and complex CRR tasks

### Open Question 3
- Question: How does LLM performance on GAR tasks scale with model size, and what architectural changes could mitigate compositionality gap?
- Basis in paper: Authors observe increasing compositionality gap with model size but don't investigate architectural modifications
- Why unresolved: Paper focuses on analyzing existing models but not potential architectural changes or training strategies
- What evidence would resolve it: Experimenting with architectural modifications and evaluating impact on GAR performance and compositionality gap

## Limitations

- Dataset Generalization: GAR uses synthetic data with controlled complexity that may not fully capture real-world compositional reasoning challenges
- Circuit Specificity: Mechanistic explanations rely heavily on qualitative interpretation of attention patterns with weak corpus evidence support
- Scaling Paradox: Observation that compositionality gaps increase with model size is counterintuitive and requires further validation

## Confidence

**High Confidence**:
- GAR benchmark effectively reveals compositionality gaps in LLMs
- Vicuna-33B analysis can identify vital attention heads through attribution patching
- Intervention experiments confirm importance of identified heads for performance

**Medium Confidence**:
- True/False heads encode abstract truth values through pattern matching
- Higher-order induction heads enable in-context learning by bridging demonstrations
- Compositionality gaps increase with model size due to circuit complexity

**Low Confidence**:
- Detailed mechanistic explanations of how True/False heads encode logical truth
- Specific bridging mechanisms of higher-order induction heads
- Causal relationship between scaling and compositionality gaps

## Next Checks

1. **Cross-dataset Circuit Validation**: Apply True/False head detection method to additional datasets beyond SNLI and GoT to test generalization across different reasoning domains and task structures.

2. **Scaling Behavior Analysis**: Conduct systematic experiments across wider range of model sizes to determine whether increasing compositionality gap is consistent pattern or specific to tested models.

3. **Causal Circuit Intervention**: Design targeted interventions that modify activation patterns of True/False heads and induction heads to test whether mechanistic explanations correctly predict effects on model behavior across different task variations.