---
ver: rpa2
title: 'MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language
  Models'
arxiv_id: '2410.17637'
source_url: https://arxiv.org/abs/2410.17637
tags:
- image
- data
- multi-image
- images
- mia-dpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MIA-DPO, a visual preference alignment approach
  designed to enhance Large Vision-Language Models' (LVLMs) ability to handle multi-image
  inputs. The method addresses the scarcity of diverse multi-image training data and
  high annotation costs by extending single-image datasets with unrelated images in
  grid collages or pic-in-pic formats.
---

# MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models

## Quick Facts
- arXiv ID: 2410.17637
- Source URL: https://arxiv.org/abs/2410.17637
- Reference count: 32
- Multi-image understanding improvements of 3.0% on LLaVA-v1.5 and 4.3% on InternLM-XC2.5

## Executive Summary
MIA-DPO addresses the challenge of improving large vision-language models' (LVLMs) multi-image understanding capabilities by introducing a visual preference alignment approach. The method tackles the scarcity of diverse multi-image training data and high annotation costs through a novel data augmentation strategy that extends single-image datasets with unrelated images in grid collages or pic-in-pic formats. By leveraging attention values from LVLMs themselves, MIA-DPO can identify and filter out hallucinated responses without requiring human annotation, extra data, or external models.

The approach demonstrates significant improvements across five multi-image benchmarks, achieving average performance boosts of 3.0% on LLaVA-v1.5 and 4.3% on InternLM-XC2.5 while maintaining strong single-image understanding capabilities. This represents a practical solution to the data scarcity problem in multi-image training, making it particularly valuable for real-world applications where diverse multi-image scenarios are common but difficult to annotate at scale.

## Method Summary
MIA-DPO introduces a visual preference alignment framework that extends Direct Preference Optimization (DPO) to multi-image scenarios. The core innovation lies in using attention values from LVLMs to detect hallucinations without external supervision. The method generates augmented training data by combining single-image samples with unrelated images in grid or pic-in-pic formats, creating synthetic multi-image pairs. During training, MIA-DPO compares model responses to identify hallucinated outputs based on attention patterns, then applies preference optimization to favor non-hallucinated responses. This self-supervised approach eliminates the need for human annotations while addressing the fundamental challenge of limited multi-image training data.

## Key Results
- Outperforms existing methods on five multi-image benchmarks
- Achieves average performance boost of 3.0% on LLaVA-v1.5
- Achieves average performance boost of 4.3% on InternLM-XC2.5
- Maintains strong single-image understanding capabilities while improving multi-image performance

## Why This Works (Mechanism)
The approach works by leveraging the internal attention mechanisms of LVLMs to detect hallucinations, which is more efficient than external supervision. By augmenting single-image data with unrelated images, the method creates diverse training scenarios that expose models to various multi-image configurations without requiring expensive annotations. The attention-based hallucination detection is particularly effective because it captures subtle inconsistencies in how models process visual information across multiple images. The preference optimization then reinforces correct behaviors by rewarding responses that demonstrate coherent multi-image understanding while avoiding hallucinated content.

## Foundational Learning

**Direct Preference Optimization (DPO)**: A training method that optimizes models based on pairwise preference comparisons rather than explicit labels. Why needed: Enables learning from implicit preferences without requiring detailed annotations. Quick check: Verify the model can distinguish between preferred and non-preferred responses.

**Attention-based hallucination detection**: Using self-attention patterns to identify when models generate content not supported by visual input. Why needed: Provides an automated way to filter hallucinations without human supervision. Quick check: Confirm attention patterns differ significantly between hallucinated and non-hallucinated responses.

**Multi-image data augmentation**: Creating synthetic multi-image training examples by combining single images with unrelated visuals. Why needed: Addresses the scarcity of diverse multi-image training data. Quick check: Ensure augmented examples maintain reasonable visual relationships and don't introduce spurious correlations.

**Visual preference alignment**: Training models to align their outputs with human preferences in visual tasks. Why needed: Improves model behavior to better match user expectations. Quick check: Validate that preference-aligned models produce more useful and accurate responses.

## Architecture Onboarding

**Component map**: Single-image dataset -> Data augmentation (grid/pic-in-pic) -> MIA-DPO training -> Attention-based hallucination filtering -> Preference optimization -> Improved LVLM

**Critical path**: Data augmentation → Attention-based hallucination detection → Preference optimization. This sequence is essential because the quality of augmented data directly impacts hallucination detection accuracy, which in turn determines the effectiveness of preference learning.

**Design tradeoffs**: The method trades computational complexity (attention analysis) for annotation cost savings. While attention-based detection adds overhead, it eliminates the need for human labels. The grid and pic-in-pic augmentation strategies prioritize diversity over natural scene coherence, which may affect real-world generalization.

**Failure signatures**: Poor attention-based hallucination detection leading to incorrect preference signals, over-reliance on augmented data causing models to expect artificial visual relationships, and potential degradation of single-image performance if multi-image training dominates.

**3 first experiments**:
1. Baseline comparison: Train with and without MIA-DPO on the same augmented dataset to isolate the contribution of attention-based preference optimization
2. Hallucination detection validation: Manually annotate a subset of responses to verify the accuracy of attention-based hallucination detection
3. Data augmentation ablation: Compare performance using grid vs. pic-in-pic formats and different degrees of augmentation to find optimal balance

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on attention-based hallucination detection may not generalize across diverse visual domains or cultural contexts
- Augmented data generation using artificial collage formats may introduce unrealistic visual relationships
- Performance improvements, while significant, are modest and may not scale proportionally with larger models
- Real-world applicability remains uncertain due to limited benchmark diversity and lack of extensive out-of-distribution testing

## Confidence

**High confidence**: The technical feasibility of the MIA-DPO approach and its ability to improve multi-image benchmark performance as reported

**Medium confidence**: The generalization of hallucination detection criteria and the effectiveness of augmented data generation methods

**Low confidence**: The real-world applicability and scalability of the approach across diverse visual domains and model architectures

## Next Checks
1. Test the attention-based hallucination detection on out-of-distribution visual data and across different cultural contexts to assess robustness
2. Evaluate the augmented data generation approach on naturally occurring multi-image datasets to verify that the improvements generalize beyond artificial collage formats
3. Conduct ablation studies to determine the relative contribution of each component (attention filtering, data augmentation) to the reported performance gains