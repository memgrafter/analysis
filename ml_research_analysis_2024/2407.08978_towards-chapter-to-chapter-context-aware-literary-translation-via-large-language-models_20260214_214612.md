---
ver: rpa2
title: Towards Chapter-to-Chapter Context-Aware Literary Translation via Large Language
  Models
arxiv_id: '2407.08978'
source_url: https://arxiv.org/abs/2407.08978
tags:
- translation
- machine
- dataset
- language
- literary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of context-aware machine translation
  for literary texts by proposing a chapter-to-chapter (Ch2Ch) translation setting
  and dataset. The authors curate a novel dataset of Chinese-English literature, JAM,
  comprising 160 books with 5,373 parallel chapters.
---

# Towards Chapter-to-Chapter Context-Aware Literary Translation via Large Language Models

## Quick Facts
- arXiv ID: 2407.08978
- Source URL: https://arxiv.org/abs/2407.08978
- Reference count: 19
- Authors: Linghao Jin; Li An; Xuezhe Ma
- Key outcome: Two-stage fine-tuning of LLMs on chapter-aligned literary data achieves BLEU scores of 16.80-15.70, outperforming baselines but struggling with repetition in long-context generation

## Executive Summary
This paper addresses the challenge of context-aware machine translation for literary texts by proposing a chapter-to-chapter (Ch2Ch) translation setting and dataset. The authors curate a novel dataset of Chinese-English literature, JAM, comprising 160 books with 5,373 parallel chapters. They investigate the performance of commonly-used machine translation models under this setting and introduce a finetuning approach for large language models (LLMs) within the domain of Ch2Ch literary translation. The finetuned LLMs yield impressive improvements over baselines, with BLEU scores of 16.80 and 15.70 for two different finetuning strategies.

## Method Summary
The study proposes chapter-to-chapter translation by extending context-aware translation to chapter-level alignments, addressing limitations of sentence and paragraph-level datasets. The authors curate the JAM corpus of 160 Chinese-English literary books with 5,373 parallel chapters, then segment chapters into chunks (≤2048 tokens) for training. They employ a two-stage fine-tuning approach: first training on general parallel data (WMT22), then adapting to the JAM dataset. Both baseline Transformer models and LLMs (ALMA-7B, LLaMA-2-7B, GPT-4) are evaluated using automatic metrics including BLEU, COMET, and BlonDe (for pronoun, entity, tense, and discourse marker accuracy).

## Key Results
- LLMs finetuned on JAM achieve BLEU scores of 16.80 (ALMA-7B) and 15.70 (LLaMA-2-7B), outperforming baseline models
- Two-stage fine-tuning (sentence-level → chapter-level) improves BLEU from 15.7 to 16.8 and BlonDe from 33.46 to 35.05
- Decoder-only LLMs show competitive performance after fine-tuning, though encoder-decoder models slightly outperform them on most metrics
- Repetition generation emerges as a significant challenge, particularly in the first half of translations and for sentences exceeding 1000 tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chapter-to-chapter translation setting addresses limitations of sentence and paragraph alignment assumptions by providing richer discourse context.
- Mechanism: Literary texts contain complex discourse structures and equivocal paragraph splits. Chapter-level alignment ensures more comprehensive context, reducing issues like tense mismatches and pronoun resolution problems.
- Core assumption: Chapter-level context is more informative and stable than paragraph-level context for literary translation.
- Evidence anchors:
  - [abstract]: "Discourse phenomena in existing document-level translation datasets are sparse, which has been a fundamental obstacle..."
  - [section 3.1]: "To address this issue, we propose chapter-to-chapter (CH2CH) translation, a pragmatic and challenging setting, by extending context-aware translation to chapter-level."
  - [corpus]: Weak - The JAM corpus provides chapter-aligned data, but no explicit discourse phenomenon annotation is mentioned to validate the richness of chapter-level context.
- Break condition: If chapter alignment introduces more noise than signal (e.g., chapters are too long or structurally inconsistent), the benefit diminishes.

### Mechanism 2
- Claim: Two-stage fine-tuning (sentence-level → chapter-level) significantly improves LLM translation performance.
- Mechanism: Initial sentence-level fine-tuning on large datasets (e.g., WMT22) provides basic translation capability, then chapter-level fine-tuning on JAM adapts the model to discourse-level phenomena and long-context generation.
- Core assumption: Sentence-level translation is a necessary prerequisite for effective chapter-level literary translation.
- Evidence anchors:
  - [section 5.1]: "Encoder-Decoder and Decoder-only Transformer models trained from scratch on JAM significantly under-perform the models trained with the 2-stage procedure."
  - [section 5.2]: "Table 3 indicates that such sentence-level fine-tuning improves BLEU from 15.7 to 16.8 and BlonDe from 33.46 to 35.05..."
  - [corpus]: Weak - No explicit comparison of sentence-level vs. chapter-level fine-tuning ablation without the intermediate step.
- Break condition: If the sentence-level data distribution is too different from literary texts, the two-stage approach may introduce negative transfer.

### Mechanism 3
- Claim: Decoder-only LLM architecture with causal language modeling is effective for chapter-level translation after appropriate fine-tuning.
- Mechanism: Decoder-only models like LLaMA-2 can leverage their text generation capabilities for translation when fine-tuned on parallel data, outperforming encoder-decoder models on document-level metrics.
- Core assumption: The inherent language understanding and generation capabilities of decoder-only LLMs transfer well to translation tasks when properly adapted.
- Evidence anchors:
  - [section 5.3]: "After finetuning on JAM, though Encoder-Decoder perform slightly better than Decoder-only model, yet still under-perform ALMA models on most of the evaluation metrics..."
  - [section 4.1]: "Decoder-only Architecture... Motivated by Zhang et al. (2018), we experiment with training a baseline model on the JAM dataset from scratch..."
  - [corpus]: Weak - No ablation study comparing decoder-only vs. encoder-decoder architectures without fine-tuning on JAM.
- Break condition: If the decoder-only model fails to handle the bidirectional information flow needed for translation, performance degrades.

## Foundational Learning

- Concept: Discourse phenomena in translation (anaphora resolution, lexical cohesion, tense consistency)
  - Why needed here: Literary translation requires maintaining coherence across sentences and paragraphs, which sentence-level models struggle with.
  - Quick check question: Can you identify examples of pronoun resolution problems that might occur when translating dialogue-heavy literary text without context?

- Concept: Context-aware neural machine translation (document-level vs. sentence-level)
  - Why needed here: Understanding the shift from sentence-aligned to chapter-aligned translation and its implications for model architecture and training.
  - Quick check question: What are the key differences between sentence-aligned, paragraph-aligned, and chapter-aligned translation settings?

- Concept: Large language model fine-tuning strategies (pre-training vs. fine-tuning vs. in-context learning)
  - Why needed here: The paper uses a two-stage fine-tuning approach, which requires understanding when and how to apply different training paradigms.
  - Quick check question: When would you choose fine-tuning over in-context learning for adapting an LLM to a new domain?

## Architecture Onboarding

- Component map:
  JAM corpus (chapter-aligned Chinese-English literature) → segmentation into chunks (≤2048 tokens) → training/validation/test splits → Baseline models (Transformer encoder-decoder, decoder-only) and LLMs (ALMA-7B, LLaMA-2-7B, GPT-4) → fine-tuning on JAM → Decoding with repetition detection and removal → Evaluation with automatic metrics (BLEU, COMET, BlonDe)

- Critical path:
  1. Data curation and chapter alignment
  2. Chunk segmentation (≤2048 tokens)
  3. Baseline model training (with/without 2-stage procedure)
  4. LLM fine-tuning (sentence-level → chapter-level)
  5. Decoding with repetition detection and removal
  6. Evaluation with automatic metrics

- Design tradeoffs:
  - Chunk size vs. context preservation: Larger chunks preserve more context but increase training complexity and repetition risk
  - Fine-tuning vs. in-context learning: Fine-tuning provides better adaptation but requires more resources
  - Decoder-only vs. encoder-decoder: Simpler architecture vs. potentially better bidirectional encoding

- Failure signatures:
  - Repetition loops in generated text (especially in long chapters)
  - Performance degradation when context exceeds 1024-2048 tokens
  - Over-reliance on sentence-level patterns that don't generalize to chapter-level discourse

- First 3 experiments:
  1. Train a decoder-only model on JAM from scratch vs. with 2-stage fine-tuning to validate the importance of sentence-level pre-training
  2. Compare repetition rates across different chunk sizes (512, 1024, 2048 tokens) to identify optimal context length
  3. Ablation study: decoder-only vs. encoder-decoder performance on JAM after fine-tuning to validate architectural effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific decoding strategies could effectively prevent repetition in long-context chapter-to-chapter translation?
- Basis in paper: [explicit] The paper identifies repetition as a significant challenge in long-context generation and discusses the need for advanced decoding strategies to address this issue.
- Why unresolved: While the paper mentions the problem of repetition and suggests that it occurs predominantly in the first half of translations, it does not propose specific decoding strategies to mitigate this issue.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of different decoding strategies (e.g., top-k sampling, nucleus sampling, repetition penalty) in reducing repetition while maintaining translation quality would provide a clear solution.

### Open Question 2
- Question: How does the performance of chapter-to-chapter translation compare to paragraph-to-paragraph translation in terms of coherence and fluency?
- Basis in paper: [inferred] The paper introduces chapter-to-chapter translation as a more pragmatic and challenging setting compared to paragraph-to-paragraph translation, suggesting potential benefits in coherence and fluency due to richer context.
- Why unresolved: Although the paper highlights the advantages of chapter-level alignments over paragraph-level alignments, it does not directly compare the two settings in terms of translation quality metrics.
- What evidence would resolve it: Comparative experiments evaluating BLEU, COMET, and other document-level metrics for both chapter-to-chapter and paragraph-to-paragraph translations would provide a clear comparison.

### Open Question 3
- Question: What are the optimal chunk sizes for training and decoding in chapter-to-chapter translation to balance context utilization and computational efficiency?
- Basis in paper: [explicit] The paper experiments with segmenting chapters into chunks of at most 512, 1024, and 2048 tokens, observing performance differences across these settings.
- Why unresolved: While the paper presents results for different chunk sizes, it does not determine the optimal size that maximizes translation quality while maintaining computational efficiency.
- What evidence would resolve it: Detailed analysis and experiments testing a range of chunk sizes, along with computational cost assessments, would identify the optimal balance for training and decoding in chapter-to-chapter translation.

## Limitations
- The evaluation relies on automatic metrics without human assessment of literary quality, cultural nuance, or stylistic preservation
- Repetition generation in long contexts is addressed through post-processing rather than architectural improvements
- No direct comparison with paragraph-level translation approaches to validate the claimed advantages of chapter-level alignment

## Confidence
- **High Confidence**: The dataset curation methodology and basic implementation of chapter-level translation are sound. The identification of repetition as a major challenge in long-context generation is well-supported by the evidence.
- **Medium Confidence**: The effectiveness of two-stage fine-tuning (sentence-level → chapter-level) is supported but could benefit from more rigorous ablation studies. The relative performance of decoder-only vs. encoder-decoder architectures after fine-tuning shows promising trends but requires further validation.
- **Low Confidence**: Claims about the superiority of Ch2Ch setting over existing document-level approaches are based on weak evidence, as no direct comparisons with paragraph-aligned datasets are provided. The effectiveness of repetition post-processing as a general solution is demonstrated but not validated across different model architectures or longer contexts.

## Next Checks
1. **Human evaluation study**: Conduct a blind human evaluation comparing translations from sentence-level, paragraph-level, and chapter-level models on the same literary passages, focusing on coherence, style preservation, and readability rather than just accuracy.

2. **Architectural ablation study**: Compare decoder-only vs. encoder-decoder performance on JAM dataset with identical training procedures (same fine-tuning strategy, same chunk sizes) to isolate the architectural contribution from training effects.

3. **Context length analysis**: Systematically evaluate model performance across different chunk sizes (256, 512, 1024, 2048 tokens) to identify the optimal context length that balances discourse information with repetition risk, and test whether architectural modifications (attention mechanisms, memory structures) can extend this limit.