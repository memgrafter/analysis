---
ver: rpa2
title: 'BASES: Large-scale Web Search User Simulation with Large Language Model based
  Agents'
arxiv_id: '2402.17505'
source_url: https://arxiv.org/abs/2402.17505
tags:
- search
- user
- behavior
- behaviors
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces BASES, a large-scale web search user simulation
  framework that uses LLM-based agents to generate human-like search behaviors. The
  core idea is to construct diverse user profiles through synergistic synthesis and
  use tailored prompting strategies to simulate search queries and clicks.
---

# BASES: Large-scale Web Search User Simulation with Large Language Model based Agents

## Quick Facts
- **arXiv ID**: 2402.17505
- **Source URL**: https://arxiv.org/abs/2402.17505
- **Authors**: Ruiyang Ren; Peng Qiu; Yingqi Qu; Jing Liu; Wayne Xin Zhao; Hua Wu; Ji-Rong Wen; Haifeng Wang
- **Reference count**: 26
- **Primary result**: LLM-based agents generate personalized web search behaviors, improving IR model performance by up to 13% in NDCG@1 using less than 1% of training data

## Executive Summary
BASES introduces a framework for large-scale web search user simulation using LLM-based agents. The core innovation lies in generating diverse user profiles through synergistic synthesis and employing tailored prompting strategies to simulate search queries and clicks. The framework is evaluated on session search and click prediction tasks in both Chinese and English, demonstrating significant improvements over baselines while requiring substantially less training data. To support further research, the authors also develop WARRIORS, a large-scale dataset of 100,000 simulated users.

## Method Summary
BASES constructs diverse user profiles using synergistic synthesis, combining manual definition of attribute values with GPT-4 collaboration for complex attributes. The framework employs two-stage prompting strategies where LLM agents first generate personalized search queries based on user profiles, then select relevant web pages from search results. The simulated behaviors are used to train BERT-based ranking models, which are evaluated on real user benchmarks and low-resource scenarios. The method is implemented using ChatGPT with GPT-3.5-Turbo-1106 as the LLM agent.

## Key Results
- Achieves up to 13% improvement in NDCG@1 over baselines using less than 1% of training data
- Improves performance in low-resource scenarios by up to 7% in NDCG@1
- Generates WARRIORS dataset with 100,000 simulated users in Chinese and English

## Why This Works (Mechanism)

### Mechanism 1
LLM-based agents can generate human-like web search behavior by conditioning on detailed user profiles. The BASES framework creates unique user profiles with static (age, gender, education) and dynamic (interest, location) attributes, then uses tailored prompting strategies to guide agents in generating search queries and clicks that reflect these profiles. Core assumption: LLM agents can accurately interpret user profiles and generate coherent, contextually appropriate search behaviors.

### Mechanism 2
Synergistic synthesis method ensures both efficiency and diversity in user profile generation. The method combines manual definition of attribute values with GPT-4 collaboration for complex attributes, using different sampling strategies based on attribute characteristics. Core assumption: GPT-4 can generate diverse and realistic attribute values when guided by coarse categories and human oversight.

### Mechanism 3
Two-stage prompting strategy (query and click) produces more accurate and personalized user behavior than single-step approaches. Query prompting focuses on generating concise, profile-relevant queries, while click prompting guides selection of relevant web pages based on profile, history, and current query. Core assumption: Separating query and click behaviors into distinct prompts allows the LLM to focus on each task more effectively than a combined approach.

## Foundational Learning

- **User profiling for personalization**: User profiles are the foundation for generating diverse and realistic search behaviors; without them, the simulation would produce generic, non-personalized results. Quick check: What are the two main categories of user attributes in BASES, and how do they differ in terms of stability over time?

- **Prompt engineering for LLM control**: The quality of generated behaviors depends heavily on how the prompts are structured; poorly designed prompts lead to arbitrary or inconsistent outputs. Quick check: Why does BASES use separate prompts for query and click behaviors instead of a single combined prompt?

- **Evaluation metrics for information retrieval**: To measure the effectiveness of BASES-generated behaviors in improving IR tasks, standard metrics like NDCG and MRR are used to compare against baselines. Quick check: What is the difference between NDCG@1 and NDCG@3, and why might both be reported in the evaluation?

## Architecture Onboarding

- **Component map**: User Profile Generator (synergistic synthesis) → LLM Agent (query prompt → click prompt) → Search Engine → WARRIORS Dataset (training/validation/test splits)
- **Critical path**: Profile generation → behavior simulation → dataset creation → model training → evaluation
- **Design tradeoffs**: Synergistic synthesis trades some realism for scalability vs. real-data-based generation; two-stage prompting trades simplicity for precision vs. single-step prompting
- **Failure signatures**: Homogenized behaviors across profiles, low consistency with human benchmarks, poor performance on IR tasks despite large dataset size
- **First 3 experiments**:
  1. Test profile generation with a small set of attributes to validate diversity and logical consistency
  2. Run LLM agent with query-only prompt to verify coherence and profile alignment
  3. Combine query and click prompts to evaluate full behavior simulation and measure consistency against human benchmarks

## Open Questions the Paper Calls Out

- How does the diversity of user profiles generated by synergistic synthesis compare to those generated by real data based generation in terms of improving IR model performance?
- What is the impact of the specific attribute values defined for user profiles on the diversity and realism of simulated search behaviors?
- How does the performance of IR models trained on BASES-generated data compare to those trained on real user data as the volume of training data increases?
- What are the limitations of using LLM-based agents to simulate user behaviors in domains other than web search, such as e-commerce or social media?

## Limitations

- Evaluation focuses primarily on English and Chinese with limited discussion of cross-lingual generalization
- Paper does not address potential biases introduced by LLM-based agents
- Does not explore how well simulated behaviors capture edge cases and rare user behaviors
- Core claims rely heavily on benchmark comparisons rather than direct comparisons with state-of-the-art user simulation methods

## Confidence

- **High Confidence**: The basic premise that LLM-based agents can generate personalized search behaviors when given detailed user profiles is well-supported by the results showing consistent behavior patterns (90% accuracy) against human benchmarks
- **Medium Confidence**: The claim that synergistic synthesis provides optimal balance between efficiency and diversity is supported by the methodology description but lacks direct comparison with alternative profile generation methods
- **Medium Confidence**: The two-stage prompting strategy's superiority over single-step approaches is demonstrated through performance metrics, but ablation studies could be more comprehensive

## Next Checks

1. Conduct detailed analysis of term overlap rates and behavioral consistency across broader range of user profiles to verify synergistic synthesis maintains diversity at scale
2. Implement and compare single-step vs. two-stage prompting approaches across multiple IR tasks to quantify specific contribution of two-stage design
3. Evaluate BASES-generated behaviors and their impact on IR performance across additional languages beyond Chinese and English to assess broader applicability