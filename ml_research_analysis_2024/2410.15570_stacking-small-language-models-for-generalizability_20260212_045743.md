---
ver: rpa2
title: Stacking Small Language Models for Generalizability
arxiv_id: '2410.15570'
source_url: https://arxiv.org/abs/2410.15570
tags:
- language
- fslm
- fine-tuning
- https
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method called fine-tuning stacks of language
  models (FSLM), which involves chaining together small language models (SLM) that
  each specialize in a specific task. The goal is to create a lightweight alternative
  to large language models (LLM) that can run in resource-limited settings.
---

# Stacking Small Language Models for Generalizability

## Quick Facts
- arXiv ID: 2410.15570
- Source URL: https://arxiv.org/abs/2410.15570
- Reference count: 7
- Method fine-tunes stacks of small language models to create lightweight alternatives to large language models

## Executive Summary
This paper introduces Fine-tuning Stacks of Language Models (FSLM), a method that chains together small language models (SLMs) each specialized for specific tasks. The approach aims to create a computationally efficient alternative to large language models (LLMs) that can operate in resource-limited settings. Using four Pythia-160M models fine-tuned on the Alpaca dataset with LoRA adapters, the FSLM stack achieves 0.3349 on tinyArc and 0.3208 on tinyMMLU, outperforming non-adapter Pythia-160M models on both benchmarks.

## Method Summary
The FSLM framework consists of chaining multiple small language models where each model specializes in a particular task or subtask. The specific implementation uses four Pythia-160M models fine-tuned using LoRA adapters on the Alpaca dataset. The models are stacked in sequence to handle complex tasks that would normally require larger models. This architecture leverages the efficiency of small models while attempting to match the performance of larger models through specialization and cooperation.

## Key Results
- FSLM achieves 0.3349 on tinyArc benchmark
- FSLM achieves 0.3208 on tinyMMLU benchmark
- Outperforms non-adapter Pythia-160M models on both tasks

## Why This Works (Mechanism)
The paper does not provide detailed mechanism explanations for why stacking small language models achieves better performance than individual models.

## Foundational Learning
1. **LoRA adapters** - Lightweight parameter-efficient fine-tuning method that reduces computational requirements
   - Why needed: Enables efficient fine-tuning of multiple small models
   - Quick check: Verify adapter parameter count is << base model parameters

2. **Model stacking** - Sequential chaining of multiple specialized models
   - Why needed: Combines strengths of different specialized models
   - Quick check: Ensure output format compatibility between stacked models

3. **Pythia-160M architecture** - 160 million parameter model architecture
   - Why needed: Provides baseline model size for stacking experiments
   - Quick check: Confirm model supports required fine-tuning techniques

## Architecture Onboarding
Component map: Input -> Model 1 (LoRA) -> Model 2 (LoRA) -> Model 3 (LoRA) -> Model 4 (LoRA) -> Output

Critical path: Task input flows through each specialized model sequentially, with each model processing and passing output to the next model in the stack.

Design tradeoffs: Smaller models enable efficient inference but may require more sophisticated coordination; stacking increases complexity but maintains low individual model size.

Failure signatures: Performance degradation if model outputs are incompatible, stack depth is insufficient for task complexity, or adapter fine-tuning is inadequate.

First experiments:
1. Test individual model performance on respective task specializations
2. Verify output compatibility between adjacent models in stack
3. Measure performance scaling with stack depth from 2 to 6 models

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on only two tiny benchmarks (tinyArc and tinyMMLU)
- No comparison to other lightweight alternatives like Phi-2 or Qwen-1.8B
- Computational efficiency claims not empirically quantified

## Confidence
- Performance claims on tiny benchmarks: Medium
- Generalizability claims: Low
- Computational efficiency claims: Low

## Next Checks
1. Evaluate FSLM stacks across broader benchmark suite including GSM8K, HumanEval, and open-domain QA tasks
2. Measure actual inference latency, memory usage, and energy consumption comparing FSLM to standalone small models and distilled large models
3. Conduct ablation studies varying stack depth (2-8 models) and adapter configurations to identify optimal architectures for different task types