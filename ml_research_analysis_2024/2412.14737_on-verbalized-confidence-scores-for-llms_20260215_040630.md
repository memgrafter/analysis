---
ver: rpa2
title: On Verbalized Confidence Scores for LLMs
arxiv_id: '2412.14737'
source_url: https://arxiv.org/abs/2412.14737
tags:
- confidence
- answer
- prompt
- uncertainty
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the reliability of verbalized confidence
  scores for large language models (LLMs). The authors assess how well LLMs can self-assess
  their uncertainty by verbalizing a confidence score as part of their output.
---

# On Verbalized Confidence Scores for LLMs

## Quick Facts
- arXiv ID: 2412.14737
- Source URL: https://arxiv.org/abs/2412.14737
- Reference count: 40
- One-line primary result: Verbalized confidence scores for LLMs can be simple, effective, and versatile when properly prompted

## Executive Summary
This paper investigates the reliability of verbalized confidence scores for large language models (LLMs). The authors assess how well LLMs can self-assess their uncertainty by verbalizing a confidence score as part of their output. Through extensive experiments on 10 datasets, 11 models, and 17 prompt methods, they find that the reliability of verbalized confidence scores strongly depends on how the model is asked. For tiny LLMs (≤8B parameters), simple prompt formulations like "probability score" work best, while large LLMs (>70B parameters) benefit from more complex methods including probability formulation, advanced descriptions, and few-shot prompting. The best-performing method, "combo," achieves an average expected calibration error of 0.07 for large LLMs, meaning confidence scores deviate by only 7% from empirical accuracy.

## Method Summary
The authors evaluate verbalized confidence scores by prompting LLMs to generate both an answer and a confidence score for closed-book, closed-ended questions across 10 datasets. They use 11 different LLMs ranging from 2B to 110B parameters and test 17 different prompt formulations. For each dataset-model-prompt combination, they sample 1000 examples, parse the responses to extract answers and confidence scores, and compute calibration metrics including Expected Calibration Error (ECE), informativeness measures (n_distinct, variance), and meaningfulness (KL divergence). The evaluation focuses on the effects of prompt formulation on calibration performance, comparing different score ranges, formulations, descriptions, and few-shot prompting strategies.

## Key Results
- Reliability of verbalized confidence scores strongly depends on prompt formulation
- Tiny LLMs (≤8B parameters) benefit most from simple prompt formulations like probability scores
- Large LLMs (>70B parameters) achieve best results with combined methods including probability formulation, advanced descriptions, and few-shot prompting
- The "combo" method achieves average ECE of 0.07 for large LLMs, indicating 7% deviation from empirical accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models (LLMs) can reliably quantify their own uncertainty by verbalizing confidence scores as part of their output tokens.
- Mechanism: The LLM accesses its internal knowledge and capacity to self-assess uncertainty, then expresses this as a confidence score in a specified format. This approach is prompt- and model-agnostic since it only requires input prompt modification and relies solely on the LLM's response.
- Core assumption: LLMs possess sufficient self-awareness to accurately assess their own uncertainty in factual correctness.
- Evidence anchors:
  - [abstract] "verbalized confidence scores can become a simple but effective and versatile uncertainty quantification method"
  - [section 2] "This approach is model-agnostic, as it does not rely on the internal mechanisms of the model."
- Break condition: If LLMs lack self-awareness or if the prompt formulation fails to elicit meaningful self-assessment, the verbalized confidence scores will be poorly calibrated or meaningless.

### Mechanism 2
- Claim: The reliability of verbalized confidence scores depends strongly on how the model is asked (prompt formulation).
- Mechanism: Different prompt formulations (score range, score formulation, advanced description, few-shot prompting) trigger different cognitive processes in the LLM, affecting its ability to accurately assess and express uncertainty.
- Core assumption: Prompt formulation can meaningfully influence the LLM's self-assessment process.
- Evidence anchors:
  - [abstract] "reliability of verbalized confidence scores strongly depends on how the model is asked"
  - [section 4.3] "In our analysis, we focus on the following prompt aspects: Score range, Score formulation, Advanced description, Few-shot prompting"
- Break condition: If the LLM's self-assessment process is rigid and insensitive to prompt formulation variations, different prompt methods would yield similar reliability.

### Mechanism 3
- Claim: Tiny LLMs (≤8B parameters) benefit from simple prompt formulations while large LLMs (>70B parameters) benefit from more complex methods.
- Mechanism: Model capacity affects the LLM's ability to process and respond to complex prompt formulations. Simple formulations reduce cognitive load for tiny models, while large models can handle and benefit from more sophisticated prompting strategies.
- Core assumption: Model capacity correlates with the ability to process and respond appropriately to complex prompt formulations.
- Evidence anchors:
  - [section 5.4] "For tiny LLMs...the best improvement in the reliability of verbalized confidence scores comes from the probscore formulation...For large LLMs...the best improvement comes from combining multiple methods"
- Break condition: If model capacity doesn't correlate with prompt processing ability, the observed differences between tiny and large LLMs would disappear.

## Foundational Learning

- Concept: Expected Calibration Error (ECE)
  - Why needed here: ECE is the primary metric for measuring how well confidence scores align with actual accuracy, which is central to evaluating the reliability of verbalized confidence scores.
  - Quick check question: How is ECE calculated, and what does an ECE of 0.07 mean in practical terms?

- Concept: Uncertainty partitioning
  - Why needed here: Understanding the different types of uncertainty (input, model, output) helps clarify what exactly is being measured when assessing verbalized confidence scores.
  - Quick check question: What distinguishes output uncertainty from model uncertainty in the context of LLMs?

- Concept: Prompt engineering
  - Why needed here: The entire reliability of verbalized confidence scores hinges on effective prompt formulation, making prompt engineering skills essential.
  - Quick check question: How might changing from "confidence score" to "probability that your answer is correct" affect the LLM's self-assessment?

## Architecture Onboarding

- Component map:
  - Input -> Prompt Engine -> LLM -> Parser -> Evaluator -> Analyzer

- Critical path:
  1. Prepare dataset samples
  2. Generate prompts with uncertainty quantification instructions
  3. Send prompts to LLM
  4. Parse responses for answers and confidence scores
  5. Evaluate calibration and other metrics
  6. Aggregate and analyze results

- Design tradeoffs:
  - Prompt complexity vs. model capability (simple for tiny LLMs, complex for large LLMs)
  - Evaluation comprehensiveness vs. computational cost (sampling vs. full evaluation)
  - Granularity of confidence scores vs. interpretability (continuous vs. discrete scales)

- Failure signatures:
  - High ECE values indicating poor calibration
  - Low variance in confidence scores suggesting lack of expressiveness
  - Uniform confidence distribution regardless of dataset difficulty indicating meaningless scores
  - Parsing failures due to LLM not adhering to specified format

- First 3 experiments:
  1. Test basic prompt formulation (percentage score) on a small dataset with one tiny and one large LLM to verify the fundamental mechanism works
  2. Compare different score formulations (percentage, probability, letter grades) on the same setup to understand prompt formulation impact
  3. Test few-shot prompting with one example on both tiny and large LLMs to identify capacity-dependent effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of LLM architecture (beyond parameter count) determine the effectiveness of different prompt methods for verbalized confidence scores?
- Basis in paper: [inferred] The paper shows that tiny and large LLMs respond differently to prompt methods, but doesn't investigate the underlying architectural reasons for this difference.
- Why unresolved: The authors distinguish between "tiny" and "large" LLMs primarily by parameter count, but acknowledge that other architectural factors might play a role. They don't explore whether differences in attention mechanisms, layer depth, or other architectural features could explain the varying effectiveness of prompt methods.
- What evidence would resolve it: Systematic experiments comparing verbalized confidence calibration across LLM architectures with similar parameter counts but different architectural designs (e.g., different attention mechanisms, layer configurations), or detailed analysis of how specific architectural components influence the LLM's ability to generate calibrated confidence scores.

### Open Question 2
- Question: How can we develop reliable verbalized confidence scores for open-ended, subjective, or long-form text generation tasks?
- Basis in paper: [explicit] Section 3.1 explicitly states that their evaluation is limited to closed-ended, objective questions with well-defined correct answers, and acknowledges that developing confidence scores for open-ended questions is "beyond the scope of this work."
- Why unresolved: The paper successfully demonstrates verbalized confidence scores for factual, multiple-choice questions but doesn't address more complex question types where correctness is subjective or ill-defined. This represents a significant gap in practical applicability since many real-world LLM applications involve such questions.
- What evidence would resolve it: Development and evaluation of prompt methods that can generate meaningful confidence scores for subjective questions (e.g., personal preferences), open-ended questions (e.g., "What is the meaning of life?"), and long-form text generation tasks, along with appropriate metrics for evaluating confidence score quality in these contexts.

### Open Question 3
- Question: What is the relationship between verbalized confidence scores and internal token logits, and can combining these approaches improve calibration?
- Basis in paper: [explicit] Section 2 discusses that verbalized confidence scores and internal logits represent two different approaches to uncertainty quantification, and mentions that token logits only reflect the likelihood of individual tokens which is influenced by grammatical structure and model alignment procedures.
- Why unresolved: While the paper identifies verbalized confidence scores as model-agnostic and separate from internal token logits, it doesn't investigate whether combining these approaches could leverage the strengths of both. The potential complementary information between high-level semantic uncertainty (verbalized) and low-level token probabilities (logits) remains unexplored.
- What evidence would resolve it: Experiments comparing the calibration of verbalized confidence scores with that obtained from token logits, and development of hybrid methods that combine both approaches to see if they can achieve better calibration than either method alone.

### Open Question 4
- Question: How do different finetuning procedures and model alignment techniques affect the reliability of verbalized confidence scores?
- Basis in paper: [inferred] Section 2 mentions that token logits are influenced by "model alignment procedures," suggesting that how models are trained and aligned could affect uncertainty quantification, but the paper doesn't systematically investigate this relationship for verbalized confidence scores.
- Why unresolved: The paper evaluates various off-the-shelf models but doesn't examine how different finetuning approaches (e.g., supervised finetuning vs. reinforcement learning from human feedback) or alignment techniques impact the ability to generate reliable verbalized confidence scores.
- What evidence would resolve it: Comparative experiments across models with different finetuning procedures and alignment techniques, measuring how these factors influence the calibration, informativeness, and meaningfulness of verbalized confidence scores, potentially leading to recommendations for training procedures that enhance uncertainty quantification capabilities.

## Limitations

- Focus on closed-book, closed-ended tasks limits generalizability to open-ended generation tasks
- Prompt formulation details are incomplete in main text, requiring reference to appendix tables
- Does not explore multilingual performance or domain-specific applications
- Limited investigation of the relationship between verbalized confidence and internal model logits

## Confidence

High confidence in core finding that verbalized confidence scores depend strongly on prompt formulation and model size
Medium confidence in specific recommendations for tiny versus large LLMs
Low confidence in claim that verbalized confidence scores are "versatile" without evidence on open-ended tasks

## Next Checks

1. Reproduce the basic calibration experiments on a smaller scale using 2-3 datasets and 2-3 models to verify the fundamental mechanism of prompt-dependent reliability works as described.

2. Test the generalizability of findings by evaluating verbalized confidence scores on open-ended generation tasks (like summarization or creative writing) rather than just closed-book questions.

3. Conduct a multilingual validation by running experiments on the same datasets translated into different languages to assess whether the prompt formulation dependencies hold across language boundaries.