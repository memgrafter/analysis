---
ver: rpa2
title: 'Filtered not Mixed: Stochastic Filtering-Based Online Gating for Mixture of
  Large Language Models'
arxiv_id: '2406.02969'
source_url: https://arxiv.org/abs/2406.02969
tags:
- expert
- moe-f
- experts
- stochastic
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MoE-F, a novel online gating mechanism for
  combining large language models (LLMs) in time-series prediction tasks. Unlike static
  mixture-of-experts methods, MoE-F uses time-adaptive stochastic filtering to dynamically
  adjust the weighting of LLM predictions at each time step.
---

# Filtered not Mixed: Stochastic Filtering-Based Online Gating for Mixture of Large Language Models

## Quick Facts
- arXiv ID: 2406.02969
- Source URL: https://arxiv.org/abs/2406.02969
- Reference count: 40
- Key outcome: MoE-F achieves 17% absolute and 48.5% relative F1-score improvement over best individual LLM expert in financial market movement prediction

## Executive Summary
This paper introduces MoE-F, a novel online gating mechanism that dynamically combines large language models (LLMs) for time-series prediction tasks. Unlike traditional static mixture-of-experts methods, MoE-F uses time-adaptive stochastic filtering to continuously adjust LLM weighting based on running performance. The approach frames expert selection as a continuous-time Hidden Markov model and leverages parallel Bayesian filtering to estimate the optimal expert combination at each time step. Theoretical guarantees support both the filtering and aggregation components, while empirical results demonstrate significant performance improvements on financial market movement tasks.

## Method Summary
MoE-F operates in two phases: parallel Bayesian filtering for expert weight estimation and robust aggregation of expert predictions. The method implements N parallel Wonham-Shiryaev filters that recursively estimate the hidden Markov process selecting the optimal expert based on observed loss measurements. After each expert generates its Bayesian estimate and reliability score, a central module computes aggregation weights using Gibbs aggregation with a softmin function. These weights are used to create a robust ensemble prediction and update the Q/intensity matrix governing expert transitions.

## Key Results
- 17% absolute and 48.5% relative F1-score improvement over best individual LLM expert on financial market movement task
- Performance gains achieved using state-of-the-art models including GPT-4, Mixtral, and Llama variants
- Demonstrated effectiveness on both financial datasets (NIFTY) and time series forecasting tasks (ETTh1, ETTh2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoE-F uses time-adaptive stochastic filtering to dynamically adjust LLM weighting at each time step based on running performance.
- Mechanism: Each expert runs a parallel Wonham-Shiryaev filter that updates the posterior distribution over the latent "best expert" process given observed loss measurements.
- Core assumption: The latent expert selection process follows a continuous-time finite-state-space Markov chain whose transitions depend only on historical performance.
- Evidence anchors: [abstract] "uses time-adaptive stochastic filtering to dynamically adjust the weighting of LLM predictions at each time step"; [section] "Our MoE-F procedure (Algorithm 1) summarized in Figure 2 operates in two steps... implements N parallel stochastic filters yielding optimal Bayesian predictions"

### Mechanism 2
- Claim: The N filter outputs are optimally aggregated using a robust Gibbs-aggregation approach to maximize predictive power.
- Mechanism: After each expert generates its Bayesian estimate and reliability score, a central module computes aggregation weights via a softmin function on negative scores.
- Core assumption: Lower loss scores reliably indicate better predictive reliability and that Gibbs aggregation optimally combines expert predictions.
- Evidence anchors: [abstract] "Subsequently, the N filter outputs are optimally aggregated to maximize their robust predictive power"; [section] "These scores are used to aggregate the expert predictions into a single predictionˆYt of Yt. This is done using the well-studied Gibbs-aggregation approach with the softmin function"

### Mechanism 3
- Claim: Theoretical optimality guarantees exist for both the parallel filtering and robust aggregation steps through Bayesian estimation and Markov chain theory.
- Mechanism: The parallel filters provide L2-optimal Bayesian estimates of the latent expert process, while the aggregation step solves a bi-level optimization problem ensuring robust Q-matrix updates.
- Core assumption: The filtering equations have closed-form solutions and the aggregation optimization is convex/convergent.
- Evidence anchors: [abstract] "theoretical optimality guarantees of the proposed filtering-based gating algorithm (via optimality guarantees for its parallel Bayesian filtering and its robust aggregation steps)"; [section] "Our main result comes in two variants... Both versions of our guarantees operate under the following regularity conditions"

## Foundational Learning

- Concept: Continuous-time Hidden Markov Models (HMMs)
  - Why needed here: The latent expert selection process is modeled as a continuous-time finite-state-space HMM, requiring understanding of state-space models and filtering theory.
  - Quick check question: Can you explain the difference between discrete-time and continuous-time HMMs and why continuous-time is appropriate for streaming time-series data?

- Concept: Stochastic Filtering (Wonham-Shiryaev Filter)
  - Why needed here: The core mechanism uses parallel stochastic filters to estimate the hidden expert selection process from noisy performance measurements.
  - Quick check question: What are the key assumptions required for the Wonham-Shiryaev filter to provide closed-form solutions?

- Concept: Gibbs Aggregation and Robust Optimization
  - Why needed here: The final aggregation step uses Gibbs-weighted combination of expert predictions with entropic regularization to ensure robustness.
  - Quick check question: How does the softmin function in Gibbs aggregation relate to minimizing KL divergence between expert predictions?

## Architecture Onboarding

- Component map: N parallel Wonham-Shiryaev filters -> Loss computation module -> Innovation process calculator -> Posterior update mechanism -> Gibbs aggregation module -> Q-matrix update -> Final ensemble prediction generator

- Critical path: Loss measurement → Parallel filter updates → Reliability scoring → Aggregation → Q-matrix update → Ensemble prediction

- Design tradeoffs:
  - Parallel vs. sequential filtering (computational efficiency vs. information sharing)
  - Hard vs. soft gating (simplicity vs. smoothness)
  - Fixed vs. adaptive λ parameter in Gibbs aggregation (stability vs. responsiveness)

- Failure signatures:
  - Filter divergence (posterior probabilities becoming unstable)
  - Aggregation collapse (all weight going to single expert)
  - Q-matrix ill-conditioning (numerical instability in matrix logarithm)

- First 3 experiments:
  1. Single expert with synthetic data to verify filter implementation
  2. Two experts with controlled performance differences to test gating mechanism
  3. Full MoE-F with diverse LLM experts on simple time-series task to validate end-to-end functionality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MoE-F algorithm's performance scale with the number of expert models, particularly when combining heterogeneous models with varying levels of specialization?
- Basis in paper: [explicit] The paper states that MoE-F is "deployable as a plug-and-play filtering harness over any heterogenous mixture of LLMs or specialized models" and demonstrates gains using a mix of general-purpose LLMs and specialized models.
- Why unresolved: The paper primarily tests MoE-F with a fixed set of 7 SOTA LLMs and a smaller set of 3 TSF models. The scalability and performance characteristics with larger and more diverse expert pools remain unexplored.
- What evidence would resolve it: Systematic experiments varying the number and types of expert models (e.g., LLMs, specialized TSF models, SDE-based models) while measuring MoE-F's F1 score, accuracy, and computational efficiency.

### Open Question 2
- Question: What is the impact of the hyperparameters (λ, α) on MoE-F's performance and robustness, and are there principled methods for their optimization?
- Basis in paper: [explicit] The paper mentions using "optimal lambda, alpha values" but does not detail the optimization process or sensitivity analysis.
- Why unresolved: The theoretical guarantees rely on these hyperparameters, yet their selection appears to be ad-hoc in the experiments. Understanding their impact is crucial for practical deployment.
- What evidence would resolve it: Ablation studies varying λ and α across a range of values, analyzing their effect on performance metrics and robustness to noisy data or expert failures.

### Open Question 3
- Question: How does MoE-F handle expert model failures or sudden drops in individual expert performance, and can it dynamically adapt by removing or replacing underperforming experts?
- Basis in paper: [inferred] The paper emphasizes MoE-F's online adaptation capabilities, stating it "allows online adaptation like adding, removing or hot-swapping experts on the fly."
- Why unresolved: While the theoretical framework suggests dynamic adaptation, the paper does not demonstrate or analyze MoE-F's behavior under expert failures or deliberate removal of experts.
- What evidence would resolve it: Experiments simulating expert failures (e.g., removing high-performing experts, introducing noisy predictions) and measuring MoE-F's ability to maintain or recover performance through re-weighting or expert substitution.

## Limitations
- The Markov assumption for the latent expert selection process may not capture more complex temporal dependencies in expert performance
- The method requires running all LLM experts at each time step, which may be computationally expensive compared to sparse gating approaches
- The reliance on loss measurements assumes reliable performance feedback, which may be noisy or delayed in real-world applications

## Confidence
- Theoretical guarantees vs. practical implementation (Medium): While the paper provides theoretical optimality guarantees, the practical applicability depends on strict regularity conditions that may not hold in real-world scenarios.
- Scalability to large expert pools (Medium): The approach requires running N parallel filters for N experts, which could become computationally prohibitive as the number of LLM experts grows.
- Empirical validation scope (Low-Medium): The evaluation is limited to two specific datasets, and the 48.5% relative improvement needs validation across diverse domains to establish generalizability.

## Next Checks
1. **Cross-domain robustness test**: Apply MoE-F to non-financial time-series datasets (e.g., energy consumption, weather forecasting) with different LLM architectures to verify generalizability of the 48.5% improvement claim.

2. **Computational complexity analysis**: Measure wall-clock time and memory usage as a function of expert count N, comparing against static ensemble methods to quantify the trade-off between performance gain and computational cost.

3. **Robustness to noise and concept drift**: Introduce controlled levels of label noise and performance drift in synthetic experiments to test whether the filtering mechanism maintains stability when expert quality varies unpredictably.