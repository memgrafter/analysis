---
ver: rpa2
title: Learning to Transform Dynamically for Better Adversarial Transferability
arxiv_id: '2405.14077'
source_url: https://arxiv.org/abs/2405.14077
tags:
- adversarial
- uni00000013
- uni00000024
- uni0000002c
- uni00000030
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of enhancing adversarial transferability,
  where adversarial examples generated on one model can fool other models. The core
  idea is to dynamically learn the optimal transformation for each iteration during
  adversarial attack generation.
---

# Learning to Transform Dynamically for Better Adversarial Transferability

## Quick Facts
- arXiv ID: 2405.14077
- Source URL: https://arxiv.org/abs/2405.14077
- Authors: Rongyi Zhu; Zeliang Zhang; Susan Liang; Zhuo Liu; Chenliang Xu
- Reference count: 40
- One-line primary result: L2T significantly outperforms existing input transformation-based attack methods, achieving higher attack success rates on various models.

## Executive Summary
This paper addresses the challenge of enhancing adversarial transferability, where adversarial examples generated on one model can fool other models. The core contribution is Learning to Transform (L2T), a method that dynamically learns the optimal transformation for each iteration during adversarial attack generation. By formulating the problem as trajectory optimization and using reinforcement learning to select the best combination of input transformations from a candidate pool, L2T significantly improves attack success rates across diverse model architectures, including normally trained, adversarially trained, and vision API models.

## Method Summary
L2T dynamically selects optimal input transformations during each iteration of adversarial attack generation. The method samples combinations of transformations from a candidate pool based on a probability distribution, then uses gradients of the transformed examples to update both the adversarial example and the sampling distribution. This reinforcement learning-based approach efficiently searches the large space of possible transformation combinations without requiring additional training modules, making it more efficient than other learn-based adversarial attack methods.

## Key Results
- L2T achieves higher attack success rates compared to existing input transformation-based methods
- The method demonstrates effectiveness across normally trained, adversarially trained, and vision API models
- L2T provides improved efficiency by avoiding the need for additional training modules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic selection of input transformations improves adversarial transferability by increasing input diversity in a targeted way
- Mechanism: In each iteration, L2T samples transformation combinations from a candidate pool based on a probability distribution. Gradients of transformed examples update both the adversarial example and the sampling distribution, learning which transformations most effectively fool the surrogate model
- Core assumption: Different benign images benefit from different optimal transformation combinations, and the optimal trajectory changes dynamically as the adversarial example evolves
- Evidence anchors: Abstract mentions "L2T increases the diversity of transformed images by selecting the optimal combination of operations from a pool of candidates, consequently improving adversarial transferability"; paper conceptualizes this as trajectory optimization problem solved with reinforcement learning
- Break condition: If probability distribution becomes biased toward a single transformation or non-optimal combination, method may not achieve desired increase in input diversity and adversarial transferability

### Mechanism 2
- Claim: Reinforcement learning-based approach efficiently searches the large space of possible transformation combinations
- Mechanism: L2T formulates finding optimal transformation trajectory as trajectory optimization problem and uses reinforcement learning strategy with gradient ascent to update sampling probabilities based on gradients of loss function with respect to transformed examples
- Core assumption: Gradients of loss function with respect to transformed examples provide signal for which transformations are more likely to fool model, and this signal can guide search for optimal transformation trajectory
- Evidence anchors: Abstract states problem is conceptualized as trajectory optimization and solved with reinforcement learning strategy; paper designs reinforcement learning-based approach capitalizing on its efficacy in navigating expansive search domains
- Break condition: If gradient signal becomes noisy or uninformative, reinforcement learning approach may fail to converge to optimal transformation trajectory

### Mechanism 3
- Claim: Method avoids need for additional training modules, making it more efficient for adversarial example generation
- Mechanism: Unlike other learn-based adversarial attack methods requiring separate model to predict optimal transformations, L2T uses reinforcement learning approach that updates sampling probabilities during attack process itself
- Core assumption: Sampling probabilities can be effectively updated during attack process without need for separate training phase
- Evidence anchors: Abstract mentions approach is more efficient for adversarial example generation as it obviates need for additional training modules; paper studies learning-based transformation methods that enjoy more diversity of transformed images leading to better adversarial transferability performance
- Break condition: If sampling probabilities cannot be effectively updated during attack process, method may not achieve desired efficiency gains

## Foundational Learning

- Concept: Adversarial examples and their transferability
  - Why needed here: Understanding problem L2T aims to solve - enhancing cross-model attack ability of adversarial examples
  - Quick check question: What is the difference between white-box and black-box attacks, and how does adversarial transferability relate to this?

- Concept: Input transformation-based adversarial attacks
  - Why needed here: Understanding specific category of adversarial attacks L2T belongs to, involving augmenting input data with transformations to improve transferability
  - Quick check question: How do input transformation-based methods differ from other categories of adversarial attacks, such as gradient-based or architecture-based methods?

- Concept: Reinforcement learning and trajectory optimization
  - Why needed here: Understanding core technique used by L2T to search for optimal transformation trajectory, involving reinforcement learning approach to navigate large search space
  - Quick check question: How does gradient ascent approach used in L2T differ from other reinforcement learning algorithms, such as Q-learning or policy gradient methods?

## Architecture Onboarding

- Component map:
  Candidate operation pool -> Sampling distribution -> Transformation combination -> Gradient calculation -> Adversarial example update

- Critical path:
  1. Initialize adversarial example and sampling distribution
  2. For each iteration:
     a. Sample combination of transformations based on current sampling distribution
     b. Apply sampled transformations to adversarial example
     c. Calculate gradients of loss function with respect to transformed examples
     d. Update adversarial example based on gradients
     e. Update sampling distribution based on gradients

- Design tradeoffs:
  - Number of candidate operations: Larger number increases search space and potential diversity but also computational cost
  - Number of samples per iteration: Larger number provides more accurate gradient estimate but increases computational cost
  - Learning rate for sampling distribution: Higher learning rate allows faster adaptation but may lead to instability

- Failure signatures:
  - Sampling distribution becomes biased toward single transformation or non-optimal combination
  - Gradients become noisy or uninformative, leading to poor updates of sampling distribution
  - Method fails to improve adversarial transferability compared to baseline methods

- First 3 experiments:
  1. Implement basic L2T algorithm with small candidate operation pool and small number of samples per iteration. Evaluate adversarial transferability on simple dataset and single surrogate model
  2. Increase number of candidate operations and samples per iteration. Evaluate adversarial transferability on larger dataset and multiple surrogate models
  3. Compare performance of L2T with baseline input transformation-based methods on diverse set of models including normally trained, adversarially trained, and vision API models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does optimal transformation trajectory vary across different image classes and categories in ImageNet dataset?
- Basis in paper: [inferred] Paper mentions each image has different optimal transformation to boost adversarial transferability but does not explore this variation across image classes
- Why unresolved: Study focuses on overall transferability improvements rather than class-specific transformation strategies
- What evidence would resolve it: Experiments showing transformation trajectories for different ImageNet classes (animals vs. objects vs. scenes) and their relative effectiveness compared to generic approaches

### Open Question 2
- Question: What is theoretical limit of adversarial transferability enhancement using dynamic input transformations, and how close does L2T approach this limit?
- Basis in paper: [inferred] Paper demonstrates significant improvements over existing methods but does not establish theoretical framework for transferability limits or discuss proximity to such limits
- Why unresolved: Without understanding theoretical bounds, unclear whether further improvements are possible or if diminishing returns have been reached
- What evidence would resolve it: Analysis of transferability ceilings through exhaustive search on small datasets, or mathematical proofs establishing bounds on transformation-based transferability improvements

### Open Question 3
- Question: How do optimal transformation trajectories change when attacking defense mechanisms versus normally trained models?
- Basis in paper: [explicit] Paper shows L2T performs well against defense mechanisms but does not analyze whether optimal transformations differ between defended and undefended models
- Why unresolved: Understanding whether defense mechanisms require fundamentally different transformation strategies could inform more effective attacks or defenses
- What evidence would resolve it: Comparative analysis of transformation trajectories when attacking defended vs. undefended models, revealing patterns or differences in optimal approaches

## Limitations

- The reinforcement learning mechanism's effectiveness depends heavily on gradient signal quality and choice of hyperparameters, with theoretical guarantees for convergence remaining unclear
- Computational overhead during attack generation could become prohibitive for larger candidate pools or higher sample counts, despite claimed efficiency advantages
- The method's performance relies on effective updating of sampling probabilities during attack process, which may fail if gradient signals become noisy or uninformative

## Confidence

- **High Confidence**: Empirical results showing L2T outperforming baseline transformation methods on diverse model architectures, including normally trained, adversarially trained, and vision API models
- **Medium Confidence**: Efficiency claims regarding avoiding additional training modules, as reinforcement learning approach still requires multiple gradient computations per iteration
- **Medium Confidence**: Theoretical framing of problem as trajectory optimization, given limited theoretical analysis of convergence properties and relationship between learned transformations and adversarial transferability

## Next Checks

1. **Gradient Signal Analysis**: Conduct ablation studies varying number of samples L and learning rate œÅ to quantify impact on ASR and training stability, particularly for difficult target models like ViT and adversarially trained networks

2. **Candidate Pool Sensitivity**: Test L2T's performance with reduced candidate operation pools (5 vs 10 categories) to determine minimum diversity required for effectiveness and identify which transformation types contribute most to transferability gains

3. **Cross-dataset Generalization**: Evaluate L2T's transferability performance when trained on ImageNet but tested against models trained on CIFAR-10 or other datasets to assess robustness across different data distributions and model architectures