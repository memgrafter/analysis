---
ver: rpa2
title: What are the limits of cross-lingual dense passage retrieval for low-resource
  languages?
arxiv_id: '2408.11942'
source_url: https://arxiv.org/abs/2408.11942
tags:
- amharic
- language
- languages
- khmer
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study analyzes the effectiveness of multilingual Dense Passage\
  \ Retrieval (mDPR) for extremely low-resource languages, focusing on Amharic and\
  \ Khmer. The authors propose extending mDPR with post-training using Translation\
  \ Language Modeling (TLM) and fine-tuning with cross-lingual question\u2013passage\
  \ alignment."
---

# What are the limits of cross-lingual dense passage retrieval for low-resource languages?

## Quick Facts
- arXiv ID: 2408.11942
- Source URL: https://arxiv.org/abs/2408.11942
- Authors: Jie Wu; Zhaochun Ren; Suzan Verberne
- Reference count: 27
- Primary result: Modest improvements (2.10-2.25% Recall@10 for Amharic, 7.70-9.62% for Khmer) from post-training mDPR for extremely low-resource languages

## Executive Summary
This study analyzes the effectiveness of multilingual Dense Passage Retrieval (mDPR) for extremely low-resource languages, focusing on Amharic and Khmer. The authors propose extending mDPR with post-training using Translation Language Modeling (TLM) and fine-tuning with cross-lingual question–passage alignment. They create curated datasets with aligned sentences between 6 language pairs from the CCAligned dataset and a dataset with aligned questions and passages for 6 language pairs. The results show that language alignment brings improvements to mDPR for low-resource languages, but the improvements are modest and the results remain low. The study concludes that fulfilling CORA's promise to enable multilingual open QA in extremely low-resource settings is challenging because the model, the data, and the evaluation approach are intertwined.

## Method Summary
The authors extend mDPR by adding Amharic and Khmer tokens to the mBERT vocabulary and post-training the model using Masked Language Modeling (MLM) and Translation Language Modeling (TLM) on aligned sentence pairs from the CCAligned dataset. They further fine-tune the post-trained model using cross-lingual question–passage alignment data created by translating questions from high-resource languages to Amharic and Khmer. The evaluation uses MKQA and AmQA datasets to measure retrieval performance through Recall@10/20 and ROUGE-1 metrics, with additional analysis of language distribution in retrieved passages.

## Key Results
- For Amharic, Recall@10 improves from 0.46% to 2.10-2.25% after post-training
- For Khmer, Recall@10 improves from 7.74% to 7.70-9.62% after post-training
- Language alignment brings improvements to mDPR for low-resource languages, but results remain low
- Models tend to retrieve mostly high-resource language passages instead of target low-resource language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extending mBERT's vocabulary with low-resource language tokens improves retrieval effectiveness.
- Mechanism: The model learns specialized embeddings for low-resource tokens through MLM, reducing the prevalence of [UNK] tokens that share identical embeddings.
- Core assumption: Randomly initialized embeddings for newly added tokens can be effectively learned through MLM.
- Evidence anchors:
  - [abstract]: "We extend mDPR by additional post-training for two low-resource languages, Amharic and Khmer."
  - [section]: "Our proposed solution involves adding Amharic and Khmer tokens to the mBERT vocab-ulary, allowing the embeddings for these newly added tokens to be learned through the MLM and TLM tasks."
  - [corpus]: Weak evidence - no direct mention of this specific vocabulary extension approach in related papers.

### Mechanism 2
- Claim: Translation Language Modeling (TLM) improves cross-lingual alignment by exposing the model to parallel sentence pairs.
- Mechanism: The model learns to map representations between low-resource and high-resource languages by processing aligned sentence pairs where tokens are masked.
- Core assumption: Parallel sentence pairs in CCAligned dataset provide sufficient quality alignment for learning cross-lingual representations.
- Evidence anchors:
  - [abstract]: "We use the TLM paradigm to map sentences of those low-resource languages to their corresponding sentences in high-resource languages."
  - [section]: "We adopt Translation Language Modeling (TLM) (Conneau and Lample, 2019) to learn better representations of tokens in the low-resource languages by aligning sentences with sentences in high-resource languages."
  - [corpus]: Weak evidence - no direct mention of TLM usage for this specific low-resource language combination in related papers.

### Mechanism 3
- Claim: Fine-tuning with cross-lingual question-passage alignment improves retrieval for low-resource languages.
- Mechanism: The model learns to map questions in low-resource languages to relevant passages in high-resource languages through supervised alignment.
- Core assumption: Translated questions in low-resource languages maintain semantic equivalence to their high-resource counterparts.
- Evidence anchors:
  - [abstract]: "We provide a dataset with aligned questions and passages for 6 language pairs."
  - [section]: "The post-trained mBERT model is further fine-tuned using question–passage alignment between the language pairs."
  - [corpus]: Weak evidence - no direct mention of this specific fine-tuning approach in related papers.

## Foundational Learning

- Concept: Dense Passage Retrieval (DPR) architecture with dual-encoder
  - Why needed here: Understanding the baseline architecture that mDPR extends
  - Quick check question: How does the dual-encoder architecture differ from traditional term-based retrieval methods?

- Concept: Multilingual BERT (mBERT) pretraining and limitations
  - Why needed here: mBERT doesn't include Amharic and Khmer in its pretraining, creating the core challenge
  - Quick check question: What languages are included in the original mBERT pretraining data?

- Concept: Translation Language Modeling (TLM) as a cross-lingual learning objective
  - Why needed here: TLM is the key technique used for cross-lingual alignment in this work
  - Quick check question: How does TLM differ from standard Masked Language Modeling (MLM)?

## Architecture Onboarding

- Component map: mBERT base model → Vocabulary extension → MLM post-training → TLM post-training → Fine-tuning with cross-lingual alignment → Evaluation on QA datasets