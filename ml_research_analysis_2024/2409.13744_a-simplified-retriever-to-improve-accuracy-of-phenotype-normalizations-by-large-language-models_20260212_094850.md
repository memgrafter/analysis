---
ver: rpa2
title: A Simplified Retriever to Improve Accuracy of Phenotype Normalizations by Large
  Language Models
arxiv_id: '2409.13744'
source_url: https://arxiv.org/abs/2409.13744
tags:
- term
- terms
- language
- large
- normalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A simplified retriever using BioBERT embeddings was developed to
  improve phenotype normalization accuracy of large language models (LLMs). Instead
  of generating term definitions, it matches HPO terms to target terms by semantic
  similarity.
---

# A Simplified Retriever to Improve Accuracy of Phenotype Normalizations by Large Language Models

## Quick Facts
- arXiv ID: 2409.13744
- Source URL: https://arxiv.org/abs/2409.13744
- Authors: Daniel B. Hier; Thanh Son Do; Tayo Obafemi-Ajayi
- Reference count: 40
- A simplified retriever using BioBERT embeddings improved phenotype normalization accuracy of LLMs from 62.3% to 90.3% on 1,820 clinical terms from OMIM

## Executive Summary
This paper presents a simplified retriever approach to improve phenotype normalization accuracy for large language models (LLMs). The method uses BioBERT embeddings to find candidate Human Phenotype Ontology (HPO) terms semantically similar to target clinical terms, then prompts GPT-4o with 20 candidates for selection. The approach achieves 90.3% accuracy compared to 62.3% baseline for standalone GPT-4o and 69% for NLP-based methods. This demonstrates that retrieval augmentation with semantically similar candidates enables LLMs to leverage their implicit knowledge of HPO terminology more effectively than generation from scratch.

## Method Summary
The approach uses BioBERT v1.1 to generate embeddings for all 30,234 HPO terms, then calculates cosine similarity to find the 20 most semantically similar candidates for each target term from 1,820 OMIM clinical terms. GPT-4o is prompted with each target term plus the 20 candidates, and selects the best matching HPO term. The system compares favorably against standalone BioBERT, spaCy, and Doc2Hpo baselines across accuracy, F1, recall, and precision metrics. The optimal number of candidates was determined empirically to be 20, where accuracy plateaus rather than improving with additional candidates.

## Key Results
- Normalization accuracy improved from 62.3% (baseline GPT-4o) to 90.3% with retriever augmentation
- The simplified retriever approach outperformed standalone BioBERT (69% accuracy) and other NLP-based methods
- LLM-based semantic matching demonstrated superiority over simple cosine similarity metrics in selecting appropriate candidates
- 20 candidate terms provided optimal balance between coverage and cognitive load for LLM selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval augmentation improves phenotype normalization accuracy by providing contextually relevant candidate terms
- Mechanism: The BioBERT retriever generates candidate terms based on semantic similarity embeddings, which the LLM uses to select the most appropriate normalization rather than generating from scratch
- Core assumption: The LLM has sufficient knowledge about HPO terms to recognize the best semantic match from a set of candidates
- Evidence anchors:
  - [abstract]: "The normalization accuracy of GPT-4o increases from a baseline of 62% without augmentation to 90.3% with retriever augmentation"
  - [section]: "By prompting GPT-4o with the 20 closest candidate terms, we enable the model to take advantage of its implicit knowledge of HPO terms and select a semantically equivalent normalization"
  - [corpus]: Weak evidence - only 5 related papers found, none specifically addressing LLM retrieval augmentation for phenotype normalization
- Break condition: If the LLM lacks sufficient pre-training exposure to HPO terminology or the retriever provides poor quality candidates

### Mechanism 2
- Claim: Semantic equivalence assessment by LLMs outperforms simple cosine similarity metrics
- Mechanism: The LLM can evaluate semantic relationships beyond vector space similarity, recognizing contextual meaning that embeddings may not capture
- Core assumption: The LLM's training data included sufficient biomedical terminology to understand semantic nuances in phenotype descriptions
- Evidence anchors:
  - [section]: "Examining the cases where large language model combined with a retriever outperformed BioBERT reveals that the 'best match' selected by large language model can deviate from the candidate term with the highest cosine similarity"
  - [section]: "This demonstrates the strength of large language models in interpreting semantic equivalence beyond simple cosine metrics"
  - [corpus]: No direct evidence found in related papers
- Break condition: When terms require domain-specific medical knowledge not present in general LLM training

### Mechanism 3
- Claim: 20 candidate terms represents an optimal balance between coverage and cognitive load for LLM selection
- Mechanism: The LLM can effectively process 20 candidates without being overwhelmed, while this number provides sufficient semantic variation to find accurate matches
- Core assumption: The distribution of semantic similarity follows a pattern where 20 candidates captures most relevant options
- Evidence anchors:
  - [section]: "We investigated the optimal number of potential matches to submit to GPT-4o or GPT-3.5-Turbo. Accuracy improved as the number of candidate normalization increased to 20 and then reached a plateau"
  - [section]: "The plateau observed for 20 candidate terms in the prompt suggests that presenting more candidates neither degrades nor improves accuracy"
  - [corpus]: No evidence found regarding optimal candidate numbers for LLM-based normalization
- Break condition: If the candidate pool contains too many semantically similar terms or if the LLM struggles with processing larger sets

## Foundational Learning

- Concept: Semantic similarity using contextual embeddings
  - Why needed here: The retriever relies on BioBERT embeddings to find candidate terms with similar meanings to target terms
  - Quick check question: How does BioBERT's contextual embedding approach differ from static word embeddings like spaCy's en_core_web_lg?

- Concept: Cosine similarity as a distance metric
  - Why needed here: The system uses cosine similarity to rank candidate terms by semantic closeness to the target term
  - Quick check question: What does a cosine similarity score of 0.95 versus 0.85 indicate about the semantic relationship between two terms?

- Concept: Prompt engineering for LLMs
  - Why needed here: The augmentation approach requires carefully structured prompts that present candidate terms and request selection of the best match
  - Quick check question: What prompt structure was used to present candidates to the LLM for selection?

## Architecture Onboarding

- Component map: Target term input → BioBERT embedding generation → Candidate retrieval via cosine similarity → LLM prompt with 20 candidates → Best match selection → HPO ID output
- Critical path: The BioBERT embedding generation and cosine similarity calculation must complete before LLM prompting can occur
- Design tradeoffs: Using BioBERT embeddings provides domain-specific semantic understanding but adds computational overhead compared to general embeddings
- Failure signatures: Low accuracy may indicate poor retriever quality (wrong candidates) or LLM inability to select appropriate matches from candidates
- First 3 experiments:
  1. Test baseline LLM performance without retrieval on a small validation set
  2. Verify BioBERT embedding generation produces reasonable similarity rankings
  3. Confirm LLM can correctly select best match when presented with clearly ranked candidates

## Open Questions the Paper Calls Out

None

## Limitations

- Limited empirical validation with only 1,820 OMIM terms and three baseline comparisons without extensive ablation studies
- Choice of 20 candidate terms lacks theoretical justification beyond observed performance plateau
- Reliance on proprietary GPT-4o model introduces dependency on external API and potential performance variability

## Confidence

- **High confidence**: The BioBERT retriever + LLM prompting approach demonstrably improves normalization accuracy compared to standalone methods, with clear numerical improvements from 62.3% to 90.3% accuracy.
- **Medium confidence**: The claim that 20 candidates represents an optimal balance is supported by observed performance plateaus but lacks theoretical grounding or broader validation across different term distributions.
- **Medium confidence**: The assertion that LLM semantic matching outperforms cosine similarity is plausible given the mechanism but relies on limited empirical comparison and no direct head-to-head testing with human experts on the same samples.

## Next Checks

1. **Dataset Generalization Test**: Apply the retriever approach to a distinct biomedical ontology (e.g., SNOMED CT or MeSH) with at least 500 clinical terms to verify the method generalizes beyond HPO normalization and OMIM-derived terms.

2. **Ablation Study on Candidate Number**: Systematically test normalization accuracy with 5, 10, 15, 20, 25, and 30 candidate terms to confirm the 20-term plateau is robust across different semantic similarity distributions and term types.

3. **Human Expert Validation**: Conduct blind evaluation where medical experts assess whether LLM-selected terms are semantically equivalent to target terms for 100 randomly sampled cases, comparing expert consensus with the LLM + retriever predictions to validate the semantic equivalence claims.