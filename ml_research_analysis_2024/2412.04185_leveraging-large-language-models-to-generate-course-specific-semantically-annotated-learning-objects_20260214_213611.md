---
ver: rpa2
title: Leveraging Large Language Models to Generate Course-specific Semantically Annotated
  Learning Objects
arxiv_id: '2412.04185'
source_url: https://arxiv.org/abs/2412.04185
tags:
- questions
- learning
- question
- prompt
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of large language models (LLMs)
  to generate semantically annotated quiz questions for higher education. The authors
  propose a retrieval-augmented generation approach using GPT-4 to create questions
  that are contextually relevant to specific university courses and address the cognitive
  dimension of understanding.
---

# Leveraging Large Language Models to Generate Course-specific Semantically Annotated Learning Objects

## Quick Facts
- arXiv ID: 2412.04185
- Source URL: https://arxiv.org/abs/2412.04185
- Reference count: 40
- Primary result: LLMs can generate structurally correct semantic annotations but struggle with relational annotations and produce questions requiring significant human review

## Executive Summary
This paper investigates using large language models to generate semantically annotated quiz questions for university courses, focusing on the "understand" cognitive dimension of Bloom's taxonomy. The authors propose a retrieval-augmented generation approach using GPT-4-turbo to create questions contextually relevant to specific courses, with annotations in the STEX semantic markup language. While structural annotations were reliably generated, relational annotations linking concepts to modules and symbols were problematic. The generated questions often contained errors or misconceptions, requiring extensive expert review before use. The study demonstrates both the potential and current limitations of LLMs in educational content generation.

## Method Summary
The authors implemented a retrieval-augmented generation pipeline using GPT-4-turbo to create semantically annotated quiz questions for computer science courses. The system takes user parameters (concept, cognitive dimension, course context) and retrieves relevant text fragments from STEX-annotated course materials. These fragments are combined with a structured prompt containing examples and evaluation criteria, then passed to the LLM. Generated questions are reviewed by domain experts before being converted to HTML via STEX-to-HTML conversion for integration into an adaptive learning system. The approach leverages the LLM's familiarity with LaTeX syntax for structural annotations while using RAG to ground questions in course-specific content.

## Key Results
- Structural semantic annotations in STEX were reliably generated, leveraging the LLM's exposure to LaTeX syntax in training data
- Relational semantic annotations (linking concepts to modules/symbols) were rarely generated correctly despite explicit prompting
- Generated questions frequently contained logical errors or misconceptions, requiring significant human expert review before educational use

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation (RAG) allows LLMs to generate contextually relevant questions by accessing specific course materials at generation time.
- Mechanism: The pipeline first retrieves text fragments from course materials that are semantically linked to the target concept via STEX annotations. These fragments are concatenated to the prompt before passing it to the LLM, thereby providing up-to-date, domain-specific context that the model did not see during pre-training.
- Core assumption: Course materials contain sufficient semantic markup to reliably identify relevant fragments for a given concept.
- Evidence anchors:
  - [abstract] "Our approach involves more targeted strategies such as retrieval-augmented generation (RAG) to produce contextually relevant and pedagogically meaningful learning objects."
  - [section] "T o allow for the model to generate questions for aspeciﬁc course regarding terminology, deﬁnitions, and notations, we opted for a technique called retrieval-augmented generation (RAG)."
- Break condition: If course materials are poorly annotated or lack relevant content, the RAG retrieval will return noisy or irrelevant fragments, degrading question quality.

### Mechanism 2
- Claim: Structural semantic annotations in STEX are reliably generated by the LLM because the syntax is well-represented in training data.
- Mechanism: The prompt provides examples of STEX structural macros (e.g., `\begin{sproblem}`, `\mcc`, `\objective`). The LLM, having seen large amounts of LaTeX code in its training corpus, can mimic these patterns accurately, producing syntactically correct STEX output without specialized fine-tuning.
- Core assumption: LaTeX syntax is sufficiently represented in the model's training data for reliable generation.
- Evidence anchors:
  - [section] "Since STEX inherits its syntax from LATEX, the LLM used does not need to be ﬁnetuned or explicitly prompted on the usage of a new or esoteric language – the huge amount of publicly available LATEX code implies that its syntax is well represented in the training data."
- Break condition: If the model is exposed to non-standard or highly specialized LaTeX usage in the course materials, it may fail to reproduce the correct structural annotations.

### Mechanism 3
- Claim: Domain experts are necessary to filter LLM-generated questions because the model produces superﬁcially plausible but educationally flawed content.
- Mechanism: Even when questions appear well-formed and syntactically correct, they may contain logical errors, reinforce misconceptions, or have unhelpful feedback. Human experts detect and correct these flaws, ensuring pedagogical quality and factual accuracy.
- Core assumption: Human expertise is required to identify subtle conceptual errors and assess alignment with learning objectives.
- Evidence anchors:
  - [abstract] "The quality of the generated questions often did not meet educational standards, highlighting that although LLMs can contribute to the pool of learning materials, their current level of performance requires significant human intervention to refine and validate the generated content."
  - [section] "One particularly striking example is the following question on the topic of propositional logic... This is the common fallacy of denying the antecedent..."
- Break condition: If expert review is skipped or performed by underqualified reviewers, flawed questions may propagate into the learning system.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG enables the LLM to generate questions grounded in the specific terminology, notation, and content of a given course rather than relying solely on its general training data.
  - Quick check question: What is the main benefit of using RAG in this context, and how does it differ from standard prompt-only approaches?

- Concept: STEX semantic annotation system
  - Why needed here: STEX provides both structural markup for quiz questions and relational annotations linking concepts, enabling the adaptive learning system to automatically determine prerequisites and learning objectives.
  - Quick check question: In STEX, what is the difference between structural and relational annotations, and why is each important for the learning assistant?

- Concept: Bloom's revised taxonomy
  - Why needed here: The cognitive dimension (e.g., remember, understand, apply) determines the type of reasoning the question should target, guiding both prompt design and question evaluation.
  - Quick check question: Why did the authors focus specifically on the "understand" dimension, and what challenges does this pose compared to the "remember" dimension?

## Architecture Onboarding

- Component map: User Interface -> RAG Retriever -> Prompt Builder -> LLM API (GPT-4-turbo) -> Expert Review Interface -> STEX-to-HTML Converter -> Adaptive Learning System

- Critical path: Parameter selection → RAG retrieval → Prompt assembly → LLM generation → Expert review → STEX conversion → Integration into adaptive learning system

- Design tradeoffs:
  - Using a commercial LLM (GPT-4-turbo) offers high-quality generation but introduces cost, vendor lock-in, and limited transparency.
  - Fixed prompt structure ensures reproducibility but reduces flexibility for edge cases or new annotation types.
  - Expert review guarantees quality but limits scalability and introduces latency.

- Failure signatures:
  - Generated questions contain logical errors or misconceptions → check prompt clarity and retrieval relevance.
  - Structural STEX annotations are malformed → verify prompt examples and LLM capability with LaTeX.
  - No relational annotations appear → inspect whether the course materials contain sufficient symbol/module declarations.
  - Retrieval returns irrelevant fragments → examine STEX semantic markup quality and search indexing.

- First 3 experiments:
  1. Generate five questions for a simple "remember" level concept with minimal course materials to validate basic prompt and structural annotation generation.
  2. Attempt relational annotation generation by providing explicit examples and calling a search function to retrieve symbol/module data, observing whether output improves.
  3. Run a full pipeline with an "understand" level concept, perform expert review, and analyze failure modes in content accuracy and feedback quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the quality of relational semantic annotations be improved in LLM-generated educational content?
- Basis in paper: [explicit] The paper notes that while structural semantic annotations were generated correctly, relational annotations were problematic and suggests exploring retrieval-augmented generation and function calling as potential solutions.
- Why unresolved: The paper mentions attempts to use retrieval-augmented generation and function calling to improve relational annotations, but these efforts did not yield the desired results, and the authors suggest that more experimentation may be needed.
- What evidence would resolve it: Systematic testing of different RAG configurations, alternative prompting strategies, and evaluation of their impact on the quality of relational annotations in generated questions.

### Open Question 2
- Question: What is the optimal number of questions to generate per prompt to balance diversity and quality in LLM-generated educational content?
- Basis in paper: [inferred] The paper discusses the trade-off between generating more questions per prompt for diversity versus generating fewer to avoid overlap and mentions that the model's performance seems to depend on the topic and amount of course materials.
- Why unresolved: The paper acknowledges this trade-off but does not provide a definitive answer, leaving it for future work.
- What evidence would resolve it: Comparative analysis of question sets generated with varying numbers of questions per prompt, evaluating both diversity and quality metrics.

### Open Question 3
- Question: How effective are LLM-generated questions in improving student learning outcomes compared to human-generated questions?
- Basis in paper: [explicit] The paper mentions that the quality of generated questions often did not meet educational standards and that human intervention is still crucial, but does not provide empirical evidence on learning outcomes.
- Why unresolved: The paper focuses on the generation process and quality assessment by experts, not on actual student performance or learning gains.
- What evidence would resolve it: Controlled studies comparing student performance and learning outcomes when using LLM-generated versus human-generated questions in adaptive learning environments.

## Limitations

- Generated questions frequently contain logical errors and misconceptions, requiring significant human expert review before educational use
- Relational semantic annotations linking concepts to modules/symbols were rarely generated correctly despite explicit prompting
- The study relied on a single expert reviewer, raising concerns about inter-rater reliability and potential bias in quality assessments

## Confidence

- **High confidence**: Structural semantic annotation generation using LaTeX syntax is reliable due to representation in training data
- **Medium confidence**: RAG approach improves contextual relevance compared to prompt-only methods, but retrieval quality heavily depends on course material annotation quality
- **Low confidence**: Current LLM performance can produce educationally sound questions without significant human intervention, given the frequency of content errors and misconceptions observed

## Next Checks

1. **Replicate relational annotation generation**: Test whether providing explicit examples of relational STEX syntax and including a search function for symbol/module lookup in the prompt improves relational annotation output quality
2. **Multi-expert validation**: Conduct blind review of generated questions by multiple domain experts to establish inter-rater reliability scores and identify systematic quality issues
3. **RAG vs. prompt-only comparison**: Generate questions for identical concepts using both RAG and prompt-only approaches, measuring content accuracy, relevance, and semantic annotation completeness to quantify RAG benefits