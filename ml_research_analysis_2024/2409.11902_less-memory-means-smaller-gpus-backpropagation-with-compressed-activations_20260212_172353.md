---
ver: rpa2
title: 'Less Memory Means smaller GPUs: Backpropagation with Compressed Activations'
arxiv_id: '2409.11902'
source_url: https://arxiv.org/abs/2409.11902
tags:
- memory
- activations
- compression
- training
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the memory bottleneck in training large-scale
  deep neural networks, particularly focusing on the significant memory consumption
  by intermediate activation maps during backpropagation. The proposed method introduces
  a compression technique that applies average pooling to activation maps immediately
  before storing them for the backward pass, reducing memory footprint without affecting
  the forward computation or loss calculation accuracy.
---

# Less Memory Means smaller GPUs: Backpropagation with Compressed Activations

## Quick Facts
- arXiv ID: 2409.11902
- Source URL: https://arxiv.org/abs/2409.11902
- Reference count: 20
- One-line primary result: Activation pooling reduces peak memory consumption by up to 36% during DNN training with minimal accuracy loss

## Executive Summary
This paper addresses the memory bottleneck in training large-scale deep neural networks by introducing a compression technique that applies average pooling to activation maps immediately before storing them for backpropagation. The method reduces peak memory consumption without affecting forward computation or loss calculation accuracy, enabling training of larger models on smaller GPUs and potentially on embedded devices. Experiments on ResNet architectures demonstrate that this approach can reduce peak memory consumption by up to 29% with (2×2) pooling and 36% with (4×4) pooling, while maintaining prediction accuracy compared to uncompressed baselines.

## Method Summary
The proposed method compresses activation maps during training by applying average pooling immediately before storing them for the backward pass. This reduces the spatial dimensions of activation maps, thereby decreasing memory footprint during backpropagation. The forward pass remains unchanged, ensuring that loss calculations are unaffected. The approach uses a custom operator to handle the compression/decompression process during backpropagation, maintaining gradient accuracy while significantly reducing memory requirements.

## Key Results
- Peak memory consumption reduced by up to 29% with (2×2) pooling and 36% with (4×4) pooling
- Prediction accuracy maintained compared to uncompressed baseline when using extended training schedules
- Gradient cosine similarity analysis reveals layer-specific sensitivity to compression, with downsample layers showing greater resilience

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pooling activation maps before backpropagation reduces peak memory usage without affecting forward computation or loss accuracy.
- Mechanism: By applying average pooling to activation maps immediately before storing them for the backward pass, the spatial dimensions of the activation maps are reduced, leading to a smaller memory footprint during the backward pass.
- Core assumption: The reduced activation maps still contain sufficient information to compute gradients accurately enough to maintain model performance.
- Evidence anchors:
  - [abstract] "This work addresses the memory bottleneck in training large-scale deep neural networks, particularly focusing on the significant memory consumption by intermediate activation maps during backpropagation."
  - [section] "During training, intermediate input activations have to be stored until backpropagation for gradient calculation. These make up the vast majority of the memory footprint."

### Mechanism 2
- Claim: The pooling operation is regular and does not introduce encoding overhead, unlike sparse approaches.
- Mechanism: Average pooling creates a regular, dense structure that does not require additional encoding or decoding, eliminating the overhead associated with sparse data structures.
- Core assumption: The regular structure of pooled activations allows for efficient memory usage without the need for complex encoding schemes.
- Evidence anchors:
  - [section] "To address the problems related to encoding overhead for sparse data structures, we pursue a slightly different approach. Instead of encoding non-zero values we use average pooling to reduce the size of the activation maps."
  - [corpus] "Sparse approaches... require additional encoding overhead, which is eliminated by the regular pooling operation."

### Mechanism 3
- Claim: Extending the training schedule can recover some of the accuracy lost due to compression.
- Mechanism: By training for more epochs, the model has more opportunities to adjust its parameters and compensate for the reduced information in the compressed activation maps.
- Core assumption: The loss of information due to compression is not so severe that it cannot be recovered with additional training time.
- Evidence anchors:
  - [section] "Experiments with more training epochs were conducted. The goal is to find a viable trade-off between the increased end-to-end training time and the reduced memory consumption."
  - [abstract] "With this approach we are able to reduce the peak memory consumption by 29% at the cost of a longer training schedule, while maintaining prediction accuracy compared to an uncompressed baseline."

## Foundational Learning

- Concept: Backpropagation and the role of activation maps
  - Why needed here: Understanding how backpropagation works and why activation maps are stored is crucial to grasping the memory bottleneck and the proposed solution.
  - Quick check question: What is the purpose of storing activation maps during the forward pass for use in the backward pass?

- Concept: Average pooling and its effects on spatial dimensions
  - Why needed here: The mechanism relies on reducing the spatial dimensions of activation maps through average pooling to achieve memory savings.
  - Quick check question: How does average pooling affect the spatial dimensions of a feature map, and what is the impact on the number of elements?

- Concept: Memory allocation and deallocation during training
  - Why needed here: Understanding the dynamic nature of memory usage during training helps in assessing the potential for memory reduction through the proposed method.
  - Quick check question: Describe the typical pattern of memory allocation and deallocation during a forward and backward pass in neural network training.

## Architecture Onboarding

- Component map: Forward pass -> Uncompressed activations computed and used for loss calculation -> Compression stage (pooling applied before storage) -> Backward pass (compressed activations used for gradient computation) -> Parameter update

- Critical path:
  1. Forward computation
  2. Pooling and storage of compressed activations
  3. Backward computation using compressed activations
  4. Parameter update

- Design tradeoffs:
  - Memory vs. accuracy: Reducing memory usage through pooling may lead to a slight decrease in accuracy, which can be mitigated by extending the training schedule.
  - Regularity vs. flexibility: The use of average pooling ensures regularity and eliminates encoding overhead but may not be as flexible as other compression methods.

- Failure signatures:
  - If the model's accuracy drops significantly and cannot be recovered with extended training, the pooling may be too aggressive.
  - If the custom operator for handling compressed activations is not implemented correctly, it could lead to incorrect gradient calculations.

- First 3 experiments:
  1. Implement the pooling operation on a simple convolutional layer and measure the reduction in memory usage for storing activations.
  2. Train a small ResNet model with (2x2) pooling on a subset of ImageNet and compare the accuracy and memory usage to an uncompressed baseline.
  3. Analyze the cosine similarity of gradients between compressed and uncompressed models to identify layers that are more sensitive to compression.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed compression technique affect the convergence speed and final accuracy across different neural network architectures beyond ResNet, particularly for vision transformers and efficient networks?
- Basis in paper: [explicit] The authors state they plan to expand experiments to other architectures including EfficientNet and Transformers like ViT and swin, suggesting this remains an open question.
- Why unresolved: Current experiments are limited to ResNet18 only, making it unclear whether the observed effects generalize to other architectures with different computational characteristics and weight-sharing patterns.
- What evidence would resolve it: Systematic experiments comparing memory savings, accuracy retention, and training dynamics across diverse architectures (CNNs, transformers, efficient networks) with varying pooling kernel sizes and training schedules.

### Open Question 2
- Question: What is the optimal strategy for adaptive, layer-specific pooling kernel sizing during training to maximize memory savings while minimizing accuracy degradation?
- Basis in paper: [inferred] The authors mention that effects are not uniform across layers and suggest that cosine similarity data could be used to automatically find viable per-layer configurations, even adaptively during training.
- Why unresolved: Current approach uses uniform pooling across all layers, while the paper identifies that downsample layers are more resilient to compression than others, suggesting a more nuanced approach could yield better results.
- What evidence would resolve it: Empirical comparison of fixed uniform pooling versus adaptive layer-specific pooling strategies, measuring both memory savings and accuracy across multiple training runs.

### Open Question 3
- Question: How does the proposed activation compression method compare to other memory optimization techniques (pruning, quantization, sparsity) in terms of the trade-off between memory savings and accuracy degradation?
- Basis in paper: [explicit] The authors position their work in relation to other compression approaches like ReSprop, Dithered Backprop, and SWAT, noting that unlike these methods, their approach focuses on memory footprint reduction rather than computational speedup.
- Why unresolved: The paper only provides preliminary results on their specific approach without comparative analysis against established memory optimization techniques, leaving the relative effectiveness unclear.
- What evidence would resolve it: Head-to-head comparison of memory savings, accuracy retention, and training time across multiple architectures for activation compression versus pruning, quantization, and other memory optimization techniques under identical experimental conditions.

## Limitations
- The approach requires extended training schedules to recover accuracy lost due to compression, introducing a trade-off between memory savings and computational time
- Results are primarily demonstrated on ResNet architectures with ImageNet, leaving uncertainty about performance on other model families or tasks
- The method does not address the entire memory bottleneck, as it focuses specifically on activation maps while other components (parameters, optimizer states) still consume significant memory

## Confidence

- Memory reduction claims: **High** - The mechanism is straightforward and the mathematical relationship between pooling size and memory reduction is well-established.
- Accuracy maintenance claims: **Medium** - While experiments show promising results, the reliance on extended training schedules and limited model diversity reduces confidence.
- Generalizability claims: **Low** - The results are primarily demonstrated on ResNet/ImageNet, with limited evidence for other architectures or domains.

## Next Checks

1. **Layer-specific sensitivity analysis**: Implement gradient cosine similarity tracking during training to identify which layers are most sensitive to compression, enabling adaptive per-layer pooling strategies.

2. **Cross-architecture evaluation**: Test the pooling compression approach on transformer-based architectures (e.g., ViT) and other vision models to assess generalizability beyond ResNet.

3. **Real-time memory profiling**: Conduct detailed memory profiling during actual training runs to verify peak memory reduction claims and identify any hidden overhead from the pooling operations or custom operators.