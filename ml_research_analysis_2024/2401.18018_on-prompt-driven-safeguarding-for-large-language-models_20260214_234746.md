---
ver: rpa2
title: On Prompt-Driven Safeguarding for Large Language Models
arxiv_id: '2401.18018'
source_url: https://arxiv.org/abs/2401.18018
tags:
- safety
- harmful
- harmless
- queries
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Safety prompts prepended to inputs are widely used to prevent large
  language models from complying with harmful queries, but their underlying mechanisms
  are poorly understood, hindering automatic optimization. This work analyzes safety
  prompts from the perspective of model representations and finds that while models
  can already distinguish harmful and harmless queries, safety prompts primarily increase
  the overall refusal probability by moving queries' representations in a "higher-refusal"
  direction.
---

# On Prompt-Driven Safeguarding for Large Language Models

## Quick Facts
- arXiv ID: 2401.18018
- Source URL: https://arxiv.org/abs/2401.18018
- Authors: Chujie Zheng; Fan Yin; Hao Zhou; Fandong Meng; Jie Zhou; Kai-Wei Chang; Minlie Huang; Nanyun Peng
- Reference count: 40
- Key outcome: DRO significantly improves safeguarding performance over human-crafted safety prompts, outperforming baselines while maintaining general capability

## Executive Summary
Safety prompts are widely used to prevent large language models from complying with harmful queries, but their optimization mechanisms remain poorly understood. This work analyzes safety prompts from the perspective of model representations and finds that they primarily increase refusal probability by moving queries' representations in a "higher-refusal" direction. Inspired by this insight, the authors propose DRO (Directed Representation Optimization), which treats safety prompts as trainable embeddings and optimizes them to move harmful queries' representations along and harmless queries' representations opposite the refusal direction.

## Method Summary
DRO optimizes continuous safety prompt embeddings by treating them as trainable parameters that shift query representations in a low-dimensional PCA space. The method first estimates a "refusal direction" using logistic regression on empirical refusal probabilities, then optimizes the prompt to move harmful queries along this direction while moving harmless queries opposite to it. A regularization term preserves the original representation to prevent degradation of general capabilities. The approach is evaluated on eight 7B LLMs using synthetic anchor data and out-of-domain benchmarks.

## Key Results
- DRO outperforms human-crafted safety prompts on out-of-domain benchmarks (MaliciousInstruct, AdvBench)
- DRO maintains general model capability as measured by AlpacaEval win rates
- DRO significantly reduces compliance with harmful queries while minimizing false refusals on harmless queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety prompts increase refusal probability by moving query representations in a "higher-refusal" direction
- Mechanism: Safety prompts act as learned embeddings that shift hidden states along a direction in representation space correlated with refusal behavior
- Core assumption: The model can already distinguish harmful from harmless queries, but fails to refuse harmful ones due to insufficient movement along the refusal direction
- Evidence anchors:
  - [abstract] "safety prompts primarily increase the overall refusal probability by moving queries' representations in a 'higher-refusal' direction"
  - [section 2.3] "different safety prompts move queries' representations in similar directions, in which models become more prone to generating refusal responses even when the queries are harmless"
  - [corpus] Found 25 related papers. Average neighbor FMR=0.417, suggesting moderate relevance but limited direct evidence for this specific mechanism
- Break condition: If representations of harmful and harmless queries are not distinguishable without safety prompts, or if safety prompts move them in different directions

### Mechanism 2
- Claim: DRO optimizes continuous safety prompts by directly controlling low-dimensional representation features
- Mechanism: DRO learns to move harmful queries' representations along the refusal direction and harmless queries opposite, using contrastive optimization in PCA space
- Core assumption: The first few principal components capture the most salient features related to harmfulness and refusal behavior
- Evidence anchors:
  - [abstract] "DRO treats safety prompts as continuous, trainable embeddings and learns to move the queries' representations along or opposite the refusal direction"
  - [section 3.2] "DRO aims to move the low-dimensional representation g(xθ) from g(x0) along or opposite the refusal direction defined by wr"
  - [corpus] Limited direct evidence; related work focuses on general prompt optimization rather than directed representation optimization
- Break condition: If PCA fails to capture the relevant features, or if the refusal direction doesn't correlate with actual refusal behavior

### Mechanism 3
- Claim: Regularization prevents degeneration of original representation while optimizing target features
- Mechanism: DRO uses orthogonal projection to preserve information in non-target dimensions while optimizing the low-dimensional features
- Core assumption: Direct optimization in low-dimensional space can cause information loss in other dimensions, degrading generation quality
- Evidence anchors:
  - [section 3.3] "to restrict ||xθ − x0|| within a reasonable range of variation, we can use the second RHS item for regularization"
  - [section 4.2] "without LU, models would suffer from largely degraded generation quality for benign instructions (52.6 vs. 63.5 on AlpacaEval)"
  - [corpus] Weak evidence; regularization techniques are common but specific application to prompt optimization is not well-documented
- Break condition: If regularization strength is mis-specified, or if the orthogonal projection doesn't preserve sufficient information

## Foundational Learning

- Concept: Principal Component Analysis (PCA)
  - Why needed here: DRO uses PCA to extract low-dimensional representation space capturing harmfulness and refusal features
  - Quick check question: If PCA explains 90% variance with 4 components, what percentage of variance is in the remaining components?

- Concept: Logistic Regression for Direction Estimation
  - Why needed here: DRO fits logistic regression to empirical refusal probabilities to estimate the refusal direction
  - Quick check question: If the normal vector of logistic regression is w, what direction does w point in terms of refusal probability?

- Concept: Contrastive Learning
  - Why needed here: DRO's optimization objective contrasts refusal probabilities with and without safety prompts
  - Quick check question: If fr(xθ) - fr(x0) > 0 for harmful queries, what should happen to the refusal probability?

## Architecture Onboarding

- Component map: Synthetic query generation -> Anchor data creation -> DRO training data -> PCA -> Logistic regression -> Continuous prompt optimization -> Regularization -> Evaluation
- Critical path:
  1. Generate anchor data with controlled harmful/harmless queries
  2. Compute PCA and fit logistic regression to estimate refusal direction
  3. Initialize continuous safety prompt from textual prompt
  4. Optimize prompt using contrastive objective + regularization
  5. Evaluate on held-out benchmarks
- Design tradeoffs:
  - PCA dimension (m): Higher m captures more features but increases optimization complexity
  - Regularization weight (β): Too high prevents optimization, too low causes degeneration
  - Anchor data size: Larger provides better direction estimation but increases computation
- Failure signatures:
  - Poor safeguarding performance: Wrong refusal direction or insufficient optimization
  - Degraded generation quality: Insufficient regularization or over-regularization
  - Overfitting: Too much optimization on synthetic data, poor generalization to real queries
- First 3 experiments:
  1. Run DRO on synthetic data only, check if refusal direction aligns with empirical probabilities
  2. Test on held-out harmless queries to measure false refusal rate
  3. Evaluate on out-of-domain benchmarks to check generalization performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do safety prompts impact LLMs' ability to recognize nuanced harmful queries that fall between clearly harmful and clearly harmless examples?
- Basis in paper: [inferred] The paper shows that models can distinguish harmful and harmless queries but doesn't explore the model's performance on ambiguous or borderline cases. The authors note that real-world queries can be ambiguous and their harmfulness may be difficult to judge.
- Why unresolved: The experiments used carefully controlled synthetic data with clear harmful/harmless distinctions. The paper acknowledges that real-world queries are often ambiguous but doesn't test model performance on such cases.
- What evidence would resolve it: Testing DRO and baseline methods on datasets containing ambiguous queries with varying degrees of harmfulness, or conducting human evaluations on borderline cases to compare model performance.

### Open Question 2
- Question: Does the optimization of safety prompts through DRO affect the model's understanding of context and reasoning abilities beyond just refusal behaviors?
- Basis in paper: [explicit] The authors note that DRO maintains general model capability as evaluated on AlpacaEval, but acknowledge this may not comprehensively test all aspects of model reasoning and context understanding.
- Why unresolved: The evaluation only tested general capability on a specific benchmark (AlpacaEval) which may not capture all aspects of reasoning and context understanding that could be affected by safety prompt optimization.
- What evidence would resolve it: Comprehensive testing on multiple reasoning benchmarks, including those specifically designed to test contextual understanding and logical reasoning, before and after DRO optimization.

### Open Question 3
- Question: How does the performance of DRO-optimized safety prompts change when applied to larger language models (e.g., 70B+ parameters) compared to the 7B models tested?
- Basis in paper: [explicit] The authors tested DRO on eight 7B models and noted that "we believe it can serve as a reasonable testbed for the 7B LLMs we experiment with" but didn't test larger models.
- Why unresolved: The paper explicitly acknowledges this limitation and suggests the results may not generalize to larger models, but doesn't investigate how scaling affects DRO's effectiveness.
- What evidence would resolve it: Testing DRO on multiple model sizes (e.g., 13B, 30B, 70B) and comparing the effectiveness of optimization across different scales, particularly looking at whether the refusal direction estimation and representation optimization behave similarly at larger scales.

## Limitations

- Results are primarily demonstrated on 7B parameter models; scalability to larger models remains untested
- The approach requires access to hidden states, limiting application to black-box APIs
- Evaluation relies on out-of-domain benchmarks that may not fully capture the complexity of real harmful queries

## Confidence

- **High confidence**: The core observation that safety prompts move representations in "higher-refusal" directions is empirically supported and reproducible
- **Medium confidence**: The DRO optimization framework works as described, but its superiority over other continuous prompt tuning methods needs more systematic comparison
- **Low confidence**: Claims about DRO preserving general capability are based on a single benchmark (AlpacaEval) and may not reflect real-world performance

## Next Checks

1. **Generalization Test**: Evaluate DRO-optimized prompts on a diverse set of real-world harmful queries (not synthetic) to verify that the learned refusal direction transfers beyond controlled "How to do" format

2. **Ablation Study**: Systematically test different PCA dimensions (m=2,4,8,16) and regularization weights (β) to understand their impact on both safeguarding performance and generation quality

3. **Adversarial Robustness**: Test whether DRO-optimized prompts are vulnerable to prompt injection or rephrasing attacks that could bypass the learned refusal direction