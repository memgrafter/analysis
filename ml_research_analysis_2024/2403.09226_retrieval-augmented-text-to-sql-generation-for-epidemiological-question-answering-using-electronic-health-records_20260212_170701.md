---
ver: rpa2
title: Retrieval augmented text-to-SQL generation for epidemiological question answering
  using electronic health records
arxiv_id: '2403.09226'
source_url: https://arxiv.org/abs/2403.09226
tags:
- data
- medical
- dataset
- text-to-sql
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a method for querying electronic health records
  and claims data to answer epidemiological questions using retrieval augmented generation
  (RAG) with text-to-SQL models. The approach incorporates medical coding steps to
  map clinical entities into precise ontology terms, enabling accurate SQL query generation.
---

# Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records

## Quick Facts
- arXiv ID: 2403.09226
- Source URL: https://arxiv.org/abs/2403.09226
- Authors: Angelo Ziletti; Leonardo D'Ambrosi
- Reference count: 8
- Key outcome: GPT-4 Turbo achieves 77.5% accuracy and 97.1% executability on epidemiological questions when using RAG with one example

## Executive Summary
This work introduces a method for querying electronic health records and claims data to answer epidemiological questions using retrieval augmented generation (RAG) with text-to-SQL models. The approach incorporates medical coding steps to map clinical entities into precise ontology terms, enabling accurate SQL query generation. Experiments on a manually curated dataset of epidemiological questions demonstrate that RAG significantly improves performance over simple prompting, with GPT-4 Turbo achieving the highest accuracy (77.5%) and executability (97.1%) when using RAG with one example. The results indicate that RAG offers a promising direction for improving large language model capabilities in this domain, particularly for industry settings with limited data.

## Method Summary
The method combines text-to-SQL generation with retrieval augmented generation (RAG) to answer epidemiological questions using electronic health records and claims data. The approach includes a medical coding step that extracts clinical entities from questions and maps them to precise ontology terms using SapBERT embeddings and GPT-4 Turbo verification. SQL queries are generated with placeholders for these entities, then filled with the normalized ontology terms. The RAG component retrieves relevant question-SQL pairs from a curated dataset to augment prompts given to language models. The approach uses the OMOP-CDM standard to ensure compatibility across different real-world data databases. Multiple language models (GPT-4 Turbo, GPT-3.5 Turbo, Claude 2.1, GeminiPro, Mixtral 8x7B) are evaluated with both simple and advanced prompts.

## Key Results
- GPT-4 Turbo achieved the highest accuracy at 77.5% and executability at 97.1% when using RAG with one example
- RAG consistently improved performance across all models compared to simple prompting, with no models performing better without RAG
- Using more retrieved examples (RAG-top2, RAG-top5) did not further improve performance and sometimes led to slight declines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval augmented generation (RAG) significantly improves the accuracy and executability of text-to-SQL generation for epidemiological questions.
- Mechanism: RAG retrieves relevant question-SQL pairs from a curated dataset to augment the prompt given to the language model, providing context-specific examples that guide the generation of accurate SQL queries.
- Core assumption: The language model can effectively leverage the retrieved examples to improve its understanding of the domain-specific language and query structure.
- Evidence anchors:
  - [abstract] "We show that our approach, which integrates a medical coding step into the text-to-SQL process, significantly improves the performance over simple prompting."
  - [section] "The inclusion of relevant examples via RAG significantly and consistently improves performance (Table 2, cf. RAG-top1/2/5 vs Prompt(advanced))."
  - [corpus] Weak evidence - the related papers focus on different aspects of RAG in EHR, not specifically on text-to-SQL generation.
- Break condition: The retrieved examples are not relevant or do not provide sufficient context for the given question, leading to no improvement or even degradation in performance.

### Mechanism 2
- Claim: The medical coding step enhances data retrieval and clinical context comprehension by mapping clinical entities to precise ontology terms.
- Mechanism: The language model generates SQL with placeholders for medical entities, which are then mapped to specific SNOMED ontology terms using a similarity-based approach followed by verification with GPT-4 Turbo.
- Core assumption: The combination of SapBERT embeddings for initial matching and GPT-4 Turbo for verification can accurately map clinical entities to the correct ontology terms.
- Evidence anchors:
  - [abstract] "Our approach, which integrates a medical coding step into the text-to-SQL process, significantly improves the performance over simple prompting."
  - [section] "To perform entity normalization, we first compute the cosine similarity of each entity's SapBERT embeddings with SNOMED ontology terms, and select the top-50 matches. Then, similarly to Yang et al. (2022), we prompt GPT-4 Turbo to verify whether a given code should be assigned to the input entity, refining the list."
  - [corpus] Weak evidence - the related papers do not focus on the specific medical coding step used in this approach.
- Break condition: The similarity-based approach fails to find relevant ontology terms, or GPT-4 Turbo verification incorrectly rejects valid mappings.

### Mechanism 3
- Claim: Using the OMOP-CDM standard ensures the approach's applicability across different real-world data (RWD) databases.
- Mechanism: By adhering to the OMOP-CDM, the approach can work with any database that conforms to this standard, potentially allowing access to a large volume of patient records.
- Core assumption: The OMOP-CDM is widely adopted and provides a comprehensive set of standardized vocabularies that cover the necessary clinical concepts.
- Evidence anchors:
  - [abstract] "To address the challenge of data retrieval variability across databases with differing data models, we leverage the OMOP-CDM. This model, underpinned by standardized vocabularies (Reich et al., 2024), harmonizes observational healthcare data and it is widely recognized as the standard for RWD analysis, with data from over 2.1 billion patient records across 34 countries (Voss et al., 2023; Reich et al., 2024)."
  - [section] No direct evidence.
  - [corpus] Weak evidence - the related papers do not discuss the use of OMOP-CDM for ensuring applicability across RWD databases.
- Break condition: The OMOP-CDM does not cover all necessary clinical concepts, or the standard is not widely adopted, limiting the approach's applicability.

## Foundational Learning

- Concept: Electronic Health Records (EHR) and Claims Data
  - Why needed here: Understanding the nature and structure of EHR and claims data is crucial for developing methods to query and analyze this data effectively.
  - Quick check question: What are the main types of information stored in EHR and claims data, and how are they typically structured?

- Concept: Text-to-SQL Generation
  - Why needed here: The core task is translating natural language epidemiological questions into executable SQL queries, which requires understanding the principles and challenges of text-to-SQL generation.
  - Quick check question: What are the main challenges in text-to-SQL generation, and how do different approaches (e.g., rule-based, fine-tuning, prompting) address these challenges?

- Concept: Retrieval Augmented Generation (RAG)
  - Why needed here: RAG is a key component of the proposed approach, and understanding its principles and how it can be applied to text-to-SQL generation is essential.
  - Quick check question: How does RAG work, and what are the benefits and potential drawbacks of using RAG for text-to-SQL generation?

## Architecture Onboarding

- Component map: Input -> Medical Coding -> SQL Generation -> RAG -> LLM -> Execution -> Output
- Critical path: Input → Medical Coding → SQL Generation → RAG → LLM → Execution → Output
- Design tradeoffs:
  - Using RAG vs. fine-tuning the language model: RAG allows for more flexibility and adaptability to new questions, but may be less efficient for a fixed set of question types.
  - Using GPT-4 Turbo vs. open-source models: GPT-4 Turbo provides better performance but is more expensive and less accessible.
- Failure signatures:
  - Incorrect or incomplete SQL queries: May indicate issues with the medical coding step or the LLM's understanding of the question.
  - Poor performance with RAG: May indicate issues with the retrieval mechanism or the quality of the dataset.
  - Execution errors: May indicate issues with the generated SQL syntax or the underlying database structure.
- First 3 experiments:
  1. Evaluate the performance of the medical coding step by comparing the mapped ontology terms to a manually curated ground truth.
  2. Assess the impact of RAG on performance by comparing the accuracy and executability of SQL queries generated with and without RAG.
  3. Test the approach on a larger and more diverse set of epidemiological questions to evaluate its generalizability and identify potential limitations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of retrieved examples (RAG-top-k) for different model sizes and types in the text-to-SQL task for epidemiological questions?
- Basis in paper: [explicit] The paper shows that using a single example (RAG-top1) leads to substantial improvements, but adding more top results (RAG-top2 and RAG-top5) does not result in a similar increase. Some models exhibit a performance peak or a minor decline with additional context.
- Why unresolved: The paper only tests up to 5 retrieved examples. The relationship between model size, model type, and optimal number of retrieved examples is not explored.
- What evidence would resolve it: Systematic testing of RAG-top-k with k ranging from 1 to 20 for each model, with a focus on identifying the point of diminishing returns.

### Open Question 2
- Question: How does the performance of RAG with text-to-SQL models generalize to other domains beyond epidemiology, such as clinical trial design or patient-specific queries?
- Basis in paper: [inferred] The paper focuses on epidemiological questions using EHR and claims data. The dataset is specifically designed for epidemiological research. The generalizability of the approach to other domains is not discussed.
- Why unresolved: The paper does not provide any evidence or discussion on the generalizability of the approach to other domains.
- What evidence would resolve it: Applying the same methodology to datasets from other domains, such as clinical trial design or patient-specific queries, and comparing the performance to the epidemiological results.

### Open Question 3
- Question: What is the impact of using different medical ontologies and coding systems on the performance of the text-to-SQL generation for epidemiological questions?
- Basis in paper: [explicit] The paper uses SNOMED ontology terms for medical entity normalization and mentions that the approach could be applied to any database conforming to the OMOP-CDM, which harmonizes observational healthcare data using standardized vocabularies.
- Why unresolved: The paper does not explore the impact of using different ontologies or coding systems on the performance of the text-to-SQL generation.
- What evidence would resolve it: Comparing the performance of the approach using different ontologies or coding systems, such as ICD-10 or LOINC, and analyzing the impact on accuracy and executability.

## Limitations

- The evaluation dataset is small (20 questions) and manually curated, limiting generalizability to real-world epidemiological queries
- All experiments use synthetic claims data rather than actual EHR data, raising questions about performance with messier clinical data
- The medical coding step relies heavily on proprietary models (GPT-4 Turbo for verification), making the approach expensive and potentially inaccessible
- The OMOP-CDM constraint, while ensuring standardization, may limit applicability to healthcare systems not using this framework

## Confidence

- **High confidence** in the core finding that RAG improves text-to-SQL generation performance compared to simple prompting
- **Medium confidence** in the medical coding step's effectiveness, as validation focuses on end-to-end performance rather than entity normalization specifically
- **Medium confidence** in the generalizability claims, given the limited evaluation dataset and use of synthetic rather than real clinical data

## Next Checks

1. Test the approach on a larger, more diverse set of epidemiological questions from multiple healthcare systems to assess robustness and identify edge cases
2. Evaluate performance with real EHR data that includes missing values, inconsistent coding, and other data quality issues common in clinical settings
3. Conduct a cost-benefit analysis comparing GPT-4 Turbo-based verification with open-source alternatives that could make the approach more accessible