---
ver: rpa2
title: Text-To-Speech Synthesis In The Wild
arxiv_id: '2409.08711'
source_url: https://arxiv.org/abs/2409.08711
tags:
- speech
- data
- training
- titw
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces the TTS In the Wild (TITW) dataset, designed
  to advance research in noisy-TTS training using real-world, in-the-wild speech data.
  TITW is created through an automated pipeline applied to the VoxCeleb1 dataset,
  resulting in two training sets: TITW-Hard (189 hours) with minimal processing and
  TITW-Easy (173 hours) enhanced using DNSMOS-based selection.'
---

# Text-To-Speech Synthesis In The Wild

## Quick Facts
- arXiv ID: 2409.08711
- Source URL: https://arxiv.org/abs/2409.08711
- Reference count: 0
- One-line primary result: Introduces TITW dataset enabling TTS models to achieve over 3.0 UTMOS score with in-the-wild speech data

## Executive Summary
This paper introduces the TITW dataset for text-to-speech synthesis in the wild, created by applying an automated pipeline to VoxCeleb1 recordings. The dataset comes in two variants: TITW-Hard (189 hours) with minimal processing and TITW-Easy (173 hours) enhanced using DNSMOS-based selection. TITW enables state-of-the-art TTS models to achieve over 3.0 UTMOS score with TITW-Easy while remaining challenging for current models with TITW-Hard. Beyond TTS, the dataset supports speech deepfake detection and spoofing-robust automatic speaker verification by leveraging its single-speaker audio structure.

## Method Summary
The TITW dataset is created through an automated pipeline applied to VoxCeleb1, involving transcription with WhisperX, segmentation, and selection based on empirically derived heuristics. The pipeline removes non-target language segments, excessively short/long segments, outlier per-word durations, and empty transcriptions. TITW-Easy incorporates enhancement and DNSMOS-based selection with a threshold of 3.0, achieving higher quality than TITW-Hard. The dataset supports four baseline TTS models (TransformerTTS, GradTTS-DiffWave, VITS, MQTTS) and is evaluated using UTMOS, DNSMOS, WER, and MCD metrics on the TITW-KSKT protocol.

## Key Results
- TITW-Easy achieves over 3.0 UTMOS score with state-of-the-art TTS models
- TITW-Hard remains challenging with UTMOS below 2.8 for current models
- TITW supports both TTS training and speech deepfake detection research
- Dataset available on HuggingFace with recipes for baseline model training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The TITW pipeline improves TTS training data quality by automatically removing non-target language segments, excessively short/long segments, outlier per-word durations, and empty transcriptions.
- Mechanism: The pipeline applies four empirically derived heuristics to filter VoxCeleb1 utterances before TTS training, reducing noise and ensuring consistency in the training set.
- Core assumption: The heuristics correctly identify and remove problematic segments while retaining useful training data.
- Evidence anchors:
  - [abstract] The TITW dataset is created through an automated pipeline applied to VoxCeleb1, involving transcription, segmentation, and selection.
  - [section 3.2] The data selection heuristics include language detection, duration thresholds, per-word duration limits, and empty transcription removal.
  - [corpus] The corpus neighbors do not directly address these specific filtering mechanisms.
- Break condition: If the heuristics are too aggressive, they may remove too much data and reduce dataset size and diversity; if too lenient, noisy segments remain that destabilize TTS training.

### Mechanism 2
- Claim: TITW-Easy achieves better TTS synthesis quality than TITW-Hard by applying DNSMOS-based data selection after speech enhancement.
- Mechanism: After initial TITW-Hard creation, TITW-Easy applies DEMUCS speech enhancement and filters utterances based on DNSMOS scores above 3.0, removing low-quality segments.
- Core assumption: DNSMOS score correlates well with TTS training effectiveness and speech enhancement improves quality without introducing artifacts.
- Evidence anchors:
  - [abstract] TITW-Easy incorporates enhancement and DNSMOS-based selection, achieving over 3.0 UTMOS score.
  - [section 3.3] DNSMOS scores are used to filter enhanced data, with utterances below threshold 3.0 removed.
  - [corpus] The corpus neighbors do not provide evidence about DNSMOS effectiveness.
- Break condition: If DNSMOS threshold is set too high, useful data may be discarded; if too low, poor quality data remains that hurts TTS performance.

### Mechanism 3
- Claim: The TITW dataset supports both TTS training and speech deepfake detection by providing paired synthetic and real speech from the same speakers.
- Mechanism: Since TITW uses VoxCeleb1 (originally for speaker verification) where all utterances are single-speaker, synthetic speech generated from TITW can be paired with real speech for deepfake detection research.
- Core assumption: Single-speaker guarantee in TITW allows meaningful comparison between synthetic and real speech for detection tasks.
- Evidence anchors:
  - [abstract] TITW's unique design supports speech deepfake detection and spoofing-robust automatic speaker verification by leveraging its single-speaker audio structure.
  - [section 1] The selection of VoxCeleb1 offers unique benefits, as each utterance is from a single speaker, supporting deepfake detection research.
  - [corpus] The corpus neighbors mention SpoofCeleb and SASV tasks but do not provide direct evidence for this mechanism.
- Break condition: If TITW contains multi-speaker segments or the synthetic/real pairing is not properly aligned, the deepfake detection utility is compromised.

## Foundational Learning

- Concept: Automatic Speech Recognition (ASR) and speech enhancement
  - Why needed here: The TITW pipeline relies on WhisperX for transcription and DEMUCS for enhancement, which are critical for creating usable training data from in-the-wild recordings.
  - Quick check question: How does WhisperX handle word-level timestamps, and why is this important for segmentation?

- Concept: Data quality metrics (DNSMOS, UTMOS, MCD)
  - Why needed here: These metrics are used to evaluate both the TITW dataset quality and the synthesized speech output, guiding the data selection and model improvement process.
  - Quick check question: What is the difference between DNSMOS and UTMOS, and when would each be used?

- Concept: TTS model architectures (e.g., TransformerTTS, GradTTS, VITS, MQTTS)
  - Why needed here: Understanding these baseline models is essential for benchmarking TITW and knowing which architectures work well with noisy data.
  - Quick check question: Why might some TTS models converge on TITW-Easy but fail on TITW-Hard?

## Architecture Onboarding

- Component map: VoxCeleb1 source data → ASR transcription (WhisperX) → Segmentation (VAD) → Data selection heuristics → Optional enhancement (DEMUCS) → DNSMOS filtering → TITW-Hard/Easy datasets → TTS models (TransformerTTS, GradTTS-DiffWave, VITS, MQTTS) trained on TITW → Evaluation using MCD, UTMOS, DNSMOS, WER metrics

- Critical path: The transcription and segmentation pipeline is critical, as errors here propagate through the entire dataset creation process and affect TTS model training.

- Design tradeoffs: Balancing dataset "wildness" (diversity, real-world variability) against training stability (consistency, noise levels). TITW-Hard prioritizes wildness, TITW-Easy prioritizes trainability.

- Failure signatures: TTS models failing to converge on TITW-Hard, high WER scores indicating poor intelligibility, low DNSMOS scores showing quality issues, or ASR errors causing incorrect transcriptions.

- First 3 experiments:
  1. Train a baseline TTS model (e.g., TransformerTTS) on TITW-Easy and evaluate using all four metrics (MCD, UTMOS, DNSMOS, WER) on TITW-KSKT protocol.
  2. Compare the same model trained on TITW-Easy vs TITW-Hard to quantify the impact of data quality on synthesis performance.
  3. Test the effect of different DNSMOS thresholds on TITW-Easy creation by generating subsets with thresholds 2.5, 3.0, and 3.5, then training models on each to find the optimal balance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can TITW-Hard be successfully used to train TTS models with future, more advanced architectures?
- Basis in paper: [explicit] The paper states that TITW-Hard remains challenging for current TTS models, with UTMOS below 2.8, and suggests that future TTS models and training schematics may enable successful training with TITW-Hard.
- Why unresolved: Current TTS models struggle with the quality of TITW-Hard, and the paper does not explore whether advancements in model architecture or training techniques could overcome these limitations.
- What evidence would resolve it: Successful training of TTS models on TITW-Hard using future state-of-the-art architectures or novel training methodologies, resulting in comparable or improved performance metrics (e.g., UTMOS, DNSMOS) to those achieved with TITW-Easy.

### Open Question 2
- Question: How does the inclusion of non-English speech in TITW affect the performance of multilingual TTS systems?
- Basis in paper: [inferred] The paper mentions that multilingual extensions are left for future work and currently focuses on English speech. However, it does not explore the impact of including non-English speech on TTS performance.
- Why unresolved: The paper does not provide data or analysis on how multilingual speech affects TTS model training or output quality, nor does it discuss potential challenges or benefits of including diverse languages.
- What evidence would resolve it: Comparative analysis of TTS models trained on monolingual (English-only) versus multilingual TITW datasets, evaluating performance metrics such as intelligibility, naturalness, and speaker similarity across different languages.

### Open Question 3
- Question: What is the impact of different speech enhancement techniques on the quality of TITW-Easy and its usability for TTS training?
- Basis in paper: [explicit] The paper uses DEMUCS for speech enhancement in TITW-Easy but does not explore the effects of alternative enhancement methods on the dataset's quality or TTS performance.
- Why unresolved: The paper focuses on one enhancement technique without comparing it to others, leaving open questions about whether different methods could yield better results for TTS training.
- What evidence would resolve it: Comparative study of TTS models trained on TITW-Easy enhanced using various speech enhancement techniques (e.g., DEMUCS, RNNoise, Wave-U-Net), with performance metrics (e.g., MCD, UTMOS, DNSMOS) evaluated to determine the most effective method.

## Limitations

- The dataset size (173-189 hours) is relatively modest compared to some existing TTS corpora, potentially limiting model generalization.
- The paper does not provide detailed implementation specifications for the data selection heuristics, making it difficult to reproduce the exact filtering process.
- The reliance on DNSMOS for quality assessment assumes this metric correlates perfectly with TTS training effectiveness, but this relationship is not empirically validated.

## Confidence

- High Confidence: The dataset creation pipeline and basic quality metrics (DNSMOS, MCD) are well-supported by the presented methodology and results. The distinction between TITW-Easy and TITW-Hard is clearly demonstrated through the reported scores.
- Medium Confidence: The claim that TITW supports speech deepfake detection research is plausible but lacks direct experimental validation in the paper. The utility for SASV research is similarly asserted without comprehensive evaluation.
- Low Confidence: The paper's claims about the dataset being "sufficiently diverse and representative" of real-world conditions are not empirically tested across varied acoustic environments or speaker demographics.

## Next Checks

1. **Cross-validation of data selection heuristics**: Re-run the TITW creation pipeline with different combinations of filtering thresholds (language detection strictness, duration limits, DNSMOS cutoff values) to determine which parameters most strongly affect TTS model convergence and speech quality.

2. **Domain adaptation study**: Train models on TITW-Easy and test on other in-the-wild datasets (not just TITW-KSKT) to verify whether the reported improvements generalize beyond the evaluation protocol used in the paper.

3. **Speaker diversity audit**: Conduct an analysis of the speaker distribution in TITW compared to VoxCeleb1 to quantify whether the automated pipeline introduces demographic biases through its filtering criteria, particularly examining whether certain accents, age groups, or recording conditions are systematically excluded.