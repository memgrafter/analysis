---
ver: rpa2
title: On Leakage of Code Generation Evaluation Datasets
arxiv_id: '2407.07565'
source_url: https://arxiv.org/abs/2407.07565
tags:
- code
- lbpp
- data
- humaneval
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines data contamination in code generation evaluation
  datasets, particularly HumanEval and MBPP, which are widely used to assess large
  language models'' (LLMs) coding abilities. The authors identify three sources of
  contamination: direct data leakage from training corpora, indirect leakage through
  synthetic data generation, and overfitting to evaluation sets during model selection.'
---

# On Leakage of Code Generation Evaluation Datasets

## Quick Facts
- arXiv ID: 2407.07565
- Source URL: https://arxiv.org/abs/2407.07565
- Authors: Alexandre Matton; Tom Sherborne; Dennis Aumiller; Elena Tommasone; Milad Alizadeh; Jingyi He; Raymond Ma; Maxime Voisin; Ellen Gilsenan-McMahon; Matthias GallÃ©
- Reference count: 5
- Primary result: Leading models perform up to 43% worse on LBPP compared to HumanEval and MBPP, highlighting potential overfitting to original benchmarks.

## Executive Summary
This paper examines data contamination in widely used code generation evaluation datasets, particularly HumanEval and MBPP. The authors identify three sources of contamination: direct data leakage from training corpora, indirect leakage through synthetic data generation, and overfitting to evaluation sets during model selection. To address this, they introduce Less Basic Python Problems (LBPP), a new benchmark of 161 prompts designed to be uncontaminated and more challenging than existing datasets.

The paper demonstrates that models optimized on contaminated benchmarks may show artificially inflated performance that doesn't reflect true coding ability. LBPP shows significant performance drops (up to 43%) compared to HumanEval and MBPP, suggesting that existing benchmarks may have been compromised by contamination. The authors release LBPP to provide a more reliable and challenging evaluation metric for code generation capabilities.

## Method Summary
The paper analyzes contamination in code generation evaluation datasets through three mechanisms: direct data leakage from web-scraped code corpora, indirect leakage through synthetic data generation pipelines, and overfitting during model selection. The authors introduce LBPP, a new benchmark of 161 prompts, created from a filtered subset of the Python Practice Problems dataset. They evaluate leading models (Mistral, Meta, OpenAI, Anthropic, Qwen, Cohere, Deepseek, Databricks) on HumanEval, MBPP, and LBPP using zero-shot evaluation, comparing performance drops and model rankings across datasets to identify contamination effects.

## Key Results
- Every HumanEval prompt occurs at least 43 times in GitHub code search, indicating widespread replication
- Models show up to 43% performance drop on LBPP compared to HumanEval and MBPP
- Strong correlation exists between synthetic dataset similarity and benchmark performance
- Some models rank noticeably higher on contaminated benchmarks than on LBPP, indicating overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models are trained on evaluation data due to widespread availability of benchmarks in web-scraped code corpora.
- Mechanism: Code generation benchmarks are small, portable, and widely replicated. Web scraping from GitHub or Stack Overflow inadvertently includes these benchmarks, leading to direct data leakage.
- Core assumption: Training data is sourced from large web corpora where benchmarks are replicated across repositories.
- Evidence anchors:
  - "40 60 80 100 120 140 160 180 200 Number of GitHub Code Search Hits for Prompt 0 10 20 30 40Count of HumanEval Prompts... Every prompt occurs at least 43 times."
  - "The most obvious reason is the simplest: many of the test datasets are of widespread use and the simplest answer is that modern LLMs are trained on this evaluation data."
- Break condition: If all web-scraped code is fully decontaminated using semantic deduplication rather than surface-level matching.

### Mechanism 2
- Claim: Synthetic data generation pipelines inadvertently replicate evaluation prompts through prompt similarity and generation biases.
- Mechanism: Large synthetic datasets like evol-instruct and StarCoder-instruct are created by sampling code, generating prompts, and validating completions. These datasets often produce semantically equivalent prompts to existing benchmarks.
- Core assumption: Synthetic data pipelines are trained on code that overlaps with benchmark sources, or the prompt generation process biases toward common problem patterns.
- Evidence anchors:
  - "Fig. 2 highlights a widespread similarity between synthetic training datasets and public evaluation data, while the similarity with LBPP is uniformly lower."
  - "Table 3 shows some examples of very similar (but not necessarily equivalent) data between evol-instruct and MBPP."
- Break condition: If synthetic data generation uses truly novel prompt seeds with semantic filters to avoid benchmark overlap.

### Mechanism 3
- Claim: Model selection during training is biased toward benchmarks, leading to overfitting on these narrow datasets.
- Mechanism: During checkpoint selection, models are evaluated primarily on HumanEval and MBPP. This creates a feedback loop where models are optimized for these benchmarks rather than general code generation capability.
- Core assumption: Checkpoint selection relies heavily on benchmark performance rather than downstream utility.
- Evidence anchors:
  - "While it may be tempting to use such benchmarks as a deciding factor between similar checkpoints, section 3.2 shows that the correlation between these benchmarks and 'solving code generation' is weak."
  - "Table 2 and Fig. 3 illustrate this problem well. Even though the correlation between MBPP/HumanEval scores and LBPP scores is strong, some models are ranking noticeably higher on MBPP/HumanEval than on LBPP."
- Break condition: If model selection uses diverse, held-out evaluation sets that include novel problem types.

## Foundational Learning

- Concept: Contamination in ML evaluation
  - Why needed here: The paper's central argument is that contamination invalidates benchmark scores as measures of true capability.
  - Quick check question: If a model scores 90% on a benchmark that was partially in its training data, what does that score actually measure?

- Concept: Semantic vs surface-level code similarity
  - Why needed here: Standard deduplication methods fail for code because variable renaming or structure changes preserve semantics.
  - Quick check question: Why might two code snippets with different variable names still be considered equivalent in contamination analysis?

- Concept: Synthetic data generation pipelines
  - Why needed here: Understanding how synthetic datasets are created is crucial to identifying indirect contamination pathways.
  - Quick check question: What are the three main steps in a typical synthetic code generation pipeline?

## Architecture Onboarding

- Component map: Data collection -> Web scraping / code repositories -> Decontamination -> Filtering (n-gram, semantic matching) -> Synthetic generation -> Prompt sampling -> Completion generation -> Unit test validation -> Evaluation -> Benchmark scoring -> Model selection

- Critical path: 1. Collect code data from repositories 2. Apply decontamination filters 3. Generate synthetic data if used 4. Train model 5. Evaluate on benchmarks 6. Select checkpoints based on scores

- Design tradeoffs:
  - Strict decontamination reduces training data but improves evaluation validity
  - Using synthetic data increases scale but risks contamination through prompt similarity
  - Relying on popular benchmarks simplifies evaluation but creates overfitting incentives

- Failure signatures:
  - High correlation between synthetic dataset similarity and benchmark performance
  - Significant performance drop when evaluated on novel, uncontaminated benchmarks
  - Model rankings change dramatically between contaminated and uncontaminated evaluations

- First 3 experiments:
  1. Run semantic deduplication on training data and measure reduction in benchmark overlap
  2. Generate synthetic data with semantic filters and compare similarity to existing benchmarks
  3. Train models with different contamination levels and evaluate on both contaminated and uncontaminated benchmarks to measure overfitting effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of models on LBPP correlate with their performance on real-world coding tasks?
- Basis in paper: [inferred] The paper discusses LBPP as a more challenging and uncontaminated benchmark, but does not explore its correlation with real-world coding performance.
- Why unresolved: The paper focuses on comparing LBPP with existing benchmarks like HumanEval and MBPP, but does not investigate how well LBPP predicts performance on actual coding tasks.
- What evidence would resolve it: Conducting a study that evaluates models on both LBPP and real-world coding tasks, and analyzing the correlation between their performances on these two sets of tasks.

### Open Question 2
- Question: What is the impact of data contamination on the perceived progress of code generation models over time?
- Basis in paper: [explicit] The paper discusses how data contamination from evaluation datasets like HumanEval and MBPP may lead to overfitting and unreliable metrics for measuring generalization capability.
- Why unresolved: While the paper identifies contamination as a problem, it does not quantify its impact on the perceived progress of models over time.
- What evidence would resolve it: Analyzing the performance trends of models on contaminated and uncontaminated datasets over time to assess how contamination affects the perceived progress in code generation capabilities.

### Open Question 3
- Question: How effective are current decontamination methods in removing contaminated data from training corpora?
- Basis in paper: [explicit] The paper mentions that common automatic decontamination methods have low recall and do not adequately capture code similarity.
- Why unresolved: The paper highlights the limitations of existing decontamination methods but does not explore alternative or improved methods.
- What evidence would resolve it: Developing and testing new decontamination techniques that can more effectively identify and remove contaminated data from training corpora, and evaluating their performance compared to current methods.

## Limitations

- The contamination analysis relies on surface-level GitHub search counts and semantic similarity metrics, which may not capture all contamination pathways
- The exact contamination status of the source Python Practice Problems dataset used to create LBPP remains uncertain
- The analysis focuses primarily on evol-instruct and StarCoder-instruct datasets, potentially missing other synthetic data sources

## Confidence

- **High confidence**: The existence of widespread data contamination in HumanEval and MBPP benchmarks, supported by direct evidence of GitHub search hits and synthetic data similarity
- **Medium confidence**: The performance degradation observed on LBPP is primarily due to contamination rather than increased task difficulty
- **Medium confidence**: The proposed mechanisms of contamination (direct leakage, synthetic data generation, and model selection bias) adequately explain the observed phenomena

## Next Checks

1. Conduct a controlled experiment training models with progressively decontaminated subsets of training data to measure the relationship between contamination levels and benchmark performance
2. Perform a comprehensive semantic deduplication analysis on multiple synthetic datasets beyond just evol-instruct and StarCoder-instruct to verify the generalizability of synthetic data contamination findings
3. Implement a blind evaluation protocol where model developers have no access to contamination status of training data, then compare results with the current analysis to validate the contamination detection methodology