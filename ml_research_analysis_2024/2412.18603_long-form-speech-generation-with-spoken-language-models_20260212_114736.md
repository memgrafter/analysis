---
ver: rpa2
title: Long-Form Speech Generation with Spoken Language Models
arxiv_id: '2412.18603'
source_url: https://arxiv.org/abs/2412.18603
tags:
- speech
- generation
- spoken
- text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpeechSSM is the first spoken language model to generate coherent,
  long-form speech in a single decoding session without text intermediates, achieving
  up to 16 minutes of natural, speaker-consistent audio. Built on linear-time state-space
  modeling with hybrid recurrence and attention, SpeechSSM surpasses Transformer-based
  models in both semantic coherence and generation efficiency over multi-minute continuations.
---

# Long-Form Speech Generation with Spoken Language Models

## Quick Facts
- arXiv ID: 2412.18603
- Source URL: https://arxiv.org/abs/2412.18603
- Reference count: 40
- SpeechSSM generates coherent, long-form speech up to 16 minutes in a single decoding session without text intermediates

## Executive Summary
SpeechSSM is the first spoken language model to generate coherent, long-form speech up to 16 minutes in a single decoding session without requiring text as an intermediate step. Built on linear-time state-space modeling with hybrid recurrence and attention, SpeechSSM achieves superior semantic coherence and generation efficiency over Transformer-based models for multi-minute continuations. The model introduces LibriSpeech-Long, a benchmark and new automated metrics for long-form spoken language evaluation, demonstrating SpeechSSM's ability to maintain content consistency and reduce repetition compared to prior models.

## Method Summary
SpeechSSM uses a Griffin hybrid state-space model architecture that interleaves gated linear recurrent units (LRUs) with local multi-query attention blocks, initialized from RecurrentGemma weights and trained on LibriLight audio data. The model processes audio using USM-v2 tokenization and SoundStream neural audio codec, with training on 4-minute sliding windows and evaluation on extended 16-minute segments. A key innovation is the use of window overlap during synthesis to maintain speaker consistency and avoid generation failures at segment boundaries.

## Key Results
- SpeechSSM-9B maintains semantic coherence at 16 minutes while SpeechTransformer-2B trained on 4-minute segments degrades
- Automated side-by-side LLM-as-judge evaluation shows SpeechSSM significantly outperforms TWIST and Spirit LM on long-form generation quality
- SpeechSSM achieves superior generation efficiency with higher throughput and reduced repetition compared to Transformer baselines

## Why This Works (Mechanism)
The hybrid SSM architecture combines the efficiency of recurrent processing with the expressiveness of local attention, enabling linear-time computation while maintaining contextual awareness. The sliding window approach with overlap synthesis addresses the challenge of maintaining speaker consistency across long-form generations, while the use of existing speech tokenization and codec infrastructure provides a practical foundation for training and evaluation.

## Foundational Learning
- State-space models: Why needed - enable linear-time sequence processing for efficient long-form generation; Quick check - verify model complexity scales linearly with sequence length
- Hybrid recurrence and attention: Why needed - combines recurrent efficiency with local contextual modeling; Quick check - confirm attention windows are small relative to sequence length
- Sliding window synthesis with overlap: Why needed - maintains speaker consistency across generation boundaries; Quick check - measure speaker embedding drift between overlapping segments
- LLM-as-judge evaluation: Why needed - provides more discriminative automated assessment than existing metrics; Quick check - validate against human judgments on sample comparisons
- Speech tokenization and codec integration: Why needed - enables end-to-end speech generation without text intermediates; Quick check - verify audio reconstruction quality from tokens

## Architecture Onboarding

**Component map:** SpeechSSM -> USM-v2 tokenizer -> SoundStream codec -> Griffin hybrid SSM -> Audio output

**Critical path:** Audio input → Tokenization → SSM processing → Synthesis → Audio output

**Design tradeoffs:** SSM vs Transformer (efficiency vs expressivity), window length vs overlap (quality vs computational cost), automated vs human evaluation (scalability vs accuracy)

**Failure signatures:** Silence/noise after training length (EOS encoding issues), speaker inconsistency over long generations, semantic drift in multi-minute continuations

**3 first experiments:**
1. Generate 16-minute continuations from SpeechSSM-9B and measure semantic similarity decay compared to 4-minute training baseline
2. Test different overlap sizes (0%, 25%, 50%) during synthesis to optimize speaker consistency
3. Compare LLM-as-judge scores against human ratings for sample pairs across model variants

## Open Questions the Paper Calls Out
1. Can semantic coherence advantage be attributed solely to architectural differences or does it require longer training sequences? The comparison mixes model architecture with training length effects.
2. What limits text-initialized models from maintaining coherence beyond training lengths? The paper shows correlation but not mechanism for why models fail to extrapolate.
3. How does LLM-as-judge compare to human evaluation for long-form speech quality? The paper demonstrates improved discrimination but lacks human validation studies.

## Limitations
- Unknown exact hyperparameters and layer configurations for Griffin hybrid SSM implementation
- Evaluation relies on automated metrics that may not capture full perceptual quality of long-form speech
- LibriSpeech-Long benchmark reprocessed from existing data rather than collected specifically for long-form evaluation

## Confidence
- High: SpeechSSM demonstrates superior efficiency and reduced repetition compared to Transformer baselines
- Medium: Claims about naturalness and speaker consistency supported by MOS scores but lack detailed perceptual studies
- Low: Semantic coherence claims over multi-minute continuations depend on embedding-based similarity measures

## Next Checks
1. Implement ablation studies varying window overlap sizes and padding strategies to determine their impact on generation quality and speaker consistency
2. Conduct controlled human evaluation studies comparing SpeechSSM outputs against ground truth and other baselines for semantic coherence, speaker consistency, and naturalness over 5-16 minute continuations
3. Test the model's ability to maintain content consistency and avoid repetition when generating beyond the training window length (e.g., 20+ minutes) to validate long-form capability claims