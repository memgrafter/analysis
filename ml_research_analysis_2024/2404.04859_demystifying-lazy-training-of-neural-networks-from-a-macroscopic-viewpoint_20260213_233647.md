---
ver: rpa2
title: Demystifying Lazy Training of Neural Networks from a Macroscopic Viewpoint
arxiv_id: '2404.04859'
source_url: https://arxiv.org/abs/2404.04859
tags:
- then
- neural
- page
- where
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the training dynamics of deep neural networks\
  \ through the lens of macroscopic limits, focusing on the interplay between initialization\
  \ schemes and the training behavior of multi-layer fully connected neural networks.\
  \ The authors introduce a unified approach that analyzes the behavior of neural\
  \ networks as the width m tends to infinity, revealing that gradient descent can\
  \ rapidly drive deep neural networks to zero training loss, irrespective of the\
  \ specific initialization schemes employed by weight parameters, provided that the\
  \ initial scale of the output function \u03BA surpasses a certain threshold."
---

# Demystifying Lazy Training of Neural Networks from a Macroscopic Viewpoint

## Quick Facts
- arXiv ID: 2404.04859
- Source URL: https://arxiv.org/abs/2404.04859
- Reference count: 40
- Primary result: Gradient descent can drive deep neural networks to zero training loss in the theta-lazy regime where initial scale κ surpasses a threshold, regardless of initialization scheme.

## Executive Summary
This paper investigates the training dynamics of deep neural networks through macroscopic limits, revealing that the initial scale κ of the output function governs training behavior more than initialization schemes. The authors introduce the theta-lazy regime where log κ / log m > 0, showing that gradient descent can rapidly achieve zero training loss by keeping parameters close to initialization. Through rigorous analysis of Gram matrix concentration and linearization, the study demonstrates that deep networks in this regime behave like kernel regression predictors, extending Neural Tangent Kernel theory to a broader class of initialization schemes.

## Method Summary
The authors analyze L-layer fully connected neural networks with width m approaching infinity, using scaling factors β_l for initialization and focusing on the initial output scale κ. They establish the theta-lazy regime through three key mechanisms: Gram matrix concentration around limiting values, positive definiteness ensuring kernel behavior, and parameter linearization when remaining close to initialization. The analysis relies on concentration inequalities for random matrices and shows that when Σ γ_l < (L+1)/2, where γ_l relates to the scaling factors, the network enters lazy training where parameters stay near their initial values throughout training.

## Key Results
- The theta-lazy regime is governed by initial scale κ rather than specific initialization schemes
- Normalized Gram matrices concentrate around limiting values with high probability
- Training dynamics can be linearized when parameters remain close to initialization
- Zero training loss is achievable regardless of initialization when κ exceeds threshold

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The theta-lazy regime is governed by the initial scale κ of the output function rather than the specific initialization scheme.
- Mechanism: As width m approaches infinity, if initial scale κ surpasses threshold (lim m→∞ log κ / log m > 0), parameters remain stuck near initialization throughout training, allowing network to behave like kernel regression predictor.
- Core assumption: Activation function satisfies Assumption 1 (analytic, bounded derivatives, asymptotic limits a ≠ b) and input data satisfies Assumption 2 (bounded, non-parallel samples).
- Evidence anchors:
  - [abstract] "gradient descent can rapidly drive deep neural networks to zero training loss, irrespective of the specific initialization schemes employed by weight parameters, provided that the initial scale of the output function κ surpasses a certain threshold"
  - [section] "Our investigation reveals that gradient descent can rapidly drive deep neural networks to zero training loss, irrespective of the specific initialization schemes employed by weight parameters, provided that the initial scale of the output function κ surpasses a certain threshold"
  - [corpus] Weak - no direct neighbor papers address this specific scaling mechanism
- Break condition: If activation function violates Assumption 1 or input data violates Assumption 2, kernel approximation breaks down.

### Mechanism 2
- Claim: The normalized Gram matrix G[l](θ) stays close to its limiting value K[l] when initial scale is large enough.
- Mechanism: Through concentration inequalities, authors show normalized Gram matrices at initialization concentrate around limiting values, ensuring positive definiteness and enabling lazy training regime.
- Core assumption: Width m is sufficiently large relative to n²/λ²_S, where λ_S is minimum eigenvalue of limiting Gram matrices.
- Evidence anchors:
  - [section] "the normalized Gram matrix G[l](θ0) is close to the normalized limiting Gram matrix K[l]" and Proposition 4 establishing eigenvalue bounds
  - [section] "with high probability, for any l ∈ [L + 1] λmin(G[l](θ0)) ≥ 3/4 λ_S"
  - [corpus] Weak - no neighbor papers provide direct evidence for this concentration mechanism
- Break condition: If m is not large enough relative to sample size n, concentration fails and Gram matrix loses positive definiteness.

### Mechanism 3
- Claim: Training dynamics can be linearized around initialization when parameters remain stuck.
- Mechanism: When parameters vary by small relative amount (RD(W[l](t)) → 0 as m→∞), network output function can be well-approximated by linearization, which corresponds to kernel regression.
- Core assumption: Scaling factors γ_l satisfy Σ γ_l < (L+1)/2, ensuring parameter variations remain bounded.
- Evidence anchors:
  - [section] "the output function of L-layer NNs is linear with respect to θ_a, hence for any l ∈ [L], if the set of parameters θ_W[l] remain stuck to its initialization throughout the whole training process, then the training dynamics of L-layer NNs can be linearized"
  - [section] "This linear approximation holds valid only when θ_W[l](t) remains within a small neighbourhood of θ_W[l](0) for every l ∈ [L]"
  - [corpus] Weak - no neighbor papers directly support this linearization mechanism
- Break condition: If scaling factors don't satisfy condition Σ γ_l < (L+1)/2, parameters will deviate significantly from initialization and linearization fails.

## Foundational Learning

- Concept: Macroscopic limit analysis
  - Why needed here: Paper studies neural network behavior as width m → ∞, requiring tools from continuum mechanics and statistical physics to analyze collective parameter behavior
  - Quick check question: How does the macroscopic limit relate to the mean-field limit in neural network theory?

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: Paper extends NTK theory by relaxing scaling condition from log κ / log m = 1/2 to log κ / log m > 0, showing broader applicability
  - Quick check question: What is the key difference between NTK scaling and theta-lazy scaling in terms of parameter initialization?

- Concept: Concentration inequalities for random matrices
  - Why needed here: Proof relies on showing that normalized Gram matrices concentrate around their limiting values with high probability
  - Quick check question: What type of concentration inequality is used to bound the deviation between G[l](θ0) and K[l]?

## Architecture Onboarding

- Component map: Multi-layer fully connected network with L hidden layers -> Parameter initialization with scaling factors β_l -> Normalized parameters W[l]/√m and a/√m -> Gram matrices G[l](θ) for each layer -> Limiting Gram matrices K[l] -> Stopping time t* defining lazy regime boundary

- Critical path: Parameter initialization → Gram matrix concentration → Positive definiteness → Training dynamics linearization → Zero training loss

- Design tradeoffs:
  - Larger initial scale κ enables lazy regime but may hurt generalization
  - Smaller width m relative to n breaks concentration and positive definiteness
  - Choice of activation function affects the limiting Gram matrices K[l]

- Failure signatures:
  - Gram matrices lose positive definiteness (eigenvalues approach zero)
  - Parameter relative distances RD(W[l](t)) grow large during training
  - Training loss plateaus before reaching zero
  - Numerical instability in computing normalized Gram matrices

- First 3 experiments:
  1. Verify Gram matrix concentration by plotting eigenvalue distributions for different m values
  2. Test lazy regime behavior by varying initial scale κ and measuring parameter variation during training
  3. Compare training dynamics with and without lazy regime condition to observe linearization effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the theta-lazy regime behave in Residual Neural Networks (ResNet) compared to fully connected networks?
- Basis in paper: [explicit] Authors hypothesize behavior may depend on specific initialization schemes due to skip-connection architecture of ResNet.
- Why unresolved: Paper focuses on fully connected networks and does not provide empirical or theoretical analysis for ResNet architectures.
- What evidence would resolve it: Experiments comparing training dynamics of ResNet with various initialization schemes, or theoretical analysis of how skip-connections affect theta-lazy regime.

### Open Question 2
- Question: What is the complete phase diagram for L-layer neural networks, including the transition across the boundary where lim m→∞ log κ / log m = 0?
- Basis in paper: [explicit] Authors state they will explore regime where lim m→∞ log κ / log m < 0 in subsequent paper and aim to provide complete analysis of transition across this boundary.
- Why unresolved: Current paper focuses on theta-lazy regime (lim m→∞ log κ / log m > 0) and does not address behavior in other regimes or nature of transition.
- What evidence would resolve it: Comprehensive theoretical or empirical study characterizing behavior of L-layer neural networks across all regimes, including transition boundary.

### Open Question 3
- Question: What is the mechanism of initial condensation for multi-layer neural networks, and what are the directions towards which weight parameters condense?
- Basis in paper: [explicit] Authors state they aim to reveal mechanism of initial condensation for multi-layer NNs and identify directions towards which weight parameters condense in subsequent paper.
- Why unresolved: Current paper focuses on theta-lazy regime and does not address phenomenon of initial condensation.
- What evidence would resolve it: Detailed theoretical or empirical analysis of initial condensation phenomenon in multi-layer neural networks, including identification of specific directions of condensation.

## Limitations
- Theory applies specifically to fully connected networks with analytic activation functions, limiting generalizability to other architectures
- Macroscopic limit approach assumes extremely wide networks (m → ∞), creating gap between theoretical predictions and practical finite-width implementations
- Focus on zero training loss achievement doesn't address generalization performance, which may degrade in theta-lazy regime

## Confidence
- **High confidence**: Mechanism that large initial scale κ enables lazy training when log κ / log m > 0 is well-supported by concentration analysis and kernel approximation framework
- **Medium confidence**: Claim that this regime is independent of specific initialization schemes holds mathematically but may have practical implications for generalization not addressed
- **Low confidence**: Extension of NTK theory to theta-lazy regime is novel but lacks empirical validation across diverse network architectures and datasets

## Next Checks
1. **Empirical verification of lazy regime boundaries**: Systematically vary initial scale κ and width m to empirically determine threshold conditions log κ / log m > 0, measuring when parameters begin to deviate from initialization during training

2. **Generalization performance testing**: Compare test accuracy between networks trained in theta-lazy regime versus those trained with standard NTK scaling (log κ / log m = 1/2) to assess practical tradeoff between training dynamics and generalization

3. **Architecture extension experiments**: Test whether theta-lazy regime persists in convolutional neural networks by adapting Gram matrix concentration analysis to structured parameter sharing in CNN architectures