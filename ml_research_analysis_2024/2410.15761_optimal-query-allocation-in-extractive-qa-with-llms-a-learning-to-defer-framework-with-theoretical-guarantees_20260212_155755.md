---
ver: rpa2
title: 'Optimal Query Allocation in Extractive QA with LLMs: A Learning-to-Defer Framework
  with Theoretical Guarantees'
arxiv_id: '2410.15761'
source_url: https://arxiv.org/abs/2410.15761
tags:
- allocation
- loss
- deferral
- optimal
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Learning-to-Defer framework that dynamically
  allocates queries to specialized agents, balancing accuracy and computational efficiency
  in extractive QA tasks. The method leverages theoretical guarantees to ensure optimal
  deferral decisions, allowing resource-constrained environments to achieve high performance
  by selectively routing queries to more capable models.
---

# Optimal Query Allocation in Extractive QA with LLMs: A Learning-to-Defer Framework with Theoretical Guarantees

## Quick Facts
- **arXiv ID**: 2410.15761
- **Source URL**: https://arxiv.org/abs/2410.15761
- **Reference count**: 38
- **Primary result**: A Learning-to-Defer framework that dynamically allocates queries to specialized agents, balancing accuracy and computational efficiency in extractive QA tasks with theoretical guarantees.

## Executive Summary
This paper proposes a Learning-to-Defer framework that dynamically allocates queries to specialized agents in extractive QA tasks, balancing accuracy and computational efficiency. The method leverages theoretical guarantees to ensure optimal deferral decisions, allowing resource-constrained environments to achieve high performance by selectively routing queries to more capable models. Evaluated on SQuADv1, SQuADv2, and TriviaQA, the approach outperforms larger models while maintaining low computational overhead, demonstrating significant improvements in both reliability and efficiency.

## Method Summary
The Learning-to-Defer framework uses a lightweight rejector model to evaluate confidence levels of both the main model and expert models, allocating queries based on optimal deferral decisions. The framework introduces a surrogate deferral loss that is both Bayes-consistent and (G,R)-consistent, ensuring that minimizing this loss leads to learned rejectors that approximate the Bayes-optimal rejector. This allows for selective routing of queries to more capable, task-specific models, achieving high performance on extractive QA tasks without deploying large models in resource-constrained environments.

## Key Results
- Outperforms Llama 3 family models across SQuADv1, SQuADv2, and TriviaQA datasets with appropriate expert allocation
- Maintains low computational overhead while achieving high performance, demonstrating favorable GFLOPs/EM ratios
- Achieves theoretical guarantees on optimal deferral through Bayes-consistent and (G,R)-consistent surrogate loss functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Learning-to-Defer framework improves efficiency by selectively routing queries to specialized models based on confidence.
- Mechanism: A lightweight rejector model evaluates the confidence of both the main model and expert models, allocating queries to the agent with the highest confidence while minimizing computational overhead.
- Core assumption: The rejector can accurately estimate confidence levels and the cost of querying each expert.
- Evidence anchors:
  - [abstract]: "Our approach integrates a principled allocation strategy with theoretical guarantees on optimal deferral that balances performance and cost."
  - [section 4.2]: "Lemma 2 (Bayes-Rejector). Given an input x∈X and any distribution D, the optimal rejection rule that minimizes the risk associated with the true deferral loss is given by: rB,i (x) = {0, if inf gi∈G i ηi 0(x)≤ minj∈ [J] ηi j(x), j∗, otherwise, with j∗ = arg minj∈ [J] ηi j(x)."
- Break condition: If the rejector cannot accurately estimate confidence or the cost parameters are not properly calibrated, the deferral decisions may become suboptimal.

### Mechanism 2
- Claim: The framework achieves theoretical guarantees on optimal deferral through surrogate loss functions.
- Mechanism: By introducing a surrogate deferral loss that is both Bayes-consistent and (G,R)-consistent, the framework ensures that minimizing the surrogate loss leads to a learned rejector that closely approximates the Bayes-optimal rejector.
- Core assumption: The surrogate loss function accurately approximates the true deferral loss and maintains desirable optimization properties.
- Evidence anchors:
  - [abstract]: "Our approach integrates a principled allocation strategy with theoretical guarantees on optimal deferral that balances performance and cost."
  - [section 4.3]: "To effectively approximate the true deferral loss while preserving the optimality of the decision rule in Lemma 2, we leverage key concepts from consistency theory... Our goal is to construct a surrogate loss that is both Bayes-consistent and (G,R)-consistent."
- Break condition: If the surrogate loss function does not accurately approximate the true deferral loss or the consistency properties are not maintained, the theoretical guarantees may not hold.

### Mechanism 3
- Claim: The framework outperforms larger models while maintaining low computational overhead by leveraging specialized experts.
- Mechanism: By selectively deferring queries to more capable, task-specific models, the framework achieves high performance on extractive QA tasks without the need for deploying large models in resource-constrained environments.
- Core assumption: The specialized experts are more capable on extractive QA tasks than the main model, and the computational overhead of querying experts is outweighed by the performance gains.
- Evidence anchors:
  - [abstract]: "Evaluated on SQuADv1, SQuADv2, and TriviaQA, the approach outperforms larger models while maintaining low computational overhead, demonstrating significant improvements in both reliability and efficiency."
  - [section 5]: "Performance: From Figure 2, we observe that our approach is able to outperform or match the Llama 3 family models across all datasets with appropriate expert allocation."
- Break condition: If the specialized experts are not significantly more capable on extractive QA tasks or the computational overhead of querying experts becomes too high, the performance gains may not outweigh the costs.

## Foundational Learning

- Concept: Extractive Question Answering (EQA)
  - Why needed here: The framework is specifically designed for EQA tasks, where models must retrieve exact spans from a given context.
  - Quick check question: What is the main difference between extractive QA and generative QA tasks?

- Concept: Confidence-based Allocation
  - Why needed here: The framework relies on accurately estimating the confidence levels of different models to make optimal deferral decisions.
  - Quick check question: How does the framework estimate the confidence of a model's prediction?

- Concept: Surrogate Loss Functions
  - Why needed here: The framework uses surrogate loss functions to approximate the true deferral loss while maintaining desirable optimization properties.
  - Quick check question: What are the key properties that a surrogate loss function should have in this framework?

## Architecture Onboarding

- Component map: Input query and context -> Rejector model (TinyBERT) -> Main model (Llama-3.2-1B) or Expert models (ALBERT-Base, ALBERT-XXL) -> Answer

- Critical path:
  1. Input query and context are processed by the rejector model
  2. Rejector model evaluates the confidence of the main model and expert models
  3. Query is allocated to the model with the highest confidence based on the optimal deferral rule
  4. Selected model processes the query and generates the answer

- Design tradeoffs:
  - Model size vs. performance: Smaller models are more efficient but may have lower performance on complex tasks
  - Number of experts vs. complexity: More experts can improve performance but increase the complexity of the allocation strategy
  - Rejector model accuracy vs. computational overhead: A more accurate rejector can improve allocation decisions but may add computational overhead

- Failure signatures:
  - High expert allocation percentage with low performance gains: Indicates suboptimal defer decisions or underperforming experts
  - Low expert allocation percentage with high performance gains: Suggests the rejector is not deferring to experts when it should
  - High computational overhead with low performance gains: Implies the cost of querying experts outweighs the performance benefits

- First 3 experiments:
  1. Baseline comparison: Evaluate the main model's performance on EQA tasks without any deferral mechanism
  2. Expert-only evaluation: Assess the performance of each expert model on EQA tasks to determine their capabilities
  3. Ablation study: Test the framework's performance with different rejector model architectures and cost parameters to identify optimal configurations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Learning-to-Defer framework be extended to handle non-extractive QA tasks such as generative QA, multi-hop reasoning, or open-domain retrieval?
- Basis in paper: [explicit] The paper explicitly states that the framework is specifically designed for extractive QA and acknowledges that the theoretical guarantees do not directly extend to more complex NLP tasks where outputs are not constrained to predefined spans.
- Why unresolved: The lack of structured outputs in these tasks introduces additional challenges in defining optimal deferral strategies and ensuring theoretical consistency.
- What evidence would resolve it: Empirical studies demonstrating the framework's performance on non-extractive QA tasks, along with theoretical analysis of how the deferral mechanisms can be adapted for these settings.

### Open Question 2
- Question: What is the optimal method for dynamically tracking and adjusting deferral costs (βj) based on real-world deployment requirements such as network latency and cloud-related costs?
- Basis in paper: [explicit] The paper mentions that there is currently no explicit and effective method to dynamically track how query allocation varies across agents in response to changes in these cost parameters.
- Why unresolved: The current cost estimation relies on predefined parameters, which may not accurately reflect real-world deployment conditions.
- What evidence would resolve it: Development and validation of a dynamic cost estimation model that can adapt to changing network conditions and deployment environments, showing improved efficiency in real-world scenarios.

### Open Question 3
- Question: How does the Learning-to-Defer framework perform under adversarial attacks or noisy data conditions, and what are the theoretical guarantees for robustness in such scenarios?
- Basis in paper: [inferred] While the paper mentions adversarial robustness in the conclusion, it does not provide detailed analysis or experiments on the framework's performance under adversarial conditions.
- Why unresolved: The paper does not address the framework's robustness to adversarial attacks or noisy data, which are critical considerations for real-world deployment.
- What evidence would resolve it: Experiments demonstrating the framework's performance under adversarial attacks and noisy data conditions, along with theoretical analysis of its robustness guarantees.

## Limitations

- The framework's performance depends heavily on the quality of expert models and proper calibration of cost parameters, which are not extensively discussed
- Theoretical guarantees rely on accurate confidence estimation from the rejector model, which may not generalize well across diverse query types or domains
- The ablation study shows that performance varies significantly with different rejector architectures, suggesting sensitivity to implementation details

## Confidence

- **High Confidence**: The core claim that selective query allocation improves computational efficiency is well-supported by empirical results showing favorable GFLOPs/EM ratios compared to baseline models.
- **Medium Confidence**: The theoretical guarantees on optimal deferral are mathematically sound but depend on assumptions about confidence estimation accuracy that may not hold in practice.
- **Medium Confidence**: The claim that the framework outperforms larger models is supported by experimental results, but the comparison is limited to a specific set of models and datasets.

## Next Checks

1. **Robustness Test**: Evaluate the framework's performance when the rejector model's confidence estimation accuracy is deliberately degraded to assess sensitivity to this critical component.

2. **Domain Transfer Test**: Apply the framework to EQA datasets from different domains (e.g., biomedical or legal texts) to validate generalizability beyond the evaluated datasets.

3. **Cost Parameter Sensitivity**: Conduct a systematic study varying the cost parameters to identify optimal configurations and assess the framework's robustness to parameter mis-specification.