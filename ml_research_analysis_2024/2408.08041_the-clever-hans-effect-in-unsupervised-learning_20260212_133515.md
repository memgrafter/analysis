---
ver: rpa2
title: The Clever Hans Effect in Unsupervised Learning
arxiv_id: '2408.08041'
source_url: https://arxiv.org/abs/2408.08041
tags:
- data
- unsupervised
- learning
- anomaly
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the presence of \"Clever Hans\" effects\u2014\
  where models make correct predictions for the wrong reasons\u2014in unsupervised\
  \ learning. Using Explainable AI techniques, particularly BiLRP and virtual layer-based\
  \ LRP, the authors analyze unsupervised models in representation learning (e.g.,\
  \ CLIP, SimCLR, BarlowTwins) and anomaly detection (e.g., D2Neighbors, PatchCore)."
---

# The Clever Hans Effect in Unsupervised Learning

## Quick Facts
- arXiv ID: 2408.08041
- Source URL: https://arxiv.org/abs/2408.08041
- Reference count: 40
- Key outcome: Unsupervised models often rely on spurious features like text annotations and high-frequency noise, leading to brittle performance that can be mitigated through feature pruning and preprocessing.

## Executive Summary
This paper investigates the presence of Clever Hans effects in unsupervised learning, where models make correct predictions for incorrect reasons. Using Explainable AI techniques, particularly BiLRP and virtual layer-based LRP, the authors analyze unsupervised models in representation learning and anomaly detection. They find that these models frequently rely on spurious features such as text annotations, human figures, or high-frequency noise, resulting in brittle performance across downstream tasks. The authors propose mitigating these effects by pruning irrelevant features or applying preprocessing layers, which restores and often improves performance.

## Method Summary
The authors employ Explainable AI techniques, specifically BiLRP (bidirectional Layer-wise Relevance Propagation) and virtual layer-based LRP, to analyze unsupervised models. They examine representation learning models (CLIP, SimCLR, BarlowTwins) and anomaly detection models (D2Neighbors, PatchCore) to identify reliance on spurious features. The methodology involves applying these explainability tools to understand feature attribution in unsupervised models and then testing mitigation strategies through feature pruning and preprocessing layers.

## Key Results
- COVID-19 detection model using PubMedCLIP relies on text artifacts rather than pathology, resulting in 40% false positive rate on heterogeneous data
- Anomaly detection models like D2Neighbors degrade significantly when high-frequency noise is removed
- Mitigation strategies through feature pruning and preprocessing layers restore and often improve performance

## Why This Works (Mechanism)
Unsupervised models learn representations without explicit labels, making them susceptible to capturing spurious correlations in the training data. When these models encounter downstream tasks, they may rely on these spurious features rather than task-relevant information. The explainability techniques reveal that models are attending to irrelevant features like text annotations, human figures, or high-frequency patterns that happen to correlate with the desired output in the training distribution but fail to generalize.

## Foundational Learning
- Explainable AI (XAI) techniques: Why needed - to understand model decision-making; Quick check - can identify feature attribution patterns
- BiLRP (bidirectional Layer-wise Relevance Propagation): Why needed - provides detailed feature importance analysis; Quick check - reveals which input regions contribute to predictions
- Virtual layer-based LRP: Why needed - enables explainability in complex architectures; Quick check - allows attribution analysis across different model components
- Representation learning: Why needed - unsupervised models learn task-agnostic features; Quick check - features must transfer to downstream tasks
- Anomaly detection: Why needed - identifies unusual patterns in data; Quick check - relies on learned normal data distribution

## Architecture Onboarding
Component map: Input -> Unsupervised Model -> Feature Extractor -> Downstream Task -> Output
Critical path: Feature extraction from input through unsupervised model to downstream task performance
Design tradeoffs: Model complexity vs. interpretability, training efficiency vs. robustness
Failure signatures: Reliance on spurious features, performance degradation on out-of-distribution data
First experiments:
1. Apply BiLRP to identify feature attribution patterns in CLIP-based models
2. Test COVID-19 detection performance on heterogeneous vs. homogeneous datasets
3. Evaluate anomaly detection performance when high-frequency noise is removed

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis primarily confined to specific model architectures and datasets
- Focus on computer vision tasks limits applicability to other domains
- Does not provide direct comparative analysis with supervised models

## Confidence
High: Unsupervised models exhibit Clever Hans effects with multiple concrete examples
Medium: Proposed mitigation strategies show promise but require more extensive validation
Low: Claim about relative susceptibility compared to supervised models lacks comparative analysis

## Next Checks
1. Test proposed mitigation strategies across broader range of unsupervised learning architectures and downstream tasks
2. Conduct systematic ablation studies removing different types of spurious features
3. Compare Clever Hans effects in unsupervised versus supervised models on identical tasks