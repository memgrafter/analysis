---
ver: rpa2
title: 'Unveiling Key Aspects of Fine-Tuning in Sentence Embeddings: A Representation
  Rank Analysis'
arxiv_id: '2405.11297'
source_url: https://arxiv.org/abs/2405.11297
tags:
- rank
- performance
- phase
- sentence
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates fine-tuning dynamics in contrastive learning-based
  (CL-based) sentence embedding models using representation rank analysis. The authors
  define two phases of fine-tuning: Phase 1 (rank increasing) and Phase 2 (rank decreasing),
  and analyze key aspects including alignment/uniformity, linguistic abilities, and
  performance-rank correlation across these phases.'
---

# Unveiling Key Aspects of Fine-Tuning in Sentence Embeddings: A Representation Rank Analysis

## Quick Facts
- arXiv ID: 2405.11297
- Source URL: https://arxiv.org/abs/2405.11297
- Authors: Euna Jung; Jaeill Kim; Jungmin Ko; Jinwoo Park; Wonjong Rhee
- Reference count: 18
- One-line primary result: Rank reduction regularization improves performance and stability of contrastive learning-based sentence embedding models by reducing average rank from 324 to 195 and improving STS-B performance from 76.85 to 79.94

## Executive Summary
This paper investigates fine-tuning dynamics in contrastive learning-based (CL-based) sentence embedding models using representation rank analysis. The authors identify two distinct phases during fine-tuning: Phase 1 (rank increasing) and Phase 2 (rank decreasing), and analyze how rank correlates with performance metrics and model stability. Through comprehensive experiments across five state-of-the-art CL-based models, they demonstrate that representation rank strongly correlates with STS performance and stability, leading to the proposal of a rank reduction (RR) regularization strategy that consistently improves both performance and stability.

## Method Summary
The authors analyze five CL-based sentence embedding models (SimCSE, MixCSE, ESimCSE, InfoCSE, PromptBERT) fine-tuned on Wikipedia using contrastive learning objectives. They compute representation rank using an energy-based measurement and identify two phases in the fine-tuning process based on rank dynamics. The rank reduction (RR) regularization strategy adds a differentiable rank penalty term to the contrastive loss, encouraging embeddings to occupy lower-dimensional subspaces. The method is evaluated on STS 2012-16, STS-B, and SICK-R datasets, with stability measured by standard deviation across random seeds.

## Key Results
- Representation rank strongly correlates with STS performance (0.85 in Phase 1, -0.81 in Phase 2)
- RR regularization reduces average rank from 324 to 195 and improves average STS-B performance from 76.85 to 79.94
- For unstable models like SimCSE and MixCSE, RR reduces standard deviation of performance across random seeds by 0.51 and 0.07 respectively
- Phase 2 rank reduction is particularly effective for performance gains, while Phase 1 rank control shows mixed results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representation rank reduction directly improves contrastive learning performance by controlling uniformity and alignment trade-offs.
- Mechanism: The RR regularization term encourages embeddings to occupy a lower-dimensional subspace while maintaining discriminative power. During Phase 2, when rank naturally decreases, this regularization accelerates performance gains by reducing representational redundancy.
- Core assumption: Lower rank representations still preserve sufficient semantic information for STS tasks while improving the cosine similarity metric's effectiveness.
- Evidence anchors:
  - [abstract] "The RR strategy reduces average rank from 324 to 195 and improves average STS-B performance from 76.85 to 79.94"
  - [section 4.3] "Performance improvement by RR can be observed for all five models"
  - [corpus] Weak evidence - only general sentence embedding papers found, no direct discussion of rank regularization mechanisms
- Break condition: If reducing rank below a certain threshold causes loss of essential semantic distinctions, performance would degrade rather than improve.

### Mechanism 2
- Claim: Phase-dependent rank dynamics create optimal windows for regularization.
- Mechanism: Phase 1 focuses on uniformity improvement (rank increases), while Phase 2 focuses on alignment recovery (rank decreases). RR regularization is most effective when applied during Phase 2 where rank reduction correlates strongly with performance gains (-0.81 correlation).
- Core assumption: The two-phase structure represents distinct optimization objectives that can be manipulated independently through regularization.
- Evidence anchors:
  - [section 3.1] "In Phase 2, rank exhibits a very similar trend to alignment, with both steadily returning towards their original values"
  - [section 3.3] "In Phase 2, the correlation remains strong but in a reversed manner at -0.81"
  - [section 4.2] "rapid and stable fine-tuning, resulting in the attainment of best performance swiftly and consistently"
- Break condition: If phase boundaries shift significantly with different architectures or training regimes, the optimal regularization timing may change.

### Mechanism 3
- Claim: Rank regularization improves model stability by reducing variance across random seeds.
- Mechanism: Higher rank representations exhibit greater sensitivity to initialization variations. By constraining rank, RR reduces the embedding space's degrees of freedom, making models less susceptible to seed-dependent performance fluctuations.
- Core assumption: Representation rank is the primary source of instability rather than other factors like learning rate or batch size.
- Evidence anchors:
  - [section 4.4] "For unstable models like SimCSE and MixCSE, RR reduces standard deviation of performance across random seeds by 0.51 and 0.07 respectively"
  - [section 4.4] "The Pearson correlation between the average rank and standard deviation is remarkably high at 0.94"
  - [section 4.3] "the average performance of SimCSE is significantly improved from 80.90 to 84.19"
- Break condition: If other sources of instability (like hard negative sampling) dominate, rank reduction may have limited effect on stability.

## Foundational Learning

- Concept: Contrastive learning loss formulation
  - Why needed here: Understanding the SimCSE loss structure is essential for implementing RR regularization correctly
  - Quick check question: What are the two components of the contrastive loss that RR regularization interacts with?

- Concept: Representation rank and effective rank calculation
  - Why needed here: RR regularization requires computing the exponential form of effective rank to make it differentiable
  - Quick check question: How does the effective rank formula differ from traditional algebraic rank, and why is this difference important for regularization?

- Concept: Phase identification in training dynamics
  - Why needed here: RR effectiveness depends on correctly identifying when Phase 1 ends and Phase 2 begins based on rank peaks
  - Quick check question: What metric is used to determine the boundary between Phase 1 and Phase 2 in the fine-tuning process?

## Architecture Onboarding

- Component map: The RR regularization integrates into the existing contrastive learning pipeline by adding a differentiable rank penalty term to the loss function. The effective rank computation requires maintaining and processing the representation matrix H during each training step.

- Critical path: 1) Compute contrastive loss as usual, 2) Calculate representation matrix H from current batch, 3) Compute effective rank using eigenvalue decomposition of Z^T Z, 4) Add rank penalty to total loss, 5) Backpropagate through both contrastive and rank components.

- Design tradeoffs: Higher RR coefficients provide more stability but risk underfitting by constraining the representation space too aggressively. Lower coefficients maintain more expressive power but may not sufficiently address instability.

- Failure signatures: If RR coefficient is too high, models may show degraded STS performance across all seeds with low variance (stable but poor). If too low, instability persists with high variance across seeds.

- First 3 experiments:
  1. Implement basic RR regularization with SimCSE-BERTbase using coefficient 1e-3, verify rank reduction from ~324 to ~195 and performance improvement
  2. Test phase-dependent RR by applying different coefficients to Phase 1 vs Phase 2, measure correlation between rank reduction and performance gains
  3. Evaluate stability improvement by running 10 seeds with and without RR, calculate standard deviation reduction across STS-B results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do CL-based models perform better at lower representation ranks, and what is the theoretical relationship between rank and performance?
- Basis in paper: [explicit] The authors acknowledge this as a limitation, stating "we did not provide a theoretical explanation for why CL-based models perform better at lower ranks."
- Why unresolved: The paper focuses on empirical analysis and regularization strategies rather than theoretical foundations. The connection between representational rank and downstream performance remains unexplored.
- What evidence would resolve it: Theoretical analysis demonstrating how lower rank representations lead to better generalization or how rank reduction affects the geometry of the embedding space in ways that improve semantic similarity tasks.

### Open Question 2
- Question: What is the optimal strategy for rank control across different fine-tuning phases, and why does Phase 2 rank reduction show more consistent benefits than Phase 1?
- Basis in paper: [explicit] The authors find that "promoting rank reduction in Phase 2 is significantly helpful" while "For Phase 1, it is unclear if either rank increase or rank decrease is helpful."
- Why unresolved: The experimental results show inconsistent effects of rank control in Phase 1, and the paper does not provide a clear explanation for this discrepancy.
- What evidence would resolve it: Controlled experiments systematically varying rank control strategies across both phases, combined with analysis of how rank affects alignment/uniformity trade-offs and linguistic ability preservation in each phase.

### Open Question 3
- Question: How does rank reduction interact with different linguistic abilities across various probing tasks, and which linguistic aspects benefit most from rank regularization?
- Basis in paper: [inferred] The paper categorizes linguistic abilities into groups based on their behavior during fine-tuning, but does not explicitly analyze how rank reduction affects these different categories.
- Why unresolved: While the paper shows that rank correlates with overall performance and stability, it does not investigate which specific linguistic abilities are most affected by rank regularization.
- What evidence would resolve it: Detailed analysis of probing task performance when applying rank reduction to models that already perform well on certain linguistic abilities, identifying which tasks benefit most from rank regularization.

## Limitations
- The analysis focuses exclusively on five CL-based models using English Wikipedia and STS benchmarks, limiting generalizability to other languages, domains, or embedding paradigms
- The Phase 1/Phase 2 boundary identification relies on observed rank peaks that may vary with different architectures or training configurations
- The RR regularization mechanism assumes rank reduction preserves semantic information without empirical validation of information loss at different rank levels

## Confidence

- High confidence: The empirical correlation between rank and performance (0.85 in Phase 1, -0.81 in Phase 2) is well-supported by the data across multiple models and metrics
- Medium confidence: The mechanism linking rank reduction to improved cosine similarity effectiveness is plausible but requires additional ablation studies to isolate from other training dynamics
- Medium confidence: The stability improvements from RR regularization are demonstrated but may depend heavily on the specific implementation details not fully disclosed in the paper

## Next Checks

1. Cross-linguistic validation: Apply RR regularization to multilingual CL-based models and evaluate whether the Phase 1/Phase 2 dynamics and performance correlations hold across different language families and cultural contexts

2. Information preservation analysis: Conduct controlled experiments reducing rank below the optimal range (190-198) to determine the minimum rank threshold where semantic information loss begins to degrade STS performance

3. Architecture ablation study: Test RR regularization on non-CL-based embedding models (e.g., masked language models fine-tuned with regression objectives) to determine whether the rank-performance relationship is specific to contrastive learning or represents a broader principle in sentence embedding optimization