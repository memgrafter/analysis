---
ver: rpa2
title: Contrastive Learning of Preferences with a Contextual InfoNCE Loss
arxiv_id: '2407.05898'
source_url: https://arxiv.org/abs/2407.05898
tags:
- card
- loss
- cards
- infonce
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of learning contextual preferences
  when only a single preferred choice is observed among many options, which can bias
  the training data. The authors adapt the InfoNCE loss from CLIP to handle this by
  constructing a similarity matrix that only includes valid preference comparisons,
  masking out invalid ones.
---

# Contrastive Learning of Preferences with a Contextual InfoNCE Loss

## Quick Facts
- arXiv ID: 2407.05898
- Source URL: https://arxiv.org/abs/2407.05898
- Reference count: 18
- Primary result: Achieves 68.80% top-1 accuracy on Magic: The Gathering draft data

## Executive Summary
This paper addresses the challenge of learning contextual preferences when only a single preferred choice is observed among many options, which can bias training data. The authors adapt the InfoNCE loss from CLIP to handle this by constructing a similarity matrix that only includes valid preference comparisons, masking out invalid ones. Experiments on Magic: The Gathering draft data show that the adapted InfoNCE loss achieves the highest accuracy (68.80%) and competitive training speed compared to triplet loss variants and standard InfoNCE, demonstrating its effectiveness for contextual preference learning.

## Method Summary
The method learns contextual preferences by adapting the InfoNCE loss to handle the single-preferred-choice problem. Instead of creating multiple pairwise preferences, it consolidates the choice set into a single multi-class comparison using an NxM similarity matrix where N is the number of samples and M is the number of unique cards. A mask is applied to retain only valid preference comparisons (positive: chosen card in context; negatives: unchosen cards in same context). The approach uses separate neural network encoders for card pools and individual cards, with a shared embedding space for contrastive learning.

## Key Results
- Achieves highest accuracy of 68.80% compared to triplet loss variants and standard InfoNCE
- Demonstrates competitive training speed while avoiding the bias from single-preferred-choice problem
- Shows that masking invalid comparisons is crucial for effective contextual preference learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adapted InfoNCE loss correctly models contextual preference by masking out invalid comparisons in the similarity matrix.
- Mechanism: Vanilla CLIP includes all pairwise combinations as contrastive examples, but many are invalid in contextual preference learning as they mix items from different contexts. The proposed method constructs an NxM matrix and applies a mask to retain only valid preference comparisons.
- Core assumption: Only comparisons within the same decision context are valid for learning preference structure.
- Evidence anchors: [abstract] "adapting CLIP to the problem... by constructing a similarity matrix that only includes valid preference comparisons, masking out invalid ones"; [section] "we create a N × M matrix where N is the number of samples and M is the number of unique cards... We create a N × M mask which marks valid and invalid comparisons"

### Mechanism 2
- Claim: The method avoids bias by treating the entire choice set as one preference constraint rather than r pairwise comparisons.
- Mechanism: Instead of creating r pairwise preferences (one chosen card vs each unchosen card), the approach consolidates this into a single multi-class comparison where the context is paired with all available options simultaneously.
- Core assumption: The choice set distribution can be modeled more efficiently as a single multi-way preference rather than multiple pairwise preferences.
- Evidence anchors: [abstract] "single preferred choice is compared against several choices, thereby blowing up the complexity and skewing the preference distribution"; [section] "we use all triplets in (C, P), including all negative examples... each sampled decision provides one row of the matrix"

### Mechanism 3
- Claim: The method maintains efficient batch processing while preserving discriminative power through selective masking.
- Mechanism: By constructing an NxM matrix and applying a mask, the approach parallelizes computation (like standard InfoNCE) while only including valid comparisons in the loss calculation.
- Core assumption: The computational efficiency gain from matrix-based processing can be preserved even with selective masking.
- Evidence anchors: [abstract] "while also alleviating problems associated with mining triplets"; [section] "we create a N × M matrix... each sampled decision provides one row of the matrix, where the card pool is related to the possible choices"

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: The paper builds on CLIP's InfoNCE framework but adapts it for contextual preferences where vanilla InfoNCE fails
  - Quick check question: What is the key difference between InfoNCE and triplet loss in how they treat positive and negative examples?

- Concept: Siamese neural networks for metric learning
  - Why needed here: The approach uses a Siamese-like structure with separate encoders for contexts and items, learning a shared embedding space
  - Quick check question: How does the weight-sharing strategy differ between the triplet loss variant and the InfoNCE variant in this paper?

- Concept: Masking in loss computation
  - Why needed here: The core innovation involves masking invalid comparisons in the similarity matrix to handle contextual constraints
  - Quick check question: What does the mask value of -1 represent in the NxM similarity matrix?

## Architecture Onboarding

- Component map:
  - Card encoder (fully connected network, 5-10 layers, 1024 neurons) -> Processes individual card representations
  - Pool encoder (convolutional neural network, 3-5 layers, 128 filters) -> Processes card pool matrices
  - Card representation (2330-dimensional vector) -> Combines numerical/categorical features, BERT text embedding, and autoencoder visual latent
  - Loss computation (masked InfoNCE with row-wise cross-entropy) -> After cosine similarity matrix construction

- Critical path: Card/Pool encoding → Cosine similarity matrix construction → Masking → Cross-entropy loss computation → Parameter updates

- Design tradeoffs:
  - Separate vs. shared encoder architectures: Separate encoders provide more flexibility but require more parameters
  - Row-wise vs. column-wise loss computation: Row-wise respects contextual constraints but may be less stable than symmetric approaches
  - Full matrix vs. sampled comparisons: Full matrix enables parallelization but requires careful masking to avoid invalid comparisons

- Failure signatures:
  - High loss but low accuracy: Masking may be too aggressive, removing valid signal
  - Low loss but poor generalization: Model may be overfitting to training contexts
  - Slow convergence: Insufficient negative examples due to aggressive masking

- First 3 experiments:
  1. Verify masking correctness by inspecting the NxM similarity matrix and mask for a small batch
  2. Compare performance with and without masking on a subset of the data to confirm the problem being solved
  3. Test different masking strategies (e.g., softer masks vs. hard 0/-1) to find optimal balance between valid signal and computational efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance change if we used a different loss function, such as a margin-based loss or a focal loss, instead of the adapted InfoNCE loss?
- Basis in paper: [explicit] The paper compares the adapted InfoNCE loss to other loss functions like triplet loss and standard InfoNCE, but does not explore other potential loss functions
- Why unresolved: The paper focuses on adapting the InfoNCE loss for contextual preference ranking and does not investigate other loss functions that might be suitable for this task
- What evidence would resolve it: Experimenting with different loss functions and comparing their performance to the adapted InfoNCE loss would provide evidence on whether other loss functions could improve the results

### Open Question 2
- Question: Would incorporating additional context information, such as the player's past choices or the current game state, further improve the model's performance?
- Basis in paper: [inferred] The paper uses the player's pool as context for predicting card preferences, but does not explore the potential benefits of incorporating more extensive context information
- Why unresolved: The paper's focus is on adapting the InfoNCE loss for contextual preference ranking, and it does not investigate the impact of incorporating more comprehensive context information
- What evidence would resolve it: Training the model with additional context information and evaluating its performance would determine if incorporating more context leads to better predictions

### Open Question 3
- Question: How does the performance of the adapted InfoNCE loss scale with the size of the dataset and the number of available cards?
- Basis in paper: [inferred] The paper evaluates the adapted InfoNCE loss on a single dataset with a specific number of cards, but does not explore how the performance changes with different dataset sizes or card pool sizes
- Why unresolved: The paper's experiments are limited to a specific dataset and card pool size, and it does not investigate the scalability of the adapted InfoNCE loss
- What evidence would resolve it: Conducting experiments with different dataset sizes and card pool sizes would provide insights into how the performance of the adapted InfoNCE loss scales with these factors

## Limitations

- Claims about avoiding bias through masking rely heavily on the assumption that all contextual information can be cleanly separated into distinct NxM matrices without overlap or ambiguity
- Evaluation is limited to a single dataset (Magic: The Gathering draft data), raising questions about generalizability to other contextual preference learning tasks
- Neural network architecture details remain underspecified, making exact reproduction challenging

## Confidence

- **High confidence**: The core mechanism of using masked similarity matrices to avoid invalid comparisons in contextual preference learning is well-supported by the described methodology and experimental results
- **Medium confidence**: Claims about avoiding the single-preferred-choice bias through consolidation are plausible but would benefit from additional ablation studies comparing different approaches to handling choice sets
- **Medium confidence**: The computational efficiency claims relative to triplet mining methods are supported by the design but lack direct timing comparisons in the results

## Next Checks

1. **Masking validation**: Create synthetic test cases with known valid/invalid comparisons to verify that the masking correctly identifies and excludes only truly invalid preference comparisons without removing valid signal

2. **Ablation on masking strategy**: Compare the proposed hard masking approach against alternative strategies (soft masks, different masking thresholds) to quantify the impact on both accuracy and training stability

3. **Generalization test**: Apply the method to a different contextual preference learning task (e.g., product recommendation in shopping baskets or document selection in context) to validate that the approach generalizes beyond the Magic: The Gathering domain