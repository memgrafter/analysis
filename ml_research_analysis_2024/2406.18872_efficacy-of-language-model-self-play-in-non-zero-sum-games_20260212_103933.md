---
ver: rpa2
title: Efficacy of Language Model Self-Play in Non-Zero-Sum Games
arxiv_id: '2406.18872'
source_url: https://arxiv.org/abs/2406.18872
tags:
- self-play
- game
- language
- games
- cooperative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically investigates whether language model self-play
  can improve performance in non-zero-sum negotiation games, focusing on a task called
  Deal or No Deal. The authors modify the game's objective to create fully cooperative,
  semi-competitive, and strictly competitive versions.
---

# Efficacy of Language Model Self-Play in Non-Zero-Sum Games

## Quick Facts
- arXiv ID: 2406.18872
- Source URL: https://arxiv.org/abs/2406.18872
- Reference count: 40
- One-line primary result: Self-play training dramatically improves language model performance in cooperative and semi-competitive negotiation games, primarily through better instruction following rather than strategic reasoning.

## Executive Summary
This paper investigates whether language model self-play can improve performance in non-zero-sum negotiation games using the Deal or No Deal task. The authors modify the game's objective to create fully cooperative, semi-competitive, and strictly competitive versions, then apply filtered behavior cloning through multiple rounds of self-play training on GPT-3.5 starting from zero-shot prompts. Self-play leads to dramatic improvements: models achieve 14-17× higher scores in self-play, with generalization to human interaction yielding 2.5-6× gains over baseline. These improvements stem primarily from better task instruction following and reduced hallucinations rather than strategic reasoning. However, self-play causes dialogue diversity to decrease in competitive settings, and strictly competitive settings show minimal improvements and poor generalization.

## Method Summary
The method uses filtered behavior cloning through multiple rounds of self-play finetuning on GPT-3.5-turbo-0125. The Deal or No Deal environment simulates item exchanges between agents with value functions, enforcing rules through error correction. Models generate dialogues in self-play games (K=500 per iteration), with high-scoring dialogues (above average) selected for finetuning. The process iterates N=10 times with three game objectives: fully cooperative (λ=1), semi-competitive (λ=0), and strictly competitive (λ=-1). Evaluation includes self-play performance and human interaction with 100 participants, measuring scores, agreement rates, error rates, and dialogue diversity metrics.

## Key Results
- Self-play improves performance 14-17× in self-play, with 2.5-6× gains over baseline when interacting with humans
- Improvements stem primarily from better task instruction following and reduced hallucinations (errors decreased from 14-56% to <1%)
- Self-play causes dialogue diversity to decrease in competitive settings, and strictly competitive setting shows minimal improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-play improves language model performance in cooperative and semi-competitive games by reinforcing agreement-oriented strategies.
- Mechanism: Models are trained on dialogues that achieved above-average scores, filtering out failed or non-productive interactions. This selective training amplifies behaviors leading to successful agreements.
- Core assumption: High-scoring games are predominantly the result of effective communication and agreement, not just lucky item allocations.
- Evidence anchors:
  - [abstract] "Self-play leads to dramatic improvements: models achieve 14-17× higher scores in self-play"
  - [section] "These improvements stem primarily from better task instruction following and reduced hallucinations rather than strategic reasoning"
  - [corpus] Weak. No direct evidence in neighbor papers about filtering high-scoring dialogues.
- Break condition: If the reward signal is noisy or not strongly correlated with actual task success, filtered behavior cloning will reinforce suboptimal or degenerate strategies.

### Mechanism 2
- Claim: Self-play enhances language model instruction following and grounding in the task context.
- Mechanism: Repeated exposure to error-correction cycles in the environment teaches models to avoid invalid proposals and hallucinations, aligning outputs with the game's constraints.
- Core assumption: The environment's error feedback is clear, consistent, and effectively corrected by the model during training.
- Evidence anchors:
  - [abstract] "These improvements stem primarily from better task instruction following and reduced hallucinations"
  - [section] "With the baseline model, errors were generated in 14% of semi-competitive games and 56% of cooperative games. After self-play finetuning, errors occurred in less than 1% of games"
  - [corpus] Weak. No explicit evidence in related work about error correction improving instruction following.
- Break condition: If the model fails to generalize error patterns beyond the training distribution, performance will degrade in unseen contexts.

### Mechanism 3
- Claim: Self-play improves task performance through increased agreement rates rather than strategic optimization.
- Mechanism: By repeatedly training on successful dialogues, models learn to prioritize reaching any valid agreement over negotiating for optimal outcomes.
- Core assumption: Agreement is easier to achieve than optimal negotiation, and the reward function heavily weights agreement completion.
- Evidence anchors:
  - [abstract] "However, at the same time, self-play causes dialogue diversity to decrease in competitive settings."
  - [section] "We found that model improvements after self-play can primarily be attributed to an increased rate of agreement."
  - [corpus] Weak. No direct evidence from neighbors about agreement rates vs strategic reasoning.
- Break condition: If the task requires genuine strategic reasoning for high performance, models will plateau despite high agreement rates.

## Foundational Learning

- Concept: Filtered Behavior Cloning
  - Why needed here: The method relies on selecting and retraining on high-performing dialogues, which is central to the self-play improvement mechanism.
  - Quick check question: What criteria determine which dialogues are kept for retraining?

- Concept: Zero-sum vs Non-zero-sum Game Theory
  - Why needed here: The paper contrasts self-play effectiveness across game types; understanding payoff structures is key.
  - Quick check question: Why does self-play fail in strictly competitive (zero-sum) settings according to the results?

- Concept: Error Correction in Reinforcement Learning Environments
  - Why needed here: The environment provides corrective feedback when models violate game rules; this shapes learning.
  - Quick check question: How does the error correction mechanism influence the training data distribution?

## Architecture Onboarding

- Component map:
  Environment -> Dialogue Generator -> Scoring System -> Filtered BC Trainer -> Human Interface

- Critical path:
  1. Generate game context (items + value functions)
  2. Run self-play dialogue (with error correction)
  3. Score outcomes
  4. Filter high-scoring dialogues
  5. Finetune model on filtered set
  6. Evaluate in self-play and with humans

- Design tradeoffs:
  - Zero-shot prompting vs few-shot examples: avoids biasing model behavior but starts from weaker baseline
  - Filtering only above-average vs all non-zero games: focuses on improvement but may discard useful borderline cases
  - Fixed number of self-play rounds vs early stopping: ensures consistent training but may overfit

- Failure signatures:
  - Agreement rate high but Pareto optimality low: model learned to agree at any cost, not optimize
  - Performance improves in self-play but not with humans: overfitting to self-play distribution
  - Vocabulary size shrinks dramatically: loss of dialogue diversity, possible mode collapse
  - Error rates remain high after training: environment feedback not being internalized

- First 3 experiments:
  1. Run 100 games of self-play with baseline GPT-3.5, measure agreement rate and error frequency
  2. Train one round of filtered BC, evaluate improvement in agreement rate
  3. Compare vocabulary size and dialogue length pre/post training to detect diversity loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of self-play iterations before performance plateaus or declines, and how does this vary across different objectives (cooperative, semi-competitive, strictly competitive)?
- Basis in paper: Explicit - The paper notes that in the cooperative setting, human-LM scores peaked and then began to decline before the 10th iteration, while in other settings performance plateaued.
- Why unresolved: The paper only tested up to 10 iterations and didn't systematically explore the optimal stopping point. The different behavior across objectives suggests there may be objective-specific optimal iteration counts.
- What evidence would resolve it: Experiments testing many more iterations (20-50+) across all objectives, with detailed tracking of performance metrics over time to identify clear peaks and decline points.

### Open Question 2
- Question: How do self-play improvements in language models differ when starting from different initial model capabilities, and what is the relationship between initial model strength and self-play effectiveness?
- Basis in paper: Explicit - The paper compared GPT-3.5 (weak baseline) with GPT-4 (strong baseline) and found GPT-3.5 improved dramatically while GPT-4 was already strong. The paper notes preliminary experiments with open-weight models showed minimal improvement.
- Why unresolved: The paper only tested two models and didn't systematically explore how initial model strength affects self-play gains. The preliminary experiments with weaker models suggest there may be a threshold effect.
- What evidence would resolve it: Systematic experiments with models of varying capabilities (e.g., different GPT versions, LLaMA models, Mixtral) tracking improvement from baseline to measure self-play effectiveness as a function of starting capability.

### Open Question 3
- Question: What specific aspects of language model self-play lead to improvements in task performance versus improvements in task instruction following and error reduction?
- Basis in paper: Explicit - The analysis section shows that improvements came primarily from better instruction following, reduced hallucinations, and higher agreement rates rather than strategic reasoning or negotiation tactics.
- Why unresolved: The paper didn't design experiments to isolate whether self-play specifically teaches strategic reasoning versus simply improves task execution through better rule following.
- What evidence would resolve it: Controlled experiments comparing self-play trained models against models trained on filtered human data (matching the agreement rates and instruction-following quality) to isolate whether strategic reasoning differences exist.

## Limitations
- Self-play effectiveness highly dependent on reward structure and game objective, with strictly competitive settings showing minimal improvement
- Improvements primarily from agreement rate increases and error reduction rather than genuine strategic reasoning
- Significant decrease in dialogue diversity during competitive self-play raises concerns about mode collapse

## Confidence
- **High confidence**: The empirical results showing improved agreement rates and reduced errors in cooperative and semi-competitive settings, supported by quantitative metrics across multiple game iterations.
- **Medium confidence**: The claim that improvements stem primarily from instruction following and error reduction rather than strategic reasoning, based on qualitative analysis of dialogues and observed performance patterns.
- **Low confidence**: The generalization results to human interaction, given the limited human evaluation sample size and the specific human demographic (US-based, native English speakers).

## Next Checks
1. **Generalization robustness**: Test the finetuned models against diverse opponent strategies (including both random and strategically sophisticated models) to verify performance improvements are not merely self-play artifacts.
2. **Strategic reasoning assessment**: Design a controlled evaluation suite with games requiring specific negotiation tactics (e.g., trade-off optimization, threat handling) to determine if models have learned genuine strategic capabilities beyond agreement-seeking behavior.
3. **Diversity preservation analysis**: Implement regularization techniques during self-play finetuning to maintain dialogue diversity while preserving performance gains, measuring both task success and linguistic variety across multiple competitive settings.