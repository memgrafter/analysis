---
ver: rpa2
title: Can Go AIs be adversarially robust?
arxiv_id: '2406.12843'
source_url: https://arxiv.org/abs/2406.12843
tags:
- training
- visits
- victim
- against
- adversary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We tested three defenses for superhuman Go AIs: adversarial training
  on hand-constructed positions, iterated adversarial training, and changing the network
  architecture to a vision transformer. None of the defenses prevented adversarial
  attacks: each defended agent was still defeated by newly trained adversaries using
  cyclic strategies.'
---

# Can Go AIs be adversarially robust?

## Quick Facts
- arXiv ID: 2406.12843
- Source URL: https://arxiv.org/abs/2406.12843
- Reference count: 40
- Primary result: Three defenses against adversarial Go AIs failed, with defended agents still vulnerable to cyclic attacks.

## Executive Summary
This paper investigates the adversarial robustness of superhuman Go AIs by testing three defense strategies against cyclic attacks: adversarial training on hand-constructed positions, iterated adversarial training, and replacing the CNN backbone with a vision transformer. Despite achieving strong average-case performance, none of the defenses prevented new adversaries from exploiting cyclic vulnerabilities. The results demonstrate that even in domains with superhuman performance and narrow attack surfaces, building robust AI systems remains challenging, highlighting the need for improved defense generalization and training diversity.

## Method Summary
The researchers evaluated three defense strategies against cyclic attacks on Go AIs. First, they implemented positional adversarial training by fine-tuning victims with hand-constructed cyclic positions. Second, they used iterated adversarial training, alternating between training victims and adversaries for nine iterations. Third, they replaced KataGo's CNN backbone with a vision transformer architecture. For each defense, they trained new adversaries against the defended models to test robustness, measuring win rates across different visit counts during MCTS search.

## Key Results
- All three defenses (positional adversarial training, iterated adversarial training, and ViT architecture) failed against newly trained adversaries
- Defended agents failed to represent cyclic shapes that amateur humans can easily learn
- Vision transformers did not eliminate cyclic vulnerabilities, suggesting the problem is fundamental to deep learning approaches
- Defenses showed poor generalization even in the favorable domain of Go with narrow attack surfaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simple cyclic attack patterns exploit fundamental blind spots in Go AI training rather than model architecture.
- Mechanism: The AI learns local shape evaluation algorithms that fail catastrophically on global cyclic dependencies, even after adversarial training.
- Core assumption: Self-play training with identical opponents creates local minima that prevent learning of cyclic shapes.
- Evidence anchors: [abstract] "Each defended system is still vulnerable to cyclic attacks, demonstrating the defenses' poor generalization even in the favorable domain of Go."

### Mechanism 2
- Claim: Adversarial training on hand-constructed positions provides insufficient diversity to generalize to new attack variants.
- Mechanism: Training data from fixed cyclic positions creates narrow defense that fails against slight variations of the same attack pattern.
- Core assumption: The space of cyclic attacks is larger than the specific positions used in training data.
- Evidence anchors: [abstract] "Though some of these defenses protect against previously discovered attacks, none withstand freshly trained adversaries."

### Mechanism 3
- Claim: Vision transformers do not inherently solve the cyclic vulnerability, suggesting the problem is fundamental to deep learning approaches rather than CNN-specific.
- Mechanism: Replacing CNN backbone with ViT maintains same vulnerability despite different inductive biases, indicating training methodology is the root cause.
- Core assumption: The cyclic vulnerability stems from training process rather than architectural limitations of CNNs.
- Evidence anchors: [abstract] "We replace KataGo’s convolutional neural network (CNN) backbone with a vision transformer (ViT) backbone to see which vulnerabilities of Go AIs are caused by the inductive biases of CNNs."

## Foundational Learning

- Concept: Monte Carlo Tree Search (MCTS) and neural network policy/value heads
  - Why needed here: Understanding how KataGo selects moves through search combined with neural network evaluation is crucial for grasping attack methodology.
  - Quick check question: How does the adversary use victim's neural network during MCTS traversal?

- Concept: Adversarial training and curriculum learning
  - Why needed here: The paper tests three defense strategies that all involve forms of adversarial training with different approaches to curriculum.
  - Quick check question: What's the difference between positional adversarial training and iterated adversarial training?

- Concept: Cyclic shapes and Go rules (superko, liberties, capture)
  - Why needed here: The entire attack methodology revolves around exploiting cyclic group vulnerabilities in Go rules.
  - Quick check question: How does a cyclic attack work and why is it difficult for AI to detect?

## Architecture Onboarding

- Component map: KataGo consists of MCTS search using a neural network with policy and value heads, trained through self-play. The defense experiments modify training data or architecture while keeping the core MCTS + neural network structure.
- Critical path: For new engineer - understand KataGo architecture → reproduce base attack → implement positional adversarial training → implement iterated adversarial training → implement ViT architecture.
- Design tradeoffs: ViT vs CNN - ViT potentially better at global patterns but slower and weaker in average case; adversarial training vs self-play - adversarial training targets specific vulnerabilities but may create narrow defenses.
- Failure signatures: If defenses appear to work but fail against new attacks, the failure is likely in generalization. If defenses fail even against original attacks, the failure is likely in implementation.
- First 3 experiments:
  1. Reproduce base-adversary attack against base-victim to verify environment setup.
  2. Implement positional adversarial training with hand-constructed cyclic positions and verify defense against base-adversary.
  3. Fine-tune base-adversary against the positionally adversarially trained model to test generalization failure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we develop new attack algorithms that require significantly less compute to train diverse adversarial strategies against superhuman Go AIs?
- Basis in paper: [explicit] The paper discusses the need for "developing new attack algorithms that require less compute to train a variety of adversaries" as a potential approach to increase the size of the attack corpus.
- Why unresolved: Current attack algorithms, while effective, still require substantial compute resources to train diverse strategies that can defeat defended Go AIs. The paper suggests this is a bottleneck to achieving full robustness.
- What evidence would resolve it: Demonstrating a new attack algorithm that achieves comparable or better attack success rates against defended Go AIs while using significantly less compute than existing methods.

### Open Question 2
- Question: How can we improve the sample efficiency of adversarial training so that defended Go AIs can generalize from a limited number of adversarial strategies?
- Basis in paper: [explicit] The paper identifies that existing algorithms for training Go AIs "do not generalize in this way" and that "may24-victim and v9 remain vulnerable to cyclic attacks even after training against many variants of cyclic attacks."
- Why unresolved: Despite training against multiple variants of cyclic attacks, the defended Go AIs still fail to represent the underlying concept of cyclic shapes that amateur humans easily learn. This suggests poor generalization of defenses.
- What evidence would resolve it: Successfully training a Go AI that remains robust against previously unseen cyclic attack variants after being trained on only a small number of cyclic attack examples.

### Open Question 3
- Question: Would alternative multi-agent reinforcement learning schemes, such as PSRO or DeepNash, automatically discover and eliminate vulnerabilities like cyclic attacks during training?
- Basis in paper: [explicit] The paper suggests "multi-agent reinforcement learning schemes like PSRO (Lanctot et al. 2017) or DeepNash (Perolat et al. 2022) may be able to automatically discover strategies like the cyclic attack and train them away" as a potential route to robustness besides adversarial training.
- Why unresolved: The paper does not explore these alternative training methods and their potential to discover and eliminate cyclic vulnerabilities automatically during training.
- What evidence would resolve it: Training a Go AI using PSRO or DeepNash and demonstrating that it naturally discovers and eliminates cyclic vulnerabilities without explicit adversarial training.

## Limitations
- The findings may not generalize to other types of adversarial vulnerabilities in Go AIs or to different board games with similar rulesets.
- The paper's conclusions are limited by the narrow scope of cyclic attacks tested.
- The lack of exploration of alternative defense strategies beyond the three approaches examined restricts the breadth of conclusions.

## Confidence
- High confidence: The core finding that all three tested defenses fail against newly trained adversaries using cyclic strategies is well-supported by empirical results across multiple attack types.
- Medium confidence: The claim that cyclic vulnerabilities represent fundamental blind spots in Go AI training rather than model architecture, as the ViT experiments provide suggestive but not definitive evidence.
- Low confidence: The broader claim that "building robust AI systems is challenging even in domains with superhuman average-case performance" extends beyond the specific evidence from Go AIs to general AI systems.

## Next Checks
1. Test the same defense strategies against non-cyclic adversarial attack patterns to determine if the failure is specific to cyclic vulnerabilities or represents a more general robustness issue.
2. Evaluate whether incorporating explicit cyclic shape recognition into the training curriculum through data augmentation can improve defense generalization.
3. Investigate whether increasing the diversity of adversarial training data through automated generation of cyclic variations can prevent adversaries from finding new winning strategies.