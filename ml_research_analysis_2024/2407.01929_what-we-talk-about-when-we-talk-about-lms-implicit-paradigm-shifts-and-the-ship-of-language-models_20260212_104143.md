---
ver: rpa2
title: 'What We Talk About When We Talk About LMs: Implicit Paradigm Shifts and the
  Ship of Language Models'
arxiv_id: '2407.01929'
source_url: https://arxiv.org/abs/2407.01929
tags:
- language
- more
- papers
- ship
- most
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the term "Language Models" has evolved
  in NLP research by analyzing 7,650 papers from major conferences (2020-2023). The
  authors developed a semi-automatic framework to extract mentions of the general
  "language model" concept and specific model names, constructing a dataset of 103
  models and 155 aliases.
---

# What We Talk About When We Talk About LMs: Implicit Paradigm Shifts and the Ship of Language Models

## Quick Facts
- arXiv ID: 2407.01929
- Source URL: https://arxiv.org/abs/2407.01929
- Authors: Shengqi Zhu; Jeffrey M. Rzeszotarski
- Reference count: 23
- Primary result: Analysis of 7,650 NLP papers shows dramatic shifts in how "Language Models" is conceptualized, with BERT dominating in 2020 (41% mentions) but overtaken by GPT models (30%) by 2023

## Executive Summary
This paper investigates how the term "Language Models" has evolved in NLP research by analyzing 7,650 papers from major conferences (2020-2023). The authors developed a semi-automatic framework to extract mentions of the general "language model" concept and specific model names, constructing a dataset of 103 models and 155 aliases. They found that while LM-related papers increased from 35% to 84% of all papers, the actual number of model mentions per paper remained stable until 2022, when it began growing exponentially. The composition of referenced models changed dramatically - BERT dominated in 2020 (41% of mentions) but by 2023 was overtaken by GPT models (30%), with Jaccard similarity between conference compositions dropping from 86% to 24% over three years.

## Method Summary
The authors analyzed 7,650 papers from ACL, EMNLP, and NAACL conferences (2020-2023) using a semi-automatic framework. They extracted mentions of general "language model" terms (LM, LLM, PLM) and specific model names, validated through manual checking. The study tracked changes in LM mentions per paper, model composition across conferences, and temporal patterns in terminology usage. Jaccard similarity was used to measure composition changes, and sunburst charts visualized model family distributions.

## Key Results
- LM-related papers increased from 35% to 84% of all papers between 2020-2023
- Average mentions of language models per paper remained stable until 2022, then grew exponentially
- BERT dominated in 2020 (41% of mentions) but was overtaken by GPT models (30%) by 2023
- Jaccard similarity between conference model compositions dropped from 86% to 24% over three years
- Papers most focused on LMs showed the greatest shift toward newer models, while less LM-centered papers maintained longer-term preferences for earlier models like BERT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Ship of LMs framework captures implicit paradigm shifts through analyzing changes in language model references over time
- Mechanism: By tracking how the term "Language Models" is used in papers and which specific models are mentioned, researchers can quantify how the field's collective understanding evolves
- Core assumption: The referents of "Language Models" change over time while the term itself remains constant
- Evidence anchors: Abstract states "The term Language Models (LMs) as a time-specific collection of models of interest is constantly reinvented"

### Mechanism 2
- Claim: Exponential growth in LM-related papers since 2022 reflects a fundamental shift in how the field conceptualizes language models
- Mechanism: The dramatic increase in papers mentioning LMs (from 35% to 84% of all papers) combined with exponential growth in average mentions per paper indicates the field is undergoing rapid conceptual transformation
- Core assumption: The increase in LM mentions reflects changing conceptual frameworks rather than just increased volume of research
- Evidence anchors: Section notes "¯NL has since been on an exponential growth, eventually being 80% higher than estimated at ACL 2023"

### Mechanism 3
- Claim: The dominance of specific models (BERT → GPT) in academic discourse shapes the field's collective understanding of what "Language Models" means
- Mechanism: As certain model families dominate citations and mentions, they become the implicit reference point for the entire concept of language models
- Core assumption: Academic discourse shapes collective understanding through repeated references to specific models
- Evidence anchors: Section shows "BERT dominated in 2020 (41% of mentions) but by 2023 was overtaken by GPT models (30%)"

## Foundational Learning

- Concept: Diachronic analysis of scientific terminology
  - Why needed here: Understanding how terms evolve over time is crucial for tracking paradigm shifts
  - Quick check question: Can you explain how the meaning of "Language Models" has changed from 2020 to 2023?

- Concept: Corpus linguistics and text mining
  - Why needed here: Extracting and analyzing model mentions from academic papers requires text processing skills
  - Quick check question: How would you extract all mentions of "BERT" from a large corpus of NLP papers?

- Concept: Jaccard similarity for composition comparison
  - Why needed here: Measuring how much the set of referenced models changes over time requires appropriate similarity metrics
  - Quick check question: If Conference A references models {BERT, GPT-2, RoBERTa} and Conference B references {GPT-3, ChatGPT, LLaMA}, what is their Jaccard similarity?

## Architecture Onboarding

- Component map: ACL Anthology → PDF extraction → text processing → keyword extraction → model validation → composition analysis → visualization
- Critical path: Paper collection → text extraction → keyword detection → model name validation → composition analysis → visualization
- Design tradeoffs: Manual validation ensures accuracy but limits scalability; automated detection increases coverage but may miss edge cases
- Failure signatures: Incomplete model dictionaries, inconsistent text extraction, skewed temporal patterns due to venue selection
- First 3 experiments:
  1. Verify text extraction works consistently across different PDF formats
  2. Test model name detection on a small sample of papers and validate results
  3. Compare LM mention counts between conferences to establish baseline patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors drove the exponential increase in LM-related papers after 2022?
- Basis in paper: The paper notes a super-linear growth in N_L after 2022 but doesn't identify specific causes beyond general observations about faster conclusions and potentially stronger evidence in writing.
- Why unresolved: The authors acknowledge this requires deeper understanding of local context and suggest qualitative studies would be especially relevant.
- What evidence would resolve it: Comparative analysis of paper content pre/post-2022 focusing on methodology changes, citation patterns, and detailed content analysis of LM-related papers.

### Open Question 2
- Question: How do the observed patterns of LM usage differ across subfields of NLP?
- Basis in paper: The paper notes limitations about using major conference papers as a proxy for the entire NLP community and suggests regional conferences on dialect or indigenous language might show different patterns.
- Why unresolved: The current dataset only includes papers from three major conferences (ACL, EMNLP, NAACL) and doesn't explore variations across different NLP subfields.
- What evidence would resolve it: Comparative analysis of LM usage patterns across different NLP subfields and conference types, including specialized and regional conferences.

### Open Question 3
- Question: What is the relationship between the evolution of LM terminology and actual model capabilities?
- Basis in paper: The paper discusses how the referents of "Language Models" have changed over time but doesn't directly examine how this relates to actual model capabilities and performance improvements.
- Why unresolved: The authors focus on textual analysis of terminology usage rather than correlating this with technical developments in model capabilities.
- What evidence would resolve it: Analysis correlating changes in LM terminology usage with objective measures of model capabilities, benchmark performance, and technical innovations over time.

## Limitations
- The analysis relies primarily on conference proceedings, potentially missing important trends in journals or workshops
- The exponential growth patterns could be partially influenced by selection bias in venue choice
- The analysis cannot distinguish between genuine conceptual shifts versus increased awareness of existing models

## Confidence
- **High confidence**: The observed growth in LM-related papers (35% to 84%) and the shift in model dominance (BERT to GPT) are well-supported by the data
- **Medium confidence**: The claim that "papers most focused on LMs showed the greatest shift toward newer models" requires further validation across different paper types
- **Medium confidence**: The interpretation of exponential growth as evidence of conceptual transformation could reflect other factors like publication incentives or funding priorities

## Next Checks
1. Validate model name detection accuracy by randomly sampling 100 papers and comparing automated extraction with manual annotation
2. Replicate the analysis using journal publications from the same time period to assess venue-specific biases
3. Conduct a qualitative analysis of paper abstracts to verify that increased LM mentions correspond to genuine conceptual discussions rather than mere citation practices