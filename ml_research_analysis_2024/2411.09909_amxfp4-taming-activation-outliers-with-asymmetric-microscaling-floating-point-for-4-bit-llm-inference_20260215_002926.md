---
ver: rpa2
title: 'AMXFP4: Taming Activation Outliers with Asymmetric Microscaling Floating-Point
  for 4-bit LLM Inference'
arxiv_id: '2411.09909'
source_url: https://arxiv.org/abs/2411.09909
tags:
- data
- e-04
- format
- quantization
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses accuracy degradation in 4-bit large language
  model inference caused by activation outliers. The authors analyze how microscaling
  effectively suppresses outliers but introduces asymmetry in group-wise quantized
  data.
---

# AMXFP4: Taming Activation Outliers with Asymmetric Microscaling Floating-Point for 4-bit LLM Inference

## Quick Facts
- arXiv ID: 2411.09909
- Source URL: https://arxiv.org/abs/2411.09909
- Reference count: 40
- Primary result: AMXFP4 outperforms MXFP4 by 3% on VQA, exceeds rotation-based methods by 1.6% on CSQA, and achieves up to 0.6 perplexity reduction on language modeling tasks, while adding only 10% hardware overhead.

## Executive Summary
AMXFP4 addresses accuracy degradation in 4-bit large language model inference caused by activation outliers. The authors analyze how microscaling effectively suppresses outliers but introduces asymmetry in group-wise quantized data. They propose AMXFP4, an asymmetric microscaling floating-point format that uses asymmetric shared scales to mitigate both outliers and asymmetry, enabling direct 4-bit casting without calibration. The method achieves significant accuracy improvements across various LLM benchmarks while maintaining minimal hardware overhead.

## Method Summary
The authors propose AMXFP4, an asymmetric microscaling floating-point format designed to handle activation outliers in 4-bit LLM inference. Unlike traditional MXFP4 which uses symmetric shared scales, AMXFP4 employs asymmetric scales (separate scales for positive and negative values) and FP8 representation for the shared scale. This approach effectively mitigates both outlier-induced and asymmetry-induced quantization errors without requiring calibration. The method groups activation elements (typically 32 per group) and applies shared asymmetric scaling, enabling fully quantized matrix multiplications through direct casting.

## Key Results
- AMXFP4 outperforms MXFP4 by 3% on VQA tasks
- AMXFP4 exceeds rotation-based methods by 1.6% on CSQA
- Achieves up to 0.6 perplexity reduction on language modeling tasks
- Adds only 10% hardware overhead compared to baseline MXFP4

## Why This Works (Mechanism)

### Mechanism 1
- Microscaling groups reduce activation outlier impact by lowering kurtosis within each group. Reducing group size from full-row to 32 elements diminishes the statistical outlier presence, making the dynamic range within each group more suitable for quantization.

### Mechanism 2
- Microscaling increases group-wise asymmetry, which degrades quantization accuracy when using symmetric data formats. As group size decreases, the mean of activation values within each group scatters away from zero, violating the assumption of symmetric distribution required by MXFP4's symmetric shared scale.

### Mechanism 3
- Asymmetric microscaling with FP8 shared scales mitigates both outliers and asymmetry without requiring calibration. AMXFP4 uses asymmetric shared scales (separate scales for positive and negative values) and FP8 representation for the shared scale, which provides better dynamic range and precision than PoT.

## Foundational Learning

- **Floating-point representation (sign, exponent, mantissa)**: Understanding how MX and AMX formats encode numbers is critical for reasoning about dynamic range and precision trade-offs.
  - Quick check: In a 4-bit floating-point format with 1-bit sign, 2-bit exponent, and 1-bit mantissa, what is the largest positive number representable?

- **Kurtosis as a measure of outlier presence**: The paper uses kurtosis to quantify how much activation values deviate from a normal distribution, which affects quantization error.
  - Quick check: If a distribution has high kurtosis, what does that imply about its tail behavior compared to a normal distribution?

- **Group-wise quantization and shared scaling**: MX and AMX formats rely on grouping elements and sharing a scale across the group to reduce per-element storage.
  - Quick check: In group-wise quantization, why might a shared scale across 32 elements be more efficient than individual scales per element?

## Architecture Onboarding

- **Component map**: Quantizer (MX/AMX) -> MAC unit -> Accumulator -> Output
- **Critical path**: Activation → Quantizer (MX/AMX) → MAC unit → Accumulator → Output
- **Design tradeoffs**:
  - Group size vs. outlier suppression: Smaller groups reduce outliers but increase scale management overhead
  - Symmetric vs. asymmetric scales: Symmetric scales are simpler but less accurate for asymmetric data; asymmetric scales improve accuracy but add hardware complexity
  - PoT vs. FP8 shared scales: PoT is hardware-efficient but may clamp large values; FP8 provides better dynamic range at the cost of extra multiplications
- **Failure signatures**:
  - High perplexity or accuracy drop indicates quantization error from either outlier or asymmetry issues
  - Calibration-dependent methods failing on new data distributions suggest overfitting to calibration set
  - Hardware inefficiency if group size is too small or asymmetric scaling adds excessive overhead
- **First 3 experiments**:
  1. Compare MXFP4 vs. AMXFP4 perplexity on Wikitext-2 with varying group sizes (128, 32) to validate asymmetry handling
  2. Measure calibration time and accuracy sensitivity for QuaRot vs. AMXFP4 on ARC-Challenge to demonstrate calibration-free advantage
  3. Evaluate MT-Bench scores for MXFP4-PoT vs. AMXFP4-FP8 to assess real-world conversational quality improvement

## Open Questions the Paper Calls Out

- **How does the AMXFP4 format's performance scale with model size beyond the 405B parameter context mentioned in the introduction?**: The paper mentions "Scaling Large Language Models (LLMs) with extended context lengths" but only validates on smaller models up to 13B parameters, despite mentioning 405B in the introduction.

- **What is the impact of AMXFP4 on training-time memory consumption and compute requirements, not just inference?**: The paper focuses exclusively on inference efficiency and 4-bit quantization benefits, but doesn't address whether AMXFP4 could be used during training.

- **How does AMXFP4 perform on specialized domains like scientific computing, code generation, or mathematical reasoning compared to general language tasks?**: While the paper mentions performance across "diverse LLM benchmarks" and shows STEM scores, it lacks granular analysis of how AMXFP4 affects specialized domain accuracy.

- **What are the long-term implications of asymmetric microscaling on model calibration and fine-tuning stability?**: The paper emphasizes calibration-free inference but doesn't explore how AMXFP4 might affect subsequent fine-tuning or model stability over time.

## Limitations

- **Hardware cost validation**: The detailed breakdown of the claimed 10% hardware overhead is not provided, making it difficult to assess realism across different hardware platforms.
- **Calibration-free assertion**: The claim that AMXFP4 does not require calibration lacks extensive cross-model validation across diverse architectures and activation distributions.
- **Generalization to other modalities**: The evaluation focuses on text-based tasks, with effectiveness for multimodal models (e.g., those with vision or audio inputs) remaining unexplored.

## Confidence

- **High confidence**: The mechanism of reducing kurtosis through microscaling is well-supported by empirical evidence and aligns with statistical principles of outlier suppression.
- **Medium confidence**: The claim that AMXFP4 achieves calibration-free quantization is supported by the proposed asymmetric scale design, but lacks extensive cross-model validation.
- **Low confidence**: The 10% hardware overhead estimate is not fully substantiated with detailed hardware implementation or synthesis results.

## Next Checks

1. **Hardware overhead validation**: Implement AMXFP4 in RTL and synthesize it on a target ASIC/FPGA platform to measure actual area, power, and latency overhead compared to MXFP4.
2. **Cross-model robustness test**: Evaluate AMXFP4 on a diverse set of LLM architectures (e.g., OPT, BLOOM, GPT-NeoX) and tasks (e.g., summarization, code generation) to confirm calibration-free quantization across varying activation distributions.
3. **Multimodal extension**: Apply AMXFP4 to a multimodal LLM (e.g., Flamingo or PaLI) and measure accuracy degradation in vision-language tasks to assess generalizability beyond text-only models.