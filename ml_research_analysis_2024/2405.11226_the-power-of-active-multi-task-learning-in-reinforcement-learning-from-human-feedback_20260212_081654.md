---
ver: rpa2
title: The Power of Active Multi-Task Learning in Reinforcement Learning from Human
  Feedback
arxiv_id: '2405.11226'
source_url: https://arxiv.org/abs/2405.11226
tags:
- task
- learning
- lemma
- sample
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies reinforcement learning from human feedback
  (RLHF) by formulating it as a contextual dueling bandit problem with a shared linear
  representation across tasks. The key idea is to exploit task relevance to reduce
  sample complexity in multi-task RLHF: source tasks more relevant to the target task
  are allocated more samples.'
---

# The Power of Active Multi-Task Learning in Reinforcement Learning from Human Feedback

## Quick Facts
- arXiv ID: 2405.11226
- Source URL: https://arxiv.org/abs/2405.11226
- Authors: Ruitao Chen; Liwei Wang
- Reference count: 40
- Key outcome: Achieves $\tilde{O}(\|\nu\|_1^2 k(d+M)/\varepsilon^2)$ sample complexity for multi-task RLHF with known task relevance

## Executive Summary
This paper introduces an active multi-task learning approach for reinforcement learning from human feedback (RLHF) that exploits task relevance to reduce sample complexity. The authors formulate RLHF as a contextual dueling bandit problem with shared linear representations across tasks. By strategically allocating more samples to source tasks that are more relevant to the target task, the algorithm achieves significant sample complexity improvements over uniform sampling baselines. The approach works both when task relevance is known and when it must be estimated from data.

## Method Summary
The method formulates multi-task RLHF as a contextual dueling bandit problem where tasks share a common low-rank linear representation. When task relevance is known, samples are allocated proportionally to each source task's relevance to the target task, and policies are learned via regularized maximum likelihood estimation. When task relevance is unknown, an additional estimation step uses Lasso regression to recover the relevance coefficients. The algorithm leverages the shared representation structure to pool information across tasks while focusing sampling effort on the most informative source tasks for the target task.

## Key Results
- Achieves sample complexity of $\tilde{O}(\|\nu\|_1^2 k(d+M)/\varepsilon^2)$ for source tasks with known relevance, linear in k for target task
- When task relevance is unknown, adds estimation step with sample complexity independent of ε
- Demonstrates significant sample complexity reduction compared to uniform sampling baselines
- Theoretical analysis shows sample complexity scales with the rank of the task relevance structure

## Why This Works (Mechanism)
The algorithm exploits the low-rank structure of shared linear representations across tasks. By identifying which source tasks are most relevant to the target task, it can allocate samples more efficiently rather than treating all tasks equally. This relevance-aware sampling reduces the number of samples needed to learn high-quality policies for the target task.

## Foundational Learning
- **Contextual dueling bandits**: Framework for learning from pairwise comparisons rather than absolute rewards; needed for modeling human preference feedback
- **Shared linear representations**: Assumption that tasks share common feature representations; enables information transfer across tasks
- **Task relevance coefficients**: Measures of how informative each source task is for learning the target task; determines optimal sample allocation
- **Lasso regression**: Technique for sparse estimation of task relevance when unknown; provides sample complexity guarantees
- **Regularized maximum likelihood estimation**: Method for learning policies from bandit feedback; balances fit and generalization
- **Low-rank structure**: Mathematical property enabling sample complexity reduction; captures task relationships

## Architecture Onboarding

Component map: Task relevance estimation -> Sample allocation -> Policy learning -> Evaluation

Critical path: The algorithm's performance critically depends on accurate task relevance estimation, as poor estimates lead to suboptimal sample allocation and degraded learning efficiency.

Design tradeoffs: Balances between exploration (learning task relevance) and exploitation (focusing on relevant tasks). The method trades increased algorithmic complexity for sample complexity reduction.

Failure signatures: Poor performance when task relevance estimates are inaccurate, when the shared linear representation assumption is violated, or when the number of tasks is too large relative to available samples.

First experiments:
1. Validate theoretical sample complexity bounds on synthetic data with known task relevance structure
2. Test robustness to relevance estimation errors by intentionally corrupting the relevance matrix
3. Compare against uniform sampling baselines on realistic RLHF benchmarks with ground truth relevance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is the algorithm's performance to the choice of the prior task relevance ν₀ when it is unknown?
- Basis in paper: [explicit] The paper states "Without loss of generality, we assume access to a prior, ν₀ = {ν₀ₘ}ᴹₘ₌₁ ∈ ℝᴹ satisfying ∥ν₀∥₁ = 1, for the ν* defined in (3.9)." It also mentions that if no information is available, setting each element of ν₀ equal to 1/M represents a non-informative prior.
- Why unresolved: The paper does not provide empirical results or theoretical analysis on how the choice of ν₀ affects the sample complexity or estimation accuracy of ν*.
- What evidence would resolve it: Experiments comparing the algorithm's performance with different choices of ν₀ (e.g., uniform prior vs. informed prior) on synthetic or real-world datasets.

### Open Question 2
- Question: Can the theoretical bounds be improved when the task relevance coefficients ν are sparse (i.e., only a few source tasks are highly relevant to the target task)?
- Basis in paper: [inferred] The current theoretical analysis does not explicitly leverage sparsity in the task relevance coefficients. The sample complexity depends on ∥ν∥₁, which can be large even if only a few tasks are relevant.
- Why unresolved: The proof techniques used in the paper (e.g., Cauchy-Schwarz inequality) do not inherently exploit sparsity in ν.
- What evidence would resolve it: A modified algorithm and theoretical analysis that explicitly incorporates sparsity assumptions on ν, potentially using techniques from sparse recovery literature.

### Open Question 3
- Question: How does the algorithm scale with the number of tasks M in practice, especially when M is very large?
- Basis in paper: [explicit] The sample complexity includes terms like k(d + M), indicating that the complexity grows linearly with M. The paper also mentions that when samples are drawn uniformly from each task, the sample complexity should be much larger.
- Why unresolved: While the theoretical analysis shows linear dependence on M, it does not provide practical insights into how the algorithm performs when M is extremely large (e.g., thousands of tasks).
- What evidence would resolve it: Empirical studies evaluating the algorithm's runtime and memory usage as M increases, along with potential optimizations for handling large-scale multi-task learning scenarios.

## Limitations
- Assumes linear reward representations with shared structure across tasks, which may not hold in complex real-world RLHF scenarios
- Algorithm's performance is sensitive to accuracy of task relevance estimates, with potential failure modes from incorrect allocation
- Contextual dueling bandit formulation may oversimplify the rich feedback structure available in typical RLHF settings
- Theoretical framework doesn't adequately address robustness to relevance estimation errors

## Confidence
High confidence: Theoretical sample complexity bounds and core algorithmic framework for known relevance scenarios
Medium confidence: Lasso-based relevance estimation approach and associated sample complexity guarantees
Low confidence: Empirical validation section (appears absent), making practical utility difficult to assess

## Next Checks
1. Implement synthetic experiments varying the rank and structure of the task relevance matrix to empirically validate theoretical sample complexity claims across different regimes of task relatedness
2. Design stress tests where relevance estimation is intentionally corrupted to quantify algorithm's robustness to estimation errors and identify failure modes in sample allocation
3. Compare performance against uniform sampling baselines on realistic RLHF benchmarks where ground truth relevance is known, measuring both sample efficiency and final policy quality