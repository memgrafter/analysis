---
ver: rpa2
title: 'CamelEval: Advancing Culturally Aligned Arabic Language Models and Benchmarks'
arxiv_id: '2409.12623'
source_url: https://arxiv.org/abs/2409.12623
tags:
- data
- char09
- char10
- arabic
- char2e
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Juhaina, a 9.24 billion parameter Arabic-English
  bilingual LLM designed to align with Arabic cultural values and linguistic norms.
  The model was post-trained on Gemma 2 using a combination of supervised fine-tuning
  and human preference alignment through curated Arabic datasets.
---

# CamelEval: Advancing Culturally Aligned Arabic Language Models and Benchmarks

## Quick Facts
- arXiv ID: 2409.12623
- Source URL: https://arxiv.org/abs/2409.12623
- Authors: Zhaozhi Qian; Faroq Altam; Muhammad Alqurishi; Riad Souissi
- Reference count: 40
- Juhaina achieved second place on Open Arabic LLM Leaderboard with 60.18 average score

## Executive Summary
This paper presents Juhaina, a 9.24 billion parameter Arabic-English bilingual LLM designed to align with Arabic cultural values and linguistic norms. The model was post-trained on Gemma 2 using a combination of supervised fine-tuning and human preference alignment through curated Arabic datasets. To address limitations in existing Arabic LLM benchmarks, the authors introduced CamelEval, a new evaluation framework using the llm-as-a-judge approach with both translated and culturally curated prompts. Juhaina achieved strong performance, ranking second on the Open Arabic LLM Leaderboard and demonstrating competitive win rates against much larger models on CamelEval.

## Method Summary
Juhaina was developed through a two-stage post-training process starting from Gemma 2. First, supervised fine-tuning (SFT) was performed using Llama Pro with block expansion, training approximately 600k human-annotated Arabic prompts. Second, alignment was achieved using ORPO (Monolithic Odds Ratio Preference Optimization) trained on 40k preference pairs collected from native Arabic speakers. The CamelEval benchmark was created using translated open-source prompts and curated culturally-specific prompts, evaluated through an LLM-as-a-judge framework comparing model outputs pairwise.

## Key Results
- Juhaina ranked second on Open Arabic LLM Leaderboard with 60.18 average score
- Achieved 98.47% win rate against Gemma2-9B-IT on CamelEval curated prompts
- Demonstrated cultural alignment capabilities while maintaining English proficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Juhaina achieves strong cultural alignment by post-training on curated Arabic datasets with native annotator feedback rather than relying on machine-translated corpora.
- Mechanism: Human-curated, high-quality Arabic data is used in both supervised fine-tuning and preference alignment stages, ensuring responses reflect regional norms, accurate facts, and nuanced cultural contexts. The use of ORPO without a reference model preserves memory efficiency while integrating SFT loss to stabilize training.
- Core assumption: Native Arabic speakers can effectively curate prompts and preferences that capture cultural nuances better than automated translation or synthetic generation alone.
- Evidence anchors:
  - [abstract] "This paper introduces Juhaina, a Arabic-English bilingual LLM specifically designed to align with the values and preferences of Arabic speakers."
  - [section] "For the alignment data, the annotators provided preference feedback on pairs of answers generated by the LLM."
  - [corpus] Weak: No direct corpus evidence provided for ORPO stability or preference data quality; inferred from methodology description.
- Break condition: If native annotators introduce their own cultural biases or if the preference data is insufficiently diverse, the model may overfit to a narrow cultural perspective.

### Mechanism 2
- Claim: CamelEval's LLM-as-a-judge framework offers more nuanced evaluation of conversational and instruction-following abilities than traditional multiple-choice benchmarks like OALL.
- Mechanism: Two competing models generate responses to the same prompts, and a third judge model evaluates win rates, capturing helpfulness, coherence, and cultural relevance in a more holistic way than NLL or accuracy metrics.
- Core assumption: LLM judges can reliably differentiate between culturally aligned and generic responses, and their pairwise comparisons are less affected by response length or format variance than absolute scoring.
- Evidence anchors:
  - [abstract] "CamelEval is a new evaluation benchmark...specifically designed to assess their conversational abilities and instruction-following proficiency within Arabic contexts."
  - [section] "CamelEval enables a better coverage of LLM capabilities, especially the ability to generate helpful conversations and following user instruction."
  - [corpus] Weak: No direct corpus evidence for judge reliability; inferred from AlpacaEval methodology adoption.
- Break condition: If the judge model itself is biased toward its own outputs or toward certain response styles, evaluation results may be skewed.

### Mechanism 3
- Claim: Llama Pro's block expansion technique enables Juhaina to gain Arabic language capabilities without losing English proficiency by training only newly added parameters.
- Mechanism: During SFT, block expansion introduces extra free parameters; only these are trained, allowing the model to specialize in Arabic while preserving the base Gemma 2 English capabilities.
- Core assumption: Freezing most parameters and training only the expanded blocks is sufficient to acquire new linguistic skills without catastrophic forgetting.
- Evidence anchors:
  - [section] "Llama Pro performs block expansion on the base model to introduce additional free parameters. During the training stage, only these newly added parameters are trained."
  - [section] "Our experiments indicate that, compared to standard SFT, Llama Pro enables the LLM to acquire additional capabilities in the Arabic language without compromising its existing capabilities in English."
  - [corpus] Weak: No corpus evidence; relies on experimental claim.
- Break condition: If the expanded parameter space is insufficient for Arabic complexity, or if interference occurs, model performance in either language may degrade.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT) with human-annotated responses
  - Why needed here: Juhaina needs to learn to generate coherent, contextually appropriate Arabic text rather than just memorizing translated data.
  - Quick check question: Why is it important to use human-generated responses instead of machine-generated ones during SFT?

- Concept: Preference Alignment via Human Feedback
  - Why needed here: To ensure the model's responses are not only correct but also culturally respectful and stylistically aligned with Arabic-speaking users' expectations.
  - Quick check question: How does pairwise preference feedback help shape model behavior differently than supervised labels?

- Concept: LLM-as-a-Judge Evaluation Framework
  - Why needed here: Traditional metrics like NLL fail to capture conversational quality and cultural appropriateness, which are critical for real-world Arabic AI assistants.
  - Quick check question: What is a key limitation of using normalized log-likelihood for evaluating instruction-following models?

## Architecture Onboarding

- Component map:
  - Gemma 2 base model -> Llama Pro SFT with block expansion -> ORPO alignment with human preferences -> CamelEval evaluation

- Critical path:
  1. Data curation → 2. SFT fine-tuning → 3. Preference alignment → 4. Evaluation on CamelEval

- Design tradeoffs:
  - Freezing most parameters during SFT preserves English skills but may limit Arabic adaptation depth
  - Using LLM judges speeds evaluation but introduces judge-specific biases
  - Curated prompt generation ensures cultural relevance but is labor-intensive

- Failure signatures:
  - Low win rates on CamelEval curated set → misalignment with Arabic cultural norms
  - High variance in judge evaluations → unreliable assessment
  - Performance drop on English tasks → over-specialization to Arabic

- First 3 experiments:
  1. Compare win rates of Juhaina vs Gemma2-9B-IT on CamelEval curated prompts to confirm cultural alignment gains.
  2. Run SFT with full-parameter tuning vs Llama Pro block expansion to quantify preservation of English capabilities.
  3. Test CamelEval judge reliability by swapping judge models and measuring score variance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which Llama Pro's block expansion contributes to better Arabic language acquisition compared to standard SFT?
- Basis in paper: [explicit] The paper mentions that Llama Pro enables the LLM to acquire additional capabilities in Arabic without compromising English capabilities, but does not provide detailed explanation of the mechanism.
- Why unresolved: The paper only states that Llama Pro was used and performed better, but doesn't explain why or how the block expansion specifically helps with Arabic language acquisition.
- What evidence would resolve it: Detailed ablation studies comparing standard SFT vs Llama Pro with various block sizes, or analysis of parameter updates showing differences in Arabic vs English token processing.

### Open Question 2
- Question: How does the performance of Juhaina on CamelEval translate to real-world user satisfaction in practical applications?
- Basis in paper: [inferred] The paper extensively discusses benchmark performance but doesn't connect this to actual user experience or practical deployment scenarios.
- Why unresolved: The paper focuses on automated evaluation metrics but doesn't provide evidence of how these translate to human user satisfaction or real-world utility.
- What evidence would resolve it: User studies or field tests with actual Arabic speakers using Juhaina for practical tasks, measuring satisfaction, task completion rates, and perceived cultural alignment.

### Open Question 3
- Question: What specific biases or limitations exist in CamelEval's curated prompts that could affect its evaluation of cultural alignment?
- Basis in paper: [explicit] The paper acknowledges that "the typical constraints associated with using LLMs as evaluators also apply to CamelEval" and mentions potential judge bias, but doesn't specify limitations of the curated prompts themselves.
- Why unresolved: While the paper discusses general limitations of LLM-as-a-judge approaches, it doesn't examine potential biases in the prompt creation process or coverage gaps in cultural topics.
- What evidence would resolve it: Analysis of prompt distribution across different Arabic cultural contexts, examination of prompt diversity across different dialects and regions, and comparison with other cultural alignment benchmarks.

## Limitations
- Lack of publicly available curated datasets used for Juhaina's training and CamelEval's evaluation sets limits independent validation
- LLM-as-a-judge methodology introduces potential judge-specific biases that could skew evaluation results
- ORPO implementation details and parameter-efficient tuning effects remain underspecified

## Confidence
**High Confidence**: The core methodology of using human-annotated Arabic datasets for cultural alignment is well-established in prior work on culturally aware LLMs.

**Medium Confidence**: The specific claim that ORPO with SFT integration provides stable training without reference model degradation requires empirical validation.

**Low Confidence**: The evaluation of cultural alignment through win rates against Gemma2-9B-IT assumes judge reliability without providing calibration data.

## Next Checks
1. Run CamelEval with multiple judge models (including human annotators where possible) to measure inter-judge agreement and identify potential biases in cultural assessment.
2. Perform comprehensive overlap analysis between Juhaina's training data and CamelEval test sets to confirm the claimed <1% contamination rate and validate cultural alignment improvements.
3. Evaluate Juhaina's performance on non-Arabic cultural prompts to determine if the alignment training introduces cultural bias that limits the model's ability to handle diverse contexts appropriately.