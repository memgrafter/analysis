---
ver: rpa2
title: Measuring Fingerprints of Web-filtered Text Datasets and Fingerprint Propagation
  Through Training
arxiv_id: '2412.02857'
source_url: https://arxiv.org/abs/2412.02857
tags:
- datasets
- data
- sequences
- dataset
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors study dataset classification for pretraining datasets
  of large language models (LLMs). Using a classifier based on a 160M parameter autoregressive
  transformer, they find that sequences from different pretraining datasets, including
  C4, RefinedWeb, and FineWeb, can be accurately classified, even when the datasets
  are derived from similar sources and curation steps.
---

# Measuring Fingerprints of Web-filtered Text Datasets and Fingerprint Propagation Through Training

## Quick Facts
- arXiv ID: 2412.02857
- Source URL: https://arxiv.org/abs/2412.02857
- Reference count: 40
- Primary result: Pretraining datasets from similar sources can be accurately classified (74.8-91.2% accuracy) based on subtle formatting, vocabulary, and content differences

## Executive Summary
This paper investigates dataset classification for large language model pretraining datasets, revealing that even datasets derived from similar sources can be accurately distinguished based on subtle differences in their curation pipelines. Using a 160M parameter autoregressive transformer classifier, the authors demonstrate that datasets like C4, RefinedWeb, and FineWeb exhibit unique statistical fingerprints that persist through model training. These fingerprints propagate to generated text, enabling estimation of pretraining mixture proportions. The findings suggest that small differences in dataset processing steps induce detectable patterns that can inform about pretraining mixtures and potential data sources.

## Method Summary
The authors collect seven open-source pretraining datasets derived from CommonCrawl and preprocess them to extract text sequences. They train a 160M parameter autoregressive transformer model on 160M training tokens per dataset for N-way classification tasks, where N ranges from 3 to 7 depending on the experiment. The trained classifier is evaluated on 8192 test sequences per dataset to measure classification accuracy. The study also examines fingerprint propagation by generating sequences from LLMs trained on these datasets and classifying the outputs. Additional experiments analyze cross-dataset generalization through perplexity measurements and mixture dataset training.

## Key Results
- Classifier achieves 74.8% accuracy distinguishing between C4, FineWeb, and RefinedWeb (vs 33.3% random chance)
- Accuracy increases to 91.2% when classifying all seven datasets simultaneously
- Fingerprint propagation: Generated text from LLMs is classified with 89.15% accuracy, only 0.61% lower than original data accuracy
- Mixing datasets improves cross-dataset generalization, reducing perplexity on held-out data
- Human performance on dataset classification task is significantly worse than the model's performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Small differences in dataset curation steps induce detectable fingerprints
- **Mechanism:** Curation steps (language filtering, heuristic filtering, deduplication, ML-based filtering) introduce subtle statistical biases in text formatting, vocabulary distribution, and thematic content
- **Core assumption:** Statistical properties of text sequences are stable enough to survive preprocessing and model training
- **Evidence anchors:** Abstract states small filtering differences induce fingerprints; section 4.1 shows humans perform significantly worse than models

### Mechanism 2
- **Claim:** Dataset fingerprints propagate through model training
- **Mechanism:** LLMs learn training data statistical distributions during pretraining and preserve these in generated outputs
- **Core assumption:** Models retain and reproduce training data statistical properties in generated outputs
- **Evidence anchors:** Abstract mentions generated sequences can be accurately classified; section 5 shows 89.15% accuracy on generated data

### Mechanism 3
- **Claim:** Dataset fingerprints enable estimation of pretraining mixture proportions
- **Mechanism:** Generated sequences proportionally represent training mixture, allowing classifier to estimate proportions
- **Core assumption:** Generated sequences reflect training mixture proportions
- **Evidence anchors:** Abstract discusses generated sequences being classified; section 5.1 shows estimated proportions approximate true proportions

## Foundational Learning

- **Concept:** Dataset classification experiments for detecting biases/fingerprints
  - **Why needed here:** Builds on Torralba and Efros's vision dataset classification framework, extending it to text datasets
  - **Quick check question:** What fundamental insight from dataset classification experiments does this paper build upon?

- **Concept:** Cross-dataset generalization and its relationship to dataset distinguishability
  - **Why needed here:** Shows distinguishable datasets lead to poor cross-dataset generalization, which mixing datasets can improve
  - **Quick check question:** How does the ability to distinguish between datasets relate to a model's ability to generalize across them?

- **Concept:** Fingerprint propagation through training and model attribution
  - **Why needed here:** Understanding how dataset characteristics persist through training is crucial for estimating mixture proportions and inferring finetuning sources
  - **Quick check question:** Why does fingerprint propagation through training enable estimation of pretraining mixture proportions?

## Architecture Onboarding

- **Component map:** Dataset collection → Preprocessing → Classifier training → Classification → Analysis of results
- **Critical path:** Dataset → Preprocessing → Classifier training → Classification → Analysis of results
- **Design tradeoffs:** Model size vs. classification accuracy (25M to 410M parameters tested); training data volume vs. performance (60M to 1.92B tokens tested); sequence length vs. accuracy (0-2000 tokens analyzed); pretraining vs. direct classification training (pretrained model performs 3.17% better)
- **Failure signatures:** Low classification accuracy despite dataset differences; poor cross-dataset generalization despite high in-dataset performance; failure to detect fingerprints in generated text; inconsistent mixture proportion estimates
- **First 3 experiments:**
  1. Implement 160M transformer classifier and train on C4, FineWeb, and RefinedWeb (480M tokens) to verify 74.8% accuracy baseline
  2. Generate sequences from Falcon-7B, DCLM-7B, and FineWeb-Edu-1.8B using single-token prompts and classify them to verify fingerprint propagation
  3. Create mixture dataset from C4, FineWeb, and RefinedWeb and train on mixture, then evaluate cross-dataset perplexity to confirm improved generalization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific aspects of formatting, vocabulary, and content in pretraining datasets contribute most significantly to distinguishability?
- **Basis in paper:** [explicit] The paper identifies formatting, vocabulary, and content as key distinguishing features but notes no single feature fully explains distinguishability
- **Why unresolved:** The paper conducts ablation studies but does not isolate exact contributions of each feature or determine which combinations are most critical
- **What evidence would resolve it:** Controlled experiments systematically varying each feature (e.g., format, word frequencies, thematic categories) and measuring classification accuracy

### Open Question 2
- **Question:** How do fingerprint propagation patterns differ between pretrained and instruction-finetuned models?
- **Basis in paper:** [explicit] The paper shows fingerprints persist through finetuning but also that finetuning causes outputs to diverge from original data
- **Why unresolved:** The paper provides limited quantitative analysis of how finetuning specifically alters fingerprint characteristics, and does not explore intermediate stages of finetuning
- **What evidence would resolve it:** Longitudinal studies tracking fingerprint evolution through various finetuning stages, and comparative analysis of pretrained vs finetuned model outputs

### Open Question 3
- **Question:** Can dataset classification be effectively used to detect data contamination or unauthorized data usage in LLM training?
- **Basis in paper:** [inferred] The paper demonstrates that fingerprints propagate through training and can be detected in generated outputs, suggesting potential forensic applications
- **Why unresolved:** The paper does not explore practical applications for detecting data contamination, unauthorized training data, or adversarial attacks using dataset classification techniques
- **What evidence would resolve it:** Empirical studies testing dataset classification on intentionally contaminated training data, or attempting to detect specific unauthorized data sources in trained models

## Limitations

- Analysis focuses exclusively on CommonCrawl-derived datasets, limiting generalizability to other data sources
- Relies on a single classifier architecture (160M parameter transformer), leaving open whether results generalize across different model families
- Assumes generated text proportionally reflects training mixture, which may not hold for models with complex generation strategies or instruction tuning

## Confidence

- **High Confidence**: Fundamental finding that datasets with similar sources can be distinguished with high accuracy (74.8-91.2%) is well-supported by experimental results and ablation studies
- **Medium Confidence**: Fingerprint propagation through training is supported by 89.15% accuracy on generated data, but lacks investigation of temporal stability or evolution through training stages
- **Low Confidence**: Implications for model attribution and copyright are speculative extensions beyond experimental scope

## Next Checks

1. **Cross-Domain Generalization**: Replicate classification experiments using datasets from different sources (academic papers, code repositories, social media) to determine whether observed fingerprints are specific to CommonCrawl-derived datasets or represent a more general phenomenon

2. **Temporal Stability Analysis**: Track how dataset fingerprints evolve as models undergo continued training, including fine-tuning and instruction tuning phases, to understand whether initial fingerprints persist or are overwritten by new training data

3. **Practical Attribution Study**: Conduct controlled experiment where multiple models are trained on known mixtures of datasets, then attempt to reverse-engineer pretraining mixtures using only generated text samples, measuring accuracy against ground truth mixture proportions