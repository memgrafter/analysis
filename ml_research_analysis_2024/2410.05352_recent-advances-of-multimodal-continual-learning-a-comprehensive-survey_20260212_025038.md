---
ver: rpa2
title: 'Recent Advances of Multimodal Continual Learning: A Comprehensive Survey'
arxiv_id: '2410.05352'
source_url: https://arxiv.org/abs/2410.05352
tags:
- learning
- task
- mmcl
- methods
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey presents the first comprehensive overview of multimodal
  continual learning (MMCL), addressing the challenge of training models on evolving
  multimodal data while preventing catastrophic forgetting. The paper categorizes
  MMCL methods into four main approaches: regularization-based, architecture-based,
  replay-based, and prompt-based.'
---

# Recent Advances of Multimodal Continual Learning: A Comprehensive Survey

## Quick Facts
- arXiv ID: 2410.05352
- Source URL: https://arxiv.org/abs/2410.05352
- Reference count: 40
- Presents the first comprehensive survey of multimodal continual learning (MMCL)

## Executive Summary
This survey provides the first comprehensive overview of multimodal continual learning, addressing the challenge of training models on evolving multimodal data while preventing catastrophic forgetting. The paper systematically categorizes MMCL methods into four main approaches: regularization-based, architecture-based, replay-based, and prompt-based. Each category is explained with representative architectures and key innovations. The survey also summarizes current datasets and benchmarks, and discusses promising future research directions including improved modality interaction strategies, parameter-efficient fine-tuning, and trustworthy learning approaches.

## Method Summary
The paper categorizes MMCL methods into four main approaches: regularization-based methods that use knowledge distillation and regularization terms to preserve previous knowledge; architecture-based methods that employ dynamic or modular architectures to adapt to new tasks; replay-based methods that store and replay previous data or use pseudo-replay strategies; and prompt-based methods that leverage prompt tuning for efficient adaptation. The survey provides detailed explanations of representative architectures from each category and discusses their respective advantages and limitations.

## Key Results
- MMCL presents unique challenges beyond simple stacking of unimodal CL methods, particularly regarding modality imbalance and complex modality interactions
- The survey identifies four main categories of MMCL methods with different approaches to preventing catastrophic forgetting
- Current benchmarks and datasets are summarized, along with promising future research directions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MMCL goes beyond simple stacking of unimodal CL methods because multimodal data introduces unique challenges that are not present in unimodal settings.
- Mechanism: The combination of multiple modalities creates complex interactions (alignment and fusion challenges) that interact with catastrophic forgetting in ways that unimodal CL methods cannot address. These modality interactions introduce spatial disorder and distribution heterogeneity that compound forgetting issues.
- Core assumption: The multimodal nature of data fundamentally changes the learning dynamics compared to unimodal settings.
- Evidence anchors:
  - [abstract]: "The primary challenge of MMCL is that it goes beyond a simple stacking of unimodal CL methods, as such straightforward approaches often yield unsatisfactory performance"
  - [section]: "The multimodal nature of MMCL introduces the following four challenges... These challenges not only stand alone but may also exacerbate the catastrophic forgetting issue"

### Mechanism 2
- Claim: Modality imbalance at both data and parameter levels creates performance degradation in MMCL systems.
- Mechanism: When different modalities have varying data availability or converge at different rates during training, the modality with better performance dominates optimization while others are under-optimized, leading to worse performance than unimodal counterparts.
- Core assumption: Different modalities have inherently different learning dynamics and resource requirements.
- Evidence anchors:
  - [abstract]: "Modality imbalance refers to the uneven processing or representation of different modalities within a multimodal system"
  - [section]: "At the parameter level, the learning of different modality-specific components may converge at varying rates, leading to a holistic imbalanced learning process across all modalities"

### Mechanism 3
- Claim: Pre-trained MM models with zero-shot capability introduce a unique challenge in MMCL that traditional CL methods don't face.
- Mechanism: When fine-tuning pre-trained MM models, there's a risk of degrading the pre-trained zero-shot capabilities, leading to negative forward transfer where performance on future tasks actually decreases.
- Core assumption: Pre-trained models encode valuable general knowledge that can be lost during task-specific fine-tuning.
- Evidence anchors:
  - [abstract]: "During continuous fine-tuning of MMCL, some of the initial capabilities derived from pre-training foundations, such as performing zero-shot tasks, may diminish"
  - [section]: "Such degradation risk may lead to severe performance decay on future tasks [46], known as negative forward transfer in MMCL"

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding why models lose previously learned knowledge is fundamental to grasping MMCL challenges
  - Quick check question: What causes catastrophic forgetting in neural networks, and why is it particularly problematic in continual learning settings?

- Concept: Modality interaction (alignment and fusion)
  - Why needed here: The unique challenges in MMCL stem from how different modalities interact during learning
  - Quick check question: How do modality alignment and fusion differ, and why do they become more complex in continual learning settings?

- Concept: Transfer learning and fine-tuning
  - Why needed here: Many MMCL methods rely on pre-trained models, making understanding how fine-tuning affects pre-trained knowledge essential
  - Quick check question: What are the trade-offs between adapting a pre-trained model to new tasks versus preserving its original capabilities?

## Architecture Onboarding

- Component map: Modality encoders (vision, language, etc.) → Modality interaction layers (alignment, fusion) → Task-specific modules (heads, adapters) → Output
- Critical path: Modality encoder → Modality interaction → Task-specific processing → Output
- Design tradeoffs:
  - Fixed vs. dynamic architecture: Fixed architectures are simpler but may lack flexibility; dynamic architectures can adapt but increase complexity
  - Direct vs. pseudo replay: Direct replay is more faithful but requires storage; pseudo replay is more scalable but may be less accurate
  - Parameter-efficient vs. full fine-tuning: Parameter-efficient methods save resources but may limit adaptation capability
- Failure signatures:
  - Mode collapse: One modality dominates training at the expense of others
  - Spatial disorder: Misalignment between modalities during continual learning
  - Negative forward transfer: Performance degradation on future tasks
- First 3 experiments:
  1. Baseline unimodal CL: Implement EWC on single modality to establish baseline performance and forgetting metrics
  2. Simple multimodal stacking: Combine two unimodal CL methods and measure performance degradation to validate MMCL challenges
  3. Memory replay validation: Implement basic episodic memory replay on multimodal data to assess improvement over stacking approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop unified MMCL methods that effectively handle modality imbalance at both data and parameter levels while maintaining computational efficiency?
- Basis in paper: [explicit] The paper identifies modality imbalance as a key challenge affecting MMCL performance and notes that few methods address parameter-level imbalance.
- Why unresolved: Current methods either focus on data-level imbalance through sampling strategies or parameter-level imbalance through gradient modulation, but rarely address both simultaneously while maintaining efficiency.
- What evidence would resolve it: A method that demonstrates improved performance on imbalanced multimodal datasets while showing computational efficiency comparable to single-modality approaches.

### Open Question 2
- Question: What novel prompt architectures could enable effective knowledge retention in MMCL methods while preserving pre-trained zero-shot capabilities?
- Basis in paper: [explicit] The paper identifies degradation of pre-trained zero-shot capability as a major challenge and notes that prompt-based methods are underrepresented in MMCL.
- Why unresolved: While prompt-based methods show promise for addressing computational costs and zero-shot preservation, current approaches lack sophisticated architectures for complex multimodal interactions.
- What evidence would resolve it: A prompt-based MMCL method that maintains zero-shot performance on unseen tasks while achieving competitive results on learned tasks across multiple modalities.

### Open Question 3
- Question: How can federated learning techniques be effectively extended to the MMCL setting to enhance model trustworthiness while addressing privacy concerns?
- Basis in paper: [inferred] The paper discusses trustworthy MMCL as a future direction and mentions federated learning's potential for enhancing robustness and privacy.
- Why unresolved: While federated continual learning exists for unimodal settings, extending these approaches to handle multiple modalities and their complex interactions remains unexplored.
- What evidence would resolve it: A federated MMCL framework that demonstrates improved privacy guarantees and robustness against malicious attacks while maintaining performance comparable to centralized MMCL methods.

## Limitations
- The survey doesn't provide quantitative comparisons of method performance across different scenarios
- Specific implementation details for each MMCL method are not provided
- The effectiveness of different approaches may vary significantly depending on specific modality combinations and task characteristics

## Confidence

**High confidence** in the categorization of MMCL challenges (catastrophic forgetting, modality imbalance, modality interaction, computational cost) as these are well-established concepts in the literature. **Medium confidence** in the proposed four-category taxonomy of MMCL methods, as this framework is logical but may not capture all nuances of existing approaches. **Low confidence** in specific method recommendations without empirical validation, as the survey doesn't provide quantitative performance comparisons.

## Next Checks

1. **Quantitative comparison validation**: Implement 2-3 representative methods from different categories on the same benchmark to empirically compare their effectiveness on modality imbalance and forgetting prevention.

2. **Modality interaction stress test**: Design experiments that specifically test how different MMCL methods handle severe modality imbalance (e.g., 10:1 data ratio between modalities) to validate the claims about modality imbalance challenges.

3. **Pre-trained capability preservation test**: Evaluate whether MMCL methods can maintain zero-shot transfer capabilities by measuring performance on held-out tasks that were never seen during fine-tuning but were within the pre-trained model's capabilities.