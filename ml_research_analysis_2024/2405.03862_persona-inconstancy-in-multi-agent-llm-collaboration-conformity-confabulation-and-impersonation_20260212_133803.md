---
ver: rpa2
title: 'Persona Inconstancy in Multi-Agent LLM Collaboration: Conformity, Confabulation,
  and Impersonation'
arxiv_id: '2405.03862'
source_url: https://arxiv.org/abs/2405.03862
tags:
- agents
- group
- agent
- opinions
- ukraine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines whether large language models can reliably
  simulate cross-national collaboration and debate. Researchers assigned AI agents
  with different national personas to groups of five and had them discuss international
  relations questions, analyzing private responses and chat transcripts.
---

# Persona Inconstancy in Multi-Agent LLM Collaboration: Conformity, Confabulation, and Impersonation

## Quick Facts
- arXiv ID: 2405.03862
- Source URL: https://arxiv.org/abs/2405.03862
- Authors: Razan Baltaji; Babak Hemmatian; Lav R. Varshney
- Reference count: 37
- Multi-agent LLM collaboration shows significant conformity and persona inconstancy issues

## Executive Summary
This study examines whether large language models can reliably simulate cross-national collaboration and debate by assigning AI agents with different national personas to groups of five for international relations discussions. The research found that while multi-agent discussions can produce collective decisions reflecting diverse perspectives, AI agents struggle with maintaining consistent personas and are susceptible to conformity based on perceived peer pressure. Agents frequently changed their expressed views during discussion based on group composition, with lone dissenters most likely to conform. The study also identified rare but disruptive instances of confabulation and impersonation where agents deviated from their assigned identities.

## Method Summary
The study used OpenAI's GPT-3.5-Turbo model through the AutoGen framework with a three-phase experimental setup: Onboarding (assigning personas and collecting private opinions), Debate/Collaboration (moderating discussions and tracking opinion changes), and Reflection (collecting final private opinions). Researchers assigned agents national personas from the GlobalOpinionQA dataset and analyzed pre-discussion, chat transcript, and post-discussion responses to measure conformity, confabulation, and impersonation rates across different entropy levels of group opinion diversity.

## Key Results
- Initial opinion diversity (entropy) drives group decision outcomes more than debate vs collaboration instructions
- AI agents maintain assigned personas during private opinion reporting but deviate during discussion
- Peer pressure effects differ from humans - any voiced opinion exerts influence proportional to its frequency
- 1.9% confabulation rate and 2.4% impersonation incidents indicate moderate persona adherence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI agents maintain assigned national personas during private opinion reporting but can deviate during discussion.
- Mechanism: Agents adopt personas through explicit instructions and role-based prompting, but discussion context and peer opinions can override persona cues.
- Core assumption: Persona adherence depends on the strength of initial instructions versus the salience of conversational context.
- Evidence anchors:
  - [abstract] "AI agents are susceptible to conformity due to perceived peer pressure and occasional challenges in maintaining consistent personas"
  - [section] "Using a simple heuristic to find instances when an agent says 'As an X agent' where X is incompatible with their assigned national identity"
  - [corpus] "Found 25 related papers (using 8)" - weak evidence, no direct support for persona maintenance mechanisms
- Break condition: When conversational context mentions other nationalities, agents may adopt those identities instead of their assigned ones.

### Mechanism 2
- Claim: Initial opinion diversity (entropy) drives group decision outcomes more than debate vs collaboration instructions.
- Mechanism: Higher entropy in private responses creates more diverse discussion content, which influences the final group decision regardless of whether agents are told to debate or collaborate.
- Core assumption: The generative nature of LLMs produces novel responses based on input diversity, making the entropy of private responses a stronger predictor than interaction style.
- Evidence anchors:
  - [abstract] "opinion diversity seems to exert its effect partly by reducing the outsize influence of chat initiators on collective decisions"
  - [section] "A group's initial opinion diversity, the entropy S of private responses during the Onboarding stage before inter-agent discussion, emerged as a stronger determinant of conversation contents and collective decisions"
  - [corpus] No direct evidence in corpus for entropy effects
- Break condition: When groups have very low entropy (near unanimity), the initiator's opinion dominates regardless of interaction style.

### Mechanism 3
- Claim: Peer pressure effects in LLM agents differ from humans - any voiced opinion exerts influence proportional to its frequency.
- Mechanism: Unlike human studies where dominance relationships strongly influence conformity, LLM agents appear to weight all voiced opinions by their frequency in the conversation.
- Core assumption: The model treats conversational context as weighted evidence, lacking the nuanced social dynamics humans use to assess opinion dominance.
- Evidence anchors:
  - [abstract] "the AI agents' submission to peer pressure or influence was less affected by the dominance relationships between various viewpoints"
  - [section] "any opinion voiced during discussion exerts influence on the Reflection responses in close correspondence to its frequency"
  - [corpus] No direct evidence in corpus for peer pressure mechanisms
- Break condition: When a single agent consistently voices an opinion, it gains disproportionate influence despite being a minority view.

## Foundational Learning

- Concept: Shannon entropy calculation for measuring opinion diversity
  - Why needed here: The study uses entropy to classify groups by opinion diversity and analyze conformity patterns
  - Quick check question: How do you calculate Shannon entropy for a set of opinions where 3 agents favor admission and 2 oppose?
  - Answer: S = -p(A)log(p(A)) - p(B)log(p(B)) where p(A) = 3/5 and p(B) = 2/5

- Concept: Persona consistency metrics and detection
  - Why needed here: The study identifies impersonation and confabulation as forms of persona inconstancy that need to be measured
  - Quick check question: What heuristic does the study use to detect when an agent impersonates a different nationality?
  - Answer: Looking for statements like "As an X agent" where X differs from the assigned identity

- Concept: Multi-agent discussion frameworks and moderation
  - Why needed here: The study uses AutoGen with a chat manager to moderate discussions and collect group predictions
  - Quick check question: What triggers the termination of a discussion in this framework?
  - Answer: When any agent requests discussion termination

## Architecture Onboarding

- Component map: AutoGen framework -> Chat manager -> Multiple GPT-3.5-turbo agents -> Assistant agent -> Onboarding/Reflection phases

- Critical path:
  1. Onboarding: Assign personas, collect private opinions
  2. Discussion: Moderate debate/collaboration, track opinion changes
  3. Reflection: Collect final private opinions
  4. Analysis: Calculate entropy, detect conformity/confabulation/impersonation

- Design tradeoffs:
  - Fixed group size (5 agents) vs. flexible sizing for different entropy classes
  - Debate vs. collaboration instructions affecting conformity rates
  - Simple heuristic detection vs. more sophisticated persona consistency analysis

- Failure signatures:
  - High impersonation rates indicating weak persona adherence
  - Confabulation rates >1% suggesting unreliable opinion generation
  - Conformity patterns that don't match human psychological models

- First 3 experiments:
  1. Replicate entropy classification with balanced dataset to verify conformity patterns
  2. Test persona consistency with different prompting strategies (system prompts vs. role-play instructions)
  3. Compare LLM agent conformity to human study baselines using same entropy classes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design AI agents that maintain consistent personas while still being open to persuasion through strong arguments?
- Basis in paper: Explicit - The paper discusses agents changing opinions based on peer pressure and context rather than argument quality.
- Why unresolved: The paper identifies conformity and persona inconstancy as problems but doesn't propose solutions for maintaining persona consistency while allowing for genuine persuasion.
- What evidence would resolve it: Experimental results showing AI agents maintaining core persona traits while updating opinions based on argument strength rather than group composition.

### Open Question 2
- Question: What specific mechanisms in language model architectures cause AI agents to adopt personas mentioned in conversation rather than their assigned identities?
- Basis in paper: Inferred - The paper notes impersonation occurs when other nationalities are mentioned in discussion.
- Why unresolved: The paper identifies impersonation as a problem but doesn't investigate the underlying architectural reasons for this behavior.
- What evidence would resolve it: Analysis of attention patterns and token generation probabilities showing how mention of different identities triggers persona shifts.

### Open Question 3
- Question: How does the order of agent contributions affect conformity rates and final group decisions in multi-agent discussions?
- Basis in paper: Explicit - The paper notes initiators have outsized influence on outcomes but doesn't systematically vary contribution order.
- Why unresolved: The paper observes initiator effects but doesn't experimentally manipulate speaking order to understand its impact on conformity and decision-making.
- What evidence would resolve it: Experiments comparing conformity rates and decision outcomes across different agent contribution sequences.

## Limitations

- The heuristic detection methods for confabulation and impersonation may undercount subtler inconsistencies
- The study cannot definitively separate causation from model artifacts in entropy-driven conformity
- Results are based on GPT-3.5-turbo and may not generalize to other model architectures

## Confidence

- Persona adherence mechanism: Medium
- Entropy-driven conformity: Medium
- Peer pressure frequency effects: Medium

## Next Checks

1. Replicate the experiment using GPT-4 and Claude models to determine if persona consistency varies by architecture
2. Implement more sophisticated persona consistency metrics (semantic similarity, identity coherence scores) beyond the simple heuristic approach
3. Conduct ablation studies removing peer opinions from discussion to isolate baseline persona adherence versus context-driven conformity