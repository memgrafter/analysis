---
ver: rpa2
title: Masked Language Modeling Becomes Conditional Density Estimation for Tabular
  Data Synthesis
arxiv_id: '2405.20602'
source_url: https://arxiv.org/abs/2405.20602
tags:
- data
- macode
- dataset
- bias
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MaCoDE, a method for generating synthetic data
  from heterogeneous tabular datasets with high machine learning utility. The approach
  redefines the multi-class classification task of Masked Language Modeling (MLM)
  as histogram-based non-parametric conditional density estimation.
---

# Masked Language Modeling Becomes Conditional Density Estimation for Tabular Data Synthesis

## Quick Facts
- **arXiv ID**: 2405.20602
- **Source URL**: https://arxiv.org/abs/2405.20602
- **Reference count**: 40
- **Primary result**: MaCoDE achieves high machine learning utility while preserving statistical characteristics in synthetic tabular data generation

## Executive Summary
This paper presents MaCoDE, a novel method for generating synthetic tabular data by reinterpreting Masked Language Modeling (MLM) as histogram-based non-parametric conditional density estimation. The approach bridges the theoretical gap between distributional learning and MLM, demonstrating that minimizing the orderless multi-class classification loss leads to minimizing the total variation distance between conditional distributions. By discretizing continuous variables into bins and using a transformer encoder architecture, MaCoDE can handle arbitrary combinations of target and conditional variables while maintaining flexibility in data privacy levels through temperature scaling.

## Method Summary
MaCoDE transforms continuous variables to [0,1] using CDF, discretizes them into L bins, and applies transformer encoder-based MLM with a masking scheme. The model learns conditional densities by minimizing the log-likelihood of masked entries across all columns, enabling generation of synthetic data that preserves statistical characteristics and machine learning utility. The method can handle incomplete training datasets without re-training by treating masked entries as missing data, and adjusts privacy levels through temperature parameter τ.

## Key Results
- MaCoDE achieves high machine learning utility (SMAPE, F1 score) across 10 real-world datasets while preserving statistical fidelity
- The method handles training datasets with missing values effectively, including multiple imputations
- Privacy levels can be adjusted easily through temperature scaling without requiring re-training
- MaCoDE demonstrates competitive performance compared to state-of-the-art tabular data synthesis methods

## Why This Works (Mechanism)

### Mechanism 1
Minimizing the orderless multi-class classification loss leads to minimizing the total variation distance between conditional distributions. By reframing MLM as histogram-based non-parametric conditional density estimation, the model learns to approximate conditional density functions for each column conditioned on arbitrary combinations of other columns. The discretization function and mask distribution allow for consistent estimation without assuming conditional independence.

### Mechanism 2
The model handles incomplete training datasets by treating masked entries as missing data. During training, masked tokens are replaced with a special bin index (0) and embedded through distinct embedding vectors. The objective function minimizes log-likelihood of masked entries, allowing learning of conditional densities even with missing values. This approach handles both MAR and MNAR missingness mechanisms through the masking strategy.

### Mechanism 3
Random column generation reflects tabular data's lack of inherent ordering. Unlike auto-regressive methods that generate columns in fixed order, MaCoDE generates one column at a time conditioned on masked subset sizes from p to 1 in descending order. The mask distribution ensures proper training of conditional densities for arbitrary conditioning sets, enabled by the transformer encoder's ability to handle variable-length inputs.

## Foundational Learning

- **Concept**: Masked Language Modeling (MLM)
  - **Why needed here**: MLM handles missing data and learns conditional distributions in tabular data by predicting masked tokens based on context, analogous to imputing missing values.
  - **Quick check question**: What is the main difference between MLM in NLP and its application in tabular data synthesis?

- **Concept**: Conditional Density Estimation
  - **Why needed here**: The goal is to estimate conditional densities for each column conditioned on arbitrary combinations of other columns, crucial for generating synthetic data with high machine learning utility.
  - **Quick check question**: How does discretization of continuous variables into bins enable histogram-based non-parametric conditional density estimation?

- **Concept**: Transformer Encoder Architecture
  - **Why needed here**: The transformer encoder processes embedded inputs and learns conditional distributions, with its ability to handle variable-length inputs and arbitrary conditioning sets being essential for the model's functionality.
  - **Quick check question**: What are the key architectural components of a transformer encoder that enable it to handle tabular data synthesis?

## Architecture Onboarding

- **Component map**: Input (discretized bin indices) -> Embedding layer (learnable vectors) -> Transformer encoder (conditional distributions) -> Output layer (bin index prediction) -> Discretization function (continuous/categorical transformation)

- **Critical path**: 1) Discretize input data into bin indices, 2) Replace masked entries with bin index 0 and embed, 3) Process through transformer encoder to learn conditional distributions, 4) Predict bin index of masked tokens, 5) Generate synthetic data by sampling from learned conditional distributions

- **Design tradeoffs**: Discretization granularity vs. model complexity (finer discretization increases accuracy but model complexity), mask distribution design affects conditioning set learning, temperature parameter τ balances synthetic data quality and privacy

- **Failure signatures**: Poor synthetic data quality indicates inaccurate conditional density learning, inability to handle missing data suggests masking strategy issues, privacy leakage occurs if temperature τ is improperly tuned

- **First 3 experiments**: 1) Train on small dataset with no missing values, evaluate MLu, 2) Introduce missing values, assess handling of incomplete data, 3) Vary temperature τ, evaluate privacy-quality trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the weakly consistent estimator property hold for continuous columns with unbounded support?
- **Basis in paper**: The paper states consistency is theoretically valid only for bounded continuous variables and acknowledges this limitation
- **Why unresolved**: Only proves weak consistency for bounded continuous variables, acknowledges this limitation explicitly
- **What evidence would resolve it**: Extending theoretical proof to unbounded continuous variables or developing modifications for unbounded support

### Open Question 2
- **Question**: How does performance compare to state-of-the-art methods on extremely large tabular datasets (millions of rows)?
- **Basis in paper**: Largest dataset used contains only 580K rows, relatively small for modern applications
- **Why unresolved**: Experiments limited to datasets with at most 580K rows
- **What evidence would resolve it**: Running extensive experiments on million-row datasets and comparing against current state-of-the-art methods

### Open Question 3
- **Question**: Can MaCoDE be extended to handle time-series tabular data with temporal dependencies?
- **Basis in paper**: Treats tabular data as lacking intrinsic ordering and generates columns randomly, which may not capture temporal patterns
- **Why unresolved**: Current formulation designed for static tabular data without temporal considerations
- **What evidence would resolve it**: Developing and testing modified version incorporating temporal information on time-series datasets

## Limitations
- Theoretical claims about total variation distance minimization would benefit from more rigorous proof
- Privacy analysis relies on distance-based metrics without formal privacy guarantees or differential privacy bounds
- Method evaluated on only 10 datasets, which may not represent all tabular data types

## Confidence

- **High Confidence**: The core mechanism of reframing MLM as histogram-based conditional density estimation is technically sound and well-supported by theoretical analysis and experimental results
- **Medium Confidence**: Claims about handling incomplete training data are supported by experiments but lack comparison to specialized imputation methods; temperature parameter effectiveness for privacy control is demonstrated but not rigorously quantified
- **Low Confidence**: Assertion that MaCoDE outperforms all existing methods on all metrics is overstated; some baselines show competitive performance on specific metrics

## Next Checks

1. **Privacy Leakage Analysis**: Conduct membership inference attacks on generated synthetic data to quantify actual privacy leakage beyond distance-based metrics, validating claims about easy privacy adjustment without re-training

2. **Large-Scale Comparative Study**: Evaluate MaCoDE against recent LLM-based tabular data generation methods on the same benchmarks to assess relative performance, particularly for datasets with high missingness rates

3. **Ablation on Discretization Granularity**: Systematically vary the number of bins (L) and analyze the trade-off between computational complexity, statistical fidelity, and MLu to determine optimal discretization strategies for different data types