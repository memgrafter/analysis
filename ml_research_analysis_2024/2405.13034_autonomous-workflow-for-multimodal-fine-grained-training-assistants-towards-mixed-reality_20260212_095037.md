---
ver: rpa2
title: Autonomous Workflow for Multimodal Fine-Grained Training Assistants Towards
  Mixed Reality
arxiv_id: '2405.13034'
source_url: https://arxiv.org/abs/2405.13034
tags:
- assembly
- training
- language
- lego
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an autonomous workflow integrating AI agents
  into extended reality (XR) applications for fine-grained training. A cerebral language
  agent combines large language models with memory, planning, and MR tool interaction,
  supported by a vision-language agent for multimodal context understanding.
---

# Autonomous Workflow for Multimodal Fine-Grained Training Assistants Towards Mixed Reality

## Quick Facts
- arXiv ID: 2405.13034
- Source URL: https://arxiv.org/abs/2405.13034
- Authors: Jiahuan Pei; Irene Viola; Haochen Huang; Junxiao Wang; Moonisa Ahsan; Fanghua Ye; Jiang Yiming; Yao Sai; Di Wang; Zhumin Chen; Pengjie Ren; Pablo Cesar
- Reference count: 22
- Key outcome: Autonomous workflow integrating AI agents into XR applications achieves BLEU-4 scores improving from ~2-25 to 54.71 after fine-tuning on LEGO-MRTA dataset

## Executive Summary
This paper presents an autonomous workflow for integrating AI agents into extended reality (XR) applications for fine-grained training. The system combines a cerebral language agent (LLM with memory, planning, and tool interaction) with a vision-language agent to enable intelligent instruction-following in MR environments. The authors create LEGO-MRTA, a multimodal dataset of 1,423 synthetic conversations and 26k context-response pairs grounded in LEGO instruction manuals, then fine-tune nine 7B open-source LLMs to demonstrate significant performance improvements.

## Method Summary
The authors develop an autonomous workflow consisting of two main AI agents: a cerebral language agent that integrates LLM with memory, planning, and MR tool interaction, and a vision-language agent for multimodal context understanding. They create the LEGO-MRTA dataset by extracting instructions from 65 LEGO manuals, generating synthetic conversations using a commercial LLM with prompt templates, and creating vision-language pairs using MiniGPT-v2. The dataset is then used to fine-tune nine 7B open-source LLMs using LoRA with sequence length 1024, learning rate 5e-5, 3 epochs, and batch size 4.

## Key Results
- BLEU-4 scores improve from ~2-25 to 54.71 after fine-tuning on the proposed dataset
- ThemeACC scores increase from ~26-55% to 81.90% across evaluated models
- Nine 7B open-source LLMs show significant performance gains after LoRA fine-tuning
- The workflow demonstrates improved instruction-following and user interaction in XR assembly training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating LLMs with memory and planning enables autonomous decision-making in MR environments.
- Mechanism: The cerebral language agent combines an LLM with memory, planning, and tool interaction to decide actions based on past experiences.
- Core assumption: LLMs can effectively use past experiences stored in memory to make informed decisions in sequential task execution.
- Evidence anchors: [abstract] "a cerebral language agent that integrates LLM with memory, planning, and interaction with XR tools and a vision-language agent, enabling agents to decide their actions based on past experiences."

### Mechanism 2
- Claim: The vision-language agent bridges visual context understanding with language for the cerebral agent's planning.
- Mechanism: The vision-language agent interprets multimodal inputs (images, videos) into metadata that the cerebral language agent uses for planning.
- Core assumption: Visual information can be accurately translated into language that the LLM can process for planning purposes.
- Evidence anchors: [abstract] "a vision-language agent that understands and transmits usersâ€™ visual context to language, which can be utilized by the cerebral language agent iteratively."

### Mechanism 3
- Claim: Grounding conversations on both instruction manuals and MR tool responses creates realistic human-like interactions.
- Mechanism: The dataset generation process uses both instruction manuals and simulated tool responses to create conversations that closely resemble natural human language.
- Core assumption: Combining instruction manuals with realistic tool responses will result in conversations that mimic natural human interactions.
- Evidence anchors: [section 4.4] "We generate conversations grounded on both the instruction manual and simulated tool responses using a commercial LLM."

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: LLMs are the core component of the cerebral language agent, enabling it to understand and generate human-like text for interactions.
  - Quick check question: What are the key components that make LLMs effective for natural language understanding and generation?

- Concept: Vision-Language Models (VLMs)
  - Why needed here: VLMs are used in the vision-language agent to interpret visual information (images, videos) and translate it into language that the LLM can use for planning.
  - Quick check question: How do VLMs bridge the gap between visual understanding and language processing?

- Concept: Multimodal Datasets
  - Why needed here: The dataset combines multiple modalities (text, images) to create a comprehensive training environment for the AI agents in the MR context.
  - Quick check question: What are the benefits of using multimodal datasets for training AI agents in complex environments like MR?

## Architecture Onboarding

- Component map:
  - Cerebral Language Agent: LLM + Memory + Planning + Tool Interaction
  - Vision-Language Agent: VLM for object detection and assembly state detection
  - MR Application: Provides functional tools and responds to agent interactions
  - Dataset: LEGO-MRTA (instruction manuals, conversations, XR responses, VQA)

- Critical path:
  1. User interacts with MR application
  2. Vision-language agent interprets visual context
  3. Cerebral language agent uses interpreted context and past experiences to plan
  4. Cerebral language agent interacts with MR application through tools
  5. Process iterates for each user interaction

- Design tradeoffs:
  - Using a commercial LLM vs. open-source LLMs: Commercial LLMs may have better performance but raise privacy concerns; open-source LLMs offer more control but may require fine-tuning.
  - Simulating conversations vs. collecting real user data: Simulation allows for controlled data generation but may lack real-world diversity; real data is more diverse but harder to collect and may contain biases.

- Failure signatures:
  - Poor task completion: If the agents fail to complete assembly tasks, it could indicate issues with the planning component or the effectiveness of tool interactions.
  - Unnatural conversations: If conversations don't resemble natural human language, it could indicate problems with the grounding process or the quality of the generated responses.
  - Inaccurate object detection: If the vision-language agent fails to accurately detect objects, it could indicate issues with the VLM or the quality of the input images.

- First 3 experiments:
  1. Test the cerebral language agent's ability to follow simple instructions without vision input to isolate the LLM's planning capabilities.
  2. Test the vision-language agent's object detection accuracy on a set of LEGO images to evaluate the VLM's performance.
  3. Test the integrated system on a simple LEGO assembly task to evaluate the interaction between the cerebral language agent, vision-language agent, and MR application.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of LLM with memory, planning, and functional tools affect the overall performance of the cerebral language agent in real-world XR assembly tasks?
- Basis in paper: [explicit] The paper discusses the design of a cerebral language agent that integrates LLM with memory, planning, and interaction with MR tools.
- Why unresolved: The paper evaluates the performance of LLMs on the proposed dataset but does not specifically assess the impact of the LLM integration with memory, planning, and functional tools in real-world XR assembly tasks.
- What evidence would resolve it: Conducting real-world XR assembly tasks with and without the integration of LLM with memory, planning, and functional tools to compare performance metrics.

### Open Question 2
- Question: What are the limitations of the current vision-language agent in accurately detecting and interpreting complex visual contexts in dynamic XR environments?
- Basis in paper: [inferred] The paper mentions the vision-language agent's role in bridging the gap between visual context and language but does not provide detailed insights into its limitations.
- Why unresolved: The paper does not delve into specific limitations or challenges faced by the vision-language agent in complex XR environments.
- What evidence would resolve it: Testing the vision-language agent in various dynamic XR scenarios and documenting instances where it fails or struggles to accurately detect and interpret visual contexts.

### Open Question 3
- Question: How scalable is the proposed autonomous workflow for integrating AI agents into XR applications beyond the LEGO assembly use case?
- Basis in paper: [inferred] The paper presents a pilot MR application for LEGO assembly but does not explore the scalability of the workflow to other assembly tasks or industries.
- Why unresolved: The paper focuses on a specific use case and does not provide insights into the workflow's adaptability to different assembly tasks or industries.
- What evidence would resolve it: Implementing the autonomous workflow in diverse assembly tasks or industries and evaluating its performance and adaptability in those contexts.

## Limitations
- Synthetic data generation using an unspecified commercial LLM makes exact reproduction difficult
- Dataset limited to LEGO assembly tasks, potentially limiting generalizability to other domains
- Evaluation relies primarily on automated metrics without extensive user studies or real-world deployment validation

## Confidence

- **High Confidence**: The architectural design combining cerebral language agents with vision-language components is technically sound and well-justified
- **Medium Confidence**: The performance improvements after fine-tuning (BLEU-4 from ~2-25 to 54.71) are significant but may be partly attributed to the synthetic nature of the training data
- **Low Confidence**: Claims about "natural human-like interactions" lack rigorous human evaluation beyond automated metrics

## Next Checks
1. **Generalization Test**: Evaluate the fine-tuned models on real user interactions rather than synthetic data to verify that performance gains translate to practical use cases
2. **Domain Transfer**: Test the workflow on non-LEGO assembly tasks (furniture, electronics, etc.) to assess domain generalization capabilities
3. **Ablation Study**: Systematically remove components (memory, planning, vision-language integration) to quantify their individual contributions to performance gains