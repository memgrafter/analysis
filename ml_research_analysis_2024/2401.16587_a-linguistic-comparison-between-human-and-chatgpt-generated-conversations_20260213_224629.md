---
ver: rpa2
title: A Linguistic Comparison between Human and ChatGPT-Generated Conversations
arxiv_id: '2401.16587'
source_url: https://arxiv.org/abs/2401.16587
tags:
- human
- chatgpt
- dialogues
- linguistic
- conversations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study employs LIWC analysis to compare linguistic features
  of 19.5K human and ChatGPT-3.5-generated dialogues. ChatGPT dialogues show higher
  proficiency in social processes, analytical style, cognition, attentional focus,
  and positive emotional tone, aligning with recent findings that LLMs are "more human
  than human." However, human dialogues exhibit greater variability and authenticity.
---

# A Linguistic Comparison between Human and ChatGPT-Generated Conversations

## Quick Facts
- arXiv ID: 2401.16587
- Source URL: https://arxiv.org/abs/2401.16587
- Authors: Morgan Sandler; Hyesun Choung; Arun Ross; Prabu David
- Reference count: 33
- One-line primary result: ChatGPT-generated dialogues exhibit higher linguistic proficiency than human dialogues but with lower variability

## Executive Summary
This study employs LIWC analysis to compare linguistic features of 19.5K human and ChatGPT-3.5-generated dialogues. ChatGPT dialogues show higher proficiency in social processes, analytical style, cognition, attentional focus, and positive emotional tone, aligning with recent findings that LLMs are "more human than human." However, human dialogues exhibit greater variability and authenticity. No significant difference was found in positive or negative affect between the two groups. Classifier analysis of dialogue embeddings revealed implicit coding of affect valence, despite no explicit mention of affect. The research also introduces the 2GPTEmpathicDialogues dataset, a companion to the EmpathicDialogues dataset, for AI research on language modeling.

## Method Summary
The study generated 19,533 ChatGPT dialogues using OpenAI's API with temperature 0.5 and max tokens 250, mirroring scenarios from the EmpathicDialogues dataset. LIWC-22 analysis extracted 118 linguistic features from both human and AI dialogues, with statistical comparisons using t-tests and Levene's tests (p < .001 after Bonferroni correction). Embeddings were extracted using text-embedding-ada-002, and SVM, Random Forest, and MLP classifiers were trained for binary valence classification. UMAP visualization reduced embeddings to 3D for analysis.

## Key Results
- ChatGPT dialogues show higher proficiency in social processes, analytical style, cognition, attentional focus, and positive emotional tone
- Human dialogues exhibit greater variability and authenticity across all linguistic categories
- No significant difference in positive or negative affect between human and ChatGPT dialogues
- Classifier analysis reveals implicit coding of affect valence in dialogue embeddings despite no explicit mention of affect

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ChatGPT-generated dialogues exhibit higher linguistic proficiency in social processes, cognition, and analytical style than human dialogues.
- **Mechanism:** ChatGPT's training on vast human language data allows it to predict and generate language that aligns with established patterns of "proficient" communication, including higher use of social markers, cognitive complexity, and structured analytical language.
- **Core assumption:** The LIWC categories used (social processes, cognition, analytical thinking) accurately capture linguistic proficiency, and ChatGPT's training data contains sufficient examples of these patterns to model them effectively.
- **Evidence anchors:**
  - [abstract]: "ChatGPT dialogues show higher proficiency in social processes, analytical style, cognition, attentional focus, and positive emotional tone"
  - [section]: "ChatGPT conversations were slightly more socially sensitive than human conversations" and "ChatGPT ( M = 64.79, SD = 23.91) conversations were better (δ = 1.25, large effect) in attentional focus than human conversations"
  - [corpus]: Weak/no direct evidence. The corpus shows related work on LLM detection but doesn't specifically anchor to these LIWC findings.
- **Break condition:** If LIWC categories don't accurately measure proficiency, or if ChatGPT's training data lacks diversity in social/cognitive contexts, the mechanism fails.

### Mechanism 2
- **Claim:** ChatGPT embeddings contain implicit valence information despite no explicit mention of affect in the dialogues.
- **Mechanism:** The LLM's training process captures emotional valence as a latent feature in its embeddings because human language used in training contains consistent emotional patterns that become encoded in the model's representations.
- **Core assumption:** Emotional valence is a consistent, learnable pattern in language that becomes embedded in LLM representations even when not explicitly labeled.
- **Evidence anchors:**
  - [abstract]: "Classifier analysis of dialogue embeddings indicates implicit coding of the valence of affect despite no explicit mention of affect in the conversations"
  - [section]: "the SVM exhibited the best performance for both datasets, achieving an average weighted F1-score of 90.0% on the human-generated dataset and 95.3% on the ChatGPT-generated dataset"
  - [corpus]: Weak evidence. Corpus shows related detection work but doesn't specifically validate implicit valence encoding.
- **Break condition:** If emotional valence patterns are too subtle or context-dependent to be consistently encoded, or if the classifier is exploiting spurious correlations rather than genuine emotional information.

### Mechanism 3
- **Claim:** ChatGPT-generated dialogues show lower variance than human dialogues across all linguistic categories.
- **Mechanism:** The temperature setting (0.5) and the nature of LLM training (predicting the most probable next tokens) constrain output to more consistent patterns, reducing linguistic variability compared to the diverse, context-dependent nature of human conversation.
- **Core assumption:** The temperature parameter and LLM architecture inherently reduce variance, and this effect is measurable across all LIWC categories.
- **Evidence anchors:**
  - [abstract]: "human dialogues exhibit greater variability and authenticity" while "ChatGPT dialogues show higher proficiency in social processes, analytical style, cognition, attentional focus, and positive emotional tone"
  - [section]: "the variance between human- and ChatGPT-generated conversations was different at p < . 001 for all 118 linguistic categories" and "The lower variance in ChatGPT conversations can be attributed to the temperature setting of 0.5"
  - [corpus]: No direct evidence in corpus about variance differences between human and LLM-generated text.
- **Break condition:** If temperature has minimal effect on variance, or if human conversations in the dataset are unusually homogeneous, the mechanism breaks.

## Foundational Learning

- **Concept: LIWC (Linguistic Inquiry and Word Count)**
  - Why needed here: The entire analysis methodology relies on LIWC's categorization of language into psychological constructs. Understanding how LIWC works is essential to interpret the findings.
  - Quick check question: What are the three main components of LIWC analysis mentioned in the paper, and how do they differ?

- **Concept: LLM embeddings and their interpretability**
  - Why needed here: The valence classification experiment depends on understanding how embeddings capture semantic information, including implicit emotional content.
  - Quick check question: According to the paper, what was the performance difference between human and ChatGPT embeddings in the valence classification task, and what might this suggest?

- **Concept: Statistical significance and effect sizes**
  - Why needed here: The paper reports numerous statistical tests and effect sizes. Understanding these concepts is crucial for evaluating the strength and reliability of the findings.
  - Quick check question: What effect size threshold indicates a "large effect" in this study, and which finding exceeded this threshold?

## Architecture Onboarding

- **Component map:** Dataset generation (Python script with two ChatGPT instances) -> LIWC analysis (118 linguistic features) -> Statistical comparisons (t-tests, Levene's tests) -> Embedding extraction (text-embedding-ada-002) -> Valence classification (SVM, RF, MLP) -> UMAP visualization (3D reduction)

- **Critical path:** The core workflow is: generate dialogues → run LIWC analysis → perform statistical comparisons → extract embeddings → run valence classification → visualize with UMAP.

- **Design tradeoffs:** The study uses ChatGPT-3.5 specifically rather than newer models, which limits generalizability but ensures reproducibility. The temperature setting of 0.5 reduces variance but may not represent all possible ChatGPT outputs.

- **Failure signatures:** Inconsistent role confusion between ChatGPT instances, significant differences in word count between human and AI dialogues, or classifiers failing to distinguish valence would indicate methodological issues.

- **First 3 experiments:**
  1. Reproduce the LIWC analysis on a small sample to verify the statistical pipeline works correctly.
  2. Test the embedding extraction and classification pipeline on a known sentiment dataset to validate the methodology.
  3. Run UMAP visualization on a subset of data to ensure the dimensionality reduction produces interpretable clusters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do newer LLM models (GPT-4, Claude, Gemini) compare to ChatGPT-3.5 in terms of linguistic features and authenticity when generating empathic dialogues?
- Basis in paper: [explicit] The paper notes that findings are specific to ChatGPT-3.5-Turbo and may not apply to newer models.
- Why unresolved: The study only analyzed ChatGPT-3.5, and the paper explicitly states that comparisons with newer models are needed.
- What evidence would resolve it: A comparative study analyzing linguistic features of GPT-4, Claude, and Gemini dialogues using the same LIWC methodology and dataset.

### Open Question 2
- Question: What role does the arousal dimension play in classifying emotions in LLM-generated dialogues, and how would incorporating it affect the accuracy of valence classification?
- Basis in paper: [inferred] The paper mentions that future research should include additional emotional dimensions like arousal to analyze embeddings.
- Why unresolved: The current classification only uses valence, which may be overly simplistic given the complexity of emotions.
- What evidence would resolve it: Classification experiments incorporating both valence and arousal dimensions, comparing results with the current binary valence-only approach.

### Open Question 3
- Question: How does the inclusion of context and cultural nuances in LLM training data affect the authenticity and variability of generated dialogues?
- Basis in paper: [inferred] The paper discusses the limitations of LLMs in capturing human-like variability and the importance of context in emotional expression.
- Why unresolved: The study highlights the need for more nuanced classification and the impact of training data on linguistic features, but does not explore cultural context.
- What evidence would resolve it: Analysis of LLM-generated dialogues across different cultural contexts, comparing linguistic features and authenticity scores with human dialogues from the same cultures.

## Limitations
- Findings are specific to ChatGPT-3.5 with temperature 0.5 and may not generalize to newer models or different generation settings
- High word count disparity between human (avg 58 words) and ChatGPT (avg 300 words) dialogues may introduce normalization artifacts
- Absence of human evaluators to validate perceived "authenticity" of dialogues leaves characterization subjective

## Confidence

- **High Confidence:** The LIWC statistical comparisons showing ChatGPT's higher proficiency in social processes, analytical style, and cognition (supported by multiple p < .001 findings and large effect sizes)
- **Medium Confidence:** The valence classification results (90-95% F1 scores) suggesting embeddings capture emotional information implicitly, though the mechanism remains speculative
- **Medium Confidence:** The claim about lower variance in ChatGPT dialogues, which is statistically significant but may reflect the temperature setting more than inherent model properties

## Next Checks
1. Test the stability of findings across different ChatGPT temperature settings (0.1, 0.7, 1.0) to determine if the proficiency differences persist under varied generation conditions
2. Conduct a human evaluation study where participants rate dialogues on authenticity and social sensitivity to validate the LIWC-based conclusions
3. Apply the same analysis pipeline to a newer model (ChatGPT-4 or Claude) to assess whether the "more human than human" pattern extends to current-generation LLMs