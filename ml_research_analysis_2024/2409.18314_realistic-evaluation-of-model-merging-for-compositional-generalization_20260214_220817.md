---
ver: rpa2
title: Realistic Evaluation of Model Merging for Compositional Generalization
arxiv_id: '2409.18314'
source_url: https://arxiv.org/abs/2409.18314
tags:
- merging
- uni0065
- uni0061
- methods
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Model merging has become a popular technique for combining fine-tuned
  models to create improved multitask models. However, different merging methods are
  typically evaluated in disparate settings, making comparison difficult.
---

# Realistic Evaluation of Model Merging for Compositional Generalization

## Quick Facts
- **arXiv ID:** 2409.18314
- **Source URL:** https://arxiv.org/abs/2409.18314
- **Reference count:** 40
- **Primary result:** Model merging performance varies significantly across applications and is often anticorrelated between held-in and generalization performance

## Executive Summary
This paper presents a comprehensive evaluation of eight popular model merging methods across image classification, image generation, and natural language processing tasks, with a focus on compositional generalization. The study reveals that merging performance varies significantly across different applications, with held-in and generalization performance often being anticorrelated, particularly for cross-lingual generalization tasks. The research also highlights that increasing the number of merged models tends to decrease held-in performance while improving generalization capabilities. Additionally, the paper emphasizes the importance of practical considerations such as computational costs and hyperparameter sensitivity when selecting a merging method for real-world applications.

## Method Summary
The study evaluates eight model merging methods (simple averaging, SLERP, Task Arithmetic, DARE, TIES, Fisher Merging, RegMean, and MaTS) across three domains: image classification using DomainNet, image generation using Stable Diffusion 2.1, and natural language processing tasks in multiple languages. The evaluation process involves fine-tuning constituent models on specific tasks, applying each merging method with hyperparameter sweeps, and assessing performance on both held-in tasks (those used during merging) and generalization tasks (new compositions of the constituent capabilities). The researchers measure accuracy for classification tasks, CLIP score and FID for generation tasks, and accuracy for NLP tasks, providing a unified framework for comparing merging methods across diverse applications.

## Key Results
- Merging method performance varies significantly across different applications and domains
- Held-in and generalization performance are often anticorrelated, particularly for cross-lingual NLP tasks
- Increasing the number of merged models generally decreases held-in performance but improves generalization
- Practical considerations like computational costs and hyperparameter sensitivity significantly impact method selection

## Why This Works (Mechanism)
Model merging works by combining the parameter spaces of individually fine-tuned models to create a unified model capable of performing multiple tasks. The various merging methods differ in how they combine parameters - through simple averaging, geometric interpolation (SLERP), task-specific adjustments, or statistical approaches (Fisher Merging, RegMean). The effectiveness depends on how well the merged parameter space captures the relevant features for both the original tasks and new compositional tasks. The anticorrelation between held-in and generalization performance suggests that optimizing for specific tasks may not transfer well to novel compositions, particularly in cross-lingual settings where linguistic structures differ significantly from the training distribution.

## Foundational Learning
- **Model merging fundamentals**: Understanding how parameter averaging and interpolation techniques combine learned representations
  - Why needed: Essential for grasping how different merging methods operate and their theoretical basis
  - Quick check: Can explain the difference between simple averaging and SLERP interpolation

- **Compositional generalization**: The ability of models to perform tasks composed of capabilities learned from different sources
  - Why needed: Central concept being evaluated - how well merged models handle unseen task combinations
  - Quick check: Can define compositional generalization and provide examples

- **Cross-lingual generalization challenges**: Understanding why models struggle to generalize across languages
  - Why needed: Explains the observed anticorrelation pattern in NLP tasks
  - Quick check: Can articulate why cross-lingual tasks present unique generalization challenges

- **Computational cost analysis**: Evaluating the trade-offs between merging method complexity and performance gains
  - Why needed: Practical consideration for selecting appropriate merging methods
  - Quick check: Can compare computational requirements of different merging methods

## Architecture Onboarding

**Component Map:** Data preprocessing -> Model fine-tuning -> Merging method application -> Evaluation (held-in + generalization)

**Critical Path:** The most time-consuming step is fine-tuning constituent models on individual tasks, followed by the merging process itself. For expensive methods like RegMean and MaTS, the computational cost can be substantial, particularly for large models.

**Design Tradeoffs:** Simple methods (averaging, SLERP) are computationally efficient but may underperform complex methods on challenging tasks. Advanced methods (Fisher Merging, RegMean, MaTS) can achieve better performance but require significantly more computational resources and careful hyperparameter tuning.

**Failure Signatures:** Poor merging performance often manifests as either catastrophic forgetting of constituent tasks or inability to generalize to compositional tasks. Computational failures typically occur with memory-intensive methods when merging large models.

**Three First Experiments:**
1. Implement and test simple averaging and SLERP on a small-scale vision task to establish baseline performance
2. Compare Task Arithmetic and DARE on a single NLP task to evaluate task-specific merging approaches
3. Run Fisher Merging on a small image generation task to assess statistical merging methods

## Open Questions the Paper Calls Out
The paper identifies several unresolved questions regarding model merging for compositional generalization. The underlying reason for the anticorrelation between held-in and generalization performance in cross-lingual NLP tasks remains unexplained, though the authors hypothesize that cross-lingual generalization presents unique challenges compared to cross-domain generalization. The paper also does not explore how merging methods perform when combining models with different initializations or architectures, focusing instead on models with identical starting points. Additionally, while the study shows that generalization performance generally increases with more merged models, it does not provide specific guidance on the optimal number of models to merge, leaving open the question of balancing generalization gains against computational costs.

## Limitations
- Evaluation focuses on a limited set of compositional generalization scenarios, potentially limiting generalizability
- Computational cost analysis, while acknowledged, does not fully explore practical implementation challenges at scale
- The study does not investigate merging models with different initializations or architectures
- Optimal selection criteria for real-world applications remain partially unresolved

## Confidence
**High confidence** in findings regarding variability of merging method performance across domains and the anticorrelation between held-in and generalization performance. **Medium confidence** in generalizability beyond studied tasks and datasets. **Low confidence** in optimal selection criteria for real-world applications due to complex interplay between practical constraints and performance outcomes.

## Next Checks
1. **Cross-Domain Generalization**: Test merging methods on a broader range of compositional tasks, including multi-modal scenarios combining visual and textual understanding
2. **Scalability Analysis**: Conduct experiments with larger model sizes (10B+ parameters) to validate computational cost estimates and performance trends
3. **Dynamic Merging**: Implement and evaluate adaptive merging strategies that adjust weights during merging based on task performance