---
ver: rpa2
title: Unlocking the Power of LSTM for Long Term Time Series Forecasting
arxiv_id: '2408.10006'
source_url: https://arxiv.org/abs/2408.10006
tags:
- slstm
- memory
- time
- lstm
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of long-term time series forecasting
  using Long Short-Term Memory (LSTM) networks. The authors propose a new method called
  P-sLSTM that builds upon the recently introduced sLSTM by incorporating patching
  and channel independence techniques.
---

# Unlocking the Power of LSTM for Long Term Time Series Forecasting

## Quick Facts
- arXiv ID: 2408.10006
- Source URL: https://arxiv.org/abs/2408.10006
- Reference count: 11
- Outperforms LSTM by 20% and achieves SOTA performance on 5 multivariate time series datasets

## Executive Summary
This paper addresses the challenge of long-term time series forecasting using Long Short-Term Memory (LSTM) networks. The authors propose P-sLSTM, a new method that builds upon sLSTM by incorporating patching and channel independence techniques. Patching divides time series data into smaller segments to help capture long-term dependencies, while channel independence processes each channel separately to prevent overfitting. The proposed model outperforms the original LSTM by 20% in accuracy and achieves comparable performance against state-of-the-art models such as Transformers and MLPs.

## Method Summary
P-sLSTM combines the recently introduced sLSTM with two key innovations: patching and channel independence. Patching divides time series into smaller segments that can be processed independently, while channel independence processes each channel separately to prevent overfitting. The model uses exponential gating (replacing sigmoid with exp) to allow more dynamic memory control. The architecture processes multivariate time series by first applying channel independence to reshape data, then patching to split into manageable segments, followed by sLSTM processing, and finally combining outputs through linear layers.

## Key Results
- P-sLSTM outperforms LSTM by 20% in accuracy across benchmark datasets
- Achieves 23 first-place and 10 second-place rankings out of 40 settings on 5 popular multivariate datasets
- Demonstrates exceptional performance across multiple datasets and prediction length settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Patching allows LSTM to capture long-term dependencies by breaking long sequences into smaller manageable patches that each can be processed independently.
- Mechanism: Time series sequences are divided into patches of size P, processed separately by the sLSTM backbone, and then recombined. This avoids the inherent short-memory limitation of sLSTM by ensuring each patch is short enough for effective learning while the linear layers combine the information globally.
- Core assumption: The original long sequence contains redundant or locally stationary patterns such that information loss from patching is minimal.
- Evidence anchors:
  - [abstract] "Patching divides time series data into smaller segments to help the model capture long-term dependencies"
  - [section] "inspired by the success of patches in transformer architectures (Nie et al. 2023), we use patches to artificially divide original time series into components"
- Break condition: If time series have abrupt, non-stationary transitions that span patch boundaries, information critical to forecasting may be lost.

### Mechanism 2
- Claim: Channel independence prevents overfitting by processing each time series channel separately.
- Mechanism: Multivariate time series are split into M channels, each processed independently through its own sLSTM pathway. This reduces parameter sharing across channels, lowering model capacity and thus reducing overfitting risk.
- Core assumption: Channels in multivariate time series are largely independent in their temporal dynamics.
- Evidence anchors:
  - [abstract] "channel independence helps prevent overfitting by processing each channel separately"
  - [section] "We evaluate the performance of proposed P-sLSTM model on 5 popular multivariate datasets"
  - [section] "Channel Independence (CI) has been proved to avoid overfitting problems and improve the computational efficiency in Transformer-based models"
- Break condition: If channels are highly correlated, this independence could waste model capacity and hurt performance.

### Mechanism 3
- Claim: Exponential gating in sLSTM revises memory storage decisions dynamically, improving adaptability to changing data.
- Mechanism: Forget and input gates use exp(·) instead of sigmoid, allowing values outside [0,1]. This enables the model to amplify or suppress past information more aggressively, adjusting memory retention in response to new inputs.
- Core assumption: The importance of past information varies significantly over time in TSF tasks.
- Evidence anchors:
  - [abstract] "the recently introduced sLSTM for Natural Language Processing (NLP) introduces exponential gating and memory mixing that are beneficial for long term sequential learning"
  - [section] "Unlike the sigmoid activation function, exponential gating allows for a broader range of values, providing the forget gate with more diverse outputs in different situations"
- Break condition: If forget gate outputs grow unbounded, it can cause numerical overflow and degrade learning stability.

## Foundational Learning

- Concept: Markov Chain Process for RNNs
  - Why needed here: To analyze the memory property of sLSTM and prove it exhibits geometric ergodicity (short memory) under certain conditions.
  - Quick check question: What is the transition function MsLSTM mapping from and to in the sLSTM process?

- Concept: Geometric Ergodicity
  - Why needed here: Determines whether the RNN has short or long memory; critical for understanding limitations of sLSTM and why patching is necessary.
  - Quick check question: What condition on the forget gate ensures geometric ergodicity in sLSTM?

- Concept: Patching and Channel Independence
  - Why needed here: These are the key architectural modifications that overcome sLSTM's short-memory limitation and overfitting respectively.
  - Quick check question: How does the patching operator transform input dimensions from (B·M)×L to (B·M)×N×P?

## Architecture Onboarding

- Component map: Input -> Channel Independence -> Patching -> Linear Layer 1 -> sLSTM Block -> Flatten -> Linear Layer 2 -> Output
- Critical path: Channel independence → patching → linear projection → sLSTM blocks → flatten → final projection
- Design tradeoffs:
  - Patching reduces sequence length per sLSTM pass but adds complexity of combining patch outputs
  - Channel independence reduces overfitting but may ignore cross-channel dependencies
  - Exponential gating offers more dynamic memory control but risks numerical instability
- Failure signatures:
  - Patch size too small → loss of temporal continuity
  - Patch size too large → short-memory problem returns
  - CI when channels are correlated → wasted capacity
  - Exponential forget gate unbounded → overflow/gradient issues
- First 3 experiments:
  1. Test sLSTM vs P-sLSTM on a synthetic dataset with known long-range dependencies; measure forecasting accuracy as look-back window increases
  2. Vary patch size on a standard dataset (e.g., Weather) and plot MSE vs patch size to find optimal patch size
  3. Compare CI vs CM variants on a multivariate dataset to quantify overfitting reduction

## Open Questions the Paper Calls Out

- **Open Question 1**: How can patching techniques be designed to preserve more of the original periodicity of time series data while still enabling long-term dependency capture?
  - Basis in paper: [explicit] The authors suggest that current patching methods may destroy original temporal sequential information and propose exploring more complex patching techniques to preserve periodicity.
  - Why unresolved: The paper identifies this as a limitation but does not provide specific solutions or evaluate alternative patching approaches that better maintain temporal structure.
  - What evidence would resolve it: Experimental results comparing different patching strategies (e.g., overlapping patches, adaptive patch sizes based on periodicity) that demonstrate improved forecasting accuracy while preserving temporal dependencies.

- **Open Question 2**: Can LSTM-based models be effectively parallelized to match the computational efficiency of transformer-based models while maintaining their interpretability advantages?
  - Basis in paper: [explicit] The authors acknowledge that LSTMs/RNNs cannot be computed in parallel and suggest exploring mLSTM as a potential solution.
  - Why unresolved: The paper mentions mLSTM as a possibility but does not investigate its effectiveness for time series forecasting or compare its performance and efficiency to existing models.
  - What evidence would resolve it: Benchmarking studies showing mLSTM's performance on TSF tasks compared to both traditional LSTMs and transformer-based models, including training time and memory usage metrics.

- **Open Question 3**: What are the optimal conditions and mechanisms for applying Channel Independence (CI) techniques to RNN-based models beyond time series forecasting?
  - Basis in paper: [explicit] The authors demonstrate CI's effectiveness in preventing overfitting in P-sLSTM and suggest it could be expanded to more neural network structures.
  - Why unresolved: The paper only applies CI to P-sLSTM for TSF and does not explore its applicability or benefits in other domains or network architectures.
  - What evidence would resolve it: Empirical studies applying CI to various RNN architectures across different domains (e.g., NLP, speech recognition) showing consistent improvements in performance and/or generalization.

## Limitations
- Patching mechanism borrows from vision/transformers without validation for time series data
- Channel independence assumption may fail for highly correlated multivariate time series
- Exponential gating mechanism lacks rigorous stability guarantees

## Confidence
- **High confidence**: Experimental results showing P-sLSTM outperforms LSTM by 20% and achieves state-of-the-art performance on benchmark datasets
- **Medium confidence**: Theoretical justification for exponential gating improvements, supported by references to sLSTM NLP work but lacking direct TSF validation
- **Low confidence**: Generalization claims beyond the five tested datasets, particularly for highly correlated multivariate time series where channel independence may fail

## Next Checks
1. Conduct ablation studies on a synthetic dataset with controlled channel correlation to quantify the cost/benefit tradeoff of channel independence when channels are dependent vs independent
2. Test numerical stability across different datasets by monitoring exponential gate activations during training to ensure no overflow or vanishing gradient issues occur
3. Evaluate P-sLSTM on a dataset with known abrupt transitions spanning patch boundaries to measure information loss from the patching mechanism and determine optimal patch sizing strategies