---
ver: rpa2
title: Epsilon-Greedy Thompson Sampling to Bayesian Optimization
arxiv_id: '2403.00540'
source_url: https://arxiv.org/abs/2403.00540
tags:
- function
- greedy
- optimization
- sampling
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new approach to improve the exploitation
  capabilities of Thompson sampling (TS) in Bayesian optimization (BO) by incorporating
  the epsilon-greedy policy from reinforcement learning. The method combines the generic
  TS (which promotes exploration) with a sample-average TS (which promotes exploitation)
  using a probability parameter epsilon.
---

# Epsilon-Greedy Thompson Sampling to Bayesian Optimization

## Quick Facts
- arXiv ID: 2403.00540
- Source URL: https://arxiv.org/abs/2403.00540
- Authors: Bach Do; Taiwo Adebiyi; Ruda Zhang
- Reference count: 40
- Key outcome: ε-greedy TS improves exploitation in BO by mixing generic TS and sample-average TS, outperforming or matching component methods while maintaining computational efficiency.

## Executive Summary
This paper introduces ε-greedy Thompson sampling (TS) to improve exploitation in Bayesian optimization (BO). The method combines generic TS (which promotes exploration) with sample-average TS (which favors exploitation) using a probability parameter ε. Small ε values prioritize exploitation while large values prioritize exploration. Experiments on benchmark functions demonstrate that ε-greedy TS with appropriate ε settings outperforms or matches its two extremes while maintaining computational efficiency comparable to generic TS.

## Method Summary
The method implements ε-greedy TS by randomly switching between generic TS (with probability ε) and sample-average TS (with probability 1-ε). Generic TS samples a single path from the GP posterior for exploration, while sample-average TS averages Ns sample paths to approximate the posterior mean for exploitation. The GP model uses squared exponential covariance with random features for efficient sampling. The acquisition function minimizer selects the next evaluation point, which is then evaluated on the objective function to update the dataset.

## Key Results
- ε-greedy TS outperforms or matches generic TS and sample-average TS on benchmark functions when properly tuned
- Small ε values (e.g., 0.1) improve exploitation while maintaining reasonable exploration
- Computational cost of ε-greedy TS approaches that of generic TS as ε increases
- Method shows robustness across different benchmark functions compared to component methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ε-greedy TS improves exploitation by mixing generic TS and sample-average TS according to ε.
- Mechanism: With probability ε, the algorithm samples a single path from the GP posterior (exploration); with probability 1-ε, it averages Ns sample paths (exploitation). Small ε shifts weight toward exploitation.
- Core assumption: Averaging Ns sample paths approximates the GP posterior mean, whose minimizer favors exploitation.
- Evidence anchors:
  - [abstract] "We first delineate two extremes of TS, namely the generic TS and the sample-average TS. The former promotes exploration, while the latter favors exploitation. We then adopt the ε-greedy policy to randomly switch between these two extremes."
  - [section] "The generic TS and the averaging TS for a sufficiently large number of sample paths represent two extremes of TS: the former and latter address exploration and exploitation, respectively."
- Break condition: If Ns is too small, the average no longer approximates the posterior mean and the exploitation benefit disappears.

### Mechanism 2
- Claim: Exploration arises naturally from GP uncertainty, while ε controls deliberate exploitation/exploration balance.
- Mechanism: High posterior uncertainty yields diverse sample paths, so even the generic TS step explores. ε modulates the frequency of these exploration steps versus exploitation steps.
- Core assumption: The GP posterior uncertainty correlates with regions of the input space that are under-sampled.
- Evidence anchors:
  - [abstract] "When the GP posterior exhibits high uncertainty, TS tends to produce diverse input variable points during the optimization process, thereby prioritizing exploration."
  - [section] "As the number of observations increases, it transitions to exploiting the knowledge gained about the true objective function."
- Break condition: If the GP model is inaccurate (e.g., in high dimensions), uncertainty no longer reflects true lack of information, so exploration becomes random rather than informative.

### Mechanism 3
- Claim: Small ε reduces computational cost by invoking the cheaper generic TS more often.
- Mechanism: Generic TS samples a single path, which is computationally cheaper than averaging Ns paths. Increasing ε shifts work toward the cheaper operation.
- Core assumption: Sampling a single path is significantly cheaper than averaging Ns paths.
- Evidence anchors:
  - [section] "Figure 6 shows approximate distributions of unit runtime for selecting the next input variable point using the averaging TS with Ns = 50, ε-greedy TS with different ε values and Ns = 50, and the generic TS for the 2d Ackley and 2d Rosenbrock functions."
  - [section] "Figure 6 show marginal differences in empirical mode values of unit runtime for Ns = 20, Ns = 50, and the generic TS."
- Break condition: If Ns is small enough, the cost difference between generic TS and averaging TS becomes negligible.

## Foundational Learning

- Concept: Gaussian Processes (GPs) as surrogate models
  - Why needed here: The entire BO framework relies on a probabilistic model of the objective function; TS samples from this GP posterior.
  - Quick check question: What are the two outputs of a GP posterior prediction at a new point?
- Concept: Exploitation-exploration trade-off in sequential decision making
  - Why needed here: ε-greedy TS is a direct mechanism for balancing these competing objectives; understanding this trade-off is essential to tuning ε.
  - Quick check question: In ε-greedy, what happens when ε = 0 and when ε = 1?
- Concept: Thompson Sampling as posterior sampling for decision making
  - Why needed here: ε-greedy TS builds on TS by modifying how samples are drawn; knowing the vanilla TS mechanism is prerequisite.
  - Quick check question: In BO via TS, how is the next evaluation point chosen from a sampled path?

## Architecture Onboarding

- Component map: GP model builder -> sample-path generator (single or averaged) -> ε-greedy selector -> acquisition point minimizer -> objective evaluation -> dataset update
- Critical path: Sample generation -> acquisition minimization -> objective evaluation -> model update
- Design tradeoffs: (1) Ns vs exploitation strength (larger Ns -> stronger exploitation but higher cost); (2) ε vs exploration/exploitation balance; (3) single-path vs averaged sampling for runtime
- Failure signatures: (1) Performance plateaus early -> likely insufficient exploration (ε too small or Ns too large); (2) Noisy or divergent behavior -> GP model too uncertain or Ns too small; (3) Excessive runtime -> Ns too large relative to budget
- First 3 experiments:
  1. Run ε-greedy TS with ε = 0.5 and Ns = 20 on 2d Ackley; compare median best-found value to generic TS
  2. Vary ε ∈ {0.1, 0.5, 0.9} with Ns fixed at 50 on 2d Rosenbrock; plot convergence curves
  3. Fix ε = 0.5, vary Ns ∈ {10, 50, 100} on 6d Hartmann; measure runtime and final objective value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of ε that balances exploration and exploitation for a given black-box objective function?
- Basis in paper: [explicit] The paper states that the optimal ε strongly depends on the problem of interest and there exists an optimal ε that corresponds to the best performance of ε-greedy TS for each problem.
- Why unresolved: The paper acknowledges that it is unclear to determine whether a black-box objective function favors exploitation or exploration to locate its minimum, and finding the optimal ε requires problem-specific knowledge.
- What evidence would resolve it: Empirical studies comparing the performance of ε-greedy TS with various ε values on a diverse set of benchmark functions and real-world optimization problems would provide insights into the optimal ε for different problem classes.

### Open Question 2
- Question: How does the performance of ε-greedy TS scale with increasing dimensionality of the input space?
- Basis in paper: [inferred] The paper mentions that deliberate exploitation of ε-greedy TS is hindered by an increase in the number of input variables, and the advantage of ε-greedy TS over other methods is problem-dependent.
- Why unresolved: The paper only tests the performance of ε-greedy TS on problems with up to 10 dimensions, and the scalability to higher dimensions is not explored.
- What evidence would resolve it: Experiments evaluating the performance of ε-greedy TS on optimization problems with dimensions higher than 10, comparing it with other state-of-the-art Bayesian optimization methods, would provide insights into its scalability.

### Open Question 3
- Question: How does the choice of the covariance function in the Gaussian process model affect the performance of ε-greedy TS?
- Basis in paper: [inferred] The paper uses the squared exponential covariance function for building GP models but does not investigate the impact of using different covariance functions.
- Why unresolved: The performance of Bayesian optimization methods, including ε-greedy TS, can be sensitive to the choice of the covariance function, which encodes assumptions about the smoothness and structure of the objective function.
- What evidence would resolve it: Experiments comparing the performance of ε-greedy TS with different covariance functions (e.g., Matérn, periodic, rational quadratic) on a range of benchmark functions and real-world problems would provide insights into the impact of the covariance function choice.

### Open Question 4
- Question: Can the value of ε be adapted during the optimization process to improve the performance of ε-greedy TS?
- Basis in paper: [explicit] The paper mentions investigating the adaptability of ε-greedy TS to varying ε during the optimization process as a future direction.
- Why unresolved: The current implementation of ε-greedy TS uses a fixed value of ε throughout the optimization process, and the potential benefits of adapting ε based on the progress of the optimization or the uncertainty in the GP model are not explored.
- What evidence would resolve it: Developing and testing adaptive strategies for updating the value of ε during the optimization process, such as based on the uncertainty in the GP model or the progress of the optimization, and comparing their performance with the fixed ε approach would provide insights into the benefits of adaptive ε.

## Limitations

- The optimal ε value is problem-dependent and requires tuning, which may limit practical applicability
- Performance in high-dimensional spaces is unclear, as the method is only tested on problems up to 10 dimensions
- The computational advantage over generic TS depends on the choice of Ns and may diminish for small Ns

## Confidence

- Claim about ε-greedy TS improving exploitation without sacrificing exploration: Medium confidence
- Claim about computational efficiency approaching generic TS: Medium confidence
- Claim about high GP uncertainty naturally driving exploration: Low confidence

## Next Checks

1. Test ε-greedy TS on high-dimensional problems (d > 10) to assess scalability and GP model accuracy
2. Vary Ns systematically (e.g., Ns ∈ {10, 50, 100}) to quantify the trade-off between exploitation strength and computational cost
3. Compare ε-greedy TS with adaptive ε strategies (e.g., decaying ε) to evaluate robustness across optimization stages