---
ver: rpa2
title: R+R:Understanding Hyperparameter Effects in DP-SGD
arxiv_id: '2411.02051'
source_url: https://arxiv.org/abs/2411.02051
tags:
- learning
- effects
- batch
- hyperparameter
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a replication study to verify claims about
  hyperparameter effects in differentially private stochastic gradient descent (DP-SGD).
  The authors synthesize existing conjectures about how batch size, epochs, learning
  rate, and clipping threshold affect model accuracy, then test these through a large-scale
  factorial experiment across multiple datasets, model architectures, and privacy
  budgets.
---

# R+R:Understanding Hyperparameter Effects in DP-SGD

## Quick Facts
- arXiv ID: 2411.02051
- Source URL: https://arxiv.org/abs/2411.02051
- Reference count: 40
- Key outcome: Replication study verifies learning rate and clipping threshold as most important DP-SGD hyperparameters with inverse interaction, but fails to replicate batch size claims

## Executive Summary
This paper conducts a comprehensive replication study to verify conjectures about hyperparameter effects in differentially private stochastic gradient descent (DP-SGD). The authors synthesize claims from related work about how batch size, epochs, learning rate, and clipping threshold affect model accuracy, then systematically test these through a large-scale factorial experiment across multiple datasets, model architectures, and privacy budgets. Using functional analysis of variance and interpretable machine learning methods, they find that learning rate and clipping threshold are consistently the most important hyperparameters and exhibit a strong inverse interaction effect. However, they cannot consistently replicate claims about batch size being the most important hyperparameter or its interaction with epochs. The study highlights the importance of proper experimental design in understanding hyperparameter effects and suggests that while some insights generalize, others are context-dependent.

## Method Summary
The study employs a factorial experimental design with 3822 hyperparameter tuples evaluated across six datasets (CIFAR-10, SVHN, ImageNette, CIFAR-100, IMDB, NEWS), six model architectures (ResNet-18, ResNet-34, DP-CNN, DenseNet-121, RNN, LSTM), and three privacy budgets (ε = 3, 5, 7.5). Hyperparameters include batch size (16 to N), epochs (25-500), learning rate (1e-5 to 10), and clipping threshold (1e-5 to 10). The authors use Opacus for DP-SGD implementation and analyze results using functional ANOVA to quantify main and interaction effects, supplemented by ICE and ALE plots for visualization.

## Key Results
- Learning rate and clipping threshold are consistently the most important hyperparameters across all scenarios
- These two hyperparameters exhibit a strong inverse interaction effect, with optimal values related by lr·C ≈ const
- Batch size is not the most important hyperparameter, contradicting previous conjectures
- Epochs show task-dependent effects: beneficial for image classification but negligible for text classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning rate and clipping threshold exhibit a strong inverse interaction effect on model accuracy in DP-SGD.
- Mechanism: When learning rate is high, model accuracy is maximized by using a low clipping threshold, and vice versa. This occurs because the product of learning rate and clipping threshold should remain near a constant value for optimal performance, balancing gradient updates with noise amplification.
- Core assumption: The interaction between learning rate and clipping threshold is consistent across different datasets, model architectures, and privacy budgets.
- Evidence anchors:
  - [abstract] "they find that learning rate and clipping threshold are the most important hyperparameters and exhibit a strong interaction effect—their optimal values are inversely related."
  - [section] "we were able to replicate the conjectured relationship between the clipping threshold and learning rate."
- Break Condition: If the optimal product lr·C varies significantly across different model architectures or datasets, the inverse relationship no longer holds as a general rule.

### Mechanism 2
- Claim: Batch size is not the most important hyperparameter for DP-SGD, contrary to previous conjectures.
- Mechanism: While batch size affects privacy budget through privacy amplification, its impact on model accuracy is limited compared to learning rate and clipping threshold. The ALE plots show minimal variance in accuracy attributable to batch size changes.
- Core assumption: The relationship between batch size and accuracy is consistent across different privacy budgets and model architectures.
- Evidence anchors:
  - [abstract] "they cannot consistently replicate claims about batch size being the most important or its interaction with epochs."
  - [section] "based on our results, the batch size does neither exhibit a strong main effect... nor exhibits a consistent effect across datasets or model architectures."
- Break Condition: If specific model architectures or datasets show batch size having a dominant effect on accuracy, the general claim would break down.

### Mechanism 3
- Claim: Number of epochs has a task-dependent effect on model accuracy in DP-SGD.
- Mechanism: For image classification tasks, increasing epochs (up to a threshold) improves accuracy, while for text classification, the effect is negligible or even negative. This suggests that the optimal number of epochs is highly dependent on the specific learning task and model architecture.
- Core assumption: The relationship between epochs and accuracy is monotonic within each task domain.
- Evidence anchors:
  - [section] "based on our results, increasing the number of epochs (up to some threshold) does seem to increase the expected model accuracy for some tasks (i.e. image classification). However, for other tasks (i.e. text classification) the main effect seems to be negligible."
- Break Condition: If certain datasets or architectures show non-monotonic relationships between epochs and accuracy, the task-dependent generalization would break down.

## Foundational Learning

- Concept: Differential Privacy and DP-SGD
  - Why needed here: Understanding how DP-SGD modifies standard SGD through gradient clipping and noise addition is essential to grasp why hyperparameters have different effects compared to non-private learning.
  - Quick check question: What are the two main modifications DP-SGD makes to standard SGD to ensure differential privacy?

- Concept: Functional Analysis of Variance (fANOVA)
  - Why needed here: fANOVA is used to quantify the importance of hyperparameters and their interactions by measuring the variance they explain in model accuracy. This statistical method is crucial for understanding which hyperparameters matter most.
  - Quick check question: How does fANOVA differ from traditional ANOVA when analyzing hyperparameter importance?

- Concept: Individual Conditional Expectation (ICE) and Accumulated Local Effect (ALE) plots
  - Why needed here: These interpretable machine learning methods are used to visualize and understand the main and interaction effects of hyperparameters on model accuracy. They provide insights that go beyond simple importance rankings.
  - Quick check question: What is the key difference between ICE plots and ALE plots in terms of what they reveal about hyperparameter effects?

## Architecture Onboarding

- Component map: Experiment sets (simple image, intermediate image, text classification) -> Hyperparameter space definition -> Factorial experiments -> fANOVA analysis -> ICE/ALE visualization -> Comparison to conjectures
- Critical path: Define hyperparameter ranges → Run factorial experiments → Collect accuracy data → Apply fANOVA to determine importance → Use ICE/ALE to visualize main and interaction effects → Compare results to conjectures from related work
- Design tradeoffs: The choice of using a fractional factorial design balances computational cost with the ability to detect main and interaction effects. Using multiple datasets and architectures increases generalizability but also increases computational requirements.
- Failure signatures: If the regression models have high mean absolute error (>0.1 on scaled accuracy), the analysis may not be reliable. Inconsistent effects across datasets or architectures suggest that the conclusions may not generalize.
- First 3 experiments:
  1. Run a small pilot study with a subset of hyperparameters on CIFAR-10 to verify that the experiment setup is working correctly.
  2. Perform a one-factor-at-a-time (OFAT) experiment on learning rate and clipping threshold to confirm the expected inverse relationship before running the full factorial design.
  3. Execute the full factorial experiment for the simple image classification task and verify that the fANOVA results show learning rate and clipping threshold as the most important hyperparameters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do the hyperparameter effects differ between image and text classification tasks in DP-SGD?
- Basis in paper: [explicit] The authors observe that the batch size has negligible effects on image classification but significant effects on text classification, and the learning rate-clipping threshold interaction differs between domains.
- Why unresolved: The paper does not investigate the underlying reasons for these domain-specific differences, only reports their existence.
- What evidence would resolve it: Comparative studies analyzing gradient dynamics, model architectures, and dataset characteristics across domains could reveal why hyperparameter sensitivity varies.

### Open Question 2
- Question: Can the inverse relationship between learning rate and clipping threshold be formally proven or theoretically derived?
- Basis in paper: [explicit] The authors replicate findings suggesting an inverse relationship (lr·C = const) but note it's weaker for text classification.
- Why unresolved: While empirical evidence supports this relationship, no theoretical framework explains why it emerges or under what conditions it holds.
- What evidence would resolve it: A mathematical analysis of gradient noise, privacy amplification, and convergence dynamics could establish theoretical foundations for this relationship.

### Open Question 3
- Question: What is the optimal approach to balance the number of epochs and batch size in DP-SGD beyond simply minimizing noise?
- Basis in paper: [explicit] The authors find no consistent interaction between epochs and batch size, contradicting the common assumption that large batches should be paired with fewer epochs.
- Why unresolved: The lack of interaction suggests other factors (not just noise) influence the trade-off, but the paper doesn't identify what these factors are.
- What evidence would resolve it: Studies examining the interplay between convergence speed, generalization, and privacy budget consumption across different architectures and datasets could clarify optimal balancing strategies.

## Limitations

- The study's conclusions about hyperparameter effects are based on a specific set of datasets, model architectures, and privacy budgets, which may not generalize to all DP-SGD applications
- The computational constraints necessitated a fractional factorial design, potentially missing some higher-order interactions between hyperparameters
- The analysis relies on regression models to estimate effects, which may not capture all nonlinearities in the relationship between hyperparameters and accuracy

## Confidence

- High confidence: Learning rate and clipping threshold as most important hyperparameters, inverse interaction between them
- Medium confidence: Task-dependent effects of epochs, general importance of hyperparameters over batch size
- Low confidence: Specific numerical thresholds for optimal hyperparameter values across all scenarios

## Next Checks

1. Validate the inverse relationship between learning rate and clipping threshold on additional datasets not used in the original study
2. Conduct targeted experiments to verify the exact shape of the interaction surface between the two most important hyperparameters
3. Test the reproducibility of the findings using a different DP-SGD implementation framework to ensure results are not implementation-specific