---
ver: rpa2
title: Large Language Models Still Face Challenges in Multi-Hop Reasoning with External
  Knowledge
arxiv_id: '2412.08317'
source_url: https://arxiv.org/abs/2412.08317
tags:
- knowledge
- reasoning
- content
- answer
- afraid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the multi-hop reasoning ability of large
  language models (LLMs) using external knowledge. The authors evaluate GPT-3.5 on
  four reasoning benchmarks under Chain-of-Thought prompting.
---

# Large Language Models Still Face Challenges in Multi-Hop Reasoning with External Knowledge

## Quick Facts
- arXiv ID: 2412.08317
- Source URL: https://arxiv.org/abs/2412.08317
- Authors: Haotong Zhang
- Reference count: 40
- Key outcome: This paper investigates the multi-hop reasoning ability of large language models (LLMs) using external knowledge. The authors evaluate GPT-3.5 on four reasoning benchmarks under Chain-of-Thought prompting. Key findings include: LLMs perform poorly without external knowledge, adding external knowledge improves performance but distractors still degrade results, providing inconsistent (counterfactual) knowledge confuses models, LLMs struggle with non-sequential reasoning paths, and they show limited generalization to samples requiring more reasoning hops. The study reveals significant limitations in current LLMs' reasoning capabilities, highlighting the gap between model performance and human-level reasoning.

## Executive Summary
This paper investigates the multi-hop reasoning ability of large language models using external knowledge. The author evaluates GPT-3.5 (text-davinci-002) on four reasoning benchmarks (HotpotQA, EntailmentBank, QASC, bAbI15) using Chain-of-Thought prompting with exemplars. The experiments reveal that LLMs perform poorly without external knowledge, but adding relevant knowledge significantly improves performance. However, LLMs struggle with distractors, inconsistent (counterfactual) knowledge, non-sequential reasoning paths, and generalization to tasks requiring more reasoning hops. These findings highlight significant limitations in current LLMs' reasoning capabilities and the gap between model performance and human-level reasoning.

## Method Summary
The study evaluates GPT-3.5 (text-davinci-002) on four reasoning benchmarks using few-shot learning with Chain-of-Thought prompting. The experiments compare model performance with and without external knowledge, with/without distractors, and with counterfactual knowledge. The author uses accuracy, exact match (EM), OneHit, precision, and recall as evaluation metrics. The experimental setup involves constructing prompts with exemplars and external knowledge, running the model on test samples, and analyzing the results to identify patterns of success and failure.

## Key Results
- LLMs perform poorly without external knowledge but improve significantly when provided with relevant knowledge.
- Adding distractors degrades performance, but LLMs still outperform their internal knowledge-only baseline.
- Providing inconsistent (counterfactual) knowledge confuses models, leading to hallucinations or ignoring the new information.
- LLMs struggle with non-sequential reasoning paths even with Chain-of-Thought prompting.
- LLMs show limited generalization to samples requiring more reasoning hops than the exemplars provided.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can perform reasoning with external knowledge but struggle when that knowledge is inconsistent with their internal knowledge.
- Mechanism: The model uses external knowledge to supplement its internal knowledge during reasoning, but when presented with counterfactual information, it becomes confused and either hallucinates or ignores the new information.
- Core assumption: LLMs have a strong tendency to rely on their internal knowledge, even when external knowledge is provided.
- Evidence anchors:
  - [abstract] "providing inconsistent (counterfactual) knowledge confuses models"
  - [section] "providing knowledge inconsistent with the model's internal knowledge causes great confusion"
  - [corpus] No direct evidence in corpus; assumption based on experimental observations
- Break condition: If the external knowledge is overwhelmingly convincing or the internal knowledge is weak/absent, the model might rely more on the external information.

### Mechanism 2
- Claim: LLMs struggle with non-sequential reasoning paths, even with Chain-of-Thought prompting.
- Mechanism: The model attempts to follow the sequential reasoning format provided in the prompt but fails to understand the non-linear relationships between facts, leading to incorrect reasoning paths.
- Core assumption: LLMs are primarily trained on sequential text data and may not inherently understand complex, non-sequential relationships.
- Evidence anchors:
  - [abstract] "LLMs struggle with non-sequential reasoning paths"
  - [section] "LLMs still struggle with problem decomposition and reasoning path generation on non-sequential reasoning tasks even with chain-of-thought prompting"
  - [corpus] No direct evidence in corpus; assumption based on experimental observations
- Break condition: If the non-sequential reasoning task can be broken down into smaller, sequential sub-tasks, the model might perform better.

### Mechanism 3
- Claim: LLMs have limited generalization ability when the number of reasoning hops increases.
- Mechanism: The model tries to imitate the format of the exemplars in the prompt but fails to truly understand the concept of decomposition, leading to over-decomposition or wrong decomposition in multi-hop reasoning tasks.
- Core assumption: Few-shot learning through prompting primarily teaches the model to mimic the format rather than understand the underlying concepts.
- Evidence anchors:
  - [abstract] "they show limited generalization to samples requiring more reasoning hops"
  - [section] "LLMs only exhibit limited generalization ability on the number of reasoning hops"
  - [corpus] No direct evidence in corpus; assumption based on experimental observations
- Break condition: If the model is provided with exemplars that closely match the complexity of the test samples, its performance might improve.

## Foundational Learning

- Concept: Internal vs. External Knowledge
  - Why needed here: Understanding the difference between knowledge stored in the model's parameters (internal) and additional information provided for specific tasks (external) is crucial for interpreting the experimental results.
  - Quick check question: What is the main advantage of using external knowledge in LLM reasoning tasks?

- Concept: Chain-of-Thought Prompting
  - Why needed here: This technique is used throughout the experiments to guide the LLM's reasoning process. Understanding how it works is essential for interpreting the results and limitations.
  - Quick check question: How does Chain-of-Thought prompting help LLMs perform better on reasoning tasks?

- Concept: Multi-hop Reasoning
  - Why needed here: The paper focuses on LLMs' ability to perform reasoning that requires combining multiple pieces of information. Understanding this concept is crucial for grasping the experimental setup and results.
  - Quick check question: What distinguishes multi-hop reasoning from single-hop reasoning?

## Architecture Onboarding

- Component map:
  - LLM (text-davinci-002) -> Prompt templates (various for different experiments) -> External knowledge sources (HotpotQA, EntailmentBank, QASC, bAbI15 datasets) -> Evaluation metrics (accuracy, precision, recall, EM, OneHit)

- Critical path:
  1. Load LLM and datasets
  2. Construct prompts based on experiment type
  3. Run LLM on test samples with constructed prompts
  4. Evaluate results using appropriate metrics
  5. Analyze errors and identify patterns

- Design tradeoffs:
  - Using few-shot learning (prompting) vs. fine-tuning: Prompting is less resource-intensive but may have lower performance and generalization.
  - Providing external knowledge vs. relying on internal knowledge: External knowledge can improve accuracy but may introduce confusion if inconsistent.
  - Using complex reasoning tasks vs. simpler tasks: More complex tasks better reveal limitations but may be harder to interpret.

- Failure signatures:
  - Hallucinations: Model generates information not present in the provided knowledge or its internal knowledge.
  - Incorrect knowledge selection: Model chooses irrelevant knowledge even when relevant knowledge is available.
  - Wrong decomposition: Model fails to break down complex problems into manageable sub-problems.
  - Over-reliance on internal knowledge: Model ignores external knowledge even when it's more relevant.

- First 3 experiments:
  1. Test LLM performance on HotpotQA with and without external knowledge to observe the impact of external information.
  2. Provide inconsistent (counterfactual) knowledge in QASC to observe how the model handles conflicting information.
  3. Test LLM on non-sequential reasoning samples from EntailmentBank to evaluate its ability to handle complex reasoning paths.

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies heavily on a single LLM model (text-davinci-002) and a limited number of samples (100 per experiment), which may not fully capture the generalizability of the results across different model architectures or larger datasets.
- The exact exemplars used in Chain-of-Thought prompting are not fully specified, which could impact reproducibility.
- The paper doesn't address potential confounding factors such as prompt phrasing variations or temperature settings during inference.

## Confidence
- High Confidence: LLMs perform poorly without external knowledge - Supported by direct comparisons in experiments with clear performance drops.
- Medium Confidence: LLMs struggle with non-sequential reasoning paths - Based on experimental observations, but the underlying mechanisms are not fully explored.
- Low Confidence: LLMs have limited generalization to samples requiring more reasoning hops - The evidence suggests a pattern, but the exact cause (format imitation vs. conceptual understanding) is not conclusively determined.

## Next Checks
1. **Cross-Model Validation**: Repeat the experiments with multiple LLM architectures (e.g., GPT-4, Claude, LLaMA) to assess if the observed limitations are consistent across different models or specific to text-davinci-002.

2. **Controlled Knowledge Experiments**: Design experiments where the external knowledge provided is systematically varied in terms of relevance and consistency with the model's internal knowledge. Measure how performance changes when external knowledge is partially correct, completely irrelevant, or overwhelmingly convincing.

3. **Reasoning Path Analysis**: Implement a detailed analysis of the reasoning paths generated by the model, particularly for non-sequential tasks. Use techniques like path visualization or step-by-step correctness checking to identify exactly where the model's reasoning breaks down and whether this correlates with specific types of non-linear relationships.