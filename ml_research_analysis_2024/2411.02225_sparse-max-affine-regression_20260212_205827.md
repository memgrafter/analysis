---
ver: rpa2
title: Sparse Max-Affine Regression
arxiv_id: '2411.02225'
source_url: https://arxiv.org/abs/2411.02225
tags:
- lemma
- sparse
- where
- theorem
- sp-gd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies variable selection in convex piecewise linear
  regression, where the model is given as the maximum of $k$ affine functions $x \mapsto
  \max{j \in [k]} \langle aj^\star, x \rangle + bj^\star$. The proposed Sparse Gradient
  Descent (Sp-GD) algorithm solves this problem under sparsity constraints on the
  weight vectors.
---

# Sparse Max-Affine Regression

## Quick Facts
- **arXiv ID:** 2411.02225
- **Source URL:** https://arxiv.org/abs/2411.02225
- **Reference count:** 40
- **Primary result:** Sp-GD achieves ε-accurate parameter estimates from O(max(ε⁻²σ_z², 1)s log(d/s)) observations under sparsity and sub-Gaussian assumptions

## Executive Summary
This paper introduces Sparse Gradient Descent (Sp-GD) for variable selection in convex piecewise linear regression models of the form y = max_{j∈[k]} ⟨a_j⋆, x⟩ + b_j⋆ + z. The algorithm solves the non-convex sparse max-affine regression problem under joint sparsity constraints on the weight vectors. Theoretical guarantees show that Sp-GD achieves ε-accurate parameter estimates with sample complexity that scales favorably with the sparsity level s rather than the ambient dimension d. The paper also introduces Real Maslov Dequantization (RMD) to transform sparse generalized polynomials into sparse max-affine models, extending the theoretical guarantees to polynomial regression.

## Method Summary
The proposed method consists of two main components: the Sp-GD algorithm and an initialization scheme. Sp-GD performs projected gradient descent using generalized gradients and adaptive step sizes inversely proportional to the empirical probability that each linear model achieves the maximum. The initialization scheme uses sparse PCA to estimate the subspace spanned by the jointly sparse weight vectors, followed by an r-covering search to provide an initial estimate within the basin of attraction. For polynomial models, the RMD transformation converts them to max-affine form before applying Sp-GD. The method requires sub-Gaussian covariates with anti-concentration properties and assumes joint sparsity of the weight vectors.

## Key Results
- Sp-GD achieves ε-accurate parameter estimates from O(max(ε⁻²σ_z², 1)s log(d/s)) observations
- Initialization scheme provides ε-accurate starting points from O(ε⁻² max(σ_z⁴, σ_z², 1)s² log⁴(d)) observations
- RMD transformation converts sparse generalized polynomials to max-affine models with exponentially decaying error in temperature parameter ς
- Numerical results demonstrate phase transitions consistent with theoretical sample complexity bounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sp-GD converges linearly to an ε-accurate estimate under sub-Gaussian noise when initialized within the basin of attraction.
- Mechanism: The algorithm uses a generalized gradient with adaptive step sizes inversely proportional to the empirical probability πt_j that the j-th linear model achieves the maximum. This step size concentrates around the inverse of the true selection probability, enabling stable descent in the non-convex landscape.
- Core assumption: Covariates satisfy sub-Gaussianity and anti-concentration properties; initialization is sufficiently close to the true parameters.
- Evidence anchors:
  - [abstract] "A non-asymptotic local convergence analysis is provided for Sp-GD under sub-Gaussian noise when the covariate distribution satisfies the sub-Gaussianity and anti-concentration properties."
  - [section 2.2] "Since the nonlinear least squares problem for the model in (1) is non-convex, Sp-GD provides a local convergence guarantee presented in the following pseudo-theorem."
  - [corpus] Weak - no direct evidence from neighbors about max-affine convergence.

### Mechanism 2
- Claim: The initialization scheme using sparse PCA (sPCA) and r-covering search provides an ε-accurate estimate from O(ε⁻² max(σ⁴_z, σ²_z, 1)s² log⁴(d)) observations when weight vectors are jointly s-sparse.
- Mechanism: sPCA estimates the joint support and subspace spanned by the weight vectors by solving a semidefinite program over the Fantope with an ℓ₁ penalty. This reduces the ambient dimension from d to s in the subspace estimation phase. The r-covering search then samples candidate parameters from this reduced subspace.
- Core assumption: The ground-truth weight vectors {a⋆_j} are jointly supported on a set of cardinality ≤ s (joint sparsity).
- Evidence anchors:
  - [abstract] "The initialization scheme uses sparse principal component analysis to estimate the subspace spanned by {a⋆_j}k_j=1, then applies an r-covering search to estimate the model parameters."
  - [section 3.1] "To retain the gain due to Theorem 2.3, we propose an initialization scheme modified from the spectral method by [Ghosh et al., 2021] that provides an error rate depending on s instead of d."
  - [corpus] Weak - no direct evidence from neighbors about sparse PCA for max-affine initialization.

### Mechanism 3
- Claim: Real Maslov Dequantization (RMD) transforms sparse generalized polynomials into sparse max-affine models with exponentially decaying error in the temperature parameter ς.
- Mechanism: RMD applies the transformation y = Re{ς log w} and x_l = ς log u_l to convert the polynomial model into a max-affine form. The quantization error z_ς decays as O(exp(-√ϑ∆/ς)), allowing the sparse max-affine regression machinery to learn the exponents and coefficients.
- Core assumption: The covariate x after transformation satisfies sub-Gaussianity and anti-concentration; ς is sufficiently small.
- Evidence anchors:
  - [abstract] "A new transformation named Real Maslov Dequantization (RMD) is proposed to transform sparse generalized polynomials into sparse max-affine models. The error decay rate of RMD is shown to be exponentially small in its temperature parameter."
  - [section 4] "Building on the non-asymptotic theory in Theorem 2.3, we can show the analysis of estimating the exponents in (4)."
  - [corpus] Weak - no direct evidence from neighbors about Maslov dequantization for polynomial approximation.

## Foundational Learning

- **Sub-Gaussian distributions and anti-concentration properties**
  - Why needed here: These properties ensure that the empirical measure of polytopes concentrates around their expectations, which is critical for bounding the operator norms and selection probabilities in the convergence analysis.
  - Quick check question: What is the relationship between the sub-Gaussian norm of a random vector and the concentration of its empirical covariance?

- **Generalized gradients for non-smooth optimization**
  - Why needed here: Max-affine functions are piecewise linear and non-smooth, so standard gradients don't exist. The generalized gradient (Clarke gradient) provides a valid descent direction for the projected gradient descent algorithm.
  - Quick check question: How does the generalized gradient differ from the subgradient in non-smooth optimization?

- **Sparse PCA and the Fantope relaxation**
  - Why needed here: The joint sparsity structure allows dimensionality reduction from d to s in the subspace estimation phase, dramatically improving sample complexity. The Fantope relaxation provides a convex surrogate for the sparse PCA problem.
  - Quick check question: Why is the Fantope (set of k-dimensional projection matrices) used as the constraint set in the sparse PCA relaxation?

## Architecture Onboarding

- **Component map:**
  - Data generation: Covariates x ∈ R^d, noise z, target y = max_j ⟨a⋆_j, x⟩ + b⋆_j + z
  - Sp-GD algorithm: Projected gradient descent with generalized gradients and adaptive step sizes
  - Initialization: sPCA on moment matrix → r-covering search → initial parameter estimate
  - RMD transformation: For polynomial models, transform to max-affine form before applying Sp-GD
  - Evaluation: Relative error between estimated and true parameters, Monte Carlo simulations

- **Critical path:**
  1. Generate data from max-affine or transformed polynomial model
  2. Apply initialization (sPCA + r-covering)
  3. Run Sp-GD with adaptive step sizes
  4. Evaluate estimation error
  5. Verify theoretical guarantees empirically

- **Design tradeoffs:**
  - Sp-GD vs plain GD/SGD: Sp-GD handles sparsity and non-smoothness but requires careful initialization
  - sPCA vs PCA: sPCA exploits joint sparsity for better sample complexity but requires solving a SDP
  - RMD vs direct polynomial regression: RMD enables using max-affine machinery but introduces approximation error

- **Failure signatures:**
  - Sp-GD diverges: Initialization outside basin of attraction, violated anti-concentration assumption
  - High estimation error: Insufficient samples for initialization, incorrect choice of r parameter
  - RMD approximation poor: Temperature parameter ς too large, violated sub-Gaussian assumption after transformation

- **First 3 experiments:**
  1. Verify Sp-GD convergence on a small synthetic max-affine problem with known parameters and varying noise levels
  2. Test the initialization scheme's subspace estimation accuracy for different sparsity levels s and ambient dimensions d
  3. Apply RMD to a simple polynomial model and verify the exponential decay of the approximation error as ς decreases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the r-covering search in the initialization scheme be made more practical for small separation parameters r?
- Basis in paper: [explicit] "One noteworthy limitation of the initialization is the r-covering search inherited from the previous work on non-sparse max-affine regression [Ghosh et al., 2021]. The cost of constructing the r-covering and the exhaustive search over it increases exponentially in the subspace dimension."
- Why unresolved: The paper identifies this as a limitation but does not provide alternative solutions or algorithms that avoid the r-covering search while maintaining theoretical guarantees.
- What evidence would resolve it: A new initialization algorithm that provides theoretical performance guarantees without requiring r-covering search, or empirical results showing that random sampling from the estimated subspace works as well as r-covering search for practical values of r.

### Open Question 2
- Question: Can Sp-GD be analyzed from random initialization to extend the known theoretical results on single-index models?
- Basis in paper: [explicit] "Alternatively, it would also be intriguing to explore a potential analysis of Sp-GD from random initialization that will extend the known theoretical results on single-index models [Tan and Vershynin, 2019, Chandrasekher et al., 2022]."
- Why unresolved: The paper only provides local convergence guarantees that require a suitable initial estimate within the basin of attraction. Random initialization analysis could provide stronger guarantees but remains unexplored.
- What evidence would resolve it: Theoretical analysis showing that Sp-GD converges to the ground truth parameters with high probability when initialized randomly, or empirical results demonstrating successful convergence from random initialization across various problem settings.

### Open Question 3
- Question: How does additive sub-Gaussian noise in the generalized polynomial domain affect the error bounds after RMD transformation?
- Basis in paper: [explicit] "On the other hand, assuming additive sub-Gaussian noise in the generalized polynomial domain requires further investigation."
- Why unresolved: The paper analyzes the dequantization error from RMD transformation but only considers the case where distortion is due to RMD alone. The interaction between additive noise in the original domain and the RMD transformation is not addressed.
- What evidence would resolve it: Theoretical analysis showing how additive sub-Gaussian noise in the generalized polynomial domain propagates through the RMD transformation and affects the final estimation error, including modified sample complexity bounds that account for both RMD error and noise.

## Limitations

- The favorable sample complexity O(s log(d/s)) relies critically on the joint sparsity assumption, which may not hold in many practical settings
- The initialization scheme requires an r-covering search with exponential computational cost in the subspace dimension
- The theoretical guarantees are local, requiring initialization within a specific basin of attraction around the true parameters

## Confidence

- **High Confidence:** The local convergence guarantee for Sp-GD under sub-Gaussian noise and anti-concentration properties. The theoretical framework for max-affine regression is well-established, and the adaptive step size mechanism is sound.
- **Medium Confidence:** The initialization scheme's sample complexity bound. While the sPCA approach is theoretically justified, the practical performance depends heavily on the joint sparsity assumption and the choice of r parameter.
- **Low Confidence:** The exponential decay of the RMD approximation error. The analysis assumes specific conditions on the covariates after transformation, and the practical implementation details for achieving the theoretical error rates are not fully specified.

## Next Checks

1. **Robustness to sparsity assumption:** Test Sp-GD with non-jointly sparse weight vectors to quantify the degradation in sample complexity and estimation error.
2. **Initialization sensitivity:** Systematically vary the r parameter in the r-covering search and measure its impact on the initialization error and subsequent convergence of Sp-GD.
3. **RMD approximation quality:** Experimentally verify the exponential decay of the quantization error as a function of ς for different polynomial models and covariate distributions.