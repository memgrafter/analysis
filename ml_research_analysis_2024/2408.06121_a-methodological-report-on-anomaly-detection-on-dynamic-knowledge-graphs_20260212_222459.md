---
ver: rpa2
title: A Methodological Report on Anomaly Detection on Dynamic Knowledge Graphs
arxiv_id: '2408.06121'
source_url: https://arxiv.org/abs/2408.06121
tags:
- data
- anomaly
- detection
- dynamic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses anomaly detection in dynamic knowledge graphs\
  \ (DKGs) for Kubernetes-based micro-services environments. It proposes three data\
  \ representations\u2014sequential, hierarchical, and inter-service dependency\u2014\
  to capture increasingly complex structural information in DKGs."
---

# A Methodological Report on Anomaly Detection on Dynamic Knowledge Graphs

## Quick Facts
- arXiv ID: 2408.06121
- Source URL: https://arxiv.org/abs/2408.06121
- Reference count: 40
- The paper proposes three data representations for DKGs and achieves F1 score of 0.514 vs baseline 0.068 on ISWC 2024 ADDKG dataset.

## Executive Summary
This paper addresses anomaly detection in dynamic knowledge graphs (DKGs) for Kubernetes-based micro-services environments. It proposes three data representations—sequential, hierarchical, and inter-service dependency—to capture increasingly complex structural information in DKGs. Machine learning and deep learning models, including SVM, XGB, and self-attention mechanisms, are tested on these representations. The approach significantly improves performance over baseline methods on the ISWC 2024 ADDKG dataset, achieving an F1 score of 0.514 compared to the baseline of 0.068. Ensemble learning combining SVM, XGB, and self-attention with unanimous voting further enhances precision and recall, demonstrating the effectiveness of integrating graph structures and advanced models for anomaly detection in dynamic and complex systems.

## Method Summary
The methodology involves three key data representations for DKGs: sequential (temporal attributes aggregated into feature matrices), hierarchical (multi-hop structural-temporal information from Node to Service), and inter-service dependency (self-attention on service interactions). Models tested include SVM, XGB, LSTM, GRU, TCN, and Self-Attention. The pipeline processes TTL format RDF triples, constructs the three representations, trains individual models with class-weighted loss for imbalanced data (4% anomalies), and combines top performers through ensemble voting (hard-majority, hard-unanimous, soft voting).

## Key Results
- F1 score of 0.514 achieved using ensemble voting on hierarchical and inter-service representations
- Sequential representation baseline F1 score of 0.068 significantly outperformed
- Unanimous voting ensemble of SVM, XGB, and Self-Attention yields best overall performance
- Class weighting effectively addresses extreme class imbalance in ADDKG dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential data representation aggregates temporal attributes of all node types into a unified feature matrix for each timestamp.
- Mechanism: By concatenating time-evolving attributes of Node, Pod, and Service categories, the model receives a complete temporal view of system states without preserving explicit graph topology.
- Core assumption: The temporal sequence of attribute values alone contains sufficient information to distinguish anomalous from normal system behavior.
- Evidence anchors:
  - [section] "we can get a˜X∈RT×D, where D = DN ode + DP od + DService =∑3
k=1 Nk∗dk"
  - [section] "A sliding window then captures temporal dependencies by concatenating the historical information of the past H time steps"
- Break condition: If anomalies depend heavily on structural relationships (e.g., missing edge patterns) rather than purely temporal attribute changes, this representation will fail.

### Mechanism 2
- Claim: Hierarchical aggregation propagates attribute information from lower-level nodes (Node, Pod) up to higher-level nodes (Service), preserving multi-hop structural relationships.
- Mechanism: For each Service instance, attributes are first aggregated from its child Pods, then from those Pods' parent Nodes, using mean/max/min operations, creating a structural-temporal representation.
- Core assumption: Multi-hop neighborhood information is essential for detecting anomalies that manifest through indirect dependencies rather than direct node attributes.
- Evidence anchors:
  - [section] "we can progressively aggregate their instances' time-evolving attributes from Nodes to Services using aggregation methods such as mean, max, and min"
  - [section] "This aggregation process is analogous to a multi-hop operation, where information is aggregated from multiple hops away from the target Service"
- Break condition: If anomalies are localized to individual nodes or edges without propagation through the hierarchy, this multi-hop aggregation may dilute relevant signals.

### Mechanism 3
- Claim: Inter-service dependencies captured through self-attention mechanisms allow the model to weigh the importance of relationships between Service instances.
- Mechanism: Self-attention computes relevance scores between all pairs of Service representations at each timestamp, allowing the model to focus on the most informative service interactions for anomaly detection.
- Core assumption: Anomalies often emerge from abnormal patterns of interaction between services rather than from individual service behavior alone.
- Evidence anchors:
  - [section] "SA works by computing the relevance (or attention) between each pair of elements in a sequence, enabling the model to weigh the importance of each element relative to others"
  - [section] "Another method is adding implicitly the inter-service dependencies using the model layers, like Self-Attention, LSTM to capture the correlations among vertices during the model training"
- Break condition: If inter-service correlations are weak or non-existent, or if the attention mechanism overfits to normal interaction patterns, anomaly detection performance will degrade.

## Foundational Learning

- Concept: Knowledge Graph (KG) representation with RDF triples (subject, predicate, object)
  - Why needed here: The ADDKG dataset uses TTL format, which is a textual syntax for RDF that represents KGs as triples, requiring understanding of this data structure to extract features
  - Quick check question: What are the three components of an RDF triple in the context of DKGs?

- Concept: Dynamic Graph vs Static Graph representations
  - Why needed here: The paper distinguishes between CTDG (continuous-time) and DTDG (discrete-time) dynamic graphs, with DTDG being used as a sequence of static graph snapshots for this problem
  - Quick check question: How does the paper model the DKG dataset in terms of time representation?

- Concept: Imbalanced classification and class weights
  - Why needed here: The dataset has only 4% anomalies, requiring class weighting in loss functions to prevent the model from always predicting normal
  - Quick check question: What technique does the paper use to handle the extreme class imbalance in the ADDKG dataset?

## Architecture Onboarding

- Component map: TTL parsing → attribute extraction → sequential/hierarchical/inter-service representation construction → model training (with validation split) → ensemble voting → final prediction
- Critical path: TTL parsing → feature construction (D1/D2/D3) → model training (with validation split) → ensemble voting → final prediction
- Design tradeoffs: Sequential representation (simple but loses structure) vs Hierarchical (preserves multi-hop relationships but increases dimensionality) vs Inter-service (captures interactions but computationally expensive)
- Failure signatures: Poor recall despite high precision (model too conservative), F1 score similar to baseline (representation/model ineffective), model fails to detect any anomalies (training data issue or hyperparameter problem)
- First 3 experiments:
  1. Train SVM on sequential data (D1) with default parameters to establish baseline performance
  2. Train Self-Attention on hierarchical data (D2) with 2-hop aggregation to test structural benefit
  3. Implement unanimous voting ensemble of SVM, XGB, and SA on D2+D3 to verify ensemble effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed ensemble methods perform with a larger and more diverse set of individual models?
- Basis in paper: [explicit] The paper discusses combining SVM, XGB, and Self-Attention models using ensemble methods, achieving the best performance with unanimous voting.
- Why unresolved: The study only tested a limited set of models and voting mechanisms due to daily submission limits and computational resource constraints.
- What evidence would resolve it: Testing the ensemble methods with a broader range of models and voting mechanisms on the same dataset.

### Open Question 2
- Question: Can graph neural networks (GNNs) outperform the current deep learning models (MLP, TCN, LSTM, GRU, Self-Attention) in this anomaly detection task?
- Basis in paper: [inferred] The paper mentions that the application of anomaly detection to dynamic knowledge graphs is limited and suggests that GNNs remain a promising direction for further investigation.
- Why unresolved: The study did not incorporate GNNs in its methodology or experiments.
- What evidence would resolve it: Implementing and comparing GNN-based models with the current deep learning models on the same dataset.

### Open Question 3
- Question: How would the performance of the proposed approach change if it were applied to real-time anomaly detection in dynamic environments?
- Basis in paper: [explicit] The paper suggests that integrating real-time anomaly detection would benefit practical applications in dynamic environments.
- Why unresolved: The experiments were conducted on a static dataset, and the paper does not provide evidence of real-time performance.
- What evidence would resolve it: Testing the approach on streaming data with real-time processing requirements and comparing its performance to the static case.

## Limitations
- Evaluation based on single proprietary dataset with limited external validation
- Ensemble voting mechanism lacks ablation studies to quantify individual model contributions
- Hyperparameter sensitivity analysis is absent, leaving performance robustness questions
- Assumes temporal attribute sequences contain sufficient anomaly signals without systematic validation

## Confidence

- **High Confidence**: The three data representation mechanisms (sequential, hierarchical, inter-service) are clearly defined and technically sound based on the described aggregation methods.
- **Medium Confidence**: Performance improvements over baseline (F1: 0.514 vs 0.068) are substantial but require independent validation on alternative datasets to confirm robustness.
- **Low Confidence**: Claims about ensemble voting's superiority lack rigorous statistical significance testing and hyperparameter optimization documentation.

## Next Checks

1. **Dataset Generalization Test**: Apply the complete pipeline (all three representations + ensemble voting) to an independently collected DKG dataset from a different micro-services environment to verify transfer learning capabilities.
2. **Structural vs Temporal Ablation**: Design experiments that systematically remove temporal features while preserving graph structure (and vice versa) to quantify the relative importance of each information type for anomaly detection.
3. **Hyperparameter Sensitivity Analysis**: Conduct grid searches or Bayesian optimization across key hyperparameters (window size H, attention head count, ensemble voting thresholds) to establish performance bounds and identify optimal configurations.