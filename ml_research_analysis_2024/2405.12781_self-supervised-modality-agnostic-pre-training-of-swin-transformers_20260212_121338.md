---
ver: rpa2
title: Self-Supervised Modality-Agnostic Pre-Training of Swin Transformers
arxiv_id: '2405.12781'
source_url: https://arxiv.org/abs/2405.12781
tags:
- swin
- medical
- segmentation
- pre-training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SwinFUSE, a modality-agnostic self-supervised
  pre-training approach for medical image segmentation. The core idea is to pre-train
  a Swin Transformer on both CT and MRI data, learning complementary features via
  a Domain Invariance Module (DIM) that highlights salient regions.
---

# Self-Supervised Modality-Agnostic Pre-Training of Swin Transformers

## Quick Facts
- arXiv ID: 2405.12781
- Source URL: https://arxiv.org/abs/2405.12781
- Authors: Abhiroop Talasila; Maitreya Maity; U. Deva Priyakumar
- Reference count: 0
- Key outcome: SwinFUSE achieves 27% improvement on out-of-distribution modalities while maintaining 1-2% performance on in-distribution tasks

## Executive Summary
This paper proposes SwinFUSE, a self-supervised pre-training approach for medical image segmentation that learns modality-agnostic features from both CT and MRI data. The method uses a Domain Invariance Module (DIM) with co-attention to highlight salient regions across modalities, combined with multi-task pre-training using contrastive learning, masked volume inpainting, and 3D rotation prediction. When evaluated on BraTS21 and MSD datasets, SwinFUSE demonstrates superior generalization to unseen modalities while maintaining competitive performance on in-distribution tasks.

## Method Summary
SwinFUSE pre-trains a Swin Transformer on paired CT and MRI volumes using a Domain Invariance Module (DIM) that implements co-attention between modalities. The DIM consists of two parallel 4-layer multi-head attention blocks where each modality's query attends to the other's key and value, forcing the model to extract shared anatomical features. Pre-training employs three proxy tasks: contrastive learning for global representation, masked volume inpainting for local reconstruction, and 3D rotation prediction for geometric invariance. A density matching loss using kernel density estimation and Jensen-Shannon divergence regularizes the feature space across modalities. The pre-trained encoder is then fine-tuned for segmentation on target datasets.

## Key Results
- Achieves 27% improvement on out-of-distribution modalities compared to single-modality models
- Maintains modest 1-2% performance trade-off on in-distribution tasks
- Demonstrates superior generalizability for real-world medical imaging applications
- Outperforms nnUNet and standard Swin UNETR on cross-modal transfer tasks

## Why This Works (Mechanism)

### Mechanism 1
The Domain Invariance Module (DIM) improves cross-modal generalization by learning modality-invariant attention weights that highlight salient regions. The DIM uses co-attention across CT and MRI embeddings, computing query-key-value attention where each modality's query attends to the other's key and value. This forces the model to extract features that are relevant regardless of modality origin. Core assumption: Attention weights that highlight consistent anatomical regions across modalities can be learned without explicit correspondence labels.

### Mechanism 2
Multi-task pre-training (contrastive learning + masked volume inpainting + rotation prediction) enables robust feature learning transferable to segmentation. By combining these proxy tasks, the model learns spatial context, local reconstruction, and global orientation invariance simultaneously, producing representations that encode both semantic and geometric information. Core assumption: Proxy tasks designed for natural images transfer meaningfully to 3D medical volumes.

### Mechanism 3
Density matching via kernel density estimation and Jensen-Shannon divergence regularizes the feature space to improve domain alignment. KDE estimates feature densities from the MHA block outputs, and JSD loss minimizes the divergence between density estimates from CT and MRI, pushing the model toward a shared feature distribution. Core assumption: Feature distributions from different modalities can be meaningfully aligned without explicit domain labels.

## Foundational Learning

- Concept: Vision Transformers and self-attention
  - Why needed here: SwinFUSE builds directly on Swin Transformer architecture; understanding patch embeddings, multi-head attention, and shifted window mechanisms is essential to grasp how the DIM integrates.
  - Quick check question: What is the difference between global self-attention in ViT and local self-attention in Swin Transformer?

- Concept: Self-supervised learning objectives (contrastive, reconstruction, pretext)
  - Why needed here: The paper combines multiple SSL objectives; understanding how these proxy tasks shape feature learning is key to interpreting results.
  - Quick check question: How does contrastive learning encourage invariance to augmentations while preserving discriminative information?

- Concept: Domain adaptation and generalization
  - Why needed here: The paper's main contribution is improving out-of-distribution performance; knowing the distinction between domain shift, domain adaptation, and generalization is critical.
  - Quick check question: What is the difference between domain adaptation and domain generalization?

## Architecture Onboarding

- Component map: Input 3D volumes → Patch embedding layer → DIM (co-attention) → Swin Transformer encoder → Proxy task heads (pre-training) OR Segmentation decoder (fine-tuning)
- Critical path: Augmentation → Patch embedding → DIM → Swin Encoder → Proxy task heads (pre-training) OR Segmentation decoder (fine-tuning)
- Design tradeoffs: Multi-task pre-training vs. task-specific fine-tuning balances generality with specialization; Co-attention DIM vs. modality-specific encoders trades model complexity for shared representation learning; KDE/JSD regularization vs. raw task loss adds domain alignment but may hurt in-distribution accuracy
- Failure signatures: If DIM collapses to identity (no modality interaction), co-attention provides no benefit; If KDE bandwidth is mis-estimated, JSD loss may dominate or vanish; If proxy tasks are misaligned with segmentation, fine-tuning may require extensive retraining
- First 3 experiments:
  1. Verify DIM co-attention: Feed paired CT/MRI patches, check if attention weights differ meaningfully between modalities
  2. Test proxy task balance: Train with only contrastive, only inpainting, only rotation; compare learned representations
  3. Validate density matching: Visualize feature distributions from CT vs. MRI before/after JSD loss; check for alignment

## Open Questions the Paper Calls Out

### Open Question 1
How does SwinFUSE's performance compare to state-of-the-art self-supervised learning methods on medical image segmentation tasks? The paper compares SwinFUSE to nnUNet and Swin UNETR, which are single-modality models, but does not compare to other self-supervised learning methods.

### Open Question 2
How does the performance of SwinFUSE change when pre-trained on a larger and more diverse dataset? The paper mentions that SwinFUSE is pre-trained on a subset of 180 patients from the SynthRad dataset, which is relatively small compared to other medical image datasets.

### Open Question 3
How does the performance of SwinFUSE change when using different backbone architectures, such as 3D-ResNet or DenseNet? The paper uses the Swin Transformer as the backbone architecture for SwinFUSE, but does not explore the impact of using different backbone architectures on its performance.

## Limitations
- Limited validation on truly unseen modalities beyond CT/MRI
- No ablation studies isolating DIM contribution versus parameter sharing
- KDE-based density matching introduces sensitive hyperparameters without sensitivity analysis

## Confidence

- Mechanism 1 (DIM co-attention): Medium - The design is novel but lacks ablation studies isolating its contribution
- Mechanism 2 (Multi-task pre-training): High - Well-established SSL principles support this combination
- Mechanism 3 (Density matching): Low - Novel approach with no direct comparison to simpler alternatives

## Next Checks

1. **DIM Ablation with Single-Modality Training**: Train identical models with DIM disabled and modality-specific encoders to quantify the exact contribution of the co-attention mechanism versus simple parameter sharing.

2. **Cross-Modality Feature Visualization**: Extract and visualize intermediate feature maps from CT and MRI samples, measuring their alignment via metrics like CKA similarity or through t-SNE visualization to empirically verify modality-invariance.

3. **Out-of-Distribution Generalization**: Test SwinFUSE on completely different medical imaging modalities (ultrasound, PET, histopathology) to validate whether the learned features generalize beyond CT/MRI domains, and analyze failure modes when anatomical representations are fundamentally different.