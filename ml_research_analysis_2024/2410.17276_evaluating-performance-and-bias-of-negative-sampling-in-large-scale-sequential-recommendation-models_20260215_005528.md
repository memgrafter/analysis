---
ver: rpa2
title: Evaluating Performance and Bias of Negative Sampling in Large-Scale Sequential
  Recommendation Models
arxiv_id: '2410.17276'
source_url: https://arxiv.org/abs/2410.17276
tags:
- sampling
- negative
- items
- popularity
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates different negative sampling strategies for\
  \ large-scale sequential recommendation models, addressing the challenge of balancing\
  \ model performance and popularity bias. The authors implement six negative sampling\
  \ methods\u2014random, in-batch, popularity, mixed, adaptive, and adaptive with\
  \ mixed\u2014using the SASRec model."
---

# Evaluating Performance and Bias of Negative Sampling in Large-Scale Sequential Recommendation Models

## Quick Facts
- arXiv ID: 2410.17276
- Source URL: https://arxiv.org/abs/2410.17276
- Reference count: 40
- Primary result: Adaptive with mixed sampling achieves highest performance by selecting "hard" negatives while balancing popularity bias

## Executive Summary
This paper systematically evaluates six negative sampling strategies for large-scale sequential recommendation models, addressing the critical trade-off between model performance and popularity bias. Using the SASRec model architecture, the authors conduct extensive experiments across three benchmark datasets with varying popularity biases, employing 20x repeats and comprehensive hyperparameter optimization. The study reveals that while random negative sampling achieves high overall performance, it reinforces popularity bias by favoring head items. In contrast, popularity-based methods offer more balanced performance across popularity cohorts but at the cost of lower overall accuracy. The research introduces popularity-aware metrics to reveal imbalances hidden by average metrics and provides practical guidance for selecting appropriate negative sampling methods based on dataset characteristics.

## Method Summary
The study implements and evaluates six negative sampling methods (random, in-batch, popularity, mixed, adaptive, and adaptive with mixed) using SASRec as the backbone model. The experimental pipeline includes global temporal data splitting, oversampling factor optimization, and 20x repeated experiments with hyperparameter tuning. The authors evaluate performance using standard metrics (HR@10, NDCG@10) alongside new popularity-aware metrics (HR_cohort for head/mid/tail cohorts and Balance using Gini coefficient). Each dataset undergoes systematic testing with different sampling strategies, negative sample sizes, and batch configurations to assess both overall performance and popularity bias effects.

## Key Results
- Random negative sampling reinforces popularity bias, achieving high overall performance but favoring head items
- Popularity-based methods (in-batch and global popularity) provide more balanced performance across cohorts at the cost of lower overall accuracy
- Adaptive with mixed sampling achieves the highest performance by selecting "hard" negatives while maintaining reasonable popularity balance
- Model performance is highly dependent on dataset distribution characteristics, with ML-10M benefiting most from adaptive with mixed sampling due to its heavy head item bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random negative sampling reinforces popularity bias by over-representing popular items in the negative set
- Mechanism: Random negative sampling selects items uniformly from the entire corpus, but because popular items appear more frequently in the dataset, they are overrepresented in the negative samples. This causes the model to learn stronger distinctions between popular items and their negatives, leading to over-exposure of popular items in recommendations
- Core assumption: The dataset distribution reflects real-world item popularity, and uniform sampling from the corpus maintains this popularity distribution
- Evidence anchors:
  - [abstract] "We find that commonly used random negative sampling reinforces popularity bias and performs best for head items"
  - [section 5.2] "The reinforcement leads to the model performing much better for head items than either mid or tail items. Specifically, the ratio of head NDCG to tail NDCG is greater than 1 and greater than in other methods"
  - [corpus] Weak evidence - corpus shows related work on popularity bias but doesn't directly address random sampling's reinforcement effect
- Break condition: When the dataset is perfectly balanced across popularity cohorts, random sampling would no longer reinforce popularity bias

### Mechanism 2
- Claim: Adaptive with mixed sampling achieves highest performance by selecting "hard" negatives
- Mechanism: Adaptive sampling filters the initial random sample to retain only the top-K highest-scoring negative items for each positive item. These "hard" negatives are items the model is likely to confuse with positives, providing more informative training signals. When combined with mixed sampling, it also incorporates in-batch negatives for additional diversity
- Core assumption: The model's initial scoring of negative items accurately identifies which negatives are "hard" and would provide the most training signal
- Evidence anchors:
  - [abstract] "Adaptive with mixed sampling achieves the highest performance across datasets by selecting 'hard' negatives"
  - [section 4.2] "Adaptive[28]: Given a tensor of negatives generated by RNS, ANS only retains the top ð¾ highest scoring negative items for each item in the user's sequence"
  - [corpus] Assumption: Related work on hard negative sampling (Chen et al. 2023) supports the concept but doesn't directly validate the specific implementation
- Break condition: When the initial random sampling is too sparse to provide meaningful "hard" negatives, or when the model's scoring mechanism fails to identify truly informative negatives

### Mechanism 3
- Claim: Popularity-based sampling penalizes popular items to reduce popularity bias
- Mechanism: Popularity negative sampling selects negative items based on their global popularity, which means popular items are more likely to appear as negatives. This forces the model to learn stronger distinctions between popular items and their negatives, reducing the over-exposure of popular items in recommendations. However, this comes at the cost of lower overall accuracy
- Core assumption: Over-penalizing popular items in the negative set will lead to more balanced recommendations across popularity cohorts
- Evidence anchors:
  - [abstract] "Popularity-based methods (in-batch and global popularity negative sampling) can offer balanced performance at the cost of lower overall model performance results"
  - [section 5.2] "An alternative to random sampling is to sample negative items based on their global popularity[14]. This method can help reduce performance imbalance by increasing HR@10 for mid and head cohorts but at the cost of low HR@10 for the head cohort"
  - [corpus] Weak evidence - corpus shows related work on popularity bias but doesn't directly address popularity-based negative sampling as a solution
- Break condition: When the dataset has very low popularity skew, the benefits of popularity-based sampling diminish and the accuracy cost becomes unjustified

## Foundational Learning

- Concept: Sequential recommendation models and self-attention architecture
  - Why needed here: The paper evaluates negative sampling methods specifically for SASRec, a self-attentive sequential recommendation model. Understanding how this model processes user interaction sequences is crucial for interpreting the results
  - Quick check question: How does SASRec differ from traditional collaborative filtering models in terms of input representation and prediction task?

- Concept: Negative sampling theory and its impact on model training
  - Why needed here: The entire study revolves around different negative sampling strategies and their effects on model performance and bias. Understanding why negative sampling is necessary and how different strategies affect learning is fundamental
  - Quick check question: What is the computational advantage of negative sampling, and how does it approximate the full negative set?

- Concept: Popularity bias in recommendation systems
  - Why needed here: The paper introduces new metrics to measure popularity bias and evaluates how different sampling methods affect this bias. Understanding what popularity bias is and why it's problematic is essential for grasping the study's contributions
  - Quick check question: How does popularity bias affect recommendation diversity, and what are the typical metrics used to measure it?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model initialization -> Negative sampling module -> Training loop -> Evaluation pipeline -> Results aggregation

- Critical path:
  1. Data loading and preprocessing with temporal splitting
  2. Model initialization with specified hyperparameters
  3. Negative sampling strategy selection and mini-batch generation
  4. Training loop with BCE loss computation
  5. Validation and testing with both standard and popularity-aware metrics
  6. Results aggregation across 20 runs

- Design tradeoffs:
  - Random sampling vs. adaptive sampling: Speed vs. performance - random sampling is faster but reinforces popularity bias, while adaptive sampling is slower but achieves higher performance
  - Number of negatives: More negatives generally improve performance but increase computational cost
  - Oversampling factor: Higher values improve accuracy but reduce training speed

- Failure signatures:
  - Model converges but shows high popularity bias: Likely using random sampling on a heavily skewed dataset
  - Very low overall performance: May be using popularity sampling on a dataset where popularity bias is actually beneficial
  - Training instability: Incorrect negative sampling implementation or hyperparameter misconfiguration

- First 3 experiments:
  1. Run SASRec with random negative sampling on ML-10M dataset to establish baseline performance
  2. Implement and test in-batch negative sampling to compare with random sampling
  3. Add popularity-aware metrics (HR_cohort and Balance) to evaluate popularity bias in the baseline model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does negative sampling method selection impact long-term recommendation diversity metrics beyond popularity bias?
- Basis in paper: [explicit] The authors note that popularity bias affects diversity and that their metrics focus on popularity cohorts, but don't examine other diversity dimensions like category coverage or serendipity
- Why unresolved: The study focuses specifically on popularity-based cohorts (head, mid, tail) but doesn't explore whether these findings generalize to other forms of diversity measurement
- What evidence would resolve it: Experiments measuring category-level diversity, long-term user engagement with diverse items, or serendipity metrics across different negative sampling methods

### Open Question 2
- Question: How do different negative sampling methods perform when scaled to billion-item catalogs with extreme sparsity?
- Basis in paper: [inferred] The authors mention their work addresses "millions or billions of options" but only test on datasets with millions of items; they also note computational constraints with negative sampling
- Why unresolved: The experiments were conducted on datasets with up to 10M items, and the computational optimizations described may not scale linearly to billion-item catalogs
- What evidence would resolve it: Performance benchmarks of the same sampling methods on billion-scale datasets, including runtime comparisons and accuracy metrics at different catalog sizes

### Open Question 3
- Question: How does the optimal oversampling factor (OSF) change with different dataset characteristics and model architectures?
- Basis in paper: [explicit] The authors discuss OSF as a hyperparameter affecting accuracy vs speed trade-off, but only provide empirical observations without theoretical justification
- Why unresolved: The paper shows that OSF affects performance but doesn't provide guidance on how to select it based on dataset properties or model architecture
- What evidence would resolve it: Theoretical analysis or empirical studies showing how OSF should be tuned based on dataset size, sparsity, popularity distribution, and model complexity

## Limitations
- The evaluation is restricted to the SASRec model architecture, limiting generalizability to other sequential or non-sequential recommendation approaches
- The three benchmark datasets, while diverse, may not capture the full spectrum of real-world recommendation scenarios
- Computational costs of adaptive sampling methods may limit practical deployment in extremely large-scale systems

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Experimental methodology and statistical significance of results (20x repeats with hyperparameter optimization) | High |
| Generalizability of findings across different recommendation model architectures | Medium |
| Effectiveness of the proposed popularity-aware metrics | Medium |

## Next Checks

1. **Cross-architecture validation**: Test the negative sampling strategies on alternative sequential recommendation models (e.g., BERT4Rec, GRU4Rec) to verify if the observed patterns hold across different architectures

2. **Dataset diversity expansion**: Evaluate the methods on additional datasets with different characteristics (e.g., music streaming, news recommendations) to assess generalizability beyond the current scope

3. **Production feasibility analysis**: Conduct a cost-benefit analysis of adaptive sampling methods in terms of computational resources, training time, and recommendation quality to determine practical deployment viability