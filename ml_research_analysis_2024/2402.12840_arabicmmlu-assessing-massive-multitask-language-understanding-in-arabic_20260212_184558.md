---
ver: rpa2
title: 'ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic'
arxiv_id: '2402.12840'
source_url: https://arxiv.org/abs/2402.12840
tags:
- arabic
- language
- questions
- bloomz
- school
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ArabicMMLU, the first multi-task language
  understanding benchmark for Arabic, comprising 40 tasks and 14,575 multiple-choice
  questions across educational levels in Arabic-speaking countries. The dataset covers
  diverse subjects, with over 50% containing Arabic-specific contexts.
---

# ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic

## Quick Facts
- arXiv ID: 2402.12840
- Source URL: https://arxiv.org/abs/2402.12840
- Reference count: 21
- Top Arabic model achieves 62.3% accuracy on the benchmark

## Executive Summary
This paper introduces ArabicMMLU, the first multi-task language understanding benchmark for Arabic, comprising 40 tasks and 14,575 multiple-choice questions across educational levels in Arabic-speaking countries. The dataset covers diverse subjects, with over 50% containing Arabic-specific contexts. Evaluations of 35 models (22 multilingual, 11 Arabic-centric, and 2 closed-source) reveal that even top Arabic models struggle, with the best achieving only 62.3% accuracy, while GPT-4 reaches 72.5%. The results highlight significant room for improvement in Arabic language models, particularly for reasoning and knowledge-intensive tasks.

## Method Summary
The authors created ArabicMMLU by collecting school exam questions from 8 Arabic-speaking countries, covering educational levels from elementary to secondary. The dataset includes 40 tasks across 12 subjects, with 14,575 multiple-choice questions. Quality control involved native Arabic speakers verifying question accuracy and answer keys. Models were evaluated using zero-shot and few-shot approaches with both Arabic and English prompts, and results were analyzed across subject groups, education levels, and countries.

## Key Results
- Top Arabic model (Jais-chat 30B) achieves 62.3% accuracy, significantly below GPT-4's 72.5%
- Over 50% of questions contain Arabic-specific cultural or regional contexts
- Arabic-centric models outperform multilingual models by more than 14 points
- Model performance shows strong correlation with parameter count across languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Arabic-specific benchmarks improve assessment of LLM reasoning in regional contexts
- Mechanism: The dataset uses actual Arabic school exam questions across 8 countries, capturing local curriculum and cultural knowledge not present in translated English benchmarks
- Core assumption: Arabic-specific content differs meaningfully from translated content in reasoning requirements
- Evidence anchors:
  - [abstract]: "Over 50% of the questions in our dataset are tailored to Arabic-specific contexts"
  - [section 1]: "Given that Arabic is one of the most widely-spoken languages in the world... it is critically important that datasets be constructed for the language that are regionally- and culturally-localized"
  - [corpus]: Weak - only 5/25 neighbor papers are Arabic-focused; most are general LLMs or non-Arabic languages
- Break condition: If Arabic-specific content doesn't require different reasoning skills than translated content, or if model performance is similar on both

### Mechanism 2
- Claim: Native speaker involvement ensures dataset quality and cultural relevance
- Mechanism: Data collection involved 10 native Arabic speakers from different countries with education and professional backgrounds
- Core assumption: Native speakers can identify culturally-relevant content and ensure answer accuracy
- Evidence anchors:
  - [section 3.1]: "Our internal workers are Master's students and Research Assistants in Computer Science, while the external workers hold Bachelor's degrees"
  - [section 3.2]: "96% of the questions and answer keys match on average"
  - [corpus]: Assumption - corpus doesn't directly address native speaker involvement quality
- Break condition: If native speakers introduce biases or if automated quality control would be more consistent

### Mechanism 3
- Claim: Model size correlates with Arabic performance, but Arabic-centric models outperform multilingual models
- Mechanism: Larger parameter models show improved accuracy, with Arabic-specific models (Jais-chat 30B) outperforming multilingual models (BLOOMZ 7B)
- Core assumption: Arabic language modeling benefits from both scale and language-specific pretraining
- Evidence anchors:
  - [section 4.2]: "BLOOMZ (7B) achieves an accuracy 15.9 points higher than BLOOMZ (560M)"
  - [section 4.2]: "BLOOMZ (7B) and mT0 (13B), their performance lags behind Jais, with a disparity of more than 14 points"
  - [corpus]: Assumption - corpus doesn't provide comparative model performance data
- Break condition: If the correlation is due to dataset size rather than model capability, or if multilingual models catch up with further training

## Foundational Learning

- Concept: Zero-shot vs few-shot learning
  - Why needed here: The paper compares model performance across both settings to understand knowledge generalization
  - Quick check question: In zero-shot, are models given any examples before answering ArabicMMLU questions?

- Concept: Prompt engineering and language choice
  - Why needed here: Different prompt languages (Arabic vs English) and output formats (Arabic vs Latin script) significantly affect model performance
  - Quick check question: Why does using English prompts with English alphabetic output perform better than Arabic prompts with Arabic alphabetic output?

- Concept: Calibration and confidence scoring
  - Why needed here: The paper analyzes whether models are well-calibrated by comparing confidence scores with actual accuracy
  - Quick check question: What correlation coefficient indicates good calibration between model confidence and accuracy?

## Architecture Onboarding

- Component map:
  - Data pipeline: Web scraping → Metadata extraction → Quality filtering → Annotation verification
  - Model evaluation: Prompt formatting → Probability scoring → Answer selection → Performance aggregation
  - Analysis: Subject grouping → Education level analysis → Negation detection → Calibration calculation

- Critical path:
  1. Data collection from 8 countries
  2. Quality control verification
  3. Model evaluation with consistent prompts
  4. Performance analysis across dimensions
  5. Results interpretation and documentation

- Design tradeoffs:
  - Multiple countries vs. dataset size: Including more countries reduces question volume per country but increases cultural coverage
  - Native speakers vs. automated processing: Human involvement ensures quality but introduces potential biases
  - Zero-shot vs few-shot evaluation: Zero-shot tests general knowledge, few-shot tests instruction-following ability

- Failure signatures:
  - Low calibration scores indicate poor probability estimation
  - Performance gaps between Arabic and Latin script outputs suggest script-specific tokenization issues
  - Country-specific performance differences may indicate curriculum mismatches

- First 3 experiments:
  1. Run all models with English prompt/English output vs Arabic prompt/Arabic output to confirm optimal configuration
  2. Test few-shot performance with 1-3 examples to see if base models improve while instruction-tuned models degrade
  3. Analyze negation questions separately to verify model sensitivity to negative phrasing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ArabicMMLU compare to other existing Arabic benchmarks like Dolphin, OCRA, and LAraBench in terms of model accuracy and task diversity?
- Basis in paper: [inferred] The paper introduces ArabicMMLU as the first MMLU-style dataset for Arabic, but does not compare its performance to other existing Arabic benchmarks.
- Why unresolved: The paper focuses on evaluating models on ArabicMMLU specifically and does not provide a comparative analysis with other Arabic benchmarks.
- What evidence would resolve it: Conducting experiments to evaluate the same models on Dolphin, OCRA, and LAraBench and comparing their performance to ArabicMMLU results.

### Open Question 2
- Question: What is the impact of including dialectical Arabic in ArabicMMLU on model performance, and how does it compare to models trained solely on Modern Standard Arabic?
- Basis in paper: [explicit] The paper mentions that ArabicMMLU focuses on Modern Standard Arabic (MSA) and acknowledges that multilingual and Arabic LLMs are often exposed to both MSA and dialectical Arabic.
- Why unresolved: The paper does not explore the performance of models on dialectical Arabic or compare it to MSA-only performance.
- What evidence would resolve it: Extending ArabicMMLU to include dialectical Arabic questions and evaluating model performance on this extended dataset compared to the original MSA-only dataset.

### Open Question 3
- Question: How does the accuracy of ArabicMMLU questions vary across different educational systems within the same country, and what factors contribute to these variations?
- Basis in paper: [inferred] The paper mentions that ArabicMMLU includes questions from various educational levels and countries, but does not analyze performance variations within the same country.
- Why unresolved: The paper provides overall performance metrics but does not delve into performance differences across educational systems within countries.
- What evidence would resolve it: Analyzing ArabicMMLU results by specific educational systems (e.g., public vs. private schools, different curricula) within each country and identifying factors contributing to performance variations.

## Limitations

- Dataset may have gaps in coverage across the 8 countries, with only 5 of 25 neighbor papers focusing on Arabic
- Model performance comparisons don't control for pretraining data composition or instruction-tuning quality differences
- Calibration analysis lacks statistical significance testing and confidence intervals for correlation coefficients

## Confidence

- High Confidence: Dataset creation methodology and quality control procedures are well-documented and reproducible
- Medium Confidence: Claims about Arabic-specific content differentiation and native speaker involvement quality
- Low Confidence: Claims about model calibration quality and the relative importance of scale versus language-specific pretraining

## Next Checks

1. **Dataset Composition Validation**: Conduct detailed analysis of question distribution across countries and subjects to verify claimed "over 50% Arabic-specific" content with manual verification

2. **Model Performance Attribution Study**: Design controlled experiments isolating effects of model scale, pretraining data composition, and instruction-tuning quality on performance differences between Arabic-centric and multilingual models

3. **Calibration Statistical Analysis**: Perform hypothesis testing on calibration metrics with confidence intervals and examine whether performance differences in calibration are statistically significant across prompt/output configurations