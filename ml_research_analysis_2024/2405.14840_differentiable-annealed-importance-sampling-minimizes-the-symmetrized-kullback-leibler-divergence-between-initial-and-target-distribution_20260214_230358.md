---
ver: rpa2
title: Differentiable Annealed Importance Sampling Minimizes The Symmetrized Kullback-Leibler
  Divergence Between Initial and Target Distribution
arxiv_id: '2405.14840'
source_url: https://arxiv.org/abs/2405.14840
tags:
- mean
- dais
- distribution
- divergence
- dais0
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the initial distribution of differentiable
  annealed importance sampling (DAIS) and shows that, in the limit of many transitions,
  DAIS minimizes the symmetrized Kullback-Leibler (KL) divergence between its initial
  and target distribution. This theoretical result suggests that DAIS can be used
  for variational inference, with its initial distribution serving as a compact, explicit
  approximation to the target distribution.
---

# Differentiable Annealed Importance Sampling Minimizes The Symmetrized Kullback-Leibler Divergence Between Initial and Target Distribution

## Quick Facts
- arXiv ID: 2405.14840
- Source URL: https://arxiv.org/abs/2405.14840
- Authors: Johannes Zenn; Robert Bamler
- Reference count: 40
- Primary result: DAIS0 provides more accurate uncertainty estimates than standard VI, IWVI, and MSC in many cases

## Executive Summary
This paper investigates the initial distribution of differentiable annealed importance sampling (DAIS) and proves that, in the limit of many transitions, DAIS minimizes the symmetrized Kullback-Leibler divergence between its initial and target distribution. This theoretical result enables using DAIS0—the initial distribution learned by DAIS—as a compact variational approximation that balances mode-seeking and mass-covering behavior. Empirically, DAIS0 outperforms standard variational inference, importance-weighted variational inference, and Markovian score climbing on synthetic and real-world datasets, particularly for uncertainty estimation in high-dimensional settings.

## Method Summary
The method extends differentiable annealed importance sampling (DAIS) by optimizing the initial distribution q0 through the ELBO objective. The algorithm uses Hamiltonian Monte Carlo dynamics for forward and backward transitions between annealed distributions defined by a schedule βk. During training, DAIS maximizes a lower bound on the log normalization constant, which implicitly minimizes the Jensen-Shannon divergence between q0 and the target. At inference time, DAIS0 uses only the learned initial distribution q0 without expensive AIS steps, providing a compact explicit representation of the approximate posterior.

## Key Results
- DAIS0 achieves better mean absolute error for standard deviation estimates than VI, IWVI, and MSC in Gaussian process regression experiments
- DAIS0 outperforms other methods on 3 out of 5 Bayesian logistic regression datasets
- Increasing the number of annealing transitions K improves performance in moderate dimensions (d ≤ 50)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In the limit of many transitions, DAIS minimizes the Jensen-Shannon divergence between its initial and target distribution
- Mechanism: As the number of annealing transitions K increases, the gap between the lower bound and the true normalization constant becomes proportional to the symmetrized KL divergence
- Core assumption: The transitions between consecutive annealing distributions are perfect and the βk are equally spaced
- Evidence anchors: [abstract] "DAIS minimizes the symmetrized Kullback-Leibler (KL) divergence (Jensen-Shannon divergence) between the initial and target distribution"
- Break condition: If the transition kernels are imperfect or the βk are not equally spaced, the asymptotic behavior may deviate

### Mechanism 2
- Claim: DAIS0 provides a compact explicit representation of the approximate posterior
- Mechanism: DAIS0 uses the initial distribution q0 from DAIS training as a variational approximation at inference time, avoiding expensive AIS steps while maintaining analytical tractability
- Core assumption: The initial distribution q0 learned by DAIS is a good approximation of the target distribution
- Evidence anchors: [abstract] "DAIS0 often provides more accurate uncertainty estimates than standard variational inference"
- Break condition: If q0 poorly approximates the target, inference quality degrades

### Mechanism 3
- Claim: DAIS0 achieves a balance between mode-seeking and mass-covering behavior
- Mechanism: By implicitly minimizing the Jensen-Shannon divergence, DAIS0 averages the properties of reverse KL (mode-seeking) and forward KL (mass-covering)
- Core assumption: The Jensen-Shannon divergence truly balances the two KL divergences in high-dimensional settings
- Evidence anchors: [abstract] "DAIS0 (implicitly minimizing the Jensen-Shannon divergence) indeed outperforms MSC (minimizing the forward KL divergence)"
- Break condition: If dimensionality is too high, the averaging effect may break down

## Foundational Learning

- Concept: Annealed Importance Sampling (AIS)
  - Why needed here: DAIS builds on AIS by making the initial distribution differentiable and optimizing it
  - Quick check question: What is the role of the backward transition kernels Bk in AIS?

- Concept: Variational Inference (VI)
  - Why needed here: DAIS0 is a form of VI using the initial distribution as a variational approximation
  - Quick check question: How does minimizing the reverse KL divergence affect the shape of the variational distribution?

- Concept: Kullback-Leibler (KL) divergence and its variants
  - Why needed here: The paper contrasts reverse KL, forward KL, and symmetrized KL (Jensen-Shannon) divergences
  - Quick check question: What is the key difference between reverse KL and forward KL in terms of mode-seeking vs mass-covering behavior?

## Architecture Onboarding

- Component map: Initial distribution q0 (trainable parameters) -> Annealing schedule βk -> Forward and backward transition kernels (HMC-based) -> Objective: lower bound on log Z (ELBO) -> DAIS0: inference-only mode using q0

- Critical path: 1. Initialize q0 as factorized normal 2. Define annealing schedule βk 3. Optimize q0 parameters to maximize ELBO_N,K DAIS 4. For inference with DAIS0, use q0 directly

- Design tradeoffs: Training cost: DAIS0 requires expensive AIS steps during training vs Inference cost: DAIS0 is cheap (just evaluate q0) vs DAIS (requires full AIS) vs Accuracy: DAIS0 may be less accurate than full DAIS but better than standard VI in high dimensions

- Failure signatures: Poor training convergence: check HMC step size and mass matrix vs q0 collapses to single mode: too few transitions K or too aggressive optimization vs High variance in estimates: insufficient particles N during training

- First 3 experiments: 1. Bimodal Gaussian target: test mode-seeking vs mass-covering behavior 2. Synthetic Gaussian process: compare uncertainty estimates to analytic solution 3. Bayesian logistic regression: validate on real datasets and compare to HMC "ground truth"

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the approximation quality of DAIS0's initial distribution q0 scale with the number of annealing transitions K in high-dimensional settings?
- Basis in paper: [explicit] The paper shows that DAIS minimizes the Jensen-Shannon divergence between q0 and the target in the limit of many transitions, and provides empirical evidence that increasing K improves performance in moderate dimensions
- Why unresolved: The paper only provides limited empirical results for high-dimensional settings, and the theoretical analysis is asymptotic
- What evidence would resolve it: Comprehensive experiments testing DAIS0 across a wide range of dimensions (e.g., 100-1000) with varying K, comparing approximation quality metrics to established methods

### Open Question 2
- Question: What is the computational complexity trade-off between DAIS0 and standard variational inference methods when considering both training and inference time?
- Basis in paper: [explicit] The paper notes that DAIS0 is more expensive than VI at training time but cheaper than DAIS at inference time, and suggests it may be useful for downstream tasks requiring explicit posterior approximations
- Why unresolved: The paper does not provide detailed computational complexity analysis or runtime comparisons between DAIS0 and VI for various problem sizes
- What evidence would resolve it: Detailed benchmarking studies comparing wall-clock time and computational resources required for training and inference across different problem scales and architectures

### Open Question 3
- Question: How robust is DAIS0 to the choice of forward and backward transition kernels in the annealed importance sampling process?
- Basis in paper: [inferred] The theoretical result assumes perfect transitions between annealing distributions, and the experiments use Hamiltonian Monte Carlo, but the paper does not explore sensitivity to kernel choice
- Why unresolved: The paper does not systematically investigate how different transition kernels (e.g., random walk Metropolis, Langevin dynamics) affect the quality of the learned q0 distribution
- What evidence would resolve it: Controlled experiments comparing DAIS0 performance using various transition kernels while keeping other parameters fixed, measuring approximation quality and convergence properties

## Limitations
- The theoretical claim about symmetrized KL minimization relies on assumptions about perfect transitions and equally spaced annealing schedules that may not hold in practice
- The corpus evidence supporting the theoretical claims is notably weak, with no direct citations found
- The method requires expensive AIS steps during training, making it computationally intensive compared to standard VI

## Confidence
- Theoretical claim about symmetrized KL minimization: **Low**
- DAIS0 provides better uncertainty estimates than VI: **Medium**
- DAIS0 balances mode-seeking and mass-covering behavior: **Low**

## Next Checks
1. Implement a simplified DAIS experiment with a known bimodal Gaussian target to empirically verify the symmetrized KL minimization claim by measuring the actual divergence between q0 and the target distribution.
2. Run ablation studies varying the number of transitions K and particles N to identify the minimum requirements for stable DAIS training and assess sensitivity to these hyperparameters.
3. Compare DAIS0 uncertainty estimates against full DAIS on the same synthetic and real-world tasks to quantify the accuracy trade-off between the compact representation and the full sampling approach.