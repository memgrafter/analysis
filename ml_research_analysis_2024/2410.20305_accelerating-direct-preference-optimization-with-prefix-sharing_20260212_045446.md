---
ver: rpa2
title: Accelerating Direct Preference Optimization with Prefix Sharing
arxiv_id: '2410.20305'
source_url: https://arxiv.org/abs/2410.20305
tags:
- prefix
- packing
- sharing
- preference
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces prefix sharing, a technique to accelerate
  training for offline paired preference optimization algorithms like DPO. The core
  idea is to combine chosen and rejected responses into a single sequence with a shared
  prefix, reducing redundant computation of the prompt.
---

# Accelerating Direct Preference Optimization with Prefix Sharing

## Quick Facts
- arXiv ID: 2410.20305
- Source URL: https://arxiv.org/abs/2410.20305
- Authors: Franklin Wang; Sumanth Hegde
- Reference count: 40
- Key outcome: Prefix sharing achieves 1.1-1.5x training throughput improvements for DPO across popular datasets without affecting convergence

## Executive Summary
This paper introduces prefix sharing, a technique to accelerate training for offline paired preference optimization algorithms like DPO. The core idea is to combine chosen and rejected responses into a single sequence with a shared prefix, reducing redundant computation of the prompt. A custom attention mask prevents cross-response contamination. The method achieves 1.1-1.5x training throughput improvements across popular DPO datasets without affecting convergence. When combined with sequence packing, consistent 1.3-1.6x speedups are observed, even for datasets with shorter sequences. The approach is applicable to other paired preference tuning methods beyond DPO.

## Method Summary
Prefix sharing combines chosen and rejected responses with a shared prompt into a single sequence, computing the prompt only once. A custom block-sparse attention mask prevents the rejected response from attending to the chosen response region while allowing both responses to attend to the shared prefix. This maintains computational equivalence to traditional paired format while eliminating redundant prefix computation. The method uses FlexAttention to support custom masks and can be combined with sequence packing for additional efficiency gains, particularly beneficial for datasets with high prefix-to-completion ratios.

## Key Results
- Prefix sharing achieves 1.1-1.5× improvement in training throughput on popular DPO datasets
- Sequence packing combined with prefix sharing provides 1.3-1.6× speedups across all tested datasets
- No effect on convergence or model quality is observed
- Memory usage decreases due to reduced sequence lengths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prefix sharing eliminates redundant computation of the shared prompt by combining chosen and rejected responses into a single sequence.
- Mechanism: When training on preference data, each prompt is repeated twice in the traditional paired format—once for each response. By placing the prompt, chosen response, and rejected response into a single sequence, the model only computes the prompt once. A custom attention mask ensures that the two responses don't contaminate each other during attention computation.
- Core assumption: The shared prompt's activations are identical for both responses due to causal masking, making it safe to compute them only once.
- Evidence anchors:
  - [abstract] "Our method achieves 1.1-1.5× improvement in training throughput on popular DPO datasets, without any effect on convergence."
  - [section 3.1] "We employ a custom attention mask to delineate the two responses during the model's forward pass."
- Break condition: If the shared prefix contains any task-specific context that varies between chosen and rejected responses, or if causal masking doesn't fully isolate the two response streams, cross-contamination could occur.

### Mechanism 2
- Claim: The custom block-sparse attention mask prevents cross-response contamination while maintaining computational equivalence to the traditional format.
- Mechanism: The attention mask blocks the rejected response from attending to the chosen response region while allowing both responses to attend to the shared prefix. This ensures that the log probabilities are computed independently for each response, just as in the traditional paired format.
- Core assumption: The causal attention mask in standard transformers already prevents queries from attending to future keys, so only the inter-response contamination needs to be addressed.
- Evidence anchors:
  - [abstract] "To prevent cross-response contamination, we use a custom block-sparse attention mask."
  - [section 3.1] "We mask out the region where the rejected response attends to the chosen response, ensuring that the log probabilities of both responses are computed independently of each other."
- Break condition: If the attention implementation doesn't properly respect the custom mask, or if the mask construction is incorrect, the model might learn from cross-response contamination.

### Mechanism 3
- Claim: Sequence packing combined with prefix sharing provides additional efficiency gains by maximizing GPU utilization.
- Mechanism: When packing multiple sequences into a batch, the redundant prefix in traditional paired format means each packed unit is longer. With prefix sharing, each packed unit is shorter (shared prefix + chosen + rejected), allowing more sequences to be packed within the same maximum sequence length limit. This increases effective batch size and improves compute utilization.
- Core assumption: GPU compute is the bottleneck rather than memory bandwidth, making it beneficial to maximize the number of sequences processed per batch.
- Evidence anchors:
  - [section 4.3.2] "Packing provides a significant boost in training throughput for datasets with low median overall lengths... since prefix sharing decreases the number of tokens in each packing unit."
  - [section 3.2] "Without a redundant shared prefix, the maximum sequence length for each packing unit is much shorter."
- Break condition: If memory bandwidth becomes the bottleneck, or if the overhead of packing/unpacking outweighs the benefits, sequence packing may not provide additional gains.

## Foundational Learning

- Concept: Attention mechanisms and causal masking
  - Why needed here: Understanding how attention works is crucial to grasp why prefix sharing is safe (causal masking already prevents queries from attending to future keys) and how the custom mask prevents inter-response contamination.
  - Quick check question: In a causal attention mask, can token at position i attend to token at position j if j > i?

- Concept: Direct Preference Optimization (DPO) loss function
  - Why needed here: Understanding DPO is essential to see why both chosen and rejected responses need to be processed and how their log probabilities are used in the loss calculation.
  - Quick check question: What is the mathematical form of the DPO loss function and what does each term represent?

- Concept: Sequence packing algorithms
  - Why needed here: Understanding how sequence packing works (like first-fit-decreasing algorithms) helps explain why prefix sharing makes packing more effective.
  - Quick check question: How does sequence packing typically work in NLP, and what is the "bin packing problem" it's trying to solve?

## Architecture Onboarding

- Component map:
  - Data loader -> Custom attention mask generator -> FlexAttention layer -> Sequence packing module -> Loss calculation

- Critical path:
  1. Data loading and preprocessing (combine responses, create mask)
  2. Forward pass through model with custom attention
  3. Loss calculation (extract relevant log probabilities)
  4. Backward pass
  5. Parameter update

- Design tradeoffs:
  - FlexAttention vs FlashAttention-3: FlexAttention supports custom masks but is slower for standard causal attention; FlashAttention-3 is faster but can't handle prefix sharing
  - Memory vs compute: Prefix sharing reduces memory usage but may add complexity to attention implementation
  - Packing efficiency: Packing with prefix sharing allows more sequences per batch but requires careful mask management

- Failure signatures:
  - Cross-contamination between responses (model learns from incorrect signal)
  - Degraded model quality (convergence issues or worse performance)
  - Unexpected memory usage patterns
  - Suboptimal speedups (less than theoretical maximum)

- First 3 experiments:
  1. Microbenchmark MLP layer with different prefix lengths to verify linear speedup
  2. Microbenchmark attention layer with FlexAttention + prefix sharing vs FlashAttention-3
  3. Full model training on a small dataset (like UltraFeedback) with and without prefix sharing to verify convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does prefix sharing affect model quality and downstream performance beyond training speed?
- Basis in paper: [inferred] The paper focuses on training throughput improvements but mentions convergence is unaffected. No experiments on actual model quality metrics are presented.
- Why unresolved: The paper only benchmarks training speed and memory usage, not evaluating whether the models trained with prefix sharing perform differently on downstream tasks.
- What evidence would resolve it: Direct comparison of model quality metrics (accuracy, perplexity, task-specific benchmarks) between models trained with and without prefix sharing.

### Open Question 2
- Question: What is the optimal attention mask design for prefix sharing across different model architectures?
- Basis in paper: [explicit] The paper uses a custom block-sparse attention mask with FlexAttention, but acknowledges this is simpler than custom kernel approaches used for inference.
- Why unresolved: The paper doesn't explore alternative attention mask designs or compare performance across different architectures (GPT, BERT-style, etc.).
- What evidence would resolve it: Benchmarking different attention mask implementations and their impact on speed/memory across multiple model architectures.

### Open Question 3
- Question: How does prefix sharing scale with increasingly long prompts and responses in real-world applications?
- Basis in paper: [inferred] The paper shows benefits for datasets with long prefixes but doesn't explore extreme cases or practical limits of the approach.
- Why unresolved: Experiments focus on existing datasets but don't test theoretical scaling limits or edge cases with extremely long sequences.
- What evidence would resolve it: Testing prefix sharing on synthetic datasets with progressively longer prompts/responses to identify breaking points and scaling behavior.

## Limitations

- Effectiveness highly dependent on prefix-to-completion ratio; minimal gains for datasets with short prompts or very long responses
- Limited evaluation to DPO algorithm only; broader applicability to other preference optimization methods remains theoretical
- No rigorous mathematical proof of equivalence to traditional paired format beyond empirical validation

## Confidence

- Training throughput improvements: High confidence (directly measured across multiple datasets)
- No effect on convergence: Medium confidence (claimed but not thoroughly validated with statistical tests)
- Broader applicability beyond DPO: Low confidence (stated but not empirically validated)

## Next Checks

1. **Convergence validation**: Run full training with prefix sharing vs traditional paired format on at least two datasets (one with high prefix ratio like MetaMath-DPO and one with low ratio like Ultrafeedback) for the full training duration, comparing both training loss curves and final model quality metrics. Include statistical significance testing on the results.

2. **Cross-contamination verification**: Design a controlled experiment where the attention mask is deliberately corrupted (e.g., by 1-5% random errors) to quantify how much cross-response contamination affects model quality. This would establish the sensitivity of the method to mask implementation errors.

3. **Broader applicability test**: Implement prefix sharing for another paired preference tuning method (such as PPO-based alignment or Bradley-Terry preference optimization) and verify that it provides similar throughput improvements without affecting convergence, confirming the claim of broader applicability beyond DPO.