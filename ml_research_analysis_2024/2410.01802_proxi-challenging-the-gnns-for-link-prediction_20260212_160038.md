---
ver: rpa2
title: 'PROXI: Challenging the GNNs for Link Prediction'
arxiv_id: '2410.01802'
source_url: https://arxiv.org/abs/2410.01802
tags:
- indices
- graph
- node
- link
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PROXI, a simple machine learning model that
  challenges the dominance of Graph Neural Networks (GNNs) in link prediction tasks.
  PROXI leverages proximity information between node pairs using both structural (graph-based)
  and domain (attribute-based) indices.
---

# PROXI: Challenging the GNNs for Link Prediction

## Quick Facts
- arXiv ID: 2410.01802
- Source URL: https://arxiv.org/abs/2410.01802
- Reference count: 22
- Primary result: PROXI achieves competitive or superior performance to state-of-the-art GNNs on link prediction tasks using engineered proximity features

## Executive Summary
This paper introduces PROXI, a machine learning model that challenges the dominance of Graph Neural Networks (GNNs) in link prediction tasks. PROXI leverages proximity information between node pairs using both structural (graph-based) and domain (attribute-based) indices. The method treats link prediction as a binary classification problem, using traditional ML classifiers like XGBoost on engineered features rather than learning node representations. Across twelve benchmark datasets spanning homophilic and heterophilic networks, PROXI achieves highly competitive or superior performance compared to state-of-the-art GNN models. Notably, integrating PROXI with existing GNNs significantly boosts their performance by up to 11%.

## Method Summary
PROXI combines 20 proximity indices (10 structural and 10 domain) to create feature vectors for node pairs, which are then fed into traditional ML classifiers like XGBoost. The structural indices include common neighbors, Jaccard, Salton, Sørensen, Adamic-Adar, and path length metrics, while domain indices capture similarity in node attributes through methods like common digits, class identifier matching, and L1/Cosine distances. The model is trained on labeled node pairs and can be integrated with GNNs by concatenating its proximity indices with learned node embeddings.

## Key Results
- PROXI achieves highly competitive or superior performance compared to state-of-the-art GNN models on twelve benchmark datasets
- Integrating PROXI with existing GNNs significantly boosts their performance by up to 11%
- Combining structural and domain proximity indices consistently outperforms using either alone
- PROXI demonstrates strong performance across both homophilic and heterophilic networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PROXI's engineered proximity indices provide more direct and informative representations of node pairs than GNN-based node embeddings for link prediction.
- Mechanism: Instead of learning node embeddings and then combining them, PROXI directly computes similarity metrics between node pairs using both structural (graph-based) and domain (attribute-based) proximity indices. This bypasses the transitivity limitations inherent in GNN similarity measures.
- Core assumption: The link prediction task is inherently non-transitive, meaning that common neighbors do not guarantee a link, making direct node pair representations more effective than derived node embeddings.
- Evidence anchors:
  - [abstract] "PROXI achieves highly competitive or superior performance compared to state-of-the-art GNN models"
  - [section] "link prediction task is generally not transitive. Specifically, if u ∼ v and v ∼ w, it does not necessarily imply that u ∼ w"
  - [corpus] Weak evidence - no direct mention of transitivity in corpus papers
- Break condition: If the dataset structure is such that transitivity is actually a strong signal for link formation, or if node attributes are highly uninformative.

### Mechanism 2
- Claim: Combining structural and domain proximity indices creates a synergistic effect that outperforms using either alone.
- Mechanism: The model aggregates 20 different proximity metrics (10 structural, 10 domain) that capture different aspects of node pair relationships. XGBoost can learn which combinations are most predictive for different dataset characteristics.
- Core assumption: Different proximity metrics capture complementary information about node pair relationships, and the ML classifier can effectively learn which combinations are most predictive.
- Evidence anchors:
  - [abstract] "Our results support recent theoretical studies, indicating that current GNNs may not be significantly better than traditional models"
  - [section] "our intuition leads us to believe that by aggregating all of this information, the ML classifier can make finer determinations within the latent space"
  - [corpus] No direct evidence in corpus papers about index synergy
- Break condition: If the dataset has very limited attribute information or if the graph structure is too noisy for proximity indices to be informative.

### Mechanism 3
- Claim: PROXI can be integrated with existing GNN models to boost their performance by providing better edge representations.
- Mechanism: The proximity indices are concatenated with GNN node embeddings (via Hadamard product) to create enhanced edge representations that capture both learned patterns and explicit proximity information.
- Core assumption: GNNs can benefit from additional structured features that capture proximity relationships that may be difficult to learn directly from graph structure.
- Evidence anchors:
  - [abstract] "integrating PROXI with existing GNNs significantly boosts their performance by up to 11%"
  - [section] "We took this path, and concatenate our proximity indices {α(u, v)} through MLP to prediction heads of classical GNN models"
  - [corpus] No direct evidence in corpus papers about GNN-PROXI integration
- Break condition: If the GNN model already has strong edge representation learning mechanisms or if the additional features cause overfitting.

## Foundational Learning

- Concept: Graph Neural Networks and message passing framework
  - Why needed here: Understanding the baseline that PROXI is challenging - GNNs learn node representations through iterative message passing and aggregation
  - Quick check question: What is the fundamental limitation of message-passing GNNs identified in theoretical studies that motivates PROXI?

- Concept: Proximity-based link prediction methods
  - Why needed here: PROXI builds on traditional proximity indices (common neighbors, Jaccard, Adamic-Adar, etc.) but extends them significantly
  - Quick check question: How does PROXI's approach to using proximity indices differ from traditional single-index methods?

- Concept: Homophily vs heterophily in graphs
  - Why needed here: PROXI is tested on both homophilic and heterophilic networks, demonstrating its versatility across different graph structures
  - Quick check question: How does the homophily ratio affect the choice of proximity indices in PROXI?

## Architecture Onboarding

- Component map: Input graph and attributes -> Compute 20 proximity indices -> Create feature vectors for node pairs -> Train XGBoost classifier -> Binary link prediction output
- Critical path: Parse input graph and attributes -> Compute all 20 proximity indices for training node pairs -> Train XGBoost classifier on the feature matrix -> For inference, compute indices for test node pairs and predict
- Design tradeoffs: PROXI trades the end-to-end learning of GNNs for feature engineering expertise, more interpretable but requires domain knowledge, computationally intensive for large graphs due to pairwise index computation, less flexible than GNNs but more efficient for standard link prediction
- Failure signatures: Poor performance on graphs with very sparse attributes, overfitting on small datasets with many features, computational bottlenecks on extremely large graphs due to O(n²) pairwise computations, underperformance when graph structure is highly dynamic
- First 3 experiments: 1) Run PROXI on a small homophilic dataset (e.g., CORA) with default hyperparameters to verify basic functionality 2) Compare performance of structural-only vs domain-only vs combined indices on a dataset to observe synergy 3) Integrate PROXI with a simple GNN (GCN) on TEXAS dataset to verify the PROXI-GNN boost mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different machine learning classifiers compare in performance when using PROXI indices, and what makes XGBoost particularly effective?
- Basis in paper: [explicit] The authors conducted an ablation study comparing different ML classifiers (Logistic Regression, Naive Bayes, QDA, XGBoost) with PROXI indices, finding XGBoost generally outperformed the others.
- Why unresolved: While the authors showed XGBoost performed best overall, they didn't deeply investigate why XGBoost was superior or explore its hyperparameter sensitivity in detail.
- What evidence would resolve it: Systematic experiments varying XGBoost hyperparameters and comparing against other classifiers with optimized settings, along with theoretical analysis of why tree-based methods might better capture the structure of proximity features.

### Open Question 2
- Question: How would PROXI perform on datasets with node attributes in different formats beyond the three categories studied (binary, categorical, and real-valued vectors)?
- Basis in paper: [inferred] The authors note that their domain proximity indices depend on the format of node attribute vectors and acknowledge this as a limitation, suggesting their approach may not generalize to all possible attribute formats.
- Why unresolved: The authors only tested on datasets with attributes matching their predefined categories and didn't explore how their method would handle more complex or mixed attribute types.
- What evidence would resolve it: Testing PROXI on diverse datasets with attributes in various formats (e.g., text, images, time series) and developing domain proximity indices for these formats.

### Open Question 3
- Question: What is the theoretical justification for why combining structural and domain proximity indices leads to performance improvements beyond what either set achieves individually?
- Basis in paper: [explicit] The authors observed that combining structural and domain indices consistently improved performance across all datasets, but didn't provide a theoretical explanation for this synergy.
- Why unresolved: While empirical results showed benefits from combining indices, the paper didn't explore the theoretical foundations explaining why these different types of proximity information complement each other.
- What evidence would resolve it: Developing a theoretical framework that explains how different types of proximity information capture complementary aspects of graph structure and node relationships, and how their combination leads to better representations.

## Limitations
- Computational complexity of computing all 20 proximity indices for every node pair becomes prohibitive for very large graphs
- Performance heavily depends on the informativeness of node attributes, making it potentially less effective on graphs with sparse or uninformative features
- Requires domain knowledge to design effective proximity indices and may not generalize well to novel graph structures

## Confidence
- High confidence: PROXI's competitive performance on benchmark datasets, the empirical evidence showing PROXI-GNN integration improves GNN performance by up to 11%
- Medium confidence: The claim that proximity indices are more effective than learned embeddings due to the non-transitive nature of link prediction
- Medium confidence: The synergy between structural and domain indices - the paper provides evidence but doesn't fully explore which specific index combinations are most valuable

## Next Checks
1. Ablation study on index importance: Systematically remove individual proximity indices to quantify their contribution and identify the most critical features
2. Scalability benchmark: Test PROXI on progressively larger graphs to establish clear scaling limits and identify bottlenecks
3. Cross-dataset generalization: Train PROXI on one dataset type (e.g., citation networks) and evaluate on another (e.g., social networks) to assess domain transferability