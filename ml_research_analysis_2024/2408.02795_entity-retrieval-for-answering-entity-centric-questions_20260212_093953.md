---
ver: rpa2
title: Entity Retrieval for Answering Entity-Centric Questions
arxiv_id: '2408.02795'
source_url: https://arxiv.org/abs/2408.02795
tags:
- retrieval
- entity
- question
- documents
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Entity Retrieval, a novel approach for answering
  entity-centric questions that leverages salient entities in the question to directly
  retrieve relevant knowledge base articles rather than relying on similarity-based
  document retrieval. Experiments on two entity-centric question answering datasets
  show that Entity Retrieval outperforms both dense and sparse retrieval methods,
  achieving higher nDCG scores (e.g., 0.695 vs.
---

# Entity Retrieval for Answering Entity-Centric Questions

## Quick Facts
- arXiv ID: 2408.02795
- Source URL: https://arxiv.org/abs/2408.02795
- Authors: Hassan S. Shavarani; Anoop Sarkar
- Reference count: 40
- Primary result: Entity Retrieval outperforms dense and sparse retrieval methods on entity-centric question answering, achieving 0.695 nDCG vs 0.522 for BM25 on EntityQuestions dev set

## Executive Summary
This paper introduces Entity Retrieval, a novel approach for answering entity-centric questions that leverages salient entities in the question to directly retrieve relevant knowledge base articles rather than relying on similarity-based document retrieval. Experiments on two entity-centric question answering datasets show that Entity Retrieval outperforms both dense and sparse retrieval methods, achieving higher nDCG scores while retrieving fewer documents. The method also demonstrates better real-time efficiency, requiring less memory and disk space compared to dense retrieval methods.

## Method Summary
Entity Retrieval identifies salient entities in questions and uses them as direct pointers to retrieve corresponding Wikipedia articles. The method bypasses traditional similarity-based retrieval by looking up Wikipedia articles based on entity names, then truncates the articles to the first W words for use as augmentation documents. When entity annotations are unavailable, automatically identified entities through entity linking (using SPEL) are used instead. The approach is compared against BM25, DPR, and ANCE retrieval methods on two entity-centric QA datasets, measuring nDCG, EM/F1 scores, and resource usage metrics.

## Key Results
- Entity Retrieval achieves 0.695 nDCG on EntityQuestions dev set vs 0.522 for BM25
- The method retrieves fewer documents while maintaining higher accuracy compared to traditional retrieval
- Entity Retrieval requires 25% less response generation time and significantly less memory/disk space than BM25

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity Retrieval achieves higher accuracy by leveraging salient entities as direct pointers to relevant knowledge base articles
- Mechanism: Instead of matching questions to passages via similarity scoring, Entity Retrieval uses named entities in the question to retrieve Wikipedia articles corresponding to those entities, then truncates the articles to the first W words for use as augmentation documents
- Core assumption: Salient entities in the question directly correspond to the knowledge needed to answer entity-centric questions
- Evidence anchors:
  - [abstract] "depends on the salient entities within the question to identify the retrieval documents"
  - [section 3] "Entity Retrieval leverages the salient entities within the questions to identify and retrieve their corresponding knowledge base articles"
  - [corpus] "Average neighbor FMR=0.528" - corpus analysis shows moderate similarity to entity-centric QA methods, suggesting the approach is not entirely novel but fills a specific niche
- Break condition: When questions lack clear salient entities or when entity linking fails to identify the correct entities, Entity Retrieval performance degrades significantly

### Mechanism 2
- Claim: Entity Retrieval reduces noise by retrieving fewer, more focused documents compared to traditional retrieval methods
- Mechanism: By retrieving only Wikipedia articles for the identified entities rather than the top-k most similar passages, Entity Retrieval limits the number of documents and thus reduces the chance of including irrelevant information
- Core assumption: Including irrelevant documents degrades retrieval-augmented QA performance
- Evidence anchors:
  - [section 4.4] "Entity Retrieval effectively minimizes the retrieval of such documents" (referring to irrelevant documents)
  - [section 4.4] "nDCG scores along the x-axis...the likelihood of retrieving irrelevant documents also rises, leading to a decline in retrieval performance"
  - [corpus] Weak - no direct corpus evidence supporting the noise reduction claim specifically for this method
- Break condition: When questions require information from multiple entities or when the relevant information is not contained in the first W words of the entity's Wikipedia article

### Mechanism 3
- Claim: Entity Retrieval offers superior real-time efficiency by eliminating the need for large indexes and similarity computations
- Mechanism: Entity Retrieval bypasses the need to load and search through massive passage indexes by directly looking up Wikipedia articles based on entity names, significantly reducing memory and disk requirements
- Core assumption: Traditional dense and sparse retrieval methods require substantial computational resources due to index storage and similarity calculations
- Evidence anchors:
  - [section 4.7] "significantly more disk space to store its indexes, and over ten times higher main memory demands"
  - [section 4.7] "Entity Retrieval is 25% faster than BM25 in response generation while demanding the total memory and disk space of a standard personal computer"
  - [corpus] "Average neighbor citations=0.0" - indicates this efficiency advantage may be under-explored in related literature
- Break condition: When the knowledge base becomes extremely large or when entity lookups require complex disambiguation that negates the computational savings

## Foundational Learning

- Concept: Entity Linking
  - Why needed here: Entity Retrieval depends on accurately identifying and linking entities in questions to their corresponding knowledge base entries
  - Quick check question: Given the question "Who is the CEO of Microsoft?", what entity would an entity linker need to identify and where would it link it?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The paper builds on RAG architecture but proposes an alternative retrieval method specifically for entity-centric questions
  - Quick check question: In standard RAG, what is the typical flow from question to answer generation?

- Concept: Knowledge Base Question Answering (KBQA)
  - Why needed here: Understanding how entity information from knowledge bases has been traditionally used in QA helps contextualize Entity Retrieval's approach
  - Quick check question: How does KBQA typically differ from open-domain QA in terms of answer sources?

## Architecture Onboarding

- Component map:
  Question -> Entity Linking (SPEL or other method) -> Wikipedia article lookup -> Document truncation (first W words) -> LLM prompt -> Answer generation
- Critical path:
  1. Entity identification and linking
  2. Wikipedia article retrieval
  3. Document truncation and formatting
  4. LLM prompt construction and generation
- Design tradeoffs:
  - Shorter documents (50-100 words) reduce noise but may miss relevant information
  - Longer documents (300-1000 words) increase coverage but introduce more noise
  - Entity linking accuracy directly impacts retrieval quality
  - Wikipedia as knowledge base limits domain applicability
- Failure signatures:
  - Low EM/F1 scores when entity linker fails to identify relevant entities
  - Poor performance on questions requiring multi-entity reasoning
  - Degradation when answers are not in the first W words of entity articles
  - Inefficiency when entity disambiguation requires multiple lookups
- First 3 experiments:
  1. Compare Entity Retrieval (W=100) vs. BM25 vs. DPR on EntityQuestions dev set, measuring nDCG@4
  2. Test Entity Retrieval with different W values (50, 100, 300, 1000) on same dataset to find optimal truncation length
  3. Evaluate Entity Retrieval using SPEL-identified entities vs. ground truth entity annotations on EntityQuestions test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do ambiguities in entity linking affect the performance of Entity Retrieval, and what strategies can be employed to ensure the most contextually appropriate entities are selected?
- Basis in paper: [inferred] The paper acknowledges that entity linking can occasionally result in ambiguous entities and suggests that future studies should focus on ensuring the selection of the most contextually appropriate entities for retrieval.
- Why unresolved: The paper does not explore the impact of entity ambiguities on Entity Retrieval performance, nor does it propose specific strategies to address this issue.
- What evidence would resolve it: Experiments comparing Entity Retrieval performance with and without strategies to handle entity ambiguities, such as context-aware entity disambiguation techniques, would provide insights into the impact of ambiguities and potential solutions.

### Open Question 2
- Question: How does the performance of Entity Retrieval vary across different knowledge bases and ontologies, particularly in specialized domains such as medical or legal fields?
- Basis in paper: [inferred] The paper focuses on Wikipedia as the knowledge base, acknowledging the importance of exploring other knowledge bases and ontologies, especially in different domains like UMLS in the medical field.
- Why unresolved: The paper does not investigate the performance of Entity Retrieval using other knowledge bases or ontologies, limiting the generalizability of the findings to different domains.
- What evidence would resolve it: Comparative studies evaluating Entity Retrieval performance using various knowledge bases and ontologies, including domain-specific ones, would demonstrate its versatility and potential limitations in different contexts.

### Open Question 3
- Question: What is the impact of the length of retrieved documents on the performance of Entity Retrieval, and is there an optimal document length that balances information richness and model comprehension?
- Basis in paper: [explicit] The paper experiments with different document lengths (50, 100, 300, and 1000 words) and notes that longer documents do not necessarily enhance model recall, suggesting that beyond a certain length, the model may become overwhelmed by noise.
- Why unresolved: While the paper provides some insights into the impact of document length, it does not determine an optimal length that balances information richness and model comprehension across different question types and domains.
- What evidence would resolve it: Systematic experiments varying document lengths and analyzing their impact on Entity Retrieval performance for different question types and domains would help identify an optimal document length that maximizes retrieval accuracy and model comprehension.

## Limitations

- The method relies heavily on accurate entity linking, with performance degrading significantly when entities are not clearly identifiable
- The approach is constrained to Wikipedia as a knowledge base, limiting applicability to domains where Wikipedia coverage is insufficient
- The truncation strategy may miss relevant information for complex questions requiring deeper context from full articles

## Confidence

- **High Confidence**: The efficiency claims regarding memory and disk space requirements are well-supported by direct comparisons in section 4.7, showing concrete metrics for Entity Retrieval versus BM25 and DPR
- **Medium Confidence**: The accuracy improvements demonstrated through nDCG scores are compelling, but the analysis assumes Wikipedia articles contain sufficient information for answering entity-centric questions, which may not hold for all question types
- **Medium Confidence**: The noise reduction claim is supported by nDCG trends in section 4.4, but the corpus evidence is weak, and the analysis could benefit from more direct comparisons of document relevance distributions

## Next Checks

1. **Multi-Entity Question Performance**: Test Entity Retrieval on questions requiring information from multiple entities to assess whether the method's entity-centric approach limits its ability to handle complex reasoning across entities

2. **Entity Linking Robustness Analysis**: Systematically evaluate Entity Retrieval performance when using automatically identified entities (via SPEL) versus ground truth entity annotations to quantify the impact of entity linking accuracy on overall performance

3. **Optimal Truncation Length Validation**: Conduct experiments with varying truncation lengths (W values) on both datasets to determine whether the default 100-word truncation is optimal or if different question types benefit from different document lengths