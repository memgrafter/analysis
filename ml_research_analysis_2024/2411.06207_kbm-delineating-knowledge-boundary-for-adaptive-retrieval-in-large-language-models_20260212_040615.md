---
ver: rpa2
title: 'KBM: Delineating Knowledge Boundary for Adaptive Retrieval in Large Language
  Models'
arxiv_id: '2411.06207'
source_url: https://arxiv.org/abs/2411.06207
tags:
- knowledge
- performance
- unknown
- accuracy
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KBM, a method to determine whether a large
  language model (LLM) needs retrieval-augmented generation (RAG) for answering a
  question. KBM models the LLM's knowledge boundary by assessing its accuracy and
  certainty on a query, using thresholds to classify questions as "known" or "unknown."
  For known queries, the LLM answers directly; for unknown ones, RAG is triggered.
---

# KBM: Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models

## Quick Facts
- arXiv ID: 2411.06207
- Source URL: https://arxiv.org/abs/2411.06207
- Authors: Zhen Zhang; Xinyu Wang; Yong Jiang; Zile Qiao; Zhuo Chen; Guangyu Li; Feiteng Mu; Mengting Hu; Pengjun Xie; Fei Huang
- Reference count: 27
- Primary result: KBM reduces RAG retrieval ratio by 13.5% to 32.5% while maintaining or improving QA performance across 11 English and Chinese datasets.

## Executive Summary
This paper introduces KBM (Knowledge Boundary Model), a method to determine when a large language model needs retrieval-augmented generation (RAG) for answering questions. KBM learns to classify queries as "known" (answerable from LLM's internal knowledge) or "unknown" (requiring retrieval) by assessing both accuracy and certainty on sampled responses. By acting as a gatekeeper that selectively triggers RAG, KBM significantly reduces retrieval costs while maintaining or slightly improving end-to-end performance. The method is validated across 11 diverse datasets and demonstrates effectiveness in dynamic knowledge, long-tail static knowledge, and multi-hop reasoning scenarios.

## Method Summary
KBM models an LLM's knowledge boundary by assessing its accuracy and certainty on a given query. During training, it generates soft labels by sampling multiple LLM responses to the same query and applies a threshold (τ=0.9) to classify queries as known or unknown. The resulting labeled dataset fine-tunes a small classifier that, at inference time, decides whether to trigger RAG. KBM uses two approaches: accuracy-based (requiring ground truth labels) and certainty-based (using entropy of 30 sampled responses as a label-free proxy). The method can be plugged into existing RAG systems and was evaluated using Qwen2-7B-Instruct fine-tuned on formatted QA pairs derived from soft labels.

## Key Results
- Reduces RAG retrieval ratio by 13.5% to 32.5% across 11 English and Chinese datasets
- Maintains or slightly improves end-to-end QA performance (EM/Accuracy) compared to baseline RAG
- Demonstrates effectiveness in dynamic knowledge, long-tail static knowledge, and multi-hop reasoning scenarios
- Can be plugged into other LLMs to improve retrieval efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KBM reduces retrieval by learning to classify questions as "known" or "unknown" based on LLM accuracy and certainty.
- Mechanism: During training, KBM generates soft labels by sampling multiple LLM responses to the same query, then sets a threshold (e.g., τ=0.9) to classify each query. The labeled dataset fine-tunes a classifier that decides whether to trigger RAG at inference time.
- Core assumption: LLM responses to the same query at temperature T=1 produce a stable distribution of correct/incorrect answers that correlates with whether the query is in-domain knowledge.
- Evidence anchors: [abstract]: "propose a Knowledge Boundary Model (KBM) to express the known/unknown of a given question"; [section]: "The key to implementing this solution is how to enable LLM express confidence"
- Break condition: If the LLM's knowledge is non-stationary (e.g., after fine-tuning on new data), the threshold may become invalid.

### Mechanism 2
- Claim: Certainty (entropy-based) captures uncertainty without requiring ground-truth answers.
- Mechanism: For each query, KBM samples 30 responses, computes word/phrase entropy, and normalizes it to a certainty score. High entropy → low certainty → unknown query → trigger RAG.
- Core assumption: Entropy of sampled responses is a reliable proxy for whether the query is in the LLM's knowledge boundary.
- Evidence anchors: [section]: "we compute the entropy distribution of words and phrases from the 30 generated answers to establish the model's level of certainty"; [section]: "We find a Pearson correlation coefficient of 0.64 between accuracy and certainty"
- Break condition: If the LLM's generation process is highly deterministic (low temperature), entropy may not vary enough to distinguish known from unknown.

### Mechanism 3
- Claim: KBM improves end-to-end performance by selectively applying RAG only when beneficial.
- Mechanism: KBM acts as a gatekeeper: if the query is "known," LLM answers directly; if "unknown," RAG is triggered. This avoids harmful RAG effects on high-confidence queries.
- Core assumption: A single binary decision (known/unknown) is sufficient to optimize the tradeoff between retrieval cost and accuracy.
- Evidence anchors: [abstract]: "significantly decreasing the proportion of retrievals required for optimal end-to-end performance"; [section]: "the KBM method, based on Accuracy and Certainty, demonstrates significant enhancements across several intervals"
- Break condition: If RAG provides marginal gains even for "known" queries (e.g., due to context-sensitive reasoning), the binary gate may be too coarse.

## Foundational Learning

- Concept: **Uncertainty quantification in generative models**
  - Why needed here: KBM relies on measuring how uncertain the LLM is about a query, either via accuracy sampling or entropy of responses.
  - Quick check question: Given two queries A and B, if LLM's sampled responses to A are all identical and correct while B's vary widely, which one should KBM classify as "known"?
    - Answer: A, because certainty is high and accuracy is stable.

- Concept: **Soft labeling for knowledge boundary detection**
  - Why needed here: The paper constructs training data without explicit human labels by deriving "known/unknown" status from LLM's own performance metrics.
  - Quick check question: If an LLM achieves 85% accuracy on a query across 30 samples, should it be labeled "known" or "unknown" under a τ=0.9 threshold?
    - Answer: "Unknown", because 0.85 < 0.9.

- Concept: **Retrieval-augmented generation pipeline**
  - Why needed here: KBM plugs into an existing RAG system; understanding how RAG works (retrieve → concatenate → generate) is essential to reason about its cost-benefit trade-off.
  - Quick check question: In a standard RAG pipeline, what happens to the query before it is passed to the generator?
    - Answer: Retrieved context chunks are concatenated to the query as additional input tokens.

## Architecture Onboarding

- Component map: User query → KBM prompt wrapper → KBM model (accuracy/certainty scorer) → Threshold decision → (if unknown) Google retriever → RAG prompt → Answer generator → Final answer
- Critical path: KBM inference → threshold check → (conditional) RAG retrieval → answer generation
- Design tradeoffs:
  - Binary gating vs. continuous confidence score: binary is simpler and cheaper to implement but may misclassify edge cases
  - Accuracy-based vs. certainty-based: accuracy needs ground truth labels, certainty is label-free but noisier
  - Sampling budget: 30 samples per query gives stable statistics but increases upfront cost
- Failure signatures:
  - High false-positive rate (known queries incorrectly triggering RAG) → threshold too low or certainty miscalibrated
  - High false-negative rate (unknown queries not triggering RAG) → threshold too high or sampling insufficient
  - Degraded performance after model update → training data distribution shift
- First 3 experiments:
  1. Ablation on threshold τ: sweep τ from 0.7 to 0.95 on a dev set and plot retrieval ratio vs. accuracy drop
  2. Comparison of accuracy vs. certainty: train two KBMs, one with each labeling method, and compare end-to-end performance on a balanced test set
  3. Effect of sampling budget: vary I from 5 to 50 samples and measure stability of KBM predictions and inference latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the effectiveness of KBM be improved for long-tail static knowledge across diverse question formats?
- Basis in paper: [inferred] The paper mentions that KBM shows only marginal improvements over the random baseline for multiple-choice questions in CMMLU, MMLU, and OBQA datasets, suggesting challenges in sourcing relevant information for these formats.
- Why unresolved: The paper does not provide a detailed exploration of methods to enhance KBM's performance for long-tail knowledge, particularly in diverse question formats beyond short answer and reading comprehension tasks.
- What evidence would resolve it: Experimental results demonstrating improved performance of KBM on long-tail knowledge across various question formats, especially multiple-choice, would provide evidence of effective methods to enhance KBM's capabilities.

### Open Question 2
- Question: How does the performance of KBM vary across different LLM parameter sizes and knowledge boundaries?
- Basis in paper: [explicit] The paper notes that KBM is validated on 11 datasets and demonstrates comparable effectiveness with All RAG, but also mentions that the varying knowledge boundaries of LLMs significantly influence the enhancement effects observed with KBM.
- Why unresolved: The paper does not provide a comprehensive analysis of how KBM's performance varies with different LLM sizes and knowledge boundaries, which could affect its generalizability and effectiveness.
- What evidence would resolve it: Detailed comparative studies showing KBM's performance across a range of LLM sizes and knowledge boundaries would clarify its adaptability and effectiveness in different contexts.

### Open Question 3
- Question: What are the potential methods to further reduce the proportion of retrievals that trigger RAG while maintaining high performance?
- Basis in paper: [explicit] The paper highlights that KBM reduces the retrieval ratio by 13.5% to 32.5% while maintaining or slightly improving end-to-end performance, but also acknowledges the need for further optimization of the RAG pipeline.
- Why unresolved: The paper suggests room for improvement in reducing retrieval triggers but does not explore specific methods or strategies to achieve this goal beyond the current implementation.
- What evidence would resolve it: Experimental results showcasing innovative methods or strategies that further reduce the retrieval ratio while maintaining or enhancing performance would provide evidence of potential improvements in KBM's efficiency.

## Limitations
- Performance gains vary significantly across different LLMs when using KBM as plug-in, with larger improvements observed when KBM's knowledge boundary differs substantially from the generative LLM's boundary
- Shows only marginal improvements for multiple-choice datasets (CMMLU, MMLU, OBQA) due to challenges sourcing relevant information for this format
- The 30-sample entropy estimate's robustness across different domains and temperature settings requires more rigorous sensitivity analysis

## Confidence
- **High confidence**: The core mechanism of using accuracy and certainty thresholds to gate RAG retrieval is well-supported by ablation studies and quantitative results across 11 datasets
- **Medium confidence**: The entropy-based certainty measure as a reliable proxy for knowledge boundary detection is supported by correlation analysis (Pearson r=0.64) but lacks external validation
- **Medium confidence**: The plug-and-play capability with other LLMs is demonstrated on two additional models but lacks systematic evaluation across different model families, sizes, and domains

## Next Checks
1. **Temporal robustness test**: Retrain KBM on LLM checkpoints from different time periods (e.g., pre- and post-fine-tuning) and measure performance degradation to assess sensitivity to knowledge distribution shifts
2. **Continuous gating experiment**: Replace the binary known/unknown decision with a continuous confidence score that modulates the weight of retrieved context, comparing end-to-end performance against the current threshold-based approach
3. **Sampling budget sensitivity**: Conduct a systematic ablation study varying the number of samples (I=5, 10, 15, 20, 30, 50) to identify the point of diminishing returns in both accuracy and computational efficiency, particularly for high-throughput deployment scenarios