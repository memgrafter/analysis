---
ver: rpa2
title: Procedure-Aware Surgical Video-language Pretraining with Hierarchical Knowledge
  Augmentation
arxiv_id: '2410.00263'
source_url: https://arxiv.org/abs/2410.00263
tags:
- surgical
- pretraining
- knowledge
- visual
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a surgical video-language pretraining framework
  that addresses the knowledge domain gap and data scarcity in surgical datasets.
  The authors introduce a hierarchical knowledge augmentation approach using large
  language models to enrich and refine surgical texts at multiple levels (narration,
  keystep, and abstract), and develop a Procedure-Encoded Surgical Knowledge-Augmented
  Video-Language Pretraining (PeskaVLP) framework that combines language supervision
  with visual self-supervision.
---

# Procedure-Aware Surgical Video-language Pretraining with Hierarchical Knowledge Augmentation

## Quick Facts
- arXiv ID: 2410.00263
- Source URL: https://arxiv.org/abs/2410.00263
- Reference count: 40
- Key outcome: Proposes PeskaVLP framework achieving state-of-the-art zero-shot surgical phase recognition (45.1%/34.2 accuracy/F1 on Cholec80)

## Executive Summary
This paper addresses the knowledge domain gap and data scarcity challenges in surgical video-language pretraining by introducing a hierarchical knowledge augmentation approach. The authors develop PeskaVLP, a procedure-aware framework that combines language supervision with visual self-supervision using a Dynamic Time Warping-based loss function. The method enriches surgical texts at multiple levels using large language models and achieves strong zero-shot performance across multiple surgical phase recognition datasets.

## Method Summary
The approach involves hierarchical knowledge augmentation using LLM to enrich surgical texts at narration, keystep, and abstract levels, followed by a dual-encoder architecture combining ResNet50 visual encoder with ClinicalBert textual encoder. The PeskaVLP framework applies clip-level language and visual self-supervision objectives, phase-level contrastive learning, and video-level DTW-based alignment. Training alternates across these hierarchies to capture procedural dependencies and temporal alignment between video frames and text sequences.

## Key Results
- Achieves state-of-the-art zero-shot phase recognition accuracy/F1 scores of 45.1%/34.2 on Cholec80
- Demonstrates strong generalizability with 26.5%/23.6 on AutoLaparo and 46.7%/28.6 on StrasBypass70
- Shows superior cross-modal retrieval performance compared to baseline methods
- Provides robust visual representations for downstream surgical scene understanding tasks

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical knowledge augmentation reduces textual information loss by enriching surgical concepts at multiple levels using LLM prompts with different behaviors (recipe, dictionary, summarizer). This approach leverages LLM internal knowledge as a reliable surgical knowledge base to correct errors and expand semantic coverage without distorting original meaning.

### Mechanism 2
Combining language supervision with visual self-supervision improves data efficiency and robustness in clip-level pretraining. The PeskaVLP framework adds SimSiam-based visual self-supervision to the contrastive language loss, providing complementary high-quality signals that address noisy ASR-transcribed narrations in small surgical datasets.

### Mechanism 3
Procedure-aware pretraining with DTW-based loss captures temporal dependencies in surgical workflows. The framework applies Dynamic Time Warping to align video frames with text sequences at phase/video levels, using temporally reversed texts as hard negatives to leverage the assumption that surgical actions follow routine order.

## Foundational Learning

- **Concept: Contrastive Learning with InfoNCE**
  - Why needed here: Forms the core objective for matching video clips to their corresponding texts across all hierarchies
  - Quick check question: What is the role of the temperature parameter β in the InfoNCE loss?

- **Concept: Dynamic Time Warping (DTW)**
  - Why needed here: Aligns video frames to text sequences while handling temporal misalignments common in surgical procedures
  - Quick check question: How does DTW differ from simple frame-wise cosine similarity in aligning sequences?

- **Concept: Self-Supervised Learning (SimSiam)**
  - Why needed here: Provides visual-only supervision that complements noisy ASR narrations and improves data efficiency
  - Quick check question: What is the key difference between SimSiam and contrastive self-supervised methods like MoCo?

## Architecture Onboarding

- **Component map:** Visual encoder (ResNet50) -> Textual encoder (ClinicalBert) -> Clip-level loss (LecNCE_clip) -> Phase-level loss (LecNCE_phase) -> Video-level loss (LecNCE_video) -> LLM augmentation modules

- **Critical path:** Pretraining → Hierarchical knowledge augmentation → Multi-level loss application → Evaluation on surgical phase recognition / cross-modal retrieval

- **Design tradeoffs:**
  - ResNet50 vs ViT: ResNet50 chosen for better performance and lower parameter count in surgical domain
  - Single encoder vs separate encoders: Single set of encoders shared across hierarchies to ensure consistent feature space
  - DTW vs attention: DTW chosen for direct alignment without extra temporal modules

- **Failure signatures:**
  - Degraded zero-shot performance → likely noisy text or poor initialization
  - Overfitting on SVL → insufficient augmentation diversity or too many epochs
  - Poor cross-modal retrieval → modality gap not closed or alignment loss ineffective

- **First 3 experiments:**
  1. Ablation: Remove LLM augmentation, retrain, compare zero-shot Cholec80 accuracy
  2. Ablation: Remove visual self-supervision (Lvv_clip), retrain, compare few-shot linear probing
  3. Ablation: Replace DTW loss with simple contrastive loss, retrain, compare video-abstract retrieval R@1

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several important questions remain:

### Open Question 1
How would the PeskaVLP framework perform on surgical datasets with significantly different procedural routines than those in the pretraining dataset? The paper evaluates on three datasets covering different procedures but doesn't explore performance on datasets with substantially different procedural routines like cardiac surgery or neurosurgery.

### Open Question 2
How does the choice of LLM affect the quality of knowledge augmentation and subsequent model performance? The paper uses one LLM for augmentation without comparing to alternatives or analyzing sensitivity to the specific LLM choice.

### Open Question 3
What is the optimal balance between language supervision and visual self-supervision in the clip-level pretraining objective? The paper sets fixed loss weights without exploring how different weightings affect performance.

### Open Question 4
How does the PeskaVLP model's performance change when applied to surgical videos with varying levels of surgical expertise in the narration? The paper uses lecture videos but doesn't investigate whether expert-level vs. student-level narrations impact the learned representations.

## Limitations
- The LLM-based hierarchical knowledge augmentation relies heavily on unspecified prompt templates and model choices, creating uncertainty about reproducibility and potential hallucination risks
- The DTW-based alignment assumes surgical procedures follow routine order, which may not hold for complex or variant cases, limiting generalizability
- Performance on AutoLaparo (26.5% accuracy) remains notably lower than other datasets, suggesting domain or procedural differences not fully addressed by the method

## Confidence

- **High confidence**: Claims about improved zero-shot performance over baselines on Cholec80 and StrasBypass70; claims about the general architecture combining language supervision with visual self-supervision
- **Medium confidence**: Claims about the effectiveness of LLM-based hierarchical knowledge augmentation in enriching surgical texts; claims about DTW-based loss capturing procedural alignment
- **Low confidence**: Claims about the specific LLM model and prompt templates used; claims about the exact implementation of DTW loss and frame-text pairing strategy

## Next Checks

1. **Prompt reproducibility**: Reconstruct the exact LLM prompts and models used for hierarchical knowledge augmentation, then measure text quality improvements (semantic coverage, factual consistency) before and after augmentation

2. **DTW robustness test**: Apply the DTW-based loss to surgical videos with known procedural variations (e.g., different entry points or tool usage orders) and measure alignment accuracy degradation

3. **Cross-center generalizability**: Test zero-shot phase recognition on surgical videos from additional clinical centers or countries not seen during pretraining to assess true domain generalization