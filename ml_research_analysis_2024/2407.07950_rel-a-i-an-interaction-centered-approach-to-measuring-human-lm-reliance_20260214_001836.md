---
ver: rpa2
title: 'Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM Reliance'
arxiv_id: '2407.07950'
source_url: https://arxiv.org/abs/2407.07950
tags:
- reliance
- human
- what
- language
- rely
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Rel-A.I., an interaction-centered framework
  for evaluating human reliance on large language model (LLM) outputs. The method
  measures reliance through a gamified trivia task where participants decide whether
  to trust LLM-generated answers based on epistemic markers like "I'm certain it's...".
---

# Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM Reliance

## Quick Facts
- arXiv ID: 2407.07950
- Source URL: https://arxiv.org/abs/2407.07950
- Authors: Kaitlyn Zhou; Jena D. Hwang; Xiang Ren; Nouha Dziri; Dan Jurafsky; Maarten Sap
- Reference count: 26
- One-line primary result: Human reliance on LLM outputs is significantly influenced by interactional context including model presentation, prior interaction patterns, and subject domain.

## Executive Summary
This paper introduces Rel-A.I., an interaction-centered framework for evaluating human reliance on large language model (LLM) outputs. The method measures reliance through a gamified trivia task where participants decide whether to trust LLM-generated answers based on epistemic markers like "I'm certain it's...". The study examines three interactional contexts: model presentation (e.g., greetings), prior interaction patterns, and subject domain. Results show that human reliance is significantly influenced by these contextual factors—people rely 30% more on models perceived as competent, rely differently on identical expressions depending on the model's typical confidence level, and show 5-9% higher reliance for computational domains like math versus non-computational subjects.

## Method Summary
The study employs a gamified trivia task where participants answer questions and can choose to use LLM-generated responses, gaining or losing points based on their reliance decisions. Three experiments were conducted with 50 participants each, recruited via Prolific. The framework uses epistemic markers from 9 publicly deployed LLMs and measures reliance rates alongside meta-level perception questions about the AI agent's warmth, competence, and humanlikeness. The method specifically examines how model presentation (greetings), prior interaction patterns (confidence distribution), and subject domain influence human reliance behaviors.

## Key Results
- People rely 30% more on models perceived as competent and show strong correlations (Pearson's r: 0.78-0.96) between perceived attributes and reliance rates
- Identical epistemic markers trigger different reliance behaviors depending on the model's typical confidence level (e.g., 20% more reliance in Bunconf vs Bconf conditions)
- Computational domains (math, abstract algebra) show 5-9% higher reliance rates compared to non-computational subjects like law or philosophy
- Standard calibration metrics are insufficient for evaluating LLM safety as identical expressions can trigger vastly different reliance behaviors depending on interaction context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human reliance on LLM outputs varies significantly based on the perceived warmth, competence, and humanlikeness of the model.
- Mechanism: The introduction of greetings and social cues alters user perceptions of the model's attributes, which in turn influences their willingness to rely on the model's outputs. This is because humans form mental models of AI agents based on repeated interactions and attribute characteristics like warmth and competence to them.
- Core assumption: Perceived attributes of AI agents (warmth, competence, humanlikeness) directly correlate with human reliance behaviors.
- Evidence anchors:
  - [abstract] "We find that contextual characteristics significantly affect human reliance behavior. For example, people rely 30% more on LMs that are perceived as more competent."
  - [section] "We cluster users based on their perception of Agreet’s attributes and observe the reliance rates in each cluster. Figure 3 illustrates that perceptions of warmth, competence, and humanlikeness are all strongly correlated with changes to reliance rates (Pearson’s r: 0.78, 0.96, and 0.93 respectively)."
- Break condition: If the perceived attributes of the AI agent do not correlate with human reliance, or if the introduction of greetings does not alter user perceptions.

### Mechanism 2
- Claim: The distribution of confidence levels in an LLM's prior responses affects how users interpret and rely on subsequent moderate-confidence expressions.
- Mechanism: Users form mental models of an LLM's typical confidence level based on prior interactions. When a moderate-confidence expression appears from a typically confident model, it is interpreted as less reliable than the same expression from a typically unconfident model. This is because users adapt their interpretation of epistemic markers based on the model's historical behavior.
- Core assumption: Users interpret epistemic markers (expressions of uncertainty) in the context of the model's overall confidence distribution.
- Evidence anchors:
  - [abstract] "Statements of moderate certainty appearing in a highly certain model (as verbalized) will be relied on significantly less than the same statement appearing from a typically not confident model."
  - [section] "Specifically, we see expressions like 'I’m pretty sure it’s...', 'I think it’s...', and 'It’s likely it’s...', relied upon 20%, 18%, and 15% more frequently in Bunconf than in Bconf."
- Break condition: If users do not adapt their interpretation of epistemic markers based on the model's historical confidence distribution, or if the distribution of confidence levels does not affect reliance.

### Mechanism 3
- Claim: The domain of the interaction (e.g., computational vs. non-computational) influences human reliance on LLM outputs.
- Mechanism: Humans have preconceived notions about AI proficiency in different domains. They tend to rely more on LLM outputs in computational domains (like math) compared to non-computational domains (like law or philosophy), regardless of the model's expressed confidence. This is because humans view AI agents as machines proficient in calculations.
- Core assumption: Human reliance on LLM outputs is influenced by the perceived domain expertise of the model.
- Evidence anchors:
  - [abstract] "Responses in domains which are most computational heavy (i.e., math and abstract algebra) are relied on more frequently than those in non-computational subjects."
  - [section] "For example, the expression 'I would say it’s...' is relied on 17% more often among a math question (e.g., 'Determine whether the polynomial in Z[x] satisfies an Eisenstein criterion...') than a question from world religion (e.g., 'According to Jaina traditions, what does the term ajiva mean?')."
- Break condition: If the domain of the interaction does not influence human reliance on LLM outputs, or if humans do not have preconceived notions about AI proficiency in different domains.

## Foundational Learning

- Concept: Epistemic markers (expressions of uncertainty and confidence)
  - Why needed here: Understanding how epistemic markers influence human reliance is central to the study. The paper examines how different expressions of uncertainty (e.g., "I'm certain it's...", "I think it might be...") affect user trust and reliance on LLM outputs.
  - Quick check question: What is the difference between a "strengthener" and a "weakener" in the context of epistemic markers?

- Concept: Calibration in machine learning
  - Why needed here: The paper contrasts traditional calibration metrics (aligning model probabilities with accuracy) with its interaction-centered approach. Understanding calibration is crucial for grasping why the authors argue that standard metrics are insufficient.
  - Quick check question: What is the difference between linguistic calibration and numerical calibration?

- Concept: Human-computer interaction (HCI) and mental models
  - Why needed here: The paper draws on HCI literature to explain how humans form mental models of AI agents and how these models influence reliance behaviors. Understanding mental models is key to interpreting the experimental results.
  - Quick check question: How do mental models of AI agents influence human reliance behaviors according to HCI research?

## Architecture Onboarding

- Component map: Rel-A.I. framework consists of three core components: (1) self-incentivized trivia task with point-based scoring, (2) epistemic markers from publicly deployed LLMs, (3) meta-level perception questions about AI agent attributes
- Critical path: Designing engaging gamified task, selecting representative epistemic markers, analyzing correlation between user perceptions and reliance behaviors
- Design tradeoffs: Trades experimental control for ecological validity; predefined epistemic markers may limit generalizability
- Failure signatures: Low engagement with gamified task, unrepresentative epistemic markers, insignificant correlation between perceptions and reliance behaviors
- First 3 experiments:
  1. Varying the presentation of the model (RQ1): Test how greetings and social cues affect user perceptions and reliance behaviors
  2. Influence of prior interactions (RQ2): Test how confidence distribution in prior responses affects interpretation of moderate-confidence expressions
  3. Domain-specific reliance (RQ3): Test how computational vs. non-computational domains influence reliance on LLM outputs

## Open Questions the Paper Calls Out

- How do multi-turn interactions with LLMs affect human reliance compared to single-turn interactions?
- How does reliance on LLM-generated epistemic markers vary across different cultural contexts and languages?
- How does the modality of LLM interactions (text, speech, movement) influence human reliance on epistemic markers?

## Limitations

- The sample size of 50 participants per experiment may not capture full diversity of human-AI interaction patterns across demographics
- The gamified trivia task may not fully replicate the stakes and complexity of real-world decision-making scenarios
- Findings may not generalize to non-English speaking populations or different cultural contexts
- The study focuses on epistemic markers from publicly deployed LLMs, which may not represent full range of uncertainty expressions in production systems

## Confidence

- **High confidence**: The core finding that human reliance on LLM outputs is significantly influenced by interactional context (presentation, prior interactions, and domain) is well-supported by experimental results and aligns with HCI research
- **Medium confidence**: Specific magnitude of reliance differences (e.g., 30% more reliance on perceived competent models) may vary in different populations or real-world settings
- **Low confidence**: Extrapolation that standard calibration metrics are "insufficient" for LLM safety evaluation requires additional evidence from real-world deployment scenarios

## Next Checks

1. **External validation study**: Replicate experiments with diverse participant pool (different ages, cultural backgrounds, technical expertise) to test generalizability of contextual reliance patterns

2. **Real-world scenario testing**: Adapt Rel-A.I. framework to evaluate reliance in high-stakes domains (medical diagnosis, legal advice) where consequences of over/underreliance are more severe than trivia games

3. **Longitudinal interaction analysis**: Extend framework to study how reliance patterns evolve over extended interactions with same LLM model, testing whether initial perceptions persist or change with continued use