---
ver: rpa2
title: 'A Benchmark for Maximum Cut: Towards Standardization of the Evaluation of
  Learned Heuristics for Combinatorial Optimization'
arxiv_id: '2406.11897'
source_url: https://arxiv.org/abs/2406.11897
tags:
- heuristics
- eco-dqn
- instances
- performance
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MaxCut-Bench, an open-source benchmark suite
  for evaluating heuristics on the Maximum Cut problem. The benchmark includes diverse
  graph datasets and implementations of both traditional and learned heuristics.
---

# A Benchmark for Maximum Cut: Towards Standardization of the Evaluation of Learned Heuristics for Combinatorial Optimization

## Quick Facts
- arXiv ID: 2406.11897
- Source URL: https://arxiv.org/abs/2406.11897
- Authors: Ankur Nath; Alan Kuhnle
- Reference count: 40
- One-line primary result: Tabu Search consistently outperforms most learned heuristics for Maximum Cut, suggesting the need for better baselines in ML-based CO research.

## Executive Summary
This paper introduces MaxCut-Bench, an open-source benchmark suite for evaluating heuristics on the Maximum Cut problem. The benchmark includes diverse graph datasets and implementations of both traditional and learned heuristics. The authors systematically evaluate several learned heuristics, including S2V-DQN, ECO-DQN, and others, against traditional methods. Key findings show that Tabu Search, a simple local search heuristic, consistently outperforms most learned heuristics in terms of objective value, scalability, and generalization. Additionally, a naive greedy algorithm often matches or exceeds the performance of learned methods. The study highlights the importance of carefully chosen baselines and suggests that Tabu Search should be included in future evaluations. The benchmark aims to standardize research in machine learning for combinatorial optimization problems.

## Method Summary
The paper establishes a comprehensive benchmark for Maximum Cut by creating MaxCut-Bench, which includes implementations of traditional heuristics (Forward Greedy, Reversible Greedy, Tabu Search, Extremal Optimization) and learned heuristics (S2V-DQN, ECO-DQN, LS-DQN, Gflow-CombOpt, RUN-CSP, ANYCSP). The benchmark evaluates these algorithms across diverse graph distributions including Erdös-Rényi, Barabási-Albert, Watts-Strogatz, Holme-Kim, GSet, Physics, and Sherrington-Kirkpatrick instances ranging from 70 to 2000 vertices. The experimental setup involves training on 4000 random graphs, validating on 50 held-out graphs, and testing on 100 instances per distribution. Performance is measured using average approximation ratios normalized by best-known solution values.

## Key Results
- Tabu Search consistently outperforms all evaluated learned heuristics except ANYCSP across diverse graph distributions
- Simple greedy algorithms often match or exceed the performance of learned methods
- ECO-DQN's performance can be matched by replacing its GNN with linear regression on Tabu Search-related features
- ANYCSP, a reinforcement learning approach, shows promising results but requires significantly higher computational resources

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Deep learning heuristics that combine a GNN with a traditional heuristic can outperform the traditional heuristic alone.
- **Mechanism:** The GNN learns a policy to guide the selection of vertices during the greedy construction, potentially finding better moves than the heuristic's default selection rule.
- **Core assumption:** The GNN can learn from the distribution of problem instances to make better vertex selections than simple greedy rules.
- **Evidence anchors:**
  - [abstract] "S2V-DQN, which has become a highly influential work with thousands of citations. It may be thought as the forward greedy algorithm, guided by a GNN."
  - [section] "The key distinction lies in how they select vertices: while Forward Greedy opts for the vertex that maximizes objective value increase, S2V-DQN delegates this decision to its GNN."
- **Break Condition:** If the training data distribution doesn't contain sufficient diversity or the GNN cannot learn the underlying structure, the learned policy may perform no better than the heuristic's default rule.

### Mechanism 2
- **Claim:** Incorporating features related to Tabu Search into ECO-DQN allows it to learn a search behavior similar to Tabu Search without requiring a GNN.
- **Mechanism:** The handcrafted features capture the essence of Tabu Search's memory mechanism, and a simple linear regression on these features can approximate the GNN's learned policy.
- **Core assumption:** The Tabu Search-related features are sufficient to encode the information needed for effective local search, making the GNN's nonlinear function approximation unnecessary.
- **Evidence anchors:**
  - [abstract] "we find that the performance of ECO-DQN remains the same or is improved if the GNN is replaced by a simple linear regression on a subset of the features that are related to Tabu Search."
  - [section] "ECO-DQN employs seven handcrafted features per vertex for its state space. Upon closer examination, we find that two of these features are closely related to the traditional heuristic, Tabu Search."
- **Break Condition:** If the Tabu Search-related features do not capture all the necessary information for effective search, or if the linear regression cannot adequately model the relationship between features and optimal moves, the performance may degrade.

### Mechanism 3
- **Claim:** ANYCSP, a global search heuristic, consistently outperforms local search heuristics like Tabu Search.
- **Mechanism:** ANYCSP uses a reinforcement learning approach to learn a global search strategy that can escape local optima more effectively than local search methods.
- **Core assumption:** The reinforcement learning agent can learn a policy that balances exploration and exploitation across the entire solution space, leading to better solutions than local search.
- **Evidence anchors:**
  - [abstract] "ANYCSP, a global search heuristic, is the best performing of the evaluated heuristics, indicating that a reinforcement learning approach to general constraint satisfaction for CO problems is a promising direction for future research."
  - [section] "we analyze ANYCSP in its default configuration and observe that it consistently finds near-optimal solutions (that is, solutions near the value found by the Quantum Annealing algorithms), and superior in value to TS and the other traditional local search algorithms."
- **Break Condition:** If the state representation or reward function is not well-suited to the problem, or if the learning process is inefficient, the agent may not learn an effective global search strategy.

## Foundational Learning

- **Concept:** Graph Neural Networks (GNNs) for learning node/graph representations
  - Why needed here: GNNs are used to learn policies for selecting vertices during greedy construction and to guide local search in the evaluated heuristics.
  - Quick check question: What are the key operations in a GNN that allow it to aggregate information from a node's neighbors?

- **Concept:** Reinforcement Learning (RL) for policy learning
  - Why needed here: RL is used to train the agents in S2V-DQN, ECO-DQN, LS-DQN, RUN-CSP, and ANYCSP to learn policies for vertex selection and solution construction.
  - Quick check question: What is the difference between value-based and policy-based RL methods, and which is more suitable for the vertex selection problem?

- **Concept:** Local Search Heuristics (e.g., Tabu Search, Greedy algorithms)
  - Why needed here: These are the baseline algorithms against which the learned heuristics are compared, and understanding their mechanisms is crucial for interpreting the results.
  - Quick check question: What is the role of the tabu tenure parameter in Tabu Search, and how does it affect the search process?

## Architecture Onboarding

- **Component map:** Data generation → Preprocessing → Model training/evaluation → Performance comparison → Analysis
- **Critical path:** Data generation → Preprocessing → Model training/evaluation → Performance comparison → Analysis
- **Design tradeoffs:**
  - Model complexity vs. training time: More complex models may achieve better performance but require longer training times.
  - Generalization vs. specialization: Models trained on specific distributions may not generalize well to unseen distributions.
  - Exploration vs. exploitation: The balance between exploring new solutions and exploiting known good solutions affects the search process.
- **Failure signatures:**
  - Overfitting: Model performs well on training data but poorly on test data
  - Poor generalization: Model performs well on training distribution but poorly on unseen distributions
  - Inefficient search: Model gets stuck in local optima or explores the solution space inefficiently
- **First 3 experiments:**
  1. Reproduce the results for S2V-DQN and Forward Greedy on a small ER graph dataset to verify the implementation.
  2. Train and evaluate ECO-DQN and SoftTabu on a small weighted graph dataset to compare the impact of the GNN.
  3. Evaluate ANYCSP and Tabu Search on a diverse set of graph distributions to compare their performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can learned heuristics be developed that consistently outperform Tabu Search across diverse graph distributions?
- Basis in paper: Explicit - The paper demonstrates that Tabu Search consistently outperforms all evaluated learned heuristics except ANYCSP across various instances and distributions.
- Why unresolved: While Tabu Search shows superior performance, the paper only evaluates existing learned heuristics. New approaches or architectural innovations might yield better results.
- What evidence would resolve it: A systematic comparison of new learned heuristics against Tabu Search on the same diverse benchmark suite showing consistent improvement.

### Open Question 2
- Question: What is the fundamental limitation preventing GNNs from improving upon traditional heuristics in combinatorial optimization?
- Basis in paper: Inferred - The paper shows that ECO-DQN's performance can be matched by a simple linear regression on Tabu Search-related features, suggesting the GNN adds little value.
- Why unresolved: The paper demonstrates the lack of improvement but does not identify the root cause of why GNNs fail to enhance traditional heuristics.
- What evidence would resolve it: Analysis of GNN architectures and feature representations to identify why they fail to capture problem structure beyond what simple features provide.

### Open Question 3
- Question: How can generalization be improved for learned heuristics across different graph distributions?
- Basis in paper: Explicit - The paper shows learned heuristics experience significant performance drops when tested on distributions different from their training data.
- Why unresolved: The paper demonstrates the problem but doesn't propose solutions for improving cross-distribution generalization.
- What evidence would resolve it: Development and evaluation of new training methodologies or architectural modifications that show improved performance across unseen graph distributions.

### Open Question 4
- Question: What is the optimal balance between computational complexity and solution quality for learned heuristics?
- Basis in paper: Explicit - The paper notes ANYCSP's superior performance comes at the cost of significantly higher computational resources.
- Why unresolved: The paper identifies the trade-off but doesn't establish guidelines for when the additional computational cost is justified.
- What evidence would resolve it: Empirical studies quantifying the performance gain per unit of additional computational cost across different problem scales and instance types.

## Limitations

- **Limited empirical scope**: The benchmark primarily evaluates algorithms on Maximum Cut instances, which may not generalize to other combinatorial optimization problems.
- **Hyperparameter sensitivity**: Several algorithms show performance variations based on hyperparameter choices, but the study does not systematically explore this sensitivity.
- **Implementation details**: Some algorithms, particularly quantum annealing implementations, lack complete specification of their configuration parameters.

## Confidence

- **High confidence**: Claims about Tabu Search's consistent outperformance of most learned heuristics
- **Medium confidence**: The finding that simple greedy algorithms often match or exceed learned heuristics
- **Medium confidence**: The claim that ANYCSP represents a promising direction for future research

## Next Checks

1. **Reproduce key results**: Implement Forward Greedy and S2V-DQN on small ER graph datasets to verify the baseline performance claims and implementation correctness.
2. **Hyperparameter sensitivity analysis**: Systematically vary learning rates and network architectures for GNN-based methods to assess their impact on performance and determine if results are robust.
3. **Cross-domain generalization**: Evaluate the best-performing algorithms from this benchmark on a different combinatorial optimization problem (e.g., Minimum Vertex Cover) to test the generalizability of the findings.