---
ver: rpa2
title: Can we obtain significant success in RST discourse parsing by using Large Language
  Models?
arxiv_id: '2403.05065'
source_url: https://arxiv.org/abs/2403.05065
tags:
- parsing
- llama
- discourse
- parsers
- latexit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using decoder-only LLMs for RST discourse
  parsing. The authors convert both top-down and bottom-up parsing strategies into
  prompts and fine-tune Llama 2 using QLoRA.
---

# Can we obtain significant success in RST discourse parsing by using Large Language Models?

## Quick Facts
- arXiv ID: 2403.05065
- Source URL: https://arxiv.org/abs/2403.05065
- Reference count: 15
- Key outcome: Llama 2 with 70B parameters achieves state-of-the-art RST discourse parsing results on multiple datasets

## Executive Summary
This paper investigates the use of decoder-only Large Language Models (LLMs) for Rhetorical Structure Theory (RST) discourse parsing. The authors explore both top-down and bottom-up parsing strategies, converting them into prompts for fine-tuning Llama 2 using QLoRA. The experimental results demonstrate that the 70 billion parameter model outperforms existing parsers on RST-DT, Instr-DT, and GUM corpus datasets. Additionally, the model shows good generalizability when trained on GUM and evaluated on RST-DT.

## Method Summary
The authors employ a fine-tuning approach using Llama 2, a decoder-only LLM, with QLoRA for efficient parameter adaptation. They convert both top-down and bottom-up RST parsing strategies into prompts, which are then used to fine-tune the model. The experimental setup includes datasets such as RST-DT, Instr-DT, and GUM corpus. The model's performance is evaluated based on its ability to accurately parse discourse structures, with particular attention to its generalizability across different datasets.

## Key Results
- Llama 2 with 70B parameters achieves state-of-the-art results on RST-DT, outperforming existing parsers by 2-3 points.
- The model demonstrates improved performance on Instr-DT (0.4-3.7 points) and GUM corpus (1.5-6 points) compared to previous methods.
- Good generalizability is observed when the model is trained on GUM and evaluated on RST-DT, achieving similar performance to parsers trained directly on RST-DT.

## Why This Works (Mechanism)
The success of the LLM-based approach in RST discourse parsing can be attributed to the model's ability to understand and generate complex discourse structures through fine-tuning. By converting parsing strategies into prompts, the model leverages its pre-trained language understanding capabilities to accurately identify and construct discourse trees. The use of QLoRA allows for efficient fine-tuning of the large model, making it feasible to adapt the 70B parameter Llama 2 for this specific task.

## Foundational Learning
- Rhetorical Structure Theory (RST): A framework for analyzing discourse structure by identifying hierarchical relationships between text spans. Needed to understand the target parsing task; quick check: review basic RST concepts and tree structures.
- QLoRA (Quantized Low-Rank Adaptation): A parameter-efficient fine-tuning method that reduces memory requirements for large models. Needed to make fine-tuning of 70B parameter models feasible; quick check: understand quantization and LoRA mechanics.
- Decoder-only LLMs: Language models that generate text based on input context, suitable for tasks requiring sequential generation. Needed as the base architecture for discourse parsing; quick check: compare decoder-only vs encoder-decoder models.

## Architecture Onboarding
- Component map: Input text -> Prompt engineering (top-down/bottom-up) -> Llama 2 (70B) -> QLoRA fine-tuning -> Discourse tree output
- Critical path: The prompt design and fine-tuning process are critical, as they directly influence the model's ability to parse discourse structures accurately.
- Design tradeoffs: Using a large 70B parameter model offers high performance but requires significant computational resources. QLoRA mitigates this by enabling efficient fine-tuning.
- Failure signatures: Poor performance may result from inadequate prompt engineering or insufficient fine-tuning data, leading to incorrect discourse tree structures.
- First experiments:
  1. Evaluate the impact of different prompt engineering strategies on parsing accuracy.
  2. Test the model's performance on a small subset of the dataset before full-scale fine-tuning.
  3. Compare the results of top-down and bottom-up parsing strategies to identify the most effective approach.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions, but it leaves several areas for further exploration, such as the model's performance on non-RST discourse frameworks and its robustness to noisy or out-of-domain inputs.

## Limitations
- The generalizability of the fine-tuned Llama 2 model to discourse parsing tasks beyond the RST framework is not fully explored.
- The sensitivity of prompt engineering to minor modifications is not investigated, which could affect reproducibility.
- The computational cost and practicality of using a 70B parameter model with QLoRA for real-world applications are not discussed.

## Confidence
- **High**: The claim that Llama 2 with 70B parameters achieves state-of-the-art results on RST-DT, Instr-DT, and GUM is supported by experimental data.
- **Medium**: The assertion that the model demonstrates good generalizability when trained on GUM and evaluated on RST-DT is plausible but lacks cross-validation with other datasets.
- **Low**: The paper does not address potential biases in the training data or the robustness of the model to noisy or out-of-domain inputs, which could limit its applicability.

## Next Checks
1. Evaluate the model's performance on non-RST discourse frameworks, such as SDRT or PDTB, to assess its generalizability.
2. Conduct ablation studies to determine the impact of prompt engineering variations on parsing accuracy.
3. Analyze the computational efficiency and scalability of the fine-tuned model for large-scale or real-time discourse parsing tasks.