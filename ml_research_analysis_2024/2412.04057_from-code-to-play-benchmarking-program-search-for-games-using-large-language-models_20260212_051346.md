---
ver: rpa2
title: 'From Code to Play: Benchmarking Program Search for Games Using Large Language
  Models'
arxiv_id: '2412.04057'
source_url: https://arxiv.org/abs/2412.04057
tags:
- game
- games
- program
- each
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a framework for using large language models
  (LLMs) to generate executable program code for games. The framework employs an evolutionary
  hill-climbing algorithm where LLM-generated code serves as both initial seeds and
  mutation operators.
---

# From Code to Play: Benchmarking Program Search for Games Using Large Language Models

## Quick Facts
- **arXiv ID:** 2412.04057
- **Source URL:** https://arxiv.org/abs/2412.04057
- **Reference count:** 40
- **Key outcome:** Framework uses LLMs with evolutionary hill-climbing to generate game code across 29 tasks; performance depends more on task complexity than model size, with OpenAI o1-mini excelling in Python and Gemini 2.0 Flash in Java.

## Executive Summary
This work introduces a framework for using large language models to generate executable program code for games through an evolutionary hill-climbing algorithm. The framework employs LLM-generated code as both initial seeds and mutation operators, enabling automated program search across diverse game domains. It is evaluated across 29 tasks spanning Python and Java, including Atari game variants, Baba is You puzzles, maze generation, and 12 tabletop games from the TAG framework.

Results show that LLM performance depends more on task complexity than model size, with no single model consistently outperforming others. Smaller models occasionally match or exceed larger ones in specific tasks, and multiple trials across different models yield better results than single long runs. OpenAI models (particularly o1-mini) perform best for Python tasks, while Gemini 2.0 Flash excels in Java tabletop game heuristics. The framework successfully generates functional code for most tasks, though success rates and code quality vary significantly by domain and model family. Open-source models require substantially more iterations to match closed-source model performance.

## Method Summary
The framework employs an evolutionary hill-climbing algorithm where LLM-generated code serves as both initial seeds and mutation operators. It is evaluated across 29 tasks spanning Python and Java, including Atari game variants, Baba is You puzzles, maze generation, and 12 tabletop games from the TAG framework. The approach combines evolutionary search with LLM generation capabilities to explore the program space systematically.

## Key Results
- LLM performance depends more on task complexity than model size, with no single model consistently outperforming others
- Smaller models occasionally match or exceed larger ones in specific tasks, and multiple trials across different models yield better results than single long runs
- OpenAI models (particularly o1-mini) perform best for Python tasks, while Gemini 2.0 Flash excels in Java tabletop game heuristics

## Why This Works (Mechanism)
The framework leverages LLMs' ability to understand programming contexts and generate syntactically correct code, combined with evolutionary search to explore program space systematically. By using LLM outputs as both seeds and mutation operators, the approach can navigate complex program landscapes while maintaining code quality through iterative refinement.

## Foundational Learning
- **Evolutionary hill-climbing algorithm** - needed to systematically explore program space; quick check: verify convergence behavior on benchmark functions
- **LLM code generation capabilities** - needed to produce syntactically correct initial code and mutations; quick check: test generation accuracy on standard coding benchmarks
- **Program search optimization** - needed to balance exploration and exploitation in code space; quick check: measure search efficiency on synthetic program landscapes
- **Cross-language code generation** - needed to handle both Python and Java tasks; quick check: validate language-specific syntax and conventions

## Architecture Onboarding
**Component Map:** LLM Generator -> Evolutionary Operator -> Code Evaluator -> Fitness Function -> Selection Mechanism

**Critical Path:** LLM generation → code mutation → fitness evaluation → selection → iteration

**Design Tradeoffs:** Fixed iteration limits vs. performance-based stopping criteria; single long runs vs. multiple shorter trials; model size vs. task complexity matching

**Failure Signatures:** Low success rates on complex tasks despite sufficient iterations; performance plateaus without improvement; code generation failures on specific domains

**3 First Experiments:**
1. Test framework on simple Python functions before game code generation
2. Compare single model vs. ensemble approach on identical tasks
3. Measure impact of iteration count vs. model size on success rates

## Open Questions the Paper Calls Out
None

## Limitations
- Success rates vary dramatically across task domains, suggesting current complexity measures may not fully capture challenges
- Performance measurement lacks systematic evaluation of code quality beyond basic functionality
- Lack of statistical significance testing for performance differences between model families introduces uncertainty

## Confidence
- LLM performance depends more on task complexity than model size - Medium confidence
- Multiple trials across different models yield better results than single long runs - Medium confidence
- OpenAI models perform best for Python tasks, Gemini 2.0 Flash for Java - Low confidence

## Next Checks
1. Implement statistical significance testing across model families to distinguish meaningful performance differences from random variation
2. Develop standardized complexity metrics and test whether task complexity measures predict success rates across all domains
3. Evaluate code quality metrics (efficiency, readability, maintainability) beyond basic functionality to better understand practical utility of generated programs