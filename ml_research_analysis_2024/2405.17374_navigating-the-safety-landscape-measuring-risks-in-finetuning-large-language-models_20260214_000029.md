---
ver: rpa2
title: 'Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language
  Models'
arxiv_id: '2405.17374'
source_url: https://arxiv.org/abs/2405.17374
tags:
- safety
- landscape
- finetuning
- prompt
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of "safety basin" in the parameter
  space of aligned LLMs, where random perturbations to model weights maintain safety
  within a local neighborhood but compromise it sharply outside. Based on this discovery,
  the authors propose the VISAGE metric to measure safety risks in finetuning by probing
  the local safety landscape.
---

# Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models

## Quick Facts
- arXiv ID: 2405.17374
- Source URL: https://arxiv.org/abs/2405.17374
- Reference count: 40
- Authors: ShengYun Peng; Pin-Yu Chen; Matthew Hull; Duen Horng Chau
- Primary result: Introduces safety basin concept and VISAGE metric for measuring safety risks in LLM finetuning

## Executive Summary
This paper introduces the concept of "safety basin" in the parameter space of aligned LLMs, where random perturbations to model weights maintain safety within a local neighborhood but compromise it sharply outside. Based on this discovery, the authors propose the VISAGE metric to measure safety risks in finetuning by probing the local safety landscape. Experiments on popular open-source LLMs (LLaMA2, LLaMA3, Vicuna, Mistral) show that finetuning with harmful data moves models outside the safety basin, while finetuning with mixed safe and harmful data preserves safety. The safety landscape also reveals that system prompts play a critical role in maintaining safety, with this protection transferring to perturbed variants within the basin.

## Method Summary
The method involves random weight perturbations along normalized directions with layer-wise normalization, followed by safety evaluation using AdvBench prompts with ASR measured by refusal keyword detection. The VISAGE metric is computed as the average difference between maximum safety score and perturbed model safety scores across random directions. Safety landscapes are visualized by perturbing models along random directions and plotting ASR across perturbation ranges. System prompts are evaluated by comparing VISAGE scores across different prompt configurations.

## Key Results
- Random perturbations maintain safety within a local basin but cause sharp drops outside
- VISAGE metric effectively measures safety risks in finetuning
- Finetuning with harmful data moves models outside safety basin
- System prompts provide transferable safety protection within the basin
- Jailbreaking prompts are highly sensitive to weight perturbations

## Why This Works (Mechanism)

### Mechanism 1: Safety Basin Stability
The aligned model's parameters occupy a local minimum in the safety landscape where perturbations along random directions don't significantly affect safety performance until they cross a sharp boundary. This basin contrasts with capability landscapes where performance gradually declines.

### Mechanism 2: VISAGE Metric Measurement
VISAGE computes the average difference between maximum safety score and perturbed model safety scores across random directions, providing a task-agnostic safety metric. The average depth of the safety basin correlates with finetuning robustness.

### Mechanism 3: System Prompt Protection Transfer
System prompts provide safety guardrails that remain effective even when model weights are slightly perturbed, as long as perturbations stay within the basin. This protection transfers to perturbed variants within the safety basin.

## Foundational Learning

- **Parameter space topology and landscape analysis**: Understanding how safety behaves across different regions of the parameter space is fundamental to the safety basin discovery and VISAGE metric. *Quick check: What distinguishes a basin-shaped landscape from a gradual slope landscape in parameter space?*

- **Adversarial robustness and perturbation analysis**: The safety basin concept relies on understanding how small perturbations affect model behavior, which is directly related to adversarial robustness literature. *Quick check: How does random perturbation analysis differ from targeted adversarial attacks?*

- **System prompt engineering and safety alignment**: The research shows system prompts play a critical role in maintaining safety, requiring understanding of how prompts interact with model parameters. *Quick check: What mechanisms allow system prompts to influence model behavior beyond simple text preprocessing?*

## Architecture Onboarding

- **Component map**: Safety landscape visualization -> VISAGE metric computation -> System prompt evaluation
- **Critical path**: Sample random direction → perturb model weights → evaluate safety → visualize results (landscape); Generate multiple random directions → compute safety margins → average results (VISAGE); Apply different prompts → compute VISAGE scores → compare (system prompts)
- **Design tradeoffs**: 1D vs 2D landscape visualization (efficiency vs resolution); keyword detection vs human evaluation (speed vs precision); random direction sampling vs comprehensive coverage (feasibility vs thoroughness)
- **Failure signatures**: VISAGE scores don't correlate with finetuning outcomes; safety basin disappears under different metrics; system prompt effects don't transfer to perturbed models
- **First 3 experiments**: 1) Replicate 1D-random safety landscape for new LLM; 2) Compute VISAGE scores for multiple models; 3) Test system prompt transfer by perturbing models within basin

## Open Questions the Paper Calls Out

**Open Question 1**: How can the width and depth of the safety basin be reliably measured and quantified as distinct metrics beyond the VISAGE score? The paper needs a systematic framework that defines and measures basin width and depth separately.

**Open Question 2**: What is the precise relationship between model size and safety basin width, and can this relationship be leveraged to design training objectives that explicitly optimize for wider safety basins? The paper observes correlation but needs to establish causation and explore training modifications.

**Open Question 3**: Can adversarial training techniques be developed that specifically target the safety basin boundaries to create models with more robust safety alignment? The paper identifies sensitivity at boundaries but hasn't explored defensive exploitation through adversarial training.

## Limitations
- Safety basin phenomenon may not generalize beyond tested models (LLaMA2, LLaMA3, Vicuna, Mistral)
- ASR-based safety evaluation using keyword detection is a coarse proxy
- Random perturbation analysis examines only linear directions, not full nonlinear topology

## Confidence

**High confidence**: Existence of safety basin phenomenon for tested models; finetuning with harmful data moves models outside basin

**Medium confidence**: VISAGE metric's ability to predict finetuning safety risks; system prompts provide transferable safety protection

**Low confidence**: Generalizability of safety basin concept to all LLMs; effectiveness of weight perturbation as jailbreak defense; VISAGE as universally task-agnostic metric

## Next Checks

1. **Cross-model validation**: Test safety basin phenomenon on additional LLM families (GPT-series, Claude, etc.) and different model scales to verify if basin topology holds across diverse architectures.

2. **Evaluation robustness**: Replace keyword-based ASR with human evaluation or more sophisticated safety metrics to determine if safety basin and VISAGE results are artifacts of evaluation method.

3. **Perturbation method sensitivity**: Compare random perturbation analysis with targeted adversarial perturbations and gradient-based methods to determine if safety basin is unique to random directions or represents a more fundamental property.