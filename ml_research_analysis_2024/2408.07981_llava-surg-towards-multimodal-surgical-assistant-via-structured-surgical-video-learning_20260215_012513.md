---
ver: rpa2
title: 'LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured Surgical
  Video Learning'
arxiv_id: '2408.07981'
source_url: https://arxiv.org/abs/2408.07981
tags:
- surgical
- video
- data
- llav
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLaVA-Surg, the first multimodal large language
  model for surgical video understanding. The authors create Surg-QA, a 102K surgical
  video-instruction dataset generated via a novel two-stage pipeline that reduces
  LLM hallucinations and enables cost-effective data generation from surgical lecture
  videos.
---

# LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured Surgical Video Learning

## Quick Facts
- arXiv ID: 2408.07981
- Source URL: https://arxiv.org/abs/2408.07981
- Authors: Jiajie Li; Garrett Skinner; Gene Yang; Brian R Quaranto; Steven D Schwaitzberg; Peter C W Kim; Jinjun Xiong
- Reference count: 36
- Primary result: First multimodal large language model for surgical video understanding, achieving state-of-the-art zero-shot performance with GPT evaluation score of 2.45

## Executive Summary
This paper introduces LLaVA-Surg, the first multimodal large language model specifically designed for surgical video understanding and conversation. The authors develop Surg-QA, a 102K surgical video-instruction dataset created through a novel two-stage pipeline that reduces LLM hallucinations by first extracting structured surgical knowledge from transcripts before generating question-answer pairs. LLaVA-Surg is trained by fine-tuning a vision-language model on Surg-QA and demonstrates superior zero-shot performance on surgical video question-answering tasks, outperforming general-domain models with a GPT evaluation score of 2.45 compared to 1.32-1.04 for baselines.

## Method Summary
LLaVA-Surg employs a vision-language architecture using CLIP ViT-L/14 as the frozen visual encoder and Llama-3-70B as the language backbone. The model processes surgical videos through a temporal-fusion operation that combines frame-level features with spatial pooling, followed by linear projection to the LLM input space. Training involves fine-tuning on Surg-QA, a 102K dataset of surgical video-instruction pairs generated from over 44,000 surgical video clips across 2,201 procedures using a two-stage pipeline. The pipeline first extracts structured surgical knowledge (observations, reasons, plans, deductions) from transcripts using an open-source LLM, then generates high-quality question-answer pairs from this structured information, significantly reducing hallucinations compared to direct end-to-end generation.

## Key Results
- LLaVA-Surg achieves state-of-the-art zero-shot performance with GPT evaluation score of 2.45 (vs 1.32-1.04 for baselines)
- Human expert evaluation confirms high correlation with GPT scoring (ρ=0.94), validating the evaluation framework
- The model demonstrates strong multimodal conversational capabilities for answering open-ended questions about surgical procedures
- Surg-QA dataset contains 102K surgical video-instruction pairs derived from 44K+ surgical video clips across 2,201 procedures

## Why This Works (Mechanism)

### Mechanism 1
The two-stage question-answer generation pipeline significantly reduces LLM hallucinations by first extracting structured surgical knowledge from transcripts and then generating Q&A pairs from this verified foundation. This approach constrains the generation phase to work with actual surgical facts rather than hallucinated content, ensuring high-quality data by extracting only surgery-related information.

### Mechanism 2
Using an open-source LLM for data generation provides cost-effective dataset creation while maintaining quality. The two-stage approach simplifies the generation task enough that a less expensive, locally deployed LLM can be used instead of premium paid services, making large-scale data generation economically feasible.

### Mechanism 3
Training on Surg-QA enables LLaVA-Surg to achieve state-of-the-art performance through comprehensive coverage of surgical procedures and knowledge levels. The large-scale (102K pairs) and diverse dataset allows the model to develop robust understanding and reasoning capabilities specific to surgical contexts across different procedures and reasoning levels.

## Foundational Learning

- Concept: Surgical video knowledge pyramid (Levels 1-4)
  - Why needed here: Understanding the four levels of surgical knowledge is crucial for designing appropriate training data and evaluation metrics
  - Quick check question: What are the key differences between Level 2 (action recognition) and Level 3 (higher-order reasoning) in surgical video understanding?

- Concept: Multimodal instruction tuning
  - Why needed here: The process of training a model to follow instructions using both visual and textual data is fundamental to LLaVA-Surg's architecture and training methodology
  - Quick check question: How does the temporal-fusion operation in the video encoder contribute to the model's ability to understand surgical procedures?

- Concept: Zero-shot evaluation methodology
  - Why needed here: Understanding how GPT-3.5-Turbo evaluation works and its correlation with human expert judgment is essential for interpreting results and validating the evaluation framework
  - Quick check question: What are the advantages and limitations of using GPT-3.5-Turbo for evaluating surgical video question-answering performance compared to human expert evaluation?

## Architecture Onboarding

- Component map: CLIP ViT-L/14 visual encoder (frozen) -> Temporal-fusion operation -> Linear projection layer -> Llama-3-70B language backbone

- Critical path:
  1. Extract video frames (N frames uniformly sampled)
  2. Compute CLIP features (h ∈ RN×h×w×D)
  3. Apply temporal-fusion (t ∈ RN×D, s ∈ R(h×w)×D)
  4. Concatenate and project (f ∈ R(N+h×w)×D)
  5. Feed to LLM for instruction following

- Design tradeoffs:
  - Using frozen CLIP encoder vs. fine-tuning: Tradeoff between computational efficiency and potential performance gains
  - Two-stage data generation vs. end-to-end: Tradeoff between hallucination reduction and data generation efficiency
  - Zero-shot evaluation vs. human evaluation: Tradeoff between scalability and accuracy of assessment

- Failure signatures:
  - Low GPT evaluation scores with high human correlation: Indicates evaluation framework works but model performance needs improvement
  - Low human correlation despite high GPT scores: Suggests GPT evaluation may not align with expert judgment
  - High variance across surgery types: Indicates model may be biased toward certain procedures

- First 3 experiments:
  1. Evaluate model performance on a held-out subset of Surg-QA to establish baseline
  2. Test model's ability to handle each of the four knowledge levels separately
  3. Compare two-stage data generation approach against end-to-end generation using the same underlying LLM

## Open Questions the Paper Calls Out

### Open Question 1
How does LLaVA-Surg's performance vary across different surgical specialties and procedures in the dataset? The paper notes performance depends on surgery procedures seen in SurgQA, with results varying widely based on surgery type, but lacks detailed breakdown by specialty.

### Open Question 2
What is the long-term effectiveness and safety of using LLaVA-Surg in actual surgical training or clinical settings? The paper discusses potential as a surgical assistant but acknowledges limitations regarding hallucinations and bias, and notes potential negative societal impacts if outputs are not carefully verified.

### Open Question 3
How can the two-stage question-answer generation pipeline be further optimized to reduce hallucinations while maintaining or improving data generation efficiency? The paper demonstrates improvement over end-to-end methods but does not explore alternative architectures, prompts, or verification mechanisms that could further reduce hallucinations.

## Limitations

- The evaluation framework depends on GPT-3.5-Turbo scoring, which represents an automated proxy rather than direct human evaluation across all test cases
- The Surg-QA dataset, while substantial, is derived from publicly available surgical lecture videos which may not capture the full diversity of real-world surgical scenarios
- The model's architecture freezes the CLIP encoder, potentially limiting its ability to adapt visual representations to specific characteristics of surgical imagery

## Confidence

**High Confidence**: LLaVA-Surg achieves state-of-the-art performance on zero-shot surgical video question-answering tasks (GPT evaluation score 2.45 vs 1.32-1.04 for baselines)

**Medium Confidence**: The two-stage generation pipeline significantly reduces LLM hallucinations compared to direct end-to-end generation

**Medium Confidence**: Using an open-source LLM for data generation provides cost-effective dataset creation

## Next Checks

1. **Ablation Study on Generation Pipeline**: Conduct direct comparison between two-stage and end-to-end generation using the same underlying LLM on surgical videos, measuring hallucination rates and output quality through automated metrics and human expert evaluation.

2. **Cross-Institutional Validation**: Test LLaVA-Surg on surgical videos from institutions and sources outside the WebSurg dataset to assess generalization performance and identify potential domain-specific limitations or biases.

3. **Human Evaluation Expansion**: Implement comprehensive human evaluation protocol where expert surgeons assess model outputs across all four knowledge levels on a representative sample of the test set, providing granular performance breakdowns beyond aggregate GPT scoring.