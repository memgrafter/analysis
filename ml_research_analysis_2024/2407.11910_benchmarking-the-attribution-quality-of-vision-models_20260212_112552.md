---
ver: rpa2
title: Benchmarking the Attribution Quality of Vision Models
arxiv_id: '2407.11910'
source_url: https://arxiv.org/abs/2407.11910
tags:
- attribution
- idsds
- methods
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel in-domain single-deletion score (IDSDS)
  for evaluating the correctness of attribution methods in deep neural networks. The
  key problem addressed is the out-of-domain (OOD) issue and lack of inter-model comparison
  in existing deletion-based protocols for attribution evaluation.
---

# Benchmarking the Attribution Quality of Vision Models

## Quick Facts
- **arXiv ID**: 2407.11910
- **Source URL**: https://arxiv.org/abs/2407.11910
- **Reference count**: 40
- **One-line primary result**: Introduces IDSDS metric that reveals intrinsically explainable models like BagNet-33 outperform standard models for attribution quality while showing negative correlation between model accuracy and attribution correctness.

## Executive Summary
This paper addresses fundamental limitations in evaluating attribution methods for deep neural networks, specifically the out-of-domain (OOD) issue and lack of inter-model comparison in existing deletion-based protocols. The authors propose a novel in-domain single-deletion score (IDSDS) that fine-tunes models on images with randomly deleted patches to align train and test domains. This allows for reliable attribution evaluation without information leakage. The paper evaluates 23 attribution methods across multiple vision models and conducts systematic studies on how model design choices affect attribution quality, revealing an accuracy-explainability trade-off where models like BagNet-33 outperform standard architectures despite lower accuracy.

## Method Summary
The core methodology involves fine-tuning vision models on ImageNet with a data augmentation scheme where 50% of training images have one random patch deleted (replaced with zero baseline). The IDSDS metric then measures the Spearman rank-order correlation between output drops from patch deletions and attribution sums per patch. This protocol enables in-domain evaluation by aligning train and test domains while allowing fair inter-model comparisons independent of model calibration. The evaluation covers 23 attribution methods across multiple model architectures (ResNet variants, VGG, ViT, BagNet) and investigates the effects of architectural choices like batch normalization and bias terms on attribution quality.

## Key Results
- Intrinsically explainable models (BagNet-33) significantly outperform standard models (ResNet-50) on attribution quality metrics
- Removing batch normalization and bias terms consistently improves attribution quality across different attribution methods
- Strong negative correlation between model accuracy and IDSDS, confirming the accuracy-explainability trade-off
- Raw attribution values can outperform absolute values for certain attribution methods when updated at each degradation step

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on images with randomly deleted patches aligns the train and test domains, eliminating out-of-domain (OOD) issues in deletion-based attribution evaluation.
- Mechanism: By randomly deleting one of 16 non-overlapping patches in half the training images and fine-tuning the model, the model learns to handle both intact and partially deleted images. During evaluation, the same patch deletion process is applied to test images, ensuring the domain shift is minimal.
- Core assumption: The OOD issue arises because the model is not trained on partially deleted images, so the output changes during deletion-based evaluation are due to domain shifts rather than the removal of important features.
- Evidence anchors:
  - [abstract]: "We propose a novel evaluation protocol that overcomes two fundamental limitations of the widely used incremental-deletion protocol, i.e., the out-of-domain issue and lacking inter-model comparisons."
  - [section]: "To align the train and test domains for our IDSDS, we substitute one random patch in half of the training images with the considered zero baseline... This allows for inter-model comparisons and an exact alignment of the train and test domains without the issue of class information leakage."
  - [corpus]: Found related papers on data attribution evaluation and benchmarking, suggesting the OOD issue is a recognized challenge in the field.
- Break condition: If the fine-tuning process significantly alters the model's learned features or if the patch deletion strategy introduces new information (class leakage), the alignment would be compromised.

### Mechanism 2
- Claim: The IDSDS protocol enables fair inter-model comparisons by being independent of model calibration and attribution method-specific training.
- Mechanism: Unlike incremental-deletion protocols that depend on model outputs and are sensitive to calibration changes, the IDSDS measures the correlation between output drops from patch deletions and attribution sums. This correlation is a direct measure of how well the attribution method identifies important patches, regardless of the model's calibration.
- Core assumption: The correlation between output drops and attribution sums is a reliable proxy for attribution correctness that is not influenced by model calibration differences.
- Evidence anchors:
  - [abstract]: "Unlike the incremental-deletion protocol, the IDSDS can only improve if the actual task of ranking the patch importances is more effectively solved and not due to mere changes in the output calibration."
  - [section]: "As described so far, our IDSDS may appear similar to [55]. However, in the following, we go beyond [55] by, first, contributing an exact alignment of the train and test domains that is independent of the specific attribution method and image label..."
  - [corpus]: Related work on evaluation metrics for attribution methods suggests that model calibration can significantly impact evaluation results, supporting the need for a calibration-independent metric.
- Break condition: If the correlation between output drops and attribution sums is not a reliable measure of attribution correctness, or if model calibration changes significantly impact the output drops, the inter-model comparison capability would be compromised.

### Mechanism 3
- Claim: Using raw attribution values instead of absolute values can lead to more correct attributions, especially when the amount of deleted information is small.
- Mechanism: When updating the attribution at each degradation step (as opposed to using a fixed attribution for the original input), the sign of the attribution becomes more important for identifying important features. This is because the intervened image is very similar to the original image for which the attribution was computed, making the magnitude of the attribution less relevant.
- Core assumption: The sign of the attribution is more informative than the magnitude when the amount of deleted information is small.
- Evidence anchors:
  - [abstract]: "We find that intrinsically explainable models outperform standard models and that raw attribution values exhibit a higher attribution quality than what is known from previous work."
  - [section]: "Intriguingly, taking the absolute value of the attribution maps hurts correctness for the better performing methods... which is in stark contrast to findings from related work... When updating the attribution at each degradation step, we see a gain in IDS for the raw attribution values of I×G, IG, and IG-U, making them the best-performing methods."
  - [corpus]: Related work on attribution methods and their evaluation suggests that the choice between raw and absolute attributions can significantly impact the results, supporting the need for empirical validation.
- Break condition: If the amount of deleted information is not small, or if the attribution methods do not produce reliable signs, using raw attributions could lead to incorrect conclusions.

## Foundational Learning

- Concept: Domain alignment in machine learning evaluation
  - Why needed here: The paper addresses the out-of-domain (OOD) issue in deletion-based attribution evaluation, where the model is not trained on partially deleted images, leading to unreliable results.
  - Quick check question: What is the primary cause of the OOD issue in deletion-based attribution evaluation, and how does the IDSDS protocol address it?

- Concept: Spearman rank-order correlation coefficient
  - Why needed here: The IDSDS protocol uses the Spearman rank-order correlation coefficient to measure the correlation between output drops from patch deletions and attribution sums, which is a key component of the evaluation metric.
  - Quick check question: How does the Spearman rank-order correlation coefficient differ from the Pearson correlation coefficient, and why is it more suitable for measuring the correlation between rankings in the IDSDS protocol?

- Concept: Batch normalization and bias terms in neural networks
  - Why needed here: The paper investigates the effect of removing batch normalization layers and bias terms on attribution correctness, finding that their removal consistently improves attribution quality.
  - Quick check question: What is the role of batch normalization and bias terms in neural networks, and how might their removal affect the model's attribution quality?

## Architecture Onboarding

- Component map:
  Fine-tuning component -> IDSDS calculation component -> Attribution method evaluation component -> Model design analysis component

- Critical path:
  1. Fine-tune the model on images with randomly deleted patches.
  2. For each image in the test set, delete one patch and compute the output drop.
  3. Compute the attribution sum for each patch using the chosen attribution method.
  4. Calculate the Spearman rank-order correlation between the output drops and attribution sums.
  5. Rank the attribution methods based on their IDSDS scores.

- Design tradeoffs:
  - Granularity vs. computational cost: Evaluating on a patch level (16 patches) instead of a pixel level increases granularity but also increases computational cost.
  - Fine-tuning vs. evaluation speed: Fine-tuning the model to align domains improves evaluation accuracy but increases the time required for evaluation.
  - Raw vs. absolute attributions: Using raw attributions can lead to more correct attributions in some cases but may be less robust to noise.

- Failure signatures:
  - Low IDSDS scores: Indicate that the attribution method is not correctly identifying important patches.
  - High variance in IDSDS scores across different seeds: Suggest that the evaluation protocol is not stable or reliable.
  - Significant drop in accuracy after fine-tuning: Indicate that the fine-tuning process is negatively impacting the model's performance.

- First 3 experiments:
  1. Fine-tune a ResNet-50 model on ImageNet with the proposed data augmentation scheme and evaluate its accuracy on both uncorrupted and corrupted (with deleted patches) images.
  2. Compute the IDSDS for a set of attribution methods (e.g., Integrated Gradients, Grad-CAM) using the fine-tuned model and compare the results to those obtained with the original model.
  3. Investigate the effect of removing batch normalization layers and bias terms on the IDSDS scores of different attribution methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications beyond removing batch normalization and bias terms could further improve attribution quality while maintaining high classification accuracy?
- Basis in paper: [explicit] The paper demonstrates that removing batch normalization and bias terms consistently improves attribution quality across various attribution methods.
- Why unresolved: The paper only explores the removal of these two specific components. There may be other architectural modifications that could have similar or greater effects on attribution quality.
- What evidence would resolve it: Systematic ablation studies testing various architectural modifications (e.g., different normalization techniques, activation functions, attention mechanisms) and their effects on both classification accuracy and attribution quality metrics like IDSDS.

### Open Question 2
- How does the attribution quality of models change when evaluated on datasets with different characteristics (e.g., fine-grained classification, out-of-distribution samples, or datasets with different image statistics)?
- Basis in paper: [inferred] The paper only evaluates on ImageNet-1000, a general-purpose classification dataset with relatively large objects and diverse categories.
- Why unresolved: ImageNet may not be representative of all vision tasks. Attribution quality could vary significantly on datasets with different characteristics.
- What evidence would resolve it: Evaluating attribution quality on multiple datasets with varying characteristics (fine-grained classification like CUB-200, out-of-distribution datasets, datasets with different object sizes or image statistics) and comparing the relative rankings of attribution methods and model architectures.

### Open Question 3
- What is the relationship between attribution quality and model robustness to adversarial attacks or distribution shifts?
- Basis in paper: [inferred] The paper establishes a negative correlation between accuracy and attribution quality (the accuracy-explainability trade-off), but does not explore the relationship with robustness.
- Why unresolved: While the trade-off between accuracy and attribution quality is established, it remains unclear how attribution quality relates to other important model properties like robustness.
- What evidence would resolve it: Systematic studies measuring both attribution quality (using IDSDS) and robustness (to adversarial attacks and distribution shifts) across different model architectures and training procedures, analyzing whether models with better attribution quality also exhibit better or worse robustness properties.

## Limitations
- The IDSDS protocol's effectiveness depends on the fine-tuning process not altering the model's learned features significantly, which could compromise domain alignment if not properly controlled.
- The correlation between output drops and attribution sums as a measure of attribution correctness needs further validation, particularly for methods where raw vs. absolute attributions show different performance.
- The protocol requires fine-tuning models on augmented data, increasing computational cost and evaluation time compared to standard evaluation protocols.

## Confidence
- **High Confidence**: The out-of-domain issue in deletion-based attribution evaluation and the need for domain alignment
- **Medium Confidence**: The superiority of intrinsically explainable models like BagNet-33 for attribution quality
- **Medium Confidence**: The improvement in attribution quality when removing batch normalization and bias terms

## Next Checks
1. **Replication of Fine-tuning Effects**: Fine-tune a standard ResNet-50 on ImageNet with the patch deletion augmentation and measure both accuracy on uncorrupted images and the variance in IDSDS scores across different training seeds to assess stability.

2. **Raw vs. Absolute Attribution Validation**: For attribution methods where raw attributions outperform absolute values (I×G, IG, IG-U), test their performance on a controlled dataset where ground truth feature importance is known to verify the claim about sign information being more important.

3. **Batch Normalization Ablation Study**: Systematically remove batch normalization layers from multiple model architectures (ResNet, VGG, etc.) and measure the impact on both model accuracy and attribution quality across different attribution methods to establish causality.