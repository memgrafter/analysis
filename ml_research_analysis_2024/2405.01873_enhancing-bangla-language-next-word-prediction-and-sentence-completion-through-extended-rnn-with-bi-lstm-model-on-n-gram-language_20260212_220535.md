---
ver: rpa2
title: Enhancing Bangla Language Next Word Prediction and Sentence Completion through
  Extended RNN with Bi-LSTM Model On N-gram Language
arxiv_id: '2405.01873'
source_url: https://arxiv.org/abs/2405.01873
tags:
- word
- prediction
- bangla
- language
- next
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a Bi-LSTM model for Bangla next-word prediction
  and sentence completion. The approach leverages large datasets from news sources
  to train 1-gram to 5-gram models, each optimized for input sequences of different
  lengths.
---

# Enhancing Bangla Language Next Word Prediction and Sentence Completion through Extended RNN with Bi-LSTM Model On N-gram Language

## Quick Facts
- arXiv ID: 2405.01873
- Source URL: https://arxiv.org/abs/2405.01873
- Reference count: 21
- High accuracy achieved: 99% for 4-gram and 5-gram predictions

## Executive Summary
This study introduces a Bi-LSTM model for next-word prediction and sentence completion in the Bangla language. The approach trains separate Bi-LSTM models for uni-gram to 5-gram sequences using a 1.7 GB corpus from news sources. By leveraging bidirectional processing and specialized n-gram models, the system achieves high accuracy, particularly for longer sequences. The model addresses the vanishing gradient problem in RNNs and demonstrates strong performance on Bangla language tasks.

## Method Summary
The method involves collecting Bangla text from news sources, preprocessing it by cleaning unwanted characters and numbers, and generating five distinct n-gram datasets (uni-gram to 5-gram). Five separate Bi-LSTM models are trained, each optimized for a specific n-gram level. The Bi-LSTM architecture uses embedding layers, two Bi-LSTM layers with 100 units each, and fully connected layers with ReLU and softmax activations. The models are trained for up to 300 epochs, and predictions are made by selecting the appropriate model based on input sequence length.

## Key Results
- 99% accuracy for 4-gram and 5-gram predictions
- 35% accuracy for uni-gram predictions
- 75% accuracy for bi-gram predictions
- 95% accuracy for tri-gram predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Bi-LSTM architecture effectively captures long-range dependencies in Bangla text by processing sequences in both forward and backward directions, mitigating the vanishing gradient problem.
- Mechanism: The Bi-LSTM employs two separate LSTM networks: one processes the input sequence from past to future, while the other processes it from future to past. Their outputs are combined, allowing the model to consider context from both directions at each time step.
- Core assumption: Bangla language exhibits sufficient bidirectional dependencies that justify the computational overhead of Bi-LSTM over unidirectional LSTM.
- Evidence anchors:
  - [abstract]: "The model employs bidirectional processing to capture context from both past and future words, addressing the vanishing gradient problem in RNNs."
  - [section]: "Two separate recurrent neural networks (RNNs) are integrated to create bidirectional RNNs. This setup lets the network look at information from both directions (like before and after) for each point in the sequence."
  - [corpus]: Weak evidence - corpus analysis shows related papers focus on Bangla NLP but doesn't directly validate the bidirectional dependency assumption for this specific model.
- Break condition: The mechanism fails if the dataset contains predominantly short-range dependencies where unidirectional context is sufficient, making the additional backward pass unnecessary complexity.

### Mechanism 2
- Claim: The n-gram language model approach, combined with Bi-LSTM, provides superior accuracy by leveraging statistical patterns at multiple sequence lengths (uni-gram to 5-gram).
- Mechanism: The model trains separate Bi-LSTM instances for each n-gram level, allowing each to specialize in predicting the next word based on the specific number of preceding words. This multi-model approach captures patterns that might be missed by a single model trained on all sequence lengths.
- Core assumption: Different sequence lengths (1-gram to 5-gram) represent distinct and useful statistical patterns in Bangla text that can be better captured by specialized models rather than a unified approach.
- Evidence anchors:
  - [section]: "We have evaluated our proposed approach using a corpus dataset, training five different models with identical configurations for up to 300 epochs... Our 4-gram and 5-gram models exhibit an average accuracy of 99% and 99.74%, respectively."
  - [abstract]: "The approach leverages large datasets from news sources to train 1-gram to 5-gram models, each optimized for input sequences of different lengths."
  - [corpus]: Weak evidence - corpus analysis shows related papers exist but doesn't validate the specific claim about multi-model n-gram approach superiority.
- Break condition: The mechanism fails if the computational cost of maintaining five separate models outweighs the accuracy benefits, particularly for deployment in resource-constrained environments.

### Mechanism 3
- Claim: The dataset preprocessing pipeline, including cleaning and n-gram generation, creates high-quality training data that directly contributes to the model's high accuracy.
- Mechanism: The preprocessing removes noise (commas, numbers) and generates five distinct datasets (Uni-gram, Bi-gram, Tri-gram, 4-gram, 5-gram) from the cleaned corpus. This structured data preparation ensures the model receives consistent, relevant training examples.
- Core assumption: The quality of preprocessing directly correlates with model performance, and removing noise while preserving linguistic structure is sufficient for Bangla text.
- Evidence anchors:
  - [section]: "After collecting the data, we used several functions to remove unwanted commas, numbers, etc. from our dataset and five distinct datasets were generated from the cleaned dataset: Unigram, Bigram, Trigram, 4-gram, and 5-gram."
  - [abstract]: "We constructed a corpus dataset from various news portals, including bdnews24, BBC News Bangla, and Prothom Alo."
  - [corpus]: Weak evidence - corpus analysis shows related papers but doesn't validate the specific preprocessing effectiveness claim.
- Break condition: The mechanism fails if critical linguistic information is lost during preprocessing (e.g., punctuation that marks sentence boundaries), or if the cleaning introduces biases that the model learns.

## Foundational Learning

- Concept: N-gram language models
  - Why needed here: The paper explicitly builds and trains five separate models (1-gram to 5-gram) as the foundation for the prediction system, making understanding n-grams essential for comprehending the architecture.
  - Quick check question: What is the difference between a bi-gram and a tri-gram in terms of input and output structure?

- Concept: Vanishing gradient problem in RNNs
  - Why needed here: The paper specifically mentions addressing the vanishing gradient problem as a motivation for using LSTM and Bi-LSTM architectures, which is central to understanding why these choices were made.
  - Quick check question: Why do standard RNNs struggle with long sequences, and how do LSTM gates help solve this?

- Concept: Bidirectional processing in neural networks
  - Why needed here: The Bi-LSTM architecture is the core innovation, and understanding how forward and backward passes combine is essential to grasping the model's mechanism.
  - Quick check question: How does a Bi-LSTM combine information from both directions at each time step?

## Architecture Onboarding

- Component map: Data collection → Preprocessing (cleaning, n-gram generation) → Five separate Bi-LSTM models (Uni-gram to 5-gram) → Prediction pipeline (selects appropriate model based on input length) → Sentence completion module
- Critical path: Data preprocessing → Model training → Prediction selection → Output generation
- Design tradeoffs: Using five separate models increases accuracy for each sequence length but adds complexity and memory overhead compared to a single unified model; Bi-LSTM doubles computation compared to unidirectional LSTM but captures richer context
- Failure signatures: 1) Accuracy drops significantly on longer sequences (>5 words) 2) Model shows bias toward news-style language from training corpus 3) Prediction latency increases beyond acceptable thresholds 4) Training becomes unstable with NaN losses
- First 3 experiments:
  1. Train and evaluate a single Bi-LSTM model on all n-gram levels vs. the five-model approach to quantify accuracy vs. complexity tradeoff
  2. Replace Bi-LSTM with unidirectional LSTM to measure the contribution of bidirectional processing
  3. Test the model on Bangla text from different domains (social media, literature, technical writing) to assess generalization beyond news corpus

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions. However, the following questions arise from the limitations and assumptions made in the study:

1. How does the model handle context beyond the last five words in longer input sequences?
2. What is the impact of dataset diversity on the model's prediction accuracy for different Bangla dialects or regional variations?
3. How does the Bi-LSTM model compare to other advanced architectures, such as Transformer-based models, for Bangla language prediction tasks?

## Limitations
- The computational complexity of maintaining five separate Bi-LSTM models versus a unified approach is not analyzed.
- The 99% accuracy for 4-gram and 5-gram predictions may reflect dataset bias toward news-style language rather than true generalization capability.
- The relatively low 35% accuracy for uni-gram predictions suggests the model may struggle with basic word prediction tasks.

## Confidence

- **High Confidence**: The core mechanism of Bi-LSTM architecture and its ability to capture bidirectional context is well-established in the literature.
- **Medium Confidence**: The reported accuracy metrics, while impressive, require independent verification on diverse Bangla text sources beyond news corpora.
- **Low Confidence**: The assumption that maintaining separate models for each n-gram level is superior to a unified approach lacks comparative analysis with alternative architectures.

## Next Checks

1. **Cross-domain validation**: Test the trained models on Bangla text from non-news sources (social media, literature, technical documents) to assess generalization beyond the news corpus.
2. **Computational efficiency analysis**: Compare the five-model approach against a single unified Bi-LSTM model in terms of training time, inference latency, and memory requirements.
3. **Ablation study**: Systematically remove the bidirectional component to quantify the exact contribution of backward processing to overall accuracy improvements.