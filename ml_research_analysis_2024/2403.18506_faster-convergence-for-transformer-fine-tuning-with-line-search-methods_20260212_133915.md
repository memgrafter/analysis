---
ver: rpa2
title: Faster Convergence for Transformer Fine-tuning with Line Search Methods
arxiv_id: '2403.18506'
source_url: https://arxiv.org/abs/2403.18506
tags:
- line
- size
- search
- step
- adam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper extends line search optimization methods to Transformer
  architectures for NLP tasks. It combines the Armijo line search with the Adam optimizer
  and introduces a layer-wise line search approach (PLASLS) that applies different
  step sizes to different network components.
---

# Faster Convergence for Transformer Fine-tuning with Line Search Methods

## Quick Facts
- arXiv ID: 2403.18506
- Source URL: https://arxiv.org/abs/2403.18506
- Reference count: 27
- The paper extends line search optimization methods to Transformer architectures for NLP tasks, achieving significant performance improvements on small datasets.

## Executive Summary
This paper introduces a novel approach to fine-tuning Transformer models using line search optimization methods, specifically combining the Armijo line search with the Adam optimizer. The key innovation is PLASLS (Piecewise Line Search), which applies different step sizes to different network components, allowing parts of the network to learn at different speeds. The method shows substantial improvements on small datasets and short training runs, with about 3% accuracy gains over standard Adam on GLUE tasks when training for 5 epochs.

## Method Summary
The authors combine the Armijo line search criterion with Adam optimizer to automatically determine step sizes during training. They introduce PLASLS, which splits network parameters into components (e.g., layers or QKV groups) and performs line search separately on each component. The method includes a merging strategy to prevent components from becoming "frozen" when step sizes drop too low. The approach is implemented as a PyTorch optimizer package with default hyperparameters, making it easy to use without manual tuning.

## Key Results
- PLASLS achieves about 3% higher accuracy than Adam on GLUE tasks when training for 5 epochs
- The method shows significant improvements on small datasets (500 samples) where standard optimizers struggle
- ADAMSLS eliminates the need for manual learning rate tuning while maintaining or improving convergence speed

## Why This Works (Mechanism)

### Mechanism 1
Layer-wise line search allows different components of the network to learn at different speeds, improving overall convergence on small datasets. By applying the Armijo line search criterion separately to each component, PLASLS adapts the step size locally, capturing variation in loss w.r.t. each component.

### Mechanism 2
Combining Armijo line search with Adam eliminates the need for manual learning rate tuning while maintaining or improving convergence speed. ADAMSLS uses Adam's update direction as the search direction in the Armijo line search criterion, replacing fixed learning rates with automatically determined step sizes.

### Mechanism 3
PLASLS automatically detects and merges network components with very low step sizes, preventing parts of the network from becoming "frozen" during training. The algorithm monitors each component's step size and merges components below a threshold with the component having the second-lowest step size.

## Foundational Learning

- **Concept:** Stochastic Gradient Descent (SGD) and its variants (e.g., Adam)
  - Why needed here: PLASLS and ADAMSLS build upon SGD and Adam as their base optimizers, so understanding their update rules and limitations is crucial.
  - Quick check question: What is the difference between the update rule for SGD and Adam?

- **Concept:** Line search methods (e.g., Armijo line search)
  - Why needed here: The core innovation of this paper is applying line search to Transformers. Understanding how line search selects step sizes by evaluating the loss at different points is essential.
  - Quick check question: How does the Armijo line search criterion determine if a step size is acceptable?

- **Concept:** Transformer architecture and fine-tuning
  - Why needed here: The paper focuses on fine-tuning pre-trained Transformer models (BERT) for downstream NLP tasks. Knowing the structure of Transformers and the challenges of fine-tuning is necessary to appreciate the context.
  - Quick check question: What is the role of the [CLS] token in BERT, and why is it used for classification tasks?

## Architecture Onboarding

- **Component map:** PyTorch optimizer implementation with line search logic -> Network parameter splitting logic (e.g., by layer or by QKV) -> Merging logic for components with very low step sizes -> Integration with HuggingFace's BERT model and GLUE datasets

- **Critical path:**
  1. Initialize optimizer with network parameters
  2. Split parameters into components (layers, QKV, etc.)
  3. For each training step:
     - Compute gradient and Adam direction
     - For each component, perform line search to find step size
     - Merge components with step sizes below threshold
     - Update parameters using component-wise step sizes

- **Design tradeoffs:**
  - Granularity of parameter splitting: More components allow finer-grained adaptation but increase computational cost
  - Merging threshold: Lower threshold prevents premature merging but may allow some components to become "frozen"
  - Initial step size: Using a large initial step size for the first line search can help set a good starting point, but may cause instability

- **Failure signatures:**
  - Numerical underflow in step sizes, leading to components becoming "frozen"
  - Instability or divergence if the initial step size is too large
  - Reduced performance if components are merged too aggressively

- **First 3 experiments:**
  1. Compare ADAMSLS vs ADAM on a small GLUE dataset (e.g., SST-2) for 5 epochs
  2. Vary the granularity of parameter splitting in PLASLS (e.g., by layer vs. by QKV) on SST-2
  3. Test the effect of different merging thresholds in PLASLS on SST-2

## Open Questions the Paper Calls Out

### Open Question 1
What causes the very low step sizes observed in PLASLS and can numerical cancellation errors be definitively proven as the cause? The authors suspect numerical cancellation errors but do not provide conclusive evidence or formal analysis of this phenomenon.

### Open Question 2
What is the optimal strategy for automatically merging network components in PLASLS when step sizes drop below threshold? The paper only tests a simple averaging strategy and does not explore alternative merging strategies or theoretical justification for when and how to merge components.

### Open Question 3
Can the faster initial convergence of PLASLS or ADAMSLS be combined with the long-term convergence of ADAM without switching optimizers? The authors propose this as future work but do not explore any hybrid approaches or modifications to the line search mechanism that could achieve this goal.

## Limitations
- The computational overhead introduced by line search methods, particularly PLASLS, is not thoroughly analyzed
- The method's effectiveness appears highly dependent on the choice of component granularity and merging thresholds, which lack systematic sensitivity analysis
- The merging strategy for components with low step sizes appears to be an ad-hoc solution to numerical issues

## Confidence

- **High Confidence:** The theoretical foundation of combining Armijo line search with Adam is well-established in optimization literature
- **Medium Confidence:** The performance improvements on GLUE tasks are demonstrated, but evaluation is limited to BERT-base and a subset of GLUE datasets
- **Low Confidence:** The merging strategy for components with low step sizes appears to be an ad-hoc solution to numerical issues

## Next Checks

1. **Computational Overhead Analysis:** Measure and report the actual training time per epoch for ADAMSLS and PLASLS compared to standard Adam, across different dataset sizes and batch configurations

2. **Hyperparameter Sensitivity Study:** Systematically vary the merging threshold Î» and component granularity in PLASLS to understand their impact on convergence speed and final accuracy across multiple tasks

3. **Generalization Testing:** Evaluate the proposed methods on larger Transformer models (e.g., BERT-large, RoBERTa) and additional NLP tasks beyond GLUE to assess the scalability and broader applicability of the approach