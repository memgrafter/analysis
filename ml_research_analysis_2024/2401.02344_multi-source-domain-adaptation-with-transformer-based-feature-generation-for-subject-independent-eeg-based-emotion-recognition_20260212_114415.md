---
ver: rpa2
title: Multi-Source Domain Adaptation with Transformer-based Feature Generation for
  Subject-Independent EEG-based Emotion Recognition
arxiv_id: '2401.02344'
source_url: https://arxiv.org/abs/2401.02344
tags:
- source
- domain
- target
- adaptation
- subjects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of subject-independent EEG-based
  emotion recognition, where variations in brain signal patterns across individuals
  can reduce the effectiveness of deep learning models. The proposed method, MSDA-TF,
  combines a transformer-based feature generator with multi-source domain adaptation.
---

# Multi-Source Domain Adaptation with Transformer-based Feature Generation for Subject-Independent EEG-based Emotion Recognition

## Quick Facts
- arXiv ID: 2401.02344
- Source URL: https://arxiv.org/abs/2401.02344
- Reference count: 0
- Primary result: Achieves 0.92 ± 0.04 accuracy and 0.92 ± 0.05 F1-score on SEED dataset

## Executive Summary
This paper addresses the challenge of subject-independent EEG-based emotion recognition by proposing MSDA-TF, a method that combines a transformer-based feature generator with multi-source domain adaptation. The approach leverages correlation-based grouping of source subjects and moment matching across multiple domains to improve cross-subject generalization. Experimental results on the SEED dataset demonstrate state-of-the-art performance, significantly outperforming existing methods in subject-independent settings.

## Method Summary
MSDA-TF combines CNN blocks to capture local spatial, temporal, and spectral EEG features, followed by a transformer encoder to extract global dependencies. During domain adaptation, source subjects are grouped based on Pearson correlation values, and moments of the feature distribution of multiple sources are aligned with the target subject. The method uses leave-one-subject-out cross-validation on the SEED dataset, where 14 subjects serve as sources and 1 as the target, achieving high accuracy and F1-score through this multi-source moment matching approach.

## Key Results
- Achieves 0.92 ± 0.04 accuracy and 0.92 ± 0.05 F1-score on SEED dataset
- Outperforms state-of-the-art algorithms for subject-independent EEG emotion recognition
- Successfully validates the effectiveness of correlation-based source grouping and multi-source moment matching

## Why This Works (Mechanism)

### Mechanism 1
Grouping source subjects based on correlation values before domain adaptation improves target domain performance by reducing inter-subject variability. The method first calculates Pearson correlation across all training subjects to quantify similarity in brain responses. Subjects with highest correlation values are grouped into distinct source domains, creating more homogeneous source distributions, allowing the moment matching adaptation to better align source moments with target moments.

### Mechanism 2
Combining CNN blocks for local feature extraction with a transformer encoder for global dependencies captures both shallow spatial-temporal-spectral EEG features and long-range interactions. The CNN module uses multiple 2D convolutions to extract spatial, temporal, and spectral characteristics from EEG data. These local features are then divided into patches and projected into a lower-dimensional space. The transformer encoder applies multi-head self-attention across these patches to model global dependencies, compensating for CNN's local-only receptive field.

### Mechanism 3
Moment matching across multiple source domains and the target domain aligns the statistical distributions of features, improving cross-subject generalization. The method computes p-order moments (mean, variance, etc.) for source and target feature distributions. It minimizes the distance between these moments across all source-target pairs and all source-source pairs simultaneously. This multi-source moment alignment reduces domain shift beyond simple mean alignment.

## Foundational Learning

- Concept: Domain adaptation in transfer learning
  - Why needed here: EEG-based emotion recognition suffers from high inter-subject variability, making models trained on one subject perform poorly on another. Domain adaptation techniques are needed to transfer knowledge from labeled source subjects to an unlabeled target subject.
  - Quick check question: What is the main difference between traditional transfer learning and domain adaptation in the context of EEG emotion recognition?

- Concept: Transformer architecture and self-attention
  - Why needed here: While CNNs can extract local spatial-temporal-spectral features from EEG, they cannot capture long-range dependencies across channels and time windows that may be important for emotion recognition. The transformer's self-attention mechanism addresses this limitation.
  - Quick check question: How does self-attention in transformers differ from the local receptive fields of convolutional layers?

- Concept: Statistical moment matching for distribution alignment
  - Why needed here: Simple mean alignment (like MMD) may not be sufficient to bridge the distribution gap between source and target subjects. Higher-order moment matching captures more complex distributional differences that affect model performance.
  - Quick check question: What is the difference between first-order (mean) and higher-order moment alignment in domain adaptation?

## Architecture Onboarding

- Component map: Input EEG -> CNN Module (C1, C2) -> Patch and Position Embedding -> Transformer Encoder (8 blocks, 8 heads) -> Classifiers (K separate softmax) -> Domain Adaptation Loss (Moment Matching)

- Critical path: CNN → Patch Embedding → Transformer Encoder → Classifiers → Domain Adaptation Loss

- Design tradeoffs:
  - Multiple source domains vs single source: More complex grouping but better adaptation to subject variability
  - Higher-order moment matching vs simpler methods: More computationally expensive but potentially better alignment
  - Separate classifiers vs shared classifier: Allows source-specific decision boundaries but increases parameters

- Failure signatures:
  - Poor correlation-based grouping: If Pearson correlation doesn't capture relevant similarity, adaptation may degrade
  - Overfitting to source groups: Too few subjects per group may cause overfitting during classifier training
  - Moment matching instability: If feature distributions are too different, moment alignment may not converge

- First 3 experiments:
  1. Baseline: Train on source subjects without domain adaptation, test on target subject
  2. Single-source adaptation: Treat all source subjects as one domain, apply moment matching to target
  3. Correlation grouping validation: Verify that Pearson correlation groups subjects with similar recognition performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MSDA-TF compare when applied to datasets other than SEED, such as DEAP or DREAMER?
- Basis in paper: [inferred] The paper only validates MSDA-TF on the SEED dataset, leaving its generalizability to other EEG datasets unexplored.
- Why unresolved: The paper does not provide experimental results or analysis for other datasets.
- What evidence would resolve it: Testing MSDA-TF on additional EEG datasets and comparing performance metrics (accuracy, F1-score) to the SEED dataset results.

### Open Question 2
- Question: What is the impact of varying the number of source domains (K) on the performance of MSDA-TF?
- Basis in paper: [explicit] The paper sets K to 4 but does not explore the effects of using a different number of source domains.
- Why unresolved: The paper does not provide a sensitivity analysis for the hyperparameter K.
- What evidence would resolve it: Conducting experiments with different values of K and analyzing the resulting performance metrics to determine the optimal number of source domains.

### Open Question 3
- Question: How does MSDA-TF perform when applied to cross-session EEG-based emotion recognition, where the same subject is recorded in different sessions?
- Basis in paper: [inferred] The paper focuses on subject-independent recognition but does not address cross-session variability.
- Why unresolved: The paper does not include experiments or analysis for cross-session scenarios.
- What evidence would resolve it: Evaluating MSDA-TF on cross-session datasets and comparing performance metrics to subject-independent results.

## Limitations

- Dataset Generalization: Results validated only on SEED dataset with 15 subjects and three discrete emotion classes; performance on other datasets unknown.
- Correlation-Based Grouping Assumption: Assumes Pearson correlation effectively captures relevant brain response similarities, which may not hold for all emotion-related EEG patterns.
- Computational Complexity: Multi-source moment matching approach increases computational cost compared to single-source adaptation, though exact complexity is not quantified.

## Confidence

**High Confidence**: Core methodology of combining CNN feature extraction with transformer-based global dependency modeling is well-established. Use of differential entropy features and standard preprocessing pipelines follows accepted EEG analysis practices. Leave-one-subject-out cross-validation is appropriate for subject-independent evaluation.

**Medium Confidence**: Multi-source domain adaptation framework with correlation-based grouping is novel but lacks ablation studies showing whether correlation grouping specifically improves performance compared to random grouping or other clustering methods.

**Low Confidence**: Claims about superiority of higher-order moment matching over simpler alignment methods are not fully supported without direct comparisons to mean-only alignment or other state-of-the-art domain adaptation techniques on the same dataset.

## Next Checks

1. **Ablation Study on Grouping Strategy**: Compare performance using random subject grouping versus correlation-based grouping, and test different numbers of groups to determine if the 4-group structure is optimal or dataset-specific.

2. **Generalization to Alternative Datasets**: Validate the method on at least one additional EEG emotion recognition dataset (such as DEAP or DREAMER) with different subject counts, emotion categories, or acquisition parameters to assess real-world applicability.

3. **Computational Efficiency Analysis**: Measure and compare training time, memory usage, and inference latency against baseline methods to quantify the practical cost of the multi-source moment matching approach.