---
ver: rpa2
title: Improving LLM Group Fairness on Tabular Data via In-Context Learning
arxiv_id: '2412.04642'
source_url: https://arxiv.org/abs/2412.04642
tags:
- fairness
- arxiv
- prompt
- positive
- demographic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically investigates four empirical approaches
  to improve group fairness of large language models (LLMs) on tabular prediction
  tasks: fair prompt optimization, soft prompt tuning, strategic selection of few-shot
  examples, and self-refinement via chain-of-thought reasoning. Experiments across
  four tabular datasets using open-source and proprietary LLMs demonstrate the effectiveness
  of these methods in enhancing demographic parity while maintaining high overall
  performance.'
---

# Improving LLM Group Fairness on Tabular Data via In-Context Learning

## Quick Facts
- **arXiv ID:** 2412.04642
- **Source URL:** https://arxiv.org/abs/2412.04642
- **Reference count:** 40
- **One-line primary result:** Fair prompt optimization and fair few-shot examples emerge as the most effective universal approaches for improving LLM fairness on tabular data while maintaining high accuracy.

## Executive Summary
This paper investigates four empirical approaches to improve group fairness of large language models on tabular prediction tasks: fair prompt optimization, soft prompt tuning, strategic selection of few-shot examples, and self-refinement via chain-of-thought reasoning. Experiments across four tabular datasets using multiple open-source and proprietary LLMs demonstrate that fair few-shot examples and fair prompt optimization are the most effective universal approaches, achieving optimal accuracy-fairness trade-offs. The findings provide actionable insights for practitioners to select appropriate methods based on their specific requirements and constraints.

## Method Summary
The paper systematically evaluates four in-context learning approaches for improving group fairness in LLMs on tabular data. Fair prompt optimization uses a meta-LLM to iteratively refine fairness-specific instructions, while soft prompt tuning optimizes prompt embeddings with demographic parity regularization. Fair few-shot examples strategically select and manipulate positive example ratios across groups, and self-refinement applies chain-of-thought reasoning for post-hoc corrections. The methods are evaluated on four datasets (Adult Income, German Credit, ACS Income, ACS Coverage) using demographic parity ratio as the primary fairness metric, with accuracy and equalized odds as secondary metrics.

## Key Results
- Fair prompt optimization and fair few-shot examples consistently achieved the best accuracy-fairness trade-offs across all datasets and LLMs
- Soft prompt tuning showed variable performance, often sensitive to hyperparameters and pseudo-label quality
- Self-refinement frequently produced unstable results and degraded overall performance
- All four methods successfully improved demographic parity ratios above the 0.9 threshold while maintaining reasonable accuracy levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair prompt optimization improves group fairness by explicitly instructing the model to balance selection rates across groups.
- Mechanism: The meta-LLM iteratively refines fairness-specific instructions based on feedback from previous iterations, incorporating demographic parity scores and group selection rates. This creates a dynamic optimization loop where prompts are tailored to improve fairness.
- Core assumption: The meta-LLM can generate effective fairness instructions that the downstream model will follow when making predictions.
- Evidence anchors:
  - [abstract] "fair prompt optimization, soft prompt tuning, strategic selection of few-shot examples, and self-refining predictions via chain-of-thought reasoning"
  - [section 5.1] "we propose to optimize a fairness-specific prompt (highlighted in blue on the left panel on Figure 1), appended to the task-specific instructions"
  - [corpus] Weak evidence - no direct corpus support found for this specific meta-LLM optimization approach
- Break Condition: The meta-LLM fails to generate effective fairness instructions, or the downstream model ignores these instructions during prediction.

### Mechanism 2
- Claim: Soft prompt tuning improves fairness by directly optimizing the prompt embeddings with a fairness regularizer.
- Mechanism: Continuous optimization in the embedding space allows for the incorporation of a fairness penalty that equalizes the likelihood of positive label predictions across different groups within a batch. This approach is similar to traditional in-processing fairness interventions but applied to the prompt space.
- Core assumption: Optimizing the prompt embeddings with a fairness regularizer will lead to fairer predictions without significantly compromising accuracy.
- Evidence anchors:
  - [abstract] "soft prompt tuning, strategic selection of few-shot examples, and self-refining predictions via chain-of-thought reasoning"
  - [section 5.2] "we suggest optimizing a soft prompt by fine-tuning tokens in the embedding space... applies a penalty designed to equalize the likelihoods of tokens corresponding to positive labels across groups within a batch"
  - [corpus] Weak evidence - no direct corpus support found for this specific soft prompt tuning approach with fairness regularization
- Break Condition: The optimization process becomes too sensitive to hyperparameters, or the reliance on pseudo-labels leads to poor generalization.

### Mechanism 3
- Claim: Fair few-shot examples improve fairness by strategically selecting and presenting examples that influence the model's predictions towards balanced outcomes across groups.
- Mechanism: The method selects nearest-neighbor examples to test instances, uses zero-shot model predictions as pseudo-labels, and adjusts the ratio of positive examples between groups to enhance fairness. This approach leverages the in-context learning capabilities of LLMs to steer predictions.
- Core assumption: The selection of relevant examples and the manipulation of positive example ratios in-context will effectively influence the model's predictions to achieve better demographic parity.
- Evidence anchors:
  - [abstract] "strategic selection of few-shot examples, and self-refining predictions via chain-of-thought reasoning"
  - [section 5.3] "we propose a strategy for constructing fair few-shot examples, which differs from the previous methods in three ways... we extensively manipulate the distributions of positive and negative in-context examples between groups"
  - [corpus] Moderate evidence - related work on in-context learning and example selection for fairness exists in the corpus
- Break Condition: The nearest-neighbor selection strategy fails to find relevant examples, or the manipulation of positive example ratios leads to significant accuracy degradation.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The paper relies heavily on in-context learning methods to improve fairness in LLMs on tabular data. Understanding how LLMs learn from instructions and examples is crucial for grasping the proposed approaches.
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and what are its advantages and limitations for fairness improvement?

- Concept: Group fairness and demographic parity
  - Why needed here: The paper focuses on improving group fairness, specifically demographic parity, in LLM predictions on tabular data. Understanding these concepts is essential for evaluating the effectiveness of the proposed methods.
  - Quick check question: What is demographic parity, and how does it differ from other fairness notions like equalized odds or individual fairness?

- Concept: Prompt engineering and optimization
  - Why needed here: The paper proposes fair prompt optimization as a key method for improving fairness. Understanding how prompts can be engineered and optimized to influence model behavior is crucial for implementing this approach.
  - Quick check question: What are the key principles of effective prompt engineering, and how can prompts be optimized to achieve specific goals like fairness improvement?

## Architecture Onboarding

- Component map: Language models -> Fair prompt optimization (meta-LLM) -> Soft prompt tuning (embedding optimization) -> Fair few-shot examples (example selection/manipulation) -> Self-refinement (post-hoc correction)
- Critical path: Select method → Prepare data/prompts → Run optimization → Evaluate fairness/accuracy → Adjust as needed
- Design tradeoffs: Balancing fairness improvement with accuracy, interpretability of methods, computational efficiency, and availability of training data
- Failure signatures: Methods failing to improve fairness significantly, accuracy degradation, optimization sensitivity to hyperparameters, poor generalization from pseudo-labels, introduction of new biases
- First 3 experiments:
  1. Implement and test fair prompt optimization on a small dataset to evaluate its effectiveness and identify potential issues.
  2. Experiment with soft prompt tuning on a subset of the data to assess its impact on fairness and accuracy, and tune the hyperparameters for optimal performance.
  3. Conduct a comparative study of fair few-shot examples and self-refinement methods on a held-out validation set to determine which approach works best for the specific use case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific hyperparameters are most critical for soft prompt tuning's success in achieving fairness-accuracy trade-offs?
- Basis in paper: [explicit] "This could potentially be attributed to the sensitivity of the tuning procedure to hyperparameters or the reliance on pseudo-labels."
- Why unresolved: The paper acknowledges hyperparameter sensitivity but does not systematically investigate which specific hyperparameters (learning rate, batch size, regularization weight) have the most significant impact on fairness outcomes.
- What evidence would resolve it: A comprehensive ablation study varying each hyperparameter independently while measuring fairness and accuracy trade-offs would identify the most critical parameters.

### Open Question 2
- Question: How do the fairness improvements generalize across different protected attributes beyond gender?
- Basis in paper: [explicit] "For each of the datasets, we focus on 'gender' as the protected attribute."
- Why unresolved: The paper only examines gender as the protected attribute, limiting understanding of whether the methods generalize to other demographic dimensions like race, age, or socioeconomic status.
- What evidence would resolve it: Replicating the experiments across multiple protected attributes on the same datasets would demonstrate generalizability.

### Open Question 3
- Question: What is the long-term stability of fairness improvements achieved through these in-context methods?
- Basis in paper: [inferred] The paper presents static evaluations but does not address temporal stability or model drift over time.
- Why unresolved: In-context learning methods may be sensitive to prompt variations, data distribution shifts, or model updates, but the paper does not investigate how fairness improvements persist across different time periods or model versions.
- What evidence would resolve it: Longitudinal studies tracking fairness metrics across multiple time periods and model updates would establish the durability of improvements.

## Limitations
- Focus on binary gender as the sole sensitive attribute restricts applicability to more complex real-world scenarios
- Evaluation limited to four datasets with relatively small sample sizes (1,000 test examples per dataset)
- Computational requirements for soft prompt tuning and fair few-shot example generation could pose practical deployment constraints
- Does not address potential unintended consequences of fairness interventions or introduction of new biases

## Confidence

**High Confidence Claims:**
- The four empirical approaches are technically feasible and can be implemented using current LLM architectures
- Experimental results showing improved demographic parity ratios are reproducible given the described methodology

**Medium Confidence Claims:**
- Fair prompt optimization and fair few-shot examples are "universally effective" across different LLMs and datasets (moderate evidence from 4 models, 4 datasets)
- Trade-offs between accuracy and fairness are well-characterized within experimental scope but may vary in different contexts

**Low Confidence Claims:**
- Assertion that methods are "actionable insights for practitioners" requires real-world deployment validation
- Claim that self-refinement "often leads to unstable results" is based on limited empirical evidence

## Next Checks
1. **Multi-attribute Fairness Extension**: Replicate experiments with datasets containing multiple sensitive attributes (race, age, intersectional categories) to assess generalizability beyond binary gender protection.

2. **Real-world Deployment Simulation**: Implement the four methods in a simulated production environment with streaming data, measuring not only accuracy and demographic parity but also computational efficiency, memory usage, and model drift over time.

3. **Unintended Consequences Analysis**: Conduct a comprehensive bias audit of models trained with each fairness method, examining whether improvements in demographic parity introduce or amplify biases along other dimensions not explicitly protected against.