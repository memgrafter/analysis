---
ver: rpa2
title: Teaching Transformers Causal Reasoning through Axiomatic Training
arxiv_id: '2407.07612'
source_url: https://arxiv.org/abs/2407.07612
tags:
- causal
- causes
- training
- axiomatic
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes axiomatic training, a novel method for teaching
  transformers causal reasoning directly from symbolic demonstrations of causal axioms,
  bypassing the need for data from real-world causal systems. The approach generates
  synthetic training data as <premise, hypothesis, conclusion tuples based on simple
  causal graphs and trains models to apply axioms like transitivity and d-separation.
---

# Teaching Transformers Causal Reasoning through Axiomatic Training

## Quick Facts
- **arXiv ID**: 2407.07612
- **Source URL**: https://arxiv.org/abs/2407.07612
- **Reference count**: 32
- **Key outcome**: A 67M parameter model trained on synthetic causal axioms generalizes to complex graph structures and improves performance on causal reasoning benchmarks.

## Executive Summary
This paper introduces axiomatic training, a novel approach for teaching transformers causal reasoning directly from symbolic demonstrations of causal axioms rather than data from real-world causal systems. The method generates synthetic training data as <premise, hypothesis, conclusion> tuples based on simple causal graphs and trains models to apply axioms like transitivity and d-separation. Results show that models trained from scratch on simple chains (3-6 nodes) generalize remarkably well to complex graphs including longer chains (7-15 nodes), reversed orders, branching structures, and shuffled statements. When fine-tuning Llama-3-8B-Instruct with axiomatic data, the approach yields significant improvements on causal reasoning benchmarks like CLEAR and Corr2Cause, sometimes surpassing GPT-4.

## Method Summary
The axiomatic training method involves generating synthetic training data as <premise, hypothesis, conclusion> tuples from simple causal graphs (3-6 nodes) representing axioms like transitivity and d-separation. A 67M parameter transformer is trained from scratch using a custom tokenizer (69 vocab) and various positional encodings (NoPE, LPE, SPE, RoPE). The model learns to apply causal axioms by recognizing patterns in symbolic demonstrations rather than memorizing data distributions. Evaluation tests generalization to complex graphs (7-15 nodes), reversed chains, branching structures, and benchmark performance on CLEAR and Corr2Cause datasets, with additional fine-tuning experiments on Llama-3-8B-Instruct.

## Key Results
- Models trained on simple chains (3-6 nodes) with edge flipping generalize to complex structures including longer chains (7-15 nodes), reversed orders, branching, and shuffled statements
- NoPE positional encoding enables strong generalization to both longer sequences and complex graph structures
- Fine-tuning Llama-3-8B-Instruct on axiomatic data significantly improves performance on CLEAR and Corr2Cause benchmarks, sometimes surpassing GPT-4

## Why This Works (Mechanism)

### Mechanism 1
The model learns to apply causal axioms by recognizing patterns in symbolic demonstrations rather than memorizing data distributions. The training data consists of <premise, hypothesis, conclusion> tuples generated from synthetic causal graphs. By seeing multiple examples of the same axiom, the model learns the logical structure of how premises lead to conclusions, not just surface features.

### Mechanism 2
Diversity in training data (edge flipping, node names, graph structure) enables generalization to complex unseen structures. Training on simple chains with random edge reversals and varying node names forces the model to learn invariant features of the axiom rather than memorizing specific graph configurations. This diversity helps the model handle branching, reversed chains, and longer sequences at test time.

### Mechanism 3
Removing positional encoding (NoPE) improves generalization to longer sequences and complex structures. Without fixed positional embeddings, the model relies on attention patterns to capture positional information, which may be more flexible for handling variable-length causal chains and structural variations.

## Foundational Learning

- **Concept**: Causal axioms (transitivity, d-separation)
  - Why needed here: The model must understand these axioms to generate correct <premise, hypothesis, conclusion> pairs and evaluate its reasoning.
  - Quick check question: Can you explain in one sentence what the transitivity axiom means in causal reasoning?

- **Concept**: Graph theory basics (DAGs, paths, colliders, forks)
  - Why needed here: The synthetic data generation and evaluation depend on understanding graph structures and their properties.
  - Quick check question: What's the difference between a collider and a fork in a causal graph?

- **Concept**: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how transformers process sequences is crucial for interpreting the model's behavior and the impact of positional encoding choices.
  - Quick check question: How does self-attention allow a transformer to capture relationships between different parts of a sequence?

## Architecture Onboarding

- **Component map**: Synthetic data generation -> Custom tokenizer (69 vocab) -> 67M parameter GPT-2-like decoder (12 layers, 8 heads, 512 dim) -> Positional encoding modules (RoPE, SPE, LPE, NoPE) -> Evaluation on synthetic and benchmark datasets
- **Critical path**: Data generation → Tokenization → Model training (from scratch or fine-tuning) → Evaluation on synthetic and benchmark datasets
- **Design tradeoffs**: Smaller model (67M) vs. larger pretrained LMs - trade-off between avoiding pretraining contamination and leveraging existing knowledge. NoPE vs. PE - trade-off between generalization and positional sensitivity.
- **Failure signatures**: Random guessing on out-of-distribution examples, poor performance on reversed chains, sensitivity to node name changes, degradation with increased branching factor.
- **First 3 experiments**:
  1. Train on OCC (only sequential chains) and evaluate on reversed chains to test edge-level generalization.
  2. Train with different PEs (NoPE, SPE, LPE, RoPE) on TS2 and compare performance on longer sequences.
  3. Fine-tune Llama-3-8B-Instruct on d-separation data and evaluate on CLEAR benchmark to test real-world applicability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does axiomatic training generalize to other causal axioms beyond transitivity and d-separation?
- Basis in paper: The paper mentions that "such symbolic data can be cheaply generated for multiple axioms and added to the finetuning data of language models."
- Why unresolved: The paper only evaluates transitivity and d-separation axioms, leaving open whether this approach works for other causal axioms like the do-calculus rules or Markov equivalence.
- What evidence would resolve it: Empirical results showing improved performance on causal reasoning benchmarks when training on other causal axioms beyond transitivity and d-separation.

### Open Question 2
- Question: How does the performance of axiomatic training scale with model size?
- Basis in paper: The paper uses a 67M parameter model trained from scratch and fine-tunes a 8B parameter model, but doesn't systematically explore different model sizes or compare scaling behavior.
- Why unresolved: The paper doesn't provide results for intermediate model sizes or analyze how the benefits of axiomatic training change as model capacity increases.
- What evidence would resolve it: A systematic study varying model sizes (e.g., 100M, 500M, 1B, 3B parameters) while keeping training data constant to understand scaling effects.

### Open Question 3
- Question: Can axiomatic training be effectively combined with pretraining on synthetic axiomatic data?
- Basis in paper: The discussion section mentions "pretraining on synthetic axiomatic data may enhance language models' reasoning abilities" and suggests this as a promising direction.
- Why unresolved: The paper only explores finetuning on axiomatic data for pretrained models, not pretraining on synthetic axiomatic data from scratch.
- What evidence would resolve it: Results comparing models pretrained on synthetic axiomatic data versus models finetuned on axiomatic data, measuring performance on causal reasoning benchmarks.

## Limitations
- The approach's effectiveness on real-world causal systems with noisy, incomplete, or ambiguous relationships remains unproven
- The custom tokenizer implementation details are not fully specified, which could affect reproducibility
- Scalability to extremely large, complex real-world causal graphs has not been validated

## Confidence

**High confidence**: The mechanism of learning causal axioms through symbolic demonstrations rather than data memorization is well-supported by experimental results showing generalization to unseen graph structures and improved benchmark performance.

**Medium confidence**: The claim that removing positional encoding improves generalization to longer sequences is supported but could be context-dependent; the effectiveness might vary with different axiom types or graph complexities.

**Low confidence**: The scalability of axiomatic training to extremely large, complex real-world causal graphs remains unproven, as all experiments use synthetic data with controlled properties.

## Next Checks
1. Test the trained model on real-world causal datasets with noisy or incomplete information to assess robustness beyond synthetic data.
2. Conduct ablation studies with varying levels of data diversity to determine the minimum requirements for effective axiom learning.
3. Evaluate the approach on larger transformer architectures (e.g., 1B+ parameters) to verify if the performance gains scale proportionally.