---
ver: rpa2
title: 'GATE X-E : A Challenge Set for Gender-Fair Translations from Weakly-Gendered
  Languages'
arxiv_id: '2402.14277'
source_url: https://arxiv.org/abs/2402.14277
tags:
- gender
- source
- translation
- gendered
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces GATE X-E, a benchmark dataset for evaluating
  gender fairness in translations from weakly-gendered languages (Turkish, Hungarian,
  Finnish, Persian) into English. Each translation includes feminine, masculine, and
  neutral variants.
---

# GATE X-E : A Challenge Set for Gender-Fair Translations from Weakly-Gendered Languages

## Quick Facts
- arXiv ID: 2402.14277
- Source URL: https://arxiv.org/abs/2402.14277
- Reference count: 40
- Primary result: Benchmark dataset for gender-fair translations from weakly-gendered languages to English with 1250-1850 instances per language pair

## Executive Summary
This paper introduces GATE X-E, a benchmark dataset designed to evaluate gender fairness in machine translation from weakly-gendered languages (Turkish, Hungarian, Finnish, Persian) into English. The dataset provides feminine, masculine, and neutral variants for each translation, enabling comprehensive evaluation of gender-rewriting systems. A GPT-4-based solution using chain-of-thought prompting is evaluated on this dataset, showing high accuracy on pronoun-only translations but lower performance when gendered nouns are present. The work addresses the critical need for evaluating gender debiasing in translation systems that operate between languages with fundamentally different gender marking systems.

## Method Summary
The authors create GATE X-E by first generating English translations from weakly-gendered source languages, then having annotators produce feminine, masculine, and neutral variants for each translation. The evaluation uses a GPT-4-based rewriting system that employs chain-of-thought prompting to identify and modify gender-marked elements. The system processes source-translation pairs through a step-by-step analysis to determine which entities are referred to by gendered pronouns (AGMEs) and produces three gender variants. Accuracy is measured through exact match comparison with reference variants, with human evaluation providing error analysis.

## Key Results
- GPT-4 achieves high accuracy (0.96-0.99) on pronoun-only subsets of GATE X-E
- Accuracy drops significantly (0.5-0.8) on gendered-noun subsets due to coreference complexity
- GPT-3.5 Turbo performs reasonably well in zero-shot and few-shot settings as a cost-effective alternative
- Human evaluation reveals most errors are missing changes rather than extraneous modifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gendered pronoun-only translations from weakly-gendered languages can be rewritten accurately using GPT-4 by replacing surface forms while preserving non-AGME gender markers.
- Mechanism: The rewriting process leverages the observation that when no gender is marked in the source, all gendered pronouns in the translation refer to AGMEs. This allows for straightforward surface-form substitution without needing deeper semantic understanding or coreference resolution.
- Core assumption: The source language has no gendered pronouns, so any gendered pronouns in the translation must refer to AGMEs.
- Evidence anchors:
  - [abstract]: "It achieves high accuracy on the pronoun-only subset of GATE X-E, while..."
  - [section 3]: "For our source languages, if the only gender markers in the target sentence are gendered pronouns, there typically cannot be gender markers in the source sentence..."
- Break condition: If the source contains gender-marked nouns that determine the gender of pronouns in the translation, or if pronouns refer to non-AGMEs.

### Mechanism 2
- Claim: GPT-4's chain-of-thought prompting enables accurate identification of AGMEs by systematically analyzing source-target alignments and gender markings.
- Mechanism: The prompt guides GPT-4 through a step-by-step process: identifying unique individuals in the target, finding corresponding source words, determining which source words mark gender, and designating AGMEs as those referred to by gendered words in the target but not in the source.
- Core assumption: GPT-4 can reliably perform the semantic analysis required to align individuals across source and target sentences and determine gender marking.
- Evidence anchors:
  - [section 4.1]: "Our solution uses chain-of-thought prompting (Wang et al., 2023) to elicit GPT-4 to produce three variant translations..."
  - [section 4.1]: "Each step in the prompt is accompanied by detailed clarifications and example vocabulary."
- Break condition: If GPT-4 fails to correctly align individuals across source and target, or if it cannot reliably determine which source words mark gender.

### Mechanism 3
- Claim: The dataset structure with feminine, masculine, and neutral variants enables effective evaluation of gender-rewriting systems by providing clear reference targets for each gender assignment.
- Mechanism: By including all three gender variants for each translation, the dataset allows for exact match accuracy evaluation, ensuring that rewriters produce the correct surface forms for each gender assignment without ambiguity.
- Core assumption: The reference variants in the dataset are correct and comprehensive, covering all valid gender interpretations.
- Evidence anchors:
  - [abstract]: "Each translation is accompanied by feminine, masculine, and neutral variants."
  - [section 2.3]: "For each language, a second annotator then reviewed the data to correct errors and inconsistencies."
- Break condition: If the reference variants contain errors or if the dataset is missing valid gender interpretations.

## Foundational Learning

- Concept: Gender marking in languages
  - Why needed here: Understanding how gender is marked differently in source and target languages is crucial for identifying AGMEs and determining when rewriting is necessary.
  - Quick check question: What is the key difference in how Turkish and English mark gender in pronouns?

- Concept: Coreference resolution
  - Why needed here: Determining which pronouns refer to which individuals is essential for accurate rewriting, especially when multiple individuals are mentioned.
  - Quick check question: In the sentence "He asked his sister if she would visit," which pronouns refer to the same person?

- Concept: Surface form substitution
  - Why needed here: The rewriting process often involves simply replacing pronouns with their gendered or gender-neutral counterparts without changing the underlying meaning.
  - Quick check question: What are the feminine, masculine, and neutral forms of the pronoun "she"?

## Architecture Onboarding

- Component map:
  GATE X-E dataset -> GPT-4 rewriting system -> Evaluation pipeline

- Critical path:
  1. Load GATE X-E instance (source + original translation + gender variants)
  2. Extract test tuple for specific gender transformation
  3. Feed source-translation pair to GPT-4 with prompt
  4. Extract three gender variants from GPT-4 output
  5. Compare against reference variants for exact match accuracy

- Design tradeoffs:
  - Using GPT-4 vs. rule-based systems: GPT-4 handles complex cases but is more expensive; rule-based systems are cheaper but may miss nuanced cases
  - Exact match vs. semantic similarity metrics: Exact match is stricter but may penalize valid alternative phrasings

- Failure signatures:
  - GPT-4 produces "None" output for negative examples that should remain unchanged
  - GPT-4 modifies words that should remain unchanged (extraneous changes)
  - GPT-4 fails to modify words that should be changed (missing changes)
  - GPT-4 produces inconsistent variants (e.g., changing meaning beyond gender)

- First 3 experiments:
  1. Run GPT-4 rewriting on pronoun-only subset of GATE X-E and measure accuracy
  2. Run GPT-4 rewriting on gendered-noun subset and compare accuracy drop
  3. Evaluate GPT-4's handling of negative examples by checking if outputs match original translation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different GPT-4 prompting strategies (e.g., few-shot vs. zero-shot) on the accuracy of gender-neutral rewriting?
- Basis in paper: [explicit] The paper compares the performance of GPT-3.5 Turbo in zero-shot and few-shot settings for gender-neutral rewriting.
- Why unresolved: The paper does not explore the impact of different prompting strategies on GPT-4's performance in gender-neutral rewriting.
- What evidence would resolve it: Experiments comparing the accuracy of gender-neutral rewriting using different prompting strategies (e.g., few-shot vs. zero-shot) with GPT-4.

### Open Question 2
- Question: How does the length of the source sentence affect the difficulty of translation gender rewriting?
- Basis in paper: [inferred] The paper mentions that longer sentences may increase the scope for modifications unrelated to gender-neutral rewriting.
- Why unresolved: The paper does not provide a quantitative analysis of the relationship between sentence length and rewriting difficulty.
- What evidence would resolve it: Experiments measuring the accuracy of translation gender rewriting as a function of source sentence length.

### Open Question 3
- Question: What is the potential of open-source models (e.g., BLOOM, OPT) for translation gender rewriting compared to GPT-4?
- Basis in paper: [explicit] The paper mentions that the potential of open-source models remains unexplored and could be beneficial.
- Why unresolved: The paper does not evaluate the performance of open-source models on translation gender rewriting.
- What evidence would resolve it: Experiments comparing the accuracy of translation gender rewriting using open-source models and GPT-4.

## Limitations

- The approach shows significant accuracy degradation (0.5-0.8) on gendered-noun subsets due to coreference resolution complexity
- The dataset construction process may introduce bias through translator assumptions when translating from gender-neutral source languages
- Exact match evaluation may penalize valid alternative phrasings that maintain semantic equivalence

## Confidence

- High confidence in claims about dataset construction and overall framework
- Medium confidence in GPT-4 rewriting performance (dependent on prompt template)
- Low confidence in generalizability across different language pairs and text types

## Next Checks

1. Test the GPT-4 rewriting system on additional weakly-gendered languages (e.g., Japanese, Korean, Basque) to assess cross-linguistic generalizability

2. Conduct blind human evaluation where professional translators assess GPT-4 generated variants vs. references, measuring inter-annotator agreement

3. Implement GPT-3.5 Turbo variant and measure accuracy-cost tradeoff across different use cases to determine practical viability