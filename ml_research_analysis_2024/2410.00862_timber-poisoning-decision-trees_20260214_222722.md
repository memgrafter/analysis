---
ver: rpa2
title: Timber! Poisoning Decision Trees
arxiv_id: '2410.00862'
source_url: https://arxiv.org/abs/2410.00862
tags: []
core_contribution: The paper introduces Timber, the first white-box poisoning attack
  targeting decision trees. The core idea leverages sub-tree retraining to efficiently
  estimate the damage of poisoning a given training instance.
---

# Timber! Poisoning Decision Trees

## Quick Facts
- arXiv ID: 2410.00862
- Source URL: https://arxiv.org/abs/2410.00862
- Reference count: 40
- Primary result: First white-box poisoning attack targeting decision trees that achieves 60% F1 score reduction on Musk2 dataset

## Executive Summary
Timber introduces the first white-box poisoning attack specifically designed for decision trees and random forests. The attack leverages sub-tree retraining to efficiently estimate the damage of poisoning individual training instances, prioritizing attacks based on computational cost. By annotating each node with stable instances, Timber enables early stopping for larger datasets, achieving both superior effectiveness and efficiency compared to existing poisoning baselines. Experiments show Timber can reduce F1 scores from 0.88 to 0.34 on Musk2 dataset while being significantly faster than full-tree retraining approaches.

## Method Summary
Timber is a white-box poisoning attack that exploits the structure of decision trees by using sub-tree retraining instead of full-tree retraining. The attack annotates each node with stable instances to identify which training instances affect specific nodes, then prioritizes poisoning based on computational cost. For each candidate instance, Timber estimates damage by flipping its label and retraining only the affected subtree. The attack uses a greedy strategy that sorts instances by their retraining cost, enabling early stopping when sufficient damage is achieved. Timber can be extended to random forests by applying the annotation procedure to individual trees and aggregating scores.

## Key Results
- Timber reduces F1 score from 0.88 to 0.34 on Musk2 dataset, outperforming Entropy baseline (0.49)
- Achieves 60% reduction in F1 score while maintaining significant efficiency gains through sub-tree retraining
- Defenses like kNN and bagging reduce but don't eliminate Timber's effectiveness, with residual F1 drops of 0.16-0.17

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sub-tree retraining is more efficient than full-tree retraining for most poisoning attacks
- Mechanism: By annotating nodes with stable instances, Timber identifies when a poisoned instance doesn't affect a node's best split, allowing the attack to only retrain the subtree where the poisoned instance impacts the best split
- Core assumption: A single poisoned instance rarely affects the best split of nodes high in the tree, especially the root
- Evidence anchors: [abstract] "Timber is based on a greedy attack strategy leveraging sub-tree retraining to efficiently estimate the damage performed by poisoning a given training instance"
- Break condition: If poisoned instances frequently affect high-level nodes, the efficiency gain diminishes as more of the tree needs retraining

### Mechanism 2
- Claim: Sorting training instances by computational cost of sub-tree retraining enables efficient early stopping
- Mechanism: Timber annotates each node with stable instances and assigns a score to each training instance based on the percentage of training instances in the first node where the instance is not stable
- Core assumption: Prioritizing instances with lower computational cost for sub-tree retraining leads to faster convergence of the attack
- Evidence anchors: [abstract] "The attack relies on a tree annotation procedure, which enables the sorting of training instances so that they are processed in increasing order of the computational cost of sub-tree retraining"
- Break condition: If the early stopping condition is not met even after processing all instances, the efficiency gain from sorting is lost

### Mechanism 3
- Claim: Timber can be extended to random forest ensembles by applying the annotation procedure to individual trees and aggregating scores
- Mechanism: Timber's annotation procedure can be directly applied to each tree in a random forest ensemble. The score of an instance is redefined as the mean (or maximum) of the scores computed for the individual trees
- Core assumption: The computational cost of retraining sub-trees in an ensemble is the aggregate of the costs for each individual tree
- Evidence anchors: [abstract] "We also discuss an extension of Timber to traditional random forest models"
- Break condition: If the ensemble uses interdependent trees (e.g., gradient boosted decision trees), the extension of Timber is not straightforward and may not be effective

## Foundational Learning

- Concept: Decision Tree Learning
  - Why needed here: Understanding how decision trees are constructed is crucial for comprehending how Timber exploits their structure for efficient poisoning
  - Quick check question: What is the main criterion used to determine the best split when constructing a decision tree?

- Concept: Information Gain and Entropy
  - Why needed here: Timber leverages information gain and entropy to annotate nodes with stable instances and prioritize attacks
  - Quick check question: How does flipping the label of a training instance affect the information gain of a split?

- Concept: Ensemble Methods (Random Forest)
  - Why needed here: Timber is extended to random forest ensembles, so understanding how random forests work is essential
  - Quick check question: How does a random forest make predictions for a new instance?

## Architecture Onboarding

- Component map: Tree annotation module -> Score calculation module -> Attack strategy module -> Sub-tree retraining module
- Critical path:
  1. Train a decision tree on the clean training data
  2. Annotate the tree with stable instances
  3. For each instance, calculate its score and prioritize instances with lower scores
  4. Poison the instance with the lowest score and retrain only the affected subtree
  5. Repeat steps 3-4 until the early stopping condition is met or the maximum number of poisoned instances is reached

- Design tradeoffs:
  - Efficiency vs. effectiveness: Timber prioritizes efficiency by using sub-tree retraining and early stopping, but this may lead to a slightly lower attack effectiveness compared to retraining the entire tree
  - Scalability vs. complexity: Timber's extension to random forest ensembles improves scalability but adds complexity to the score calculation and attack strategy modules

- Failure signatures:
  - If the tree annotation module fails to correctly identify stable instances, the sub-tree retraining module may retrain unnecessary parts of the tree, reducing efficiency
  - If the score calculation module assigns incorrect scores to instances, the attack strategy module may prioritize the wrong instances, reducing effectiveness

- First 3 experiments:
  1. Train a decision tree on a small dataset and manually verify the stability annotations for each node
  2. Poison a single instance and compare the runtime of sub-tree retraining vs. full-tree retraining
  3. Extend Timber to a random forest ensemble and compare the attack effectiveness and efficiency with the single tree case

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would Timber be against Gradient Boosted Decision Trees (GBDTs) compared to Random Forests?
- Basis in paper: [explicit] The paper explicitly states that Timber cannot be readily generalized to GBDTs due to their sequential training nature, which makes sub-tree retraining ineffective
- Why unresolved: The paper only discusses the theoretical limitations and does not provide experimental evidence or comparisons between Timber's effectiveness on GBDTs versus Random Forests
- What evidence would resolve it: Experimental results showing Timber's performance on GBDTs, including accuracy/F1 score degradation and runtime comparisons with Random Forests

### Open Question 2
- Question: What is the impact of different label flip strategies on Timber's effectiveness?
- Basis in paper: [inferred] The paper assumes label flipping attacks targeting the positive class but does not explore variations in attack strategies
- Why unresolved: The paper focuses on a single label flipping strategy and does not investigate how alternative strategies might affect Timber's performance or the robustness of the target model
- What evidence would resolve it: Comparative experiments evaluating Timber's effectiveness using different label flip strategies, including random flipping, feature-based flipping, and hardness-based flipping

### Open Question 3
- Question: How does the choice of kNN or bagging defense parameters affect their ability to mitigate Timber attacks?
- Basis in paper: [explicit] The paper evaluates two defenses (kNN and bagging) with various parameter settings but does not provide a detailed analysis of how specific parameter choices influence their effectiveness against Timber
- Why unresolved: While the paper shows that defenses provide some mitigation, it does not explore the sensitivity of defense performance to parameter variations or identify optimal parameter settings for each dataset
- What evidence would resolve it: Sensitivity analysis of defense performance across different parameter values, including kNN neighbor count (N), threshold (Î·), and bagging subset size (K), to identify parameter settings that maximize defense effectiveness against Timber

## Limitations

- The core assumption that poisoned instances rarely affect high-level nodes may not hold for all datasets or attack scenarios
- The extension to random forest ensembles is described but not extensively validated experimentally
- Computational cost savings from sub-tree retraining depend heavily on the specific structure of the decision tree

## Confidence

- High: Timber's effectiveness in reducing F1 scores compared to baseline attacks
- Medium: Efficiency gains from sub-tree retraining mechanism
- Medium: Extension to random forest ensembles

## Next Checks

1. Verify the tree annotation procedure by examining stability scores across different node depths in the tree
2. Test Timber's performance on additional datasets with varying decision tree structures to assess generalizability
3. Implement and validate the random forest extension on a synthetic ensemble with known inter-tree dependencies