---
ver: rpa2
title: Cross-Lingual Multi-Hop Knowledge Editing
arxiv_id: '2407.10275'
source_url: https://arxiv.org/abs/2407.10275
tags:
- knowledge
- editing
- edits
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of cross-lingual multi-hop knowledge
  editing in large language models (LLMs), where new knowledge needs to be efficiently
  updated across multiple languages while maintaining consistency in multi-step reasoning.
  The authors identify significant gaps in the performance of existing methods when
  applied to non-English languages, particularly due to difficulties in retrieving
  and transferring fact edits across languages.
---

# Cross-Lingual Multi-Hop Knowledge Editing

## Quick Facts
- arXiv ID: 2407.10275
- Source URL: https://arxiv.org/abs/2407.10275
- Authors: Aditi Khandelwal; Harman Singh; Hengrui Gu; Tianlong Chen; Kaixiong Zhou
- Reference count: 16
- One-line primary result: Introduces CLEVER-CKE, a novel retrieval-augmented knowledge editing framework that significantly outperforms previous methods in cross-lingual multi-hop knowledge editing tasks.

## Executive Summary
This work addresses the challenge of cross-lingual multi-hop knowledge editing in large language models (LLMs), where new knowledge needs to be efficiently updated across multiple languages while maintaining consistency in multi-step reasoning. The authors identify significant gaps in the performance of existing methods when applied to non-English languages, particularly due to difficulties in retrieving and transferring fact edits across languages. To tackle this, they introduce CLEVER-CKE, a retrieval-augmented knowledge editing framework that decomposes multi-hop questions into sub-questions and retrieves relevant cross-lingual fact edits using a specially trained retriever.

## Method Summary
CLEVER-CKE employs a retrieve, verify and generate approach to knowledge editing in LLMs. It first decomposes multi-hop questions into sub-questions using in-context examples, then retrieves relevant edited facts for each sub-question using a DistilBERT multilingual encoder fine-tuned with language-aware and hard-negative mining based contrastive losses. The retrieved facts are verified through similarity thresholds, and the LLM generates sub-answers using the retrieved facts if verified, otherwise relying on its internal knowledge. Finally, the LLM extracts the final answer from the sub-answers. The contrastive losses improve cross-lingual retrieval and fine-grained understanding of fact edits by encouraging the model to distinguish between semantically similar edits across languages.

## Key Results
- CLEVER-CKE significantly outperforms previous methods, achieving up to 30% improvement in knowledge editing accuracy.
- The framework demonstrates more than 25% improvement in hop-wise accuracy on the real-world temporal dataset CROLIN-MQUAKE-T.
- Extensive experiments on three LLMs (ChatGPT, LLaMa-2-7B, and Vicuna-1.5-7B), eight languages, and two datasets validate the effectiveness of CLEVER-CKE in bridging the performance gap between English-centric and cross-lingual knowledge editing.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The retriever trained with language-aware and hard-negative contrastive losses improves cross-lingual fact retrieval accuracy.
- Mechanism: The retriever learns to distinguish semantically similar edits across languages by contrasting positive edit pairs (same fact in two languages) against hard negatives (same-language edits with similar surface form but different semantic content).
- Core assumption: The contrastive loss with hard negatives forces the model to focus on fine-grained semantic differences rather than surface-level lexical overlap.
- Evidence anchors:
  - [abstract] "The key innovation lies in the use of language-aware and hard-negative mining based contrastive losses, which improve both cross-lingual retrieval and fine-grained understanding of fact edits."
  - [section] "LSD promotes learning the fine-grained knowledge about subject, relation and object in a cross-lingual setting and encourages the model to distinguish the semantic nuances in different edits."
- Break condition: If the retriever fails to generate meaningful hard negatives, or if the distance metric does not capture semantic similarity across languages, the contrastive objective may not yield improvements.

### Mechanism 2
- Claim: Decomposing multi-hop questions into sub-questions enables targeted retrieval of relevant fact edits, reducing error propagation in reasoning chains.
- Mechanism: The LLM breaks down a complex multi-hop query into simpler sub-questions; for each sub-question, the retriever fetches the most relevant edited fact, which the LLM then uses to answer the sub-question. This localized retrieval limits the impact of retrieval errors to a single reasoning step.
- Core assumption: Sub-question decomposition is accurate and the retriever can reliably retrieve edits relevant to each sub-question.
- Evidence anchors:
  - [section] "Given a multi-hop question Q, LLM is prompted using in-context examples to decompose it into various sub-questions Qsub = {q1, q2, . . .}."
  - [section] "For each sub-question q, CLEVER-CKE retrieves the top-1 candidate r ∈ F using cosine similarity."
- Break condition: If the LLM cannot decompose the question correctly or the retriever fails to retrieve relevant edits for any sub-question, the chain reasoning will break.

### Mechanism 3
- Claim: Cross-lingual edit consistency loss encourages the model to differentiate between edits in different languages, enhancing multilingual knowledge editing performance.
- Mechanism: By contrasting a question in English with its correct translated edited fact (positive) against a random edit in another language (negative), the retriever learns to map questions to the correct language-specific edits.
- Core assumption: The negative sampling of random edits from other languages provides a meaningful signal for learning cross-lingual consistency.
- Evidence anchors:
  - [section] "We employ a contrastive, triplet margin loss LCLEC focused on improving cross-lingual retrieval. Here, the anchor is Qen, a question in English. The edited fact for answering that question, TL1(e), serves as the positive example, and a random edit TL2(erand) forms the negative example."
- Break condition: If the random negative edits are not sufficiently dissimilar or the model overfits to English-centric patterns, the loss may not improve cross-lingual generalization.

## Foundational Learning

- Concept: Contrastive learning with hard negative mining
  - Why needed here: To force the retriever to focus on fine-grained semantic distinctions between edits, especially across languages with similar surface forms.
  - Quick check question: What is the difference between standard contrastive loss and the hard-negative variant used here?
- Concept: Multi-hop reasoning decomposition
  - Why needed here: To localize retrieval errors and enable stepwise correction of reasoning chains when knowledge edits ripple through multiple facts.
  - Quick check question: How does breaking a multi-hop question into sub-questions reduce error propagation?
- Concept: Cross-lingual embedding alignment
  - Why needed here: To ensure that facts edited in one language can be correctly retrieved when answering questions in another language.
  - Quick check question: Why is it important for the retriever to map questions in one language to edits in another?

## Architecture Onboarding

- Component map:
  - LLM backbone -> Retriever (DistilBERT multilingual encoder) -> Fact memory -> LLM backbone
- Critical path:
  1. Input multi-hop question → LLM decomposition into sub-questions
  2. For each sub-question: Retriever fetches top-1 edit → Verification via similarity threshold
  3. LLM generates sub-answer using retrieved fact if verified, else internal knowledge
  4. LLM extracts final answer from sub-answers
- Design tradeoffs:
  - Retriever complexity vs. inference latency: Using DistilBERT keeps latency low but may limit semantic understanding vs. larger models
  - Threshold tuning: Stricter thresholds reduce false positives but may miss valid edits; looser thresholds increase coverage but risk noise
  - Bilingual vs. multilingual training: Bilingual setting improves performance per language but requires separate models per language pair
- Failure signatures:
  - Low recall in retrieval: Retriever often fails to retrieve relevant edits → Check embedding quality and contrastive loss tuning
  - High contradiction errors: LLM frequently contradicts retrieved facts → Investigate threshold setting or retriever relevance scoring
  - Poor cross-lingual transfer: Edits in one language rarely retrieved for questions in another → Evaluate cross-lingual embedding alignment
- First 3 experiments:
  1. Train retriever on English-only data, evaluate on cross-lingual questions to establish baseline performance gap
  2. Add hard-negative contrastive loss, compare retrieval accuracy vs. baseline
  3. Train bilingual retriever (English + target language), compare to multilingual retriever across languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CLEVER-CKE's performance scale with increasing number of languages in the multilingual retriever setting?
- Basis in paper: [explicit] The paper notes that CLEVER-CKE's losses lead to reduced interference of languages and more generalization compared to PokeMQA-CL, but does not provide detailed scaling analysis across multiple languages.
- Why unresolved: The paper only provides a limited comparison of bilingual vs. multilingual retriever performance for a subset of languages, without exploring how performance changes as the number of languages increases.
- What evidence would resolve it: Detailed experiments showing CLEVER-CKE's performance as the number of languages in the multilingual retriever setting increases from 2 to 8, compared to baselines.

### Open Question 2
- Question: What is the impact of CLEVER-CKE on low-resource languages, given the current analysis focuses on medium to high-resource languages?
- Basis in paper: [inferred] The paper acknowledges its analysis is limited to medium to high-resource languages and mentions extending to low-resource languages as future work, suggesting this is an unexplored area.
- Why unresolved: The paper does not provide any empirical data or analysis on how CLEVER-CKE performs with low-resource languages, which could have different challenges compared to the studied languages.
- What evidence would resolve it: Experiments applying CLEVER-CKE to low-resource languages, with detailed performance metrics and comparison to existing methods, would provide insights into its effectiveness in these settings.

### Open Question 3
- Question: How does the quality of translations affect CLEVER-CKE's performance, particularly for languages with lower BLEU scores?
- Basis in paper: [explicit] The paper discusses translation quality and backtranslation BLEU scores, noting that Chinese has lower scores, but does not investigate how this impacts CLEVER-CKE's performance.
- Why unresolved: While the paper ensures translation quality through human verification, it does not explore the relationship between translation quality (as measured by BLEU scores) and knowledge editing performance.
- What evidence would resolve it: Correlation analysis between translation quality metrics (like BLEU scores) and CLEVER-CKE's performance across different languages, with experiments varying translation quality, would clarify this relationship.

## Limitations
- The framework relies heavily on the LLM's ability to correctly decompose multi-hop questions, and errors in this step can propagate through the reasoning chain.
- The effectiveness of the contrastive losses depends on the availability of high-quality hard negatives, which may not always be present in the training data.
- The paper does not address how the framework scales to languages with significantly different linguistic structures or handles scenarios with ambiguous or contradictory edited facts.

## Confidence
- **High confidence**: The claim that CLEVER-CKE improves cross-lingual multi-hop knowledge editing accuracy, supported by extensive experiments on multiple LLMs and languages.
- **Medium confidence**: The effectiveness of the language-aware and hard-negative mining based contrastive losses in improving cross-lingual retrieval and fine-grained understanding of fact edits.
- **Low confidence**: The claim that the framework can handle languages with significantly different linguistic structures or scenarios with ambiguous or contradictory edited facts.

## Next Checks
1. **Reproduce retriever performance**: Train the retriever with the specified contrastive losses (LSD, LCLEC, LBCE) on a subset of the CROLIN-MQUAKE dataset and evaluate its cross-lingual retrieval accuracy on a held-out test set. Compare the performance to a baseline retriever trained without the contrastive losses to quantify the impact of the proposed losses.

2. **Analyze error propagation**: Conduct an ablation study to assess the impact of LLM-based sub-question decomposition on the overall accuracy of CLEVER-CKE. Compare the performance of CLEVER-CKE with and without the sub-question decomposition step on a set of multi-hop questions, and analyze the types of errors that occur in each case.

3. **Evaluate scalability to diverse languages**: Test the performance of CLEVER-CKE on a set of languages with significantly different linguistic structures (e.g., languages with non-Latin scripts, agglutinative languages, or languages with complex morphology). Compare the knowledge editing accuracy across these languages to the performance on the eight languages evaluated in the paper, and identify any patterns or challenges that arise.