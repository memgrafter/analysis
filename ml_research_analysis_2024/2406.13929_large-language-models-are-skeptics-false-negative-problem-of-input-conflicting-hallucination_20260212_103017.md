---
ver: rpa2
title: 'Large Language Models are Skeptics: False Negative Problem of Input-conflicting
  Hallucination'
arxiv_id: '2406.13929'
source_url: https://arxiv.org/abs/2406.13929
tags:
- 'false'
- context
- question
- rewriting
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a "false negative problem" in large language
  models, where they are predisposed to deny true statements when assessing factuality
  based on context. This bias leads to input-conflicting hallucinations, where models
  generate responses inconsistent with the input context.
---

# Large Language Models are Skeptics: False Negative Problem of Input-conflicting Hallucination

## Quick Facts
- arXiv ID: 2406.13929
- Source URL: https://arxiv.org/abs/2406.13929
- Reference count: 36
- Key finding: LLMs exhibit false negative bias, denying true statements when assessing factuality based on context

## Executive Summary
This paper identifies a significant bias in large language models (LLMs) where they are predisposed to deny true statements when assessing factuality based on context. This "false negative problem" leads to input-conflicting hallucinations, where models generate responses inconsistent with the input context. The study systematically examines this bias across multiple models including Mistral, ChatGPT, and GPT-4, finding that LLMs exhibit greater overconfidence when responding with False. The research also demonstrates that context and query rewriting can effectively mitigate this problem in some models, though model-specific behaviors were observed, particularly with GPT-4.

## Method Summary
The authors employed a systematic experimental approach using All-True and All-False prompts to characterize the false negative bias across different LLMs. They measured overconfidence levels when models responded with False versus True statements. The study then analyzed the impact of context and query rewriting techniques as potential mitigation strategies. Model-specific behaviors were examined by testing Mistral, ChatGPT, and GPT-4 under controlled conditions to identify patterns and variations in how different architectures handle context-based factuality assessment.

## Key Results
- LLMs consistently show greater overconfidence when denying true statements compared to affirming false ones
- The false negative problem occurs across multiple models including Mistral, ChatGPT, and GPT-4
- Context and query rewriting effectively mitigate the false negative problem in Mistral and ChatGPT
- GPT-4 exhibits increased false negatives with rewriting, suggesting model-specific behaviors
- The bias represents a fundamental challenge in LLM factuality assessment that requires architectural attention

## Why This Works (Mechanism)
The false negative problem arises from the interaction between parametric knowledge stored in model weights and contextual information provided during inference. When LLMs encounter conflicting information between their training-based knowledge and input context, they tend to default to denial rather than reconciliation. This mechanism appears to be rooted in the model's optimization objectives and training paradigms, where negative responses may be reinforced as safer or more conservative outputs. The bias likely stems from the model's attempt to minimize hallucination risk, but paradoxically creates a systematic error pattern that undermines factual accuracy.

## Foundational Learning
1. **Parametric vs Contextual Knowledge**: Understanding how LLMs balance stored knowledge against input context is crucial for diagnosing factuality issues. Quick check: Trace how a model responds when context contradicts training data.

2. **Overconfidence Metrics**: Measuring confidence levels in model responses requires careful calibration between probability scores and actual accuracy. Quick check: Compare confidence distributions for True vs False responses across multiple test cases.

3. **Hallucination Types**: Distinguishing between input-conflicting hallucinations and other forms helps target mitigation strategies. Quick check: Categorize hallucination instances by their relationship to input context.

## Architecture Onboarding

Component Map:
Input Context -> Factuality Assessment Module -> Confidence Calibration -> Output Generation

Critical Path:
Context processing → Knowledge retrieval/synthesis → Confidence scoring → Response generation

Design Tradeoffs:
The false negative bias represents a tradeoff between hallucination prevention and factual accuracy. Conservative denial reduces some hallucination risks but introduces systematic false negatives. This tradeoff is particularly acute in safety-critical applications where both types of errors carry significant costs.

Failure Signatures:
- Consistent denial of verifiable true statements
- Higher confidence scores on false negative responses
- Context inconsistency between input and output
- Model-specific variations in response patterns

First Experiments:
1. Test false negative rates on mixed-truth prompts beyond All-True/All-False scenarios
2. Compare confidence calibration across different temperature settings
3. Evaluate mitigation effectiveness using alternative rewriting strategies

## Open Questions the Paper Calls Out
None

## Limitations
- The underlying causes of the false negative bias remain incompletely understood
- Findings may not generalize to all LLM architectures beyond the tested models
- Focus on binary prompts may not capture complexity of real-world factuality assessment
- Model-specific behaviors suggest solutions may not be universally applicable

## Confidence
- High confidence in the existence of false negative bias
- Medium confidence in systematic characterization across models
- Medium confidence in proposed mitigation strategies
- Low confidence in universal solutions due to model-specific behaviors

## Next Checks
1. Test the false negative problem across a broader range of LLM architectures and training paradigms to assess generalizability
2. Evaluate the bias in more complex, mixed-truth scenarios beyond All-True and All-False prompts
3. Investigate the relationship between model size, training data, and susceptibility to the false negative problem