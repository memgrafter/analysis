---
ver: rpa2
title: Exploring Spatial Schema Intuitions in Large Language and Vision Models
arxiv_id: '2402.00956'
source_url: https://arxiv.org/abs/2402.00956
tags:
- image
- language
- human
- experiment
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores whether large language models (LLMs) and vision-language
  models (VLMs) can capture human intuitions about spatial schemas in language, despite
  lacking embodied experience. The authors reproduce three psycholinguistic experiments
  to test this hypothesis, comparing model responses with human data.
---

# Exploring Spatial Schema Intuitions in Large Language and Vision Models

## Quick Facts
- arXiv ID: 2402.00956
- Source URL: https://arxiv.org/abs/2402.00956
- Authors: Philipp Wicke; Lennart Wachowiak
- Reference count: 17
- Key outcome: Larger LLMs, particularly GPT-4, show moderate to strong correlations with human judgments on spatial schemas, while VLMs perform poorly, especially on abstract representations.

## Executive Summary
This paper investigates whether large language models (LLMs) and vision-language models (VLMs) can capture human intuitions about spatial schemas in language, despite lacking embodied experience. The authors reproduce three psycholinguistic experiments comparing model responses with human data, finding that larger models like GPT-4 show moderate to strong correlations with human judgments, particularly in textual tasks. However, VLMs, especially open-source ones, perform poorly, and all models tend to give polarized responses. The results suggest LLMs can reflect spatial intuitions to some extent, but discrepancies remain, possibly due to the lack of embodied experience and relative rating strategies.

## Method Summary
The study reproduces three psycholinguistic experiments (Gibbs et al. 1994, Beitel et al. 2001, Richardson et al. 2001) using pre-trained LLMs (GPT-3, GPT-3.5, GPT-4, LLaMA-2) and VLMs (IDEFICS, GPT-4 Vision) without fine-tuning. Models were prompted with task instructions and image schema definitions, then their responses were compared to human data using Spearman correlation coefficients. The experiments tested intuitions about spatial schemas underlying verb usage, preposition phrases, and verb concreteness rankings.

## Key Results
- GPT-4 shows moderate to strong correlations (0.4-0.7+) with human judgments across most image schemas
- VLMs perform poorly, especially on abstract line drawings, with GPT-4 Vision achieving moderate correlations only on the UP-DOWN schema
- All models exhibit polarized responses, selecting only 1 or 7 on the 7-point scale
- Larger models consistently outperform smaller models in capturing spatial intuitions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger language models, particularly GPT-4, can capture human intuitions about spatial schemas in language despite lacking embodied experience.
- Mechanism: These models have been trained on vast amounts of text, allowing them to model word usage, contextual relationships, and associations with schema definitions, enabling them to reflect spatial intuitions to some extent.
- Core assumption: The training data contains sufficient examples of spatial language use and schema-related concepts for the model to learn implicit patterns.
- Evidence anchors:
  - [abstract] "larger models, particularly GPT-4, show moderate to strong correlations with human judgments, especially in textual tasks"
  - [section] "Another possibility is that the training data contains sufficient examples of spatial language use and schema-related concepts for the model to learn implicit patterns."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.429" (weak corpus evidence)
- Break condition: If the training data lacks diverse or sufficient examples of spatial language use, or if the model's capacity is insufficient to capture complex relationships.

### Mechanism 2
- Claim: The discrepancy between model and human responses may be due to the lack of embodied experience and relative rating strategies.
- Mechanism: Humans use embodied experiences to ground spatial concepts, and they rate items relative to each other. Models lack this grounding and can only rate items in isolation, leading to less nuanced responses.
- Core assumption: Embodied experiences are crucial for fully understanding and applying spatial schemas in language.
- Evidence anchors:
  - [abstract] "discrepancies remain, possibly due to the lack of embodied experience and relative rating strategies"
  - [section] "Humans use embodied experiences to ground spatial concepts, and they rate items relative to each other. Models lack this grounding and can only rate items in isolation, leading to less nuanced responses."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.429" (weak corpus evidence)
- Break condition: If models develop mechanisms to simulate relative rating or if embodied experience is not as crucial as assumed.

### Mechanism 3
- Claim: Vision-language models (VLMs) perform poorly on spatial tasks due to their training on natural images, which are unsuitable for interpreting highly abstract line drawings.
- Mechanism: VLMs are trained on natural images, and their understanding of spatial concepts may not generalize well to abstract representations like line drawings used in the experiments.
- Core assumption: The type of visual input significantly impacts a model's ability to understand spatial concepts.
- Evidence anchors:
  - [abstract] "VLMs, particularly open-source ones, perform poorly, and all models tend to give polarized responses"
  - [section] "VLMs are trained on natural images, which are unsuitable for interpreting the highly abstract line drawings."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.429" (weak corpus evidence)
- Break condition: If VLMs can be trained on more diverse visual data, including abstract representations, or if the abstract representations are not as crucial for understanding spatial concepts.

## Foundational Learning

- Concept: Spatial schemas and their role in language and cognition.
  - Why needed here: The paper investigates whether models can capture human intuitions about spatial schemas in language, so understanding what spatial schemas are and how they relate to language is fundamental.
  - Quick check question: What are spatial schemas, and how do they influence our understanding and use of language?

- Concept: Embodied cognition and its relationship to language.
  - Why needed here: The paper explores the role of embodiment in language models, comparing them to embodied systems in robotics. Understanding embodied cognition is crucial for interpreting the results and implications of the study.
  - Quick check question: How does embodied cognition relate to language processing, and what are the implications of lacking embodied experience for language models?

- Concept: Large language models and their capabilities.
  - Why needed here: The paper uses LLMs and VLMs to reproduce psycholinguistic experiments, so understanding how these models work and their limitations is essential for interpreting the results.
  - Quick check question: What are large language models, and how do they process and generate language?

## Architecture Onboarding

- Component map: LLMs (GPT-3, GPT-3.5, GPT-4, LLaMA-2) and VLMs (IDEFICS, GPT-4 Vision) -> psycholinguistic experiments (Gibbs et al., Beitel et al., Richardson et al.) -> human response data comparison
- Critical path: Select models and prompts -> Run experiments -> Extract model responses -> Compute Spearman correlations with human data -> Analyze results
- Design tradeoffs: Larger models yield better correlations but at higher computational cost; open-source models are more accessible but perform worse than proprietary models
- Failure signatures: Low correlations with human responses, polarized responses (only 1 or 7), meaningless VLMs responses
- First 3 experiments:
  1. Experiment 1 - Gibbs et al. (1994): Test participants' intuitions about image schemas underlying various uses of the verb "to stand"
  2. Experiment 2 - Beitel et al. (2001): Repeat experimental paradigm with new set of phrases containing preposition "on"
  3. Experiment 3 - Richardson et al. (2001): Present participants with verbs ranked by concreteness and ask them to choose images representing actions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are the original human participant responses to temporal linguistic changes over the past 23-30 years?
- Basis in paper: [explicit] The paper notes that original experiments were conducted 23-30 years ago and acknowledges potential disparities between original data and contemporary linguistic trends.
- Why unresolved: The paper plans to replicate studies to gauge temporal robustness but has not yet done so.
- What evidence would resolve it: Conducting new experiments with contemporary participants using same stimuli as original studies.

### Open Question 2
- Question: To what extent do larger LLMs (e.g., GPT-4) rely on parsing original papers' results in training data versus capturing spatial intuitions through other means?
- Basis in paper: [explicit] The paper suggests models' reproduction of observed patterns might be influenced by original experiment papers being in training data, but notes parsing original papers' results effectively is unlikely.
- Why unresolved: Difficult to prove whether original papers' results were parsed during training, and paper doesn't provide clear method to disentangle this effect from other potential sources of models' spatial intuitions.
- What evidence would resolve it: Training LLMs on corpus that excludes original papers and testing their performance on same tasks.

### Open Question 3
- Question: How do multilingual LLMs compare to monolingual ones in capturing spatial schema intuitions across different languages?
- Basis in paper: [explicit] The paper acknowledges reproduced studies solely feature English language and notes multilingual analysis is subject of future work.
- Why unresolved: Current study only investigates English-language tasks and doesn't explore how well multilingual LLMs can capture spatial intuitions in other languages.
- What evidence would resolve it: Conducting same experiments with multilingual LLMs using stimuli from various languages.

## Limitations

- Reliance on correlational evidence without establishing causal mechanisms for why larger models better capture spatial intuitions
- Binary nature of model responses (extreme ratings 1 or 7) rather than utilizing full 7-point scale, potentially indicating fundamental limitations
- Poor VLM performance on abstract line drawings raises questions about generalizability to real-world vision-language applications

## Confidence

**High Confidence**: Larger models (especially GPT-4) show stronger correlations with human spatial judgments than smaller models across multiple experiments and schemas.

**Medium Confidence**: Models capture spatial intuitions "to some extent" despite lacking embodied experience, though mechanism remains unclear and extent of genuine understanding versus pattern matching is uncertain.

**Low Confidence**: Discrepancies between models and humans are primarily due to lack of embodied experience and relative rating strategies - this is speculative and not directly tested.

## Next Checks

1. **Ablation study on training data**: Systematically test whether spatial reasoning capabilities emerge from specific types of training data by comparing models trained on corpora with varying proportions of spatial language content.

2. **Controlled prompting experiments**: Design experiments that explicitly prompt models to use the full rating scale and provide confidence estimates, then analyze whether this reduces response polarization and improves correlation with human data.

3. **Embodiment simulation tests**: Create experiments where models are provided with simulated spatial experiences (through descriptive text or virtual environments) before spatial reasoning tasks, to test whether this improves performance compared to models without such priming.