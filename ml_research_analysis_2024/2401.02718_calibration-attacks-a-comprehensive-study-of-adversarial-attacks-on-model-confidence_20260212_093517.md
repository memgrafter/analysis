---
ver: rpa2
title: 'Calibration Attacks: A Comprehensive Study of Adversarial Attacks on Model
  Confidence'
arxiv_id: '2401.02718'
source_url: https://arxiv.org/abs/2401.02718
tags:
- attacks
- calibration
- attack
- confidence
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive study on calibration attacks,
  a novel form of adversarial attacks that manipulate model confidence scores without
  altering predicted labels. The authors propose four types of calibration attacks:
  underconfidence, overconfidence, maximum miscalibration, and random confidence attacks,
  conducted in both black-box and white-box settings.'
---

# Calibration Attacks: A Comprehensive Study of Adversarial Attacks on Model Confidence

## Quick Facts
- **arXiv ID**: 2401.02718
- **Source URL**: https://arxiv.org/abs/2401.02718
- **Reference count**: 40
- **Primary result**: Novel adversarial attacks that manipulate model confidence scores without changing predicted labels, achieving 10× increases in ECE and KS scores while maintaining accuracy.

## Executive Summary
This paper introduces calibration attacks, a novel class of adversarial attacks that specifically target model confidence scores while preserving predicted labels. The authors propose four attack types (underconfidence, overconfidence, maximum miscalibration, and random confidence) that can be executed in both black-box and white-box settings. These attacks are highly effective across different model architectures and datasets, significantly increasing calibration errors while maintaining high accuracy. The study reveals that calibration attacks are difficult to detect using existing methods and can transfer across different model architectures, posing a significant threat to model reliability. The authors also evaluate various defense methods, including their proposed Calibration Attack Adversarial Training (CAAT) and Compression Scaling (CS), but find significant limitations in handling these attacks.

## Method Summary
The authors propose four types of calibration attacks that manipulate model confidence scores without altering predicted labels. These attacks are implemented using both black-box (Square Attack) and white-box (PGD) frameworks with l∞ norm bounded perturbations (ϵ=0.05). The attacks target confidence calibration while maintaining label consistency through specific constraints. Models are fine-tuned on ImageNet-pretrained ResNet-50 and Vision Transformer architectures across CIFAR-100, Caltech-101, and GTSRB datasets. The study evaluates attack effectiveness using Expected Calibration Error (ECE), Kolmogorov-Smirnov Calibration Error (KS), accuracy, average confidence, and query efficiency metrics. Various defense methods including temperature scaling, compression scaling, and adversarial training are investigated for their robustness against these attacks.

## Key Results
- Calibration attacks increase ECE and KS scores by over 10 times while maintaining accuracy on all tested datasets and models
- Vision Transformers are more susceptible to calibration attacks than ResNets, requiring more queries for similar effect
- Attacks transfer across different model architectures, demonstrating their effectiveness in black-box scenarios
- Existing calibration detection methods fail to identify calibration attacks effectively
- Proposed defenses (CAAT and CS) show significant limitations in handling calibration attacks

## Why This Works (Mechanism)
Calibration attacks exploit the model's confidence estimation mechanism by introducing small perturbations that significantly alter probability distributions while preserving the maximum likelihood class. The attacks work by manipulating the softmax outputs to either understate or overstate confidence levels without changing the predicted label. This is achieved through careful optimization that maintains the argmax of the output distribution while shifting the probability mass away from or toward the predicted class. The effectiveness stems from the fact that many models, particularly attention-based architectures, have inherent calibration issues that these attacks can exacerbate. The transferability across architectures suggests that the attack targets fundamental aspects of confidence estimation that are common across different model types.

## Foundational Learning
- **Expected Calibration Error (ECE)**: Measures the discrepancy between predicted confidence and actual accuracy; needed to quantify miscalibration; quick check: calculate bin-wise accuracy vs confidence differences
- **Kolmogorov-Smirnov Calibration Error (KS)**: Statistical test measuring maximum difference between cumulative distribution functions; needed for alternative calibration metric; quick check: compute maximum CDF difference between predicted and actual distributions
- **Temperature Scaling**: Post-processing technique to calibrate model outputs; needed for defense evaluation; quick check: validate temperature parameter on held-out calibration set
- **l∞ norm constraints**: Maximum perturbation bound per pixel; needed to ensure imperceptible attacks; quick check: verify all perturbations stay within specified epsilon bound
- **Gradient-based optimization**: Method for finding adversarial perturbations; needed for white-box attack implementation; quick check: confirm gradient flow through model during attack iterations
- **Query-based optimization**: Black-box attack approach using function evaluations; needed for gradient-free attack scenarios; quick check: monitor query count efficiency during attack execution

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Model Fine-tuning -> Attack Generation -> Evaluation Metrics -> Defense Application

**Critical Path**: Image Input → Model Forward Pass → Confidence Extraction → Perturbation Optimization → Label Consistency Check → Attack Success Evaluation

**Design Tradeoffs**: The study balances attack effectiveness against imperceptibility through l∞ norm constraints, choosing between black-box (query-efficient but potentially less effective) and white-box (gradient-based but requires access) attack strategies, and evaluating multiple model architectures to understand attack transferability.

**Failure Signatures**: Attacks failing to maintain label consistency (predicted label changes), defenses not properly reducing calibration error, or metrics not reflecting actual confidence-miscalibration relationship.

**First Experiments**:
1. Implement basic PGD attack with label preservation constraint and verify it maintains predicted labels while increasing ECE
2. Test Square Attack on a single image and visualize confidence distribution changes
3. Apply temperature scaling as defense and measure its effectiveness in reducing attack-induced miscalibration

## Open Questions the Paper Calls Out
- What specific architectural properties of vision transformers (ViTs) make them more susceptible to calibration attacks compared to convolutional neural networks (ResNets)?
- How do calibration attacks perform on models trained with different regularization techniques, such as dropout, weight decay, or batch normalization?
- What is the relationship between a model's inherent calibration error and its susceptibility to calibration attacks?

## Limitations
- Exact dropout values for PGD-based calibration attacks are not fully specified, which could affect attack implementation
- Specific temperature scaling ranges and bin mappings in Compression Scaling (CS) defense are not detailed
- Limited evaluation of detection methods for calibration attacks, despite claiming they are difficult to detect

## Confidence

**High Confidence**: Effectiveness of calibration attacks in increasing ECE and KS scores while maintaining accuracy across multiple datasets and model architectures.

**Medium Confidence**: Transferability of calibration attacks across different model architectures, based on testing only ResNet-50 and ViT.

**Low Confidence**: Robustness of proposed defense methods (CAAT and CS) against calibration attacks, given the paper highlights significant limitations without extensive validation.

## Next Checks
1. Implement a rigorous check to ensure calibration attacks maintain predicted label consistency by verifying the constraint in Equation 5, comparing predicted labels before and after attack.
2. Conduct thorough hyperparameter tuning for temperature scaling (TS) and Compression Scaling (CS) defenses, validating temperature scaling on the validation set and verifying bin ranges and scaling factors in CS.
3. Expand evaluation of attack transferability by testing calibration attacks on additional model architectures beyond ResNet-50 and ViT, such as MobileNet or EfficientNet.