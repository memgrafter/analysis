---
ver: rpa2
title: Growing Efficient Accurate and Robust Neural Networks on the Edge
arxiv_id: '2410.07691'
source_url: https://arxiv.org/abs/2410.07691
tags:
- training
- robust
- growth
- data
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training deep neural networks
  on resource-constrained Edge devices, which is hindered by high computational complexity
  and lack of robustness to common corruptions. The authors propose GEARnn (Growing
  Efficient, Accurate, and Robust neural networks) to grow and train robust networks
  in-situ on Edge devices.
---

# Growing Efficient Accurate and Robust Neural Networks on the Edge

## Quick Facts
- arXiv ID: 2410.07691
- Source URL: https://arxiv.org/abs/2410.07691
- Authors: Vignesh Sundaresha; Naresh Shanbhag
- Reference count: 40
- One-line primary result: GEARnn achieves 2.3×-2.8× reduction in training time and energy while maintaining clean and robust accuracy within 1% of baseline networks on Edge devices.

## Executive Summary
This paper introduces GEARnn, a method for training and growing efficient, accurate, and robust neural networks directly on resource-constrained Edge devices. The key innovation is a two-phase approach that first grows a network using clean data (One-Shot Growth) and then robustifies it using Efficient Robust Augmentation (ERA). This sequential approach achieves higher robustness while significantly reducing training energy consumption compared to traditional robust training methods. The authors demonstrate that GEARnn can achieve comparable clean and robust accuracies to larger baseline networks while requiring only a fraction of the training cost.

## Method Summary
GEARnn addresses the challenge of training robust networks on Edge devices by employing a two-phase approach. First, it uses One-Shot Growth (OSG) to grow a small initial backbone network using clean data until it reaches the device's memory constraints. Then, it robustifies the grown network using Efficient Robust Augmentation (ERA), which modifies the AugMix augmentation strategy to reduce computational overhead while maintaining robustness. The method can be applied in either a one-phase (GEARnn-1) or two-phase (GEARnn-2) configuration, with the two-phase approach showing superior robustness through clean data initialization.

## Key Results
- GEARnn achieves 2.3×-2.8× reduction in training time and energy compared to robustly trained baselines while maintaining comparable clean and robust accuracy
- GEARnn-2 (2-phase) achieves higher robustness than GEARnn-1 (1-phase) at the same number of robust training epochs and training time
- One-Shot Growth (OSG) provides the best training efficiency, clean and robust accuracies at iso-model size compared to multi-shot growth methods
- ERA with (W,D,J)=(1,3,4) provides comparable robustness to AugMix while significantly reducing training cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clean data initialization in Phase-1 provides a better starting point for robust training in Phase-2, leading to faster convergence and higher robustness.
- Mechanism: OSG growth on clean data creates a network that is already well-structured and has learned meaningful features. This clean initialization acts as a good starting point for the robust training phase, allowing the network to reach lower loss values more quickly compared to starting from random initialization or direct robust growth.
- Core assumption: The features learned from clean data are transferable and beneficial for learning robustness to common corruptions.
- Evidence anchors:
  - [abstract] "GEARnn-2 employs the 2-Phase (sequential growth and robust training) approach... achieves higher robustness at the same: (a) number of robust training epochs at final model size"
  - [section] "GEARnn-2 achieves higher robustness at the same: (a) number of robust training epochs at final model size, and (b) training time"
  - [corpus] Weak - no direct corpus evidence for clean data initialization benefits
- Break condition: If the clean data features are not transferable to the corrupted data domain, the initialization benefit would disappear.

### Mechanism 2
- Claim: One-Shot Growth (OSG) is more efficient than multi-step growth methods while maintaining comparable accuracy.
- Mechanism: OSG performs a single growth step sandwiched between two training stages, reducing the overhead associated with multiple growth iterations. The growth neighborhood search and neuron initialization in OSG are designed to efficiently explore the architecture space in one step.
- Core assumption: A single well-designed growth step can achieve similar architecture optimization as multiple smaller steps.
- Evidence anchors:
  - [abstract] "One-Shot Growth (OSG) achieves the best training efficiency, clean and robust accuracies at iso-model size compared to m-Shot (m > 1) Growth"
  - [section] "Table 4 indicates that OSG is comparable or better than the other m-Shot Growth methods in all the metrics"
  - [corpus] Weak - no direct corpus evidence for one-shot growth efficiency
- Break condition: If the growth neighborhood is too large or the single step cannot adequately explore the architecture space, multi-step growth might be necessary.

### Mechanism 3
- Claim: Efficient Robust Augmentation (ERA) with reduced width, depth, and consistency samples provides comparable robustness to standard AugMix while significantly reducing training cost.
- Mechanism: ERA uses (W, D, J) = (1, 3, 4) instead of AugMix's (3, 3, 3), reducing the computational overhead of generating augmented samples while maintaining the essential properties needed for robustness.
- Core assumption: The core robustness benefits come from the stochastic transform chains rather than the specific width, depth, or number of consistency samples.
- Evidence anchors:
  - [section] "We choose (W, D, J) = (1, 3, 4) based on our diagnosis (shown in Appendix B.1) to improve the efficiency without compromising on robustness"
  - [section] "Table 8 shows different modifications of the stochastic chains obtained by varying (W, D, J) values... ERA, we pick the modification with (W, D, J) = (1, 3, 4)"
  - [corpus] Weak - no direct corpus evidence for ERA parameter choices
- Break condition: If the reduced augmentation parameters fail to cover the corruption spectrum adequately, robustness would degrade.

## Foundational Learning

- Concept: Neural network growth algorithms (expanding width/depth during training)
  - Why needed here: Understanding how networks can be efficiently grown rather than trained from scratch is crucial for the OSG component of GEARnn
  - Quick check question: What are the trade-offs between growing width versus depth in neural networks?

- Concept: Data augmentation techniques for robustness
  - Why needed here: ERA is built upon robust augmentation principles, and understanding these techniques is essential for modifying and optimizing them
  - Quick check question: How do stochastic transform chains in AugMix contribute to robustness against common corruptions?

- Concept: Edge computing constraints (memory, energy, latency)
  - Why needed here: The entire motivation for GEARnn is to address the computational limitations of Edge devices
  - Quick check question: What are the typical memory and energy constraints when training on a Jetson Xavier NX compared to a GPU server?

## Architecture Onboarding

- Component map: Small backbone network -> OSG growth using clean data -> Post-growth training -> ERA augmentation generation -> Robust training -> Final network

- Critical path: 1. Initialize small backbone network 2. Perform OSG using clean data (backbone training → growth → post-growth training) 3. Generate augmented data using ERA 4. Perform robust training on augmented data 5. Measure and report accuracy, robustness, and efficiency metrics

- Design tradeoffs:
  - Growth ratio vs. model size: Higher growth ratio creates larger models but may improve accuracy
  - ERA parameters (W,D,J) vs. robustness: Lower values reduce cost but may impact robustness
  - Training epochs vs. energy consumption: More epochs improve accuracy but increase energy cost

- Failure signatures:
  - Memory overflow during growth step on Edge device
  - Degraded robustness when using ERA instead of full AugMix
  - Convergence issues if clean data initialization is poor

- First 3 experiments:
  1. Implement OSG with clean data on a small CNN (e.g., VGG-19) on GPU to verify growth functionality
  2. Add ERA module and compare robustness against standard AugMix on the same architecture
  3. Port the complete GEARnn-2 pipeline to Jetson Xavier NX and measure energy consumption against baseline

## Open Questions the Paper Calls Out

- What is the theoretical basis for why 2-Phase approach with clean data initialization leads to faster convergence in robust training compared to 1-Phase approach?
- How would GEARnn perform on more complex tasks like object detection or semantic segmentation on resource-constrained Edge devices?
- What is the optimal trade-off between the number of growth epochs (Eg) and the growth ratio (γ) for achieving the best balance between accuracy, robustness, and training efficiency?
- How does the performance of GEARnn compare to other network architecture search methods when considering robustness to common corruptions on Edge devices?

## Limitations

- The OSG and ERA components lack detailed implementation specifications, making exact reproduction challenging
- Evaluation focuses primarily on CIFAR-scale datasets with VGG-19, limiting insights into performance on larger, more complex datasets or architectures
- Robustness evaluation uses synthetic corruption benchmarks rather than real-world noisy data that Edge devices typically encounter

## Confidence

- High Confidence: The overall training efficiency claims (2.3×-2.8× reduction in training time and energy) are well-supported by controlled experiments comparing GEARnn against baselines under identical hardware constraints
- Medium Confidence: The robustness benefits of the 2-Phase approach (GEARnn-2) over 1-Phase (GEARnn-1) are demonstrated on benchmark datasets, though the mechanism linking clean initialization to improved robustness, while plausible, lacks direct empirical validation
- Low Confidence: The claim that OSG is superior to multi-step growth methods is based on limited comparisons with specific variants (m-Shot Growth). The paper doesn't explore alternative growth strategies or provide ablation studies on growth parameters

## Next Checks

1. Reproduce OSG Growth Behavior: Implement the One-Shot Growth algorithm on a standard CNN architecture (e.g., ResNet-18) and verify that single-step growth achieves comparable accuracy to multi-step approaches while reducing training iterations

2. Validate ERA Robustness Transfer: Test ERA's effectiveness beyond CIFAR datasets by applying it to a larger dataset (e.g., ImageNet-32) and comparing robustness against common corruptions with the original AugMix baseline

3. Measure Edge Device Scalability: Deploy GEARnn on a range of Edge devices with varying memory constraints (e.g., Jetson Nano, Raspberry Pi 4) to evaluate how the growth ratio and ERA parameters need to be adjusted for different hardware profiles