---
ver: rpa2
title: 'OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding'
arxiv_id: '2402.15321'
source_url: https://arxiv.org/abs/2402.15321
tags:
- challenge
- scene
- masks
- open-vocabulary
- workshop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reports on the first OpenSUN3D workshop challenge focused
  on open-vocabulary 3D scene understanding, which aims to move beyond closed-set
  object recognition to arbitrary text-based queries about 3D scenes. The challenge
  involved segmenting object instances in 3D point clouds based on open-vocabulary
  text queries describing object properties, semantics, materials, and affordances.
---

# OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding

## Quick Facts
- arXiv ID: 2402.15321
- Source URL: https://arxiv.org/abs/2402.15321
- Authors: Francis Engelmann; Ayca Takmaz; Jonas Schult; Elisabetta Fedele; Johanna Wald; Songyou Peng; Xi Wang; Or Litany; Siyu Tang; Federico Tombari; Marc Pollefeys; Leonidas Guibas; Hongbo Tian; Chunjie Wang; Xiaosheng Yan; Bingwen Wang; Xuanyang Zhang; Xiao Liu; Phuc Nguyen; Khoi Nguyen; Anh Tran; Cuong Pham; Zhening Huang; Xiaoyang Wu; Xi Chen; Hengshuang Zhao; Lei Zhu; Joan Lasenby
- Reference count: 22
- Primary result: First quantitative benchmark for open-vocabulary 3D instance segmentation with mAP scores ranging from 2.67 to 6.08

## Executive Summary
The OpenSUN3D workshop challenge addressed the emerging field of open-vocabulary 3D scene understanding, moving beyond traditional closed-set object recognition to arbitrary text-based queries about 3D scenes. Using the ARKitScenes dataset containing RGB-D sequences and 3D mesh reconstructions from iPad-captured indoor scenes, the challenge tasked participants with segmenting object instances based on open-vocabulary text queries describing object properties, semantics, materials, and affordances. Three winning teams achieved mAP scores of 6.08, 4.13, and 2.67 respectively, demonstrating promising but still limited performance that highlights remaining challenges in this domain.

## Method Summary
The winning methods combined 2D image segmentation approaches like Grounding SAM and Grounding Dino with 3D reconstruction techniques, using CLIP for text-image alignment and various filtering and merging strategies to produce 3D instance masks. The pipeline involved detecting objects in 2D images, projecting these detections into 3D space using camera parameters and depth maps, then refining the masks through multi-view voting and 3D-specific segmentation methods. Teams implemented different strategies for handling false positives, including image-level non-maximum suppression and CLIP-based confidence filtering, with the best results achieved by combining these techniques effectively.

## Key Results
- PICO-MR team achieved highest mAP of 6.08 using Grounding SAM with image-level NMS and Bidirectional Merging
- CRP team achieved second-highest mAP of 4.13 using Grounding Dino with CLIP-based confidence filtering
- KAIST team achieved third-highest mAP of 2.67 using SAM3D with 2D-to-3D conversion
- All methods showed significant performance drop at higher IoU thresholds (AP50 >> AP25 >> mAP)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP-based alignment enables cross-modal object recognition in 3D scenes
- Mechanism: The winning methods use CLIP to embed text queries and 2D/3D visual features into a shared embedding space, allowing cosine similarity matching to identify relevant objects regardless of whether they were seen during training
- Core assumption: CLIP embeddings capture sufficient semantic similarity between natural language descriptions and visual features to enable reliable object retrieval
- Evidence anchors: [abstract] "Recently, large visual-language models (VLMs), such as CLIP [17], have demonstrated impressive generalization capabilities trained on internet-scale image-language pairs"
- Break condition: CLIP embeddings fail to capture relevant semantic relationships for domain-specific objects or when text queries use highly abstract or context-dependent descriptions

### Mechanism 2
- Claim: 2D-to-3D projection with multi-view voting improves 3D instance localization accuracy
- Mechanism: Methods first detect objects in 2D images using models like Grounding SAM, then project these detections into 3D space using camera parameters and depth maps, with voting across multiple views to refine the final 3D masks
- Core assumption: 2D detection models generalize well to the specific objects and contexts in ARKitScenes despite being trained on different datasets
- Evidence anchors: [section] "We leverage Grounding Dino [14] and SAM [12] to generate class-agnostic 2D segmentation masks...SAM3D [20] is subsequently applied to the generated 2D instance masks to produce elevated 3D segmentation masks"
- Break condition: Projection errors accumulate due to camera calibration inaccuracies or when objects lack sufficient depth information for reliable 3D reconstruction

### Mechanism 3
- Claim: Image-level non-maximum suppression (NMS) reduces false positives in open-vocabulary detection
- Mechanism: The winning PICO-MR team implements an iterative image-level NMS where paired images are concatenated and re-evaluated with Grounding SAM to suppress redundant or incorrect detections
- Core assumption: Objects that appear similar in isolation can be distinguished through contextual comparison with other images in the sequence
- Evidence anchors: [section] "To address this issue, we designed an image-level non-maximum suppression (NMS) method to suppress excessive false positives...We keep the image parts with detected masks and filter the image parts without masks"
- Break condition: The iterative NMS process removes true positive detections or fails when objects have consistent appearances across the image sequence

## Foundational Learning

- Concept: 3D reconstruction from RGB-D sequences
  - Why needed here: The challenge uses ARKitScenes which provides RGB-D frames and corresponding 3D mesh reconstructions, requiring understanding of how depth maps and camera poses combine to create 3D geometry
  - Quick check question: How do you convert a 2D pixel coordinate with depth value into a 3D point in world coordinates using camera intrinsics and extrinsics?

- Concept: Open-vocabulary versus closed-set classification
  - Why needed here: Unlike traditional 3D scene understanding that recognizes only pre-defined object classes, this challenge requires recognizing arbitrary objects described by natural language
  - Quick check question: What is the fundamental difference between training a classifier on a fixed set of object classes versus using a language model to match arbitrary text descriptions to visual features?

- Concept: Instance segmentation evaluation metrics
  - Why needed here: The challenge uses mAP, AP50, and AP25 metrics which require understanding of intersection-over-union (IoU) thresholds and how they evaluate segmentation quality
  - Quick check question: If a predicted mask has IoU of 0.6 with ground truth, does it count as a true positive for AP50, AP25, or both?

## Architecture Onboarding

- Component map: RGB-D camera pipeline -> 2D object detector (Grounding SAM/Dino) -> CLIP embedding & ranking -> 2D-to-3D projection -> Multi-view voting -> 3D instance refinement (SAM3D) -> Output masks with confidence scores
- Critical path: Text query -> CLIP embedding -> 2D detection -> 3D projection -> Mask refinement -> Evaluation
- Design tradeoffs: Using 2D detectors provides better performance than 3D-only methods but introduces projection errors; CLIP enables open-vocabulary but may not capture fine-grained 3D-specific semantics
- Failure signatures: Low mAP scores indicate either poor 2D detection recall, inaccurate 3D projections, or CLIP failing to align text with relevant visual features; high AP50 but low AP25 suggests coarse segmentation masks
- First 3 experiments:
  1. Run baseline 2D detector (Grounding SAM) on sample ARKitScenes images and visualize detection quality and false positive rate
  2. Implement simple 2D-to-3D projection using provided camera parameters and evaluate projection accuracy on objects with ground truth 3D positions
  3. Test CLIP embedding similarity between text queries and corresponding 2D cropped images to verify semantic alignment quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of open-vocabulary 3D scene understanding be improved beyond the current mAP scores of 6.08, 4.13, and 2.67 achieved by the top three teams?
- Basis in paper: [explicit] The paper reports that the absolute scores are still low for all submitted methods, highlighting the remaining challenges of open-vocabulary 3D scene understanding.
- Why unresolved: The paper does not provide specific insights into the factors limiting performance or potential solutions to improve it.
- What evidence would resolve it: Experiments comparing different approaches, datasets, or model architectures that demonstrate significant improvements in mAP scores for open-vocabulary 3D scene understanding tasks.

### Open Question 2
- Question: How can the issue of false positives in 2D object detection models be effectively addressed when dealing with partial views of indoor scenes in 3D reconstruction?
- Basis in paper: [inferred] The PICO-MR team's method description mentions that many different objects look similar without context, causing the open-set detection model to yield more false positives.
- Why unresolved: The paper does not provide a comprehensive analysis of the factors contributing to false positives or a detailed comparison of different approaches to mitigate them.
- What evidence would resolve it: Comparative studies of various techniques, such as image-level non-maximum suppression, context-aware filtering, or multi-view consistency checks, that demonstrate their effectiveness in reducing false positives in 2D object detection for 3D scene understanding.

### Open Question 3
- Question: What are the limitations and potential improvements for SAM3D when applied to the ARKitScenes dataset for 3D instance segmentation?
- Basis in paper: [explicit] The CRP team's method description mentions that SAM3D did not perform well on the ARKitScenes dataset, leading to low-quality masks in the final output.
- Why unresolved: The paper does not provide a detailed analysis of the specific issues with SAM3D on the ARKitScenes dataset or suggestions for improving its performance.
- What evidence would resolve it: Experiments comparing SAM3D with alternative 3D instance segmentation methods on the ARKitScenes dataset, along with ablation studies to identify the key factors affecting SAM3D's performance and potential modifications to address them.

## Limitations

- Performance remains significantly limited with mAP scores below 7, indicating fundamental challenges in open-vocabulary 3D understanding
- Methods rely heavily on 2D detection models trained on different datasets, potentially limiting generalization to ARKitScenes domain
- CLIP-based semantic matching may struggle with highly abstract or context-dependent queries requiring deeper semantic understanding

## Confidence

- **High confidence**: The basic methodology of combining 2D segmentation with 3D reconstruction and CLIP-based text-image alignment is technically sound and has been validated by multiple teams achieving similar results
- **Medium confidence**: The specific implementation details of the winning methods, particularly the image-level NMS and 3D merging strategies, may vary in effectiveness depending on the specific dataset characteristics and parameter choices
- **Low confidence**: The generalizability of these approaches to different 3D datasets beyond ARKitScenes and their performance on more complex queries involving abstract concepts or rare objects

## Next Checks

1. **Cross-dataset validation**: Test the winning methods on a different 3D dataset (such as ScanNet or Matterport3D) to evaluate generalization beyond ARKitScenes and identify dataset-specific limitations

2. **Query complexity analysis**: Systematically evaluate performance across different query types (material properties, affordances, situational context) to identify which aspects of open-vocabulary understanding remain challenging and require targeted improvements

3. **2D-to-3D accuracy benchmarking**: Conduct controlled experiments measuring the accuracy of 2D-to-3D projection under varying camera calibration quality and object depth ranges to quantify the contribution of projection errors to overall performance limitations