---
ver: rpa2
title: Boosting Code-Switching ASR with Mixture of Experts Enhanced Speech-Conditioned
  LLM
arxiv_id: '2409.15905'
source_url: https://arxiv.org/abs/2409.15905
tags:
- speech
- experts
- recognition
- code-switching
- connector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a speech-conditioned large language model
  (SC-LLM) with a mixture of experts (MoE) connector to tackle Mandarin-English code-switching
  ASR. The core idea is to use an MoE-based connector with language-specialized experts
  and a two-stage training strategy: first, mapping speech to text with language-specific
  experts, then learning generalized representations and applying an IDIT mechanism
  to constrain predicted tokens at word/character level.'
---

# Boosting Code-Switching ASR with Mixture of Experts Enhanced Speech-Conditioned LLM

## Quick Facts
- arXiv ID: 2409.15905
- Source URL: https://arxiv.org/abs/2409.15905
- Reference count: 28
- Key outcome: Achieves MER of 7.76% on ASRU-2019 Mandarin-English code-switching dataset, outperforming previous models by over 10% relative improvement

## Executive Summary
This paper proposes a speech-conditioned large language model (SC-LLM) architecture enhanced with a mixture of experts (MoE) connector to tackle the challenges of Mandarin-English code-switching automatic speech recognition (ASR). The core innovation lies in using language-specialized experts for Mandarin and English within an MoE framework, combined with a two-stage progressive training strategy and an IDIT mechanism for optimal tokenization. The approach significantly improves code-switching ASR performance, achieving state-of-the-art results on the ASRU-2019 dataset.

## Method Summary
The method employs a Whisper-large V3 speech encoder connected to an MoE-based connector that routes frames to language-specific experts (Mandarin, English, or blank). A two-stage training strategy is used: first training the connector with language-specialized experts to establish basic alignment, then activating all experts for general representations while applying the IDIT mechanism and LoRA fine-tuning. The IDIT mechanism tokenizes Chinese at character level and English at word level using interruption tokens to improve token prediction accuracy. The architecture uses Qwen2-7B as the LLM backbone.

## Key Results
- Achieves MER of 7.76% on ASRU-2019 Mandarin-English code-switching dataset
- Outperforms previous end-to-end models (Conformer) and large-scale audio-language models by over 10% relative improvement
- Demonstrates significant improvements over baseline models including Whisper-large V3, Qwen2-Audio, and SenseVoice-small

## Why This Works (Mechanism)

### Mechanism 1
MoE-based connector routes frames to language-specific experts (Mandarin, English, or blank) improving code-switching recognition. Speech frames are processed by a router that outputs probability distributions over experts, with each frame routed to the expert with highest probability. The core assumption is that phonetic and linguistic features of Mandarin vs English can be effectively separated by routing decisions. Weak evidence from neighboring papers on MoE for CS-ASR, but no direct validation of this specific routing mechanism.

### Mechanism 2
IDIT mechanism constrains tokenization to word-level for English and character-level for Chinese, reducing insertion/deletion errors. Special interruption token is inserted between words/characters before tokenization, forcing tokenizer to split at these boundaries, then the interruption token is removed. The core assumption is that tokenization granularity should match speech recognition unit boundaries. No direct evidence in neighboring papers about IDIT mechanism specifically.

### Mechanism 3
Two-stage progressive training strategy (LSE initialization then IDIT + LoRA fine-tuning) improves model performance. Stage 1 trains connector with language-specialized experts without IDIT to establish basic alignment. Stage 2 activates all experts for general representations and applies IDIT with LoRA fine-tuning. The core assumption is that gradual progression from specialized to generalized learning allows better expert collaboration. Weak evidence from neighboring papers discussing MoE training but not this specific two-stage approach.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Enables efficient scaling by routing inputs to specialized experts rather than activating all parameters
  - Quick check question: How does MoE differ from traditional dense layers in terms of parameter efficiency?

- Concept: Speech tokenization and subword units
  - Why needed here: Different tokenization strategies (word-level vs character-level) impact recognition accuracy for different languages
  - Quick check question: Why might character-level tokenization work better for Chinese while word-level works better for English?

- Concept: LoRA (Low-Rank Adaptation) fine-tuning
  - Why needed here: Allows efficient adaptation of large LLMs to speech recognition task without full fine-tuning
  - Quick check question: What is the computational advantage of LoRA compared to full fine-tuning of LLM parameters?

## Architecture Onboarding

- Component map: Speech encoder (Whisper-large V3) → MoE connector (router + experts) → LLM (Qwen2-7B) with LoRA adapter → Output
- Critical path: Audio → Encoder → Connector → LLM → Text prediction
- Design tradeoffs: MoE provides efficiency but adds routing complexity; IDIT adds preprocessing overhead but improves token prediction accuracy
- Failure signatures: High MER indicates routing errors or tokenization issues; CER/WER imbalance suggests language-specific problems
- First 3 experiments:
  1. Replace MoE connector with simple linear layer to establish baseline
  2. Test IDIT mechanism alone by disabling MoE to isolate its impact
  3. Vary number of experts in MoE to find optimal balance between performance and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the SC-LLM with MoE connector scale with larger speech encoder and LLM models? The paper only tested with specific model sizes (Whisper-large V3 and Qwen2-7B) and did not investigate how performance changes with larger architectures.

### Open Question 2
How robust is the proposed method to different code-switching patterns and language pair combinations? The evaluation is limited to one specific language pair (Mandarin-English) and dataset, leaving generalizability to other CS scenarios unexplored.

### Open Question 3
What is the computational efficiency trade-off between the MoE connector and traditional linear connectors during inference? While parameter count is addressed, inference latency, memory usage, and throughput comparisons are not provided.

## Limitations

- The effectiveness of the MoE routing mechanism for code-switching scenarios lacks strong external validation, with neighboring literature focusing more on hierarchical routing than this specific approach
- The IDIT mechanism, while theoretically sound, lacks direct validation in the neighboring corpus and represents a significant architectural risk
- The two-stage training strategy's effectiveness is based primarily on the authors' ablation study rather than independent validation

## Confidence

**High Confidence Claims:**
- The MER of 7.76% on ASRU-2019 dataset is achieved and represents state-of-the-art performance
- The overall architecture (Whisper encoder + MoE connector + Qwen2 LLM + LoRA) is correctly implemented
- Code-switching ASR performance improvements are measurable and significant

**Medium Confidence Claims:**
- The MoE routing mechanism specifically improves language separation in code-switching
- The IDIT mechanism effectively handles word-level vs character-level tokenization differences
- The two-stage training strategy provides meaningful benefit over single-stage approaches

**Low Confidence Claims:**
- The exact contribution of each mechanism component to overall performance
- Whether the architecture would generalize to other language pairs beyond Mandarin-English
- The robustness of the approach to different speech quality conditions

## Next Checks

1. **Ablation of MoE routing effectiveness**: Replace the MoE connector with a simple linear layer while keeping all other components (including IDIT) identical. Compare MER, CER, and WER to determine the specific contribution of the routing mechanism versus other architectural choices.

2. **IDIT mechanism isolation test**: Train a variant without MoE but with IDIT mechanism active, and another variant with MoE but without IDIT. This would isolate whether the tokenization improvements are primarily due to MoE routing or the IDIT mechanism itself.

3. **Expert routing analysis**: During inference, record and analyze the router's probability distributions over experts. Compute the percentage of frames routed to each expert and examine whether routing decisions correlate with actual language boundaries in the audio. This would validate whether the MoE is actually learning meaningful language separation.