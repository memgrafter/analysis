---
ver: rpa2
title: 'Positive-Sum Fairness: Leveraging Demographic Attributes to Achieve Fair AI
  Outcomes Without Sacrificing Group Gains'
arxiv_id: '2409.19940'
source_url: https://arxiv.org/abs/2409.19940
tags:
- fairness
- performance
- group
- subgroups
- positive-sum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "positive-sum fairness," a novel fairness
  framework for medical AI that allows performance disparities between demographic
  subgroups when overall performance improves without harming any subgroup. The authors
  argue that traditional group fairness metrics, which penalize performance differences
  between subgroups, may be inappropriate for medical AI where each improvement can
  save lives.
---

# Positive-Sum Fairness: Leveraging Demographic Attributes to Achieve Fair AI Outcomes Without Sacrificing Group Gains

## Quick Facts
- arXiv ID: 2409.19940
- Source URL: https://arxiv.org/abs/2409.19940
- Reference count: 40
- Primary result: Positive-sum fairness allows medical AI performance disparities when all subgroups benefit from overall improvements

## Executive Summary
This paper introduces "positive-sum fairness," a novel fairness framework for medical AI that allows performance disparities between demographic subgroups when overall performance improves without harming any subgroup. The authors argue that traditional group fairness metrics, which penalize performance differences between subgroups, may be inappropriate for medical AI where each improvement can save lives. They evaluate four CNN models for chest X-ray analysis using MIMIC-CXR-JPG data: M1 (baseline), M2 (uses race metadata), M3 (predicts race from image features), and M4 (removes race information using gradient reversal). Results show M2 improves overall performance while widening disparities, which is considered acceptable under positive-sum fairness since all subgroups benefit. M4 improves fairness metrics but harms at least one subgroup's performance. The study demonstrates that positive-sum fairness provides a more nuanced approach to evaluating medical AI fairness by distinguishing between harmful and non-harmful performance disparities.

## Method Summary
The study evaluates four DenseNet-121 CNN models for chest X-ray analysis on 14 findings (lung lesions, pneumonia, pleural effusion, consolidation) using MIMIC-CXR-JPG data (237,972 training, 1,959 validation, 3,403 test images). Models include: M1 baseline, M2 using race metadata, M3 predicting race from image features, and M4 removing race with gradient reversal. All models are trained with AdamW optimizer (learning rate 0.002), cosine annealing warm-up scheduler, and evaluated using AUROC scores with bootstrapping (300 samples). The positive-sum fairness framework compares overall performance and subgroup performance changes against a baseline, allowing disparities when all subgroups improve.

## Key Results
- M2 (race-aware) achieves higher overall performance with increased disparities but improves all subgroup performance
- M4 (race-removal) improves fairness metrics but harms at least one subgroup's performance
- Positive-sum fairness successfully distinguishes between beneficial disparities (all subgroups improve) and harmful disparities (some subgroups lose performance)
- Traditional group fairness metrics would penalize M2's beneficial disparities while rewarding M4's harmful improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Positive-sum fairness allows performance disparities between subgroups when overall performance improves without harming any subgroup
- Mechanism: The framework evaluates fairness by comparing performance changes against a baseline, ensuring no subgroup experiences performance loss while allowing disparities that result from collective benefit
- Core assumption: Medical AI improvements can be measured meaningfully against a baseline where no subgroup's performance decreases
- Evidence anchors:
  - [abstract] "positive-sum fairness, which states that an increase in performance that results in a larger group disparity is acceptable as long as it does not come at the cost of individual subgroup performance"
  - [section] "inequitable performance can be acceptable as long as it does not come at the expense of other subgroups and allows a higher overall performance to be achieved"
- Break condition: If the baseline model already has optimal performance for some subgroups, preventing any further improvement for those groups while others improve

### Mechanism 2
- Claim: Sensitive attributes correlated with disease can be leveraged to improve overall performance without compromising fairness
- Mechanism: Models M2 and M3 explicitly use race metadata to enhance prediction accuracy, while M4 uses gradient reversal to minimize demographic shortcuts
- Core assumption: Race information contains clinically relevant features that can improve disease prediction when used appropriately
- Evidence anchors:
  - [abstract] "This allows sensitive attributes correlated with the disease to be used to increase performance without compromising on fairness"
  - [section] "we compare three models which use sensitive attributes to a baseline model. The way sensitive attributes are used by the model is known to have an impact on the fairness and performance of the model"
- Break condition: If race correlates with disease purely due to historical bias rather than clinical relevance, or if using race information introduces new forms of discrimination

### Mechanism 3
- Claim: Traditional group fairness metrics can be misleading in medical AI by penalizing beneficial performance improvements
- Mechanism: The framework distinguishes between harmful disparities (where some subgroups lose performance) and non-harmful disparities (where all subgroups improve but at different rates)
- Core assumption: Fairness metrics should consider the direction and magnitude of performance changes, not just final disparity values
- Evidence anchors:
  - [section] "we could not find any fairness evaluation framework or definition which allows to compare different models from the prism of harmful and non harmful disparities"
  - [section] "Unlike other fairness definitions which aim to minimize the disparity between subgroups or maximize the worst performance among subgroups, positive-sum fairness tries to avoid gains to a group which come at the expense of another group while maintaining the overall performance"
- Break condition: If performance improvements are too small to justify increased disparities, or if disparities grow so large that they create practical inequities in healthcare delivery

## Foundational Learning

- Concept: Understanding of group fairness metrics in machine learning
  - Why needed here: The paper directly contrasts positive-sum fairness with traditional group fairness approaches
  - Quick check question: What is the key difference between demographic parity and equal opportunity in fairness metrics?

- Concept: Medical imaging domain knowledge
  - Why needed here: The study uses chest X-ray analysis and references disease correlations with demographic factors
  - Quick check question: Why might race be a relevant feature in medical imaging analysis, and what are the ethical considerations?

- Concept: CNN model architectures and training techniques
  - Why needed here: The paper evaluates four different CNN models with specific architectural modifications for fairness
  - Quick check question: How does gradient reversal work to remove sensitive attribute information from model predictions?

## Architecture Onboarding

- Component map:
  - Baseline model (M1): DenseNet-121 backbone with finding branch
  - Race-aware model (M2): Adds race metadata as input feature
  - Race-exploiting model (M3): Predicts race from image features alongside findings
  - Race-removal model (M4): Uses gradient reversal to minimize race information

- Critical path: Model training → Performance evaluation → Fairness assessment using both traditional and positive-sum frameworks

- Design tradeoffs: Using sensitive attributes improves performance but may increase disparities; removing attributes reduces disparities but may harm overall performance

- Failure signatures: Performance improvements that come at the expense of specific subgroups, or fairness improvements that significantly reduce overall accuracy

- First 3 experiments:
  1. Train baseline M1 and evaluate performance across demographic subgroups
  2. Implement M2 with race metadata and compare performance/fairness trade-offs
  3. Implement M4 with gradient reversal and analyze whether it achieves better fairness without harming subgroups

## Open Questions the Paper Calls Out

- How can positive-sum fairness be adapted to handle multiple sensitive attributes simultaneously (e.g., race, sex, age) without prioritizing one over others?
- How can we develop a robust baseline for positive-sum fairness that doesn't require comparison to existing models?
- How can positive-sum fairness be evaluated for out-of-distribution performance to ensure it doesn't exacerbate disparities in different healthcare settings?

## Limitations

- Evaluation relies entirely on retrospective analysis of chest X-ray data with limited demographic diversity
- Framework has not been validated on different medical imaging modalities or clinical tasks
- Assumes performance improvements can be meaningfully measured against a baseline
- Ethical implications of using race as a predictive feature require careful consideration

## Confidence

- High confidence: The mechanism of positive-sum fairness as a conceptual framework and its distinction from traditional group fairness metrics
- Medium confidence: The empirical results showing M2's performance improvements across all subgroups while M4 harms at least one subgroup's performance
- Medium confidence: The interpretation that M2's increased disparities are acceptable under positive-sum fairness while M4's fairness improvements come at a performance cost

## Next Checks

1. Validate the framework on a different medical imaging task (e.g., dermatological imaging) to test generalizability across disease types with varying demographic correlations
2. Conduct a prospective clinical study to assess whether positive-sum fairness predictions hold when models are deployed in real healthcare settings with diverse patient populations
3. Perform sensitivity analysis by testing different baseline models and threshold definitions to ensure the framework's conclusions are robust to baseline selection