---
ver: rpa2
title: Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry with
  GPT-4-Turbo
arxiv_id: '2405.02128'
source_url: https://arxiv.org/abs/2405.02128
tags:
- dataset
- multi-hop
- question
- generated
- single-hop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces 'RetChemQA,' a benchmark dataset for evaluating
  machine learning models in reticular chemistry, containing ~90,000 Q&A pairs (45,000
  single-hop and 45,000 multi-hop). Generated using GPT-4-Turbo from ~2,530 research
  papers, the dataset includes factual, reasoning, and true/false questions categorized
  by difficulty.
---

# Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry with GPT-4-Turbo

## Quick Facts
- arXiv ID: 2405.02128
- Source URL: https://arxiv.org/abs/2405.02128
- Reference count: 0
- Introduces RetChemQA benchmark dataset with ~90,000 Q&A pairs for reticular chemistry

## Executive Summary
This study introduces 'RetChemQA,' a comprehensive benchmark dataset for evaluating machine learning models in reticular chemistry. Generated using GPT-4-Turbo from ~2,530 research papers, the dataset contains ~90,000 Q&A pairs (45,000 single-hop and 45,000 multi-hop) covering factual, reasoning, and true/false questions categorized by difficulty. The multi-hop dataset achieves 98.3% accuracy and 93.4% precision, with an 84% hallucination capture rate, while the single-hop dataset achieves 94.8% accuracy and 94.3% precision, with a 22% hallucination capture rate. Additionally, a synthesis conditions dataset is released, with an 'Obedience' score of ~70% for correct extraction without experimental characterization data.

## Method Summary
The researchers collected ~2,530 research papers from publishers including NAS, ACS, RSC, Elsevier, and Nature Publishing Group, focusing on reticular chemistry. They used OpenAI's GPT-4-Turbo (gpt-4-0125-preview) with specific prompts to generate Q&A pairs from both main manuscripts and supplementary information. The dataset was categorized into factual, reasoning, and true/false questions, and evaluated using accuracy, precision, hallucination rate, and hallucination capture rate metrics. A synthesis conditions dataset was also extracted using similar methods, though with lower performance metrics.

## Key Results
- Multi-hop dataset achieves 98.3% accuracy and 93.4% precision with 84% hallucination capture rate
- Single-hop dataset achieves 94.8% accuracy and 94.3% precision with 22% hallucination capture rate
- Synthesis conditions dataset achieves ~70% 'Obedience' score for correct extraction without experimental characterization data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM can generate accurate single-hop Q&A pairs when the answer is directly extractable from a single contiguous piece of text.
- Mechanism: For single-hop questions, the model leverages its language understanding to retrieve and synthesize facts from a single sentence or paragraph, ensuring the answer is factually grounded in the provided context.
- Core assumption: The source document contains a clear, unambiguous statement that directly answers the question.
- Evidence anchors:
  - [abstract] "In the example shown above, the answer generated is from a single contiguous piece of text taken from the MS..."
  - [section] "A single-hop Q&A is defined as one that requires only a single step of reasoning to answer; often this involves retrieving information from a single sentence of a given paper."
- Break condition: The source text does not contain a direct, unambiguous answer, leading the model to generate hallucinated or incomplete responses.

### Mechanism 2
- Claim: The LLM can generate multi-hop Q&A pairs by integrating information from multiple parts of a document, including both the main text and supplementary information.
- Mechanism: For multi-hop questions, the model synthesizes answers by combining information spread across different sections or documents, enabling it to handle more complex reasoning tasks.
- Core assumption: The required information is present across multiple parts of the document and can be logically connected to form a coherent answer.
- Evidence anchors:
  - [abstract] "The multi-hop dataset achieves 98.3% accuracy and 93.4% precision..."
  - [section] "A multi-hop Q&A is defined as one that requires multiple steps of reasoning to answer; often this involves retrieving information from multiple different parts of a MS."
- Break condition: The model fails to correctly integrate information from multiple sources, resulting in incomplete or incorrect answers.

### Mechanism 3
- Claim: The LLM can identify and correct its own hallucinated questions, particularly in the multi-hop dataset, leading to high hallucination capture rates.
- Mechanism: When the model generates a question not supported by the provided context, it can recognize this error and provide an appropriate response, such as stating it cannot answer from the given information.
- Core assumption: The model has sufficient self-awareness to detect when a question is not grounded in the provided context and can respond appropriately.
- Evidence anchors:
  - [abstract] "Interestingly, we find that when the task at hand is more complex for example, the task of generating a multi-hop Q&A pair, the LLM although hallucinates more is also more ‘careful’ in then evaluating the answers it generates..."
  - [section] "Hallucination Capture Rate: This is a measure of the LLM's ability to identify and correct a hallucinated (out-of-context) question it has generated itself."
- Break condition: The model fails to recognize its own hallucinations, leading to incorrect answers or incomplete responses.

## Foundational Learning

- Concept: Language understanding and generation in large language models.
  - Why needed here: The core functionality of the LLM in generating and evaluating Q&A pairs relies on its ability to understand and generate human-like text.
  - Quick check question: Can the LLM accurately summarize a given paragraph in its own words?

- Concept: Multi-hop reasoning and information integration.
  - Why needed here: Generating multi-hop Q&A pairs requires the ability to connect information from multiple sources and synthesize a coherent answer.
  - Quick check question: Can the LLM answer a question that requires combining information from two separate paragraphs?

- Concept: Self-awareness and error correction in language models.
  - Why needed here: The ability to identify and correct hallucinations is crucial for maintaining the accuracy and reliability of the generated Q&A pairs.
  - Quick check question: When given a question not supported by the provided context, can the LLM recognize this and respond appropriately?

## Architecture Onboarding

- Component map: Document preprocessing -> Prompt engineering -> GPT-4-Turbo inference -> Q&A pair evaluation -> Dataset compilation
- Critical path: Preprocess documents to extract relevant text -> Generate Q&A pairs using GPT-4-Turbo with appropriate prompts -> Evaluate the generated pairs using defined metrics -> Compile the final dataset for release
- Design tradeoffs:
  - Prompt complexity vs. model performance: More detailed prompts may lead to better results but require more careful design and testing
  - Single-hop vs. multi-hop: Multi-hop questions are more complex and may have lower precision but provide a more comprehensive assessment of the model's reasoning capabilities
  - Dataset size vs. quality: Larger datasets may provide more coverage but could include more noise or hallucinations
- Failure signatures:
  - Low accuracy or precision scores indicate issues with the model's ability to generate correct answers
  - High hallucination rates suggest the model is generating questions or answers not supported by the provided context
  - Incomplete or incorrect synthesis conditions indicate issues with the model's ability to extract relevant information from the source documents
- First 3 experiments:
  1. Generate a small set of single-hop Q&A pairs from a single document and manually evaluate their quality
  2. Generate a small set of multi-hop Q&A pairs from a single document and compare their performance to the single-hop pairs
  3. Extract synthesis conditions from a small set of documents and evaluate the model's ability to follow the provided instructions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific improvements in LLM performance could be achieved by developing a standardized synthesis conditions information file (.sif) format?
- Basis in paper: [explicit] The paper suggests developing a synthesis conditions information file (.sif) to standardize reporting of synthesis conditions.
- Why unresolved: The paper only proposes this as a potential solution but does not test or quantify its effectiveness.
- What evidence would resolve it: Comparative studies measuring LLM performance in extracting synthesis conditions from papers with and without .sif format standardization.

### Open Question 2
- Question: How does the hallucination capture rate of GPT-4-Turbo in multi-hop question generation compare to other state-of-the-art LLMs?
- Basis in paper: [explicit] The paper reports an 84% hallucination capture rate for multi-hop questions generated by GPT-4-Turbo.
- Why unresolved: The paper only evaluates GPT-4-Turbo and does not benchmark against other models.
- What evidence would resolve it: Comparative analysis of hallucination capture rates across different LLMs on the same multi-hop question generation task.

### Open Question 3
- Question: What is the impact of varying the number and distribution of question types in the prompt on the quality and diversity of generated Q&A pairs?
- Basis in paper: [inferred] The paper notes that the LLM did not strictly follow the specified distribution of question types for the multi-hop dataset.
- Why unresolved: The paper does not explore how different prompt configurations affect the output.
- What evidence would resolve it: Systematic experiments varying prompt parameters and measuring resulting Q&A pair characteristics.

### Open Question 4
- Question: How does the accuracy of GPT-4-Turbo in generating reticular chemistry-specific Q&A pairs compare to its performance on general domain datasets like SQuAD?
- Basis in paper: [explicit] The paper reports 94.8% accuracy for single-hop and 98.3% for multi-hop reticular chemistry Q&A pairs.
- Why unresolved: The paper does not benchmark against general domain performance.
- What evidence would resolve it: Direct comparison of GPT-4-Turbo performance on RetChemQA versus established general domain Q&A benchmarks using identical evaluation metrics.

## Limitations

- Dataset Generalization: High performance metrics may be inflated due to GPT-4-Turbo evaluating its own outputs; applicability to other domains or language models untested
- Hallucination Detection: 22% hallucination capture rate for single-hop questions suggests limitations in consistent error detection and correction
- Synthesis Conditions Extraction: ~70% 'Obedience' score indicates room for improvement in extracting detailed experimental procedures

## Confidence

- High Confidence: Dataset generation methodology and general approach to creating single-hop and multi-hop Q&A pairs are well-documented and reproducible
- Medium Confidence: Effectiveness of hallucination detection and correction mechanism is reported but generalizability and robustness across different contexts are uncertain
- Low Confidence: Dataset's performance in real-world applications or with other language models has not been tested; impact of prompt variations on Q&A pair quality is unclear

## Next Checks

1. Test the RetChemQA dataset with other language models (e.g., LLaMA, Claude) to assess its generalizability and identify potential model-specific biases or limitations

2. Apply the RetChemQA dataset to practical tasks in reticular chemistry research, such as literature review or automated knowledge extraction, to evaluate its utility and identify areas for improvement

3. Systematically vary the prompts used for generating Q&A pairs and evaluate the impact on accuracy, precision, and hallucination rates to identify optimal prompt structures and improve dataset robustness