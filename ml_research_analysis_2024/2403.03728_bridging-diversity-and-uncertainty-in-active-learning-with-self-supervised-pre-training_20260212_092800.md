---
ver: rpa2
title: Bridging Diversity and Uncertainty in Active learning with Self-Supervised
  Pre-Training
arxiv_id: '2403.03728'
source_url: https://arxiv.org/abs/2403.03728
tags:
- learning
- active
- samples
- performance
- budget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating diversity-based
  and uncertainty-based sampling strategies in active learning, particularly with
  self-supervised pre-trained models. The authors introduce a simple heuristic called
  TCM that mitigates the cold start problem while maintaining strong performance across
  various data levels.
---

# Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training

## Quick Facts
- arXiv ID: 2403.03728
- Source URL: https://arxiv.org/abs/2403.03728
- Authors: Paul Doucet; Benjamin Estermann; Till Aczel; Roger Wattenhofer
- Reference count: 12
- One-line primary result: TCM consistently outperforms existing methods across various datasets and data regimes by combining diversity-based and uncertainty-based sampling strategies.

## Executive Summary
This paper addresses the challenge of integrating diversity-based and uncertainty-based sampling strategies in active learning, particularly with self-supervised pre-trained models. The authors introduce a simple heuristic called TCM that mitigates the cold start problem while maintaining strong performance across various data levels. TCM combines TypiClust for diversity sampling with Margin for uncertainty sampling. Experiments demonstrate that TCM consistently outperforms existing methods across various datasets in both low and high data regimes. The key results show that TCM achieves an accuracy improvement of 5-10% compared to random sampling and outperforms its underlying methods TypiClust and Margin during the complete training process. The authors also provide clear guidelines for practitioners on how to easily use active learning in their own setting.

## Method Summary
TCM (Transition from Clustering to Margin) is a simple heuristic that combines two active learning methods: TypiClust for diversity sampling and Margin for uncertainty sampling. The method starts with TypiClust to select diverse and typical samples that cover the complete data distribution, avoiding the poor initial performance of uncertainty-based methods when few labeled samples are available. After a predetermined number of steps (3 for tiny/low, 2 for medium, 1 for high), TCM transitions to Margin for uncertainty sampling to refine decision boundaries. The approach relies on self-supervised pre-trained backbones to provide robust feature representations, which simplify both sample querying and classifier training.

## Key Results
- TCM achieves an accuracy improvement of 5-10% compared to random sampling across various datasets and budget sizes.
- TCM outperforms its underlying methods TypiClust and Margin during the complete training process, demonstrating the effectiveness of combining diversity and uncertainty sampling strategies.
- The simple heuristic of using a fixed step size equal to the initial budget size and transitioning from TypiClust to Margin after a predetermined number of steps consistently achieves strong performance across different data regimes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TCM improves performance by transitioning from diversity-based to uncertainty-based sampling at the optimal early stage when using pre-trained backbones.
- Mechanism: Initially applies TypiClust for diversity sampling to cover the data distribution, then switches to Margin for uncertainty sampling once the model has learned general rules, allowing focus on decision boundaries.
- Core assumption: Pre-trained backbones learn representations that make the transition point from diversity to uncertainty sampling occur earlier than when training from scratch.
- Evidence anchors:
  - [abstract] "By initially applying TypiClust for diversity sampling and subsequently transitioning to uncertainty sampling with Margin, our approach effectively combines the strengths of both strategies."
  - [section] "In this work, we provide a new heuristic called TCM on how to combine two such methods, namely TypiClust (Hacohen et al., 2022) and Margin. TypiClust shows excellent performance in low data regimes, while Margin excels afterwards. We specifically analyze the setting where a self-supervised pre-trained backbone model is available."
  - [corpus] Weak evidence - no direct mention of backbone transition dynamics in related papers.
- Break condition: If the pre-trained backbone does not learn effective representations, the transition point may be delayed, reducing TCM's effectiveness.

### Mechanism 2
- Claim: TCM's simple heuristic achieves consistent performance across different budget sizes and datasets.
- Mechanism: Uses a fixed step size equal to the initial budget size and switches from TypiClust to Margin after a predetermined number of steps (3 for tiny/low, 2 for medium, 1 for high), adapting to various data regimes without complex switching algorithms.
- Core assumption: The performance of TypiClust is mostly independent of step size, while Margin's performance is robust across different step sizes.
- Evidence anchors:
  - [section] "Based on our ablations, we devise the following simple heuristic for TCM. We use a step size equal to the size of the initial budget of each setting."
  - [section] "Surprisingly, the results show that overall, there is no clear difference in performance for different step sizes."
  - [corpus] Weak evidence - no direct mention of step size robustness in related papers.
- Break condition: If the step size significantly affects Margin's performance, TCM's simple heuristic may not be optimal.

### Mechanism 3
- Claim: TCM mitigates the cold start problem inherent in uncertainty-based methods by starting with diversity-based sampling.
- Mechanism: Begins with TypiClust to select diverse and typical samples that cover the complete data distribution, avoiding the poor initial performance of uncertainty-based methods when few labeled samples are available.
- Core assumption: Diversity-based methods like TypiClust perform better than uncertainty-based methods in low data regimes due to better coverage of the input domain.
- Evidence anchors:
  - [section] "Early on in the training, diversity-based methods tend to perform better as with limited samples it is harder to cover the complete data distribution. Further, in that stage, the classifier uncertainty is a weak predictor of hard samples. This is also called the 'cold start problem' (Mittal et al., 2019) of uncertainty-based methods."
  - [section] "TypiClust queries diverse samples by first clustering, and then selecting the most typical sample from each cluster."
  - [corpus] Weak evidence - no direct mention of cold start problem mitigation in related papers.
- Break condition: If the dataset is inherently diverse or the initial budget is large, the cold start problem may be less pronounced, reducing the benefit of starting with TypiClust.

## Foundational Learning

- Concept: Self-supervised pre-training
  - Why needed here: Provides robust feature representations that simplify both sample querying and classifier training, allowing TCM to achieve strong performance even in low data regimes.
  - Quick check question: What is the main advantage of using self-supervised pre-trained models in active learning compared to training from scratch?

- Concept: Diversity-based vs. uncertainty-based sampling
  - Why needed here: TCM combines both strategies to achieve optimal performance across different data regimes - diversity-based sampling for initial coverage and uncertainty-based sampling for refining decision boundaries.
  - Quick check question: What are the key differences between diversity-based and uncertainty-based sampling strategies in active learning?

- Concept: Active learning cold start problem
  - Why needed here: TCM addresses this issue by starting with diversity-based sampling (TypiClust) before transitioning to uncertainty-based sampling (Margin), avoiding the poor initial performance of uncertainty-based methods.
  - Quick check question: What is the cold start problem in active learning, and how does TCM mitigate it?

## Architecture Onboarding

- Component map:
  - Self-supervised pre-trained backbone (e.g., SimCLR, DINO)
  - TypiClust for initial diversity sampling
  - Margin for subsequent uncertainty sampling
  - Linear prediction head on top of the backbone

- Critical path:
  1. Pre-train backbone using self-supervised learning
  2. Apply TypiClust for initial sampling rounds
  3. Transition to Margin for remaining sampling rounds
  4. Fine-tune linear prediction head after each sampling round

- Design tradeoffs:
  - Simple heuristic vs. complex switching algorithms (e.g., SelectAL)
  - Fixed step size vs. adaptive step size based on performance
  - Early transition vs. delayed transition from diversity to uncertainty sampling

- Failure signatures:
  - Poor performance in low data regimes: Backbone may not provide effective representations
  - Inconsistent performance across datasets: Step size or transition point may need adjustment
  - Overfitting to initial samples: Margin may be selecting too similar samples due to large step size

- First 3 experiments:
  1. Compare TCM with TypiClust and Margin individually on CIFAR10 with varying initial budgets
  2. Evaluate the effect of different transition points (number of steps before switching from TypiClust to Margin) on TCM's performance
  3. Test TCM's robustness to different step sizes for Margin sampling rounds on CIFAR100

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- The underlying mechanisms by which TCM achieves its performance gains, particularly the role of pre-trained backbones in enabling earlier transitions, are not fully validated.
- The fixed step size heuristic, while simple, may not be optimal for all datasets or scenarios, and the paper does not explore adaptive step sizes or more sophisticated switching algorithms.
- The effectiveness of TCM in real-world applications with complex data distributions or noisy labels is not addressed.

## Confidence
- High confidence in the overall performance improvement of TCM compared to random sampling and its underlying methods (TypiClust and Margin).
- Medium confidence in the specific mechanisms by which TCM achieves its performance gains, particularly the role of pre-trained backbones in enabling earlier transitions.
- Medium confidence in the robustness of the fixed step size heuristic across different datasets and budget sizes.

## Next Checks
1. Conduct ablation studies to analyze the impact of different transition points from TypiClust to Margin on TCM's performance, and determine if there is an optimal transition point that maximizes accuracy across datasets.
2. Evaluate TCM's performance on additional datasets with varying characteristics, such as higher dimensionality or different class distributions, to assess its generalizability beyond the tested datasets.
3. Investigate the effect of using different pre-trained backbones (e.g., contrastive vs. generative) on TCM's performance, and analyze how the choice of backbone influences the transition dynamics and overall effectiveness of the method.