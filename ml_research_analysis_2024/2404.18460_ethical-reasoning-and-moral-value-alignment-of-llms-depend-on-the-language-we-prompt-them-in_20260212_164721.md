---
ver: rpa2
title: Ethical Reasoning and Moral Value Alignment of LLMs Depend on the Language
  we Prompt them in
arxiv_id: '2404.18460'
source_url: https://arxiv.org/abs/2404.18460
tags:
- ethical
- moral
- language
- llms
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how three popular LLMs - GPT-4, ChatGPT,
  and Llama2-70B-Chat - perform ethical reasoning across six languages: English, Spanish,
  Russian, Chinese, Hindi, and Swahili. Building on Rao et al.'
---

# Ethical Reasoning and Moral Value Alignment of LLMs Depend on the Language we Prompt them in

## Quick Facts
- arXiv ID: 2404.18460
- Source URL: https://arxiv.org/abs/2404.18460
- Authors: Utkarsh Agarwal; Kumar Tanmay; Aditi Khandelwal; Monojit Choudhury
- Reference count: 0
- This study investigates how three popular LLMs perform ethical reasoning across six languages: English, Spanish, Russian, Chinese, Hindi, and Swahili, finding significant language-dependent moral biases.

## Executive Summary
This study examines the cross-lingual ethical reasoning capabilities of three popular LLMs (GPT-4, ChatGPT, and Llama2-70B-Chat) across six languages. The researchers found that while GPT-4 demonstrates more consistent ethical reasoning across languages, all models show significant performance degradation in low-resource languages like Hindi and Swahili. Notably, the nature of moral bias varies substantially across languages for all models, suggesting that language choice significantly impacts LLM ethical reasoning outputs.

## Method Summary
The researchers built on Rao et al. (2023) by evaluating three LLMs on four moral dilemmas with 18 pairs of contrasting ethical policies from deontology, virtue ethics, and consequentialism frameworks. These prompts were translated into six languages (English, Spanish, Russian, Chinese, Hindi, Swahili) using Google Translate. For each model-language combination, they conducted baseline experiments (120 trials) to establish default resolutions, then policy-based experiments (432 trials) to measure how ethical policies influenced reasoning. They calculated accuracy, bias (sticking to baseline despite opposing policy), and confusion (deviating from baseline without clear reason) metrics using temperature=0, top_p=0.95, and presence_penalty=1.

## Key Results
- GPT-4 shows the most consistent ethical reasoning across languages with the least language-dependent bias
- All models perform significantly worse in low-resource languages (Hindi and Swahili) compared to high-resource languages
- The pattern of moral bias varies substantially across languages for all models, even GPT-4
- ChatGPT and Llama2-70B-Chat exhibit significant moral value bias when prompted in languages other than English

## Why This Works (Mechanism)
### Mechanism 1: Foreign Language Effect in LLMs
- **Claim:** LLMs exhibit different moral biases depending on the language of the prompt, mirroring the human foreign language effect
- **Mechanism:** Models trained primarily on English data have weaker representations in low-resource languages, causing them to rely more on surface patterns than deep ethical reasoning when prompted in these languages
- **Core assumption:** Quality of multilingual representations correlates with training data volume per language

## Foundational Learning
- **Foreign Language Effect:** Psychological phenomenon where people make different decisions when using a non-native language, typically showing reduced emotional reactivity and increased utilitarian reasoning. Needed to contextualize why language choice affects LLM ethical reasoning. Quick check: Compare human decision patterns across native vs. foreign languages.
- **Deontological Ethics:** Moral framework focused on adherence to rules and duties regardless of consequences. Needed as one of the ethical policy types tested. Quick check: Verify that policies correctly reflect duty-based reasoning patterns.
- **Consequentialism:** Ethical framework judging actions by their outcomes rather than inherent rightness. Needed as one of the ethical policy types tested. Quick check: Ensure policies align with outcome-focused reasoning.
- **Virtue Ethics:** Moral framework emphasizing character and virtues rather than rules or consequences. Needed as one of the ethical policy types tested. Quick check: Confirm policies reflect character-based reasoning.
- **Cross-lingual Transfer:** The ability of models to apply knowledge from high-resource to low-resource languages. Needed to understand performance gaps in Hindi and Swahili. Quick check: Compare performance correlation between language pairs.

## Architecture Onboarding
- **Component Map:** Moral dilemmas (4) -> Ethical policies (18 pairs) -> Translated prompts (6 languages) -> LLMs (3 models) -> Output resolutions -> Accuracy/bias/confusion metrics
- **Critical Path:** Prompt generation (translation) -> Model inference (temperature=0, top_p=0.95, presence_penalty=1) -> Resolution classification -> Metric calculation
- **Design Tradeoffs:** Using machine translation enables scalability but risks cultural misalignment; temperature=0 ensures consistency but may limit nuanced reasoning; focusing on three models enables depth but limits generalizability
- **Failure Signatures:** Poor performance in Hindi/Swahili indicates translation or representation issues; inconsistent bias patterns across languages suggests prompt sensitivity; GPT-4's relative consistency validates methodology
- **First Experiments:** 1) Replicate baseline experiments with human-validated translations to isolate translation effects; 2) Test additional moral dilemmas to assess generalizability; 3) Vary temperature settings to understand their impact on cross-lingual reasoning

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the multilingual performance gap in ethical reasoning stem from inherent cultural differences or simply from model training data disparities across languages?
- Basis in paper: [explicit] The paper notes that bias patterns are similar between English-Spanish and Chinese-Hindi across models, suggesting there might be more to this than just performance disparity, but doesn't definitively answer whether cultural differences or training data disparities are the root cause.
- Why unresolved: The study doesn't directly compare cultural values across languages or analyze training data composition by language.
- What evidence would resolve it: Comparative analysis of training data distributions across languages, coupled with cross-cultural value surveys and their correlation with model performance.

### Open Question 2
- Question: Can the Foreign Language Effect observed in humans be replicated and measured in LLMs across different languages and ethical frameworks?
- Basis in paper: [explicit] The paper discusses the Foreign Language Effect in humans and asks whether LLMs exhibit similar behavior, but doesn't provide a systematic study of this phenomenon.
- Why unresolved: The study focuses on accuracy and bias metrics but doesn't specifically measure how emotional/cognitive distance affects ethical reasoning in different languages.
- What evidence would resolve it: Controlled experiments varying emotional content and cognitive complexity in dilemmas across languages, measuring consistency in ethical reasoning.

### Open Question 3
- Question: How can ethical reasoning capabilities in low-resource languages be improved without compromising performance in high-resource languages?
- Basis in paper: [explicit] The paper identifies poor performance in Hindi and Swahili but doesn't explore solutions for improving ethical reasoning in these languages.
- Why unresolved: The study is observational and doesn't test interventions like targeted fine-tuning, multilingual pretraining, or prompt engineering specific to low-resource languages.
- What evidence would resolve it: Experimental results comparing different training strategies, multilingual datasets, or prompting techniques on ethical reasoning performance across languages.

## Limitations
- Translation quality and cultural appropriateness of machine-translated prompts may significantly impact results, especially for languages with different ethical frameworks
- The study uses a limited set of four moral dilemmas and one source of ethical policies, which may not represent the full spectrum of ethical reasoning challenges
- The unusual temperature=0 and presence_penalty=1 configuration may affect models' reasoning patterns differently than standard settings

## Confidence
- **High Confidence**: All three models perform poorly in low-resource languages (Hindi and Swahili) compared to high-resource languages
- **Medium Confidence**: GPT-4 shows less language-dependent bias than ChatGPT and Llama2-70B-Chat
- **Low Confidence**: Specific patterns of moral bias variation across the six languages for each model

## Next Checks
1. **Cultural Validation Study**: Recruit native speakers of each language to review translated prompts and assess cultural appropriateness, then compare model performance using human-validated versus machine translations.
2. **Controlled Prompt Variation**: Replicate experiments with systematic variations in prompt structure, response option ordering, and ethical framework presentation to determine whether cross-lingual differences persist.
3. **Expanded Moral Scenarios**: Test the same models across a broader and more diverse set of moral dilemmas from multiple cultural contexts to assess generalizability of observed patterns.