---
ver: rpa2
title: A Neural Network Alternative to Tree-based Models
arxiv_id: '2410.17758'
source_url: https://arxiv.org/abs/2410.17758
tags:
- features
- feature
- data
- attention
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tabular data are ubiquitous in scientific disciplines such as biology,
  but current artificial neural networks (ANNs) underperform compared to tree-based
  models like XGBoost on these tasks, especially in terms of interpretability. To
  address this gap, we introduce a sparse neural network model with attention mechanisms,
  called sTabNet, which dynamically enforces sparsity by leveraging feature relationships
  rather than data point relationships.
---

# A Neural Network Alternative to Tree-based Models

## Quick Facts
- arXiv ID: 2410.17758
- Source URL: https://arxiv.org/abs/2410.17758
- Reference count: 40
- Primary result: Sparse neural network with attention (sTabNet) outperforms tree-based models on biological tabular datasets while providing superior interpretability

## Executive Summary
This paper addresses the performance gap between neural networks and tree-based models on tabular data by introducing sTabNet, a sparse neural network architecture that enforces sparsity through feature relationships and attention mechanisms. The model demonstrates state-of-the-art performance on biological datasets while offering built-in interpretability that surpasses post-hoc methods like SHAP. sTabNet achieves sparsity either through domain knowledge or unsupervised methods like random walks on feature graphs, and shows adaptability as a foundational model across different tabular domains.

## Method Summary
sTabNet is a sparse neural network model that enforces structural sparsity before training by constructing a binary mask matrix from feature relationships. Features are grouped into clusters using domain knowledge (e.g., biological pathways) or unsupervised methods (clustering or random walks on feature graphs). A binary matrix A is created where A[i,j]=1 if feature i belongs to cluster j, then element-wise multiplied with the weight matrix to create sparse connections. The model incorporates attention mechanisms to automatically determine feature importance during training, providing direct interpretability without requiring post-hoc explainability methods.

## Key Results
- Achieves state-of-the-art performance on biological datasets, outperforming XGBoost and other tree-based models
- Demonstrates superior interpretability through attention mechanisms that directly reveal feature relevance
- Shows adaptability across in-domain and out-of-domain tasks, functioning as a foundational model for tabular data
- Successfully handles survival analysis tasks with specialized loss functions like Breslow approximation

## Why This Works (Mechanism)

### Mechanism 1
The model enforces sparsity structurally by using a binary mask matrix derived from feature relationships. Features are clustered using domain knowledge or unsupervised methods like random walks on feature graphs, creating a binary matrix A where A[i,j]=1 indicates feature i belongs to cluster j. This matrix is element-wise multiplied with the weight matrix to enforce sparsity: H = σ(X · (A ⊙ W)). This works because feature relationships capture meaningful structure that improves generalization and interpretability, though spurious relationships could hurt performance.

### Mechanism 2
Attention weights serve as direct measures of feature importance, outperforming post-hoc methods like SHAP. The model uses a tabular-adapted attention mechanism (scaled dot-product or content-based) that computes importance scores during training. These scores are used both for interpretability and feature selection. The core assumption is that end-to-end trained attention will align with true feature importance for the task, though poorly tuned attention may not reliably reflect importance.

### Mechanism 3
sTabNet functions as a foundational model by being pre-trained on rich datasets (e.g., multi-omics) and fine-tuned or used for feature extraction on new datasets. The learned sparse, attention-weighted representation generalizes across related tabular domains. This works because the pre-training captures useful representations that transfer to new tasks, though performance may degrade if new domains are too different.

## Foundational Learning

- Concept: Feature grouping/clustering and graph-based random walks
  - Why needed here: These methods construct the sparsity-inducing mask matrix A without relying on external domain knowledge
  - Quick check question: Can you explain how cosine similarity between features leads to edges in the feature graph used for random walks?

- Concept: Attention mechanisms adapted for tabular data
  - Why needed here: Standard attention (e.g., for sequences) must be reformulated to score feature importance in a static tabular setting
  - Quick check question: What is the difference between scaled dot-product attention and content-based attention in this context?

- Concept: Survival analysis loss functions (e.g., Breslow approximation)
  - Why needed here: Survival tasks require specialized loss formulations different from standard classification/regression
  - Quick check question: Why can't we use mean squared error for survival time prediction?

## Architecture Onboarding

- Component map: Input layer → optional attention layer → sparse layer (with mask A) → dense layer(s) → output layer
- Critical path:
  1. Build feature graph or load domain knowledge
  2. Generate mask matrix A via clustering or random walks
  3. Construct sparse layer with A ⊙ W
  4. Train with attention and loss appropriate to task
- Design tradeoffs:
  - Sparsity vs. model capacity: too sparse → underfitting; too dense → overfitting
  - Attention complexity: more complex attention → better interpretability but higher cost
  - Random walk hyperparameters (steps, walks) → control locality vs. global coverage
- Failure signatures:
  - Accuracy drops sharply after enforcing sparsity → A is too restrictive
  - Attention weights are uniform → attention layer not learning
  - Model predicts majority class only → insufficient capacity or poor initialization
- First 3 experiments:
  1. Train sTabNet on synthetic dataset with known informative/noisy features; check if attention separates them
  2. Compare sTabNet vs. XGBoost on tabular benchmark; measure accuracy and interpretability (feature ranking)
  3. Test fine-tuning: pre-train on multi-omics, fine-tune on single-cell; compare to training from scratch

## Open Questions the Paper Calls Out
The paper itself does not explicitly call out open questions, though several implications remain unexplored.

## Limitations
- Implementation details for the sparse layer and attention mechanisms are not fully specified
- Domain knowledge integration from Gene Ontology and Reactome databases lacks specific pathway details
- Limited comparison with other recent neural network approaches for tabular data
- Minimal hyperparameter tuning makes it unclear how much performance depends on architectural choices versus careful optimization

## Confidence
- **High confidence**: Basic mechanism of enforcing sparsity through feature grouping and attention-based feature importance
- **Medium confidence**: Claims about superior interpretability compared to SHAP
- **Medium confidence**: Claims about adaptability as a foundational model

## Next Checks
1. Implement sparse layer verification using synthetic datasets with known feature relationships to verify random walk-based sparsity enforcement
2. Conduct attention mechanism ablation study testing different attention mechanisms (Bahdanau, Luong, Graves) and activation functions
3. Perform cross-domain generalization test pre-training on multi-omics and evaluating performance across multiple out-of-domain tabular datasets