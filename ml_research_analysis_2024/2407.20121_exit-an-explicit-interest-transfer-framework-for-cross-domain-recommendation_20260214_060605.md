---
ver: rpa2
title: 'EXIT: An EXplicit Interest Transfer Framework for Cross-Domain Recommendation'
arxiv_id: '2407.20121'
source_url: https://arxiv.org/abs/2407.20121
tags:
- domain
- interest
- user
- transfer
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles cross-domain recommendation, focusing on preventing
  negative transfer when transferring knowledge between domains with significant differences,
  such as between search and recommendation systems. The proposed EXplicit Interest
  Transfer (EXIT) framework uses supervised learning to explicitly model interest
  transfer.
---

# EXIT: An EXplicit Interest Transfer Framework for Cross-Domain Recommendation

## Quick Facts
- **arXiv ID**: 2407.20121
- **Source URL**: https://arxiv.org/abs/2407.20121
- **Reference count**: 40
- **Key outcome**: EXIT framework achieves 1.23% CTCVR and 3.65% GTV improvements in Meituan's homepage recommendation system

## Executive Summary
EXIT addresses the challenge of negative transfer in cross-domain recommendation by explicitly modeling which source domain interests should be transferred to the target domain. The framework constructs an Interest Combination Label (ICL) to serve as ground truth for beneficial transfers and uses a Scene Selector Network (SSN) to dynamically adjust transfer intensity based on user contexts. EXIT has been deployed in Meituan's production system, demonstrating significant improvements in both offline metrics (AUC, Logloss) and online A/B tests (CTCVR, GTV).

## Method Summary
EXIT is a multi-task learning framework that explicitly models interest transfer between domains. It consists of three components: an Interest Prediction Network (IPN) with separate towers for target and source domain interest prediction, an Interest Combination Label (ICL) that provides supervised ground truth for beneficial transfers, and a Scene Selector Network (SSN) that models transfer intensity under fine-grained contexts. The framework uses a joint loss function combining cross-entropy losses for domain-specific interest predictions with an L1 loss for transfer probability matching the ICL.

## Key Results
- 1.23% improvement in Click-Through Conversion Rate (CTCVR)
- 3.65% improvement in Gross Transaction Value (GTV)
- Successful deployment in Meituan's homepage recommendation system
- Outperforms existing methods in both offline and online evaluations

## Why This Works (Mechanism)

### Mechanism 1
The EXIT framework prevents negative transfer by explicitly modeling which source domain interest signals are appropriate for the target domain. EXIT constructs an Interest Combination Label (ICL) that serves as ground truth for beneficial transfers, allowing supervised learning to filter out inappropriate interest signals. This works under the assumption that source and target domains share the same set of users and items, enabling construction of labels based on cross-domain purchase behavior patterns.

### Mechanism 2
The Scene Selector Network (SSN) dynamically adjusts transfer intensity based on fine-grained user contexts to match real interest patterns. SSN takes user, item, and context embeddings plus scene embeddings as input to output an interest transfer probability that varies by specific scenarios (e.g., weekday vs weekend, office vs shopping mall). This assumes user interest preferences have strong correlation with specific contexts like time, location, and user demographics.

### Mechanism 3
The multi-interest joint loss enables simultaneous learning of target domain interests, source domain interests, and transfer probabilities in a unified framework. EXIT uses separate towers for target and source domain interest prediction, plus SSN output, all trained together with cross-entropy losses for interest predictions and L1 loss for transfer probability matching ICL. This assumes learning source and target domain interests separately is fundamental, and joint training with appropriate supervision enables better overall performance.

## Foundational Learning

- **Concept: Cross-domain recommendation fundamentals** - Why needed here: Understanding how knowledge transfer between domains works and why negative transfer occurs is crucial for grasping EXIT's motivation. Quick check: What are the main challenges in cross-domain recommendation when source and target domains have different service functions?

- **Concept: Supervised learning with custom labels** - Why needed here: EXIT uses a novel Interest Combination Label for supervised training, which is key to its explicit transfer approach. Quick check: How does EXIT construct the ICL labels, and what do different label values (0, 1, 2) represent?

- **Concept: Multi-task learning architecture** - Why needed here: EXIT uses a multi-task framework with separate towers for target and source domain interests, requiring understanding of how to coordinate multiple learning objectives. Quick check: What are the three components of EXIT's loss function, and how do they work together?

## Architecture Onboarding

- **Component map**: User features → Embedding layer → IPN towers → SSN → Combined interest prediction → Loss calculation → Model update

- **Critical path**: User features flow through embedding layer to separate IPN towers for target and source domains, then to SSN for context-aware transfer modeling, combined for final prediction, with joint loss driving model updates

- **Design tradeoffs**: Explicit vs implicit transfer (EXIT chooses explicit supervision over traditional unsupervised blending, adding complexity but better controlling negative transfer), context sensitivity (SSN adds computational overhead but enables adaptive transfer based on user scenarios), label construction (ICL requires careful engineering but enables supervised learning of transfer patterns)

- **Failure signatures**: Training divergence (joint loss may not converge if components aren't properly balanced), negative transfer persists (if ICL construction is flawed, inappropriate interests may still transfer), overfitting to contexts (SSN may memorize specific scenarios rather than learning generalizable patterns)

- **First 3 experiments**:
  1. Ablation study: Remove SSN to measure impact of context-aware transfer
  2. Ablation study: Remove ICL to measure impact of explicit supervision
  3. Parameter sensitivity: Test different weightings (λ1, λ2, λ3) in the joint loss function

## Open Questions the Paper Calls Out

### Open Question 1
How does the Interest Combination Label (ICL) handle cases where user interests in the source and target domains are both present but fundamentally incompatible (e.g., searching for medical supplies when healthy but being recommended them in the context of general shopping)? The paper discusses scenarios where source domain interests might be inappropriate for transfer but doesn't fully detail how ICL handles such fundamental incompatibilities.

### Open Question 2
What is the optimal granularity for scene features in the Scene Selector Network (SSN), and how does this affect transfer performance across different business domains? The paper mentions using various scene features but doesn't systematically analyze the impact of feature granularity or domain-specific differences.

### Open Question 3
How does the EXIT framework scale to scenarios with many more source domains, and what are the computational bottlenecks in such cases? While the paper mentions that unified modeling allows easy expansion to more domains, it doesn't provide empirical evidence or analysis of scaling behavior.

## Limitations
- Effectiveness depends on shared user/item space between domains for ICL construction, limiting generalizability
- Requires careful engineering of context features and label construction, adding complexity
- Industrial deployment context may limit generalizability to other recommendation domains

## Confidence
- **High confidence** in the core mechanism of explicit interest transfer through ICL supervision
- **Medium confidence** in SSN's context-aware transfer modeling due to limited details on scene feature engineering
- **Medium confidence** in overall performance claims due to lack of full architectural and hyperparameter disclosure

## Next Checks
1. Test EXIT on domains with different user/item spaces to evaluate generalizability when core assumption is violated
2. Conduct ablation studies on SSN component by removing context features to quantify actual benefit
3. Validate robustness of ICL construction by testing alternative label generation methods, particularly for group consistency interest calculation