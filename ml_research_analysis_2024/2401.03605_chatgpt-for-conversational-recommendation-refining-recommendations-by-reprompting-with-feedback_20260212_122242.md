---
ver: rpa2
title: 'ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting
  with Feedback'
arxiv_id: '2401.03605'
source_url: https://arxiv.org/abs/2401.03605
tags:
- chatgpt
- recommendation
- recommendations
- user
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates ChatGPT as a conversational recommender,
  introducing a pipeline that iteratively refines recommendations through reprompting
  with user feedback. It evaluates four content embedding levels, three prompt styles
  (zero-shot, few-shot, Chain-of-Thought), and multiple conversation depths.
---

# ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting with Feedback

## Quick Facts
- arXiv ID: 2401.03605
- Source URL: https://arxiv.org/abs/2401.03605
- Reference count: 30
- Key outcome: Reprompting ChatGPT with user feedback significantly improves recommendation relevance (precision up to 0.637, nDCG up to 0.656) and matches supervised NMF performance when evaluated with GPT-3 embeddings

## Executive Summary
This study investigates ChatGPT as a conversational recommender system by introducing an iterative feedback pipeline. The approach involves generating initial recommendations, collecting user feedback, and reprompting ChatGPT to refine subsequent suggestions. Results demonstrate that this iterative process significantly improves recommendation relevance compared to single prompts. The study also evaluates four content embedding levels, three prompt styles, and finds that ChatGPT exhibits popularity bias that can be mitigated through higher sampling temperature and explicit instructions.

## Method Summary
The study uses the HetRec2011 movie dataset with four content embedding levels ranging from basic information to full Wikipedia text. Fifty users with sufficient interactions are sampled and split into example, feedback, and evaluation sets. Three prompt styles (zero-shot, few-shot, Chain-of-Thought) are tested with iterative feedback loops of 3-5 prompts, each generating 5-10 recommendations. Final 20 recommendations are evaluated against held-out test sets using cosine similarity-based relevance matching with GPT-3 embeddings.

## Key Results
- Reprompting with feedback significantly improves recommendation relevance, with precision reaching 0.637 and nDCG reaching 0.656
- ChatGPT performance matches supervised NMF models when evaluated using GPT-3 embeddings
- ChatGPT exhibits strong popularity bias toward top-rated films, but this can be mitigated through higher temperature and explicit instructions to avoid popular items

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reprompting with user feedback significantly improves recommendation relevance by iteratively refining the model's understanding of user preferences.
- Mechanism: ChatGPT's conversational ability allows it to receive explicit feedback (positive/negative) on its recommendations, which is incorporated into subsequent prompts. This iterative process enables the model to progressively adjust its recommendations to better match the user's stated preferences, moving beyond the initial static understanding.
- Core assumption: The model's embeddings and reasoning can effectively incorporate feedback to refine recommendations, and the feedback provided is accurate and representative of the user's true preferences.
- Evidence anchors:
  - [abstract]: "We find that reprompting ChatGPT with feedback is an effective strategy to improve recommendation relevancy..."
  - [section 4.2]: "We find that reprompting is effective in making the model's final recommendations more relevant... We see that the model is able to match more relevant items to the user as p increases, which also shows an increase in precision."
  - [corpus]: Weak evidence. Corpus neighbors focus on diversity and novelty bias but not on the feedback refinement mechanism itself.

### Mechanism 2
- Claim: ChatGPT exhibits popularity bias in its recommendations, favoring top-rated items, but this bias can be mitigated through prompt engineering.
- Mechanism: The model's training data likely over-represents popular items, leading to their over-recommendation. By explicitly instructing the model to recommend less popular items and increasing the sampling temperature, the diversity of recommendations can be increased at the cost of some precision.
- Core assumption: The popularity bias is a result of the training data distribution and can be overridden by explicit instructions in the prompt.
- Evidence anchors:
  - [abstract]: "ChatGPT exhibits popularity bias toward top-rated films, but this can be mitigated using higher sampling temperature and explicit instructions to avoid popular items..."
  - [section 4.4]: "We find that the list of most frequently recommended items coincides with the IMDB top 250 movies list... A high temperature and restricting popular recommendation has a profound effect on recommendation variety."
  - [corpus]: Moderate evidence. The neighbor "Exploring Diversity, Novelty, and Popularity Bias in ChatGPT's Recommendations" directly addresses this bias.

### Mechanism 3
- Claim: ChatGPT's performance in a conversational recommendation setting is comparable to supervised NMF models when evaluated using GPT-3 embeddings.
- Mechanism: The iterative feedback process allows ChatGPT to learn user preferences in a way that is competitive with models trained on explicit interaction data. The evaluation using GPT-3 embeddings provides a fair comparison by using the same semantic representation for both the model's recommendations and the ground truth.
- Core assumption: The GPT-3 embeddings are a suitable proxy for user preference and can be used to fairly evaluate both ChatGPT and traditional models.
- Evidence anchors:
  - [abstract]: "performance matches that of supervised NMF models when evaluated with GPT-3 embeddings."
  - [section 4.3]: "Interestingly, the NMF models evaluated using their own learned embeddings perform similarly to ChatGPT... This may indicate that ChatGPT with iterative feedback is as effective as a supervised model..."
  - [corpus]: Weak evidence. Corpus neighbors focus on different aspects of ChatGPT in recommendation, not direct performance comparison with supervised models.

## Foundational Learning

- Concept: Prompt Engineering (Zero-shot, Few-shot, Chain-of-Thought)
  - Why needed here: Different prompt styles significantly impact ChatGPT's performance in generating relevant recommendations. Understanding how to structure prompts is crucial for optimizing the conversational recommendation pipeline.
  - Quick check question: What is the difference between zero-shot and few-shot prompting, and when might each be more appropriate for a conversational recommender?

- Concept: Content Embeddings and Similarity Measures
  - Why needed here: The study relies on generating embeddings for movie items using text-ada-embedding-002 and using cosine similarity to determine relevance. A solid understanding of how embeddings capture semantic meaning and how similarity is computed is essential for interpreting the results.
  - Quick check question: How does the choice of content level (amount of information used to generate embeddings) affect the similarity distribution between items, and why is this important for recommendation?

- Concept: Popularity Bias and its Mitigation
  - Why needed here: The study investigates ChatGPT's tendency to recommend popular items and explores methods to reduce this bias. Understanding the sources and implications of popularity bias is crucial for developing fair and diverse recommendation systems.
  - Quick check question: What are the potential trade-offs between reducing popularity bias and maintaining recommendation accuracy, and how can these trade-offs be managed through prompt engineering?

## Architecture Onboarding

- Component map: Data Preparation -> Prompt Construction -> Recommendation Generation -> Evaluation
- Critical path: User interaction data → Content embedding generation → Initial prompt creation → ChatGPT API calls → Feedback incorporation → Final recommendation evaluation
- Design tradeoffs: Higher content embedding levels provide more information but may introduce noise; higher sampling temperature increases diversity but reduces precision
- Failure signatures: High unmatched ratio indicates title extraction/matching issues; low precision suggests inappropriate embeddings or relevance thresholds
- First experiments: 1) Test title extraction and fuzzy matching pipeline; 2) Validate content embedding quality across levels; 3) Compare prompt styles on small user sample

## Open Questions the Paper Calls Out

1. Does iterative feedback improve recommendation performance beyond ChatGPT's initial output, and is this improvement consistent across different content embedding levels?
2. Can the popularity bias mitigation techniques (high temperature and restricting popular recommendations) be combined without significantly degrading recommendation performance?
3. How does ChatGPT's performance as a conversational recommender compare to state-of-the-art recommendation algorithms beyond the NMF baseline?

## Limitations

- Evaluation relies on GPT-3 embeddings which may not perfectly align with human judgment of recommendation quality
- Simulated feedback loop with held-out user interactions rather than real conversational data
- Limited comparison to state-of-the-art recommendation algorithms beyond NMF baseline

## Confidence

- **High confidence**: Reprompting with feedback improves recommendation relevance (supported by consistent precision and nDCG improvements across experiments)
- **Medium confidence**: ChatGPT performance matches supervised NMF models when evaluated with GPT-3 embeddings (based on limited user sampling and simulated feedback)
- **Medium confidence**: Popularity bias mitigation through temperature and instructions (demonstrates effectiveness but may have unintended consequences on other recommendation dimensions)

## Next Checks

1. Conduct human evaluation study to validate whether GPT-3 embedding-based relevance matching correlates with actual user satisfaction of recommendations
2. Test the conversational pipeline with real user interactions rather than simulated feedback to assess how the model handles ambiguous or contradictory feedback
3. Perform ablation study on the popularity bias mitigation strategy to quantify trade-offs between novelty and precision across different user segments and recommendation contexts