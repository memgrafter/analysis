---
ver: rpa2
title: Pre-trained Large Language Models for Financial Sentiment Analysis
arxiv_id: '2401.05215'
source_url: https://arxiv.org/abs/2401.05215
tags:
- sentiment
- financial
- language
- text
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses financial sentiment analysis by classifying
  financial news titles into positive, negative, or neutral categories. The challenge
  lies in limited training data due to high labeling costs and domain-specific language
  patterns.
---

# Pre-trained Large Language Models for Financial Sentiment Analysis

## Quick Facts
- arXiv ID: 2401.05215
- Source URL: https://arxiv.org/abs/2401.05215
- Reference count: 24
- Key outcome: Llama2-7B with supervised fine-tuning achieves 90% accuracy on financial sentiment analysis, outperforming FinBERT by 4 percentage points

## Executive Summary
This paper addresses financial sentiment analysis by classifying financial news titles into positive, negative, or neutral categories. The challenge lies in limited training data due to high labeling costs and domain-specific language patterns. The authors propose adapting pre-trained large language models (LLMs), specifically Llama2-7B, using supervised fine-tuning (SFT) to overcome data scarcity. They evaluate three methods: few-shot prediction, SFT, and classification head. The SFT method achieves 90% accuracy, outperforming the previous state-of-the-art (FinBERT) by 4 percentage points. The study demonstrates that pre-trained LLMs, even with fewer parameters, are highly effective for financial sentiment analysis. Future work includes exploring larger LLMs like Llama2-70B.

## Method Summary
The authors address financial sentiment analysis by adapting pre-trained LLMs to the task using supervised fine-tuning. They evaluate three approaches: few-shot prediction, supervised fine-tuning, and a classification head architecture. The Llama2-7B model is fine-tuned on the Financial PhraseBank dataset using a block-diagonal attention mask to prevent interference between samples during training. The SFT approach is trained for 5 epochs with cosine annealing learning rate schedule (3e-5 to 3e-6), gradient clipping of 1.0, micro batch size of 4, and sequence length of 1024. The classification head provides probability outputs useful for practical applications requiring confidence scores.

## Key Results
- Llama2-7B with SFT achieves 90% accuracy on Financial PhraseBank test set
- Outperforms previous state-of-the-art FinBERT by 4 percentage points
- SFT significantly improves over few-shot prediction baseline
- Classification head provides calibrated probabilities for practical applications

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning Llama2-7B with supervised fine-tuning effectively adapts the model to financial sentiment classification despite limited training samples. Llama2-7B is pre-trained on a large, diverse corpus and thus already captures robust linguistic representations. SFT allows these representations to be refined using domain-specific labeled examples, producing task-adapted embeddings for sentiment classification. Core assumption: Pre-trained LLMs have transferable representations that can be effectively fine-tuned with small domain datasets without catastrophic forgetting.

### Mechanism 2
Modeling the task as a multi-choice classification rather than free-form generation yields higher accuracy and additional utility (confidence scores). By framing the sentiment classification as selecting from {"positive", "negative", "neutral"}, the model can output calibrated probabilities, which can be thresholded or used for downstream decision-making. The classification head architecture ensures consistent output format. Core assumption: Classification is better constrained than generation for sentiment labeling, enabling higher precision and confidence estimation.

### Mechanism 3
Using a block-diagonal attention mask during fine-tuning prevents interference between samples, improving sample efficiency and training stability. The block-diagonal mask restricts each question-answer pair's attention to its own context, ensuring that information from other pairs in the same sequence does not leak. This enforces independence and avoids negative interference in shared transformer layers. Core assumption: Sample independence is beneficial for few-shot adaptation; cross-sample attention in a single batch can harm convergence.

## Foundational Learning

- Concept: Pre-trained language model transfer learning
  - Why needed here: The problem has scarce labeled financial data; pre-trained models provide general language understanding that can be specialized via fine-tuning.
  - Quick check question: What is the key advantage of using a pre-trained model over training from scratch when labeled data is limited?

- Concept: Supervised fine-tuning (SFT) vs prompt engineering
  - Why needed here: SFT allows the model to learn from labeled data directly, improving performance over few-shot prompting, especially when high accuracy is required.
  - Quick check question: How does SFT differ from in-context learning (few-shot prompting) in terms of model adaptation?

- Concept: Classification vs generation framing for structured tasks
  - Why needed here: Sentiment analysis is a structured classification task; framing it as generation may be less precise and harder to evaluate than classification.
  - Quick check question: What is one advantage of framing a sentiment classification task as a multi-choice problem instead of a free-form generation task?

## Architecture Onboarding

- Component map: Pre-trained Llama2-7B base model -> Tokenizer (BPE, 32k vocab) -> Prompt construction pipeline -> Supervised fine-tuning training loop with block-diagonal attention mask -> Classification head (optional) -> Evaluation pipeline

- Critical path: 1. Load Llama2-7B weights 2. Prepare prompt template and tokenization 3. Construct training sequences with EOS/BOS tokens 4. Apply block-diagonal attention mask 5. Train with SFT for fixed epochs 6. Evaluate accuracy and optionally probability calibration

- Design tradeoffs:
  - Few-shot prompting: lower compute, weaker accuracy
  - SFT: higher accuracy, needs labeled data, higher compute
  - Classification head: provides probabilities, adds architectural complexity
  - Block-diagonal mask: reduces interference, may limit context use

- Failure signatures:
  - Overfitting: training accuracy >> validation accuracy
  - Underfitting: both train and validation accuracy remain low
  - Calibration failure: confidence scores don't match actual accuracy
  - Catastrophic forgetting: general language ability degrades after SFT

- First 3 experiments:
  1. Load Llama2-7B and run a single few-shot prediction to verify prompt formatting and output consistency.
  2. Fine-tune Llama2-7B on 10% of Financial PhraseBank training data and evaluate accuracy to test sample efficiency.
  3. Compare few-shot vs SFT vs classification head accuracy on the full validation set to confirm performance ranking.

## Open Questions the Paper Calls Out

### Open Question 1
Does increasing the number of parameters in the LLM (e.g., using LLaMA2-70B instead of LLaMA2-7B) lead to further improvements in financial sentiment analysis accuracy? Basis: The authors mention future work includes using LLMs with larger number of parameters (e.g., LLaMA2-70B). Why unresolved: The paper only tested LLaMA2-7B and did not experiment with larger models.

### Open Question 2
How does the performance of the SFT method compare to other fine-tuning techniques, such as prompt tuning or adapter-based fine-tuning, for financial sentiment analysis? Basis: The paper focuses on SFT but does not compare it to other fine-tuning methods. Why unresolved: The authors only evaluated the SFT method and did not explore alternative fine-tuning approaches.

### Open Question 3
Can the LLM-based approach be effectively applied to other financial NLP tasks, such as stock price prediction or risk assessment, beyond sentiment analysis? Basis: The paper focuses on financial sentiment analysis but does not explore other financial NLP tasks. Why unresolved: The authors only investigated the application of LLMs to sentiment analysis and did not extend their study to other financial NLP tasks.

## Limitations
- Evaluation is limited to a single dataset (Financial PhraseBank), which may not generalize to other financial text domains or languages
- Limited benchmarking against other recent LLM-based approaches, primarily comparing against FinBERT
- Analysis of why certain architectural choices (like the block-diagonal attention mask) work is largely empirical without theoretical justification

## Confidence
- **High confidence**: The core claim that SFT on Llama2-7B achieves 90% accuracy on Financial PhraseBank, outperforming FinBERT by 4 percentage points, is well-supported by the experimental results presented.
- **Medium confidence**: The assertion that pre-trained LLMs are highly effective for financial sentiment analysis with limited data, while supported by results, could benefit from more extensive ablation studies and comparisons with other domains.
- **Medium confidence**: The claim that the block-diagonal attention mask improves sample efficiency is supported by results but lacks theoretical justification or comparison with alternative masking strategies.

## Next Checks
1. Test the trained model on additional financial sentiment datasets (e.g., FiQA, Financial News Headlines dataset) to assess generalization beyond Financial PhraseBank.
2. Conduct an ablation study removing the block-diagonal attention mask to quantify its contribution to the reported performance gains.
3. Evaluate model calibration by computing Brier scores and reliability diagrams to verify that the classification probabilities are well-calibrated, not just accurate.