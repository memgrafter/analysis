---
ver: rpa2
title: 'G-VEval: A Versatile Metric for Evaluating Image and Video Captions Using
  GPT-4o'
arxiv_id: '2412.13647'
source_url: https://arxiv.org/abs/2412.13647
tags:
- video
- score
- caption
- captions
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "G-VEval is a novel evaluation metric for image and video captioning\
  \ that leverages GPT-4o's multimodal capabilities and chain-of-thought reasoning.\
  \ It supports three evaluation modes\u2014reference-free, reference-only, and combined\u2014\
  accommodating both image and video inputs."
---

# G-VEval: A Versatile Metric for Evaluating Image and Video Captions Using GPT-4o

## Quick Facts
- arXiv ID: 2412.13647
- Source URL: https://arxiv.org/abs/2412.13647
- Reference count: 40
- G-VEval achieves state-of-the-art correlation with human annotations for image and video captioning evaluation

## Executive Summary
G-VEval is a novel evaluation metric that leverages GPT-4o's multimodal capabilities and chain-of-thought reasoning to assess image and video captions. It introduces three evaluation modes (reference-free, reference-only, combined) and uses expected score calculation based on probabilistic outputs to improve alignment with human judgment. The metric demonstrates superior performance across multiple benchmarks, including a new MSVD-Eval dataset with a four-dimensional ACCR framework for video captioning.

## Method Summary
G-VEval uses GPT-4o to evaluate captions through structured chain-of-thought reasoning, supporting both image and video inputs. The metric calculates expected scores using GPT-4o's probabilistic outputs rather than single deterministic values. Three evaluation modes accommodate different assessment needs: reference-free (visual grounding only), reference-only (language alignment only), and combined (both visual and textual evaluation). For video captioning, G-VEval employs a four-dimensional ACCR framework (Accuracy, Completeness, Conciseness, Relevance) to provide granular assessment.

## Key Results
- Outperforms traditional metrics (CIDEr, SPICE, CLIPScore) and advanced methods (V-GPT, VL-Eval) on Flickr8k-Expert, Flickr8k-CF, and V ATEX-EV AL datasets
- Introduces MSVD-Eval dataset with ACCR framework, achieving higher correlation with human judgments than existing metrics
- Demonstrates strong performance on FOIL hallucination detection benchmark, identifying caption-image mismatches effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: G-VEval uses chain-of-thought (CoT) reasoning to improve multimodal evaluation performance
- Mechanism: GPT-4o is prompted with structured evaluation steps that guide it through a step-by-step reasoning process, mimicking human evaluation methodology
- Core assumption: Structured CoT prompting improves LLM evaluation performance compared to simple scoring prompts
- Evidence anchors:
  - [abstract] "G-VEval uses chain-of-thought reasoning in large multimodal models"
  - [section] "The Evaluation Steps leverage the Chain-of-Thought (CoT) approach, guiding GPT-4o to perform the task in a structured, step-by-step manner"
  - [corpus] Weak - no direct citation to CoT prompting literature in corpus neighbors

### Mechanism 2
- Claim: Expected score calculation using probabilistic outputs improves evaluation reliability
- Mechanism: G-VEval calculates expected values by considering the probabilistic distribution of GPT-4o's outputs rather than taking single scores
- Core assumption: The variance of conditional score distributions (Es(s|Rj)) is small enough that E(s) ≈ Es(s|Rj) approximation holds
- Evidence anchors:
  - [section] "G-VEval handles probabilistic outputs by calculating the expected value of the score using the log probabilities (log-probs) provided by GPT-4o"
  - [section] "The observed variance of Es(s|Rj) for both settings is low (0.014 for scoring and 0.0087 for rating)"
  - [corpus] Missing - no corpus neighbors discuss probabilistic evaluation scoring

### Mechanism 3
- Claim: The ACCR framework reduces inter-human variance in video captioning evaluation
- Mechanism: Breaking evaluation into four specific dimensions (Accuracy, Completeness, Conciseness, Relevance) forces consistent assessment criteria across human evaluators
- Core assumption: Structured multi-dimensional evaluation criteria reduce subjective variance compared to single holistic scores
- Evidence anchors:
  - [abstract] "MSVD-Eval dataset introduces a four-dimensional ACCR framework (Accuracy, Completeness, Conciseness, Relevance)"
  - [section] "The ACCR dimensions allow for a detailed and multi-dimensional evaluation of video captions, offering more than just an overall score"
  - [section] "This approach forces both the evaluation metrics and human annotators to assess captions from the same angles, leading to more consistent and fair evaluations"
  - [corpus] Weak - no corpus neighbors discuss ACCR or multi-dimensional evaluation frameworks

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: G-VEval relies on CoT to guide GPT-4o through structured evaluation steps
  - Quick check question: How does CoT prompting differ from simple instruction following in LLMs?

- Concept: Expected value calculation from probabilistic distributions
  - Why needed here: G-VEval uses expected score calculation rather than single deterministic outputs
  - Quick check question: What mathematical condition must hold for E(s) ≈ Es(s|Rj) approximation to be valid?

- Concept: Multimodal embedding and cross-modal alignment
  - Why needed here: G-VEval leverages GPT-4o's vision capabilities to evaluate visual content alongside text
  - Quick check question: How do vision-language models like GPT-4o encode and align visual and textual information?

## Architecture Onboarding

- Component map:
  Input preparation (reference captions + visual content) -> Prompt assembly with appropriate evaluation mode -> GPT-4o execution with multimodal input -> Probabilistic output extraction -> Expected score calculation -> Final evaluation score generation

- Critical path:
  1. Input preparation (reference captions + visual content)
  2. Prompt assembly with appropriate evaluation mode
  3. GPT-4o execution with multimodal input
  4. Probabilistic output extraction
  5. Expected score calculation
  6. Final evaluation score generation

- Design tradeoffs:
  - Reference-free vs. reference-based evaluation: Tradeoff between visual grounding and language alignment
  - ACCR vs. holistic scoring: Granularity vs. simplicity
  - Expected score vs. single score: Statistical robustness vs. computational efficiency
  - Short video clips vs. longer videos: Feasibility vs. comprehensive evaluation

- Failure signatures:
  - High variance in Es(s|Rj) distributions indicates breakdown of expected score approximation
  - Inconsistent CoT reasoning across similar inputs suggests prompt instability
  - Low correlation with human judgments indicates evaluation criteria misalignment
  - GPT-4o vision encoder failures (e.g., with complex visual scenes)

- First 3 experiments:
  1. Compare G-VEval scoring vs. rating settings on Flickr8k-Expert to validate expected score advantage
  2. Test reference-free vs. combined evaluation modes on V ATEX-EV AL to assess visual content importance
  3. Implement ablation study removing CoT steps to measure impact on evaluation performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does G-VEval's performance scale with video length beyond the 10-second limit tested in the paper?
- Basis in paper: [explicit] The paper notes G-VEval is "currently designed for short-form videos (under 10 seconds)" and suggests "extensions such as scene detection may be necessary for longer videos"
- Why unresolved: The experiments only tested on short videos (MSVD dataset), and no evaluation was performed on longer videos to validate the scalability claim
- What evidence would resolve it: Experiments testing G-VEval on videos longer than 10 seconds with and without scene detection techniques, comparing performance metrics

### Open Question 2
- Question: What is the impact of GPT-4o model updates on G-VEval's consistency over time?
- Basis in paper: [explicit] The paper mentions "future updates to the GPT-4o model or changes in prompts could affect this consistency" as a potential concern
- Why unresolved: The experiments only tested on current GPT-4o implementation, with no longitudinal study of how model updates might affect scoring consistency
- What evidence would resolve it: Repeated testing of G-VEval on the same datasets after GPT-4o updates, tracking changes in correlation scores and variance

### Open Question 3
- Question: How does G-VEval's cost compare to other metrics when scaled to large-scale video captioning evaluation?
- Basis in paper: [explicit] The paper notes "One major drawback is its cost. While GPT-4o is relatively more affordable than some alternatives, it remains more expensive than other metrics due to the token-based pricing model"
- Why unresolved: The paper doesn't provide quantitative cost comparisons for large-scale evaluation scenarios or cost-benefit analysis against alternatives
- What evidence would resolve it: Detailed cost analysis comparing G-VEval to other metrics across different dataset sizes, including time and monetary costs for evaluation tasks

## Limitations
- Performance may not generalize to datasets beyond those tested
- High computational costs due to GPT-4o API pricing
- Potential instability as GPT-4o model updates occur

## Confidence

**High Confidence:**
- The three evaluation modes (reference-free, reference-only, combined) are clearly defined and implemented
- Correlation results with human judgments on established datasets are reproducible given GPT-4o API access
- FOIL hallucination detection results demonstrate robustness against caption-image mismatches

**Medium Confidence:**
- Superiority over existing metrics may depend on specific dataset characteristics
- Expected score calculation improvement needs more extensive validation
- ACCR framework's effectiveness in reducing human evaluation variance is promising but requires broader testing

**Low Confidence:**
- Generalization to other video captioning datasets beyond MSVD-Eval
- Long-term stability as GPT-4o model updates occur
- Cost-effectiveness for practical deployment

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary the chain-of-thought prompting structure while keeping evaluation criteria constant to quantify the contribution of CoT reasoning to performance gains.

2. **Cross-Model Validation**: Implement G-VEval using alternative multimodal models (e.g., Gemini Pro Vision, Claude 3 with vision) to test whether the evaluation methodology generalizes beyond GPT-4o.

3. **Long-Form Video Evaluation**: Extend the ACCR framework to evaluate longer video sequences (beyond the 10-second clips used in MSVD-Eval) to assess scalability and whether the four-dimensional approach maintains effectiveness for extended temporal content.