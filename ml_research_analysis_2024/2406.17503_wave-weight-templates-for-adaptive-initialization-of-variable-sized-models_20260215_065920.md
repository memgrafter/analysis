---
ver: rpa2
title: 'WAVE: Weight Templates for Adaptive Initialization of Variable-sized Models'
arxiv_id: '2406.17503'
source_url: https://arxiv.org/abs/2406.17503
tags:
- weight
- knowledge
- templates
- initialization
- learngenes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes WAVE, a novel approach that reformulates variable-sized
  model initialization from a multi-task perspective, where initializing each model
  size is treated as a distinct task. WAVE employs shared, size-agnostic weight templates
  alongside size-specific weight scalers to achieve consistent initialization across
  various model sizes.
---

# WAVE: Weight Templates for Adaptive Initialization of Variable-sized Models

## Quick Facts
- arXiv ID: 2406.17503
- Source URL: https://arxiv.org/abs/2406.17503
- Authors: Fu Feng; Yucheng Xie; Jing Wang; Xin Geng
- Reference count: 6
- One-line primary result: Achieves state-of-the-art performance in initializing variable-sized models through shared weight templates and lightweight weight scalers.

## Executive Summary
WAVE proposes a novel approach to variable-sized model initialization by treating each model size as a distinct task within a multi-task learning framework. The method employs shared, size-agnostic weight templates constructed through Learngene-style distillation, combined with lightweight weight scalers to adapt these templates to specific model architectures. This approach enables efficient initialization of models with varying depths and widths while maintaining strong performance across diverse downstream tasks.

## Method Summary
WAVE introduces a knowledge condensation process where pre-trained models are distilled into compact weight templates using Kronecker-based rules within the Learngene framework. These templates, combined with size-specific weight scalers, enable initialization of target models of arbitrary dimensions. The method treats initialization of each model size as a distinct task, allowing shared templates to generalize across different architectures. Knowledge condensation is performed via an auxiliary model constrained by weight templates, followed by a lightweight adaptation phase where only the weight scalers are trained on minimal data.

## Key Results
- Achieves state-of-the-art performance in initializing models of various depths and widths
- Weight templates demonstrate task-agnostic properties, enabling transfer across diverse downstream datasets
- Knowledge condensation process is computationally efficient and enables rapid model initialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared weight templates act as compact, structured representations of pre-trained model knowledge that can be efficiently reused across variable-sized architectures.
- Mechanism: Learngene-style distillation compresses ancestor model parameters into weight templates using Kronecker-based rules, which are then combined via lightweight weight scalers to reconstruct layer-wise weight matrices for target models.
- Core assumption: Structural patterns in pre-trained weight matrices are generalizable enough that a finite set of templates can reconstruct weights for diverse model sizes without significant performance loss.
- Evidence anchors:
  - [abstract]: "These weight templates, constructed within the Learngene framework, integrate knowledge from pre-trained models through a distillation process constrained by Kronecker-based rules."
  - [section]: "We reuse and weight a series of weight templates to construct corresponding weight matrices in the way of Layer-Wise Scaling Kernels (Liu et al. 2022), which is implemented by Kronecker Product."
  - [corpus]: Weak. No direct empirical support in neighbors; related works focus on quantization or initialization but not adaptive template reuse.
- Break condition: If weight templates are too small to capture essential correlations, or if the Kronecker expansion fails to preserve fine-grained feature interactions, performance will degrade sharply.

### Mechanism 2
- Claim: Knowledge condensation via distillation into templates is a one-time, computationally cheap process that enables efficient model initialization thereafter.
- Mechanism: An auxiliary model, constrained by weight templates, distills knowledge from ancestry models. The templates are trained using soft distillation loss and classification loss, and the auxiliary model is discarded after condensation.
- Core assumption: The distillation process successfully extracts the core transferable knowledge without overfitting to the specific architecture of the ancestry model.
- Evidence anchors:
  - [abstract]: "These weight templates, constructed within the Learngene framework, integrate knowledge from pre-trained models through a distillation process constrained by Kronecker-based rules."
  - [section]: "The learning process of weight templates adopts soft distillation loss and classification loss provided by the auxiliary model... the loss L is only used to update the parameters of the weight template and weight scalers."
  - [corpus]: Missing. No corpus support found; this is inferred from Learngene literature.
- Break condition: If the distillation step does not sufficiently generalize or the templates fail to capture task-agnostic patterns, downstream initialization performance will be poor.

### Mechanism 3
- Claim: Weight scalers provide a small, trainable parameter set that learns how to adaptively connect templates to target model dimensions.
- Mechanism: After template construction, target models are initialized by training only the weight scalers from a small amount of data. The templates remain frozen; the scalers learn the combination rules needed for the specific architecture.
- Core assumption: A small number of parameters in the weight scalers is sufficient to learn the adaptive connection rules without overfitting.
- Evidence anchors:
  - [abstract]: "Target models are then initialized by concatenating and weighting these templates, with adaptive connection rules established by lightweight weight scalers, whose parameters are learned from minimal training data."
  - [section]: "When initializing models of variable sizes, it is only necessary to initialize weight scalers with sizes tailored according to the target model size. The weight scalers then learn the connection rules of weight templates for the specific architecture model from a small amount of data."
  - [corpus]: Weak. Neighbors discuss quantization and parameter-efficient tuning but not this specific adaptive connection mechanism.
- Break condition: If the weight scalers are too small or the adaptation phase uses insufficient data, initialization quality will drop, especially for large model size changes.

## Foundational Learning

- Concept: Kronecker Product for structured matrix reconstruction
  - Why needed here: Enables efficient reuse of small templates to reconstruct large weight matrices without full parameter storage.
  - Quick check question: How does the Kronecker product between a template and scaler reconstruct a full weight matrix in terms of shape and rank?
- Concept: Knowledge distillation for model compression
  - Why needed here: Allows condensing a large pre-trained model into a smaller set of templates while preserving core knowledge.
  - Quick check question: What loss terms are used to ensure the distilled templates retain both task-specific and structural knowledge?
- Concept: Multi-task learning perspective for model initialization
  - Why needed here: Treats initialization of each model size as a distinct task, enabling shared templates to generalize across sizes.
  - Quick check question: How does viewing model initialization as a multi-task problem improve generalization compared to size-specific initialization?

## Architecture Onboarding

- Component map: Auxiliary Model -> Weight Templates (T) -> Weight Scalers (S) -> Target Model
- Critical path:
  1. Condense ancestry model knowledge into weight templates via distillation (150 epochs)
  2. For each target model, initialize appropriate weight scalers based on size
  3. Train scalers on minimal data to learn adaptive connection rules
  4. Reconstruct target model weights using templates and trained scalers
- Design tradeoffs:
  - Template size vs. flexibility: Smaller templates allow more adaptable expansions but risk losing structured knowledge; larger templates preserve more structure but reduce flexibility
  - Number of templates vs. storage: More templates improve coverage but increase storage cost; fewer templates reduce footprint but may generalize poorly
  - Distillation duration vs. quality: Longer distillation may yield better templates but increases upfront cost; shorter may be insufficient for robust knowledge extraction
- Failure signatures:
  - Poor initialization performance: Templates may be too small or distilled poorly
  - Slow or unstable adaptation: Weight scalers may be too few or data too limited
  - Inconsistent behavior across model sizes: Template design may not generalize well across depth/width ranges
- First 3 experiments:
  1. Ablation: Train a small DeiT model using templates with only MSA weights vs. full model (templates + MLPs) to assess contribution of each component
  2. Scalability: Initialize models of increasing depth/width using fixed templates to measure performance drop-off and identify size limits
  3. Transferability: Initialize the same target model architecture using templates distilled from different ancestry models to measure task-agnostic effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the weight templates (t1 and t2) affect the flexibility and efficiency of the WAVE method in initializing variable-sized models?
- Basis in paper: [explicit] The paper mentions that "t1 and t2 determine the size of the weight templates, which is manually set and is of great significance for the flexibility of weight templates when expanding weight matrix and the extraction of structural common knowledge."
- Why unresolved: The paper does not provide empirical evidence or a detailed analysis of how varying the size of weight templates impacts the performance and efficiency of the WAVE method.
- What evidence would resolve it: Conducting experiments with different sizes of weight templates and analyzing the trade-offs between flexibility, computational efficiency, and model performance would provide insights into the optimal size of weight templates.

### Open Question 2
- Question: Can the WAVE method be extended to other types of neural network architectures beyond Vision Transformers, such as Convolutional Neural Networks or Recurrent Neural Networks?
- Basis in paper: [inferred] The paper focuses on Vision Transformers and does not explore the applicability of the WAVE method to other neural network architectures.
- Why unresolved: The paper does not provide any analysis or experiments to determine whether the principles and techniques used in WAVE can be generalized to other types of neural networks.
- What evidence would resolve it: Applying the WAVE method to different neural network architectures and evaluating its effectiveness in initializing models of various sizes would demonstrate its generalizability and potential for broader application.

### Open Question 3
- Question: How does the knowledge condensation process in WAVE compare to other knowledge distillation techniques in terms of preserving and transferring structured knowledge?
- Basis in paper: [explicit] The paper describes the knowledge condensation process in WAVE, which uses knowledge distillation to integrate knowledge from pre-trained models into weight templates.
- Why unresolved: The paper does not provide a comparative analysis of the knowledge condensation process in WAVE with other knowledge distillation techniques, such as traditional knowledge distillation or contrastive learning-based methods.
- What evidence would resolve it: Conducting experiments to compare the effectiveness of the knowledge condensation process in WAVE with other knowledge distillation techniques in terms of preserving and transferring structured knowledge would provide insights into the relative strengths and weaknesses of each approach.

## Limitations

- Knowledge condensation requires significant upfront computational cost (150 epochs on ImageNet-1K), which may offset initialization efficiency gains
- Method heavily depends on quality of initial weight templates and auxiliary model's distillation capability
- Limited exploration of applicability beyond Vision Transformers, leaving architectural generalization uncertain

## Confidence

- Mechanism 1: Medium - Empirical results are compelling but lack direct comparative studies with alternative template-based approaches
- Mechanism 2: Low - Knowledge condensation process is described but not empirically validated; relies on Learngene literature
- Mechanism 3: Medium - Weight scalers are lightweight and learnable, but sensitivity to template size and data quantity is not thoroughly explored

## Next Checks

1. **Ablation Study**: Evaluate initialization performance when using templates distilled from models trained on different tasks to quantify task-agnostic effectiveness.

2. **Template Size Sensitivity**: Systematically vary the number and dimensions of weight templates to determine the minimum viable configuration that maintains performance.

3. **Generalization Boundaries**: Test initialization across architectures beyond Vision Transformers (e.g., MLP-Mixers, ConvNets) to assess the method's architectural flexibility.