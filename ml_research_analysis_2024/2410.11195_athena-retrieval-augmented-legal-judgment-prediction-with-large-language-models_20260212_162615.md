---
ver: rpa2
title: 'Athena: Retrieval-augmented Legal Judgment Prediction with Large Language
  Models'
arxiv_id: '2410.11195'
source_url: https://arxiv.org/abs/2410.11195
tags:
- legal
- uni00000013
- case
- judgment
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Athena is a retrieval-augmented legal judgment prediction framework
  that leverages large language models to predict legal outcomes. It constructs a
  knowledge base of accusations with semantic retrieval capabilities, allowing LLMs
  to access relevant legal context during inference.
---

# Athena: Retrieval-augmented Legal Judgment Prediction with Large Language Models

## Quick Facts
- arXiv ID: 2410.11195
- Source URL: https://arxiv.org/abs/2410.11195
- Authors: Xiao Peng; Liang Chen
- Reference count: 7
- Primary result: Athena achieves up to 95% accuracy on legal judgment prediction by retrieving accusation candidates with semantic similarity

## Executive Summary
Athena is a retrieval-augmented framework that enhances legal judgment prediction by leveraging large language models with external knowledge bases. The framework constructs a semantic knowledge base of accusations and retrieves top-k relevant candidates based on similarity with legal case descriptions. By providing domain-specific context during inference, Athena addresses LLM hallucinations and limited legal domain knowledge. Experimental results on the CAIL2018 dataset demonstrate significant performance improvements across multiple model sizes, with optimal performance achieved through careful tuning of retrieval parameters and prompt engineering.

## Method Summary
Athena implements a retrieval-augmented generation approach for legal judgment prediction by constructing a knowledge base of accusation descriptions and retrieving semantically similar candidates during inference. The framework vectorizes legal case descriptions and accusation examples using embeddings, stores them in a vector database, and performs semantic similarity search to retrieve top-k accusation candidates. These candidates are incorporated into LLM prompts alongside legal syllogism instructions and query-rewritten descriptions. The system processes legal case inputs through a semantic retrieval module, assembles prompts with retrieved context, runs inference on LLMs (GPT-3.5-turbo, GPT-4-turbo, or GPT-4o), and parses structured outputs for final judgment prediction.

## Key Results
- Athena achieves up to 95% accuracy on CAIL2018 dataset using GPT-4o with optimal parameter tuning
- Significant performance improvements across all tested LLM sizes compared to baseline approaches
- Identifies "lost-in-the-middle" phenomenon where performance initially increases then decreases with larger in-context window sizes
- Query rewriting improves retrieval hit rate by transforming accusation labels into legal case-like descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation (RAG) reduces LLM hallucinations by providing domain-specific context during inference
- Mechanism: Athena constructs a knowledge base of accusation descriptions and retrieves top-k semantically similar candidates based on vector similarity, which are then incorporated into the LLM's prompt to guide judgment prediction
- Core assumption: Legal case descriptions contain sufficient semantic overlap with accusation descriptions to enable meaningful retrieval
- Evidence anchors:
  - [abstract] "RAG utilizes an external knowledge base to accurately provide contextual information for LLM to infer with much less hallucinations"
  - [section 3.3] "By assessing the semantic similarity between the query and descriptions in the knowledge base, the top-k accusation candidates are retrieved"
  - [corpus] Weak - related works focus on RAG in legal contexts but don't specifically validate hallucination reduction
- Break condition: When legal case descriptions lack sufficient semantic similarity to accusation descriptions, retrieval becomes ineffective

### Mechanism 2
- Claim: Query rewriting improves retrieval performance by transforming accusation labels into legal case-like descriptions
- Mechanism: Athena uses LLMs to generate descriptive examples for each accusation label, which are then vectorized and stored in the knowledge base to improve semantic matching
- Core assumption: LLM-generated descriptions capture the essential characteristics of accusations in a format that matches legal case descriptions
- Evidence anchors:
  - [section 3.3] "Each accusation label is fed into the LLM to generate a description and a corresponding legal case example"
  - [section 4.3.2] "Following the query rewriting procedure, the average Hit Rate improves significantly"
  - [corpus] Weak - related works mention retrieval but don't specifically address query rewriting effectiveness
- Break condition: When LLM-generated descriptions fail to capture accusation semantics or when legal experts don't refine the generated content

### Mechanism 3
- Claim: There exists an optimal in-context window size that balances retrieval effectiveness against "lost-in-the-middle" phenomenon
- Mechanism: Athena's ablation study reveals that LLM performance initially improves with larger context windows (more retrieved candidates) but then degrades due to attention limitations
- Core assumption: LLMs have limited effective context comprehension that varies with model size
- Evidence anchors:
  - [section 4.3.1] "with an increase in the in-context window size k, the performance of Athena initially increases, then declines"
  - [section 4.3.1] "GPT-3.5-turbo reaches its performance plateau at the Top-16 earlier than GPT-4o"
  - [corpus] Weak - related works discuss long-context limitations but not specifically for legal judgment prediction
- Break condition: When model architecture changes significantly or when different attention mechanisms are used

## Foundational Learning

- Concept: Vector embeddings and semantic similarity
  - Why needed here: Athena relies on vectorization of legal case descriptions and accusation examples to enable semantic retrieval
  - Quick check question: How does cosine similarity between vector embeddings determine the relevance of retrieved accusations?

- Concept: Prompt engineering and in-context learning
  - Why needed here: Athena's effectiveness depends on properly structuring prompts that incorporate retrieved candidates and instructions for the LLM
  - Quick check question: What elements should be included in Athena's prompt template to maximize LLM performance?

- Concept: Data distribution and class imbalance
  - Why needed here: The CAIL2018 dataset exhibits severe imbalance, affecting Athena's accuracy across different accusation types
  - Quick check question: How does class imbalance in the dataset affect the weighted versus average accuracy metrics?

## Architecture Onboarding

- Component map: Legal case analyzer → Knowledge base construction (deduplication → query rewriting → vectorization) → Vector database → Retrieval module (semantic similarity search) → Prompt template → LLM inference → Structured output parser
- Critical path: Legal case → Semantic retrieval → Prompt assembly → LLM inference → Judgment prediction
- Design tradeoffs: Larger k values increase retrieval accuracy but risk "lost-in-the-middle" degradation; query rewriting improves retrieval but adds LLM dependency; knowledge base construction adds latency but improves accuracy
- Failure signatures: Low Hit Rate indicates poor semantic matching; accuracy plateau suggests "lost-in-the-middle" effect; significant gap between average and weighted accuracy indicates class imbalance issues
- First 3 experiments:
  1. Test retrieval effectiveness with different k values on a small subset to observe "lost-in-the-middle" phenomenon
  2. Compare query rewriting versus original descriptions on retrieval Hit Rate
  3. Evaluate baseline LLM performance versus Athena with varying model sizes to quantify RAG impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal in-context window size k for Athena across different LLM capabilities, and how does this vary with task complexity?
- Basis in paper: [explicit] The ablation study on in-context window size shows that performance follows a "lost-in-the-middle" pattern where accuracy initially increases then decreases with larger window sizes, with different optimal sizes for GPT-3.5-turbo (Top-16) versus GPT-4o (Top-32).
- Why unresolved: The study only tested a limited range of k values and focused on a single dataset. The relationship between window size, model capacity, and task complexity needs more systematic investigation across diverse legal scenarios.
- What evidence would resolve it: A comprehensive study testing multiple k values across different model sizes and legal task complexities, measuring both retrieval hit rates and final prediction accuracy.

### Open Question 2
- Question: How effective are alternative query rewriting strategies compared to the current LLM-generated descriptions for improving retrieval performance?
- Basis in paper: [explicit] The query rewriting ablation study showed significant improvement over original descriptions, but only tested one rewriting approach using LLM-generated descriptions.
- Why unresolved: The paper only explored a single query rewriting method. Different rewriting strategies (legal expert-crafted, template-based, or multi-stage rewriting) could potentially yield better retrieval performance.
- What evidence would resolve it: Comparative experiments testing multiple query rewriting approaches on the same retrieval tasks, measuring hit rates and downstream prediction accuracy.

### Open Question 3
- Question: Can multi-accusation legal cases be effectively handled by extending Athena's single-accusation framework, and what architectural modifications would be needed?
- Basis in paper: [explicit] The data distribution analysis identified multi-accusation cases (e.g., "misappropriation" containing "misappropriation of public funds") as challenging scenarios, noting they might need an agentic workflow for proper handling.
- Why unresolved: The current framework treats legal judgment prediction as single-accusation classification. Multi-accusation cases require either sequential processing or simultaneous prediction of multiple labels, which is not addressed.
- What evidence would resolve it: Experiments testing Athena's performance on multi-accusation cases using either sequential multi-stage prompting or multi-label classification approaches, compared against ground truth multi-accusation labels.

## Limitations

- The framework's effectiveness depends heavily on semantic similarity between legal case descriptions and accusation labels, which may not capture all legal nuances
- Significant performance gap exists between different LLM models (75.8% for GPT-3.5-turbo versus 95% for GPT-4o), indicating substantial model dependency
- The "lost-in-the-middle" phenomenon represents a fundamental limitation in how LLMs process retrieved context, though the exact mechanisms remain unclear
- Class imbalance in the CAIL2018 dataset (68 accusation types) creates accuracy disparities between common and rare accusations

## Confidence

**High Confidence**: The effectiveness of retrieval-augmented generation in improving legal judgment prediction accuracy is well-supported by experimental results showing consistent improvements across multiple model sizes and configurations.

**Medium Confidence**: The "lost-in-the-middle" phenomenon and its relationship to in-context window size is observed but requires further investigation to determine whether it's specific to the CAIL2018 dataset or represents a general LLM limitation.

**Low Confidence**: The specific contribution of query rewriting versus other retrieval mechanisms cannot be isolated, as the study doesn't provide ablation tests comparing raw descriptions with rewritten ones.

## Next Checks

1. **Cross-dataset validation**: Test Athena's performance on legal judgment prediction datasets from different jurisdictions (e.g., European, American) to verify whether the retrieval effectiveness generalizes beyond the Chinese legal system used in CAIL2018.

2. **Retrieval quality isolation**: Conduct controlled experiments comparing retrieval effectiveness when using original accusation labels versus query-rewritten descriptions, while keeping all other components constant, to quantify the specific contribution of the query rewriting mechanism.

3. **Attention mechanism ablation**: Test Athena with different LLM architectures that have varying attention mechanisms (e.g., sliding window attention, hierarchical attention) to determine whether the "lost-in-the-middle" phenomenon is model-specific or represents a fundamental limitation in how LLMs process retrieved context.