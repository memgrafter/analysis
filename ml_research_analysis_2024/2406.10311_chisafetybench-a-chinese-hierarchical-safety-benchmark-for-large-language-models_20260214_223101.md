---
ver: rpa2
title: 'CHiSafetyBench: A Chinese Hierarchical Safety Benchmark for Large Language
  Models'
arxiv_id: '2406.10311'
source_url: https://arxiv.org/abs/2406.10311
tags:
- safety
- chinese
- evaluation
- llms
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CHiSafetyBench, a Chinese hierarchical safety
  benchmark for evaluating large language models (LLMs) in identifying risky content
  and refusing risky questions. The benchmark features a hierarchical safety taxonomy
  covering 5 risk areas and 31 categories, with 1,861 multiple-choice questions and
  462 risky questions.
---

# CHiSafetyBench: A Chinese Hierarchical Safety Benchmark for Large Language Models
## Quick Facts
- arXiv ID: 2406.10311
- Source URL: https://arxiv.org/abs/2406.10311
- Reference count: 26
- Key outcome: Chinese hierarchical safety benchmark with 1,861 MCQ and 462 risky questions, showing Qwen series highest accuracy (88.39%) but all models need safety improvement

## Executive Summary
CHiSafetyBench introduces a comprehensive Chinese safety benchmark for evaluating large language models across five risk areas and 31 categories. The benchmark employs a dual-task approach with multiple-choice questions for risk identification and question-answering tasks for refusal capabilities. An automatic LLM-based evaluator demonstrates strong correlation with human judgment (Kendall Tau=0.815, Spearman=0.842, Pearson=0.825), enabling efficient safety assessment. Testing of 12 state-of-the-art Chinese LLMs reveals varying performance, with the Qwen series achieving the highest risk identification accuracy while all models show significant room for improvement in refusal capabilities.

## Method Summary
The benchmark operationalizes the Chinese government's "Basic Security Requirements for Generative AI" standard into a two-level hierarchical taxonomy covering 5 risk areas and 31 categories. The dataset comprises 1,861 multiple-choice questions and 462 risky questions collected through manual generation, web scraping, and existing benchmarks. An automatic evaluation method using Qwen-72B as evaluator classifies responses into four refusal levels, validated against human judgment on 545 samples. The evaluation measures accuracy for risk identification and rejection rate, responsibility rate, and harm rate for refusal capabilities across 12 SOTA Chinese LLMs.

## Key Results
- Qwen series achieved highest risk identification accuracy at 88.39%
- All tested models showed significant room for improvement in refusal capabilities, with highest rejection rate at 77.71%
- Automatic evaluation method validated with strong correlation to human judgment (Kendall Tau=0.815, Spearman=0.842, Pearson=0.825)
- Non-positive correlation observed between model size and risk content identification capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical taxonomy mapping to dataset design enables fine-grained safety assessment across 5 risk areas and 31 categories.
- Mechanism: The Chinese government's "Basic Security Requirements for Generative AI" standard is operationalized into a two-level taxonomy. Each risk area (e.g., Discrimination, Violation of Values) is subdivided into categories (e.g., ethnic discrimination, inciting subversion). This structure ensures every dataset item is mapped to a specific safety concern.
- Core assumption: The official taxonomy comprehensively captures all relevant Chinese safety risks and that mapping data to categories preserves fidelity.
- Evidence anchors:
  - [abstract]: "CHiSafetyBench incorporates a dataset that covers a hierarchical Chinese safety taxonomy consisting of 5 risk areas and 31 categories."
  - [section]: "Utilizing the hierarchical safety taxonomy outlined in the 'Basic Security Requirements for Generative Artificial Intelligence Service' standard issued by the Chinese government..."
  - [corpus]: Weak; no neighbor papers directly validate the completeness of the taxonomy mapping.
- Break condition: If real-world safety risks in Chinese contexts evolve faster than the taxonomy, or if category definitions overlap or conflict, the mapping will misclassify data and reduce assessment validity.

### Mechanism 2
- Claim: Dual-task format (MCQ + QA) provides complementary coverage of risk identification and refusal capabilities.
- Mechanism: MCQ tasks measure whether the model can correctly label content as risky; QA tasks measure whether the model can refuse to answer risky prompts. This separation ensures both perception and behavioral safety are tested.
- Core assumption: Risk identification (perception) and refusal (behavior) are distinct skills; both are necessary for full safety evaluation.
- Evidence anchors:
  - [abstract]: "This dataset comprises two types of tasks: multiple-choice questions and question-answering, evaluating LLMs from the perspectives of risk content identification and the ability to refuse answering risky questions respectively."
  - [section]: "The MCQ data primarily focuses on risk content identification, while the QA data consists of risky questions that must be refused."
  - [corpus]: Weak; no neighbor benchmarks explicitly validate the complementary coverage claim.
- Break condition: If a model can identify risks but still answers risky questions, or vice versa, the dual-task format will expose the gap; however, if the tasks are too similar, redundancy may occur.

### Mechanism 3
- Claim: Automatic LLM-based evaluator substitutes for human judgment with high correlation (Kendall Tau=0.815, Spearman=0.842, Pearson=0.825).
- Mechanism: The Qwen-72B evaluator classifies responses into four refusal levels (Non-refusal with Risky, Non-refusal with Harmless, Direct Refusal, Refusal with Responsible Guidance). Numerical scores enable automated rejection and responsibility metrics.
- Core assumption: The evaluator's prompt design and mapping table accurately reflect nuanced human safety judgments.
- Evidence anchors:
  - [abstract]: "Utilizing this benchmark, we validate the feasibility of automatic evaluation as a substitute for human evaluation..."
  - [section]: "The correlation between Qwen's evaluation and human evaluation are found to be Kendall Tau=0.815, Spearman=0.842, Pearson=0.825..."
  - [corpus]: Weak; no neighbor works provide independent replication of the automatic evaluator's effectiveness.
- Break condition: If the evaluator's prompt template cannot capture subtle safety nuances, or if model outputs diverge from training distribution, correlation with human judgment will drop.

## Foundational Learning

- Concept: Hierarchical taxonomy design and mapping to evaluation tasks
  - Why needed here: Ensures every safety concern is explicitly represented in the benchmark and can be measured independently.
  - Quick check question: Can you explain how a two-level taxonomy (5 areas Ã— 31 categories) improves granularity compared to a flat list of risks?

- Concept: Dual-task evaluation (identification vs. refusal)
  - Why needed here: Separates perception (knowing what is risky) from behavior (acting safely), revealing model weaknesses in each dimension.
  - Quick check question: If a model scores high on MCQ but low on QA refusal, what safety gap does that indicate?

- Concept: Automated evaluator design and correlation validation
  - Why needed here: Enables scalable, repeatable safety testing without manual annotation overhead.
  - Quick check question: How would you test whether the evaluator's refusal-level mapping remains valid if new model architectures are introduced?

## Architecture Onboarding

- Component map:
  Dataset builder -> Taxonomy mapper -> MCQ generator + QA generator -> Automatic evaluator (Qwen-72B) -> Metrics calculator -> Results dashboard
- Critical path:
  1. Load taxonomy -> 2. Generate / collect MCQ and QA items -> 3. Deduplicate -> 4. Feed items to target LLM -> 5. Capture responses -> 6. Run evaluator -> 7. Compute metrics -> 8. Report
- Design tradeoffs:
  - Taxonomy granularity vs. dataset size: More categories require more data per category.
  - Human vs. automatic evaluation: Human is accurate but slow; automatic is fast but needs validation.
  - Evaluator choice: Open-source LLM ensures reproducibility but may inherit its own biases.
- Failure signatures:
  - Low MCQ accuracy across all models -> taxonomy or dataset quality issue.
  - High variance in evaluator correlation across risk areas -> evaluator prompt may not generalize.
  - Rejection rates clustered near zero -> evaluator prompt may be too strict or data not truly risky.
- First 3 experiments:
  1. Run a small sample (e.g., 50 MCQ + 20 QA) through the pipeline to verify metric calculations.
  2. Compare evaluator output vs. manual judgment on a random subset to confirm correlation holds locally.
  3. Test whether adding a new risk category (e.g., "AI-generated misinformation") breaks the evaluator's refusal mapping.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs in Chinese safety benchmarks compare to their performance in English benchmarks when using equivalent evaluation methodologies?
- Basis in paper: [explicit] The paper notes that "The research mentioned above has been conducted within the English-speaking community, while the evaluation of Chinese LLMs remains largely unexplored and lags behind" and that "there are few Chinese safety benchmarks available"
- Why unresolved: The paper only evaluates Chinese LLMs on CHiSafetyBench but does not compare their performance to English LLMs on equivalent English benchmarks, nor does it examine whether safety taxonomies and evaluation methods are truly equivalent across languages.
- What evidence would resolve it: Direct comparative studies using the same evaluation methodologies (like automatic evaluation methods) on equivalent English and Chinese safety benchmarks, with performance metrics reported side-by-side.

### Open Question 2
- Question: What is the optimal size of Chinese LLMs for safety performance, given that larger models don't necessarily achieve better results in risk content identification?
- Basis in paper: [explicit] "The Qwen series and Yi series have optimal models with sizes of 14B and 6B, respectively, while the maximum sizes for these series are 110B and 34B. This indicates a non-positive correlation between model size and risk content identification capabilities."
- Why unresolved: The paper observes a non-positive correlation but doesn't identify the optimal model size range for safety performance, nor does it explain the mechanisms behind this counterintuitive finding.
- What evidence would resolve it: Systematic testing across multiple model sizes within each series to identify the size threshold where safety performance plateaus or declines, combined with analysis of how model capacity relates to safety-specific capabilities.

### Open Question 3
- Question: How do different automatic evaluation methods (keyword matching, fine-tuning classifiers, GPT API, human evaluation) compare in terms of accuracy, cost-effectiveness, and transferability for Chinese LLM safety assessment?
- Basis in paper: [explicit] The paper discusses various evaluation methods including "keyword matching," "fine-tuning a classifier," "GPT API usage," and "human evaluation," ultimately choosing an LLM-based automatic evaluation method
- Why unresolved: The paper validates the effectiveness of their chosen automatic evaluation method but doesn't provide a comparative analysis of alternative automatic evaluation approaches for Chinese safety assessment.
- What evidence would resolve it: Head-to-head comparison studies using multiple automatic evaluation methods on the same Chinese safety datasets, measuring correlation with human evaluations and analyzing computational costs and transferability across different safety domains.

## Limitations

- Automatic evaluator correlation validated on only 545 samples, which may not capture full complexity across all 31 categories
- Benchmark covers only Chinese language contexts and may not generalize to other languages or cultural settings
- Relatively low refusal rates across all tested models (highest at 77.71%) suggest benchmark may capture particularly challenging safety scenarios

## Confidence

- **High confidence**: The hierarchical taxonomy structure and dataset construction methodology are well-documented and align with official Chinese government standards. The benchmark's task design (MCQ + QA) provides clear separation between identification and refusal capabilities.
- **Medium confidence**: The automatic evaluation methodology shows good correlation with human judgment but requires independent replication. The performance rankings of different LLMs appear consistent but may shift with different evaluator models.
- **Low confidence**: The completeness of the 31-category taxonomy in capturing all relevant safety risks, and whether the refusal metrics (RR-1, RR-2, HR) fully capture the nuances of responsible AI behavior.

## Next Checks

1. **Evaluator generalizability test**: Validate the automatic evaluator's correlation with human judgment on an independent dataset (minimum 500 samples) across all 31 categories to ensure the correlation coefficients are robust and not dataset-specific.

2. **Cross-lingual safety assessment**: Test whether models trained on CHiSafetyBench maintain safety performance when evaluated on equivalent English-language safety benchmarks to assess cultural transferability of the safety mechanisms.

3. **Adversarial safety probing**: Generate adversarial variants of the 462 risky questions that preserve the underlying safety concern but use different linguistic formulations to test whether models' refusal capabilities generalize beyond the specific phrasings in the benchmark.