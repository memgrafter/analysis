---
ver: rpa2
title: 'MSP-Podcast SER Challenge 2024: L''antenne du Ventoux Multimodal Self-Supervised
  Learning for Speech Emotion Recognition'
arxiv_id: '2407.05746'
source_url: https://arxiv.org/abs/2407.05746
tags:
- speech
- emotion
- text
- system
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the MSP-Podcast SER Challenge 2024 submission
  focusing on Task 1: Categorical Emotion Recognition from speech. The system employs
  an ensemble of five subsystems, each using different encoder combinations (WavLM,
  HuBERT, RoBERTa) and training strategies, including self-supervised learning fine-tuning
  across speech and text modalities.'
---

# MSP-Podcast SER Challenge 2024: L'antenne du Ventoux Multimodal Self-Supervised Learning for Speech Emotion Recognition

## Quick Facts
- **arXiv ID:** 2407.05746
- **Source URL:** https://arxiv.org/abs/2407.05746
- **Reference count:** 0
- **Primary result:** 0.35% F1-macro score on development set for categorical emotion recognition

## Executive Summary
This paper presents a submission to the MSP-Podcast SER Challenge 2024 focusing on Task 1: Categorical Emotion Recognition from speech. The system employs an ensemble approach combining five subsystems that leverage different combinations of WavLM, HuBERT, and RoBERTa encoders with self-supervised learning fine-tuning across speech and text modalities. The subsystems are fused at the score level using an SVM classifier. The approach achieved a modest 0.35% F1-macro score on the development set, highlighting both the challenge of emotion recognition from naturalistic speech data and the potential of multimodal fusion strategies.

## Method Summary
The proposed system consists of an ensemble of five subsystems, each using different encoder combinations (WavLM, HuBERT, RoBERTa) and training strategies. The subsystems employ self-supervised learning fine-tuning across both speech and text modalities. The final predictions are obtained through score-level fusion using an SVM classifier. The approach combines multiple pre-trained models to capture diverse feature representations from the speech data, with the ensemble strategy aimed at improving robustness and performance across the eight emotional states in the dataset.

## Key Results
- Achieved 0.35% F1-macro score on development set
- WavLM demonstrated effectiveness as speech encoder
- Multimodal fusion provided performance benefits
- Self-supervised pre-training showed potential for emotion recognition tasks

## Why This Works (Mechanism)
The ensemble approach works by combining complementary strengths of different pre-trained models (WavLM, HuBERT, RoBERTa) that capture distinct aspects of speech and text information. WavLM's masked speech prediction capability helps in understanding acoustic patterns associated with emotions, while HuBERT's hidden unit BERT approach provides robust speech representations. The RoBERTa component captures textual information that may correlate with emotional states. The SVM-based score fusion allows the system to learn optimal combination weights for the different subsystem outputs, potentially compensating for individual subsystem weaknesses.

## Foundational Learning
- **Self-supervised learning**: Pre-training models on unlabeled speech and text data without emotion labels, then fine-tuning for emotion recognition; needed to leverage large amounts of available unlabeled data and learn general speech representations before emotion-specific adaptation
- **Multimodal fusion**: Combining information from both speech (acoustic) and text (linguistic) modalities; required because emotions can be expressed through both acoustic features and linguistic content
- **Ensemble learning**: Combining multiple diverse models to improve overall performance; necessary to capture different aspects of emotion expression and reduce individual model biases
- **Score-level fusion with SVM**: Using a classifier to combine prediction scores from different models rather than averaging; needed to learn optimal combination strategy based on the relative strengths of each subsystem
- **Pre-trained encoders (WavLM, HuBERT, RoBERTa)**: Leveraging models pre-trained on large datasets for transfer learning; required to benefit from learned representations without needing massive labeled emotion datasets
- **Emotion categorization**: Classifying speech into discrete emotional categories; fundamental task requiring models to distinguish subtle acoustic and linguistic patterns associated with different emotions

## Architecture Onboarding

Component map: WavLM/HuBERT (speech) -> Feature extraction -> Subsystem -> SVM fusion -> Final prediction
WavLM/HuBERT (text) -> Feature extraction -> Subsystem -> SVM fusion -> Final prediction
RoBERTa (text) -> Feature extraction -> Subsystem -> SVM fusion -> Final prediction

Critical path: Raw audio/text input -> Pre-trained encoder fine-tuning -> Feature extraction -> Subsystem prediction -> SVM score fusion -> Final emotion classification

Design tradeoffs: The system trades model complexity (five subsystems) for potential performance gains through diversity. Using pre-trained models reduces training time but limits architectural flexibility. Score-level fusion is simpler than feature-level fusion but may miss some cross-modal interactions.

Failure signatures: Poor performance on specific emotions suggests encoder limitations. Low diversity between subsystems indicates insufficient architectural differences. SVM fusion failure points to score distribution issues or poor subsystem calibration.

First experiments: 1) Train each subsystem individually to establish baseline performance, 2) Test different fusion strategies (weighted averaging vs SVM), 3) Evaluate subsystem performance on individual emotion classes to identify weaknesses

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Low performance (0.35% F1-macro) indicates challenges with the naturalistic speech data
- Heavy reliance on pre-trained models without ablation studies to quantify individual contributions
- Self-supervised learning component lacks detailed training procedure descriptions
- SVM score-level fusion may not optimally combine diverse feature representations
- No analysis of potential biases in the dataset or robustness across different recording conditions

## Confidence
- **Medium** confidence in core methodology due to use of established multimodal encoders and ensemble approaches, though limited performance metrics and lack of detailed implementation specifics reduce reproducibility confidence
- **Low** confidence in claims about self-supervised learning benefits due to insufficient empirical validation and comparison with supervised baselines
- **Medium** confidence in WavLM effectiveness assertion based on system design, but not rigorously validated against alternatives

## Next Checks
1. Conduct ablation studies to isolate the contribution of each encoder and training strategy to overall performance
2. Implement and test alternative fusion strategies beyond SVM score-level fusion, such as late fusion with learned weights or early fusion of multimodal features
3. Evaluate the system on additional emotion recognition benchmarks to assess generalizability beyond the MSP-Podcast dataset