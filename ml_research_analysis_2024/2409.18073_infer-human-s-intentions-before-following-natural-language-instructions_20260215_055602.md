---
ver: rpa2
title: Infer Human's Intentions Before Following Natural Language Instructions
arxiv_id: '2409.18073'
source_url: https://arxiv.org/abs/2409.18073
tags:
- human
- reasoning
- goal
- actions
- robot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a framework (FISER) for following natural language
  instructions in collaborative embodied tasks by explicitly inferring human intentions
  as intermediate reasoning steps. The method separates social reasoning (inferring
  human goals and robot tasks) from embodied reasoning (grounded planning and action
  execution), using a Transformer-based model with step-wise supervision.
---

# Infer Human's Intentions Before Following Natural Language Instructions

## Quick Facts
- **arXiv ID:** 2409.18073
- **Source URL:** https://arxiv.org/abs/2409.18073
- **Authors:** Yanming Wan; Yue Wu; Yiping Wang; Jiayuan Mao; Natasha Jaques
- **Reference count:** 40
- **Primary result:** FISER achieves 64.5% success rate on HandMeThat benchmark, outperforming end-to-end approaches and LLM Chain-of-Thought prompting

## Executive Summary
This paper addresses the challenge of following ambiguous natural language instructions in collaborative embodied tasks by proposing FISER, a framework that explicitly infers human intentions as intermediate reasoning steps. The key insight is that by separating social reasoning (inferring human goals and robot tasks) from embodied reasoning (grounded planning and action execution), the model can better disambiguate instructions using context from human trajectories. The framework demonstrates that explicitly modeling hidden variables representing human goals and intentions significantly improves performance compared to purely end-to-end approaches.

The method uses a Transformer-based model trained from scratch that predicts the robot's task and optionally the human's plan before making action plans. Evaluated on the HandMeThat benchmark, FISER achieves an average 64.5% success rate, outperforming both end-to-end approaches and sophisticated Chain-of-Thought prompting methods on large pre-trained language models. The results indicate that pretraining and domain-specific prompts are insufficient for LLMs to perform well on these tasks, while smaller, task-specific models excel.

## Method Summary
The method uses a Transformer-based model that processes four input streams: world state description, human trajectory, natural language instruction, and model's past outputs. These are processed through separate transformer layers and fused via an MLP-based modality interaction module. The model performs social reasoning to infer the human's plan (Gh) and the robot's task (Gr), then uses embodied reasoning to generate grounded actions. Training can be done end-to-end or in a multi-staged manner where social reasoning and embodied reasoning modules are trained separately. The model uses cross-entropy loss for predictions including plan recognition (QSVO), task recognition (object prediction), and action triples.

## Key Results
- FISER achieves 64.5% average success rate on HandMeThat benchmark, outperforming end-to-end approaches
- Small-scale models trained from scratch outperform Chain-of-Thought prompting on GPT-4 Turbo by approximately 70%
- Multi-staged training (separating social and embodied reasoning) performs better than end-to-end training
- The method shows particular strength on Level 4 tasks (inherently ambiguous) with 43.4% success rate compared to 35.3% for the end-to-end approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicitly modeling human intentions as intermediate reasoning steps improves performance on ambiguous instruction-following tasks compared to end-to-end approaches.
- **Mechanism:** The framework separates social reasoning (inferring human goals and robot tasks) from embodied reasoning (grounded planning and action execution). By predicting the robot's task (Gr) and optionally the human's plan (Gh) before planning actions, the model can disambiguate ambiguous instructions using context from human trajectories.
- **Core assumption:** Human intentions can be effectively inferred from observed historical actions and natural language instructions, and this inference improves downstream planning performance.
- **Evidence anchors:**
  - [abstract] "We empirically demonstrate that using social reasoning to explicitly infer human intentions before making action plans surpasses purely end-to-end approaches."
  - [section 3.2] "Our insight into this broad problem class is to leverage the causal relation between the human's behavior and their instruction by explicitly modeling hidden, unobserved variables representing the human's goals and intentions."

### Mechanism 2
- **Claim:** Training smaller Transformer models from scratch outperforms prompting large pre-trained LLMs for social and embodied reasoning tasks in household environments.
- **Mechanism:** Small-scale models trained specifically on the task domain learn representations better suited to the specific reasoning challenges, while large pre-trained models lack the necessary domain-specific knowledge and struggle with the long input sequences.
- **Core assumption:** Domain-specific training is more effective than leveraging general pre-trained knowledge for tasks requiring both social reasoning and embodied planning in household environments.
- **Evidence anchors:**
  - [abstract] "training small-scale models from scratch on this task outperforms our most sophisticated CoT prompting methods for large pre-trained LLMs, indicating that pretraining and domain-specific prompts are insufficient for LLMs to perform well"
  - [section 6] "Results show that training much smaller, more efficient Transformer-based models from scratch is exhibiting about 70% increased performance than prompting state-of-the-art pre-trained LLMs."

### Mechanism 3
- **Claim:** Multi-staged training (separating social and embodied reasoning modules) performs better than end-to-end training for this task.
- **Mechanism:** By training social reasoning and embodied reasoning as separate modules with distinct representations, the model avoids interference between the different types of reasoning required. The embodied reasoning module doesn't receive gradients from the social reasoning module, allowing it to learn task-specific representations.
- **Core assumption:** Social reasoning and embodied reasoning require sufficiently different representations that mixing them in a single end-to-end model hurts performance.
- **Evidence anchors:**
  - [section 5.2] "Results show that training in a multi-staged manner works better than end-to-end in our tasks. It may imply that the low-level grounded planning (embodied reasoning) is requiring a sufficiently different task representation from inferring human's internal goals (social reasoning)"

## Foundational Learning

- **Concept:** Causal reasoning and goal inference
  - Why needed here: The model must infer hidden human intentions from observed actions and language, which requires understanding causal relationships between goals, actions, and outcomes.
  - Quick check question: How would you design a model to infer that a person gathering books intends to store them, given only their actions of picking up books and placing them in a box?

- **Concept:** Natural language grounding to executable actions
  - Why needed here: The system must translate ambiguous natural language instructions into specific robot actions in a shared environment.
  - Quick check question: Given the instruction "Could you pass that from the sofa?" and a scene with multiple books and one coat on the sofa, how would you determine which object the human is referring to?

- **Concept:** Multi-modal representation learning
  - Why needed here: The model must integrate information from world states, human trajectories, natural language instructions, and past model outputs to make decisions.
  - Quick check question: How would you design a representation that can effectively combine object locations, human action history, and natural language descriptions for decision-making?

## Architecture Onboarding

- **Component map:** World description → Trajectory processing → Plan recognition (p*) → Task recognition (Gr) → Action prediction → Environment interaction
- **Critical path:** World description → Trajectory processing → Plan recognition (p*) → Task recognition (Gr) → Action prediction → Environment interaction
- **Design tradeoffs:**
  - End-to-end vs multi-staged training: Multi-staged avoids gradient interference but requires more complex training pipeline
  - Long vs short input sequences: Longer sequences provide more context but strain model capacity and computation
  - Abstract vs concrete goal representations: More abstract representations (predicates) allow generalization but may be harder to learn
- **Failure signatures:**
  - Planning Failure: Hallucination (going to wrong locations), missing steps, invalid actions
  - Redundant Behavior: Giving human objects already at target location or recently manipulated
  - Incorrect Intention: Misinterpreting human goals leading to wrong object selection
- **First 3 experiments:**
  1. Train Transformer+FISER model and measure success rate on Level 1 tasks (pure planning) vs Level 2 tasks (requires social reasoning)
  2. Compare multi-staged vs end-to-end training by measuring both success rate and intermediate prediction accuracy
  3. Test the impact of human plan recognition stage by training with and without PR on Level 4 tasks (inherently ambiguous)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the improvement from explicit intention inference depend on the complexity of the instruction or the size of the environment?
- **Basis in paper:** [inferred] The paper shows FISER outperforms end-to-end approaches and LLMs, but doesn't analyze performance variation across task complexity or environmental density.
- **Why unresolved:** The paper evaluates overall success rates but doesn't provide detailed breakdowns of performance across different task complexities or environmental sizes.
- **What evidence would resolve it:** Systematic experiments varying instruction ambiguity levels and object counts in the environment, with corresponding performance metrics.

### Open Question 2
- **Question:** How does the proposed human plan recognition stage affect sample efficiency during training?
- **Basis in paper:** [explicit] The paper mentions training in multi-staged vs end-to-end manners but doesn't analyze the sample efficiency or convergence speed differences between these approaches.
- **Why unresolved:** While the paper compares final performance, it doesn't report training curves or the number of samples required to reach specific performance thresholds.
- **What evidence would resolve it:** Training curves comparing sample efficiency of different model variants, and analysis of the number of episodes needed to reach specific performance levels.

### Open Question 3
- **Question:** What is the impact of using different predicate representations for human goals on the overall system performance?
- **Basis in paper:** [explicit] The paper mentions using a simplified form of predicates (Q, S, V, O) but doesn't explore how different predicate representations might affect performance.
- **Why unresolved:** The paper only evaluates one specific predicate representation scheme and doesn't investigate alternatives or their relative effectiveness.
- **What evidence would resolve it:** Comparative experiments using different predicate representation schemes (e.g., more complex logical forms) with corresponding performance evaluations.

## Limitations

- The evaluation focuses on a text-based environment, which may not capture all the complexities of real-world embodied interaction
- The performance gap between smaller task-specific models and large LLMs may not generalize to other domains or more general instruction-following tasks
- The method requires detailed human trajectory data, which may not always be available in real-world applications

## Confidence

- **High confidence:** The core claim that explicitly modeling human intentions as intermediate reasoning steps improves performance on ambiguous instruction-following tasks is well-supported by the empirical results across multiple difficulty levels.
- **Medium confidence:** The claim about multi-staged training outperforming end-to-end training is supported by ablation studies, but the exact reasons for this advantage (representation differences vs. training dynamics) remain somewhat speculative.
- **Medium confidence:** The superiority of smaller models over large LLMs is demonstrated within the specific benchmark, but the generalizability of this finding to other domains or more general instruction-following tasks requires further validation.

## Next Checks

1. **Domain Generalization Test:** Evaluate FISER on a different instruction-following benchmark (e.g., ALFRED or TEACh) to assess whether the advantages of explicit intention inference and smaller model training transfer to other environments.

2. **Ablation on Social Reasoning Components:** Conduct a more detailed ablation study isolating the contributions of human plan recognition (Gh) vs. robot task recognition (Gr) to identify which component provides the most benefit in ambiguous scenarios.

3. **Robustness to Trajectory Noise:** Test the model's performance when human trajectories contain errors or omissions to assess how sensitive the intention inference mechanism is to imperfect observation data.