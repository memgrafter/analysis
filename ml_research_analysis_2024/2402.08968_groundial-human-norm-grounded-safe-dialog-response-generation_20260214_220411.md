---
ver: rpa2
title: 'GrounDial: Human-norm Grounded Safe Dialog Response Generation'
arxiv_id: '2402.08968'
source_url: https://arxiv.org/abs/2402.08968
tags:
- response
- responses
- safety
- safe
- groundial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GrounDial tackles the problem of unsafe responses from LLM-based
  dialog systems by grounding responses to human social norms (Rules-of-Thumb) without
  requiring fine-tuning. It uses a hybrid approach combining in-context learning (ICL)
  and human-norm-guided decoding (HGD).
---

# GrounDial: Human-norm Grounded Safe Dialog Response Generation

## Quick Facts
- arXiv ID: 2402.08968
- Source URL: https://arxiv.org/abs/2402.08968
- Reference count: 5
- Primary result: Hybrid ICL+HGD approach achieves 0.7735 safety and 0.4638 agreement scores on ProsocialDialog without fine-tuning

## Executive Summary
GrounDial addresses unsafe responses from LLM-based dialog systems by grounding them to human social norms without requiring fine-tuning. It uses a hybrid approach combining in-context learning (ICL) and human-norm-guided decoding (HGD). ICL explicitly grounds responses by prepending retrieved Rules-of-Thumb (RoTs) to the context, while HGD steers token probabilities toward the RoT using reinforcement learning during decoding. Experiments with BlenderBot on the ProsocialDialog dataset show that GrounDial achieves strong safety and agreement scores, outperforming vanilla models and approaching fine-tuned performance without the cost and generalization limitations of retraining.

## Method Summary
GrounDial grounds LLM dialog responses to human social norms using a hybrid approach of in-context learning and human-norm-guided decoding. The method retrieves contextually relevant RoTs using MPNet embeddings and cosine similarity, then generates responses through two complementary mechanisms: ICL prepends the RoT to the input context, and HGD uses reinforcement learning to steer token probabilities during decoding toward the RoT distribution. The approach works with frozen LLMs like BlenderBot, avoiding fine-tuning costs while achieving strong safety and agreement performance.

## Key Results
- GrounDial achieves safety score of 0.7735 and agreement score of 0.4638 using retrieved RoTs
- Outperforms vanilla BlenderBot (0.6702 safety, 0.3200 agreement) on ProsocialDialog dataset
- Approaches fine-tuned BlenderBot performance (0.8722 safety, 0.3916 agreement) without additional training
- Ablation shows ICL improves agreement but not safety, while HGD improves safety but reduces agreement, demonstrating complementary effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GrounDial grounds responses to human social norms (Rules-of-Thumb) through in-context learning (ICL) by prepending retrieved RoTs to the input context.
- Mechanism: ICL explicitly instructs the LLM to generate responses that adhere to the prepended RoT, making the safety requirement clear during generation.
- Core assumption: LLMs can leverage in-context information effectively to guide response generation without requiring parameter updates.
- Evidence anchors:
  - [abstract] "It uses a hybrid approach combining in-context learning (ICL) and human-norm-guided decoding (HGD). ICL explicitly grounds responses by prepending retrieved RoTs to the context"
  - [section 2.2.2] "r∗ is appended in front of the original context; (r∗||x) is fed into f (·) instead of x"
  - [corpus] Weak evidence - no direct citation to ICL effectiveness in this specific safety context
- Break condition: If the LLM's capacity to process in-context information is insufficient, or if the prepended RoT is irrelevant or poorly retrieved, the grounding effect will be minimal.

### Mechanism 2
- Claim: Human-norm-guided decoding (HGD) implicitly steers token probabilities toward the RoT using reinforcement learning during decoding.
- Mechanism: HGD treats the LLM's probability distribution as a policy and updates it at each decoding step to approximate the RoT's distribution, using a reward function based on cross-entropy and KL divergence.
- Core assumption: Policy gradient methods can effectively adjust token probabilities during decoding to align with external guidance without catastrophic forgetting of the original task.
- Evidence anchors:
  - [abstract] "HGD steers token probabilities toward the RoT using reinforcement learning during decoding"
  - [section 2.2.3] "The policy at each step is updated to approximate the distribution of the retrieved RoT... RRL,t = CE(πt, π∗t ) − β · KL(πt||π∗t )"
  - [corpus] No direct evidence of reinforcement learning success in this specific application
- Break condition: If the reward function is poorly designed or the policy updates destabilize the original generation process, the decoding may produce incoherent or unsafe responses.

### Mechanism 3
- Claim: The hybrid approach of ICL and HGD achieves complementary effects, where ICL improves RoT relevance and HGD enhances safety.
- Mechanism: ICL ensures the response is grounded to the RoT (improving agreement), while HGD prevents the response from being too aligned with unsafe content (improving safety).
- Core assumption: ICL and HGD address different failure modes of safe response generation, and their combination covers both safety and relevance.
- Evidence anchors:
  - [abstract] "A hybrid approach of in-context learning and human-norm-guided decoding of GrounDial enables the response to be quantitatively and qualitatively safer"
  - [section 3.5] "ICL improves the agreement score but the safety score remains low... Conversely, HGD enhances safety but notably reduces the agreement score. High safety and agreement scores are attained only when both ICL and HGD are employed simultaneously"
  - [corpus] No corpus evidence for hybrid effectiveness
- Break condition: If either component fails to contribute its intended effect, the hybrid system may perform worse than using both components effectively.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: Enables grounding to RoTs without fine-tuning, leveraging the LLM's ability to follow instructions provided in the prompt.
  - Quick check question: How does prepending RoTs to the context influence the LLM's generation without changing its parameters?

- Concept: Reinforcement learning for decoding
  - Why needed here: Allows dynamic adjustment of token probabilities during generation to align with safety norms, beyond static in-context cues.
  - Quick check question: What role does the reward function (CE and KL divergence) play in updating the policy during HGD?

- Concept: Sentence embedding retrieval
  - Why needed here: Retrieves relevant RoTs based on user input similarity, ensuring the grounding is contextually appropriate.
  - Quick check question: How does cosine similarity between MPNet embeddings determine the relevance of retrieved RoTs?

## Architecture Onboarding

- Component map:
  User input -> MPNet encoding -> RoT retrieval (top-k) -> ICL prepending -> LLM input -> HGD policy updates -> final response

- Critical path:
  1. User input → MPNet encoding → RoT retrieval (top-k)
  2. RoTs + context → ICL prepending → LLM input
  3. LLM decoding with HGD policy updates → final response

- Design tradeoffs:
  - Using frozen LLM vs. fine-tuning: avoids retraining costs but may limit adaptation
  - Top-k RoT retrieval vs. single RoT: more context but risk of conflicting norms
  - HGD update iterations: more iterations may improve alignment but increase latency

- Failure signatures:
  - Poor RoT retrieval → off-topic or irrelevant responses
  - ICL only → high agreement but low safety
  - HGD only → high safety but low agreement
  - Both ICL and HGD ineffective → responses similar to vanilla LLM

- First 3 experiments:
  1. Ablation study: ICL only vs. HGD only vs. hybrid to quantify complementary effects
  2. RoT retrieval quality: Compare top-1, top-3, and top-5 retrieval performance on agreement scores
  3. HGD sensitivity: Vary β and update iterations to find optimal balance between safety and coherence

## Open Questions the Paper Calls Out

- Question: How effective is GrounDial when applied to larger, more advanced LLM-based dialog systems beyond BlenderBot?
  - Basis in paper: [inferred] The paper demonstrates GrounDial's effectiveness with BlenderBot but does not explore its performance on other, potentially more advanced models.
  - Why unresolved: The paper focuses on a single dialog system (BlenderBot) without exploring the method's applicability to other models.
  - What evidence would resolve it: Testing GrounDial on a variety of dialog systems, including larger and more advanced models, and comparing the results.

- Question: What are the long-term effects of GrounDial on user trust and engagement in dialog systems?
  - Basis in paper: [inferred] The paper focuses on immediate safety and agreement scores but does not address the potential long-term impact on user experience.
  - Why unresolved: The study is limited to short-term quantitative metrics and lacks user studies on trust and engagement over time.
  - What evidence would resolve it: Longitudinal user studies measuring trust and engagement with dialog systems using GrounDial over extended periods.

- Question: How does GrounDial handle ambiguous or context-dependent social norms that may not have clear RoTs?
  - Basis in paper: [explicit] The paper discusses the use of Rules-of-Thumb (RoTs) for grounding responses but does not address the challenge of ambiguous or context-dependent norms.
  - Why unresolved: The paper assumes clear and applicable RoTs for all contexts, which may not reflect real-world complexity.
  - What evidence would resolve it: Testing GrounDial in scenarios with ambiguous or context-dependent norms and evaluating its ability to generate appropriate responses.

## Limitations

- The effectiveness of MPNet-based RoT retrieval in consistently finding contextually relevant rules remains weakly validated
- Reinforcement learning during decoding may introduce artifacts like misspellings, suggesting potential instability
- Evaluation relies on ParlAI's built-in safety classifiers whose robustness to adversarial inputs or domain shifts is unclear

## Confidence

- **High confidence**: The hybrid approach (ICL + HGD) achieves complementary effects on safety and agreement, as shown by ablation experiments and quantitative results
- **Medium confidence**: MPNet-based RoT retrieval provides contextually relevant grounding, though retrieval quality and its impact on downstream performance are not deeply analyzed
- **Medium confidence**: Reinforcement learning during decoding effectively steers token probabilities toward RoTs without destabilizing generation, but artifacts suggest potential instability
- **Low confidence**: The safety and agreement classifiers used for evaluation are robust and generalize well, as their exact architecture and performance characteristics are not disclosed

## Next Checks

1. **Retrieval quality audit**: Analyze the distribution of cosine similarities between user contexts and retrieved RoTs to quantify how often top-3 RoTs are contextually relevant. Manually annotate a subset of retrievals to measure precision and recall of RoT relevance.

2. **Ablation on RoT count**: Compare performance when using top-1, top-3, and top-5 RoTs to determine if retrieving more rules improves agreement without harming safety, and assess if conflicting RoTs degrade performance.

3. **HGD stability test**: Systematically vary β (trust-region coefficient) and RL update iterations to find the stability-accuracy tradeoff. Measure hallucination rates, coherence scores, and safety gains to identify the optimal configuration and detect overfitting or instability.