---
ver: rpa2
title: 'Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5
  Benchmark'
arxiv_id: '2405.20574'
source_url: https://arxiv.org/abs/2405.20574
tags:
- leaderboard
- benchmark
- open
- evaluation
- ko-h5
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5
  Benchmark for evaluating Large Language Models (LLMs) in Korean. The Ko-H5 benchmark
  includes datasets derived from popular English benchmarks via machine and human
  translation, plus a new Korean-specific dataset, Ko-CommonGen v2.
---

# Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark

## Quick Facts
- arXiv ID: 2405.20574
- Source URL: https://arxiv.org/abs/2405.20574
- Reference count: 37
- Introduces the Open Ko-LLM Leaderboard and Ko-H5 Benchmark for evaluating Korean LLMs with private test sets to minimize data contamination

## Executive Summary
This paper presents the Open Ko-LLM Leaderboard and Ko-H5 Benchmark, a comprehensive evaluation framework for Large Language Models in Korean. The Ko-H5 benchmark includes Korean translations of popular English benchmarks (ARC, HellaSwag, MMLU, TruthfulQA) plus a new Korean-specific dataset, Ko-CommonGen v2. To ensure fair evaluation, the leaderboard uses private test sets that show minimal overlap with common training datasets. The work includes empirical analyses of performance trends across model sizes and types, demonstrating the importance of both benchmark diversity and contamination prevention.

## Method Summary
The Open Ko-LLM Leaderboard uses the Ko-H5 benchmark consisting of Korean-translated English datasets and a new Korean-specific task. English benchmarks were translated using GPT-4 with human review for cultural alignment and domain knowledge filtering. Private test sets are maintained to minimize data contamination, with overlap analysis showing low percentages against popular training datasets. Models are evaluated through a Hugging Face Space interface with automatic scoring across all tasks, and scores are aggregated into a Ko-H5 score representing average performance.

## Key Results
- Private test sets show minimal overlap with popular training datasets, ensuring fair evaluation
- Ko-CommonGen v2 provides orthogonal evaluation dimension with mid-level correlation to other tasks but low correlation with Ko-TruthfulQA
- Temporal analysis reveals stepwise performance improvements, with larger models showing more consistent gains
- Model size and type significantly influence performance improvement patterns over time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Private test sets minimize data contamination and enable fair model evaluation.
- Mechanism: The Ko-H5 benchmark datasets are kept private after human curation, reducing overlap with popular training datasets used by top-performing models.
- Core assumption: Data contamination from publicly available benchmarks significantly impacts model evaluation fairness.
- Evidence anchors:
  - [section] "We show that our private test sets have little overlap with some of the most popular training datasets used by top models in the Open Ko-LLM Leaderboard, empirically solidifying the argument for private test sets."
  - [corpus] "Found 25 related papers...Top related titles: Pitfalls of Evaluating Language Models with Open Benchmarks"
- Break condition: If training datasets expand to include more Korean-specific data, overlap with private test sets may increase despite current low percentages.

### Mechanism 2
- Claim: Adding orthogonal tasks like Ko-CommonGen v2 differentiates the leaderboard from English counterparts.
- Mechanism: Ko-CommonGen v2 has mid-level correlation with other tasks but low correlation with Ko-TruthfulQA, acting as a third evaluation axis.
- Core assumption: Common knowledge generation is a distinct capability from reasoning and truthfulness evaluation.
- Evidence anchors:
  - [section] "Specifically, the newly added Ko-CommonGen v2 dataset has mid-level correlation with the Ko-ARC, Ko-HellaSwag, and Ko-MMLU datasets while having low correlation with the Ko-TruthfulQA dataset."
  - [abstract] "Moreover, we present empirical support for the need to expand beyond set benchmarks."
- Break condition: If model performance on Ko-CommonGen v2 saturates quickly like other common knowledge tasks, its orthogonal value diminishes.

### Mechanism 3
- Claim: Model size and type influence performance improvement patterns over time.
- Mechanism: Larger models show stepwise performance improvements, and instruction-tuned models follow pretrained model performance trends.
- Core assumption: Model capacity enables learning of orthogonal capabilities that improve benchmark scores.
- Evidence anchors:
  - [section] "As the bracket moves toward the three to seven and seven to fourteen billion parameters, the aforementioned correlation steadily increases to a positive value."
  - [abstract] "Temporal analyses reveal stepwise performance improvements and the importance of model size and type."
- Break condition: If new training techniques eliminate the size-dependent performance gaps, this mechanism's explanatory power weakens.

## Foundational Learning

- Concept: Data contamination in LLM evaluation
  - Why needed here: Understanding why private test sets are crucial for fair evaluation
  - Quick check question: What happens to benchmark scores when models are trained on evaluation data?

- Concept: Correlation analysis in benchmark design
  - Why needed here: Evaluating whether added tasks provide orthogonal evaluation dimensions
  - Quick check question: If two tasks have correlation coefficient 0.9, what does this imply about their evaluation overlap?

- Concept: Temporal analysis of model performance
  - Why needed here: Understanding how different model sizes and types improve over time
  - Quick check question: What pattern would indicate a critical model size for performance improvement?

## Architecture Onboarding

- Component map: Open Ko-LLM Leaderboard consists of Hugging Face Space interface -> Private Ko-H5 benchmark datasets -> Automated evaluation pipeline -> Community discussion forum

- Critical path: Model submission → Automated evaluation on private Ko-H5 datasets → Score calculation and ranking → Community discussion and feedback loop for benchmark improvements

- Design tradeoffs: Private test sets ensure fairness but limit transparency; human translation review improves quality but increases costs; maintaining separate Korean benchmarks adds overhead but provides linguistic specificity

- Failure signatures: High overlap percentages between test sets and training data, community confusion about evaluation criteria, rapid saturation of certain task scores, or declining submission quality due to poor documentation

- First 3 experiments:
  1. Measure overlap percentages between private test sets and commonly used Korean training datasets using aggressive deduplication parameters
  2. Calculate correlation coefficients between all Ko-H5 task pairs to identify orthogonal evaluation dimensions
  3. Track Ko-H5 score improvements over time for different model size brackets to identify performance trends

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Ko-CommonGen v2 dataset specifically differentiate the Open Ko-LLM Leaderboard from the English Open LLM Leaderboard, and what unique linguistic challenges does it address for Korean language evaluation?
- Basis in paper: Explicit - The paper mentions that Ko-CommonGen v2 "brings more diversity to the Ko-H5 benchmark" and "differentiates the Open Ko-LLM Leaderboard from the English Open LLM Leaderboard by bringing more diversity to the evaluation suite."
- Why unresolved: The paper does not provide specific details on how Ko-CommonGen v2 addresses unique Korean linguistic challenges or what makes it distinct from the English CommonGen dataset.
- What evidence would resolve it: Comparative analysis of Ko-CommonGen v2 and English CommonGen datasets, including linguistic features, task difficulty, and performance metrics for Korean and English LLMs.

### Open Question 2
- Question: What is the critical model size threshold identified in the temporal analysis that enables rapid performance improvement in Korean LLMs, and how does this compare to English LLMs?
- Basis in paper: Explicit - The paper mentions a "potential critical model size that enables rapid performance improvement" in the temporal analysis section.
- Why unresolved: The paper does not specify the exact model size threshold or provide a direct comparison with English LLMs.
- What evidence would resolve it: Detailed analysis of performance improvements across different model size brackets for both Korean and English LLMs, identifying the specific threshold where rapid improvement occurs.

### Open Question 3
- Question: How effective are the current private test sets in preventing data contamination, and what are the long-term implications for the Open Ko-LLM Leaderboard's reliability as Korean LLMs continue to evolve?
- Basis in paper: Explicit - The paper states that "our private test sets have little overlap with some of the most popular training datasets used by top models in the Open Ko-LLM Leaderboard" and emphasizes the importance of private test sets in preventing data contamination.
- Why unresolved: The paper does not discuss the long-term effectiveness of private test sets or potential future challenges as the Korean LLM landscape evolves.
- What evidence would resolve it: Longitudinal studies tracking data contamination over time, analysis of emerging training datasets, and evaluation of the private test sets' effectiveness against new Korean LLM architectures.

## Limitations
- Translation quality variations may introduce systematic biases across different task types
- Correlation analysis assumes linear relationships that may not capture complex task interactions
- Temporal analysis is based on limited timeframe and may not reflect long-term trends
- Private test sets limit transparency despite ensuring fairness

## Confidence
- High Confidence: The necessity of private test sets for fair evaluation
- Medium Confidence: The orthogonal value of Ko-CommonGen v2
- Medium Confidence: Model size and type influence on performance trends

## Next Checks
1. Conduct comprehensive analysis of training data contamination by expanding overlap checks to include additional Korean-specific training datasets and fine-tuning corpora
2. Test robustness of translated benchmarks by evaluating both Korean and original English versions with multilingual models to ensure translation quality doesn't introduce systematic biases
3. Extend temporal analysis by monitoring Ko-H5 scores over additional 6-12 month period to identify whether observed stepwise improvements represent sustained trends or temporary plateaus in Korean LLM development