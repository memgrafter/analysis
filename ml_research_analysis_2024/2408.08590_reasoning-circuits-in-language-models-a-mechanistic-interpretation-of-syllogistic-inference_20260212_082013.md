---
ver: rpa2
title: 'Reasoning Circuits in Language Models: A Mechanistic Interpretation of Syllogistic
  Inference'
arxiv_id: '2408.08590'
source_url: https://arxiv.org/abs/2408.08590
tags:
- circuit
- heads
- reasoning
- attention
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a mechanistic interpretation of syllogistic
  reasoning in language models, uncovering a structured error-correction mechanism
  involving middle-term suppression and information propagation via mover heads. Using
  activation patching and circuit ablation, the authors identify a sufficient and
  necessary circuit for symbolic syllogistic inference, which generalizes across syllogistic
  schemes achieving 60% accuracy.
---

# Reasoning Circuits in Language Models: A Mechanistic Interpretation of Syllogistic Inference

## Quick Facts
- **arXiv ID**: 2408.08590
- **Source URL**: https://arxiv.org/abs/2408.08590
- **Reference count**: 33
- **Primary result**: Middle-term suppression circuit is sufficient and necessary for symbolic syllogistic inference in GPT-2 Medium

## Executive Summary
This paper presents a mechanistic interpretation of syllogistic reasoning in language models by uncovering a structured error-correction mechanism. Through activation patching and circuit ablation, the authors identify a middle-term suppression circuit involving specific attention heads that enables valid conclusion derivation from syllogistic premises. The circuit achieves >60% accuracy across syllogistic schemes and generalizes across model sizes and architectures, though belief biases introduce contamination from world knowledge in non-symbolic settings.

## Method Summary
The authors analyze GPT-2 Medium's reasoning for AAA-1 syllogisms using activation patching experiments with middle-term and all-term corruption to identify key attention heads. They employ Logit Lens analysis to examine hidden activations and perform circuit ablation to verify necessity and sufficiency of identified components. The methodology includes progressive head removal from downstream to upstream layers, with performance restoration serving as the sufficiency test.

## Key Results
- Identified h11.10 as a middle-term suppression head that corrects predictions by suppressing duplicated middle-term information
- Discovered induction heads that aggregate duplicated middle-term information at specific positions
- Demonstrated >60% accuracy generalization across syllogistic schemes
- Showed circuit transferability across model sizes and architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Middle-term suppression is a sufficient and necessary circuit component for syllogistic inference
- **Mechanism**: Specific attention heads suppress duplicated middle-term information at the prediction position, shifting from incorrect to correct conclusions
- **Core assumption**: The model encodes a correction mechanism that distinguishes between repeated middle terms and valid conclusions
- **Evidence anchors**: Activation patching reveals h11.10 strongly suppresses logits when attending to same token as output; negative diagonal pattern in OV circuit logit lens confirms suppression function
- **Break condition**: Removal of middle-term duplication prevents suppression circuit from functioning

### Mechanism 2
- **Claim**: Induction heads aggregate duplicated middle-term information for transitive reasoning
- **Mechanism**: Induction heads at specific layers attend to matching terms in premises, aggregating duplicated middle-term information to single position before suppression
- **Core assumption**: Model uses induction heads to identify and collect repeated term information as part of reasoning process
- **Evidence anchors**: Path patching confirms h11.10 operates based on induction heads (h5.8, h6.1, h6.15, h7.2) formed at specific positions
- **Break condition**: Ablation of induction heads prevents middle-term information aggregation

### Mechanism 3
- **Claim**: Mover heads propagate term-related information to final prediction position with negative movers causing initial bias
- **Mechanism**: Attention heads move information from term positions to last token position, with early negative movers biasing predictions toward middle term later corrected by suppression
- **Core assumption**: Model uses combination of positive and negative mover heads to transfer and refine term information during prediction
- **Evidence anchors**: Information flow analysis identifies positive copy heads (h14.14, h15.14, h18.12) and negative copy heads (h9.9, h11.1, h12.1, h17.2, h23.10)
- **Break condition**: Mover head ablation prevents proper term information transfer

## Foundational Learning

- **Concept**: Activation patching
  - **Why needed here**: To isolate causal contribution of specific components to model performance on syllogistic reasoning
  - **Quick check question**: How does activation patching differentiate between necessary and sufficient components in a reasoning circuit?

- **Concept**: Logit lens
  - **Why needed here**: To analyze hidden activations by projecting them into embedding space, revealing how attention heads influence output logits
  - **Quick check question**: What does a negative diagonal pattern in the OV circuit logit lens indicate about an attention head's function?

- **Concept**: Circuit ablation
  - **Why needed here**: To evaluate whether identified components are both necessary (performance degrades when removed) and sufficient (performance restored when only these components are preserved)
  - **Quick check question**: How does mean ablation help distinguish between necessary and sufficient components in a reasoning circuit?

## Architecture Onboarding

- **Component map**: Input → Token and positional embeddings → Transformer layers (attention and MLP) → m-suppression head aggregates and suppresses middle-term information → Mover heads propagate term information → Prediction head outputs conclusion
- **Critical path**: The model processes syllogistic premises through attention layers where h11.10 identifies and suppresses duplicated middle terms, while mover heads transfer relevant information to the prediction position for final output
- **Design tradeoffs**: Circuit analysis requires balancing computational cost (384 heads in GPT-2 Medium) with analytical depth, focusing on specific heads and positions enables tractable analysis but may miss broader circuit dynamics
- **Failure signatures**: Erratic activation patterns in small models (Pythia 70M, 160M) indicate insufficient model capacity for stable reasoning; loss of middle-term suppression or mover head function leads to failure on valid syllogisms
- **First 3 experiments**:
  1. Apply middle-term corruption to GPT-2 Medium and measure patching scores to identify m-suppression head
  2. Apply all-term corruption and analyze residual stream patching to identify mover heads and their information flow patterns
  3. Perform circuit ablation by progressively removing identified heads and measure performance degradation to assess necessity and sufficiency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the precise computational mechanisms that allow the middle-term suppression circuit to disentangle from belief biases in non-symbolic syllogisms?
- **Basis in paper**: The paper identifies that belief biases contaminate the content-independent reasoning circuit with additional attention heads encoding contextualized world knowledge
- **Why unresolved**: While demonstrating that belief biases affect reasoning performance, the study does not fully characterize how these biases manifest at the mechanistic level
- **What evidence would resolve it**: Detailed causal analysis isolating individual attention heads responsible for belief bias contamination, combined with ablation studies showing whether targeted removal of these heads restores sufficiency of the symbolic circuit in non-symbolic syllogisms

### Open Question 2
- **Question**: Does the middle-term suppression circuit generalize to syllogistic schemes beyond the AAA-1 schema, particularly those involving different quantifiers or negative premises?
- **Basis in paper**: The paper shows the circuit is sufficient and necessary for syllogisms achieving high accuracy (≥60%), including some AII and IAI schemes, but fails for schemes like AOO-2 and OAO-3
- **Why unresolved**: The study does not investigate whether the suppression mechanism can be adapted or extended to handle different logical structures
- **What evidence would resolve it**: Circuit discovery and ablation studies applied to all 15 unconditionally valid syllogistic schemes, with analysis of whether different quantifier types require distinct mechanistic adaptations

### Open Question 3
- **Question**: How does model size influence the complexity and purity of the syllogistic reasoning circuit, and at what scale does world knowledge begin to dominate over formal reasoning?
- **Basis in paper**: The paper observes that as model size increases, attention head contributions become more complex, and symbolic dataset accuracy decreases while non-symbolic accuracy increases
- **Why unresolved**: The study does not establish a precise threshold or scaling relationship where formal reasoning circuits become overwhelmed by memorized patterns
- **What evidence would resolve it**: Systematic scaling experiments tracking circuit purity and reasoning accuracy across model sizes, coupled with analysis of attention head patterns to identify when world knowledge mechanisms emerge and dominate

## Limitations
- Circuit analysis performed on single model (GPT-2 Medium) with specific syllogistic form (AAA-1), limiting generalizability
- Focus on symbolic reasoning leaves open questions about belief bias effects in non-symbolic settings
- Complexity of full 24-layer model makes it challenging to rule out alternative pathways or compensatory mechanisms

## Confidence
- **High confidence**: Identification of h11.10 as middle-term suppression head and demonstration of its causal role through activation patching; circuit ablation results showing necessity and sufficiency
- **Medium confidence**: Generalization of circuit across syllogistic schemes and transferability to other model sizes
- **Low confidence**: Precise mechanism of information aggregation by induction heads and exact nature of mover head interactions

## Next Checks
1. **Cross-task validation**: Test identified circuit on broader range of reasoning tasks beyond syllogisms (spatial reasoning, numerical inference) to assess generalizability
2. **Model architecture ablation**: Perform systematic ablation of model components (residual connections, layer normalization) to determine which architectural elements are essential for circuit function
3. **Belief bias quantification**: Design controlled experiments to quantify extent of belief bias interference in non-symbolic syllogisms and test interventions to mitigate this contamination