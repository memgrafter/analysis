---
ver: rpa2
title: Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling
arxiv_id: '2403.03516'
source_url: https://arxiv.org/abs/2403.03516
tags:
- multilingual
- retrieval
- dense
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UMR, an unsupervised method for training
  multilingual dense retrievers without any paired data. The approach leverages the
  sequence likelihood estimation capabilities of multilingual language models to generate
  pseudo labels for training dense retrievers.
---

# Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling

## Quick Facts
- arXiv ID: 2403.03516
- Source URL: https://arxiv.org/abs/2403.03516
- Reference count: 16
- One-line primary result: UMR achieves 48.0% R@5kt on XOR-Retrieve and 44.9% F1 on XOR-Full, outperforming strong supervised baselines

## Executive Summary
This paper introduces UMR, an unsupervised method for training multilingual dense retrievers without any paired data. The approach leverages the sequence likelihood estimation capabilities of multilingual language models to generate pseudo labels for training dense retrievers. The proposed two-stage framework iteratively improves multilingual dense retrievers by first performing unsupervised multilingual reranking and then knowledge-distilled retriever training. Experimental results on XOR-TyDi QA demonstrate that UMR outperforms or performs comparably to supervised baselines, showcasing the potential of training multilingual retrievers without paired data.

## Method Summary
UMR is a two-stage iterative framework for unsupervised multilingual dense retrieval. In the first stage, it uses a multilingual language model (mT5-XL) to perform unsupervised reranking by estimating the negative log-likelihood of generating queries from documents. In the second stage, it trains a dense retriever using knowledge distillation, where the KL divergence loss transfers the reranker's probability distribution to the retriever. The process iterates twice, refreshing the retrieval index in each iteration to avoid overfitting to the same documents.

## Key Results
- UMR achieves 48.0% R@5kt on XOR-Retrieve task
- UMR achieves 44.9% F1 on XOR-Full task
- Outperforms strong supervised baselines on XOR-TyDi QA

## Why This Works (Mechanism)

### Mechanism 1
Generative pseudo labeling works because multilingual LMs can estimate conditional probabilities that reflect relevance. The model estimates P(query | document) using sequence likelihood, which inversely correlates with relevance quality. A pre-trained multilingual LM without instruction tuning can still produce meaningful relevance scores through probability estimation.

### Mechanism 2
Knowledge distillation from a reranker to a retriever improves retrieval performance. The KL divergence loss transfers the reranker's fine-grained probability distribution to the retriever. The reranker's probability distribution contains useful information beyond just the top-ranked document.

### Mechanism 3
Iterative training prevents overfitting to the same top-k passages. Each iteration refreshes the retrieval index with a better retriever, exposing it to new documents. The retriever's improvement in each iteration is substantial enough to change the top-k documents retrieved.

## Foundational Learning

- **Concept**: Conditional probability estimation using sequence likelihood
  - Why needed here: Forms the basis of the pseudo-labeling mechanism without requiring paired data
  - Quick check question: Can you explain how P(q|d) relates to relevance ranking?

- **Concept**: Knowledge distillation through KL divergence
  - Why needed here: Enables transfer of reranker knowledge to the retriever without hard labels
  - Quick check question: What's the difference between KL loss and cross-entropy loss in this context?

- **Concept**: In-batch negative sampling in dense retrieval
  - Why needed here: Provides diverse negative examples for training the retriever efficiently
  - Quick check question: How does in-batch negative sampling differ from using a separate negative corpus?

## Architecture Onboarding

- **Component map**: Multilingual query collection -> Initial mContriever -> Unsupervised reranking (mLM-based) -> Knowledge distillation (retriever training) -> Iterative loop with index refresh

- **Critical path**: 1. Retrieve top-k documents using current retriever 2. Rerank using multilingual LM likelihood estimation 3. Train new retriever using KL distillation loss 4. Build new index and repeat

- **Design tradeoffs**: Reranking vs. direct retriever training (reranking provides better supervision but is slower); Temperature parameter (controls distribution sharpness; too high loses signal, too low creates sparse gradients); Batch size (larger batches provide more in-batch negatives but increase memory requirements)

- **Failure signatures**: Poor initial mContriever performance → all subsequent iterations suffer; KL loss divergence → temperature too high or reranker too weak; No improvement across iterations → retriever not learning or document pool too small

- **First 3 experiments**: 1. Test reranking quality: Compare mContriever + rerank vs. mContriever alone on development set 2. Validate knowledge distillation: Train with and without KL loss, measure retrieval improvement 3. Test iterative convergence: Run 1, 2, and 3 iterations, measure performance changes

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of UMR change when using different multilingual language models for the reranking stage? The paper only uses mt5-xl for reranking but does not explore other models.

### Open Question 2
How does the performance of UMR vary with the number of iterations in the iterative training process? The paper mentions using two iterations but does not explore the effect of varying the number of iterations.

### Open Question 3
How does the performance of UMR change when using different numbers of top-k documents for reranking? The paper uses top-100 documents for reranking but does not explore the effect of using different numbers of documents.

## Limitations

- **Implementation details missing**: The paper lacks specific details on how the mT5-XL model generates fine-grained relevance scores for each document-query pair
- **Generalization concerns**: Results are only validated on XOR-TyDi QA, a specific multilingual retrieval dataset
- **Computational overhead**: The iterative reranking approach requires multiple passes over the entire corpus, which could be prohibitively expensive for larger datasets

## Confidence

- **High confidence**: The core insight that generative pseudo labeling can work for unsupervised multilingual retrieval
- **Medium confidence**: The iterative training mechanism's effectiveness
- **Medium confidence**: The knowledge distillation approach using KL divergence

## Next Checks

1. **Ablation on pseudo-labeling method**: Compare UMR's generative pseudo-labeling approach against alternative unsupervised methods (e.g., BM25-based pseudo-labels, synthetic query generation) to isolate the contribution of the mT5-XL likelihood estimation

2. **Cross-dataset validation**: Evaluate UMR on another multilingual retrieval dataset (such as MLQA or XQuAD) to assess whether the performance gains generalize beyond XOR-TyDi QA

3. **Efficiency analysis**: Measure the computational overhead of the iterative reranking approach compared to end-to-end retriever training, including wall-clock time and memory requirements for different corpus sizes