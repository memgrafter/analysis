---
ver: rpa2
title: 'SyntaxShap: Syntax-aware Explainability Method for Text Generation'
arxiv_id: '2402.09259'
source_url: https://arxiv.org/abs/2402.09259
tags:
- syntaxshap
- explanations
- words
- tokens
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SyntaxShap, a model-agnostic explainability
  method for text generation tasks that incorporates syntactic dependencies from dependency
  parsing trees. The method extends Shapley values by only considering coalitions
  constrained by the dependency tree structure, computing word contributions based
  on allowed coalitions at each tree level.
---

# SyntaxShap: Syntax-aware Explainability Method for Text Generation

## Quick Facts
- arXiv ID: 2402.09259
- Source URL: https://arxiv.org/abs/2402.09259
- Authors: Kenza Amara; Rita Sevastjanova; Mennatallah El-Assady
- Reference count: 28
- Primary result: SyntaxShap produces more faithful, coherent, and semantically aligned explanations than baseline methods while being computationally faster than NaiveShap

## Executive Summary
SyntaxShap introduces a novel model-agnostic explainability method for text generation that incorporates syntactic dependencies from dependency parsing trees. The method extends Shapley value theory by restricting coalition formation to those consistent with the dependency tree structure, computing word contributions based on allowed coalitions at each tree level. Evaluated against Random, LIME, NaiveShap, and Partition baselines using faithfulness metrics (fidelity, probability divergence@k, accuracy@k), coherency, and semantic alignment, SyntaxShap demonstrates superior performance in producing explanations that are more faithful to model predictions, coherent across semantically similar sentences, and better aligned with human linguistic expectations, particularly for negation detection.

## Method Summary
SyntaxShap computes word importance scores for text generation tasks by extending Shapley value theory with dependency tree constraints. Given an input sentence, the method first generates a dependency tree using spaCy's parser. For each word, it evaluates marginal contributions by averaging over allowed coalitions at each tree level, where coalitions are restricted to those consistent with the syntactic structure. The weighted variant SyntaxShap-W applies inverse level weights to prioritize words higher in the dependency tree. The method is evaluated on three datasets (Generics KB, ROCStories, Negation) using two autoregressive models (GPT-2 and Mistral 7B), measuring faithfulness through fidelity and probability divergence metrics, coherency through similarity of explanations for semantically similar predictions, and semantic alignment through negation token importance ranking.

## Key Results
- SyntaxShap produces more faithful explanations than Random, LIME, NaiveShap, and Partition baselines across all evaluation metrics
- SyntaxShap achieves faster computation (O(nL2^n/L)) compared to NaiveShap (O(n2^n)) while maintaining similar faithfulness
- SyntaxShap-W demonstrates improved semantic alignment, particularly in ranking negation tokens when predictions don't reflect them
- The method shows superior coherency, with more similar explanations for semantically related sentences compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Using dependency tree constraints in coalition formation produces more faithful explanations than unconstrained Shapley methods.
- **Mechanism**: The dependency tree naturally encodes syntactic relationships between words. By restricting coalitions to those consistent with the tree structure, SyntaxShap only evaluates marginal contributions that respect linguistic dependencies, reducing noise from semantically meaningless coalitions.
- **Core assumption**: The dependency tree accurately captures the syntactic relationships that influence next-token predictions in autoregressive LMs.
- **Evidence anchors**:
  - [abstract]: "Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree."
  - [section 3.3]: "Our method (in Figure 1) considers syntactic word dependencies... SyntaxShap computes the contribution of words only considering allowed coalitions S constraint on the dependency tree structure."

### Mechanism 2
- **Claim**: Weighted contributions based on tree level improve explanation quality by prioritizing syntactically central words.
- **Mechanism**: Words higher in the dependency tree (closer to root) are given greater weight in the final importance calculation. This reflects their syntactic centrality and presumed importance for prediction.
- **Core assumption**: Words at higher levels in the dependency tree are more influential for next-token prediction than lower-level words.
- **Evidence anchors**:
  - [section 3.4]: "In the context of text data and syntactic dependencies, we assume that words at the top of the tree should be given more importance since they are the syntactic foundations of the sentence."
  - [section 5.4]: Weighted SyntaxShap-W shows improved semantic alignment compared to unweighted version in ranking negation tokens.

### Mechanism 3
- **Claim**: The tree-based coalition construction enables computational efficiency compared to naive Shapley computation.
- **Mechanism**: Instead of evaluating all 2^n coalitions, SyntaxShap only evaluates coalitions consistent with the dependency tree, reducing the number from O(2^n) to O(nL2^n/L).
- **Core assumption**: The dependency tree structure meaningfully reduces the coalition space without losing explanatory power.
- **Evidence anchors**:
  - [section 3.4]: "SyntaxShap has the advantage of being much faster, with a computational complexity of O(nL2n/L) against O(n2n) for NaiveShap."
  - [section 5.2]: SyntaxShap is shown to be faster while maintaining similar faithfulness to NaiveShap.

## Foundational Learning

- **Concept**: Shapley values and cooperative game theory
  - Why needed here: SyntaxShap is fundamentally built on Shapley value theory but modifies it to respect syntactic constraints
  - Quick check question: What are the four original Shapley axioms and which ones does SyntaxShap violate?

- **Concept**: Dependency parsing and syntactic relationships
  - Why needed here: The method requires understanding how dependency trees encode grammatical relationships between words
  - Quick check question: How does a dependency tree differ from a constituency tree in representing sentence structure?

- **Concept**: Autoregressive language model mechanics
  - Why needed here: Understanding how LMs generate next tokens is crucial for interpreting what explanations should reveal
  - Quick check question: In a transformer-based autoregressive model, how does the prediction for token t depend on previous tokens?

## Architecture Onboarding

- **Component map**: Input sentence → Dependency parser (spaCy) → Dependency tree → Coalition evaluation (multiple rounds) → Importance scores for each word → Evaluation metrics (fidelity, coherency, semantic alignment)
- **Critical path**: Dependency parsing → Coalition generation → Importance calculation → Evaluation
  - Bottleneck: Dependency parsing accuracy and coalition computation time for long sentences
- **Design tradeoffs**:
  - Tree constraints vs. faithfulness: More constraints = faster but potentially less accurate
  - Weighted vs. unweighted: Weighted gives syntactic priority but adds assumption burden
  - Top-K evaluation vs. single prediction: More robust to stochasticity but computationally heavier
- **Failure signatures**:
  - Poor fidelity scores indicate coalition constraints are too restrictive
  - Low coherency scores suggest dependency parser errors or inappropriate tree structure
  - Semantic misalignment with negation tokens suggests weighting scheme is flawed
- **First 3 experiments**:
  1. Compare SyntaxShap vs. NaiveShap on a small dataset (n<10) to validate faithfulness equivalence
  2. Test coherency metric on negation dataset with controlled perturbations
  3. Benchmark computation time scaling with sentence length to confirm O(nL2^n/L) complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can explainability methods be designed to explicitly account for the differences between human and language model reasoning in text generation tasks?
- Basis in paper: [explicit] The paper discusses the misalignment between human and AI model reasoning and highlights the need for cautious evaluation strategies in explainable AI.
- Why unresolved: While the paper acknowledges this misalignment and suggests that future work should design evaluation methods that show the importance of words for the model and the reasons for potential divergence from human expectations, it does not provide concrete solutions or methodologies for addressing this challenge.
- What evidence would resolve it: Developing and evaluating novel explainability methods that explicitly model and visualize the differences between human and LM reasoning, along with user studies demonstrating improved understanding of LM behavior.

### Open Question 2
- Question: How can SyntaxShap be extended to handle languages other than English or more complex grammatical structures?
- Basis in paper: [inferred] The paper mentions that SyntaxShap relies on dependency parsing using spaCy, which sometimes generates debatable dependencies from a linguistic perspective and its accuracy drops when implemented for languages other than English.
- Why unresolved: The paper focuses on English sentences and does not explore how SyntaxShap could be adapted for other languages or more complex grammatical structures.
- What evidence would resolve it: Extending SyntaxShap to handle multiple languages or complex grammatical structures, along with experiments demonstrating improved explainability quality across diverse linguistic contexts.

### Open Question 3
- Question: How can explainability methods be designed to provide more linguistically-aware assessment of explanation quality in text generation tasks?
- Basis in paper: [explicit] The paper suggests that future work should consider the different roles of tokens (e.g., function words vs. content words) when evaluating explanation quality and proposes controlled perturbations on input sentences to evaluate the role of semantics and syntax.
- Why unresolved: While the paper identifies the need for more linguistically-tailored evaluation methods, it does not provide concrete approaches for incorporating linguistic knowledge into the assessment of explanation quality.
- What evidence would resolve it: Developing and evaluating novel evaluation metrics that explicitly incorporate linguistic knowledge, such as part-of-speech tagging or semantic roles, along with experiments demonstrating improved alignment with human expectations.

## Limitations

- Dependency Parser Dependency: SyntaxShap's performance is fundamentally limited by the accuracy of the underlying dependency parser, creating a single point of failure where poor parsing directly translates to poor explanations.
- Syntactic Assumption: The method assumes syntactic centrality correlates with prediction importance, which may not hold for all language models or tasks where semantic or contextual information dominates.
- Computational Scaling: While more efficient than NaiveShap, the O(nL2^n/L) scaling still becomes prohibitive for longer sentences, requiring multiple evaluations of the language model for each coalition.

## Confidence

**High Confidence**: Mechanisms 1 & 3 (dependency constraints and computational efficiency) are well-supported by theoretical framework and empirical results, with directly measurable faithfulness improvements and speedup over NaiveShap.

**Medium Confidence**: Mechanism 2 (weighted contributions) shows some improvement in semantic alignment metrics but the assumption that higher-level words are more important for predictions is not thoroughly validated across different model architectures or tasks.

**Low Confidence**: Generalizability across diverse language models, tasks, and domains is not extensively tested, with evaluation focusing on specific GPT-2 and Mistral models for particular generation tasks.

## Next Checks

1. **Parser Robustness Test**: Evaluate SyntaxShap's performance using multiple dependency parsers (e.g., spaCy vs. CoreNLP) on the same dataset to quantify the impact of parsing accuracy on explanation quality, measuring fidelity scores across parsers.

2. **Model Architecture Comparison**: Apply SyntaxShap to different language model architectures (RNN, CNN, transformer variants) and compare faithfulness scores to test whether syntactic assumptions hold across architectural differences in sequential information processing.

3. **Task Generalization Study**: Test SyntaxShap on non-generative tasks like text classification or sentiment analysis to evaluate whether syntactic dependencies that benefit generation tasks also improve explainability for other NLP tasks, comparing performance against task-specific methods.