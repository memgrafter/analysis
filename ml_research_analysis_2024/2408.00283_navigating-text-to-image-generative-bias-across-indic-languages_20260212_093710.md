---
ver: rpa2
title: Navigating Text-to-Image Generative Bias across Indic Languages
arxiv_id: '2408.00283'
source_url: https://arxiv.org/abs/2408.00283
tags:
- deva
- languages
- images
- indic
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the IndicTTI benchmark to assess biases in
  text-to-image generation models for 30 Indic languages. The benchmark evaluates
  four models (Stable Diffusion, Alt Diffusion, Midjourney, DALL-E 3) using 1000 prompts
  across English and 30 Indic languages.
---

# Navigating Text-to-Image Generative Bias across Indic Languages

## Quick Facts
- arXiv ID: 2408.00283
- Source URL: https://arxiv.org/abs/2408.00283
- Reference count: 37
- Primary result: DALL-E 3 significantly outperforms other models on Indic language correctness metrics, while all models show script-to-culture correlation biases

## Executive Summary
This paper introduces the IndicTTI benchmark to evaluate biases in text-to-image generation models for 30 Indic languages. The study assesses four leading models (Stable Diffusion, Alt Diffusion, Midjourney, DALL-E 3) using 1000 prompts translated into 31 languages (English plus 30 Indic languages). The benchmark introduces six metrics measuring correctness and representation, revealing that DALL-E 3 significantly outperforms other models on Indic languages, while all models exhibit systematic cultural correlations between scripts and generated imagery. The findings highlight the need for more inclusive and culturally-aware TTI systems.

## Method Summary
The IndicTTI benchmark evaluates four text-to-image models on 1000 COCO-NLLB dataset prompts translated into 31 languages. For each prompt-language pair, images are generated and evaluated using three correctness metrics (CLGC, IGC, LGC) and three representation metrics (SCAL, SCWL, DWL). The correctness metrics measure semantic similarity between generated images and reference images using high-level features from BLIP-2, while representation metrics assess self-consistency within languages. The benchmark identifies performance gaps and cultural biases across models and languages.

## Key Results
- DALL-E 3 significantly outperforms other models on all correctness metrics for Indic languages, with the largest margin in CLGC scores
- All models show strong script-to-culture correlations in generated images, with Stable Diffusion and Alt Diffusion frequently depicting Indian religious figures and cultural elements
- Self-consistency within languages remains relatively high even when correctness is low, suggesting stable latent space representations
- Models struggle particularly with Bengali script languages, showing significantly lower performance across all metrics
- Midjourney shows strong script-to-culture associations, often generating culturally-associated imagery regardless of prompt content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DALL-E 3's superior multilingual training enables better semantic understanding across Indic languages
- Mechanism: Advanced multilingual training allows DALL-E 3 to better understand semantic relationships across different scripts and languages, resulting in higher correctness scores
- Core assumption: Multilingual training data and architectural design directly improve cross-lingual semantic understanding
- Evidence anchors:
  - [abstract]: "Results show DALL-E 3 outperforms others with significantly higher correctness scores for Indic languages"
  - [section 5]: "Across all the metrics, we observe that Dalle3 outperforms all other models when evaluated for Indic languages with a significant margin"
- Break condition: Limited or biased multilingual training data would diminish performance advantage

### Mechanism 2
- Claim: Script-specific cultural associations emerge due to language-script-culture correlations in training data
- Mechanism: TTI models learn correlations between scripts and cultural elements from training data, leading to systematic generation of culturally-associated imagery
- Core assumption: Training datasets contain sufficient correlations between scripts and cultural imagery
- Evidence anchors:
  - [abstract]: "Qualitative analysis reveals cultural correlations in generated images"
  - [section 6]: "We notice a correlation between different language scripts and the images generated corresponding to them"
- Break condition: Curated training data without script-culture correlations would eliminate the phenomenon

### Mechanism 3
- Claim: Self-consistency remains high due to stable latent space representations for text
- Mechanism: Models maintain consistent latent space embeddings for the same text across multiple generations, leading to high self-consistency even when images are semantically incorrect
- Core assumption: Diffusion process creates stable representations for input text regardless of semantic accuracy
- Evidence anchors:
  - [section 5]: "When compared with other metrics, there is a low variation in the values of self-consistency within the language"
  - [section 5]: "While the model may generate incorrectly, it associates a given text for generation in the latent space consistently"
- Break condition: Variable diffusion sampling would decrease self-consistency

## Foundational Learning

- Concept: Cross-lingual semantic similarity metrics
  - Why needed here: The benchmark relies on measuring semantic similarity between images and text across languages using cosine similarity of high-level features
  - Quick check question: How would you modify the similarity metric if you needed to account for cultural context differences in image interpretation?

- Concept: Script encoding and representation in multimodal models
  - Why needed here: Understanding how different scripts are encoded and processed differently by TTI models is crucial for interpreting results
  - Quick check question: What architectural components in CLIP or similar models handle script-specific features differently from language-specific features?

- Concept: Catastrophic forgetting in multilingual models
  - Why needed here: The paper observes that AltDiffusion performs worse on English after multilingual training, suggesting catastrophic forgetting
  - Quick check question: What regularization techniques could be applied to mitigate catastrophic forgetting when training a model on multiple languages?

## Architecture Onboarding

- Component map: Prompt translation → Image generation (4 models) → Feature extraction (BLIP-2) → Metric computation (6 metrics) → Analysis
- Critical path: Prompt translation → image generation → feature extraction → metric computation. Bottlenecks likely occur in feature extraction for large image sets (124K images for open-source models).
- Design tradeoffs: Using high-level semantic features from BLIP-2 trades fine-grained detail for computational efficiency and cross-modal compatibility. Using English as reference language for LGC simplifies computation but may introduce bias.
- Failure signatures: Low CLGC but high SCWL suggests correct latent space encoding but poor cross-modal mapping. Very low IGC across all models indicates dataset or evaluation pipeline issues rather than model-specific problems.
- First 3 experiments:
  1. Run the benchmark on a subset of 5 languages with 100 prompts to validate the complete pipeline and identify bottlenecks
  2. Compare metric results using different feature extractors (CLIP, BLIP-2, custom) to assess robustness
  3. Perform ablation study by removing script-specific tokens from prompts to test if cultural correlations are prompt-dependent

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of the translation process contribute most to the observed biases in text-to-image generation for Indic languages?
- Basis in paper: [explicit] The paper mentions that "the generation performance for Indic prompts is indirectly affected by the quality of these translations" and that translation quality varies across languages.
- Why unresolved: While the paper acknowledges translation quality as a factor, it does not provide a detailed analysis of which translation aspects have the most significant impact on generation biases.
- What evidence would resolve it: A controlled study comparing text-to-image generation results using different translation methods or analyzing specific translation errors and their effects on generated images.

### Open Question 2
- Question: How do the cultural biases observed in text-to-image generation models for Indic languages compare to those in models for other multilingual contexts?
- Basis in paper: [inferred] The paper focuses exclusively on Indic languages and does not compare the observed biases to those in other multilingual contexts.
- Why unresolved: The paper provides a comprehensive analysis of biases in Indic languages but lacks a comparative perspective with other multilingual scenarios.
- What evidence would resolve it: A comparative study evaluating text-to-image generation biases across different multilingual datasets would help contextualize the findings.

### Open Question 3
- Question: What specific architectural or training data modifications could improve the multilingual performance of text-to-image generation models?
- Basis in paper: [explicit] The paper observes that DALL-E 3 outperforms other models in Indic languages but does not explore the underlying reasons or propose specific improvements for other models.
- Why unresolved: While the paper highlights the superior performance of DALL-E 3, it does not investigate the technical factors contributing to this success or suggest actionable improvements for other models.
- What evidence would resolve it: An analysis of DALL-E 3's architecture and training data, followed by experiments applying similar techniques to other models.

## Limitations
- The benchmark relies on a subset of COCO captions translated into Indic languages, which may not represent full cultural diversity within each language
- The evaluation focuses on semantic correctness without assessing nuanced cultural appropriateness or potential harmful stereotypes
- The comparison is limited to 1000 prompts, potentially missing edge cases or rare cultural elements
- Self-consistency metrics assume stable latent space representations are desirable, but this may not align with creative diversity expectations
- The paper does not explore how different sampling strategies or inference parameters might affect observed biases

## Confidence

- High Confidence: DALL-E 3 outperforms other models on Indic language correctness metrics (CLGC, IGC, LGC)
- Medium Confidence: Script-to-culture correlations emerge in generated images
- Medium Confidence: Self-consistency remains high even when correctness is low
- Low Confidence: AltDiffusion's poor English performance is solely due to catastrophic forgetting from multilingual training

## Next Checks

1. Perform cross-validation by running the benchmark on a separate, independently curated dataset of culturally diverse Indic language prompts to verify if performance patterns hold beyond the COCO-NLLB subset.

2. Conduct a systematic ablation study by removing script-specific tokens and cultural references from prompts to determine whether observed script-to-culture correlations are inherent to the models or prompt-dependent artifacts.

3. Implement and compare multiple feature extractors (CLIP, BLIP-2, and a custom culturally-aware model) to assess the robustness of correctness metrics and determine if model performance differences are consistent across different semantic similarity measures.