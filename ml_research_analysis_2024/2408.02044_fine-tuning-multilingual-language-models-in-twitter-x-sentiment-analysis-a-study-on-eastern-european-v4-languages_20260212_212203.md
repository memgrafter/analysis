---
ver: rpa2
title: 'Fine-tuning multilingual language models in Twitter/X sentiment analysis:
  a study on Eastern-European V4 languages'
arxiv_id: '2408.02044'
source_url: https://arxiv.org/abs/2408.02044
tags:
- sentiment
- tweets
- language
- llama2
- tweet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores fine-tuning large language models for aspect-based
  sentiment analysis on Twitter data in underrepresented Eastern European languages
  (Czech, Slovak, Polish, Hungarian) during the Russia-Ukraine conflict. Multiple
  LLMs (BERT, BERTweet, Llama2, Llama3, Mistral) were fine-tuned using manually annotated
  datasets with translations to English via DeepL and Helsinki translator.
---

# Fine-tuning multilingual language models in Twitter/X sentiment analysis: a study on Eastern-European V4 languages

## Quick Facts
- arXiv ID: 2408.02044
- Source URL: https://arxiv.org/abs/2408.02044
- Authors: Tomáš Filip; Martin Pavlíček; Petr Sosík
- Reference count: 30
- One-line primary result: Fine-tuning multilingual LLMs on 6K Twitter tweets in Czech, Slovak, Polish, and Hungarian achieved state-of-the-art ABSA performance, with translation to English consistently improving results

## Executive Summary
This study investigates fine-tuning large language models for aspect-based sentiment analysis on Twitter data in underrepresented Eastern European languages during the Russia-Ukraine conflict. The research demonstrates that fine-tuning with as few as 6,000 multilingual tweets significantly outperforms in-context learning, achieving state-of-the-art results. Multiple models including BERT, BERTweet, Llama2, Llama3, and Mistral were evaluated, with Llama2 and Mistral achieving the highest macro-averaged F1-scores (72.8), closely matching models trained on much larger datasets. Translation to English consistently improved performance across all models, even for multilingual pre-trained models.

## Method Summary
The study collected Twitter/X data filtered by keywords (Ukraine, Russia, Zelensky, Putin) and languages (Czech, Slovak, Polish, Hungarian) from April to May 2023. Manually annotated datasets were created with sentiment labels (negative/positive/neutral) towards Russia or Ukraine. Several LLMs were fine-tuned using PEFT adapter-based technique for 10 epochs, testing different translation modes (DeepL, Helsinki translator, none). Model performance was evaluated using macro-averaged F1-score, accuracy, precision, and recall metrics on test splits.

## Key Results
- Fine-tuning with 6K multilingual tweets achieved state-of-the-art results, outperforming in-context learning
- Llama2 and Mistral achieved highest macro-averaged F1-scores (72.8), closely matching models trained on larger datasets
- Translation to English consistently improved performance across all models, even for multilingual pre-trained models
- Polish tweets showed consistently lower performance across all models compared to Czech, Slovak, and Hungarian

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning with as few as 6,000 multilingual tweets can achieve state-of-the-art results, outperforming in-context learning.
- Mechanism: Fine-tuning adapts model parameters to the specific task and data distribution, whereas in-context learning relies on prompting and lacks parameter adaptation.
- Core assumption: The dataset is representative and the fine-tuning process is effective for the specific task and languages.
- Evidence anchors:
  - [abstract]: "fine-tuning with as few as 6K multilingual tweets provided significantly better (SOTA level) results than in-context learning"
  - [section 4]: "Fine-tuning improved dramatically the performance of Llama2/3 over vanilla models (compare Fig. 3 and Tab. 4)"

### Mechanism 2
- Claim: Translation to English consistently improves performance across all models, even for multilingual pre-trained models.
- Mechanism: Translation provides a common linguistic space for models to operate in, reducing the complexity of handling multiple languages and potentially leveraging the models' stronger English capabilities.
- Core assumption: The translation process is accurate and does not introduce significant errors or biases.
- Evidence anchors:
  - [abstract]: "Translation to English consistently improved performance across all models"
  - [section 4]: "all tested LLMs performed better with English-translated datasets"

### Mechanism 3
- Claim: Some models are much better fine-tunable on multilingual Twitter tasks than others.
- Mechanism: Differences in model architecture, pre-training data, and fine-tuning techniques can lead to varying degrees of adaptability to specific tasks and languages.
- Core assumption: The differences in fine-tunability are due to inherent model characteristics rather than random chance.
- Evidence anchors:
  - [abstract]: "some models are much better fine-tunable on multilingual Twitter tasks than others"
  - [section 4]: "It seems that some models (Llama2, Mistral, BERT) strongly benefited from fine-tuning while some others (Llama3, BERTweet) were more 'stiff' and less tunable"

## Foundational Learning

- Concept: Aspect-based sentiment analysis (ABSA)
  - Why needed here: ABSA is the core task of the study, involving identifying sentiment towards specific aspects in text.
  - Quick check question: What are the three phases of ABSA according to the paper?

- Concept: Fine-tuning
  - Why needed here: Fine-tuning is the primary method used to adapt pre-trained models to the specific ABSA task and languages.
  - Quick check question: What is the difference between fine-tuning and in-context learning according to the paper?

- Concept: Multilingual language models
  - Why needed here: The study focuses on fine-tuning multilingual models for sentiment analysis in underrepresented Eastern European languages.
  - Quick check question: Which models were used in the experiments and what are their key characteristics?

## Architecture Onboarding

- Component map: Data collection (Twitter API) → Data annotation (manual labeling) → Translation (Helsinki, DeepL) → Model fine-tuning (BERT, BERTweet, Llama2, Llama3, Mistral) → Evaluation (accuracy, recall, precision, F1-score)
- Critical path: Data annotation → Model fine-tuning → Evaluation
- Design tradeoffs: Using translation to English simplifies the task but may introduce errors; fine-tuning requires labeled data but can achieve better results than in-context learning.
- Failure signatures: Poor performance on specific languages or aspects, overfitting to the training data, translation errors affecting model performance.
- First 3 experiments:
  1. Fine-tune Llama2 on the Czech/Slovak dataset with DeepL translation and evaluate on the test set.
  2. Fine-tune Mistral on the Polish dataset without translation and evaluate on the test set.
  3. Fine-tune BERT on the Hungarian dataset with Helsinki translation and evaluate on the test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of Polish tweets caused the consistently lower performance across models compared to Czech, Slovak, and Hungarian tweets?
- Basis in paper: [explicit] The paper states "almost all models performed poorer for the Polish language" and that "many positive Polish tweets...were classified as negative by the models," with Polish tweets containing "more complex thoughts reflecting the fact that Poland was historically more interconnected with Ukraine than CS/SK or HU."
- Why unresolved: While the paper identifies that Polish tweets were more complex and historically contextualized, it does not specify what linguistic features, cultural references, or historical contexts created this difficulty. The analysis mentions "more complex thoughts" but doesn't break down what makes them complex from a computational perspective.
- What evidence would resolve it: A detailed linguistic analysis comparing Polish tweets to other languages showing specific features (idiomatic expressions, historical references, sarcasm patterns) that models failed to capture, or a breakdown of misclassified examples with explanations of why the models struggled with them.

### Open Question 2
- Question: Why did Llama3 perform approximately 6% worse than Llama2 despite Llama3's awareness of contemporary context and potential access to the same Twitter/X data used for fine-tuning?
- Basis in paper: [explicit] The paper notes that "Llama3 scored by approx. 6% F1 worse than Llama2" and mentions that "Llama3 was aware of the contemporary context...which is essential to understand the tweets" and "could also potentially access in its pre-training period the same Twitter/X data we used for fine-tuning."
- Why unresolved: The paper identifies the performance gap but doesn't explain the underlying reasons. The authors speculate about pre-training differences but don't investigate what specific architectural or training differences between Llama2 and Llama3 caused this unexpected result.
- What evidence would resolve it: Comparative analysis of the training methodologies, architectural differences, or tokenization strategies between Llama2 and Llama3 that would explain why the more contextually aware model performed worse on this specific task.

### Open Question 3
- Question: What specific translation strategies or linguistic features in the DeepL translations provided advantages over Helsinki translations and original languages, and how can these be quantified?
- Basis in paper: [explicit] The paper states that "the DeepL gave better results than the Helsinki translator" and that "a good translation to English still provided an advantage over the use of the original languages even for multilingual pre-trained models."
- Why unresolved: While the paper demonstrates that DeepL outperformed Helsinki and that translation to English improved results, it doesn't analyze what specific translation features (word choice, sentence structure, handling of idioms) made DeepL superior or quantify the linguistic differences between translation methods.
- What evidence would resolve it: A comparative linguistic analysis of DeepL vs Helsinki translations showing specific translation choices, sentence restructuring patterns, or handling of domain-specific terminology that contributed to the performance differences, along with quantitative measures of these differences.

## Limitations
- Language-specific performance disparities, particularly the consistently lower performance on Polish tweets, suggest potential issues with data quality or model adaptation
- Translation process introduces potential errors and biases that weren't systematically evaluated for their impact on model performance
- Dataset size, while showing effectiveness with 6K tweets, may not be representative of all aspects of the Russia-Ukraine conflict discourse

## Confidence

**High Confidence**: The core finding that fine-tuning outperforms in-context learning is well-supported by the experimental results and aligns with established machine learning principles.

**Medium Confidence**: The claim that as few as 6K tweets can achieve state-of-the-art results requires additional context about dataset quality and distribution.

**Low Confidence**: The assertion that certain models (Llama2, Mistral) are inherently better fine-tunable than others (Llama3, BERTweet) may be influenced by specific implementation choices rather than fundamental architectural differences.

## Next Checks

1. **Dataset Diversity Validation**: Conduct a systematic analysis of the annotated datasets to verify balanced representation across all four languages and both conflict aspects (Russia/Ukraine), checking for potential sampling biases that could explain the Polish performance gap.

2. **Translation Quality Assessment**: Implement a blind evaluation where human annotators assess the accuracy of both DeepL and Helsinki translations for a subset of tweets across all languages, quantifying how translation errors correlate with model performance degradation.

3. **Cross-Conflict Generalization Test**: Apply the same fine-tuning methodology to a different geopolitical conflict scenario (e.g., Israel-Palestine) in the same language families to determine if the 6K tweet effectiveness threshold holds across different domains and if language-specific performance patterns persist.