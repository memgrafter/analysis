---
ver: rpa2
title: 'Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions
  Using RL from Knowledge Feedback'
arxiv_id: '2403.18349'
source_url: https://arxiv.org/abs/2403.18349
tags:
- arxiv
- knowledge
- reliability
- data
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of model reliability to address
  the problem of hallucinations in large language models (LLMs). The authors propose
  a novel alignment framework called Reinforcement Learning from Knowledge Feedback
  (RLKF) to improve LLM reliability.
---

# Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback

## Quick Facts
- arXiv ID: 2403.18349
- Source URL: https://arxiv.org/abs/2403.18349
- Reference count: 13
- One-line primary result: RLKF framework significantly improves LLM reliability by training models to refuse unknown questions using knowledge feedback and reinforcement learning

## Executive Summary
This paper addresses the critical problem of hallucinations in large language models by introducing the concept of model reliability and proposing a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). The framework leverages knowledge feedback to dynamically determine a model's knowledge boundary and trains a reliable reward model to encourage refusal of out-of-knowledge questions. Experiments on mathematical questions demonstrate that RLKF substantially enhances LLM reliability, achieving significant improvements in precision, accountability, and overall reliability compared to baseline methods.

## Method Summary
The RLKF framework introduces knowledge feedback to dynamically determine the model's knowledge boundary and trains a reliable reward model to encourage refusal of out-of-knowledge questions. The method involves constructing reliable preference data using knowledge feedback, training a reliable reward model on both generic helpfulness preference data and synthesized reliable preference data, and optimizing the LLM policy using reinforcement learning with PPO algorithm. The approach is evaluated on synthetic arithmetic questions and GSM8K datasets using a pre-trained LLAMA 2-CHAT 7B model.

## Key Results
- RLKF significantly enhances model reliability on both in-domain and out-of-domain datasets
- The framework achieves substantial improvements in precision, accountability, and overall reliability metrics
- The reliable reward model (RelyRM) demonstrates superior performance in detecting knowledge boundaries compared to traditional reward models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLKF uses self-consistency across multiple sampled responses to determine knowledge boundaries
- Mechanism: By analyzing the distribution of correctness across multiple samplings, the framework infers whether a question is within or beyond the model's knowledge boundary
- Core assumption: Self-consistency across multiple sampled responses is a reliable proxy for whether the model "knows" the answer
- Evidence anchors: [section 4.2.1]: "we determine whether a question lies within the model's knowledge boundary by analyzing the distribution of correctness across multiple samplings of model responses"
- Break condition: If self-consistency fails to correlate with actual knowledge, the boundary detection becomes unreliable

### Mechanism 2
- Claim: The reliable reward model is trained to differentiate between helpful responses and appropriate refusals
- Mechanism: RRM is first trained on generic helpfulness preference data, then fine-tuned with synthesized reliable preference data that includes refusal examples
- Core assumption: A reward model can be adapted to model-specific knowledge boundaries by fine-tuning with reliable preference data
- Evidence anchors: [section 4.3]: "We first train the reward model using generic helpfulness preference data... Then, we continuously train RM with the synthesized reliable preference data"
- Break condition: If the reward model overfits to the reliable preference data, it may become too conservative

### Mechanism 3
- Claim: PPO optimization using the RRM's scalar reward improves alignment with the model's knowledge boundary
- Mechanism: PPO fine-tunes the LLM policy to provide correct answers when possible and refuse when uncertain
- Core assumption: Optimizing the LLM policy against the RRM's reward will lead to better alignment with the model's knowledge boundary
- Evidence anchors: [section 5.2]: "RLKF significantly enhances the model's reliability on both in-domain and out-of-domain datasets"
- Break condition: If the RL optimization process diverges, the model may fail to learn the intended behavior

## Foundational Learning

- Concept: Self-consistency as a knowledge boundary detector
  - Why needed here: To infer whether a question is within or beyond the model's knowledge without external labels
  - Quick check question: If a model gives the same answer across multiple samplings, does that mean it "knows" the answer? (Answer: Not always; consistency can be due to biases or memorized patterns)

- Concept: Preference data synthesis for refusal learning
  - Why needed here: To teach the reward model that refusal can be preferable to an incorrect answer
  - Quick check question: Why is refusal included in the comparison pairs? (Answer: To explicitly train the reward model to value refusal over wrong answers)

- Concept: RLHF baseline comparison
  - Why needed here: RLKF builds on RLHF but modifies it to include knowledge feedback and refusal preference
  - Quick check question: What is the key difference between RLHF and RLKF reward models? (Answer: RLHF reward models are model-agnostic and focus on helpfulness; RLKF reward models are model-specific and include refusal preference)

## Architecture Onboarding

- Component map:
  - LLM Policy (LLAMA-2 7B) -> Reward Model (RM) -> Reliable Reward Model (RRM) -> PPO Optimizer -> LLM Policy
  - Knowledge Feedback Generator -> Reliable Preference Data
  - Self-Consistency Module -> Knowledge Boundary Detection

- Critical path:
  1. Generate knowledge feedback by sampling responses and checking consistency
  2. Synthesize reliable preference data (RPD) from feedback
  3. Train RRM on helpfulness preference data + RPD
  4. Use RRM to provide reward during PPO fine-tuning of LLM policy

- Design tradeoffs:
  - Using self-consistency vs. external verification: Self-consistency is cheaper but noisier; external verification is more accurate but costly
  - Including refusal in preference data: Improves honesty but may reduce overall answer rate
  - Training on in-domain vs. out-of-domain RPD: In-domain RPD is more reliable; out-of-domain RPD helps generalization but is noisier

- Failure signatures:
  - Model becomes overly conservative: RRM overweights refusal; check RPD quality and reward scaling
  - Model fails to refuse unknown questions: RRM underweights refusal; check knowledge feedback generation and RPD balance
  - Training instability: Reward signal too noisy; reduce PPO epochs or adjust reward scaling

- First 3 experiments:
  1. Run knowledge feedback generation on a small arithmetic test set; check if self-consistency correlates with correctness
  2. Train RRM on helpfulness preference data only; evaluate on synthesized reliable preference data to confirm it prefers responses over refusals
  3. Fine-tune LLM policy with PPO using RRM; measure reliability metrics on in-domain test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RLKF performance scale with model size, particularly when comparing Llama-2 7B to larger models like Llama-2 13B or GPT-4?
- Basis in paper: [inferred] The paper mentions resource constraints and DeepSpeed-Chat framework limitations as reasons for only testing on Llama-2 7B
- Why unresolved: Experiments were limited to Llama-2 7B due to resource constraints and framework limitations
- What evidence would resolve it: Conducting experiments with larger models and extensive hyperparameter tuning for the PPO training phase

### Open Question 2
- Question: How does RLKF perform on tasks outside of mathematical reasoning, such as knowledge-based question answering or creative writing?
- Basis in paper: [explicit] The paper explicitly states that "our method has only been experimented with mathematical reasoning problems"
- Why unresolved: Experiments were limited to mathematical reasoning tasks
- What evidence would resolve it: Conducting experiments on various tasks outside of mathematical reasoning

### Open Question 3
- Question: How does RLKF compare to other methods for improving LLM honesty and reliability, such as retrieval-augmented generation or uncertainty estimation techniques?
- Basis in paper: [inferred] The paper discusses other methods for mitigating hallucinations but does not directly compare RLKF to these methods
- Why unresolved: The paper does not provide a direct comparison between RLKF and other methods
- What evidence would resolve it: Conducting experiments comparing RLKF to other methods on the same tasks and datasets

## Limitations
- The synthetic arithmetic dataset construction details are underspecified, making it difficult to assess generalization beyond simple numerical patterns
- The reliable reward model's performance metrics lack baseline comparisons and statistical significance testing
- The core assumption that self-consistency reliably indicates knowledge boundaries lacks direct empirical validation

## Confidence
- High confidence: The RLKF framework's overall architecture and training procedure are clearly specified and logically coherent
- Medium confidence: The mechanism of using self-consistency to detect knowledge boundaries is plausible but not rigorously validated
- Low confidence: The paper claims substantial improvements without showing whether these gains come at the cost of helpfulness

## Next Checks
1. Test the correlation between self-consistency scores and actual knowledge by conducting controlled experiments where model responses are verified against ground truth
2. Evaluate the reliable reward model's ability to generalize beyond the synthetic arithmetic domain by testing on diverse question types
3. Measure the tradeoff between reliability and helpfulness by conducting human evaluations to determine whether RLKF-trained models refuse too many questions they could answer correctly