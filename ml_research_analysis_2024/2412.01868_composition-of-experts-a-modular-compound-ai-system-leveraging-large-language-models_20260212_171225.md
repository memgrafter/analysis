---
ver: rpa2
title: 'Composition of Experts: A Modular Compound AI System Leveraging Large Language
  Models'
arxiv_id: '2412.01868'
source_url: https://arxiv.org/abs/2412.01868
tags:
- expert
- experts
- llms
- router
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes the Composition of Experts (CoE), a modular
  compound AI system that dynamically routes inputs to the most suitable expert LLM
  from a set of available models. CoE uses a two-step routing approach: first classifying
  inputs into categories, then mapping categories to experts, enabling efficient resource
  utilization and improved performance.'
---

# Composition of Experts: A Modular Compound AI System Leveraging Large Language Models

## Quick Facts
- arXiv ID: 2412.01868
- Source URL: https://arxiv.org/abs/2412.01868
- Reference count: 13
- One-line primary result: CoE achieves 59.4 on Arena-Hard with 31B average active parameters

## Executive Summary
This paper introduces Composition of Experts (CoE), a modular compound AI system that dynamically routes inputs to the most suitable expert LLM from a set of available models. The system uses a two-step routing approach: first classifying inputs into categories, then mapping categories to experts, enabling efficient resource utilization and improved performance. Using open-weight LLMs and a semi-supervised approach to construct training data, CoE achieves competitive results on benchmarks like Arena-Hard and MT-Bench while maintaining significantly reduced computational overhead compared to individual expert models.

## Method Summary
CoE implements a two-step routing approach where inputs are first mapped to categories using text embeddings, then routed to the best expert for that category through a mixed-integer linear program optimization. The system is trained using a semi-supervised pipeline to construct high-quality training data, with the category router trained as a multi-class classifier and the category-to-expert mapping solved as an optimization problem. Robust-CoE extends this with uncertainty quantification to handle distribution shifts by routing uncertain prompts to a general category.

## Key Results
- Achieves 59.4 score on Arena-Hard with merely 31 billion average active parameters
- Scores 9.06 on MT-Bench with 54 billion average active parameters
- Demonstrates superior performance with reduced computational overhead compared to individual expert models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic routing through a category router followed by category-to-expert mapping enables efficient expert selection.
- Mechanism: Inputs are first mapped to categories using embeddings, then routed to the best expert for that category, minimizing active parameters while maintaining performance.
- Core assumption: Category embeddings provide enough separation to train a high-accuracy router.
- Evidence anchors:
  - [abstract] "CoE uses a two-step routing approach: first classifying inputs into categories, then mapping categories to experts"
  - [section] "2D t-SNE plot for prompt-embeddings... colors indicate the category label... clustering of different categories into distinct cluster"
  - [corpus] Weak - no explicit coverage of embedding-based category routing in related work.
- Break condition: Category embeddings become too overlapping for the router to achieve high accuracy.

### Mechanism 2
- Claim: Two-step training (category router first, then category-to-expert mapping) allows modular system updates.
- Mechanism: Router is trained as a multi-class classifier on category-labeled data, then category-to-expert mapping is solved as a MILP optimization problem.
- Core assumption: Router training data can be constructed in a semi-supervised way with high confidence.
- Evidence anchors:
  - [abstract] "We propose a two-step routing approach... followed by a category-to-expert mapping"
  - [section] "Training the category router requires labeled training data... high quality prompts in the training data are needed"
  - [corpus] Missing - no explicit mention of semi-supervised routing training in neighbors.
- Break condition: Semi-supervised labeling produces insufficient quality data for the router.

### Mechanism 3
- Claim: Robust-CoE with uncertainty quantification improves generalization to unseen prompt distributions.
- Mechanism: Router outputs uncertainty scores (entropy); prompts above threshold are routed to 'general' category with its own expert assignment.
- Core assumption: Uncertainty quantification can reliably detect distribution shift.
- Evidence anchors:
  - [abstract] "Robust-CoE with uncertainty quantified routing to adapt to new prompt workloads"
  - [section] "uncertainty quantification has been exhaustively studied in the active learning literature"
  - [corpus] Missing - no coverage of uncertainty quantification for routing in neighbors.
- Break condition: Uncertainty metric fails to distinguish distribution shift from normal variation.

## Foundational Learning

- Concept: Text embeddings and their separability for classification tasks
  - Why needed here: Category router relies on embeddings to cluster prompts into distinct categories
  - Quick check question: Can you explain why t-SNE visualization showed clear category clusters in the data?

- Concept: Mixed Integer Linear Programming formulation
  - Why needed here: Category-to-expert mapping is solved as an MILP to optimize expert selection under parameter budget
  - Quick check question: What is the difference between binary variables and integer variables in optimization?

- Concept: Semi-supervised learning and active learning principles
  - Why needed here: High-quality router training data is constructed semi-supervised from unlabeled prompts
  - Quick check question: How does entropy-based filtering improve the quality of semi-supervised labels?

## Architecture Onboarding

- Component map:
  - Router: k-NN classifier on text embeddings (intfloat/e5-mistral-7b-instruct)
  - Expert pool: Multiple open-weight LLMs (Qwen, Gemma, Llama variants)
  - Memory system: SN40L RDU with tiered DDR/HBM architecture
  - Training pipeline: Semi-supervised data construction → Router training → MILP optimization

- Critical path:
  1. Input prompt → text embedding
  2. Router (k-NN) → category prediction
  3. Category-to-expert mapping → expert selection
  4. Load expert from DDR to HBM
  5. Execute inference → return result

- Design tradeoffs:
  - Memory vs. performance: More experts in DDR increases coverage but requires more memory
  - Router complexity vs. accuracy: Simple k-NN is fast but may not capture complex patterns
  - Parameter budget vs. performance: Higher B allows better experts but increases serving cost

- Failure signatures:
  - Router accuracy drops below 95% → category separation insufficient
  - MILP solver fails to find feasible solution → parameter budget too restrictive
  - High entropy on most prompts → uncertainty threshold too low

- First 3 experiments:
  1. Train router on small seed dataset and evaluate category separation with t-SNE
  2. Solve MILP with synthetic win-rates to verify optimization works
  3. Deploy minimal CoE (B=7B) on a small benchmark to validate end-to-end flow

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit on the scalability of CoE when adding new expert LLMs, and how does this limit depend on the diversity of the expert pool?
- Basis in paper: Inferred from the discussion of modularity and the challenges of adding new experts to CoE.
- Why unresolved: The paper does not provide a theoretical analysis of the scalability limits of CoE or how they depend on expert pool diversity.
- What evidence would resolve it: A mathematical model or empirical study showing how CoE performance scales with the number of experts and their diversity.

### Open Question 2
- Question: How does the performance of CoE compare to traditional model ensemble methods when the number of experts is very large (e.g., 1000+ experts)?
- Basis in paper: Inferred from the comparison of CoE to model ensembles and the discussion of inference costs.
- Why unresolved: The paper only evaluates CoE with a small number of experts (5) and does not compare its performance to ensemble methods at scale.
- What evidence would resolve it: An empirical comparison of CoE and ensemble methods using a large number of experts on various benchmarks.

### Open Question 3
- Question: What is the impact of the category router's accuracy on the overall performance of CoE, and how can we optimize the category router to minimize this impact?
- Basis in paper: Inferred from the discussion of the category router's role in CoE and the importance of high accuracy.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between category router accuracy and CoE performance, nor does it discuss optimization strategies.
- What evidence would resolve it: An empirical study showing how CoE performance varies with category router accuracy, along with proposed optimization techniques for the router.

### Open Question 4
- Question: How does the choice of text embedding model affect the performance of the category router, and what are the best practices for selecting an embedding model for CoE?
- Basis in paper: Inferred from the use of a specific text embedding model (intfloat/e5-mistral-7b-instruct) and the discussion of embedding quality.
- Why unresolved: The paper does not explore the impact of different text embedding models on category router performance or provide guidelines for model selection.
- What evidence would resolve it: An empirical comparison of CoE performance using different text embedding models, along with a set of best practices for model selection based on the findings.

### Open Question 5
- Question: How does the uncertainty quantification approach in Robust-CoE perform when the input prompt distribution significantly differs from the training data distribution?
- Basis in paper: Inferred from the introduction of Robust-CoE and its uncertainty quantification approach.
- Why unresolved: The paper does not provide a detailed analysis of Robust-CoE's performance under significant distribution shifts or discuss potential limitations of the approach.
- What evidence would resolve it: An empirical study evaluating Robust-CoE's performance on prompts from significantly different distributions than the training data, along with an analysis of the approach's limitations and potential improvements.

## Limitations

- The system's performance relies heavily on the quality of text embeddings for category separation, which may degrade with more diverse or complex prompt distributions
- The semi-supervised training pipeline assumes that k-NN clustering will produce reliable category labels, but this assumption may not hold across all domains
- The MILP formulation requires careful tuning of constraints and win-rate calculations that aren't fully specified in the paper

## Confidence

- **High Confidence**: The two-step routing architecture and its implementation on SambaNova SN40L hardware are well-specified and reproducible. The parameter budget optimization and memory hierarchy design are clearly articulated.
- **Medium Confidence**: The semi-supervised training pipeline and category separation results appear sound based on t-SNE visualizations, but the generalizability to other domains remains untested. The MILP formulation is theoretically correct but implementation details are sparse.
- **Low Confidence**: The uncertainty quantification mechanism for Robust-CoE lacks empirical validation on diverse distribution shifts. The assumption that category embeddings will remain separable as the prompt space grows is not rigorously tested.

## Next Checks

1. Test category router performance on out-of-distribution prompts from domains not represented in the training data to validate Robust-CoE's uncertainty quantification.
2. Conduct ablation studies removing the MILP optimization step to quantify the performance gain from optimal category-to-expert mapping versus random assignment.
3. Evaluate the system's behavior when the parameter budget B is varied across different operational scenarios to identify the sweet spot between performance and cost.