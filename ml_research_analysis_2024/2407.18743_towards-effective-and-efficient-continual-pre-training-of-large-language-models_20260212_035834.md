---
ver: rpa2
title: Towards Effective and Efficient Continual Pre-training of Large Language Models
arxiv_id: '2407.18743'
source_url: https://arxiv.org/abs/2407.18743
tags:
- data
- training
- scientific
- synthetic
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a continual pre-training approach for Llama-3
  (8B) to enhance Chinese language ability and scientific reasoning ability while
  preserving original capabilities. The method uses a two-stage training strategy:
  bilingual adaptation and synthetic enhancement, with carefully curated data mixtures
  including real-world and synthetic datasets.'
---

# Towards Effective and Efficient Continual Pre-training of Large Language Models

## Quick Facts
- arXiv ID: 2407.18743
- Source URL: https://arxiv.org/abs/2407.18743
- Authors: Jie Chen; Zhipeng Chen; Jiapeng Wang; Kun Zhou; Yutao Zhu; Jinhao Jiang; Yingqian Min; Wayne Xin Zhao; Zhicheng Dou; Jiaxin Mao; Yankai Lin; Ruihua Song; Jun Xu; Xu Chen; Rui Yan; Zhewei Wei; Di Hu; Wenbing Huang; Ji-Rong Wen
- Reference count: 28
- Primary result: Llama-3 (8B) enhanced with Chinese language ability and scientific reasoning capability while preserving original capabilities

## Executive Summary
This paper presents a continual pre-training approach for Llama-3 (8B) that enhances Chinese language ability and scientific reasoning while preserving original English capabilities. The method uses a two-stage training strategy: bilingual adaptation and synthetic enhancement, with carefully curated data mixtures including real-world and synthetic datasets. Extensive experiments show significant performance improvements: +8.81 on C-Eval, +6.31 on CMMLU, +12.00 on MATH, and +4.13 on SciEval, without hurting original English capabilities.

## Method Summary
The method employs a two-stage continual pre-training approach. First, a bilingual adaptation stage uses topic-based data mixture with perplexity-based curriculum learning on 98.5B tokens of real data to build language foundations. Second, a synthetic enhancement stage adds 1.5B tokens of multidisciplinary scientific QA pairs to improve reasoning capabilities. The training uses topic classifiers for fine-grained data control and monitors perplexity changes to prevent catastrophic forgetting. The entire process requires only ~100B training tokens while achieving significant performance gains across Chinese, scientific, and mathematical benchmarks.

## Key Results
- +8.81 improvement on C-Eval benchmark
- +6.31 improvement on CMMLU benchmark  
- +12.00 improvement on MATH benchmark
- +4.13 improvement on SciEval benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic QA data improves scientific reasoning without harming general capabilities when properly mixed
- Mechanism: The model learns structured knowledge through QA format while maintaining general knowledge via balanced data mixture
- Core assumption: Synthetic data quality is sufficient and properly curated
- Evidence anchors: [abstract], [section 3.3], corpus inference

### Mechanism 2
- Claim: Topic-based data mixture with PPL-based curriculum balances learning across domains
- Mechanism: Fine-grained topic classification + PPL tracking enables adaptive sampling that prevents catastrophic forgetting
- Core assumption: Topic classifier is accurate enough to distinguish relevant categories
- Evidence anchors: [section 3.2.1], corpus inference

### Mechanism 3
- Claim: Two-stage training (bilingual adaptation → synthetic enhancement) enables efficient capability development
- Mechanism: First stage builds language foundation, second stage adds specialized knowledge without destabilizing base capabilities
- Core assumption: Stage separation prevents interference between different learning objectives
- Evidence anchors: [section 3.1], [section 4.2], corpus validation

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper explicitly addresses preventing original capabilities from degrading during CPT
  - Quick check question: What is the primary risk when adapting a pre-trained model to new domains?

- Concept: Data mixture and curriculum learning
  - Why needed here: The paper's core innovation involves sophisticated data curation strategies to balance new and old abilities
  - Quick check question: How does adjusting data mixture ratios based on performance metrics help maintain balanced capabilities?

- Concept: Synthetic data generation for domain adaptation
  - Why needed here: The paper generates synthetic scientific QA pairs to enhance reasoning abilities efficiently
  - Quick check question: What are the key advantages of using synthetic data versus collecting real domain-specific data?

## Architecture Onboarding

- Component map: Llama-3-8B base model → Bilingual adaptation stage (92.5B tokens) → Synthetic enhancement stage (7.5B tokens) → Topic classifiers + PPL monitoring system
- Critical path: Data preparation → Two-stage training → Evaluation across benchmarks
- Design tradeoffs: High-quality synthetic data vs. compute cost, fine-grained topic control vs. classifier accuracy, stage separation vs. training efficiency
- Failure signatures: Performance degradation on original benchmarks, synthetic data contamination, classifier drift, stage interference
- First 3 experiments:
  1. Validate topic classifier accuracy on held-out web pages
  2. Test PPL-based curriculum on small dataset with known difficulty progression
  3. Generate and validate synthetic QA pairs on a single scientific discipline before full-scale synthesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of synthetic to real data for continual pre-training of LLMs across different domains?
- Basis in paper: [explicit] The paper explores varying synthetic data ratios (1:10, 2:10, 3:10, 4:10) and finds performance peaks at 2:10 before declining at higher ratios
- Why unresolved: The study only tested ratios up to 4:10; higher ratios or domain-specific optimal ratios were not explored
- What evidence would resolve it: Systematic testing of synthetic-to-real data ratios from 5:10 to 9:10 across multiple domains (scientific, coding, general knowledge) with performance comparison

### Open Question 2
- Question: How does synthetic data quality degradation impact model performance in different learning stages?
- Basis in paper: [explicit] The paper tests corruption levels from 0.0 to 0.7 and finds performance degrades significantly above 0.5
- Why unresolved: The study only tests uniform corruption across the entire dataset; it doesn't examine staged corruption or degradation in specific knowledge domains
- What evidence would resolve it: Experiments applying different corruption levels to synthetic data in different curriculum stages, measuring domain-specific performance impacts

### Open Question 3
- Question: What is the relationship between synthetic data diversity and model generalization across unseen tasks?
- Basis in paper: [inferred] The paper generates synthetic data from nine scientific disciplines but doesn't test generalization to unseen disciplines or interdisciplinary tasks
- Why unresolved: The evaluation focuses on known benchmarks; transfer to novel problem types or interdisciplinary reasoning was not tested
- What evidence would resolve it: Testing model performance on tasks requiring knowledge synthesis across multiple disciplines not represented in the synthetic training data

### Open Question 4
- Question: How does the format of synthetic data (QA pairs vs. natural text) affect different types of reasoning capabilities?
- Basis in paper: [explicit] The paper generates synthetic data in QA format but compares it against normal text data without isolating the format effect
- Why unresolved: The performance gains from synthetic data could be due to content quality, format alignment with downstream tasks, or both
- What evidence would resolve it: Controlled experiments generating equivalent synthetic content in both QA and natural text formats, comparing performance across different task types

### Open Question 5
- Question: What is the optimal curriculum strategy for synthetic data across different model sizes and pre-training histories?
- Basis in paper: [explicit] The paper finds easy-to-difficult curriculum works best for TinyLlama but doesn't test different strategies for larger models or models with different pre-training
- Why unresolved: The curriculum strategy may depend on model capacity, pre-existing knowledge, or the complexity of the synthetic data
- What evidence would resolve it: Systematic testing of curriculum strategies (easy-to-difficult, difficult-to-easy, mixed, discipline-based) across multiple model sizes and pre-training configurations

## Limitations
- Synthetic Data Quality Dependence: Heavy reliance on Mistral-7B-generated synthetic data without independent quality validation
- Topic Classifier Accuracy: No validation of classifier accuracy or error rates, creating potential single point of failure
- Resource Requirements: Substantial computational requirements limit practical applicability for many research groups

## Confidence
- Performance Improvements: High
- Two-Stage Training Strategy: High
- Catastrophic Forgetting Prevention: Medium
- Synthetic Data Effectiveness: Low

## Next Checks
1. **Topic Classifier Validation**: Implement held-out validation set with human-labeled topics to measure actual classifier accuracy
2. **Synthetic Data Quality Audit**: Generate random sample of synthetic QA pairs and have domain experts evaluate correctness and relevance
3. **Ablation Study on Training Stages**: Run experiments with only bilingual adaptation and only synthetic enhancement to quantify marginal contributions