---
ver: rpa2
title: Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms
arxiv_id: '2406.02900'
source_url: https://arxiv.org/abs/2406.02900
tags:
- reward
- training
- should
- learning
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates reward over-optimization in Direct Alignment
  Algorithms (DAAs) like DPO, IPO, and SLiC. While DAAs avoid the traditional RLHF
  pipeline's reward model, they still exhibit similar over-optimization patterns at
  higher KL budgets.
---

# Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms

## Quick Facts
- arXiv ID: 2406.02900
- Source URL: https://arxiv.org/abs/2406.02900
- Reference count: 40
- Key outcome: DAAs exhibit reward over-optimization patterns similar to RLHF, with performance degrading at higher KL budgets and before completing single epochs

## Executive Summary
This paper investigates reward over-optimization in Direct Alignment Algorithms (DAAs) like DPO, IPO, and SLiC, which aim to align LLMs with human preferences without explicit reward modeling. The authors find that DAAs suffer from similar over-optimization issues as traditional RLHF pipelines, with performance deteriorating across various KL budgets. Through extensive experiments with Pythia models (1B, 2.8B, 6.9B parameters) on the TL;DR dataset, they demonstrate that DAAs place probability mass on out-of-distribution sequences due to the under-constrained nature of the optimization problem. The work develops scaling laws showing that smaller models and higher KL budgets exhibit stronger over-optimization, highlighting fundamental challenges in LLM alignment even without explicit reward modeling.

## Method Summary
The paper evaluates DAAs (DPO, IPO, SLiC) across different model scales (1B, 2.8B, 6.9B Pythia parameters) on the Reddit TL;DR summarization dataset. Models undergo supervised fine-tuning on TL;DR data, followed by DAA training with KL penalty coefficients β ranging from 0.01 to 1.0. The optimization uses RMSProp with learning rate 0.5 × 10⁻⁶ and batch size 128. Performance is evaluated using GPT-4 as judge for win rates against human answers, KL divergence between optimized and reference policies, and implicit reward accuracy. The study also develops scaling laws relating model capacity, KL budget, and over-optimization severity.

## Key Results
- DAAs exhibit hump-shaped performance curves as a function of KL budget, with degradation occurring before completing a single epoch
- Smaller models (1B) show stronger over-optimization than larger models (6.9B), with length regularization reducing but not eliminating the issue
- DAAs place probability mass on out-of-distribution sequences due to under-constrained optimization, with IPO showing stronger length exploitation than DPO
- Scaling laws demonstrate that model capacity and KL budget jointly influence over-optimization severity, with weaker models extrapolating more strongly on simple features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DAAs place probability mass on out-of-distribution (OOD) sequences due to the under-constrained nature of the optimization problem
- Mechanism: The loss function used in DAAs is not strictly convex, allowing multiple global optima. Some of these optima place high weight on OOD responses because the distributional constraint of the policy is not adequately enforced.
- Core assumption: The intersection of the null space of the query matrix Q and the span of the win-or-lose prompt-response vectors is non-trivial, and there exists at least one OOD response for each prompt.
- Evidence anchors:
  - [section] "The second constraint of proposition 1 is often trivially satisfied by the dimension of the response space as we are unlikely to see every response to a prompt. The first constraint is harder, but can be satisfied by conflicting preferences."
  - [section] "This means that the expected implicit reward represents a forward KL divergence between the reference policy and the optimization policy, thus it is expected to be negative and decrease with training as the optimization model moves away from the reference."
- Break condition: If the DAA loss function is modified to be strictly convex or if the distributional constraint is made more stringent.

### Mechanism 2
- Claim: DAAs suffer from OOD bootstrapping, similar to offline RL, due to the dense per-token reward function
- Mechanism: The dense per-token reward function used in DAAs leads to an implicit Q-learning formulation where the value function is bootstrapped from OOD tokens. This bootstrapping becomes more optimistic with smaller beta values, exacerbating the OOD issue.
- Core assumption: The dense per-token reward function is strictly more general than the sparse reward function, and the Bellman equation holds for the optimal Q-function.
- Evidence anchors:
  - [section] "With a direct substitution, we then have Q∗(yi, (x, y<i)) = r(x, y≤i) + β log πref(yi|(x, y<i)) + β log Xyi∈|V| eQ∗(y,(x,y<i))/β | {z } OOD bootstrapping"
  - [section] "For small values of beta the estimate β log Xyi∈|V| eQ∗(y,(x,y<i))/β ≈ maxy∈|V| Q∗(y, (x, y<i)) that is smaller parameter values yield a more optimistic estimate, which results in a higher level of OOD bootstrapping."
- Break condition: If the dense per-token reward function is replaced with a sparse reward function or if the beta parameter is set to a large value.

### Mechanism 3
- Claim: Model capacity and KL budget influence the degree of OOD extrapolation, with smaller models and lower KL budgets leading to stronger extrapolation
- Mechanism: Under limited capacity (either from model size or KL budget), the model extrapolates more strongly based on simpler features like length, which can lead to OOD issues.
- Core assumption: The model's extrapolation behavior is influenced by its capacity and the complexity of the features it prioritizes.
- Evidence anchors:
  - [section] "We observe two main effects; first, there is a clear scaling law behavior. Weaker models extrapolate across the simple length feature to a much higher degree than stronger ones."
  - [section] "Based on these results we formulate the hypothesis that under limited capacity, either from model capability or limited KL budgets, the model will extrapolate more strongly based on simpler features, which can lead to OOD issues."
- Break condition: If the model capacity is increased or if the KL budget is set to a higher value.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding the standard RLHF pipeline and its limitations is crucial for appreciating the motivation behind Direct Alignment Algorithms (DAAs) and their over-optimization issues.
  - Quick check question: What are the three main stages of the RLHF pipeline, and how does each stage contribute to the alignment of LLMs with human preferences?

- Concept: Reward Over-optimization
  - Why needed here: Reward over-optimization is a key phenomenon observed in both RLHF and DAAs, and understanding its causes and consequences is essential for interpreting the results of this paper.
  - Quick check question: What are the two main explanations for reward over-optimization in RLHF, and how do they relate to the issues observed in DAAs?

- Concept: Convexity and Optimization
  - Why needed here: The convexity of the loss function used in DAAs plays a crucial role in determining the existence of multiple global optima and the allocation of probability mass to OOD sequences.
  - Quick check question: Why is the DAA loss function not strictly convex, and how does this non-convexity contribute to the over-optimization problem?

## Architecture Onboarding

- Component map: Supervised fine-tuned LLM -> DAA training (DPO/IPO/SLiC) -> Aligned LLM with reduced reward over-optimization

- Critical path:
  1. Initialize LLM with supervised fine-tuning on the TL;DR dataset
  2. Apply the chosen DAA loss function to align the LLM with human preferences
  3. Evaluate the aligned LLM using GPT-4 as a judge for win rates and KL divergence

- Design tradeoffs:
  - Model size vs. over-optimization: Larger models are less prone to over-optimization but require more computational resources
  - KL budget vs. performance: Higher KL budgets lead to better performance up to a point, after which over-optimization sets in
  - Loss function choice: Different DAA loss functions (DPO, IPO, SLiC) exhibit varying degrees of over-optimization and KL control

- Failure signatures:
  - Hump-shaped performance curve as a function of KL budget
  - Degradation in performance within a single epoch of training
  - Strong correlation between model length and performance, indicating over-optimization on simple features

- First 3 experiments:
  1. Evaluate the over-optimization phenomenon in DAAs using different model sizes (1B, 2.8B, 6.9B) and loss functions (DPO, IPO, SLiC) on the TL;DR dataset
  2. Develop scaling laws for reward model over-optimization in DAAs and compare them to those observed in RLHF
  3. Investigate the effect of length regularization on over-optimization and the relationship between model capacity, KL budget, and OOD extrapolation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Direct Alignment Algorithms (DAAs) behave at model scales larger than those tested in this paper?
- Basis in paper: [inferred] The paper explicitly states computational limitations prevented testing at larger scales, yet scaling effects were observed within the tested range.
- Why unresolved: The authors acknowledge that their results might not generalize to larger models due to computational constraints.
- What evidence would resolve it: Empirical results showing over-optimization patterns in DAAs at scales beyond 6.9B parameters, particularly examining whether the scaling laws hold.

### Open Question 2
- Question: Can the under-constrained nature of DAA optimization be formally characterized to prevent out-of-distribution (OOD) behavior?
- Basis in paper: [explicit] The authors demonstrate that DAA objectives are under-constrained and allow for OOD solutions, proposing a theoretical framework for understanding this issue.
- Why unresolved: While the paper identifies the problem and provides a theoretical explanation, it does not offer concrete solutions or regularization techniques to address the under-constrained nature.
- What evidence would resolve it: Development and empirical validation of regularization methods or alternative optimization frameworks that constrain DAA solutions to avoid OOD behavior.

### Open Question 3
- Question: What is the relationship between the β coefficient in DAAs and the extent of reward over-optimization?
- Basis in paper: [explicit] The authors observe that different β values affect KL divergence and performance, noting that smaller β values yield more optimistic estimates leading to higher OOD bootstrapping.
- Why unresolved: The paper provides initial observations but does not systematically explore the full parameter space or establish definitive scaling relationships between β and over-optimization.
- What evidence would resolve it: Comprehensive empirical studies mapping β values to over-optimization metrics across multiple model sizes and datasets, establishing clear thresholds or guidelines for β selection.

## Limitations
- Theoretical analysis relies on simplifying assumptions about reward functions and optimization landscapes that may not fully capture practical scenarios
- Scaling laws developed lack theoretical grounding explaining why relationships hold across different model families and datasets
- Results may be specific to the TL;DR dataset and Pythia model family, requiring validation on diverse preference datasets

## Confidence

- **High confidence**: The empirical observation of over-optimization in DAAs across different KL budgets and model sizes (Section 5). The consistency of hump-shaped performance curves across DPO, IPO, and SLiC variants provides strong evidence.
- **Medium confidence**: The theoretical explanation of OOD bootstrapping through implicit Q-learning (Section 4). While the mathematical derivation is sound, the practical implications require further validation.
- **Medium confidence**: The scaling laws relating model capacity, KL budget, and over-optimization severity (Section 6). These patterns are empirically robust but lack theoretical explanation.

## Next Checks

1. **Cross-dataset validation**: Replicate the over-optimization experiments on different preference datasets (e.g., Anthropic HH, open-source human preference datasets) to verify whether the observed scaling laws generalize beyond the TL;DR dataset.

2. **Theoretical formalization**: Develop rigorous mathematical proofs for the relationship between beta values, KL divergence, and OOD bootstrapping, potentially using information-theoretic bounds on the optimization landscape.

3. **Architecture ablation**: Systematically vary the model architecture (different attention mechanisms, layer normalization strategies) while holding the scaling law variables constant to isolate whether the observed patterns stem from architectural properties versus model capacity alone.