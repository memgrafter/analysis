---
ver: rpa2
title: Exploring SSL Discrete Tokens for Multilingual ASR
arxiv_id: '2409.08805'
source_url: https://arxiv.org/abs/2409.08805
tags:
- discrete
- tokens
- speech
- multilingual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the use of SSL-generated discrete tokens\
  \ as input features for multilingual ASR, focusing on seven non-English languages\
  \ (German, Dutch, French, Spanish, Italian, Portuguese, Polish) using the 6000-hour\
  \ Multilingual LibriSpeech corpus. The study compares three types of discrete tokens\u2014\
  semantic tokens from WavLM-Large and XLSR-53 models, and acoustic tokens from EnCodec\u2014\
  against traditional Fbank features in both monolingual and multilingual settings."
---

# Exploring SSL Discrete Tokens for Multilingual ASR

## Quick Facts
- arXiv ID: 2409.08805
- Source URL: https://arxiv.org/abs/2409.08805
- Authors: Mingyu Cui; Daxin Tan; Yifan Yang; Dingdong Wang; Huimeng Wang; Xiao Chen; Xie Chen; Xunying Liu
- Reference count: 35
- Key outcome: Discrete tokens from fine-tuned XLSR-53 achieve comparable or superior performance to Fbank features, with average WER reductions of 0.31% and 1.76% absolute on dev and test sets respectively

## Executive Summary
This paper investigates the use of SSL-generated discrete tokens as input features for multilingual ASR across seven non-English languages. The study compares semantic tokens from WavLM-Large and XLSR-53 models, and acoustic tokens from EnCodec, against traditional Fbank features using the Zipformer-Transducer architecture. Results show that fine-tuned XLSR-53 discrete tokens outperform Fbank features, particularly for low-resource languages like Polish, while training with discrete tokens is significantly faster than with Fbank features. Multilingual training using shared k-means clustering underperforms monolingual training, suggesting language-specific tokenization remains optimal for ASR performance.

## Method Summary
The study evaluates three types of discrete tokens—semantic tokens from WavLM-Large and XLSR-53 models, and acoustic tokens from EnCodec—as input features for multilingual ASR. The researchers fine-tuned XLSR-53 on seven target languages (German, Dutch, French, Spanish, Italian, Portuguese, Polish) before extracting discrete tokens using k-means clustering. These tokens were compared against traditional Fbank features using the Zipformer-Transducer architecture on the 6000-hour Multilingual LibriSpeech corpus. Models were trained for 40 epochs for languages with >1000 hours and 150 epochs for <1000 hours, with data augmentation techniques applied throughout training.

## Key Results
- Discrete tokens from fine-tuned XLSR-53 achieve comparable or superior performance to Fbank features, with average WER reductions of 0.31% and 1.76% absolute on dev and test sets respectively
- Particularly strong results for Polish with 6.82% absolute WER reduction, and training time less than 35% of Fbank-based training
- Multilingual training using shared k-means clustering underperforms monolingual training, suggesting language-specific tokenization remains optimal
- Training with discrete tokens is significantly faster than with Fbank features across all languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning XLSR-53 on specific languages improves discrete token quality for those languages compared to using a generic model.
- Mechanism: The fine-tuned XLSR-53 model learns language-specific acoustic patterns and semantic structures, producing discrete tokens that better capture the phonetic and prosodic features of each target language.
- Core assumption: Language-specific fine-tuning of SSL models results in discrete tokens that are more representative of the target language's acoustic characteristics.
- Evidence anchors:
  - [abstract] "Moreover, we fine-tuned XLSR-53 for the seven language domains before extracting discrete tokens."
  - [section] "We fine-tuned XLSR-53 for the seven language domains before extracting discrete tokens."
- Break condition: If the language-specific fine-tuning doesn't improve WER performance compared to the generic model, or if the improvement is marginal across all languages.

### Mechanism 2
- Claim: Discrete tokens provide faster training times compared to Fbank features due to their compressed representation of acoustic information.
- Mechanism: Discrete tokens, being indices rather than continuous vectors, require less computational resources for processing and can be learned more efficiently by the ASR model.
- Core assumption: The compact representation of discrete tokens allows for more efficient computation and faster convergence during training.
- Evidence anchors:
  - [abstract] "Training with discrete tokens was significantly faster than with Fbank features, especially for low-resource languages like Polish (less than 35% of Fbank training time)."
  - [section] "The substantial reduction in training time can be attributed to the compact representation provided by discrete tokens, which effectively condense the rich acoustic information present in speech signals."
- Break condition: If training with discrete tokens does not result in faster convergence or if the time savings are negligible.

### Mechanism 3
- Claim: Zipformer-Transducer architecture is well-suited for processing discrete tokens in multilingual ASR tasks.
- Mechanism: The Zipformer's efficient encoder structure, combined with the Transducer's streaming capabilities, effectively handles the discrete token input while maintaining strong performance across multiple languages.
- Core assumption: The Zipformer-Transducer architecture can effectively process discrete tokens and leverage their information for accurate ASR.
- Evidence anchors:
  - [abstract] "The Zipformer-Transducer architecture was employed for all experiments."
  - [section] "The Zipformer Transducer [31] architecture is adopted for ASR implemented with the k2 and icefall framework."
- Break condition: If the Zipformer-Transducer architecture does not outperform other architectures when using discrete tokens, or if it shows significant degradation in multilingual settings.

## Foundational Learning

- Concept: Self-Supervised Learning (SSL) in speech processing
  - Why needed here: Understanding how SSL models like WavLM and XLSR-53 generate discrete tokens is crucial for grasping the paper's methodology.
  - Quick check question: What are the main differences between WavLM and XLSR-53 in terms of their training data and objectives?

- Concept: Discrete token generation from SSL models
  - Why needed here: The paper relies on converting continuous speech representations into discrete tokens using k-means clustering or direct quantization.
  - Quick check question: How does the k-means clustering approach differ from EnCodec's direct quantization method in generating discrete tokens?

- Concept: Zipformer-Transducer architecture
  - Why needed here: This architecture is used for the ASR models in the paper, and understanding its components is essential for interpreting the results.
  - Quick check question: What are the key differences between Zipformer and Conformer architectures, and how do these differences impact performance with discrete tokens?

## Architecture Onboarding

- Component map:
  - Audio input → Discrete token generation (WavLM, XLSR-53, or EnCodec) → Zipformer encoder → Stateless predictor → Joint network → Output
  - Each discrete token model (WavLM, XLSR-53, EnCodec) generates different types of tokens for input to the ASR system
  - Zipformer encoder processes the discrete tokens efficiently
  - Stateless predictor handles label prediction
  - Joint network combines encoder and predictor outputs

- Critical path:
  - Discrete token generation and input preparation
  - Zipformer encoder processing
  - Joint network computation for output probability
  - Model training and evaluation

- Design tradeoffs:
  - Using fine-tuned vs. generic SSL models for discrete token generation
  - Choice between semantic tokens (WavLM, XLSR-53) and acoustic tokens (EnCodec)
  - Trade-off between model complexity and training efficiency
  - Balancing language-specific tokenization vs. shared tokenization for multilingual tasks

- Failure signatures:
  - Poor WER performance across languages, especially for low-resource languages
  - Inefficient training times not improving over Fbank features
  - Model convergence issues, particularly with shared k-means clustering
  - Degradation in performance when scaling to more languages or longer training durations

- First 3 experiments:
  1. Implement Zipformer-Transducer with Fbank features as baseline, comparing performance to discrete token inputs.
  2. Test different SSL models (WavLM, XLSR-53, EnCodec) for discrete token generation, evaluating their impact on ASR performance.
  3. Compare monolingual training with discrete tokens to multilingual training using shared k-means clustering, assessing the effect on WER and training efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of discrete tokens vary across different language families or phonological types in multilingual ASR?
- Basis in paper: [explicit] The paper evaluates seven languages but does not analyze performance differences based on language families or phonological characteristics.
- Why unresolved: The study treats all seven languages as a single group without investigating whether certain language families benefit more from discrete tokens than others.
- What evidence would resolve it: Comparative analysis of WER performance across language families (e.g., Romance vs. Germanic vs. Slavic) when using discrete tokens versus Fbank features.

### Open Question 2
- Question: What is the optimal number of k-means clusters for discrete tokenization in multilingual ASR across different languages?
- Basis in paper: [explicit] The ablation study shows shared k-means with 2000 clusters underperforms monolingual models, but the optimal cluster count for multilingual scenarios remains unclear.
- Why unresolved: The paper only tests 2000 and 4000 clusters for shared k-means, leaving the question of whether intermediate or higher cluster counts might yield better multilingual performance.
- What evidence would resolve it: Systematic evaluation of WER performance across a range of cluster counts (e.g., 2000-10000) for both monolingual and multilingual models.

### Open Question 3
- Question: How do different SSL model architectures (semantic vs. acoustic tokens) affect ASR performance across languages with varying acoustic characteristics?
- Basis in paper: [explicit] The paper compares WavLM-Large, XLSR-53, and EnCodec but does not analyze their relative performance based on language-specific acoustic properties.
- Why unresolved: While the paper notes that EnCodec underperforms, it does not investigate why certain token types might be better suited for specific language characteristics.
- What evidence would resolve it: Detailed analysis correlating SSL model architecture performance with language-specific acoustic features (e.g., tonal languages, consonant clusters, vowel harmony).

### Open Question 4
- Question: What is the impact of language similarity on the effectiveness of multilingual training with discrete tokens?
- Basis in paper: [inferred] The paper shows multilingual training underperforms monolingual training, but does not explore whether related languages benefit more from multilingual approaches.
- Why unresolved: The study treats all seven languages equally without investigating whether certain language pairs or groups might benefit from shared tokenization.
- What evidence would resolve it: Comparative analysis of multilingual model performance when training on language pairs with varying degrees of similarity (e.g., DE-NL vs. DE-PL).

## Limitations

- Limited language diversity with focus on European languages from similar linguistic families
- Single architecture constraint using only Zipformer-Transducer without comparison to alternatives
- Lack of detailed ablations for critical design choices such as fine-tuning duration and k-means parameters
- Absence of normalized efficiency metrics beyond absolute training time differences

## Confidence

**High Confidence**:
- Discrete tokens from fine-tuned XLSR-53 outperform Fbank features with average WER reductions of 0.31% (dev) and 1.76% (test)
- Training with discrete tokens is significantly faster than Fbank features, especially for low-resource languages like Polish (less than 35% of Fbank training time)
- Shared k-means clustering across languages underperforms monolingual training

**Medium Confidence**:
- Zipformer-Transducer architecture is well-suited for processing discrete tokens in multilingual ASR tasks
- Fine-tuning XLSR-53 on specific languages improves discrete token quality for those languages compared to using a generic model
- Discrete tokens provide faster training times compared to Fbank features due to their compressed representation

**Low Confidence**:
- The specific mechanism by which fine-tuning XLSR-53 improves token quality for each language (only weak supporting evidence from corpus)
- Zipformer-Transducer architecture's superiority for discrete tokens over other architectures (no comparative studies with alternatives)
- The optimal number of clusters for k-means discretization across different languages (no sensitivity analysis provided)

## Next Checks

1. **Architecture comparison experiment**: Replicate the study using alternative ASR architectures (Conformer-Transducer, CTC-Transformer) with the same discrete token inputs to verify if Zipformer's performance advantage is architecture-specific or generalizes across models.

2. **K-means parameter sensitivity analysis**: Conduct systematic experiments varying the number of clusters (e.g., 100, 200, 500, 1000) for each language independently to determine optimal discretization granularity and its relationship to language characteristics and resource availability.

3. **Cross-lingual transfer validation**: Design experiments where discrete tokens are extracted from one language's SSL model and tested on another language to quantify the language-specificity of token quality and inform decisions about shared vs. language-specific tokenization strategies in truly low-resource scenarios.