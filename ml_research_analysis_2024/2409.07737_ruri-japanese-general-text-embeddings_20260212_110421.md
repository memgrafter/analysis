---
ver: rpa2
title: 'Ruri: Japanese General Text Embeddings'
arxiv_id: '2409.07737'
source_url: https://arxiv.org/abs/2409.07737
tags:
- datasets
- japanese
- training
- used
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of high-performance general-purpose
  Japanese text embedding models by developing Ruri, a series of Japanese text embeddings.
  The core method involves contrastive pre-training using synthesized datasets generated
  by LLMs, constructing a Japanese reranker for dataset filtering and knowledge distillation,
  and fine-tuning on high-quality datasets.
---

# Ruri: Japanese General Text Embeddings

## Quick Facts
- **arXiv ID:** 2409.07737
- **Source URL:** https://arxiv.org/abs/2409.07737
- **Authors:** Hayato Tsukagoshi; Ryohei Sasano
- **Reference count:** 14
- **Primary result:** Developed Ruri, a series of Japanese text embeddings that outperform existing models on the Japanese Massive Text Embedding Benchmark (JMTEB) with an average score of 73.31

## Executive Summary
This paper addresses the lack of high-performance general-purpose Japanese text embedding models by developing Ruri, a series of Japanese text embeddings. The core method involves contrastive pre-training using synthesized datasets generated by LLMs, constructing a Japanese reranker for dataset filtering and knowledge distillation, and fine-tuning on high-quality datasets. The model demonstrates state-of-the-art performance on multiple Japanese retrieval benchmarks, outperforming existing multilingual and Japanese embedding models.

## Method Summary
The Ruri model development follows a two-stage training approach: first, contrastive pre-training on large-scale synthesized datasets (AutoWikiQA and AutoWikiNLI) generated using LLMs, then supervised fine-tuning on high-quality labeled datasets. A Japanese reranker is constructed in two stages and used for knowledge distillation to improve the dual-encoder model's retrieval performance. The approach uses task-homogeneous batching, improved contrastive loss with in-batch negatives, and hard negative mining to enhance training efficiency and performance.

## Key Results
- Ruri achieves an average score of 73.31 on the Japanese Massive Text Embedding Benchmark (JMTEB), outperforming existing multilingual and Japanese embedding models
- Ruri-PTbase and Ruri-base models achieve the highest performance among existing Japanese rerankers on JQaRA (nDCG@10) and JaCWIR (MAP@10) datasets
- On MIRACL, Ruri models achieve the highest Recall@30 scores among Japanese models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive pre-training on large-scale synthesized datasets improves retrieval performance for Japanese text embeddings
- Mechanism: The model learns to map semantically similar texts close together in embedding space through contrastive learning, using synthesized QA and NLI pairs to provide abundant training examples where native Japanese data is scarce
- Core assumption: Synthetic data generated by LLMs captures the semantic relationships needed for effective contrastive learning in Japanese
- Evidence anchors:
  - [abstract]: "We collected datasets for building Japanese embedding models and released them under a permissive license... constructed a synthetic dataset using LLMs"
  - [section]: "To address the lack of Japanese retrieval datasets, we constructed a synthetic dataset using LLMs for two types of datasets commonly used in embedding model training: QA datasets and natural language inference (NLI) datasets"
- Break condition: If synthetic data quality is too low or doesn't capture Japanese semantic nuances, the contrastive learning signal becomes ineffective

### Mechanism 2
- Claim: Two-stage training (contrastive pre-training + supervised fine-tuning) significantly outperforms models trained only with fine-tuning
- Mechanism: Pre-training on weakly supervised data builds general semantic understanding, then fine-tuning on high-quality labeled data adapts the model to specific downstream tasks while preserving the general capabilities
- Core assumption: The general semantic representations learned during pre-training transfer effectively to downstream tasks
- Evidence anchors:
  - [abstract]: "Our contributions... 3. We created a large-scale dataset for contrastive pre-training in Japanese, demonstrating its effectiveness by outperforming existing multilingual models, even when using contrastive pre-training alone"
  - [section]: "Table 13 shows the results. We can clearly observe that the presence or absence of contrastive pre-training has a significant impact on post-supervised fine-tuning performance"
- Break condition: If the pre-training data distribution differs significantly from downstream tasks, the transfer benefit diminishes

### Mechanism 3
- Claim: Knowledge distillation from a cross-encoder reranker improves the retrieval performance of dual-encoder models
- Mechanism: The reranker captures fine-grained interactions between queries and documents, and distilling this knowledge into the dual-encoder helps it better estimate relevance without the computational cost of cross-encoding
- Core assumption: The reranker's relevance scores contain information that can be effectively captured by the dual-encoder's embedding space
- Evidence anchors:
  - [abstract]: "We developed a Japanese reranker, achieving the highest performance among existing Japanese rerankers"
  - [section]: "Knowledge distillation from cross-encoders is a method where the model is trained to align the similarity score distributions between query and document produced by the cross-encoder and those produced by the dual-encoder"
- Break condition: If the reranker and dual-encoder architectures are too different, the distillation signal may not transfer effectively

## Foundational Learning

- Concept: Contrastive learning with in-batch negatives
  - Why needed here: The paper uses contrastive pre-training with improved contrastive loss that leverages in-batch negatives for training efficiency
  - Quick check question: What is the difference between in-batch negatives and mined hard negatives in contrastive learning?

- Concept: Knowledge distillation from cross-encoders to dual-encoders
  - Why needed here: The paper applies knowledge distillation from a reranker (cross-encoder) to improve the dual-encoder embedding model's retrieval performance
  - Quick check question: How does knowledge distillation from a cross-encoder help a dual-encoder model estimate relevance without expensive cross-encoding?

- Concept: Task-homogeneous batching
  - Why needed here: The paper uses task-homogeneous batching to prevent shortcut learning and manage sequence length variations across datasets
  - Quick check question: Why would mixing examples from different datasets in the same batch potentially lead to shortcut learning in contrastive training?

## Architecture Onboarding

- Component map: Data collection → Dataset preprocessing → Contrastive pre-training (small/base/large) → Reranker construction (stage 1 + stage 2) → Supervised fine-tuning → Evaluation
- Critical path: Synthetic data generation → Contrastive pre-training → Reranker training → Knowledge distillation → Fine-tuning
- Design tradeoffs: The model uses relatively short context length (512 tokens) compared to recent LLMs (up to 32k tokens), trading off context capacity for computational efficiency
- Failure signatures: Poor retrieval performance on JMTEB indicates issues with either pre-training data quality, reranker training, or knowledge distillation
- First 3 experiments:
  1. Train the contrastive pre-trained model without synthetic datasets and compare retrieval performance to verify synthetic data contribution
  2. Train the reranker without contrastive pre-training as base model to verify the benefit of pre-training for reranking
  3. Train the final model without knowledge distillation from reranker to verify its contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would diverse prefixes be compared to the simple "クエリ: " and "文章: " prefixes used in Ruri?
- Basis in paper: [explicit] The paper mentions that using more diverse prefixes could potentially improve overall performance, citing recent models employing instructions or more detailed prefixes
- Why unresolved: The paper only used simple Japanese prefixes and did not experiment with diverse prefixes
- What evidence would resolve it: Experiments comparing Ruri's performance using diverse prefixes (like instructions or more detailed prefixes) against the current simple prefixes on the Japanese Massive Text Embedding Benchmark (JMTEB) would provide concrete evidence of the impact of prefix diversity

### Open Question 2
- Question: Is knowledge distillation from cross-encoders truly necessary for Japanese text embedding models, given the reported training instability?
- Basis in paper: [explicit] The paper notes that while knowledge distillation from cross-encoders has been introduced in previous models, it was observed to make training slightly unstable during Ruri's development. It also mentions that recent models based on LLMs do not seem to incorporate this technique
- Why unresolved: The paper acknowledges the potential instability but does not provide conclusive evidence on whether the benefits outweigh the drawbacks in the context of Japanese text embeddings
- What evidence would resolve it: Comparative experiments training Japanese text embedding models with and without knowledge distillation from cross-encoders, measuring both performance and training stability, would clarify the necessity and trade-offs of this technique

### Open Question 3
- Question: What is the impact of incorporating code datasets on the performance of Japanese text embedding models for code search tasks?
- Basis in paper: [explicit] The paper states that Ruri does not use code datasets and therefore cannot be applied for code search. It suggests that developing a bilingual model with both Japanese and English vocabularies might be necessary for code search in Japanese-specific models
- Why unresolved: The paper does not explore the potential benefits or challenges of incorporating code datasets into Japanese text embedding models
- What evidence would resolve it: Experiments training Japanese text embedding models with and without code datasets, evaluating their performance on code search tasks and other relevant benchmarks, would provide insights into the impact of code datasets and the feasibility of bilingual models for this purpose

## Limitations

- The synthetic data generation process relies heavily on LLM outputs without detailed validation of Japanese semantic quality, which could introduce biases or errors
- The evaluation focuses primarily on retrieval tasks, with limited assessment of the embeddings for other downstream applications like clustering or classification
- The knowledge distillation mechanism from cross-encoders to dual-encoders is implemented but lacks detailed ablation studies showing the specific contribution of each component

## Confidence

- **High confidence**: The two-stage training approach (contrastive pre-training + supervised fine-tuning) shows clear performance improvements over single-stage methods, supported by ablation studies in Table 13
- **Medium confidence**: The effectiveness of knowledge distillation from the reranker is demonstrated through final performance gains, but the ablation study removing knowledge distillation (Table 10) shows only a 0.53 point drop
- **Low confidence**: The paper claims Ruri achieves the highest performance among existing Japanese rerankers, but this is based on comparison with only a few existing models (R2J, RankGPT)

## Next Checks

1. **Synthetic data quality validation**: Conduct human evaluation of a sample of synthetic QA and NLI pairs from AutoWikiQA and AutoWikiNLI to assess semantic accuracy and Japanese language quality

2. **Reranker distillation ablation study**: Perform a more detailed ablation by training models with: (a) no reranker knowledge distillation, (b) reranker trained without contrastive pre-training, and (c) reranker trained with contrastive pre-training but without knowledge distillation

3. **Cross-linguistic transfer experiment**: Test whether the Japanese Ruri embeddings can be effectively adapted to other East Asian languages (Korean, Chinese) through fine-tuning, given their shared character-based writing systems