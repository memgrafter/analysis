---
ver: rpa2
title: Pairwise Similarity Distribution Clustering for Noisy Label Learning
arxiv_id: '2404.01853'
source_url: https://arxiv.org/abs/2404.01853
tags:
- samples
- noisy
- label
- learning
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Pairwise Similarity Distribution Clustering
  (PSDC), a method for training deep neural networks with noisy labels. PSDC divides
  training samples into clean and noisy sets by analyzing the pairwise similarity
  structure between samples, then applies semi-supervised learning to the resulting
  sets.
---

# Pairwise Similarity Distribution Clustering for Noisy Label Learning

## Quick Facts
- arXiv ID: 2404.01853
- Source URL: https://arxiv.org/abs/2404.01853
- Authors: Sihan Bai
- Reference count: 40
- Primary result: Achieves 94.2% accuracy on CIFAR-10 with 40% asymmetric noise

## Executive Summary
This paper introduces Pairwise Similarity Distribution Clustering (PSDC), a method for training deep neural networks with noisy labels by analyzing pairwise similarity structures between samples. PSDC divides training data into clean and noisy sets using Gaussian Mixture Models (GMM) on similarity distributions, then applies semi-supervised learning to each set. The approach demonstrates improved robustness compared to loss-based methods, particularly under severe label noise conditions, achieving state-of-the-art performance on CIFAR-10/100 and Clothing1M datasets.

## Method Summary
PSDC computes pairwise cosine similarities between all samples within each label group, then uses row-summed affinity matrices as features for GMM clustering to separate clean from noisy samples. The method assumes clean samples exhibit high mutual similarity while noisy samples have low similarity to others in their assigned label group. After separation, semi-supervised learning with MixMatch and contrastive learning (SimCLR) is applied to the clean and noisy sets respectively, using a two-network co-teaching framework to prevent error accumulation. The process iterates with updated feature representations improving sample separation over time.

## Key Results
- Achieves 94.2% accuracy on CIFAR-10 with 40% asymmetric noise
- Outperforms state-of-the-art methods by 0.8-2.5% on CIFAR-10/100 with various noise rates
- Reaches 75.55% accuracy on real-world Clothing1M dataset
- Demonstrates consistent performance across symmetric and asymmetric noise patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise similarity structure can distinguish clean from noisy samples regardless of label noise level
- Mechanism: Computes cosine similarity between all sample pairs within each label group, using row-summed affinity matrices as features for GMM clustering. Clean samples exhibit high mutual similarity while noisy samples have low similarity to others in their assigned label group.
- Core assumption: Clean samples representing the same class have more similar features than noisy samples incorrectly assigned to that class
- Evidence anchors:
  - [abstract] "we take the pairwise similarity between sample pairs to represent the sample structure"
  - [section] "Clean samples that belong to the label set depict the same thing, so they have similar features. However, noise samples that should not be in the set depict different things, thus they do not share similar features with clean samples."

### Mechanism 2
- Claim: GMM clustering on affinity distributions provides robust sample separation without requiring clean labels
- Mechanism: After computing row-sums of affinity matrices, PSDC fits a two-component GMM to model the mixture of clean and noisy sample distributions. Samples are classified based on posterior probabilities from the GMM components.
- Core assumption: The affinity distributions of clean and noisy samples follow distinct Gaussian distributions that GMM can separate
- Evidence anchors:
  - [abstract] "the Gaussian Mixture Model (GMM) to model the similarity distribution between sample pairs belonging to the same noisy cluster"
  - [section] "According to the central limit theorem, the mean value of ð‘Žð‘– for both clean and noise samples follows a normal distribution."

### Mechanism 3
- Claim: Semi-supervised learning on the separated clean and noisy sets improves final model performance
- Mechanism: Clean samples are used for supervised training while noisy samples are used for unsupervised learning with pseudo-labels, creating a self-improving cycle where better feature extraction leads to better sample separation
- Core assumption: Even noisy samples contain useful information that can be leveraged through unsupervised learning when combined with the clean set supervision
- Evidence anchors:
  - [abstract] "which can power any of the off-the-shelf semi-supervised learning regimes to further train networks"
  - [section] "Therefore, the resulting samples can be further taken to learn discriminative features representation in a supervised and unsupervised manner, respectively."

## Foundational Learning

- Concept: Gaussian Mixture Models and Expectation-Maximization
  - Why needed here: GMM is used to model the mixture of affinity distributions from clean and noisy samples, then separate them based on posterior probabilities
  - Quick check question: How does GMM determine which samples belong to which component without labeled training data?

- Concept: Central Limit Theorem
  - Why needed here: Justifies modeling the row-summed affinity distributions as Gaussian, which is critical for GMM clustering to work
  - Quick check question: Why does summing many independent affinity values lead to a normal distribution?

- Concept: Cosine similarity as feature representation
  - Why needed here: Provides a scale-invariant measure of sample similarity that captures the structural relationships needed for clustering
  - Quick check question: How does cosine similarity differ from Euclidean distance when comparing high-dimensional feature vectors?

## Architecture Onboarding

- Component map: Feature extractor -> Projection head -> Pairwise similarity computation -> Affinity matrix row-summarization -> GMM clustering -> Sample separation -> Semi-supervised learning -> Updated features

- Critical path: Feature extraction â†’ Pairwise similarity computation â†’ GMM clustering â†’ Sample separation â†’ Semi-supervised training â†’ Updated features â†’ Repeat

- Design tradeoffs:
  - Memory vs accuracy: Computing full pairwise affinity matrices is O(nÂ²) in memory and computation
  - Early vs late sample selection: Starting sample selection too early may use poor features, but waiting too long wastes training time
  - GMM complexity: Two-component GMM is simple but may not capture complex distributions

- Failure signatures:
  - Low clean set purity despite apparent separation
  - Clean and noisy sample affinity distributions with high overlap
  - Performance degradation when noise rate exceeds certain thresholds
  - Slow convergence of the sample selection process

- First 3 experiments:
  1. Verify pairwise similarity structure: Plot affinity distributions for clean vs noisy samples on a small dataset with known labels
  2. Test GMM clustering effectiveness: Apply GMM to affinity distributions and measure separation quality against ground truth
  3. Validate semi-supervised training: Train on separated sets and measure performance improvement compared to supervised training only

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the pairwise similarity distribution and noisy label prior be combined to complement each other in noisy label learning?
- Basis in paper: [explicit] The paper states in the conclusion that future work should investigate how the pairwise sample structure and noisy label prior can be utilized to complement each other.
- Why unresolved: The paper focuses on using pairwise similarity distributions as an alternative to noisy labels for sample selection, but does not explore combining both approaches to potentially leverage the strengths of each.
- What evidence would resolve it: Experimental results comparing methods that use pairwise similarity only, noisy labels only, and a combination of both would demonstrate whether combining these approaches improves performance in noisy label learning tasks.

### Open Question 2
- Question: How does the performance of PSDC vary with different network architectures and their feature extraction capabilities?
- Basis in paper: [inferred] The paper uses PreAct ResNet18 for CIFAR-10/100 and ResNet50 for Clothing1M, but does not explore how different architectures affect the pairwise similarity distributions and sample selection accuracy.
- Why unresolved: Different network architectures may extract features with varying levels of discriminative power, which could impact the effectiveness of the pairwise similarity distribution clustering approach.
- What evidence would resolve it: Experiments using various network architectures (e.g., different depths, widths, or types of architectures) and analyzing how their feature extraction capabilities correlate with PSDC performance would provide insights into the method's robustness across architectures.

### Open Question 3
- Question: How does PSDC perform on datasets with instance-dependent label noise compared to class-conditional noise?
- Basis in paper: [explicit] The paper mentions instance-dependent noise in the context of related work but does not evaluate PSDC on such noise patterns.
- Why unresolved: The theoretical analysis and experiments focus on class-conditional noise, but real-world scenarios often involve instance-dependent noise where the probability of label corruption depends on the specific instance.
- What evidence would resolve it: Experiments applying PSDC to datasets with instance-dependent noise patterns and comparing its performance to class-conditional noise scenarios would reveal the method's effectiveness across different noise types.

## Limitations

- Computational complexity of O(nÂ²) for pairwise similarity calculations may limit scalability to larger datasets
- Method may fail when noisy samples share visual characteristics with their assigned class, causing affinity distributions to overlap
- Performance under extreme noise conditions (>50% noise rate) and with highly similar-looking classes remains unverified

## Confidence

- **High confidence**: GMM modeling approach for affinity distributions is well-established, and semi-supervised learning component follows standard practices
- **Medium confidence**: Theoretical justification for using pairwise similarities relies on specific assumptions about feature distributions that may not hold universally
- **Low confidence**: Method's performance under extreme noise conditions and with highly similar-looking classes remains unverified

## Next Checks

1. **Distribution Overlap Analysis**: Quantitatively measure the overlap between clean and noisy sample affinity distributions across different noise rates to identify the failure threshold where GMM clustering becomes unreliable

2. **Scalability Testing**: Evaluate PSDC's performance and computational requirements on larger datasets (e.g., ImageNet-32 or subsets of ImageNet) to assess practical scalability limitations

3. **Cross-dataset Generalization**: Test PSDC on diverse datasets beyond CIFAR and Clothing1M, including datasets with fine-grained classes and varying visual similarity structures, to validate the method's robustness to different data distributions