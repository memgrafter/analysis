---
ver: rpa2
title: 'ProductAgent: Benchmarking Conversational Product Search Agent with Asking
  Clarification Questions'
arxiv_id: '2407.00942'
source_url: https://arxiv.org/abs/2407.00942
tags:
- user
- product
- questions
- clarification
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of product demand clarification,
  where an agent asks clarification questions to refine user queries in e-commerce
  search. The proposed ProductAgent integrates product feature summarization, query
  generation, and dynamic product retrieval to iteratively improve search results
  through conversation.
---

# ProductAgent: Benchmarking Conversational Product Search Agent with Asking Clarification Questions

## Quick Facts
- **arXiv ID:** 2407.00942
- **Source URL:** https://arxiv.org/abs/2407.00942
- **Reference count:** 40
- **Primary result:** ProductAgent improves retrieval performance (HIT@10 scores increase from ~20 to ~70+ across 5 turns) by asking clarification questions to refine user queries in e-commerce search.

## Executive Summary
This paper introduces ProductAgent, a conversational product search agent that asks clarification questions to refine user queries in e-commerce search. The agent integrates product feature summarization, query generation, and dynamic product retrieval to iteratively improve search results through conversation. To evaluate this task, the authors create PROCLARE, a benchmark that uses an LLM-driven user simulator to generate 2,000 dialogues across 20 product categories. Experiments show that ProductAgent significantly improves retrieval performance by progressively narrowing user intent, demonstrating the effectiveness of conversational clarification in product search.

## Method Summary
ProductAgent addresses product demand clarification by asking targeted clarification questions to refine user queries. The system uses SQL and vector-based retrieval methods for different stages: SQL for precise exact-matching in category analysis, and vector retrieval for semantic similarity in final recommendations. The agent generates multi-choice clarification questions based on product feature summarization from retrieved items. PROCLARE benchmark evaluates this approach using an LLM-driven user simulator that generates realistic conversation data without human annotation. The system is tested across 20 product categories with 1 million product documents, measuring retrieval performance through HIT@10 and MRR@10 scores across multiple dialogue turns.

## Key Results
- ProductAgent significantly improves retrieval performance, with HIT@10 scores increasing from ~20 to ~70+ across 5 dialogue turns
- Using both SQL and vector-based retrieval provides complementary benefits, with SQL excelling at exact attribute matching and vector retrieval at semantic similarity
- LLM-driven user simulator provides scalable and consistent evaluation, generating 2,000 dialogues without human annotation costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ProductAgent improves retrieval performance by asking clarification questions that progressively narrow user intent.
- Mechanism: The agent uses product feature summarization to create a dynamic knowledge base from retrieved items, then generates targeted multi-choice questions that directly address gaps in understanding.
- Core assumption: The summarization accurately reflects the current set of relevant products and their distinguishing features.
- Evidence anchors:
  - [abstract] "ProductAgent significantly improves retrieval performance (e.g., HIT@10 scores increasing from ~20 to ~70+ across 5 turns) by progressively narrowing user intent"
  - [section 4.2] "Category Analysis... retrieves relevant product items... summarized as statistics... serve as a dynamic knowledge base"
- Break condition: If the summarization fails to capture key distinguishing features or includes irrelevant attributes, the clarification questions will not effectively narrow the search space.

### Mechanism 2
- Claim: Using both structured SQL retrieval and vector-based non-SQL retrieval provides complementary benefits for different stages of the conversation.
- Mechanism: SQL retrieval with Text2SQL enables precise exact-matching for category analysis and statistic generation, while vector-based retrieval handles semantic similarity for final product recommendations.
- Core assumption: Different retrieval methods are better suited for different types of queries - exact attribute matching vs semantic similarity.
- Evidence anchors:
  - [section 4.2] "we employ SQL retrieval in the stage 1 since it enables us to efficiently retrieve items through exact matching... we leverage non-SQL retrieval in the stage 2 because non-SQL retrievers can return product items ordered by their relevance"
  - [section 5.3] "Our experiments employ three widely-adopted retrievers: BM25, General Text Embedding (GTE), and CoROM"
- Break condition: If the distinction between retrieval types becomes blurred or if one method dominates, the complementary benefits may be lost.

### Mechanism 3
- Claim: The LLM-driven user simulator provides a scalable and consistent evaluation method that eliminates human annotation costs.
- Mechanism: The simulator answers clarification questions based on ground truth product knowledge, generating realistic conversation data without human involvement.
- Core assumption: The simulator can accurately model human response patterns while maintaining evaluation consistency.
- Evidence anchors:
  - [section 5.4] "we harness LLMs as intelligent user simulators... generating simulated user responses, thus eliminating the need for human utterances"
  - [section 5.4] "we restrict the user simulator to only answer the given questions with provided candidates, thus simulating a more realistic and reliable situation"
- Break condition: If the simulator's responses deviate significantly from human behavior or if it introduces systematic biases, the evaluation validity is compromised.

## Foundational Learning

- Concept: Information retrieval evaluation metrics (MRR@10, HIT@10)
  - Why needed here: The paper evaluates retrieval performance using these standard metrics to quantify how well the agent finds relevant products
  - Quick check question: What is the difference between MRR@10 and HIT@10, and when would you prefer one over the other?

- Concept: Chain-of-thought prompting and tool integration with LLMs
  - Why needed here: ProductAgent uses carefully designed prompts to enable effective interaction between LLMs and external tools like SQL databases and retrievers
  - Quick check question: How does chain-of-thought prompting help LLMs better utilize external tools compared to direct prompting?

- Concept: Named Entity Recognition (NER) and entity schema design
  - Why needed here: The paper applies NER preprocessing to transform diverse product data into a unified structured format that enables consistent SQL retrieval
  - Quick check question: Why is it important to compress 54 hierarchical entity labels into 10 unified labels for the retrieval system?

## Architecture Onboarding

- Component map: User query → Category Analysis → Item Search → Clarification Question Generation → User response → repeat until satisfactory results
- Critical path: User query → Category Analysis → Item Search → Clarification Question Generation → User response → repeat until satisfactory results
- Design tradeoffs: SQL vs vector retrieval (exact vs semantic matching), structured vs unstructured memory storage, multiple LLM backbones vs single optimized model
- Failure signatures: Invalid SQL queries from Text2SQL tool, trivial questions that don't narrow search space, retrieval performance plateauing despite more turns
- First 3 experiments:
  1. Test ProductAgent with simplified dataset and single product category to verify end-to-end flow
  2. Compare retrieval performance with and without clarification questions using the same LLM backbone
  3. Test different LLM backbones (GPT-4 vs Qwen vs GPT-3.5) to establish performance baseline differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of clarification questions to ask per turn to maximize retrieval performance without causing user fatigue?
- Basis in paper: [inferred] The paper tests with n=3 clarification questions per turn but doesn't explore optimal numbers or user fatigue effects.
- Why unresolved: The paper assumes 3 questions is optimal but doesn't empirically test different quantities or measure user satisfaction/fatigue.
- What evidence would resolve it: Controlled experiments varying the number of questions per turn (e.g., 1-5) while measuring both retrieval performance and simulated user satisfaction metrics.

### Open Question 2
- Question: How does the diversity of clarification questions affect retrieval performance and user experience over multiple dialogue turns?
- Basis in paper: [explicit] Section 7.4 notes that ProductAgent tends to generate similar clarification questions in later turns, and Section A.3 analyzes similarity trends.
- Why unresolved: The paper identifies this as a problem but doesn't propose or test solutions for maintaining question diversity.
- What evidence would resolve it: Implementation and evaluation of diversity-promoting strategies for question generation, measuring their impact on both retrieval scores and question similarity metrics.

### Open Question 3
- Question: Would incorporating additional product features like pricing and customer reviews into the knowledge base improve retrieval performance?
- Basis in paper: [explicit] Section 8 (Limitations) explicitly states that the dataset lacks crucial information like pricing and reviews.
- Why unresolved: The paper acknowledges this limitation but doesn't explore how these features might enhance the agent's performance.
- What evidence would resolve it: Experiments comparing retrieval performance with and without pricing/review features integrated into the SQL database and clarification question generation process.

## Limitations
- The reliance on a single e-commerce dataset (AliMe KG) may introduce domain-specific biases that limit generalizability
- The use of an LLM-driven user simulator rather than real human interactions may not capture the full complexity of human clarification behaviors
- The evaluation focuses primarily on retrieval metrics without assessing user satisfaction or conversational naturalness

## Confidence
- **High confidence**: The core mechanism of using clarification questions to progressively narrow user intent (Mechanism 1) is well-supported by experimental results showing HIT@10 improvements from ~20 to ~70+ across 5 turns
- **Medium confidence**: The LLM-driven user simulator evaluation (Mechanism 3) provides consistent and scalable evaluation but may not fully capture human response patterns
- **Low confidence**: The generalizability of the approach to domains beyond e-commerce product search and to real human interactions remains uncertain

## Next Checks
1. **Human evaluation validation**: Conduct a small-scale user study with real human participants to compare the LLM-driven user simulator's responses against actual human clarification patterns, measuring response diversity, naturalness, and alignment with ground truth product knowledge.

2. **Cross-domain generalization test**: Apply ProductAgent to a different domain (e.g., travel booking or restaurant recommendations) using the same evaluation framework to assess whether the clarification mechanism transfers effectively to non-product search contexts.

3. **Robustness stress testing**: Design adversarial scenarios including contradictory user responses, ambiguous product attributes, and incomplete product information to evaluate how ProductAgent handles edge cases and whether its clarification questions remain effective under challenging conditions.