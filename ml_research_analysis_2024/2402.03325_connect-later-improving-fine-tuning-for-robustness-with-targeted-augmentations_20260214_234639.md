---
ver: rpa2
title: 'Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations'
arxiv_id: '2402.03325'
source_url: https://arxiv.org/abs/2402.03325
tags:
- augmentations
- fine-tuning
- targeted
- pretraining
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving out-of-distribution
  (OOD) performance in unsupervised domain adaptation. While contrastive pretraining
  with generic augmentations is promising, the authors show that standard fine-tuning
  after pretraining does not consistently improve OOD error over supervised learning
  from scratch.
---

# Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations

## Quick Facts
- arXiv ID: 2402.03325
- Source URL: https://arxiv.org/abs/2402.03325
- Reference count: 40
- Primary result: Connect Later improves average OOD error over standard fine-tuning and supervised learning with targeted augmentations across 4 real-world datasets

## Executive Summary
This paper addresses the challenge of improving out-of-distribution (OOD) performance in unsupervised domain adaptation. While contrastive pretraining with generic augmentations shows promise, standard fine-tuning after pretraining does not consistently improve OOD error over supervised learning from scratch. The authors propose "Connect Later": after pretraining with generic augmentations, fine-tune with targeted augmentations designed with knowledge of the distribution shift. This approach leverages good representations learned within each domain during pretraining while better connecting domains during fine-tuning. Connect Later achieves state-of-the-art results on astronomical time-series classification (AstroClassification) by 2.5%, wildlife species identification (iWildCam-WILDS) with ResNet-50 by 0.9%, and tumor identification (Camelyon17-WILDS) with DenseNet121 by 1.1%.

## Method Summary
Connect Later involves two key phases: (1) pretraining a model using contrastive learning or masked autoencoding on unlabeled source and target data with generic augmentations like masking and cropping, and (2) fine-tuning the pretrained model using targeted augmentations specifically designed to address the distribution shift. The targeted augmentations are created by identifying feature spaces where source and target domains differ, fitting the target feature distribution, and defining transformations to align source inputs with the target distribution on these features. Fine-tuning proceeds in two stages: first with linear probing on pretrained features, then joint fine-tuning of both head and encoder. This approach enables the model to leverage the good within-domain representations from pretraining while effectively connecting the domains for the specific task.

## Key Results
- Achieves state-of-the-art on AstroClassification (astronomical time-series) with 2.5% improvement
- Improves iWildCam-WILDS wildlife species identification with ResNet-50 by 0.9%
- Enhances Camelyon17-WILDS tumor identification with DenseNet121 by 1.1%
- Demonstrates 11% relative improvement over standard fine-tuning on the new Redshifts astronomical dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining learns good representations within each domain, but generic augmentations may fail to connect the source and target domains in the input space.
- Mechanism: Contrastive pretraining optimizes to bring augmented views of the same input closer while pushing apart views of different inputs. If the domains are far apart in input space, generic augmentations (e.g., cropping, masking) may not sufficiently align them, leading to misaligned connectivity in the learned representations.
- Core assumption: The pretraining augmentation distribution Apre does not adequately connect the source and target domains for the given distribution shift.
- Evidence anchors:
  - [abstract] "Pretraining improves OOD error when the generic data augmentations used (e.g., masking or cropping) connect the source and target domains, which may be far apart in the input space."
  - [section 3] "In this paper, we show on real-world tasks that standard fine-tuning after pretraining does not consistently improve OOD error over simply training from scratch on labeled source data."
- Break condition: If generic augmentations happen to connect the domains well for the task, then pretraining + standard fine-tuning may already work well and Connect Later provides little benefit.

### Mechanism 2
- Claim: Targeted augmentations designed with knowledge of the distribution shift can better connect the domains during fine-tuning, enabling the pretrained representations to transfer effectively.
- Mechanism: By augmenting the source inputs to match the target distribution on a feature space where the domains differ (e.g., redshift for astronomical data, background for wildlife images), the fine-tuning step can leverage the good within-domain representations learned during pretraining while also aligning the domains for the specific task.
- Core assumption: We can identify a feature space Z where the source and target domains differ, and design augmentations that transform source inputs to match the target distribution on Z.
- Evidence anchors:
  - [abstract] "To better leverage pretraining for distribution shifts, we propose Connect Later: after pretraining with generic augmentations, fine-tune with targeted augmentations designed with knowledge of the distribution shift."
  - [section 4.2] "We provide a general methodology for the design of such augmentations. Certain aspects, such as the selection of feature space z and transformation distribution T could be learned from the unlabeled data itself, which we leave for future work."
- Break condition: If the distribution shift cannot be characterized by a small number of features, or if designing effective targeted augmentations requires domain knowledge not available, Connect Later may not provide significant benefits over other domain adaptation methods.

### Mechanism 3
- Claim: Connect Later achieves zero OOD error in a simple theoretical example where contrastive pretraining alone and ERM with targeted augmentations both fail.
- Mechanism: In the example, pretraining learns representations where the classes are linearly separable within each domain, but the domains are misaligned. Targeted augmentations alone cannot connect all target inputs to source inputs, but pretraining allows label information to propagate to all target inputs, enabling 0 OOD error when combined with targeted augmentations.
- Core assumption: The theoretical example captures key aspects of the problem, including the misalignment of domains in the pretraining augmentation graph and the limitation of ERM with targeted augmentations.
- Evidence anchors:
  - [section 4] "In our simple binary classification example in Appendix E, where the connectivity structure is misaligned, both standard fine-tuning with contrastive pretraining and ERM + targeted augmentations have high OOD error, while Connect Later achieves 0 OOD error."
- Break condition: If the theoretical assumptions do not hold for a given dataset (e.g., if the augmentation graph structure is different, or if linear probing is not sufficient), the 0 OOD error guarantee may not translate to practical gains.

## Foundational Learning

- Concept: Contrastive learning and pretraining
  - Why needed here: Connect Later relies on pretraining with contrastive learning to learn good representations within each domain before applying targeted augmentations during fine-tuning.
  - Quick check question: What is the objective of contrastive learning, and how does it differ from masked autoencoding?

- Concept: Domain adaptation and distribution shift
  - Why needed here: Connect Later is designed for unsupervised domain adaptation, where the goal is to learn a model that generalizes well from a labeled source domain to an unlabeled target domain with a distribution shift.
  - Quick check question: What are some common approaches to domain adaptation, and how does Connect Later differ from them?

- Concept: Data augmentation and its role in robustness
  - Why needed here: Both pretraining and fine-tuning in Connect Later rely on data augmentations, but with different goals (generic vs. targeted). Understanding the role of augmentations in improving robustness is key to designing effective targeted augmentations.
  - Quick check question: How do generic data augmentations improve model robustness, and what are the limitations of this approach for domain adaptation?

## Architecture Onboarding

- Component map: Pretraining with generic augmentations -> Targeted augmentation design -> Fine-tuning with targeted augmentations -> Evaluation
- Critical path:
  1. Pretrain model on unlabeled data with generic augmentations
  2. Design targeted augmentations based on distribution shift
  3. Fine-tune pretrained model with linear probing, then joint fine-tuning with targeted augmentations
  4. Evaluate ID and OOD performance
- Design tradeoffs:
  - Choice of pretraining method (contrastive vs. masked autoencoding) and augmentation strength
  - Complexity of targeted augmentation design (simple vs. learned)
  - Balance between ID and OOD performance
- Failure signatures:
  - OOD performance no better than ERM or standard fine-tuning
  - ID performance significantly worse than other methods
  - Pretrained representations not reusable for other tasks
- First 3 experiments:
  1. Ablation: Compare Connect Later with pretraining alone, targeted augmentations alone, and ERM baseline on a small dataset
  2. Sensitivity: Vary the strength of pretraining augmentations (e.g., masking percentage) and targeted augmentations, measure impact on ID and OOD performance
  3. Scalability: Test Connect Later with larger models and more complex targeted augmentation designs, measure computational cost and performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we learn targeted augmentations from source and target data distributions rather than relying on domain knowledge?
- Basis in paper: [explicit] The authors state "Certain aspects, such as the selection of feature space z and transformation distribution T could be learned from the unlabeled data itself, which we leave for future work."
- Why unresolved: The paper provides a methodology for designing targeted augmentations but relies on domain knowledge and manual feature selection rather than learning these transformations directly from data.
- What evidence would resolve it: A method that automatically learns effective targeted augmentations from unlabeled source and target data, demonstrating comparable or superior performance to manually designed augmentations across multiple domain adaptation tasks.

### Open Question 2
- Question: What is the relationship between the strength of pretraining augmentations and their effectiveness in connecting source and target domains for different types of distribution shifts?
- Basis in paper: [inferred] The authors show that Connect Later is relatively robust to masking percentage but note that increasing augmentation strength may indiscriminately increase connectivity between examples from different classes.
- Why unresolved: While the paper tests different masking percentages, it doesn't systematically explore how varying augmentation strength affects domain connectivity across different types of distribution shifts or datasets.
- What evidence would resolve it: A comprehensive study mapping augmentation strength to domain connectivity measures across diverse distribution shifts, identifying optimal augmentation strategies for different shift types.

### Open Question 3
- Question: How does the choice of pretraining algorithm (contrastive learning vs. masked autoencoding) affect the transferability of representations across different domain adaptation scenarios?
- Basis in paper: [explicit] The authors apply Connect Later to both contrastive pretraining and masked autoencoding, noting that "masked autoencoding has been linked to contrastive learning" but don't systematically compare their effectiveness.
- Why unresolved: The paper demonstrates Connect Later works with both pretraining methods but doesn't provide a comparative analysis of when one method might be preferable over the other for domain adaptation.
- What evidence would resolve it: A systematic comparison of contrastive learning and masked autoencoding across diverse domain adaptation benchmarks, identifying scenarios where each method excels and the underlying reasons for their differences.

### Open Question 4
- Question: Can the Connect Later framework be extended to semi-supervised domain adaptation settings where only a small amount of labeled target data is available?
- Basis in paper: [inferred] The paper focuses on unsupervised domain adaptation but doesn't explore scenarios with limited target labels, which are common in real-world applications.
- Why unresolved: While the framework shows promise for unsupervised domain adaptation, its performance and design considerations for semi-supervised settings remain unexplored.
- What evidence would resolve it: An extension of Connect Later to semi-supervised domain adaptation, demonstrating improved performance over both fully supervised and unsupervised approaches when limited target labels are available.

## Limitations

- The theoretical guarantees assume specific connectivity structures that may not hold in real-world datasets, limiting the generalizability of the 0 OOD error claim.
- The effectiveness of Connect Later heavily depends on correctly identifying the feature space Z where domains differ and designing appropriate targeted augmentations, which may require domain expertise not always available.
- The paper does not provide detailed hyperparameter settings for pretraining and targeted augmentation design, making faithful reproduction challenging.

## Confidence

- High confidence: Connect Later improves OOD performance on the tested datasets compared to standard fine-tuning and supervised learning with targeted augmentations
- Medium confidence: The mechanism of misaligned connectivity structure explaining pretraining failures is correct, but may not capture all failure modes
- Low confidence: The theoretical example accurately represents practical scenarios and the 0 OOD error guarantee will hold in most cases

## Next Checks

1. Test Connect Later on datasets with different types of distribution shifts (e.g., covariate shift, prior probability shift) to evaluate its robustness to various shift types.
2. Compare Connect Later with other domain adaptation methods like domain adversarial training and domain-invariant representation learning to establish its relative effectiveness.
3. Analyze the impact of pretraining augmentation strength and targeted augmentation complexity on ID and OOD performance to identify optimal configurations.