---
ver: rpa2
title: Benchmarking Large Language Models for Image Classification of Marine Mammals
arxiv_id: '2410.19848'
source_url: https://arxiv.org/abs/2410.19848
tags:
- marine
- species
- llms
- performance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of accurately classifying marine
  mammal images using AI models. The authors introduce a new dataset of 1,423 images
  of 65 marine mammal species, classified into three hierarchical levels: species,
  medium, and group.'
---

# Benchmarking Large Language Models for Image Classification of Marine Mammals

## Quick Facts
- arXiv ID: 2410.19848
- Source URL: https://arxiv.org/abs/2410.19848
- Authors: Yijiashun Qi; Shuzhang Cai; Zunduo Zhao; Jiaming Li; Yanbin Lin; Zhiqiang Wang
- Reference count: 38
- Primary result: Introduced a 1,423-image marine mammal dataset and evaluated traditional ML, pre-trained DL, zero-shot LLMs, and LLM-based MAS for species/medium/group classification

## Executive Summary
This paper addresses the challenge of accurately classifying marine mammal images using AI models. The authors introduce a new dataset of 1,423 images of 65 marine mammal species, classified into three hierarchical levels: species, medium, and group. They evaluate various approaches, including traditional machine learning (ML) algorithms with neural network embeddings, pre-trained deep learning (DL) models (VGG and ResNet), zero-shot learning models (CLIP), multimodal large language models (LLMs), and a novel LLM-based multi-agent system (MAS). Results show that traditional ML models (KNN and SVM) excel at species-level classification, while pre-trained DL models and LLMs perform well at medium and group levels. The LLM-based MAS further improves classification accuracy, demonstrating the potential of combining multiple LLMs for enhanced performance. The dataset and findings provide valuable resources for advancing AI-driven marine mammal conservation research.

## Method Summary
The study evaluates multiple approaches for marine mammal image classification: traditional ML models (KNN, SVM) using DINOv2 embeddings, pre-trained deep learning models (VGG-11, ResNet-50) fine-tuned on the dataset, zero-shot learning with multimodal LLMs (CLIP, LLaVA, GPT-4o, Gemini-1.5 Pro), and an LLM-based multi-agent system that ensembles multiple LLM predictions. The dataset contains 1,423 images across 65 species, split into species-level (65 classes), medium-level (10 classes), and group-level (4 classes) classification tasks. Traditional ML models achieve highest species-level accuracy, while LLMs and pre-trained DL models perform better at broader taxonomic levels. The MAS system further improves LLM performance through ensemble predictions.

## Key Results
- Traditional ML models (SVM: 64.62% species, 90.00% medium, 100.00% group) excel at species-level classification
- Pre-trained DL models and LLMs perform well at medium (91.54% accuracy) and group levels
- LLM-based MAS improves accuracy to 56.15% in zero-shot CoT setting, outperforming single LLM models
- Dataset and benchmarks provide valuable resources for marine mammal conservation research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining multiple LLM predictions in a multi-agent system (MAS) improves classification accuracy compared to single LLM models.
- Mechanism: The MAS aggregates outputs from multiple LLM agents, each potentially making different predictions. The ensemble (Critic Agent) uses these diverse outputs to make a more informed final decision, reducing the impact of individual model biases.
- Core assumption: Different LLMs have complementary strengths and weaknesses in their predictions, and combining them leads to better overall performance.
- Evidence anchors:
  - [abstract] states "The results demonstrate the strengths of traditional models and LLMs in different aspects, and the MAS can further improve the classification performance."
  - [section] "the MAS consistently boosts accuracy. Notably, under the zero-shot CoT setting, MAS reaches an accuracy of 56.15% when GPT is used as the ensemble model, outperforming all other LLM settings."
  - [corpus] Weak evidence - no directly relevant corpus papers found that specifically address LLM multi-agent systems for image classification tasks.
- Break condition: If the individual LLMs are too similar in their predictions, the ensemble won't gain much benefit. If one LLM consistently dominates, the ensemble becomes less effective.

### Mechanism 2
- Claim: Traditional ML models (KNN, SVM) with DINOv2 embeddings excel at fine-grained species-level classification, while pre-trained DL models perform well at medium and group levels.
- Mechanism: DINOv2 embeddings capture rich visual features that traditional classifiers can leverage for precise species identification. Pre-trained DL models like ResNet and VGG have learned hierarchical feature representations that work well for broader taxonomic categories.
- Core assumption: Different levels of classification require different types of feature representations - fine-grained for species, more abstract for broader categories.
- Evidence anchors:
  - [abstract] "Results show that traditional ML models (KNN and SVM) excel at species-level classification, while pre-trained DL models and LLMs perform well at medium and group levels."
  - [section] "SVM performs better than KNN and fine-tuned neural networks across all three levels. Its accuracy is 64.62%, 90.00%, and 100.00% at species-level, medium-level, and group-level classifications."
  - [corpus] Weak evidence - no directly relevant corpus papers found that compare these specific model types across different classification granularities for marine mammals.
- Break condition: If the feature extraction fails to capture distinguishing characteristics at any level, or if the dataset has insufficient examples per class.

### Mechanism 3
- Claim: Zero-shot learning with multimodal LLMs like GPT-4o and Gemini-1.5 Pro works effectively for medium and group-level classification but struggles with species-level due to prediction bias toward common species.
- Mechanism: These LLMs leverage their pre-trained knowledge from massive multimodal datasets to recognize patterns and classify images without specific training. Their performance is limited by the frequency distribution of species in their training data.
- Core assumption: The LLMs' pre-trained knowledge contains sufficient information about marine mammals to make reasonable classifications at broader taxonomic levels, but lacks the specific, balanced information needed for species-level identification.
- Evidence anchors:
  - [abstract] "pre-trained DL models and LLMs perform well at medium and group levels"
  - [section] "Both GPT-4o and Gemini-1.5 Pro experience a decrease in performance when using CoT, with their accuracy falling short of SVM, VGG, and ResNet, all of which achieve over 60.00%. At the medium level, Gemini achieves an accuracy of 91.54% either with or without CoT"
  - [corpus] Weak evidence - no directly relevant corpus papers found that specifically analyze LLM performance bias toward common species in zero-shot marine mammal classification.
- Break condition: If the LLM training data has better representation of rare species, or if the classification task is simplified to only include the most common species.

## Foundational Learning

- Concept: Multimodal LLMs (understanding both text and images)
  - Why needed here: The paper evaluates models like GPT-4o and Gemini-1.5 Pro that can process both visual and textual information for classification tasks.
  - Quick check question: What are the key differences between models designed specifically for vision tasks (like LLaVA) versus general LLMs with vision capabilities (like GPT-4o)?

- Concept: Zero-shot learning
  - Why needed here: The paper uses zero-shot approaches where models classify marine mammal images without specific training on the dataset.
  - Quick check question: How does zero-shot learning differ from few-shot learning, and what are the advantages and disadvantages of each approach?

- Concept: Multi-agent systems and ensemble methods
  - Why needed here: The paper introduces an LLM-based multi-agent system that aggregates predictions from multiple models.
  - Quick check question: What are the key benefits of using ensemble methods in machine learning, and how do they typically improve model performance?

## Architecture Onboarding

- Component map: Image → DINOv2 embedding → SVM/KNN → species-level classification OR Image → LLM → medium/group classification OR MAS: multiple LLMs → ensemble → final classification

- Critical path: Image → DINOv2 embedding → SVM/KNN → species-level classification OR Image → LLM → medium/group classification OR MAS: multiple LLMs → ensemble → final classification

- Design tradeoffs:
  - Traditional ML + DINOv2: Requires feature extraction but provides strong species-level accuracy
  - Pre-trained DL: Requires fine-tuning but provides good performance across all levels
  - Zero-shot LLMs: No training required but suffers from species-level bias
  - MAS: Additional complexity but improves overall accuracy

- Failure signatures:
  - Low accuracy across all models: Dataset quality issues or insufficient data per class
  - LLMs perform poorly: Insufficient marine mammal representation in LLM training data
  - MAS doesn't improve accuracy: LLMs are too similar in their predictions or one LLM dominates

- First 3 experiments:
  1. Test DINOv2 feature extraction on a small sample of images and visualize embeddings to verify quality
  2. Run a single LLM (GPT-4o) on a few test images to verify zero-shot classification capability
  3. Create a minimal MAS with two LLMs and verify that the ensemble produces different predictions than individual models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-based multi-agent systems be further optimized for species-level classification of marine mammals?
- Basis in paper: [explicit] The paper discusses the development of an LLM-based multi-agent system (MAS) that improves classification accuracy, particularly at the species level, but notes that there is still room for improvement compared to traditional methods.
- Why unresolved: The MAS shows promise in enhancing LLM performance, but it does not fully match the accuracy of traditional models like SVM, KNN, and fine-tuned neural networks at the species level.
- What evidence would resolve it: Comparative studies evaluating different configurations of MAS, including varying numbers of agents, types of foundational models, and ensemble strategies, to determine the optimal setup for species-level classification.

### Open Question 2
- Question: What are the potential biases in LLMs when classifying marine mammals, and how can these biases be mitigated?
- Basis in paper: [explicit] The paper mentions that LLMs' predictions often show a bias toward more common species due to the frequency of species in the pre-training data, which can skew predictions in zero-shot settings.
- Why unresolved: The paper identifies the presence of bias but does not explore specific methods to mitigate it or quantify the extent of the bias across different LLM models.
- What evidence would resolve it: Analysis of LLM predictions on a balanced dataset with equal representation of species, and development of techniques such as data augmentation, re-weighting, or fine-tuning to reduce bias.

### Open Question 3
- Question: How does the performance of LLMs vary with different prompt strategies in marine mammal classification tasks?
- Basis in paper: [explicit] The paper evaluates different prompting strategies, such as zero-shot and zero-shot Chain-of-Thought, and finds variations in performance, particularly at the species level where CoT sometimes decreases accuracy.
- Why unresolved: While the paper tests several prompting strategies, it does not explore the full range of possible prompts or the reasons behind the performance differences observed.
- What evidence would resolve it: Systematic experimentation with a broader set of prompts, including variations in phrasing, context, and reasoning steps, to identify the most effective strategies for different classification levels and LLM models.

## Limitations

- Dataset size (1,423 images across 65 species) is relatively small for deep learning applications, potentially limiting generalizability
- Evaluation focuses on accuracy metrics without considering computational efficiency, inference time, or deployment constraints critical for practical conservation applications
- LLM zero-shot learning results are sensitive to training data distribution, showing bias toward common species that may not represent rare or endangered marine mammals

## Confidence

- **High Confidence:** Traditional ML models (KNN, SVM) with DINOv2 embeddings for species-level classification - supported by strong quantitative results and well-established methodology
- **Medium Confidence:** Pre-trained DL models (VGG, ResNet) performance across classification levels - results are consistent but limited by dataset size
- **Medium Confidence:** Zero-shot LLM performance at medium and group levels - demonstrated effectiveness but with acknowledged species-level limitations
- **Low Confidence:** MAS improvement claims - while results show positive trends, the ensemble approach's benefits could vary significantly with different LLM combinations

## Next Checks

1. **Dataset Expansion Validation:** Acquire additional marine mammal images to increase dataset size by at least 50% and re-evaluate all model performances to test scalability and robustness claims.

2. **Environmental Condition Testing:** Systematically vary image conditions (lighting, water clarity, angles) in test sets to validate model robustness across different real-world deployment scenarios.

3. **Computational Efficiency Analysis:** Measure inference times and resource requirements for MAS implementation versus single-model approaches to quantify the practical trade-offs between accuracy gains and computational costs.