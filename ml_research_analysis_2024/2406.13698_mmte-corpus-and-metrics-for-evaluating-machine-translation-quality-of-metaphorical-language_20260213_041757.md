---
ver: rpa2
title: 'MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical
  Language'
arxiv_id: '2406.13698'
source_url: https://arxiv.org/abs/2406.13698
tags:
- translation
- metaphor
- metaphorical
- language
- literal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMTE, the first systematic evaluation framework
  for machine translation of metaphorical language. The authors create a multilingual
  metaphor corpus by post-editing translations of 315 metaphorical and 332 literal
  sentences between English and Chinese/Italian.
---

# MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language

## Quick Facts
- **arXiv ID**: 2406.13698
- **Source URL**: https://arxiv.org/abs/2406.13698
- **Reference count**: 26
- **Primary result**: Introduces MMTE, the first systematic evaluation framework for machine translation of metaphorical language, demonstrating that translating metaphors is more challenging than literal expressions

## Executive Summary
This paper introduces MMTE, the first systematic evaluation framework for machine translation quality of metaphorical language. The authors create a multilingual metaphor corpus by post-editing translations of 315 metaphorical and 332 literal sentences between English and Chinese/Italian. They propose four human evaluation metrics: Metaphorical Equivalence (full/part/non-equivalent), Emotion, Authenticity, and Quality. Experiments show that metaphorical translations receive lower quality scores than literal translations, with full-equivalent metaphors achieving significantly higher quality scores. The study also finds that emotional content is better preserved in equivalent metaphor translations and that translation difficulty varies between language pairs, with Chinese-English showing lower quality than Italian-English.

## Method Summary
The study employs a post-editing approach with 18 native-speaking annotators to create a multilingual metaphor corpus from the MOH dataset. Machine translations are generated using four MT models (Google Cloud, Youdao, Helsinki-NLP/opus-mt, GPT-4o) for English-Chinese and English-Italian pairs. Human annotators evaluate translations using four criteria: Quality, Equivalence, Emotion, and Authenticity. The Metaphorical Equivalence metric categorizes translations as full/part/non-equivalent based on preservation of metaphorical meaning. Automatic metrics including BLEU, ROUGE, BERTScore, and GPT-4o are also used for evaluation.

## Key Results
- Metaphorical translations receive significantly lower quality scores than literal translations
- Full-equivalent metaphor translations achieve significantly higher quality scores than non-equivalent ones
- Emotional content is better preserved in equivalent metaphor translations
- Translation quality varies between language pairs, with Chinese-English showing lower scores than Italian-English

## Why This Works (Mechanism)
The effectiveness of MMTE stems from its comprehensive approach to metaphor translation evaluation. By combining post-edited reference translations with multiple evaluation criteria, the framework captures both surface-level accuracy and deeper semantic equivalence. The Metaphorical Equivalence metric specifically addresses the core challenge of metaphor translation - preserving non-literal meaning across languages. The use of multiple MT models and human evaluation criteria provides a robust assessment that accounts for both linguistic and cultural nuances in metaphor interpretation.

## Foundational Learning
- **Metaphorical Equivalence classification**: Distinguishing between full, partial, and non-equivalent metaphor translations is essential for understanding how well meaning is preserved across languages. Quick check: Annotators correctly categorize 80% of metaphor translations into equivalence levels.
- **Emotion preservation in translation**: Metaphorical language often carries emotional weight that must be maintained for effective communication. Quick check: Emotion ratings correlate with quality scores above 0.7.
- **Post-editing methodology**: Creating reference translations through post-editing provides a more natural baseline than direct translation. Quick check: Post-edited references show higher human ratings than raw MT outputs.
- **Cross-linguistic metaphor variation**: Metaphors may not translate directly due to cultural and linguistic differences. Quick check: Metaphor equivalence rates differ by at least 15% between language pairs.

## Architecture Onboarding
- **Component map**: MOH dataset -> MT models (Google Cloud, Youdao, Helsinki-NLP/opus-mt, GPT-4o) -> Post-editing by annotators -> Human evaluation (Quality, Equivalence, Emotion, Authenticity) -> Automatic evaluation (BLEU, ROUGE, BERTScore, GPT-4o)
- **Critical path**: Source sentences → MT generation → Post-editing → Human evaluation → Quality assessment
- **Design tradeoffs**: Post-editing provides natural references but introduces potential bias; using multiple MT models increases coverage but adds complexity; human evaluation captures nuance but is subjective and resource-intensive
- **Failure signatures**: Inconsistent equivalence labeling across annotators; difficulty distinguishing between literal and metaphorical translations; subjective variation in emotion ratings
- **First experiments**: 1) Replicate metaphor equivalence classification with new annotators; 2) Compare automatic metrics correlation with human quality scores; 3) Test translation quality variation across additional language pairs

## Open Questions the Paper Calls Out
- How can automatic evaluation metrics be improved to better capture metaphorical equivalence?
- What role does cultural context play in metaphor translation across different language families?
- Can metaphor translation quality be predicted based on linguistic features of the source language?
- How do different translation strategies (e.g., domestication vs. foreignization) affect metaphor preservation?

## Limitations
- Small sample size of 647 sentences may not represent full complexity of metaphorical language
- Post-editing approach may introduce bias by adapting metaphors to more literal expressions
- Subjective evaluation metrics may not fully capture nuanced quality of metaphorical translation
- Only two target languages considered, limiting generalizability across language families
- Focus on English as source language restricts cross-linguistic analysis
- Limited exploration of metaphor types and domains within the corpus

## Confidence
- **High confidence**: Core finding that metaphorical translations receive lower quality scores than literal translations
- **Medium confidence**: Full-equivalent metaphors achieve significantly higher quality scores
- **Medium confidence**: Emotional content better preserved in equivalent metaphor translations
- **Medium confidence**: Translation difficulty varies between language pairs, with Chinese-English showing lower quality than Italian-English

## Next Checks
1. Conduct a follow-up study with a larger and more diverse corpus of metaphorical sentences, including multiple language pairs from different language families, to test the generalizability of the findings
2. Implement a blind evaluation where annotators are not given the source sentences to reduce potential bias in assessing metaphorical equivalence and emotional preservation
3. Develop and test additional objective metrics for metaphorical translation quality that complement the human evaluation criteria, potentially using semantic similarity measures or metaphorical pattern recognition
4. Explore the impact of different metaphor types (structural, orientational, ontological) on translation quality across languages
5. Investigate the role of cultural context in metaphor translation by including annotators from different cultural backgrounds