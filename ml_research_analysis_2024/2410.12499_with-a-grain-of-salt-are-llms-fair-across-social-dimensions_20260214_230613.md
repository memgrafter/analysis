---
ver: rpa2
title: 'With a Grain of SALT: Are LLMs Fair Across Social Dimensions?'
arxiv_id: '2410.12499'
source_url: https://arxiv.org/abs/2410.12499
tags:
- bias
- across
- generation
- debate
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SALT (Social Appropriateness in LLM-Generated
  Text), a methodology for detecting bias in large language models across gender,
  religion, and race. The approach generates prompts for seven bias triggers (General
  Debate, Positioned Debate, Career Advice, Story Generation, Problem-Solving, Cover-Letter
  Writing, and CV Generation) and evaluates Llama and Gemma models using automated
  pairwise comparisons via GPT-4o-as-a-Judge.
---

# With a Grain of SALT: Are LLMs Fair Across Social Dimensions?

## Quick Facts
- arXiv ID: 2410.12499
- Source URL: https://arxiv.org/abs/2410.12499
- Authors: Samee Arif; Zohaib Khan; Maaidah Kaleem; Suhaib Rashid; Agha Ali Raza; Awais Athar
- Reference count: 21
- Primary result: Introduces SALT methodology for detecting bias in LLMs across gender, religion, and race using automated pairwise comparisons

## Executive Summary
This paper introduces SALT (Social Appropriateness in LLM-Generated Text), a methodology for detecting bias in large language models across gender, religion, and race. The approach generates prompts for seven bias triggers and evaluates Llama and Gemma models using automated pairwise comparisons via GPT-4o-as-a-Judge. Bias is quantified by win rates after anonymizing outputs. Results show consistent polarization across models, with notable variations across languages. Human evaluations confirm high agreement with LLM judgments. The study highlights the need for bias mitigation strategies and provides a scalable framework for bias detection.

## Method Summary
The SALT methodology generates prompts using GPT-4o for seven bias triggers (General Debate, Positioned Debate, Career Advice, Story Generation, Problem-Solving, Cover-Letter Writing, and CV Generation). These prompts are programmatically combined with group labels to form the full prompt set. Llama and Gemma models generate outputs for each prompt, which are then anonymized using GPT-4o-mini to remove demographic cues. GPT-4o-as-a-Judge evaluates anonymized outputs pairwise, and win/loss counts are aggregated to compute Bias Scores. The methodology is validated through human evaluation with high agreement (Cohen's Kappa 0.72-0.76) between LLM judgments and human annotators.

## Key Results
- All tested models exhibited consistent polarization toward certain groups across all bias triggers
- German language prompts showed the least bias compared to English and Arabic
- Human evaluations confirmed high agreement with LLM judgments (Cohen's Kappa 0.72-0.76)
- Bias patterns varied significantly across different tasks and social dimensions
- Both small-scale and mid-scale models exhibited similar bias levels, suggesting model size doesn't necessarily mitigate bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated debates can be reliably evaluated for bias using automated judges (GPT-4o).
- Mechanism: The paper compares outputs from two groups on a given task after anonymizing demographic cues. GPT-4o-as-a-Judge evaluates which output is better, and the win/loss count across all pairwise comparisons quantifies bias toward one group.
- Core assumption: The automated judge provides consistent and unbiased evaluations that align with human judgment.
- Evidence anchors: Human evaluations confirm high agreement with LLM judgments (Cohen's Kappa 0.72-0.76). The Kappa score between GPT-4o and Annotator 1 is 0.72, and between GPT-4o and Annotator 2 is 0.76, indicating high agreement.

### Mechanism 2
- Claim: Prompt templates combined with synthetic entity generation yield a scalable, diverse bias detection dataset.
- Mechanism: GPT-4o generates 25 diverse examples for each entity (e.g., topics, professions, events) for a given prompt template. These are programmatically combined with group labels to form the full prompt set.
- Core assumption: The synthetic generation process produces varied and meaningful prompts that can elicit biased responses from LLMs.
- Evidence anchors: Unlike traditional approaches that rely on manually created prompts, the methodology leverages synthetic data generation to produce diverse and scalable inputs.

### Mechanism 3
- Claim: Anonymizing outputs before evaluation reduces evaluation bias due to explicit group identifiers.
- Mechanism: GPT-4o-mini removes all demographic references (names, gender, race, religion, etc.) from LLM-generated texts before the pairwise comparison.
- Core assumption: Removing explicit identifiers sufficiently masks group identity so the judge cannot infer it from context or style.
- Evidence anchors: To ensure a fair comparison and eliminate any gender, religious, or racial identifiers that could influence judgment, GPT-4o-mini is used to remove these references from LLM-generated texts.

## Foundational Learning

- Concept: Inter-annotator agreement (Cohen's Kappa)
  - Why needed here: To validate that the automated judge's bias evaluations match human judgment, establishing reliability of the methodology.
  - Quick check question: If Kappa is 0.72 between LLM and human, what does this indicate about their level of agreement according to standard interpretation?

- Concept: Bias quantification via win/loss counts
  - Why needed here: Provides a simple, interpretable metric (Bias Score = (Wins - Losses) / Total Comparisons) to measure systematic favoritism across groups.
  - Quick check question: If Group A wins 60 out of 100 comparisons and Group B wins 40, what is the Bias Score for Group A?

- Concept: Prompt template + entity generation pattern
  - Why needed here: Enables scalable creation of diverse bias detection prompts without manual effort, essential for covering multiple tasks, languages, and groups.
  - Quick check question: If you have 7 triggers, 5 religions, and 25 generated topics per trigger, how many religion-related prompts are created?

## Architecture Onboarding

- Component map: GPT-4o prompt generation → Prompt assembly → Llama/Gemma generation → GPT-4o-mini anonymization → GPT-4o judge comparison → Win/loss tally → Bias Score calculation

- Critical path: GPT-4o prompt generation → Prompt assembly → Llama/Gemma generation → GPT-4o-mini anonymization → GPT-4o judge comparison → Win/loss tally → Bias Score calculation

- Design tradeoffs:
  - Using GPT-4o as judge ensures high quality but adds cost and potential bias propagation
  - Synthetic prompt generation is scalable but may miss nuanced real-world scenarios
  - Anonymization reduces explicit bias cues but may not fully mask group identity inferred from context

- Failure signatures:
  - Low or inconsistent Kappa scores between judge and human evaluations indicate unreliable bias detection
  - Large variance in Bias Scores across different triggers for the same group suggests trigger-specific anomalies
  - Bias scores not aligning with expected patterns may indicate prompt or evaluation issues

- First 3 experiments:
  1. Generate a small set of prompts (e.g., 5 per trigger) for one group pair, run through the pipeline, and manually verify anonymization and judge outputs
  2. Compute Bias Scores for one model on one category (e.g., gender) and compare to human-annotated subset to confirm agreement
  3. Test anonymization effectiveness by checking if judge outputs differ significantly when comparing anonymized vs. non-anonymized pairs for the same content

## Open Questions the Paper Calls Out

- Question: How does the bias manifest when gender, religion, and race categories are intersected (e.g., Muslim female vs. Christian male)?
  - Basis in paper: The paper mentions future work could explore compounded biases when these dimensions intersect
  - Why unresolved: The current study focuses on atomic levels of each category separately, not their intersections
  - What evidence would resolve it: Analysis of bias patterns when prompts combine multiple demographic attributes to reveal intersectional bias effects

- Question: Does increasing model size effectively reduce bias, or is the bias primarily determined by training data?
  - Basis in paper: The paper notes that both small-scale and mid-scale models exhibit similar bias levels, suggesting model size doesn't necessarily mitigate bias
  - Why unresolved: The study only compared models of different sizes but didn't systematically test whether bias reduction correlates with model capacity
  - What evidence would resolve it: Testing a wider range of model sizes (including larger models) and analyzing whether bias scores decrease proportionally with model capacity

- Question: How do bias patterns evolve as LLMs are updated to newer versions?
  - Basis in paper: The paper suggests future work could study how biases evolve with newer model versions
  - Why unresolved: The current study only examined specific versions of Llama and Gemma models without comparing them to earlier or later iterations
  - What evidence would resolve it: Systematic comparison of bias scores across multiple versions of the same model family to track bias evolution over time

## Limitations
- The synthetic prompt generation quality and representativeness remain uncertain without validation against real-world scenarios
- Using GPT-4o-as-a-Judge introduces risk of bias propagation from the judge itself
- The assumption that anonymization fully masks group identity is unverified and critical to the methodology's validity

## Confidence

- High Confidence: The methodology for quantifying bias via win/loss counts is straightforward and mathematically sound. The high agreement between LLM judgments and human evaluations (Cohen's Kappa 0.72-0.76) provides strong support for the reliability of the automated evaluation process.

- Medium Confidence: The scalability of synthetic prompt generation is demonstrated, but without validation of prompt quality and real-world representativeness, the completeness of bias detection remains uncertain. The multilingual evaluation shows promise but covers only three languages, limiting generalizability.

- Low Confidence: The assumption that anonymization fully masks group identity is the weakest link. Without empirical validation that the judge cannot infer group identity from anonymized outputs, the entire bias quantification methodology is vulnerable to systematic error.

## Next Checks
1. **Anonymization Robustness Test**: Create a controlled experiment where the same content is evaluated with and without anonymization, ensuring the judge's outputs are statistically similar. This would validate that anonymization does not introduce bias or allow group identity inference.

2. **Prompt Quality Validation**: Manually review a random sample of synthetic prompts to assess their diversity, realism, and ability to elicit biased responses. Compare against a set of manually crafted prompts to ensure synthetic generation is not missing critical scenarios.

3. **Judge Bias Audit**: Conduct an adversarial evaluation where known biased outputs are presented to the judge, measuring its ability to detect bias consistently. Additionally, test the judge's performance on a balanced dataset to quantify any inherent preference for certain groups or content styles.