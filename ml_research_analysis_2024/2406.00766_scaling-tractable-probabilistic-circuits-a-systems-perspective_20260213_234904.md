---
ver: rpa2
title: 'Scaling Tractable Probabilistic Circuits: A Systems Perspective'
arxiv_id: '2406.00766'
source_url: https://arxiv.org/abs/2406.00766
tags:
- node
- nodes
- product
- blocks
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PyJuice, a GPU-based system that significantly
  accelerates training and inference of probabilistic circuits (PCs). By employing
  block-based parallelization and a novel compilation process, PyJuice reduces redundant
  memory reloads and leverages GPU Tensor Cores, achieving 1-2 orders of magnitude
  speedup and 2-5x memory savings over existing systems.
---

# Scaling Tractable Probabilistic Circuits: A Systems Perspective

## Quick Facts
- arXiv ID: 2406.00766
- Source URL: https://arxiv.org/abs/2406.00766
- Reference count: 40
- Primary result: GPU-based system achieving 1-2 orders of magnitude speedup and 2-5x memory savings for training probabilistic circuits

## Executive Summary
This paper introduces PyJuice, a GPU-based system that significantly accelerates training and inference of probabilistic circuits (PCs). By employing block-based parallelization and a novel compilation process, PyJuice reduces redundant memory reloads and leverages GPU Tensor Cores, achieving 1-2 orders of magnitude speedup and 2-5x memory savings over existing systems. The approach also handles numerical underflow via PC flows, simplifying backpropagation. Evaluated on four common PC structures—PD, RAT-SPN, HCLT, and HMM—PyJuice trains models with up to 2 billion edges efficiently. It enables larger-scale experiments, improves perplexity in language models, and advances density estimation on image datasets. The work establishes new performance baselines and sets the stage for scalable PC applications.

## Method Summary
PyJuice is a GPU-based system that accelerates probabilistic circuit training through block-based parallelization and efficient compilation. The key innovation is dividing sum and product nodes into blocks, reducing memory reloads by computing multiple outputs per memory access. The system compiles PCs into compact index tensors for efficient GPU execution, uses Tensor Cores for matrix operations, and employs PC flows to handle numerical underflow during backpropagation. The architecture includes a compiler, forward/backward kernels, memory manager, and dynamic programming module for optimal layer partitioning. This enables training PCs with billions of edges while maintaining exact inference capabilities.

## Key Results
- Achieves 1-2 orders of magnitude speedup compared to existing PC systems (Juice.jl, SPFlow, EiNet, Dynamax)
- Reduces GPU memory usage by 2-5x through efficient memory management and block-based parallelization
- Enables training of PCs with up to 2 billion edges, expanding the scale of tractable probabilistic modeling
- Improves language model perplexity and image density estimation performance through larger, more complex PC architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Block-based parallelization reduces excessive memory reloads by a factor of M/KM compared to node-based parallelization.
- Mechanism: Instead of each processor independently computing the output of every sum node, PyJuice divides sum nodes into blocks of KM nodes and product nodes into blocks of KN nodes. Each processor computes KM outputs by iterating through connected product node blocks, loading each input probability only once per block.
- Core assumption: The PC structure exhibits block-sparsity where pairs of sum and product node blocks are either fully connected or unconnected.
- Evidence anchors:
  - [section]: "Specifically, in Figure 3, define KM = KN = 2, we compute the output of m0 and m1 by first calculating the weighted sum w.r.t. the input probability of n0 and n1 in step #1, and then accumulate the probabilities coming from n2 and n3 in step #2."
  - [section]: "The fundamental principle guiding our design is to properly group, or allocate, sum edges to different processors to minimize the reloading of product nodes' outputs."
- Break condition: If the PC lacks block-sparsity or has irregular connectivity patterns that prevent effective block grouping.

### Mechanism 2
- Claim: PC flows provide mathematically equivalent gradients while being more numerically convenient for backpropagation.
- Mechanism: Instead of directly computing parameter gradients, PyJuice backpropagates PC flows Fn(x) recursively defined through the PC structure. This avoids storing intermediate values and eliminates inter-thread-block barriers.
- Core assumption: PC flows are a factor of θn,c away from the desired gradients and can be computed recursively using the same layer structure as the forward pass.
- Evidence anchors:
  - [section]: "PC flows exhibit a straightforward recursive definition, facilitating a seamless transformation into an efficient implementation for the backward pass."
  - [section]: "While similar results have been established in a slightly different context (Peharz et al., 2020a), we prove the following equations in Section B.2 for completeness: Fn(x) = ∂logp nr (x)/∂logp n(x) and Fn,c(x) = θn,c · ∂logp nr (x)/∂θn,c."
- Break condition: If the PC structure contains consecutive sum or product nodes that cannot be collapsed.

### Mechanism 3
- Claim: Compiling PC layers into balanced workloads reduces inter-thread-block barriers and improves memory efficiency.
- Mechanism: PyJuice partitions sum node blocks into groups with similar numbers of connected child node blocks, padding with pseudo-product node blocks when necessary. This enables balanced workload allocation and efficient index tensor construction.
- Core assumption: Different sum node blocks connect to different numbers of product node blocks, creating workload imbalance that can be mitigated through grouping.
- Evidence anchors:
  - [section]: "The second issue is illustrated by the node block {m2, m3}, which connects to two child node blocks, while the others connect to only one."
  - [section]: "Partitioning a layer into groups with the same number of children allows us to use different kernel launching hyperparameters according to the specific setup of every node group (e.g., number of nodes) to achieve better performance."
- Break condition: If the layer contains sum node blocks with highly variable numbers of children that cannot be effectively grouped.

## Foundational Learning

- Concept: Probabilistic Circuits (PCs) as tractable deep generative models
  - Why needed here: Understanding the basic PC structure and inference capabilities is essential for grasping why PyJuice's optimizations are effective
  - Quick check question: What structural constraints make PCs tractable for exact inference?

- Concept: GPU memory hierarchy and Tensor Cores
  - Why needed here: PyJuice leverages Tensor Cores for matrix multiplications and optimizes memory access patterns between L1/L2 cache and HBM
  - Quick check question: How does block-based parallelization reduce memory reloads in the context of GPU memory hierarchy?

- Concept: Dynamic programming for optimal grouping
  - Why needed here: The layer partitioning algorithm uses dynamic programming to minimize overhead while maintaining sparsity tolerance
  - Quick check question: What is the time complexity of the DP algorithm and how does it handle unique values in the input?

## Architecture Onboarding

- Component map: Compiler -> Forward pass kernel -> Backward pass kernel -> Memory manager -> Dynamic programming module
- Critical path:
  1. Compile PC structure into index tensors (one-time cost)
  2. Forward pass through sum and product layers using block-based parallelization
  3. Backward pass computing PC flows recursively
  4. Parameter updates using accumulated flows
  5. Repeat for training epochs
- Design tradeoffs:
  - Memory vs computation: Recomputing product node probabilities during backward pass saves memory but increases computation time
  - Block size vs sparsity: Larger block sizes improve efficiency but require more padding for sparse PCs
  - Compilation time vs runtime: More sophisticated compilation produces better runtime performance but takes longer to compile
- Failure signatures:
  - Out-of-memory errors: Indicate insufficient GPU memory for the PC size and batch configuration
  - Poor speedup: Suggest ineffective block grouping due to irregular connectivity patterns
  - Numerical underflow: May occur if logsumexp trick implementation has precision issues
- First 3 experiments:
  1. Compile and run forward pass on a small PD structure (e.g., 10K nodes) to verify basic functionality
  2. Test block-based parallelization on a fully connected sum layer with varying block sizes to measure IO reduction
  3. Compare runtime and memory usage against Juice.jl on a medium-sized PC (e.g., 100K nodes) with different batch sizes

## Open Questions the Paper Calls Out
- [No open questions identified in the provided text]

## Limitations
- Theoretical guarantees for block-based parallelization are limited to specific PC structures with regular connectivity patterns
- Compilation overhead and its impact on overall training time for very large PCs remains an open question
- Performance benefits may vary significantly for PCs with irregular connectivity patterns that don't exhibit assumed block-sparsity

## Confidence
- **High confidence**: Claims about memory usage improvements (2-5x savings) and basic speedup from Tensor Core utilization are well-supported by implementation details and standard GPU optimization principles
- **Medium confidence**: 1-2 orders of magnitude speedup claims depend heavily on specific PC structures and datasets used in evaluation
- **Medium confidence**: PC flows mechanism for handling numerical underflow is mathematically rigorous but needs more systematic evaluation across different PC architectures

## Next Checks
1. Benchmark PyJuice on PCs with highly irregular connectivity patterns to test limits of block-based parallelization approach
2. Conduct ablation studies measuring compilation overhead across different PC sizes to determine break-even point
3. Test numerical stability of PC flows against traditional log-space computations on PCs with extreme parameter values