---
ver: rpa2
title: Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts
arxiv_id: '2409.13266'
source_url: https://arxiv.org/abs/2409.13266
tags:
- keyphrases
- keyphrase
- domain
- data
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting keyphrase generation
  models to new domains when in-domain labeled data is scarce or expensive to obtain.
  The proposed silk method extracts silver-standard keyphrases from citation contexts
  to create synthetic labeled data for unsupervised domain adaptation.
---

# Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts

## Quick Facts
- arXiv ID: 2409.13266
- Source URL: https://arxiv.org/abs/2409.13266
- Reference count: 36
- Key outcome: silk method improves F1@M scores by up to 4.9 points on domain adaptation for keyphrase generation

## Executive Summary
This paper addresses the challenge of adapting keyphrase generation models to new scientific domains when labeled in-domain data is scarce. The proposed silk method extracts silver-standard keyphrases from citation contexts to create synthetic labeled data for unsupervised domain adaptation. By fine-tuning a pre-trained BART model on these synthetic samples, silk significantly improves in-domain performance across three scientific domains (NLP, Astrophysics, and Paleontology). Human evaluation confirms the quality of the generated keyphrases, with 80% being relevant and well-formed.

## Method Summary
The silk method works by extracting citation contexts from scientific papers and mining keyphrase candidates from titles, abstracts, and these contexts. A scoring function combining salience, relevance, and reliability metrics ranks the candidates, which are then filtered to create silver-standard keyphrase sets. These synthetic samples are used to few-shot fine-tune a pre-trained BART model, with the top-1K samples yielding optimal performance. The approach requires no in-domain labeled data while significantly improving domain-specific keyphrase generation.

## Key Results
- silk improves F1@5 and F1@10 scores by up to 4.9 points compared to baselines
- Top-1K synthetic samples consistently outperform random and bottom-1K orderings
- No catastrophic forgetting observed during adaptation; slight improvements on KP20k test set
- 80% of silk-generated keyphrases rated as relevant and well-formed in human evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Citation contexts contain high-quality phrases that represent the cited paper's key concepts.
- Mechanism: Citation contexts often highlight the contributions of a cited paper, making them reliable sources for mining keyphrases that capture the document's main ideas.
- Core assumption: Authors cite papers specifically to reference their contributions, and these references are typically accompanied by descriptive text that accurately summarizes the cited work's key concepts.
- Evidence anchors:
  - [abstract] "Citation contexts —text passages within the citing document containing the reference— often highlight the contributions of a cited paper"
  - [section] "Earlier research on keyphrase extraction has emphasized the value of citation context information as a feature for ranking phrases"
  - [corpus] Weak evidence - the corpus analysis only shows related papers but doesn't directly validate citation context quality
- Break condition: If citation contexts are used for purposes other than highlighting contributions (e.g., criticism, comparison), the extracted phrases may not represent key concepts.

### Mechanism 2
- Claim: The confidence ranking based on salience, relevance, and reliability scores effectively selects high-quality synthetic samples.
- Mechanism: The scoring function combines multiple criteria (presence in title/abstract/citation contexts, semantic similarity to title, and citation frequency) to rank keyphrase candidates, ensuring that selected phrases are both relevant and reliable.
- Core assumption: The combination of these three criteria provides a robust estimate of keyphrase quality that correlates with human judgment.
- Evidence anchors:
  - [abstract] "We determine the confidence of our method by averaging the scores of its silver-standard keyphrases"
  - [section] "We observe that the random and top-1K ordering schemes lead to improvements, with top-1K yielding the best results"
  - [corpus] Weak evidence - no direct corpus validation of the scoring function's effectiveness
- Break condition: If the scoring criteria are not well-calibrated for the target domain, the ranking may not select the most useful samples.

### Mechanism 3
- Claim: Few-shot fine-tuning on high-confidence synthetic samples effectively adapts the model to new domains without catastrophic forgetting.
- Mechanism: By carefully selecting and ordering synthetic samples based on confidence scores, the model can learn domain-specific patterns while maintaining its general capabilities.
- Core assumption: The synthetic samples are representative enough of the target domain to provide useful adaptation signals, but not so different that they cause the model to forget its initial training.
- Evidence anchors:
  - [abstract] "Experiments on three scientific domains... show that silk significantly improves in-domain performance"
  - [section] "We observe no drop in performance for our adapted models... rather surprisingly, we notice small improvements"
  - [corpus] Moderate evidence - the corpus analysis shows domain differences but doesn't directly validate the adaptation mechanism
- Break condition: If the domain shift is too large or the synthetic samples are too noisy, the model may either fail to adapt or experience catastrophic forgetting.

## Foundational Learning

- Concept: Citation context mining and analysis
  - Why needed here: The entire method relies on extracting meaningful keyphrases from citation contexts, requiring understanding of how citations are structured and used in scientific writing
  - Quick check question: What are the three main components used to score keyphrase candidates in the silk method?

- Concept: Domain adaptation through few-shot learning
  - Why needed here: The paper's contribution is demonstrating how to adapt a pre-trained model to new domains using limited synthetic data
  - Quick check question: What is the key difference between the silk method and traditional self-learning approaches for domain adaptation?

- Concept: Evaluation metrics for keyphrase generation
  - Why needed here: Understanding F1@M scores, present vs. absent keyphrases, and statistical significance testing is crucial for interpreting the results
  - Quick check question: Why does the paper report separate scores for present and absent keyphrases?

## Architecture Onboarding

- Component map:
  Citation context extractor → Keyphrase candidate generator → Scoring function → Confidence-based ranking → Few-shot fine-tuning pipeline
  External dependencies: spacy (for phrase extraction), SPECTER (for embeddings), GROBID (for PDF parsing in paleo domain)

- Critical path:
  1. Extract citation contexts from input documents
  2. Generate keyphrase candidates from titles, abstracts, and citation contexts
  3. Score candidates using salience, relevance, and reliability metrics
  4. Select optimal subset of keyphrases based on constraints
  5. Rank documents by confidence and select top-N for fine-tuning
  6. Fine-tune BART model on selected synthetic samples

- Design tradeoffs:
  - Quality vs. quantity: Using top-1K samples vs. larger datasets
  - Domain specificity vs. generalization: Balancing adaptation to new domains while maintaining core capabilities
  - Computational cost vs. performance: Few-shot fine-tuning vs. full re-training

- Failure signatures:
  - Poor performance on in-domain test sets despite adaptation
  - Catastrophic forgetting of initial domain capabilities
  - Bias towards highly cited papers' keyphrases
  - Overfitting to synthetic data characteristics

- First 3 experiments:
  1. Validate confidence ranking by comparing performance of top-1K vs. bottom-1K vs. random samples
  2. Test different values of N (500, 1K, 2K) to find optimal number of synthetic samples
  3. Compare silk method against self-learning approach using the same documents but different labeling strategy

## Open Questions the Paper Calls Out
- How does the performance of silk vary with different citation context extraction methods?
- Can silk be effectively applied to domains outside of scientific papers, such as news articles or social media posts?
- How does the performance of silk compare to other unsupervised domain adaptation methods for keyphrase generation?
- How does the quality of synthetic samples generated by silk impact the performance of adapted keyphrase generation models?

## Limitations
- Relies on citation contexts as proxy for keyphrase quality, which may not generalize to domains with different citation practices
- Performance depends heavily on quality of pre-trained BART model and KP20k dataset
- Scoring mechanism has only been validated within scientific domains

## Confidence
- High confidence: The general approach of using citation contexts for synthetic keyphrase generation and the overall experimental methodology
- Medium confidence: The specific scoring function parameters (α=2, λx=0.85, λr=0.75) and their domain-specific effectiveness
- Medium confidence: The claim that silk outperforms self-learning approaches across all domains and metrics

## Next Checks
1. Test the silk method on non-scientific domains (e.g., news articles, legal documents) to assess generalizability of citation context mining
2. Conduct ablation studies to isolate the contribution of each scoring component (salience, relevance, reliability) to overall performance
3. Evaluate the robustness of the method when citation contexts are sparse or when documents have few citations