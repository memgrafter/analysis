---
ver: rpa2
title: 'Group and Shuffle: Efficient Structured Orthogonal Parametrization'
arxiv_id: '2406.10019'
source_url: https://arxiv.org/abs/2406.10019
tags:
- matrices
- orthogonal
- matrix
- block
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GS-matrices, a structured matrix class that
  generalizes Monarch matrices and enables efficient orthogonal parametrization. The
  method uses alternating products of block-diagonal matrices and permutations to
  form dense orthogonal matrices with fewer parameters and computations than prior
  approaches like BOFT.
---

# Group and Shuffle: Efficient Structured Orthogonal Parametrization

## Quick Facts
- arXiv ID: 2406.10019
- Source URL: https://arxiv.org/abs/2406.10019
- Reference count: 25
- Primary result: GS-matrices require only 1+⌈log_b(r)⌉ matrices versus 1+⌈log_2(r)⌉ for block butterfly matrices to form dense orthogonal matrices

## Executive Summary
This paper introduces GS-matrices, a structured matrix class that generalizes Monarch matrices and enables efficient orthogonal parametrization for neural network fine-tuning. The method uses alternating products of block-diagonal matrices and permutations to form dense orthogonal matrices with fewer parameters and computations than prior approaches like BOFT. The authors demonstrate superior performance on orthogonal fine-tuning (GLUE benchmark), subject-driven generation (Stable Diffusion), and 1-Lipschitz neural networks (CIFAR-100) compared to LoRA, OFT, and BOFT.

## Method Summary
GS-matrices are constructed as products of block-diagonal matrices with permutation matrices, generalizing both Monarch and butterfly matrix structures. Orthogonality is enforced through Cayley parametrization applied to each block of the diagonal matrices. The method is applied to orthogonal fine-tuning by multiplying the pretrained weight matrix with a learnable orthogonal matrix Q, initialized as identity. An extension to two-sided orthogonal fine-tuning (Double GSOFT) adapts both left and right singular vectors of the weight matrix. The framework is also adapted for orthogonal convolutions using grouped convolutions with channel shuffling.

## Key Results
- GS-matrices require only 1+⌈log_b(r)⌉ matrices versus 1+⌈log_2(r)⌉ for block butterfly matrices to form dense matrices
- Superior performance on GLUE benchmark compared to LoRA, OFT, and BOFT
- Better subject-driven generation results on Stable Diffusion with higher CLIP similarities
- Mixed results on CIFAR-100 for 1-Lipschitz neural networks, with GSConv showing some advantages but not consistently outperforming baselines

## Why This Works (Mechanism)

### Mechanism 1
GS-matrices require fewer matrices to construct dense orthogonal matrices than block butterfly matrices by using alternating block-diagonal matrices with permutations for more efficient information transmission between neuron groups. The chosen permutations (P(k,n)) optimally mix information across blocks, reducing the required number of layers.

### Mechanism 2
The orthogonal parametrization of GS-matrices preserves key spectral properties during fine-tuning by enforcing orthogonality at the block level using Cayley parametrization. This guarantees the overall matrix remains orthogonal, preserving angles between neuron activations and preventing gradient vanishing/exploding.

### Mechanism 3
Two-sided orthogonal fine-tuning (Double GSOFT) adapts both left and right singular vectors, improving adaptation capacity by applying orthogonal transformations to both sides of the weight matrix (QUW0QV). This allows adaptation of both row and column spaces, similar to LoRA but with orthogonal constraints.

## Foundational Learning

- **Structured matrices and their computational advantages**: Understanding how GS-matrices generalize Monarch and butterfly matrices is crucial for grasping their efficiency claims. *Quick check: How does the block-diagonal plus permutation structure enable efficient computation compared to dense matrices?*

- **Orthogonal matrices and their properties**: The entire method relies on maintaining orthogonality for stability during fine-tuning. *Quick check: Why does enforcing orthogonality at the block level guarantee orthogonality of the entire matrix?*

- **Parameter-efficient fine-tuning methods**: GS matrices are applied within the PEFT framework, so understanding LoRA, OFT, and BOFT is essential. *Quick check: How does the multiplicative nature of orthogonal fine-tuning differ from the additive nature of LoRA?*

## Architecture Onboarding

- **Component map**: Block-diagonal matrices -> Permutation matrices -> Cayley parametrization -> Weight matrix multiplication
- **Critical path**: 1) Construct GS matrices with appropriate block sizes and permutations, 2) Enforce orthogonality via Cayley parametrization on blocks, 3) Initialize Q as identity, 4) Apply Q to pretrained weights during forward pass, 5) Backpropagate through structured matrix operations
- **Design tradeoffs**: Expressiveness vs. parameter efficiency (more blocks increase capacity but also parameters), computational efficiency vs. adaptation capacity (fewer matrices are faster but may limit adaptation), orthogonality constraints vs. flexibility (strict orthogonality ensures stability but may restrict some transformations)
- **Failure signatures**: Training instability (orthogonality being violated), poor performance (insufficient expressiveness or suboptimal permutation choices), memory issues (block sizes too large for available resources)
- **First 3 experiments**: 1) Verify GS matrix construction: Create a small GS matrix and confirm it matches expected structure, 2) Test orthogonality preservation: Apply Cayley parametrization and verify resulting matrix is orthogonal, 3) Compare to baseline: Implement OFT with same architecture and compare parameter counts and forward pass speed

## Open Questions the Paper Calls Out

### Open Question 1
Can GS-matrices without orthogonality constraints be applied to tasks outside of deep learning parameter-efficient fine-tuning? The paper suggests GS-matrices could have broader applications beyond the specific use cases explored, as they mention "GS -matrices without orthogonality constraints is another promising direction to consider." This remains unresolved as the paper primarily focuses on orthogonal parametrization and its applications in fine-tuning.

### Open Question 2
Is there a theoretical guarantee that GS-matrices can represent all orthogonal matrices in the GS(Pm+1,...,P1) class? The paper states "It is not clear if an analog to Theorem 1 is correct in this case as well" when discussing higher-order GS-matrices, indicating uncertainty about whether all orthogonal matrices can be represented. While Theorem 1 proves that orthogonal blocks are sufficient for 2-matrix GS-matrices, the paper does not extend this proof to the general case with more than two block-diagonal matrices.

### Open Question 3
How does the choice of permutation matrices affect the expressive power of GS-matrices in practice? The paper uses specific permutations (P(k,n)) but does not explore how different permutation strategies might impact performance or whether the current choice is optimal.

## Limitations

- Limited empirical validation of efficiency claims comparing GS-matrices to butterfly matrices in terms of actual speed and memory usage
- CIFAR-100 results are inconclusive with GSConv showing mixed performance compared to baseline architectures
- Extension to 1-Lipschitz neural networks and convolutional architectures lacks thorough ablation studies and exploration of hyperparameter sensitivity

## Confidence

- **High confidence**: The basic GS-matrix construction and Cayley parametrization implementation appear sound and well-founded in linear algebra principles
- **Medium confidence**: The orthogonal fine-tuning results on GLUE benchmark show consistent improvements over baselines, though some tasks show marginal gains
- **Low confidence**: The CIFAR-100 results are inconclusive, with GSConv performing worse than competitors in some metrics, and the two-sided fine-tuning extension lacks thorough ablation studies

## Next Checks

1. **Efficiency validation**: Implement timing benchmarks comparing GS-matrix forward passes against butterfly matrices for various block sizes to empirically verify the theoretical efficiency claims.

2. **Ablation on permutation choice**: Systematically test different permutation strategies beyond the proposed P(k,n) to determine if the current choice is optimal or if alternative permutations yield better performance.

3. **CIFAR-100 robustness analysis**: Conduct additional experiments with varying Lipschitz constants and training procedures to better understand when GSConv outperforms or underperforms baseline architectures.