---
ver: rpa2
title: 'RedEx: Beyond Fixed Representation Methods via Convex Optimization'
arxiv_id: '2401.07606'
source_url: https://arxiv.org/abs/2401.07606
tags:
- u1d456
- u1d434
- u1d449
- u1d461
- u1d443
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Reduced Extractor-Expander (RedEx) architecture,
  designed to bridge the gap between neural networks and fixed representation methods
  like kernels and random features. RedEx is as expressive as neural networks and
  can be trained efficiently in a layer-wise fashion via a convex program with semi-definite
  constraints.
---

# RedEx: Beyond Fixed Representation Methods via Convex Optimization

## Quick Facts
- **arXiv ID**: 2401.07606
- **Source URL**: https://arxiv.org/abs/2401.07606
- **Reference count**: 40
- **One-line primary result**: RedEx is an architecture that bridges neural networks and fixed representation methods, achieving neural network expressiveness with provable optimization guarantees.

## Executive Summary
This paper introduces RedEx (Reduced Extractor-Expander), a novel architecture designed to bridge the gap between neural networks and fixed representation methods like kernels and random features. RedEx can be trained efficiently in a layer-wise fashion via convex optimization with semi-definite constraints, achieving neural network expressiveness while maintaining provable optimization guarantees. The core innovation lies in using an extractor matrix with orthogonal rows to select important input directions, followed by quadratic expansion and linear transformation.

## Method Summary
RedEx operates by first extracting relevant input directions through an orthogonal matrix, then applying a quadratic expansion to capture higher-order interactions, and finally using a linear transformation to map to the output space. The architecture can be trained layer-wise using semi-definite programming for multi-layer networks, or via standard gradient descent for single-output cases using a norm formulation equivalent to the trace norm. The method extends naturally to convolutional settings and maintains theoretical guarantees on expressive power and sample complexity.

## Key Results
- RedEx can efficiently learn sparse-parity functions that fixed representation methods cannot, achieving loss O(1) with high probability
- The architecture achieves neural network expressiveness with only a quadratic increase in size compared to the circuit it expresses
- For scalar outputs, RedEx can be trained using standard gradient descent due to its equivalence to the trace norm
- RedEx provides provable optimization guarantees through its layer-wise training approach

## Why This Works (Mechanism)

### Mechanism 1
RedEx can learn sparse-parity-type functions efficiently because the layer-wise training allows each layer to progressively extract relevant input directions and build higher-degree polynomial features. The extractor matrix with orthogonal rows ensures well-conditioned optimization, while the quadratic expansion effectively increases feature space dimension to capture parity relationships that fixed representations miss.

### Mechanism 2
For scalar outputs, RedEx's norm formulation allows training with standard gradient descent because the RedEx norm is equivalent to the trace norm. This equivalence enables reformulation of the optimization problem into an unconstrained convex problem, avoiding the computational overhead of semi-definite programming.

### Mechanism 3
Convolutional RedEx extends the architecture to handle translation-invariant data by applying the same extractor to all input patches and then expanding each patch quadratically. This allows the network to learn local features invariant to spatial position, similar to standard convolutional neural networks.

## Foundational Learning

- **Semi-definite programming (SDP)**: Used to enforce orthogonality and norm constraints during RedEx training; check: difference between SDP and linear programs, why SDPs enforce orthogonality
- **Trace norm and nuclear norm**: RedEx norm equals trace norm for scalar outputs, enabling unconstrained optimization; check: relationship between trace norm, nuclear norm, and matrix rank
- **Kravchuk polynomials**: Used in the sparse-parity problem as orthogonal polynomials with respect to specific distributions; check: properties of orthogonal polynomials and their use in sparse-parity problems

## Architecture Onboarding

- **Component map**: Input -> Extractor matrix (orthogonal rows) -> Quadratic expansion -> Linear transformation -> Output
- **Critical path**: 1) Initialize extractor with orthogonal rows, 2) Compute quadratic expansion of extracted features, 3) Apply linear transformation, 4) Compute loss and gradients, 5) Update parameters using SDP solver or gradient descent, 6) Repeat layer-wise
- **Design tradeoffs**: Width parameter controls expressive power vs. sample complexity; orthogonality constraint ensures well-conditioned optimization vs. potential loss of flexibility; quadratic expansion captures interactions vs. computational cost; layer-wise training provides provable convergence vs. potential suboptimality
- **Failure signatures**: Slow convergence (orthogonality too restrictive), poor generalization (width too large, insufficient regularization), high computational cost (inefficient solver or excessive layers)
- **First 3 experiments**: 1) Train on synthetic sparse-parity dataset vs. kernel methods, 2) Vary width parameter and observe effects, 3) Implement convolutional extension and test on small image dataset

## Open Questions the Paper Calls Out

- Can RedEx's generalization rate be improved from O(1/√A) to O(1/A), similar to Wang and Lin (2021)?
- Is there a simple closed-form expression for the RedEx norm when the output dimension ≥ 2?
- Can RedEx be trained more efficiently without using heavy convex SDP solvers?

## Limitations

- Limited empirical validation across diverse tasks, with heavy reliance on theoretical bounds
- Computational complexity of solving SDPs for large-scale problems not thoroughly analyzed
- Claims about expressive power rely heavily on theoretical analysis rather than extensive empirical demonstrations

## Confidence

- **High confidence**: Claims about RedEx's ability to learn sparse-parity functions efficiently (supported by rigorous proofs)
- **Medium confidence**: Claims about layer-wise training convergence and generalization bounds (supported by theory but limited empirical validation)
- **Medium confidence**: Claims about convolutional RedEx extension (described conceptually with minimal experimental evidence)

## Next Checks

1. Implement RedEx on CIFAR-10 to measure training time, memory usage, and performance compared to standard neural networks
2. Train RedEx with relaxed orthogonality constraints to measure impact on convergence, expressive power, and generalization
3. Test RedEx on diverse benchmark tasks (classification, regression, structured prediction) to evaluate general applicability beyond sparse-parity problems