---
ver: rpa2
title: A Data Selection Approach for Enhancing Low Resource Machine Translation Using
  Cross-Lingual Sentence Representations
arxiv_id: '2409.02712'
source_url: https://arxiv.org/abs/2409.02712
tags:
- translation
- machine
- language
- score
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving machine translation
  quality for low-resource language pairs, specifically English-Marathi. It proposes
  a data filtering approach using cross-lingual sentence representations to mitigate
  the impact of noisy data on translation model performance.
---

# A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations

## Quick Facts
- arXiv ID: 2409.02712
- Source URL: https://arxiv.org/abs/2409.02712
- Authors: Nidhi Kowtal; Tejas Deshpande; Raviraj Joshi
- Reference count: 28
- Primary result: IndicSBERT-based filtering improves English-Marathi translation from BLEU 28.9 to 35.6

## Executive Summary
This paper addresses the challenge of improving machine translation quality for low-resource language pairs, specifically English-Marathi. The authors propose a data filtering approach using cross-lingual sentence representations to mitigate the impact of noisy data on translation model performance. By leveraging a multilingual SBERT model, specifically IndicSBERT, to assess semantic equivalence between original and translated sentences, the method retains linguistically correct translations while discarding problematic instances. The results demonstrate significant improvements in translation quality across multiple evaluation metrics.

## Method Summary
The authors employ IndicSBERT to compute semantic similarity scores between English-Marathi sentence pairs in a noisy parallel corpus (3.6M sentences). Sentence pairs with similarity scores below 0.7 are filtered out, reducing the dataset to approximately 1.5M pairs. This cleaned dataset is then used to fine-tune an IndicBART model, which is compared against a baseline model trained on the full noisy dataset. The evaluation uses multiple metrics including BLEU, METEOR, CHRF, CHRF++, and IndicSBERT scores on a curated test set.

## Key Results
- BLEU Score improved from 28.9 to 35.6
- METEOR Score improved from 28.7 to 34.1
- CHRF Score improved from 37.044 to 43.674
- CHRF++ Score improved from 37.860 to 44.223
- IndicSBERT Score improved from 75.3 to 78.2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IndicSBERT's cross-lingual sentence embeddings can accurately identify semantically equivalent sentence pairs, enabling effective filtering of noisy data.
- Mechanism: The model computes similarity scores between original English sentences and their Marathi translations, retaining only pairs with scores above a threshold (0.7), thus removing misaligned or low-quality translations.
- Core assumption: The cross-lingual embeddings from IndicSBERT preserve semantic equivalence well enough to distinguish correct translations from noisy ones.
- Evidence anchors:
  - [abstract] "Specifically, we employ an IndicSBERT similarity model to assess the semantic equivalence between original and translated sentences, allowing us to retain linguistically correct translations while discarding instances with substantial deviations."
  - [section] "We employed an IndicSBERT similarity model to assess the semantic equivalence between original and translated sentences, allowing us to retain linguistically correct translations while discarding instances with substantial deviations."
- Break condition: If the semantic embeddings fail to capture Marathi-specific linguistic nuances or if the similarity threshold is too high/low, the filtering may remove useful data or retain noisy data.

### Mechanism 2
- Claim: Removing duplicates and filtering by similarity score improves translation quality by training on cleaner, more consistent data.
- Mechanism: The pipeline removes duplicate sentence pairs and low-similarity pairs, reducing redundancy and noise, which leads to more robust model learning and better translation outputs.
- Core assumption: Noise in the dataset directly harms model performance and filtering improves the signal-to-noise ratio for training.
- Evidence anchors:
  - [abstract] "The results demonstrate a significant improvement in translation quality over the baseline post-filtering with IndicSBERT."
  - [section] "The results demonstrate a significant improvement in translation quality over the baseline post-filtering with IndicSBERT."
- Break condition: If the filtered dataset becomes too small, it may lack diversity, harming the model's generalization ability.

### Mechanism 3
- Claim: Using a fine-tuned IndicBART model on the filtered dataset leverages the benefits of the cleaner training data to improve translation accuracy.
- Mechanism: The model is trained on the 1 million filtered sentences rather than the full noisy dataset, allowing it to learn more accurate translation patterns without interference from incorrect or ambiguous examples.
- Core assumption: The filtered dataset maintains sufficient coverage and diversity for effective training.
- Evidence anchors:
  - [abstract] "The proposed method shows improved BLEU Score (28.9 to 35.6), METEOR Score (28.7 to 34.1), CHRF Score (37.044 to 43.674), CHRF++ Score (37.860 to 44.223), and IndicSBERT Score (75.3 to 78.2) compared to the baseline model."
  - [section] "We trained the model only on the filtered sentences with a similarity score greater than 0.7. The outputs of all three models—the original IndicBART, the fine-tuned IndicBART, and the filtered dataset using IndicSBERT are thus analyzed."
- Break condition: If the filtering process removes too many sentences or introduces bias, the model's performance could degrade.

## Foundational Learning

- Concept: Cross-lingual sentence embeddings and similarity metrics
  - Why needed here: To understand how IndicSBERT computes semantic similarity across languages, which is the core of the filtering approach.
  - Quick check question: How does a multilingual SBERT model produce comparable embeddings for sentences in different languages?

- Concept: Machine translation evaluation metrics (BLEU, METEOR, CHRF)
  - Why needed here: To interpret the reported improvements and assess translation quality quantitatively.
  - Quick check question: What does a BLEU score of 35.6 indicate about translation accuracy compared to 28.9?

- Concept: Data preprocessing and filtering strategies
  - Why needed here: To grasp the importance of cleaning noisy datasets before model training, especially for low-resource languages.
  - Quick check question: Why might removing duplicate sentences help reduce noise in a parallel corpus?

## Architecture Onboarding

- Component map:
  Input (raw corpus) -> Preprocessor (duplicate removal) -> Filter (IndicSBERT similarity scoring) -> Output (cleaned corpus) -> Trainer (IndicBART fine-tuning) -> Evaluator (multiple metrics)

- Critical path:
  1. Load and preprocess raw dataset
  2. Compute IndicSBERT similarity scores for each sentence pair
  3. Filter pairs with similarity ≥ 0.7
  4. Train IndicBART on filtered dataset
  5. Evaluate using multiple metrics on curated test set

- Design tradeoffs:
  - Threshold selection: Higher threshold yields cleaner data but smaller dataset; lower threshold retains more data but may include noise.
  - Computational cost: IndicSBERT scoring is expensive; balance between quality and efficiency.
  - Domain coverage: Filtering may bias dataset toward certain types of sentences.

- Failure signatures:
  - Low improvement in BLEU/METEOR after filtering may indicate poor semantic alignment or threshold misconfiguration.
  - Drastic dataset size reduction may suggest overly aggressive filtering.
  - Degradation in specific linguistic phenomena may point to bias introduced by filtering.

- First 3 experiments:
  1. Run IndicSBERT similarity scoring on a small sample and manually inspect high vs. low similarity pairs to verify filtering logic.
  2. Train a baseline IndicBART on the full dataset, then repeat on a 50% randomly sampled dataset to establish upper/lower bounds for filtering impact.
  3. Vary the similarity threshold (0.6, 0.7, 0.8) and measure trade-offs between dataset size and evaluation metric improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of IndicSBERT filtering vary across different low-resource language pairs beyond English-Marathi?
- Basis in paper: [explicit] The authors mention their method provides "a valuable framework for enhancing translation quality in other low-resource language translation tasks."
- Why unresolved: The paper only evaluates their approach on English-Marathi. Performance may differ significantly for other language pairs due to varying linguistic structures and dataset characteristics.
- What evidence would resolve it: Testing the same filtering approach on multiple other low-resource language pairs (e.g., Hindi-Tamil, Bengali-Gujarati) and comparing performance gains across them.

### Open Question 2
- Question: What is the optimal similarity threshold for IndicSBERT filtering across different translation tasks?
- Basis in paper: [explicit] The authors use 0.7 as the threshold but don't explore how different thresholds affect results or whether this is optimal.
- Why unresolved: The choice of 0.7 appears arbitrary and may not be ideal for all scenarios or language pairs.
- What evidence would resolve it: Conducting experiments with various similarity thresholds (e.g., 0.6, 0.65, 0.7, 0.75, 0.8) and analyzing the trade-off between data retention and translation quality.

### Open Question 3
- Question: How does the proposed filtering method compare to other data selection techniques for low-resource machine translation?
- Basis in paper: [inferred] The paper focuses on their filtering approach but doesn't compare it to other data selection methods like cross-entropy difference, word frequency-based filtering, or domain-specific filtering.
- Why unresolved: Without comparative analysis, it's unclear whether IndicSBERT filtering is superior to alternative approaches or context-dependent.
- What evidence would resolve it: Benchmarking against multiple established data selection methods using the same datasets and evaluation metrics.

## Limitations
- The optimal similarity threshold (0.7) is not empirically validated through sensitivity analysis.
- The filtering approach may introduce bias toward certain sentence types, potentially reducing model generalization.
- The computational cost of IndicSBERT scoring is not addressed, limiting scalability to larger datasets.

## Confidence

**High Confidence:** The general approach of using cross-lingual sentence embeddings for data filtering is well-established, and the observed improvements in translation metrics are statistically significant based on the reported numbers.

**Medium Confidence:** The specific threshold of 0.7 for similarity scoring and its optimal value for this dataset remain uncertain without ablation studies or sensitivity analysis.

**Low Confidence:** The claim that IndicSBERT specifically is superior to other multilingual embedding models for this task lacks direct comparison or ablation experiments.

## Next Checks

1. Conduct threshold sensitivity analysis by experimenting with different similarity thresholds (e.g., 0.6, 0.7, 0.8) and measuring the trade-off between dataset size and translation quality improvements.

2. Perform manual inspection of randomly sampled high and low similarity pairs to verify that the filtering correctly identifies semantically equivalent translations and removes noisy pairs.

3. Test the filtering approach using alternative multilingual embedding models (e.g., LASER, mBERT) to assess whether IndicSBERT is uniquely effective or if the method generalizes across models.