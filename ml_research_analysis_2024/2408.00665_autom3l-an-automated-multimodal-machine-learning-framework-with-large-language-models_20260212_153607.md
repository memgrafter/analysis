---
ver: rpa2
title: 'AutoM3L: An Automated Multimodal Machine Learning Framework with Large Language
  Models'
arxiv_id: '2408.00665'
source_url: https://arxiv.org/abs/2408.00665
tags:
- data
- autom3l
- multimodal
- user
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoM3L is an LLM-powered framework for automated multimodal machine
  learning. It automates the entire pipeline, including modality inference, feature
  engineering, model selection, pipeline assembly,
---

# AutoM3L: An Automated Multimodal Machine Learning Framework with Large Language Models

## Quick Facts
- arXiv ID: 2408.00665
- Source URL: https://arxiv.org/abs/2408.00665
- Authors: Daqin Luo; Chengjian Feng; Yuxuan Nong; Yiqing Shen
- Reference count: 40
- Primary result: AutoM3L outperforms traditional AutoML methods on multimodal datasets while being more user-friendly

## Executive Summary
AutoM3L is an LLM-powered framework that automates the entire multimodal machine learning pipeline. It uses specialized LLM modules to handle modality inference, feature engineering, model selection, pipeline assembly, and hyperparameter optimization. The framework demonstrates superior performance compared to traditional rule-based AutoML methods while offering improved usability through natural language interaction and interactive visualization.

## Method Summary
AutoM3L employs five specialized LLM modules that process structured table representations of multimodal data through in-context learning and code generation. The framework uses MI-LLM for modality inference, AFE-LLM for feature engineering (filtering and imputation), MS-LLM for model selection from a dynamic model zoo, PA-LLM for pipeline assembly and code generation, and HPO-LLM for hyperparameter optimization. The system is evaluated on six multimodal datasets using 10-fold cross-validation and compared against AutoKeras and AutoGluon baselines.

## Key Results
- AutoM3L achieved competitive or superior performance on six multimodal datasets compared to traditional AutoML methods
- The framework demonstrated significant improvements in user-friendliness with lower cognitive load scores (33.75 vs 48.50 for AutoGluon) and higher usability ratings (88.45 vs 65.33 for AutoGluon)
- LLM-based feature engineering showed consistent improvements across all test datasets, while AutoGluon and AutoKeras suffered performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AutoM3L automates multimodal pipeline construction by using LLMs as controllers for modality inference, feature engineering, model selection, pipeline assembly, and hyperparameter optimization.
- Mechanism: The framework uses specialized LLM modules (MI-LLM, AFE-LLM, MS-LLM, PA-LLM, HPO-LLM) that process structured table representations of multimodal data through in-context learning and code generation.
- Core assumption: LLMs can accurately interpret semantic information from column names, data samples, and user instructions to make decisions about data processing and model configuration.
- Evidence anchors:
  - [abstract] "AutoM3L comprehends data modalities and selects appropriate models based on user requirements, providing automation and interactivity."
  - [section 2.3] "AFE-LLM leverages the exceptional capabilities of LLMs to enhance this process...utilizes two distinct prompts, resulting in two core components: AFE-LLMfilter and AFE-LLMimputed."
  - [corpus] Weak evidence - no direct comparisons with LLM-based AutoML frameworks in literature
- Break condition: LLM fails to correctly interpret data semantics, produces incorrect code, or cannot handle the context length limits for model card selection.

### Mechanism 2
- Claim: The LLM-based feature engineering module outperforms rule-based approaches by intelligently filtering irrelevant features and imputing missing data.
- Mechanism: AFE-LLMfilter uses in-context learning with examples to identify irrelevant attributes, while AFE-LLMimputed fills missing values by discerning patterns and inter-attribute relationships from data samples.
- Core assumption: LLMs can understand the task context and data relationships well enough to make informed decisions about feature relevance and missing value imputation.
- Evidence anchors:
  - [section 2.3] "The AFE-LLMfilter component effectively sifts through the data to eliminate irrelevant or redundant attributes...The AFE-LLMimputed component is dedicated to data imputation, ensuring the completeness and reliability of essential data."
  - [Table 3] "Automated feature engineering...effectively mitigates the impact of noisy data. Across all test datasets, automated feature engineering showed improvements, while AutoGluon and AutoKeras suffered from performance degradation."
  - [corpus] Weak evidence - limited literature on LLM-based feature engineering in AutoML
- Break condition: LLM misclassifies relevant features as irrelevant or makes incorrect imputation decisions that harm model performance.

### Mechanism 3
- Claim: The LLM-based model selection module intelligently matches data modalities with appropriate models from a dynamic model zoo.
- Mechanism: MS-LLM filters model cards by applicable modality type, uses text similarity metrics to identify top candidates, then selects the most suitable model based on user requirements and data descriptions.
- Core assumption: LLMs can effectively parse model card descriptions and user requirements to make optimal model selection decisions.
- Evidence anchors:
  - [section 2.4] "MS-LLM...effectively match each modality with the appropriate model...The model card captures a wide range of details...To streamline the generation of these cards, we leverage LLM-enhanced tools."
  - [Table 11] "we generated 10 different commands from a user instruction using GPT-3.5, all of which were accurately mapped to the lightweight models, indicating the robustness of MS-LLM."
  - [corpus] Weak evidence - no direct evidence of LLM-based model selection in AutoML literature
- Break condition: LLM fails to understand model capabilities or user requirements, leading to suboptimal model selection.

## Foundational Learning

- Concept: Multimodal data representation
  - Why needed here: AutoM3L requires understanding how different data modalities (image, text, tabular) can be structured and processed together
  - Quick check question: How would you represent a dataset containing product images, text descriptions, and numerical specifications in a structured table format?

- Concept: In-context learning
  - Why needed here: LLM modules rely on in-context learning to perform tasks without additional training, using examples and prompts
  - Quick check question: What are the key components of an effective in-context learning prompt for an LLM?

- Concept: Code generation with LLMs
  - Why needed here: PA-LLM generates executable code for model fusion and data processing pipelines
  - Quick check question: What are the main challenges when using LLMs to generate code for machine learning pipelines?

## Architecture Onboarding

- Component map: MI-LLM → AFE-LLM → MS-LLM → PA-LLM → HPO-LLM
- Critical path: Modality inference → Feature engineering → Model selection → Pipeline assembly → Hyperparameter optimization
- Design tradeoffs: LLM-based automation vs. rule-based approaches (flexibility vs. determinism), context length limitations vs. model card comprehensiveness
- Failure signatures: Incorrect modality classification, feature filtering errors, wrong model selection, code generation failures, hyperparameter search space issues
- First 3 experiments:
  1. Test MI-LLM on a simple multimodal dataset with clear column names to verify modality inference accuracy
  2. Validate AFE-LLMfilter by introducing known irrelevant features and checking if they're correctly removed
  3. Test MS-LLM with a small model zoo to ensure correct model selection based on user requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AutoM3L scale with the size and complexity of multimodal datasets, particularly those with more than three modalities or larger numbers of features?
- Basis in paper: [inferred] The paper evaluates AutoM3L on six multimodal datasets and discusses future plans to incorporate more diverse data modalities like graph, audio, and point clouds. This suggests that the current evaluation is limited and scalability is a potential concern.
- Why unresolved: The paper does not provide experimental results on datasets with more than three modalities or larger feature spaces. The scalability of the framework to handle increased complexity remains an open question.
- What evidence would resolve it: Experimental results demonstrating AutoM3L's performance on larger and more complex multimodal datasets, including those with four or more modalities and significantly more features than those currently evaluated.

### Open Question 2
- Question: What are the specific sources and types of biases introduced by the LLM components in AutoM3L, and how do these biases impact the fairness and accuracy of the model selection and feature engineering processes?
- Basis in paper: [explicit] The paper discusses potential biases in LLMs, such as gender and industry biases, and proposes mitigation strategies like fine-tuning and improved prompt engineering. However, it does not provide concrete evidence of the biases' impact on AutoM3L's performance.
- Why unresolved: The paper acknowledges the potential for bias but does not quantify its effects on the framework's outcomes. The specific biases and their impact on model selection and feature engineering are not empirically validated.
- What evidence would resolve it: Quantitative analysis demonstrating the presence and magnitude of biases in AutoM3L's outputs, along with controlled experiments showing how these biases affect model performance and fairness across different demographic groups or data characteristics.

### Open Question 3
- Question: How does the computational overhead introduced by the LLM components in AutoM3L compare to the computational cost of the actual model training, and is this overhead justified by the improvements in automation and performance?
- Basis in paper: [explicit] The paper provides a comparison of training times between AutoM3L and AutoGluon, showing a minimal increase in training time due to LLM usage. However, it does not provide a comprehensive cost-benefit analysis of the LLM overhead.
- Why unresolved: While the paper mentions that the LLM overhead is minimal, it does not explore the trade-offs between the computational cost of LLM integration and the benefits in terms of automation, user-friendliness, and model performance. The justification for the overhead is not fully established.
- What evidence would resolve it: A detailed analysis comparing the computational costs (both time and monetary) of AutoM3L with and without LLM components, alongside a comprehensive evaluation of the improvements in automation, user experience, and model performance. This would help determine whether the LLM overhead is justified by the gains achieved.

## Limitations

- Framework performance heavily depends on LLM API availability and cost, which is not addressed in the paper
- Comparisons are limited to only two baselines (AutoKeras and AutoGluon) without including other contemporary AutoML frameworks
- User study sample size is small (10 participants), limiting generalizability of usability findings

## Confidence

- **High confidence**: The framework's modular architecture and the general approach of using LLMs for automated pipeline construction are well-specified and theoretically sound. The quantitative performance improvements over traditional AutoML methods on the tested datasets are supported by experimental results.

- **Medium confidence**: The LLM modules' ability to accurately perform modality inference, feature engineering, and model selection is demonstrated but relies on the assumption that LLMs can consistently interpret semantic information from structured tables. The user study results showing improved usability are promising but based on a limited sample size.

- **Low confidence**: The framework's generalization to diverse real-world multimodal datasets beyond the tested examples is not established. The cost-effectiveness and practical deployment considerations (API dependencies, context length limitations) are not thoroughly addressed.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate AutoM3L on a broader range of multimodal datasets from different domains to assess its generalization capability beyond the six tested datasets.

2. **Baseline expansion**: Compare AutoM3L against additional contemporary AutoML frameworks (e.g., TPOT, H2O AutoML, Auto-sklearn) to provide a more comprehensive performance benchmark.

3. **Scalability analysis**: Test the framework's performance and efficiency on larger-scale multimodal datasets to evaluate its practical applicability in real-world scenarios with high-dimensional data.