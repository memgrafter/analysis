---
ver: rpa2
title: 'CF-OPT: Counterfactual Explanations for Structured Prediction'
arxiv_id: '2405.18293'
source_url: https://arxiv.org/abs/2405.18293
tags:
- explanations
- counterfactual
- explanation
- number
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CF-OPT, a method to generate counterfactual
  explanations for structured prediction models that combine deep learning and optimization
  layers. The core idea is to find close and plausible counterfactual examples by
  leveraging variational autoencoders (VAEs) to model the plausibility of explanations
  in a lower-dimensional latent space.
---

# CF-OPT: Counterfactual Explanations for Structured Prediction

## Quick Facts
- arXiv ID: 2405.18293
- Source URL: https://arxiv.org/abs/2405.18293
- Reference count: 40
- Primary result: Introduces CF-OPT, a method for generating plausible counterfactual explanations for structured prediction models by leveraging variational autoencoders in latent space with cost-aware training and first-order optimization

## Executive Summary
This paper addresses the challenge of generating counterfactual explanations for structured prediction models that combine deep learning with optimization layers. The authors propose CF-OPT, a method that generates close and plausible counterfactual examples by leveraging variational autoencoders (VAEs) to model plausibility in a lower-dimensional latent space. The approach introduces a novel cost-aware VAE training objective and latent hypersphere plausibility regularization, using modified differential method of multipliers (MDMM) for efficient optimization. Experiments demonstrate that CF-OPT can generate high-quality explanations efficiently, with significant improvements in plausibility compared to naive adversarial search methods.

## Method Summary
CF-OPT generates counterfactual explanations for structured prediction models by reformulating the problem in latent space using a VAE. The method first encodes input features into a lower-dimensional latent space where plausibility constraints are defined as a thickened hypersphere. A cost-aware VAE training objective is introduced to maintain relevance to the downstream optimization task. The modified differential method of multipliers (MDMM) then solves the constrained optimization problem through gradient descent on the primal variable and gradient ascent on the dual variable. The approach handles relative, absolute, and ε-explanations while ensuring generated counterfactuals remain close to the data manifold and satisfy plausibility constraints.

## Key Results
- CF-OPT generates plausible counterfactual explanations efficiently, with latency orders of magnitude faster than naive adversarial search methods
- The latent hypersphere plausibility regularization significantly improves explanation quality, reducing reconstruction error and decision-focused reconstruction error
- CF-OPT scales well with the number of features and decisions, though absolute explanations are more sensitive to combinatorial complexity
- Experiments on Warcraft maps and tabular data demonstrate the method's effectiveness across different structured prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CF-OPT generates plausible counterfactual explanations by leveraging variational autoencoders (VAEs) to model the plausibility of explanations in a lower-dimensional latent space
- Mechanism: The method uses a VAE to encode the feature space into a lower-dimensional latent space, where a plausibility constraint is defined as a thickened hypersphere. This constraint ensures that generated counterfactuals remain close to the data manifold, avoiding adversarial examples
- Core assumption: The latent space distribution of the VAE closely approximates the true encoded data distribution, which is modeled as an isotropic Gaussian
- Evidence anchors:
  - [abstract]: "We build upon variational autoencoders a principled way of obtaining counterfactuals: working in the latent space leads to a natural notion of plausibility of explanations."
  - [section]: "Thus, we reformulate the plausibility constraint x ∈ D of problem (2) in latent space, and state that a close and plausible —latent— counterfactual explanation zalt is a solution to the problem:..."
  - [corpus]: Weak corpus alignment - most neighbor papers focus on generative counterfactual explanations but not specifically in structured prediction contexts with optimization layers
- Break condition: If the VAE fails to capture the true data manifold or if the latent space dimensionality is insufficient, the plausibility constraint may not effectively prevent adversarial examples

### Mechanism 2
- Claim: The cost-aware VAE training objective improves the quality of counterfactual explanations by accounting for the downstream optimization task
- Mechanism: A modified VAE training objective includes an additional term that penalizes the distance between predicted costs before and after reconstruction. This ensures that the reconstructed features maintain their relevance to the optimization layer
- Core assumption: The prediction model's performance is sensitive to small perturbations in the input features, and maintaining cost consistency improves explanation quality
- Evidence anchors:
  - [abstract]: "We finally introduce a variant of the classic loss for V AE training that improves their performance in our specific structured context."
  - [section]: "We adopt an empirical approach to this issue and penalize the distance between predicted costs before and after reconstruction during training."
  - [corpus]: Weak corpus alignment - while cost-aware training is mentioned in some neighbor papers, the specific application to structured prediction with optimization layers is unique
- Break condition: If the cost reconstruction error is not sufficiently reduced or if the additional regularization term overfits the training data, the explanations may become less relevant to the downstream task

### Mechanism 3
- Claim: The modified differential method of multipliers (MDMM) efficiently solves the counterfactual explanation problem by relaxing the constrained optimization problem into an augmented Lagrangian
- Mechanism: The method performs gradient descent on the primal variable and gradient ascent on the dual variable to reach a local minimum that satisfies the explanation constraint. This approach is well-suited for the highly non-convex optimization problem
- Core assumption: The energy function (augmented Lagrangian) is differentiable almost everywhere with respect to the latent code, allowing for gradient-based optimization
- Evidence anchors:
  - [abstract]: "These provide the foundations of CF-OPT, a first-order optimization algorithm that can find counterfactual explanations for a broad class of structured learning architectures."
  - [section]: "The idea of the MDMM is to perform a gradient descent on E w.r.t z and a gradient ascent on E w.r.t λ in order to reach a local minimum z∗ that satisfies the constraint χ(z∗) = 0."
  - [corpus]: Weak corpus alignment - while first-order optimization methods are common in counterfactual explanations, the specific application of MDMM to structured prediction is novel
- Break condition: If the energy function is not sufficiently smooth or if the initialization is poor, the algorithm may converge to a suboptimal or infeasible solution

## Foundational Learning

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: VAEs provide a principled way to model the plausibility of counterfactual explanations by encoding the feature space into a lower-dimensional latent space where plausibility constraints can be defined
  - Quick check question: How does the VAE's latent space distribution approximate the true encoded data distribution, and why is this approximation important for defining plausibility constraints?

- Concept: Counterfactual Explanations
  - Why needed here: Counterfactual explanations provide actionable insights to achieve desired outcomes by suggesting minimal changes to input features. In the context of structured prediction, they help explain the decision-making process of complex pipelines
  - Quick check question: What are the differences between relative, absolute, and ε-explanations, and how do they apply to the structured prediction setting?

- Concept: First-Order Optimization Methods
  - Why needed here: First-order optimization methods, such as the modified differential method of multipliers (MDMM), are used to efficiently solve the constrained optimization problem of finding counterfactual explanations
  - Quick check question: How does the MDMM differ from other first-order optimization methods, and why is it well-suited for the highly non-convex optimization problem in structured prediction?

## Architecture Onboarding

- Component map:
  Input: Feature vector x ∈ X ⊆ R^nx (covariate/context) -> Prediction model φ: Transforms x into intermediate input θ ∈ Θ ⊆ R^nθ -> Optimization layer: Returns decision y* ∈ argmin y∈Y θ^T y, where Y ⊆ R^ny is the feasible set -> VAE: Encodes feature space into lower-dimensional latent space Z ⊆ R^nz, where plausibility constraints are defined -> MDMM: Solves the counterfactual explanation problem by relaxing the constrained optimization into an augmented Lagrangian

- Critical path:
  1. Encode input feature vector x into latent space using VAE encoder
  2. Define plausibility constraint as thickened hypersphere in latent space
  3. Solve counterfactual explanation problem using MDMM
  4. Decode latent code back into feature space using VAE decoder
  5. Evaluate explanation quality using reconstruction error and decision-focused metrics

- Design tradeoffs:
  - Latent space dimensionality: Higher dimensionality may capture more complex data manifolds but increase computational cost
  - Plausibility regularization weight β: Higher values ensure more plausible explanations but may increase reconstruction error
  - Cost-aware regularization weight α: Higher values improve cost reconstruction but may overfit the training data

- Failure signatures:
  - High reconstruction error: VAE fails to capture the true data manifold
  - Adversarial examples: Plausibility constraint is not effective in preventing unrealistic explanations
  - Slow convergence: MDMM struggles to find a feasible solution within the maximum number of iterations

- First 3 experiments:
  1. Evaluate the impact of latent space dimensionality on explanation quality and computational cost
  2. Compare the proposed latent hypersphere regularization with alternative plausibility constraints (e.g., centered ball)
  3. Analyze the sensitivity of explanations to the cost-aware regularization weight α and plausibility regularization weight β

## Open Questions the Paper Calls Out
The paper identifies extending the work to categorical or discrete features as a promising research direction, noting that the current formulation relies on continuous feature spaces for proximity measurements and optimization.

## Limitations
- Sensitivity to combinatorial complexity, particularly for absolute explanations where exponential search may be required
- Heavy dependence on VAE's ability to capture true data manifold, which is not guaranteed
- MDMM requires careful hyperparameter tuning and may struggle with highly non-convex optimization landscapes

## Confidence
- High confidence: The core mechanism of using VAEs for plausibility constraints in latent space is well-established and supported by empirical results
- Medium confidence: The cost-aware VAE training objective and its impact on explanation quality, as this requires further validation on diverse datasets
- Low confidence: The scalability of CF-OPT for high-dimensional feature spaces and complex combinatorial problems, given the limited experimental evaluation

## Next Checks
1. Evaluate CF-OPT's performance on datasets with significantly higher feature dimensionality and more complex combinatorial constraints to assess scalability
2. Compare the proposed latent hypersphere plausibility regularization with alternative constraints (e.g., centered ball) to determine the optimal choice for different problem settings
3. Analyze the sensitivity of CF-OPT's explanations to the VAE's latent space dimensionality and the cost-aware regularization weight α to identify potential failure modes