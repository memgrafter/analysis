---
ver: rpa2
title: Post-processing fairness with minimal changes
arxiv_id: '2408.15096'
source_url: https://arxiv.org/abs/2408.15096
tags:
- fairness
- rbmd
- changes
- accuracy
- post-processing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RBMD (Ratio-Based Model Debiasing), a post-processing
  fairness method that is model-agnostic, does not require sensitive attributes at
  test time, and explicitly minimizes prediction changes. The method applies a multiplicative
  factor to the logit values of a black-box classifier's probability scores to achieve
  fairness.
---

# Post-processing fairness with minimal changes

## Quick Facts
- arXiv ID: 2408.15096
- Source URL: https://arxiv.org/abs/2408.15096
- Reference count: 17
- Achieves competitive fairness-accuracy trade-offs while minimizing prediction changes through multiplicative adjustment

## Executive Summary
This paper introduces RBMD (Ratio-Based Model Debiasing), a post-processing fairness method that operates without requiring sensitive attributes at test time. The approach applies a learned multiplicative factor to black-box classifier logits to achieve fairness while explicitly minimizing changes to original predictions. RBMD uses adversarial training to reconstruct the sensitive attribute from predictions, enabling fairness enforcement without access to sensitive data during inference. The method demonstrates competitive performance on Law School and COMPAS datasets compared to four baseline debiasing algorithms.

## Method Summary
RBMD is a post-processing fairness method that applies a multiplicative factor to the logit values of a black-box classifier's probability scores. The method consists of a ratio network that predicts this multiplicative factor, which is then applied to the original model's logits. RBMD uses adversarial training where an adversary attempts to reconstruct the sensitive attribute from the debiased predictions, with the strength of this reconstruction serving as a proxy for fairness. The optimization jointly minimizes accuracy loss, enforces fairness through the adversarial constraint, and explicitly penalizes deviations from the original predictions through an L_ratio term.

## Key Results
- RBMD achieves competitive fairness-accuracy trade-offs while performing fewer changes to original predictions compared to alternatives
- The method performs comparably to state-of-the-art fairness-aware methods without requiring sensitive attributes at test time
- RBMD facilitates interpretability by identifying which instances are targeted for prediction changes through surrogate decision trees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RBMD achieves fairness without requiring sensitive attributes at test time by using adversarial reconstruction of the sensitive attribute from model predictions.
- Mechanism: An adversarial model h_wh takes the rescaled logit values r_wg(X)f_logit(X) and attempts to predict the sensitive attribute S. The stronger this adversary performs, the more the predictions have been adjusted to remove sensitive attribute information, thus improving fairness.
- Core assumption: The adversarial reconstruction of the sensitive attribute can serve as a proxy for measuring and enforcing fairness in a fairness-blind setting.
- Evidence anchors:
  - [abstract] "Our approach leverages a multiplicative factor applied to the logit value of probability scores produced by a black-box classifier."
  - [section 2.2] "To ensure fairness (constraint [2]), we propose to leverage the technique proposed by Zhang et al. (2018), which relies on a dynamic reconstruction of the sensitive attribute from the predictions using an adversarial model h_wh : Y → S to estimate and mitigate bias."
  - [corpus] "FairBest: Balancing AUC Performance and Fairness with Proportional Optimal Transport" - mentions fairness-aware methods but not this specific adversarial approach.
- Break condition: If the adversarial model cannot effectively reconstruct the sensitive attribute from predictions, the fairness enforcement mechanism fails.

### Mechanism 2
- Claim: RBMD minimizes prediction changes by explicitly penalizing deviations of the ratio r_wg(X) from 1 through L_ratio.
- Mechanism: The loss function includes a term L_ratio(r_wg(X)) = ||r_wg(X) - 1||_2^2 that encourages the multiplicative factor to stay close to 1, thus preserving the original predictions whenever possible.
- Core assumption: A multiplicative factor near 1 will produce predictions close to the original black-box model's predictions.
- Evidence anchors:
  - [section 2.2] "Although several functions could be considered, we focus in the rest of the paper on ensuring that the score f(X) is not modified unnecessarily, by defining L_ratio(r_wg(X)) = ||r_wg(X) - 1||_2^2."
  - [section 3.2.2] "RBMD consistently achieves lower values of changes P compared to its competitors."
  - [corpus] "Controlled Model Debiasing through Minimal and Interpretable Updates" - focuses on minimal changes but uses different approach.
- Break condition: If λ_ratio is set too low, the ratio network may deviate significantly from 1, causing unnecessary prediction changes.

### Mechanism 3
- Claim: RBMD achieves competitive fairness-accuracy trade-offs by jointly optimizing accuracy, fairness, and minimal changes through a min-max optimization framework.
- Mechanism: The optimization problem arg min_wg max_wh balances three objectives: maintaining accuracy (LY), enforcing fairness through adversarial reconstruction (LS), and minimizing changes (L_ratio) with hyperparameters λ_fair and λ_ratio controlling their relative importance.
- Core assumption: The three objectives can be effectively balanced through appropriate hyperparameter tuning to achieve both fairness and accuracy.
- Evidence anchors:
  - [abstract] "Results show RBMD achieves competitive fairness-accuracy trade-offs while performing fewer changes to original predictions compared to alternatives."
  - [section 2.2] "Thus, Equation 1 for Demographic Parity 2 becomes: min_wg E[LY(g_wg(X), Y)] s.t E[LS(h_wh(r_wg(X)f_logit(X)), S)] ≥ ϵ' and E[L_ratio(r_wg(X))] ≤ η'"
  - [corpus] "FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport" - mentions balancing objectives but different methodology.
- Break condition: If hyperparameters are poorly tuned, the optimization may prioritize one objective at the expense of others.

## Foundational Learning

- Concept: Adversarial training for fairness
  - Why needed here: RBMD uses adversarial reconstruction of the sensitive attribute to enforce fairness without requiring the sensitive attribute at test time.
  - Quick check question: How does an adversarial model help achieve fairness when the sensitive attribute is not available during inference?

- Concept: Post-processing fairness methods
  - Why needed here: RBMD is a post-processing technique that adjusts predictions of an existing black-box classifier rather than retraining the model.
  - Quick check question: What distinguishes post-processing from pre-processing and in-processing fairness methods?

- Concept: Multiplicative adjustment of model scores
  - Why needed here: RBMD applies a learned multiplicative factor to the logit values of the original model's probability scores to achieve fairness with minimal changes.
  - Quick check question: Why use a multiplicative factor rather than additive adjustments when debiasing model predictions?

## Architecture Onboarding

- Component map:
  - Black-box classifier (f) -> Ratio network (r_wg) -> Adversarial model (h_wh) -> Loss functions (LY, LS, L_ratio) -> Hyperparameters (λ_fair, λ_ratio)

- Critical path:
  1. Train black-box classifier on training data
  2. Train RBMD by optimizing r_wg with adversarial training
  3. At inference, apply multiplicative factor to black-box logits
  4. Pass through sigmoid to get final probability scores

- Design tradeoffs:
  - Model agnosticism vs. potential for more tailored solutions
  - No sensitive attribute at test time vs. potentially less precise fairness control
  - Explicit minimal changes objective vs. potential trade-off with fairness gains

- Failure signatures:
  - High standard deviation in P (percentage of changes) across runs
  - Inability to reach desired fairness thresholds even with hyperparameter tuning
  - Ratio network outputting values far from 1 consistently

- First 3 experiments:
  1. Compare RBMD's fairness-accuracy trade-off on Law School dataset with Oracle method
  2. Measure percentage of prediction changes P for different hyperparameter settings
  3. Train decision tree surrogate to interpret which instances RBMD targets for prediction changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RBMD's performance compare when extended to achieve Equalized Odds fairness rather than just Demographic Parity?
- Basis in paper: [explicit] The paper explicitly states this as a future work direction, noting "future works include adapting the proposed approach to achieve Equalized Odds"
- Why unresolved: The current RBMD formulation is specifically designed for Demographic Parity and Equalized Odds would require modifications to the adversarial reconstruction component
- What evidence would resolve it: Empirical results showing RBMD's accuracy-fairness trade-off and prediction change minimization when adapted for Equalized Odds on standard datasets

### Open Question 2
- Question: What is the theoretical upper bound on prediction changes that RBMD can achieve while maintaining reasonable fairness and accuracy levels?
- Basis in paper: [inferred] The paper emphasizes RBMD's minimal prediction changes but doesn't establish theoretical limits or analyze worst-case scenarios
- Why unresolved: The method's constraint on prediction changes is implemented through a regularization term rather than theoretically derived bounds
- What evidence would resolve it: Mathematical analysis proving the relationship between the ratio network's architecture, the regularization parameter, and maximum achievable prediction changes

### Open Question 3
- Question: How does RBMD's performance scale with dataset size and feature dimensionality compared to other post-processing methods?
- Basis in paper: [inferred] The paper evaluates RBMD on two specific datasets but doesn't analyze computational complexity or scalability properties
- Why unresolved: The neural network-based ratio estimation could have different scaling characteristics than simpler post-processing methods
- What evidence would resolve it: Runtime and memory usage comparisons of RBMD versus competitors across datasets of varying sizes and dimensions, with complexity analysis

### Open Question 4
- Question: How sensitive is RBMD to the choice of hyperparameters (λ_fair and λ_ratio) and is there a systematic way to select optimal values?
- Basis in paper: [explicit] The paper mentions conducting grid searches for hyperparameters but doesn't provide guidance on systematic selection methods
- Why unresolved: The paper shows results for specific hyperparameter choices but doesn't explore the full parameter space or provide selection criteria
- What evidence would resolve it: Analysis of RBMD's performance stability across different hyperparameter ranges and development of a principled hyperparameter selection framework

## Limitations
- Evaluation on only two datasets with binary sensitive attributes limits external validity
- Performance on multi-class sensitive attributes or continuous outcomes is unknown
- Computational overhead of adversarial training may be prohibitive for large-scale deployment

## Confidence
- Core mechanism claims: High (well-supported by mathematical formulation and empirical results)
- Comparative claims: Medium (limited to two datasets, race as sensitive attribute only)
- State-of-the-art performance claims: Medium (reasonable but requires broader benchmarking)

## Next Checks
1. Evaluate RBMD on additional fairness datasets with different sensitive attributes (gender, age, disability status) and multi-class sensitive attributes to assess broader applicability.

2. Conduct systematic ablation studies varying λ_fair and λ_ratio across wider ranges to identify optimal settings and understand trade-offs between fairness, accuracy, and prediction changes.

3. Measure and compare the training time and inference latency of RBMD against baselines, particularly for large-scale models, to assess practical deployment feasibility.