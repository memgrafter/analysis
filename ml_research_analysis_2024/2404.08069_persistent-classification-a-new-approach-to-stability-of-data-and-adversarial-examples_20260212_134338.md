---
ver: rpa2
title: 'Persistent Classification: A New Approach to Stability of Data and Adversarial
  Examples'
arxiv_id: '2404.08069'
source_url: https://arxiv.org/abs/2404.08069
tags:
- adversarial
- examples
- decision
- persistence
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a new framework for analyzing adversarial\
  \ examples in classification problems using a stability and persistence metric based\
  \ on Gaussian perturbations around data points. The key idea is to define a (\u03B3\
  ,\u03C3)-stable point as one where at least a fraction \u03B3 of points sampled\
  \ from a Gaussian neighborhood with standard deviation \u03C3 maintain the same\
  \ classification."
---

# Persistent Classification: A New Approach to Stability of Data and Adversarial Examples

## Quick Facts
- arXiv ID: 2404.08069
- Source URL: https://arxiv.org/abs/2404.08069
- Authors: Brian Bell; Michael Geyer; David Glickenstein; Keaton Hamm; Carlos Scheidegger; Amanda Fernandez; Juston Moore
- Reference count: 40
- Key outcome: New framework analyzing adversarial examples using stability and persistence metrics based on Gaussian perturbations

## Executive Summary
This paper introduces a novel framework for analyzing adversarial examples in classification problems using stability and persistence metrics derived from Gaussian perturbations around data points. The key insight is that adversarial examples exhibit significantly lower persistence (stability under perturbations) compared to natural examples, indicating they are more vulnerable to small changes. The authors demonstrate that adversarial examples tend to cross decision boundaries at more oblique angles and develop a manifold alignment gradient metric that improves robustness when incorporated into training.

## Method Summary
The method involves sampling Gaussian neighborhoods around data points and measuring classification stability to define (γ,σ)-stable points and γ-persistence. The framework uses Monte Carlo sampling from Gaussian distributions, a bisection algorithm for computing persistence, PCA for manifold alignment, and adversarial training with manifold alignment gradient metrics. Experiments were conducted on MNIST and ImageNet datasets using various neural network architectures and adversarial attack methods including IGSM, L-BFGS, FGSM, MIFGSM, BIM, PGD, R+FGSM, and CW.

## Key Results
- Adversarial examples have significantly lower persistence than natural examples under Gaussian perturbations
- Adversarial examples tend to cross decision boundaries at more oblique angles compared to natural examples
- Training with manifold alignment gradient metric improves robustness to adversarial attacks

## Why This Works (Mechanism)

### Mechanism 1
Gaussian sampling around a point reveals how entrenched it is in its class. For adversarial examples, the classification probability drops quickly as the perturbation variance increases, indicating proximity to the decision boundary or high curvature regions. Natural examples maintain classification probability over larger variance ranges. Core assumption: Gaussian distribution adequately captures local neighborhood geometry. Evidence anchors: [abstract] "adversarial examples have significantly lower persistence than natural examples" and [section] "Small persistence indicates that the classifier is unstable in a small neighborhood of x". Break condition: If data manifold is highly non-smooth or discontinuous, Gaussian sampling may not capture true geometry.

### Mechanism 2
By interpolating between natural and adversarial points and measuring angles with respect to the decision boundary, we find adversarial interpolants are closer to orthogonal. This suggests adversarial examples lie in regions where decision boundaries are oriented such that small perturbations in certain directions easily flip the classification. Core assumption: Decision boundaries can be approximated by local linear subspaces. Evidence anchors: [section] "both natural and adversarial linear interpolants tend to cross at acute angles with respect to the decision boundary, with adversarial attacks tending to be closer to orthogonal". Break condition: If decision boundaries are highly curved or have complex topology, local angle measurements may not generalize.

### Mechanism 3
By projecting gradients onto a lower-dimensional manifold (e.g., PCA subspace) and penalizing off-manifold components, we encourage the model to learn features aligned with the data manifold, making it more robust to perturbations that exploit decision boundary geometry. Core assumption: Data lies on or near a low-dimensional manifold and aligning gradients improves robustness. Evidence anchors: [abstract] "we develop a manifold alignment gradient metric and demonstrate the increase in robustness that can be achieved when training with the addition of this metric". Break condition: If true data manifold is higher-dimensional than projection space or if projection destroys important discriminative information.

## Foundational Learning

- **Concept**: Gaussian distribution and concentration of measure
  - Why needed here: Understanding how probability mass concentrates around the mean in high dimensions is crucial for interpreting the stability and persistence metrics.
  - Quick check question: What is the typical radius within which 95% of the mass of a high-dimensional Gaussian lies relative to its standard deviation?

- **Concept**: Decision boundary geometry and angle measurements
  - Why needed here: Analyzing how interpolation vectors cross decision boundaries at different angles helps explain why adversarial examples are vulnerable.
  - Quick check question: How would you compute the angle between a vector and a hyperplane defined by a normal vector?

- **Concept**: Manifold learning and dimensionality reduction
  - Why needed here: The manifold alignment gradient metric relies on projecting data and gradients onto a lower-dimensional subspace.
  - Quick check question: What is the difference between PCA and other manifold learning techniques like t-SNE or UMAP?

## Architecture Onboarding

- **Component map**: Classifier -> Gaussian sampling module -> Persistence computation module -> Angle measurement module -> Manifold alignment training module
- **Critical path**: For analyzing adversarial examples: (1) Load pre-trained classifier, (2) Generate adversarial examples, (3) Sample Gaussian neighborhoods, (4) Compute stability/persistence, (5) Measure angles with decision boundaries
- **Design tradeoffs**: Gaussian sampling vs uniform sampling (easier to implement but may miss certain geometric features), high-dimensional vs low-dimensional manifold projections (balance between computational efficiency and representational power)
- **Failure signatures**: High variance in persistence estimates (increase number of samples), persistent misclassification of adversarial examples as natural (check attack generation), poor manifold alignment (try different dimensionality or manifold learning technique)
- **First 3 experiments**:
  1. Compute persistence for a small set of MNIST images (both natural and adversarial) using a simple CNN.
  2. Visualize Gaussian samples around an adversarial example and observe classification probability decay.
  3. Train a model with manifold alignment regularization and compare adversarial robustness to a baseline model.

## Open Questions the Paper Calls Out

### Open Question 1
How does the persistence metric vary with different choices of the probability measure (e.g., uniform on balls vs. Gaussian) for sampling the neighborhood of a point? The paper discusses the choice of Gaussian sampling over uniform sampling on balls and mentions potential qualitative differences, but does not provide empirical comparisons. What evidence would resolve it: Experiments comparing persistence values using Gaussian sampling versus uniform sampling on balls for various datasets and classifiers would provide concrete evidence of any differences.

### Open Question 2
What is the relationship between the obliqueness of the decision boundary with respect to test points and the susceptibility to adversarial attacks? The paper observes that both natural and adversarial linear interpolants tend to cross the decision boundary at acute angles, with adversarial attacks tending to be closer to orthogonal. It suggests that obliqueness may be related to adversarial vulnerability. What evidence would resolve it: Further analysis quantifying the relationship between obliqueness and adversarial vulnerability, such as measuring the correlation between obliqueness and the required perturbation magnitude for successful attacks, would help establish causality.

### Open Question 3
How does the proposed manifold alignment gradient metric compare to other robustness techniques, such as adversarial training or data augmentation? The paper introduces the manifold alignment gradient metric and shows that it can improve robustness to adversarial attacks, but does not compare its performance to other state-of-the-art robustness techniques. What evidence would resolve it: Comparative experiments evaluating the manifold alignment gradient metric against other robustness techniques, such as adversarial training or data augmentation, on various datasets and attack scenarios would provide a clearer understanding of its relative strengths and weaknesses.

## Limitations
- The persistence metric's reliance on Gaussian sampling may not capture non-smooth or discontinuous data manifolds, potentially limiting its applicability to real-world data distributions
- The decision boundary angle analysis assumes local linearity, which may not hold for complex, high-dimensional classification boundaries
- The manifold alignment gradient metric's effectiveness depends on the assumption that data lies on a low-dimensional manifold, which may not always be true

## Confidence
- **High confidence**: The core observation that adversarial examples exhibit lower persistence under Gaussian perturbations than natural examples is well-supported by experiments on MNIST and ImageNet datasets.
- **Medium confidence**: The relationship between decision boundary crossing angles and adversarial vulnerability is demonstrated but relies on simplifying assumptions about boundary geometry.
- **Medium confidence**: The effectiveness of manifold alignment gradient training for improving robustness is shown but requires further validation across diverse architectures and datasets.

## Next Checks
1. Test persistence metric performance on non-image datasets with known non-smooth manifolds (e.g., text or graph data) to evaluate its generalizability beyond the MNIST/ImageNet domains.
2. Implement and validate the manifold alignment gradient training approach across multiple network architectures (beyond alexnet and vgg16) and datasets to assess robustness improvements.
3. Conduct ablation studies on the Gaussian sampling parameters (number of samples, σ range) to determine sensitivity of persistence estimates to these choices.