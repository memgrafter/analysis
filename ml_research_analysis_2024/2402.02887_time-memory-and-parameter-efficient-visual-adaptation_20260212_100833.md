---
ver: rpa2
title: Time-, Memory- and Parameter-Efficient Visual Adaptation
arxiv_id: '2402.02887'
source_url: https://arxiv.org/abs/2402.02887
tags:
- backbone
- parameters
- network
- which
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a parameter-efficient visual adaptation method
  called Low-Rank Side Adaptation (LoSA) that achieves superior accuracy-efficiency
  trade-offs compared to existing approaches. LoSA operates by learning a lightweight
  parallel network that refines features from a frozen pretrained backbone, without
  requiring backpropagation through the backbone.
---

# Time-, Memory- and Parameter-Efficient Visual Adaptation

## Quick Facts
- arXiv ID: 2402.02887
- Source URL: https://arxiv.org/abs/2402.02887
- Authors: Otniel-Bogdan Mercea; Alexey Gritsenko; Cordelia Schmid; Anurag Arnab
- Reference count: 40
- Primary result: LoSA achieves state-of-the-art accuracy-parameter trade-offs on VTAB-1K benchmark while significantly reducing training time and memory usage.

## Executive Summary
This paper introduces Low-Rank Side Adaptation (LoSA), a parameter-efficient method for adapting large pretrained vision models to downstream tasks. The key innovation is learning a lightweight parallel network that refines frozen backbone features, eliminating the need to backpropagate through the large backbone model. This design enables significant reductions in training time and memory usage while maintaining high accuracy. Experiments demonstrate that LoSA achieves superior accuracy-efficiency trade-offs compared to existing approaches across multiple vision benchmarks.

## Method Summary
LoSA operates by freezing a pretrained vision transformer backbone and learning a parallel side network composed of alternating low-rank mixer modules. The method selects evenly spaced backbone activations and passes each through corresponding low-rank mixer blocks with residual connections. The side network refines these frozen features for the target task without requiring gradient computation through the backbone. For video tasks, spatial and temporal mixing are performed separately. The approach uses low-rank matrix factorization to reduce parameters while maintaining representational power, and trains with SGD using cosine learning rate schedules.

## Key Results
- Achieves state-of-the-art accuracy-parameter trade-offs on VTAB-1K benchmark
- Scales effectively to large-scale models including ViT-e (4B parameters) for video classification
- Outperforms fully-finetuning smaller backbones and previous adaptor-based methods in training efficiency and accuracy

## Why This Works (Mechanism)

### Mechanism 1
Freezing the backbone and learning a parallel side network avoids gradient computation through the large pretrained model, reducing training time and memory. By not backpropagating through the backbone, the method eliminates the need to cache intermediate activations and compute gradients with respect to them, which are the main sources of memory and computational overhead during training. The core assumption is that the backbone features contain sufficient information to be refined by a lightweight parallel network for downstream tasks.

### Mechanism 2
Alternating low-rank mixer modules (channel mixing and token mixing) allow efficient modeling of both feature interactions and spatial/temporal structure. Low-rank projections reduce the number of parameters while maintaining representational power; alternating along channel and token dimensions captures both intra-feature and inter-token relationships without needing full-rank dense layers. The core assumption is that low-rank factorization preserves enough expressiveness for adaptation while being computationally cheap.

### Mechanism 3
Using the backbone's final encoder output as input to the side network provides richer context than using the backbone input or intermediate activations. The final encoder block output is a sum of MLP and MHSA block outputs, capturing both token-level and feature-level information; feeding this to the side network allows it to refine a more complete representation. The core assumption is that the backbone's final layer output is more informative than earlier layers for adaptation.

## Foundational Learning

- Concept: Gradient flow and backward pass in neural networks
  - Why needed here: Understanding why freezing the backbone saves memory/time requires knowing that gradients w.r.t. activations are the main cost drivers.
  - Quick check question: What is the main difference in backward pass computation between full finetuning and LoSA?

- Concept: Low-rank matrix factorization
  - Why needed here: The adaptor function uses low-rank MLP projections; knowing how rank affects capacity vs efficiency is key to tuning.
  - Quick check question: How does reducing rank from r=16 to r=4 affect parameter count and expected accuracy?

- Concept: Residual connections and feature refinement
  - Why needed here: The side network uses residual connections to preserve backbone features while refining them; understanding this ensures correct implementation.
  - Quick check question: Why does adding y_{i-1} back in Eq. 3 help preserve backbone information?

## Architecture Onboarding

- Component map: Frozen backbone -> Backbone activations -> Parallel side network (low-rank mixers) -> Refined features -> Classification head
- Critical path:
  1. Forward pass through frozen backbone to get activations
  2. Select evenly spaced backbone activations
  3. Pass each activation through corresponding low-rank mixer block
  4. Residual add with previous side network output
  5. Final refined output to classifier
- Design tradeoffs:
  - Backbone frozen vs finetuned: Saves memory/time but may limit adaptation if features are poor
  - Rank r in low-rank mixer: Higher r → more capacity, more parameters, slower; lower r → faster, fewer parameters, risk underfitting
  - Number of side network layers: More layers → deeper refinement, better accuracy, higher cost; fewer layers → faster, may underfit
- Failure signatures:
  - Training accuracy plateaus early: Likely rank too low or too few side layers
  - Memory usage unexpectedly high: Possible bug in gradient checkpointing or incorrect backbone freezing
  - Accuracy worse than linear probing: Backbone features may be too domain-shifted; consider unfreezing last few layers
- First 3 experiments:
  1. Verify gradient flow is blocked: check that backbone weights are not updated during training
  2. Ablation of rank: train with r=4, r=8, r=16 on a small dataset and compare accuracy/efficiency
  3. Test side network input: compare using backbone input vs backbone output on a validation set

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important ones emerge from the limitations section and experimental scope.

## Limitations
- Limited ablation studies on critical design choices like rank values and side network depth
- No systematic exploration of memory profiling comparing activation caching costs vs side network parameters
- Focus exclusively on vision transformers without exploring other architectures

## Confidence

**Major Uncertainties:**
The paper provides strong empirical evidence for LoSA's efficiency gains but lacks ablation studies on critical design choices. The choice of rank values (r=4, 8, 16) and the number of side network layers are not systematically explored, making it unclear how sensitive performance is to these hyperparameters. Additionally, while memory savings from freezing the backbone are claimed, detailed memory profiling comparing activation caching costs vs. side network parameters is not provided.

**Confidence Labels:**
- **High confidence** in LoSA's parameter efficiency and training time claims, as these are directly measurable and well-supported by the experimental setup
- **Medium confidence** in the accuracy improvements over baselines, as results are strong but limited to specific benchmarks and model scales
- **Low confidence** in the generalizability of LoSA to other architectures or tasks beyond vision and video classification, given the lack of experiments in NLP or multimodal domains

## Next Checks

1. Conduct systematic ablation studies on rank values (r=2, 4, 8, 16, 32) and side network depth to identify sensitivity and optimal configurations
2. Perform detailed memory profiling to quantify the exact savings from freezing the backbone vs. gradient checkpointing alternatives
3. Test LoSA on a diverse set of tasks (e.g., NLP fine-tuning, multimodal adaptation) to assess cross-domain generalization and identify potential failure modes