---
ver: rpa2
title: Rejection via Learning Density Ratios
arxiv_id: '2405.18686'
source_url: https://arxiv.org/abs/2405.18686
tags:
- rejection
- theorem
- learning
- density
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for learning rejection
  functions by comparing data distributions to idealized distributions that maximize
  a model's performance. The authors propose using density ratios between idealized
  and data distributions to make rejection decisions, connecting this approach to
  distributionally robust optimization and generalized variational inference.
---

# Rejection via Learning Density Ratios

## Quick Facts
- arXiv ID: 2405.18686
- Source URL: https://arxiv.org/abs/2405.18686
- Authors: Alexander Soen; Hisham Husain; Philip Schulz; Vu Nguyen
- Reference count: 40
- One-line primary result: Density ratio rejectors using KL-divergence and α=3-divergence are competitive with or superior to baseline rejection methods, especially at high acceptance coverage (>95%).

## Executive Summary
This paper introduces a novel framework for learning rejection functions by comparing data distributions to idealized distributions that maximize a model's performance. The authors propose using density ratios between idealized and data distributions to make rejection decisions, connecting this approach to distributionally robust optimization and generalized variational inference. They derive optimal density ratio rejectors for KL-divergence and α-divergences, including closed-form solutions for α > 1 cases. The framework is evaluated on classification datasets (HAR, Gas Drift, MNIST) with both clean and noisy variants.

## Method Summary
The framework learns rejection functions by finding an idealized distribution Q that minimizes expected loss under φ-divergence regularization, then uses the density ratio ρ = dQ/dP to threshold rejection decisions. For classification, the method provides closed-form solutions for KL-divergence (α=1) and χ²-divergence (α=3) cases. The approach generalizes existing rejection methods and connects to Chow's rule when using KL-divergence with a Bayes optimal classifier. The method requires a calibrated base classifier and estimates the expected loss L'(x) using the classifier's output probabilities.

## Key Results
- Density ratio rejectors using KL-divergence and α=3-divergence outperform or match baseline methods (DEFER, GCE, CSS, PredRej) across all datasets
- Performance advantage is most pronounced at high acceptance coverage (>95%) where rejection is most needed
- Method's effectiveness depends on base classifier calibration quality, with performance degrading as noise increases
- Closed-form solutions for α-divergences provide computational advantages over iterative approaches

## Why This Works (Mechanism)

### Mechanism 1
The proposed density ratio rejector is effective because it learns an idealized distribution that minimizes the expected loss, then uses the ratio between this idealized and true data distribution to make rejection decisions. The framework frames rejection as a distributional problem: first learn an idealized distribution Q that minimizes a model's loss function under φ-divergence regularization, then use the density ratio to threshold rejection. This is equivalent to directly optimizing a density ratio objective with the φ-divergence regularization term. The core assumption is that the idealized distribution Q, when used as input data, would minimize the expected loss of the pretrained model.

### Mechanism 2
The α-divergence family provides a flexible way to construct idealized distributions, generalizing the KL-divergence and allowing for different rejection behaviors. The framework uses φ-divergences, specifically the α-divergence family, to regularize the idealized distribution learning. Different values of α (e.g., α = 1 for KL, α = 3 for χ²) lead to different forms of the density ratio rejector, allowing for trade-offs between different rejection characteristics. The core assumption is that the α-divergence provides a meaningful measure of distributional difference for the rejection task.

### Mechanism 3
The density ratio rejector generalizes existing rejection methods by connecting to Chow's rule and other optimal rejection policies. The framework shows that when using the KL-divergence and a Bayes optimal classifier, the density ratio rejector recovers Chow's rule for optimal rejection. This connection demonstrates that the proposed method is a principled extension of existing rejection approaches. The core assumption is that the underlying classifier is well-calibrated, and the loss function is proper.

## Foundational Learning

- **Concept**: Distributionally Robust Optimization (DRO)
  - Why needed here: The framework draws a connection between idealized distributions for rejection and adversarial distributions in DRO. Understanding DRO helps explain the theoretical underpinnings of the method.
  - Quick check question: How does the inner maximization problem in DRO relate to the idealized distribution learning in the rejection framework?

- **Concept**: Generalized Variational Inference (GVI)
  - Why needed here: The framework frames the idealized distribution learning as a GVI problem, generalizing Bayesian inference. Understanding GVI provides context for the regularization approach used.
  - Quick check question: How does the GVI formulation with φ-divergences differ from standard variational inference?

- **Concept**: Proper Loss Functions
  - Why needed here: The framework relies on proper loss functions (like log-loss) for classification to ensure well-defined rejection policies. Understanding properness is crucial for the method's theoretical guarantees.
  - Quick check question: What is the difference between a proper and an improper loss function, and why does it matter for rejection?

## Architecture Onboarding

- **Component map**: Pretrained classifier (h) -> Loss function (ℓ) -> Idealized distribution learner -> Density ratio estimator -> Rejection decision maker

- **Critical path**:
  1. Pretrain a classifier h on the data
  2. Calibrate the classifier to ensure well-calibrated probability estimates
  3. Choose an α-divergence (e.g., KL for α=1, χ² for α=3)
  4. Learn the idealized distribution Q by optimizing the density ratio objective with the chosen α-divergence
  5. Compute the density ratio ρ = dQ/dP
  6. Threshold ρ to make rejection decisions

- **Design tradeoffs**:
  - Choice of α-divergence: Different α values lead to different rejection behaviors. KL (α=1) is well-understood and connects to optimal rejection policies, while χ² (α=3) provides a closed-form solution for bounded losses
  - Regularization parameter λ: Controls the trade-off between fitting the idealized distribution and staying close to the true data distribution
  - Rejection threshold τ: Determines the acceptance coverage vs. accuracy trade-off

- **Failure signatures**:
  - Poor rejection performance: May indicate a poorly calibrated classifier, an inappropriate α-divergence, or an incorrect choice of λ or τ
  - High variance in rejection decisions: May indicate sensitivity to the base classifier's randomness or noise in the data
  - Computational issues: May indicate difficulties in learning the idealized distribution or computing the density ratio, especially for high-dimensional data

- **First 3 experiments**:
  1. Evaluate the density ratio rejector on a clean dataset with a well-calibrated classifier. Compare its performance to baseline rejection methods in terms of accuracy vs. acceptance coverage trade-off
  2. Introduce label noise to the dataset and evaluate the rejector's robustness. Compare its performance to baseline methods as the noise level increases
  3. Vary the α-divergence parameter (e.g., α=1 for KL, α=3 for χ²) and evaluate the impact on rejection performance. Analyze the trade-offs between different α values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of density ratio rejectors vary with different base model architectures beyond the two-layer neural networks tested?
- Basis in paper: [inferred] The authors note that their findings extend to different noise rates and architectures, but only test with two-layer neural networks for tabular data and convolutional networks for MNIST
- Why unresolved: The paper does not explore a wide range of base model architectures or depths, leaving open the question of how density ratio rejectors would perform with deeper or more complex models
- What evidence would resolve it: Experiments comparing density ratio rejectors with various base model architectures (e.g., deeper networks, transformers, ensemble methods) across multiple datasets would provide evidence of the approach's robustness to model choice

### Open Question 2
- Question: Can density ratio rejectors be effectively trained using gradient-based methods rather than closed-form solutions, and what are the computational trade-offs?
- Basis in paper: [explicit] The authors discuss the potential for iterative learning via gradients but found it infeasible in practice, noting difficulties with estimating normalization terms and model capacity
- Why unresolved: The paper does not provide a working implementation of gradient-based density ratio learning, leaving the question of whether this approach could be made practical and how it would compare to closed-form solutions
- What evidence would resolve it: A successful implementation of gradient-based density ratio learning that achieves comparable or better performance than closed-form methods, along with a detailed analysis of computational costs and convergence properties, would resolve this question

### Open Question 3
- Question: How do density ratio rejectors perform in regression tasks compared to classification, and what modifications are needed for the regression setting?
- Basis in paper: [explicit] The authors discuss extending their framework to regression and note the challenge of estimating conditional variance, but do not provide experimental results for regression tasks
- Why unresolved: The paper focuses primarily on classification and only briefly mentions regression, without empirical evaluation or detailed discussion of regression-specific modifications to the approach
- What evidence would resolve it: Experiments applying density ratio rejectors to regression datasets with various loss functions and variance estimation techniques, along with comparisons to existing regression rejection methods, would provide insight into the approach's effectiveness in this setting

## Limitations

- Performance is tightly coupled to base classifier calibration quality, with degradation as noise increases
- Framework assumes proper loss functions and well-defined φ-divergence regularization may not hold in all scenarios
- Computational complexity of learning idealized distributions may be prohibitive for very high-dimensional data

## Confidence

**High Confidence**: The theoretical framework connecting density ratio estimation to rejection decisions is sound, with clear mathematical derivations for KL-divergence and α-divergence cases. The connection to distributionally robust optimization provides strong theoretical grounding.

**Medium Confidence**: The empirical results showing competitive performance against baseline methods are promising, but the evaluation is limited to three datasets with specific noise types. The variance observed at higher noise levels suggests the method may be sensitive to data quality and classifier calibration.

**Low Confidence**: The generalizability of the approach to other divergence measures beyond the examined α-divergence family, and its performance on diverse real-world datasets with different types of noise or distribution shift, remains uncertain without further validation.

## Next Checks

1. **Calibration Sensitivity Analysis**: Systematically vary the calibration quality of base classifiers (from perfectly calibrated to poorly calibrated) and measure the impact on density ratio rejector performance across all datasets.

2. **Cross-Dataset Robustness**: Test the framework on additional datasets with different characteristics (e.g., tabular data, natural language, multi-label classification) to assess generalizability beyond the current evaluation.

3. **Real-World Distribution Shift**: Evaluate the method on datasets with natural distribution shift (rather than synthetic noise) to assess practical utility in deployment scenarios where data distribution changes over time.