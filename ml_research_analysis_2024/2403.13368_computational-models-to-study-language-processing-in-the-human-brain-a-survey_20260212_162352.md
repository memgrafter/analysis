---
ver: rpa2
title: 'Computational Models to Study Language Processing in the Human Brain: A Survey'
arxiv_id: '2403.13368'
source_url: https://arxiv.org/abs/2403.13368
tags:
- language
- brain
- processing
- word
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews the use of computational language
  models (statistical, shallow embedding, and large language models) to study brain
  language processing. By applying a unified evaluation framework across multiple
  datasets, it demonstrates that no single model type consistently outperforms others
  across all tasks.
---

# Computational Models to Study Language Processing in the Human Brain: A Survey

## Quick Facts
- arXiv ID: 2403.13368
- Source URL: https://arxiv.org/abs/2403.13368
- Reference count: 40
- One-line primary result: No single computational model type consistently outperforms others across all language processing tasks in brain studies

## Executive Summary
This survey systematically reviews computational language models (statistical, shallow embedding, and large language models) for studying brain language processing. Through unified evaluation across multiple datasets, the research reveals that large language models excel at semantic representation tasks, particularly for eye-tracking and fMRI-word data, while statistical models perform comparably in fMRI-discourse tasks and shallow embeddings show strong performance in semantic decoding. The findings highlight the need for standardized evaluation practices and larger, diverse benchmark datasets to draw robust conclusions about computational models' ability to illuminate cognitive load, parsing strategies, and neural mechanisms of language processing.

## Method Summary
The study trains statistical, shallow embedding, and large language models from scratch on identical English and Chinese datasets (Wikipedia and Xinhua news). Word embeddings are extracted from optimal layers and used in encoding models to predict brain activity (fMRI, EEG, eye-tracking). Ten-fold cross-validation and paired t-tests (p=0.001 threshold) assess prediction accuracy. The unified framework enables direct comparison across model types while controlling for dataset differences, revealing task-specific strengths and weaknesses of each computational approach.

## Key Results
- Large language models excel in semantic representation tasks, particularly for eye-tracking and fMRI-word data
- Statistical models perform comparably to LLMs in fMRI-discourse tasks
- Shallow embeddings show strong performance in semantic decoding tasks
- No single model type consistently outperforms others across all language processing tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models provide richer semantic representations that better align with brain activity during language tasks than statistical or shallow embedding models.
- Mechanism: LLMs use deep transformer architectures with attention mechanisms that capture hierarchical and contextual dependencies, allowing them to encode complex linguistic features (semantic, syntactic, and morphological) more effectively than models relying on fixed word embeddings or explicit grammars.
- Core assumption: Brain language processing involves dynamic, context-dependent representation of meaning that correlates with the layered, attention-based encoding in LLMs.
- Evidence anchors:
  - [abstract] "Large language models excel in semantic representation tasks, particularly for eye-tracking and fMRI-word data"
  - [section 3.2] "Recent research has leveraged text embeddings to explore language processing in the human brain, correlating brain activation from linguistic stimuli with corresponding stimulus embeddings"
  - [corpus] Weak - only 1/8 related papers focus on language models and brain alignment
- Break condition: If brain activity during language processing is better explained by simpler, more interpretable models (e.g., explicit grammars) or if the transformer attention patterns do not map meaningfully to neural dynamics.

### Mechanism 2
- Claim: Statistical language models capture syntactic structure and parsing strategies that correlate with neural signals related to syntactic processing.
- Mechanism: SLMs use formal grammars (CFG, CCG) and parsing strategies (top-down, bottom-up, left-corner) to explicitly model hierarchical sentence structure, allowing direct measurement of syntactic complexity (rule counts, node counts) that aligns with brain regions involved in syntax.
- Core assumption: Human brain processes syntactic structure incrementally during comprehension, and this can be modeled by formal grammars that simulate parsing operations.
- Evidence anchors:
  - [section 3.1.1] "Two frequently used metrics are rule counts and node counts. Prior research shows a correlation between parser-applied rules, node count... and related neural activity"
  - [section 2.2] "The structural language model utilizes formal grammars... to calculate word probabilities based on preceding words and syntactic structure of the sentence"
  - [corpus] Weak - no direct corpus evidence for syntactic parsing brain alignment
- Break condition: If syntactic processing in the brain does not rely on explicit hierarchical structure building, or if neural signals do not correlate with parsing metrics.

### Mechanism 3
- Claim: Shallow embedding models provide efficient semantic quantification that maps onto brain semantic processing, particularly for eye-tracking data.
- Mechanism: SEMs learn distributed word representations based on co-occurrence statistics (e.g., Word2Vec, GloVe), capturing semantic relationships in a continuous vector space that correlates with neural representations of meaning.
- Core assumption: Brain semantic processing can be approximated by static semantic vectors learned from large text corpora, where similar words have similar representations.
- Evidence anchors:
  - [section 3.2] "Shallow embedding models, with diverse training data, design, and goals, share a common aim: capturing semantic nuances for improved language processing tasks"
  - [section 4] "Concerning surprisal and entropy reduction, all models demonstrate similar levels of effectiveness"
  - [corpus] Weak - no corpus papers specifically address shallow embeddings and brain alignment
- Break condition: If semantic processing in the brain requires dynamic, context-dependent representations rather than static embeddings, or if correlations with neural data are weak or inconsistent.

## Foundational Learning

- Concept: Brain imaging modalities (fMRI, EEG, eye-tracking) and what they measure
  - Why needed here: Understanding which neural signals correspond to which cognitive processes is essential for interpreting model-brain alignment studies
  - Quick check question: What is the main difference between fMRI and EEG in terms of temporal vs spatial resolution?

- Concept: Language model architectures (n-grams, RNNs, transformers) and their strengths/weaknesses
  - Why needed here: Different models capture different aspects of language (syntax vs semantics vs context), affecting their ability to model brain processes
  - Quick check question: Which model type is most likely to capture long-range dependencies in language?

- Concept: Cognitive metrics (surprisal, entropy reduction, parsing metrics) and their theoretical basis
  - Why needed here: These metrics bridge computational models and brain activity by quantifying cognitive load and processing difficulty
  - Quick check question: How does surprisal mathematically relate to word probability?

## Architecture Onboarding

- Component map: Wikipedia/Xinhua news -> Model training -> Cognitive metric computation -> Encoding/decoding model -> Brain data comparison -> Statistical evaluation
- Critical path: Training computational models on the same dataset -> Computing cognitive metrics -> Predicting brain activity -> Validating with cross-validation and statistical tests
- Design tradeoffs: Model complexity vs interpretability, static vs dynamic representations, explicit grammar vs learned representations
- Failure signatures: Inconsistent results across datasets, overfitting to specific brain regions, weak statistical significance
- First 3 experiments:
  1. Train n-gram, RNN, and transformer models on the same Wikipedia corpus and compute surprisal metrics for a standard sentence set
  2. Use these surprisal values to predict fMRI responses from an existing language comprehension dataset using ridge regression
  3. Compare prediction accuracy across model types using paired t-tests to identify which captures brain activity best

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do structural language models adequately capture the full range of syntactic processing complexity in the human brain?

## Limitations
- Weak corpus evidence (only 1/8 related papers) for some claims about language model-brain alignment
- Focus on English and Chinese datasets may limit generalizability across languages and cultures
- Individual differences in brain structure and language processing not accounted for
- Standard benchmarks may not capture all aspects of human language processing

## Confidence

**High Confidence**: Large language models excel in semantic representation tasks for eye-tracking and fMRI-word data (supported by strong abstract and corpus evidence).

**Medium Confidence**: Statistical models perform comparably in fMRI-discourse tasks (supported by corpus evidence but lacking direct empirical validation).

**Low Confidence**: Shallow embeddings show strong performance in semantic decoding (based on limited corpus evidence requiring additional validation).

## Next Checks

1. **Dataset Diversity Validation**: Test computational models across additional languages and cultural contexts beyond English and Chinese to assess generalizability.

2. **Individual Differences Analysis**: Conduct studies with diverse participant populations to determine how individual brain structure and language processing styles affect model-brain alignment.

3. **Longitudinal Study**: Perform a longitudinal study to track how model-brain alignment changes over time as both computational models and understanding of brain language processing evolve.