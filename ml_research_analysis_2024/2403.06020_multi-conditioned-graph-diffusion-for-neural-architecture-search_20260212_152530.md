---
ver: rpa2
title: Multi-conditioned Graph Diffusion for Neural Architecture Search
arxiv_id: '2403.06020'
source_url: https://arxiv.org/abs/2403.06020
tags:
- search
- accuracy
- architectures
- learning
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiNAS, a novel graph diffusion-based approach
  for neural architecture search (NAS) that generates high-performing neural network
  architectures. The core method leverages discrete conditional graph diffusion processes
  and multi-conditioned classifier-free guidance to impose constraints like high accuracy
  and low hardware latency.
---

# Multi-conditioned Graph Diffusion for Neural Architecture Search

## Quick Facts
- arXiv ID: 2403.06020
- Source URL: https://arxiv.org/abs/2403.06020
- Authors: Rohan Asthana; Joschua Conrad; Youssef Dawoud; Maurits Ortmanns; Vasileios Belagiannis
- Reference count: 37
- Primary result: Introduces DiNAS, a graph diffusion-based approach that generates high-performing neural architectures in under 0.2 seconds

## Executive Summary
This paper presents DiNAS, a novel neural architecture search method that leverages discrete conditional graph diffusion processes combined with multi-conditioned classifier-free guidance. The approach generates neural network architectures optimized for both accuracy and hardware latency constraints. DiNAS operates entirely in discrete space to preserve graph structure, using a Graph Transformer network for denoising. The method achieves state-of-the-art performance across six standard benchmarks including ImageNet, generating novel architectures rapidly without requiring separate predictor models.

## Method Summary
DiNAS employs a discrete graph diffusion model trained on neural architecture benchmarks. The core approach uses a Graph Transformer network to denoise corrupted graph representations through a reverse diffusion process. Multi-conditioned classifier-free guidance allows simultaneous optimization of multiple objectives (accuracy and latency) by conditioning the generation process. The method discretizes continuous performance metrics into classes and trains the diffusion model to generate architectures matching target performance constraints. Unlike previous approaches, DiNAS is fully differentiable and requires only single-model training, achieving generation speeds of under 0.2 seconds per architecture.

## Key Results
- Achieves state-of-the-art performance across six standard NAS benchmarks
- Generates high-performing architectures in under 0.2 seconds per architecture
- Outperforms existing methods in tabular, surrogate, and hardware-aware benchmarks
- Demonstrates strong generalization capabilities across different datasets

## Why This Works (Mechanism)

### Mechanism 1
Conditional graph diffusion with classifier-free guidance enables efficient and accurate architecture generation. The diffusion model learns a latent space of architectures and uses conditional guidance to sample high-performing architectures without external predictors. Core assumption: The learned latent space contains sufficient information to guide sampling toward desired performance constraints. Evidence anchors: [abstract] "We then propose a multi-conditioned classifier-free guidance approach applied to graph diffusion networks to jointly impose constraints such as high accuracy and low hardware latency." [section] "Ho & Salimans (2021) came up with the classifier-free guidance, which develops the classifier using the generative model itself." Break condition: If the latent space does not adequately capture the relationship between architecture structure and performance, guidance will be ineffective.

### Mechanism 2
Multi-conditioned guidance allows simultaneous optimization of multiple performance metrics. By conditioning on multiple metrics (e.g., accuracy and latency), the model can generate architectures satisfying complex constraints. Core assumption: The performance metrics are independent enough to be conditioned on simultaneously without interference. Evidence anchors: [abstract] "We then propose a multi-conditioned classifier-free guidance approach applied to graph diffusion networks to jointly impose constraints such as high accuracy and low hardware latency." [section] "Consider the unconditional noise modelq that corrupts the data progressively fort time steps. Our objective is to estimate the reverse conditional diffusion processˆq(Gt−1|Gt,y 1,y 2,...,yk), given thek indepen-dent conditions y1,y 2,...,yk." Break condition: If the metrics are highly correlated or conflicting, multi-conditioning may lead to suboptimal results.

### Mechanism 3
Discrete graph diffusion preserves structural information better than continuous diffusion. By operating in discrete space, the model maintains graph sparsity and structural properties during generation. Core assumption: The discrete representation of graphs is sufficient to capture all necessary architectural features. Evidence anchors: [section] "Training a diffusion model to generate graphs in the same manner, however, leads to the loss of graph sparsity and structural information. DiGress, a discrete diffusion approach proposed by Vignac et al. (2023), addresses this problem with a Markov processes as discrete noise model." [corpus] "Towards Lightweight Graph Neural Network Search with Curriculum Graph Sparsification" (related work on graph sparsity). Break condition: If the discrete representation is too restrictive, it may limit the expressiveness of the generated architectures.

## Foundational Learning

- Concept: Graph representation of neural architectures
  - Why needed here: The entire approach operates on graph-structured data, requiring understanding of how architectures map to graphs.
  - Quick check question: How would you represent a neural network cell as a directed acyclic graph?

- Concept: Diffusion models and denoising processes
  - Why needed here: The core generation mechanism is based on diffusion models, requiring understanding of forward and reverse processes.
  - Quick check question: What is the difference between the forward and reverse processes in a diffusion model?

- Concept: Classifier-free guidance
  - Why needed here: This technique is used to condition the diffusion model without external classifiers.
  - Quick check question: How does classifier-free guidance differ from traditional classifier-based guidance in diffusion models?

## Architecture Onboarding

- Component map:
  Graph Transformer network -> Discrete diffusion process (forward and reverse) -> Multi-conditioned classifier-free guidance module -> Positional encoding for DAG representation -> Loss function combining node and edge cross-entropy

- Critical path:
  1. Train Graph Transformer on discrete graph diffusion task
  2. Implement multi-conditioned guidance mechanism
  3. Integrate positional encoding for DAGs
  4. Evaluate on benchmark datasets
  5. Optimize for speed and accuracy

- Design tradeoffs:
  - Discrete vs. continuous diffusion: Discrete preserves structure but may limit expressiveness
  - Number of conditioning metrics: More metrics allow complex constraints but increase computational cost
  - Guidance scale: Higher values improve sample quality but reduce diversity

- Failure signatures:
  - Poor performance on benchmarks: May indicate issues with latent space learning or guidance mechanism
  - Slow generation times: Could suggest inefficiencies in the Graph Transformer or sampling process
  - Lack of novelty in generated architectures: Might indicate overfitting to training data

- First 3 experiments:
  1. Train on NAS-Bench-101 and evaluate on held-out architectures to verify basic functionality
  2. Implement single-condition guidance (accuracy only) and compare to unconditional generation
  3. Add multi-condition guidance (accuracy + latency) and evaluate on hardware-aware benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of guidance scale parameter (γ) affect the performance and efficiency of DiNAS across different NAS benchmarks?
- Basis in paper: [explicit] The paper conducts an ablation study on the effect of different guidance scale values (-4, -2, 2, 4) on the performance of DiNAS on NAS-Bench-101 and NAS-Bench-NLP.
- Why unresolved: While the study shows that negative values of γ generally perform better, the optimal value may vary depending on the specific benchmark and search space characteristics.
- What evidence would resolve it: Systematic experiments across a wider range of NAS benchmarks with varying search space sizes and complexities to determine the relationship between γ and performance/efficiency.

### Open Question 2
- Question: How does the number of training samples affect the performance and generalization capabilities of DiNAS?
- Basis in paper: [explicit] The paper conducts an ablation study on the effect of training with different numbers of samples (100,000, 10,000, 1,000) on the performance of DiNAS on NAS-Bench-301.
- Why unresolved: The study shows a decrease in performance with fewer training samples, but the exact relationship between the number of training samples and the quality of generated architectures is not fully explored.
- What evidence would resolve it: Further experiments with varying numbers of training samples on multiple benchmarks to establish a quantitative relationship between training sample size and DiNAS performance.

### Open Question 3
- Question: How does the choice of discretization method for the target variable (e.g., accuracy) impact the performance of DiNAS?
- Basis in paper: [explicit] The paper discretizes accuracy into two classes (> fth percentile and the rest) and mentions that using higher values of f generates better-performing architectures but leads to class imbalance.
- Why unresolved: The paper only considers a simple discretization approach, and it is unclear how more sophisticated methods (e.g., clustering-based discretization) would affect the performance.
- What evidence would resolve it: Experiments comparing different discretization methods (e.g., equal-width binning, equal-frequency binning, clustering) and their impact on the quality of generated architectures.

## Limitations

- Discrete graph diffusion may face scalability challenges when applied to larger architectures beyond existing benchmarks
- Performance on real-world, large-scale architectures remains unverified
- Computational overhead of training diffusion models on larger graphs could become prohibitive

## Confidence

- **High Confidence**: The core diffusion mechanism and Graph Transformer implementation (claims supported by established literature and clear methodological descriptions)
- **Medium Confidence**: Multi-conditioned guidance effectiveness and generation speed claims (experimental results show promise but limited ablation studies)
- **Low Confidence**: Generalization to architectures outside training distributions and scalability to ImageNet-scale models (requires further empirical validation)

## Next Checks

1. **Architecture Transferability Test**: Evaluate generated architectures on CIFAR-10 (original domain) and then directly on ImageNet without retraining the diffusion model to assess domain adaptation capabilities.

2. **Novelty Verification**: Compare generated architectures against the training set using graph edit distance metrics to quantify true novelty and ensure the model isn't memorizing existing architectures.

3. **Scaling Experiment**: Gradually increase architecture complexity (number of nodes/edges) and measure how generation time and quality degrade to establish practical limits of the approach.