---
ver: rpa2
title: Tracking Changing Probabilities via Dynamic Learners
arxiv_id: '2402.10142'
source_url: https://arxiv.org/abs/2402.10142
tags:
- item
- items
- when
- time
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of online multiclass probabilistic
  prediction under non-stationarity, where a predictor must track changing probabilities
  in a lifelong, open-ended stream of items. The core method idea involves developing
  sparse moving averages (SMAs) that dynamically adapt learning rates for each predictand,
  allowing faster convergence and lower variance compared to static methods.
---

# Tracking Changing Probabilities via Dynamic Learners

## Quick Facts
- arXiv ID: 2402.10142
- Source URL: https://arxiv.org/abs/2402.10142
- Reference count: 40
- Primary result: DYAL outperforms simpler methods on tracking changing probabilities in non-stationary environments

## Executive Summary
This paper addresses the problem of online multiclass probabilistic prediction under non-stationarity, where a predictor must track changing probabilities in a lifelong, open-ended stream of items. The core method idea involves developing sparse moving averages (SMAs) that dynamically adapt learning rates for each predictand, allowing faster convergence and lower variance compared to static methods. The key contributions include a novel evaluation method using bounded log-loss that handles noise and non-stationarity, and the development of DYAL, a hybrid SMA combining sparse EMA and queuing techniques, which demonstrates superior performance in tracking changing probabilities.

## Method Summary
The paper develops sparse moving averages (SMAs) that dynamically adapt learning rates for each predictand, enabling faster convergence and lower variance compared to static methods. The core algorithm, DYAL, combines sparse exponential moving averages with queue-based estimation techniques, maintaining per-predictand learning rates that decay over time via harmonic decay. When significant changes are detected via binomial-tail tests on queue estimates, the learning rate is reset to a high value based on recent observations, allowing rapid adaptation while maintaining stability on predictable items.

## Key Results
- DYAL outperforms simpler methods on both synthetic and real-world datasets for tracking changing probabilities
- Novel bounded log-loss evaluation method handles noise and non-stationarity effectively
- Dynamic per-item learning rates enable faster adaptation while maintaining stability compared to static approaches

## Why This Works (Mechanism)

### Mechanism 1
Dynamic per-item learning rates enable faster adaptation to changing probabilities while maintaining stability on stable items. Each predictand maintains its own learning rate that decays over time via harmonic decay. When significant changes are detected via binomial-tail tests on queue estimates, the learning rate is reset to a high value based on recent observations, allowing rapid adaptation. Stable items continue with low rates for precision.

Core assumption: Items exhibit periods of stability long enough to converge before the next change, and queue-based change detection is sufficiently sensitive.

### Mechanism 2
Queue-based estimation provides unbiased initial probability estimates and change detection for new items. For each item, a small queue of count snapshots is maintained. Positive observations allocate new cells initialized to 1, negative observations increment the oldest cell. The estimator (k-1)/sum(Cj-1) from completed cells is minimum-variance unbiased. Queue estimates trigger learning rate resets when EMA estimates deviate significantly.

Core assumption: Sufficient recent observations exist to provide reliable queue estimates, and the geometric distribution of inter-arrival times enables unbiased estimation.

### Mechanism 3
Bounded log-loss with noise item handling enables meaningful evaluation of probability estimates under non-stationarity. The FC() function filters out low-probability items and caps total probability mass to leave room for noise items. The loglossRuleNS() function assigns bounded loss (-ln(pN S)) when the predictor assigns zero probability to a noise item, while using unallocated mass for items marked as noise by the referee.

Core assumption: The NS-marker algorithm reliably distinguishes salient from noise items, and the pN S threshold is appropriately set relative to pmin.

## Foundational Learning

- **Exponential Moving Average (EMA) and harmonic decay**: EMA provides the base smoothing mechanism that tracks changing probabilities, while harmonic decay allows initial rapid learning followed by stability. Understanding EMA's convergence properties and how decay schedules affect learning rate is essential for grasping DYAL's design.

  Quick check question: Why does EMA with harmonic decay require O(1/p*) time steps instead of O(1/β) for convergence to probability p*?

- **Rao-Blackwellization and minimum-variance unbiased estimation**: The queue-based estimator relies on Rao-Blackwellization to derive minimum-variance unbiased estimates from geometric-distributed counts. This statistical foundation justifies why queue estimates are reliable enough to reset learning rates.

  Quick check question: How does Rao-Blackwellization improve the simple MLE estimator 1/C for geometric-distributed counts?

- **KL divergence and its properties under scaling**: The bounded log-loss evaluation is based on KL divergence between distributions. Understanding how KL divergence behaves under scaling transformations is crucial for analyzing when the evaluation remains proper and when distortion occurs.

  Quick check question: What property of KL divergence ensures that scaling both distributions by the same factor doesn't change their relative distance?

## Architecture Onboarding

- **Component map**: DYAL predictor -> NS-marker referee -> FC() function -> Evaluation pipeline
- **Critical path**: Initialize empty predictor maps → For each observation: update queues, possibly listen to queue, apply weakening/strengthening, prune maps → Apply FC() before evaluation → Use NS-marker and loglossRuleNS() for scoring
- **Design tradeoffs**: Queue capacity vs. memory (O(|qM ap| * qcap)), learning rate floor vs. adaptation, NS-marker window size vs. responsiveness, change detection sensitivity vs. false positives
- **Failure signatures**: High variance in estimates (rates not decaying properly), slow adaptation to changes (binomial threshold too high or queue capacity too small), memory bloat (pruning thresholds too lenient), poor evaluation scores (NS-marker threshold misaligned with pmin)
- **First 3 experiments**:
  1. Single-item stationary binary sequence with p* = 0.1: Compare static EMA (β=0.01), harmonic EMA (βmin=0.001), and DYAL (βmin=0.001) on convergence speed and deviation rates.
  2. Multi-item non-stationary sequence with Omin=10: Generate sequences where p* oscillates between 0.025 and 0.25 for one item while others remain stable. Compare Qs (qcap=5,10), static EMA (β=0.01), harmonic EMA (βmin=0.01), and DYAL (βmin=0.01) on deviation rates and log-loss.
  3. Expedition character-level prediction: Run simplified Expedition system without concept generation, track character prediction performance over 10k time points. Compare Qs (qcap=3,5,10), static EMA (β=0.005,0.01), harmonic EMA (βmin=0.001), and DYAL (βmin=0.001,0.01) on log-loss.

## Open Questions the Paper Calls Out

### Open Question 1
How does the sensitivity of DYAL's performance to the binomial threshold parameter vary across different data sources and non-stationarity patterns? The paper presents sensitivity analysis on limited datasets, leaving open the question of robustness to parameter tuning in diverse real-world scenarios.

### Open Question 2
Can the concept of bounded log-loss be extended to handle situations where the true underlying distribution changes more rapidly or unpredictably? The paper focuses on gradual or intermittent changes, not exploring performance in scenarios with more rapid or unpredictable changes.

### Open Question 3
How does the performance of DYAL compare to other state-of-the-art online learning algorithms in non-stationary environments with varying levels of concept drift? The paper compares DYAL to other SMAs but does not include comparisons with broader online learning algorithms designed for non-stationary environments.

## Limitations
- Empirical validation primarily on synthetic data and specific real-world datasets (Expedition character sequences, Unix command logs)
- NS-marker algorithm is simplistic, relying only on recent observation counts without semantic understanding
- Bounded log-loss evaluation depends heavily on proper calibration of the noise threshold pNS relative to pmin

## Confidence
- **High confidence**: Theoretical foundations for SMA, EMA with harmonic decay, and Rao-Blackwellization are well-established
- **Medium confidence**: Superiority of DYAL over simpler methods demonstrated empirically but relies on controlled synthetic data
- **Low confidence**: Evaluation method's robustness to different noise patterns and generalizability of NS-marker algorithm across domains remain uncertain

## Next Checks
1. **Cross-dataset generalization test**: Apply DYAL to multiple diverse real-world streams (web clickstreams, financial time series, sensor data) with varying non-stationarity patterns
2. **NS-marker sensitivity analysis**: Systematically vary the NS-marker parameters (cN S, pNS) across a grid to identify optimal settings and understand failure modes
3. **Theoretical convergence bounds**: Derive formal convergence rate bounds for DYAL under different non-stationarity assumptions to complement empirical results