---
ver: rpa2
title: Safety Filters for Black-Box Dynamical Systems by Learning Discriminating Hyperplanes
arxiv_id: '2402.05279'
source_url: https://arxiv.org/abs/2402.05279
tags:
- safety
- control
- learning
- safe
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of designing safety filters for
  black-box dynamical systems by learning control input constraints without relying
  on specific certificate functions. The core method introduces the concept of a discriminating
  hyperplane that defines a half-space constraint on the control input at each state,
  serving as a sufficient condition for safety.
---

# Safety Filters for Black-Box Dynamical Systems by Learning Discriminating Hyperplanes

## Quick Facts
- arXiv ID: 2402.05279
- Source URL: https://arxiv.org/abs/2402.05279
- Authors: Will Lavanakul; Jason J. Choi; Koushil Sreenath; Claire J. Tomlin
- Reference count: 15
- Primary result: Learning-based safety filters using discriminating hyperplanes maintain system safety across multiple test systems with minimal constraint violations

## Executive Summary
This paper introduces a novel approach to designing safety filters for black-box dynamical systems without requiring explicit knowledge of system dynamics or specific certificate functions. The method learns control input constraints through discriminating hyperplanes that define safe half-spaces at each state. Two learning paradigms are proposed: supervised learning from pre-verified control invariant sets and reinforcement learning from safe trajectory data. The approach demonstrates effectiveness across various benchmark systems while maintaining safety guarantees.

## Method Summary
The core innovation is the concept of a discriminating hyperplane that serves as a sufficient condition for safety by constraining the control input space at each state. The method learns these hyperplanes through either supervised learning, where pre-verified control invariant sets provide training data, or reinforcement learning, which directly learns from safe trajectory experiences. At runtime, the learned safety filter checks proposed control inputs against the discriminating hyperplane and projects unsafe inputs onto the safe half-space, ensuring safety while allowing maximum performance within constraints.

## Key Results
- The learned safety filters maintain system safety across kinematic vehicle models, inverted pendulums, and robotic systems
- Minimal constraint violations observed during testing compared to baseline methods
- Competitive performance achieved relative to existing safety filter approaches while requiring less system knowledge

## Why This Works (Mechanism)
The approach leverages the mathematical property that a discriminating hyperplane can define a half-space of safe control inputs without requiring full knowledge of system dynamics. By learning these hyperplanes directly from data rather than deriving them analytically, the method bypasses the need for specific certificate functions or explicit system models. The hyperplane acts as a sufficient condition: any control input within the safe half-space guarantees safety, while unsafe inputs are projected back to safety, creating a robust filtering mechanism.

## Foundational Learning
- Discriminating hyperplanes: Mathematical constructs that separate safe from unsafe control inputs; needed because they provide sufficient safety conditions without requiring exact system models
- Control invariant sets: Regions of state space from which the system can be driven to safety; quick check: verify these sets are pre-computed for the target system
- Half-space constraints: Geometric representation of safe control regions; needed to enable efficient real-time safety checks
- Supervised vs. reinforcement learning paradigms: Different approaches for learning discriminating hyperplanes; quick check: assess data availability for each approach
- Safety projection operators: Methods for mapping unsafe inputs to safe regions; needed to maintain safety while minimizing performance degradation

## Architecture Onboarding

Component map:
Sensor inputs -> State estimation -> Discriminating hyperplane lookup -> Control input projection -> Actuator commands

Critical path:
State estimation → Hyperplane evaluation → Safety check → Control projection → System actuation

Design tradeoffs:
- Accuracy vs. computational complexity in hyperplane representation
- Learning data requirements vs. safety guarantees
- Real-time performance vs. constraint tightness
- Supervised learning precision vs. reinforcement learning flexibility

Failure signatures:
- Constraint violations indicating insufficient learning or model mismatch
- Excessive conservatism suggesting overly restrictive hyperplanes
- Computational delays causing timing violations
- Inconsistent safety projections across similar states

First experiments:
1. Validate safety filter on a simple 2D kinematic vehicle with known dynamics
2. Test learning performance with varying amounts of training data
3. Evaluate real-time computational requirements on embedded hardware

## Open Questions the Paper Calls Out
None

## Limitations
- Primarily validated through simulation on low-dimensional systems
- Assumes availability of pre-verified control invariant sets or sufficient safe trajectory data
- Computational complexity of real-time hyperplane evaluation not thoroughly addressed
- Scalability to high-dimensional systems with fast dynamics remains unproven

## Confidence

High confidence: Theoretical soundness of discriminating hyperplanes as safety conditions
Medium confidence: Empirical effectiveness across tested benchmark systems
Medium confidence: Comparative performance against existing safety filter methods

## Next Checks

1. Test the method on high-dimensional systems (e.g., 10+ state variables) to evaluate scalability and computational feasibility
2. Validate the approach on hardware implementations with real-world disturbances and measurement noise
3. Conduct systematic ablation studies to quantify the impact of different learning algorithms and hyperparameters on safety performance