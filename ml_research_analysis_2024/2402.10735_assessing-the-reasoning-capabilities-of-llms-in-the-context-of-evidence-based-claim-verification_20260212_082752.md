---
ver: rpa2
title: Assessing the Reasoning Capabilities of LLMs in the context of Evidence-based
  Claim Verification
arxiv_id: '2402.10735'
source_url: https://arxiv.org/abs/2402.10735
tags:
- reasoning
- claim
- evidence
- llms
- abductive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces RECV, a new benchmark for assessing large\
  \ language models' reasoning capabilities in evidence-based claim verification.\
  \ It proposes a framework to decompose claim-evidence pairs into atomic reasoning\
  \ types\u2014deductive and abductive\u2014and creates three datasets (VitaminC,\
  \ CLIMATE-FEVER, PHEMEPlus) with increasing complexity."
---

# Assessing the Reasoning Capabilities of LLMs in the context of Evidence-based Claim Verification

## Quick Facts
- arXiv ID: 2402.10735
- Source URL: https://arxiv.org/abs/2402.10735
- Reference count: 22
- Key outcome: LLMs perform up to three times worse on abductive reasoning compared to deductive reasoning in evidence-based claim verification

## Executive Summary
This paper introduces RECV, a benchmark framework for assessing large language models' reasoning capabilities in evidence-based claim verification. The framework decomposes claim-evidence pairs into atomic reasoning types—deductive and abductive—and creates three datasets (VitaminC, CLIMATE-FEVER, PHEMEPlus) with increasing complexity. Experiments with three state-of-the-art LLMs show that while models perform reasonably well on deductive reasoning, they struggle significantly with abductive reasoning, achieving up to three times worse performance. The study reveals that Chain-of-Thought prompting improves deductive reasoning but often harms abductive reasoning and overall task performance.

## Method Summary
The paper proposes a framework that breaks down claim-evidence pairs into deductive and abductive reasoning types using similarity metrics (BERTScore, BLEURT, cosine similarity). Three datasets were created with varying complexity: VitaminC (natural language editing), CLIMATE-FEVER (climate change claims), and PHEMEPlus (rumor detection). Three proprietary LLMs (Claude V3 Sonnet, GPT-4, GPT-4o) were evaluated using different prompting strategies (Zero-Shot, Chain-of-Thought, Manual Chain-of-Thought) in both with and without rationale generation settings. Performance was measured using macro F1 scores for claim veracity classification and qualitative evaluation of generated rationales.

## Key Results
- LLMs achieve significantly lower performance on abductive reasoning (up to 3x worse) compared to deductive reasoning
- Chain-of-Thought prompting improves performance on deductive reasoning but is detrimental for abductive reasoning tasks
- Generated rationales show semantic similarity to human explanations, particularly for deductive reasoning cases
- The RECV benchmark effectively exposes the limitations of current LLMs in handling complex reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can distinguish deductive from abductive reasoning when evaluating claim-evidence pairs
- Mechanism: RECV framework uses similarity metrics and manual annotation to decompose claims into atomic reasoning steps
- Core assumption: Abductive reasoning exhibits lower BERTScore and higher lexical overlap than deductive reasoning
- Evidence anchors: Abstract mentions framework for breaking down claims into atomic reasoning types; Section 5 describes using embedding-based similarity metrics

### Mechanism 2
- Claim: Chain-of-Thought prompting improves deductive reasoning but harms abductive reasoning performance
- Mechanism: CoT provides explicit reasoning steps that align with deduction but introduces uncertainty when evidence is incomplete
- Core assumption: CoT benefits tasks with clear logical progression but disrupts plausible hypothesis generation
- Evidence anchors: Abstract notes rationale generation is not always beneficial; Section 6 shows CoT improves simple verification but harms complex claims

### Mechanism 3
- Claim: Generated rationales by LLMs show semantic similarity to human explanations, especially for deductive reasoning
- Mechanism: LLM-generated explanations follow similar logical structures to human reasoning when evidence directly supports claims
- Core assumption: Semantic similarity indicates genuine reasoning capability rather than pattern matching
- Evidence anchors: Abstract states rationales are semantically similar to human explanations; Section 7 notes deductive rationales resemble assertions

## Foundational Learning

- Concept: Atomic reasoning types (deductive vs abductive)
  - Why needed here: RECV framework requires clear distinction between reasoning types to create benchmark and evaluate LLM performance
  - Quick check question: Can you explain the difference between deductive reasoning (evidence directly supports/refutes claim) and abductive reasoning (evidence suggests plausible hypothesis)?

- Concept: Similarity metrics for text classification
  - Why needed here: Framework uses BERTScore, BLEURT, and cosine similarity to separate deductive from abductive samples
  - Quick check question: How would you use three different similarity metrics to classify text pairs into two categories when one category is expected to be much rarer?

- Concept: Chain-of-Thought prompting strategies
  - Why needed here: Paper evaluates ZS, M-CoT, and ZS CoT to understand how explicit reasoning steps affect different reasoning types
  - Quick check question: What's the difference between Zero-Shot, Manual Chain-of-Thought, and Zero-Shot Chain-of-Thought prompting, and when might each be most effective?

## Architecture Onboarding

- Component map: RECV framework → dataset sampling → annotation pipeline → LLM evaluation → qualitative analysis → benchmark creation
- Critical path: Claim-evidence pair decomposition → reasoning type labeling → LLM evaluation → performance analysis → rationale quality assessment
- Design tradeoffs: Manual annotation ensures quality but limits scalability; heuristic sampling balances class imbalance but introduces noise
- Failure signatures: Poor IAA scores indicate annotation ambiguity; performance degradation in CoT suggests reasoning path divergence; hallucinated rationales indicate explanation quality issues
- First 3 experiments:
  1. Apply RECV framework to a small sample of claims and manually verify reasoning type classification accuracy
  2. Test different similarity metric thresholds on held-out data to optimize abductive sample detection
  3. Evaluate LLM performance on deductive-only subset to establish baseline before testing abductive cases

## Open Questions the Paper Calls Out
None

## Limitations
- Study relies on proprietary LLMs without access to model weights or training data, limiting reproducibility
- Heuristic separation of deductive and abductive reasoning using similarity metrics may introduce noise
- Qualitative evaluation of rationales depends on subjective human judgment

## Confidence
- High confidence: LLMs perform significantly worse on abductive reasoning compared to deductive reasoning
- Medium confidence: Chain-of-Thought prompting harms abductive reasoning performance
- Medium confidence: Semantic similarity between generated and human rationales

## Next Checks
1. Conduct ablation studies by varying similarity metric thresholds and evaluating their impact on reasoning type classification accuracy
2. Perform cross-validation with different prompt formulations to test the robustness of Chain-of-Thought effects
3. Implement automated metrics (e.g., ROUGE, BERTScore) to supplement human evaluation of rationale quality