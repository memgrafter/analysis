---
ver: rpa2
title: Equivariant Test-Time Training with Operator Sketching for Imaging Inverse
  Problems
arxiv_id: '2411.05771'
source_url: https://arxiv.org/abs/2411.05771
tags:
- imaging
- deep
- sketched
- image
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a computationally efficient variant of Equivariant
  Imaging (EI) regularization for unsupervised training of deep imaging networks,
  addressing the high computational cost associated with the standard EI approach.
  The authors introduce a sketched EI regularization technique that leverages randomized
  sketching to accelerate the computation, making it particularly suitable for high-dimensional
  imaging problems.
---

# Equivariant Test-Time Training with Operator Sketching for Imaging Inverse Problems

## Quick Facts
- arXiv ID: 2411.05771
- Source URL: https://arxiv.org/abs/2411.05771
- Authors: Guixian Xu; Jinglai Li; Junqi Tang
- Reference count: 8
- The paper proposes a computationally efficient variant of Equivariant Imaging (EI) regularization for unsupervised training of deep imaging networks, addressing the high computational cost associated with the standard EI approach.

## Executive Summary
This paper introduces a sketched EI regularization technique that leverages randomized sketching to accelerate computation in unsupervised imaging inverse problems. The method combines operator sketching with deep image prior (DIP) frameworks to achieve significant computational acceleration while maintaining reconstruction quality. The approach is particularly effective for high-dimensional imaging problems like X-ray CT and multicoil MRI reconstruction, where it demonstrates order-of-magnitude computational speedup compared to standard EI-based approaches. The authors also develop a parameter-efficient method for network adaptation by optimizing only normalization layers during test-time training.

## Method Summary
The proposed method extends the Deep Image Prior framework with sketched Equivariant Imaging regularization for unsupervised image reconstruction. The approach splits the measurement operator into minibatches using subsampling sketches, which reduces computational overhead without significantly compromising performance. The network architecture uses a U-Net structure, and training employs Adam optimizer with learning rate 5×10^-4 for 5,000 iterations. The method is applied to sparse-view X-ray CT reconstruction and multicoil MRI reconstruction tasks, demonstrating substantial computational acceleration compared to standard DIP and EI-regularized DIP approaches while maintaining comparable reconstruction quality.

## Key Results
- The sketched EI-DIP approach achieves order-of-magnitude computational acceleration over standard DIP and EI-regularized DIP in single-input internal learning settings
- Significant computational acceleration is demonstrated in test-time training scenarios, making the method particularly suitable for high-dimensional imaging problems
- The method maintains comparable reconstruction quality while achieving 2-3x speedup in absolute runtime for single-input internal learning settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sketching the EI regularization term reduces computational cost while maintaining reconstruction quality.
- Mechanism: The proposed method replaces the full measurement operator A with sketched minibatches AM := MA, where M is a random sketching operator. This allows for efficient approximation of the EI regularization term through dimensionality reduction techniques.
- Core assumption: The network F_θ is L-Lipschitz continuous and the measurement operator A has a fast-decaying singular value spectrum.
- Evidence anchors:
  - [abstract]: "The authors introduce a sketched EI regularization technique that leverages randomized sketching to accelerate the computation"
  - [section 2.2]: "Theorem 1 (Approximation bound for Sketched EI regularization)... Our sketched EI regularizer is an effective approximation of the original EI"
  - [corpus]: Weak evidence - no direct corpus papers discussing sketching in EI regularization
- Break condition: If the sketching operator M is poorly chosen or the operator A does not have a fast-decaying singular value spectrum, the approximation quality will degrade significantly.

### Mechanism 2
- Claim: Optimizing only normalization layers provides parameter-efficient test-time adaptation.
- Mechanism: By restricting parameter updates to normalization layers during test-time training, the method achieves computational efficiency while maintaining adaptation capability.
- Core assumption: The pretrained network contains sufficient learned features that can be adapted through normalization layer adjustments.
- Evidence anchors:
  - [abstract]: "They also develop a parameter-efficient method for network adaptation by optimizing only normalization layers"
  - [section 1.1]: "one can mitigate this distribution shift by adapting the network via the DIP framework"
  - [corpus]: Weak evidence - no direct corpus papers discussing normalization-layer-only adaptation for test-time training
- Break condition: If the distribution shift is too large, adaptation through normalization layers alone may be insufficient to achieve good performance.

### Mechanism 3
- Claim: The sketched EI regularization provides a statistically effective approximation of the original EI regularization.
- Mechanism: The theoretical analysis shows that the sketched version bounds the original EI regularization with a deviation that scales as O(1/√m), where m is the sketch size.
- Core assumption: The Lipschitz continuity assumption on the network and the concentration properties of the sketching operator.
- Evidence anchors:
  - [section 2.2]: "Theorem 1... demonstrates that our sketched EI regularizer is an effective approximation of the original EI"
  - [section 5]: "We provide the proof for the motivational bound presented in Theorem 1"
  - [corpus]: Weak evidence - no direct corpus papers providing theoretical bounds for sketched EI regularization
- Break condition: If the sketch size m is too small relative to the problem dimensionality, the approximation error will exceed acceptable thresholds.

## Foundational Learning

- Concept: Randomized sketching and stochastic optimization
  - Why needed here: Understanding how operator sketching reduces computational complexity while maintaining approximation quality is crucial for grasping the proposed method
  - Quick check question: How does the choice of sketching operator M affect the approximation quality and computational speedup in imaging inverse problems?

- Concept: Equivariant Imaging (EI) regularization
  - Why needed here: The proposed method builds upon EI regularization, so understanding its principles and computational challenges is essential
  - Quick check question: What is the main computational bottleneck in standard EI regularization that the sketching approach aims to address?

- Concept: Deep Image Prior (DIP) framework
  - Why needed here: The proposed Sketched EI-DIP extends the DIP framework with sketched EI regularization for improved efficiency
  - Quick check question: How does the EI regularization term in the loss function help the network learn beyond the range space of the measurement operator?

## Architecture Onboarding

- Component map:
  - Measurement operator A (X-ray CT/Multi-coil MRI) -> Sketching operator M (sub-sampling operator) -> Deep network F_θ (U-Net architecture) -> Optimization algorithm (Adam/SGD) -> Loss function (Sketched EI-DIP formulation)

- Critical path:
  1. Precompute A†y (stable approximation of pseudo-inverse)
  2. Define set of N sketching operators M1, M2, ... MN
  3. In each training iteration:
     - Sample a random sketching operator Mi
     - Compute sketched measurement consistency term
     - Compute sketched EI regularization term
     - Update network parameters (restricted to normalization layers for test-time adaptation)

- Design tradeoffs:
  - Sketch size m vs. approximation quality: Larger sketches provide better approximation but reduce computational speedup
  - Number of minibatches N vs. convergence speed: More minibatches may improve convergence but increase per-iteration cost
  - Parameter restriction (normalization layers only) vs. adaptation capability: More restricted updates are faster but may provide less adaptation

- Failure signatures:
  - Poor reconstruction quality: Likely caused by overly aggressive sketching or insufficient sketch size
  - Slow convergence: May indicate need for more minibatches or different learning rate
  - Numerical instability: Could result from ill-conditioned sketching operators or inappropriate choice of M

- First 3 experiments:
  1. Implement standard EI-DIP on a simple CT reconstruction task to establish baseline performance
  2. Add sketching to the EI regularization term with varying sketch sizes to evaluate approximation quality
  3. Implement parameter-efficient test-time adaptation by restricting updates to normalization layers and compare with full parameter updates

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important unresolved issues emerge from the work:

1. The precise relationship between sketch size m and approximation error δ in the sketched EI regularization, and how this relationship affects the trade-off between computational efficiency and reconstruction quality.
2. How the proposed sketched EI regularization performs on inverse problems beyond X-ray CT and multicoil MRI, such as Positron Emission Tomography (PET) or dynamic imaging problems.
3. Whether the sketched EI regularization can be extended to handle more complex measurement operators, such as those arising in compressed sensing or blind deconvolution problems, where the operator A is not known exactly.

## Limitations
- The theoretical analysis assumes Lipschitz continuity of the network and fast-decaying singular values of the measurement operator, which may not hold in practice for all imaging modalities.
- Empirical validation is limited to only two imaging tasks (CT and MRI) with specific parameters, making generalization uncertain.
- The computational savings, while substantial in test-time training scenarios, show only 2-3x speedup in absolute runtime for single-input internal learning settings.

## Confidence

### Confidence Labels
- High confidence: The computational efficiency improvements are well-demonstrated through timing experiments and theoretical analysis of the sketching approximation error.
- Medium confidence: The reconstruction quality preservation when using sketched EI regularization, as validated only on CT and MRI tasks with specific parameters.
- Low confidence: The generalization of the proposed method to other imaging modalities and inverse problems beyond the tested CT and MRI applications.

## Next Checks

1. Test the proposed sketched EI regularization on a different inverse problem (e.g., Fourier-based MRI reconstruction or compressed sensing MRI) to verify cross-modality generalization.
2. Conduct ablation studies with different sketching operators (beyond subsampling sketches) to understand the impact on approximation quality and computational efficiency.
3. Evaluate the method's performance with varying sketch sizes and minibatch numbers to establish optimal tradeoff between approximation accuracy and computational speedup.