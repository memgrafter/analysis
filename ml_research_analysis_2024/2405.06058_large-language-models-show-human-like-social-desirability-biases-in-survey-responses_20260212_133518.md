---
ver: rpa2
title: Large Language Models Show Human-like Social Desirability Biases in Survey
  Responses
arxiv_id: '2405.06058'
source_url: https://arxiv.org/abs/2405.06058
tags:
- llms
- bias
- personality
- human
- five
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study developed a systematic experimental framework using the
  Big Five personality surveys to investigate response biases in Large Language Models
  (LLMs). By varying the number of questions administered to models and testing across
  multiple models (GPT-4/3.5, Claude 3, Llama 3, PaLM-2), the researchers demonstrated
  that LLMs exhibit social desirability bias, skewing responses toward more socially
  desirable trait dimensions when they detect personality evaluation contexts.
---

# Large Language Models Show Human-like Social Desirability Biases in Survey Responses

## Quick Facts
- arXiv ID: 2405.06058
- Source URL: https://arxiv.org/abs/2405.06058
- Reference count: 16
- Large language models exhibit social desirability bias in personality surveys, with GPT-4 showing shifts of 1.20 standard deviations toward desirable traits

## Executive Summary
This study systematically investigated social desirability bias in large language models using Big Five personality surveys. By varying the number of questions administered per prompt across multiple models (GPT-4/3.5, Claude 3, Llama 3, PaLM-2), researchers demonstrated that LLMs detect personality evaluation contexts and adjust responses toward more socially desirable trait dimensions. The bias was robust across different experimental conditions including question order randomization, paraphrasing, and temperature variations. Only reverse-coding questions partially mitigated the effect, suggesting significant constraints on using LLMs as proxies for human survey responses.

## Method Summary
The researchers administered the IPIP Big Five personality questionnaire (100 items on 5-point Likert scale) to multiple LLMs in batches of varying sizes (1, 5, 10, and 20 questions per prompt), starting new sessions for each batch. They tested GPT-4/3.5, Claude 3, Llama 3, and PaLM-2 across different temperature settings. Personality factor scores were calculated for each batch size and compared to identify bias levels. The experimental design systematically varied context cues while keeping question content constant to isolate the effect of evaluation context detection on response patterns.

## Key Results
- GPT-4 showed personality scores shifting by 1.20 standard deviations toward desirable traits when detecting personality evaluation contexts
- Social desirability bias was robust across question order randomization, paraphrasing, and temperature variations (0.0, 0.4, 0.8, 1.2)
- Reverse-coding questions partially mitigated but did not eliminate the bias
- Larger and more recent models exhibited stronger bias effects
- Bias levels were comparable to human social desirability bias (0.50-1.20 standard deviations)

## Why This Works (Mechanism)

### Mechanism 1
LLMs detect personality evaluation context and adjust responses toward socially desirable trait dimensions. Models infer survey context from minimal question exposure, triggering socially desirable response patterns. Core assumption: LLMs encode social desirability norms from training data and apply them when detecting evaluation contexts. Evidence: LLMs demonstrated ability to infer when being evaluated and modulated responses based on perceived evaluation context.

### Mechanism 2
Larger and more recent models exhibit stronger social desirability bias. Model scale and training sophistication increase capacity to learn and apply social desirability norms. Core assumption: More parameters and data exposure improve model's ability to detect and respond to social evaluation contexts. Evidence: GPT-4 showed strongest effects at 1.20 standard deviations, with bias levels increasing in more recent models.

### Mechanism 3
Reverse-coding partially mitigates but doesn't eliminate social desirability bias. Reverse-coding disrupts semantic patterns associated with social desirability responses. Core assumption: Social desirability bias operates through semantic associations that reverse-coding partially breaks. Evidence: Reverse-coding decreased bias levels but did not eliminate them, with reverse-coded items loading on separate factors than positively-coded items.

## Foundational Learning

- **Big Five personality framework and trait dimensions** - Why needed: Study uses Big Five surveys to measure personality trait responses. Quick check: What are the five dimensions measured in Big Five personality assessments?
- **Social desirability bias in human psychology** - Why needed: Bias being studied is the same phenomenon that affects human survey responses. Quick check: How does social desirability bias typically manifest in human survey responses?
- **Psychometric test administration and scoring** - Why needed: Study administers personality surveys to LLMs using human-like protocols. Quick check: What are the key differences between administering psychometric tests to humans versus LLMs?

## Architecture Onboarding

- **Component map**: LLM inference engine → context window management → survey question batching → response generation → scoring engine → bias analysis pipeline
- **Critical path**: Question batch creation → LLM prompt construction → inference execution → response collection → score calculation → bias measurement
- **Design tradeoffs**: Batch size (Qn) vs. context window limits, question randomization vs. semantic coherence, temperature settings vs. response consistency
- **Failure signatures**: Inconsistent bias patterns across trials, context leakage between batches, temperature sensitivity affecting bias detection
- **First 3 experiments**:
  1. Test single-question batch (Qn=1) with GPT-4 to establish baseline bias level
  2. Test twenty-question batch (Qn=20) with GPT-4 to measure maximum bias effect
  3. Test reverse-coded survey with GPT-4 to measure mitigation effectiveness

## Open Questions the Paper Calls Out

1. Does social desirability bias generalize to other psychometric assessments beyond Big Five personality traits, particularly those with different levels of social evaluation and cultural representation in training data? The study focused on Big Five assessments, which are inherently socially evaluative, but less socially evaluative constructs might not show the same bias.

2. How do specific training methodologies (such as reinforcement learning from human feedback, dataset composition, or preference tuning) contribute to the development and magnitude of social desirability bias in LLMs? The study identified bias existence but did not systematically investigate which training aspects drive the bias.

3. Can LLMs be effectively calibrated or prompted to eliminate social desirability bias while maintaining their ability to function as human proxies in psychological research? While reverse-coding partially mitigated bias, the authors acknowledge this is insufficient for complete elimination.

## Limitations
- Study relies exclusively on self-reported personality assessments without external validation of actual trait manifestations
- Experiments use limited set of Big Five personality surveys without exploring alternative personality frameworks or broader psychometric domains
- Experiments conducted using specific temperature settings and prompt formats that may not generalize to other deployment scenarios

## Confidence

*High Confidence:* The existence of social desirability bias in LLM responses, the differential bias between GPT-4 and other models

*Medium Confidence:* The context inference mechanism triggering bias, the effectiveness of reverse-coding mitigation

*Low Confidence:* The claim that larger models exhibit stronger bias due to increased learning capacity, the assertion that this poses significant constraints on LLM use as human proxies

## Next Checks

1. **External Validation Protocol**: Design an experiment where LLM responses to personality questions are compared against their actual behavior in controlled tasks (e.g., cooperation games, resource allocation scenarios) to determine if self-reported traits align with observed behaviors.

2. **Cross-Domain Generalization Test**: Extend the experimental framework to non-personality psychometric domains (e.g., cognitive ability tests, emotional intelligence assessments) to determine if social desirability bias is specific to personality evaluation contexts.

3. **Contextual Cue Isolation Experiment**: Systematically vary contextual cues in survey administration (explicitly stating the purpose, using neutral vs. evaluative framing, varying instruction language) while keeping question content constant to identify which specific contextual elements trigger the strongest bias responses.