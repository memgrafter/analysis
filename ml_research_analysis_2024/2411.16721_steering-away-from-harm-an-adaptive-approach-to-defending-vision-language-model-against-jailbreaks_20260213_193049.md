---
ver: rpa2
title: 'Steering Away from Harm: An Adaptive Approach to Defending Vision Language
  Model Against Jailbreaks'
arxiv_id: '2411.16721'
source_url: https://arxiv.org/abs/2411.16721
tags:
- steering
- adversarial
- attack
- image
- jailbreak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ASTRA, a novel framework for defending vision
  language models (VLMs) against jailbreak attacks. The core idea involves constructing
  steering vectors that capture harmful semantic directions in the model's activation
  space and then adaptively steering the model away from these directions during inference.
---

# Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks

## Quick Facts
- arXiv ID: 2411.16721
- Source URL: https://arxiv.org/abs/2411.16721
- Reference count: 40
- Introduces ASTRA, a framework using adaptive activation steering to defend VLMs against jailbreak attacks with state-of-the-art performance

## Executive Summary
This paper introduces ASTRA, a novel framework for defending vision language models (VLMs) against jailbreak attacks through adaptive activation steering. The framework constructs steering vectors that capture harmful semantic directions in the model's activation space by analyzing which visual tokens contribute most to jailbreak responses. During inference, ASTRA applies adaptive steering only when the input exhibits characteristics of adversarial examples, preserving performance on benign inputs while effectively blocking harmful outputs. Extensive experiments across multiple models and attack types demonstrate ASTRA's superior performance, achieving up to 9× faster inference than existing methods while maintaining strong defense against both known and unseen attack strategies.

## Method Summary
ASTRA defends VLMs by constructing steering vectors that represent harmful semantic directions in activation space. The framework first generates adversarial images using PGD attacks with various perturbation levels, then identifies visual tokens most strongly associated with jailbreak responses through random ablation and Lasso regression. During inference, ASTRA applies adaptive steering by computing the projection between calibrated input activation and the steering vector, only applying the steering transformation when this projection indicates harmful content. This approach achieves strong defense effectiveness while preserving utility on benign inputs, with the steering mechanism being 9× faster than existing baselines.

## Key Results
- ASTRA achieves state-of-the-art performance defending MiniGPT-4, Qwen2-VL, and LLaVA against jailbreak attacks
- On MiniGPT-4, ASTRA reduces toxicity scores by 11.30% and attack success rates by 9.09% compared to undefended models
- The framework demonstrates strong transferability, defending against unseen attack types including structured-based and perturbation-based variants
- ASTRA achieves up to 9× faster inference than existing defense methods while maintaining robust performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASTRA uses image attribution to identify visual tokens most strongly associated with jailbreak responses
- Mechanism: The framework randomly ablates visual tokens from adversarial images, measures the impact on jailbreak probability, fits a Lasso surrogate model to attribute importance scores, and constructs steering vectors from the top-k attributed tokens
- Core assumption: Visual tokens can be quantitatively ranked by their contribution to jailbreak responses through ablation studies
- Evidence anchors:
  - [abstract] "we randomly ablate the visual tokens from the adversarial images and identify those most strongly associated with jailbreaks"
  - [section 3.1] "We conduct random ablation of certain tokens and compute the impact of exclusion/inclusion on inducing the jailbreak"
  - [corpus] Weak evidence - no neighboring papers directly address visual token attribution for jailbreak detection
- Break condition: If ablation studies show uniform token importance across adversarial images, or if Lasso surrogate fails to identify meaningful patterns

### Mechanism 2
- Claim: Adaptive steering with calibrated activation preserves benign performance while blocking harmful outputs
- Mechanism: The framework computes the projection between calibrated activation (input activation minus mean activation) and steering vector, applying steering only when this projection is positive
- Core assumption: Adversarial and benign activations form distinguishable clusters in activation space that can be separated by steering direction
- Evidence anchors:
  - [abstract] "resulting in little performance drops on benign inputs while strongly avoiding harmful outputs under adversarial inputs"
  - [section 3.2] "When hl does not contain any positive component of the steering vector (harmful direction), the max term is 0, leaving activations unchanged"
  - [corpus] No direct corpus evidence for this specific adaptive projection mechanism in VLMs
- Break condition: If adversarial and benign activations cluster indistinguishably around non-origin points, making projection unreliable

### Mechanism 3
- Claim: Steering vectors constructed from adversarial images generalize across different attack intensities and types
- Mechanism: Steering vectors are averaged across multiple adversarial images with different perturbation levels, creating a transferable representation of harmful directions
- Core assumption: Harmful semantic directions exist consistently across different attack parameters and can be captured by averaging steering vectors
- Evidence anchors:
  - [abstract] "ASTRA exhibits good transferability, defending against unseen attacks"
  - [section 4.2] "In ID scenario, adversarial images used for steering vector construction and test evaluations are drawn from same classes"
  - [corpus] Weak evidence - neighboring papers focus on different defense mechanisms, not transferability through steering vector construction
- Break condition: If steering vectors from one attack intensity fail to defend against significantly different intensities or attack types

## Foundational Learning

- Concept: Activation steering in transformer models
  - Why needed here: ASTRA builds directly on activation steering techniques adapted from LLMs to VLMs
  - Quick check question: How does adding a scaled steering vector to model activations change the output distribution?

- Concept: Adversarial examples and perturbation-based attacks
  - Why needed here: The framework constructs steering vectors from adversarial images created using PGD attacks
  - Quick check question: What distinguishes perturbation-based attacks from structured-based attacks in VLMs?

- Concept: Image attribution and feature importance
  - Why needed here: Image attribution identifies which visual tokens contribute most to harmful responses
  - Quick check question: How does random ablation help identify important features in input data?

## Architecture Onboarding

- Component map: Input preprocessing -> Adversarial image generation -> Random ablation -> Lasso attribution -> Vector averaging -> Inference-time steering -> Output generation
- Critical path: Adversarial image generation → steering vector construction → inference-time steering application
- Design tradeoffs: 
  - Steering at inference time vs. training-time defenses (faster deployment but potentially less robust)
  - Adaptive vs. fixed steering coefficients (better utility preservation but requires calibration)
  - Image attribution vs. whole-image steering (more targeted but computationally heavier)

- Failure signatures:
  - High toxicity scores despite steering indicate poor steering vector quality
  - Excessive refusal rates on benign inputs suggest over-aggressive steering
  - Slow inference despite claims of efficiency points to implementation issues

- First 3 experiments:
  1. Test steering vector construction on simple adversarial examples to verify ablation studies identify meaningful tokens
  2. Compare adaptive steering with fixed steering on validation set to measure utility preservation
  3. Evaluate transferability by testing steering vectors against different attack intensities on held-out data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ASTRA vary when using different surrogate models (e.g., Lasso, Ridge, ElasticNet) for image attribution?
- Basis in paper: [explicit] The paper mentions using Lasso for fitting the linear surrogate model to analyze the influence of masking subsets of visual tokens on the likelihood of jailbreaks.
- Why unresolved: The paper only evaluates the performance of ASTRA using Lasso for the surrogate model. It is unclear whether other surrogate models would perform better or worse in terms of defense effectiveness and computational efficiency.
- What evidence would resolve it: Conduct experiments comparing the performance of ASTRA using different surrogate models (e.g., Lasso, Ridge, ElasticNet) on the same evaluation metrics (toxicity score, attack success rate) and computational resources.

### Open Question 2
- Question: What is the impact of using different image attribution techniques (e.g., LIME, SHAP, Integrated Gradients) on the performance of ASTRA?
- Basis in paper: [explicit] The paper mentions using image attribution to identify visual tokens most strongly associated with jailbreaks, but does not specify which attribution technique is used.
- Why unresolved: The paper does not provide information on which image attribution technique is used or how different techniques might affect the performance of ASTRA.
- What evidence would resolve it: Conduct experiments comparing the performance of ASTRA using different image attribution techniques (e.g., LIME, SHAP, Integrated Gradients) on the same evaluation metrics (toxicity score, attack success rate) and computational resources.

### Open Question 3
- Question: How does the performance of ASTRA scale with the size of the VLM (e.g., number of parameters, layers)?
- Basis in paper: [explicit] The paper mentions that ASTRA achieves state-of-the-art performance on three popular open-sourced VLMs with different sizes (MiniGPT-4-13B, Qwen2-VL-7B, and LLaVA-v1.5-13B).
- Why unresolved: The paper does not provide information on how the performance of ASTRA scales with the size of the VLM or whether it is applicable to even larger VLMs.
- What evidence would resolve it: Conduct experiments evaluating the performance of ASTRA on VLMs with different sizes (e.g., number of parameters, layers) on the same evaluation metrics (toxicity score, attack success rate) and computational resources.

### Open Question 4
- Question: How does the performance of ASTRA compare to other activation engineering techniques (e.g., activation addition, activation subtraction) in terms of defense effectiveness and computational efficiency?
- Basis in paper: [explicit] The paper mentions that ASTRA is inspired by recent advancements in activation steering in Large Language Models (LLMs) and proposes an adaptive steering approach.
- Why unresolved: The paper does not provide a comparison between ASTRA and other activation engineering techniques in terms of defense effectiveness and computational efficiency.
- What evidence would resolve it: Conduct experiments comparing the performance of ASTRA with other activation engineering techniques (e.g., activation addition, activation subtraction) on the same evaluation metrics (toxicity score, attack success rate) and computational resources.

## Limitations

- The transferability claims are limited to specific variants of known attack types, with effectiveness against entirely novel attack strategies untested
- The token attribution mechanism may identify statistical artifacts rather than semantically meaningful features, with no validation of the attribution results
- The steering layer selection (20 for 13B models, 14 for 7B models) appears arbitrary without sensitivity analysis or explanation of how these values were determined

## Confidence

**High Confidence**: The core mechanism of adaptive activation steering is well-defined and the experimental results showing improved performance over undefended models are clearly demonstrated. The distinction between perturbation-based and structured-based attacks is well-established in the literature.

**Medium Confidence**: Claims about inference efficiency (9× faster than baselines) are supported by measurements but lack comparison to alternative fast defense methods. The utility preservation results are promising but rely on benchmark scores that may not fully capture real-world performance degradation.

**Low Confidence**: The generalizability claims to completely unseen attack strategies lack rigorous testing. The theoretical basis for why averaged steering vectors from one attack intensity should transfer to others is not established.

## Next Checks

**Check 1**: Test steering vector transferability by constructing vectors from one attack type (e.g., PGD with ϵ=32/255) and evaluating against a completely different attack methodology not based on perturbations, such as prompt-only jailbreaks or semantic attacks. Measure both defense effectiveness and utility preservation.

**Check 2**: Validate the token attribution mechanism by conducting ablation studies on the identified top-k tokens to confirm they correspond to semantically meaningful visual features. Compare with random token selection to ensure the attribution process identifies non-trivial features.

**Check 3**: Perform sensitivity analysis on the steering layer selection by testing multiple layer choices and measuring the impact on both defense performance and inference efficiency. This would establish whether the current layer choices are optimal or merely convenient.