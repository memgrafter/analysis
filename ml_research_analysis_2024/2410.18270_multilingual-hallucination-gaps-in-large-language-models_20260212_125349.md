---
ver: rpa2
title: Multilingual Hallucination Gaps in Large Language Models
arxiv_id: '2410.18270'
source_url: https://arxiv.org/abs/2410.18270
tags:
- lang
- language
- factscore
- languages
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates multilingual hallucination gaps in large
  language models (LLMs) by examining how frequently models generate false information
  across different languages. Using the FactScore metric, researchers evaluated LLM-generated
  biographies in 19 languages, comparing outputs to Wikipedia pages.
---

# Multilingual Hallucination Gaps in Large Language Models

## Quick Facts
- arXiv ID: 2410.18270
- Source URL: https://arxiv.org/abs/2410.18270
- Reference count: 25
- Primary result: Significant variation in hallucination rates across languages, with low-resource languages showing higher hallucination frequencies

## Executive Summary
This study investigates multilingual hallucination gaps in large language models by examining how frequently models generate false information across different languages. Using the FactScore metric, researchers evaluated LLM-generated biographies in 19 languages, comparing outputs to Wikipedia pages. The study employed three open-source LLM families (LLaMA, Qwen, Aya) with different model sizes and used multiple prompt templates. Results revealed significant variation in hallucination rates across languages, with low-resource languages showing higher hallucination frequencies compared to high-resource languages. The FactScore metric showed increased standard deviation in low-resource languages, indicating less consistent performance. Additionally, different experimental setups (prompt language, knowledge source selection) produced varying FactScore outcomes.

## Method Summary
The study generated biographies of 485 notable figures using three LLM families (LLaMA-3, Qwen2, Aya-23) across 19 languages. Researchers used three prompt templates and three experimental setups comparing different combinations of prompt language and knowledge source language. The FactScore metric was computed by decomposing generated text into atomic facts using Mistral-7B-Instruct-v0.3 and comparing them to Wikipedia pages. Language detection was performed using py3langid as a sanity check. The study analyzed hallucination rates across language resource levels and examined the impact of model size and experimental setup variations.

## Key Results
- Significant variation in hallucination rates across languages, with low-resource languages showing higher hallucination frequencies
- Larger models generally perform better but still exhibit hallucinations in low-resource languages
- FactScore metric shows increased standard deviation in low-resource languages, indicating less consistent performance
- Different experimental setups (prompt language, knowledge source selection) produce varying FactScore outcomes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual hallucination gaps occur because low-resource languages have less training data, leading to weaker language modeling and factual grounding.
- Mechanism: When a model is trained on limited multilingual data, it lacks sufficient exposure to factual patterns in low-resource languages. This results in higher hallucination rates when generating text in those languages.
- Core assumption: The hallucination rate is inversely proportional to the amount and quality of training data available for each language.
- Evidence anchors:
  - [abstract] "Results revealed significant variation in hallucination rates across languages, with low-resource languages showing higher hallucination frequencies compared to high-resource languages."
  - [section] "Our findings also indicate that model size and architecture influence these gaps. Larger models generally perform better but still show hallucinations in low-resource languages."
  - [corpus] "Weak or missing corpus evidence: The corpus doesn't directly address the data scarcity hypothesis, but the focus on multilingual hallucination metrics suggests this is a known concern."
- Break condition: If hallucination rates don't correlate with language resource levels or if models with similar training data show different hallucination patterns.

### Mechanism 2
- Claim: The FACTSCORE metric's reliability varies across languages due to evaluator model performance differences.
- Mechanism: The FACTSCORE metric uses an LLM (Mistral) to evaluate factual accuracy. If this evaluator performs inconsistently across languages, the measured hallucination rates will be unreliable, especially for low-resource languages.
- Core assumption: The LMEVAL (Mistral) has equal performance across all languages, which is not accurate in practice.
- Evidence anchors:
  - [abstract] "The study also found that larger models generally performed better but still exhibited hallucinations in low-resource languages."
  - [section] "However, unlike with the model Inst-LLaMA, adding NP did not enhance performance. We will thus use the Retrieveâ†’Mistral method for all subsequent experiments."
  - [corpus] "The corpus focuses on comparing hallucination detection metrics but doesn't specifically address evaluator bias across languages."
- Break condition: If alternative evaluation methods (human annotation, different LMEVALs) produce consistent results across languages.

### Mechanism 3
- Claim: Knowledge source quality variations across languages create systematic bias in hallucination measurement.
- Mechanism: Wikipedia content quality varies significantly across languages. When comparing generated text to Wikipedia, models in languages with lower-quality Wikipedia pages will appear to hallucinate more, even if their actual performance is similar.
- Core assumption: Wikipedia is equally comprehensive and accurate across all languages used in the study.
- Evidence anchors:
  - [abstract] "The study also found that larger models generally performed better but still exhibited hallucinations in low-resource languages."
  - [section] "Recognizing that Wikipedia content quality can vary across languages, we start by comparing these summaries to each other."
  - [corpus] "The corpus doesn't provide evidence about Wikipedia quality variations, but this is a known issue in multilingual NLP research."
- Break condition: If hallucination rates remain consistent when using alternative knowledge sources or when comparing to higher-quality references.

## Foundational Learning

- Concept: Multilingual model training dynamics
  - Why needed here: Understanding how models acquire multilingual capabilities explains why hallucination gaps exist across languages.
  - Quick check question: What are the two main ways multilingual models can be trained, and how might each affect hallucination rates in low-resource languages?

- Concept: Hallucination detection metrics
  - Why needed here: The study uses FACTSCORE, so understanding its components and limitations is crucial for interpreting results.
  - Quick check question: What are the three main steps in the FACTSCORE pipeline, and where might errors be introduced?

- Concept: Wikipedia as a knowledge source
  - Why needed here: The study relies on Wikipedia for fact-checking, so understanding its strengths and limitations is essential.
  - Quick check question: What are two potential issues with using Wikipedia as a knowledge source for hallucination detection across multiple languages?

## Architecture Onboarding

- Component map:
  LMSUBJ (LLaMA, Qwen, Aya) -> LMEVAL (Mistral-7B-Instruct-v0.3) -> Knowledge Source (Wikipedia) -> FactScore metric

- Critical path:
  1. Generate biographies using LMSUBJ with different prompt templates and languages
  2. Filter generated content through sanity checks
  3. Compute FactScore using LMEVAL and knowledge source comparison
  4. Analyze results across language categories and experimental setups

- Design tradeoffs:
  - Using Wikipedia provides multilingual coverage but introduces quality variations
  - FACTSCORE automation enables large-scale evaluation but may miss nuanced errors
  - Different prompt languages reveal model capabilities but complicate cross-language comparison

- Failure signatures:
  - High variance in FactScore across prompt templates indicates model instability
  - Consistently low scores in specific languages suggest systematic issues
  - Language detection failures indicate poor multilingual generation

- First 3 experiments:
  1. Compare FactScore results when using lang-prompt vs en-prompt for the same languages
  2. Test the impact of different knowledge sources (lang Wikipedia vs English Wikipedia) on hallucination detection
  3. Evaluate the effect of model size within each family on hallucination rates across resource levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multilingual hallucination gaps evolve when LLMs are tested on tasks beyond biographies, such as scientific topics or historical events?
- Basis in paper: [inferred] The authors mention in the Limitations section that future work could explore how these gaps evolve when LLMs are confronted with other tasks, such as other types of Wikipedia articles or text about historical events.
- Why unresolved: The current study only focuses on biographies, which may not represent the full range of tasks where multilingual hallucinations could occur. Different types of content might have varying levels of complexity, ambiguity, and available reference materials, potentially affecting hallucination rates.
- What evidence would resolve it: Conducting experiments using the same methodology (FACTSCORE) on different types of content (e.g., scientific articles, historical events) and comparing the results across languages would provide insights into whether the observed multilingual hallucination gaps are consistent across various domains.

### Open Question 2
- Question: How does the performance of the FACTSCORE metric vary across languages when using different knowledge sources beyond Wikipedia, such as domain-specific databases or multilingual corpora?
- Basis in paper: [explicit] The authors discuss the limitations of using Wikipedia as a knowledge source, noting that its coverage and accuracy vary across languages. They also mention that future work could extend the metric to extrinsic hallucinations, which would be useful for languages where Wikipedia coverage may be weaker.
- Why unresolved: The current study relies solely on Wikipedia as the knowledge source, which may not be representative of all languages or domains. Different knowledge sources might have varying levels of quality, comprehensiveness, and bias, which could impact the performance of the FACTSCORE metric.
- What evidence would resolve it: Evaluating the FACTSCORE metric using alternative knowledge sources (e.g., domain-specific databases, multilingual corpora) and comparing the results across languages would provide insights into the robustness and generalizability of the metric.

### Open Question 3
- Question: How can the FACTSCORE metric be improved to better handle multilingual contexts, particularly in low-resource languages where translations and evaluations may be less accurate?
- Basis in paper: [explicit] The authors discuss the limitations of the FACTSCORE metric in a multilingual setting, noting that scores vary based on the chosen knowledge source and that translations are generated by the LM EVAL within the pipeline. They suggest that addressing this behavior might improve results in the (lang, lang) setting.
- Why unresolved: The current implementation of the FACTSCORE metric relies on translations and evaluations by the LM EVAL, which may not perform equally well across all languages. This can lead to inconsistencies and biases in the results, particularly for low-resource languages.
- What evidence would resolve it: Developing and testing alternative approaches to handle translations and evaluations in the FACTSCORE metric, such as using human annotations or more robust translation models, would provide insights into how to improve its performance in multilingual contexts.

## Limitations
- The study relies on the FactScore metric, which uses LLM-based evaluation that may introduce bias, particularly for low-resource languages
- Wikipedia quality and coverage vary significantly across languages, potentially confounding the results
- The study focuses only on biographies, which may not generalize to other text types or domains

## Confidence

**High confidence:** The finding that hallucination rates vary across languages and that low-resource languages show higher hallucination frequencies

**Medium confidence:** The claim that larger models generally perform better but still exhibit hallucinations in low-resource languages

**Medium confidence:** The observation that different experimental setups (prompt language, knowledge source selection) produce varying FactScore outcomes

**Low confidence:** The precise quantification of hallucination gaps due to potential metric biases and knowledge source quality variations

## Next Checks
1. **Cross-metric validation**: Replicate key findings using alternative hallucination detection methods (human annotation, different LMEVALs, or multiple knowledge sources) to verify that observed language gaps are consistent across evaluation approaches.

2. **Controlled knowledge source comparison**: Test hallucination detection when using English Wikipedia versus native language Wikipedia for the same set of languages to isolate the impact of knowledge source quality from actual model performance differences.

3. **Translation robustness analysis**: Evaluate how generation quality changes when prompts are translated by different systems (GPT-4 vs native generation) to determine if observed gaps are due to prompt quality versus model capability differences.