---
ver: rpa2
title: 'TANQ: An open domain dataset of table answered questions'
arxiv_id: '2405.07765'
source_url: https://arxiv.org/abs/2405.07765
tags:
- table
- question
- answer
- questions
- tanq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TANQ introduces the first open-domain QA benchmark requiring table-based
  answers from multi-source evidence. It extends the QAMPARI dataset by adding attributes
  via Wikidata relations and extracting supporting evidence from Wikipedia (text,
  tables, infoboxes).
---

# TANQ: An open domain dataset of table answered questions

## Quick Facts
- arXiv ID: 2405.07765
- Source URL: https://arxiv.org/abs/2405.07765
- Reference count: 24
- Best baseline: 60.7 F1 (Gemini Flash, oracle setting)

## Executive Summary
TANQ introduces the first open-domain QA benchmark requiring table-based answers from multi-source evidence. It extends the QAMPARI dataset by adding attributes via Wikidata relations and extracting supporting evidence from Wikipedia (text, tables, infoboxes). The dataset includes 1,395 questions across three types (simple, composition, intersection) and eight reasoning skills (filtering, conversions, calculations, approximation). Evaluated on state-of-the-art models, the best-performing baseline (Gemini Flash, oracle setting) achieves 60.7 F1, lagging human performance by 12.3 points. Performance drops significantly in closed-book settings, highlighting the challenge of retrieving and synthesizing multi-hop information into structured tables.

## Method Summary
The TANQ dataset is constructed through an automated pipeline that extends QAMPARI questions using Wikidata relations, extracts supporting evidence from Wikipedia across multiple formats, and generates answer tables with source attribution. The evaluation uses a modified relative mapping similarity (RMS) metric adapted for TANQ's diverse table structures. Models are evaluated in three settings: closed-book (no external access), oracle (ground truth Wikipedia articles provided), and open-book (using tools like WikiSearch and Python for computation).

## Key Results
- Best baseline (Gemini Flash, oracle) achieves 60.7 F1, 12.3 points below human performance
- Closed-book setting drops to 43.4 F1, highlighting retrieval challenges
- Small models like Gemma outperform larger models on TANQ despite lower overall performance
- All models struggle with numerical reasoning, conversions, and approximation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based question extension preserves semantic intent while adding multi-hop complexity.
- Mechanism: Automated pipeline extends QAMPARI questions by appending Wikidata relations to answer entities, maintaining original intent through structured templates.
- Core assumption: Wikidata triples reliably represent real-world relationships that can be expressed as natural language questions.
- Evidence anchors:
  - [abstract] "We use QAMPARI as seed dataset which is an open-domain QA dataset with lists of entities as answers"
  - [section] "We query the WD knowledge graph to extract additional relations rext linked to ea"
  - [corpus] Weak - no direct neighbor citations about automated question extension pipelines
- Break condition: If Wikidata lacks consistent relation patterns or if template-based extensions distort question semantics.

### Mechanism 2
- Claim: Evidence extraction from multiple Wikipedia sources enables robust table answer generation.
- Mechanism: Multi-source retrieval (text, tables, infoboxes) provides comprehensive coverage for each cell in the answer table.
- Core assumption: Wikipedia articles contain sufficient, verifiable information across different formats to support complex QA.
- Evidence anchors:
  - [abstract] "supporting evidence for each data point inside the table"
  - [section] "collect for each extension triple (ea, rext, eext) supporting evidence using Wikipedia"
  - [corpus] Moderate - neighbors discuss table-text retrieval but not specifically multi-source extraction for table answers
- Break condition: If Wikipedia articles lack complete coverage for requested relations or if sources contradict each other.

### Mechanism 3
- Claim: Relative mapping similarity (RMS) metrics effectively evaluate table generation quality despite format variations.
- Mechanism: RMS treats tables as unordered collections of mappings, using normalized Levenshtein distance for text and relative distance for numbers.
- Core assumption: Table content is more important than exact formatting, and small variations in expression shouldn't penalize correctness.
- Evidence anchors:
  - [section] "We adopted a version of the relative mapping similarity (RMS) metrics introduced by Liu et al."
  - [section] "RMS views tables as unordered collections of mappings from row/column headers to values"
  - [corpus] Strong - neighbors discuss table-text QA and retrieval methods that would benefit from such metrics
- Break condition: If evaluation requires strict format adherence or if semantic equivalence cannot be captured by distance metrics.

## Foundational Learning

- Concept: Wikidata query language and triple representation
  - Why needed here: Understanding how to extract relations from Wikidata triples is fundamental to extending questions
  - Quick check question: Given entity "Q12345" and relation "P31", what does the query "P31(Q12345)" return?

- Concept: Natural language inference and evidence evaluation
  - Why needed here: Models must determine if extracted Wikipedia content supports or refutes proposed statements
  - Quick check question: Given sentence "Paris is the capital of France" and evidence "France's capital is Paris", what label should the model output?

- Concept: Table structure and answer mapping
  - Why needed here: Understanding how to convert table cells into evaluable triplets is crucial for both generation and evaluation
  - Quick check question: How would you represent cell "Ilaiyaraaja" in table with headers "Movie" and "Composer" as a triplet?

## Architecture Onboarding

- Component map: Question extension module (Wikidata queries) -> Evidence extraction pipeline (Wikipedia multi-source retrieval) -> Table generation engine (answer table construction) -> Evaluation system (RMS metrics implementation) -> Model integration layer (different model settings)

- Critical path:
  1. Load QAMPARI question and answer list
  2. Query Wikidata for extension relations
  3. Extract supporting evidence from Wikipedia
  4. Generate answer table with source attribution
  5. Evaluate using RMS metrics

- Design tradeoffs:
  - Template-based vs. free-form question extension
  - Multi-source vs. single-source evidence extraction
  - Strict vs. flexible table evaluation metrics
  - Closed-book vs. oracle vs. open-book evaluation settings

- Failure signatures:
  - Missing relations in generated tables
  - Incorrect evidence attribution
  - Format mismatches in answer tables
  - Model performance drops in closed-book setting

- First 3 experiments:
  1. Test question extension pipeline with sample QAMPARI questions and verify Wikidata triples are correctly extracted
  2. Validate evidence extraction by checking if supporting content exists for generated extension triples
  3. Evaluate RMS metrics on sample tables to ensure distance calculations work as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific adaptations were made to the RMS metric to handle TANQ's diverse table structures and text content?
- Basis in paper: explicit
- Why unresolved: The paper mentions adapting RMS for TANQ but does not detail the specific changes made to handle text content and list-valued cells.
- What evidence would resolve it: A detailed explanation of the RMS metric modifications, including handling of text variations and list-valued cells.

### Open Question 2
- Question: How does the performance of smaller models like Gemma compare to larger models when fine-tuned specifically for TANQ?
- Basis in paper: inferred
- Why unresolved: The paper notes Gemma's surprising performance but does not explore fine-tuning effects on smaller models.
- What evidence would resolve it: Experimental results comparing fine-tuned smaller models against larger models on TANQ.

### Open Question 3
- Question: What are the specific limitations of the open book ReAct-based baseline in handling complex multi-hop reasoning?
- Basis in paper: explicit
- Why unresolved: The paper identifies limitations but does not provide detailed analysis of the underlying causes in the ReAct approach.
- What evidence would resolve it: A comprehensive analysis of the ReAct baseline's failure modes with specific examples and potential improvements.

## Limitations

- Question extension relies on automated templates that may distort semantic intent
- RMS evaluation may not capture all aspects of answer quality, particularly structural correctness
- Significant performance drop in closed-book setting (43.4 F1 vs 60.7 F1) indicates benchmark design challenges

## Confidence

- Question extension mechanism: **Medium** - Pipeline described systematically but lacks direct validation
- Evidence extraction robustness: **Low-Medium** - Theoretically sound but Wikipedia coverage limitations not quantified
- RMS evaluation metrics: **High** - Well-justified design consistent with established approaches

## Next Checks

1. Conduct human evaluation of question extensions to verify semantic preservation across a sample of generated questions compared to originals.
2. Test evidence extraction pipeline with edge cases where Wikipedia coverage is known to be incomplete or contradictory to measure robustness.
3. Perform ablation study removing the automated question extension step to isolate its contribution to benchmark difficulty.