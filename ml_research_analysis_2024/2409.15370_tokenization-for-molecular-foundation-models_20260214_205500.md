---
ver: rpa2
title: Tokenization for Molecular Foundation Models
arxiv_id: '2409.15370'
source_url: https://arxiv.org/abs/2409.15370
tags:
- tokens
- tokenizers
- tokenizer
- language
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates 34 tokenizers, including 19
  chemistry-specific ones, for their coverage of the SMILES molecular representation.
  The authors introduce two new tokenizers, Smirk and Smirk-GPE, which achieve full
  coverage of the OpenSMILES specification.
---

# Tokenization for Molecular Foundation Models

## Quick Facts
- arXiv ID: 2409.15370
- Source URL: https://arxiv.org/abs/2409.15370
- Reference count: 22
- This paper systematically evaluates 34 tokenizers, including 19 chemistry-specific ones, for their coverage of the SMILES molecular representation. The authors introduce two new tokenizers, Smirk and Smirk-GPE, which achieve full coverage of the OpenSMILES specification.

## Executive Summary
This paper addresses the critical gap in tokenization coverage for molecular foundation models by evaluating 34 existing tokenizers and proposing two new solutions, Smirk and Smirk-GPE. The authors systematically demonstrate that most chemistry-specific tokenizers lack complete coverage of chemical primitives defined in the OpenSMILES specification. Through N-gram language models as a low-cost proxy, they quantify the impact of tokenizer choice on model performance and information loss from unknown tokens. The proposed Smirk tokenizers achieve full coverage while maintaining reasonable vocabulary sizes, making them suitable for large-scale molecular representation learning across pharmacology, agriculture, biology, and energy storage applications.

## Method Summary
The authors conducted a comprehensive evaluation of 34 tokenizers on 1.6 billion SMILES strings from the Enamine REAL Space database. They used N-gram language models (unigram to 5-gram) as a proxy to assess tokenizer performance through cross-entropy loss metrics. The paper introduces two new tokenizers: Smirk, which decomposes bracketed atoms into constituent glyphs to avoid vocabulary explosion, and Smirk-GPE, which uses byte-pair encoding for compression. Information loss from unknown tokens was quantified using bidirectional N-gram models by measuring KL-divergence. Coverage was systematically tested against OpenSMILES primitives including elements, isotopes, chirality, bonds, and rings.

## Key Results
- Existing chemistry tokenizers systematically lack coverage of OpenSMILES primitives, with most missing rare elements and complex chemical structures
- Smirk tokenizer achieves full coverage of OpenSMILES specification through glyph decomposition of bracketed atoms
- N-gram evaluation shows Smirk delivers nearly identical tokenization for most molecules with only marginal increases in fertility compared to existing chemistry-specific tokenizers
- Information loss from unknown tokens can be quantified using bidirectional character-level N-gram models, revealing significant information gaps in current tokenizers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smirk tokenizer avoids vocabulary explosion by decomposing bracketed atoms into constituent glyphs
- Mechanism: Instead of treating entire bracketed atoms like [Cu+3] as single tokens, Smirk splits them into individual characters [ C u + 3 ], allowing any combination of isotopes, charges, and chiral centers to be represented without needing dedicated tokens for each combination
- Core assumption: The set of glyphs used by OpenSMILES is fixed and documented, making it possible to create a complete tokenizer without learning merges from data
- Evidence anchors:
  - [abstract] "The proposed tokenizers systematically integrate nuclear, electronic, and geometric degrees of freedom"
  - [section] "Unlike existing atom-wise tokenizers, our pre-tokenization scheme consists of two steps: first, splitting the SMILES string using a regular expression similar to Schwaller et al., and then decomposing bracketed atoms using another regular expression"
  - [corpus] Weak evidence - corpus doesn't directly address tokenization mechanics
- Break condition: If OpenSMILES specification changes or if there are valid SMILES strings containing characters not in the glyph set

### Mechanism 2
- Claim: N-gram language models serve as effective low-cost proxy for assessing tokenizer performance
- Mechanism: N-gram models can estimate token likelihoods based on context without requiring full transformer pretraining, allowing researchers to evaluate tokenizer choices before expensive model training
- Core assumption: N-gram performance correlates with transformer model performance for the same tokenization scheme
- Evidence anchors:
  - [abstract] "Using N-gram language models as a low-cost proxy, they assess the impact of tokenizer choice on model performance"
  - [section] "Using linear least squares regression, we found an increase of 0.62 ± 0.02 nats per additional token of fertility"
  - [corpus] Moderate evidence - corpus shows related works using N-grams as evaluation metrics
- Break condition: If transformer models rely on long-range dependencies that N-grams cannot capture, or if pretraining corpus differs significantly from evaluation corpus

### Mechanism 3
- Claim: Information loss from unknown tokens can be quantified using bidirectional N-gram models
- Mechanism: By comparing predictions from a character-level N-gram model with and without masked tokens, researchers can measure the KL-divergence representing information lost when tokenizers emit unknown tokens
- Core assumption: Character-level N-grams provide sufficient context to estimate the impact of masking tokens that would be marked as unknown
- Evidence anchors:
  - [abstract] "quantify information loss from unknown tokens"
  - [section] "the information lost to the unknown tokens: DKL(Bn||Bn) = P B · (log Bn − log Bn)"
  - [corpus] Weak evidence - corpus doesn't contain direct measurements of information loss
- Break condition: If unknown tokens occur in patterns that character-level models cannot adequately capture, or if the relationship between information loss and task performance is non-linear

## Foundational Learning

- Concept: SMILES notation and OpenSMILES specification
  - Why needed here: Understanding the molecular representation format is essential for comprehending tokenization approaches and their coverage limitations
  - Quick check question: What are the three main components of a bracketed atom in SMILES notation?

- Concept: Byte-Pair Encoding (BPE) and subword tokenization
  - Why needed here: The paper builds on BPE concepts to create Smirk-GPE, so understanding how BPE works is crucial for grasping the compression mechanism
  - Quick check question: How does BPE differ from character-level tokenization in terms of vocabulary size and sequence length?

- Concept: N-gram language models and cross-entropy loss
  - Why needed here: The paper uses N-grams as a proxy metric, so understanding how they estimate token probabilities and compute loss is essential for interpreting results
  - Quick check question: What is the relationship between N-gram order and the model's ability to capture context?

## Architecture Onboarding

- Component map: Tokenizer implementations (Smirk, Smirk-GPE) -> N-gram evaluation -> Information loss quantification -> Performance comparison
- Critical path: Tokenization → N-gram evaluation → Information loss quantification → Performance comparison
- Design tradeoffs: Open-vocabulary coverage vs. vocabulary size, fertility vs. model performance, glyph decomposition vs. merge compression
- Failure signatures: Unknown tokens in output, vocabulary explosion, poor N-gram performance, high information loss metrics
- First 3 experiments:
  1. Implement Smirk tokenizer and verify it can tokenize all elements from the periodic table plus common isotopes and charges
  2. Run N-gram evaluation on both Smirk and an existing chemistry tokenizer to compare cross-entropy loss
  3. Measure information loss for a chemistry tokenizer by comparing character-level N-gram predictions with and without masking unknown tokens

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of tokenizer affect the performance of transformer-based models on chemical property prediction tasks compared to other factors like model architecture or training data size?
- Basis in paper: [explicit] The paper states "Using N-grams as a proxy language model, we have quantified the potential impact of unknown tokens on model performance" and shows that smirk delivers nearly identical tokenization for most molecules with a marginal increase in fertility relative to chemistry-specific tokenizers.
- Why unresolved: The paper uses N-gram language models as a proxy to assess tokenizer impact, but acknowledges this is not a perfect representation of transformer-based models. The actual impact on downstream tasks like property prediction is not directly measured.
- What evidence would resolve it: A controlled experiment comparing transformer models using different tokenizers on the same chemical property prediction tasks, controlling for other variables like model architecture and training data.

### Open Question 2
- Question: What is the optimal vocabulary size for chemistry-specific tokenizers to balance coverage and model performance?
- Basis in paper: [explicit] The paper notes that "the majority of observed tokens are far more frequent, I < 15; only a fraction of the total vocabularies are utilized" and mentions that most existing tokenizers use fewer than 100 tokens despite having larger vocabularies.
- Why unresolved: The paper does not explore the impact of vocabulary size on tokenization performance or model performance, stating "We have not explored the impact of vocabulary size on tokenization performance."
- What evidence would resolve it: Systematic experiments varying vocabulary size for a given tokenizer and measuring the impact on model performance and coverage of chemical space.

### Open Question 3
- Question: How significant is the information loss from unknown tokens in practical chemistry applications, and does it vary across different chemical domains?
- Basis in paper: [explicit] The paper quantifies information loss using KL-divergence but notes that "the information lost regarding the identity of the unknown token, which may or may not impact predictions for downstream tasks."
- Why unresolved: While the paper measures information loss from unknown tokens, it does not directly measure the impact on actual chemical property prediction or design tasks. The significance likely varies depending on the specific application and chemical domain.
- What evidence would resolve it: Comparative studies of model performance on chemistry tasks using tokenizers with varying levels of coverage, especially in domains where unknown tokens are more prevalent (e.g., organometallic chemistry).

## Limitations
- The use of N-gram language models as proxies for transformer performance may not capture the full complexity of deep molecular representation learning
- The paper lacks downstream task validation showing actual performance improvements on chemical property prediction or design tasks
- The assumption that complete OpenSMILES coverage is always desirable may not hold for domain-specific applications where vocabulary constraints could be beneficial

## Confidence
- High confidence in the systematic coverage gaps in existing tokenizers and the design principles of Smirk
- Medium confidence in the broader implications for model performance due to the proxy nature of N-gram evaluations
- Medium confidence in the information loss quantification approach and its correlation with downstream task performance

## Next Checks
1. Implement a small-scale transformer pretraining study comparing Smirk and an existing tokenizer on a molecule property prediction task to validate whether N-gram proxy metrics translate to actual performance improvements
2. Conduct ablation studies on the glyph decomposition approach by systematically varying which chemical primitives are kept as single tokens versus decomposed
3. Test the Smirk tokenizers on out-of-distribution molecular data (e.g., inorganic complexes, organometallic compounds) to verify the claimed universal coverage holds beyond the organic-focused Enamine REAL Space dataset