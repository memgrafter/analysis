---
ver: rpa2
title: 'Decomposition of surprisal: Unified computational model of ERP components
  in language processing'
arxiv_id: '2409.06803'
source_url: https://arxiv.org/abs/2409.06803
tags:
- n400
- p600
- heuristic
- surprisal
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an information-theoretic model of N400 and P600
  ERP components that decomposes word surprisal into heuristic surprise (shallow processing)
  and discrepancy signal (depth processing). The model uses GPT-2 to estimate prior
  probabilities and generates heuristic interpretations via LLM prompting.
---

# Decomposition of surprisal: Unified computational model of ERP components in language processing

## Quick Facts
- arXiv ID: 2409.06803
- Source URL: https://arxiv.org/abs/2409.06803
- Reference count: 32
- Primary result: Information-theoretic model decomposing word surprisal into heuristic surprise (predicts N400) and discrepancy signal (predicts P600) with R²=0.27-0.62 across experiments

## Executive Summary
This paper presents a unified computational model linking two major ERP components (N400 and P600) to information-theoretic quantities derived from language processing. The model decomposes word surprisal into shallow heuristic processing (predicting N400) and deeper discrepancy processing (predicting P600). Using modern NLP models like GPT-2 for probability estimation and LLM prompting for interpretation generation, the model successfully simulates ERP patterns across six psycholinguistic experiments with semantic and syntactic violations.

## Method Summary
The model uses GPT-2 to estimate prior probabilities and semantic representations, Levenshtein edit distance for form-based distance, and LLM prompting to generate heuristic interpretations. For each word, it computes heuristic surprise (KL divergence from prior to heuristic interpretation) and discrepancy signal (difference between veridical and heuristic surprisal). These quantities are then used to predict N400 and P600 amplitudes through linear relationships with word surprisal.

## Key Results
- Heuristic surprise significantly predicts N400 amplitude (t=-7.32, p<0.001 in Ryskin-21)
- Discrepancy signal significantly predicts P600 amplitude (t=2.79, p<0.01 in Ryskin-21)
- N400 and P600 sum linearly to predict surprisal (R²=0.27-0.62 across experiments)
- Model qualitatively reproduces N400 for semantic violations and P600 for syntactic violations across multiple experiments

## Why This Works (Mechanism)

### Mechanism 1
Language processing proceeds from shallow heuristic interpretations to deep veridical interpretations. The N400 reflects processing difficulty at the shallow level (heuristic surprise), while the P600 reflects additional processing required to reconcile shallow and deep interpretations (discrepancy signal). This two-stage processing framework captures the cognitive trade-off between fast, shallow processing and slower, deeper processing.

### Mechanism 2
Modern NLP models like GPT-2 can approximate human cognitive processes underlying language comprehension. GPT-2 provides prior probabilities and semantic representations that approximate human expectations, while LLM prompting generates candidate heuristic interpretations. This allows computational estimation of the information-theoretic quantities needed for the model.

### Mechanism 3
The linear relationship between surprisal, N400, and P600 (S = 1/αN400 + 1/βP600) captures how cognitive resources are allocated during language processing. Total processing effort (surprisal) is partitioned between shallow and deep processing stages, with each ERP component reflecting the difficulty of its respective stage.

## Foundational Learning

- Concept: Information Theory (KL divergence, surprisal)
  - Why needed here: The entire model is built on information-theoretic foundations, using KL divergence to measure processing depth and surprisal as total processing effort
  - Quick check question: What does KL divergence measure in the context of interpretation policies?

- Concept: Event-Related Potentials (ERPs) - N400 and P600 components
  - Why needed here: The model directly links computational quantities to specific ERP components well-established in psycholinguistics
  - Quick check question: What cognitive processes are traditionally associated with N400 and P600 components?

- Concept: Noisy Channel Models and Bayesian Inference
  - Why needed here: The interpretation policy is mathematically equivalent to Bayesian inference under certain conditions, providing a cognitive interpretation
  - Quick check question: How does the distortion metric relate to a noisy channel model of language comprehension?

## Architecture Onboarding

- Component map: GPT-2 probabilities -> Heuristic interpretation generation -> Heuristic surprise calculation -> Discrepancy signal calculation -> ERP prediction

- Critical path: For each word in context, compute GPT-2 surprisal, generate heuristic interpretation candidates, calculate heuristic surprise (KL divergence from prior to heuristic), compute discrepancy signal (difference between veridical and heuristic surprisal), then predict ERP amplitudes.

- Design tradeoffs: The model trades off accuracy of cognitive modeling against computational tractability by using modern NLP models as proxies for human processing. Alternative approaches could use human-generated interpretations but would be less scalable.

- Failure signatures: Poor predictions when GPT-2 probabilities don't match human expectations, when heuristic interpretations miss the intended meaning, or when the linear relationship between surprisal and ERPs doesn't hold.

- First 3 experiments:
  1. Run qualitative simulations on AD-98 to verify the model predicts N400 for semantic violations and P600 for syntactic violations
  2. Test the model on Kim-05 to check predictions for animacy violations with/without plausible alternatives
  3. Validate on Ito-16 to confirm the model handles form-related violations that trigger both N400 and P600 effects

## Open Questions the Paper Calls Out

### Open Question 1
Does the interpretation policy remain constant across different linguistic contexts and tasks, or does it adapt based on task demands and presentation latency? The paper states that the processing depth parameter λ "plausibly varies across experiments" but does not provide experimental evidence on how λ varies across different contexts and tasks.

### Open Question 2
How does the model account for the N400 and P600 components in naturalistic text, where errors are rare and heuristic surprise and veridical surprisal are often similar? The paper mentions that "in many cases, heuristic surprise and veridical surprisal are similar, especially so in naturalistic text where errors are rare" but does not provide a detailed explanation of how the model would handle such contexts.

### Open Question 3
What is the exact relationship between the N400 and P600 components, and how do they interact during language comprehension? The paper presents a unified framework where N400 and P600 are predicted by heuristic surprise and discrepancy signal, respectively, but does not provide a detailed mechanistic explanation of how they interact during language comprehension.

## Limitations
- Reliance on GPT-2 probabilities that may not accurately reflect human linguistic expectations, particularly for rare or complex constructions
- Dependence on LLM prompting quality and specificity for heuristic interpretation generation
- Assumption of linear relationships between surprisal components and ERP amplitudes that may not hold across all experimental conditions

## Confidence

- **High Confidence**: Qualitative simulations matching ERP patterns across multiple experiments (AD-98, Kim-05, Ito-16, Chow-16S, Chow-16R)
- **Medium Confidence**: Quantitative predictions for N400 (t=-7.32, p<0.001) and P600 (t=2.79, p<0.01) in Ryskin-21
- **Low Confidence**: Linear relationship between surprisal, N400, and P600 (S = 1/αN400 + 1/βP600) with R²=0.27-0.62

## Next Checks

1. **Cross-linguistic validation**: Test the model on ERP data from non-English languages to verify the decomposition generalizes beyond the language used in the original experiments.

2. **Temporal dynamics analysis**: Examine whether the heuristic surprise and discrepancy signal components show different time courses that align with the characteristic timing of N400 (300-500ms) and P600 (500-900ms) components.

3. **Individual differences assessment**: Investigate whether the model parameters (α, β) vary systematically across participants with different language proficiencies or cognitive abilities, providing insights into individual differences in shallow-to-deep processing trade-offs.