---
ver: rpa2
title: A Reality Check on Context Utilisation for Retrieval-Augmented Generation
arxiv_id: '2412.17031'
source_url: https://arxiv.org/abs/2412.17031
tags:
- evidence
- context
- claim
- 'true'
- 'false'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares real-world and synthetic datasets for studying
  how language models utilize context in retrieval-augmented generation. It introduces
  DRUID, a dataset of real claims with manually annotated evidence, and compares it
  to synthetic datasets (CounterFact, ConflictQA).
---

# A Reality Check on Context Utilisation for Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2412.17031
- Source URL: https://arxiv.org/abs/2412.17031
- Authors: Lovisa Hagström; Sara Vera Marjanović; Haeun Yu; Arnav Arora; Christina Lioma; Maria Maistro; Pepa Atanasova; Isabelle Augenstein
- Reference count: 40
- Key outcome: Real-world datasets (DRUID) show different context utilization patterns than synthetic datasets (CounterFact, ConflictQA), with synthetic data over-representing rare characteristics and exaggerating context-repulsion behavior.

## Executive Summary
This paper challenges the validity of synthetic datasets for studying context utilization in retrieval-augmented generation systems. Through analysis of a real-world dataset (DRUID) and comparison with synthetic datasets (CounterFact, ConflictQA), the authors demonstrate that synthetic data over-represents rare characteristics like knowledge conflicts and leads to different model behavior. Using a novel ACU score, they show that no single context property predicts utilization in real-world settings, highlighting the need for real-world-aligned datasets in RAG research.

## Method Summary
The authors introduce DRUID, a real-world dataset of 5,490 claims with manually annotated evidence from fact-checking sources. They evaluate Llama 3.1 8B and Pythia 6.9B models on DRUID and synthetic datasets (CounterFact with 20,000 samples, ConflictQA with 16,046 samples) using 3-shot and 0-shot prompts. Context utilization is measured using a novel ACU score that normalizes probabilities and scales changes by potential improvement. The study analyzes correlations between context characteristics (relevance, stance, similarity, difficulty, implicitness, external sources, reliability, uncertainty, source properties) and ACU scores across all datasets.

## Key Results
- Synthetic datasets show stronger correlations between context characteristics and ACU scores than real-world data
- Fact-checking source contexts demonstrate higher ACU scores, indicating better model utilization
- No single context characteristic predicts utilization in real-world settings
- Synthetic datasets exaggerate context-repulsion behavior observed in models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Real-world retrieved context contains greater complexity and diversity than synthetic datasets, leading to different model behavior.
- Mechanism: Synthetic datasets (CounterFact, ConflictQA) oversimplify context characteristics, causing models to learn skewed context utilization patterns that do not generalize to real-world RAG.
- Core assumption: Models learn context utilization behaviors primarily from the distribution of context properties in training data.
- Evidence anchors:
  - [abstract] "synthetic datasets often fail to represent the complexity and diversity of realistically retrieved context"
  - [section 4] "CounterFact is tailored to a specific type of context usage that is not indicative of real, retrieved context"
  - [corpus] Weak - no direct comparison of real vs synthetic in cited papers
- Break condition: If models are evaluated on real-world data early in training, or if synthetic datasets are deliberately designed to mirror real-world complexity.

### Mechanism 2
- Claim: The ACU score provides a more reliable measure of context utilization than previous methods.
- Mechanism: ACU normalizes probabilities and scales changes by potential improvement, reducing sensitivity to baseline model behavior and ensuring changes are relevant to the provided context.
- Core assumption: Previous measures failed to account for baseline model bias and context relevance.
- Evidence anchors:
  - [abstract] "our novel ACU score"
  - [section 5.2] "There is no consistent measure for context usage across similar work"
  - [section 5.2] "we introduce a novel measure (ACU), which 1) uses softmax-normalized probabilities, to ensure meaningful comparison, 2) focuses on probabilities of specific tokens, to ensure relevant change, and 3) scales these values by the amount of possible increase in probability"
- Break condition: If ACU values are not correlated with downstream task performance or if normalization fails to account for extreme baseline biases.

### Mechanism 3
- Claim: Context utilization cannot be predicted by single context characteristics in real-world settings.
- Mechanism: Real-world contexts aggregate multiple features simultaneously, making singleton characteristics insufficient predictors of model behavior.
- Core assumption: Context utilization depends on complex interactions between multiple context properties rather than isolated features.
- Evidence anchors:
  - [abstract] "correlations between singleton context properties and ACU on DRUID are surprisingly small compared to other properties related to context source"
  - [section 5.4] "While we see a limited effect of any one characteristic, we highlight overarching findings below"
  - [corpus] Weak - correlation analysis methodology not detailed in cited papers
- Break condition: If future studies identify strong predictive power for individual context characteristics or if aggregation effects can be decomposed.

## Foundational Learning

- Concept: Context utilization in RAG systems
  - Why needed here: Understanding how language models leverage retrieved context is central to the paper's contribution
  - Quick check question: What distinguishes context utilization from simple context inclusion in RAG systems?

- Concept: Synthetic vs real-world dataset design
  - Why needed here: The paper's core argument depends on understanding the limitations of synthetic datasets
  - Quick check question: How do synthetic datasets like CounterFact differ from real-world datasets like DRUID in their approach to generating evidence?

- Concept: Spearman correlation analysis
  - Why needed here: The paper uses correlation analysis to evaluate relationships between context characteristics and model behavior
  - Quick check question: What does a low Spearman correlation between a context characteristic and ACU indicate about that characteristic's predictive power?

## Architecture Onboarding

- Component map:
  - Dataset creation pipeline (claims collection → evidence retrieval → annotation)
  - Context characteristic detection system (rule-based + LLM-based methods)
  - Model evaluation framework (prompt engineering → context utilization measurement → correlation analysis)
  - ACU score calculation module

- Critical path:
  1. Claim collection from fact-checking sources
  2. Automated evidence retrieval using search engines + reranking
  3. Manual annotation for relevance and stance
  4. Context characteristic detection
  5. Model evaluation with ACU scoring
  6. Correlation analysis between characteristics and ACU

- Design tradeoffs:
  - Real vs synthetic data: Real data provides ecological validity but is expensive to annotate; synthetic data is scalable but may not generalize
  - Automated vs manual annotation: Automated methods are faster but may miss nuanced context properties
  - Prompt design: Few-shot prompts improve performance but may introduce bias; zero-shot prompts are more general but less effective

- Failure signatures:
  - High variance in ACU scores across different prompts
  - Strong correlations between context characteristics and ACU only on synthetic datasets
  - Context repulsion behavior (negative ACU) on real-world data

- First 3 experiments:
  1. Compare ACU scores for the same model on CounterFact vs DRUID with identical prompts
  2. Vary the number of shot examples in prompts and measure ACU stability
  3. Test correlation between individual context characteristics and ACU on synthetic vs real-world datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics or aggregations of features make contexts from fact-checking sources more convincing to language models?
- Basis in paper: [explicit] "Context from fact-check sources have greater ACU scores. Llama and Pythia are more likely to be faithful to refuting context from a fact-checking source."
- Why unresolved: The paper identifies this effect but does not investigate the specific linguistic or structural features that contribute to this phenomenon.
- What evidence would resolve it: Fine-grained linguistic analysis of fact-checking sources vs. other sources, identifying specific patterns (assertiveness, direct argumentation, reference density, etc.) that correlate with improved context utilization.

### Open Question 2
- Question: How generalizable are the context utilization patterns observed in DRUID to other information-seeking tasks like question-answering or document summarization?
- Basis in paper: [explicit] "Our work leverages claim verification as a vehicle for studies of realistic context utilisation... It is not fully clear whether insights related to context utilisation on this task will transfer to other RAG tasks."
- Why unresolved: The study focuses specifically on claim verification, and the authors acknowledge uncertainty about transfer to other RAG tasks.
- What evidence would resolve it: Systematic comparison of context utilization patterns across multiple RAG tasks (QA, summarization, dialogue) using the same evaluation framework.

### Open Question 3
- Question: What is the causal relationship between individual context characteristics (e.g., length, perplexity, similarity) and language model context utilization in real-world settings?
- Basis in paper: [explicit] "While we include a comprehensive correlation analysis to identify the dependence between our studied characteristics and context usage, it does not give any information about causality."
- Why unresolved: The paper only measures correlations, not causation, and the authors explicitly call for causal analysis to understand effects of different context characteristics.
- What evidence would resolve it: Experimental interventions that systematically manipulate individual context characteristics while controlling for others, measuring the direct impact on model utilization.

## Limitations

- The study focuses exclusively on claim verification tasks, limiting generalizability to other RAG applications
- The ACU score has not been validated against downstream task performance metrics
- DRUID is limited to fact-checking domains and claims annotated by human fact-checkers, which may not represent all real-world RAG scenarios

## Confidence

- **High Confidence:** The finding that synthetic datasets show different context utilization patterns than real-world data (Mechanism 1). This is directly observable through the correlation analyses and ACU score comparisons.
- **Medium Confidence:** The claim that context utilization cannot be predicted by single context characteristics (Mechanism 3). While the correlation analysis shows limited predictive power, the methodology for context characteristic detection may introduce noise that obscures real relationships.
- **Low Confidence:** The assertion that the ACU score provides a more reliable measure than previous methods (Mechanism 2). The paper does not provide comparative validation against established metrics, and the normalization approach may mask important behavioral differences.

## Next Checks

1. **Dataset Generalization Test:** Apply the same evaluation framework to DRUID-like datasets from non-fact-checking domains (e.g., medical question answering, customer support) to verify whether the synthetic vs real-world divergence persists across different application areas.

2. **ACU Correlation with Task Performance:** Measure whether ACU scores correlate with actual task performance metrics (accuracy, F1-score) on downstream RAG tasks. This would validate whether the ACU score captures meaningful utilization rather than just statistical patterns.

3. **Controlled Synthetic Dataset Creation:** Design synthetic datasets that deliberately incorporate the complexity characteristics identified in DRUID (multiple evidence sources, mixed reliability, hedging language) and test whether they produce context utilization patterns closer to real-world data.