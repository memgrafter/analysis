---
ver: rpa2
title: 'RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and
  Human-like Reasoning'
arxiv_id: '2410.16502'
source_url: https://arxiv.org/abs/2410.16502
tags:
- rulebreakers
- reasoning
- llms
- premises
- logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces RULEBREAKERS, a novel dataset for evaluating
  LLMs' ability to distinguish between rulebreakers and non-rulebreakers in natural
  language reasoning. Rulebreakers are scenarios where rigid application of formal
  logic produces conclusions inconsistent with premises based on common sense and
  factual knowledge.
---

# RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning

## Quick Facts
- **arXiv ID**: 2410.16502
- **Source URL**: https://arxiv.org/abs/2410.16502
- **Reference count**: 40
- **Primary result**: Most LLMs achieve only mediocre accuracy (best 0.609 vs 0.25 baseline) on distinguishing rulebreakers from non-rulebreakers in natural language reasoning

## Executive Summary
This study introduces RULEBREAKERS, a novel dataset designed to evaluate large language models' ability to distinguish between rulebreakers and non-rulebreakers in natural language reasoning. Rulebreakers are scenarios where rigid application of formal logic produces conclusions inconsistent with premises based on common sense and factual knowledge. The dataset comprises 25,600 instances generated using four templates covering modus tollens and disjunctive syllogism with geographical and categorical entity pairs.

Seven LLMs were evaluated, with most achieving only mediocre paired accuracy (0.25 baseline, only 0.609 best model) on distinguishing rulebreakers from non-rulebreakers. Analysis revealed that models showed better performance on non-rulebreakers than rulebreakers, suggesting a tendency to over-rigidly apply logical rules rather than utilize common sense reasoning. The findings highlight a fundamental gap between formal logic-based reasoning and human-like knowledge-informed reasoning.

## Method Summary
The study introduced RULEBREAKERS, a dataset comprising 25,600 instances generated using four templates that cover modus tollens and disjunctive syllogism with geographical and categorical entity pairs. Seven LLMs were evaluated on their ability to distinguish rulebreakers from non-rulebreakers, where rulebreakers are scenarios where rigid application of formal logic produces conclusions inconsistent with premises based on common sense and factual knowledge. The evaluation used paired accuracy as the primary metric, with a 0.25 baseline representing random chance.

## Key Results
- Seven LLMs achieved only mediocre paired accuracy on distinguishing rulebreakers from non-rulebreakers, with best model scoring 0.609 versus 0.25 baseline
- Models showed better performance on non-rulebreakers (0.6713 accuracy) than rulebreakers (0.5467 accuracy)
- Two key factors affecting performance were identified: models' familiarity with entities mentioned and attention distribution patterns

## Why This Works (Mechanism)
The dataset effectively exposes a fundamental gap in LLMs' reasoning capabilities by creating scenarios where formal logic and common sense reasoning diverge. Rulebreakers are specifically designed to test whether models can override rigid logical conclusions when they conflict with factual knowledge, revealing limitations in how LLMs balance formal reasoning with world knowledge.

## Foundational Learning
- **Modus Tollens**: If P then Q; Not Q; Therefore not P - needed to understand one logical form tested; quick check: verify basic modus tollens examples
- **Disjunctive Syllogism**: P or Q; Not P; Therefore Q - needed to understand second logical form tested; quick check: verify basic disjunctive syllogism examples
- **Rulebreakers vs Non-rulebreakers**: Understanding the distinction between logically valid conclusions and factually correct conclusions - needed to grasp the core research problem; quick check: create simple examples distinguishing the two

## Architecture Onboarding

**Component Map:**
RULEBREAKERS dataset -> LLMs (various architectures) -> Evaluation metrics (paired accuracy) -> Analysis (entity familiarity, attention patterns)

**Critical Path:**
Dataset generation -> Model inference -> Accuracy calculation -> Performance analysis -> Interpretation of results

**Design Tradeoffs:**
- Controlled templates ensure consistency but may limit generalizability
- Focus on specific logical forms allows deep analysis but may miss other reasoning patterns
- Entity-based design reveals knowledge gaps but may conflate knowledge with reasoning ability

**Failure Signatures:**
- Consistent underperformance on rulebreakers versus non-rulebreakers
- Performance correlation with entity familiarity
- Attention patterns that prioritize logical structure over factual consistency

**3 First Experiments:**
1. Test models on rulebreaker detection after augmenting their knowledge bases
2. Create control dataset with logically valid but factually incorrect rulebreakers
3. Ablation study with entities models should know well versus unfamiliar entities

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Findings may not generalize beyond the four specific templates used
- Study focuses on only two logical forms (modus tollens and disjunctive syllogism)
- Doesn't directly test whether models fail due to logical rigidity or knowledge retrieval issues

## Confidence
- **High** confidence in dataset's effectiveness for core finding that LLMs struggle to distinguish rulebreakers from non-rulebreakers
- **Medium** confidence in generalizability beyond the four specific templates used
- **Low** confidence in claim that models over-rigidly apply logical rules rather than use common sense

## Next Checks
1. Conduct ablation studies testing models with entities they should know well (e.g., common animals, everyday objects) versus unfamiliar entities to isolate whether the issue is logical reasoning capability or knowledge gaps.

2. Test the same models on rulebreaker detection after augmenting their knowledge bases or using retrieval-augmented generation to determine if performance improves with better entity knowledge.

3. Design a control dataset with rulebreakers that are logically valid but factually incorrect, to determine if models fail due to logical rigidity or factual knowledge deficits.