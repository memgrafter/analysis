---
ver: rpa2
title: 'ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models'
arxiv_id: '2407.12877'
source_url: https://arxiv.org/abs/2407.12877
tags:
- evaluation
- peer
- prompt
- response
- refer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReFeR, a hierarchical framework that uses
  multiple LLMs/VLMs as peer reviewers and a more capable model as an area chair to
  evaluate generative outputs and solve reasoning tasks. It outperforms baselines
  on four NLG evaluation, two multimodal evaluation, and four reasoning datasets.
---

# ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models

## Quick Facts
- **arXiv ID**: 2407.12877
- **Source URL**: https://arxiv.org/abs/2407.12877
- **Reference count**: 40
- **Primary result**: Hierarchical framework using multiple LLMs as peer reviewers and a stronger model as area chair outperforms baselines on NLG evaluation, multimodal evaluation, and reasoning tasks, with a Lite variant being ~7.7× faster while maintaining comparable accuracy

## Executive Summary
ReFeR introduces a hierarchical framework that mimics academic peer review by using multiple smaller language models as peer reviewers to evaluate generative outputs, with a more capable model synthesizing their feedback as an area chair. The framework demonstrates state-of-the-art performance on four NLG evaluation datasets, two multimodal evaluation datasets, and four reasoning tasks. Two variants are presented: ReFeR-Turbo, which averages multiple area chair responses for higher accuracy, and ReFeR-Lite, which generates a single response for greater efficiency while maintaining comparable accuracy.

## Method Summary
ReFeR employs a two-level hierarchy where multiple peer models independently evaluate outputs using a structured prompt schema, then a more capable area chair model synthesizes these peer reviews to provide the final evaluation. The framework includes an auto-prompt generation mechanism that creates evaluation guidelines from dataset examples, though manual guidelines can also be used. The two variants differ in response generation: Turbo uses n=20 area chair responses averaged for higher accuracy, while Lite uses n=1 response for efficiency, achieving ~7.7× speedup while maintaining comparable accuracy.

## Key Results
- Outperforms baselines on four NLG evaluation datasets (TopicalChat, SummEval)
- Achieves state-of-the-art performance on two multimodal evaluation datasets (ICQD, AGIQA)
- Shows strong performance on four reasoning datasets (AQua, BBH-DU, CSQA, GSM8k)
- ReFeR-Lite variant maintains comparable accuracy to Turbo while being ~7.7× faster

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical peer review improves evaluation accuracy by leveraging diverse perspectives and synthesis
- Mechanism: Multiple smaller models evaluate outputs independently, then a stronger model synthesizes their feedback, mirroring academic peer review
- Core assumption: Diverse evaluators provide more comprehensive assessment, and stronger models can effectively synthesize peer feedback
- Evidence anchors:
  - [abstract]: "By using multiple LLMs or VLMs as evaluators and feedback providers in a system akin to academic peer review"
  - [section 2.1]: "The ReFeR framework consists of two main modules... Peer Review Body... Area chair Evaluation"
- Break condition: If peer models are too weak or similar, or if area chair cannot synthesize conflicting feedback

### Mechanism 2
- Claim: Auto-prompt generation improves evaluation consistency by providing structured evaluation guidelines
- Mechanism: Evaluation Guidelines component in prompt structure provides clear criteria for scoring, automatically generated by prompting a language model
- Core assumption: Clear, consistent evaluation criteria improve model performance on evaluation tasks
- Evidence anchors:
  - [section 2.2]: "To further refine this approach, we introduce a new module in the evaluation schema called Evaluation Guidelines"
  - [section 4.1]: Performance improvements shown across multiple evaluation tasks
- Break condition: If auto-generated guidelines are inconsistent or manual guidelines are unavailable

### Mechanism 3
- Claim: Two variants balance performance and efficiency by adjusting response generation
- Mechanism: Turbo generates multiple area chair responses for averaging, while Lite generates single response for efficiency
- Core assumption: Multiple responses improve accuracy through averaging, but single response can be sufficient for similar performance
- Evidence anchors:
  - [abstract]: "ReFeR-Turbo, which averages multiple area chair responses for higher accuracy, and ReFeR-Lite, which generates a single response for greater efficiency, being ~7.7× faster while maintaining comparable accuracy"
  - [section 2.3]: Detailed description of both variants and their differences
- Break condition: If averaging doesn't improve accuracy or single response is insufficient

## Foundational Learning

- Concept: Multi-agent collaboration and hierarchical decision-making
  - Why needed here: Understanding how multiple agents with different roles (peers vs area chair) work together to achieve better outcomes
  - Quick check question: What is the key difference between how peers and area chairs operate in this framework?

- Concept: Prompt engineering and evaluation schema design
  - Why needed here: Framework relies heavily on structured prompts with evaluation guidelines, steps, and forms
  - Quick check question: How does the Evaluation Guidelines component differ from the Evaluation Steps component in the prompt structure?

- Concept: Correlation metrics and evaluation methodology
  - Why needed here: Framework evaluated using Spearman's ρ and Kendall's τ correlations with human judgments
  - Quick check question: Why might Kendall's tau be particularly sensitive to tied predictions in this framework?

## Architecture Onboarding

- Component map: Peer Review Body (multiple smaller models) → Area Chair (larger/more capable model) → Final Score/Feedback
- Critical path: Input → Peer Evaluation → Area Chair Synthesis → Final Score/Feedback
- Design tradeoffs:
  - Model selection: Balancing capability vs cost for peers and area chair
  - Response generation: Multiple responses improve accuracy but increase cost
  - Prompt complexity: More detailed prompts improve consistency but may reduce flexibility
- Failure signatures:
  - Low correlation with human judgments despite multiple models
  - Area chair produces inconsistent or illogical syntheses
  - Performance degrades significantly when switching from Turbo to Lite
- First 3 experiments:
  1. Run individual peer evaluations on a small test set to establish baseline performance
  2. Test area chair synthesis with varying numbers of peer responses (n=1, 5, 10, 20)
  3. Compare Turbo vs Lite variants on a subset of the evaluation datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ReFeR change when using different numbers of area chairs (e.g., having multiple area chairs debate the final decision)?
- Basis in paper: [inferred] Current framework uses single area chair; impact of multiple area chairs or debate mechanism unexplored
- Why unresolved: Current framework uses single area chair; impact of multiple area chairs or debate mechanism unexplored
- What evidence would resolve it: Comparative experiments showing correlation with human judgments when using different configurations

### Open Question 2
- Question: What is the optimal balance between peer model diversity and performance for maximizing evaluation quality?
- Basis in paper: [explicit] Paper conducts peer ablation studies but exploring all combinations was impractical
- Why unresolved: While increasing peers generally improves performance, optimal trade-off between diversity and marginal gains unexplored
- What evidence would resolve it: Systematic experiments varying peer model combinations across different dimensions

### Open Question 3
- Question: How does ReFeR perform on other modalities beyond text and images, such as video, audio, or 3D content?
- Basis in paper: [explicit] Framework untested in other modalities; remains untested in other modalities
- Why unresolved: Framework's architecture could theoretically extend to other modalities, but empirical testing and identification of modality-specific challenges missing
- What evidence would resolve it: Implementation and evaluation on video quality assessment, audio quality evaluation, or 3D model evaluation tasks

## Limitations
- Framework effectiveness depends heavily on relative capabilities of area chair compared to peer reviewers, with performance degradation when using weaker models as area chairs
- Auto-prompt generation mechanism lacks detailed documentation on how evaluation guidelines are created from dataset examples
- Framework shows significantly lower correlations on skewed datasets like SummEval, suggesting limitations in handling imbalanced evaluation scenarios

## Confidence
- **High confidence**: Hierarchical evaluation architecture and existence of two distinct variants (Turbo and Lite) with stated performance differences
- **Medium confidence**: Claimed performance improvements over baselines are supported by reported correlations, though magnitude varies across datasets
- **Low confidence**: Specific mechanisms of auto-prompt generation and detailed impact of prompt schema components on model performance lack sufficient empirical validation

## Next Checks
1. **Area Chair Capability Threshold**: Systematically test framework with varying capability gaps between area chair and peer reviewers to determine minimum threshold for effective synthesis

2. **Auto-prompt Generation Robustness**: Conduct controlled experiments comparing auto-generated evaluation guidelines versus manually created ones across different task domains

3. **Dataset Skew Impact Analysis**: Perform detailed analysis on how dataset imbalance affects framework's performance, including experiments with balanced versions of skewed datasets