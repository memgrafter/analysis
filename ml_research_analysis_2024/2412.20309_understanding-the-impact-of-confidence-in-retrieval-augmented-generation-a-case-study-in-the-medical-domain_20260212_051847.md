---
ver: rpa2
title: 'Understanding the Impact of Confidence in Retrieval Augmented Generation:
  A Case Study in the Medical Domain'
arxiv_id: '2412.20309'
source_url: https://arxiv.org/abs/2412.20309
tags:
- pre-q
- aft-q
- aft-c
- confidence
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Retrieval Augmented Generation
  (RAG) improves confidence calibration in large language models for medical domain
  question answering. The researchers evaluated nine models across PubMedQA and MedMCQA
  datasets by analyzing predicted probabilities when inserting relevant or irrelevant
  documents at different positions in the input prompt.
---

# Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain

## Quick Facts
- arXiv ID: 2412.20309
- Source URL: https://arxiv.org/abs/2412.20309
- Reference count: 34
- Key outcome: This study investigates whether Retrieval Augmented Generation (RAG) improves confidence calibration in large language models for medical domain question answering. The researchers evaluated nine models across PubMedQA and MedMCQA datasets by analyzing predicted probabilities when inserting relevant or irrelevant documents at different positions in the input prompt. Results show that certain models (Phi-3.5 and Qwen) can discern whether retrieved documents are relevant to the correct answer, with relevant documents improving confidence and irrelevant documents having minimal impact. Models like Llama and Gemma exhibited unexpected behavior, suggesting they struggle to effectively incorporate inserted documents. The study demonstrates that analyzing output probabilities reveals which models function best as generators in RAG systems and provides insights for improving RAG reliability in high-stakes applications.

## Executive Summary
This study investigates whether Retrieval Augmented Generation (RAG) improves confidence calibration in large language models for medical domain question answering. The researchers evaluated nine models across PubMedQA and MedMCQA datasets by analyzing predicted probabilities when inserting relevant or irrelevant documents at different positions in the input prompt. The study reveals that certain models (Phi-3.5 and Qwen) can discern whether retrieved documents are relevant to the correct answer, with relevant documents improving confidence and irrelevant documents having minimal impact. Models like Llama and Gemma exhibited unexpected behavior, suggesting they struggle to effectively incorporate inserted documents.

## Method Summary
The researchers evaluated nine large language models (Phi-3.5, Qwen2.5, Llama2, Llama3.1, Gemma2, PMC-Llama, Meditron) on PubMedQA and MedMCQA datasets using a probability-based evaluation framework. Documents were synthetically inserted at three positions in the prompt (before question, after question, after choices) to simulate RAG conditions. The study analyzed model output probabilities for confidence calibration using entropy, best probability, accuracy, and Adaptive Calibration Error (ACE) metrics to assess how models respond to relevant versus irrelevant documents.

## Key Results
- Phi-3.5 and Qwen2.5 models demonstrated ideal behavior, improving confidence with relevant documents (entropy dropped from 0.933 to 0.051 for Phi-3.5) and maintaining confidence with irrelevant documents
- Llama and Gemma models exhibited unexpected behavior, with some showing decreased accuracy when relevant documents were inserted
- Different models showed varying effectiveness in incorporating inserted documents, suggesting model architecture influences RAG performance
- Probability-based evaluation revealed which models function best as generators in RAG systems, with implications for high-stakes applications like medical question answering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG improves confidence calibration when retrieved documents are relevant to the correct answer.
- Mechanism: When a relevant document is inserted, models like Phi-3.5 and Qwen adjust their output probabilities to reflect higher confidence in the correct answer choice. This is evidenced by lower entropy and higher best probability values in these models.
- Core assumption: The model can effectively incorporate external information when it is directly relevant to the answer.
- Evidence anchors:
  - [abstract]: "Results show that certain models (Phi-3.5 and Qwen) can discern whether retrieved documents are relevant to the correct answer, with relevant documents improving confidence"
  - [section]: "When explicitly inserting documents that contain the correct answers, Phi and Qwen demonstrated ideal behavior. For instance, from a correct entropy perspective in Table 3, Phi had a value of 0.933 under the w/o RAG setting, which dropped to 0.051 after document insertion."
  - [corpus]: Weak - no direct corpus evidence for this specific mechanism
- Break condition: The mechanism breaks if the model cannot effectively process inserted documents, as seen with Llama and Gemma models.

### Mechanism 2
- Claim: Models can discriminate between relevant and irrelevant documents, adjusting confidence accordingly.
- Mechanism: When irrelevant documents are inserted, models like Phi and Qwen maintain or decrease their confidence, showing they can identify when external information is not useful. This is demonstrated by minimal impact on confidence when irrelevant documents are inserted.
- Core assumption: The model has sufficient capacity to evaluate the relevance of retrieved documents and adjust its confidence accordingly.
- Evidence anchors:
  - [abstract]: "Models like Llama and Gemma exhibited unexpected behavior, suggesting they struggle to effectively incorporate inserted documents"
  - [section]: "Furthermore, when they determine that the document is unnecessary, they attempt to answer using their own knowledge."
  - [corpus]: Weak - no direct corpus evidence for this specific mechanism
- Break condition: The mechanism breaks if the model treats all inserted documents equally, regardless of relevance.

### Mechanism 3
- Claim: Different models exhibit varying behaviors in confidence calibration, influenced by model architecture and parameter size.
- Mechanism: While Phi and Qwen show ideal behavior, Llama and Gemma models exhibit unexpected results, suggesting that model architecture and training data play a significant role in how effectively a model can use RAG to improve confidence calibration.
- Core assumption: Model architecture and training data influence the ability to effectively use RAG for confidence calibration.
- Evidence anchors:
  - [abstract]: "Models like Llama and Gemma exhibited unexpected behavior, suggesting they struggle to effectively incorporate inserted documents"
  - [section]: "Notably, even when inserting entirely correct documents (Ans1), Llama3.1 (70B) experiences a drop in accuracy, whereas Llama3.1 (8B) shows improved accuracy even when inserting completely unrelated documents (Oth3)."
  - [corpus]: Weak - no direct corpus evidence for this specific mechanism
- Break condition: The mechanism breaks if all models, regardless of architecture, behave similarly in confidence calibration.

## Foundational Learning

- Concept: Probability distribution and entropy
  - Why needed here: Understanding how entropy changes with the insertion of relevant or irrelevant documents is crucial for evaluating confidence calibration.
  - Quick check question: How does entropy relate to model confidence, and what happens to entropy when a model is more certain about an answer?

- Concept: Calibration error metrics (ACE and ECE)
  - Why needed here: These metrics are used to evaluate whether a model's predicted probabilities align with actual accuracy, which is essential for assessing confidence calibration.
  - Quick check question: What is the difference between ACE and ECE, and why is ACE preferred for multi-class classification problems?

- Concept: Retrieval Augmented Generation (RAG) architecture
  - Why needed here: Understanding how RAG works and how documents are inserted at different positions in the prompt is crucial for analyzing the impact on confidence.
  - Quick check question: What are the different positions where documents can be inserted in a RAG prompt, and how might this affect the model's performance?

## Architecture Onboarding

- Component map: PubMedQA/MedMCQA datasets -> Multiple-choice questions -> Document insertion (before question, after question, after choices) -> Model processing -> Output probabilities -> Confidence calibration metrics (entropy, best probability, accuracy, ACE)
- Critical path: Document insertion → Model processing → Probability generation → Metric calculation → Confidence assessment
- Design tradeoffs: Using probability-based evaluation instead of free-text generation avoids dependencies on specific evaluation metrics and regular expressions, but may limit the depth of analysis compared to more qualitative approaches.
- Failure signatures: Unexpected behavior in confidence calibration, such as decreased accuracy when relevant documents are inserted or minimal impact on confidence when irrelevant documents are inserted, indicates potential issues with the model's ability to process and utilize retrieved information.
- First 3 experiments:
  1. Evaluate the baseline confidence of each model without any inserted documents.
  2. Insert relevant documents and measure the change in confidence and accuracy for each model.
  3. Insert irrelevant documents and measure the change in confidence and accuracy for each model.

## Open Questions the Paper Calls Out
None

## Limitations
- The study uses synthetic RAG scenarios where documents are programmatically inserted into prompts rather than using actual retrieval systems, potentially missing real-world retrieval noise and ranking complexity
- The methodology for selecting "irrelevant documents" is not clearly specified, with documents chosen from other questions without detailed selection criteria
- The analysis focuses on probability distributions rather than free-form generation, limiting understanding of how confidence calibration translates to actual answer quality in open-ended scenarios

## Confidence

**High Confidence Claims:**
- Certain models (Phi-3.5 and Qwen) demonstrate measurable improvements in confidence calibration when relevant documents are inserted
- Models exhibit varying behaviors in how they process and utilize inserted documents
- Entropy and best probability metrics effectively capture confidence calibration changes

**Medium Confidence Claims:**
- The ability to discern document relevance correlates with better RAG performance
- Model architecture and parameter size influence effectiveness in RAG scenarios
- Probability-based evaluation provides reliable insights into RAG effectiveness

**Low Confidence Claims:**
- The specific mechanisms by which models discriminate document relevance are fully understood
- These findings directly translate to real-world RAG systems with actual retrieval components
- The observed behaviors generalize across all medical domain question answering tasks

## Next Checks
1. **Real Retrieval Validation**: Implement actual retrieval components to test whether synthetic document insertion results hold when using real document retrieval systems with ranking and noise characteristics.

2. **Cross-Dataset Generalization**: Validate findings on additional medical datasets beyond PubMedQA and MedMCQA, particularly datasets with different question formats and answer structures to assess robustness.

3. **Open-Ended Generation Test**: Extend the analysis to free-form generation tasks where models must generate answers rather than select from multiple choices, testing whether probability-based confidence calibration translates to generation quality.