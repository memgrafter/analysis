---
ver: rpa2
title: Does Instruction Tuning Make LLMs More Consistent?
arxiv_id: '2404.15206'
source_url: https://arxiv.org/abs/2404.15206
tags:
- consistency
- different
- paraphrases
- language
- cosine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether instruction tuning improves semantic
  consistency in large language models (LLMs). The authors compare 10 instruction-tuned
  LLaMA variants against the base LLaMA-7b model using three evaluation methods: vector
  space consistency (via cosine distances on paraphrase datasets), prediction consistency
  in factual knowledge probes and downstream tasks, and mechanistic analysis of factual
  recall.'
---

# Does Instruction Tuning Make LLMs More Consistent?

## Quick Facts
- arXiv ID: 2404.15206
- Source URL: https://arxiv.org/abs/2404.15206
- Reference count: 25
- One-line primary result: Instruction tuning improves semantic consistency in LLMs across vector space, prediction, and mechanistic measures

## Executive Summary
This paper investigates whether instruction tuning improves semantic consistency in large language models by comparing 10 instruction-tuned LLaMA variants against the base LLaMA-7b model. The authors evaluate consistency using three complementary methods: vector space consistency via cosine distances on paraphrase datasets, prediction consistency in factual knowledge probes and downstream tasks, and mechanistic analysis of factual recall processes. Across all measures, instruction-tuned models show significantly greater consistency than their base counterparts. The improvements are consistent across different instruction datasets and also observed in T5 and Falcon models, demonstrating that instruction tuning generally enhances robustness to semantically irrelevant input variations.

## Method Summary
The paper evaluates consistency improvements by comparing instruction-tuned LLaMA models against the base model across three measures. First, vector space consistency is measured by computing cosine distances between representations of paraphrased and non-paraphrased inputs using MRPC, TAPACO, and PARA REL datasets. Second, prediction consistency is assessed through MMLU accuracy spread across different instructions, PARA REL factual consistency comparing top-1 predictions, and BECEL downstream consistency under paraphrasing and negation. Third, mechanistic analysis examines the factual recall process by measuring subject attribute recall rates, relation encoding similarity, and attribute extraction rates in final layers. The study uses 10 different instruction datasets to train the models and evaluates their consistency improvements across various tasks and architectures.

## Key Results
- Instruction-tuned models show significantly greater semantic consistency than base models across all evaluation metrics
- Vector representations become more semantically coherent, with paraphrases encoded closer together than non-paraphrases
- Instruction tuning improves subject attribute retrieval and extraction rates in final layers, correlating positively with factual consistency
- Improvements are consistent across different instruction datasets and observed in LLaMA, T5, and Falcon model families

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning improves semantic consistency by making vector representations more coherent, with paraphrases encoded closer together and non-paraphrases farther apart.
- Mechanism: IFT introduces diverse surface forms encoding the same behavior, which encourages the model to develop more semantically aligned representations. The model learns to ignore irrelevant input variations while preserving semantic meaning.
- Core assumption: The instruction datasets contain multiple paraphrases of the same semantic content, forcing the model to learn invariant representations.
- Evidence anchors:
  - [abstract] "instruction-tuned models show significantly greater consistency. Vector representations become more semantically coherent, with paraphrases encoded more closely together than non-paraphrases."
  - [section] "We find that IFT models exhibit bigger relative differences in all the datasets evaluated, both LLaMA models as well as T5 and Falcon IFT counterparts."
- Break condition: If instruction datasets lack sufficient paraphrase diversity or contain mostly task-specific instructions without semantic variation.

### Mechanism 2
- Claim: Instruction tuning improves factual consistency by enhancing subject attribute recall in the early layers of the model.
- Mechanism: IFT improves the enrichment of subject representations with related attributes during factual knowledge recall. The model becomes better at retrieving and encoding attributes associated with subjects before processing the relation information.
- Core assumption: The attribute recall process occurs in early layers and significantly impacts final prediction consistency.
- Evidence anchors:
  - [abstract] "instruction tuning improves subject attribute retrieval and extraction rates in the final layers, which correlates positively with factual consistency."
  - [section] "We find that LLaMA has lower attributes rate than all IFT models, thus showing that IFT improves the recall of subject's attributes across-the-board."
- Break condition: If attribute recall improvements don't translate to better predictions due to downstream extraction failures.

### Mechanism 3
- Claim: Instruction tuning improves consistency when the multi-head self-attention (MHSA) extraction mechanism in later layers remains effective.
- Mechanism: IFT models show higher extraction rates when the MHSA layers successfully extract the predicted attribute from the enriched subject and relation representations. Models with deteriorated MHSA extraction after IFT show lower consistency despite improved attribute recall.
- Core assumption: The extraction mechanism in later layers is crucial for translating improved representations into consistent predictions.
- Evidence anchors:
  - [abstract] "instruction tuning improves subject attribute retrieval and extraction rates in the final layers, which correlates positively with factual consistency."
  - [section] "We compute the Pearson correlation between the IFT models' extraction rates and theirPARA REL consistency scores, and we find significant (p-value <= 0.05) positive correlations of 0.65-0.92 in layers 13-21."
- Break condition: If MHSA extraction rates are high but consistency remains low, suggesting other failure points in the pipeline.

## Foundational Learning

- Concept: Vector space consistency and cosine similarity
  - Why needed here: The paper's primary consistency metric compares how closely paraphrases are encoded versus non-paraphrases using cosine distances between vector representations.
  - Quick check question: What does a higher cosine distance between paraphrases versus non-paraphrases indicate about model consistency?

- Concept: Factual knowledge recall mechanisms
  - Why needed here: Understanding the three-way process (subject enrichment, relation propagation, attribute extraction) is crucial for interpreting the mechanistic analysis of why IFT improves consistency.
  - Quick check question: In the factual recall process, at which stage does instruction tuning primarily improve consistency according to the paper?

- Concept: Zero-shot learning evaluation
  - Why needed here: The paper evaluates consistency improvements on zero-shot tasks (MMLU) and factual probes, requiring understanding of how models perform without task-specific fine-tuning.
  - Quick check question: How does the paper account for prompting variability when evaluating zero-shot consistency?

## Architecture Onboarding

- Component map: Input representation layer → transformer blocks (up to 30 layers) → output projection layer → fine-tuning adaptation modules that modify weight distributions based on instruction data
- Critical path: For consistency evaluation: input → representation layer → transformer blocks → output layer → consistency metric computation. For factual recall: input → subject enrichment (early layers) → relation processing (middle layers) → attribute extraction (late layers) → prediction
- Design tradeoffs: Instruction tuning trades general language modeling capability for task-following ability. The paper shows this tradeoff improves consistency but may reduce performance on some tasks. The choice of instruction dataset significantly impacts consistency outcomes.
- Failure signatures: Low consistency despite instruction tuning indicates either insufficient paraphrase diversity in the instruction dataset or deterioration of the extraction mechanism in later layers. High spread values across different paraphrases suggest the model hasn't learned semantic invariance.
- First 3 experiments:
  1. Replicate the cosine distance analysis comparing paraphrases vs non-paraphrases across different instruction-tuned models to verify representational consistency improvements.
  2. Implement the attribute extraction rate analysis on factual recall tasks to measure whether improved subject attribute recall correlates with consistency.
  3. Conduct the MMLU spread analysis by evaluating models across different instructions and shuffling orders to quantify zero-shot consistency improvements.

## Open Questions the Paper Calls Out

- Question: Does instruction tuning improve semantic consistency across different language families or is the effect primarily observed in English?
  - Basis in paper: [inferred] The paper focuses exclusively on English instruction tuning datasets and evaluation, with no mention of multilingual consistency effects
  - Why unresolved: The paper only evaluates LLaMA, T5, and Falcon models on English datasets (MRPC, TAPACO, PARA REL, MMLU, BECEL), leaving open whether the consistency improvements generalize to other languages
  - What evidence would resolve it: Testing instruction-tuned models across multiple language families using cross-lingual paraphrase datasets and factual knowledge probes would reveal whether the consistency improvements transfer

- Question: What is the relationship between instruction tuning dataset size and the degree of consistency improvement?
  - Basis in paper: [explicit] The paper uses 10 different instruction datasets of varying sizes (from 15k to 210k examples) but doesn't analyze the correlation between dataset size and consistency gains
  - Why unresolved: While the paper shows that instruction tuning generally improves consistency, it doesn't investigate whether larger instruction datasets produce proportionally larger consistency improvements or if there's a diminishing returns effect
  - What evidence would resolve it: Controlled experiments training instruction-tuned models on datasets of systematically varying sizes while measuring consistency metrics would establish whether dataset size predicts consistency improvements

- Question: Do instruction-tuned models maintain their consistency advantages when evaluated on adversarial or out-of-distribution prompts?
  - Basis in paper: [inferred] The paper evaluates consistency using relatively standard datasets and prompts, without testing model robustness to intentionally misleading or semantically ambiguous inputs
  - Why unresolved: The current evaluation shows instruction-tuned models are more consistent on typical inputs, but doesn't reveal whether this advantage persists when models encounter prompts designed to exploit weaknesses in their instruction-following behavior
  - What evidence would resolve it: Evaluating instruction-tuned models against adversarial prompt sets containing subtle semantic inconsistencies, misleading instructions, or edge-case scenarios would determine if consistency gains are robust or superficial

## Limitations
- The mechanistic analysis relies on proxy measurements (cosine distances, attribute recall rates) rather than direct observation of internal representations
- The correlation between improved attribute recall and consistency, while statistically significant, doesn't establish causation
- The study focuses primarily on LLaMA models with only brief validation on T5 and Falcon, limiting generalizability claims

## Confidence

**High confidence** in the core finding that instruction tuning improves semantic consistency: Multiple independent measures (vector space analysis, factual consistency, downstream task consistency) converge on this conclusion across different model families.

**Medium confidence** in the mechanistic explanation: While the paper provides a plausible three-stage model of how instruction tuning improves consistency, the evidence is primarily correlational rather than causal.

**Medium confidence** in generalizability: The results are convincing for LLaMA-family models, but broader claims about all LLMs require more diverse architectural validation.

## Next Checks
1. Conduct ablation studies removing specific instruction dataset components to isolate which aspects (paraphrase diversity, task instructions, format variations) drive consistency improvements.
2. Perform causal intervention experiments by directly modifying subject attribute representations in early layers and measuring downstream consistency changes to validate the mechanistic hypothesis.
3. Test consistency improvements across a broader range of model architectures (GPT-family, Claude, Gemini) and instruction tuning approaches to assess generalizability beyond LLaMA and similar decoder-only transformers.