---
ver: rpa2
title: 'A-Bench: Are LMMs Masters at Evaluating AI-generated Images?'
arxiv_id: '2406.03070'
source_url: https://arxiv.org/abs/2406.03070
tags:
- lmms
- quality
- aigis
- arxiv
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces A-Bench, the first diagnostic benchmark for
  evaluating Large Multimodal Models (LMMs) on AI-generated image (AIGI) assessment.
  The key innovation lies in shifting focus from evaluating LMM-based metrics to directly
  assessing the LMMs themselves on fundamental semantic understanding and quality
  perception tasks.
---

# A-Bench: Are LMMs Masters at Evaluating AI-generated Images?

## Quick Facts
- **arXiv ID:** 2406.03070
- **Source URL:** https://arxiv.org/abs/2406.03070
- **Reference count:** 40
- **Primary result:** LMMs lag significantly behind human performance on AIGI evaluation, with a 14.62% gap compared to top LMM (Qwen2-VL-72B)

## Executive Summary
This paper introduces A-Bench, the first diagnostic benchmark for evaluating Large Multimodal Models (LMMs) on AI-generated image (AIGI) assessment. The key innovation shifts focus from evaluating LMM-based metrics to directly assessing LMMs themselves on fundamental semantic understanding and quality perception tasks. Testing 23 prominent LMMs reveals that even the best LMMs lag significantly behind human performance, particularly struggling with nuanced semantic understanding and quality perception tasks like identifying generative distortions.

## Method Summary
A-Bench comprises 2,864 AIGIs from 16 text-to-image models, each paired with human-annotated question-answer pairs spanning high-level semantic understanding and low-level quality perception. The evaluation tests LMMs using zero-shot assessment on Yes-or-No and What questions, with GPT-assisted validation of responses. The benchmark is divided into two key diagnostic subsets: A-BenchP 1→high-level semantic understanding, and A-BenchP 2→low-level quality perception, covering basic recognition, bag-of-words pitfalls discrimination, outside knowledge realization, technical quality perception, aesthetic quality evaluation, and generative distortion assessment.

## Key Results
- LMMs excel at basic recognition tasks but struggle with nuanced semantic understanding and quality perception
- The best-performing LMM (Qwen2-VL-72B) shows a 14.62% gap compared to human performance
- LMMs are particularly weak at identifying generative distortions and handling complex prompts
- Performance varies significantly across different subcategories of AIGI evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A-Bench's focus on direct LMM assessment rather than LMM-based metric evaluation improves diagnostic accuracy of AIGI assessment capabilities.
- **Mechanism:** By shifting from evaluating LMM-based metrics (e.g., SRCC/PLCC) to directly testing whether LMMs can answer fundamental perceptual questions, A-Bench exposes specific weaknesses in semantic understanding and quality perception that indirect metrics might obscure.
- **Core assumption:** The fundamental perceptual questions represent the core basis of all LMM-based evaluators and are therefore more diagnostic than metric performance alone.
- **Evidence anchors:** [abstract] "We introduce A-Bench in this paper, a benchmark designed to diagnose whether LMMs are masters at evaluating AIGIs." [section] "We move away from computing SRCC/PLCC criteria and instead examine whether the fundamental perceptual questions can be correctly answered, which is the core basis of all LMM-based evaluators."

### Mechanism 2
- **Claim:** Comprehensive sampling across diverse AIGI sources ensures robust evaluation of LMM capabilities across different generation models and quality scenarios.
- **Mechanism:** By using AIGIs from 16 different text-to-image models and covering a wide quality spectrum through uniform sampling, A-Bench tests LMMs across diverse generation characteristics and quality distributions that they would encounter in real-world applications.
- **Core assumption:** LMM performance varies significantly across different AIGI sources and quality levels, and testing across this diversity reveals more complete capability profiles.
- **Evidence anchors:** [abstract] "Various generative models are utilized for AIGI creation, and various LMMs are employed for evaluation, which ensures a comprehensive validation scope." [section] "For evaluating low-level quality perception, we employ uniform sampling to encompass a wide spectrum of visual quality and the corresponding quality distributions are illustrated in Fig 3 (b) and (c)."

### Mechanism 3
- **Claim:** The diagnostic approach reveals specific capability gaps that can guide targeted improvements in LMM AIGI evaluation.
- **Mechanism:** By subdividing semantic understanding into Basic Recognition, Bag-of-Words Pitfalls Discrimination, and Outside Knowledge Realization, and quality perception into Technical Quality Perception, Aesthetic Quality Evaluation, and Generative Distortion Assessment, A-Bench identifies precise areas where LMMs fail, enabling focused research and development.
- **Core assumption:** Identifying specific subcategories of failure provides more actionable insights for improvement than aggregate performance measures.
- **Evidence anchors:** [abstract] "We then define two key diagnostic subsets: A-BenchP 1→high-level semantic understanding, and A-BenchP 2→low-level quality perception." [section] "The detailed focused aspects can be overviewed in Fig. 3 (a)."

## Foundational Learning

- **Concept:** Multi-modal evaluation benchmarks
  - **Why needed here:** Understanding how benchmarks like COCO Caption, GQA, and OK-VQA work provides context for why A-Bench's approach differs and what gaps it fills in AIGI evaluation.
  - **Quick check question:** What is the key limitation of existing multi-modal benchmarks that A-Bench addresses in the context of AIGI evaluation?

- **Concept:** Text-to-image generation characteristics
  - **Why needed here:** Knowledge of how different T2I models (SD1.4, SD1.5, SDXL, DALLE2, DALLE3, etc.) generate AIGIs and their common failure modes helps understand why diverse AIGI sources are crucial for comprehensive evaluation.
  - **Quick check question:** Why would using AIGIs from only one T2I model be insufficient for evaluating LMM AIGI assessment capabilities?

- **Concept:** Quality assessment methodologies
  - **Why needed here:** Understanding traditional IQA/IAA methods and their limitations with AIGIs provides context for why new approaches like A-Bench are necessary.
  - **Quick check question:** What specific quality aspects of AIGIs are traditional IQA methods unable to assess, according to the paper?

## Architecture Onboarding

- **Component map:** AIGI collection from 16 T2I models → Human expert annotation of question-answer pairs → LMM testing (zero-shot) → Performance analysis → Diagnostic insights
- **Critical path:** Prompt collection → AIGI generation → Quality sampling and selection → Human expert annotation → LMM testing (zero-shot) → Performance analysis → Diagnostic insights
- **Design tradeoffs:** Manual annotation ensures quality but limits scale; comprehensive sampling ensures diversity but increases complexity; direct LMM testing provides diagnostic insights but requires extensive computation
- **Failure signatures:** LMMs consistently failing on specific subcategories indicates capability gaps; poor performance across all subcategories suggests fundamental limitations; variance across different AIGI sources indicates sensitivity to generation characteristics
- **First 3 experiments:**
  1. Test a single LMM on a small subset of AIGIs from one T2I model to verify basic functionality of the evaluation pipeline
  2. Run the full evaluation on a single LMM across all AIGIs to validate end-to-end benchmarking process
  3. Compare performance of one open-source and one proprietary LMM on the same AIGI subsets to identify capability differences

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the performance of LMMs on AIGI evaluation compare to humans when evaluating AIGIs generated by newer, more advanced text-to-image models not included in A-Bench?
  - **Basis in paper:** [explicit] The paper states that the A-Bench dataset is constructed using 16 text-to-image models, and the performance of LMMs is evaluated on this dataset. However, it does not explore the generalizability of the results to newer models.
  - **Why unresolved:** The paper does not investigate the performance of LMMs on AIGIs generated by newer models that were not part of the A-Bench dataset. This limits the generalizability of the findings to the latest advancements in text-to-image generation.
  - **What evidence would resolve it:** Conducting a similar evaluation of LMMs on AIGIs generated by newer text-to-image models would provide evidence of the generalizability of the A-Bench results.

- **Open Question 2:** What is the impact of varying the complexity of prompts on the performance of LMMs in AIGI evaluation?
  - **Basis in paper:** [explicit] The paper discusses the Bag-of-Words Pitfalls Discrimination subcategory, which tests the ability of LMMs to handle complex prompts with rich descriptive attributes or complex object relationships. However, it does not explore the impact of varying prompt complexity on overall performance.
  - **Why unresolved:** The paper does not systematically investigate how different levels of prompt complexity affect the performance of LMMs in AIGI evaluation. This leaves open the question of whether LMMs can effectively handle increasingly complex prompts.
  - **What evidence would resolve it:** Conducting experiments with AIGIs generated from prompts of varying complexity levels would provide evidence of the impact of prompt complexity on LMM performance.

- **Open Question 3:** How do the performance results of LMMs on AIGI evaluation change when considering AIGIs with different quality levels?
  - **Basis in paper:** [explicit] The paper mentions that the A-Bench dataset includes AIGIs with varying quality levels, but it does not analyze how the performance of LMMs differs across these quality levels.
  - **Why unresolved:** The paper does not provide insights into how the performance of LMMs varies when evaluating AIGIs of different quality levels. This is important for understanding the robustness of LMMs in real-world scenarios where AIGI quality can vary widely.
  - **What evidence would resolve it:** Analyzing the performance of LMMs on subsets of the A-Bench dataset with different quality levels would provide evidence of how quality affects LMM performance.

## Limitations

- The evaluation relies on human-annotated question-answer pairs, which introduces potential subjectivity in question formulation and answer validation
- The zero-shot evaluation methodology may not fully capture the potential of LMMs when given task-specific fine-tuning or prompt engineering
- The benchmark's focus on fundamental perceptual questions may not directly translate to real-world AIGI evaluation performance

## Confidence

- **High Confidence:** The methodology for collecting diverse AIGIs from 16 text-to-image models and the overall framework for evaluating LMMs on fundamental perceptual questions are well-established and clearly described.
- **Medium Confidence:** The claim that LMMs significantly lag behind human performance is supported by the results, but the absolute magnitude of the performance gap and its implications for real-world applications require further validation.
- **Medium Confidence:** The assertion that LMMs struggle with nuanced semantic understanding and quality perception is supported by subcategory analysis, but the specific reasons for these struggles (architecture limitations vs. evaluation approach) are not fully explored.

## Next Checks

1. **Cross-Validation with Alternative Annotation Methods:** Re-evaluate a subset of AIGIs using multiple human annotators and different question formulations to assess the robustness of the human ground truth and identify potential subjectivity in the annotation process.

2. **Controlled Fine-Tuning Experiments:** Take the best-performing LMMs and evaluate their performance on A-Bench after fine-tuning on task-specific AIGI evaluation datasets to determine whether zero-shot limitations are masking LMM capabilities.

3. **Real-World Application Testing:** Apply the top LMMs to actual AIGI evaluation tasks in practical applications (e.g., content moderation, quality control in creative workflows) and compare their performance against the A-Bench diagnostic results to validate the benchmark's predictive value.