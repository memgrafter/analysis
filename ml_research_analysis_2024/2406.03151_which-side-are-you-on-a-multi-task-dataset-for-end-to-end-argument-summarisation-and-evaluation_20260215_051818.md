---
ver: rpa2
title: Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation
  and Evaluation
arxiv_id: '2406.03151'
source_url: https://arxiv.org/abs/2406.03151
tags:
- task
- evidence
- human
- argument
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-task dataset for end-to-end argument
  summarization and evaluation, covering claim and evidence identification, evidence
  convincingness ranking, argumentative essay summarization, and automated evaluation
  of generated essays. The dataset contains 14k examples with full annotations for
  all tasks and evaluates multiple generative baselines including representative LLMs.
---

# Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation and Evaluation

## Quick Facts
- arXiv ID: 2406.03151
- Source URL: https://arxiv.org/abs/2406.03151
- Reference count: 39
- This paper introduces a multi-task dataset for end-to-end argument summarization and evaluation, covering claim and evidence identification, evidence convincingness ranking, argumentative essay summarization, and automated evaluation of generated essays.

## Executive Summary
This paper introduces a multi-task dataset for end-to-end argument summarization and evaluation, covering claim and evidence identification, evidence convincingness ranking, argumentative essay summarization, and automated evaluation of generated essays. The dataset contains 14k examples with full annotations for all tasks and evaluates multiple generative baselines including representative LLMs. While LLMs show promising results on individual tasks, their end-to-end performance significantly deteriorates when processing all four tasks in succession, both in automated measures and human evaluation. This challenge presented by the proposed dataset motivates future research on end-to-end argument mining and summarization systems.

## Method Summary
The authors create a multi-task dataset for end-to-end argument summarization by collecting 14k examples from open forums, news, and existing datasets. They annotate these examples for four tasks: evidence detection (Task 1), evidence convincingness ranking (Task 2), argumentation summary generation (Task 3A), and summary quality evaluation (Task 4). They evaluate multiple baselines including FlanT5, BERT, RoBERTa, and various LLMs on both individual tasks and end-to-end pipeline performance. The tasks are formulated as classification problems for Tasks 1 and 4, contrastive learning for Task 2, and generation tasks for Task 3A, with Task 3B using human preference ranking.

## Key Results
- LLMs show promising results on individual argument mining tasks but their end-to-end performance significantly deteriorates when processing all four tasks in succession
- The dataset covers 14k examples with full annotations for all four tasks
- Human evaluation confirms that while generated summaries are of high quality, the end-to-end pipeline faces significant challenges
- Multiple generative baselines including FlanT5, BERT, RoBERTa, and various LLMs are evaluated across all tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific fine-tuning improves performance on individual argument mining tasks.
- Mechanism: Training FlanT5, BERT, and RoBERTa models on task-specific labeled data (Tasks 1, 2, 3, 4) enables them to learn task-specific patterns and representations, leading to better performance compared to zero-shot inference.
- Core assumption: The proposed dataset provides sufficient and high-quality labeled examples for each task to enable effective fine-tuning.
- Evidence anchors:
  - [abstract] "We evaluate multiple generative baselines for each of these tasks, including representative LLMs."
  - [section] "All classification models were trained for 15 epochs (batch size of 64 (large), 32 (3B) and 8 (11B) respectively)"
  - [corpus] Weak evidence - no specific results on individual task performance after fine-tuning.
- Break condition: Insufficient labeled data for fine-tuning, or the tasks require more complex reasoning than the models can capture.

### Mechanism 2
- Claim: Contrastive learning can effectively rank evidence convincingness.
- Mechanism: Siamese Neural Networks with contrastive loss functions learn to embed evidence and claims in a shared space, where the distance between embeddings reflects the convincingness of the evidence for the claim.
- Core assumption: The semantic similarity between evidence and claim embeddings correlates with the convincingness of the evidence.
- Evidence anchors:
  - [abstract] "Task 2 selects the appropriate evidence for each claim to make it the most persuasive"
  - [section] "We formulate Tasks 2 and 3 as a contrastive learning task. Conceptually, we consider the evidence closest to the claim in the semantic embedding space as the most supporting candidate for it."
  - [corpus] Weak evidence - no specific results on evidence ranking performance.
- Break condition: The semantic similarity does not capture the convincingness of the evidence, or the evidence requires external knowledge not present in the embeddings.

### Mechanism 3
- Claim: LLMs can generate argumentative summaries aligned with human preferences.
- Mechanism: LLMs like GPT-3.5, GPT-4, and Bard generate summaries by selecting and arranging claims and evidence in a coherent narrative, guided by their pre-training on large text corpora.
- Core assumption: The pre-training of LLMs on diverse text data enables them to generate coherent and persuasive arguments.
- Evidence anchors:
  - [abstract] "We use LLMs (including GPT-4 (OpenAI, 2023), GPT-3.5*, GPT-3 (Brown et al., 2020), Bard† (Anil et al., 2023), LlaMA-65B (Touvron et al., 2023), GLM-130B (Zeng et al., 2022),Vicuna-13B‡,Alpaca-13B§ and BLOOM-176B (Scao et al., 2022)) to automatically generate the summaries"
  - [section] "The input of the model is in the form of a concatenated pair<stance, topic, claim 1, evidence 1, evidence 2,..., claim 2, ....>. The output will be a single summary."
  - [corpus] Weak evidence - no specific results on summary generation quality.
- Break condition: The pre-training data does not cover argumentative writing, or the LLMs struggle to select and arrange evidence coherently.

## Foundational Learning

- Concept: Argument mining
  - Why needed here: Understanding the structure and components of arguments (claims, evidence, premises, conclusions) is crucial for designing and evaluating the proposed tasks.
  - Quick check question: What are the key components of an argument, and how do they relate to each other?

- Concept: Fine-tuning large language models
  - Why needed here: The proposed approach relies on fine-tuning pre-trained models (FlanT5, BERT, RoBERTa) on task-specific data to achieve good performance.
  - Quick check question: What are the key steps and considerations in fine-tuning a pre-trained language model on a new task?

- Concept: Contrastive learning
  - Why needed here: The evidence convincingness ranking task is formulated as a contrastive learning problem, requiring an understanding of Siamese networks and contrastive loss functions.
  - Quick check question: How does contrastive learning work, and what are its advantages and limitations compared to other ranking approaches?

## Architecture Onboarding

- Component map:
  - Task 1: Evidence Detection - FlanT5, BERT, RoBERTa classification models
  - Task 2: Evidence Convincingness Ranking - Siamese Neural Networks with contrastive loss
  - Task 3A: Argumentation Summary - FlanT5, PEGASUS summarization models
  - Task 3B: Summary Ranking - Bradley-Terry model and Item Response Theory
  - Task 4: Summary Quality Checking - FlanT5, BERT, RoBERTa classification models

- Critical path: Task 1 → Task 2 → Task 3A → Task 3B → Task 4 (end-to-end pipeline)
  - Design tradeoff: Individual task performance vs. end-to-end pipeline performance
  - Failure signatures: Degradation in end-to-end performance due to error propagation from earlier tasks
  - First 3 experiments:
    1. Evaluate individual task performance with golden inputs
    2. Evaluate end-to-end pipeline performance with golden inputs
    3. Evaluate end-to-end pipeline performance with predicted inputs from earlier tasks

- Design tradeoffs:
  - Task formulation: Classification vs. ranking vs. generation
  - Model choice: Pre-trained vs. fine-tuned vs. custom architectures
  - Data collection: Expert annotation vs. crowdsourcing vs. LLM generation

- Failure signatures:
  - Individual task performance degrades significantly with noisy inputs
  - End-to-end pipeline performance much lower than individual task performance
  - Generated summaries lack coherence, persuasiveness, or faithfulness to the input claims and evidence

- First 3 experiments:
  1. Evaluate individual task performance with golden inputs
  2. Evaluate end-to-end pipeline performance with golden inputs
  3. Evaluate end-to-end pipeline performance with predicted inputs from earlier tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of large language models (LLMs) on individual tasks compare to their end-to-end performance when processing all four tasks in succession?
- Basis in paper: The paper explicitly states that while LLMs show promising results on individual tasks, their end-to-end performance significantly deteriorates when processing all four tasks in succession.
- Why unresolved: The paper provides a benchmark but does not explore the specific reasons for the performance drop in end-to-end processing, such as cascading errors or task-specific challenges.
- What evidence would resolve it: A detailed analysis of error propagation across tasks and a comparison of task-specific vs. end-to-end performance metrics would clarify the underlying causes.

### Open Question 2
- Question: To what extent do the human evaluations of machine-generated summaries correlate with automated metrics like ROUGE and BLEURT?
- Basis in paper: The paper mentions that human evaluations show high quality of generating summaries and that there is a correlation between human evaluation and Rouge/BLEURT scores, but it does not quantify the strength of this correlation.
- Why unresolved: The paper provides correlation coefficients but does not explore the implications of these correlations for the reliability of automated metrics in evaluating argument quality.
- What evidence would resolve it: A more detailed statistical analysis of the correlation between human evaluations and automated metrics, including potential limitations and biases, would provide a clearer picture of their reliability.

### Open Question 3
- Question: How do the different settings (e.g., Best-Evi, Top2-Evi, All-Evi) impact the performance of models in generating debate scripts?
- Basis in paper: The paper mentions that the performance of models varies depending on the evidence settings, with the All-Evi setting performing best, but it does not provide a detailed analysis of why this is the case.
- Why unresolved: The paper does not explore the specific challenges posed by each setting or the strategies models use to handle different evidence inputs.
- What evidence would resolve it: A comparative analysis of model performance across settings, including error analysis and insights into model behavior, would help understand the impact of evidence settings on debate script generation.

## Limitations

- The evaluation relies heavily on LLM-generated summaries for both training and evaluation, which may introduce circularity and bias
- The human evaluation component lacks sufficient detail regarding rater selection, training, and inter-rater reliability
- The dataset size (14k examples) may be insufficient for training large language models, especially for the end-to-end pipeline

## Confidence

- **High confidence**: The dataset creation methodology and task definitions are clearly specified. The formulation of individual tasks (claim/evidence identification, convincingness ranking, summarization, and evaluation) follows established argument mining frameworks.

- **Medium confidence**: The reported performance degradation in end-to-end pipeline evaluation is plausible given the cascading nature of multi-task systems, but the exact magnitude and causes are not fully explored.

- **Low confidence**: The paper claims to motivate future research, but the specific gaps that need addressing and the unique challenges presented by this dataset versus existing argument mining datasets are not clearly articulated.

## Next Checks

1. **Individual task performance analysis**: Evaluate and report baseline model performance on each individual task separately before end-to-end evaluation to establish performance ceilings and identify which tasks contribute most to pipeline degradation.

2. **Error propagation analysis**: Conduct ablation studies by replacing predicted outputs from earlier tasks with golden labels at each stage of the pipeline to quantify how much performance degradation is due to error propagation versus task complexity.

3. **Dataset quality validation**: Conduct an independent analysis of the dataset's annotation quality and diversity, including inter-annotator agreement scores, topic coverage analysis, and bias assessment across different domains and perspectives.