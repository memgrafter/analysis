---
ver: rpa2
title: Amortizing intractable inference in diffusion models for vision, language,
  and control
arxiv_id: '2405.20971'
source_url: https://arxiv.org/abs/2405.20971
tags:
- diffusion
- posterior
- learning
- training
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of amortized sampling from intractable
  posteriors when a diffusion model serves as the prior. It introduces Relative Trajectory
  Balance (RTB), a data-free training objective that learns a diffusion posterior
  sampler by matching the product of prior trajectory density and constraint reward
  to the posterior trajectory density.
---

# Amortizing intractable inference in diffusion models for vision, language, and control

## Quick Facts
- arXiv ID: 2405.20971
- Source URL: https://arxiv.org/abs/2405.20971
- Authors: Siddarth Venkatraman; Moksh Jain; Luca Scimeca; Minsu Kim; Marcin Sendera; Mohsin Hasan; Luke Rowe; Sarthak Mittal; Pablo Lemos; Emmanuel Bengio; Alexandre Adam; Jarrid Rector-Brooks; Yoshua Bengio; Glen Berseth; Nikolay Malkin
- Reference count: 40
- One-line primary result: Introduces Relative Trajectory Balance (RTB) for learning diffusion posterior samplers, achieving state-of-the-art results on D4RL benchmarks while matching or outperforming existing fine-tuning methods in generative modeling tasks.

## Executive Summary
This paper introduces Relative Trajectory Balance (RTB), a data-free training objective for learning diffusion posterior samplers that can handle intractable constraints across vision, language, and control domains. RTB enables diffusion models to sample from posterior distributions of the form p(x) âˆ p_Î¸(x)r(x), where p_Î¸ is a pretrained diffusion prior and r(x) is an arbitrary constraint function. The method generalizes to discrete diffusion models and autoregressive language models, offering both on-policy and off-policy training options for improved exploration and mode coverage.

## Method Summary
The method trains a diffusion posterior sampler to match the product of prior trajectory density and constraint reward to the posterior trajectory density via the RTB objective. The approach involves fine-tuning a pretrained diffusion prior with RTB loss using trajectories sampled from either the current posterior model (on-policy) or an exploration strategy like replay buffers (off-policy). The RTB objective is derived from the generative flow network perspective and ensures asymptotically unbiased sampling when the prior satisfies trajectory balance.

## Key Results
- Achieves state-of-the-art performance on D4RL benchmarks for reinforcement learning control tasks
- Matches or outperforms existing fine-tuning methods like DPO and DDPO on text-to-image generation with Stable Diffusion
- Demonstrates effective classifier-guided image generation and discrete diffusion infilling for language tasks
- Shows successful generalization to discrete diffusion models and autoregressive language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RTB learns a diffusion posterior sampler that matches the product of prior trajectory density and constraint reward to the posterior trajectory density.
- Mechanism: RTB minimizes the squared log-ratio between the product of a scalar normalizing constant, the posterior diffusion process, and the constraint, versus the product of the constraint and the prior diffusion process, for every denoising trajectory.
- Core assumption: The prior diffusion model satisfies trajectory balance (TB) with respect to some target density.
- Evidence anchors:
  - [abstract]: "We state and prove the asymptotic correctness of a data-free learning objective, relative trajectory balance, for training a diffusion model that samples from this posterior"
  - [section]: "If ð‘ ðœƒ, ð‘post ðœ™ , and the scalar ð‘ ðœ™ jointly satisfy the relative trajectory balance (RTB) constraint ð‘ ðœ™ Â· ð‘post ðœ™ (x0, xÎ”ð‘¡ , . . . , x1) = ð‘Ÿ (x1) ð‘ ðœƒ (x0, xÎ”ð‘¡ , . . . , x1) for every denoising trajectoryx0 â†’ xÎ”ð‘¡ â†’ Â· Â· Â· â†’x1, then ð‘post ðœ™ (x1) âˆ ð‘ ðœƒ (x1)ð‘Ÿ (x1)"
  - [corpus]: Weak evidence; no direct corpus mention of RTB, but related work on diffusion posterior sampling exists.
- Break condition: The prior diffusion model does not satisfy TB, or the constraint function is not positive.

### Mechanism 2
- Claim: RTB can be optimized off-policy, allowing efficient exploration and mode coverage.
- Mechanism: Trajectories for training can be sampled from any distribution, not just the current posterior model, enabling exploration strategies like replay buffers or noised trajectories from high-reward samples.
- Core assumption: Off-policy training does not introduce significant bias when using the RTB objective.
- Evidence anchors:
  - [abstract]: "Experiments illustrate the broad potential of unbiased inference of arbitrary posteriors under diffusion priors"
  - [section]: "This offers two advantages over on-policy simulation-based methods: (1) the ability to optimize LRTB as an off-policy objective, i.e., sampling trajectories for training from a distribution different from ð‘post ðœ™ itself"
  - [corpus]: Weak evidence; no direct corpus mention of RTB off-policy training, but related work on off-policy methods for diffusion models exists.
- Break condition: Off-policy training introduces significant bias that outweighs the benefits of exploration.

### Mechanism 3
- Claim: RTB generalizes to arbitrary sequential generative processes, including discrete diffusion models and autoregressive models.
- Mechanism: The RTB objective can be applied to any Markovian sequential generative process by replacing Gaussian transition densities with appropriate transition probability masses or trivial backward processes.
- Core assumption: The sequential generative process can be formulated as a generative flow network.
- Evidence anchors:
  - [abstract]: "Our methods generalize to discrete diffusion models and extend existing methods for autoregressive language models"
  - [section]: "While our discussion was focused on diffusion models for continuous spaces, the RTB objective can be applied to any Markovian sequential generative process, in particular, one that can be formulated as a generative flow network"
  - [corpus]: Weak evidence; no direct corpus mention of RTB generalization, but related work on GFlowNets and discrete diffusion models exists.
- Break condition: The sequential generative process cannot be formulated as a generative flow network.

## Foundational Learning

- Concept: Diffusion models as hierarchical generative models
  - Why needed here: Understanding the Markovian generative process and the connection between trajectory balance and likelihood maximization is crucial for grasping RTB.
  - Quick check question: What is the relationship between the KL divergence between distributions over trajectories and the data log-likelihood under the model?

- Concept: Generative flow networks (GFlowNets)
  - Why needed here: GFlowNets provide the theoretical foundation for RTB and the interpretation of diffusion models as flow networks.
  - Quick check question: How does the trajectory balance constraint ensure that a forward process samples from a target distribution?

- Concept: Off-policy reinforcement learning
  - Why needed here: Understanding off-policy training methods is essential for leveraging RTB's ability to learn from trajectories sampled from different distributions.
  - Quick check question: What are the advantages and disadvantages of off-policy training compared to on-policy training?

## Architecture Onboarding

- Component map:
  - Prior diffusion model (ð‘ ðœƒ) -> Posterior diffusion model (ð‘post ðœ™) -> Constraint function (ð‘Ÿ (x)) -> Normalizing constant model (log ð‘ ðœ™) -> Training algorithm (RTB optimization)

- Critical path:
  1. Initialize prior diffusion model
  2. Initialize posterior diffusion model as a copy of the prior
  3. Sample trajectories from any distribution
  4. Compute RTB loss for each trajectory
  5. Update posterior model parameters using gradient descent

- Design tradeoffs:
  - Off-policy vs. on-policy training: Off-policy training allows for more efficient exploration but may introduce bias.
  - Stochastic subsampling vs. full trajectory gradients: Stochastic subsampling reduces memory usage but increases gradient variance.
  - LoRA fine-tuning vs. full model fine-tuning: LoRA is more parameter-efficient but may limit the expressiveness of the posterior model.

- Failure signatures:
  - Mode collapse: The posterior model fails to capture all modes of the target distribution.
  - High variance in gradients: The RTB objective may have high variance, leading to unstable training.
  - Poor mode coverage: The posterior model may not adequately explore the space of possible trajectories.

- First 3 experiments:
  1. Toy 2D Gaussian mixture model: Visualize the learned posterior distribution and compare it to the true posterior.
  2. MNIST classifier-guided image generation: Evaluate the quality and diversity of generated images using metrics like FID and classifier accuracy.
  3. Text-to-image generation with Stable Diffusion: Fine-tune a pretrained text-to-image model using RTB and compare the results to existing methods like DPOK and DDPO.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the asymptotic correctness of Relative Trajectory Balance (RTB) degrade with finite discretization steps in practice?
- Basis in paper: [explicit] The paper proves asymptotic correctness of RTB but acknowledges that guarantees on error incurred by imperfect fit of the prior model, amortization, and time discretization have not been obtained.
- Why unresolved: The theoretical analysis focuses on the continuous-time limit, while practical implementations necessarily use discrete-time approximations. The relationship between discretization error and sampling quality remains unclear.
- What evidence would resolve it: Empirical studies varying the number of diffusion steps while measuring the gap between RTB-generated samples and the true posterior distribution, potentially using metrics like KL divergence or Wasserstein distance.

### Open Question 2
- Question: Can RTB be extended to handle non-differentiable reward functions effectively?
- Basis in paper: [inferred] The paper primarily discusses differentiable constraints and mentions that RTB can handle "arbitrary black-box likelihoods," but focuses on differentiable constraints in experiments and the Langevin inductive bias formulation.
- Why unresolved: While the theoretical framework suggests RTB could work with non-differentiable rewards, practical implementation details and performance characteristics for such cases are not explored.
- What evidence would resolve it: Experiments applying RTB to tasks with non-differentiable reward functions (e.g., discrete classification tasks or reinforcement learning environments with sparse rewards) and comparing performance to differentiable cases.

### Open Question 3
- Question: What is the optimal balance between on-policy and off-policy training trajectories for RTB?
- Basis in paper: [explicit] The paper discusses the flexibility of mixing on-policy training with off-policy exploration but does not provide systematic analysis of the trade-offs or guidelines for when each approach is preferable.
- Why unresolved: While off-policy training is mentioned as beneficial for mode coverage, the paper doesn't explore how the ratio of on-policy to off-policy samples affects convergence speed, sample efficiency, or mode coverage in different problem settings.
- What evidence would resolve it: Controlled experiments varying the proportion of on-policy vs. off-policy training data across different problem domains and measuring the impact on convergence rate, mode coverage, and sample efficiency.

## Limitations
- Theoretical correctness relies on the trajectory balance property of the prior diffusion model, which is not universally satisfied
- Off-policy training benefits lack rigorous analysis of potential bias introduction
- Cross-domain generalization claims are theoretically sound but empirically validated only for discrete diffusion models

## Confidence
- Theoretical framework (High): The derivation of RTB from GFlowNet principles and the proof of asymptotic correctness are mathematically sound and well-grounded in existing literature.
- Empirical validation (Medium): Experiments show competitive performance across diverse tasks, but direct comparisons to specialized methods in each domain are limited.
- Generalization claims (Medium): While the framework is theoretically applicable to discrete diffusion and autoregressive models, empirical validation is only shown for discrete diffusion.

## Next Checks
1. Test RTB on a diffusion model that does not satisfy trajectory balance to evaluate robustness when core assumptions are violated.
2. Compare off-policy vs on-policy training with controlled bias measurements across different exploration strategies.
3. Apply RTB to a non-Gaussian sequential generative process (e.g., normalizing flow) to verify cross-domain generalization claims.