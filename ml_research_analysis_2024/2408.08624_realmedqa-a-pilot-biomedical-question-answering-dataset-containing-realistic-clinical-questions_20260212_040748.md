---
ver: rpa2
title: 'RealMedQA: A pilot biomedical question answering dataset containing realistic
  clinical questions'
arxiv_id: '2408.08624'
source_url: https://arxiv.org/abs/2408.08624
tags:
- questions
- question
- realmedqa
- pairs
- bioasq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RealMedQA is a biomedical question answering dataset containing
  realistic clinical questions and verified guideline-based answers. Questions were
  generated by medical students and a large language model (LLM), then verified by
  medical professionals.
---

# RealMedQA: A pilot biomedical question answering dataset containing realistic clinical questions

## Quick Facts
- arXiv ID: 2408.08624
- Source URL: https://arxiv.org/abs/2408.08624
- Reference count: 0
- Key outcome: RealMedQA dataset contains realistic clinical questions with verified guideline-based answers; LLM generation is more cost-efficient than human generation

## Executive Summary
RealMedQA is a biomedical question answering dataset containing realistic clinical questions and verified guideline-based answers derived from NICE clinical guidelines. Questions were generated by medical students and a large language model (LLM), then verified by medical professionals. The study demonstrates that LLM-generated QA pairs are more cost-efficient than human-generated ones, and that RealMedQA has lower lexical similarity between questions and answers compared to existing datasets like BioASQ, making it more challenging for question answering models. The dataset aims to provide a realistic benchmark for evaluating QA systems intended for clinical settings where questions reflect actual clinician information needs rather than general biomedical comprehension.

## Method Summary
The study employed a three-stage process: (1) Data collection from NICE guidelines, extracting recommendations from the "Conditions and diseases" category, (2) Question generation by humans (up to 5 questions per recommendation) and GPT-3.5 Turbo LLM (up to 20 questions per recommendation), and (3) Verification by medical students using Likert scale ratings for plausibility and answerability. The verification process used Cohen's kappa to measure inter-rater agreement, and "ideal" QA pairs were identified as those rated completely plausible and completely answered. The dataset was then evaluated using multiple QA models (BM25, BERT variants, Contriever) with metrics like recall@k, MAP@k, and nDCG@k.

## Key Results
- LLM-generated QA pairs are more cost-efficient than human-generated pairs for creating "ideal" questions
- RealMedQA exhibits lower lexical similarity between questions and answers compared to BioASQ
- Contriever outperforms other models on RealMedQA due to the lower lexical similarity between questions and answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated QA pairs are more cost-efficient than human-generated ones
- Mechanism: LLMs can rapidly generate large volumes of candidate questions at near-zero marginal cost, while human generators require paid labor for each question
- Core assumption: The cost of LLM API calls is negligible compared to human labor costs, and verification cost dominates total cost for LLM-generated pairs
- Evidence anchors:
  - Table 4 shows the costs of human question generators versus LLM generation
  - The study demonstrates that LLM is more cost-efficient for generating "ideal" QA pairs
  - Corpus neighbors discuss LLM use in biomedical QA but don't directly address cost-efficiency comparison

### Mechanism 2
- Claim: RealMedQA has lower lexical similarity between questions and answers than BioASQ
- Mechanism: RealMedQA questions reflect actual clinical information needs while BioASQ questions often derive from article titles and abstracts
- Core assumption: Clinical guideline-based questions require more abstract reasoning about patient care than comprehension-style questions, reducing lexical overlap
- Evidence anchors:
  - Example comparing "What considerations should be addressed while revising the management programme for a child or adolescent?" from RealMedQA with BioASQ questions
  - The study achieves lower lexical similarity between questions and answers than BioASQ
  - Corpus neighbors discuss biomedical QA but don't directly compare lexical similarity between datasets

### Mechanism 3
- Claim: Contriever outperforms other models on RealMedQA due to lower lexical similarity
- Mechanism: When questions and answers have low lexical overlap, dense vector representations can capture semantic similarity better than sparse methods like BM25
- Core assumption: Contriever's contrastive learning approach is better suited for semantic matching when lexical features are less informative
- Evidence anchors:
  - Contriever consistently outperforms BM25 on RealMedQA while matching performance on BioASQ
  - Improved performance of Contriever on RealMedQA implies lower lexical similarity between questions and answers
  - Corpus neighbors discuss biomedical QA models but don't specifically analyze Contriever's performance relative to lexical similarity

## Foundational Learning

- Concept: Information retrieval vs. extractive QA
  - Why needed here: RealMedQA focuses on retrieving guideline recommendations rather than extracting answers from text spans
  - Quick check question: What is the key difference between information retrieval and extractive QA in terms of what the model needs to output?

- Concept: Contrastive learning for dense retrieval
  - Why needed here: Contriever uses contrastive loss to learn embeddings that capture semantic similarity
  - Quick check question: How does contrastive learning help dense retrieval models handle semantic similarity when lexical overlap is low?

- Concept: Cohen's kappa for inter-rater agreement
  - Why needed here: The verification process uses Cohen's kappa to measure agreement between medical students
  - Quick check question: What does a Cohen's kappa value of 0.22 indicate about the agreement between verifiers for LLM-generated questions?

## Architecture Onboarding

- Component map: NICE API extraction -> Human/LLM question generation -> Medical student verification -> QA model evaluation (BM25, BERT, Contriever) with recall@k, MAP@k, nDCG@k metrics
- Critical path: The bottleneck is the verification step where medical students must manually assess each QA pair for plausibility and answerability
- Design tradeoffs: Using LLM generation increases volume but requires more verification; using only human generation ensures quality but is expensive and slow; the current hybrid approach balances these factors
- Failure signatures: Low inter-rater agreement indicates unclear verification criteria; poor model performance on RealMedQA suggests the dataset may be too challenging or questions/answers are poorly matched
- First 3 experiments:
  1. Run the same evaluation pipeline on a small subset of RealMedQA with different verification criteria to see if inter-rater agreement improves
  2. Test a fine-tuned biomedical model on RealMedQA to see if domain adaptation helps with the semantic matching challenge
  3. Generate a new set of questions using a more recent LLM (like GPT-4) and compare verification costs and agreement to the original LLM results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between LLM-generated and human-generated questions for maximizing both quality and cost-efficiency in biomedical QA datasets?
- Basis in paper: The paper compares LLM and human question generation costs and yields, showing LLMs are more cost-efficient but humans have higher quality output
- Why unresolved: The paper shows a trade-off but doesn't determine the optimal ratio or method for combining both approaches
- What evidence would resolve it: Comparative studies testing different ratios of LLM-to-human questions, measuring both quality metrics and cost-efficiency

### Open Question 2
- Question: How can inter-verifier agreement be improved for evaluating whether LLM-generated questions are adequately answered by recommendations?
- Basis in paper: The paper notes substantial disagreement among verifiers, especially for LLM-generated questions, and identifies lack of clarity in what constitutes "completely" answered
- Why unresolved: The paper identifies the problem but doesn't propose standardized evaluation criteria or training methods for verifiers
- What evidence would resolve it: Development and testing of standardized rubrics, training protocols, or automated agreement metrics that improve consistency

### Open Question 3
- Question: Would retrieval-augmented generation (RAG) workflows outperform the current retrieval-only approach in providing clinically useful answers?
- Basis in paper: The paper mentions RAG as a potential improvement but focuses only on retrieval of guideline recommendations
- Why unresolved: The paper doesn't implement or compare RAG against their retrieval-only approach
- What evidence would resolve it: Direct comparison of RAG systems versus retrieval-only systems on the same clinical QA tasks with clinician evaluations of answer usefulness

## Limitations
- The verification process showed only moderate inter-rater agreement (Cohen's kappa of 0.22 for answerability), indicating subjective evaluation challenges
- The clinical relevance of the dataset hasn't been validated by practicing clinicians, only medical students
- Exact prompting strategy for LLM generation and specific instructions for human generators are not fully specified

## Confidence
- LLM cost-efficiency claim: Medium confidence (assumes verification costs dominate total costs)
- Lexical similarity comparison to BioASQ: High confidence (based on direct measurements)
- Model performance comparisons: High confidence (quantitative metrics provided)

## Next Checks
1. Test the same evaluation pipeline with different verification criteria to assess inter-rater agreement robustness
2. Evaluate a fine-tuned biomedical model on RealMedQA to determine if domain adaptation improves performance
3. Compare verification costs and agreement for questions generated by more recent LLMs like GPT-4 versus the original GPT-3.5 Turbo results