---
ver: rpa2
title: 'Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification'
arxiv_id: '2406.08993'
source_url: https://arxiv.org/abs/2406.08993
tags:
- graphs
- graph
- accuracy
- gnns
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reevaluates the performance of classic Graph Neural
  Networks (GNNs) for node classification against recent Graph Transformers (GTs).
  The authors find that the previously reported superiority of GTs may be overstated
  due to suboptimal hyperparameter configurations in GNN evaluations.
---

# Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification

## Quick Facts
- arXiv ID: 2406.08993
- Source URL: https://arxiv.org/abs/2406.08993
- Reference count: 40
- Classic GNNs with hyperparameter tuning outperform recent Graph Transformers on node classification tasks

## Executive Summary
This study reevaluates the performance of classic Graph Neural Networks (GNNs) for node classification against recent Graph Transformers (GTs). The authors find that the previously reported superiority of GTs may be overstated due to suboptimal hyperparameter configurations in GNN evaluations. With careful hyperparameter tuning, classic GNN models (GCN, GAT, and GraphSAGE) achieve state-of-the-art performance, ranking first on 17 out of 18 diverse datasets, including homophilous, heterophilous, and large-scale graphs.

## Method Summary
The researchers conducted extensive experiments using three classic GNN models (GCN, GAT, and GraphSAGE) across 18 diverse datasets. They implemented comprehensive hyperparameter tuning including normalization (BN/LN), dropout rates (0.2, 0.3, 0.5, 0.7), residual connections, and network depth (1-10 layers). Models were trained for 2500 epochs with Adam optimizer, and performance was evaluated through 5 independent runs to report mean scores and standard deviations. The study included detailed ablation experiments to assess the impact of individual hyperparameters on different graph types.

## Key Results
- Classic GNNs achieved state-of-the-art performance on 17 out of 18 datasets after hyperparameter tuning
- Normalization is essential for large-scale graphs but less significant on smaller graphs
- Residual connections significantly enhance performance, especially on heterophilous graphs
- Dropout consistently improves performance across all graph types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classic GNNs with tuned hyperparameters can outperform or match advanced GTs in node classification tasks.
- Mechanism: Proper tuning of normalization, dropout, residual connections, and network depth in GNNs compensates for their theoretical limitations and matches the expressiveness of GTs.
- Core assumption: Suboptimal hyperparameter configurations in previous GNN evaluations led to underestimated performance.
- Evidence anchors:
  - [abstract]: "With slight hyperparameter tuning, these classic GNN models achieve state-of-the-art performance, matching or even exceeding that of recent GTs across 17 out of the 18 diverse datasets examined."
  - [section]: "Our findings suggest that the previously reported superiority of GTs may have been overstated due to suboptimal hyperparameter configurations in GNNs."
  - [corpus]: The related paper "Can Classic GNNs Be Strong Baselines for Graph-level Tasks?" supports this by reevaluating GNNs with proper tuning.
- Break condition: If hyperparameter tuning is not properly performed, GNNs will not reach their potential performance.

### Mechanism 2
- Claim: Normalization is essential for large-scale graphs but less significant on smaller-scale graphs.
- Mechanism: Normalization stabilizes the training process by reducing covariate shift, which is more pronounced in large-scale graphs due to diverse node features.
- Core assumption: Large graphs display a wider variety of node features, resulting in different data distributions across the graph.
- Evidence anchors:
  - [section]: "We observe that the ablation of normalization does not lead to substantial deviations on small graphs. However, normalization becomes consistently crucial on large-scale graphs..."
  - [corpus]: The evidence is not explicitly present in the corpus, indicating a potential area for further investigation.
- Break condition: If the graph size is small and feature distributions are uniform, normalization may not significantly impact performance.

### Mechanism 3
- Claim: Residual connections significantly enhance performance, especially on heterophilous graphs.
- Mechanism: Residual connections mitigate gradient instabilities and address over-smoothing and over-squashing issues, which are more prevalent in heterophilous graphs.
- Core assumption: Heterophilous graphs have complex structures that require deeper layers to effectively capture diverse relationships between nodes.
- Evidence anchors:
  - [section]: "Our ablation studies have yielded valuable insights... (3) residual connections can significantly enhance performance, especially on heterophilous graphs..."
  - [corpus]: The corpus does not provide direct evidence for this mechanism, suggesting it is based on the paper's experimental findings.
- Break condition: If the graph is homophilous or does not require deep layers, the impact of residual connections may be minimal.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: Understanding the basics of GNNs is crucial for grasping how hyperparameter tuning can enhance their performance.
  - Quick check question: What is the main difference between GNNs and Graph Transformers (GTs)?

- Concept: Hyperparameter Tuning
  - Why needed here: Knowledge of hyperparameter tuning is essential to understand how it can improve GNN performance.
  - Quick check question: Which hyperparameters are most critical for optimizing GNN performance?

- Concept: Normalization Techniques
  - Why needed here: Understanding normalization is important for grasping its role in stabilizing training, especially for large-scale graphs.
  - Quick check question: How do Layer Normalization (LN) and Batch Normalization (BN) differ in their application to GNNs?

## Architecture Onboarding

- Component map: GNN Models (GCN, GAT, GraphSAGE) -> Hyperparameters (Normalization, Dropout, Residual Connections, Network Depth) -> Datasets (Homophilous, Heterophilous, Large)
- Critical path: Hyperparameter tuning -> Model training -> Performance evaluation
- Design tradeoffs: Model complexity vs. performance vs. computational efficiency
- Failure signatures: Poor convergence with improper normalization placement, overfitting with excessive dropout, vanishing gradients without residual connections
- First experiments: 1) Baseline GNN performance without tuning, 2) Impact of normalization on large-scale graphs, 3) Effect of residual connections on heterophilous graphs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do Graph Transformers maintain their performance advantage on node classification when classic GNNs are optimally tuned?
- Basis in paper: [explicit] The paper shows classic GNNs outperform or match GTs on 17/18 datasets after hyperparameter tuning
- Why unresolved: The paper only tested three specific GNN architectures (GCN, GAT, GraphSAGE) against specific GT models. There may be other GT variants or GNN architectures not tested
- What evidence would resolve it: Comprehensive benchmarking of a wider range of both GNN and GT architectures with exhaustive hyperparameter tuning across diverse graph types

### Open Question 2
- Question: What is the fundamental limitation that prevents Graph Transformers from consistently outperforming classic GNNs on node classification tasks?
- Basis in paper: [inferred] Classic GNNs achieve state-of-the-art performance despite GTs' theoretical advantages in expressiveness and capturing long-range dependencies
- Why unresolved: The paper demonstrates GNNs can match or exceed GT performance but doesn't provide a theoretical explanation for why this occurs
- What evidence would resolve it: Theoretical analysis identifying specific properties of node classification that favor local message passing over global attention mechanisms

### Open Question 3
- Question: How do residual connections specifically improve GNN performance on heterophilous graphs compared to homophilous graphs?
- Basis in paper: [explicit] Ablation studies show residual connections have a "more pronounced effect on heterophilous graphs" with significant accuracy reductions when removed
- Why unresolved: The paper observes the effect but doesn't explain the underlying mechanism for why residual connections are more critical for heterophilous graphs
- What evidence would resolve it: Detailed analysis of how residual connections preserve node distinguishability in heterophilous graphs versus homophilous graphs during message passing

## Limitations
- The study focuses primarily on node classification tasks and may not generalize to other graph learning tasks
- While hyperparameter tuning is thorough, it may not explore the entire space of possible configurations
- The study does not investigate the computational efficiency of GNNs versus GTs

## Confidence

- High confidence: Classic GNNs with proper hyperparameter tuning can achieve state-of-the-art performance on node classification tasks
- Medium confidence: Specific findings regarding the importance of normalization, dropout, and residual connections
- Medium confidence: Claim that GTs' superiority may be overstated

## Next Checks

1. Conduct additional experiments on a broader range of graph learning tasks (e.g., link prediction, graph classification) to validate the generalizability of the findings.
2. Perform a comprehensive computational efficiency analysis comparing classic GNNs and GTs, including training time, memory usage, and scalability to even larger graphs.
3. Investigate the impact of alternative hyperparameter tuning strategies, such as automated hyperparameter optimization methods, to ensure the robustness of the reported results.