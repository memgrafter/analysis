---
ver: rpa2
title: Backdoor Graph Condensation
arxiv_id: '2407.11025'
source_url: https://arxiv.org/abs/2407.11025
tags:
- graph
- condensation
- attack
- trigger
- condensed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Backdoor Graph Condensation (BGC), the first
  method to inject backdoor triggers into condensed graphs for graph neural networks
  (GNNs). Unlike existing graph backdoor attacks that poison the original graph during
  GNN training, BGC addresses the unique challenges of graph condensation by: (1)
  selecting representative nodes in the original graph for trigger injection to maintain
  condensed graph quality, and (2) updating triggers dynamically throughout the condensation
  process to ensure their effectiveness.'
---

# Backdoor Graph Condensation

## Quick Facts
- arXiv ID: 2407.11025
- Source URL: https://arxiv.org/abs/2407.11025
- Reference count: 40
- Key outcome: First backdoor attack method for graph condensation achieving near-100% attack success while preserving model utility

## Executive Summary
This paper introduces Backdoor Graph Condensation (BGC), the first method to inject backdoor triggers into condensed graphs for graph neural networks (GNNs). Unlike existing graph backdoor attacks that poison the original graph during GNN training, BGC addresses the unique challenges of graph condensation by selecting representative nodes for trigger injection and updating triggers dynamically throughout the condensation process. The attacker is modeled as a malicious graph condensation service provider with access to the dataset but no knowledge of the target model.

Extensive experiments on four datasets (Cora, Citeseer, Flickr, Reddit) and four condensation methods demonstrate BGC's effectiveness, achieving near-100% attack success rates while maintaining clean test accuracy close to that of clean models. The method outperforms adapted versions of previous backdoor attacks and maintains effectiveness across different GNN architectures. Two representative defenses (Prune and Randsmooth) fail to mitigate BGC, suffering significant trade-offs between defense effectiveness and model utility.

## Method Summary
BGC is a novel backdoor attack method that targets graph condensation by injecting triggers into the original graph rather than the condensed graph. The method consists of three main components: a poisoned node selector that identifies representative nodes based on their centrality and influence in the graph structure, an adaptive trigger generator that creates and updates triggers throughout the condensation process, and a surrogate GCN model used to optimize triggers when the target model is unknown. The attacker iteratively updates the surrogate model and trigger generator to ensure trigger retention in the final condensed graph while maintaining model utility.

## Key Results
- Achieves near-100% attack success rates on four benchmark datasets
- Maintains clean test accuracy close to that of clean models
- Outperforms adapted versions of previous backdoor attacks
- Maintains effectiveness across different GNN architectures (GCN, GraphSage, SGC, MLP, APPNP, ChebyNet)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method avoids degrading condensed graph quality by injecting triggers into the original large graph rather than the condensed graph.
- Mechanism: Instead of directly poisoning the small condensed graph (which would be easily detectable and degrade model utility), the attacker injects triggers into the original large graph and updates them throughout the condensation process to ensure their retention in the final condensed graph.
- Core assumption: Trigger effectiveness can be preserved through the gradient-based optimization steps of graph condensation if triggers are updated dynamically.
- Evidence anchors:
  - [abstract] "Unlike existing graph backdoor attacks that poison the original graph during GNN training, BGC addresses the unique challenges of graph condensation"
  - [section III] "Therefore, we turn to a more practical strategy, injecting triggers into the original graph throughout the condensation process to carry out the attacks."
- Break condition: If the condensation process removes or alters trigger structures beyond recovery, or if the update frequency is too low to maintain trigger presence.

### Mechanism 2
- Claim: Selecting representative nodes for poisoning maximizes backdoor effectiveness within a limited budget.
- Mechanism: The method trains a GCN node selector to identify representative nodes based on their centrality and influence in the graph structure, then limits poisoning to these nodes to maintain condensed graph quality while ensuring effective backdoor injection.
- Core assumption: Representative nodes have disproportionate influence on the condensed graph's structure and the downstream GNN's gradients, making them optimal targets for backdoor injection.
- Evidence anchors:
  - [section IV-B] "Previous studies [36], [37] indicate that representative samples play a pivotal role in the gradient of dataset condensation, exerting a substantial influence on the quality of condensed datasets."
  - [section IV-B] "injecting too many triggers into the graphs can affect the GNN utility. Therefore, to facilitate successful backdoor attacks, we limit the budget of poisoning and we propose to select representative nodes to poison."
- Break condition: If the node selection metric fails to identify truly influential nodes, or if the limited poisoning budget prevents effective trigger propagation.

### Mechanism 3
- Claim: Dynamic trigger updates throughout condensation preserve backdoor effectiveness against unknown target models.
- Mechanism: The method iteratively updates triggers during condensation using a surrogate GCN model, solving a tri-level optimization problem by sequentially optimizing the surrogate model and trigger generator for several epochs at each condensation update step.
- Core assumption: A surrogate model can approximate the behavior of unknown target models sufficiently well to enable effective trigger optimization.
- Evidence anchors:
  - [section IV-D] "Since the target model f is invisible to attackers, we propose to optimize the trigger generator fg and generate condensed graph S to successfully attack the surrogate GCN model fc."
  - [section IV-D] "This significantly reduces the number of iterations, improving the efficiencies."
- Break condition: If the surrogate model poorly approximates target models, or if the sequential approximation fails to converge to effective triggers.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their vulnerability to backdoor attacks
  - Why needed here: The attack targets GNNs trained on condensed graphs, so understanding GNN architecture and attack vectors is fundamental
  - Quick check question: How does a backdoor attack differ from a standard adversarial attack on GNNs?

- Concept: Graph condensation as a bi-level optimization problem
  - Why needed here: The attack exploits the condensation process itself, requiring understanding of how condensed graphs are generated from large graphs
  - Quick check question: What is the relationship between the condensed graph and the original graph in terms of gradient matching?

- Concept: Node representation and centrality in graph structures
  - Why needed here: The attack relies on selecting representative nodes based on their structural importance in the graph
  - Quick check question: Why would poisoning high-degree nodes potentially harm model utility more than poisoning representative nodes?

## Architecture Onboarding

- Component map: Poisoned Node Selector -> Adaptive Trigger Generator -> Surrogate GCN Model -> Condensed Graph Generator -> Trigger Attachment Module

- Critical path: 
  1. Train poisoned node selector on original graph
  2. Select representative nodes within poisoning budget
  3. Initialize trigger generator and surrogate model
  4. Iteratively: update surrogate model → update trigger generator → update condensed graph
  5. Output backdoored condensed graph and trigger generator

- Design tradeoffs:
  - Poisoning budget vs. attack effectiveness: larger budgets improve attack success but risk model utility degradation
  - Trigger size vs. stealth: larger triggers improve attack success but increase detectability
  - Update frequency vs. computational cost: more frequent updates improve trigger retention but increase runtime

- Failure signatures:
  - Low attack success rate despite successful condensation
  - Significant drop in clean test accuracy compared to clean models
  - Trigger structures failing to persist through condensation updates
  - Surrogate model poorly approximating target model behavior

- First 3 experiments:
  1. Validate node selection: Run the selector on Cora dataset and verify selected nodes have high centrality scores
  2. Test trigger update mechanism: Monitor trigger structure persistence across 10 condensation iterations on a small graph
  3. Evaluate budget impact: Compare attack success rates and clean accuracy across different poisoning budgets (0.1x, 0.5x, 1.0x node count)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do backdoor attacks perform against graph condensation when the attacker has only partial access to the original graph (e.g., federated or crowdsourced scenarios)?
- Basis in paper: [explicit] The paper discusses limitations in real-world settings where attackers may only control a subset of nodes/edges.
- Why unresolved: The paper focuses on the scenario where the attacker has full access to the dataset but lacks knowledge of the target model. It does not explore the effectiveness of backdoor attacks under partial data access constraints.
- What evidence would resolve it: Experimental results comparing the attack success rate and model utility of backdoor attacks when the attacker has partial vs. full access to the graph data.

### Open Question 2
- Question: Can backdoor attacks against graph condensation be effective when the condensation algorithm is hidden (e.g., a commercial API) and the attacker cannot dynamically optimize triggers?
- Basis in paper: [explicit] The paper mentions that if the condensation algorithm is hidden, dynamic trigger optimization becomes infeasible.
- Why unresolved: The paper assumes the attacker knows the condensation algorithm and can optimize triggers accordingly. It does not investigate the effectiveness of backdoor attacks under black-box condensation settings.
- What evidence would resolve it: Experimental results demonstrating the attack success rate and model utility of backdoor attacks against hidden condensation algorithms, potentially using invariant triggers or surrogate models.

### Open Question 3
- Question: How does the effectiveness of backdoor attacks against graph condensation vary with different graph structures (e.g., heterogeneous graphs, dynamic graphs)?
- Basis in paper: [inferred] The paper evaluates the proposed attack on four benchmark datasets (Cora, Citeseer, Flickr, Reddit) but does not explicitly investigate the impact of graph structure on attack performance.
- Why unresolved: The paper does not explore the relationship between graph structure and the effectiveness of backdoor attacks against graph condensation.
- What evidence would resolve it: Experimental results comparing the attack success rate and model utility of backdoor attacks across graphs with varying structures (e.g., different node degrees, edge densities, graph sizes).

## Limitations
- Effectiveness relies on surrogate model accurately approximating unknown target models
- Primarily demonstrated on small academic datasets and four specific condensation methods
- Computational overhead of dynamic trigger updates may be prohibitive for very large graphs

## Confidence
- High confidence: BGC's basic mechanism of injecting triggers into representative nodes during condensation
- Medium confidence: Claim that BGC maintains clean model utility close to baseline
- Low confidence: Claim that existing defenses are ineffective against BGC

## Next Checks
1. Test BGC against modern graph condensation methods not included in the original experiments (e.g., recent meta-learning approaches)
2. Evaluate BGC's performance on industrial-scale graphs with millions of nodes to assess scalability and practical feasibility
3. Implement and test additional defense mechanisms beyond Prune and Randsmooth to comprehensively evaluate BGC's robustness