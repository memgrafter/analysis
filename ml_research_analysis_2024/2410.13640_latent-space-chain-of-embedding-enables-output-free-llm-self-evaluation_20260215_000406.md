---
ver: rpa2
title: Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation
arxiv_id: '2410.13640'
source_url: https://arxiv.org/abs/2410.13640
tags:
- language
- arxiv
- self-evaluation
- auroc
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Chain-of-Embedding (CoE), a method for output-free\
  \ LLM self-evaluation using latent space trajectory features. CoE tracks hidden\
  \ state changes across all layers during inference, capturing the model\u2019s internal\
  \ reasoning path."
---

# Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation

## Quick Facts
- arXiv ID: 2410.13640
- Source URL: https://arxiv.org/abs/2410.13640
- Authors: Yiming Wang; Pei Zhang; Baosong Yang; Derek F. Wong; Rui Wang
- Reference count: 40
- Primary result: Chain-of-Embedding (CoE) uses latent space trajectory features to enable output-free LLM self-evaluation, outperforming ten baselines across four domains with up to 9.83% AUROC improvement.

## Executive Summary
This paper introduces Chain-of-Embedding (CoE), a method for output-free LLM self-evaluation using latent space trajectory features. CoE tracks hidden state changes across all layers during inference, capturing the model's internal reasoning path. Two scalar metrics—CoE-R (real-space) and CoE-C (complex-space)—are derived from average magnitude and angle changes between adjacent hidden states, with CoE-C offering improved robustness. Experiments across four domains (Mathematics, Reasoning, Knowledge, Understanding) and seven models show CoE outperforms ten baselines in AUROC, FPR95, and AUPR, with gains up to 9.83%. It remains effective on high-difficulty tasks and multilingual data, scales well with model size, and requires only milliseconds of computation after base inference, making it suitable for real-time, large-scale deployment.

## Method Summary
Chain-of-Embedding (CoE) captures the latent thinking path of LLMs through progressive hidden states across all layers. The method computes two scalar metrics—CoE-R (real-space) and CoE-C (complex-space)—from average magnitude and angle changes between adjacent hidden states. These metrics quantify the geometric properties of the reasoning trajectory, with correct responses producing more convoluted paths. The approach requires no training and operates in a label-free manner, making it suitable for real-time deployment with only milliseconds of post-inference computation.

## Key Results
- CoE outperforms ten baselines (including Verbal Confidence, PSA Pipeline, Maximum Softmax Probability, Perplexity, Entropy, Temperature Scaling, Energy, MC Dropout, LN-Entropy, and Eigenscore) in AUROC, FPR95, and AUPR metrics
- Performance gains reach up to 9.83% AUROC improvement over the best baseline across four domains (Mathematics, Reasoning, Knowledge, Understanding)
- CoE-C (complex-space) metric shows superior robustness compared to CoE-R (real-space), particularly on high-difficulty tasks
- Method remains effective on multilingual data and scales well with model size from 7B to 72B parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Embedding (CoE) captures the latent thinking path of LLMs through progressive hidden states.
- Mechanism: The model's internal hidden states across layers form a trajectory in latent space. Correct responses produce more convoluted trajectories with larger magnitude changes and smaller angle changes compared to incorrect responses.
- Core assumption: The geometric properties of the CoE trajectory differ systematically between correct and incorrect responses, reflecting the model's internal reasoning quality.
- Evidence anchors:
  - [abstract] "CoE consists of all progressive hidden states produced during the inference time, which can be treated as the latent thinking path of LLMs."
  - [section 2.1] "We find that when LLMs respond correctly and incorrectly, their CoE features differ, these discrepancies assist us in estimating LLM response correctness."
  - [corpus] Weak evidence - related papers discuss uncertainty and self-evaluation but don't specifically validate CoE trajectory differences.
- Break condition: If hidden state changes are primarily noise or don't systematically differ between correct/incorrect responses, the geometric features would lose discriminative power.

### Mechanism 2
- Claim: The two scalar metrics CoE-R and CoE-C effectively combine magnitude and angle features to assess response correctness.
- Mechanism: CoE-R uses real-space combination by averaging normalized magnitude and angle changes. CoE-C uses complex-space combination by representing changes as points in the complex plane and averaging their real/imaginary parts.
- Core assumption: Both magnitude and angle features contribute complementary information about the reasoning path quality.
- Evidence anchors:
  - [section 3] "We propose two ways as follows: CoE-R: Real-Space Combination" and "CoE-C: Complex-Space Combination."
  - [section 4.3] Component ablation study shows both components contribute positively in most settings.
  - [corpus] Weak evidence - related work mentions uncertainty estimation but not this specific geometric combination approach.
- Break condition: If either magnitude or angle feature becomes corrupted or irrelevant for certain tasks, the combination metrics would lose effectiveness.

### Mechanism 3
- Claim: CoE evaluation requires only milliseconds of computation after base inference, making it suitable for real-time deployment.
- Mechanism: After extracting all hidden states during base LLM inference, CoE computation involves only simple arithmetic operations (addition, multiplication, square roots, trigonometry) that can be executed in parallel.
- Core assumption: The computational overhead of CoE is negligible compared to base LLM inference.
- Evidence anchors:
  - [abstract] "requires only milliseconds of computation after base inference, making it suitable for real-time, large-scale deployment."
  - [section 4.3] "Table 3 compares the execution time (excluding the first base inference, which is mandatory) of our CoE method with the above two types of methods."
  - [corpus] Weak evidence - related papers discuss efficiency but not this specific millisecond-level computation claim.
- Break condition: If hidden state extraction or CoE computation becomes bottlenecked (e.g., very large models with high-dimensional states), the real-time advantage would diminish.

## Foundational Learning

- Concept: Hidden state trajectory analysis in transformer models
  - Why needed here: Understanding how hidden states evolve across layers is fundamental to CoE's mechanism of capturing reasoning paths.
  - Quick check question: What information do intermediate hidden states in transformer layers encode according to prior research?

- Concept: Uncertainty estimation and confidence calibration
  - Why needed here: CoE is positioned as an alternative to traditional uncertainty estimation methods, so understanding their limitations is crucial.
  - Quick check question: Why do traditional softmax-based confidence measures often fail for LLM self-evaluation?

- Concept: Geometric features in latent space (magnitude and angle)
  - Why needed here: CoE relies on measuring changes in magnitude and angle between adjacent hidden states to quantify trajectory characteristics.
  - Quick check question: How do magnitude and angle features in latent space relate to semantic similarity and reasoning complexity?

## Architecture Onboarding

- Component map: Base LLM inference -> Hidden state extraction -> Feature computation -> Metric calculation -> Classification
- Critical path: Base LLM inference → Hidden state extraction → CoE computation → Correctness classification
- Design tradeoffs:
  - CoE-R vs CoE-C: CoE-R is simpler but less robust to outliers; CoE-C is more complex but handles anomalies better
  - Real-time vs accuracy: Millisecond computation ensures deployment feasibility but may sacrifice some precision compared to more complex methods
  - Black-box vs white-box: Requires access to internal states, limiting applicability to open models
- Failure signatures:
  - Poor performance on specific domains (e.g., Mathematics) may indicate CoE trajectory features aren't discriminative enough
  - Inconsistent results across similar tasks suggest sensitivity to prompt variations or sampling strategies
  - Computational bottlenecks would manifest as latency exceeding real-time requirements
- First 3 experiments:
  1. Verify CoE feature distributions differ between correct and incorrect responses on a simple dataset
  2. Compare CoE-R and CoE-C performance on tasks with known answer patterns
  3. Benchmark computation time of CoE metrics against base inference to confirm millisecond-level overhead

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CoE maintain effectiveness on tasks involving longer sequences (thousands of tokens) where trajectory features might be harder to extract?
- Basis in paper: [explicit] The paper notes that some math tasks involve solutions with thousands of tokens and mentions CoE's performance on high-difficulty tasks, but does not specifically test sequence length scaling.
- Why unresolved: The experiments use relatively short tasks (GSM8K, CommonsenseQA) or math tasks where even long solutions may have structured intermediate states. The impact of very long sequences on trajectory feature extraction remains unknown.
- What evidence would resolve it: Testing CoE on tasks with progressively longer sequences (e.g., code generation, extended reasoning) while measuring AUROC and comparing with baseline methods.

### Open Question 2
- Question: Can CoE be extended to non-Transformer architectures like RNNs or hybrid models where hidden state interpretation differs?
- Basis in paper: [inferred] The paper explicitly defines CoE for Transformer-based models and mentions "large Transformer-based language models," implying the method may not generalize to other architectures.
- Why unresolved: The method relies on the specific layer-by-layer hidden state progression in Transformers. Different architectures may have different hidden state dynamics that would require modified feature extraction.
- What evidence would resolve it: Implementing CoE on RNN-based or hybrid models and comparing performance metrics with Transformer baselines.

### Open Question 3
- Question: What is the optimal number of layers to sample for CoE computation, and does this vary by task type or model size?
- Basis in paper: [explicit] The paper uses all L layers in CoE computation but notes computational efficiency, suggesting layer selection could be optimized.
- Why unresolved: Using all layers may be computationally wasteful, and certain layers might contribute more to correctness detection than others. The trade-off between performance and efficiency is unexplored.
- What evidence would resolve it: Systematic ablation studies testing CoE performance with different layer subsets across various tasks and model sizes.

## Limitations
- The method requires access to internal hidden states, making it inapplicable to closed models where intermediate activations are not accessible
- Performance gains over baselines are relatively modest (up to 9.83% AUROC improvement), suggesting the method may not be universally superior
- The computational overhead during base inference (hidden state extraction) is not explicitly quantified in the paper

## Confidence
- High confidence: The core mechanism of using hidden state trajectories for self-evaluation is technically sound and well-implemented
- Medium confidence: The comparative performance against baselines is convincing, though the absolute gains are modest
- Medium confidence: The claim of millisecond-level post-inference computation is supported but not extensively validated across different hardware configurations

## Next Checks
1. **Cross-domain robustness test**: Evaluate CoE on additional task types (e.g., code generation, creative writing) to assess generalizability beyond the four domains tested
2. **Closed model compatibility**: Investigate whether CoE can be approximated using only accessible outputs (e.g., logits) for models where hidden states are not available
3. **Scalability analysis**: Measure actual latency impact of hidden state extraction during base inference on different model sizes and hardware to confirm real-time feasibility