---
ver: rpa2
title: 'DyGMamba: Efficiently Modeling Long-Term Temporal Dependency on Continuous-Time
  Dynamic Graphs with State Space Models'
arxiv_id: '2408.04713'
source_url: https://arxiv.org/abs/2408.04713
tags:
- temporal
- dygmamba
- time
- information
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DyGMamba addresses the challenge of modeling long-term temporal
  dependencies in continuous-time dynamic graphs (CTDGs) by leveraging state space
  models (SSMs), specifically the Mamba SSM. The model employs a node-level SSM to
  encode historical node interactions and a time-level SSM to capture edge-specific
  temporal patterns, enabling dynamic selection of critical information from interaction
  histories.
---

# DyGMamba: Efficiently Modeling Long-Term Temporal Dependency on Continuous-Time Dynamic Graphs with State Space Models

## Quick Facts
- arXiv ID: 2408.04713
- Source URL: https://arxiv.org/abs/2408.04713
- Reference count: 39
- Key outcome: DyGMamba achieves state-of-the-art performance in dynamic link prediction on CTDGs, particularly for datasets requiring long-term temporal dependencies, while maintaining computational efficiency.

## Executive Summary
DyGMamba addresses the challenge of modeling long-term temporal dependencies in continuous-time dynamic graphs (CTDGs) by leveraging state space models (SSMs), specifically the Mamba SSM. The model employs a node-level SSM to encode historical node interactions and a time-level SSM to capture edge-specific temporal patterns, enabling dynamic selection of critical information from interaction histories. This approach allows DyGMamba to maintain low computational complexity while effectively modeling extended temporal contexts. Experimental results demonstrate that DyGMamba achieves state-of-the-art performance in most cases, particularly on datasets requiring long-term temporal dependencies.

## Method Summary
DyGMamba combines node-level and time-level Mamba SSM blocks to model temporal dependencies in CTDGs. The node-level SSM encodes sequences of historical interactions for each node, while the time-level SSM learns patterns in the intervals between repeated interactions. These components work together to dynamically select important information from the interaction history. The model also employs a patching technique to reduce computational complexity by treating adjacent temporal neighbors as single patches. DyGMamba is evaluated on seven real-world CTDG datasets for dynamic link prediction tasks.

## Key Results
- DyGMamba achieves state-of-the-art performance in most cases on dynamic link prediction tasks
- The model demonstrates high efficiency in modeling long-term temporal dependencies while maintaining low computational complexity
- Ablation studies confirm the effectiveness of both node-level and time-level SSM components, with dynamic information selection based on learned temporal patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DyGMamba achieves superior performance by combining a node-level SSM for encoding long interaction histories with a time-level SSM for capturing edge-specific temporal patterns, then dynamically selecting critical information based on these patterns.
- Mechanism: The node-level SSM processes long sequences of historical interactions in a recurrent fashion, maintaining low computational complexity while encoding temporal dependencies. The time-level SSM learns patterns in the intervals between repeated interactions, and its output is used to weight the importance of different historical interactions during the selection phase.
- Core assumption: Long-term temporal dependencies in CTDGs are best captured by modeling both the sequence of interactions (via node-level SSM) and the temporal patterns within those interactions (via time-level SSM), and that dynamic selection based on learned patterns improves performance.
- Evidence anchors:
  - [abstract]: "DyGMamba first leverages a node-level SSM to encode the sequence of historical node interactions. Another time-level SSM is then employed to exploit the temporal patterns hidden in the historical graph, where its output is used to dynamically select the critical information from the interaction history."
  - [section]: "DyGMamba first leverages a node-level Mamba SSM to encode historical node interactions. Another time-level Mamba SSM is then employed to exploit the edge-specific temporal patterns, where its output is used to dynamically select the critical information from the interaction history."
- Break condition: If the temporal patterns learned by the time-level SSM do not correlate with the importance of historical interactions for prediction, the dynamic selection mechanism would not improve performance.

### Mechanism 2
- Claim: The use of Mamba SSM, which has selective state spaces, allows DyGMamba to maintain low computational complexity while effectively modeling long sequences.
- Mechanism: Mamba SSM uses a selection mechanism that maps input data to the SSM's parameters, making it input-dependent and time-variant. This allows it to dynamically choose important information from the input context, which is crucial for tasks requiring context-aware reasoning over long sequences.
- Core assumption: Mamba SSM's selective state spaces are more efficient than traditional self-attention mechanisms for modeling long sequences, and its input-dependent parameters allow for better context-aware reasoning.
- Evidence anchors:
  - [abstract]: "DyGMamba demonstrates high efficiency in modeling long-term temporal dependencies in CTDGs."
  - [section]: "Since Mamba is proven effective and efficient in long sequence modeling (Gu & Dao, 2023), it maintains low computational complexity and is scalable in modeling long-term temporal dependencies."
- Break condition: If the computational complexity of Mamba SSM scales poorly with sequence length or if its selection mechanism does not effectively identify important information, the efficiency advantage would be lost.

### Mechanism 3
- Claim: The patching technique reduces computational complexity by treating p temporally adjacent neighbors as a single patch, decreasing the sequence length by approximately p times.
- Mechanism: By grouping adjacent neighbors into patches, the number of elements in the input sequence to the SSM is reduced, leading to fewer recurrent computations. This significantly cuts down on GPU memory usage and per epoch training time.
- Core assumption: Treating adjacent temporal neighbors as a single unit does not significantly degrade the model's ability to capture important temporal patterns, and the reduction in sequence length outweighs any potential loss in granularity.
- Evidence anchors:
  - [section]: "We employ the patching technique proposed by (Yu et al., 2023) to save computational resources when dealing with a large number of temporal neighbors. We treat p temporally adjacent neighbors as a patch and flatten their features."
  - [section]: "Patching decreases the length of the sequence by roughly p times, making great contribution in saving computational resources."
- Break condition: If the temporal patterns of interest are highly sensitive to the exact timing of individual interactions, or if the patch size is too large to capture necessary temporal nuances, the patching technique could hurt performance.

## Foundational Learning

- Concept: Continuous-Time Dynamic Graphs (CTDGs)
  - Why needed here: Understanding the structure and challenges of CTDGs is fundamental to grasping why DyGMamba's approach is necessary. CTDGs consist of a stream of events with individual timestamps, requiring models to handle irregular time intervals and long-term dependencies.
  - Quick check question: What is the key difference between discrete-time dynamic graphs (DTDGs) and continuous-time dynamic graphs (CTDGs), and why does this difference make CTDG modeling more challenging?

- Concept: State Space Models (SSMs) and Mamba SSM
  - Why needed here: DyGMamba is built on Mamba SSM, so understanding how SSMs work, particularly the selective state spaces in Mamba, is crucial for understanding the model's architecture and efficiency.
  - Quick check question: How does the selection mechanism in Mamba SSM differ from traditional SSMs, and why is this difference important for modeling long sequences?

- Concept: Dynamic Link Prediction
  - Why needed here: The task DyGMamba is evaluated on is dynamic link prediction, so understanding the task setup (predicting whether an interaction will occur at a future timestamp given past interactions) is essential for interpreting the results.
  - Quick check question: In the context of dynamic link prediction on CTDGs, what information does a model need to predict, and what are the main challenges in making these predictions?

## Architecture Onboarding

- Component map: Historical interactions → Node-Level SSM encoding → Time-Level SSM pattern learning → Dynamic selection → Prediction
- Critical path: Historical interactions → Node-Level SSM encoding → Time-Level SSM pattern learning → Dynamic selection → Prediction
- Design tradeoffs:
  - Using SSMs vs. Transformers: SSMs offer linear computational complexity, making them more scalable for long sequences, but may have different representational capabilities than Transformers.
  - Patching vs. no patching: Patching reduces computational cost but may lose some temporal granularity.
  - Node-level vs. edge-level processing: Separate processing of node histories simplifies the architecture but requires additional mechanisms (like the time-level SSM) to capture edge-specific information.
- Failure signatures:
  - Performance degradation with increasing sequence length could indicate that the SSM layers are not effectively capturing long-term dependencies.
  - Instability or poor performance on inductive link prediction tasks could suggest issues with how the model handles unseen nodes.
  - High computational cost or memory usage could indicate that the patching size is too small or that the SSM parameters are not optimized.
- First 3 experiments:
  1. Validate the core functionality by running DyGMamba on a small, synthetic CTDG dataset with known temporal patterns and checking if it can learn and utilize these patterns for prediction.
  2. Test the impact of the time-level SSM by comparing performance with and without this component on a dataset where edge-specific temporal patterns are expected to be important.
  3. Evaluate the efficiency gains from patching by comparing performance and resource usage with different patch sizes on a large CTDG dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limitations of using SSMs for modeling extremely long temporal dependencies in CTDGs?
- Basis in paper: [inferred] The paper discusses the efficiency of DyGMamba in modeling long-term temporal dependencies but does not explore the limits of SSMs in handling extremely long sequences or the potential issues that might arise with very large datasets.
- Why unresolved: The paper does not provide a detailed analysis of the scalability of SSMs beyond the tested dataset sizes or discuss potential bottlenecks or limitations when scaling to even larger graphs or longer temporal sequences.
- What evidence would resolve it: Experiments testing DyGMamba on datasets with significantly longer temporal sequences or larger graphs, along with a detailed analysis of the computational resources and time required, would provide insights into the scalability limits of SSMs in this context.

### Open Question 2
- Question: How does the performance of DyGMamba compare to other state-of-the-art models on datasets with varying levels of temporal sparsity or density?
- Basis in paper: [inferred] The paper presents results on several datasets but does not analyze how DyGMamba's performance varies with the temporal sparsity or density of interactions in the graph.
- Why unresolved: The paper focuses on the overall performance of DyGMamba across different datasets but does not investigate how the model's effectiveness changes with the frequency of interactions or the distribution of timestamps in the data.
- What evidence would resolve it: A study comparing DyGMamba's performance on datasets with different temporal densities, along with an analysis of how the model's parameters or architecture might need to be adjusted for optimal performance in sparse or dense temporal scenarios, would address this question.

### Open Question 3
- Question: What are the implications of using different time encoding functions on the performance of DyGMamba?
- Basis in paper: [explicit] The paper mentions using a time encoding function from TGAT but does not explore the impact of using alternative time encoding methods or the potential benefits of customizing the time encoding for specific types of CTDGs.
- Why unresolved: The paper adopts a specific time encoding approach without discussing its suitability for different types of temporal patterns or the potential improvements that could be achieved with alternative encoding methods.
- What evidence would resolve it: Experiments comparing DyGMamba's performance using different time encoding functions, along with an analysis of how the choice of encoding affects the model's ability to capture temporal patterns in various types of CTDGs, would provide insights into the importance of time encoding in this context.

## Limitations
- Performance improvements are particularly pronounced on datasets requiring long-term temporal dependencies, suggesting the model may not be universally superior
- Efficiency claims require more detailed computational analysis to verify scalability claims
- The dynamic selection mechanism lacks sufficient empirical validation to confirm its practical impact on performance

## Confidence
- High confidence: The core architectural design combining node-level and time-level SSMs is well-specified and theoretically grounded
- Medium confidence: Performance claims are supported by experimental results but show variability across datasets
- Low confidence: Efficiency claims regarding computational complexity and resource usage require more detailed analysis and comparison with baseline models

## Next Checks
1. **Temporal Pattern Generalization Test**: Evaluate DyGMamba's performance on datasets with varying temporal patterns and interaction frequencies to verify the time-level SSM's ability to capture diverse temporal dependencies across different CTDG characteristics.

2. **Computational Complexity Analysis**: Conduct detailed profiling of DyGMamba's computational requirements across different sequence lengths and patch sizes to validate the claimed efficiency gains and identify potential scaling bottlenecks.

3. **Dynamic Selection Mechanism Validation**: Design ablation studies that systematically test the impact of the dynamic information selection component under controlled conditions, including scenarios where temporal patterns are deliberately manipulated to test the selection mechanism's responsiveness.