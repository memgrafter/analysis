---
ver: rpa2
title: Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems
arxiv_id: '2404.01616'
source_url: https://arxiv.org/abs/2404.01616
tags:
- speech
- indo-european
- languages
- text
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to build speech-to-text
  retrieval systems by leveraging pre-trained large language models (LLMs) without
  requiring speech data during LLM pre-training. The method discretizes speech into
  acoustic units and extends the LLM's embedding layer to support both text and audio
  tokens.
---

# Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems

## Quick Facts
- **arXiv ID**: 2404.01616
- **Source URL**: https://arxiv.org/abs/2404.01616
- **Reference count**: 40
- **Primary result**: Achieves 10% absolute improvement in Recall@1 for speech-to-text retrieval across 102 languages using pre-trained LLMs

## Executive Summary
This paper introduces a novel approach to build speech-to-text retrieval systems by leveraging pre-trained large language models (LLMs) without requiring speech data during LLM pre-training. The method discretizes speech into acoustic units and extends the LLM's embedding layer to support both text and audio tokens. The model is trained using a contrastive loss to align speech and text embeddings in a shared space. The proposed approach significantly outperforms previous methods on speech-to-text retrieval across 102 languages, achieving a 10% absolute improvement in Recall@1, despite only being trained on 21 languages. Additionally, the model demonstrates cross-lingual speech and text matching capabilities, which can be further enhanced by incorporating readily available machine translation data.

## Method Summary
The approach involves discretizing speech into acoustic units that can be processed alongside text tokens by an extended LLM embedding layer. The model uses a contrastive loss function to align speech and text embeddings in a shared space. By leveraging pre-trained LLMs without requiring speech data during their initial training, the method creates a cross-modal retrieval system that can handle both speech and text inputs. The acoustic unit discretization enables the LLM to process speech data without architectural modifications to the base model.

## Key Results
- Achieves 10% absolute improvement in Recall@1 for speech-to-text retrieval
- Works across 102 languages despite training on only 21 languages
- Demonstrates cross-lingual speech and text matching capabilities
- Performance can be further enhanced by incorporating machine translation data

## Why This Works (Mechanism)
The method works by extending pre-trained LLMs to handle cross-modal inputs through acoustic unit discretization and embedding layer adaptation. The contrastive loss ensures that semantically similar speech and text pairs have aligned embeddings in a shared space, enabling effective retrieval. By leveraging existing LLM architectures without requiring speech-specific pre-training, the approach efficiently utilizes available language resources while maintaining the model's linguistic capabilities across multiple languages.

## Foundational Learning

1. **Acoustic Unit Discretization**
   - Why needed: Converts continuous speech signals into discrete tokens that LLMs can process
   - Quick check: Verify acoustic units preserve phonetic information and enable cross-modal alignment

2. **Contrastive Learning for Cross-modal Alignment**
   - Why needed: Ensures speech and text embeddings of semantically similar pairs are close in shared space
   - Quick check: Measure embedding similarity between aligned speech-text pairs vs. random pairs

3. **Zero-shot Cross-lingual Transfer**
   - Why needed: Enables retrieval across languages not seen during training
   - Quick check: Evaluate performance on truly unseen languages with no training data

4. **Embedding Layer Extension**
   - Why needed: Allows LLM to process both text and audio tokens in a unified framework
   - Quick check: Compare retrieval performance with and without embedding layer adaptation

5. **Multi-language Representation Learning**
   - Why needed: Enables the model to handle diverse linguistic patterns across 102 languages
   - Quick check: Analyze embedding space structure across different language families

## Architecture Onboarding

**Component Map:**
Pre-trained LLM -> Extended Embedding Layer -> Acoustic Unit Discretizer -> Contrastive Loss Function

**Critical Path:**
Speech input → Acoustic discretization → Extended embedding → Contrastive alignment → Retrieval output

**Design Tradeoffs:**
- Acoustic unit discretization vs. end-to-end speech processing
- Contrastive loss vs. alternative alignment methods
- Cross-lingual generalization vs. language-specific optimization

**Failure Signatures:**
- Poor alignment between speech and text embeddings
- Degradation in performance for low-resource languages
- Computational overhead from embedding layer extension

**First Experiments:**
1. Evaluate baseline LLM performance on text-only retrieval
2. Test acoustic unit discretization quality on speech transcription
3. Measure cross-modal alignment effectiveness using similarity scores

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on Recall@1 metrics, potentially missing quality nuances in ranked outputs
- Performance claims based on specific 21 training languages and 102 test languages may not generalize
- Acoustic unit discretization may introduce information loss compared to end-to-end processing
- Cross-lingual enhancement claims from machine translation data are not thoroughly validated

## Confidence

**High confidence:**
- Core methodology of extending LLM embeddings for cross-modal retrieval is technically sound
- Reported 10% absolute improvement in Recall@1 is credible for the tested conditions

**Medium confidence:**
- Cross-lingual matching claims are supported but need more extensive ablation studies
- Improvement from machine translation data incorporation is promising but not definitively proven

## Next Checks

1. Evaluate model performance on truly zero-shot languages (completely absent from both training and validation sets) to verify cross-lingual generalization claims

2. Conduct comprehensive analysis of retrieval quality beyond Recall@1, including mean average precision and human evaluation of ranked outputs

3. Measure computational overhead and inference latency when extending LLM embedding layers for cross-modal inputs, particularly for real-time applications