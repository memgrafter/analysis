---
ver: rpa2
title: 'Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation'
arxiv_id: '2403.06988'
source_url: https://arxiv.org/abs/2403.06988
tags:
- generation
- decoding
- constrained
- domino
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper identifies a key challenge in constrained decoding:\
  \ subword-token misalignment with grammar terminals that degrades accuracy and efficiency.\
  \ To address this, it introduces DOMINO, a minimally invasive constrained decoding\
  \ algorithm that leverages precomputation and speculative decoding to enforce grammars\
  \ with negligible overhead\u2014achieving up to 2.7x speedup over unconstrained\
  \ decoding while maintaining or improving task accuracy."
---

# Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation

## Quick Facts
- **arXiv ID**: 2403.06988
- **Source URL**: https://arxiv.org/abs/2403.06988
- **Reference count**: 40
- **Key outcome**: DOMINO achieves up to 2.7x speedup over unconstrained decoding while maintaining or improving task accuracy through minimally invasive constrained generation

## Executive Summary
This paper introduces DOMINO, a constrained decoding algorithm that addresses the challenge of subword-token misalignment with grammar terminals in language model generation. DOMINO achieves fast, accurate constrained generation by precomputing subterminal trees that align subword tokens with grammar terminals, combined with speculative decoding to propose multiple tokens in parallel. The method significantly outperforms existing approaches like GUIDANCE and llama.cpp across multiple tasks including JSON generation, C code, XML, and schema-guided generation, achieving up to 77% higher throughput while maintaining or improving accuracy.

## Method Summary
DOMINO is a constrained decoding algorithm that enforces formal language grammars during LLM generation through minimally invasive means. The core innovation involves precomputing subterminal trees that map vocabulary tokens to valid grammar continuations, avoiding token misalignment issues common in existing approaches. The method combines a scanner (NFA for terminals) and parser (grammar state tracker) to validate generated text in real-time. DOMINO supports speculative decoding for parallel token proposals and opportunistic masking to reduce overhead when models naturally adhere to grammar. The algorithm requires offline precomputation of subterminal trees but achieves negligible runtime overhead, often outperforming unconstrained generation in throughput.

## Key Results
- DOMINO achieves up to 2.7x speedup over unconstrained decoding while maintaining or improving task accuracy
- DOMINO significantly outperforms GUIDANCE and llama.cpp in both accuracy and throughput across JSON, C code, XML, and schema-guided tasks
- Speculative decoding with DOMINO achieves 1.7x throughput compared to unconstrained generation for schema-driven JSON tasks

## Why This Works (Mechanism)

### Mechanism 1: Subterminal Tree Precomputation
DOMINO achieves minimally invasive constraining by precomputing subterminal trees that align subword tokens with grammar terminals, preventing token misalignment during generation. The algorithm constructs a character scanner NFA for the grammar, then lifts it to the subterminal level by enumerating all possible subterminal sequences for each vocabulary token. These sequences are organized into prefix trees per scanner state. At inference, the parser prunes these trees based on current grammar state, ensuring only valid continuations are allowed. This mechanism assumes subterminal trees can be precomputed offline and remain valid for inference, with prefix tree traversal efficiently masking invalid tokens without online parsing of the full vocabulary.

### Mechanism 2: Speculative Decoding
DOMINO leverages predictable grammar structure to propose multiple tokens in parallel, achieving up to 77% higher throughput over unconstrained generation. The algorithm learns a simple count-based model P(l | α, β) from parser state α and scanner state β during warm-up, predicting the next token with high probability. It proposes s tokens in parallel and validates them with a single forward pass, discarding invalid ones without backtracking. This mechanism assumes the grammar structure is predictable enough that simple frequency counts from a few warm-up samples provide accurate token predictions for speculative decoding.

### Mechanism 3: Opportunistic Masking
DOMINO reduces overhead by checking the model-proposed token first before computing full masks, effective when models naturally adhere to grammar. Instead of traversing subterminal trees from root for every token, DOMINO first checks if the model's proposed token has a valid parser path. Only if invalid does it compute the full mask by traversing the tree, saving computation when the model already generates valid tokens. This mechanism assumes language models often naturally generate tokens that are valid continuations in predictable grammars, making the first-check optimization frequently applicable.

## Foundational Learning

- **Context-Free Grammars (CFGs)**: Understanding how grammars define valid sequences and how parsers recognize them is essential since DOMINO enforces CFGs through its parser-scanner architecture. Quick check: What is the difference between a regular language and a context-free language, and why does DOMINO support CFGs but not all regular expressions?

- **Subword Tokenization (BPE)**: DOMINO must align subword tokens with grammar terminals, requiring understanding of how BPE splits words into subword units and how this can misalign with grammar boundaries. Quick check: How does the tokenization of "int32" as ["int", "32"] differ from a grammar that expects "int" as a terminal followed by a number, and why is this misalignment problematic?

- **Non-deterministic Finite Automata (NFA)**: DOMINO builds NFAs for grammar terminals and combines them into a scanner, requiring understanding of NFA construction and traversal algorithms. Quick check: How does Thompson's construction algorithm create an NFA from a regular expression, and how does DOMINO extend this to handle multiple terminals?

## Architecture Onboarding

- **Component map**: Grammar parser -> NFA constructor -> Subterminal tree builder (offline) -> Scanner -> Parser -> Prefix tree traverser -> Speculative decoder -> Opportunistic masker -> LLM inference backend
- **Critical path**: Token generation loop → Scanner advance → Parser update → Prefix tree traversal → Token masking → LLM forward pass → Token selection → Repeat until EOS
- **Design tradeoffs**:
  - Precomputation vs. flexibility: Precomputing subterminal trees enables speed but requires grammars to be known ahead of time; dynamic grammars would need online computation
  - Speculative depth vs. accuracy: Larger speculative batches (s) increase throughput but may reduce accuracy if predictions are wrong; s must be tuned per grammar
  - Lookahead depth (k) vs. coverage: Higher k ensures more complete token coverage but increases traversal time; k=∞ is minimally invasive but may be slower
- **Failure signatures**:
  - Low accuracy with high k: Token misalignment persists despite lookahead; likely grammar terminal definitions are too coarse or vocabulary too fine-grained
  - No speedup with speculative decoding: Grammar is too unpredictable; count-based model cannot learn useful patterns; consider opportunistic masking instead
  - High overhead despite precomputed trees: Prefix trees are too large; vocabulary is too diverse; consider grammar simplification or vocabulary filtering
- **First 3 experiments**:
  1. Basic functionality: Run DOMINO with a simple JSON grammar and Mistral 7B on template-based prompts; verify output validity and measure accuracy vs unconstrained
  2. Lookahead ablation: Vary k from 0 to ∞ on the same JSON task; measure accuracy, perplexity, and throughput to find optimal k
  3. Speculative decoding: Enable speculative decoding with s=5 on schema-driven JSON; compare throughput and accuracy against opportunistic masking mode

## Open Questions the Paper Calls Out

### Open Question 1
How does DOMINO's performance scale with increasingly complex grammars? The paper evaluates DOMINO on JSON, C code, XML, and schema-driven tasks, noting that C code generation incurs the most overhead due to grammar complexity. This remains unresolved as the paper provides limited analysis on how DOMINO's efficiency changes with grammar size and complexity beyond a few examples.

### Open Question 2
What is the impact of speculative decoding's count-based model on DOMINO's accuracy in diverse linguistic contexts? The paper describes a count-based model for speculative decoding that learns from parser states but notes it may not be effective for unpredictable structures like C code. This remains unresolved as the paper does not explore how well the count-based model generalizes across different domains or prompts with varying distributions.

### Open Question 3
Can DOMINO be extended to handle more expressive formalisms beyond context-free grammars, such as context-sensitive grammars or semantic constraints? The paper mentions DOMINO could be extended to other forms of constraining but focuses on context-free grammars and regexes. This remains unresolved as the paper does not explore the theoretical or practical limits of DOMINO's expressiveness beyond CFGs and regexes.

## Limitations
- Precomputation of subterminal trees requires grammars to be known ahead of time, limiting flexibility for dynamic grammars
- Speculative decoding effectiveness depends on grammar predictability, which may not hold for complex language tasks beyond controlled schema generation
- Limited experimental validation across diverse grammar types and vocabulary sizes, particularly for edge cases where token misalignment is severe

## Confidence
- **High confidence**: DOMINO's basic mechanism for subword-token alignment through precomputation is technically sound and the experimental results for simple JSON generation are reproducible
- **Medium confidence**: The claimed 2.7x speedup over unconstrained decoding is supported by experiments but may not generalize to more complex grammars or larger models
- **Low confidence**: Claims about speculative decoding's effectiveness across diverse grammar types lack sufficient experimental validation beyond controlled JSON schema tasks

## Next Checks
1. **Cross-grammar generalization test**: Evaluate DOMINO on C code generation and XML tasks with varying grammar complexities to verify if speedup benefits persist beyond schema-driven JSON, particularly testing the speculative decoding mechanism on less predictable grammars.

2. **Vocabulary stress test**: Systematically vary vocabulary size and subword tokenization granularity to measure how precomputed subterminal trees scale, identifying the point where precomputation overhead outweighs benefits.

3. **Edge case failure analysis**: Generate adversarial test cases where the grammar and vocabulary have high token misalignment potential, measuring accuracy degradation and identifying specific failure modes in the scanner-parser coordination.