---
ver: rpa2
title: 'The Role of Large Language Models in Musicology: Are We Ready to Trust the
  Machines?'
arxiv_id: '2409.01864'
source_url: https://arxiv.org/abs/2409.01864
tags:
- llms
- musicology
- arxiv
- music
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates Large Language Models (LLMs) in musicology,
  finding they are less reliable than retrieval-augmented models using domain-specific
  knowledge. A pilot survey of 33 music professionals shows limited current use and
  trust in LLMs, but high expectations for future impact.
---

# The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?

## Quick Facts
- arXiv ID: 2409.01864
- Source URL: https://arxiv.org/abs/2409.01864
- Reference count: 9
- Current LLMs achieve 32.75-58.75% accuracy on musicology benchmark vs 100% for retrieval-augmented models

## Executive Summary
This work evaluates Large Language Models (LLMs) in musicology, finding they are less reliable than retrieval-augmented models using domain-specific knowledge. A pilot survey of 33 music professionals shows limited current use and trust in LLMs, but high expectations for future impact. The authors created TrustMus, a 400-question benchmark validated by human experts, using a semi-automatic methodology involving retrieval-augmented generation and multiple-choice question generation. Evaluation results show open-source LLMs achieve 32.75-58.75% accuracy on TrustMus, compared to 100% for a retrieval-augmented model using music dictionaries. The study concludes that LLMs in musicology require domain-specific knowledge integration and further research to ensure reliability and trustworthiness.

## Method Summary
The authors created TrustMus, a 400-question musicology benchmark, using a semi-automatic methodology. They first used The New Grove Dictionary of Music and Musicians as a knowledge source, generating questions via retrieval-augmented generation with a fine-tuned RAG LLM. Automated filters removed non-musicology, ambiguous, or too-easy questions, and classified remaining items into four categories. Human experts validated questions until 100 per category were secured. The benchmark was then used to evaluate various open-source LLMs and compare their performance against retrieval-augmented and GPT models.

## Key Results
- Open-source LLMs achieve 32.75-58.75% accuracy on TrustMus benchmark
- Retrieval-augmented model using music dictionaries achieves 100% accuracy
- Survey of 33 music professionals shows limited current trust but high future expectations for LLM impact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-Augmented Generation (RAG) with domain-specific lexicons substantially reduces hallucinations in LLM outputs for musicology.
- Mechanism: RAG models retrieve precise, authoritative definitions from specialized music dictionaries (e.g., The Grove Dictionary) and use them as context for generating answers, ensuring factual grounding.
- Core assumption: The underlying music dictionaries contain comprehensive, authoritative entries that can directly answer the generated questions without additional synthesis.
- Evidence anchors:
  - [abstract] states "current vanilla LLMs are less reliable than retrieval augmented generation from music dictionaries" with accuracy 32.75-58.75% vs. 100% for RAG.
  - [section 3] describes using "The Grove Dictionary Online" as the source text for RAG-based question generation.
  - [corpus] shows related work on RAG and role-based security, implying contextual retrieval is effective in domain-specific tasks.
- Break condition: If the domain lexicon is incomplete or contains outdated entries, RAG accuracy will degrade; also, if retrieval fails to fetch relevant context, the LLM may still hallucinate.

### Mechanism 2
- Claim: A semi-automatic pipeline combining automated filtering, difficulty estimation, and human validation yields a high-quality benchmark despite limited manual effort.
- Mechanism: The workflow first uses a fine-tuned RAG LLM to generate questions, then filters out non-musicology or ambiguous items, applies a difficulty filter by testing simpler models, and finally validates with human experts until target counts per category are met.
- Core assumption: Automated heuristics (e.g., LLM self-assessment of ambiguity, simple model difficulty checks) correlate strongly with true question quality and relevance.
- Evidence anchors:
  - [section 3] describes the exact steps: generating 7,500 questions, filtering to 2,632, then to 3,285, and finally validating 100 per category.
  - [section 4.1] notes the difficulty filter removed questions that simpler models answered correctly, ensuring domain specificity.
  - [corpus] mentions automated scenario-based testing, supporting the idea that automation can reliably filter content before expert review.
- Break condition: If automated filters are too aggressive, valid questions are lost; if too lenient, many low-quality items pass, reducing benchmark utility.

### Mechanism 3
- Claim: Category-balanced sampling (People, Instruments & Technology, Genres/Forms/Theory, Culture & History) ensures comprehensive coverage of musicology subdisciplines.
- Mechanism: After initial automated classification via CoT prompts, human annotators validate until 100 questions per category are secured, preventing overrepresentation of any single subfield.
- Core assumption: Musicology topics are sufficiently separable into these four categories and that 100 questions per category provide stable statistical performance estimates.
- Evidence anchors:
  - [section 3] lists the four categories and mentions "100 valid ones per class were identified (on average, 17% of those assessed were discarded)."
  - [section 4.2] shows model performance broken down by category, indicating coverage was intentional and measured.
  - [corpus] discusses trust terrain mapping in software engineering, implying that balanced categorical analysis is a known best practice.
- Break condition: If the category definitions overlap or are incomplete, questions may be miscategorized, biasing results.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG grounds LLM answers in authoritative sources, mitigating hallucinations in knowledge-intensive domains like musicology.
  - Quick check question: What is the primary difference between vanilla LLM output and RAG-augmented output in terms of factual accuracy?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT enhances LLM reasoning during question generation and classification, improving relevance and reducing ambiguity.
  - Quick check question: How does CoT prompting influence the quality of generated multiple-choice questions?

- Concept: PageRank-like relevance scoring for document selection
  - Why needed here: It identifies the most central and informative articles from the music dictionary, ensuring generated questions target key concepts.
  - Quick check question: Why might a PageRank-like algorithm be preferred over random article selection for benchmark creation?

## Architecture Onboarding

- Component map:
  - Grove Dictionary corpus → PageRank relevance → RAG-based question generator → LLM self-filter → difficulty filter → human validator → TrustMus benchmark.
  - Evaluation pipeline: benchmark questions → multiple open-source LLMs (quantized/local) → accuracy per category → comparison to GPT APIs.

- Critical path:
  1. Retrieve relevant Grove articles.
  2. Generate candidate questions via RAG.
  3. Filter ambiguous/incorrect items.
  4. Apply difficulty filter.
  5. Human validation to meet category quotas.
  6. Evaluate models on validated set.

- Design tradeoffs:
  - Automated filtering reduces manual workload but risks false positives/negatives.
  - Using quantized small models for filtering saves compute but may misclassify edge cases.
  - Category balancing ensures breadth but may dilute depth in any single subfield.

- Failure signatures:
  - Low inter-annotator agreement during validation → category definitions unclear.
  - High dropout rate after difficulty filter → over-stringent criteria.
  - Consistent model underperformance on one category → imbalanced training data in that domain.

- First 3 experiments:
  1. Run the full pipeline on a small subset (e.g., 50 articles) to measure filter precision/recall.
  2. Compare RAG vs. vanilla LLM accuracy on a held-out validation set before human review.
  3. Test whether difficulty filter thresholds can be relaxed without sacrificing domain specificity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can domain-specific knowledge be effectively integrated into large language models to improve their reliability in musicology?
- Basis in paper: [explicit] The paper highlights that current LLMs are less reliable than retrieval-augmented models using domain-specific knowledge from music dictionaries, suggesting the need for specialized models in musicology.
- Why unresolved: The paper identifies the need for domain-specific knowledge but does not provide a detailed methodology or framework for integrating such knowledge into LLMs.
- What evidence would resolve it: Developing and testing a framework or model that successfully integrates musicology-specific knowledge into LLMs, demonstrating improved accuracy and reliability in musicology-related tasks.

### Open Question 2
- Question: What are the ethical and legal implications of using large language models in musicology, particularly concerning data privacy and copyright issues?
- Basis in paper: [explicit] The paper mentions the need to ensure LLMs' reliability to avoid misinformation, protect user privacy, and mitigate training data biases, but does not delve into specific ethical or legal challenges.
- Why unresolved: The paper acknowledges these concerns but does not explore the specific ethical and legal challenges or propose solutions to address them.
- What evidence would resolve it: Conducting a comprehensive study on the ethical and legal implications of using LLMs in musicology, including case studies and proposed guidelines for responsible use.

### Open Question 3
- Question: How can interdisciplinary collaboration between technologists, musicologists, and content owners enhance the development and application of large language models in musicology?
- Basis in paper: [explicit] The paper suggests that collaboration between technological, musicological, and content owner communities is essential for the proper development of LLMs in musicology.
- Why unresolved: While the paper emphasizes the importance of collaboration, it does not provide a detailed strategy or framework for fostering such interdisciplinary partnerships.
- What evidence would resolve it: Establishing and evaluating a collaborative framework that brings together technologists, musicologists, and content owners to develop and refine LLMs for musicology, with measurable outcomes and benefits.

## Limitations

- Benchmark size (400 questions) may be insufficient for reliable performance estimation across all musicology domains
- Human validation process lacks transparency regarding inter-annotator agreement and specific criteria
- 100% accuracy claim for RAG model assumes perfect retrieval and direct dictionary answers, which may not hold in practice

## Confidence

- High Confidence: The general finding that retrieval-augmented models outperform vanilla LLMs in domain-specific tasks is well-supported by the 32.75-58.75% vs. 100% accuracy comparison. The survey results showing limited current use but high future expectations among music professionals are straightforward descriptive statistics.
- Medium Confidence: The semi-automatic benchmark creation methodology is plausible and well-described, but the lack of detailed validation criteria and inter-annotator agreement statistics introduces uncertainty about the quality and consistency of the final benchmark.
- Low Confidence: The specific accuracy numbers for individual models and categories should be interpreted cautiously due to the small benchmark size and potential sampling bias in question selection.

## Next Checks

1. Replicate the benchmark creation pipeline on a small subset (e.g., 50 articles) to measure the precision and recall of automated filters and estimate inter-annotator agreement rates.

2. Test model performance on a held-out validation set before human review to assess whether the difficulty filter is too aggressive and causing loss of valid questions.

3. Analyze per-category performance variance to determine if any musicological subfields are systematically underrepresented or if model weaknesses align with specific knowledge domains.