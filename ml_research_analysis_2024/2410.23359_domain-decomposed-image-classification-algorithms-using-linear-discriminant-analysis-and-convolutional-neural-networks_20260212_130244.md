---
ver: rpa2
title: Domain-decomposed image classification algorithms using linear discriminant
  analysis and convolutional neural networks
arxiv_id: '2410.23359'
source_url: https://arxiv.org/abs/2410.23359
tags:
- classification
- data
- training
- image
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents three domain-decomposed approaches for image
  classification: two CNN-based models with transfer learning (CNN-DNN-transfer and
  DD-CNN-transfer) and a novel LDA-DNN approach. The CNN models decompose input images
  into subimages, train local networks in parallel, and combine results through transfer
  learning to improve accuracy and reduce training time.'
---

# Domain-decomposed image classification algorithms using linear discriminant analysis and convolutional neural networks

## Quick Facts
- arXiv ID: 2410.23359
- Source URL: https://arxiv.org/abs/2410.23359
- Reference count: 27
- Primary result: Domain-decomposed CNN approaches achieve up to 91.2% accuracy with 1.57-2.3x speedup

## Executive Summary
This paper presents three domain-decomposed approaches for image classification: two CNN-based models with transfer learning (CNN-DNN-transfer and DD-CNN-transfer) and a novel LDA-DNN approach. The CNN models decompose input images into subimages, train local networks in parallel, and combine results through transfer learning to improve accuracy and reduce training time. The LDA-DNN approach similarly decomposes images but uses local LDA projections combined with a DNN for classification. Experiments on CIFAR-10, TF-Flowers, and chest CT datasets show that both CNN approaches achieve similar high accuracy (up to 91.2%) while reducing training time by 1.57-2.3x compared to global models. The LDA-DNN performs comparably to global LDA but worse than CNN approaches, though it offers advantages in hyperparameter tuning and deterministic behavior.

## Method Summary
The paper proposes three domain-decomposed image classification methods. The CNN-DNN-transfer approach trains local CNNs on non-overlapping subimages in parallel, then combines their probability outputs through a small DNN for global classification. The DD-CNN-transfer method decomposes the CNN architecture itself into sub-networks, each trained on subimages, then composes them into a global CNN using transfer learning. The LDA-DNN approach computes local LDA projections for each subimage, concatenates the resulting probability distributions, and uses a DNN to learn the optimal fusion for global classification. All approaches use 2×2 or 4×4 decomposition depending on image size, with CNNs trained for 150 epochs locally followed by 50 epochs of global training with transfer learning initialization.

## Key Results
- CNN-DNN-transfer and DD-CNN-transfer achieve 91.2% accuracy on CIFAR-10 and TF-Flowers with 1.57-2.3x speedup
- LDA-DNN achieves comparable accuracy to global LDA but shows overfitting (20% gap between training and validation)
- Chest CT experiments show 91.2% accuracy but are unreliable due to only 9 positive COVID-19 cases
- All approaches demonstrate successful parallelization and reduced training time compared to global models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain decomposition combined with transfer learning improves CNN accuracy and reduces training time
- Mechanism: Images are decomposed into subimages, each processed by proportionally smaller CNNs trained in parallel. Local CNN weights initialize a global network, requiring fewer epochs to reach high accuracy
- Core assumption: Subimage-local CNNs capture meaningful local features that generalize when composed into a global decision
- Evidence anchors: Both CNN models show improved classification accuracies compared to global CNN without transfer learning while speeding up training
- Break condition: If local subimages do not preserve discriminative class boundaries, the global model will fail to converge or overfit

### Mechanism 2
- Claim: Decomposing LDA into local subspaces and fusing via DNN improves classification over global LDA
- Mechanism: Each subimage is projected onto its own low-dimensional LDA space; local probability distributions are concatenated and fed to a small DNN that learns the optimal fusion for global classification
- Core assumption: Local LDA projections retain discriminative information sufficient for accurate global classification when combined nonlinearly
- Evidence anchors: LDA-DNN shows increased classification accuracies compared to global LDA for considered test problems
- Break condition: If local subspaces are too low-dimensional or too small, discriminative information is lost and the DNN cannot recover it

### Mechanism 3
- Claim: The CNN-DNN-coherent model with transfer learning achieves higher accuracy than training from scratch
- Mechanism: After parallel local CNN training, their weights initialize a unified CNN-DNN model, which is then trained end-to-end; transfer learning accelerates convergence to better minima
- Core assumption: Initializing with local CNN parameters provides a better starting point for the global optimization landscape
- Evidence anchors: Using CNN-DNN-coherent model with transfer learning leads to improved classification accuracies compared to training without transfer learning
- Break condition: If transfer initialization is too far from optimal global solution, further training may diverge or stagnate

## Foundational Learning

- Concept: Domain decomposition in numerical PDEs (e.g., additive Schwarz methods)
  - Why needed here: Provides mathematical analogy for splitting image domain and combining local solutions globally
  - Quick check question: In additive Schwarz, what property must local solvers preserve to ensure global convergence?

- Concept: Transfer learning in deep learning
  - Why needed here: Explains why initializing global model with pretrained local weights accelerates convergence and improves accuracy
  - Quick check question: In transfer learning, what is the difference between feature extraction and fine-tuning?

- Concept: Linear discriminant analysis and Fisher's criterion
  - Why needed here: Forms basis for computing optimal low-dimensional projections in LDA-DNN approach
  - Quick check question: What is the objective function maximized in LDA, and how does it relate to class separability?

## Architecture Onboarding

- Component map:
  - Input: Original image (HxW or HxWxD)
  - Decomposition: Non-overlapping subimages (N partitions)
  - Local models: Proportional CNNs or LDA instances
  - Fusion: DNN (CNN-DNN) or concatenated probabilities (LDA-DNN)
  - Output: Global class probabilities

- Critical path:
  1. Decompose input into subimages
  2. Train local models in parallel
  3. Initialize or fuse local outputs
  4. Train/evaluate global model
  5. Return final classification

- Design tradeoffs:
  - Decomposition granularity (N): More subimages → more parallelism but risk of losing global context
  - DNN complexity in CNN-DNN/LDA-DNN: Larger DNN → better fusion but longer training and overfitting risk
  - Transfer learning epochs: More epochs → better fine-tuning but diminishing returns and longer training

- Failure signatures:
  - Accuracy stalls or drops after fusion: Likely local models not capturing discriminative features
  - Overfitting on training data but poor validation: Fusion model too complex or local subspaces too low-dimensional
  - Slow convergence: Poor initialization or suboptimal decomposition

- First 3 experiments:
  1. Train local CNNs on 2×2 subimages of CIFAR-10 and measure individual vs fused accuracy
  2. Replace CNN fusion with simple averaging; compare to DNN fusion to quantify benefit
  3. Sweep LDA subspace dimension d on CIFAR-10; plot training vs validation accuracy to find optimal d

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of dimension parameter d affect the classification accuracy of the LDA-DNN approach, and what strategies could optimize this choice?
- Basis in paper: The paper notes that LDA-DNN uses a fixed choice of d for all local LDAs and suggests that more sophisticated strategies might exist, such as choosing higher values of d for subimages covering more classes
- Why unresolved: The paper uses a constant choice of d for all considered subspaces and does not explore adaptive strategies for selecting d based on the characteristics of local subimages
- What evidence would resolve it: Experiments comparing classification accuracies using different strategies for choosing d, such as adaptive selection based on class distribution in subimages, would provide insights into optimal dimension selection

###