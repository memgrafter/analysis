---
ver: rpa2
title: How Interpretable Are Interpretable Graph Neural Networks?
arxiv_id: '2406.07955'
source_url: https://arxiv.org/abs/2406.07955
tags:
- subgraph
- graph
- interpretable
- cited
- submt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the expressivity of interpretable graph neural
  networks (XGNNs) using subgraph multilinear extension (SubMT). It shows that existing
  XGNNs fail to accurately approximate SubMT, leading to degraded interpretability.
---

# How Interpretable Are Interpretable Graph Neural Networks?

## Quick Facts
- **arXiv ID**: 2406.07955
- **Source URL**: https://arxiv.org/abs/2406.07955
- **Reference count**: 40
- **Primary result**: Proposes GMT, a new interpretable GNN architecture that significantly improves interpretability and generalizability over state-of-the-art methods by up to 10%

## Executive Summary
This paper analyzes the expressivity of interpretable graph neural networks (XGNNs) using subgraph multilinear extension (SubMT). It shows that existing XGNNs fail to accurately approximate SubMT, leading to degraded interpretability. To address this, the authors propose a new architecture called Graph Multilinear neT (GMT) with two variants: GMT-lin and GMT-sam. GMT-lin uses a linearized approach, while GMT-sam employs random subgraph sampling to better approximate SubMT. Experiments on 12 graph benchmarks demonstrate that GMT significantly improves interpretability and generalizability over state-of-the-art methods by up to 10%, across both regular and geometric graphs.

## Method Summary
The paper proposes Graph Multilinear neT (GMT) with two variants: GMT-lin (linearized approach) and GMT-sam (random subgraph sampling). GMT-lin modifies message passing with single attention application, while GMT-sam uses Monte Carlo sampling of subgraphs to better approximate the SubMT objective. The method involves training a subgraph extractor using information bottleneck, sampling discrete subgraphs, and retraining a classifier on the extracted subgraphs. The approach is evaluated on 12 graph benchmarks including BA-2Motifs, MUTAG, MNIST-75sp, Spurious-Motif, Graph-SST2, OGBG-Molhiv, ACTS Track, TAU3MU, SYNMOL, PLBIND, CITESEER, PUBMED, COAUTHOR-CS, and COAUTHOR-PHYSICS.

## Key Results
- GMT-sam with random subgraph sampling achieves up to 10% improvement in interpretability and generalizability over state-of-the-art methods
- Counterfactual fidelity effectively measures both interpretability and generalization performance of XGNNs
- Existing XGNNs fail to accurately approximate subgraph multilinear extension (SubMT), leading to degraded interpretability
- GMT-lin and GMT-sam variants show consistent improvements across regular and geometric graph benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Existing XGNNs fail to accurately approximate subgraph multilinear extension (SubMT), leading to degraded interpretability and generalization.
- Mechanism: The SubMT formulation requires evaluating a weighted expectation of subgraph predictions, but existing methods approximate this with a single "soft" subgraph input to the classifier, which only equals the expectation under linearity conditions that rarely hold in practice.
- Core assumption: SubMT provides the theoretically correct formulation for subgraph-based graph classification, and the approximation error directly impacts both interpretability and generalization.
- Evidence anchors:
  - [abstract]: "we find that the existing XGNNs can have a huge gap in fitting SubMT. Consequently, the SubMT approximation failure will lead to the degenerated interpretability"
  - [section]: "fc(bGc) = fc(E[Gc]) = EGc [fc(Gc)], where the last equality adheres to the equality of Eq. 7. Obviously fc(·) is a non-linear function"
- Break condition: If the classifier GNN is truly linear or if the attention scores are binary (0 or 1), then the approximation error vanishes.

### Mechanism 2
- Claim: GMT-sam with random subgraph sampling provides a more accurate approximation of SubMT than existing methods.
- Mechanism: By drawing multiple i.i.d. samples from the subgraph distribution and averaging classifier predictions, GMT-sam estimates the expectation in SubMT directly rather than approximating it with a soft subgraph, with accuracy improving as the number of samples increases.
- Core assumption: The SubMT objective can be accurately estimated through Monte Carlo sampling, and the variance decreases with more samples.
- Evidence anchors:
  - [abstract]: "GMT-sam employs random subgraph sampling to better approximate SubMT"
  - [section]: "Theorem 5.1. Given the attention matrix bA... GMT-sam ϵC/2 -approximates SubMT and satisfies (δ, 1 − ϵC/δ) counterfactual fidelity"
- Break condition: If the subgraph distribution is highly peaked around a single subgraph, random sampling provides little benefit over the soft subgraph approach.

### Mechanism 3
- Claim: The counterfactual fidelity metric effectively measures both interpretability and generalization performance of XGNNs.
- Mechanism: Counterfactual fidelity measures how sensitive predictions are to small perturbations of the extracted subgraph, which captures both the faithfulness of the interpretation and the model's ability to generalize under distribution shifts.
- Core assumption: A faithful interpretation should lead to predictions that are sensitive to changes in the interpreted subgraph, and this sensitivity correlates with generalization performance.
- Evidence anchors:
  - [abstract]: "we consider a direct notion that jointly consider the interpretability and generalizabiliy to measure the causal faithfulness of XGNNs"
  - [section]: "if the extracted interpretable subgraph bGc is faithful to the target label, then the predictions made based on bGc are sensitive to any perturbations on bGc"
- Break condition: If the model is overly confident or the perturbations are too small to change predictions meaningfully, counterfactual fidelity may not correlate with true interpretability.

## Foundational Learning

- Concept: Submodular function maximization and multilinear extension
  - Why needed here: The paper connects interpretable subgraph learning to multilinear extension, which is a key tool for optimizing submodular functions
  - Quick check question: What is the key property that makes multilinear extension useful for submodular function maximization?

- Concept: Information bottleneck principle
  - Why needed here: XGNNs use information bottleneck to identify subgraphs that are informative about labels while being minimally dependent on the input graph
  - Quick check question: How does the information bottleneck principle balance between retaining label information and minimizing input dependency?

- Concept: Counterfactual reasoning and causal inference
  - Why needed here: The paper uses counterfactual fidelity to measure interpretability, which requires understanding how predictions change under hypothetical interventions
  - Quick check question: What is the difference between counterfactual reasoning and associational reasoning in causal inference?

## Architecture Onboarding

- Component map:
  - Graph encoder (GIN/PNA/EGNN) -> Node representations
  - Subgraph extractor (attention mechanism) -> Edge sampling probabilities
  - GMT-sam: Random subgraph sampler (Gumbel-softmax + MCMC) -> Discrete subgraphs
  - Classifier GNN -> Predictions (with/without retraining)
  - GMT-lin: Modified message passing with single attention application

- Critical path:
  1. Forward pass through graph encoder
  2. Compute attention scores for edges/nodes
  3. For GMT-sam: Sample discrete subgraphs using Gumbel-softmax
  4. Pass subgraphs through classifier (multiple times for GMT-sam)
  5. Compute loss and backpropagate
  6. For GMT-sam: Retrain classifier with frozen subgraph extractor

- Design tradeoffs:
  - Sampling rounds vs. computational cost (GMT-sam)
  - Weighted message passing vs. linearity (GMT-lin)
  - Shared vs. separate encoders for subgraph extraction and classification
  - Edge-centric vs. node-centric sampling approaches

- Failure signatures:
  - Low counterfactual fidelity despite high accuracy
  - Degraded performance with increased sampling rounds
  - Trivial solutions where attention scores collapse to prior
  - Optimization instability during subgraph learning

- First 3 experiments:
  1. Compare GMT-sam with different numbers of sampling rounds on BA-2Motifs to find the sweet spot
  2. Validate that GMT-lin outperforms GSAT on simple linear GNN architectures
  3. Test counterfactual fidelity on a synthetic dataset where ground truth subgraphs are known

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the number of subgraph sampling rounds and the counterfactual fidelity? Is there an optimal number beyond which the fidelity plateaus or even degrades?
- Basis in paper: [explicit] The paper states "Although the estimation of SubMT will be more accurate with the increased sampling rounds, it unnecessarily brings improvements. First, as shown in Fig. 3, the performance may be saturated with moderately sufficient samplings. Besides, the performance may degenerate as more sampling rounds can affect the optimization."
- Why unresolved: The paper only shows a qualitative relationship in Fig. 3, not a quantitative one. It does not provide a formula or threshold for optimal sampling rounds.
- What evidence would resolve it: A detailed study plotting counterfactual fidelity against sampling rounds for multiple datasets and architectures, showing the saturation point and any degradation at high sampling rates.

### Open Question 2
- Question: How does the SubMT approximation gap vary with different graph neural network architectures beyond the linearized GNN? Are there specific architectural choices that inherently minimize this gap?
- Basis in paper: [explicit] "When given more complicated GNNs, the approximation error to SubMT can be even higher, as verified in Appendix F.6."
- Why unresolved: The paper only provides empirical evidence for a limited set of architectures (GIN, PNA, EGNN) and does not analyze the underlying reasons for the gap in different architectures.
- What evidence would resolve it: A theoretical analysis of how different GNN architectural components (e.g., attention mechanisms, aggregation functions, depth) affect the SubMT approximation gap, along with empirical validation on a diverse set of architectures.

### Open Question 3
- Question: Can the neural SubMT learned by GMT be generalized across different tasks or datasets, or is it task-specific? What are the limitations of this approach?
- Basis in paper: [inferred] The paper proposes learning a neural SubMT to reduce computational overhead, but does not explore its generalization capabilities.
- Why unresolved: The paper only demonstrates the effectiveness of neural SubMT within the same dataset and task. It does not investigate whether the learned model can be transferred or adapted to new tasks or datasets.
- What evidence would resolve it: Experiments evaluating the performance of GMT with neural SubMT on unseen tasks or datasets, and an analysis of the factors that influence its generalization ability.

## Limitations
- The computational cost of GMT-sam with multiple sampling rounds raises practical deployment concerns, particularly for large graphs
- The assumption that improved SubMT approximation directly translates to better generalization needs more rigorous validation across diverse domain shifts
- Empirical evidence for interpretability gains is primarily measured through counterfactual fidelity, which lacks direct comparison to established interpretability metrics

## Confidence

**High confidence**: The theoretical analysis of SubMT approximation failures in existing XGNNs and the mechanism by which GMT-sam improves this approximation through sampling

**Medium confidence**: The empirical claims about 10% improvement in interpretability and generalization, as these are evaluated on a specific set of benchmarks and may not generalize to all graph domains

**Medium confidence**: The claim that counterfactual fidelity effectively captures both interpretability and generalization, as this metric is novel and its correlation with human-interpretable explanations needs further validation

## Next Checks
1. **Ablation study on sampling rounds**: Systematically vary the number of sampling rounds in GMT-sam across different graph sizes to establish the trade-off between approximation accuracy and computational cost
2. **Cross-domain generalization test**: Evaluate GMT's performance when training and test graphs come from different domains (e.g., molecular graphs vs. social networks) to validate the generalization claims
3. **Human evaluation of interpretability**: Conduct user studies comparing GMT-generated explanations with those from existing XGNNs to validate that improved counterfactual fidelity correlates with human-interpretable explanations