---
ver: rpa2
title: Calibrating the Confidence of Large Language Models by Eliciting Fidelity
arxiv_id: '2404.02655'
source_url: https://arxiv.org/abs/2404.02655
tags:
- confidence
- language
- ours
- calibration
- fidelity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of overconfidence in RLHF-optimized
  large language models by decomposing model confidence into two components: uncertainty
  about the question and fidelity to the generated answer. The proposed UF Calibration
  method estimates confidence by sampling multiple responses to measure uncertainty
  (via information entropy) and then probing fidelity through iterative option replacement
  with "All other options are wrong." Experiments on six RLHF language models (including
  GPT-3.5-Turbo, GPT-4-Turbo, and LLaMA2 variants) across four MCQA datasets show
  significant improvements in calibration metrics (ECE, IPR, CE) compared to baseline
  methods.'
---

# Calibrating the Confidence of Large Language Models by Eliciting Fidelity

## Quick Facts
- arXiv ID: 2404.02655
- Source URL: https://arxiv.org/abs/2404.02655
- Authors: Mozhi Zhang; Mianqiu Huang; Rundong Shi; Linsen Guo; Chong Peng; Peng Yan; Yaqian Zhou; Xipeng Qiu
- Reference count: 26
- Primary result: UF Calibration method significantly improves confidence calibration in RLHF models across 6 models and 4 datasets without requiring access to per-token logits

## Executive Summary
This paper addresses the problem of overconfidence in RLHF-optimized large language models by proposing UF Calibration, a method that decomposes model confidence into uncertainty about the question and fidelity to the generated answer. The approach samples multiple responses to estimate uncertainty via entropy and then probes fidelity through iterative option replacement with "All other options are wrong." Experiments demonstrate significant improvements in calibration metrics (ECE, IPR, CE) compared to baseline methods while maintaining compatibility with black-box models. The authors introduce two novel metrics (IPR for monotonicity and CE for confidence evenness) and argue that truly well-calibrated confidence should balance all three metrics rather than optimizing any single one.

## Method Summary
The UF Calibration method decomposes model confidence into uncertainty and fidelity components. First, it samples K answers for each question to estimate uncertainty via the entropy of the answer distribution. Then, it builds a fidelity chain by iteratively replacing each candidate answer with "All other options are wrong" and observing if the model still selects that option. The final confidence score is calculated as Conf(Q, ai) = (1 - Uncertainty(Q)) * F(ai), where F(ai) is the weighted average fidelity across the hierarchical chain. This approach achieves good calibration without requiring access to per-token logits, making it broadly applicable to black-box models.

## Key Results
- UF Calibration consistently achieves the lowest expected calibration error (ECE) across all temperatures and parameter scales
- The method demonstrates significant improvements in calibration metrics (ECE, IPR, CE) compared to baseline methods across 6 RLHF models and 4 MCQA datasets
- Temperature robustness is achieved through greedy decoding for fidelity elicitation, showing stable calibration performance across temperature variations
- The proposed IPR and CE metrics effectively capture different aspects of calibration quality beyond traditional ECE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UF Calibration works by decomposing confidence into uncertainty and fidelity, then estimating each separately.
- Mechanism: The method first samples multiple responses to estimate uncertainty via entropy of the answer distribution. It then elicits fidelity by replacing each candidate answer with "All other options are wrong" and observing if the model still selects that option, creating a hierarchical chain that indicates confidence in each answer.
- Core assumption: A model's confidence in an answer correlates with its resistance to switching when that answer is isolated against other options.
- Evidence anchors:
  - [abstract]: "We decompose the language model confidence into the Uncertainty about the question and the Fidelity to the answer generated by language models."
  - [section 3.2]: "If the model has high fidelity to the previously selected answer (ai, oi), it should select (ai, 'All other options are wrong.') in the subsequent round of inquiry rather than any other option."
  - [corpus]: No direct corpus evidence for the "All other options are wrong" probing technique specifically.

### Mechanism 2
- Claim: The proposed method achieves good calibration by balancing three metrics: ECE, IPR, and CE.
- Mechanism: ECE measures overall calibration error, IPR ensures monotonicity (low confidence correlates with low accuracy), and CE ensures confidence is evenly distributed rather than clustered in specific ranges. The authors argue truly well-calibrated confidence should balance all three rather than optimizing any single metric.
- Core assumption: A well-calibrated model should distribute confidence across different confidence levels rather than always expressing high confidence.
- Evidence anchors:
  - [section 5]: "We believe ECE, IPR, and CE evaluate calibration from different perspectives and there is a trade-off between these three metrics. We suggest that truly well-calibrated confidence should achieve a balance among ECE, IPR, and CE."
  - [abstract]: "We propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on Truly Well-Calibrated Confidence."
  - [corpus]: No direct corpus evidence for the specific combination of these three metrics as the definition of "truly well-calibrated confidence."

### Mechanism 3
- Claim: The method is robust to temperature scaling and parameter scaling variations.
- Mechanism: Unlike logit-based methods that require careful temperature tuning, UF Calibration uses greedy decoding for fidelity elicitation which produces consistent hierarchical chains regardless of sampling temperature. As model size increases, both self-awareness and calibration improve.
- Core assumption: Greedy decoding for fidelity elicitation produces temperature-invariant results.
- Evidence anchors:
  - [section 4.3]: "Our proposed calibration method consistently achieves the lowest expected calibration error across all temperatures, showing remarkable robustness to temperature variations. This is because, in eliciting model fidelity, our method always employs Greedy Decoding rather than Sampling."
  - [section 5]: "As shown in Figure 5, we evaluate the calibration of various methods at different parameter scales on the LLaMA2-Chat series models. Our proposed method exhibits good calibration across different amounts of model parameters."
  - [corpus]: No direct corpus evidence for temperature robustness claims beyond this paper.

## Foundational Learning

- Concept: Information entropy as uncertainty measure
  - Why needed here: The method uses entropy of the answer frequency distribution to quantify uncertainty about the question.
  - Quick check question: If a model samples answers [A, A, B, A, C] from 5 trials, what would be the entropy-based uncertainty score?

- Concept: Hierarchical chain construction for fidelity estimation
  - Why needed here: The method builds chains by iteratively replacing answers with "All other options are wrong" to determine how resistant the model is to changing its answer.
  - Quick check question: If a model's fidelity chain is A→B→C, what does this imply about the model's confidence in option A versus C?

- Concept: Reliability diagram interpretation
  - Why needed here: The method introduces IPR and CE metrics that require understanding how confidence bins relate to accuracy in reliability diagrams.
  - Quick check question: If a reliability diagram shows most predictions in the 0.8-0.9 confidence bin but accuracy is only 60%, what does this indicate about calibration?

## Architecture Onboarding

- Component map: Sampling -> Entropy Calculation -> Fidelity Elicitation -> Confidence Estimation
- Critical path: Sampling → Fidelity Elicitation → Confidence Estimation. Each phase depends on the previous one, with fidelity elicitation being the most computationally expensive.
- Design tradeoffs: The method trades computational cost (multiple model invocations) for black-box compatibility. The hierarchical chain construction requires O(N) model calls per answer, but avoids needing logits.
- Failure signatures: If confidence estimates are consistently too high/too low across datasets, check the entropy calculation or fidelity chain construction. If results vary wildly with temperature, verify greedy decoding is being used correctly.
- First 3 experiments:
  1. Test with a simple 2-option question and K=10 to verify entropy calculation and basic confidence estimation.
  2. Test with a question where you know the correct answer to verify the fidelity chain construction correctly identifies high-confidence answers.
  3. Test with temperature variation (0.1, 1.0, 2.0) to verify temperature robustness of the method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the UF Calibration method perform on open-ended generation tasks beyond multiple-choice questions?
- Basis in paper: [inferred] The paper mentions that the method is mainly applicable to scenarios where the set of answers is known, such as multiple-choice question answering, text classification, sentiment classification, and preference labeling in RLHF.
- Why unresolved: The paper primarily focuses on MCQA datasets and does not explore the method's applicability to open-ended generation tasks.
- What evidence would resolve it: Experiments applying UF Calibration to open-ended generation tasks, such as summarization or dialogue generation, would demonstrate its broader applicability.

### Open Question 2
- Question: Can the number of model invocations be reduced while maintaining calibration accuracy?
- Basis in paper: [explicit] The paper acknowledges that the method involves multiple invocations of language models and mentions that reducing the number of callings is an area for future study.
- Why unresolved: The paper does not provide a detailed analysis of how the number of invocations affects calibration accuracy or explore techniques to reduce invocations.
- What evidence would resolve it: Comparative experiments showing calibration performance with varying numbers of model invocations, and the development of techniques to achieve similar accuracy with fewer invocations.

### Open Question 3
- Question: How does the UF Calibration method handle questions with more than five options?
- Basis in paper: [inferred] The paper evaluates the method on datasets with four to five options, but does not explicitly discuss performance on questions with more options.
- Why unresolved: The paper does not provide results or analysis for questions with more than five options, leaving uncertainty about scalability.
- What evidence would resolve it: Experiments testing the method on datasets with questions containing more than five options, such as those with 10 or more choices, would clarify its scalability.

## Limitations
- The "All other options are wrong" probing technique lacks extensive validation across diverse question types and model architectures
- Computational cost scales linearly with the number of options and sampling budget, potentially limiting practical deployment
- The claim that truly well-calibrated confidence requires balancing ECE, IPR, and CE is presented as a novel insight but lacks strong empirical justification

## Confidence

- **High Confidence**: The core mechanism of decomposing confidence into uncertainty and fidelity (Mechanism 1) is well-grounded in information theory and shows consistent results across multiple models and datasets. The temperature robustness claim (Mechanism 3) is directly supported by experimental results showing stable ECE across temperature variations.

- **Medium Confidence**: The claim about balancing ECE, IPR, and CE as the definition of "truly well-calibrated confidence" (Mechanism 2) is conceptually interesting but the supporting evidence is primarily comparative rather than demonstrating why this specific balance is optimal. The calibration improvements over baselines are statistically significant but the practical significance in real-world applications remains unclear.

- **Low Confidence**: The effectiveness of the "All other options are wrong" probing technique for fidelity elicitation has limited external validation. While the method shows good results in the tested scenarios, its robustness to adversarial questions or models with different reasoning patterns is uncertain.

## Next Checks

1. **Cross-domain robustness test**: Apply UF Calibration to non-MCQA tasks (e.g., open-ended generation with confidence scoring) to verify the method's generalizability beyond the tested format.

2. **Ablation study on fidelity probing**: Compare the "All other options are wrong" technique against simpler fidelity elicitation methods (e.g., direct confidence scoring, entropy-based confidence) to quantify the specific contribution of the hierarchical chain approach.

3. **Real-world deployment simulation**: Implement a cost-benefit analysis measuring the tradeoff between improved calibration and computational overhead in a realistic deployment scenario with rate limits and latency constraints.