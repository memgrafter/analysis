---
ver: rpa2
title: Language Model Alignment in Multilingual Trolley Problems
arxiv_id: '2407.02273'
source_url: https://arxiv.org/abs/2407.02273
tags:
- moral
- languages
- language
- sparing
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the moral alignment of large language models
  (LLMs) with human preferences across 107 languages using a multilingual adaptation
  of the Moral Machine experiment. The authors develop the Multilingual Trolley Problems
  (MULTITP) dataset, which includes 98,440 trolley problem scenarios translated into
  107 languages, covering six moral dimensions: species, gender, fitness, status,
  age, and number of lives.'
---

# Language Model Alignment in Multilingual Trolley Problems

## Quick Facts
- arXiv ID: 2407.02273
- Source URL: https://arxiv.org/abs/2407.02273
- Reference count: 40
- Primary result: Most LLMs show significant misalignment with human moral preferences across 107 languages

## Executive Summary
This paper evaluates the moral alignment of large language models with human preferences across 107 languages using a multilingual adaptation of the Moral Machine experiment. The authors develop the Multilingual Trolley Problems (MULTITP) dataset containing 98,440 trolley problem scenarios translated into 107 languages, covering six moral dimensions: species, gender, fitness, status, age, and number of lives. They assess 19 different LLMs and find that most models do not align well with human preferences, with misalignment scores ranging from 0.55 to 1.45 (0 indicates perfect alignment). Only three models (Llama 3.1 70B, Llama 3 70B, and Llama 3 8B) show misalignment scores below 0.6.

## Method Summary
The study procedurally generates 98,440 trolley problem vignettes with systematic variation across six moral dimensions, then translates them into 107 languages using Google Translate. Researchers evaluate 19 LLMs using token-forcing prompts and apply jailbreaking techniques to reduce refusal rates. They compute misalignment scores by comparing model preferences to human judgments from 40 million responses across 233 countries, analyzing language sensitivity and clustering patterns while testing correlation with language resource levels.

## Key Results
- Most LLMs show misalignment scores between 0.55-1.45, with only three models below 0.6
- Language sensitivity varies significantly (14.7-24.7 standard deviation) across languages
- No significant correlation found between alignment scores and language resource levels
- Jailbroken models show different alignment patterns but don't necessarily improve cross-lingual consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual alignment of LLMs with human moral preferences is not strongly correlated with language resource levels.
- Mechanism: The dataset uses procedurally generated trolley problems and controlled translations rather than relying on pre-existing text from the web, which removes bias toward high-resource languages.
- Core assumption: Procedural generation and Google Translate ensure equal exposure and quality across languages.
- Evidence anchors: Abstract states no significant correlation found; section notes this challenges "language inequality" hypothesis.

### Mechanism 2
- Claim: Systematic parametric variation in trolley problem design allows isolation of moral preference dimensions.
- Mechanism: Procedural generation creates 98,440 vignettes by varying six moral dimensions (species, gender, fitness, status, age, number of lives) using controlled templates.
- Core assumption: Controlled variation eliminates confounding factors and ensures each dimension is tested independently.
- Evidence anchors: Abstract mentions capturing preferences across six moral dimensions; section describes procedural generation enabling systematic variation.

### Mechanism 3
- Claim: Jailbreaking techniques reduce refusal rates and reveal underlying moral preferences in LLMs.
- Mechanism: Applying an uncensoring technique (Arditi et al., 2025) to models with high refusal rates steers them away from refusals on ethically sensitive instructions.
- Core assumption: Jailbreak successfully removes safety layer without altering model's internal moral reasoning.
- Evidence anchors: Abstract discusses jailbreaking to measure increased bias; section describes applying uncensoring technique to models with high refusal rates.

## Foundational Learning

- Concept: Cross-lingual evaluation of AI alignment
  - Why needed here: The study's core contribution is assessing how well LLMs align with human moral judgments across 107 languages, requiring understanding of multilingual NLP and cross-cultural bias.
  - Quick check question: What is the difference between testing alignment in a single language versus multiple languages, and why does it matter for fairness?

- Concept: Procedural generation of controlled test data
  - Why needed here: The dataset is built by generating trolley problem vignettes with systematic variation rather than using static human-written prompts, ensuring coverage and control.
  - Quick check question: How does procedural generation help isolate specific moral dimensions compared to using naturally occurring text?

- Concept: Evaluation metrics for alignment
  - Why needed here: The misalignment score (L2 distance between human and model preference vectors) quantifies alignment; understanding its construction is key to interpreting results.
  - Quick check question: If a model's misalignment score is 0.6, what does that mean in terms of preference differences across the six moral dimensions?

## Architecture Onboarding

- Component map: Procedural vignette generation -> Multilingual translation (Google Translate) -> Dataset assembly (98,440 vignettes in 107 languages) -> 19 LLMs evaluation -> Token-forced prompt format -> Binary choice output -> Compute per-dimension preferences -> Aggregate to misalignment score -> Analyze by language and country

- Critical path:
  1. Generate vignettes in English with systematic variation across six moral dimensions
  2. Translate vignettes into 107 languages using Google Translate
  3. For each LLM, run inference on all vignettes in all languages with temperature=0 and token forcing
  4. Compute per-language misalignment scores by comparing model preferences to human judgments
  5. Aggregate and analyze results (global scores, language sensitivity, clustering, correlation with speaker counts)

- Design tradeoffs:
  - Procedural generation ensures control but may lack naturalness of human-written prompts
  - Google Translate is fast and covers many languages but may introduce subtle translation biases
  - Token forcing reduces refusals but may not reflect natural model behavior
  - Aggregating by country via weighted language averages is practical but masks dialect differences

- Failure signatures:
  - High refusal rates -> incomplete preference vectors -> inflated misalignment
  - Language sensitivity scores near zero -> possible bias toward high-resource languages
  - Strong correlation between misalignment and speaker count -> resource-based inequality
  - Inconsistent outputs across paraphrases -> low robustness

- First 3 experiments:
  1. Run a small batch (e.g., 100 vignettes in 5 languages) on one model to verify translation quality and token-forcing works
  2. Compute per-dimension preferences for a single language to check if the six dimensions are correctly isolated
  3. Test language sensitivity by comparing misalignment scores across two languages with very different speaker counts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs' moral judgments change when evaluated using multimodal inputs (e.g., text + images) compared to text-only inputs?
- Basis in paper: [inferred] The paper focuses exclusively on text-based LLMs and acknowledges that the original Moral Machine study included visual demonstrations, suggesting this as a future direction.
- Why unresolved: The study only evaluates text-based models, leaving the impact of visual context on moral reasoning unexplored.
- What evidence would resolve it: Testing multimodal LLMs (e.g., GPT-4V, Gemini) on the same MULTITP dataset and comparing alignment scores to text-only models.

### Open Question 2
- Question: Does jailbreaking LLMs lead to consistent moral preferences across all languages, or does it introduce new language-specific biases?
- Basis in paper: [explicit] The paper discusses jailbreaking to reduce refusals and explores its effects on alignment, but does not analyze cross-linguistic consistency post-jailbreaking.
- Why unresolved: Jailbreaking is tested on a subset of models and languages, with no analysis of whether preferences become more or less consistent across linguistic contexts.
- What evidence would resolve it: Applying jailbreaking to all 19 models across all 107 languages and measuring alignment variance pre- and post-jailbreaking.

### Open Question 3
- Question: Are there systematic cultural differences in LLM moral preferences beyond what is captured by language, and how can these be disentangled?
- Basis in paper: [inferred] The paper acknowledges that language-to-country mapping is imperfect due to multilingual countries and suggests dialect support as a future improvement.
- Why unresolved: The current methodology uses language as a proxy for culture, but does not account for regional or dialectal variations within the same language.
- What evidence would resolve it: Designing prompts in regional dialects (e.g., en-us vs. en-gb) and comparing alignment scores to identify systematic cultural differences.

## Limitations

- Google Translate use for 107 languages introduces uncertainty about translation fidelity and potential subtle biases not fully captured by manual review of small subsets
- Procedural generation approach may not reflect the diversity of naturally occurring moral reasoning expressed in real-world text, potentially limiting ecological validity
- Token-forcing and jailbreaking techniques may not represent authentic model behavior, as interventions could alter underlying moral reasoning rather than simply removing safety filters

## Confidence

- High confidence in finding most LLMs show significant misalignment with human moral preferences across multiple languages
- Medium confidence in claim that language resource levels don't correlate with alignment scores
- Low confidence in interpretation that language sensitivity scores definitively indicate equal treatment across languages

## Next Checks

1. Conduct comprehensive manual review of translations across a representative sample of languages, particularly low-resource languages, to assess whether translation quality variations could explain observed alignment differences.

2. Analyze the training corpora of evaluated LLMs to quantify actual exposure to different languages, testing whether procedural generation truly neutralizes resource-based advantages in practice.

3. Re-run the evaluation using naturally phrased moral dilemma prompts without token-forcing or jailbreaking to assess whether alignment scores and patterns persist under more realistic conditions.