---
ver: rpa2
title: 'CART: A Generative Cross-Modal Retrieval Framework with Coarse-To-Fine Semantic
  Modeling'
arxiv_id: '2406.17507'
source_url: https://arxiv.org/abs/2406.17507
tags:
- retrieval
- cart
- arxiv
- semantic
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of cross-modal retrieval by proposing
  a generative retrieval framework (CART) that assigns identifiers to multimodal candidates
  and treats generating these identifiers as the retrieval target. CART uses a coarse-to-fine
  semantic modeling scheme, combining K-Means and RQ-VAE to discretize multimodal
  data into token sequences, and employs a feature fusion strategy to align query
  and candidate semantics.
---

# CART: A Generative Cross-Modal Retrieval Framework with Coarse-To-Fine Semantic Modeling

## Quick Facts
- **arXiv ID**: 2406.17507
- **Source URL**: https://arxiv.org/abs/2406.17507
- **Reference count**: 40
- **Primary result**: Achieves SOTA cross-modal retrieval performance using generative retrieval with coarse-to-fine semantic modeling

## Executive Summary
This paper addresses cross-modal retrieval by proposing CART, a generative retrieval framework that assigns identifiers to multimodal candidates and treats generating these identifiers as the retrieval target. The approach uses a coarse-to-fine semantic modeling scheme, combining K-Means clustering for high-level semantic grouping with RQ-VAE for fine-grained residual quantization. This hierarchical identifier generation, coupled with a coarse-fine feature fusion strategy in the decoder, enables CART to achieve excellent retrieval performance while maintaining stable efficiency regardless of dataset size.

## Method Summary
CART converts multimodal data into hierarchical identifiers through a two-stage quantization process: K-Means clustering creates coarse semantic tokens representing broad categories, while RQ-VAE performs residual quantization on embeddings relative to cluster centers to create fine-grained tokens. The model generates captions for each item to serve as semantically aligned queries, then trains a sequence-to-sequence model to map queries to identifiers. A coarse-fine feature fusion strategy integrates multi-level semantic representations from the encoder, and a consistency loss ensures generated captions maintain semantic alignment with their items. The framework is evaluated across six diverse datasets covering image, audio, and video retrieval tasks.

## Key Results
- Achieves state-of-the-art retrieval performance across six benchmark datasets
- Maintains stable retrieval efficiency independent of dataset size
- Outperforms traditional single-tower and dual-tower models on both accuracy and efficiency metrics
- Demonstrates effectiveness across multiple modalities including image, audio, and video retrieval

## Why This Works (Mechanism)

### Mechanism 1
Coarse-to-fine semantic identifiers improve retrieval accuracy by capturing both high-level and low-level semantic distinctions in multimodal data. The approach combines K-Means clustering for coarse-level semantic grouping with RQ-VAE for fine-level residual quantization, creating hierarchical identifiers that preserve both broad category information and subtle intra-category differences.

### Mechanism 2
Coarse-to-fine feature fusion in the decoder improves retrieval performance by integrating multi-level semantic representations from the encoder. The decoder processes encoder outputs through two branches - coarse fusion that concatenates all encoder layer outputs and fine fusion that uses memory-augmented gating to weight individual layer contributions, then combines these features for generation.

### Mechanism 3
Generative retrieval with identifier-based generation achieves stable efficiency regardless of dataset size. Instead of computing similarity scores between all query-candidate pairs, the model generates candidate identifiers directly using autoregressive generation, with retrieval speed determined by generation steps rather than dataset size.

## Foundational Learning

- **Concept**: Vector quantization and discretization of continuous embeddings
  - **Why needed**: Multimodal data needs to be converted into discrete token sequences that can be generated autoregressively
  - **Quick check**: What is the difference between coarse clustering (K-Means) and fine quantization (RQ-VAE) in terms of what semantic information they preserve?

- **Concept**: Autoregressive generation and sequence-to-sequence modeling
  - **Why needed**: The retrieval task is formulated as generating identifiers token-by-token
  - **Quick check**: How does the model compute p(t_i | E(q), t<i, θ_i) for generating the i-th token in an identifier?

- **Concept**: Feature fusion and multi-level representation integration
  - **Why needed**: Different encoder layers capture different semantic granularities, and the fusion strategy needs to effectively combine these for optimal generation
  - **Quick check**: What is the mathematical difference between the coarse fusion (concatenation + linear projection) and fine fusion (weighted sum with memory-augmented gating)?

## Architecture Onboarding

- **Component map**: Query → Encoder → Coarse/Fine Fusion → Decoder → Identifier Generation → Candidate Matching
- **Critical path**: Natural language query processed through 3-layer transformer encoder, combined via coarse-fine fusion, then decoded autoregressively to generate hierarchical identifier
- **Design tradeoffs**: Identifier length vs. model complexity; encoder depth vs. fusion effectiveness; clustering granularity vs. semantic coherence
- **Failure signatures**: Poor R@1 scores despite good R@10; inconsistent performance across dataset sizes; high training loss but good validation loss; generation of invalid identifiers
- **First 3 experiments**: 1) Ablation study removing coarse fusion to measure contribution of multi-level aggregation; 2) Vary identifier length (4, 5, 6 tokens) to find optimal balance between discriminative power and generation efficiency; 3) Compare against dual-tower baseline on throughput vs. accuracy tradeoff curve with varying candidate set sizes

## Open Questions the Paper Calls Out

### Open Question 1
How does CART's generative retrieval performance scale with dataset size, particularly when approaching real-world web-scale datasets? The paper demonstrates stable efficiency up to 1M candidates but doesn't test beyond this scale.

### Open Question 2
What is the impact of using different backbone models for semantic identifier generation on CART's retrieval performance? The paper uses ImageBind but doesn't explore alternative embedding architectures.

### Open Question 3
How does CART perform when queries are multimodal rather than unimodal text? The paper states CART "only supports natural language queries" as a current limitation.

### Open Question 4
What is the optimal balance between identifier granularity and retrieval efficiency in CART, and how does this trade-off vary across different modalities? The paper shows performance varies with identifier configuration but doesn't determine optimal granularity for each modality.

## Limitations

- The framework's performance heavily depends on the quality of initial multimodal embeddings from ImageBind, with no thorough exploration of sensitivity to embedding quality
- Claims of stable efficiency gains are primarily validated through computational experiments rather than theoretical analysis
- The paper shows strong performance on benchmark datasets but doesn't explore edge cases, domain shifts, or transfer to completely different types of multimodal data

## Confidence

**High confidence** in core mechanism claims (coarse-to-fine identifiers improve accuracy; generative retrieval achieves stable efficiency)
**Medium confidence** in feature fusion mechanism claims (memory-augmented gating contribution unclear)
**Low confidence** in generalizability claims across all six datasets (doesn't explore edge cases or domain shifts)

## Next Checks

1. **Sensitivity Analysis**: Conduct experiments varying the embedding quality and source to quantify how much performance depends on initial embedding quality versus the CART framework itself.

2. **Identifier Space Scaling**: Systematically test the efficiency claims by varying identifier space size from hundreds to millions of candidates, measuring actual generation time and accuracy trade-offs.

3. **Ablation of Fine Fusion**: Remove the memory-augmented gating component while keeping coarse fusion, then compare against simpler weighted average approaches to determine if additional complexity provides measurable benefits.