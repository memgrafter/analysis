---
ver: rpa2
title: Off-Policy Evaluation from Logged Human Feedback
arxiv_id: '2406.10030'
source_url: https://arxiv.org/abs/2406.10030
tags:
- policy
- feedback
- human
- reward
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies off-policy evaluation from logged human feedback
  in large language models (LLMs). The authors formalize the problem as evaluating
  a new LLM's alignment with human preferences using data logged from a different
  model, without collecting new human feedback.
---

# Off-Policy Evaluation from Logged Human Feedback

## Quick Facts
- arXiv ID: 2406.10030
- Source URL: https://arxiv.org/abs/2406.10030
- Reference count: 40
- Key outcome: Proposes novel estimators for off-policy evaluation using logged human feedback in LLMs, achieving accurate policy value prediction and optimization without new human feedback

## Executive Summary
This paper addresses the challenge of evaluating new large language models (LLMs) using only logged data from a different model, without collecting additional human feedback. The authors formalize this as an off-policy evaluation problem where human feedback takes the form of ranked lists of responses. They propose both model-based and model-free estimators, including novel SetIPS and SetDR methods that exploit the structure of ranked list feedback to reduce variance compared to standard inverse propensity scoring. The paper provides theoretical guarantees of unbiasedness and demonstrates empirical performance on synthetic and real-world datasets.

## Method Summary
The paper proposes off-policy evaluation methods for LLMs using logged human feedback data. The approach treats human preference as a contextual bandit problem where each query generates a ranked list of responses, and humans indicate their preferred response. The authors introduce model-based estimators that learn a reward model to predict human preferences, and model-free estimators that reweight logged data using importance sampling. Key innovations include SetIPS and SetDR methods that reduce variance by reweighting response sets instead of full permutations, leveraging that human feedback depends only on which response is preferred rather than the complete ranking. The methods are evaluated on both synthetic data with known ground truth and real-world human feedback data.

## Key Results
- Novel SetIPS estimator reduces variance by exploiting set structure of ranked list feedback
- Model-based estimator provides interpretable policy alignment through direct probability estimation
- Doubly-robust methods combine variance reduction with consistency guarantees
- Empirical results show accurate absolute policy value prediction and ranking capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SetIPS reduces variance by exploiting ranked list feedback structure
- Mechanism: Reweights only response sets rather than full permutations, leveraging that human preference depends only on set membership
- Core assumption: Human preference depends solely on response set, not ranking order
- Evidence anchors: [abstract] "improve upon standard inverse propensity scoring by exploiting the structure of ranked list feedback" and [section] "The key insight is that the human feedback tells us which response in At is preferred"
- Break condition: If human preferences depend on response order within sets, SetIPS loses variance advantage

### Mechanism 2
- Claim: Model-based estimator provides interpretable policy alignment
- Mechanism: Directly estimates probability that first response aligns with human preference using Plackett-Luce structure
- Core assumption: Reward model can be parameterized as probability distribution over response sets
- Evidence anchors: [abstract] "the mean reward is the probability that the policy aligns with human feedback" and [section] "The strength of the DM estimator is that the policy π only needs to be sampled from"
- Break condition: If reward model misspecification occurs, estimator becomes biased

### Mechanism 3
- Claim: Doubly-robust methods combine variance reduction with consistency
- Mechanism: Uses reward model as variance reduction technique in IPS framework, canceling out when either model is correct or propensities are correct
- Core assumption: Either reward model is correctly specified OR propensity scores are correctly estimated
- Evidence anchors: [abstract] "We analyze unbiasedness of our estimators" and [section] "The DR estimator is unbiased when the DM is unbiased or the propensities in the IPS estimator are correctly specified"
- Break condition: If both reward model and propensity scores are misspecified, DR estimator becomes biased

## Foundational Learning

- Concept: Off-policy evaluation in contextual bandit settings
  - Why needed here: Extends standard OPE techniques to handle ranked list feedback structures
  - Quick check question: How does standard IPS differ from SetIPS in terms of the space being reweighted?

- Concept: Plackett-Luce model and choice probabilities
  - Why needed here: Human feedback model assumes choices follow Plackett-Luce distribution, fundamental to estimator design
  - Quick check question: Why does Plackett-Luce model allow reweighting sets instead of permutations?

- Concept: Variance reduction techniques in importance sampling
  - Why needed here: Paper introduces advanced estimators targeting variance reduction through set-based reweighting
  - Quick check question: What is the relationship between action space size and variance of standard IPS estimators?

## Architecture Onboarding

- Component map: Logging policy π0 -> generates ranked lists At from query xt -> Human provides preferred permutation At,* -> Estimator module computes policy value using DM, IPS, SetIPS, DR, or SetDR methods -> Optimization module uses gradient-based methods to improve evaluated policy -> Reward model component learns w parameters for DM/DR estimators

- Critical path: 1. Receive logged dataset (xt, At, At,*) 2. Choose estimator type based on requirements (variance vs bias vs interpretability) 3. For model-based estimators, train reward model using MLE 4. Compute policy value estimate using chosen estimator 5. For optimization, compute gradients and update policy parameters

- Design tradeoffs:
  - Model-based (DM): Low variance but potential bias if reward model misspecified
  - Model-free (IPS): Unbiased but high variance, especially when policies differ
  - Set-based (SetIPS): Lower variance than IPS by exploiting set structure, but requires computing propensities over sets
  - Doubly-robust: Combines benefits but more complex computation

- Failure signatures:
  - High variance estimates: Policies π and π0 are very different, or L is large
  - Biased estimates: Reward model misspecification, or propensities computed incorrectly
  - Computational issues: L is too large for exact propensity computation, requiring approximations

- First 3 experiments:
  1. Synthetic experiment with known ground truth to validate estimator accuracy across different K values
  2. Relative ranking experiment to test policy comparison capability against RLHF/DPO
  3. Real-world LLM experiment on Nectar dataset to validate on actual human feedback data

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the empirical validation raises several important areas for future research regarding scalability, robustness to model misspecification, and optimal logging policy design.

## Limitations

- Reliance on Plackett-Luce choice model assumption may not hold in practice
- Doubly-robust methods require either correct reward model specification or correct propensity estimation
- Scalability challenges for very large action spaces (large L) not fully addressed
- Limited ablation studies on model misspecification scenarios

## Confidence

High confidence in theoretical framework and unbiasedness proofs (well-established in OPE literature with rigorous derivations)
Medium confidence in practical performance claims (empirical validation relies on synthetic data and one real-world dataset)
Low confidence in scalability claims for very large action spaces (paper acknowledges computational challenges but lacks extensive experiments)

## Next Checks

1. **Stress test SetIPS variance reduction**: Systematically vary degree to which human preferences depend on response order versus set membership, measuring when SetIPS breaks down compared to standard IPS
2. **Robustness to reward model misspecification**: Implement controlled experiments where reward model is intentionally misspecified with varying degrees of noise, measuring impact on all estimator types
3. **Large-scale action space validation**: Extend experiments to L > 100 action spaces, testing computational approximations and measuring accuracy degradation compared to exact computation