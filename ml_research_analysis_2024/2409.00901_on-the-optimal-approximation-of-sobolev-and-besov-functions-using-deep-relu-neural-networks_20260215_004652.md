---
ver: rpa2
title: On the optimal approximation of Sobolev and Besov functions using deep ReLU
  neural networks
arxiv_id: '2409.00901'
source_url: https://arxiv.org/abs/2409.00901
tags:
- neural
- network
- networks
- approximation
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the approximation of Sobolev and Besov functions
  using deep ReLU neural networks with varying width and depth. The key problem is
  determining the optimal approximation rate for functions in Sobolev spaces W s,q([0,1]^d)
  and Besov spaces B s,q,r([0,1]^d) under the Lp norm.
---

# On the optimal approximation of Sobolev and Besov functions using deep ReLU neural networks

## Quick Facts
- arXiv ID: 2409.00901
- Source URL: https://arxiv.org/abs/2409.00901
- Reference count: 11
- Primary result: Deep ReLU networks with width W and depth L can achieve approximation rate O((WL)^{-2s/d}) for Sobolev and Besov functions under the Sobolev embedding condition 1/q - 1/p < s/d.

## Executive Summary
This paper studies the optimal approximation of Sobolev and Besov functions using deep ReLU neural networks with varying width and depth. The key contribution is establishing that under the Sobolev embedding condition, deep ReLU networks can achieve the optimal approximation rate O((WL)^{-2s/d}), where W is width, L is depth, and s is the smoothness parameter. The paper introduces a novel encoding method for sparse vectors using deep ReLU networks that balances width and depth parameters to achieve this optimal rate.

## Method Summary
The paper develops a novel encoding method for sparse vectors using deep ReLU networks with varying width and depth, which serves as the key technical tool for proving optimal approximation rates. The approach involves approximating Sobolev/Besov functions by piecewise polynomials, discretizing the coefficients, encoding them using the novel vector encoding method, and constructing networks to approximate both base functions and products. The proof strategy bounds the error as the sum of discretization error and network approximation error, showing both can be controlled by appropriate parameter choices.

## Key Results
- Deep ReLU networks with width W and depth L achieve approximation rate O((WL)^{-2s/d}) for Sobolev and Besov functions under the Sobolev embedding condition 1/q - 1/p < s/d.
- The approximation error depends on the product WL rather than just width or depth alone, showing the importance of balancing these parameters.
- The paper generalizes previous results limited to fixed width networks or specific function spaces to the broader class of Sobolev and Besov spaces.
- Applications to nonparametric regression show deep neural networks can achieve minimax optimal rates for these function classes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep ReLU networks with width W and depth L can achieve approximation rate O((WL)^{-2s/d}) for Sobolev and Besov functions under the Sobolev embedding condition 1/q - 1/p < s/d.
- Mechanism: The key technical tool is a novel encoding method for sparse vectors using deep ReLU networks with varying width and depth. This encoding balances width and depth parameters to achieve the optimal rate, showing that the approximation error depends on the product WL rather than just width or depth alone.
- Core assumption: The Sobolev embedding condition 1/q - 1/p < s/d holds, guaranteeing that Sobolev/Besov spaces are compactly embedded in Lp.
- Evidence anchors:
  - [abstract]: "The key tool in our proof is a novel encoding of sparse vectors by using deep ReLU neural networks with varied width and depth, which may be of independent interest."
  - [section 4.2]: "We consider the problem of how efficiently deep ReLU neural networks can represent integer vectors."
- Break condition: If the Sobolev embedding condition fails (1/q - 1/p ≥ s/d), the compact embedding does not hold and the approximation rate O((WL)^{-2s/d}) may not be achievable.

### Mechanism 2
- Claim: The approximation error can be bounded by the sum of discretization error and network approximation error, both of which can be controlled by choosing appropriate parameters.
- Mechanism: The proof strategy involves approximating Sobolev/Besov functions by piecewise polynomials, discretizing the coefficients, encoding them using Theorem 4.6, and constructing networks to approximate the base functions and products.
- Core assumption: Piecewise polynomial approximation can achieve the desired error bounds for Sobolev and Besov functions.
- Evidence anchors:
  - [section 4.3]: "Since the seminal work of Yarotsky [2017], it is well-known that polynomials can be efficiently approximated by deep ReLU neural networks."
  - [section 4.4]: "By using the Bramble-Hilbert lemma, one can show that the coefficients satisfy the following bound..."
- Break condition: If the discretization level δ is not chosen appropriately relative to the polynomial degree k and Sobolev smoothness s, the discretization error may dominate and prevent achieving the optimal rate.

### Mechanism 3
- Claim: The neural network can implement the index computation for good regions Ωℓ,ϵ and the encoding/decoding of sparse vectors to represent piecewise polynomial coefficients.
- Mechanism: Lemma 4.4 constructs networks to compute the index i of good regions, while Theorem 4.6 encodes sparse vectors representing coefficients. These are combined with polynomial approximation techniques.
- Core assumption: The index computation and sparse vector encoding can be implemented efficiently by ReLU networks with the stated width and depth bounds.
- Evidence anchors:
  - [section 4.1]: "Lemma 4.4. Let ℓ ∈ N0 and 0 < ϵ < b −ℓ. For any L ∈ N, there exists qd ∈ N Nd,1(2db⌈ℓ/L⌉, L) such that qd(x) = ind (i)."
  - [section 4.2]: "Theorem 4.6. Let N, M ∈ N and x = (x1, ..., xN)⊺ ∈ ZN satisfy ∥x∥1 ≤ M. If N ≥ M, then for any S, T ∈ N, there exists g ∈ N N (W, L) with W = 22 max(...) + 10, L = 4S(T + 2), such that g(n) = xn for n = 1, ..., N."
- Break condition: If the width W or depth L constraints in Theorem 4.6 are violated, the sparse vector encoding may not be achievable, breaking the construction.

## Foundational Learning

- Concept: Sobolev spaces W^{s,q} and Besov spaces B^{s}_{q,r}
  - Why needed here: The paper studies approximation of functions in these spaces by neural networks.
  - Quick check question: What is the Sobolev embedding condition 1/q - 1/p < s/d and why is it necessary for the approximation results?

- Concept: Neural network architecture and complexity measures (width W, depth L)
  - Why needed here: The paper analyzes how the approximation rate depends on these parameters and establishes optimal bounds.
  - Quick check question: How does the product WL relate to the total number of parameters in a fully connected ReLU network?

- Concept: Approximation theory and nonlinear approximation
  - Why needed here: The paper uses techniques from approximation theory to bound the error between functions and their neural network approximations.
  - Quick check question: What is the difference between linear and nonlinear approximation methods in the context of neural network approximation?

## Architecture Onboarding

- Component map: Index computation (Lemma 4.4) → Sparse vector encoding (Theorem 4.6) → Polynomial approximation (Lemma 4.9) → Piecewise polynomial construction (Proposition 4.10) → Sobolev/Besov function approximation (Propositions 4.11, 2.1-2.2)
- Critical path: The encoding of sparse vectors (Theorem 4.6) is the key technical contribution that enables the optimal approximation rate.
- Design tradeoffs: The paper balances width and depth parameters to achieve the optimal rate O((WL)^{-2s/d}). Increasing one parameter while decreasing the other may not improve the rate.
- Failure signatures: If the Sobolev embedding condition fails or the width/depth bounds in Theorem 4.6 are violated, the optimal approximation rate may not be achievable.
- First 3 experiments:
  1. Verify the Sobolev embedding condition 1/q - 1/p < s/d holds for the target function space.
  2. Implement the index computation network (Lemma 4.4) and test on sample inputs.
  3. Implement the sparse vector encoding network (Theorem 4.6) and verify it can represent test vectors with the stated width and depth bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise values of the minimal width W* and depth L* needed for deep ReLU networks to achieve optimal approximation rates for Sobolev and Besov functions?
- Basis in paper: Explicit - The paper states that W* and L* depend on s, r, p, q and d, but their exact values are not estimated in the paper.
- Why unresolved: The paper only provides bounds on W* and L*, but does not give precise estimates. Determining these values would require more detailed analysis of the network constructions and approximation error bounds.
- What evidence would resolve it: Proving tight upper and lower bounds on W* and L* that match, showing the exact minimal values needed for optimal rates.

### Open Question 2
- Question: How can the results be generalized to approximate functions on low-dimensional manifolds rather than the unit cube?
- Basis in paper: Inferred - The paper mentions that practical data often lies on low-dimensional manifolds, and suggests generalizing the results to manifolds as an open problem.
- Why unresolved: The current proofs rely on specific properties of the unit cube and do not directly extend to general manifolds. New techniques would be needed to handle the geometry of manifolds.
- What evidence would resolve it: Extending the network constructions and error bounds to functions on Riemannian manifolds, showing optimal approximation rates in this setting.

### Open Question 3
- Question: What are the optimal approximation rates for deep ReLU networks under the Sobolev embedding boundary condition 1/q - 1/p = s/d?
- Basis in paper: Explicit - The paper explicitly states that the boundary case is "much more subtle" and is not studied in this work.
- Why unresolved: The strict embedding condition 1/q - 1/p < s/d used in the paper does not cover the boundary case. New techniques may be needed to handle this regime.
- What evidence would resolve it: Proving approximation bounds that match the known lower bounds for this boundary case, showing the optimal rates hold even at the boundary.

## Limitations
- The main theoretical claims rely heavily on the Sobolev embedding condition 1/q - 1/p < s/d, which is assumed throughout the analysis.
- The paper references Siegel [2023] for the vector encoding method but doesn't provide complete algorithmic details, creating potential implementation gaps.
- The constants and numerical parameters needed for practical implementation are not fully specified.

## Confidence

- **High confidence**: The theoretical framework connecting Sobolev/Besov spaces to neural network approximation, assuming the Sobolev embedding condition holds.
- **Medium confidence**: The optimal rate O((WL)^{-2s/d}) for the product of width and depth, as this depends on the correct implementation of the novel encoding method which is not fully detailed in the paper.
- **Low confidence**: Practical implementation details for achieving the theoretical bounds, particularly the vector encoding step and numerical stability considerations.

## Next Checks

1. **Verify Sobolev embedding conditions**: For specific target function spaces, explicitly check that 1/q - 1/p < s/d holds and determine the threshold values where the approximation theory breaks down.
2. **Implement and test vector encoding**: Build a prototype implementation of Theorem 4.6's sparse vector encoding method and validate it can represent test vectors within the stated width and depth bounds.
3. **Numerical validation of rates**: Conduct computational experiments to verify the O((WL)^{-2s/d}) approximation rate empirically by varying width and depth parameters on test functions from Sobolev and Besov spaces.