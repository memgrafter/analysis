---
ver: rpa2
title: 'HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying Real-World
  Claims'
arxiv_id: '2410.12377'
source_url: https://arxiv.org/abs/2410.12377
tags:
- evidence
- score
- claim
- system
- hero
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents HerO, a fact-checking system that uses only
  publicly available large language models (LLMs) for automated fact verification.
  The system employs a three-step pipeline: evidence retrieval, question generation,
  and veracity prediction.'
---

# HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying Real-World Claims

## Quick Facts
- arXiv ID: 2410.12377
- Source URL: https://arxiv.org/abs/2410.12377
- Reference count: 7
- Primary result: 2nd place in AVeriTeC shared task with AVeriTeC score of 0.57 using only open LLMs

## Executive Summary
This paper presents HerO, a fact-checking system that achieves competitive performance using only publicly available large language models (LLMs) for automated fact verification. The system employs a three-step pipeline: evidence retrieval using hypothetical fact-checking documents to augment queries, question generation using in-context examples, and veracity prediction using fine-tuned LLMs. HerO achieved 2nd place in the AVeriTeC shared task, demonstrating that open LLMs can effectively verify real-world claims without relying on proprietary models like GPT-4.

## Method Summary
HerO implements a three-step pipeline for fact verification. First, evidence retrieval uses a hybrid approach combining BM25 and dense retrieval, augmented by hypothetical fact-checking documents generated by Llama-3.1-70b to improve query effectiveness. Second, question generation uses Llama-3-8b with claims as in-context examples to generate relevant questions. Third, veracity prediction employs fine-tuned Llama-3.1-70b with in-context examples and annotator rationales from the training data. The system uses SFR-embedding-2 for dense retrieval and implements LoRA fine-tuning for the veracity prediction step.

## Key Results
- Achieved 2nd place on AVeriTeC leaderboard with score of 0.57
- Outperformed baseline system across all evaluation metrics (Q score, Q+A score, A VeriTeC score)
- Demonstrates competitive performance using only open LLMs without proprietary models
- Successfully verified real-world claims using Llama-3.1-70b and Llama-3-8b models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HyDE-FC improves evidence retrieval by generating synthetic fact-checking documents
- Mechanism: Generates hypothetical fact-checking documents based on claims to augment retrieval queries, creating more informative queries
- Core assumption: Generated documents capture relevant semantic content that improves retrieval effectiveness
- Evidence anchors: [section] "Inspired by a previous study on hypothetical document embedding (Gao et al., 2023), we utilize an instruction-following LM to generate hypothetical fact-checking documents to augment a retrieval query"

### Mechanism 2
- Claim: Fine-tuned LLMs with in-context examples outperform zero-shot approaches for veracity prediction
- Mechanism: Uses fine-tuned llama-3.1-70b with in-context examples from training data, incorporating annotator rationales into prompts
- Core assumption: Fine-tuning improves model performance on specific task compared to in-context learning alone
- Evidence anchors: [section] "Our best model uses the fine-tuned llama-3.1-70b-it that predicts the veracity label after generating the explanation"

### Mechanism 3
- Claim: Instruction-tuned open LLMs achieve competitive performance compared to proprietary models
- Mechanism: Uses only publicly available, instruction-tuned LLMs (Llama-3.1-70b, Llama-3-8b) without proprietary models like GPT-4
- Core assumption: Open LLMs have reached sufficient capability to handle complex fact-checking tasks
- Evidence anchors: [abstract] "HerO achieved 2nd place on the leaderboard with the AVeriTeC score of 0.57, suggesting the potential of open LLMs for verifying real-world claims"

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: System needs to retrieve relevant evidence from web documents before making veracity predictions
  - Quick check question: How does the hybrid retrieval approach (BM25 + dense retrieval) improve over using either method alone?

- Concept: In-context learning and few-shot prompting
  - Why needed here: System uses in-context examples for both question generation and veracity prediction to guide model outputs
  - Quick check question: What is the difference in performance between using 10 in-context examples versus fine-tuning the model?

- Concept: Instruction-tuning of LLMs
  - Why needed here: System relies on instruction-tuned models (Llama-3.1) rather than base models for better task performance
  - Quick check question: Why does instruction-tuning improve performance on fact-checking tasks compared to base models?

## Architecture Onboarding

- Component map: Claim → Evidence Retrieval (HyDE-FC + BM25 + Dense Retrieval) → Question Generation → Veracity Prediction → Output
- Critical path: Evidence retrieval is most critical as it directly impacts quality of information available for subsequent steps
- Design tradeoffs: Open LLMs vs. proprietary models (transparency vs. potential performance), fine-tuning vs. in-context learning (cost vs. flexibility), hybrid retrieval (complexity vs. effectiveness)
- Failure signatures: Poor evidence retrieval leads to wrong predictions regardless of subsequent step quality; bad in-context examples can mislead model
- First 3 experiments:
  1. Compare evidence retrieval performance with and without HyDE-FC augmentation using A score as metric
  2. Test different in-context sample sizes (0, 5, 10, 20) for question generation and measure Q score impact
  3. Compare fine-tuned model performance against zero-shot and few-shot baselines on held-out validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HerO's performance compare to systems using proprietary LLMs like GPT-4o on the AVeriTeC dataset?
- Basis in paper: [explicit] Paper mentions HerO achieved 2nd place with 0.57 score while winning system used GPT-4o (Schlichtkrull et al., 2024)
- Why unresolved: Paper does not provide direct comparison of HerO's performance against GPT-4o on AVeriTeC dataset
- What evidence would resolve it: Direct comparison of HerO's performance against GPT-4o on AVeriTeC dataset using same evaluation metrics

### Open Question 2
- Question: How does HerO's performance change when using different open LLMs for each step of the fact-checking process?
- Basis in paper: [inferred] Paper mentions using Llama-3.1-70b and Llama-3-8b but does not explore performance impact of using different open LLMs for each step
- Why unresolved: Paper does not provide comprehensive analysis of performance impact of using different open LLMs for each step
- What evidence would resolve it: Systematic comparison of HerO's performance when using different open LLMs for each step using same evaluation metrics

### Open Question 3
- Question: How does HerO's performance change when using different evidence retrieval methods?
- Basis in paper: [inferred] Paper mentions using hybrid approach combining BM25 and dense retrieval but does not explore performance impact of different evidence retrieval methods
- Why unresolved: Paper does not provide comprehensive analysis of performance impact of different evidence retrieval methods
- What evidence would resolve it: Systematic comparison of HerO's performance when using different evidence retrieval methods using same evaluation metrics

## Limitations
- System relies on proprietary retrieval infrastructure (Pinecone) which may not be accessible to all researchers
- Large model sizes (70B parameters) create accessibility barriers for independent replication
- Absolute performance scores (0.57 AVeriTeC score) indicate significant room for improvement even with fine-tuning
- Evidence retrieval mechanism using HyDE-FC lacks direct comparative analysis against other retrieval augmentation techniques

## Confidence

- High confidence: System architecture and three-step pipeline are clearly specified and reproducible
- Medium confidence: Claims about HyDE-FC improving retrieval are plausible but lack direct comparative evidence
- Medium confidence: Performance claims are supported by leaderboard results but absolute scores indicate limitations
- Low confidence: Generalizability claims beyond AVeriTeC dataset are not explicitly tested

## Next Checks

1. Conduct ablation studies comparing evidence retrieval performance with and without HyDE-FC augmentation on held-out validation set, measuring A score differences
2. Test system's robustness by evaluating performance across different claim types (numerical claims vs. factual assertions) to identify domain-specific weaknesses
3. Implement and evaluate smaller, more accessible open LLM alternatives (Llama-3-8b or Phi-3) to assess minimum viable model size for competitive performance