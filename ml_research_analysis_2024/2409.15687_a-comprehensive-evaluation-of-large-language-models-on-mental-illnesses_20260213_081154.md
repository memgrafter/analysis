---
ver: rpa2
title: A Comprehensive Evaluation of Large Language Models on Mental Illnesses
arxiv_id: '2409.15687'
source_url: https://arxiv.org/abs/2409.15687
tags:
- llama
- mistral
- mental
- performance
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first comprehensive evaluation of 33 large
  language models (LLMs), ranging from 2B to 405B parameters, on mental health tasks
  using six datasets. Models were tested on binary disorder detection, disorder severity
  evaluation, and psychiatric knowledge assessment using both zero-shot and few-shot
  learning approaches.
---

# A Comprehensive Evaluation of Large Language Models on Mental Illnesses

## Quick Facts
- arXiv ID: 2409.15687
- Source URL: https://arxiv.org/abs/2409.15687
- Reference count: 40
- 33 LLMs evaluated (2B-405B parameters) on mental health tasks using six datasets

## Executive Summary
This study presents the first comprehensive evaluation of 33 large language models on mental health tasks, ranging from binary disorder detection to psychiatric knowledge assessment. The research examines both zero-shot and few-shot learning approaches across six datasets, testing models from 2B to 405B parameters. GPT-4 and Llama 3 achieved up to 85% accuracy on disorder detection, while Llama 3.1 405b demonstrated exceptional psychiatric knowledge accuracy at 91.2%. The study reveals that prompt engineering significantly impacts performance, with structured prompts improving results across tasks. However, ethical constraints from LLM providers limited comprehensive evaluation of sensitive queries, highlighting the tension between safety measures and clinical effectiveness in mental health applications.

## Method Summary
The study evaluated 33 LLMs across three mental health tasks using six datasets: binary disorder detection, disorder severity evaluation, and psychiatric knowledge assessment. Models were tested using both zero-shot and few-shot learning approaches, with systematic prompt engineering employing four templates per task. Each dataset was reduced to 1000 samples using fair random sampling for balanced evaluation. Performance was measured using accuracy, balanced accuracy, mean absolute error, and invalid response rates. The evaluation framework included models ranging from 2B to 405B parameters, including GPT-4, Llama 3, Claude, and various open-source alternatives.

## Key Results
- GPT-4 and Llama 3 achieved up to 85% accuracy on binary disorder detection tasks
- Few-shot learning reduced mean absolute error by 1.3 points for Phi-3-mini in severity evaluation
- Llama 3.1 405b demonstrated exceptional psychiatric knowledge assessment accuracy at 91.2%
- Prompt engineering significantly impacted performance, with structured prompts improving results by up to 22.58% for smaller models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prompt engineering significantly improves LLM performance on mental health tasks, especially for smaller models.
- **Mechanism:** Structured prompts with clear instructions (e.g., role assignment, explicit output format, repetition of key instructions) guide LLMs to produce more accurate and consistent responses. The study found that prompt BIN-4, which included a repetition line, led to a 22.58% accuracy improvement for Gemma 7b on the Dreaddit dataset.
- **Core assumption:** LLMs can be effectively guided through well-crafted prompts to improve their performance on specific tasks.
- **Evidence anchors:**
  - [abstract]: "prompt engineering played a crucial role in improving performance across tasks"
  - [section]: "The results presented in this study highlight the significant gains that can be achieved through careful prompt design and refinement"
  - [corpus]: Weak; no direct mention of prompt engineering in related papers, but the general concept of "structured prompts" is implied in some studies.
- **Break condition:** If prompts become too restrictive, they may limit the model's ability to generalize or provide nuanced responses.

### Mechanism 2
- **Claim:** Few-shot learning enhances disorder severity evaluation by providing context and examples.
- **Mechanism:** Providing a few examples of social media posts with corresponding severity ratings helps LLMs understand the task's context and complexities, leading to more accurate severity predictions. The study found that few-shot learning reduced the Mean Absolute Error (MAE) by 1.3 points for the Phi-3-mini model.
- **Core assumption:** LLMs can effectively learn from a small number of examples to improve their performance on complex tasks.
- **Evidence anchors:**
  - [abstract]: "FS learning notably enhanced disorder severity evaluations, reducing the Mean Absolute Error (MAE) by 1.3 points for the Phi-3-mini model"
  - [section]: "By comparing the results of these two approaches, we aimed to determine how much additional context in the form of examples improved the model's ability to accurately gauge disorder severity"
  - [corpus]: Weak; while some related papers mention fine-tuning, few-shot learning is not explicitly discussed in the corpus.
- **Break condition:** If the examples provided are not representative or of poor quality, few-shot learning may not improve or could even harm performance.

### Mechanism 3
- **Claim:** Newer LLM architectures generally outperform older models in psychiatric knowledge assessment tasks.
- **Mechanism:** Recent LLMs, likely trained on more comprehensive and diverse datasets including medical and scientific text, demonstrate a better understanding of psychiatric knowledge. The study found that the Llama 3.1 family, particularly Llama 3.1 405b, achieved high accuracy in the psychiatric knowledge assessment task.
- **Core assumption:** Training data quality and diversity, along with architectural improvements, contribute to better performance on knowledge-intensive tasks.
- **Evidence anchors:**
  - [abstract]: "Recent models, such as Llama 3.1 405b, demonstrated exceptional psychiatric knowledge assessment accuracy at 91.2%"
  - [section]: "The general trend observed from these results is that while model size matters, more recent models perform significantly better than their older counterparts"
  - [corpus]: Weak; the corpus does not provide direct evidence for this mechanism, but the general trend of improving LLM performance over time is implied.
- **Break condition:** If newer models are trained on datasets with biases or inaccuracies, their performance may not necessarily be better.

## Foundational Learning

- **Concept:** Binary disorder detection vs. severity evaluation
  - **Why needed here:** The study evaluates LLMs on both binary classification (detecting the presence or absence of a disorder) and severity estimation (gauging the level of severity). Understanding the difference is crucial for interpreting results and choosing appropriate models.
  - **Quick check question:** What is the main difference between binary disorder detection and severity evaluation tasks?

- **Concept:** Zero-shot vs. few-shot learning
  - **Why needed here:** The study employs both zero-shot (ZS) and few-shot (FS) learning approaches. ZS relies on the model's pre-existing knowledge, while FS provides examples for context. Understanding these approaches is essential for grasping the study's methodology and results.
  - **Quick check question:** How do zero-shot and few-shot learning differ in their approach to training LLMs?

- **Concept:** Prompt engineering and its impact
  - **Why needed here:** The study emphasizes the importance of prompt engineering in improving LLM performance. Understanding how prompts are structured and their effect on model outputs is crucial for replicating and building upon the study's findings.
  - **Quick check question:** How can prompt engineering influence the performance of LLMs on specific tasks?

## Architecture Onboarding

- **Component map:** Datasets (DEPTWEET, SDCNL, SAD, Dreaddit, RED SAM, DepSeverity, MedMCQA) -> Models (33 LLMs from 2B-405B parameters) -> Tasks (Binary detection, Severity evaluation, Knowledge assessment) -> Evaluation metrics (Accuracy, Balanced Accuracy, MAE, invalid rates) -> Prompt templates (4 per task)
- **Critical path:** 1. Select dataset and task 2. Choose LLM and prompt template 3. Run evaluation and collect results 4. Analyze performance and identify trends
- **Design tradeoffs:**
  - Model size vs. performance: Smaller models can sometimes outperform larger ones
  - Zero-shot vs. few-shot learning: FS generally improves accuracy but increases computational cost
  - Prompt structure vs. flexibility: Structured prompts improve consistency but may limit nuanced responses
- **Failure signatures:**
  - High invalid response rates (>5%) suggest prompt engineering issues - try adding the repetition line "(Please return only the label and no other text)"
  - Significant performance variation across datasets: Model may not generalize well to different conditions
  - Poor instruction-following: Model struggles to adhere to prompt formatting or task requirements
- **First 3 experiments:**
  1. Evaluate a small model (e.g., Gemma 7b) on binary disorder detection using prompt BIN-1
  2. Compare performance of GPT-4 and Llama 3 on severity evaluation using few-shot learning
  3. Test the impact of prompt engineering by modifying BIN-1 to include a repetition line (BIN-1.1)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural improvements in newer models (like Llama 3.1) contribute to their superior performance in psychiatric knowledge assessment compared to older, larger models?
- Basis in paper: [explicit] The paper notes that Llama 3.1 models outperformed older Llama 3 models and even GPT-4 in psychiatric knowledge assessment, despite being released around the same time period as other models where size seemed to be the deciding factor.
- Why unresolved: The paper does not delve into the specific architectural differences or training data composition that led to this improvement in the Llama 3.1 models.
- What evidence would resolve it: A detailed analysis of the training data used for Llama 3.1 models, focusing on the proportion of medical/psychiatric data, and a comparison of the architectural differences between Llama 3 and Llama 3.1.

### Open Question 2
- Question: How does the inclusion of real-world clinical data (e.g., anonymized patient records, therapist-annotated case histories) in the training of LLMs impact their performance on mental health tasks compared to models trained solely on social media data?
- Basis in paper: [inferred] The paper highlights the limitations of relying solely on social media data for mental health tasks, noting concerns about data quality, representativeness, and labeling subjectivity. It also mentions the potential benefits of integrating RAG with clinical foundations for future work.
- Why unresolved: The study primarily focused on evaluating LLMs trained on publicly available datasets, without incorporating real-world clinical data.
- What evidence would resolve it: A comparative study evaluating the performance of LLMs trained on a combination of social media data and anonymized clinical data against those trained solely on social media data, across various mental health tasks.

### Open Question 3
- Question: What is the optimal balance between safety controls and clinical effectiveness in LLMs for mental health applications, and how can this balance be achieved without compromising either aspect?
- Basis in paper: [explicit] The paper discusses the trade-offs between safety and accuracy, noting that models with stricter ethical guidelines sometimes performed worse on mental health tasks due to their heightened sensitivity to prompts related to mental health issues.
- Why unresolved: The study does not provide a definitive answer on how to achieve the optimal balance between safety and clinical effectiveness, as this is a complex ethical and technical challenge.
- What evidence would resolve it: A framework for evaluating the safety and clinical effectiveness of LLMs in mental health applications, incorporating both quantitative metrics (e.g., accuracy, invalid response rates) and qualitative assessments (e.g., clinician feedback, patient perspectives).

## Limitations

- Ethical constraints from LLM providers prevented full evaluation of sensitive mental health queries, limiting comprehensive assessment
- Study relied on 1000-sample subsets for each dataset, which may not capture full complexity and variability of real-world mental health expressions
- Focus on English-language social media data potentially limits generalizability to other languages and cultural contexts

## Confidence

- **High Confidence:** Binary disorder detection results showing GPT-4 and Llama 3 achieving up to 85% accuracy
- **Medium Confidence:** Severity evaluation improvements through few-shot learning (1.3 MAE reduction for Phi-3-mini)
- **Medium Confidence:** Prompt engineering effectiveness, though results show significant variability across models and datasets
- **Low Confidence:** Psychiatric knowledge assessment results for newer models, as the dataset and question quality were not fully validated

## Next Checks

1. Replicate the binary disorder detection task using a different sampling strategy (e.g., stratified sampling) to verify the stability of the 85% accuracy results across multiple model architectures
2. Conduct an ablation study on prompt engineering by systematically removing each component (role assignment, explicit format, repetition line) to quantify their individual contributions to performance improvements
3. Test model performance on out-of-domain mental health data (e.g., clinical notes or therapist transcripts) to assess generalization beyond social media posts and identify potential domain-specific limitations