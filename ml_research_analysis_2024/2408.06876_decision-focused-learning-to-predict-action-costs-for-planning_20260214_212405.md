---
ver: rpa2
title: Decision-Focused Learning to Predict Action Costs for Planning
arxiv_id: '2408.06876'
source_url: https://arxiv.org/abs/2408.06876
tags:
- planning
- action
- costs
- training
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of predicting action costs for
  automated planning from correlated features using Decision-Focused Learning (DFL),
  which trains models to optimize solution quality rather than prediction accuracy.
  The core challenges are that planning systems don't support negative action costs,
  and DFL requires many repeated planner calls during training.
---

# Decision-Focused Learning to Predict Action Costs for Planning

## Quick Facts
- arXiv ID: 2408.06876
- Source URL: https://arxiv.org/abs/2408.06876
- Authors: Jayanta Mandi; Marco Foschini; Daniel Holler; Sylvie Thiebaux; Jorg Hoffmann; Tias Guns
- Reference count: 18
- Primary result: DFL approach consistently achieves lower regret than MSE training across multiple planning domains

## Executive Summary
This paper addresses the challenge of predicting action costs for automated planning when features are correlated with costs. Traditional approaches train models to minimize prediction error, but this doesn't guarantee good planning decisions. The authors propose Decision-Focused Learning (DFL) that directly optimizes for solution quality by computing gradients through the planning process. They introduce methods to handle negative cost predictions (which planners reject) and use solution caching to speed up training, achieving significantly better planning outcomes than standard regression approaches.

## Method Summary
The authors develop a DFL framework that trains a predictive model to minimize planning regret rather than prediction error. They generate synthetic planning data with correlated features and ground truth costs, then train linear models using minibatch SGD with SPO+ subgradient formulation. Key innovations include transformations for negative cost predictions (add-min and thresholding), an explicit penalty to avoid negative predictions, and solution caching to reduce expensive planner calls during training. The approach is evaluated across multiple planning domains including shortest path, transport, and rovers problems.

## Key Results
- DFL consistently achieves lower regret than MSE training across all tested planning domains
- Solution caching (20% of training instances) significantly reduces training time without sacrificing solution quality
- The add-min transformation for negative predictions outperforms simple thresholding by preserving information about relative cost ordering
- Planning with approximate methods (boundn, no-bound) can be substantially faster than optimal planning while maintaining low regret

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DFL directly trains models to minimize decision regret rather than prediction error, leading to better plans.
- Mechanism: By computing gradients using SPO+, the model learns to adjust predictions so that the resulting planning solutions have lower regret, even if individual predictions are less accurate.
- Core assumption: The subgradient of SPO+ provides useful direction for gradient descent despite being based on a convex upper bound of regret.
- Evidence anchors:
  - [abstract] "demonstrate that the proposed gradient computation consistently yields significantly better plans than predictions aimed at minimizing prediction error"
  - [section] "the motivation of DFL is to directly train an ML model to predict Ĉ in a manner that minimizes regret"

### Mechanism 2
- Claim: The "add-min" transformation avoids information loss when negative predictions occur, preserving relative ordering of action costs.
- Mechanism: By adding the absolute value of the minimum predicted cost to all actions, negative predictions are made positive while maintaining their relative differences, allowing planners to use them without distortion.
- Core assumption: Planners only require non-negative costs, not necessarily all-zero-negative values.
- Evidence anchors:
  - [section] "Our idea is to add a scalar value to each element of the cost vector, if any element in it is negative"
  - [section] "add-min regret demonstrates greater fidelity to true LP regret"

### Mechanism 3
- Claim: Solution caching dramatically reduces training time by reusing previously computed plans.
- Mechanism: Storing action count vectors from training instances allows the system to avoid recomputing plans for similar predicted costs, reducing the number of planner calls needed.
- Core assumption: Similar predicted cost vectors will lead to similar optimal plans, making caching effective.
- Evidence anchors:
  - [abstract] "caching can significantly reduce training time without sacrificing solution quality"
  - [section] "keeping p as low as 5% is often sufficient for DFL training"

## Foundational Learning

- Concept: Automatic differentiation and computational graphs
  - Why needed here: DFL requires computing gradients through the planning optimization, which lies outside standard neural network operations
  - Quick check question: Why can't we just use PyTorch's autograd to compute gradients through the planner?

- Concept: Subgradients vs gradients in non-differentiable optimization
  - Why needed here: Planning solutions are discrete, making them non-differentiable, so SPO+ provides a subgradient approximation
  - Quick check question: What happens to the gradient when a small change in predicted costs causes a completely different plan?

- Concept: Caching and memoization strategies
  - Why needed here: Repeated planner calls during training are expensive, so caching previously computed solutions speeds up training
  - Quick check question: How do you determine which predicted cost vectors are "similar enough" to reuse cached solutions?

## Architecture Onboarding

- Component map:
  Feature encoder (linear model or neural network) -> Cost predictor (produces action cost vector Ĉ) -> Cost transformer (add-min or thresholding for negative values) -> Planner (Fast Downward with various configurations) -> Gradient computation module (SPO+ with or without explicit penalty) -> Cache manager (stores and retrieves previously computed plans)

- Critical path:
  1. Input features → cost predictor → cost transformation
  2. Transformed costs → planner → action count vector
  3. Action count vector + true costs → regret calculation
  4. Regret → SPO+ subgradient → model update

- Design tradeoffs:
  - Accuracy vs speed: Using optimal planning gives better gradients but slower training; caching speeds up training but may reduce gradient quality
  - Transformation method: add-min preserves information but may create large positive values; thresholding is simpler but loses ordering information
  - Penalty weight: Higher λ in SPO+P forces predictions away from negatives but may over-constrain the model

- Failure signatures:
  - High variance in regret across epochs: Gradients may be too noisy or the planner is finding different optimal solutions
  - Training stalls: Learning rate may be too low or the transformation is causing all costs to be too similar
  - Cache misses dominate: The cost space may be too diverse for effective caching

- First 3 experiments:
  1. Compare add-min vs thresholding on a small planning domain with known ground truth
  2. Test different caching percentages (5%, 10%, 20%) on medium-sized instances
  3. Evaluate boundn planning vs optimal planning on large instances to measure regret-quality tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can DFL be extended to handle state-dependent action costs where the cost of an action depends on the current state?
- Basis in paper: [explicit] The authors mention "Future work includes reducing computational costs further, as well as DFL for state-dependent action cost prediction or other action components; for which SPO and related techniques is insufficient."
- Why unresolved: Current SPO-based techniques assume action costs are independent of state, but many real-world planning problems involve costs that vary based on the current situation.
- What evidence would resolve it: A novel DFL framework that successfully incorporates state-dependent costs into the prediction model and demonstrates improved planning outcomes compared to state-independent approaches.

### Open Question 2
- Question: What is the optimal balance between solution caching percentage and planning quality in DFL for large-scale planning problems?
- Basis in paper: [inferred] The paper shows that caching 20% of solutions works well, but also notes that 10% doesn't consistently outperform MSE. The optimal percentage likely depends on problem complexity and size.
- Why unresolved: The paper only tests 10% and 20% caching ratios. The relationship between caching percentage, computational savings, and solution quality across different problem scales remains unexplored.
- What evidence would resolve it: A comprehensive study across diverse problem sizes and domains showing the trade-off curve between caching percentage, training time reduction, and regret performance.

### Open Question 3
- Question: Can alternative gradient computation methods beyond SPO (such as implicit differentiation approaches) provide better convergence properties for DFL in planning?
- Basis in paper: [explicit] The authors note that "For an extensive analysis of existing DFL techniques, we refer readers to the survey by Mandi et al. [2023]" and focus specifically on SPO in this work.
- Why unresolved: SPO provides a robust subgradient but may not be optimal for planning-specific characteristics. Other DFL techniques like implicit differentiation or black-box solvers could potentially yield better gradients.
- What evidence would resolve it: Empirical comparison of multiple DFL gradient computation methods (SPO, implicit differentiation, black-box solvers) on the same planning benchmarks, showing which approach consistently achieves lowest regret with fastest convergence.

## Limitations

- The paper demonstrates strong results on synthetic domains but validation on real-world planning problems remains untested
- The caching mechanism's effectiveness depends heavily on the similarity structure of the cost space, which may vary significantly across different planning domains
- The transformation methods for negative predictions could potentially introduce bias in the learned models

## Confidence

- High confidence: The fundamental DFL approach and gradient computation using SPO+ is sound and well-established
- Medium confidence: The add-min transformation provides better information preservation than thresholding, though empirical tradeoffs warrant further study
- Medium confidence: Solution caching provides significant speedups without quality degradation in tested domains

## Next Checks

1. Test the proposed methods on real-world planning domains (e.g., robotics navigation, logistics planning) to verify generalization beyond synthetic data
2. Conduct ablation studies varying the penalty weight λ in SPO+P to understand its impact on prediction quality and training stability
3. Evaluate the sensitivity of caching effectiveness to different feature distributions and cost structures across diverse planning domains