---
ver: rpa2
title: 'Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation'
arxiv_id: '2408.03505'
source_url: https://arxiv.org/abs/2408.03505
tags:
- encoder
- pipeline
- training
- bubbles
- bubble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Optimus addresses the inefficiency of training multimodal large
  language models (MLLMs) caused by substantial GPU idle times due to heterogeneous
  modality models and complex data dependencies. It introduces a novel approach of
  scheduling encoder computations within LLM bubbles, leveraging separate parallel
  plans for encoders and LLMs to enable all GPUs to hold both model states.
---

# Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation

## Quick Facts
- **arXiv ID:** 2408.03505
- **Source URL:** https://arxiv.org/abs/2408.03505
- **Reference count:** 40
- **Primary result:** Accelerates MLLM training by 20.5%-21.3% with ViT-22B and GPT-175B models over 3072 GPUs compared to state-of-the-art baselines

## Executive Summary
Optimus addresses the inefficiency of training multimodal large language models (MLLMs) caused by substantial GPU idle times due to heterogeneous modality models and complex data dependencies. It introduces a novel approach of scheduling encoder computations within LLM bubbles, leveraging separate parallel plans for encoders and LLMs to enable all GPUs to hold both model states. Optimus uses a two-stage dependency management strategy and decomposes encoder computations into kernels to exploit sub-millisecond bubbles. Evaluations show that Optimus accelerates MLLM training by 20.5%-21.3% with ViT-22B and GPT-175B models over 3072 GPUs compared to state-of-the-art baselines, and outperforms existing MLLM training systems by 20.3% on average.

## Method Summary
Optimus accelerates MLLM training by exploiting GPU bubbles - idle times caused by heterogeneous parallelism in multimodal models. The system separates parallel plans for encoder and LLM components, ensuring all GPUs hold both model states. It decomposes encoder computations into kernels to fit within sub-millisecond bubbles and uses a two-stage dependency management strategy: local scheduling for iteration and encoder-internal dependencies, and global ordering for microbatch-level encoder-LLM dependencies. The approach schedules encoder computations during tensor, data, and pipeline parallelism bubbles in the LLM, significantly improving GPU utilization and overall training efficiency.

## Key Results
- Accelerates MLLM training by 20.5%-21.3% with ViT-22B and GPT-175B models over 3072 GPUs compared to state-of-the-art baselines
- Outperforms existing MLLM training systems by 20.3% on average
- Reduces GPU idle time from 31.2% to 15.3% through bubble exploitation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimus reduces GPU idle time in MLLM training by scheduling encoder computations within LLM bubbles.
- **Mechanism:** The system exploits sub-millisecond bubbles caused by tensor parallelism (TP) communications and longer bubbles from data and pipeline parallelism (DP and PP). By decomposing encoder layer computations into kernels, it schedules these kernels during the LLM bubbles, enabling efficient use of GPU cycles.
- **Core assumption:** Encoder computations can be decomposed into kernels small enough to fit within sub-millisecond bubbles and that the overall data dependencies can be managed to allow this interleaving.
- **Evidence anchors:**
  - [abstract] "Optimus uses a two-stage dependency management strategy and decomposes encoder computations into kernels to exploit sub-millisecond bubbles."
  - [section] "To address this, we decompose encoder layer computations into sequences of kernels, enabling the effective utilization of these brief bubbles."
- **Break condition:** If encoder kernel sizes exceed bubble durations or if data dependencies between encoder and LLM cannot be managed without causing training instability.

### Mechanism 2
- **Claim:** Colocating encoder and LLM model states on all GPUs enables scheduling encoder computations during LLM bubbles.
- **Mechanism:** Instead of using a unified parallel strategy, Optimus assigns separate parallel plans for encoders and LLMs, ensuring that each GPU holds both model states. This allows any GPU to execute encoder computations during LLM bubbles, rather than being idle.
- **Core assumption:** The memory overhead of colocating both models on each GPU is manageable and does not outweigh the performance gains from bubble utilization.
- **Evidence anchors:**
  - [abstract] "To make scheduling encoder computation possible for all GPUs, Optimus searches the separate parallel plans for encoder and LLM."
  - [section] "To ensure that each GPU possesses both encoder and LLM model states, we propose assigning separate parallel plans to encoders and LLMs across all GPUs."
- **Break condition:** If the memory overhead becomes prohibitive, leading to out-of-memory errors or if the performance gains do not justify the increased memory usage.

### Mechanism 3
- **Claim:** Dual-stage dependency management (local scheduling and global ordering) ensures correct data dependencies while allowing encoder computations in LLM bubbles.
- **Mechanism:** Local scheduling handles iteration dependencies and encoder-internal dependencies. Global ordering ensures microbatch-level dependencies between encoders and LLM by sequencing encoder forward and backward pass times across microbatches.
- **Core assumption:** The microbatch-level dependencies can be managed without causing data race conditions or incorrect gradient computations.
- **Evidence anchors:**
  - [abstract] "We employ a two-stage dependency management strategy: local scheduling to address the first two types of dependencies and global ordering to handle the encoder-LLM microbatch-level dependencies."
  - [section] "To manage these dependencies, we employ a two-stage dependency management strategy: local scheduling to address the first two types of dependencies and global ordering to handle the encoder-LLM microbatch-level dependencies."
- **Break condition:** If the global ordering fails to correctly sequence the dependencies, leading to incorrect model updates or training divergence.

## Foundational Learning

- **Concept: Tensor Parallelism (TP)**
  - Why needed here: Understanding TP is crucial because it is a major source of bubbles in MLLM training, and Optimus exploits these bubbles by scheduling encoder computations during TP communications.
  - Quick check question: What are the primary communication patterns in TP that cause GPU bubbles, and how does Optimus address them?

- **Concept: Pipeline Parallelism (PP)**
  - Why needed here: PP is another source of bubbles due to data dependencies between pipeline stages. Optimus manages these dependencies through local scheduling and global ordering to allow encoder computations during PP bubbles.
  - Quick check question: How do warm-up and cool-down phases in PP contribute to bubbles, and what strategies does Optimus use to minimize their impact?

- **Concept: Data Parallelism (DP)**
  - Why needed here: DP involves communication overhead for gradient aggregation, causing bubbles. Optimus schedules encoder computations during these DP bubbles to improve GPU utilization.
  - Quick check question: What are the differences between all-gather and reduce-scatter operations in DP, and how do they contribute to bubble formation?

## Architecture Onboarding

- **Component map:**
  - Model Planner -> Bubble Scheduler -> Dependency Manager
- **Critical path:**
  1. Model Planner determines parallel plans for encoders and LLMs.
  2. Bubble Scheduler initializes schedules with coarse-grained bubble exploitation.
  3. Bubble Scheduler refines schedules with fine-grained bubble exploitation.
  4. Dependency Manager ensures correct microbatch-level dependencies.
- **Design tradeoffs:**
  - Memory overhead vs. performance gain: Colocating both models increases memory usage but improves bubble utilization.
  - Scheduling complexity vs. runtime efficiency: Fine-grained scheduling is more complex but can lead to better performance.
  - Flexibility vs. specificity: Optimus is designed for typical MLLM architectures and may need adaptation for complex computation graphs.
- **Failure signatures:**
  - Increased iteration time: Indicates inefficient bubble utilization or scheduling errors.
  - Out-of-memory errors: Suggests the memory overhead of colocation is too high.
  - Training divergence: Points to issues with dependency management or incorrect data ordering.
- **First 3 experiments:**
  1. **Experiment with a simple MLLM:** Start with a small-scale MLLM (e.g., ViT-3B + GPT-11B) to verify basic functionality and measure bubble utilization.
  2. **Experiment with varying GPU counts:** Scale the number of GPUs while keeping the batch size constant to observe the impact on bubble ratios and scheduling efficiency.
  3. **Experiment with multi-encoder MLLM:** Test Optimus on an MLLM with multiple encoders to validate its ability to handle complex dependencies and scheduling scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Optimus's performance scale when applied to MLLM models with more than two encoders or complex multi-branch architectures?
- Basis in paper: [inferred] The paper discusses Optimus's ability to handle multi-branch encoders but focuses primarily on two-encoder configurations in experiments.
- Why unresolved: The paper only evaluates multi-encoder MLLMs with two encoders, leaving uncertainty about performance with more complex architectures.
- What evidence would resolve it: Experimental results showing Optimus's performance on MLLMs with three or more encoders, including iteration time and MFU metrics compared to baseline methods.

### Open Question 2
- Question: What is the impact of dynamic kernel execution time variations on Optimus's bubble scheduling accuracy, and how significant is the performance degradation in practice?
- Basis in paper: [explicit] The paper mentions that Optimus simplifies execution by not considering fluctuations in CUDA kernel runtime and suggests real-time performance monitoring as a possible solution.
- Why unresolved: The paper does not provide empirical data on how kernel execution time variations affect Optimus's scheduling accuracy or the resulting performance impact.
- What evidence would resolve it: Empirical measurements of Optimus's scheduling efficiency under varying kernel execution times, including the percentage of suboptimal scheduling and its effect on iteration time.

### Open Question 3
- Question: How does Optimus's memory overhead change when scaling to models with significantly different parameter ratios between encoders and LLMs?
- Basis in paper: [explicit] The paper analyzes memory overhead in Section 4.5, showing up to 12% overhead in tested configurations, but does not explore models with different encoder-LLM parameter ratios.
- Why unresolved: The memory analysis is limited to specific model sizes used in experiments, and it's unclear how the overhead scales with different encoder-LLM parameter ratios.
- What evidence would resolve it: Memory usage measurements for a range of MLLM configurations with varying encoder-LLM parameter ratios, showing the relationship between parameter distribution and memory overhead.

## Limitations
- The memory overhead of colocating encoder and LLM model states on all GPUs may become prohibitive for very large models
- The approach may not generalize well to MLLM architectures with complex multi-branch dependencies or irregular computation graphs
- The scheduling strategy assumes relatively stable kernel execution times and may not handle dynamic runtime variations effectively

## Confidence
**High confidence:** The core observation that GPU bubbles exist in MLLM training due to heterogeneous parallelism is well-established. The decomposition of encoder computations into kernels for bubble exploitation is a straightforward application of existing parallel computing principles. The experimental results showing 20.5%-21.3% speedup over baselines are concrete and specific.

**Medium confidence:** The effectiveness of the two-stage dependency management strategy is plausible given prior work on pipeline parallelism, but the specific implementation details for multimodal models are not fully detailed. The memory overhead claims are reasonable but not thoroughly validated across different model scales.

**Low confidence:** The claim that Optimus can handle "typical" MLLM architectures without adaptation is vague. The paper doesn't clearly define what constitutes "typical" or provide evidence that the approach generalizes beyond the specific ViT-22B and GPT-175B configuration tested.

## Next Checks
1. **Memory overhead validation:** Test Optimus with progressively larger encoder and LLM combinations (e.g., ViT-6B + GPT-11B, ViT-11B + GPT-33B) to determine the practical limits of the colocation strategy and quantify the actual memory overhead compared to baseline approaches.

2. **Bubble utilization profiling:** Instrument the system to measure actual bubble utilization rates at different scales (e.g., 1024, 2048, 3072 GPUs) to verify that the claimed performance gains directly correlate with improved GPU utilization rather than other factors.

3. **Generalization test:** Implement a multi-encoder MLLM configuration (e.g., text + image + audio encoders) to evaluate whether Optimus can maintain its performance advantages with more complex dependency structures and irregular computation graphs.