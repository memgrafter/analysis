---
ver: rpa2
title: Large Language Models for Ingredient Substitution in Food Recipes using Supervised
  Fine-tuning and Direct Preference Optimization
arxiv_id: '2412.04922'
source_url: https://arxiv.org/abs/2412.04922
tags:
- ingredient
- recipe
- substitution
- prompt
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of ingredient substitution in recipes
  using large language models (LLMs). The authors fine-tune Mistral7B with supervised
  fine-tuning and direct preference optimization on the Recipe1MSub dataset, employing
  various prompt strategies and parameter-efficient fine-tuning methods.
---

# Large Language Models for Ingredient Substitution in Food Recipes using Supervised Fine-tuning and Direct Preference Optimization

## Quick Facts
- arXiv ID: 2412.04922
- Source URL: https://arxiv.org/abs/2412.04922
- Reference count: 36
- Primary result: Mistral7B with SFT and DPO achieves Hit@1 of 22.04%, outperforming GISMO baseline

## Executive Summary
This paper addresses ingredient substitution in food recipes using large language models. The authors fine-tune Mistral7B with supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on the Recipe1MSub dataset. Through systematic experimentation with different prompt strategies and parameter-efficient fine-tuning methods, they achieve a Hit@1 score of 22.04%, demonstrating that LLMs can effectively learn ingredient substitution tasks when properly fine-tuned with appropriate context and optimization techniques.

## Method Summary
The authors fine-tune Mistral7B on the Recipe1MSub dataset using supervised fine-tuning with QLoRA parameter-efficient fine-tuning, followed by Direct Preference Optimization. The method involves preparing the dataset with recipe titles, constructing prompts with source ingredient and recipe title, fine-tuning with SFT using QLoRA (rank 64, alpha 32, learning rate 2e-4), and applying DPO using a preference dataset. The approach systematically explores different prompt contexts and PEFT techniques to optimize performance on the ingredient substitution task.

## Key Results
- Mistral7B with SFT and DPO achieves Hit@1 score of 22.04%, outperforming the GISMO baseline
- Prompting with source ingredient + recipe title yields higher Hit@1 than including full recipe ingredients or instructions
- QLoRA fine-tuning achieves higher performance than LoRA or GaLore for Mistral7B on this dataset
- DPO further improves substitution quality over SFT alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting with source ingredient + recipe title yields higher Hit@1 than including full recipe ingredients or instructions.
- Mechanism: The recipe title acts as a concise, high-signal context that narrows the substitution space while avoiding prompt dilution from lengthy details.
- Core assumption: LLMs perform better when prompts are short, focused, and contain only the most salient context for the task.
- Evidence anchors: [abstract] Mentions experimentation with "different contexts related to recipes" and concludes "best result was obtained with the source ingredient and the recipe title." [section] "When the context is considered, results in Table 3 clearly indicate that for both the models, the best result is achieved when source ingredient and recipe title are used in the context."

### Mechanism 2
- Claim: SFT using QLoRA achieves higher performance than LoRA or GaLore for Mistral7B on this dataset.
- Mechanism: QLoRA quantizes model weights and applies low-rank adapters, enabling more efficient fine-tuning without losing precision compared to full LoRA or gradient-projection methods.
- Core assumption: Model quantization does not harm downstream task accuracy when the dataset is sufficiently large and representative.
- Evidence anchors: [section] "QLoRA gave the best result among them. Therefore subsequent experiments for all PEFT techniques are conducted using these values." [section] Table 6 shows Mistral 7B with QLoRA: Hit@1 21.75%, outperforming LoRA (15.45%) and GaLore (20.19%).

### Mechanism 3
- Claim: DPO further improves substitution quality over SFT alone.
- Mechanism: DPO directly optimizes the model to prefer correct substitutions over incorrect ones without requiring a separate reward model, improving alignment with user intent.
- Core assumption: Preference data (triplets of prompt, chosen answer, rejected answer) is representative of the task and sufficient to shift model behavior.
- Evidence anchors: [abstract] "The best results are produced by the Mistral7-Base LLM after fine-tuning and DPO. This result outperforms the strong baseline of GISMO, with a Hit@1 score of 22.04." [section] "Only DPO outperforms the SFT results. The significant drop in multi-task learning result is surprising."

## Foundational Learning

- Concept: Zero-shot vs few-shot vs fine-tuning distinction
  - Why needed here: Determines how much task-specific data and tuning are required to achieve strong performance.
  - Quick check question: Why did zero-shot and one-shot results lag far behind SFT in this paper?

- Concept: Prompt engineering patterns (Persona, Template, Context Manager)
  - Why needed here: Influences how well the model interprets the task and extracts relevant context.
  - Quick check question: What happens to Hit@1 if you add more context like cooking instructions instead of just the title?

- Concept: Parameter-efficient fine-tuning (LoRA, QLoRA, GaLore)
  - Why needed here: Enables adaptation of large models within GPU memory limits.
  - Quick check question: Which PEFT technique achieved the highest Hit@1 and why?

## Architecture Onboarding

- Component map: Recipe1MSubs → add titles → split train/val/test → prompt builder (source ingredient + recipe title) → SFT (QLoRA) → DPO (optional) → evaluation → Hit@1
- Critical path: 1) Load and preprocess dataset with titles 2) Construct prompt template 3) Fine-tune base LLM with SFT (QLoRA) 4) Apply DPO if improving over SFT 5) Evaluate Hit@1 on test set
- Design tradeoffs:
  - Full SFT vs QLoRA: memory vs possible accuracy loss
  - Including recipe ingredients vs just title: context richness vs prompt dilution
  - Multi-task learning vs single-task: knowledge transfer vs over-fitting risk
- Failure signatures:
  - Hit@1 plateaus or drops after SFT: likely over-fitting or suboptimal PEFT rank/alpha
  - Very low zero-shot performance: base model lacks culinary knowledge
  - DPO degrades results: preference data too small or biased
- First 3 experiments:
  1. Compare zero-shot vs one-shot vs SFT Hit@1 for Llama-2 7B and Mistral 7B
  2. Test prompt variants: ingredient-only, ingredient+title, ingredient+ingredients, ingredient+title+instructions
  3. Run QLoRA fine-tuning with different rank/alpha combinations on small subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using even larger language models (e.g., Mistral 123B) on the performance of the ingredient substitution task?
- Basis in paper: [explicit] The authors mention that using an LLM with a larger parameter count (e.g., Mistral 123B) may be able to take advantage of larger datasets and additional optimization techniques, but they do not have sufficient computational resources to experiment with these large models.
- Why unresolved: The authors did not have access to the necessary computational resources to experiment with larger models.
- What evidence would resolve it: Experiments using larger language models (e.g., Mistral 123B) on the ingredient substitution task with the same dataset and evaluation metrics would provide evidence of the impact of model size on performance.

### Open Question 2
- Question: How does the performance of the LLM-based ingredient substitution system vary with different dataset sizes and fine-tuning strategies?
- Basis in paper: [explicit] The authors experimented with two dataset sizes (15,000 samples and the full dataset of 49,044 samples) and observed that different LLMs behaved differently when the dataset size increased. They also tried different fine-tuning strategies like two-stage fine-tuning, multi-task learning, and DPO, with varying results.
- Why unresolved: The authors did not conduct a comprehensive study on the impact of dataset size and fine-tuning strategies on the performance of the system.
- What evidence would resolve it: A systematic study varying dataset sizes, fine-tuning strategies, and their combinations, along with their impact on performance metrics like Hit@1, would provide insights into the optimal configuration for the task.

### Open Question 3
- Question: What are the limitations of the current LLM-based ingredient substitution system, and how can they be addressed in future research?
- Basis in paper: [explicit] The authors acknowledge that their results are far from optimal and suggest potential improvements like using larger LLMs, more fine-grained experiments, and pre-training a base LLM with the recipe corpus.
- Why unresolved: The authors have identified potential areas for improvement but have not yet explored them in their research.
- What evidence would resolve it: Future research implementing the suggested improvements and evaluating their impact on the performance of the ingredient substitution system would provide evidence of their effectiveness in addressing the limitations of the current approach.

## Limitations
- Dataset construction transparency lacking, with no detailed methodology for selecting substitute ingredients or ensuring title accuracy
- Prompt template specifics not provided, making it difficult to assess whether performance gains are due to prompt structure or implementation details
- Preference data construction methodology insufficiently detailed, creating uncertainty about bias in DPO improvements

## Confidence
- High confidence: QLoRA outperforms LoRA and GaLore for Mistral7B on this task (supported by direct experimental comparison with clear numerical results)
- Medium confidence: Prompt context matters significantly (shows clear performance differences but lacks detailed prompt templates and generalizability beyond single dataset)
- Medium confidence: DPO effectiveness claim (shows modest improvement over SFT but preference data construction methodology is insufficiently detailed)

## Next Checks
1. **Ablation study on prompt components** - Systematically test the contribution of each prompt element (ingredient, title, ingredients list, instructions) by removing them one at a time while keeping the model and dataset constant.

2. **Cross-dataset validation** - Evaluate the best-performing model (Mistral7B + SFT + DPO) on an independent ingredient substitution dataset to assess whether performance gains generalize beyond Recipe1MSub.

3. **Preference data quality analysis** - Conduct a human evaluation of the chosen vs rejected answer pairs used in DPO to verify that the preference dataset contains meaningful distinctions and doesn't introduce bias.