---
ver: rpa2
title: Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration
arxiv_id: '2410.08102'
source_url: https://arxiv.org/abs/2410.08102
tags:
- data
- actor
- selection
- each
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of efficient pretraining data selection
  for language models, addressing conflicts between different data selection methods
  like quality scoring, domain weighting, and topic diversity. The proposed solution
  is a multi-actor collaborative framework where independent data selection methods
  (actors) prioritize data based on their criteria and update their rules using the
  current model state, while a console dynamically integrates their outputs and adjusts
  their contributions during training.
---

# Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration

## Quick Facts
- arXiv ID: 2410.08102
- Source URL: https://arxiv.org/abs/2410.08102
- Reference count: 40
- Primary result: Up to 10.5% average performance gain over state-of-the-art baselines

## Executive Summary
This paper introduces a multi-actor collaborative framework for efficient pretraining data selection in language models. The framework addresses conflicts between different data selection methods (quality scoring, domain weighting, topic diversity) by using independent actors that prioritize data based on their criteria while dynamically adjusting their contributions through a central console. The method achieves superior data efficiency and faster convergence while reducing computational overhead compared to existing online data selection methods.

## Method Summary
The proposed method implements a multi-actor collaborative framework where independent data selection methods (actors) prioritize data based on their specific criteria - quality, domain, or topic. Each actor stores initial labels for the entire dataset and updates its prioritization rules using influence functions computed from the current model state. A console dynamically integrates actor outputs and adjusts their weights based on performance impact during training. The framework combines offline priors with online model-derived preferences through a sampled holdout set and CPU-based calculations for actor updates, achieving computational efficiency while maintaining adaptivity.

## Key Results
- Achieves up to 10.5% average performance gain across multiple language model benchmarks
- Improves data efficiency and convergence speed compared to state-of-the-art baselines
- Shows better performance than existing methods across models from 1.3B to 8B parameters
- Ablation studies confirm the necessity of all core components for performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-actor collaboration dynamically resolves conflicts between data selection methods by weighting actors based on their impact on model performance.
- Mechanism: Each actor independently prioritizes data based on its heuristic (quality, domain, or topic), computes rewards using influence functions, and updates its internal weights. The actor console aggregates these scores and adjusts collaborative weights based on actor performance, ensuring optimal data selection over training stages.
- Core assumption: Influence functions accurately estimate the impact of individual data points on downstream task performance, enabling effective reward computation.
- Evidence anchors:
  - [abstract] "a console is designed to adjust the impacts of different actors at various stages and dynamically integrate information from all actors"
  - [section] "the console resolves these by adjusting the weights θA to prioritize the actors that have a greater positive impact on the model's performance"
- Break condition: If influence function estimates are inaccurate or computationally prohibitive, the reward signals would misguide actor weight updates, leading to suboptimal data selection.

### Mechanism 2
- Claim: The framework achieves superior data efficiency by combining offline priors with online model-derived preferences.
- Mechanism: Before training, each actor stores initial labels (quality, domain, topic) for the entire dataset. During training, actors update their weights based on the current model's feedback, allowing data selection to adapt to the model's evolving learning state while leveraging initial heuristic knowledge.
- Core assumption: Offline labeling provides sufficient initial guidance that can be effectively refined by online updates during training.
- Evidence anchors:
  - [abstract] "each data selection method independently prioritizes data based on its criterion and updates its prioritization rules using the current state of the model"
  - [section] "Before the training process, some initial information...is computed for the entire pretraining corpus, and this information is stored separately in each actor's memory"
- Break condition: If initial offline labels are poor quality or become irrelevant as the model evolves, the framework would fail to provide meaningful guidance despite online updates.

### Mechanism 3
- Claim: The multi-actor framework reduces computational overhead compared to existing online data selection methods while maintaining adaptivity.
- Mechanism: Instead of re-labeling the entire dataset at each training stage (as MATES does), the framework uses a sampled holdout set and CPU-based calculations for actor parameter updates, making the computational overhead negligible compared to heavy LM training.
- Core assumption: Sampling-based actor updates provide sufficient signal for effective collaboration without requiring full dataset re-evaluation.
- Evidence anchors:
  - [section] "we find that a group of light-weight actors collaboratively enables superior data selection, which is more computational efficiency than any method that requires a heavy data processing or label procedure"
  - [section] "the collaborative, dynamic learning procedure introduced in our multi-actor framework is computational efficient; by using a sampled holdout set and CPU-based calculations"
- Break condition: If the sampled holdout set fails to capture sufficient diversity or if CPU-based calculations become bottlenecks, the computational efficiency gains would diminish.

## Foundational Learning

- Concept: Influence functions in deep learning
  - Why needed here: The framework uses influence functions to estimate the impact of individual data points on model performance, which serves as the reward signal for actor updates.
  - Quick check question: What does the influence function equation r(xi) = -∇ML(Tref|M)⊤H⁻¹∇ML(xi|M) compute in terms of model behavior?

- Concept: Multi-armed bandit problems
  - Why needed here: The actor console's dynamic weight adjustment resembles a multi-armed bandit problem where different arms (actors) are selected based on their performance rewards.
  - Quick check question: How does the sliding average update rule wj_A ← (1-η_A)·w_j_A + η_A·R_j_A relate to exploration-exploitation tradeoffs in bandit problems?

- Concept: Curriculum learning and data mixing strategies
  - Why needed here: The framework builds upon existing data selection methods like domain mixing (RegMix) and quality-based selection (QuRating), requiring understanding of these techniques.
  - Quick check question: What distinguishes the multi-actor approach from traditional domain mixing methods that use fixed weights?

## Architecture Onboarding

- Component map:
  Pretraining data pool -> Actors (Quality, Domain, Topic) -> Actor console -> Model -> Reference tasks (LAMBADA, SQuAD, Jeopardy)

- Critical path:
  1. Offline labeling: Annotate entire dataset with quality, domain, and topic information
  2. Actor initialization: Set initial weights using RegMix framework
  3. Training loop: At update intervals, sample data, compute rewards via influence functions, update actor weights
  4. Score aggregation: Actor console combines actor scores with dynamic weights
  5. Data selection: Select top-k scoring data points for next training stage

- Design tradeoffs:
  - Fixed vs. dynamic weights: Fixed weights simplify implementation but miss model adaptation opportunities
  - Sampling frequency: More frequent updates capture model changes better but increase computational cost
  - Reference task selection: Different reference tasks may bias actor preferences in different directions

- Failure signatures:
  - Poor performance on specific task types indicates individual actor weaknesses
  - Degraded performance over training stages suggests inadequate weight adaptation
  - High computational overhead indicates inefficient sampling or update mechanisms

- First 3 experiments:
  1. Single-actor baseline: Implement and evaluate each actor (quality, domain, topic) independently to establish baseline performance
  2. Fixed-weight multi-actor: Combine all three actors with equal, non-adaptive weights to test collaborative benefits without dynamic adjustment
  3. Dynamic-weight validation: Implement the full multi-actor framework with dynamic weight adjustment and compare against fixed-weight version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed multi-actor collaborative methods scale to larger language models (e.g., 10B+ parameters) and does the performance gain remain consistent or diminish with scale?
- Basis in paper: [inferred] The paper evaluates the method on models up to 8B parameters, but notes plans to continue training larger models according to scaling laws. The authors hypothesize the method has strong potential for larger models, but empirical validation is missing.
- Why unresolved: The experiments were limited to models up to 8B parameters due to computational constraints, leaving open whether the 10.5% average performance gain observed in smaller models would hold or decrease when applied to much larger models.
- What evidence would resolve it: Training and evaluating the multi-actor framework on models with 10B+ parameters using the same methodology and benchmarks would show whether the performance improvements scale proportionally, diminish, or plateau.

### Open Question 2
- Question: Can the proposed method be effectively extended to incorporate additional data selection criteria beyond quality, domain, and topic, such as syntactic complexity or factual accuracy, without significantly increasing computational overhead?
- Basis in paper: [explicit] The paper describes a guideline for integrating new criteria as actors and claims the framework is flexible and extensible, but does not test adding new criteria beyond the three implemented.
- Why unresolved: While the framework is described as modular, the paper only validates the approach with three specific actors (quality, domain, topic). It is unclear how adding new, potentially more complex criteria would affect both performance and computational efficiency.
- What evidence would resolve it: Implementing and evaluating the framework with additional actors (e.g., factual accuracy, syntactic complexity) and measuring both performance gains and computational overhead would clarify the method's extensibility limits.

### Open Question 3
- Question: What is the impact of different reference task selections on the multi-actor collaborative method's performance, and how sensitive is the method to the choice of reference tasks compared to other data selection approaches?
- Basis in paper: [explicit] The paper includes an ablation study on reference task selection, showing marginal impact on average performance (within 0.5 points) and that performance remains significantly better than random selection across choices.
- Why unresolved: While the paper demonstrates that reference task choice has minimal impact, it does not explore whether certain combinations of reference tasks might be optimal for specific downstream task categories or whether this sensitivity differs from other methods like MATES that rely heavily on reference task selection.
- What evidence would resolve it: Systematic testing of different reference task combinations across various downstream task categories and comparing sensitivity to reference task choice against other methods would clarify the optimal configuration and relative robustness.

## Limitations
- The computational efficiency claims lack empirical validation through runtime measurements
- The framework's performance on extremely large models (10B+ parameters) remains untested
- The method's extensibility to additional selection criteria beyond quality, domain, and topic is theoretical

## Confidence
- **High Confidence**: The core multi-actor framework architecture and its ability to combine multiple data selection criteria are well-supported by the ablation studies showing performance degradation when individual actors are removed.
- **Medium Confidence**: Claims about 10.5% average performance gains and improved data efficiency are supported by benchmark results, though the specific experimental conditions and hyperparameter choices affect reproducibility.
- **Low Confidence**: Computational efficiency claims lack empirical validation through runtime measurements, making it difficult to assess the practical benefits relative to implementation complexity.

## Next Checks
1. Implement runtime profiling to measure actual computational overhead of the multi-actor framework versus baseline methods, including influence function calculations and actor weight updates.
2. Conduct sensitivity analysis on the sliding average factor ηA and influence function hyperparameters to determine their impact on performance and stability.
3. Test the framework's robustness across different reference task sets to verify that actor weight adjustments generalize beyond the specific LAMBADA/SQuAD/Jeopardy combination used in the paper.