---
ver: rpa2
title: 'RecGPT: Generative Personalized Prompts for Sequential Recommendation via
  ChatGPT Training Paradigm'
arxiv_id: '2404.08675'
source_url: https://arxiv.org/abs/2404.08675
tags:
- user
- recommendation
- personalized
- prompts
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RecGPT, a sequential recommendation framework
  inspired by ChatGPT's training paradigm. The core idea is to model recommendation
  as a conversation where users' historical actions are inputs and the model generates
  personalized prompts as feedback.
---

# RecGPT: Generative Personalized Prompts for Sequential Recommendation via ChatGPT Training Paradigm

## Quick Facts
- arXiv ID: 2404.08675
- Source URL: https://arxiv.org/abs/2404.08675
- Reference count: 33
- Primary result: RecGPT outperforms state-of-the-art sequential recommendation methods on four public datasets and shows significant improvements in online A/B testing on Kuaishou video APP

## Executive Summary
RecGPT introduces a novel sequential recommendation framework inspired by ChatGPT's training paradigm. The method models recommendation as a conversation where users' historical actions are inputs and the model generates personalized prompts as feedback. By leveraging a Transformer decoder pre-trained autoregressively and then fine-tuned with user IDs and segment IDs to generate personalized prompts, RecGPT captures multi-step user preference evolution. The approach uses an auto-regressive two-step recall mechanism during inference, demonstrating superior performance over existing methods on both offline benchmarks and online A/B testing.

## Method Summary
RecGPT employs a two-stage training paradigm: pre-training a Transformer decoder autoregressively on user behavior sequences, followed by fine-tuning with personalized prompts and segment IDs. The model generates K personalized prompt items using auto-regressive decoding, conditioned on the user's historical sequence and user ID. These prompts are treated as additional context during fine-tuning, effectively providing multi-step feedback beyond just clicked items. During inference, an auto-regressive two-step recall approach is used to recommend items, where the model first recalls m items and then n items conditioned on the first set, capturing the evolution of user preferences over time.

## Key Results
- RecGPT achieves state-of-the-art performance on four public datasets (Sports, Beauty, Toys, Yelp) with significant improvements in HR@10 and NDCG@10 metrics
- Online A/B testing on Kuaishou video APP demonstrates substantial improvements in user engagement metrics compared to the previous ComiRec model
- The two-step auto-regressive recall approach outperforms single-step inner product scoring methods in capturing multi-step user preference evolution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The auto-regressive two-step recall in RecGPT captures multi-step user preference evolution better than single-step inner product scoring
- Mechanism: The model generates multiple user embedding vectors sequentially via auto-regressive decoding, then uses these vectors to recall top-k items in two steps (first m items, then n items conditioned on the first)
- Core assumption: User preferences evolve in a sequential, context-dependent manner that can be captured by generating intermediate embedding vectors rather than a single static user vector
- Evidence anchors: [abstract] "we predict several user interests as user representations in an autoregressive manner. For each interest vector, we recall several items with the highest similarity and merge the items recalled by all interest vectors into the final result."

### Mechanism 2
- Claim: Personalized prompts generated from the pre-trained model improve recommendation accuracy by incorporating both clicked and unclicked items as feedback signals
- Mechanism: The model generates K personalized prompt items using auto-regressive decoding, conditioned on the user's historical sequence and user ID
- Core assumption: Unclicked items contain implicit preference signals that, when incorporated as prompts, help the model better model user intent and avoid overfitting to clicked items
- Evidence anchors: [abstract] "we propose incorporating unclicked items as personalized prompts in the training of models"

### Mechanism 3
- Claim: The two-stage training paradigm (pre-training + prompt-tuning) enables effective transfer learning from general sequence modeling to recommendation-specific tasks
- Mechanism: The model first pre-trains a Transformer decoder autoregressively on user behavior sequences to learn general sequential patterns, then fine-tunes with personalized prompts and segment IDs to adapt to recommendation-specific ranking tasks
- Core assumption: Pre-training on large-scale sequential data provides a strong foundation that can be specialized to recommendation through prompt-based fine-tuning without catastrophic forgetting
- Evidence anchors: [abstract] "we adopt the two-stage paradigm of ChatGPT, including pre-training and fine-tuning"

## Foundational Learning

- Concept: Transformer decoder with masked self-attention
  - Why needed here: The model needs to generate sequences autoregressively while attending only to previous items, which is exactly what the Transformer decoder architecture provides
  - Quick check question: Why does the masked self-attention matrix have zeros in the lower triangular part and negative infinity elsewhere?

- Concept: Pre-training and fine-tuning paradigm
  - Why needed here: Pre-training on large sequential data provides general sequence modeling capabilities, while fine-tuning adapts the model to recommendation-specific ranking tasks with personalized prompts
  - Quick check question: What would happen if we skipped pre-training and trained the model from scratch with personalized prompts?

- Concept: Personalized prompt generation via auto-regressive decoding
  - Why needed here: The model needs to generate context-specific prompts that capture both user preferences and temporal dynamics, which is achieved through auto-regressive decoding conditioned on user ID and history
  - Quick check question: How does the segment ID embedding help distinguish generated prompts from original behavior items?

## Architecture Onboarding

- Component map: User ID, item sequence, position embeddings -> Transformer decoder blocks with masked self-attention -> Auto-regressive decoder producing K personalized items -> Segment embeddings (distinguish original items from generated prompts) -> Item probability distributions for ranking

- Critical path:
  1. Pre-train Transformer decoder on user behavior sequences (auto-regressive)
  2. Generate K personalized prompts via auto-regressive decoding
  3. Fine-tune with prompt-enhanced sequences and segment IDs
  4. Inference: Auto-regressive two-step recall for item ranking

- Design tradeoffs:
  - Auto-regressive vs. parallel decoding: Auto-regressive provides better sequence coherence but is slower
  - Number of prompts (K): More prompts provide richer context but increase computational cost and risk of noise
  - Two-step vs. multi-step recall: Two-step balances performance and computational efficiency given data sparsity

- Failure signatures:
  - Poor performance on sparse datasets: May indicate insufficient data for auto-regressive generation
  - Overfitting to training prompts: May occur if K is too large or prompts are too similar to training data
  - Degraded performance after fine-tuning: May indicate catastrophic forgetting of pre-trained knowledge

- First 3 experiments:
  1. Pre-train baseline: Train only the Transformer decoder on user sequences without prompts, measure HR@10/NDCG@10
  2. Prompt generation ablation: Test different K values (0, 1, 3, 5, 10) to find optimal prompt quantity
  3. Two-step recall ablation: Compare RecGPT (two-step) vs RecGPT1 (single-step inner product) on all datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RecGPT vary with different sizes of the generative personalized prompts window (K) across various datasets and user demographics?
- Basis in paper: [explicit] The paper discusses the influence of the hyper-parameter K, which controls the size of the generative personalized prompts window, and mentions that increasing K enhances the model's performance
- Why unresolved: While the paper provides some insights into the effect of K, it does not explore how this parameter interacts with different datasets or user demographics
- What evidence would resolve it: Conducting experiments on diverse datasets and user groups with varying K values would provide a clearer picture of how this parameter affects RecGPT's performance in different contexts

### Open Question 2
- Question: What is the optimal ratio of two-step recall (m:n) for RecGPT across different recommendation scenarios, and how does this ratio impact the model's ability to capture user preferences?
- Basis in paper: [explicit] The paper mentions the parameter m:n, which controls the ratio of two-step recall, and provides results for different ratios
- Why unresolved: The paper does not delve into the optimal ratio for different recommendation scenarios or the underlying reasons for the observed performance
- What evidence would resolve it: Systematic experimentation with varying m:n ratios across different recommendation scenarios, coupled with an analysis of the model's performance, would shed light on the optimal settings and their impact on capturing user preferences

### Open Question 3
- Question: How does RecGPT's performance compare to other state-of-the-art sequential recommendation methods when considering different types of user feedback (e.g., clicks, purchases, ratings)?
- Basis in paper: [inferred] The paper mentions that RecGPT uses an auto-regressive two-step recall method to capture the evolution of user preferences over time, which implies that it can handle different types of user feedback
- Why unresolved: The paper does not provide a direct comparison of RecGPT's performance with other methods when considering various types of user feedback
- What evidence would resolve it: Conducting experiments that compare RecGPT's performance with other methods across different types of user feedback would provide valuable insights into its effectiveness in handling diverse user interactions

## Limitations

- Data sparsity impact: The paper doesn't thoroughly analyze performance degradation in sparse user sequences, which is concerning for real-world scenarios where many users have short interaction histories
- Prompt generation quality assessment: The paper assumes auto-generated prompts improve recommendation quality but doesn't provide direct evidence that these generated items are meaningful or relevant to users
- Computational cost analysis: The paper lacks detailed computational complexity analysis of the two-step auto-regressive recall approach and its trade-off with the number of prompts

## Confidence

- High confidence: The overall improvement claims over baseline methods are well-supported by experimental results showing consistent gains across all four datasets and metrics
- Medium confidence: The mechanism claims about personalized prompts capturing multi-step user preference evolution and incorporating unclicked items as feedback are plausible but not definitively proven
- Low confidence: The claim that the model captures "multi-step user preference evolution" through auto-regressive generation lacks direct validation

## Next Checks

1. Sequence length sensitivity analysis: Systematically evaluate RecGPT's performance across user sequences of varying lengths (e.g., 5, 10, 20, 50 interactions) to quantify how performance degrades with sparsity and determine minimum viable sequence length for effective prompt generation

2. Prompt quality evaluation: Conduct human evaluation or clustering analysis of the auto-generated prompts to verify they represent meaningful user interests rather than random or irrelevant items. Compare prompt diversity and relevance against baseline methods that don't use generated prompts

3. Ablation on prompt quantity: Test a broader range of K values (0, 1, 2, 5, 10, 20) across all datasets to determine the optimal number of prompts and identify whether there's a point of diminishing returns or negative impact from excessive prompt generation