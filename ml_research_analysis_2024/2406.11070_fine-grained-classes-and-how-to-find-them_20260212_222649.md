---
ver: rpa2
title: Fine-grained Classes and How to Find Them
arxiv_id: '2406.11070'
source_url: https://arxiv.org/abs/2406.11070
tags:
- fine-grained
- classes
- coarse
- class
- falcon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FALCON is a method that discovers fine-grained classes within coarsely
  labeled data without supervision. It jointly learns a fine-grained classifier and
  class relationships between coarse and fine classes by optimizing a loss that combines
  coarse classification, fine-grained consistency and confidence, and entropy regularization.
---

# Fine-grained Classes and How to Find Them

## Quick Facts
- arXiv ID: 2406.11070
- Source URL: https://arxiv.org/abs/2406.11070
- Authors: Matej Grcić; Artyom Gadetsky; Maria Brbić
- Reference count: 40
- Key outcome: FALCON discovers fine-grained classes from coarse supervision, achieving 22% improvement on tieredImageNet with 608 classes

## Executive Summary
FALCON addresses the challenge of discovering fine-grained classes within coarsely labeled data without requiring fine-grained supervision. The method jointly learns a fine-grained classifier and class relationships between coarse and fine classes by optimizing a loss that combines coarse classification, fine-grained consistency and confidence, and entropy regularization. FALCON can leverage multiple datasets with incompatible coarse labels and demonstrates state-of-the-art performance on eight image classification tasks and a single-cell classification task.

## Method Summary
FALCON discovers fine-grained classes from coarsely labeled data by modeling the relationship between coarse and fine classes using a binary matrix M. The method alternates between training a fine-grained classifier and inferring class relationships by solving a discrete optimization problem. It combines coarse classification loss with fine-grained consistency and confidence losses to enable better separation of fine-grained classes within a coarse class. The discrete optimization problem for inferring class relationships is efficiently solved by approximating the coarse supervision loss with a linear form using Taylor expansion.

## Key Results
- Achieves 22% improvement over baselines on tieredImageNet with 608 fine-grained classes
- Outperforms state-of-the-art methods in discovering fine-grained classes from coarse supervision
- Demonstrates effectiveness on eight image classification tasks and a single-cell classification task
- Shows ability to leverage multiple datasets with incompatible coarse labels

## Why This Works (Mechanism)

### Mechanism 1
Fine-grained classes can be discovered by leveraging coarse labels through the combination of fine-grained predictions and class relationships. FALCON models the relationship between coarse and fine classes using a binary matrix M, where each fine-grained class is associated with exactly one coarse class. The method alternates between training a fine-grained classifier and inferring class relationships by solving a discrete optimization problem that balances assignment of fine classes to coarse classes.

### Mechanism 2
Combining coarse classification loss with fine-grained consistency and confidence losses enables better separation of fine-grained classes within a coarse class. The method introduces additional objectives: LNN enforces consistency in local fine-grained predictions by maximizing dot products between predictions for a sample and its neighbors within the same coarse class; Lconf encourages confident assignment by minimizing cross entropy between fine-grained predictions and a target distribution sharpened by coarse labels.

### Mechanism 3
The discrete optimization problem for inferring class relationships can be efficiently solved by approximating the coarse supervision loss with a linear form. The method approximates the coarse classification loss using Taylor expansion, reformulating it as a linear function of M. This allows efficient inference of class relationships by solving an integer quadratic program with linear constraints.

## Foundational Learning

- **Self-supervised pretraining (MoCoV3)**: Provides good initialization for the feature extractor, which is crucial for the subsequent fine-grained discovery task.
  - Quick check question: Why initialize with self-supervised pretraining instead of random initialization or supervised pretraining?

- **Nearest neighbor retrieval in feature space**: Used to enforce consistency of fine-grained predictions within the same coarse class by maximizing similarity between a sample and its neighbors.
  - Quick check question: How does the choice of distance metric affect the quality of nearest neighbor retrieval?

- **Graph edit distance (GED)**: Used to evaluate how well the inferred coarse-fine class relationships match the ground truth relationships.
  - Quick check question: What are the limitations of using GED to compare learned and ground-truth class relationships?

## Architecture Onboarding

- **Component map**: Feature extractor (ResNet18/50 or MLP) -> Fine-grained classifier (linear layer) -> EMA parameters -> Nearest neighbor retriever -> Discrete optimizer (Gurobi)

- **Critical path**: 1) Pretrain feature extractor with self-supervised learning, 2) Initialize fine-grained classifier and class relationships, 3) Alternate between training fine-grained classifier with combined losses and inferring class relationships, 4) Evaluate fine-grained clustering accuracy and ARI

- **Design tradeoffs**: Number of fine-grained classes KF must be known or estimated; sensitivity to this hyperparameter exists but is manageable. Choice between single-dataset and multi-dataset training affects performance and implementation complexity. Use of EMA parameters vs. direct predictions for consistency loss.

- **Failure signatures**: Poor clustering accuracy suggests issues with fine-grained classifier training or class relationship inference. High graph edit distance indicates incorrect recovery of coarse-fine class relationships. Degenerate solutions (all samples assigned to same class) suggest insufficient regularization.

- **First 3 experiments**: 1) Run FALCON on CIFAR100 with ground truth KF to establish baseline performance, 2) Test sensitivity to KF by running with 80%, 100%, 120%, and 150% of true KF, 3) Compare performance when using actual vs. estimated class relationships to validate the inference component

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the limitations and future work discussions, some open questions that arise include: how the method scales to datasets with significantly larger numbers of fine-grained classes, how robust it is to noisy or incorrect coarse labels, and whether it can be extended to handle multi-label classification scenarios.

## Limitations
- Assumes hierarchical relationship between fine and coarse classes, which may not hold in all real-world scenarios
- Performance heavily depends on accurate estimation of the number of fine-grained classes (KF)
- Discrete optimization problem becomes computationally expensive for very large numbers of classes
- Limited evaluation on non-image data beyond the single-cell classification task

## Confidence
- **High confidence**: The method's ability to discover fine-grained classes from coarse labels when the hierarchical assumption holds
- **Medium confidence**: The effectiveness of the alternating optimization approach and the linear approximation for class relationship inference
- **Medium confidence**: The robustness to estimation errors in KF based on presented results

## Next Checks
1. Test FALCON on datasets where fine-grained classes do not follow a strict hierarchical structure to evaluate the robustness of the hierarchical assumption
2. Evaluate scalability by testing on datasets with significantly larger numbers of coarse and fine-grained classes (e.g., 1000+ coarse classes)
3. Apply FALCON to non-image domains (e.g., text or audio classification) with multiple incompatible label hierarchies to test cross-domain applicability