---
ver: rpa2
title: Contextualization with SPLADE for High Recall Retrieval
arxiv_id: '2405.03972'
source_url: https://arxiv.org/abs/2405.03972
tags:
- splade
- cost
- documents
- retrieval
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes using SPLADE to generate contextualized sparse
  features for High Recall Retrieval (HRR). Instead of using PLMs as classifiers,
  which are inefficient for HRR, SPLADE encodes documents as sparse vectors leveraging
  PLM contextualization.
---

# Contextualization with SPLADE for High Recall Retrieval

## Quick Facts
- arXiv ID: 2405.03972
- Source URL: https://arxiv.org/abs/2405.03972
- Reference count: 40
- Primary result: SPLADE-based contextualized sparse features reduce total review cost by 10-18% on two HRR collections, with combined SPLADE+BM25 features achieving 10-27% cost reduction

## Executive Summary
This work proposes using SPLADE to generate contextualized sparse features for High Recall Retrieval (HRR) in Technology-Assisted Review (TAR) workflows. Instead of using pre-trained language models directly as classifiers, which are inefficient for HRR, SPLADE encodes documents as sparse vectors leveraging PLM contextualization. These features are combined with BM25 for logistic regression in TAR workflows, achieving significant cost reductions while maintaining 80% recall targets.

## Method Summary
The method uses SPLADE's masked language modeling to encode documents as sparse vectors, which are then combined with BM25 features for logistic regression in TAR workflows. The approach involves two phases: one-phase with iterative relevance feedback and two-phase with uncertainty sampling. Documents are encoded using a pre-trained SPLADE model, top predicted tokens are selected as sparse features, and logistic regression models are trained using both BM25 and SPLADE features. The TARexp framework is used to implement the experiments across RCV1-v2 news articles and Jeb Bush email collections.

## Key Results
- SPLADE-based contextualized sparse features reduce total review cost by 10-18% compared to BM25 baseline
- Combining SPLADE and BM25 features further improves effectiveness, achieving 10-27% cost reduction
- Effectiveness is consistent across both one-phase and two-phase TAR workflows
- SPLADE shows robustness to domain mismatch when pre-trained on large retrieval corpora

## Why This Works (Mechanism)

### Mechanism 1
- SPLADE transforms documents into contextualized sparse vectors using a masked language model (MLM) head, improving document representation for linear classifiers
- Core assumption: The MLM head's prediction over vocabulary space, when aggregated and sparsified, provides better document representation than traditional statistical features like BM25
- Evidence: Abstract states SPLADE "encodes documents as sparse vectors leveraging PLM contextualization" and section 3 describes using MLM ability to encode documents as sparse vectors

### Mechanism 2
- Combining SPLADE and BM25 features provides complementary signals, leading to further cost reduction in TAR workflows
- Core assumption: There is valuable complementary information between contextualized sparse features and traditional statistical features that improves classification performance
- Evidence: Abstract mentions "combining SPLADE and BM25 features further improves effectiveness, achieving 10-27% cost reduction" and section 4 describes training separate logistic regression models for each feature set

### Mechanism 3
- SPLADE's retrieval fine-tuning on large corpora makes it more effective than language models fine-tuned only on target collection
- Core assumption: Retrieval fine-tuning provides better domain adaptation than simple MLM fine-tuning on target collection
- Evidence: Section 5.1 shows out-of-box BERT Large incurs 19% cost increment on RCV1, while SPLADE performs better

## Foundational Learning

- **High Recall Retrieval (HRR) optimization**: Understanding HRR is crucial because the proposed method is specifically designed for minimizing review cost while achieving high recall targets
  - Quick check: What distinguishes HRR from standard ad-hoc retrieval problems?

- **Technology-Assisted Review (TAR) workflows**: The proposed SPLADE approach is evaluated within TAR iterative workflows, so understanding how one-phase and two-phase workflows operate is essential
  - Quick check: How do one-phase and two-phase TAR workflows differ in terms of cost structure and classifier updating?

- **Learned Sparse Retrieval (LSR) vs Dense Retrieval**: SPLADE is an LSR model, and understanding the efficiency trade-offs between sparse and dense representations is key to appreciating the proposed approach
  - Quick check: What are the main efficiency advantages of learned sparse retrievers over dense retrievers?

## Architecture Onboarding

- **Component map**: SPLADE model (pretrained with retrieval fine-tuning) → Document encoding (tokenization + MLM prediction + aggregation + top-k selection) → Sparse feature matrix → Logistic regression classifier → TAR workflow (sampling + human review + retraining)
- **Critical path**: Document encoding → Logistic regression training → Document ranking → Human review → Retraining cycle
- **Design tradeoffs**: SPLADE provides contextualization but requires retrieval fine-tuning; BM25 is efficient but lacks semantic understanding; combining both adds complexity but improves effectiveness
- **Failure signatures**: Poor recall achievement, high review costs, classifier failing to improve over iterations, domain mismatch causing performance degradation
- **First 3 experiments**:
  1. Run baseline BM25 with logistic regression on RCV1 collection to establish reference performance
  2. Implement SPLADE encoding with logistic regression and compare recall/cost against baseline
  3. Test combined BM25+SPLADE approach to verify if complementary signals provide additional benefits

## Open Questions the Paper Calls Out

### Open Question 1
- How do SPLADE features perform when combined with other learned sparse retrieval methods beyond BM25, such as EPIC or other LSR models?
- Basis: The paper demonstrates combining SPLADE with BM25 improves performance, suggesting potential benefits from combining with other LSR methods
- Why unresolved: The study only experiments with combining SPLADE and BM25 features
- What evidence would resolve it: Experiments comparing SPLADE combined with various LSR methods against SPLADE alone and BM25 alone in the same TAR workflows

### Open Question 2
- Does the effectiveness of SPLADE features vary significantly across different types of HRR collections?
- Basis: The paper tests on news articles and legal documents, but differences in performance across collection types are not deeply analyzed
- Why unresolved: While results show SPLADE helps in both collections, the paper does not systematically analyze how collection characteristics impact SPLADE's effectiveness
- What evidence would resolve it: Comparative experiments across diverse HRR collections measuring SPLADE's relative effectiveness in each domain

### Open Question 3
- How sensitive is SPLADE's effectiveness to the choice of PLM architecture when used as a feature extractor for HRR?
- Basis: The paper uses SPLADE (BERT-based) without comparing alternative PLM architectures
- Why unresolved: The study uses SPLADE without comparing different PLM architectures that might offer different contextualization patterns
- What evidence would resolve it: Experiments replacing SPLADE with equivalent sparse retrievers based on different PLMs and measuring effectiveness differences

## Limitations
- Domain generalization claims are based on only two collections (news and email), limiting confidence in cross-domain robustness
- The exact mechanism by which SPLADE outperforms traditional BM25 is not rigorously validated through ablation studies
- The interaction between SPLADE and BM25 features during iterative TAR learning is not fully characterized

## Confidence
- **SPLADE Effectiveness Claims**: High confidence - well-supported by experimental results on two collections with consistent methodology
- **Combined Features Claims**: High confidence - strong empirical support with direct testing of the combination approach
- **Domain Mismatch Claims**: Medium confidence - evidence exists but comparison doesn't fully isolate retrieval fine-tuning effects

## Next Checks
1. Conduct ablation study on SPLADE components to isolate the contribution of contextualization versus other design choices
2. Test SPLADE on additional collections from different domains to verify claimed robustness to domain mismatch
3. Implement more sophisticated combination methods than simple averaging to test whether complementarity between SPLADE and BM25 is robust to the combination approach