---
ver: rpa2
title: Leveraging Foundation Models for Zero-Shot IoT Sensing
arxiv_id: '2407.19893'
source_url: https://arxiv.org/abs/2407.19893
tags:
- data
- class
- unseen
- classes
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of zero-shot learning for IoT
  sensing tasks using foundation models (FMs), which typically fail to recognize unseen
  classes different from training. The authors propose a method that aligns IoT data
  embeddings with semantic embeddings generated by an FM's text encoder for zero-shot
  IoT sensing.
---

# Leveraging Foundation Models for Zero-Shot IoT Sensing

## Quick Facts
- arXiv ID: 2407.19893
- Source URL: https://arxiv.org/abs/2407.19893
- Reference count: 36
- Key outcome: Achieves 73.0%–89.6% open-set detection F1 and 47.6%–62.1% GZSL harmonic mean

## Executive Summary
This paper tackles the challenge of zero-shot learning for IoT sensing tasks where foundation models struggle to recognize unseen classes. The authors propose aligning IoT data embeddings with semantic embeddings from a foundation model's text encoder using cross-attention fusion of learnable soft prompts and auxiliary hard prompts. Data augmentation via conditional GAN synthesizes unseen class IoT data to fine-tune feature extractors and reduce bias toward seen classes. Evaluated on mmWave, IMU, and Wi-Fi datasets, the approach shows strong open-set detection and generalized zero-shot learning performance compared to baselines.

## Method Summary
The method aligns IoT data embeddings with semantic embeddings generated by a frozen text encoder (CLIP) for zero-shot IoT sensing. It employs cross-attention to combine a learnable soft prompt optimized on training data and an auxiliary hard prompt encoding domain knowledge of the IoT sensing task. Supervised contrastive learning aligns IoT and text embeddings, while conditional GAN synthesizes unseen class IoT samples for fine-tuning the feature extractor and embedding projector, addressing bias toward seen classes.

## Key Results
- Open-set detection F1 scores: 73.0% to 89.6%
- Generalized zero-shot learning harmonic mean scores: 47.6% to 62.1%
- Superior performance compared to various baselines on mmWave, IMU, and Wi-Fi datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention fusion of learnable soft prompts and auxiliary hard prompts creates task-relevant semantic embeddings that better align with IoT signal patterns.
- Mechanism: The soft prompt (learnable vector) adapts to the specific distribution of IoT data embeddings through supervised contrastive learning, while the hard prompt (text from LLM) injects domain physics knowledge. Cross-attention uses the hard prompt as keys and soft prompt as query/value, allowing the model to weight and combine relevant semantic context dynamically.
- Core assumption: The physics principles governing IoT sensor signal generation can be encoded as textual descriptions that, when fused with learned prompt context, produce more discriminative class prototypes than either prompt alone.
- Evidence anchors:
  - [abstract]: "we employ cross-attention to combine a learnable soft prompt optimized on training data and an auxiliary hard prompt that encodes domain knowledge of the IoT sensing task"
  - [section 4.1]: "To leverage the physics principles governing the generation of the IoT sensor signals, we further use a hard prompt to give auxiliary class-specific information...we use cross-attention to fuse the soft and hard prompts"
  - [corpus]: Weak evidence; related papers focus on visual-semantic correlation but do not detail cross-attention prompt fusion for IoT signals.
- Break condition: If the hard prompt generation by LLM fails to capture task-relevant physics, or if the soft prompt optimization overfits to seen classes, the alignment between IoT and semantic embeddings will degrade, harming ZSL performance.

### Mechanism 2
- Claim: Supervised contrastive learning with aligned IoT and text embeddings reduces the bias toward seen classes by creating more generalizable representations.
- Mechanism: During training, positive pairs include (IoT sample, its class prototype) and (IoT samples of same class). Negative pairs include cross-class samples and different prototypes. The contrastive loss pulls embeddings of same class together while pushing apart different classes, improving discrimination in the semantic space.
- Core assumption: The supervised contrastive loss can effectively learn to map IoT features into a semantic space where distance reflects class similarity, enabling unseen classes to be positioned meaningfully relative to seen classes.
- Evidence anchors:
  - [abstract]: "we use supervised contrastive learning to align the class prototypes and IoT embeddings"
  - [section 4.3]: "the loss pulls together embeddings of positive pairs while pushing away the embeddings of negative pairs"
  - [corpus]: Related works cite contrastive learning for zero-shot learning but do not detail its application to IoT modalities specifically.
- Break condition: If the embedding projector or feature extractor cannot map IoT signals into a space compatible with text embeddings, or if the temperature parameter τ is poorly chosen, the alignment will be weak and ZSL accuracy will drop.

### Mechanism 3
- Claim: Data augmentation via conditional GAN synthesis of unseen class IoT samples mitigates the bias toward seen classes by providing synthetic training data for fine-tuning the feature extractor and projector.
- Mechanism: A conditional GAN is trained to generate IoT data samples conditioned on unseen class prototypes. These synthetic samples are used to fine-tune the feature extractor and projector, effectively exposing the model to unseen class distributions during training.
- Core assumption: The GAN can generate realistic IoT samples that preserve the underlying signal characteristics of unseen classes, allowing the feature extractor to learn representations that generalize beyond the seen class training set.
- Evidence anchors:
  - [abstract]: "we propose using data augmentation to synthesize unseen class IoT data for fine-tuning the IoT feature extractor and embedding projector"
  - [section 4.3]: "we propose to train a generative model under the Generative Adversarial Network (GAN) setting to synthesize data samples of unseen classes"
  - [corpus]: Weak evidence; the corpus mentions generative zero-shot learning but not GAN-based augmentation specifically for IoT modalities.
- Break condition: If the GAN generator fails to capture the distribution of unseen classes (e.g., mode collapse, unrealistic samples), fine-tuning on synthetic data may harm or not improve generalization, and bias toward seen classes will persist.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: It creates discriminative embeddings by pulling together similar samples and pushing apart dissimilar ones, which is essential for aligning IoT and semantic embeddings in ZSL.
  - Quick check question: Can you explain why supervised contrastive loss uses both (IoT sample, prototype) and (IoT sample, IoT sample of same class) as positive pairs?

- Concept: Prompt engineering in vision-language models
  - Why needed here: Standard fixed prompts ("a photo of {class}") do not adapt well to downstream tasks; learnable soft prompts allow the model to adjust context for better alignment with IoT data embeddings.
  - Quick check question: What is the difference between a hard prompt and a soft prompt in this context?

- Concept: Open-set detection
  - Why needed here: It distinguishes seen from unseen class samples before classification, preventing the model from misclassifying unseen data as seen due to bias.
  - Quick check question: How does the distance threshold λi in open-set detection relate to the clustering of IoT embeddings?

## Architecture Onboarding

- Component map:
  - Text encoder (CLIP frozen) -> Cross-attention module -> IoT feature extractor -> IoT embedding projector -> Open-set detector -> Specialist DNN / Foundation model

- Critical path:
  1. Extract IoT embeddings from input
  2. Compute distances to seen class clusters for open-set detection
  3. If seen → specialist DNN classification
  4. If unseen → upload to cloud, compute similarity to unseen prototypes, assign label

- Design tradeoffs:
  - Using frozen CLIP text encoder avoids costly retraining but limits prompt flexibility; learnable soft prompts mitigate this.
  - Open-set detection adds a step but reduces bias and improves GZSL accuracy.
  - GAN-based data augmentation requires additional training but helps generalize to unseen classes; risk of unrealistic samples.

- Failure signatures:
  - High false positive in open-set detection → unseen samples misclassified as seen, bias not reduced
  - Poor alignment in t-SNE visualization → contrastive loss not effective
  - GAN mode collapse → synthetic data unrealistic, fine-tuning ineffective

- First 3 experiments:
  1. Test open-set detection accuracy on validation set before and after contrastive training.
  2. Visualize t-SNE of IoT and text embeddings after training to check alignment.
  3. Compare GZSL harmonic mean with and without data augmentation on a small dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of temperature parameter τ in the supervised contrastive loss affect the alignment of IoT and text embeddings?
- Basis in paper: [explicit] The paper states that the temperature parameter τ is set to 0.2, but does not explore its impact on performance.
- Why unresolved: The paper does not investigate the sensitivity of the model to the temperature parameter, which could significantly influence the alignment quality.
- What evidence would resolve it: A comprehensive ablation study varying τ across a range of values and evaluating the impact on open-set detection and GZSL performance would clarify its importance.

### Open Question 2
- Question: How robust is the approach to different IoT sensing modalities beyond mmWave, IMU, and Wi-Fi?
- Basis in paper: [inferred] The paper evaluates the approach on three specific modalities but does not explore its generalizability to other IoT sensors like temperature, humidity, or pressure sensors.
- Why unresolved: The paper focuses on a limited set of modalities, leaving open the question of whether the approach can be effectively extended to other sensor types with different data characteristics.
- What evidence would resolve it: Evaluating the approach on diverse IoT datasets with different sensing modalities and comparing performance to modality-specific baselines would demonstrate its generalizability.

### Open Question 3
- Question: What is the impact of the auxiliary hard prompt's domain knowledge on the model's performance?
- Basis in paper: [explicit] The paper describes the use of an auxiliary hard prompt generated by GPT-3.5 to encode domain knowledge, but does not provide a detailed analysis of its contribution.
- Why unresolved: While the paper demonstrates the overall effectiveness of the approach, it does not isolate the impact of the auxiliary hard prompt from the learnable soft prompt and other components.
- What evidence would resolve it: Conducting an ablation study where the auxiliary hard prompt is removed or replaced with a generic prompt would quantify its contribution to the model's performance.

## Limitations
- Key details of IoT feature extractor and embedding projector architectures are not specified.
- Effectiveness of GAN-based data augmentation depends on generating realistic unseen-class samples, but details are lacking.
- Performance claims rely on proprietary datasets and a cloud-based foundation model.

## Confidence

- **High** for the mechanism of cross-attention combining soft and hard prompts to create task-relevant semantic embeddings.
- **Medium** for the effectiveness of supervised contrastive learning in aligning IoT and semantic embeddings, given that the alignment process is not fully detailed.
- **Medium** for the impact of data augmentation in reducing bias, due to lack of specifics on GAN training and sample quality.

## Next Checks

1. **Reproduce open-set detection:** Implement and test open-set detection accuracy on a small, controlled dataset before and after contrastive training to verify improved separation of seen and unseen classes.
2. **Analyze embedding alignment:** Visualize t-SNE embeddings of IoT and semantic prototypes after training to confirm alignment and check for clustering by class.
3. **Validate data augmentation:** Generate synthetic unseen-class samples using the proposed GAN approach and assess their realism and diversity compared to real data.