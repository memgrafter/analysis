---
ver: rpa2
title: 'CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models'
arxiv_id: '2407.15886'
source_url: https://arxiv.org/abs/2407.15886
tags:
- try-on
- training
- image
- virtual
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CatVTON proposes a lightweight and efficient diffusion-based virtual
  try-on model that eliminates redundant modules and simplifies preprocessing. It
  achieves this by concatenating garment and person images along spatial dimensions
  as inputs to a simplified denoising UNet, removing extra image encoders, ReferenceNet,
  and text-conditioned modules.
---

# CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models

## Quick Facts
- arXiv ID: 2407.15886
- Source URL: https://arxiv.org/abs/2407.15886
- Reference count: 20
- Virtual try-on diffusion model with 899.06M parameters achieving FID of 5.425 and KID of 0.411 on VITON-HD

## Executive Summary
CatVTON is a lightweight diffusion-based virtual try-on model that simplifies the architecture by concatenating garment and person images along spatial dimensions as inputs to a simplified denoising UNet. This approach eliminates redundant modules like separate image encoders, ReferenceNet, and text-conditioned components. The model achieves parameter-efficient training by only updating self-attention layers (49.57M parameters) while freezing the rest of the pre-trained network. Extensive experiments on public datasets demonstrate superior qualitative and quantitative performance compared to baseline methods, with the added benefit of eliminating the need for pose estimation, human parsing, and captioning during inference.

## Method Summary
CatVTON employs a novel lightweight architecture for virtual try-on that concatenates garment and person images along spatial dimensions as inputs to a simplified UNet. The model removes redundant components including separate image encoders, ReferenceNet, text encoder, and cross-attention modules. For training efficiency, only the self-attention layers (49.57M parameters) are updated while the rest of the pre-trained network remains frozen. The architecture uses VAE encoder-decoder structure for latent space operations and incorporates Classifier-free guidance with DREAM strategy. The model is trained on public datasets (VITON-HD, DressCode, DeepFashion) and demonstrates strong generalization performance in in-the-wild scenarios despite training on only 73K samples.

## Key Results
- Achieves FID of 5.425 and KID of 0.411 on VITON-HD dataset, outperforming baseline methods
- Reduces memory usage by 49%+ compared to other diffusion-based methods through architectural simplification
- Demonstrates strong generalization performance in in-the-wild scenarios despite training on only 73K samples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Spatial concatenation of garment and person images enables a single UNet to process both simultaneously, eliminating the need for separate encoders and ReferenceNet.
- **Mechanism**: The shared latent space from spatial concatenation allows direct interaction between garment and person features without additional feature extraction modules.
- **Core assumption**: Spatial concatenation preserves sufficient detail and feature relationships between garment and person for accurate try-on generation.
- **Evidence anchors**: [abstract] "transfers in-shop or worn garments of arbitrary categories to target individuals by concatenating them along spatial dimensions"; [section] "we concatenate person and garment images along the spatial dimension as inputs to the original denoising UNet"
- **Break condition**: If spatial concatenation causes feature interference or loss of important spatial relationships, the model's ability to generate accurate try-on results would degrade.

### Mechanism 2
- **Claim**: Training only self-attention layers (49.57M parameters) is sufficient for adapting pre-trained diffusion models to virtual try-on tasks.
- **Mechanism**: Self-attention layers provide global receptive field for feature interaction between garment and person representations, leveraging the pre-trained model's robust priors.
- **Core assumption**: Self-attention modules are the most critical components for learning the mapping between garment and person features in virtual try-on.
- **Evidence anchors**: [abstract] "identify self-attention modules as crucial for adapting pre-trained diffusion models to the virtual try-on task"; [section] "we find that self-attention modules with a global receptive field...are the most critical part for try-on task with diffusion models"
- **Break condition**: If the task requires more localized feature processing or if the base model's priors are not well-suited to the try-on domain, training only self-attention may be insufficient.

### Mechanism 3
- **Claim**: Removing text encoders and cross-attention modules improves virtual try-on performance by eliminating unnecessary conditional information.
- **Mechanism**: Text conditions are not essential for image-based try-on tasks, and the model can learn necessary guidance directly from visual inputs.
- **Core assumption**: The visual information from garment and person images contains sufficient information for accurate try-on generation without text guidance.
- **Evidence anchors**: [abstract] "remove the text encoder and the cross-attention modules as text conditions are not essential for try-on"; [section] "we remove the text encoder and cross-attention modules in the UNet to further simplify the network"
- **Break condition**: If the model requires more explicit semantic guidance or if visual inputs alone are insufficient for complex try-on scenarios, removing text conditioning could limit performance.

## Foundational Learning

- **Concept**: Latent Diffusion Models (LDMs) and Variational Autoencoders (VAEs)
  - **Why needed here**: CatVTON relies on latent space representation where VAE encodes images and UNet performs denoising operations
  - **Quick check question**: How does the VAE encoder-decoder structure enable efficient image generation in the latent space?

- **Concept**: Self-attention mechanisms in transformers
  - **Why needed here**: Self-attention provides global feature interaction capability crucial for learning the mapping between garment and person features
  - **Quick check question**: What distinguishes self-attention from cross-attention in terms of feature interaction scope?

- **Concept**: Parameter-efficient fine-tuning strategies
  - **Why needed here**: The paper demonstrates that only 49.57M parameters (self-attention layers) need training to adapt the pre-trained model
  - **Quick check question**: What are the trade-offs between training different subsets of model parameters for task adaptation?

## Architecture Onboarding

- **Component map**: VAE encoder → Spatial concatenation → Simplified UNet (self-attention layers) → VAE decoder
- **Critical path**: VAE encoder → Spatial concatenation → Simplified UNet (self-attention layers) → VAE decoder
- **Design tradeoffs**:
  - Parameter efficiency vs. fine-tuning comprehensiveness: Training only self-attention layers (49.57M) vs. full UNet (815.45M)
  - Simplified inference vs. conditional guidance: No pose estimation or parsing vs. potentially more controlled outputs
  - Spatial concatenation vs. separate encoders: Unified latent space vs. specialized feature extraction
- **Failure signatures**:
  - Loss of garment details or pattern fidelity
  - Incorrect garment sizing or fit on target person
  - Color bleeding or texture misalignment
  - Inability to handle complex poses or occlusions
- **First 3 experiments**:
  1. **Spatial concatenation direction test**: Train with x-axis vs. y-axis concatenation to verify no directional bias
  2. **Trainable module ablation**: Compare performance of training UNet vs. transformers vs. self-attention only
  3. **Text conditioning removal validation**: Test with and without text encoder to confirm visual-only guidance sufficiency

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the self-attention module specifically interact with garment and person features to produce high-quality virtual try-on results when trained with only 49.57M parameters?
  - **Basis in paper**: [explicit] The paper states that self-attention modules are identified as crucial for adapting pre-trained diffusion models to the virtual try-on task, enabling high-quality results with only 49.57M training parameters.
  - **Why unresolved**: While the paper identifies the importance of self-attention, it does not provide a detailed analysis of the specific mechanisms by which self-attention interacts with garment and person features to achieve the observed results.
  - **What evidence would resolve it**: Detailed analysis of attention maps during training, showing how self-attention weights change for garment and person features across different layers and training steps.

- **Open Question 2**: What is the optimal balance between distortion reduction and perceptual quality when adjusting the DREAM hyperparameter λ in virtual try-on applications?
  - **Basis in paper**: [explicit] The paper mentions that λ controls the strength of DREAM and shows a trade-off between reduced distortion (lower SSIM) and perceptual quality (lower LPIPS, KID, and FID), but does not determine the optimal value.
  - **Why unresolved**: The paper only presents empirical results for specific λ values but does not provide a systematic analysis of how to find the optimal balance for different virtual try-on scenarios.
  - **What evidence would resolve it**: A comprehensive study varying λ across different datasets and garment types, measuring both quantitative metrics and conducting user studies to determine perceptual quality thresholds.

- **Open Question 3**: How would CatVTON's performance change if it were trained on larger datasets or fine-tuned for specific garment categories?
  - **Basis in paper**: [inferred] The paper mentions that CatVTON was trained solely on public datasets with 73K samples and demonstrates strong generalization performance, suggesting potential for improvement with more data or specialized training.
  - **Why unresolved**: The paper does not explore the effects of increasing dataset size or conducting category-specific fine-tuning on CatVTON's performance.
  - **What evidence would resolve it**: Experiments comparing CatVTON's performance when trained on datasets of varying sizes (e.g., 100K, 500K, 1M samples) and when fine-tuned separately for different garment categories (upper, lower, dresses).

## Limitations

- The paper lacks direct comparative studies showing spatial concatenation is superior to separate encoders for virtual try-on tasks
- No corpus evidence supports the claim that training only self-attention layers is sufficient for task adaptation
- The removal of text conditioning is asserted without empirical validation against text-conditioned alternatives

## Confidence

- **High confidence**: The architecture design and parameter efficiency claims are well-specified and reproducible
- **Medium confidence**: The performance metrics and qualitative results appear valid, though limited to three public datasets
- **Low confidence**: The mechanism claims about why spatial concatenation and self-attention training work better than alternatives lack supporting evidence

## Next Checks

1. Conduct ablation studies comparing spatial concatenation with separate encoder architectures to validate the claimed efficiency gains
2. Test training different module combinations (UNet, transformers, self-attention) to verify self-attention is indeed the optimal choice for parameter-efficient fine-tuning
3. Evaluate the model with and without text conditioning to empirically confirm that text encoders can be removed without quality degradation