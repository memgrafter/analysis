---
ver: rpa2
title: Sparse Graph Representations for Procedural Instructional Documents
arxiv_id: '2402.03957'
source_url: https://arxiv.org/abs/2402.03957
tags:
- graph
- document
- documents
- concept
- edges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the task of computing document similarity for
  procedural instructional documents, where the sequence of steps is crucial. They
  propose two algorithms, inspired by Supergenome Sorting and Hamiltonian Path, to
  construct sparse directed Joint Concept Interaction Graphs (JCIGs) that encode sequential
  information.
---

# Sparse Graph Representations for Procedural Instructional Documents

## Quick Facts
- arXiv ID: 2402.03957
- Source URL: https://arxiv.org/abs/2402.03957
- Reference count: 26
- Primary result: Directed sparse JCIGs improve accuracy by ~10 points on IFIXIT procedural documents

## Executive Summary
This paper addresses document similarity for procedural instructional documents where step sequence matters. The authors propose sparse directed Joint Concept Interaction Graphs (JCIGs) that encode sequential information by deducing dominant edge directions. Two algorithms—Supergenome Sorting and Hamiltonian Path—sparsify graphs from O(n²) to O(n) edges while preserving step-order signals. Experiments on IFIXIT instruction manuals show 10-point accuracy improvement over baseline JCIG, with comparable performance on news datasets.

## Method Summary
The method constructs sparse directed JCIGs through a pipeline: keyword extraction → community detection → sentence-to-concept assignment → pseudograph construction → dominant direction deduction (SGS/HP) → directed sparse graph → Siamese encoder + GCN + MLP classifier. The key innovation is deducing dominant edge directions from sentence-to-concept flows to capture sequential information while sparsifying the graph structure.

## Key Results
- 10-point accuracy improvement over JCIG baseline on IFIXIT instruction manuals
- Comparable performance to baseline on CNSE and CNSS Chinese news datasets
- Sparse directed graphs reduce edges from O(n²) to O(n) for computational efficiency
- HP algorithm yields O(n) edges vs SGS's O(n²/2) in worst case

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Directed JCIGs improve accuracy by encoding step-order information
- Core assumption: Dominant direction of concept transitions from sentence ordering captures sequential similarity signals
- Evidence: Abstract emphasizes "deducing dominant direction of information flow" as key idea
- Break condition: Direction signal becomes spurious for non-sequential documents

### Mechanism 2
- Claim: Graph sparsification to O(n) edges speeds up GCN without accuracy loss
- Core assumption: Single dominant edge per node pair captures essential sequential structure
- Evidence: Section 5.2 shows HP reduces edges from n² to n
- Break condition: Oversimplification loses discriminative power when many directions are equally strong

### Mechanism 3
- Claim: Siamese encoder + GCN + MLP pipeline effectively learns document-pair similarity
- Core assumption: Joint concept nodes can be meaningfully compared at node level
- Evidence: Section 5.3 describes Siamese encoder computing per-node match vectors
- Break condition: Fine/coarse concept granularity makes match vectors uninformative

## Foundational Learning

- Concept: Community detection on keyword graphs
  - Why needed: Collapses fine-grained keywords into higher-level comparable concept nodes
  - Quick check: What happens if we skip community detection and use raw keywords?

- Concept: Sentence-to-concept assignment using TF-IDF similarity
  - Why needed: Populates concept nodes with representative sentences for Siamese encoder
  - Quick check: How does setting TF-IDF threshold too high affect node representations?

- Concept: Graph sparsification via dominant direction deduction
  - Why needed: Converts dense pseudographs to sparse directed graphs tractable for GCNs
  - Quick check: Why does HP yield O(n) edges while SGS yields O(n²/2)?

## Architecture Onboarding

- Component map: Keyword extraction → Community detection → Sentence assignment → Pseudograph → D3 algorithm → Directed sparse graph → Siamese+GCN classifier
- Critical path: Keyword extraction → Community detection → Sentence assignment → Pseudograph → D3 algorithm → Directed sparse graph → Siamese+GCN classifier
- Design tradeoffs:
  - SGS keeps more edges but is slower; HP is faster but may lose nuance
  - Community detection algorithm choice has negligible impact (ablation shows)
  - Word2Vec vs contextual embeddings: Word2Vec is smaller but less expressive
- Failure signatures:
  - Low accuracy on non-sequential datasets → D3 signal is spurious
  - GCN underfitting → Graph too sparse; try SGS instead of HP
  - Siamese encoder collapse → TF-IDF threshold too high, nodes underpopulated
- First 3 experiments:
  1. Run JCIG baseline vs C-HP on IFIXIT to confirm ~10-point gain
  2. Swap HP → SGS on IFIXIT to check if accuracy drops but training time increases
  3. Test on CNSE/CNSS to verify no performance degradation

## Open Questions the Paper Calls Out

- Question: How does performance vary across different community detection algorithms?
  - Basis: Authors tested multiple algorithms on IFIXIT but didn't explore impact on other datasets
  - Resolution: Experiments with different algorithms on various datasets would provide comprehensive analysis

- Question: Can methods handle multi-step procedures with complex dependencies and branching paths?
  - Basis: Methods focus on sequential flow but don't address complex relationships within procedures
  - Resolution: Testing on datasets with complex procedural dependencies would provide insights

- Question: How does performance compare to transformer-based models on long procedural documents?
  - Basis: Authors note transformers don't scale quadratically but didn't compare performance
  - Resolution: Experiments comparing proposed methods with transformers on long documents would be valuable

## Limitations

- Sparse corpus signals with only 8 neighboring papers and low FMR (0.503) under-validates novelty
- Lacks ablation studies on sentence-to-concept assignment quality impact
- Claims community detection choice "does not affect" results without empirical backing
- HP algorithm's O(n²) → O(n) reduction may oversimplify complex procedural relationships

## Confidence

- High confidence: Directed JCIGs outperform undirected JCIGs on procedural IFIXIT documents (~10-point accuracy gain)
- Medium confidence: Proposed methods perform comparably to baseline on non-sequential CNSE/CNSS datasets
- Low confidence: Community detection algorithm choice has negligible impact; SGS vs HP tradeoff is purely computational

## Next Checks

1. Conduct ablation study varying TF-IDF similarity thresholds to quantify impact on node representation quality and downstream accuracy
2. Test HP and SGS variants on holdout IFIXIT subset with artificially reordered steps to measure sensitivity to sequential signal corruption
3. Compare Word2Vec-based embeddings against contextual embeddings (e.g., Sentence-BERT) on small subset to assess whether richer embeddings justify computational cost