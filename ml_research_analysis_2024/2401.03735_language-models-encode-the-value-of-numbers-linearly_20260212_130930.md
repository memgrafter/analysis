---
ver: rpa2
title: Language Models Encode the Value of Numbers Linearly
arxiv_id: '2401.03735'
source_url: https://arxiv.org/abs/2401.03735
tags:
- number
- language
- layer
- probes
- numbers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether and how large language models (LLMs)
  encode the value of numbers, a fundamental element in mathematical reasoning. The
  authors construct a synthetic dataset of addition problems and use linear probes
  to extract number values from the hidden states of LLaMA-2 and Mistral-7B models.
---

# Language Models Encode the Value of Numbers Linearly

## Quick Facts
- arXiv ID: 2401.03735
- Source URL: https://arxiv.org/abs/2401.03735
- Reference count: 40
- Key finding: LLMs encode numerical values linearly with high correlation (ρ > 0.99) across all layers

## Executive Summary
This paper investigates whether and how large language models encode numerical values, a fundamental component of mathematical reasoning. The authors construct a synthetic dataset of addition problems and use linear probes to extract number values from hidden states of LLaMA-2 and Mistral-7B models. Their experiments demonstrate that LLMs encode number values linearly, with probes achieving high Pearson correlation and out-of-sample R² across all layers. The encoding process begins in early layers and reaches optimal precision in intermediate layers. Through intervention experiments, the authors show that LLMs utilize these encoded values for calculations, establishing a causal relationship between encoded values and model outputs.

## Method Summary
The authors create a synthetic dataset of single-digit addition problems and analyze the hidden states of LLaMA-2 and Mistral-7B models. They employ linear probes to extract numerical values from model representations, measuring Pearson correlation and R² metrics across layers. To establish causality, they conduct intervention experiments using activation patching and linear vector additions to manipulate model outputs. The study focuses on two specific model architectures and a controlled addition task to isolate the encoding of numerical values.

## Key Results
- Linear probes achieve Pearson correlation ρ > 0.99 for number value extraction across all layers
- Out-of-sample R² performance exceeds 0.97, demonstrating robust encoding
- Number value encoding begins in early layers and achieves highest precision in intermediate layers
- Intervention experiments show causal relationship between encoded values and model calculations

## Why This Works (Mechanism)
The linear encoding of numerical values emerges from the model's need to represent and manipulate numbers for arithmetic operations. During training, the transformer architecture learns to map numerical inputs to continuous vector representations that preserve ordinal relationships. The self-attention mechanism and feed-forward layers collectively transform these representations while maintaining linear separability of number values. This encoding allows the model to perform calculations by operating on these vector representations rather than raw numerical inputs.

## Foundational Learning
- **Linear probes**: Simple classifiers that map hidden states to target values; needed to test if numerical information is linearly accessible in model representations
- **Activation patching**: Method to transfer activations between model runs; used to test causal relationships between representations and outputs
- **Pearson correlation**: Statistical measure of linear relationship; employed to quantify how well extracted values match true numbers
- **R² score**: Coefficient of determination; measures predictive performance of extracted representations
- **Transformer architecture**: Standard neural network structure; provides the framework where numerical encoding emerges
- **Synthetic dataset**: Controlled test data; isolates numerical reasoning from language understanding

## Architecture Onboarding
**Component map**: Input tokens -> Embedding layer -> Transformer blocks (LayerNorm, Self-Attention, Feed-Forward) -> Linear probe -> Number value extraction
**Critical path**: Token embedding → Early layers (initial encoding) → Intermediate layers (optimal encoding) → Output layer
**Design tradeoffs**: The study uses synthetic addition problems for controlled analysis, sacrificing real-world complexity for experimental clarity
**Failure signatures**: Poor correlation in linear probes would indicate non-linear or absent numerical encoding
**First experiments**: 1) Test linear probe performance on different arithmetic operations 2) Analyze encoding across model scales 3) Examine encoding when models are fine-tuned on numerical tasks

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Analysis limited to LLaMA-2 and Mistral-7B architectures only
- Experiments focus exclusively on single-digit addition problems
- Synthetic dataset may not capture complexity of real-world numerical reasoning
- Does not address how models handle multi-digit or complex arithmetic

## Confidence
High for the specific models and tasks studied; Medium for broader claims about numerical reasoning in LLMs generally.

## Next Checks
1. Replicate experiments with additional mathematical operations (subtraction, multiplication, division) and larger numbers
2. Apply methodology to larger models (GPT-4, Claude) and different architectures to assess universality
3. Conduct ablation studies to determine critical components of number representation