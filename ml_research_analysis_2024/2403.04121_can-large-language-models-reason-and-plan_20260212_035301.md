---
ver: rpa2
title: Can Large Language Models Reason and Plan?
arxiv_id: '2403.04121'
source_url: https://arxiv.org/abs/2403.04121
tags:
- llms
- planning
- reasoning
- knowledge
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) have been shown to struggle with planning
  and reasoning tasks despite their impressive language generation capabilities. Experiments
  demonstrate that while LLMs can extract relevant knowledge from their training data,
  they cannot autonomously generate executable plans or perform sound reasoning.
---

# Can Large Language Models Reason and Plan?
## Quick Facts
- arXiv ID: 2403.04121
- Source URL: https://arxiv.org/abs/2403.04121
- Reference count: 13
- LLMs struggle with autonomous planning and reasoning despite impressive language generation capabilities

## Executive Summary
Large language models have demonstrated remarkable capabilities in language generation but face significant limitations when tasked with planning and reasoning. Experiments across multiple planning benchmarks reveal that while LLMs can effectively retrieve relevant knowledge from their training data, they fundamentally lack the ability to autonomously generate executable plans or perform sound reasoning. Performance metrics show success rates as low as 30% in planning domains, with results deteriorating further when problems are presented with obfuscated action and object names.

The study concludes that LLMs should be viewed as approximate knowledge sources rather than autonomous planners. Their strengths lie in pattern-based retrieval and knowledge extraction, but they cannot guarantee correctness in planning tasks that require systematic search and verification. The research advocates for LLM-Modulo frameworks that combine LLMs with external verifiers or human experts to leverage their knowledge retrieval capabilities while ensuring soundness.

## Method Summary
The research evaluated GPT3, GPT3.5, and GPT4 models on planning benchmarks derived from the International Planning Competition (IPC), including the Blocks World domain. The methodology involved testing LLMs on both original planning problems and obfuscated versions where action and object names were replaced. Performance was measured by success rates in generating executable plans. The study also explored LLM-Modulo frameworks where external plan verifiers were used to check LLM-generated solutions. Human-in-the-loop iterative prompting was examined as a potential enhancement approach, though concerns about the Clever Hans effect were noted.

## Key Results
- GPT4 achieved only 30% empirical accuracy in the Blocks World domain, with performance dropping further when action and object names were obfuscated
- LLMs demonstrated capability to extract relevant knowledge from training data but failed to autonomously generate executable plans
- LLM-Modulo frameworks with external verifiers showed promise in leveraging LLM knowledge while ensuring soundness
- Human-in-the-loop iterative prompting raised concerns about Clever Hans effects, where human guidance may be misattributed to LLM capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail at autonomous planning because they rely on pattern-based approximate retrieval rather than principled reasoning.
- Mechanism: LLMs are trained as n-gram models that probabilistically reconstruct completions word-by-word. This makes them excellent at retrieving approximate patterns from training data, but unable to perform the systematic search and verification required for sound planning.
- Core assumption: Planning requires verifiable correctness guarantees that n-gram-based retrieval cannot provide.
- Evidence anchors:
  - [abstract] "LLMs showed low success rates (e.g., 30% in the Blocks World domain), which dropped further when action and object names were obfuscated"
  - [section] "Nothing in the training and use of LLMs would seem to suggest remotely that they can do any type of principled reasoning"
  - [corpus] Weak - corpus papers focus on self-reflection capabilities but don't directly address the planning failure mechanism
- Break condition: If an LLM could be trained or augmented with a verifier that guarantees correctness of generated plans, this mechanism would break down.

### Mechanism 2
- Claim: LLM performance improvements from GPT-3 to GPT-4 come from better approximate retrieval, not genuine reasoning capability.
- Mechanism: Larger training corpora enable LLMs to retrieve more relevant patterns, creating the illusion of improved reasoning when it's actually better pattern matching.
- Core assumption: The performance gain is due to memorization of more planning examples rather than acquisition of reasoning ability.
- Evidence anchors:
  - [abstract] "GPT4 reaching 30% empirical accuracy in the Blocks World (albeit still lower in other domains)"
  - [section] "One way of checking this for planning tasks is to reduce the effectiveness of approximate retrieval by obfuscating the names of the actions and objects"
  - [corpus] Weak - corpus papers discuss self-critiquing but don't test obfuscated planning problems
- Break condition: If performance remained high even with obfuscated problem names, this mechanism would be invalid.

### Mechanism 3
- Claim: Human-in-the-loop iterative prompting creates a Clever Hans effect where humans unknowingly guide LLM outputs.
- Mechanism: Humans provide subtle feedback through iterative prompting that steers LLMs toward correct answers, making it appear the LLM is reasoning when it's actually being guided.
- Core assumption: Humans can unconsciously bias LLM outputs through prompting patterns they're not aware of.
- Evidence anchors:
  - [abstract] "The problem with this is that it is highly susceptible to the Clever Hans effect"
  - [section] "The credit and blame for the ensuing accuracy, if any, falls squarely on the human in the loop"
  - [corpus] Weak - corpus papers mention self-reflection but don't specifically address the Clever Hans effect in iterative prompting
- Break condition: If experiments could isolate human guidance effects and show LLMs perform equally well without human input, this mechanism would break down.

## Foundational Learning

- Concept: N-gram language models
  - Why needed here: Understanding that LLMs are fundamentally probability-based text completion systems, not reasoning engines
  - Quick check question: What is the maximum context length that a 300-gram model like GPT-3.5 can consider when predicting the next token?

- Concept: Approximate vs. exact retrieval
  - Why needed here: Distinguishing between LLM's probabilistic pattern matching and database-style exact lookup
  - Quick check question: If an LLM has seen "A→B→C" and "A→X→C" in training, what might it generate for the sequence "A→_→C"?

- Concept: Planning domain knowledge vs. reasoning capability
  - Why needed here: Separating the knowledge acquisition problem from the reasoning problem in planning tasks
  - Quick check question: In Blocks World planning, what are the two key components that must be assembled to create an executable plan?

## Architecture Onboarding

- Component map: LLM (approximate knowledge retriever) → External verifier (sound checker) → Human-in-the-loop (when verifier unavailable)
- Critical path: Problem input → LLM approximate retrieval → External verifier check → (If failed) Human refinement → Final solution
- Design tradeoffs: Using LLMs for idea generation sacrifices correctness guarantees but gains speed and breadth of knowledge; adding verifiers restores correctness but adds latency
- Failure signatures: Plans that look reasonable but fail execution, performance drops with obfuscated inputs, human-guided improvements that don't generalize
- First 3 experiments:
  1. Test GPT-4 on Blocks World with and without obfuscated action/object names to verify approximate retrieval mechanism
  2. Implement LLM-Modulo framework with an external planner as verifier and measure performance improvement
  3. Conduct blind human evaluation of LLM-generated plans to detect Clever Hans effects in iterative prompting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which LLMs generate their responses, and to what extent does this process resemble principled reasoning versus approximate retrieval?
- Basis in paper: [explicit] The paper explicitly states that LLMs are "essentially n-gram models on steroids" that perform "approximate retrieval" rather than principled reasoning.
- Why unresolved: The paper argues that LLMs' responses are based on pattern matching and probability distributions rather than logical deduction or search algorithms. However, the precise internal mechanisms of LLMs remain complex and not fully transparent.
- What evidence would resolve it: Detailed analysis of LLM architecture and training processes, along with controlled experiments comparing LLM responses to those generated by systems using explicit reasoning algorithms.

### Open Question 2
- Question: Can LLMs be effectively fine-tuned or augmented to perform planning and reasoning tasks with guaranteed correctness?
- Basis in paper: [explicit] The paper discusses fine-tuning and iterative prompting as potential approaches to improve LLM performance, but concludes that these methods primarily convert planning into memory-based retrieval rather than enabling genuine reasoning.
- Why unresolved: While the paper suggests that fine-tuning doesn't enable true reasoning, it acknowledges that performance improvements have been observed. The potential for developing more effective fine-tuning or augmentation techniques remains an open question.
- What evidence would resolve it: Comparative studies of fine-tuned LLMs versus traditional planning systems on benchmark tasks, with rigorous evaluation of correctness and efficiency.

### Open Question 3
- Question: How can LLM-Modulo frameworks be optimized to balance the idea generation capabilities of LLMs with the soundness guarantees of external verifiers?
- Basis in paper: [explicit] The paper advocates for LLM-Modulo frameworks as a principled method for leveraging LLMs in planning and reasoning tasks, but notes that optimizing these frameworks is an open challenge.
- Why unresolved: While the paper presents LLM-Modulo as a promising approach, it does not provide specific guidance on how to design and implement these frameworks for maximum effectiveness across different domains and task types.
- What evidence would resolve it: Empirical studies comparing various LLM-Modulo architectures and integration strategies, along with theoretical analysis of their properties and limitations.

## Limitations

- Study conclusions are primarily based on experiments with specific planning benchmarks and obfuscated problems, which may not generalize to all reasoning tasks
- The Clever Hans effect in human-in-the-loop prompting is identified but not directly quantified in terms of its impact on performance measurements
- The research does not explore whether specialized fine-tuning or training for planning tasks might yield different results

## Confidence

- High confidence: LLMs cannot autonomously generate executable plans or perform sound reasoning in planning tasks (supported by empirical success rates of 30% or lower)
- Medium confidence: Performance improvements from GPT-3 to GPT-4 come from better approximate retrieval rather than genuine reasoning capability (inferred from obfuscated input results but not directly tested)
- Medium confidence: Human-in-the-loop iterative prompting creates Clever Hans effects (theoretically sound but requires direct measurement)

## Next Checks

1. **Quantify Human Guidance Effects**: Conduct a controlled experiment where the same planning problems are solved with identical prompts by both LLMs and human planners, then compare performance to isolate the Clever Hans effect magnitude.

2. **Test Transfer Learning Potential**: Fine-tune LLMs on planning-specific datasets and re-evaluate performance on both standard and obfuscated planning benchmarks to determine if reasoning capabilities can be acquired through specialized training.

3. **Evaluate Other Reasoning Domains**: Test LLMs on mathematical proof generation and logical deduction tasks with obfuscated notation to determine whether the approximate retrieval mechanism limitation generalizes beyond planning tasks.