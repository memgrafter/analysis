---
ver: rpa2
title: Unified Bayesian representation for high-dimensional multi-modal biomedical
  data for small-sample classification
arxiv_id: '2411.07043'
source_url: https://arxiv.org/abs/2411.07043
tags:
- data
- where
- will
- page
- baldur
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BALDUR, a Bayesian algorithm designed to
  handle multi-modal biomedical data with small sample sizes in high-dimensional settings
  while providing explainable solutions. BALDUR combines different data views into
  a common latent space, extracting relevant information for classification while
  pruning irrelevant features.
---

# Unified Bayesian representation for high-dimensional multi-modal biomedical data for small-sample classification

## Quick Facts
- arXiv ID: 2411.07043
- Source URL: https://arxiv.org/abs/2411.07043
- Authors: Albert Belenguer-Llorens; Carlos Sevilla-Salcedo; Jussi Tohka; Vanessa Gómez-Verdejo
- Reference count: 40
- Primary result: Introduces BALDUR, a Bayesian algorithm for multi-modal biomedical data classification with small sample sizes, achieving 68-78% accuracy on neurodegeneration datasets while providing explainable biomarker identification.

## Executive Summary
BALDUR is a Bayesian algorithm designed to handle multi-modal biomedical data with small sample sizes in high-dimensional settings while providing explainable solutions. The model combines different data views into a common latent space, extracting relevant information for classification while pruning irrelevant features. BALDUR integrates dual kernels over wide views to enhance generalization and avoid overfitting in small sample scenarios. The model's linear formulation ensures explainability, enabling biomarker identification in Parkinson's and Alzheimer's disease datasets.

## Method Summary
BALDUR uses variational inference to approximate posterior distributions over model parameters in a Bayesian framework. The method combines multi-view data through latent space projection using either dual (for wide views) or primal (for normal views) kernels. Automatic Relevance Determination (ARD) imposes sparsity over both features and latent factors, jointly pruning irrelevant features and data views. The model employs a linear Bayesian logistic regression formulation for classification, ensuring interpretability of learned feature weights as biomarkers. Training involves iterative mean-field updates of variational parameters, with dual representation reducing computational complexity when feature dimensions greatly exceed sample sizes.

## Key Results
- On BioFIND (Parkinson's disease): 68% accuracy, 70% balanced accuracy, 73% AUC, outperforming state-of-the-art models while selecting only 9.5×10⁻⁴% of features
- On ADNI (Alzheimer's disease): 78% accuracy, 80% balanced accuracy, 85% AUC, identifying brain regions associated with cognitive impairment
- Identified sleep-related biomarkers aligned with scientific literature and highlighted hippocampus, amygdala, and ventricles as relevant regions

## Why This Works (Mechanism)

### Mechanism 1
Dual kernel integration prevents overfitting in small-sample high-dimensional scenarios by replacing full feature space representations with a subset of relevance vectors. When s(m) = 1, the model replaces the D(m) x K weight matrix with a ˜N x K dual representation A(m), where ˜N << D(m). This drastically reduces the number of parameters to be learned, lowering model complexity and avoiding overfitting. The core assumption is that the subset of relevance vectors ˜X(m) sufficiently captures the discriminative information of the full feature space.

### Mechanism 2
Iterative feature selection via Automatic Relevance Determination (ARD) jointly prunes irrelevant features and data views while learning the latent representation. ARD priors over both feature weights γ(m)_d and latent factors δ(m)_k impose sparsity. During variational inference, low-precision hyperparameters drive corresponding weights toward zero, effectively removing irrelevant/redundant features and entire data views from the latent space. The core assumption is that the sparsity-inducing priors accurately reflect the true underlying structure where only a subset of features and views are relevant.

### Mechanism 3
Linear formulation ensures explainability, enabling biomarker identification by directly interpreting feature weights. Since the latent representation is a linear combination of input features (z = XW), and classification is performed via a linear Bayesian logistic regression, the magnitude and sign of the learned weights W directly indicate the contribution of each feature to the classification decision. The core assumption is that linear relationships adequately capture the discriminative patterns in the biomedical data.

## Foundational Learning

- Concept: Variational Inference
  - Why needed here: The posterior distributions over model parameters are analytically intractable due to the logistic likelihood and hierarchical Bayesian structure. Variational inference approximates these posteriors with simpler, factorized distributions, enabling efficient learning.
  - Quick check question: Why can't we compute the posterior exactly in this Bayesian model? What does the mean-field assumption mean in this context?

- Concept: Automatic Relevance Determination (ARD)
  - Why needed here: In high-dimensional biomedical data with many irrelevant features, ARD helps identify and suppress noise by assigning low precision (high variance) to irrelevant weights, driving them toward zero.
  - Quick check question: How does ARD differ from simple ℓ1 regularization? What role do the hyperparameters α and β play in the Gamma priors for ARD?

- Concept: Dual Representation in Kernel Methods
  - Why needed here: When the number of features D(m) greatly exceeds the number of samples N, working directly in the primal space becomes computationally prohibitive. The dual representation allows the model to operate in a reduced space defined by relevance vectors, making the problem tractable.
  - Quick check question: What is the computational complexity difference between primal and dual formulations when D(m) >> N? How does the choice of relevance vectors affect the solution?

## Architecture Onboarding

- Component map: Input -> Preprocess/Organize Multi-view Data -> BALDUR Model (Latent Space Projection + ARD + Variational Inference) -> Classification Probabilities + Feature Weights -> Evaluation + Biomarker Extraction
- Critical path: 1. Preprocess and organize multi-view data 2. Choose kernel representation (dual for wide views, primal for normal views) 3. Initialize variational parameters 4. Run iterative mean-field updates for all model variables 5. Compute predictions using posterior predictive distribution 6. Evaluate performance and extract feature importance
- Design tradeoffs: Dual vs. primal representation (reduced parameters vs. relevance vector selection complexity), sparsity level (interpretability vs. information retention), number of latent factors K (model capacity vs. overfitting risk)
- Failure signatures: Poor performance with high variance across folds (overfitting or insufficient regularization), all features pruned or entire views removed (too aggressive sparsity priors), slow convergence during variational inference (initialization or numerical stability issues)
- First 3 experiments: 1. Run BALDUR on synthetic dataset with known sparse structure to verify feature recovery 2. Compare BALDUR's performance and biomarker selection on BioFIND with baseline linear SVM with ℓ1 penalty 3. Test BALDUR's behavior when forcing all views into dual representation versus mixed primal/dual setup

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BALDUR's performance scale with increasing sample sizes in multi-modal biomedical datasets?
- Basis in paper: The paper emphasizes BALDUR's effectiveness in small-sample scenarios but does not explore its behavior with larger datasets.
- Why unresolved: The paper focuses on validating BALDUR in small-sample settings, leaving its scalability unexplored.
- What evidence would resolve it: Experiments comparing BALDUR's performance across datasets with varying sample sizes, particularly in large-scale multi-modal biomedical data.

### Open Question 2
- Question: Can BALDUR's feature selection process be adapted to prioritize interpretability over predictive accuracy in clinical settings?
- Basis in paper: The paper highlights BALDUR's explainability and biomarker identification capabilities but does not discuss trade-offs between interpretability and accuracy.
- Why unresolved: While BALDUR identifies biomarkers, the paper does not address how to balance interpretability with predictive performance.
- What evidence would resolve it: Studies comparing BALDUR's feature selection outcomes when prioritizing interpretability versus accuracy, with clinician feedback on usability.

### Open Question 3
- Question: How does BALDUR handle missing or incomplete data in multi-modal biomedical datasets?
- Basis in paper: The paper does not mention how BALDUR deals with missing data, which is a common issue in biomedical datasets.
- Why unresolved: Missing data is a critical challenge in biomedical research, but the paper does not address this aspect of BALDUR's functionality.
- What evidence would resolve it: Experiments testing BALDUR's performance on datasets with varying degrees of missing data, and comparisons with other models designed to handle incomplete data.

## Limitations
- Weak corpus coverage for dual kernel integration and ARD mechanisms raises concerns about novelty and generalizability
- Linear formulation may limit performance on datasets with complex nonlinear decision boundaries
- Hyperparameter tuning process is not fully detailed, and specific relevance vector selection strategy for dual kernels is not described

## Confidence
- **High confidence**: Classification performance metrics (accuracy, balanced accuracy, AUC) on both BioFIND and ADNI datasets
- **Medium confidence**: Biomarker identification claims, supported by literature alignment but limited biological validation
- **Low confidence**: Theoretical mechanisms (dual kernel integration preventing overfitting, ARD pruning irrelevant features) due to weak corpus coverage and lack of detailed ablation studies

## Next Checks
1. Conduct ablation studies comparing BALDUR with and without dual kernel integration on BioFIND genetic data to quantify overfitting reduction
2. Perform feature importance stability analysis across cross-validation folds to assess the reliability of biomarker identification
3. Test BALDUR on a synthetic dataset with known nonlinear structure to evaluate the limitations of the linear formulation