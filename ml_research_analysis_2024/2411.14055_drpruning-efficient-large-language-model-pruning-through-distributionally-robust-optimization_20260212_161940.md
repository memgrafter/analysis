---
ver: rpa2
title: 'DRPruning: Efficient Large Language Model Pruning through Distributionally
  Robust Optimization'
arxiv_id: '2411.14055'
source_url: https://arxiv.org/abs/2411.14055
tags:
- data
- pruning
- training
- loss
- drpruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DRPruning addresses uneven domain performance degradation in large
  language model (LLM) structured pruning by dynamically adjusting data distribution
  during training using distributionally robust optimization (DRO). It predicts reference
  losses via scaling laws and gradually shifts reference data ratios toward underperforming
  domains, ensuring robustness across heterogeneous and multi-domain data.
---

# DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization

## Quick Facts
- arXiv ID: 2411.14055
- Source URL: https://arxiv.org/abs/2411.14055
- Authors: Hexuan Deng; Wenxiang Jiao; Xuebo Liu; Jing Li; Min Zhang; Zhaopeng Tu
- Reference count: 40
- Key outcome: DRPruning achieves 5.59% lower perplexity, 1.52% better downstream task accuracy, and 55.4% win rate in instruction tuning compared to baselines

## Executive Summary
DRPruning addresses the challenge of uneven domain performance degradation in large language model (LLM) structured pruning through distributionally robust optimization (DRO). The method dynamically adjusts data distribution during training by predicting reference losses using scaling laws and gradually shifting focus toward underperforming domains. Experiments demonstrate consistent improvements across monolingual and multilingual settings, with robust performance under distribution shifts and computational efficiency.

## Method Summary
DRPruning combines structured pruning with DRO-based dynamic data scheduling. The method uses â„“0 regularization with hard concrete distributions and Lagrange multipliers to achieve target model sizes, then applies DRO with reference loss predictions from scaling laws to guide continued pretraining. The dynamic adjustment mechanism updates reference data ratios based on domain-specific performance deviations, ensuring balanced capability recovery across heterogeneous data distributions.

## Key Results
- 5.59% lower perplexity compared to baseline pruned models
- 1.52% improvement in downstream task accuracy across 15 benchmarks
- 55.4% win rate in instruction tuning over ReSheared baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic adjustment of data distribution based on predicted reference loss restores balanced performance across domains.
- Mechanism: After each evaluation, DRPruning predicts the minimum acceptable performance using scaling laws, then sets the reference loss to this prediction. Domains with larger deviations from the reference loss receive higher weights, dynamically increasing focus on underperforming domains.
- Core assumption: The predicted loss at the end of training accurately reflects the minimum acceptable performance for each domain.
- Evidence anchors:
  - [abstract] "Using scaling laws, we predict the loss after training as the reference loss, where larger deviations indicate poorer performance, thereby promoting capability recovery in these areas."
  - [section 3.2] "To address this, we predict the model's loss at the end of training as an estimate of the minimum acceptable performance. Specifically, we leverage scaling laws to capture training dynamics and forecast the loss based on evaluation loss trends."
  - [corpus] Weak - the corpus doesn't directly address this specific mechanism of using scaling laws for reference loss prediction.

### Mechanism 2
- Claim: Gradual adjustment of reference data ratios toward higher-loss domains ensures robustness to a wider range of distributions.
- Mechanism: DRPruning updates the reference data ratio by blending the current DRO weights with the previous reference ratio using a smoothing factor. This gradually shifts more weight to domains with higher losses while constraining the ratio to prevent degenerate cases.
- Core assumption: A gradual shift in reference data ratios will maintain training stability while improving robustness to distribution shifts.
- Evidence anchors:
  - [abstract] "We gradually increase the reference data ratio for domains with greater deviations, ensuring robustness across a wider range of distributions, particularly more challenging ones."
  - [section 3.3] "To address this, we propose a method that combines the strengths of the aforementioned approaches. We still employ Eqn. 5 to constrain the distribution within a limited range, while gradually shifting the reference data ratio towards domains with higher losses to improve the model's robustness to more challenging distributions."
  - [corpus] Weak - the corpus doesn't directly address this specific mechanism of gradually adjusting reference data ratios.

### Mechanism 3
- Claim: DRO-based dynamic data scheduling outperforms fixed data scheduling and naive DRO in both pruning and continued pretraining.
- Mechanism: DRPruning applies DRO to dynamically adjust the data ratio during training, focusing more on underperforming domains. This is validated through experiments showing improvements in perplexity, downstream task accuracy, and instruction tuning win rates.
- Core assumption: DRO-based dynamic data scheduling is more effective than fixed scheduling for handling heterogeneous and multi-domain data in LLMs.
- Evidence anchors:
  - [abstract] "Experiments in monolingual and multilingual settings show that DRPruning surpasses similarly sized models in both pruning and continued pretraining over perplexity, downstream tasks, and instruction tuning."
  - [section 4.3] "DRPruning outperforms ReSheared on average across LM benchmarks and instruction tuning. LLMs recovered by our method are better foundation models after continued pretraining."
  - [corpus] Weak - the corpus doesn't directly address this specific mechanism of DRO-based dynamic data scheduling outperforming other methods.

## Foundational Learning

- Concept: Distributionally Robust Optimization (DRO)
  - Why needed here: DRO is used to improve the model's robustness to distribution shifts by optimizing worst-case performance across distributions.
  - Quick check question: How does DRO differ from traditional empirical risk minimization in handling heterogeneous data?

- Concept: Scaling Laws
  - Why needed here: Scaling laws are used to predict the loss at the end of training, which serves as the reference loss for DRO.
  - Quick check question: What are the key parameters in scaling laws that affect the prediction of the minimum acceptable performance?

- Concept: Structured Pruning
  - Why needed here: Structured pruning is the technique used to reduce the model size, which is then followed by continued pretraining to restore capabilities.
  - Quick check question: How does structured pruning differ from unstructured pruning in terms of the components it removes?

## Architecture Onboarding

- Component map:
  Base model (e.g., Llama2-7B) -> Structured pruning component with learned masks -> DRO component with dynamic data ratio adjustment -> Scaling laws component for reference loss prediction -> Evaluation component for monitoring domain-specific performance

- Critical path:
  1. Initialize base model and pruning masks
  2. Apply structured pruning to reach target configuration
  3. Evaluate domain-specific performance and predict reference loss
  4. Update data ratio using DRO based on evaluation results
  5. Continue pretraining with adjusted data distribution
  6. Monitor performance and iterate as needed

- Design tradeoffs:
  - Granularity of pruning vs. performance retention
  - Frequency of DRO updates vs. training stability
  - Accuracy of scaling law predictions vs. computational cost
  - Domain segmentation granularity vs. complexity of hyperparameter tuning

- Failure signatures:
  - High perplexity on specific domains despite DRO adjustments
  - Slow convergence or instability during training
  - Inaccurate reference loss predictions leading to poor DRO performance
  - Excessive computational overhead from frequent DRO updates

- First 3 experiments:
  1. Validate the effectiveness of scaling law-based reference loss prediction on a small dataset.
  2. Test the impact of different DRO update frequencies on training stability and performance.
  3. Compare the performance of coarse-grained vs. fine-grained domain segmentation on a multilingual dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DRPruning change when applied to even smaller pruning ratios (e.g., pruning from 7B to 0.5B parameters)?
- Basis in paper: [explicit] The paper states that "due to computational constraints, we are unable to explore pruning to larger models, i.e., employing smaller pruning ratios."
- Why unresolved: The paper only tested pruning ratios down to 1.3B and 2.7B parameters. The authors suggest that pruning to even smaller models (like 0.5B) might lead to different outcomes, potentially including performance collapse.
- What evidence would resolve it: Experiments comparing DRPruning performance when pruning from 7B to various smaller model sizes (e.g., 0.5B, 1.0B) under the same training budget and data settings.

### Open Question 2
- Question: What is the impact of training LLMs to full convergence (hundreds or thousands of billions of tokens) after pruning, compared to the continued pretraining approach used in this paper?
- Basis in paper: [explicit] The paper mentions that "Xia et al. (2024) point out that pruned models exhibit higher training ceilings" and questions whether training to full convergence would yield better results than continued pretraining under similar settings.
- Why unresolved: The experiments in this paper used a fixed training budget (50 billion tokens for continued pretraining). The authors acknowledge that this might not be sufficient to reach the full potential of pruned models.
- What evidence would resolve it: Experiments comparing DRPruning performance with continued pretraining to full convergence (e.g., 500B or 1000B tokens) versus the current approach, measuring downstream task performance and perplexity.

### Open Question 3
- Question: How effective is DRPruning when applied to other training scenarios beyond pruning and continued pretraining, such as pretraining from scratch or cross-domain post-training?
- Basis in paper: [inferred] The paper states that "our method is expected to be applicable in broader contexts, such as pretraining from scratch and cross-domain post-training" and suggests that "broader validation would further demonstrate the superiority of our approach."
- Why unresolved: The experiments in this paper focused on pruning and continued pretraining scenarios. The authors suggest that DRPruning's dynamic adjustment of reference loss and data ratios could be beneficial in other training contexts, but this remains untested.
- What evidence would resolve it: Experiments applying DRPruning to pretraining from scratch on diverse datasets and to cross-domain post-training scenarios, comparing performance against baseline methods like standard empirical risk minimization or other data scheduling techniques.

## Limitations

- The evaluation relies heavily on held-out benchmark datasets that may not fully represent real-world deployment heterogeneity.
- The scaling law predictions are validated only against their own training trajectories rather than external benchmarks.
- The claimed computational efficiency is not quantitatively substantiated with runtime comparisons or memory usage metrics.

## Confidence

**High confidence**: The mechanism of dynamic data ratio adjustment based on DRO weights is well-established in optimization literature and empirical improvements are clearly demonstrated.

**Medium confidence**: The claim that DRPruning improves instruction tuning win rate by 55.4% is supported by reported experiments, but comparison methodology lacks full specification.

**Low confidence**: The assertion that DRPruning maintains computational efficiency while providing robust performance is not quantitatively substantiated.

## Next Checks

1. Evaluate DRPruning on intentionally corrupted versions of test sets (synthetic noise, domain shifts, or adversarial examples) to verify robustness extends beyond in-distribution performance.

2. Test the scaling law-based reference loss predictions on a completely different architecture (e.g., GPT-2 or Mistral) to determine if the approach generalizes beyond Llama2-7B.

3. Measure and compare wall-clock training time and memory consumption of DRPruning against baseline methods (ReSheared, LoRA, etc.) to validate claimed efficiency benefits in practical deployment scenarios.