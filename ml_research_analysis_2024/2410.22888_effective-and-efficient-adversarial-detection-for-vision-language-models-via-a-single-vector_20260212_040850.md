---
ver: rpa2
title: Effective and Efficient Adversarial Detection for Vision-Language Models via
  A Single Vector
arxiv_id: '2410.22888'
source_url: https://arxiv.org/abs/2410.22888
tags:
- adversarial
- vlms
- radar
- benign
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RADAR, the first large-scale dataset of adversarial
  images with diverse harmful responses, to facilitate research on vision-language
  model (VLM) safety. The authors propose NEARSIDE, a novel method that detects adversarial
  inputs by exploiting an "attacking direction" vector distilled from VLM hidden states.
---

# Effective and Efficient Adversarial Detection for Vision-Language Models via A Single Vector

## Quick Facts
- arXiv ID: 2410.22888
- Source URL: https://arxiv.org/abs/2410.22888
- Authors: Youcheng Huang; Fengbin Zhu; Jingkun Tang; Pan Zhou; Wenqiang Lei; Jiancheng Lv; Tat-Seng Chua
- Reference count: 40
- Primary result: NEARSIDE achieves 83.1% accuracy on LLaVA and 93.5% on MiniGPT-4 with 40-336x speedup over baselines

## Executive Summary
This paper introduces NEARSIDE, a novel method for detecting adversarial attacks against vision-language models (VLMs) by exploiting an "attacking direction" vector distilled from VLM hidden states. The method achieves high detection accuracy while being significantly faster than existing approaches. The authors also contribute RADAR, the first large-scale dataset of adversarial images with diverse harmful responses, to facilitate research on VLM safety. NEARSIDE demonstrates effectiveness through both same-model detection and cross-model transferability using linear transformation of embedding spaces.

## Method Summary
NEARSIDE detects adversarial inputs by extracting the last token embedding from a VLM's LLM decoder and comparing it to a pre-computed "attacking direction" vector. This attacking direction is calculated as the average difference between embeddings of adversarial and benign inputs from a training set. Detection is performed by projecting input embeddings onto this direction and applying a threshold - inputs with high similarity scores are classified as adversarial. For cross-model detection, a linear transformation matrix aligns embedding spaces between different VLMs. The method is evaluated on the RADAR dataset using PGD-based adversarial attacks targeting harmful responses.

## Key Results
- NEARSIDE achieves 83.1% accuracy on LLaVA and 93.5% on MiniGPT-4
- Method is 40-336 times faster than existing approaches like JailGuard
- Cross-model transferability works through linear transformation, with performance comparable to same-model detection
- Best detection performance occurs with larger perturbation radii (64/255 vs 16/255)

## Why This Works (Mechanism)

### Mechanism 1
Adversarial images cause VLMs to shift their embedding space behavior toward harmful outputs. The attacking direction vector captures the average difference between embeddings of benign vs adversarial inputs, representing a directional bias that correlates with harmful response generation. Core assumption: VLMs embed adversarial inputs in a systematically different region of their embedding space than benign inputs.

### Mechanism 2
Projection onto the attacking direction effectively classifies inputs as adversarial or benign. Inputs with embeddings that align closely with the attacking direction are more likely to be adversarial, as this alignment indicates similarity to the harmful behavior pattern. Core assumption: The cosine similarity between input embeddings and the attacking direction is a reliable indicator of adversarial vs benign classification.

### Mechanism 3
Cross-model transferability works through linear transformation of embedding spaces. The attacking direction learned from one VLM can be applied to another VLM by learning a linear transformation matrix that aligns their embedding spaces. Core assumption: Different VLMs learn similar representations of safety-relevant features in their embedding spaces, allowing linear alignment.

## Foundational Learning

- Concept: Steering vectors in LLMs
  - Why needed here: The attacking direction is essentially a steering vector that captures behavior change from benign to harmful responses
  - Quick check question: How are steering vectors calculated from contrastive prompt pairs?

- Concept: Adversarial attack optimization (PGD)
  - Why needed here: Understanding how adversarial images are generated helps explain why they produce the embedding shifts that the attacking direction captures
  - Quick check question: What optimization objective is minimized to generate adversarial images that elicit harmful responses?

- Concept: Linear algebra for embedding space alignment
  - Why needed here: Cross-model transferability relies on learning a linear transformation matrix to align embedding spaces between different VLMs
  - Quick check question: How does PCA dimensionality reduction help make the linear transformation computationally tractable?

## Architecture Onboarding

- Component map: RADAR dataset -> Embedding extraction module -> Attacking direction calculation -> Projection classifier -> Linear transformation module (for cross-model)
- Critical path: Embedding extraction → Attacking direction calculation → Projection classification → Threshold comparison
- Design tradeoffs:
  - Single vector vs. complex models: simplicity and efficiency vs. potential accuracy loss
  - Fixed threshold vs. adaptive: computational efficiency vs. misclassification risk
  - Linear transformation vs. model-specific: generality vs. precision loss
- Failure signatures:
  - High false positive rate: benign inputs misclassified (threshold too low)
  - High false negative rate: adversarial inputs misclassified (threshold too high or weak attacking direction)
  - Poor cross-model performance: inadequate embedding space alignment
- First 3 experiments:
  1. Test attacking direction detection on RADAR with LLaVA using different threshold values
  2. Verify benign images don't project highly onto attacking direction
  3. Test cross-model transferability from LLaVA to MiniGPT-4 with/without linear transformation

## Open Questions the Paper Calls Out

### Open Question 1
Can the attacking direction extracted from one VLM reliably detect adversarial samples in VLMs with significantly different architectures? The authors only test transferability between MiniGPT-4 and LLaVA, which share similar LLM architectures. Testing across vastly different VLM architectures would clarify transferability limits.

### Open Question 2
How does the choice of perturbation radius (ϵ) during adversarial image generation affect NEARSIDE detection effectiveness? While the paper shows performance differences across ϵ values, it doesn't investigate whether attacking directions trained on one ϵ value can detect samples generated with different ϵ values.

### Open Question 3
Can the attacking direction be adapted in real-time to detect novel adversarial attack strategies? The method relies on static attacking directions from fixed training sets and may not generalize to evolving adversarial techniques. Online learning mechanisms for updating the attacking direction would demonstrate adaptability.

## Limitations

- RADAR dataset size (4,000 samples) may limit generalizability to other VLM architectures
- Method relies on fixed thresholds that may not adapt well to different attack types
- Cross-model performance is generally inferior to same-model detection, limiting practical deployment

## Confidence

- Attacking direction mechanism: Medium - empirically supported but lacks theoretical guarantees
- Projection-based classification: High - straightforward implementation with strong performance metrics
- Cross-model transferability: Medium - promising results but depends on unproven universality of embedding representations

## Next Checks

1. **Adversarial Diversity Testing**: Evaluate NEARSIDE against multiple adversarial attack methods beyond PGD-based attacks to verify robustness across different attack strategies.

2. **Architecture Generalization Study**: Test the attacking direction and linear transformation approach across a wider range of VLM architectures (e.g., BLIP, Flamingo, CLIP) to validate embedding space alignment universality.

3. **Real-world Deployment Assessment**: Conduct experiments with streaming video inputs and continuous detection scenarios to evaluate performance under practical conditions with computational efficiency requirements.