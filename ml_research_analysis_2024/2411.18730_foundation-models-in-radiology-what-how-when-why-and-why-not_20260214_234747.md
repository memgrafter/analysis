---
ver: rpa2
title: 'Foundation Models in Radiology: What, How, When, Why and Why Not'
arxiv_id: '2411.18730'
source_url: https://arxiv.org/abs/2411.18730
tags:
- data
- arxiv
- radiology
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive review of foundation models
  (FMs) in radiology, establishing terminology and outlining their development, capabilities,
  and challenges. It details FM properties including large-scale architectures, multimodal
  processing, self-supervised learning, and emergent abilities.
---

# Foundation Models in Radiology: What, How, When, Why and Why Not

## Quick Facts
- arXiv ID: 2411.18730
- Source URL: https://arxiv.org/abs/2411.18730
- Reference count: 0
- Primary result: Comprehensive review of foundation models in radiology, establishing terminology and outlining development, capabilities, and challenges

## Executive Summary
This paper provides a comprehensive review of foundation models (FMs) in radiology, establishing terminology and outlining their development, capabilities, and challenges. It details FM properties including large-scale architectures, multimodal processing, self-supervised learning, and emergent abilities. The review describes training methodologies such as generative and contrastive pre-training, and adaptation techniques like zero-shot learning, linear probing, and instruction tuning. It highlights the need for large-scale, multi-site radiological datasets and standardized evaluation benchmarks. Key FM capabilities include enhanced patient communication, radiological report generation, diagnostic support, workflow optimization, and population insights. The paper addresses challenges including technical limitations, human-computer interaction issues, and ethical concerns like bias and environmental impact. Future directions include optimizing models for resource efficiency, developing privacy-preserving techniques, and integrating interdisciplinary knowledge for responsible FM deployment in radiology.

## Method Summary
The paper provides a conceptual framework and comprehensive review of foundation models in radiology. It synthesizes current literature on FM development, training methodologies, capabilities, and challenges. The authors propose terminology for FM applications in radiology and outline evaluation strategies. While the paper presents theoretical foundations and references empirical studies from the broader ML literature, it does not present original experimental results or specific implementation details for radiology-specific FM training and evaluation.

## Key Results
- Foundation models can learn from large unlabeled datasets, reducing reliance on expensive expert annotations
- FMs demonstrate emergent abilities that enable zero-shot performance on unseen radiological tasks
- Multimodal pre-training improves FM robustness by integrating complementary information from images, text, and clinical data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Foundation models (FMs) in radiology can learn from large unlabeled datasets, reducing reliance on expensive expert annotations.
- Mechanism: By leveraging self-supervised learning and multimodal contrastive pre-training, FMs align embeddings across images and reports, extracting rich representations without manual labels.
- Core assumption: The natural pairing of radiological images with textual reports contains sufficient information for self-supervision.
- Evidence anchors: [abstract] "trained on extensive corpora of unlabeled data and demonstrate high performance across various tasks"; [section] "In self-supervision, models learn to understand and process data by utilizing objectives from the input data itself rather than relying on labels produced by domain experts"
- Break condition: If the correspondence between images and reports is weak or noisy, self-supervised alignment fails and performance drops.

### Mechanism 2
- Claim: FMs can generalize to unseen radiological tasks through emergent abilities, enabling zero-shot performance.
- Mechanism: As model size and training data increase, representations capture latent patterns that allow adaptation to novel tasks without explicit task-specific training.
- Core assumption: Larger models with more diverse data can discover generalizable abstractions not explicitly programmed.
- Evidence anchors: [abstract] "demonstrate high performance across various tasks"; [section] "emergent abilities beyond their training objectives" and "models designed for generating images from text descriptions have shown an ability to perform image classification without task-specific training"
- Break condition: If emergent abilities do not manifest or are not clinically reliable, zero-shot use becomes unsafe.

### Mechanism 3
- Claim: Multimodal pre-training improves FM robustness and accuracy in radiology by integrating complementary information from images, text, and clinical data.
- Mechanism: Fusion modules (e.g., cross-attention) combine embeddings from modality-specific encoders, allowing the model to learn relationships between imaging findings and clinical context.
- Core assumption: Clinical diagnosis depends on integrating multiple data streams, and models can learn these relationships from aligned multimodal data.
- Evidence anchors: [section] "Combining multiple modalities leverages the interconnectedness of a patient's clinical information" and "fusion modules combine this compact information"
- Break condition: If modality alignment is poor or data is incomplete, multimodal benefits degrade or reverse.

## Foundational Learning
- Concept: Self-supervised learning in medical imaging
  - Why needed here: Enables FM training without costly expert annotations
  - Quick check question: Can the model learn to match chest X-rays to their radiology reports without labels?
- Concept: Contrastive learning for multimodal alignment
  - Why needed here: Aligns representations from images and text for joint reasoning
  - Quick check question: Does minimizing distance between image-text pairs improve retrieval accuracy?
- Concept: Instruction tuning for clinical task adaptation
  - Why needed here: Adapts FMs to specific radiological workflows and questions
  - Quick check question: Can the FM generate accurate radiology reports from images after instruction tuning?

## Architecture Onboarding
- Component map: Vision encoder → Fusion module (cross-attention) → Language decoder (for reports) or Vision decoder (for segmentation)
- Critical path: Data → Modality encoders → Contrastive alignment → Fusion → Decoder → Clinical task output
- Design tradeoffs: Large models offer better performance but require more compute; smaller models may need more labeled data
- Failure signatures: Poor alignment metrics in contrastive training, high hallucination rates in report generation, degraded performance on minority groups
- First 3 experiments:
  1. Pre-train a vision encoder on MIMIC-CXR using masked autoencoding; evaluate reconstruction quality
  2. Fine-tune on paired image-text data with contrastive loss; measure image-text retrieval accuracy
  3. Test zero-shot classification on unseen thoracic pathologies; compare to supervised baseline

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the optimal dataset size and composition required to achieve state-of-the-art performance in foundation models for radiology?
- Basis in paper: [explicit] The paper discusses the need for large-scale, multi-site datasets and mentions current datasets like RadImageNet (1M+ images) and MIMIC-CXR (300,000+ image-text pairs), but notes that future datasets must expand beyond thoracic-focused datasets and include diverse populations.
- Why unresolved: While the paper identifies the need for larger and more diverse datasets, it does not specify the exact dataset size or composition that would achieve optimal performance. The relationship between dataset characteristics and model performance remains unclear.
- What evidence would resolve it: Systematic studies comparing model performance across datasets of varying sizes, modalities, and population diversity would provide evidence for optimal dataset requirements.

### Open Question 2
- Question: How can we effectively evaluate the reliability and accuracy of foundation models in clinical settings, particularly regarding their tendency to generate hallucinations or "mistakes of fact"?
- Basis in paper: [explicit] The paper highlights that FMs are not immune to generating errors or hallucinations, and emphasizes the need for standardized evaluation benchmarks assessing reliability and accuracy.
- Why unresolved: Current evaluation methods focus on task-specific performance metrics, but lack comprehensive approaches to assess the generation of incorrect or fabricated information in clinical contexts.
- What evidence would resolve it: Development and validation of evaluation frameworks specifically designed to detect and measure hallucinations and factual errors in FM outputs across various clinical tasks.

### Open Question 3
- Question: What strategies can be implemented to prevent automation bias and overreliance on foundation models in clinical decision-making while maintaining their benefits?
- Basis in paper: [explicit] The paper discusses the risk of overreliance on AI and automation bias, which can lead to incorrect diagnoses or treatment recommendations, and emphasizes the need for safeguards and clear warnings.
- Why unresolved: While the paper identifies the risk, it does not provide specific strategies for balancing the use of FMs with human expertise to prevent overreliance while preserving the benefits of AI assistance.
- What evidence would resolve it: Clinical studies comparing different implementation strategies (e.g., different interface designs, training protocols, or decision support frameworks) to determine which approaches effectively mitigate automation bias while maintaining clinical benefits.

## Limitations
- Most claims about emergent abilities and zero-shot performance are supported by citations to other works rather than direct evidence from radiology-specific FM evaluations
- Technical implementation details (hyperparameters, architectural specifics, training procedures) are not fully specified, limiting reproducibility
- Evidence for clinical utility and safety is largely conceptual rather than demonstrated through clinical trials or large-scale deployment studies

## Confidence
- High Confidence: The conceptual framework for foundation models in radiology, including terminology, training paradigms, and general capabilities, is well-established and theoretically sound
- Medium Confidence: Claims about self-supervised learning effectiveness and multimodal benefits are supported by general ML literature but lack radiology-specific validation
- Low Confidence: Specific claims about emergent abilities, zero-shot performance, and clinical impact in radiology settings are not directly validated in this paper

## Next Checks
1. Conduct controlled experiments comparing FM-based report generation against radiologist-authored reports using standardized metrics (BLEU, ROUGE, clinical accuracy) on benchmark datasets like MIMIC-CXR
2. Perform bias analysis across demographic groups using a large, diverse radiology dataset to quantify performance disparities in FM predictions
3. Implement a resource efficiency comparison between foundation models and traditional ML approaches for common radiological tasks (classification, segmentation) measuring both computational cost and performance trade-offs