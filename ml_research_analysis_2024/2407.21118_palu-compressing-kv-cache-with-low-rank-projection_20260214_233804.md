---
ver: rpa2
title: 'Palu: Compressing KV-Cache with Low-Rank Projection'
arxiv_id: '2407.21118'
source_url: https://arxiv.org/abs/2407.21118
tags:
- palu
- compression
- low-rank
- quantization
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Palu, a novel KV-Cache compression framework
  that utilizes low-rank projection to reduce inference-time LLM memory usage. Palu
  decomposes key and value linear projection weight matrices, caches the compressed
  latent representations, and reconstructs them on-the-fly during decoding.
---

# Palu: Compressing KV-Cache with Low-Rank Projection

## Quick Facts
- arXiv ID: 2407.21118
- Source URL: https://arxiv.org/abs/2407.21118
- Authors: Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-Yan Chen, Yu-Fang Hu, Pei-Shuo Wang, Ning-Chi Huang, Luis Ceze, Mohamed S. Abdelfattah, Kai-Chiang Wu
- Reference count: 40
- Key outcome: Compresses KV-Cache by 50% while maintaining strong accuracy and delivering up to 1.89x speedup on RoPE-based attention

## Executive Summary
Palu is a novel KV-Cache compression framework that utilizes low-rank projection to reduce inference-time LLM memory usage. By decomposing key and value linear projection weight matrices and caching compressed latent representations, Palu achieves significant memory savings while maintaining accuracy. The framework includes optimizations such as group-head low-rank decomposition, efficient rank search algorithm, low-rank-aware quantization, and optimized GPU kernels.

## Method Summary
Palu decomposes the weight matrices of key and value linear layers using SVD, caches compressed intermediate states, and reconstructs full keys and values on-the-fly during decoding. The framework employs group-head low-rank decomposition to balance accuracy and computational cost, uses Fisher information for automatic rank allocation, and integrates Hadamard transformation for low-rank-aware quantization. Optimized GPU kernels are used for efficient RoPE-based attention reconstruction and matrix fusion.

## Key Results
- Compresses KV-Cache by 50% while maintaining strong accuracy
- Achieves up to 1.89x speedup on RoPE-based attention module
- When combined with quantization, achieves up to 2.91x speedup and up to 1.19 lower perplexity compared to state-of-the-art KV-Cache quantization methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Palu decomposes the weight matrices of the key and value linear layers into low-rank matrices, caches the smaller latent representations, and reconstructs the full keys and values on-the-fly during decoding.
- Mechanism: By statically decomposing the projection weight matrices offline, Palu avoids runtime overhead of computing decomposition matrices during inference. The two-step process (down-project to latent space, up-project back) reduces the memory footprint while maintaining accuracy through controlled rank allocation.
- Core assumption: Low-rank decomposition of weight matrices captures sufficient information to reconstruct accurate keys and values without significant accuracy loss.

### Mechanism 2
- Claim: Palu employs group-head low-rank decomposition (G-LRD) to balance accuracy and computational cost by decomposing matrices for groups of attention heads together.
- Mechanism: Instead of decomposing each head individually (M-LRD) or all heads jointly (J-LRD), G-LRD decomposes weight matrices for groups of heads. This captures shared information within each group while limiting computational overhead.
- Core assumption: There is shared information among attention heads within groups that can be captured through joint decomposition without excessive reconstruction cost.

### Mechanism 3
- Claim: Palu integrates low-rank-aware quantization with Hadamard transformation to eliminate low-rank-induced outliers, enabling high quantization accuracy without runtime overhead.
- Mechanism: Low-rank decomposition often creates severe outliers in latent representations. Palu uses Hadamard transformation matrices integrated into the low-rank decomposed weights offline, eliminating outliers and improving quantization accuracy.
- Core assumption: The Hadamard transformation can eliminate low-rank-induced outliers when integrated offline into the decomposed weights.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: SVD is the core mathematical technique Palu uses to decompose weight matrices into low-rank approximations.
  - Quick check question: What are the three matrices produced by SVD, and how are they used in Palu's decomposition process?

- Concept: Fisher Information
  - Why needed here: Palu uses Fisher information to automatically allocate ranks to different weight matrices based on their importance.
  - Quick check question: How does Palu use Fisher information to determine the rank allocation for each decomposed matrix?

- Concept: Transformer Multi-Head Attention Mechanism
  - Why needed here: Palu operates on the key and value projections within the multi-head attention mechanism.
  - Quick check question: How does Palu modify the standard multi-head attention computation to incorporate low-rank projections?

## Architecture Onboarding

- Component map:
  Token embeddings -> Palu Layer (decomposes key/value projection matrices, caches latent representations, reconstructs keys/values) -> Attention Mechanism (uses reconstructed keys and values) -> Output

- Critical path:
  1. Offline: Decompose key/value projection matrices using SVD
  2. Cache: Store latent representations instead of full keys/values
  3. Online: During decoding, reconstruct keys from latent representations
  4. Attention: Compute attention scores using reconstructed keys
  5. Output: Generate final attention output

- Design tradeoffs:
  - Accuracy vs. compression: Higher compression (lower rank) reduces memory but may hurt accuracy
  - Group size vs. efficiency: Larger groups capture more shared information but increase reconstruction cost
  - Quantization bits vs. quality: Lower bits save more memory but may increase perplexity

- Failure signatures:
  - High perplexity or accuracy degradation indicates insufficient rank allocation
  - Slow inference suggests reconstruction overhead is too high
  - Memory usage not reducing as expected indicates issues with the decomposition or caching

- First 3 experiments:
  1. Measure perplexity and accuracy at different compression rates (30%, 50%, 70%) on WikiText-2 to find the accuracy-compression tradeoff
  2. Compare different group sizes (1, 2, 4, 8, 32) to find the optimal balance between accuracy and reconstruction cost
  3. Test quantization integration at different bit levels (8-bit, 4-bit, 3-bit, 2-bit) to measure the impact on perplexity and memory usage

## Open Questions the Paper Calls Out

1. How does Palu's performance scale when applied to LLMs with parameter sizes significantly larger than 13B, such as 70B or 175B models?

2. What is the end-to-end model latency when combining low-rank compression and quantization in Palu?

3. How does Palu's performance compare when integrated with other efficient LLM methods, such as token eviction or weight quantization?

4. How does the choice of group size in G-LRD affect the trade-off between accuracy and reconstruction cost across different LLM architectures?

## Limitations

- Theoretical foundations for why low-rank decomposition preserves semantic information in KV-cache are not rigorously established
- Evaluation focuses primarily on perplexity and accuracy metrics without extensively exploring potential failure modes or edge cases
- Limited empirical evidence for low-rank-aware quantization claims, particularly the integration of Hadamard transformation to eliminate outliers

## Confidence

- High Confidence: Core mechanism of using low-rank decomposition for KV-cache compression is well-established, empirical results showing 50% compression with maintained accuracy are convincing
- Medium Confidence: Group-head decomposition strategy and its claimed benefits are plausible but rely on assumptions about shared information across attention heads that aren't thoroughly validated
- Low Confidence: Low-rank-aware quantization claims, particularly the integration of Hadamard transformation to eliminate outliers, are presented with limited empirical evidence

## Next Checks

1. Systematically vary rank allocations across different model layers and head groups to identify the minimum viable ranks that maintain target accuracy

2. Create controlled experiments with synthetic data to characterize when and why low-rank decomposition creates outliers, and validate whether the Hadamard transformation effectively addresses these cases

3. Test Palu on architectures beyond the evaluated models (Llama2, Mistral, Vicuna) including non-RoPE attention mechanisms and different attention patterns to assess the robustness of the compression approach