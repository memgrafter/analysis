---
ver: rpa2
title: 4-bit Shampoo for Memory-Efficient Network Training
arxiv_id: '2405.18144'
source_url: https://arxiv.org/abs/2405.18144
tags:
- shampoo
- matrix
- adamw
- quantization
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes 4-bit Shampoo, the first second-order optimizer
  with 4-bit optimizer states, enabling memory-efficient training of deep neural networks.
  The key idea is to quantize the eigenvector matrix of the preconditioner rather
  than the preconditioner itself, as small singular values are sensitive to quantization
  errors.
---

# 4-bit Shampoo for Memory-Efficient Network Training

## Quick Facts
- arXiv ID: 2405.18144
- Source URL: https://arxiv.org/abs/2405.18144
- Reference count: 40
- Memory-efficient 4-bit optimizer states achieve comparable performance to 32-bit Shampoo

## Executive Summary
This paper introduces 4-bit Shampoo, the first second-order optimizer with 4-bit optimizer states that enables memory-efficient training of deep neural networks. The key innovation is quantizing the eigenvector matrix of the preconditioner rather than the preconditioner itself, which preserves small singular values critical for accurate inverse 4-th root computation. Orthogonal rectification and linear square quantization further improve performance. Experiments on image classification and natural language modeling tasks demonstrate that 4-bit Shampoo achieves comparable performance to its 32-bit counterpart while using significantly less memory, enabling training of larger models within the same memory budget.

## Method Summary
4-bit Shampoo quantizes second-order optimizer states by factorizing the preconditioner into eigenvector matrix U and singular value matrix Λ, then quantizing only U while preserving Λ exactly. The quantized eigenvector matrix undergoes orthogonal rectification using Björck orthonormalization to restore orthogonality lost during quantization. Linear square quantization mapping is employed instead of dynamic tree quantization for better preservation of eigenvector matrix elements. The method uses block-wise normalization with block size 64 and updates preconditioners every T1 steps and inverse roots every T2 steps, while grafting learning rate scaling to the first-order optimizer.

## Key Results
- 4-bit Shampoo achieves comparable test accuracy to 32-bit Shampoo across CIFAR-100, Tiny-ImageNet, and ImageNet-1k benchmarks
- Memory usage reduced by approximately 8x compared to 32-bit Shampoo while maintaining similar wall-clock time
- Linear square quantization slightly outperforms dynamic tree quantization for eigenvector matrix elements
- Orthogonal rectification significantly improves approximation of preconditioner's eigenvector matrix

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantizing the eigenvector matrix of the preconditioner reduces quantization errors compared to quantizing the preconditioner itself, especially for small singular values.
- Mechanism: The preconditioner is factorized into eigenvector matrix U and singular value matrix Λ. Direct quantization of the full preconditioner causes large perturbations to small singular values due to the skewed distribution, which dramatically affects the inverse 4-th root computation. By quantizing only U, the singular values remain exact, preserving the structure needed for stable inverse 4-th root calculation.
- Core assumption: The singular value distribution of the preconditioner is highly skewed with many small values that are sensitive to quantization perturbations.
- Evidence anchors:
  - [abstract]: "We show that quantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is remarkably better than quantizing the preconditioner itself both theoretically and experimentally."
  - [section]: "Directly quantizing the preconditioner via block-wise quantization [8] at 4-bit precision can significantly alter the small singular values, leading to a drastic change in its inverse 4-th root..."
  - [corpus]: Weak - no direct mention of eigenvector matrix quantization advantages in corpus neighbors.
- Break condition: If the singular value distribution becomes uniform or if all singular values are large, the advantage of eigenvector matrix quantization diminishes.

### Mechanism 2
- Claim: Orthogonal rectification of the quantized eigenvector matrix improves the approximation of the preconditioner's eigenvector matrix and facilitates inverse 4-th root computation.
- Mechanism: After quantization, the eigenvector matrix U may lose orthogonality. Applying Björck orthonormalization iteratively (typically once suffices) restores orthogonality, which ensures that (VΛV^T)^s approximates the true As more accurately, particularly for negative s values used in inverse roots.
- Core assumption: The quantized eigenvector matrix deviates from orthogonality in a way that significantly impacts downstream computations.
- Evidence anchors:
  - [abstract]: "By rectifying the orthogonality of the quantized eigenvector matrix, we enhance the approximation of preconditioner's eigenvector matrix..."
  - [section]: "As the eigenvector matrix of a preconditioner is orthogonal, we apply Björck orthonormalization [4] to rectify the orthogonality of the quantized eigenvector matrix..."
  - [corpus]: Weak - no direct mention of orthogonal rectification techniques in corpus neighbors.
- Break condition: If the quantization process preserves orthogonality by chance or if the preconditioner's eigenvector matrix is not sensitive to small orthogonality deviations.

### Mechanism 3
- Claim: Linear square quantization mapping outperforms dynamic tree quantization for second-order optimizer states at 4-bit precision.
- Mechanism: Linear square quantization maps quantization indices to values using a squared function that better preserves the dynamic range and distribution characteristics of eigenvector matrix elements, which often have both positive and negative values with varying magnitudes.
- Core assumption: The element distribution of eigenvector matrices benefits more from linear square mapping than from dynamic tree mapping at low bitwidths.
- Evidence anchors:
  - [abstract]: "Besides, we find that linear square quantization slightly outperforms dynamic tree quantization when quantizing second-order optimizer states."
  - [section]: "we recommend utilizing linear square (Linear-2) quantization as R, particularly when b=4. Linear-2 quantization is defined as..."
  - [corpus]: Weak - no direct comparison of quantization mappings in corpus neighbors.
- Break condition: If the element distribution changes significantly or if higher bitwidths are used where the difference between mappings becomes negligible.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and its properties
  - Why needed here: The preconditioner is decomposed into eigenvector and singular value matrices; understanding SVD is crucial for grasping why quantizing U is beneficial.
  - Quick check question: What property of orthogonal matrices makes them particularly suitable for quantization in this context?

- Concept: Matrix inverse roots and their sensitivity to perturbations
  - Why needed here: The preconditioner requires computing inverse 4-th roots; understanding how small perturbations affect these computations explains the quantization strategy.
  - Quick check question: Why does a small perturbation in a matrix's singular values lead to a large change in its inverse 4-th root?

- Concept: Quantization techniques and their error characteristics
  - Why needed here: Different quantization mappings (linear square vs dynamic tree) have different error profiles; choosing the right one affects performance.
  - Quick check question: How does block-wise normalization with a block size of 64 help reduce quantization errors?

## Architecture Onboarding

- Component map:
  Preconditioner P -> Eigenvector matrix U + Singular value matrix Λ -> Quantizer Q -> Björck orthonormalization -> First-order optimizer F

- Critical path:
  1. Compute gradient Gt
  2. Update preconditioners Lt and Rt every T1 steps
  3. Update inverse 4-th roots bLt and bRt every T2 steps
  4. Apply preconditioned gradient: bGt = bLt * Gt * bRt
  5. Graft learning rate scaling
  6. Update parameters with first-order optimizer

- Design tradeoffs:
  - Eigenvector vs preconditioner quantization: eigenvector quantization preserves singular values but requires SVD computation
  - Orthogonal rectification frequency: more iterations improve accuracy but increase computation
  - Block size in normalization: larger blocks reduce overhead but may increase quantization error

- Failure signatures:
  - Divergence or poor convergence: likely issues with quantization errors or orthogonal rectification
  - Memory usage higher than expected: check if SVD computation or orthogonal rectification is being performed too frequently
  - Wall-clock time significantly higher: verify that update intervals T1 and T2 are appropriately set

- First 3 experiments:
  1. Implement 4-bit Shampoo with eigenvector matrix quantization only (no orthogonal rectification, linear square quantization) on a small CNN (e.g., VGG19) on CIFAR-100; compare test accuracy and memory usage against 32-bit Shampoo.
  2. Add orthogonal rectification with t1=1 and t2=4 to the previous setup; measure impact on test accuracy and training stability.
  3. Compare linear square quantization against dynamic tree quantization in the setup from experiment 1; quantify the difference in test accuracy and memory usage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does 4-bit Shampoo's performance scale when applied to extremely large language models (e.g., >10B parameters) with long context lengths (e.g., >4096 tokens)?
- Basis in paper: [inferred] The paper mentions that due to computational resource limitations, they did not test their 4-bit Shampoo on large-scale models with billions of parameters, and they did not test long context lengths.
- Why unresolved: The paper's experiments were limited to smaller models and datasets, and the authors explicitly state that they did not test their method on extremely large language models or with long context lengths.
- What evidence would resolve it: Conducting experiments with 4-bit Shampoo on extremely large language models with long context lengths, and comparing the performance, memory usage, and wall-clock time to other optimizers.

### Open Question 2
- Question: How does the orthogonal rectification technique affect the performance of 4-bit Shampoo in the presence of noise or perturbations in the input data?
- Basis in paper: [explicit] The paper mentions that the orthogonal rectification technique is used to improve the approximation of the preconditioner's eigenvector matrix, which benefits the computation of its inverse 4-th root.
- Why unresolved: The paper does not discuss how the orthogonal rectification technique performs in the presence of noise or perturbations in the input data, which could be a common scenario in real-world applications.
- What evidence would resolve it: Conducting experiments with 4-bit Shampoo on noisy or perturbed datasets, and comparing the performance with and without the orthogonal rectification technique.

### Open Question 3
- Question: How does the choice of quantization mapping (e.g., Linear-2 vs. Dynamic Tree) affect the performance of 4-bit Shampoo in different types of neural network architectures (e.g., CNNs vs. transformers)?
- Basis in paper: [explicit] The paper mentions that Linear-2 quantization slightly outperforms Dynamic Tree quantization when quantizing second-order optimizer states, but they do not discuss how this choice affects different types of neural network architectures.
- Why unresolved: The paper only provides a comparison between Linear-2 and Dynamic Tree quantization for one specific task (training Swin-Tiny on CIFAR-100), and does not discuss how this choice affects different types of neural network architectures.
- What evidence would resolve it: Conducting experiments with 4-bit Shampoo on different types of neural network architectures (e.g., CNNs, transformers) using different quantization mappings, and comparing the performance.

## Limitations

- Experimental validation primarily on CIFAR-100 and Tiny-ImageNet with limited ImageNet-1k results
- Performance claims mainly evaluated against 32-bit Shampoo rather than first-order optimizers
- Orthogonal rectification mechanism contribution not fully isolated from eigenvector quantization approach

## Confidence

- **High confidence**: The theoretical justification for quantizing eigenvector matrices instead of preconditioners is sound, based on established SVD properties and sensitivity analysis of matrix inverse roots.
- **Medium confidence**: The empirical results showing comparable performance to 32-bit Shampoo are convincing but limited in scope. The memory savings claims are supported but the practical wall-clock overhead from additional computations (SVD, orthogonal rectification) needs more thorough evaluation.
- **Low confidence**: The claim that linear square quantization "slightly outperforms" dynamic tree quantization lacks sufficient quantitative comparison and the specific scenarios where one method is preferable over the other remain unclear.

## Next Checks

1. Conduct large-scale experiments on ImageNet-1k with ViT-B/16 and Swin-T models to verify if the 4-bit approach maintains performance advantages on more challenging datasets with longer training schedules.

2. Perform controlled ablations comparing 4-bit Shampoo against 4-bit first-order optimizers (AdamW/SGDM) to determine if the second-order benefits justify the additional complexity and computational overhead.

3. Implement a memory-accuracy Pareto analysis across different bit-widths (2-bit, 3-bit, 4-bit, 8-bit) to identify optimal tradeoffs and validate whether 4-bit is indeed the sweet spot for practical deployment.