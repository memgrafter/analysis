---
ver: rpa2
title: Classifier-Free Guidance inside the Attraction Basin May Cause Memorization
arxiv_id: '2411.16738'
source_url: https://arxiv.org/abs/2411.16738
tags:
- memorization
- diffusion
- transition
- point
- guidance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how classifier-free guidance (CFG) in diffusion
  models can cause memorization of training images. The authors propose that an "attraction
  basin" in the denoising process causes CFG to steer samples toward memorized outputs.
---

# Classifier-Free Guidance inside the Attraction Basin May Cause Memorization

## Quick Facts
- arXiv ID: 2411.16738
- Source URL: https://arxiv.org/abs/2411.16738
- Authors: Anubhav Jain; Yuya Kobayashi; Takashi Shibuya; Yuhta Takida; Nasir Memon; Julian Togelius; Yuki Mitsufuji
- Reference count: 40
- Primary result: Delaying classifier-free guidance application until after a transition point in the denoising process mitigates memorization in diffusion models.

## Executive Summary
This paper investigates how classifier-free guidance (CFG) in diffusion models can cause memorization of training images. The authors propose that an "attraction basin" in the denoising process causes CFG to steer samples toward memorized outputs. They observe that applying CFG before a certain "transition point" leads to memorization, while applying it after avoids this issue. Based on this insight, they propose a simple method: delay applying CFG until this transition point is detected by monitoring when the difference between conditional and unconditional noise predictions drops sharply. They also introduce "opposite guidance" to push the trajectory away from the attraction basin sooner. Their approach successfully mitigates memorization across multiple scenarios (fine-tuning on small datasets, data duplication, trigger tokens) without requiring prompt modification or additional computational overhead.

## Method Summary
The authors propose detecting a transition point during the denoising process where the magnitude of conditional guidance drops sharply. By applying classifier-free guidance only after this transition point, they avoid steering samples into an "attraction basin" that causes memorization. They also introduce opposite guidance, which applies negative guidance before the transition point to push the trajectory away from the attraction basin sooner. The method requires no architectural changes or additional training, only modifying when and how guidance is applied during inference.

## Key Results
- Applying CFG before the transition point causes memorization, while applying it after produces non-memorized outputs
- The proposed transition point detection method successfully mitigates memorization across fine-tuning, data duplication, and trigger token scenarios
- Opposite guidance allows earlier application of CFG while maintaining non-memorized outputs
- The approach maintains image quality while reducing memorization without requiring prompt modification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classifier-free guidance applied before the transition point steers the denoising trajectory into an "attraction basin" that causes memorization.
- Mechanism: When CFG is applied early in the reverse diffusion process, the guidance signal pulls samples toward memorized training examples stored as attractors in the model's state space. This creates a funnel-like basin that narrows over time steps.
- Core assumption: Diffusion models have implicit attractor states corresponding to memorized training examples that can dominate the denoising trajectory when CFG is applied too early.
- Evidence anchors:
  - [abstract] "We argue that memorization occurs because of an attraction basin in the denoising process which steers the diffusion trajectory towards a memorized image."
  - [section] "We suspect that there is an attraction basin in the sample space of the diffusion trajectory, in which applying CFG results in memorized outputs."
  - [corpus] Weak evidence - related work discusses diffusion models as associative memory networks, but doesn't directly address attraction basins.
- Break condition: If the conditional noise prediction difference ∥ϵθ(xt, ep) − ϵθ(xt, e∅)∥2 remains low throughout the denoising process, no attraction basin exists.

### Mechanism 2
- Claim: Delaying CFG application until after the transition point allows the denoising trajectory to exit the attraction basin, producing non-memorized outputs.
- Mechanism: The transition point marks when the conditional noise prediction drops sharply, indicating the trajectory has moved away from the attraction basin. Applying CFG after this point guides toward novel, non-memorized outputs.
- Core assumption: The transition point is a reliable indicator of when the denoising trajectory exits the attraction basin.
- Evidence anchors:
  - [abstract] "We propose a simple yet effective approach to mitigate it. We argue that memorization occurs because of an attraction basin... which can be mitigated by not applying classifier-free guidance until an ideal transition point occurs."
  - [section] "We observe that applying classifier-free guidance (CFG) before a certain time step tends to produce memorized samples; we refer to this time step as the transition point. Interestingly, applying CFG after the transition point is unlikely to yield a memorized image."
  - [corpus] Moderate evidence - related work on CFG scheduling shows negative impacts of high CFG in early time steps, supporting delayed application.
- Break condition: If the transition point detection fails (e.g., no clear drop in conditional noise prediction), this mechanism cannot be reliably applied.

### Mechanism 3
- Claim: Opposite guidance pushes the denoising trajectory away from the attraction basin earlier, allowing CFG to be applied sooner without causing memorization.
- Mechanism: By applying negative CFG (opposite guidance), the trajectory is steered in the opposite direction of traditional CFG, effectively escaping the attraction basin sooner. Once escaped, normal CFG can be applied for high-quality generation.
- Core assumption: Opposite guidance creates a repulsive force that counteracts the attraction basin's pull.
- Evidence anchors:
  - [abstract] "we present a new guidance technique, opposite guidance, that escapes the attraction basin sooner in the denoising process."
  - [section] "we introduce a new concept of opposite guidance (OG)... push the trajectory away from the attraction basin of the traditional CFG trajectory sooner in the denoising process."
  - [corpus] Weak evidence - no direct corpus support for opposite guidance, but related work on negative prompts suggests directional control is possible.
- Break condition: If opposite guidance causes the trajectory to diverge too far from the conditional distribution, it may degrade image quality.

## Foundational Learning

- Concept: Classifier-free guidance (CFG) in diffusion models
  - Why needed here: The entire mitigation strategy relies on understanding how CFG steers the denoising trajectory and when to apply it.
  - Quick check question: How does CFG modify the noise prediction in diffusion models, and what parameter controls its strength?

- Concept: Diffusion model reverse process and noise scheduling
  - Why needed here: Understanding the time-dependent nature of the denoising process is crucial for identifying transition points and attraction basins.
  - Quick check question: What is the mathematical form of the reverse diffusion update, and how does it depend on time step t?

- Concept: Dynamical systems theory and attractors
  - Why needed here: The paper frames memorization in terms of attraction basins and attractors, requiring understanding of these dynamical systems concepts.
  - Quick check question: How is an attractor defined in dynamical systems theory, and how does this relate to the concept of an attraction basin in diffusion trajectories?

## Architecture Onboarding

- Component map:
  - U-Net backbone with time and conditioning embeddings
  - Noise prediction head (ϵθ)
  - Scheduler for reverse diffusion
  - Transition point detection module (conditional vs unconditional noise prediction comparison)
  - Optional opposite guidance module

- Critical path:
  1. Initialize with random noise xT
  2. For each time step t from T to 1:
     - Compute conditional and unconditional noise predictions
     - If transition point detected, apply CFG (or switch from opposite guidance to CFG)
     - Otherwise, apply zero guidance or opposite guidance
     - Update xt-1 using scheduler
  3. Return final image x0

- Design tradeoffs:
  - Static vs dynamic transition points: Static points are simpler but may not generalize across different prompts/initializations
  - Timing of CFG application: Earlier application improves quality but risks memorization; later application reduces memorization but may degrade quality
  - Opposite guidance strength: Higher strength escapes attraction basin sooner but may cause instability

- Failure signatures:
  - High similarity scores between generated and training images indicate attraction basin hasn't been escaped
  - Low CLIP scores suggest poor text alignment due to late CFG application
  - High FID scores may indicate instability from aggressive opposite guidance

- First 3 experiments:
  1. Implement static transition point (t=500) and test on fine-tuned SDv2.1 to verify memorization reduction
  2. Implement dynamic transition point detection and test on SDv1.4 with memorized prompts to verify generalization
  3. Implement opposite guidance with transition point detection and compare against standard CFG on datasets with varying memorization levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the attraction basin phenomenon occur in diffusion models beyond text-to-image generation, such as in image-to-image or audio diffusion models?
- Basis in paper: [inferred] The paper primarily focuses on text-to-image models and mentions that previous approaches are specific to text-conditional generation, but the authors propose a method that could theoretically apply to other conditioning mechanisms.
- Why unresolved: The experiments only demonstrate the attraction basin and its mitigation in text-to-image diffusion models (Stable Diffusion v1.4 and v2.1). The authors note their approach doesn't require access to the prompt, suggesting potential applicability to other conditioning mechanisms, but this is not empirically tested.
- What evidence would resolve it: Conducting similar experiments on image-to-image diffusion models (like Stable Diffusion Inpainting) or audio diffusion models (like DiffWave) to observe if transition points and attraction basins exist in those domains, and whether the proposed mitigation strategy (delayed CFG or opposite guidance) effectively reduces memorization.

### Open Question 2
- Question: What is the theoretical relationship between the attraction basin phenomenon and the noise schedule or step size used during inference?
- Basis in paper: [explicit] The paper mentions that applying CFG too late (after the transition point) can result in poor-quality images that resemble unconditional generations, and that transition points can occur at different time steps for different samples.
- Why unresolved: While the paper observes that transition points exist and affect output quality, it doesn't systematically investigate how different noise schedules (linear, cosine, DPM solvers) or varying step counts impact the location and characteristics of attraction basins.
- What evidence would resolve it: A comprehensive ablation study varying noise schedules and step counts across multiple datasets and model architectures to map how these hyperparameters affect transition point locations, attraction basin widths, and the effectiveness of mitigation strategies.

### Open Question 3
- Question: How does the attraction basin phenomenon relate to the underlying architecture of the diffusion model, such as the number of UNet layers or attention mechanisms?
- Basis in paper: [inferred] The paper observes the phenomenon across different scenarios (fine-tuning on small datasets, data duplication, trigger tokens) but doesn't investigate whether architectural differences in the diffusion models affect the presence or severity of attraction basins.
- Why unresolved: The experiments use primarily Stable Diffusion models, but don't compare models with different architectural configurations (e.g., different numbers of attention layers, varying UNet depths, or different conditioning mechanisms) to determine if these factors influence the attraction basin phenomenon.
- What evidence would resolve it: Comparing attraction basin characteristics across diffusion models with varying architectures (different depths, attention mechanisms, conditioning methods) to identify whether certain architectural choices make models more or less susceptible to memorization through attraction basins.

## Limitations

- The transition point detection method may be unreliable for certain prompts or initializations, leading to inconsistent results
- The theoretical basis for attraction basins in diffusion models lacks rigorous mathematical proof
- The effectiveness of opposite guidance is largely theoretical with limited empirical validation

## Confidence

**High Confidence**: The observation that applying CFG before a certain time step leads to memorization is well-supported by experimental results across multiple datasets and scenarios. The basic methodology of delaying CFG application is straightforward and reproducible.

**Medium Confidence**: The existence of an "attraction basin" as a general phenomenon in diffusion models is plausible but not definitively proven. The transition point detection method shows promise but may require tuning for different model architectures and generation tasks.

**Low Confidence**: The effectiveness of opposite guidance as a general solution for escaping attraction basins earlier is the most speculative claim, with limited empirical evidence provided in the paper.

## Next Checks

1. **Transition Point Robustness**: Test the transition point detection method across a wider variety of prompts, initializations, and model architectures (e.g., SDXL, Midjourney-style models) to assess its generalizability and robustness to noise in the detection signal.

2. **Mathematical Formalization**: Attempt to mathematically formalize the concept of attraction basins in diffusion models, potentially using tools from dynamical systems theory to prove the existence and properties of these basins under different guidance regimes.

3. **Opposite Guidance Efficacy**: Conduct a comprehensive ablation study of opposite guidance strength and timing, comparing it against baseline CFG and other guidance modification techniques across multiple memorization scenarios and quality metrics.