---
ver: rpa2
title: 'Exploring the Limits of Zero Shot Vision Language Models for Hate Meme Detection:
  The Vulnerabilities and their Interpretations'
arxiv_id: '2402.12198'
source_url: https://arxiv.org/abs/2402.12198
tags:
- case
- meme
- memes
- hateful
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates zero-shot vision-language models (VLMs) for
  detecting hateful memes. Six VLMs are tested across eight prompt variants on six
  multilingual datasets, covering hate, misogyny, and harmful/offensive content.
---

# Exploring the Limits of Zero Shot Vision Language Models for Hate Meme Detection: The Vulnerabilities and their Interpretations

## Quick Facts
- arXiv ID: 2402.12198
- Source URL: https://arxiv.org/abs/2402.12198
- Reference count: 38
- GPT-4O achieves highest macro-F1 (68.65%) for hate meme detection

## Executive Summary
This paper evaluates zero-shot vision-language models (VLMs) for detecting hateful memes across six multilingual datasets. Six VLMs are tested with eight prompt variants, revealing significant performance variation and vulnerabilities. GPT-4O emerges as the top performer, while error typology induction and superpixel-based occlusion reveal systematic weaknesses including implicit hate and stereotyping. The study highlights annotation quality issues and suggests improved safety guardrails should focus on these systematic error patterns rather than relying solely on model fine-tuning.

## Method Summary
The study evaluates six VLMs (IDEFICS, LLaVA-1.5, InstructBLIP, GPT-4O) across eight prompt variants on six multilingual datasets. Zero-shot inference uses temperature tuning with batch size 1. Occlusion analysis employs SLIC superpixels to identify influential regions in misclassified memes. Error typology is induced via multimodal BERTopic clustering of CLIP embeddings. Manual annotation categorizes misclassifications into 9 error types. Performance is measured using accuracy and macro-F1 scores, with weighted macro-F1 across datasets for overall ranking.

## Key Results
- GPT-4O achieves highest overall macro-F1 of 68.65% across all datasets
- LLaVA-1.5 13B performs best among open-source models at 66.72% weighted macro-F1
- OCR and definition input variants consistently improve performance over vanilla prompts
- Occlusion reveals models are rigid to perturbations, with CASE 1 (no prediction change) most frequent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4O achieves higher macro-F1 because it uses definition + OCR text as input and generates explanation output, providing better reasoning over multimodal cues.
- Mechanism: The model fuses textual definitions with OCR-extracted meme text and visual content, enabling richer contextual understanding for classification.
- Core assumption: GPT-4O's underlying architecture benefits from explicit definition inputs and explanation output prompts for zero-shot reasoning.
- Evidence anchors:
  - [abstract] "GPT-4O achieves the highest overall accuracy (68.65% macro-F1)"
  - [section] "GPT-4 O emerged to be the best model and LLAVA-1.5 13B the best open-source model. While LLAVA-1.5 13B works best with OCR & definition as input and vanilla as output, for GPT-4 O the best prompt variant is not conclusive as per Table 2."
- Break condition: If the model receives ambiguous or incomplete definitions, or if OCR extraction fails, reasoning quality degrades.

### Mechanism 2
- Claim: Superpixel-based occlusion effectively reveals which regions of a meme confuse the model, allowing systematic identification of misclassification patterns.
- Mechanism: By occluding individual superpixels and re-prompting, one can determine whether specific visual or textual regions cause misclassification.
- Core assumption: The model's prediction changes when critical regions are occluded, indicating those regions influence decision-making.
- Evidence anchors:
  - [section] "We present a novel superpixel based occlusion strategy to occlude different parts of an originally mispredicted meme."
  - [section] "If these occlusions result in a change in the model prediction. If they indeed do, then one can conclude that the occluded parts play an important role in the decision making process of the model."
- Break condition: If occlusion does not change predictions despite altering key regions, the model's decision may rely on global features or fixed reasoning paths.

### Mechanism 3
- Claim: Error typology induction via multimodal BERTopic clusters misclassified samples into interpretable patterns, revealing systematic model vulnerabilities.
- Mechanism: Embeddings from CLIP are clustered using BERTopic to identify common themes in misclassification, aligning with occlusion-based interpretations.
- Core assumption: Misclassifications group naturally by semantic similarity, reflecting underlying reasoning errors or dataset annotation issues.
- Evidence anchors:
  - [section] "We cluster the misclassified memes using multi-modal topic modeling thereby inducing a typology of error patterns."
  - [section] "Interestingly, this typology seems to highly align with the different kinds of superpixel based interpretations that we obtain."
- Break condition: If clusters are not interpretable or do not align with occlusion findings, the typology may not reflect true model vulnerabilities.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The study evaluates VLMs without fine-tuning, relying on their pre-trained knowledge to generalize to new hate meme datasets.
  - Quick check question: What distinguishes zero-shot from few-shot or fine-tuned evaluation in VLMs?

- Concept: Multimodal embeddings
  - Why needed here: CLIP embeddings combine image and OCR text features, enabling clustering of misclassifications by semantic similarity.
  - Quick check question: How do CLIP embeddings represent the combined visual and textual information in memes?

- Concept: Prompt engineering
  - Why needed here: Different prompt variants (input/output patterns) significantly affect model performance and reasoning quality.
  - Quick check question: Why might adding definitions or OCR text to prompts improve classification accuracy for hate memes?

## Architecture Onboarding

- Component map:
  Image loading -> OCR extraction -> Prompt formatting -> VLM processing -> Output parsing -> Superpixel segmentation -> Occlusion -> Re-prompting -> Clustering -> Typology induction

- Critical path: Image → OCR + Definition → Prompt → Model → Classification → Occlusion → Re-prompt → Error typology

- Design tradeoffs:
  - Open vs. closed source models: interpretability vs. performance
  - Granularity of superpixels: computational cost vs. precision in localization
  - Number of clusters in BERTopic: interpretability vs. overfitting noise

- Failure signatures:
  - Ambiguous outputs: model does not choose from provided labels
  - Low support: <90% of samples generate valid labels
  - Rigidness: occlusion does not change predictions despite targeting critical regions

- First 3 experiments:
  1. Run each VLM with vanilla input/vanilla output on FHM to establish baseline.
  2. Add OCR text to prompts and compare performance across models.
  3. Perform occlusion on a small set of misclassified samples to verify prediction changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the annotation quality of hateful meme datasets affect model performance, and what specific strategies can improve inter-annotator agreement?
- Basis in paper: [explicit] The paper discusses annotation quality as a major concern, noting that incorrect annotations in datasets can mislead models and undermine evaluation results. It references a phase-wise incremental pilot approach to improve annotation quality and suggests that inter-annotator agreement should improve after each pilot.
- Why unresolved: While the paper identifies annotation quality as a critical issue and suggests a general framework for improvement, it does not provide empirical evidence on how different annotation strategies quantitatively impact model performance or inter-annotator agreement in the context of hateful meme detection.
- What evidence would resolve it: Conducting controlled experiments comparing model performance across datasets with varying annotation quality and inter-annotator agreement levels, and measuring how specific annotation refinement strategies (e.g., feedback loops, benchmark tasks) impact both annotation quality and model accuracy.

### Open Question 2
- Question: Can the superpixel-based occlusion method be generalized to other multimodal tasks beyond hateful meme detection, and how effective is it compared to other interpretability methods?
- Basis in paper: [explicit] The paper introduces a novel superpixel-based occlusion approach to interpret model predictions on multimodal content like memes, showing it can reveal interpretable regions influencing model decisions. It notes that traditional interpretability methods like LIME, SHAP, or Grad-CAM are not well-suited for memes due to their multimodal nature and cannot be applied to closed-source models like GPT-4O.
- Why unresolved: The paper demonstrates the effectiveness of this method specifically for hateful meme detection but does not test its applicability or performance on other multimodal tasks or compare it directly with alternative interpretability techniques in different contexts.
- What evidence would resolve it: Applying the occlusion method to other multimodal tasks (e.g., multimodal sentiment analysis, visual question answering) and comparing its interpretability quality and computational efficiency against established methods like LIME, SHAP, or Grad-CAM in those domains.

### Open Question 3
- Question: How can the induced error typology clusters be effectively integrated into safety guardrails for vision-language models without requiring frequent model retraining?
- Basis in paper: [explicit] The paper proposes using error typology clusters as a way to safeguard VLMs by mapping new hate memes to existing clusters, potentially eliminating the need for costly repeated fine-tuning. It suggests these clusters could inform design of functionality-based trees and preference datasets for VLM alignment.
- Why unresolved: While the paper outlines the potential of error typology for safety guardrails, it does not provide a concrete implementation framework or empirical validation showing how these clusters can be operationalized in real-world moderation systems or how they perform compared to traditional fine-tuning approaches.
- What evidence would resolve it: Developing and testing a prototype safety guardrail system that uses error typology clusters to filter or flag content, and evaluating its accuracy, adaptability, and computational efficiency compared to models that undergo periodic fine-tuning on new data.

## Limitations
- OCR text quality and manual translation process for non-English datasets introduce potential bias and reproducibility concerns
- Superpixel segmentation may not align with semantically meaningful regions in complex meme layouts
- Manual annotation of error typology introduces subjective bias and cultural specificity limitations

## Confidence
- High confidence in model performance rankings (GPT-4O > LLaVA-1.5 13B > others) based on consistent quantitative results across multiple datasets
- Medium confidence in occlusion-based interpretability claims due to potential misalignment between superpixel boundaries and semantic regions
- Medium confidence in error typology clusters given manual annotation requirements and cultural specificity of hate content

## Next Checks
1. Test model performance with synthetically corrupted OCR text to quantify sensitivity to text extraction errors across different languages
2. Compare occlusion results using alternative segmentation methods (e.g., object detection-based regions vs. superpixels) on a subset of memes
3. Conduct inter-annotator agreement study on error typology assignment for 100 randomly selected misclassified memes to assess consistency