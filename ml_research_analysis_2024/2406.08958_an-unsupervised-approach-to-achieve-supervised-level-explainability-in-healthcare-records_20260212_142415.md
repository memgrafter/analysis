---
ver: rpa2
title: An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare
  Records
arxiv_id: '2406.08958'
source_url: https://arxiv.org/abs/2406.08958
tags:
- attention
- methods
- attingrad
- training
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of producing plausible and faithful
  explanations for automated medical coding systems without relying on costly evidence
  span annotations. The authors propose an approach that combines adversarial robustness
  training strategies with a novel feature attribution method called AttInGrad, which
  merges attention-based and gradient-based explanations.
---

# An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records

## Quick Facts
- arXiv ID: 2406.08958
- Source URL: https://arxiv.org/abs/2406.08958
- Reference count: 32
- Primary result: Achieved supervised-level explanation quality (F1: 42.2% vs 41.7%) without evidence span annotations

## Executive Summary
This paper addresses the challenge of producing plausible and faithful explanations for automated medical coding systems without relying on costly evidence span annotations. The authors propose combining adversarial robustness training strategies with a novel feature attribution method called AttInGrad, which merges attention-based and gradient-based explanations. Their approach achieves explanation quality comparable to or better than supervised methods while remaining fully unsupervised, with specific improvements in F1 score, empty rate, and recall@5 metrics.

## Method Summary
The authors implement a modified PLM-ICD architecture (PLM-CA) using RoBERTa-PM encoder and a decoder with class-wise cross attention. They compare five training strategies: BU (baseline unsupervised), BS (supervised with evidence spans), IGR (input gradient regularization), PGD (projected gradient descent), and TM (token masking). The AttInGrad feature attribution method combines attention weights with InputXGrad scores. Evaluation uses MIMIC-III and MDACE datasets with plausibility metrics (F1, Empty rate, Recall@5) and faithfulness metrics (Sufficiency, Comprehensiveness).

## Key Results
- TM + AttInGrad achieved F1 score of 42.2% versus supervised approach at 41.7%
- Empty rate improved from 13.5% (supervised) to 3.0% (unsupervised approach)
- Recall@5 reached 46.6% compared to 45.3% for supervised approach
- AttInGrad produced substantially more faithful and plausible explanations than previous methods
- Adversarial robustness training improved explanation plausibility by reducing reliance on irrelevant features

## Why This Works (Mechanism)

### Mechanism 1
Adversarial robustness training strategies (IGR and TM) improve explanation plausibility by reducing the model's reliance on irrelevant features. IGR encourages small input gradients, pushing the model to depend on fewer features, while TM teaches the model to ignore unimportant tokens by training with a sparse mask. Both strategies aim to embed invariances that prevent reliance on non-robust features.

### Mechanism 2
AttInGrad produces more plausible and faithful explanations than previous methods by combining the strengths of Attention and InputXGrad. AttInGrad multiplies the attention weights with the InputXGrad feature attributions, amplifying the importance of tokens deemed relevant by both methods while down-weighting those highlighted by only one or neither method.

### Mechanism 3
The combination of an adversarially robust model (TM) and AttInGrad produces explanations of similar or better quality than the supervised approach. The adversarially robust model (TM) reduces reliance on irrelevant features, leading to more plausible explanations, while AttInGrad provides a more accurate and robust feature attribution method.

## Foundational Learning

- Concept: Adversarial robustness in machine learning
  - Why needed here: Understanding how adversarial robustness training strategies work and their potential impact on explanation plausibility is crucial for implementing and evaluating the proposed approach.
  - Quick check question: What are the main strategies for achieving adversarial robustness in machine learning models, and how do they aim to improve model performance and interpretability?

- Concept: Feature attribution methods
  - Why needed here: Familiarity with different feature attribution methods (e.g., attention-based, gradient-based, perturbation-based) and their strengths and weaknesses is essential for understanding the proposed AttInGrad method and its advantages over previous approaches.
  - Quick check question: What are the key differences between attention-based, gradient-based, and perturbation-based feature attribution methods, and in what scenarios might each method be more appropriate?

- Concept: Medical coding and electronic health records
  - Why needed here: Understanding the context of medical coding and the challenges associated with automated medical coding systems is important for appreciating the motivation and potential impact of the proposed approach.
  - Quick check question: What are the main challenges in automated medical coding, and how can explainable AI methods help address these challenges and improve the trustworthiness of coding systems?

## Architecture Onboarding

- Component map:
  RoBERTa-PM encoder -> PLM-CA model (with class-wise cross attention) -> Adversarial robustness training (IGR, PGD, TM) -> Feature attribution methods (Attention, InputXGrad, AttInGrad) -> Evaluation metrics

- Critical path:
  1. Preprocess and load MIMIC-III and MDACE datasets
  2. Implement and train the PLM-CA model with different training strategies (BU, BS, IGR, PGD, TM)
  3. Implement and evaluate various feature attribution methods on the trained models
  4. Analyze the results and compare the performance of different combinations of models and attribution methods
  5. Identify the best combination (TM + AttInGrad) and demonstrate its effectiveness compared to the supervised approach

- Design tradeoffs:
  - Computational cost: Adversarial robustness training and feature attribution methods can be computationally expensive, requiring careful resource management and optimization
  - Model complexity: The modified PLM-CA architecture and the addition of adversarial robustness training strategies increase model complexity, which may impact training time and interpretability
  - Evaluation metrics: Choosing appropriate metrics to evaluate explanation plausibility and faithfulness is crucial, as different metrics may emphasize different aspects of explanation quality

- Failure signatures:
  - Poor explanation quality: If the explanations produced by the proposed approach are not plausible or faithful, it may indicate issues with the adversarial robustness training strategies, the feature attribution method, or the model architecture
  - High computational cost: If the training and evaluation process is excessively slow or resource-intensive, it may hinder the practical applicability of the approach
  - Lack of generalizability: If the approach does not generalize well to other medical coding systems, languages, or healthcare institutions, its impact may be limited

- First 3 experiments:
  1. Implement and train the PLM-CA model with the BU (baseline unsupervised) training strategy, and evaluate the performance of various feature attribution methods (Attention, InputXGrad, IntGrad, Deeplift) on the trained model
  2. Implement and train the PLM-CA model with the IGR (input gradient regularization) training strategy, and compare the performance of the same feature attribution methods with those from the BU model
  3. Implement and evaluate the AttInGrad feature attribution method on both the BU and IGR models, and compare its performance with the other attribution methods

## Open Questions the Paper Calls Out

### Open Question 1
How does the alignment between original tokens and their contextualized representations affect the performance of attention-based explanation methods? The authors discuss how misalignment between token positions and their encoded representations may cause attention-based methods to highlight special tokens devoid of alphanumeric characters, leading to high inter-seed variance and potentially misleading explanations.

### Open Question 2
Do adversarial robustness training strategies improve explanation plausibility by reducing the model's reliance on irrelevant features? The authors observe that adversarial robustness training strategies (IGR and TM) improve the plausibility of gradient-based explanations, but they are unable to conclusively prove that this is due to reduced reliance on irrelevant features.

### Open Question 3
How generalizable are the findings of this study across different model architectures, medical coding systems, and languages? The authors acknowledge that their study focused on a single data source (MIMIC-III) and model architecture (PLM-ICD), and that the effectiveness of their proposed methods may vary with different architectures, coding systems, languages, and healthcare institutions.

### Open Question 4
What are the relative strengths and weaknesses of different feature attribution methods for automated medical coding? The authors compare various feature attribution methods (Attention, InputXGrad, IG, Deeplift, AttInGrad) and find that AttInGrad performs best in terms of both plausibility and faithfulness, but they do not provide a comprehensive analysis of the strengths and weaknesses of each method.

## Limitations

- The effectiveness of AttInGrad relies on the assumption that attention and gradient-based methods capture complementary information, but this specific combination lacks direct empirical validation in healthcare records
- The adversarial robustness mechanisms (IGR and TM) show promise but lack detailed analysis of how exactly they reduce reliance on irrelevant features in practice
- The claim of "supervised-level explainability" may be overstated given that the supervised baseline used may not represent state-of-the-art in supervised medical coding explanations

## Confidence

- **High confidence**: Experimental setup and methodology are clearly described with appropriate datasets and evaluation metrics; baseline comparisons provide solid foundation
- **Medium confidence**: Mechanism by which adversarial robustness training improves explanation plausibility is theoretically sound but lacks direct empirical validation; AttInGrad combination shows strong results but rationale is not fully justified
- **Low confidence**: Claim of achieving "supervised-level explainability" may be overstated given the specific supervised baseline used

## Next Checks

1. **Ablation study on AttInGrad components**: Systematically evaluate AttInGrad's performance when combined with different gradient-based methods (not just InputXGrad) to validate that the attention-gradient combination is optimal rather than coincidental.

2. **Clinical expert evaluation**: Conduct user studies with healthcare professionals to assess whether the explanations generated by TM + AttInGrad actually improve clinical decision-making confidence and understanding compared to both supervised and unsupervised baselines.

3. **Cross-institutional validation**: Test the approach on medical coding datasets from different healthcare systems to evaluate generalizability across different clinical documentation styles and ICD coding practices.