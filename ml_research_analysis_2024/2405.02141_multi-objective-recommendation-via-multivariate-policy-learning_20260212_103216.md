---
ver: rpa2
title: Multi-Objective Recommendation via Multivariate Policy Learning
arxiv_id: '2405.02141'
source_url: https://arxiv.org/abs/2405.02141
tags:
- policy
- learning
- https
- sample
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a policy learning approach to optimize scalarization
  weights for multi-objective recommender systems. The key innovation is extending
  the Counterfactual Risk Minimization (CRM) framework to a continuous multivariate
  action space, where the weights are treated as actions that maximize an overall
  North Star reward.
---

# Multi-Objective Recommendation via Multivariate Policy Learning

## Quick Facts
- arXiv ID: 2405.02141
- Source URL: https://arxiv.org/abs/2405.02141
- Reference count: 40
- Primary result: 60x sample size reduction with ESS correction; significant improvements in retention and time on platform in large-scale production deployment

## Executive Summary
This paper proposes a policy learning approach to optimize scalarization weights for multi-objective recommender systems by extending the Counterfactual Risk Minimization (CRM) framework to continuous multivariate action spaces. The key innovation is treating weights as actions that maximize an overall North Star reward, with an effective sample size (ESS) correction to improve confidence interval coverage in low-sample scenarios. Extensive experiments demonstrate the approach reduces required sample size by up to 60x and achieves significant improvements in key metrics when deployed on two large-scale short-video platforms with 160M+ monthly users each.

## Method Summary
The approach extends CRM to continuous multivariate action domains where weights are treated as actions optimized to maximize a North Star reward. It introduces an ESS correction for variance estimation that improves confidence interval coverage, particularly in low-sample scenarios. The method uses multivariate normal logging policies instead of uniform randomisation to mitigate the curse of dimensionality, and leverages learnt metrics to design sensitive reward signals that improve statistical power. Policy optimization is performed via gradient ascent on self-normalised importance sampling estimators with Gaussian kernel smoothing for deterministic policies.

## Key Results
- ESS correction reduces required sample size by up to 60x for adequate confidence interval coverage in offline simulations
- Online experiments on two large-scale platforms show significant improvements in retention and time on platform
- Strong alignment between offline and online results validates the bandit learning paradigm for multi-objective recommendation
- Learnt metrics approach reduces p-values in A/B tests, improving statistical power for detecting true improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy-dependent ESS correction improves confidence interval coverage in finite-sample scenarios
- Mechanism: ESS quantifies how many independent samples from target policy would be equally informative as weighted samples from logging policy. When ESS is low, variance estimates based on normal approximations are under-estimates. The correction inflates variance estimates when ESS is low, improving coverage while converging to CLT-based estimates as sample size grows.
- Core assumption: Relationship between ESS and statistical efficiency holds across different reward structures and policy distances
- Evidence anchors: Section on ESS-corrected sample size definition; abstract claiming 60x sample size reduction

### Mechanism 2
- Claim: Multivariate normal logging policy mitigates curse of dimensionality compared to uniform distributions
- Mechanism: In high dimensions, uniform distributions concentrate most probability mass far from center, making small policy changes statistically inefficient. Multivariate normal logging policy concentrates more samples near mean, increasing effective sample size for small weight perturbations.
- Core assumption: Platform's optimal weights don't change drastically over time, making small perturbations sufficient for exploration
- Evidence anchors: Discussion of logging policy domain restrictions; linear behavior of uniform distribution in low dimensions

### Mechanism 3
- Claim: Learnt metrics maximize statistical power by minimizing type III and type II errors
- Mechanism: Learnt reward signal highly correlated with North Star metric but has lower variance, improving statistical power. This reduces p-values in A/B tests, making it easier to detect true improvements in target metric.
- Core assumption: Relationship between learnt metric and North Star metric is stable enough to learn offline and deploy online
- Evidence anchors: Section on minimizing convex lower bound on type-III and type-II errors; claim about maximizing statistical power

## Foundational Learning

- Concept: Counterfactual Risk Minimization (CRM)
  - Why needed here: CRM provides framework for optimizing policies using historical data without deploying them first, crucial for safe multi-objective weight optimization
  - Quick check question: How does CRM differ from standard importance sampling in terms of variance and bias?

- Concept: Effective Sample Size (ESS)
  - Why needed here: ESS quantifies statistical efficiency of importance sampling, allowing correction of variance estimates in low-sample scenarios
  - Quick check question: What happens to ESS when logging and target policies are identical versus completely different?

- Concept: Bias-variance tradeoff in kernel smoothing
  - Why needed here: Deterministic policies require kernel smoothing for off-policy evaluation, creating tradeoff between bias and variance that affects confidence interval reliability
  - Quick check question: How does increasing kernel bandwidth affect both bias and variance in off-policy evaluation?

## Architecture Onboarding

- Component map: Data collection layer -> Metric learning module -> Policy optimization engine -> Evaluation module -> Deployment system
- Critical path: Random weight generation → User interaction logging → Metric learning → Policy optimization → Confidence interval evaluation → Online deployment
- Design tradeoffs:
  - Uniform vs. normal logging policies: Simplicity vs. statistical efficiency in high dimensions
  - Kernel bandwidth selection: Bias reduction vs. variance control in deterministic policy evaluation
  - ESS estimator choice: Computational efficiency vs. accuracy in capturing reward structure
- Failure signatures:
  - Wide confidence intervals indicating insufficient effective sample size
  - Policy value estimates that don't match online performance (distribution shift)
  - Learnt metrics that correlate poorly with North Star in production
- First 3 experiments:
  1. Implement uniform logging policy and measure ESS for small weight perturbations
  2. Compare ESS correction methods (ℓ2 vs ℓ∞ norms) on synthetic data with known ground truth
  3. Test learnt metric performance against direct North Star metric on historical A/B test data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limitations of the effective sample size (ESS) correction approach, and under what conditions might it fail to provide meaningful improvements?
- Basis in paper: The paper mentions that the ESS correction is designed to improve coverage at small sample sizes while retaining consistency at larger sample sizes, but it does not provide a comprehensive analysis of its limitations or failure modes.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the ESS correction, but does not thoroughly explore scenarios where it might not work well or could even be detrimental.
- What evidence would resolve it: A detailed analysis of the ESS correction's performance under various conditions, such as different reward distributions, action spaces, or logging policies, would help identify its limitations and potential failure modes.

### Open Question 2
- Question: How sensitive is the proposed approach to the choice of logging policy, and what are the trade-offs between different logging policy designs in terms of variance reduction and bias introduction?
- Basis in paper: The paper discusses the importance of the logging policy in off-policy learning and mentions the "curse of dimensionality" when using uniform logging distributions, but does not provide a comprehensive comparison of different logging policy designs or their impact on the final results.
- Why unresolved: The choice of logging policy is crucial for off-policy learning, but the paper does not provide a systematic evaluation of different logging policy designs or their trade-offs.
- What evidence would resolve it: A thorough comparison of different logging policy designs, including their impact on variance, bias, and final performance, would help practitioners make informed decisions about logging policy design.

### Open Question 3
- Question: How can the proposed approach be extended to handle more complex recommendation scenarios, such as those with dynamic user preferences, contextual information, or sequential decision-making?
- Basis in paper: The paper focuses on a simplified recommendation scenario with static weights and does not address more complex scenarios that are common in real-world applications.
- Why unresolved: Real-world recommendation systems often involve dynamic user preferences, contextual information, and sequential decision-making, which are not explicitly addressed in the paper.
- What evidence would resolve it: Extending the proposed approach to handle more complex recommendation scenarios, such as those with dynamic user preferences, contextual information, or sequential decision-making, would demonstrate its applicability to real-world problems.

## Limitations
- The assumption that optimal weights don't drift significantly over time may not hold in rapidly changing environments
- Learnt metrics require substantial history of A/B tests, limiting applicability for new products or metrics
- Gaussian kernel smoothing for deterministic policies introduces bias that may be difficult to quantify in practice

## Confidence
- ESS correction mechanism: High (supported by extensive offline simulations showing 60x sample size reduction)
- Multivariate normal logging policy: Medium (supported by theoretical analysis but limited real-world validation)
- Learnt metrics approach: Medium (well-motivated but dependent on quality of historical experiments for training)

## Next Checks
1. Test ESS correction across diverse reward distributions (heavy-tailed, multimodal) and policy distances to verify relationship between ESS and statistical efficiency holds broadly
2. Extend framework to handle non-stationary weight optimization where optimal weights drift over time
3. Evaluate learnt metrics approach in scenarios with limited historical A/B test data to understand minimum data requirements for effective performance