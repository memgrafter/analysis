---
ver: rpa2
title: Mixture of Link Predictors on Graphs
arxiv_id: '2402.08583'
source_url: https://arxiv.org/abs/2402.08583
tags:
- prediction
- experts
- link
- link-moe
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Link-MoE, a mixture-of-experts model for
  link prediction in graphs. Link-MoE leverages various existing link prediction models
  as experts and uses a gating function to select the appropriate expert for each
  node pair based on pairwise information.
---

# Mixture of Link Predictors on Graphs

## Quick Facts
- arXiv ID: 2402.08583
- Source URL: https://arxiv.org/abs/2402.08583
- Reference count: 40
- Primary result: Link-MoE achieves 18.71% MRR improvement on Pubmed and 9.59% Hits@100 on ogbl-ppa

## Executive Summary
This paper introduces Link-MoE, a mixture-of-experts model for link prediction in graphs that addresses the limitation of uniform pairwise information application in GNN-based methods. Link-MoE leverages various existing link prediction models as experts and uses a gating function to select the appropriate expert for each node pair based on pairwise information. Extensive experiments on diverse real-world datasets demonstrate substantial performance improvements, with Link-MoE achieving a relative improvement of 18.71% on the MRR metric for the Pubmed dataset and 9.59% on the Hits@100 metric for the ogbl-ppa dataset compared to the best baselines.

## Method Summary
Link-MoE is a mixture-of-experts model that combines multiple GNN-based link prediction models (experts) with a gating function to adaptively select the most appropriate expert for each node pair. The gating model uses heuristics like common neighbors, Adamic-Adar, resource allocation, shortest path, Katz, and personalized PageRank scores to determine which expert should handle each specific node pair. The method employs a two-step training strategy where experts are first trained individually, then the gating model is trained to optimally route node pairs to the most suitable expert based on validation set performance.

## Key Results
- Achieves 18.71% relative improvement on MRR metric for Pubmed dataset compared to best baselines
- Demonstrates 9.59% relative improvement on Hits@100 metric for ogbl-ppa dataset
- Shows consistent performance gains across six diverse real-world datasets including citation networks and co-authorship graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different node pairs require different types of pairwise information for optimal link prediction
- Mechanism: The gating function uses various heuristics (CN, AA, RA, shortest path, Katz, PPR) to determine which expert model should handle each specific node pair
- Core assumption: Different heuristics capture distinct aspects of graph structure and feature similarity that correlate with prediction success for different node pair types
- Evidence anchors:
  - [abstract]: "different node pairs within the same dataset necessitate varied pairwise information for accurate prediction"
  - [section]: "different node pairs within the same dataset often require distinct heuristics for predictions" (Section 3)
  - [corpus]: Weak evidence - the corpus doesn't directly address this mechanism

### Mechanism 2
- Claim: Expert models encode different heuristics and complement each other
- Mechanism: Each GNN4LP model specializes in capturing specific graph patterns, and the gating function routes node pairs to the most suitable expert
- Core assumption: Different GNN4LP models have unique strengths in modeling specific graph patterns (e.g., CN-based, multi-hop, feature-based)
- Evidence anchors:
  - [abstract]: "various GNNs as experts and strategically selects the appropriate expert for each node pair"
  - [section]: "different GNN4LP models demonstrate unique strengths in different scenarios" (Section 3.2)
  - [corpus]: Weak evidence - the corpus doesn't provide direct evidence for this mechanism

### Mechanism 3
- Claim: Two-step training strategy prevents expert collapse and ensures balanced utilization
- Mechanism: Individual expert training followed by gating model training avoids the problem of only one expert being consistently selected
- Core assumption: End-to-end training of MoE models often leads to "collapse" where only a single expert dominates
- Evidence anchors:
  - [abstract]: "We employ a two-step training strategy" (Section 4.3)
  - [section]: "helps to avoid the 'collapse problem' often encountered in MoE models" (Section 4.3)
  - [corpus]: No evidence found

## Foundational Learning

- Concept: Graph Neural Networks and their limitations for link prediction
  - Why needed here: Understanding why vanilla GNNs fall short for link prediction and why pairwise representations are needed
  - Quick check question: Why can't we simply use node embeddings from a GNN for link prediction?

- Concept: Heuristic methods for link prediction
  - Why needed here: The gating function uses various heuristics as input features to select appropriate experts
  - Quick check question: What is the difference between local and global structural proximity heuristics?

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Understanding how gating functions route inputs to different experts based on learned weights
  - Quick check question: What is the "collapse problem" in MoE models and why does it occur?

## Architecture Onboarding

- Component map:
  Input (node pair, graph structure, node features) -> Gating model (MLP) -> Expert weights -> Multiple GNN4LP models (NCN, NCNC, Neo-GNN, BUDDY, MLP, Node2Vec, SEAL, GCN, NBFNet, PEG) -> Aggregated prediction

- Critical path:
  1. Compute heuristic values for node pair (i,j)
  2. Feed heuristics and node pair features to gating model
  3. Gating model outputs normalized weights for each expert
  4. Each expert computes prediction score for node pair
  5. Weighted sum of expert scores produces final prediction

- Design tradeoffs:
  - Expert diversity vs. model complexity: More experts capture more patterns but increase computational cost
  - Heuristic selection: Need to balance comprehensiveness with noise from irrelevant features
  - Two-step training vs. end-to-end: Training efficiency vs. potential for better optimization

- Failure signatures:
  - All gating weights concentrate on one expert (collapse problem)
  - Performance doesn't improve over single best expert
  - Certain experts consistently receive near-zero weights

- First 3 experiments:
  1. Ablation study: Remove one expert at a time to measure impact on performance
  2. Heuristic ablation: Test gating performance with different subsets of heuristics
  3. Expert diversity: Compare performance using only 3-4 experts vs. all available experts

## Open Questions the Paper Calls Out
- How can Link-MoE be further optimized to handle even larger graphs with billions of nodes and edges?
- Can Link-MoE be extended to handle dynamic graphs where the structure and node features change over time?
- How can Link-MoE be adapted to handle graphs with heterogeneous node types and edge types?

## Limitations
- The two-step training strategy's necessity is not fully validated - no ablation study comparing one-step vs two-step training
- Choice of specific heuristics and expert models appears somewhat arbitrary with limited justification
- The method's scalability to extremely large graphs (billions of nodes) is not addressed

## Confidence
- **High confidence**: Empirical performance improvements on benchmark datasets (18.71% MRR improvement on Pubmed, 9.59% Hits@100 on ogbl-ppa)
- **Medium confidence**: The mechanism that different node pairs require different heuristics (supported by intuition but limited direct evidence)
- **Low confidence**: The claim that two-step training is essential for preventing collapse (no comparative ablation study provided)

## Next Checks
1. **Expert collapse validation**: Monitor gating weight distributions during training to empirically verify that the two-step approach prevents collapse compared to end-to-end training
2. **Heuristic sensitivity analysis**: Systematically remove individual heuristics to determine which ones contribute most to performance gains and whether any are redundant
3. **Generalization test**: Evaluate Link-MoE on graphs with different characteristics (heterophilic vs homophilic, varying sizes) to assess robustness across diverse graph types