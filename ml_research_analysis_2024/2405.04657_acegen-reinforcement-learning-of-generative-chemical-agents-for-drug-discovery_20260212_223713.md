---
ver: rpa2
title: 'ACEGEN: Reinforcement learning of generative chemical agents for drug discovery'
arxiv_id: '2405.04657'
source_url: https://arxiv.org/abs/2405.04657
tags:
- acegen
- molecules
- drug
- design
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ACEGEN is a new toolkit for designing drugs using reinforcement
  learning (RL). It uses a modern RL library called TorchRL, which makes the code
  more efficient and reliable.
---

# ACEGEN: Reinforcement learning of generative chemical agents for drug discovery

## Quick Facts
- arXiv ID: 2405.04657
- Source URL: https://arxiv.org/abs/2405.04657
- Authors: Albert Bou; Morgan Thomas; Sebastian Dittert; Carles Navarro RamÃ­rez; Maciej Majewski; Ye Wang; Shivam Patel; Gary Tresadern; Mazen Ahmad; Vincent Moens; Woody Sherman; Simone Sciabola; Gianni De Fabritiis
- Reference count: 22
- Primary result: ACEGEN is a toolkit that uses reinforcement learning to design new molecules for drug discovery, outperforming other methods on various benchmarks.

## Executive Summary
ACEGEN is a new toolkit for designing drugs using reinforcement learning (RL). It uses a modern RL library called TorchRL, which makes the code more efficient and reliable. ACEGEN can design new molecules by learning from data and improving over time. The toolkit was tested on various drug discovery tasks, including designing molecules that target specific proteins like 5-HT2A and D2 receptors. The results show that ACEGEN can create diverse and effective molecules, often outperforming other methods. It also allows for scaffold decoration and fragment linking, which are important in drug design. Overall, ACEGEN is a powerful tool that can help scientists design better drugs more efficiently.

## Method Summary
ACEGEN leverages the TorchRL library to provide a modular reinforcement learning framework for drug discovery. The method involves pre-training Chemical Language Models (CLMs) on molecular datasets, defining custom scoring functions, and training RL agents using algorithms like REINFORCE, A2C, and PPO. The framework supports experience replay, reward shaping, and KL divergence penalties to improve sample efficiency and stability. ACEGEN can generate novel molecules by optimizing these scoring functions, allowing for flexible adaptation to diverse drug discovery objectives.

## Key Results
- ACEGEN outperforms other methods on the MolOpt benchmark, demonstrating superior performance in molecular generation tasks.
- The toolkit successfully designs molecules targeting specific proteins like 5-HT2A and D2 receptors, showcasing its practical applicability.
- ACEGEN enables scaffold decoration and fragment linking, which are crucial techniques in drug design, enhancing its utility for complex drug discovery tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ACEGEN uses TorchRL to modularize reinforcement learning for drug design, improving reliability and efficiency.
- Mechanism: By leveraging TorchRL's pre-tested, reusable RL components, ACEGEN can assemble complex RL agents for molecular generation without writing custom RL logic, reducing redundancy and improving maintainability.
- Core assumption: TorchRL's RL components are sufficiently general and well-tested to be adapted to the domain of molecular design.
- Evidence anchors:
  - [abstract] "built using TorchRL, a modern RL library that offers thoroughly tested reusable components."
  - [section] "TorchRL, a comprehensive RL library, offers well-tested, independent state-of-the-art RL components."
  - [corpus] The corpus includes related works on RL for drug discovery but lacks direct evidence on TorchRL's reliability; this is assumed from the abstract.
- Break Condition: If TorchRL's components are not sufficiently general or well-tested for the complexities of molecular design, ACEGEN's reliability and efficiency gains would be compromised.

### Mechanism 2
- Claim: ACEGEN's modular design allows for easy integration of custom scoring functions, enabling flexible adaptation to diverse drug discovery objectives.
- Mechanism: ACEGEN provides a framework where users can define their own scoring functions as Python methods, allowing the RL agent to be optimized for a wide range of desired molecular properties beyond simple benchmarks.
- Core assumption: The modular design allows for seamless integration of custom scoring functions without requiring extensive modifications to the core ACEGEN codebase.
- Evidence anchors:
  - [abstract] "ACEGEN allows the integration of custom scoring functions by providing the flexibility to define them as Python methods."
  - [section] "ACEGEN allows the integration of custom scoring functions by providing the flexibility to define them as Python methods that accept strings and return numerical values."
  - [corpus] No direct corpus evidence on the ease of integration, but the claim is explicitly stated in the paper.
- Break Condition: If the integration of custom scoring functions requires significant modifications to the core ACEGEN codebase, the claimed flexibility would be limited.

### Mechanism 3
- Claim: ACEGEN incorporates techniques like experience replay, reward shaping, and KL divergence penalties to improve the sample efficiency and stability of RL training for molecular generation.
- Mechanism: By integrating these established RL techniques, ACEGEN can learn more efficiently from limited data and maintain stable learning progress, leading to better performance in generating desired molecules.
- Core assumption: These techniques are effective in the specific context of molecular generation and do not introduce unintended biases or instability.
- Evidence anchors:
  - [abstract] "ACEGEN provides training for RL agents utilizing the following methods: REINFORCE, REINVENT, AHC, A2C, and PPO, as well as an adapted version of the PPOD algorithm."
  - [section] "ACEGEN provides training for RL agents utilizing the following methods: REINFORCE, REINVENT, AHC, A2C, and PPO, as well as an adapted version of the PPOD algorithm."
  - [corpus] The corpus mentions related works on RL for drug discovery but does not provide specific evidence on the effectiveness of these techniques in ACEGEN.
- Break Condition: If these techniques are not effective in the context of molecular generation or introduce unintended biases, ACEGEN's performance and stability could be compromised.

## Foundational Learning

- Concept: Reinforcement Learning (RL)
  - Why needed here: ACEGEN uses RL to train agents that can navigate the vast chemical space and generate molecules with desired properties.
  - Quick check question: What is the difference between value-based and policy-based RL methods, and which does ACEGEN primarily use?

- Concept: Chemical Language Models (CLMs)
  - Why needed here: ACEGEN uses CLMs to represent and generate molecular structures as strings, allowing the application of NLP techniques to drug design.
  - Quick check question: How do CLMs convert molecular graphs into strings and vice versa, and what are the advantages of this representation?

- Concept: Scoring Functions
  - Why needed here: ACEGEN uses scoring functions to evaluate the desirability of generated molecules based on properties like binding affinity, selectivity, and drug-likeness.
  - Quick check question: What are some common types of scoring functions used in drug discovery, and how can they be integrated into ACEGEN?

## Architecture Onboarding

- Component map:
  TorchRL -> Chemical Language Models -> Scoring Functions -> RL Agents -> PromptSMILES

- Critical path:
  1. Pre-train a CLM on a large dataset of molecules using teacher forcing.
  2. Define a custom scoring function that captures the desired molecular properties.
  3. Initialize an RL agent using a pre-trained CLM as the policy.
  4. Train the RL agent to maximize the scoring function using ACEGEN's RL algorithms and techniques.
  5. Use the trained RL agent to generate novel molecules with the desired properties.

- Design tradeoffs:
  - Modularity vs. Performance: ACEGEN's modular design allows for easy customization but may introduce some overhead compared to a monolithic implementation.
  - Flexibility vs. Simplicity: ACEGEN supports a wide range of RL algorithms and techniques but may require more configuration and tuning compared to a simpler approach.
  - Sample Efficiency vs. Exploration: ACEGEN incorporates techniques to improve sample efficiency but may compromise exploration of the chemical space.

- Failure signatures:
  - Poor performance: The generated molecules may not meet the desired properties or may be chemically invalid.
  - Instability: The RL training may become unstable or diverge due to issues with the scoring function or RL algorithm.
  - Limited diversity: The generated molecules may lack diversity and fail to explore the full chemical space.

- First 3 experiments:
  1. Reproduce the MolOpt benchmark results using ACEGEN's pre-trained CLMs and RL algorithms to validate the implementation.
  2. Implement a custom scoring function for a specific drug discovery task and train an RL agent to optimize it using ACEGEN.
  3. Use PromptSMILES to generate molecules with a specific scaffold or substructure and evaluate the diversity and quality of the generated molecules.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of experience replay impact the performance of different RL algorithms (REINFORCE, A2C, PPO, PPOD) in the ACEGEN framework compared to their non-experience replay counterparts?
- Basis in paper: [explicit] The paper compares the performance of various RL algorithms with and without experience replay in the MolOpt benchmark and the 5-HT2A case study.
- Why unresolved: The paper provides a comparison but does not isolate the specific contribution of experience replay to the performance differences between algorithms.
- What evidence would resolve it: A direct ablation study comparing each algorithm with and without experience replay, controlling for other hyperparameters and using the same scoring functions and datasets.

### Open Question 2
- Question: What is the optimal balance between regularization to a prior policy and reward signal optimization for different drug discovery objectives and how does this balance vary depending on the complexity of the objective function and the available training data?
- Basis in paper: [explicit] The paper discusses the importance of regularization to a prior policy in practical drug discovery and shows that implicit regularization can lead to better performance than explicit inclusion of chemistry filters in the reward signal.
- Why unresolved: The paper does not provide a systematic method for determining the optimal balance between regularization and reward signal optimization for different objectives.
- What evidence would resolve it: A comprehensive study varying the strength of regularization and reward signal components across a range of drug discovery objectives with different complexities and data availability.

### Open Question 3
- Question: How can the exploration-exploitation trade-off be effectively managed in ACEGEN to ensure the generation of diverse and chemically valid molecules while still optimizing for the desired properties?
- Basis in paper: [inferred] The paper highlights the importance of exploration in drug discovery and measures it using metrics like sphere exclusion diversity and the proportion of molecules passing chemistry filters. However, it does not provide a clear strategy for managing this trade-off.
- Why unresolved: The paper shows that different algorithms prioritize exploration and exploitation differently but does not provide a unified framework for balancing these aspects.
- What evidence would resolve it: Development and evaluation of novel RL algorithms or modifications to existing ones that explicitly address the exploration-exploitation trade-off in the context of molecular generation, potentially using techniques like curiosity-driven exploration or Bayesian optimization.

## Limitations

- TorchRL component reliability for molecular design complexity is assumed but not directly validated.
- Custom scoring function integration complexity may be underestimated in practice.
- Real-world drug discovery application beyond benchmarks remains unproven.

## Confidence

- Modular RL design efficiency: Medium
- Custom scoring function flexibility: High
- Performance improvements over baselines: Medium

## Next Checks

1. Reproduce benchmark results using ACEGEN's pre-trained CLMs and RL algorithms to verify implementation correctness.
2. Implement and integrate a custom scoring function for a specific drug discovery task, measuring ease of integration and performance impact.
3. Generate molecules for a real-world protein target (beyond benchmarks) and evaluate chemical validity and diversity using independent chemistry tools.