---
ver: rpa2
title: 'ShaRP: Explaining Rankings and Preferences with Shapley Values'
arxiv_id: '2401.16744'
source_url: https://arxiv.org/abs/2401.16744
tags:
- feature
- rank
- ranking
- features
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ShaRP, a framework that extends the QII method
  for explaining rankings by defining ranking-specific quantities of interest (QoIs),
  including score, rank, top-k, and pairwise preference. ShaRP leverages Shapley values
  to quantify feature contributions while accounting for the relative nature of rankings.
---

# ShaRP: Explaining Rankings and Preferences with Shapley Values

## Quick Facts
- arXiv ID: 2401.16744
- Source URL: https://arxiv.org/abs/2401.16744
- Authors: Venetia Pliatsika; Joao Fonseca; Kateryna Akhynko; Ivan Shevchenko; Julia Stoyanovich
- Reference count: 6
- Key outcome: Framework explaining rankings using Shapley values with multiple quantities of interest

## Executive Summary
ShaRP introduces a framework for explaining rankings by extending the QII method to define ranking-specific quantities of interest (QoIs). The framework computes Shapley values to quantify feature contributions while accounting for the relative nature of rankings. ShaRP supports both score-based and learned ranking models through black-box access and is implemented in an open-source library. Experiments demonstrate that feature importance depends on data distribution and rank stratum, and that different QoIs provide complementary insights beyond score-based analysis.

## Method Summary
ShaRP implements the QII framework with ranking-specific quantities of interest to explain feature importance in rankings. The method computes Shapley values through sampling feature coalitions, requiring only black-box access to the ranking model. Multiple QoIs (score, rank, top-k, pairwise preference) capture different aspects of ranking behavior. The framework is implemented in an open-source library and tested on both synthetic and real datasets, including CSRankings, to demonstrate its effectiveness in revealing nuanced feature importance patterns.

## Key Results
- Feature importance depends on data distribution and rank stratum, not just feature weights
- Different QoIs (score, rank, top-k, pairwise) provide complementary insights about ranking behavior
- Framework supports both score-based and learned ranking models through black-box access
- Real-world experiments with CSRankings reveal nuanced feature importance patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ShaRP's feature importance depends on data distribution and rank stratum, not just feature weights
- Mechanism: By using Shapley values to compute marginal contributions across all feature coalitions, ShaRP captures the relative nature of rankings where an item's rank depends on its relationship to other items in the dataset
- Core assumption: The ranking outcome for an item is not independent of other items in the dataset
- Evidence anchors:
  - [abstract]: "The contributions instead depend on the feature distributions, and on the subtle local interactions between the scoring features"
  - [section]: "In fact, score-based rankers are a prominent example of the so-called 'interpretable models' ... And yet, despite being syntactically 'interpretable', score-based rankers may not be 'explainable'"
  - [corpus]: Weak evidence - the corpus contains related work on Shapley values and rankings but no direct evidence for this specific mechanism
- Break condition: If rankings could be computed independently for each item (like in classification), this mechanism would fail

### Mechanism 2
- Claim: Different Quantities of Interest (QoIs) provide complementary insights about ranking behavior
- Mechanism: By defining multiple QoIs (score, rank, top-k, pairwise preference), ShaRP can explain different aspects of the ranking that cannot be captured by a single metric, revealing nuanced feature importance patterns
- Core assumption: Different aspects of ranking behavior reveal different feature importance patterns
- Evidence anchors:
  - [abstract]: "The framework supports both score-based and learned ranking models and is implemented in an open-source library. Experiments with synthetic and real datasets...demonstrate that feature importance depends on data distribution and rank stratum, and that ShaRP's QoIs provide complementary insights beyond score-based analysis"
  - [section]: "Figure 2...showing that they surface different insights about the importance of specific research areas...Figure 3a presents the top-k QoI for this dataset. The feature importance it shows is consistent with Figure 2(a) (score QoI), but the high positive impact of Systems on an department's presence at the top-k is even stronger pronounced"
  - [corpus]: Weak evidence - corpus contains related work on ranking explainability but limited evidence for QoI complementarity
- Break condition: If all QoIs yielded identical feature importance patterns, they would not provide complementary insights

### Mechanism 3
- Claim: ShaRP can explain both score-based and learned ranking models through black-box access
- Mechanism: By requiring only black-box access to the ranker, ShaRP can compute Shapley values without needing to know the internal structure of the ranking algorithm, making it applicable to both transparent score-based functions and opaque learned models
- Core assumption: Black-box access is sufficient to compute feature importance
- Evidence anchors:
  - [abstract]: "The framework supports both score-based and learned ranking models and is implemented in an open-source library"
  - [section]: "Because it relies on black-box access to the ranker, ShaRP can be used to explain both score-based and learned ranking models"
  - [corpus]: Weak evidence - corpus contains related work on ranking explainability but limited evidence for black-box applicability
- Break condition: If the ranking algorithm cannot be queried with different feature combinations, black-box access would be insufficient

## Foundational Learning

- Concept: Shapley values and cooperative game theory
  - Why needed here: ShaRP uses Shapley values to quantify feature contributions to ranking outcomes, requiring understanding of how marginal contributions are calculated across feature coalitions
  - Quick check question: How does the Shapley value formula account for all possible feature coalitions when computing feature importance?

- Concept: Ranking algorithms and learning-to-rank
  - Why needed here: Understanding both score-based ranking (where scores are computed from features) and learning-to-rank (where models predict rankings) is essential for applying ShaRP appropriately
  - Quick check question: What is the fundamental difference between how scores are computed in score-based ranking versus learning-to-rank?

- Concept: Quantities of Interest (QoIs) for ranking
  - Why needed here: Different QoIs (score, rank, top-k, pairwise preference) capture different aspects of ranking behavior, and understanding when to use each is crucial for effective analysis
  - Quick check question: Why might feature importance computed for the rank QoI differ from that computed for the score QoI?

## Architecture Onboarding

- Component map:
  Core ShaRP framework -> Sampling engine -> QoI calculators -> Black-box ranker interface -> Visualization module

- Critical path:
  1. Receive dataset and ranker
  2. Select QoI(s) to compute
  3. For each item and feature, sample coalitions and compute marginal contributions
  4. Aggregate Shapley values across strata or items
  5. Generate visualizations

- Design tradeoffs:
  - Sampling vs. exact computation: Sampling (m < |D-1|) trades accuracy for speed
  - Number of QoIs: More QoIs provide richer insights but increase computational cost
  - Stratum granularity: Finer strata reveal more local patterns but reduce sample size per stratum

- Failure signatures:
  - Identical feature importance across all QoIs: May indicate the ranking is too simple or the data distribution is uniform
  - Zero variance in feature contributions: May indicate insufficient sampling or that features have no impact on ranking
  - Feature importance that doesn't align with feature weights: Expected behavior due to the relative nature of rankings

- First 3 experiments:
  1. Run ShaRP on a simple synthetic dataset with known feature weights and verify that rank QoI differs from score QoI
  2. Apply top-k QoI to the CSRankings dataset and verify that feature importance varies across ranking strata
  3. Use pairwise QoI to explain why one item ranks higher than another in a simple dataset with two features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we extend ShaRP to support additional quantities of interest (QoIs) beyond score, rank, top-k, and pairwise preference, such as stability of rankings or fairness metrics?
- Basis in paper: [explicit] The paper mentions plans to "support additional quantities of interest" in future work.
- Why unresolved: The current implementation focuses on a limited set of QoIs, and there is no analysis of how ShaRP would handle more complex ranking aspects.
- What evidence would resolve it: A study demonstrating ShaRP's effectiveness in explaining feature importance for stability or fairness QoIs, along with an analysis of computational complexity and interpretability.

### Open Question 2
- Question: Can ShaRP be adapted to explain rankings generated by complex models, such as neural networks, and how would this affect its performance and interpretability?
- Basis in paper: [explicit] The paper mentions plans to "develop methods for learning low-complexity black-box models" as future work.
- Why unresolved: The current implementation relies on black-box access to rankers, and it is unclear how well it would perform with complex models.
- What evidence would resolve it: An experimental evaluation of ShaRP's performance and interpretability when applied to rankings generated by neural networks or other complex models.

### Open Question 3
- Question: How do ShaRP's explanations compare to those generated by other interpretability methods for rankings, such as SHAP or LIME, in terms of usability, expressiveness, and performance?
- Basis in paper: [explicit] The paper mentions plans to "develop a comprehensive benchmark that compares feature importance methods for rankings" in future work.
- Why unresolved: The current evaluation focuses on ShaRP's effectiveness, but there is no comparison with other methods.
- What evidence would resolve it: A comprehensive benchmark study comparing ShaRP's explanations to those generated by other interpretability methods for rankings, using metrics such as usability, expressiveness, and performance.

## Limitations
- Sampling-based approximation introduces errors in Shapley value estimation, especially for high-dimensional feature spaces
- Framework requires access to a well-defined scoring function or black-box ranker, limiting applicability to complex multi-step ranking processes
- Computational cost scales poorly with number of features and items, potentially limiting practical deployment for large-scale ranking systems

## Confidence
- **High Confidence**: The core mechanism of using Shapley values for ranking explainability (Mechanism 1) is well-established theoretically and the experimental results on synthetic and real datasets support the claims about feature importance dependence on data distribution and rank stratum.
- **Medium Confidence**: The claims about QoI complementarity (Mechanism 2) are supported by experimental results but would benefit from additional datasets to verify the generality of the patterns observed.
- **Medium Confidence**: The black-box applicability (Mechanism 3) is theoretically sound, but the paper provides limited evidence of performance across diverse ranking algorithms beyond score-based and simple learned models.

## Next Checks
1. Test ShaRP on datasets with known feature importance patterns to verify that the rank QoI consistently differs from the score QoI across different ranking scenarios.
2. Apply ShaRP to a ranking dataset with highly correlated features to evaluate how the framework handles multicollinearity and whether it correctly attributes importance.
3. Measure the computational time and approximation error as a function of the number of samples (m) to establish guidelines for selecting m in practice.