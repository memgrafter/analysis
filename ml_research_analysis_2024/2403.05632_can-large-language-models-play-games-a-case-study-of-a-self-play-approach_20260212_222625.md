---
ver: rpa2
title: Can Large Language Models Play Games? A Case Study of A Self-Play Approach
arxiv_id: '2403.05632'
source_url: https://arxiv.org/abs/2403.05632
tags:
- value
- mcts
- action
- player
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes combining large language models (LLMs) with
  Monte-Carlo Tree Search (MCTS) to solve deterministic turn-based zero-sum games
  like chess and Go. The core idea is to use LLMs as both action pruners to reduce
  the MCTS search space and as value function proxies to estimate node values.
---

# Can Large Language Models Play Games? A Case Study of A Self-Play Approach

## Quick Facts
- arXiv ID: 2403.05632
- Source URL: https://arxiv.org/abs/2403.05632
- Authors: Hongyi Guo; Zhihan Liu; Yufeng Zhang; Zhaoran Wang
- Reference count: 40
- One-line primary result: Combining LLMs with MCTS solves chess puzzles with 74% accuracy versus 10% for vanilla MCTS with 50 simulations

## Executive Summary
This paper proposes using large language models (LLMs) as both action pruners and value function proxies within Monte Carlo Tree Search (MCTS) to play deterministic turn-based zero-sum games like chess and Go. The approach leverages LLMs' pattern recognition capabilities to reduce the search space and provide strategic evaluations without requiring additional training. Theoretical analysis shows the suboptimality scales as O(|A|/√N + ε_pruner + ε_critic), and experiments demonstrate significant performance improvements over both standalone LLMs and vanilla MCTS.

## Method Summary
The method combines LLMs with MCTS by using the LLM to generate top-k action suggestions (pruning) and evaluate board positions (value function proxy). The LLM suggests 20 most promising moves per state with temperature 0.7, and these pruned actions are used in MCTS with UCB selection. For chess, a hybrid value function combines the LLM's positional evaluation with a rule-based material evaluator. The approach is tested on chess puzzles, 5x5 Go, and full chess games against Stockfish, using different LLMs (gpt-3.5-turbo-instruct and gpt-4) for different tasks.

## Key Results
- Chess puzzles: 74% accuracy versus 10% for vanilla MCTS with 50 simulations
- MiniGo on 5x5 board: Superior performance compared to vanilla MCTS
- Full chess games: Outperforms vanilla LLM (majority voting) baseline
- Theoretical suboptimality bound: O(|A|/√N + ε_pruner + ε_critic)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can act as effective action pruners in MCTS by filtering out low-probability moves based on their knowledge of game patterns
- Mechanism: The LLM evaluates the current game state and suggests the top 20 most promising moves based on its understanding of chess and Go patterns. This reduced action space (eA) is then used in the MCTS algorithm instead of the full action space (A), significantly reducing the branching factor
- Core assumption: The LLM's knowledge of game patterns is sufficient to identify moves that are at least as good as random sampling would produce
- Evidence anchors:
  - [abstract] "Specifically, we utilize LLMs as both action pruners and proxies for value functions without the need for additional training"
  - [section] "In this methodology, LLMs are utilized both as action pruners and value function proxies, enhancing the efficiency and effectiveness of both LLMs and MCTS"
- Break condition: The LLM's pruning becomes too aggressive, eliminating moves that would be optimal in certain game states, or if the LLM lacks sufficient knowledge of the specific game patterns being played

### Mechanism 2
- Claim: LLMs can serve as effective value function proxies in MCTS by evaluating board positions
- Mechanism: The LLM evaluates the strategic position of the board, focusing on positional advantage rather than piece values. This evaluation is combined with a rule-based material evaluator to provide a hybrid value function that estimates the expected outcome from a given position
- Core assumption: The LLM's understanding of positional chess concepts (piece coordination, pawn structure, attacking potential) is sufficiently accurate to provide useful evaluations
- Evidence anchors:
  - [section] "The LLM-based critic evaluates the strategic position of the chess board such as piece coordination, pawn structure and attacking potential, focusing on positional advantage rather than the value of the pieces"
  - [section] "We combine the rule-based critic and the LLM-based critic by adding them together"
- Break condition: The LLM's positional evaluations become systematically biased or fail to recognize specific tactical patterns, leading the MCTS to make poor decisions

### Mechanism 3
- Claim: The combination of LLM pruning and value function proxy allows MCTS to achieve sublinear suboptimality with respect to the number of simulations
- Mechanism: The theoretical analysis shows that the suboptimality of the estimated value scales as O(|eA|/√N + ε_pruner + ε_critic), where N is the number of MCTS simulations. This means that even with relatively few simulations, the method can achieve good performance because the LLM reduces the effective action space and provides reasonable value estimates
- Core assumption: The LLM's pruning and value estimation errors (ε_pruner and ε_critic) are bounded and don't grow with the number of simulations
- Evidence anchors:
  - [abstract] "We theoretically prove that the suboptimality of the estimated value in our proposed method scales with O(|eA|/√N + ε_pruner + ε_critic)"
  - [section] "Corollary 5.6 reveals that the suboptimality of the value estimation in our Algorithm 1 vanishes at a sublinear rate with respect to the number of simulations N up to errors incurred from LLM as critic and action pruner"
- Break condition: The errors ε_pruner and ε_critic grow with the complexity of the game state, or the relationship between |eA| and N breaks down for certain game types

## Foundational Learning

- Concept: Deterministic Turn-based Zero-Sum Games (DTZG)
  - Why needed here: The algorithm is specifically designed for this class of games where two players take turns making moves with perfect information and one player's gain is the other's loss
  - Quick check question: What are the key properties that distinguish a DTZG from other game types, and why does this matter for the algorithm design?

- Concept: Monte Carlo Tree Search (MCTS) with Upper Confidence Bounds (UCB)
  - Why needed here: MCTS is the core search algorithm that explores the game tree, and the UCB variant balances exploration and exploitation during the search
  - Quick check question: How does the polynomial UCB bonus in this algorithm differ from the logarithmic bonus typically used in MCTS, and what theoretical advantage does this provide?

- Concept: Value Function Iteration and Contraction Properties
- Why needed here: The theoretical analysis relies on understanding how value function iteration contracts toward the optimal value function, which is used to bound the error in the algorithm
  - Quick check question: Why is the max player's value iteration contractive with factor γ, while the min player's iteration is contractive with factor -γ?

## Architecture Onboarding

- Component map:
  - LLM Action Pruner: Generates top-k moves from current state
  - MCTS Core: Tree search algorithm with UCB selection
  - LLM Value Critic: Evaluates board positions for value estimation
  - Rule-based Evaluator: Provides material-based evaluation as baseline
  - Hybrid Value Function: Combines LLM and rule-based evaluations

- Critical path:
  1. Current state → LLM Action Pruner → Pruned action space
  2. MCTS selection using UCB on pruned space
  3. Rollout to depth limit or game end
  4. Value estimation using hybrid critic
  5. Backpropagation of values
  6. Root action selection

- Design tradeoffs:
  - Action pruning vs. search completeness: Aggressive pruning may miss optimal moves
  - LLM critic vs. rule-based critic: LLM provides positional understanding but may be less reliable
  - Search depth vs. computational cost: Deeper search provides better estimates but is more expensive
  - Number of simulations vs. accuracy: More simulations improve estimates but increase computation time

- Failure signatures:
  - Poor pruning: MCTS explores irrelevant branches, search becomes inefficient
  - Bad value estimates: MCTS makes moves that appear good locally but are strategically poor
  - Inconsistent behavior: Results vary significantly between runs due to LLM stochasticity
  - Performance degradation: Method performs worse than baseline as game complexity increases

- First 3 experiments:
  1. Chess puzzle validation: Test on mate-in-N puzzles to verify pruning and value estimation work on simple, verifiable problems
  2. MiniGo baseline comparison: Play against a fixed opponent on 5x5 board to measure improvement over vanilla MCTS
  3. Full chess game testing: Play against Stockfish at different levels to evaluate real-world performance with hybrid value function

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed LLM+MCTS approach scale with different types of games beyond chess and Go, particularly in games with more complex action spaces or different reward structures?
- Basis in paper: [explicit] The paper focuses on deterministic turn-based zero-sum games like chess and Go, but does not explore other game types.
- Why unresolved: The theoretical analysis and experiments are limited to specific game types, leaving open the question of generalizability.
- What evidence would resolve it: Experiments on a diverse set of games with varying complexities and reward structures would provide insights into the approach's scalability.

### Open Question 2
- Question: What is the impact of the choice of LLM on the performance of the LLM+MCTS approach, and how do different LLMs compare in terms of action pruning and value function approximation?
- Basis in paper: [inferred] The paper uses gpt-3.5-turbo-instruct and gpt-4 for different experiments but does not compare their performance directly.
- Why unresolved: The paper does not provide a comparative analysis of different LLMs, leaving open the question of their relative effectiveness.
- What evidence would resolve it: A systematic comparison of different LLMs on the same tasks would clarify their impact on performance.

### Open Question 3
- Question: How sensitive is the LLM+MCTS approach to the choice of hyperparameters, such as the UCB bonus function parameters and the search depth, and what is the optimal configuration for different game scenarios?
- Basis in paper: [explicit] The paper sets specific hyperparameters for the experiments but does not explore their sensitivity or optimal values.
- Why unresolved: The paper does not investigate the impact of hyperparameter tuning on performance, leaving open the question of optimal configurations.
- What evidence would resolve it: A sensitivity analysis and hyperparameter optimization study would provide insights into the approach's robustness and optimal settings.

## Limitations

- Reliance on LLM quality: The approach's performance depends heavily on the LLM's pruning accuracy and value estimation quality
- Computational cost: Querying LLMs for each MCTS simulation may be computationally expensive for practical deployment
- Limited generalizability: The paper doesn't test on games with different characteristics beyond chess and Go

## Confidence

**High Confidence**: The core claim that combining LLMs with MCTS improves performance over vanilla MCTS on chess puzzles (74% vs 10% accuracy) is well-supported by experimental results.

**Medium Confidence**: The claim that LLMs can serve as effective value function proxies for chess and Go is supported by results but relies heavily on the quality of the hybrid critic.

**Low Confidence**: The generalizability claim to other deterministic turn-based zero-sum games beyond chess and Go, as the paper doesn't test on games with different characteristics.

## Next Checks

1. **Error Bound Validation**: Design experiments to empirically measure the values of ε_pruner and ε_critic across different game states and compare them against the theoretical bounds.

2. **Cross-Model Consistency**: Repeat the chess puzzle experiments using different LLM models (e.g., Claude, Llama) to assess whether performance improvements depend on the specific LLM.

3. **Computational Efficiency Analysis**: Measure the wall-clock time per MCTS simulation with and without LLM integration, and compare the total time to reach optimal solutions.