---
ver: rpa2
title: 'MRAG: A Modular Retrieval Framework for Time-Sensitive Question Answering'
arxiv_id: '2412.15540'
source_url: https://arxiv.org/abs/2412.15540
tags:
- question
- retrieval
- temporal
- answer
- mrag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work focuses on the problem of answering time-sensitive questions
  using retrieval-augmented large language models, where existing retrievers struggle
  to identify relevant documents due to the complexity of temporal reasoning. To address
  this, the authors introduce a modular retrieval framework (MRAG) that decomposes
  each question into main content and temporal constraints, retrieves and summarizes
  evidence based on the main content, and applies a hybrid semantic-temporal ranking
  to score each fine-grained evidence sentence.
---

# MRAG: A Modular Retrieval Framework for Time-Sensitive Question Answering

## Quick Facts
- arXiv ID: 2412.15540
- Source URL: https://arxiv.org/abs/2412.15540
- Reference count: 40
- Primary result: 9.3% top-1 answer recall and 11% top-1 evidence recall improvements on time-sensitive QA

## Executive Summary
MRAG addresses the challenge of answering time-sensitive questions using retrieval-augmented large language models. Existing retrievers struggle with temporal reasoning, often conflating semantic and temporal relevance. The framework introduces a modular approach that decomposes questions into main content and temporal constraints, retrieves and summarizes evidence at the sentence level, and applies hybrid semantic-temporal ranking. On the TEMP RAG Eval benchmark, MRAG significantly outperforms baseline retrievers, achieving substantial improvements in both retrieval performance and end-to-end QA accuracy.

## Method Summary
MRAG implements a three-module framework for time-sensitive question answering. The Question Processing module decomposes each query into main content (MC) and temporal constraint (TC) using LLM prompting. The Retrieval and Summarization module retrieves relevant passages using semantic retrievers, segments them into sentences, and summarizes each passage to distill key temporal information. The Semantic-Temporal Hybrid Ranking module scores each evidence sentence using both semantic relevance (via embedding-based methods) and temporal relevance (via symbolic functions). This modular decomposition enables specialized processing for semantic and temporal dimensions separately, improving retrieval accuracy for time-sensitive queries.

## Key Results
- MRAG achieves 9.3% improvement in top-1 answer recall (AR@1) over baseline retrievers
- MRAG achieves 11% improvement in top-1 evidence recall (ER@1) for retrieving relevant evidence
- MRAG boosts end-to-end QA accuracy by 4.5% for both exact match (EM) and F1 scores
- The framework introduces approximately 2x computational overhead compared to standard RAG pipelines

## Why This Works (Mechanism)

### Mechanism 1
MRAG's modular decomposition into main content and temporal constraint improves retrieval by separating semantic from temporal relevance. Question processing splits each query into MC and TC, allowing independent semantic and temporal scoring rather than conflating them. The core assumption is that temporal reasoning can be effectively isolated and scored using symbolic functions without losing semantic context. This works because temporal constraints often require different reasoning than semantic relevance.

### Mechanism 2
Sentence-level evidence scoring with LLM summarization reduces noise from irrelevant temporal information in passages. Passages are split into sentences and summarized to distill relevant information, then ranked using hybrid semantic-temporal scoring at the sentence level. The core assumption is that most irrelevant temporal information can be filtered at the sentence level without losing critical context. This improves precision by focusing on fine-grained evidence rather than noisy full passages.

### Mechanism 3
Symbolic temporal scoring functions outperform embedding-based temporal matching for reasoning-intensive queries. Temporal scores are computed using predefined symbolic functions based on extracted timestamps and query temporal constraints, rather than relying solely on semantic similarity. The core assumption is that temporal relationships (before, after, between) can be accurately captured with symbolic functions without requiring training on temporal data. This provides interpretable and precise temporal reasoning compared to continuous embedding approaches.

## Foundational Learning

- **Concept:** Temporal reasoning in retrieval
  - Why needed here: Standard retrievers rely on keyword or semantic matching that fails for time-sensitive queries requiring temporal logic
  - Quick check question: What happens when a retriever encounters "Who won the championship as of 2021?" versus "Who won the championship in 2021?"

- **Concept:** Modular decomposition of complex queries
  - Why needed here: Separating semantic content from temporal constraints enables specialized processing for each dimension
  - Quick check question: How would you split "Who was the president after 2000 but before 2010?" into MC and TC components?

- **Concept:** Fine-grained evidence ranking
  - Why needed here: Passage-level ranking includes irrelevant temporal information; sentence-level ranking enables precise matching
  - Quick check question: Why might ranking at the sentence level outperform passage-level ranking for time-sensitive questions?

## Architecture Onboarding

- **Component map:** Question Processing -> Retrieval & Summarization -> Semantic-Temporal Hybrid Ranking -> Answer Generation
- **Critical path:** The retrieval performance directly impacts answer generation quality; failures in the first two modules cascade to downstream components
- **Design tradeoffs:** MRAG introduces computational overhead (approximately 2x runtime) for improved temporal reasoning accuracy versus standard RAG
- **Failure signatures:**
  - Retrieval failures manifest as low answer recall despite relevant passages existing in corpus
  - Summarization failures introduce hallucinations that mislead ranking
  - Temporal scoring failures occur when symbolic functions cannot handle complex temporal constraints
- **First 3 experiments:**
  1. Compare MRAG vs baseline retrievers on a small subset of TEMP RAG Eval to validate retrieval improvements
  2. Test different numbers of passages for summarization (1-10) to find optimal balance between recall and hallucination risk
  3. Evaluate semantic-only vs hybrid scoring on questions with varying temporal complexity to measure contribution of temporal component

## Open Questions the Paper Calls Out

### Open Question 1
How does MRAG perform on time-sensitive questions with implicit temporal constraints (e.g., "recently" or "traditionally") versus explicit temporal constraints? The paper focuses on explicit temporal constraints and mentions extending to implicit temporal reasoning as a future direction. This remains unresolved because the paper doesn't evaluate MRAG's capability on implicit temporal reasoning, which is acknowledged as a more complex challenge.

### Open Question 2
What is the impact of knowledge conflicts among passages on MRAG's performance, and how can these conflicts be effectively resolved? The paper mentions that knowledge conflicts among passages are left for future work, while analyzing conflicts between LLM parametric knowledge and external passages. This is unresolved because the paper identifies knowledge conflicts among passages as a limitation but doesn't explore solutions or evaluate how these conflicts affect MRAG's retrieval and QA performance.

### Open Question 3
How does MRAG's computational overhead scale with increasing corpus size and query complexity? The paper mentions that MRAG introduces approximately twice the computational overhead of standard RAG pipelines and provides a detailed analysis in Appendix M. This remains unresolved because while the paper provides a basic computational overhead assessment, it doesn't explore how this overhead scales with larger datasets or more complex queries, which is crucial for real-world deployment.

## Limitations
- The framework introduces approximately 2x computational overhead compared to standard RAG pipelines, representing a significant efficiency tradeoff
- Performance relies on LLM-based summarization, which introduces hallucination risks despite mitigation strategies
- The symbolic temporal scoring functions may struggle with complex or implicit temporal constraints requiring cross-sentence reasoning
- Evaluation is limited to Wikipedia domain and English questions, with unknown performance on other domains or languages

## Confidence

- **High confidence:** The modular decomposition approach and sentence-level evidence ranking demonstrably improve retrieval performance for time-sensitive questions, as evidenced by consistent improvements across multiple metrics and baselines
- **Medium confidence:** The symbolic temporal scoring functions provide robust performance for explicit temporal constraints, though their effectiveness for complex or implicit temporal reasoning remains partially validated
- **Medium confidence:** The LLM summarization component improves retrieval by filtering irrelevant information, though the risk of hallucination and the optimal number of passages to summarize require further investigation

## Next Checks

1. **Complex Temporal Constraint Validation:** Test MRAG on questions with implicit or multi-part temporal constraints (e.g., "first/last" combined with multiple time periods) to validate the robustness of symbolic temporal scoring functions beyond explicit date-based queries

2. **Hallucination Risk Assessment:** Systematically evaluate the LLM summarization module by comparing performance with varying numbers of summarized passages (1-10) and measuring hallucination rates when original passages are versus are not provided to the reader model

3. **Cross-Domain Generalization:** Evaluate MRAG's performance on non-Wikipedia corpora (news articles, scientific literature, social media) to assess domain transferability and identify whether the modular decomposition approach generalizes beyond the current evaluation setting