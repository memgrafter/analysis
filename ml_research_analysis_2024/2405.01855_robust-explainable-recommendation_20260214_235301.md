---
ver: rpa2
title: Robust Explainable Recommendation
arxiv_id: '2405.01855'
source_url: https://arxiv.org/abs/2405.01855
tags:
- explanations
- explainable
- which
- recommendation
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a general framework for enhancing the robustness
  and generalization of feature-aware explainable recommender systems under adversarial
  attacks. The framework employs adversarial training to make the recommender system
  resilient to perturbations in the item-feature relationships, which are critical
  for both recommendations and explanations.
---

# Robust Explainable Recommendation

## Quick Facts
- arXiv ID: 2405.01855
- Source URL: https://arxiv.org/abs/2405.01855
- Reference count: 40
- This paper introduces a general framework for enhancing the robustness and generalization of feature-aware explainable recommender systems under adversarial attacks.

## Executive Summary
This paper presents a framework to improve the robustness and generalization of feature-aware explainable recommender systems under adversarial attacks. The framework employs adversarial training to make the recommender system resilient to perturbations in the item-feature relationships, which are critical for both recommendations and explanations. By incorporating an adversarial objective into the training process, the model learns to provide stable explanations even under noisy or attacked conditions. Experiments on two popular explainable recommendation methods (CER and EFM) using three real-world datasets show that the framework significantly improves the robustness and generalization of explanations, particularly under white-box model attacks, while maintaining recommendation quality. The method is flexible and applicable across different architectures and scales.

## Method Summary
The framework introduces adversarial training to enhance the robustness and generalization of feature-aware explainable recommender systems. It trains on both clean and adversarially perturbed versions of the item-feature matrix, forcing the model to learn robust patterns that are invariant to small changes in feature representation. The method uses FGSM-based perturbations constrained to valid ranges, with equal weighting between clean and adversarial objectives. The framework is applied to two popular explainable recommendation methods (CER and EFM) and evaluated on three real-world datasets (Yelp, Amazon CDs, Amazon Kindle).

## Key Results
- The framework significantly improves explanation robustness under white-box model attacks while maintaining recommendation quality
- Equal weighting (/u1D706 = 0.5) between clean and adversarial objectives provides optimal balance between generalization and robustness
- FGSM perturbations with constrained ranges preserve valid item-feature matrix structure while providing effective adversarial pressure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial training stabilizes explanation quality under perturbed item-feature relationships
- Mechanism: The framework trains on both clean and adversarially perturbed versions of the item-feature matrix, forcing the model to learn robust patterns that are invariant to small changes in feature representation
- Core assumption: Item-feature relationships are the primary driver of both recommendation quality and explanation generation
- Evidence anchors:
  - [abstract] "By incorporating an adversarial objective into the training process, the model learns to provide stable explanations even under noisy or attacked conditions"
  - [section] "we chose to learn the perturbations directly from the total objective /u1D43F/u1D461/u1D45C/u1D461/u1D44E/u1D459 since explainable recommenders learn for a single joint objective that includes both the utilities together"
  - [corpus] Weak evidence - no directly related papers found discussing adversarial training specifically for explanation stability

### Mechanism 2
- Claim: Equal weighting between clean and adversarial objectives (/u1D706 = 0.5) balances generalization and robustness
- Mechanism: By giving equal importance to both objectives during training, the model learns to maintain clean performance while gaining resistance to attacks
- Core assumption: The optimal balance point for explanation robustness occurs at equal weighting of both objectives
- Evidence anchors:
  - [abstract] "Experiments on two popular explainable recommendation methods (CER and EFM) using three real-world datasets show that the framework significantly improves the robustness and generalization of explanations"
  - [section] "when /u1D706= 0.5, we apply an equal weightage to both the terms, which allows improvement to the explanation robustness and generalization of the model"
  - [corpus] Weak evidence - no directly related papers found discussing optimal weighting for adversarial training in explanation contexts

### Mechanism 3
- Claim: FGSM-based perturbations constrained to [-/u1D716/u1D437, /u1D716/u1D437] preserve valid item-feature matrix structure while providing adversarial pressure
- Mechanism: Fast Gradient Sign Method generates perturbations in the direction that maximally increases loss, scaled by /u1D716/u1D437 and clipped to maintain valid feature ranges
- Core assumption: Small perturbations in the direction of increasing loss provide effective adversarial pressure without destroying the fundamental structure of the item-feature relationships
- Evidence anchors:
  - [section] "We used Fast Gradient Sign Method (FGSM) which computes the direction of the gradients of the loss objective while learning"
  - [section] "we then ensure that the perturbations don't distort the values of /u1D44C to a different range outside [0, /u1D441]"
  - [corpus] Weak evidence - no directly related papers found discussing FGSM application to item-feature matrices in recommendation systems

## Foundational Learning

- Concept: Adversarial training and its application to recommendation systems
  - Why needed here: The framework builds on adversarial training principles to improve explanation robustness, requiring understanding of how adversarial examples are generated and used during training
  - Quick check question: How does the FGSM method generate perturbations, and why is the sign of the gradient used rather than the magnitude?

- Concept: Feature-aware explainable recommendation systems
  - Why needed here: The framework is designed specifically for feature-aware explainers, requiring understanding of how item features are extracted and used to generate explanations
  - Quick check question: How are item features extracted from user reviews, and what role do they play in generating explanations?

- Concept: White-box attack scenarios and their implications
  - Why needed here: The framework is evaluated under white-box attacks, requiring understanding of what information is available to attackers and how they can exploit model parameters
  - Quick check question: What distinguishes white-box attacks from black-box attacks, and what specific vulnerabilities do white-box scenarios expose?

## Architecture Onboarding

- Component map:
  Core recommender (CER/EFM) -> Adversarial perturbation generator -> Dual-objective trainer -> Clipping mechanism

- Critical path: Training loop where item-feature matrix is processed → perturbations are generated → both clean and perturbed versions are used to update model parameters → clipping ensures validity

- Design tradeoffs:
  - Smaller /u1D716/u1D437 values provide more stable training but may not provide sufficient adversarial pressure
  - Larger /u1D706 values improve robustness but may degrade clean performance
  - The framework trades some clean explanation quality for improved robustness

- Failure signatures:
  - Explanation quality collapses completely under attack (vanilla performs better than defended model)
  - Clean explanation performance drops significantly with no corresponding robustness gains
  - Training instability with oscillating loss values

- First 3 experiments:
  1. Baseline comparison: Train vanilla CER/EFM and evaluate clean and attacked performance to establish baseline metrics
  2. Perturbation sensitivity: Train with varying /u1D716/u1D437 values (0.25, 0.5, 0.75, 1.0) to find optimal perturbation magnitude
  3. Objective balancing: Train with varying /u1D706 values (0.01, 0.1, 0.5, 0.9) to find optimal balance between clean performance and robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise trade-off between the loss scale penalty (/u1D706) and the robustness improvement under adversarial attacks across different feature-aware explainable recommendation methods?
- Basis in paper: [explicit] The paper discusses the impact of /u1D706 on both generalization and robustness, noting a trade-off between the two.
- Why unresolved: While the paper provides insights into the general trend of the trade-off, it does not offer a precise quantification or a general formula that can be applied across different methods.
- What evidence would resolve it: Empirical studies across a wider range of feature-aware explainable recommendation methods, with varying values of /u1D706, to establish a clear and quantifiable relationship between /u1D706 and robustness.

### Open Question 2
- Question: How does the magnitude of perturbations (/u1D716/u1D437) impact the stability and robustness of neural network-based explainable recommendation methods versus factorization-based methods?
- Basis in paper: [explicit] The paper notes that neural network-based models like CER are more sensitive to larger perturbations compared to factorization-based models like EFM.
- Why unresolved: The paper provides a qualitative observation but does not delve into the underlying reasons for this difference or offer a detailed analysis of the impact across various perturbation magnitudes.
- What evidence would resolve it: A detailed comparative study examining the behavior of different types of models under varying magnitudes of perturbations, with a focus on identifying the factors contributing to the observed differences.

### Open Question 3
- Question: How effective is the proposed framework in improving the fairness of explanations in recommender systems, beyond just robustness and generalization?
- Basis in paper: [inferred] The paper mentions the potential for the framework to be extended to model other aspects of explainability, such as fairness, but does not explore this aspect.
- Why unresolved: The paper's primary focus is on robustness and generalization, leaving the question of fairness unexplored.
- What evidence would resolve it: Experiments evaluating the framework's impact on the fairness of explanations, using established fairness metrics, across diverse recommender systems and datasets.

## Limitations

- The framework's evaluation focuses primarily on white-box attacks, leaving black-box attack scenarios unexplored
- The optimal parameter settings (/u1D716/u1D437, /u1D706) and their generalizability across different recommendation architectures remain uncertain
- Lack of ablation studies examining individual components' contributions to the overall performance gains

## Confidence

- High confidence: The core mechanism of adversarial training for explanation robustness is well-established in the broader ML literature
- Medium confidence: The specific application to feature-aware recommendation systems and the observed improvements across multiple datasets
- Low confidence: The optimal parameter settings (/u1D716/u1D437, /u1D706) and their generalizability across different recommendation architectures

## Next Checks

1. Conduct ablation studies to isolate the contribution of adversarial training versus other framework components to the observed performance improvements
2. Evaluate the framework's performance under black-box attack scenarios to assess real-world applicability
3. Test the framework on additional explainable recommendation methods beyond CER and EFM to verify architectural generalizability