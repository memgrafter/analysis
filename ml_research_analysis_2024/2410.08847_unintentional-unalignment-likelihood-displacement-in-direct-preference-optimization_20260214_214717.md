---
ver: rpa2
title: 'Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization'
arxiv_id: '2410.08847'
source_url: https://arxiv.org/abs/2410.08847
tags:
- likelihood
- displacement
- training
- preference
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical failure mode in direct preference
  optimization (DPO) where models unintentionally decrease the likelihood of preferred
  responses during training, shifting probability mass to responses with opposite
  meanings. Through theoretical analysis and experiments across multiple models and
  datasets, the authors show this "likelihood displacement" can catastrophically undermine
  alignment, such as reducing Llama-3-8B-Instruct's refusal rate from 74.4% to 33.4%
  on unsafe prompts.
---

# Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization

## Quick Facts
- arXiv ID: 2410.08847
- Source URL: https://arxiv.org/abs/2410.08847
- Reference count: 40
- Primary result: Direct preference optimization can unintentionally decrease likelihood of preferred responses, undermining alignment

## Executive Summary
This paper identifies a critical failure mode in direct preference optimization (DPO) where models can unintentionally decrease the likelihood of preferred responses during training. The phenomenon, termed "likelihood displacement," occurs when preference pairs have similar embeddings, causing the model to shift probability mass from preferred to dispreferred responses that may have opposite meanings. Through theoretical analysis and experiments across multiple models and datasets, the authors demonstrate that this can catastrophically undermine alignment objectives, such as reducing refusal rates on unsafe prompts.

The authors introduce the Centered Hidden Embedding Similarity (CHES) score to identify problematic preference pairs and show that filtering high-CHES samples effectively prevents unintentional unalignment. While adding supervised fine-tuning terms provides some mitigation, it is less robust than the CHES-based filtering approach. The results highlight the importance of curating preference data with sufficiently distinct responses and demonstrate CHES as a practical tool for improving DPO alignment reliability.

## Method Summary
The paper analyzes direct preference optimization through both theoretical and empirical approaches. The theoretical analysis uses a linear approximation of the DPO loss and shows that preference pairs with similar hidden embeddings can cause the model to shift probability mass away from preferred responses. The authors derive that the likelihood displacement is proportional to the similarity between preference pair embeddings and inversely proportional to the variance of these embeddings. Empirically, they introduce the CHES score to quantify this similarity and demonstrate its effectiveness in identifying problematic samples across multiple models (including Llama-3-8B-Instruct) and datasets. The mitigation strategy involves filtering preference pairs with high CHES scores during training.

## Key Results
- Likelihood displacement can reduce Llama-3-8B-Instruct's refusal rate from 74.4% to 33.4% on unsafe prompts
- The CHES score effectively identifies preference pairs that cause unintentional unalignment
- Filtering high-CHES samples prevents likelihood displacement, while adding SFT terms provides less robust mitigation
- The phenomenon is driven by preference pairs with similar embeddings, particularly those expressing opposite meanings

## Why This Works (Mechanism)
Likelihood displacement occurs because DPO optimizes the log-ratio of probabilities between preferred and dispreferred responses. When these responses have similar embeddings, the optimization can inadvertently shift probability mass away from both responses toward other parts of the distribution. The DPO loss encourages the model to make the preferred response more likely relative to the dispreferred one, but if they are semantically similar, this can result in both becoming less likely overall as probability mass flows to more distinct alternatives. The CHES score captures this similarity in the hidden representation space, allowing identification of problematic pairs before they cause alignment failures.

## Foundational Learning

**Direct Preference Optimization (DPO)**: A method for aligning language models using preference data without explicit reward modeling. Why needed: DPO has become popular for RLHF alternatives but its failure modes were not well understood. Quick check: Verify you understand how DPO optimizes log-probability ratios between preference pairs.

**Likelihood Displacement**: The phenomenon where DPO training reduces the likelihood of preferred responses, potentially making them less probable than before training. Why needed: This counterintuitive failure mode can undermine alignment objectives. Quick check: Can you explain why making a response "more preferred relative to alternatives" might make it less likely overall?

**Centered Hidden Embedding Similarity (CHES)**: A metric that measures similarity between preference pair embeddings in hidden representation space. Why needed: Provides a practical tool for identifying problematic preference pairs before training. Quick check: Understand how CHES differs from raw embedding similarity and why centering matters.

## Architecture Onboarding

**Component Map**: Preference dataset -> CHES scoring -> Sample filtering -> DPO training -> Model evaluation

**Critical Path**: The key sequence is: (1) Compute CHES scores for all preference pairs, (2) Filter out high-CHES pairs, (3) Train DPO on remaining pairs, (4) Evaluate alignment metrics. The CHES computation must complete before DPO training begins.

**Design Tradeoffs**: The paper trades some preference data quantity for quality by filtering high-CHES pairs. While this may reduce the total amount of training data, it prevents catastrophic alignment failures. Alternative approaches like adding SFT terms preserve all data but provide less reliable mitigation.

**Failure Signatures**: The primary failure signature is a decrease in likelihood of preferred responses, which can manifest as reduced refusal rates on unsafe prompts, decreased task performance, or generation of contradictory responses. These failures often occur suddenly during training rather than gradually.

**3 First Experiments**: (1) Compute CHES scores on your preference dataset and examine the distribution; (2) Train DPO with and without high-CHES filtering on a small validation set; (3) Measure changes in preferred response likelihood during training to detect early signs of displacement.

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations

**Theoretical assumptions**: The analysis relies on linear approximations and assumes the LLM has learned a good prior representation, which may not hold for all models or preference distributions.

**Dataset specificity**: The phenomenon may be dataset-dependent, and the paper doesn't extensively explore how different preference dataset characteristics influence the severity of unintentional unalignment.

**Limited mitigation exploration**: While CHES filtering is shown to be effective, the paper doesn't deeply explore alternative mitigation strategies beyond supervised fine-tuning additions.

## Confidence

**High confidence**: The empirical observation of likelihood displacement reducing refusal rates from 74.4% to 33.4% is well-supported across multiple experiments and model sizes.

**Medium confidence**: The theoretical analysis explaining why similar embeddings cause likelihood displacement is logically sound but relies on simplifying assumptions that may not capture all real-world scenarios.

**Low confidence**: The claim that SFT addition provides less robust mitigation than CHES filtering is based on limited comparisons and doesn't explore the full space of potential mitigation strategies.

## Next Checks

1. **Cross-dataset generalization**: Test CHES score effectiveness across diverse preference datasets including multi-turn conversations, code generation, and factual QA to verify the universality of likelihood displacement.

2. **Scale and architecture effects**: Evaluate whether likelihood displacement becomes more or less severe as model scale increases beyond 8B parameters and across different architectures to understand if the phenomenon is architecture-dependent.

3. **Alternative preference formulations**: Experiment with different preference pair formulations (multiple reference responses, confidence-weighted preferences, or contextual embeddings) to determine if likelihood displacement can be mitigated through preference data design rather than post-hoc filtering.