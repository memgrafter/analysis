---
ver: rpa2
title: 'Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation'
arxiv_id: '2402.18919'
source_url: https://arxiv.org/abs/2402.18919
tags:
- images
- spurious
- parts
- group
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Decompose-and-Compose (DaC), a method for mitigating
  spurious correlations in image classification by leveraging the compositional nature
  of images. The key idea is to first identify the causal components of images using
  class activation maps from models trained with standard ERM, then intervene by combining
  images to create new counterfactual samples that balance underrepresented groups.
---

# Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation

## Quick Facts
- arXiv ID: 2402.18919
- Source URL: https://arxiv.org/abs/2402.18919
- Authors: Fahimeh Hosseini Noohdani; Parsa Hosseini; Aryan Yazdan Parast; Hamidreza Yaghoubi Araghi; Mahdieh Soleymani Baghshah
- Reference count: 40
- Primary result: DaC outperforms previous methods without group annotations on four benchmarks, achieving better worst-group accuracy through compositional interventions

## Executive Summary
This paper proposes Decompose-and-Compose (DaC), a method for mitigating spurious correlations in image classification by leveraging the compositional nature of images. The key idea is to first identify the causal components of images using class activation maps from models trained with standard ERM, then intervene by combining images to create new counterfactual samples that balance underrepresented groups. The method does not require group labels or knowledge of spurious features during training. Experiments on four benchmarks (Waterbirds, CelebA, Dominoes, Metashift) show that DaC outperforms previous methods without access to group annotations, achieving better worst-group accuracy. The approach is particularly effective when the model trained with ERM attends more to causal components.

## Method Summary
DaC works by first training an ERM model to identify which image components the model attends to, then using adaptive masking with xGradCAM to decompose images into causal and non-causal parts. The method combines samples from different classes using these masked components to create counterfactual data that breaks spurious correlations. The combined dataset, along with the original data, is then used to retrain the last layer of the model. The approach is flexible and can be applied to any dataset without requiring group annotations or prior knowledge of spurious features.

## Key Results
- DaC achieves significant improvements in worst-group accuracy across four benchmark datasets without requiring group annotations
- The method shows robustness to different types of spurious correlations, including background-object and attribute-label correlations
- DaC outperforms previous state-of-the-art methods that require group labels or prior knowledge of spurious features
- The approach is particularly effective when the ERM model attends more to causal components than non-causal components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models trained with standard ERM attend more to either causal or non-causal parts depending on which parts are more predictive across the entire dataset.
- Mechanism: The paper observes that the focus of ERM models shifts based on the predictive power of causal vs non-causal components. When non-causal parts are highly predictive shortcuts, models attend more to them; otherwise, they attend more to causal parts.
- Core assumption: The easiness of predicting labels from causal and non-causal parts across the whole dataset determines the model's attention pattern.
- Evidence anchors:
  - [abstract] "Based on our observations, models trained with ERM usually highly attend to either the causal components or the components having a high spurious correlation with the label"
  - [section] "Based on the easiness of inferring the label from the causal or non-causal parts across the whole dataset, the model usually attends to one of these more"
- Break condition: This mechanism breaks if the dataset contains multiple equally predictive causal and non-causal patterns, or if the model's attention is dominated by factors other than overall predictive power.

### Mechanism 2
- Claim: Adaptive masking using xGradCAM can identify the most predictive parts of an image for classification.
- Mechanism: By gradually masking out pixels with lowest attribution scores and monitoring loss changes, the method finds the optimal masking proportion that preserves predictive regions while removing non-predictive ones.
- Core assumption: The loss function's response to masking follows a predictable pattern where non-predictive parts can be removed without significant loss increase until the predictive core is reached.
- Evidence anchors:
  - [section] "we first define l(fθ(˜xp), y), in which l is the cross-entropy loss, and ˜xp is the new image obtained from x by masking out the portion p of pixels with the lowest attribution scores"
  - [section] "the elbow located at p∗ shows the optimal amount of masking for the input x"
- Break condition: This mechanism fails when the attribution scores don't accurately reflect predictive importance, or when the loss function doesn't show the expected elbow pattern due to model architecture or data characteristics.

### Mechanism 3
- Claim: Combining images from different classes using masked causal/non-causal parts creates effective counterfactual samples that balance underrepresented groups.
- Mechanism: By decomposing images into causal and non-causal components and recomposing them across different classes, the method generates new samples where non-causal parts don't match the label's spurious correlation pattern.
- Core assumption: The combined image will have the label of the causal component while the non-causal component breaks the spurious correlation.
- Evidence anchors:
  - [section] "By combining the causal and non-causal parts of two majority data-points from different labels, we make new datapoints from minority groups"
  - [section] "ˆx(i)combj = ψ(˜c(i), ˜s(j)), the non-causal part ˜s(j) does not have a spurious value corresponding to y(i), which makes this datapoint from the minority groups"
- Break condition: This mechanism breaks if the combination of causal and non-causal parts creates images that are unnatural or unclassifiable, or if the causal/non-causal decomposition is inaccurate.

## Foundational Learning

- Concept: Empirical Risk Minimization (ERM)
  - Why needed here: The paper's method builds upon ERM as the baseline training approach that suffers from spurious correlation problems
  - Quick check question: What is the primary limitation of ERM when dealing with distribution shifts caused by spurious correlations?

- Concept: Class Activation Maps (CAM) and attribution methods
  - Why needed here: The method uses xGradCAM to identify which parts of images the model attends to, which is crucial for decomposing images into causal and non-causal components
  - Quick check question: How does xGradCAM differ from standard Grad-CAM in terms of what it measures?

- Concept: Spurious correlation and causal inference
  - Why needed here: Understanding spurious correlation is fundamental to why the method works and how it addresses the problem through compositional interventions
  - Quick check question: What distinguishes a spurious correlation from a genuine causal relationship in the context of image classification?

## Architecture Onboarding

- Component map:
  Base ERM model (ResNet-50) -> Attribution score computation (xGradCAM) -> Adaptive masking module -> Image combination module -> Retraining pipeline for last layer

- Critical path:
  1. Train base ERM model
  2. Compute attribution scores for low-loss samples
  3. Apply adaptive masking to find optimal causal/non-causal separation
  4. Combine samples from different classes using masked components
  5. Retrain last layer with combined dataset

- Design tradeoffs:
  - Choosing between causal and non-causal attention assumptions based on validation performance
  - Balancing between α (combination weight) and q (proportion of selected samples)
  - Trade-off between computational cost of attribution computation and model performance

- Failure signatures:
  - Poor worst-group accuracy improvement despite successful combination
  - Combined images that are unclassifiable or unnatural
  - Model overfitting to combined data rather than learning robust features

- First 3 experiments:
  1. Test adaptive masking on a simple dataset where ground truth causal parts are known
  2. Verify that combined images from different classes actually break spurious correlations
  3. Compare worst-group accuracy improvement with different α and q values on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Decompose-and-Compose (DaC) vary with different choices of the causal flag hyperparameter, and can this hyperparameter be optimized automatically based on model behavior?
- Basis in paper: [explicit] The paper states that the causalflag hyperparameter determines whether the non-causal or causal assumption is used for identifying causal parts of images, and that this flag is set based on which assumption yields better results on the validation data.
- Why unresolved: The paper does not explore the performance impact of different choices of the causal flag hyperparameter or discuss methods for automatically determining the optimal setting.
- What evidence would resolve it: Experiments comparing DaC performance with different causalflag settings, and analysis of whether model behavior (e.g., attention patterns) can predict the optimal causal flag choice.

### Open Question 2
- Question: Can the adaptive masking algorithm be improved to better distinguish causal and non-causal image components, particularly in cases where the model's attention is split between them?
- Basis in paper: [inferred] The paper relies on xGradCAM scores to identify image components, but acknowledges that models trained with ERM may attend to both causal and non-causal parts depending on the dataset. The adaptive masking algorithm finds an elbow point in the loss curve to determine the optimal mask, but this may not always perfectly separate causal and non-causal components.
- Why unresolved: The paper does not evaluate the effectiveness of the adaptive masking algorithm in different scenarios or propose alternative methods for distinguishing causal and non-causal components.
- What evidence would resolve it: Comparative analysis of DaC performance using different masking algorithms, and evaluation of how well each method separates causal and non-causal components in various datasets.

### Open Question 3
- Question: How does the proportion of selected data (q) for combining impact DaC's performance, and is there an optimal strategy for selecting this hyperparameter that does not require validation set tuning?
- Basis in paper: [explicit] The paper discusses the effect of the proportion q of selected samples for combining, noting that increasing q can lead to more samples with higher loss being used, which may result in wrongly labeled combined images. It mentions that the best performance is achieved when q is chosen through hyperparameter tuning.
- Why unresolved: The paper does not explore the relationship between q and DaC performance in detail or propose methods for automatically selecting q without relying on validation set performance.
- What evidence would resolve it: Experiments analyzing DaC performance across a range of q values, and development of a method for estimating the optimal q based on model behavior or dataset characteristics.

## Limitations

- The effectiveness of DaC heavily depends on the accuracy of the causal/non-causal decomposition, which relies on the quality of xGradCAM attribution scores
- The method assumes that the ERM model's attention pattern is stable enough to identify meaningful causal components, but this may not hold for all datasets or model architectures
- The paper does not extensively explore the sensitivity of results to hyperparameter choices (α, q) or provide detailed ablation studies on the impact of different masking strategies

## Confidence

- **High confidence**: The general compositional approach of combining causal and non-causal parts to break spurious correlations is theoretically sound and supported by the empirical results across four benchmarks
- **Medium confidence**: The adaptive masking mechanism using xGradCAM to identify optimal decomposition points shows promise but requires more rigorous validation
- **Medium confidence**: The claim that DaC outperforms previous methods without group annotations is supported by the experimental results, though direct comparisons with some state-of-the-art methods are limited

## Next Checks

1. **Ablation study on masking strategy**: Test different attribution methods (e.g., Grad-CAM, Integrated Gradients) and masking algorithms to quantify their impact on worst-group accuracy

2. **Generalization to unseen spurious correlations**: Evaluate DaC on datasets with multiple types of spurious correlations simultaneously to test its robustness beyond the single-spurious-feature scenarios studied

3. **Cross-dataset transfer validation**: Train DaC on one dataset and evaluate its performance on another dataset with different spurious correlation patterns to assess the method's generalizability