---
ver: rpa2
title: 'MTEB-French: Resources for French Sentence Embedding Evaluation and Analysis'
arxiv_id: '2405.20468'
source_url: https://arxiv.org/abs/2405.20468
tags:
- french
- datasets
- dataset
- multilingual
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MTEB-French, the first large-scale benchmark
  for evaluating French sentence embeddings. The authors collected 15 existing datasets
  and created 3 new quality-checked datasets, covering 8 task categories.
---

# MTEB-French: Resources for French Sentence Embedding Evaluation and Analysis

## Quick Facts
- arXiv ID: 2405.20468
- Source URL: https://arxiv.org/abs/2405.20468
- Authors: Mathieu Ciancone; Imene Kerboua; Marion Schaeffer; Wissam Siblini
- Reference count: 40
- Primary result: First large-scale benchmark for evaluating French sentence embeddings across 26 tasks

## Executive Summary
This paper presents MTEB-French, the first comprehensive benchmark for evaluating French sentence embeddings. The authors collected 15 existing datasets and created 3 new quality-checked datasets, covering 8 task categories. They evaluated 51 models, including prominent French and multilingual models. Results show that no single model is best across all tasks, but large multilingual models pre-trained on sentence similarity perform exceptionally well. OpenAI's text-embedding-3-large model achieves the best average performance, but other models excel in specific tasks or have advantages like being open-source or having smaller embedding dimensions.

## Method Summary
The benchmark evaluates 51 embedding models across 26 tasks from 18 datasets covering 8 task categories: Classification, Clustering, Retrieval, Reranking, Bitext Mining, Semantic Textual Similarity, Summarization, and Pair Classification. The evaluation uses specific metrics for each task type, including accuracy for classification, F1 score for bitext mining, NDCG@k for retrieval, and Spearman correlation for STS and summarization. The benchmark includes both pre-trained and fine-tuned models, covering various architectures, dimensions (384-4096), sequence lengths (128-32768), and parameter counts (20M-7B).

## Key Results
- No single model achieves best performance across all tasks
- Large multilingual models pre-trained on sentence similarity perform exceptionally well
- OpenAI's text-embedding-3-large achieves the best average performance
- Model performance correlates with embedding dimension and parameter count
- Fine-tuned models outperform pre-trained models on specific tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned models outperform pre-trained models on specific tasks
- Mechanism: Fine-tuning adapts a model's parameters to the target task distribution, allowing it to capture task-specific patterns and relationships that are not present in the original pre-training data
- Core assumption: The fine-tuning dataset is representative of the target task and contains sufficient information for the model to learn the necessary patterns
- Evidence anchors:
  - [abstract]: "Results show that no single model is best across all tasks, but large multilingual models pre-trained on sentence similarity perform exceptionally well"
  - [section 4, Q2]: "As expected, the score most strongly correlates with whether the evaluated models were trained on a sentence similarity task"
  - [corpus]: No direct evidence; weak support
- Break condition: If the fine-tuning dataset is too small, unrepresentative, or noisy, the model may not learn effectively and could even overfit, leading to worse performance than the pre-trained model

### Mechanism 2
- Claim: Larger models generally perform better than smaller models
- Mechanism: Larger models have more parameters, which allows them to capture more complex relationships and patterns in the data
- Core assumption: The tasks in the benchmark require a sufficient level of complexity that can only be captured by larger models
- Evidence anchors:
  - [abstract]: "Results show that no single model is best across all tasks, but large multilingual models pre-trained on sentence similarity perform exceptionally well"
  - [section 4, Q2]: "Furthermore, we observe a performance correlation with the embedding dimension and the model's number of parameters, which are often correlated themselves"
  - [corpus]: No direct evidence; weak support
- Break condition: If the tasks are relatively simple and do not require a high level of model capacity, smaller models may perform just as well or even better than larger models due to their lower computational cost and faster inference time

### Mechanism 3
- Claim: Multilingual models can effectively handle French text
- Mechanism: Multilingual models are trained on a diverse range of languages, including French, allowing them to develop representations that can effectively capture the semantics and structure of the French language
- Core assumption: The multilingual models included in the benchmark have been trained on a sufficient amount of French text to develop effective representations for the language
- Evidence anchors:
  - [abstract]: "Results show that no single model is best across all tasks, but large multilingual models pre-trained on sentence similarity perform exceptionally well"
  - [section 4, Q3]: "It is surprising to note the absence of a clear correlation between the language the model is trained on and its performance on French"
  - [corpus]: No direct evidence; weak support
- Break condition: If the multilingual models have not been trained on enough French text, or if the French text in the training data is of low quality or unrepresentative of the target domain, the models may not perform well on French tasks

## Foundational Learning

- Concept: Sentence embeddings
  - Why needed here: The benchmark evaluates models based on their ability to generate high-quality sentence embeddings for French text
  - Quick check question: What is a sentence embedding and how is it different from a word embedding?

- Concept: Fine-tuning
  - Why needed here: The benchmark includes both pre-trained and fine-tuned models, and understanding the difference between them is crucial for interpreting the results
  - Quick check question: What is fine-tuning and how does it differ from pre-training?

- Concept: Task-specific evaluation
  - Why needed here: The benchmark evaluates models on a variety of tasks, and understanding the nuances of each task is important for interpreting the results
  - Quick check question: What are some common NLP tasks and how are they typically evaluated?

## Architecture Onboarding

- Component map: Data collection and preprocessing -> Model selection and loading -> Task-specific evaluation -> Result aggregation and analysis
- Critical path: Load and preprocess the datasets -> Load and initialize the models -> Evaluate each model on each task -> Aggregate the results and compute metrics -> Analyze the results and draw conclusions
- Design tradeoffs:
  - Model selection: Balancing model performance with computational cost and licensing restrictions
  - Dataset selection: Ensuring a diverse and representative set of tasks and domains
  - Evaluation metrics: Choosing appropriate metrics for each task type
- Failure signatures:
  - Low performance on specific tasks: May indicate a lack of model capacity or task-specific fine-tuning
  - High variance in performance across tasks: May indicate a lack of model generalization or domain shift
  - Slow evaluation time: May indicate a need for model optimization or hardware upgrades
- First 3 experiments:
  1. Evaluate a small set of models on a single task to validate the evaluation pipeline
  2. Evaluate a larger set of models on a subset of tasks to identify top performers
  3. Evaluate the top-performing models on all tasks to generate the final benchmark results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific French language models (e.g., sentence-camembert, Solon-embeddings) perform compared to multilingual models on non-similarity tasks like classification or clustering?
- Basis in paper: [explicit] The authors note that some French models stand out among multilingual models and perform well on the benchmark, but they don't provide detailed breakdowns of performance by task type or model architecture
- Why unresolved: The paper focuses on overall benchmark performance and correlations between model characteristics and performance, but lacks a detailed analysis of French vs. multilingual model performance on specific task types
- What evidence would resolve it: A detailed comparison of French vs. multilingual model performance on each task type (classification, clustering, retrieval, etc.) using appropriate metrics

### Open Question 2
- Question: What is the impact of sequence length on model performance for different task types, particularly for tasks involving longer documents?
- Basis in paper: [inferred] The authors note that sequence length is less correlated with performance than other characteristics, but this may be due to many datasets containing relatively short texts. They suggest that sequence length is important for generative tasks with LLMs
- Why unresolved: The paper doesn't explore the relationship between sequence length and performance in depth, particularly for tasks involving longer documents or generative tasks
- What evidence would resolve it: A detailed analysis of the relationship between sequence length and performance for different task types, including tasks with longer documents and generative tasks

### Open Question 3
- Question: How does the performance of models trained on machine-translated French data compare to models trained on natively French data?
- Basis in paper: [explicit] The authors note that a significant portion of the French data used to train selected French models comes from English datasets that have been machine translated. They suggest that this may cause reduced final performance
- Why unresolved: The paper doesn't provide a direct comparison of models trained on machine-translated vs. natively French data
- What evidence would resolve it: A comparison of models trained on machine-translated French data vs. models trained on natively French data, controlling for other factors like model architecture and training techniques

## Limitations
- Focuses exclusively on sentence-level embeddings, excluding token-level or document-level tasks
- Relies on static evaluation data without considering temporal dynamics or domain shift
- Uses a single similarity metric (cosine similarity) across all tasks, which may not be optimal for every task type
- Newly created datasets (Syntec, HAL, SummEvalFr) may have limited size or scope compared to existing datasets

## Confidence
**High Confidence Claims:**
- The benchmark construction methodology is sound with clear dataset selection criteria
- The finding that no single model dominates across all tasks is well-supported
- The observation that large multilingual models pre-trained on sentence similarity perform well is consistent with results

**Medium Confidence Claims:**
- The correlation between model characteristics and performance - while observed, causal relationships require further investigation
- The effectiveness of newly created datasets - these have limited prior validation
- The conclusion about multilingual models performing well without clear correlation to training language - this surprising finding needs more investigation

**Low Confidence Claims:**
- Direct comparisons with English MTEB performance are not provided
- The evaluation of proprietary models may not be reproducible due to API limitations
- The interpretation of why certain models perform better on specific tasks is largely speculative

## Next Checks
1. Reproduce Reranking Task Construction: Implement the BM25-based irrelevant document addition for Syntec and Alloprof datasets using the same parameters to verify the task setup and ensure reproducibility of results

2. Cross-Lingual Transfer Analysis: Conduct experiments comparing model performance on French versus English versions of the same tasks (where available) to better understand whether multilingual models are truly language-agnostic or if they still exhibit language-specific advantages

3. Temporal Validation: Re-evaluate the benchmark using models and datasets from different time periods to assess the stability of conclusions and identify whether performance trends are consistent over time