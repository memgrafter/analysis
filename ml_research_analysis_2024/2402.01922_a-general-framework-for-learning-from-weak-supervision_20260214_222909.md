---
ver: rpa2
title: A General Framework for Learning from Weak Supervision
arxiv_id: '2402.01922'
source_url: https://arxiv.org/abs/2402.01922
tags:
- learning
- supervision
- weak
- label
- pairwise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a general framework (GLWS) for learning from
  various forms of weak supervision, including instance partial labels, aggregate
  statistics, pairwise observations, and unlabeled data. The core idea is to formulate
  weak supervision as a Non-deterministic Finite Automaton (NFA) and use a forward-backward
  algorithm to efficiently compute the Expectation-Maximization (EM) objective in
  linear time.
---

# A General Framework for Learning from Weak Supervision

## Quick Facts
- arXiv ID: 2402.01922
- Source URL: https://arxiv.org/abs/2402.01922
- Reference count: 40
- Primary result: Achieves state-of-the-art performance across 11 weak supervision settings, improving ImageNet-100 accuracy by 1.28% with partial labels

## Executive Summary
This paper introduces a General Framework for Learning from Weak Supervision (GLWS) that unifies various forms of weak supervision through a Non-deterministic Finite Automaton (NFA) representation. The key innovation is using NFA with a forward-backward algorithm to efficiently compute the Expectation-Maximization (EM) objective in linear time, overcoming the exponential or factorial complexity of existing methods. The framework handles instance partial labels, aggregate statistics, pairwise observations, and unlabeled data, achieving state-of-the-art results across 11 weak supervision settings on multiple datasets.

## Method Summary
GLWS formulates weak supervision as an EM problem where the E-step computes expected log-likelihood over possible labelings. The key technical contribution is representing weak supervision constraints as an NFA and using forward-backward dynamic programming on the resulting trellis to compute EM objectives in linear time. For multi-class problems, the framework decomposes each class into a binary positive/negative problem to avoid exponential state space growth. The method is evaluated on CIFAR-10/100, STL-10, ImageNet-100, MNIST, and F-MNIST with synthetic weak supervision.

## Key Results
- Consistently achieves state-of-the-art performance across 11 weak supervision settings
- Improves ImageNet-100 accuracy by 1.28% over previous best method with partial labels
- Reduces EM computation time complexity from quadratic/factorial to linear scale
- Successfully handles instance partial labels, aggregate statistics, pairwise observations, and unlabeled data

## Why This Works (Mechanism)

### Mechanism 1: Linear-time EM via NFA and Forward-Backward
The NFA-based forward-backward algorithm reduces EM computation from exponential or factorial to linear time by implicitly representing all valid labelings consistent with weak supervision. The forward-backward algorithm computes joint probabilities over the trellis formed by the prediction sequence and NFA using dynamic programming.

### Mechanism 2: EM Unification of Unsupervised and Supervised Objectives
The EM formulation generalizes existing methods by splitting into unsupervised consistency (LU) and supervised fulfillment (LS) objectives. This captures both alignment with weak supervision distributions and constraint satisfaction through a unified framework.

### Mechanism 3: Binary Decomposition for Multi-Class Problems
Multi-class extension is achieved by treating each class as a separate binary classification problem using BCE loss, avoiding exponential state space growth that would occur with direct multi-class NFA modeling.

## Foundational Learning

- **Expectation-Maximization (EM) algorithm**: Needed to handle latent variable problem where true labels are unknown but weak supervision is available. Quick check: In EM for weak supervision, what are the two terms in the E-step objective, and what does each encourage?

- **Non-deterministic Finite Automaton (NFA)**: Needed to model the set of all valid label sequences consistent with weak supervision constraints in a finite representation. Quick check: How does an NFA differ from a DFA in terms of state transitions, and why is this difference useful for modeling weak supervision?

- **Forward-backward algorithm on trellis**: Needed to efficiently compute forward and backward probabilities over the trellis formed by prediction sequence and NFA, enabling linear-time EM computation. Quick check: What is the computational complexity of the forward-backward algorithm on an NFA with |Q| states and sequence length L?

## Architecture Onboarding

- **Component map**: NFA modeler -> Predictor -> Forward-backward engine -> Loss combiner -> Optimizer

- **Critical path**: Input batch → Predictor → log_probs → NFA modeler → NFA structure → Forward-backward engine (log-space) → EM targets, supervised preds → Loss combiner → LU + LS → Optimizer → θ update

- **Design tradeoffs**: NFA expressiveness vs. state space size (more complex W → larger NFA → more computation); Binary decomposition vs. multi-class modeling (simpler computation but potential loss of inter-class constraints); Log-space computation vs. numerical stability (prevents underflow but adds log-sum-exp overhead)

- **Failure signatures**: Numerical underflow in forward-backward (check log-space implementation and rescaling); Slow convergence (verify NFA correctly models W and predictions are meaningful); Poor performance (check if binary decomposition loses critical constraints or if W is too weak)

- **First 3 experiments**: Binary classification with simple partial labels (CIFAR-10, 50% partial ratio) to verify NFA modeling and forward-backward correctness; Multi-class partial labels using binary decomposition to verify per-class training works; Multiple instance learning with small bag sizes to verify aggregate observation handling and runtime scaling

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the GLWS framework perform on long sequence data compared to datasets with shorter sequences?
- **Basis in paper**: The paper mentions that some methods assume conditional independence of instances for aggregate observations, making them unsuitable for handling long sequence data prevalent in practical scenarios.
- **Why unresolved**: The paper evaluates GLWS on datasets with relatively short sequences (e.g., CIFAR-10, CIFAR-100). It would be valuable to see how the method scales to datasets with significantly longer sequences.
- **What evidence would resolve it**: Experiments on datasets with longer sequences (e.g., video data, time series) comparing GLWS's performance and runtime to other methods would provide insights.

### Open Question 2
- **Question**: How does the GLWS framework handle noisy weak supervision?
- **Basis in paper**: The paper mentions that the framework is "adaptable to noisy weak supervision ˆW with an inherent learnable noise model P (W | ˆW ; θ) in the EM," but this is left for future work.
- **Why unresolved**: The paper does not provide any experiments or analysis on the performance of GLWS with noisy weak supervision. It is unclear how the framework handles errors or inconsistencies in the weak supervision signals.
- **What evidence would resolve it**: Experiments on datasets with artificially added noise to the weak supervision signals, comparing GLWS's performance to other methods, would demonstrate its robustness to noise.

### Open Question 3
- **Question**: How does the GLWS framework compare to other methods in terms of computational efficiency on large-scale datasets?
- **Basis in paper**: The paper claims that GLWS reduces time complexity from quadratic or factorial to linear scale compared to existing solutions. However, the paper does not provide a detailed comparison of runtime with other methods on large-scale datasets.
- **Why unresolved**: While the paper mentions the improved computational efficiency of GLWS, it does not provide a comprehensive comparison with other methods in terms of runtime on large-scale datasets. This information would be valuable for understanding the practical applicability of the framework.
- **What evidence would resolve it**: Runtime comparisons of GLWS with other methods on large-scale datasets (e.g., ImageNet, COCO) would provide insights into its computational efficiency and scalability.

## Limitations

- Binary decomposition approach may lose expressiveness for supervision types with inter-class dependencies
- Exact numerical stability tricks beyond log-sum-exp are not fully specified
- Scalability claims depend on NFA size but the relationship between supervision complexity and state space growth is not quantified

## Confidence

- Mechanism 1 (linear-time EM): High confidence - well-supported by algorithmic description and complexity analysis
- Mechanism 2 (EM generalization): High confidence - the mathematical formulation is explicit and follows standard EM theory
- Mechanism 3 (binary decomposition): Medium confidence - the approach is clearly stated but potential expressiveness loss is acknowledged without empirical quantification

## Next Checks

1. Verify NFA modeling correctness by testing on synthetic weak supervision where ground truth label distributions are known
2. Benchmark runtime scaling on sequences of increasing length to confirm linear complexity empirically
3. Compare binary decomposition approach against multi-class NFA on supervision types with known inter-class dependencies to measure expressiveness loss