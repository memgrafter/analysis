---
ver: rpa2
title: On the Use of Large Language Models to Generate Capability Ontologies
arxiv_id: '2404.17524'
source_url: https://arxiv.org/abs/2404.17524
tags:
- ontology
- capability
- llms
- ontologies
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using Large Language Models (LLMs) to generate
  capability ontologies, which model machine functionalities. A study was conducted
  with two LLMs (GPT-4 Turbo and Claude 3) and three prompting techniques (zero-shot,
  one-shot, few-shot) to generate seven capability ontologies of varying complexity.
---

# On the Use of Large Language Models to Generate Capability Ontologies

## Quick Facts
- arXiv ID: 2404.17524
- Source URL: https://arxiv.org/abs/2404.17524
- Authors: Luis Miguel Vieira da Silva; Aljosha Köcher; Felix Gehlhoff; Alexander Fay
- Reference count: 26
- Key outcome: LLM-generated capability ontologies achieve near-perfect completeness with few-shot prompting

## Executive Summary
This paper investigates using Large Language Models (LLMs) to generate capability ontologies, which model machine functionalities in manufacturing contexts. A comprehensive study was conducted using two LLMs (GPT-4 Turbo and Claude 3) with three prompting techniques (zero-shot, one-shot, few-shot) to generate seven capability ontologies of varying complexity. The research demonstrates that LLMs can significantly reduce the effort and expertise required for ontology creation while maintaining high quality, with few-shot prompting achieving near-perfect results for complex capabilities like drilling, transport, and assembly.

## Method Summary
The study employed a semi-automated validation approach combining RDF syntax checking, OWL reasoning, and SHACL constraints to evaluate the quality of generated ontologies. Seven capability ontologies were created using two different LLMs (GPT-4 Turbo and Claude 3) with three prompting techniques (zero-shot, one-shot, few-shot). The CaSk ontology context was provided as background knowledge, and temperature was set to 0 for deterministic outputs. Generated ontologies were validated through a pipeline that included syntax validation in Protege, OWL reasoning for consistency checking, and SHACL constraints to detect hallucinations and incompleteness.

## Key Results
- Few-shot prompting achieved completeness scores of 0.96-1.00 for complex capabilities like drilling, transport, and assembly
- Claude 3 outperformed GPT-4 Turbo in generating higher-quality ontologies with fewer errors
- SHACL constraints successfully detected both hallucinations and missing required elements in generated ontologies
- Temperature=0 setting ensured deterministic and reproducible ontology generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting dramatically improves ontology completeness by providing concrete modeling examples that LLMs can learn from.
- Mechanism: Few-shot examples demonstrate both the syntax and structure of capability ontologies, including complex constraints and mathematical relations. The LLM uses these examples to perform in-context learning, adapting its output to match the demonstrated patterns.
- Core assumption: LLMs can generalize from a small number of examples to create novel capability ontologies with similar structural properties.
- Evidence anchors:
  - [abstract] "Results show that even complex capabilities can be modeled with high accuracy, with few-shot prompting achieving near-perfect results."
  - [section] "The improvement from zero-shot to one-shot prompts is clearly visible... The few-shot results — especially those generated by Claude — are close to perfect."
  - [corpus] "Average neighbor FMR=0.47" indicates moderate correlation with related ontology learning work, supporting the mechanism's validity.
- Break condition: If the examples provided are too dissimilar from the target capability, the LLM may fail to generalize correctly, leading to poor completeness scores.

### Mechanism 2
- Claim: SHACL constraints provide a robust method for validating ontology completeness and detecting hallucinations.
- Mechanism: SHACL shapes define both mandatory properties and closed shapes that restrict additional properties. This creates a validation framework that can automatically detect missing required elements and unexpected additions.
- Core assumption: The SHACL constraints comprehensively cover all required elements of a capability ontology and can distinguish between valid extensions and hallucinations.
- Evidence anchors:
  - [abstract] "To analyze the quality of the generated ontologies, a semi-automated approach based on RDF syntax checking, OWL reasoning, and SHACL constraints is used."
  - [section] "Using closed SHACL shapes, one can restrict properties that are to be applied to individuals of a certain class. Additional properties are reported as errors."
  - [corpus] Weak evidence as no direct mention of SHACL validation in corpus neighbors.
- Break condition: If SHACL shapes are incomplete or incorrectly defined, the validation may miss actual errors or falsely flag valid extensions.

### Mechanism 3
- Claim: Temperature=0 ensures deterministic outputs, making ontology generation reproducible and reliable for production use.
- Mechanism: Setting temperature to 0 removes randomness from the LLM's sampling process, forcing it to select the most probable next token. This eliminates variability between runs with the same input.
- Core assumption: Determinism is more valuable than creativity for ontology generation, where consistency and accuracy are paramount.
- Evidence anchors:
  - [section] "In this paper, an LLM is intended to provide reliable solutions for the creation of capability ontologies, so that the parameter temperature is set to 0 in order to obtain deterministic solutions."
  - [corpus] No direct evidence in corpus neighbors about temperature settings.
- Break condition: If the most probable token selection consistently produces incorrect outputs, the deterministic approach may perform worse than a slightly randomized approach.

## Foundational Learning

- Concept: OWL ontology structure and semantics
  - Why needed here: Understanding classes, properties, individuals, and restrictions is essential for interpreting LLM outputs and creating proper SHACL constraints.
  - Quick check question: What is the difference between a TBox and an ABox in OWL ontologies?

- Concept: SHACL validation principles
  - Why needed here: Engineers need to understand how to create and interpret SHACL shapes for validating ontology completeness and detecting hallucinations.
  - Quick check question: How do closed shapes in SHACL help prevent hallucinations in generated ontologies?

- Concept: Prompt engineering techniques
  - Why needed here: Different prompting strategies (zero-shot, one-shot, few-shot) significantly impact output quality and understanding these differences is crucial for effective use.
  - Quick check question: What is the key difference between one-shot and few-shot prompting in terms of context window usage?

## Architecture Onboarding

- Component map:
  Natural language task description → LLM (GPT-4 Turbo or Claude 3) → Generated ontology → Validation pipeline (syntax check → OWL reasoning → SHACL validation) → Results analysis

- Critical path:
  1. Generate ontology using selected LLM and prompting technique
  2. Perform syntax validation in Protege
  3. Run OWL reasoning to check for inconsistencies
  4. Apply SHACL constraints to detect hallucinations and incompleteness
  5. Manually review and correct any remaining issues

- Design tradeoffs:
  - Cost vs. quality: GPT-4 Turbo is cheaper per prompt but Claude 3 produces slightly better results
  - Token usage vs. context richness: Including full CaSk ontology and examples consumes significant tokens but improves output quality
  - Automation vs. accuracy: Full automation is possible but manual review still catches subtle errors

- Failure signatures:
  - Missing import statements (detected by syntax check)
  - Inconsistent modeling of disjoint classes (detected by OWL reasoning)
  - Unexpected model elements not present in task description (detected by SHACL)
  - Incorrect mathematical constraints in OpenMath expressions (detected by manual review)

- First 3 experiments:
  1. Generate the parity capability using zero-shot prompting with both LLMs to establish baseline performance
  2. Generate the transport capability using few-shot prompting to test complex resource relationships
  3. Generate the mixing capability using one-shot prompting to evaluate constraint modeling capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Limited scope with only two LLMs and seven capability ontologies tested, restricting generalizability
- Manual review process introduces potential human bias and inconsistency in error classification
- SHACL validation approach may not capture all semantic errors or domain-specific nuances
- Results may not generalize to other ontology types beyond manufacturing capabilities

## Confidence

**High Confidence**: The comparative effectiveness of few-shot prompting over zero-shot and one-shot methods is well-supported by quantitative results (completeness scores of 0.96-1.00 for Claude 3 with few-shot prompting). The experimental methodology using RDF syntax checking, OWL reasoning, and SHACL constraints provides a robust framework for quality assessment.

**Medium Confidence**: The claim that LLMs can significantly reduce effort and expertise required for ontology creation is supported but relies on assumptions about what constitutes "significant" reduction. The study demonstrates high-quality outputs but doesn't measure the actual time or expertise savings compared to manual ontology development.

**Low Confidence**: The generalizability of results to other domains or ontology types beyond manufacturing capabilities remains uncertain due to the limited scope of the study.

## Next Checks

1. **Cross-Domain Validation**: Generate capability ontologies for non-manufacturing domains (e.g., healthcare, finance, or education) using the same methodology to test generalizability of results across different knowledge domains.

2. **Long-Term Stability Testing**: Run the same generation tasks multiple times over several months to verify that the deterministic outputs remain consistent and that the LLMs don't experience concept drift or capability changes that could affect ontology quality.

3. **Expert Validation Study**: Conduct a blind study where ontology experts evaluate the LLM-generated ontologies against manually created ones without knowing their source, measuring qualitative aspects like semantic appropriateness and domain alignment that automated validation might miss.