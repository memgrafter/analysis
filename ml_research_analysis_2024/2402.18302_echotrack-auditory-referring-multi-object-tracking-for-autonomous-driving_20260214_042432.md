---
ver: rpa2
title: 'EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving'
arxiv_id: '2402.18302'
source_url: https://arxiv.org/abs/2402.18302
tags:
- tracking
- audio
- ar-mot
- echotrack
- referring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of Auditory Referring Multi-Object
  Tracking (AR-MOT), which aims to dynamically track specific objects in a video sequence
  based on audio expressions, addressing the limitations of text-based methods in
  autonomous driving scenarios. To tackle this challenge, the authors propose EchoTrack,
  an end-to-end AR-MOT framework that leverages a dual-stream vision transformer architecture.
---

# EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving

## Quick Facts
- arXiv ID: 2402.18302
- Source URL: https://arxiv.org/abs/2402.18302
- Authors: Jiacheng Lin; Jiajun Chen; Kunyu Peng; Xuan He; Zhiyong Li; Rainer Stiefelhagen; Kailun Yang
- Reference count: 40
- Introduces EchoTrack, an end-to-end AR-MOT framework achieving significant improvements in tracking performance compared to state-of-the-art methods

## Executive Summary
This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions, addressing the limitations of text-based methods in autonomous driving scenarios. The authors propose EchoTrack, an end-to-end framework that leverages a dual-stream vision transformer architecture with frequency-domain cross-attention fusion and contrastive tracking learning. The method establishes the first large-scale AR-MOT benchmarks (Echo-KITTI, Echo-KITTI+, Echo-BDD) and demonstrates significant performance improvements over existing MOT methods.

## Method Summary
EchoTrack is an end-to-end AR-MOT framework that uses frozen HuBERT for audio encoding and ResNet50 for visual feature extraction. The dual-stream architecture fuses these modalities through a Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM) that transforms features between spatiotemporal and frequency domains. A deformable DETR decoder with frame propagation generates tracking predictions, while an Audio-visual Contrastive Tracking Learning (ACTL) regime maintains semantic alignment between audio expressions and visual trajectories throughout the sequence.

## Key Results
- EchoTrack achieves HOTA scores of 65.7 on Echo-KITTI, 68.2 on Echo-KITTI+, and 61.4 on Echo-BDD
- Outperforms state-of-the-art methods (MOTR, MOTR++) by 3.2% and 2.9% on Echo-KITTI respectively
- Shows consistent improvements across DetA (72.8), AssA (59.2), and MOTA (71.3) metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM) enhances tracking by preserving frequency-domain cues that are lost in purely spatiotemporal fusion.
- Mechanism: Bi-FCFM first fuses audio and visual features in the spatiotemporal domain, then transforms them into the frequency domain via FFT. It applies adaptive Gaussian filtering to selectively retain low- and high-frequency components before converting back to the spatiotemporal domain. This allows the tracker to exploit audio cues that carry semantic information in the frequency spectrum.
- Core assumption: Frequency-domain features contain discriminative information for associating audio expressions with visual objects that spatiotemporal features alone cannot fully capture.
- Evidence anchors:
  - [abstract]: "The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from both frequency- and spatiotemporal domains."
  - [section]: "Recognizing this, we design the Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM) to facilitate transformer-based information aggregation, considering that frequency information contains essential domain cues of the audio data, which provide valuable references."
  - [corpus]: Weak or missing; no directly comparable methods in corpus.
- Break condition: If the adaptive Gaussian filter fails to preserve critical frequency cues or introduces noise, tracking performance degrades.

### Mechanism 2
- Claim: The Audio-visual Contrastive Tracking Learning (ACTL) regime improves long-range tracking by maintaining homogeneous semantic alignment between audio expressions and visual trajectories.
- Mechanism: ACTL projects audio and trajectory queries into a joint embedding space, computes a similarity matrix, and applies focal loss to push non-matching pairs apart while pulling matches together. This enforces consistent semantic alignment throughout the video sequence, reducing drift.
- Core assumption: There exists a homogeneous semantic representation between audio descriptions and visual objects that can be learned via contrastive learning.
- Evidence anchors:
  - [abstract]: "we propose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract homogeneous semantic features between expressions and visual objects by learning homogeneous features between different audio and video objects effectively."
  - [section]: "ACTL explicitly promotes interactions between audio and corresponding visual trajectory features."
  - [corpus]: Weak or missing; no directly comparable contrastive tracking methods in corpus.
- Break condition: If the contrastive loss overwhelms other learning signals, it may hinder detection or localization accuracy.

### Mechanism 3
- Claim: Integrating HuBERT audio embeddings with frozen ResNet50 visual features provides a robust multimodal representation for tracking.
- Mechanism: HuBERT encodes raw audio into contextualized embeddings; ResNet50 extracts visual features. The two streams are fused via Bi-FCFM and decoded by a deformable DETR tracker with frame propagation. The frozen audio encoder ensures stable audio representations across diverse speakers and noise conditions.
- Core assumption: Pre-trained frozen encoders (HuBERT, ResNet50) provide sufficiently discriminative features for the downstream AR-MOT task without fine-tuning.
- Evidence anchors:
  - [abstract]: "each V is associated with a corresponding audio expression A = {Ai}m i=1, with Ai indicating the feature of the i-th audio encoded using frozen HuBERT-Base."
  - [section]: "We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers."
  - [corpus]: Weak or missing; no directly comparable frozen-encoder approaches in corpus.
- Break condition: If the frozen encoders' feature distributions shift significantly in AR-MOT scenarios, performance may suffer.

## Foundational Learning

- Concept: Transformer-based multimodal fusion
  - Why needed here: EchoTrack relies on cross-attention between audio and visual streams to localize objects described by speech; understanding self-attention and cross-attention mechanisms is essential for implementing Bi-FCFM.
  - Quick check question: How does cross-attention differ from self-attention in a transformer encoder-decoder setup?

- Concept: Frequency-domain signal processing (FFT/IFFT)
  - Why needed here: Bi-FCFM explicitly transforms features into the frequency domain to capture discriminative cues; engineers must understand FFT/IFFT and how to manipulate frequency components.
  - Quick check question: What information is typically preserved in the low-frequency vs. high-frequency components of an image or audio feature map?

- Concept: Contrastive learning and similarity metrics
  - Why needed here: ACTL uses contrastive loss to align audio and visual semantics; understanding embedding similarity, focal loss, and negative sampling is critical.
  - Quick check question: How does focal loss adjust the learning rate for easy vs. hard negative pairs?

## Architecture Onboarding

- Component map:
  - Audio encoder: HuBERT-Base (frozen) -> Visual encoder: ResNet50 -> Fusion module: Bi-FCFM -> Decoder: Deformable DETR with frame propagation -> Tracking loss: Classification + bounding box regression -> ACTL module: Audio-visual contrastive loss -> Output: Object detections with referring scores

- Critical path:
  1. Encode audio (HuBERT) and visual (ResNet50) frames
  2. Fuse via Bi-FCFM (spatiotemporal → frequency → spatiotemporal)
  3. Decode with deformable DETR and propagate frames
  4. Apply ACTL to maintain semantic alignment
  5. Match and optimize with tracking + ACTL losses

- Design tradeoffs:
  - Bi-FCFM vs. pure spatiotemporal fusion: better semantic capture but higher compute
  - Frozen HuBERT vs. fine-tuned: stability vs. task-specific adaptation
  - ACTL weight: stronger alignment vs. potential interference with primary tracking loss

- Failure signatures:
  - Degraded HOTA/DetA/AssA when ACTL weight is too high
  - Blurry or noisy heatmaps from Bi-FCFM indicating poor frequency filtering
  - Audio-visual misalignment when audio embeddings are not robust to speaker variation

- First 3 experiments:
  1. Replace Bi-FCFM with vanilla cross-attention; compare HOTA and inference speed
  2. Remove ACTL; observe long-range tracking drift and HOTA change
  3. Vary ACTL loss weight (0.5, 1.0, 2.0, 4.0); identify optimal value for HOTA vs. MOTA balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can EchoTrack's performance be improved in severe object motion and occlusion scenarios?
- Basis in paper: [explicit] The authors mention this as a future research direction, stating "We plan to further enhance the tracking performance when facing severe object motion and occlusion scenarios."
- Why unresolved: The current EchoTrack framework does not explicitly address these challenging scenarios, and the authors acknowledge the need for further research in this area.
- What evidence would resolve it: Developing and testing modifications to EchoTrack that specifically target severe object motion and occlusion, and demonstrating improved performance on relevant benchmarks.

### Open Question 2
- Question: How can EchoTrack be adapted to reduce computational demands while maintaining tracking performance?
- Basis in paper: [explicit] The authors mention this as a future research direction, stating "designing more lightweight networks to reduce the computational demands of AR-MOT modeling represents a promising research avenue within AR-MOT."
- Why unresolved: The current EchoTrack framework, while effective, may be computationally intensive for real-time applications. The authors recognize the need for more efficient implementations.
- What evidence would resolve it: Proposing and evaluating lightweight variations of EchoTrack that achieve comparable tracking performance with reduced computational complexity, validated on relevant benchmarks.

### Open Question 3
- Question: How can large language models be effectively utilized for AR-MOT tasks?
- Basis in paper: [explicit] The authors mention this as a future research direction, stating "we intend to explore the possibility of unleashing the potential large language models for AR-MOT."
- Why unresolved: While EchoTrack demonstrates the effectiveness of audio-based referring for MOT, the potential of large language models in this context remains unexplored.
- What evidence would resolve it: Developing and evaluating AR-MOT approaches that incorporate large language models, demonstrating improvements in tracking performance or other relevant metrics compared to existing methods.

## Limitations

- The specific contributions of frequency-domain fusion and contrastive learning to tracking performance are difficult to isolate due to the absence of appropriate ablation baselines in the corpus.
- The frozen HuBERT assumption may not hold across diverse acoustic environments or non-standard speakers, though this is not explicitly tested.
- The benchmarks, while novel, are relatively small compared to established tracking datasets, which may limit generalization claims.

## Confidence

- **High confidence**: The AR-MOT task formulation and benchmark creation (Echo-KITTI, Echo-KITTI+, Echo-BDD) are well-defined and novel contributions.
- **Medium confidence**: The EchoTrack architecture and its performance gains over existing MOT methods are supported by experimental results, though the lack of comparable methods limits definitive conclusions.
- **Low confidence**: The specific contributions of frequency-domain fusion and contrastive learning to tracking performance are difficult to isolate due to the absence of appropriate ablation baselines in the corpus.

## Next Checks

1. Implement a spatiotemporal-only fusion baseline (removing Bi-FCFM) to quantify the actual contribution of frequency-domain processing to HOTA performance.
2. Conduct a sensitivity analysis of the ACTL loss weight parameter across a wider range (0.1, 0.5, 1.0, 2.0, 4.0) to identify optimal values and potential overfitting risks.
3. Test EchoTrack's robustness to audio quality degradation (additive noise, speaker variation) on the Echo-KITTI+ benchmark to validate the frozen HuBERT assumption.