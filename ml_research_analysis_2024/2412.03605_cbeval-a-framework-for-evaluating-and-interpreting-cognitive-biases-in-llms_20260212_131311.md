---
ver: rpa2
title: 'CBEval: A framework for evaluating and interpreting cognitive biases in LLMs'
arxiv_id: '2412.03605'
source_url: https://arxiv.org/abs/2412.03605
tags:
- bias
- cognitive
- biases
- llms
- effect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CBEval, a framework for evaluating and interpreting\
  \ cognitive biases in large language models (LLMs). The authors identify five key\
  \ cognitive biases\u2014framing effect, anchoring effect, number bias, representativeness\
  \ heuristic, and priming effect\u2014and demonstrate their presence in state-of-the-art\
  \ models like GPT-4o and GPT-4o-mini."
---

# CBEval: A framework for evaluating and interpreting cognitive biases in LLMs

## Quick Facts
- arXiv ID: 2412.03605
- Source URL: https://arxiv.org/abs/2412.03605
- Reference count: 29
- Primary result: Introduces CBEval framework that identifies five cognitive biases in LLMs using Shapley value analysis to interpret reasoning mechanisms.

## Executive Summary
This paper presents CBEval, a novel framework for detecting and interpreting cognitive biases in large language models. The authors identify five key biases—framing effect, anchoring effect, number bias, representativeness heuristic, and priming effect—and demonstrate their presence in state-of-the-art models like GPT-4o. By employing Shapley value analysis to quantify the influence of individual words on model outputs, the framework provides insights into the underlying reasoning mechanisms that lead to biased responses. The work bridges psychology and AI by showing how LLMs inherit and exhibit human-like cognitive biases, raising important questions about their reliability in decision-making tasks.

## Method Summary
The CBEval framework evaluates cognitive biases in LLMs by combining controlled prompt engineering with Shapley value analysis. The method treats input words as players in a cooperative game where the "payoff" is the model's output probability. By calculating Shapley values, the framework quantifies each word's marginal contribution to the output, effectively creating influence graphs that reveal the most significant words or phrases responsible for biased responses. The approach is validated across multiple frontier models including GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Llama 3 405B, Google Gemini, and Mistral Large, providing a cost-effective and reproducible method for bias evaluation.

## Key Results
- Discovery of a "cognitive bias barrier" in framing effects, with GPT-4o-mini showing higher barrier values than GPT-4o
- Strong round number bias identified in grading tasks, with models consistently favoring round numerical outputs
- Anchoring effects significantly influence numerical estimates in both direct estimation and letter-counting tasks
- Influence graphs constructed via Shapley values successfully identify key words/phrases driving biased outputs
- Framework validated across multiple frontier models with consistent bias patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shapley value analysis provides a mathematically principled way to attribute the influence of individual words in prompts to model outputs, revealing the underlying cognitive biases.
- Mechanism: The framework treats input words as players in a cooperative game, where the "payoff" is the model's output probability. By calculating Shapley values, the framework quantifies each word's marginal contribution to the output, effectively creating an "attention map" for the model's reasoning process.
- Core assumption: The model's output probability can be meaningfully decomposed into contributions from individual input tokens, and this decomposition reflects the model's reasoning process.
- Evidence anchors:
  - [abstract] "We employ Shapley value analysis to interpret the underlying reasoning mechanisms, constructing influence graphs to identify the most significant words or phrases responsible for biased outputs."
  - [section] "We implement our Shapley value analysis in a manner similar to [8], considering input words as players in a cooperative game to determine the output token probabilities."
- Break condition: If the model's output probability is not decomposable into independent token contributions, or if the Shapley value calculation is computationally intractable for large prompts.

### Mechanism 2
- Claim: By testing multiple cognitive biases with controlled prompt variations, the framework can identify specific weaknesses in LLM reasoning that mirror human cognitive biases.
- Mechanism: The framework designs prompts that isolate specific biases (framing effect, anchoring effect, etc.) and measures how the model's output changes with subtle prompt modifications. This controlled testing reveals which biases the model exhibits and how strongly.
- Core assumption: LLMs trained on human-generated data inherit and exhibit human-like cognitive biases, and these biases can be systematically tested through carefully designed prompts.
- Evidence anchors:
  - [abstract] "They employ Shapley value analysis to interpret the underlying reasoning mechanisms, constructing influence graphs to identify the most significant words or phrases responsible for biased outputs."
  - [section] "We evaluate prompt A on GPT-4o-mini and prompt B on GPT-4o to compare the differences in cognitive bias barrier in both models."
- Break condition: If the model's biases are not consistent across different prompts or if the prompt variations do not effectively isolate specific biases.

### Mechanism 3
- Claim: The "cognitive bias barrier" concept provides a quantitative measure of an LLM's robustness in reasoning, with higher barriers indicating better comprehension of nuanced relationships.
- Mechanism: By testing how the model responds to varying percentages in framing effect prompts, the framework identifies the point at which the model correctly interprets a lower loss percentage as a positive signal. This point serves as a quantitative measure of the model's reasoning robustness.
- Core assumption: The model's ability to correctly interpret negative framings (e.g., "loss 30% of the time" as positive) can be measured and compared across models, providing insight into their reasoning capabilities.
- Evidence anchors:
  - [abstract] "Key findings include the discovery of a 'cognitive bias barrier' in framing effects"
  - [section] "We label this as model's cognitive bias barrier, namely, the point at which an LLM is able to understand a lower loss percentage corresponds to a higher profit percentage and associates it as a positive factor."
- Break condition: If the model's responses do not follow a consistent pattern as percentages vary, or if the barrier concept does not correlate with other measures of reasoning quality.

## Foundational Learning

- Concept: Game Theory and Shapley Values
  - Why needed here: The framework uses Shapley value analysis to attribute influence in the model's reasoning process. Understanding cooperative game theory and Shapley value calculations is essential for interpreting the results.
  - Quick check question: What is the mathematical formula for calculating Shapley values, and how does it account for all possible coalitions of players?

- Concept: Cognitive Biases in Human Psychology
  - Why needed here: The framework tests for specific cognitive biases that are well-documented in human psychology. Understanding these biases is crucial for designing appropriate test prompts and interpreting results.
  - Quick check question: What is the key difference between framing effect and anchoring effect in human decision-making?

- Concept: Large Language Model Architecture and Attention Mechanisms
  - Why needed here: The framework interprets Shapley value results in the context of LLM attention mechanisms. Understanding how transformers process information is important for relating Shapley values to model behavior.
  - Quick check question: How does the attention mechanism in transformers relate to the concept of token influence in Shapley value analysis?

## Architecture Onboarding

- Component map: Prompt engineering module -> Model querying layer -> Shapley value computation engine -> Influence graph construction -> Bias evaluation reporting system
- Critical path: Prompt engineering → Model querying → Shapley value computation → Influence graph construction → Bias identification → Result interpretation
- Design tradeoffs: Computational cost vs. accuracy (Shapley values scale exponentially with prompt length), prompt specificity vs. generalizability (highly specific prompts may not generalize to real-world scenarios), and model coverage vs. resource constraints (testing multiple models requires significant computational resources).
- Failure signatures: Inconsistent bias patterns across similar prompts, computational timeouts during Shapley value calculation, model refusals to answer certain prompts, and visualization tools failing to render influence graphs.
- First 3 experiments:
  1. Replicate the framing effect experiment with GPT-4o using the stock investment prompts to verify the cognitive bias barrier concept.
  2. Test the anchoring effect using the jellybean estimation prompt to confirm the presence of anchoring bias in GPT-4o.
  3. Implement the round number bias experiment with the grading task to identify the model's preference for round numbers in numerical outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different post-training alignment methods (like RLHF) influence the manifestation and strength of cognitive biases in LLMs compared to base models?
- Basis in paper: [explicit] The paper notes that [19] present evidence on higher presence of biases in RLHF aligned models compared to base counterparts, and [18] discuss lack of creativity in RLHF tuned LLM responses.
- Why unresolved: The paper mentions these findings but does not conduct direct experiments comparing base and aligned models across multiple bias types or provide quantitative comparisons.
- What evidence would resolve it: Systematic evaluation of the same set of cognitive biases across base models and their RLHF/instruction-tuned versions, with quantitative measurements of bias strength and reasoning differences.

### Open Question 2
- Question: What is the relationship between a model's cognitive bias barrier (the point at which it understands lower loss percentages correspond to higher profit percentages) and its overall reasoning capabilities or training data composition?
- Basis in paper: [explicit] The paper discovers a "cognitive bias barrier" in framing effects and notes that GPT-4o-mini shows higher barrier values compared to GPT-4o, suggesting enhanced robustness in decision-making.
- Why unresolved: While the paper observes differences between models, it does not investigate what factors (model size, training data, architectural differences) determine these barrier values or whether they correlate with other measures of reasoning ability.
- What evidence would resolve it: Comparative analysis of cognitive bias barriers across a diverse set of models with varying sizes, training approaches, and data compositions, correlating barrier values with other reasoning benchmarks.

### Open Question 3
- Question: Can the Shapley value attribution method be reliably used to distinguish between genuine reasoning processes and token-level pattern matching in LLMs, or do both produce similar attribution patterns?
- Basis in paper: [inferred] The paper draws parallels between Shapley score attribution and attention maps, and discusses how models may exhibit cognitive biases due to knowledge gaps or reliance on frequent patterns in training data rather than true reasoning.
- Why unresolved: The paper uses Shapley values to interpret reasoning but acknowledges the similarity to attention maps and notes that both genuine reasoning and pattern matching could produce similar attribution patterns, without establishing criteria to differentiate them.
- What evidence would resolve it: Controlled experiments where models are tested on problems that require novel reasoning versus pattern matching, with detailed comparison of Shapley attribution patterns to determine distinguishing characteristics.

## Limitations
- Computational scalability issues with Shapley value analysis for longer prompts
- Limited to five specific cognitive biases, may not generalize to all reasoning errors
- Assumes word-level Shapley values accurately reflect reasoning, which may not hold for complex multi-step tasks
- Does not distinguish between genuine reasoning and pattern matching in attribution patterns

## Confidence
- **High Confidence**: The identification and demonstration of specific cognitive biases (framing effect, anchoring effect, round number bias) in GPT-4o and GPT-4o-mini are well-supported by the probability distribution analysis and Shapley value attributions shown in the results.
- **Medium Confidence**: The "cognitive bias barrier" concept as a quantitative measure of reasoning robustness is promising but requires validation across a broader range of models and bias types to establish its generalizability.
- **Medium Confidence**: The claim that LLMs inherit human-like cognitive biases from training data is plausible but not definitively proven by the study, as alternative explanations (such as architectural biases) cannot be ruled out.

## Next Checks
1. Cross-model consistency test: Replicate the framing effect experiment with additional models (e.g., Claude 3.5 Sonnet, Llama 3 405B) to verify if the cognitive bias barrier concept holds across different architectures and training approaches.

2. Ablation study on Shapley value implementation: Systematically vary the cooperative game formulation (e.g., different payoff functions v) to assess the robustness of influence attributions and identify potential implementation artifacts.

3. Real-world task validation: Apply the framework to evaluate cognitive biases in practical decision-making scenarios (e.g., medical diagnosis, financial planning) to assess whether laboratory findings translate to real-world performance limitations.