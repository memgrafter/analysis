---
ver: rpa2
title: 'Robust Active Learning (RoAL): Countering Dynamic Adversaries in Active Learning
  with Elastic Weight Consolidation'
arxiv_id: '2408.07364'
source_url: https://arxiv.org/abs/2408.07364
tags:
- uni00000013
- learning
- uni00000012
- adversarial
- uni00000055
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Robust Active Learning (RoAL), a method designed
  to counter dynamic adversarial attacks in active learning by integrating Elastic
  Weight Consolidation (EWC). Dynamic adversarial attacks, which adaptively target
  unlabeled data to degrade model performance, are shown to cause catastrophic forgetting
  in active learning frameworks.
---

# Robust Active Learning (RoAL): Countering Dynamic Adversaries in Active Learning with Elastic Weight Consolidation

## Quick Facts
- arXiv ID: 2408.07364
- Source URL: https://arxiv.org/abs/2408.07364
- Authors: Ricky Maulana Fajri; Yulong Pei; Lu Yin; Mykola Pechenizkiy
- Reference count: 37
- One-line primary result: RoAL improves robust accuracy by 10.88% on average compared to state-of-the-art methods

## Executive Summary
This paper introduces Robust Active Learning (RoAL), a method designed to counter dynamic adversarial attacks in active learning by integrating Elastic Weight Consolidation (EWC). Dynamic adversarial attacks, which adaptively target unlabeled data to degrade model performance, are shown to cause catastrophic forgetting in active learning frameworks. RoAL addresses this by combining EWC with uncertainty sampling to mitigate forgetting and enhance robustness. Extensive experiments on six datasets demonstrate RoAL's effectiveness, achieving an average improvement of 10.88% in robust accuracy compared to state-of-the-art methods like DRE and MCP.

## Method Summary
RoAL integrates EWC into the active learning process by adding a regularization term to the loss function that discourages large changes to parameters deemed important for previously learned tasks. The method uses uncertainty sampling to select the most informative samples for labeling, focusing the model on valuable learning opportunities while EWC maintains stability. The approach is evaluated across six datasets (MNIST, Fashion MNIST, CIFAR-10/100, SVHN, Caltech101) under dynamic adversarial attacks including PGD, Jitter, FAB, VNI-FGSM, and PGDL2. The implementation uses a four-layer CNN architecture with ReLU activations and max-pooling layers, trained over 10 active learning iterations with an initial labeled subset of 400 samples and 200 candidates selected per iteration.

## Key Results
- RoAL achieves an average improvement of 10.88% in robust accuracy compared to state-of-the-art methods
- The method maintains stable performance under dynamic attacks, particularly PGDL2
- RoAL is robust to variations in initial labeled data and shows consistent performance across all six tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RoAL mitigates catastrophic forgetting during dynamic adversarial attacks by combining EWC regularization with uncertainty sampling.
- Mechanism: EWC adds a penalty term to the loss function that discourages large changes to parameters deemed important for previously learned tasks, preserving knowledge. Uncertainty sampling selects the most uncertain samples for labeling, focusing the model on informative data while EWC maintains stability.
- Core assumption: The Fisher Information Matrix accurately captures parameter importance, and uncertainty sampling identifies truly informative samples that benefit both robustness and performance.
- Evidence anchors:
  - [abstract] "by integrating Elastic Weight Consolidation (EWC) into the active learning process...to mitigate catastrophic forgetting caused by dynamic adversarial attacks."
  - [section] "EWC balances stability and plasticity, helping the model retain past knowledge while adapting to new tasks."
  - [corpus] No direct evidence in corpus; this is specific to the paper's contribution.
- Break condition: If the Fisher Information Matrix is inaccurate or uncertainty sampling fails to select informative samples, the balance between stability and plasticity breaks down.

### Mechanism 2
- Claim: Dynamic adversarial attacks cause catastrophic forgetting by perturbing unlabeled data, leading the model to prioritize new, corrupted data over previously learned information.
- Mechanism: Adversarial perturbations in the unlabeled pool manipulate the acquisition function, causing the model to select and focus on corrupted samples during training. This shift in focus overwrites important parameters, resulting in forgetting of prior knowledge.
- Core assumption: The adversary can effectively craft perturbations that significantly influence the acquisition function and model training dynamics.
- Evidence anchors:
  - [abstract] "dynamic adversarial attacks...can lead to catastrophic forgetting within the active learning cycle."
  - [section] "By perturbing the unlabeled data, the adversary influences the samples chosen for labeling and training, leading to degraded model performance or incorrect predictions as the model updates over time."
  - [corpus] Weak evidence; corpus papers focus on adversarial robustness but not specifically on the forgetting mechanism in active learning.
- Break condition: If the acquisition function is robust to perturbations or if the model can distinguish between clean and corrupted samples, the forgetting effect is reduced.

### Mechanism 3
- Claim: Combining EWC with uncertainty sampling enhances robustness against dynamic adversarial attacks more effectively than either method alone.
- Mechanism: Uncertainty sampling selects informative samples, providing valuable learning opportunities, while EWC ensures that learning these new samples does not overwrite critical parameters from previous tasks. This combination results in a model that is both adaptive and stable under adversarial conditions.
- Core assumption: The combination of these two methods synergistically improves performance beyond their individual contributions.
- Evidence anchors:
  - [abstract] "Our approach leverages EWC's ability to prioritize learning important features that are robust against varying attacks. Furthermore, we employ uncertainty sampling as the acquisition function...This integration significantly enhances the robustness and efficiency of the active learning process under adversarial conditions."
  - [section] "Our extensive experiments demonstrate that active learning, optimized with EWC through uncertainty sampling, notably enhances the model's robustness when facing dynamic adversarial attacks."
  - [corpus] No direct evidence in corpus; this is a specific claim from the paper's results.
- Break condition: If uncertainty sampling becomes ineffective due to adversarial perturbations, or if EWC's regularization is too strong, hindering adaptation to new data.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding why models lose previously learned information is crucial to grasp the problem RoAL solves.
  - Quick check question: What is catastrophic forgetting, and why does it occur in neural networks during sequential learning tasks?

- Concept: Fisher Information Matrix (FIM)
  - Why needed here: FIM is used by EWC to determine the importance of parameters, so understanding its role is essential for implementing EWC.
  - Quick check question: How is the Fisher Information Matrix calculated, and what does it represent in the context of EWC?

- Concept: Uncertainty sampling
  - Why needed here: Uncertainty sampling is the acquisition function used in RoAL, so understanding how it selects informative samples is important for the overall method.
  - Quick check question: What is uncertainty sampling, and how does it differ from other acquisition functions like entropy or margin sampling?

## Architecture Onboarding

- Component map:
  - Active Learning Loop -> Uncertainty Sampling Module -> Adversarial Attack Module -> EWC Regularization Module -> Evaluation Module

- Critical path:
  1. Train initial model on labeled data.
  2. Use uncertainty sampling to select candidate samples from unlabeled data.
  3. Apply dynamic adversarial attacks to labeled data.
  4. Retrain model with EWC regularization on both clean and adversarial data.
  5. Evaluate model performance.
  6. Repeat until labeling budget is exhausted.

- Design tradeoffs:
  - EWC strength (Î»): Balancing between preserving old knowledge and learning new information.
  - Uncertainty sampling vs. other acquisition functions: Tradeoff between selecting informative samples and robustness to perturbations.
  - Complexity of adversarial attacks: More complex attacks may better simulate real-world scenarios but increase computational cost.

- Failure signatures:
  - High variance in robust accuracy across iterations: Indicates instability due to catastrophic forgetting.
  - Significant drop in accuracy after introducing adversarial examples: Suggests the model is overfitting to clean data or is not robust to attacks.
  - No improvement in performance over iterations: Could indicate issues with the acquisition function or EWC regularization.

- First 3 experiments:
  1. Implement EWC regularization on a simple model and dataset (e.g., MNIST) to observe its effect on catastrophic forgetting.
  2. Integrate uncertainty sampling as the acquisition function and compare its performance against random sampling.
  3. Apply a basic adversarial attack (e.g., PGD) during training and evaluate the model's robustness with and without EWC.

## Open Questions the Paper Calls Out
- How does RoAL's performance scale with larger datasets and more complex model architectures beyond the tested configurations?
- What is the impact of different uncertainty sampling acquisition functions (e.g., margin sampling, expected error reduction) on RoAL's performance compared to the used uncertainty sampling?
- How does RoAL's performance vary under different types and sequences of adversarial attacks beyond the tested dynamic sequence?

## Limitations
- Assumes static unlabeled data distributions, which may not reflect real-world scenarios with concept drift
- Limited comparison to state-of-the-art methods, with performance gap potentially varying with different model architectures
- Computational overhead of EWC regularization may limit applicability in resource-constrained settings

## Confidence
- **High** for core claims about RoAL's effectiveness in mitigating catastrophic forgetting under dynamic adversarial attacks
- **Medium** for the claim that RoAL outperforms state-of-the-art methods by 10.88% on average
- **Low** for generalizability to real-world scenarios with shifting unlabeled data distributions

## Next Checks
1. Test RoAL under dynamic unlabeled data distributions to assess real-world applicability and robustness to concept drift.
2. Measure and compare the training time and memory overhead of RoAL against baseline methods, particularly for large-scale datasets.
3. Evaluate RoAL with different model architectures (e.g., ResNet, Vision Transformer) and acquisition functions to determine the method's robustness to architectural choices.