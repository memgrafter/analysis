---
ver: rpa2
title: 'MuST: Multi-Scale Transformers for Surgical Phase Recognition'
arxiv_id: '2407.17361'
source_url: https://arxiv.org/abs/2407.17361
tags:
- temporal
- surgical
- must
- phase
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MuST, a multi-scale transformer approach for
  surgical phase recognition. The core idea is to use a Multi-Term Frame Encoder (MTFE)
  that samples sequences at increasing strides around a keyframe to capture short-,
  mid-, and long-term temporal information.
---

# MuST: Multi-Scale Transformers for Surgical Phase Recognition

## Quick Facts
- arXiv ID: 2407.17361
- Source URL: https://arxiv.org/abs/2407.17361
- Reference count: 40
- Key outcome: MuST achieves state-of-the-art performance on three surgical datasets with F1-scores of 77.25% (HeiChole), mAP of 98.08% (MISAW), and F1-score of 77.25% (GraSP).

## Executive Summary
This paper presents MuST, a multi-scale transformer approach for surgical phase recognition that addresses the challenge of capturing both fine-grained and long-range temporal information in surgical videos. The method employs a Multi-Term Frame Encoder (MTFE) with a temporal pyramid that samples sequences at increasing strides around keyframes, enabling simultaneous analysis at multiple temporal resolutions. A Temporal Consistency Module (TCM) further enhances long-term reasoning by processing relationships among frame embeddings across extended temporal windows. The approach demonstrates significant improvements over existing methods on three surgical datasets, with the ablation studies confirming the effectiveness of both the multi-scale architecture and the long-term consistency module.

## Method Summary
MuST uses a two-stage transformer architecture for surgical phase recognition. The first stage, MTFE, processes video frames through a temporal pyramid with progressively longer strides (1, 4, 8, 12 seconds), creating sequences that capture short-to-long term temporal information. A Multi-Temporal Attention Module enables cross-scale attention between these sequences, followed by self-attention and MLP processing. The second stage, TCM, takes the resulting frame embeddings and processes them through a long-term transformer encoder with self-attention over extensive temporal windows (90% overlap) to enforce temporal consistency in predictions. The model uses MViT-B as its backbone and is trained sequentially in two stages using cross-entropy loss and AdamW optimizer.

## Key Results
- Achieves state-of-the-art F1-score of 77.25% on HeiChole dataset
- Achieves state-of-the-art mAP of 98.08% on MISAW dataset
- Outperforms existing methods on GraSP dataset with F1-score of 77.25%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-scale sampling with increasing strides captures both fine-grained and broad temporal context simultaneously.
- Mechanism: The temporal pyramid creates sequences with progressively longer strides, allowing the model to analyze short-term details from early levels and long-term patterns from later levels in parallel.
- Core assumption: Different surgical phases have distinct temporal signatures at multiple scales, and capturing these scales improves recognition accuracy.
- Evidence anchors:
  - [abstract] "Our Multi-Term Frame Encoder computes interdependencies across a hierarchy of temporal scales by sampling sequences at increasing strides around the frame of interest."
  - [section 2] "This hierarchical structure enables simultaneous analysis at multiple temporal resolutions and captures information efficiently across various time scales."
- Break condition: If surgical phases don't exhibit multi-scale temporal patterns, or if the sampling rates are poorly chosen relative to phase durations.

### Mechanism 2
- Claim: Cross-attention between different temporal scales enables the model to learn relationships across scales.
- Mechanism: The Multi-Temporal Attention Module uses cross-attention to let each sequence attend to all other sequences in the pyramid, creating inter-scale dependencies.
- Core assumption: Information from one temporal scale can inform understanding at other scales through learned attention patterns.
- Evidence anchors:
  - [section 2] "The MTCA module computes cross-attention between each embedding sequence with the concatenation of all sequences."
  - [supplementary] "These attention maps illustrate that each head specializes in capturing different segments of theL set, featuring inter-dependencies across temporal scales."
- Break condition: If cross-scale relationships are not useful for the task, or if attention becomes too diffuse to be meaningful.

### Mechanism 3
- Claim: The Temporal Consistency Module (TCM) enforces temporal coherence by processing long sequences of frame embeddings.
- Mechanism: TCM uses a long-term transformer encoder to attend across an extensive temporal window of multi-term frame embeddings, ensuring predictions maintain consistency.
- Core assumption: Surgical phases exhibit temporal continuity, and predictions should respect this continuity through long-range dependencies.
- Evidence anchors:
  - [abstract] "Furthermore, we employ a long-term Transformer encoder over the frame embeddings to further enhance long-term reasoning."
  - [section 3.3] "The TCM significantly enhances the model's prediction coherence by incorporating long-term dependencies between frame embeddings through its transformer encoder architecture."
- Break condition: If phases don't exhibit temporal continuity, or if the long-term window is too short to capture meaningful dependencies.

## Foundational Learning

- Concept: Transformer self-attention mechanisms
  - Why needed here: The entire MuST architecture relies on transformers for both multi-scale processing and long-term consistency, requiring understanding of how self-attention works and why it's effective for sequential data.
  - Quick check question: How does self-attention differ from recurrent networks in handling long-range dependencies?

- Concept: Multi-scale feature extraction in computer vision
  - Why needed here: The temporal pyramid approach is analogous to multi-scale image processing, requiring understanding of how processing at different scales can capture different levels of information.
  - Quick check question: What are the trade-offs between fine-grained local information and broad contextual information in feature extraction?

- Concept: Temporal consistency in sequential data
  - Why needed here: The TCM module specifically addresses temporal consistency, requiring understanding of why predictions in sequential data should maintain temporal coherence and how this differs from independent frame classification.
  - Quick check question: Why is temporal consistency important in surgical phase recognition, and what are the consequences of ignoring it?

## Architecture Onboarding

- Component map: Input video -> Multi-Term Frame Encoder (MTFE) with temporal pyramid -> Multi-Temporal Attention Module -> TCM -> Classification head
- Critical path: The flow from temporal pyramid through MTFE to TCM represents the critical path for achieving multi-scale and long-term reasoning.
- Design tradeoffs:
  - Pyramid depth vs. computational cost: More pyramid levels capture more scales but increase computation
  - Window size in TCM vs. temporal context: Larger windows capture longer dependencies but increase memory usage
  - Stride selection in pyramid: Must balance coverage of different phase durations
- Failure signatures:
  - Poor performance on short-duration phases suggests insufficient fine-grained capture in the pyramid
  - Inconsistent predictions across similar phases suggests TCM isn't learning appropriate long-term dependencies
  - Overfitting on training data suggests too much model capacity relative to dataset size
- First 3 experiments:
  1. Test pyramid with single scale vs. multi-scale to validate the multi-scale approach
  2. Test MTFE alone vs. MTFE + TCM to validate the long-term consistency module
  3. Vary pyramid depth (number of scales) to find optimal scale count for the datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MuST's performance scale with increasing video duration and phase complexity?
- Basis in paper: [explicit] The paper demonstrates MuST's effectiveness on three surgical datasets but does not explore performance across varying video lengths or highly complex multi-phase procedures.
- Why unresolved: The experiments focus on specific benchmarks with moderate complexity and duration. No systematic analysis of performance degradation or adaptation requirements for longer or more complex surgical videos is presented.
- What evidence would resolve it: Experiments showing MuST's F1-scores, mAP, or other metrics across a range of video durations (e.g., 10x longer videos) and increasing numbers of phases or highly interleaved phase transitions.

### Open Question 2
- Question: What is the impact of the Temporal Consistency Module (TCM) on real-time surgical phase recognition systems?
- Basis in paper: [inferred] The paper highlights TCM's role in improving prediction coherence but only evaluates offline performance. Real-time systems require latency analysis.
- Why unresolved: The TCM processes extensive temporal windows with 90% overlap, which could introduce significant computational overhead in real-time applications. The paper does not quantify this trade-off.
- What evidence would resolve it: Benchmarking MuST's inference time and latency measurements in online setups, comparing TCM-enabled versus TCM-disabled performance, and analyzing memory/compute requirements.

### Open Question 3
- Question: How does MuST's multi-scale approach compare to alternative temporal sampling strategies like adaptive or attention-based sampling?
- Basis in paper: [explicit] The paper introduces fixed temporal strides (1, 4, 8, 12 seconds) but acknowledges other methods use adaptive sampling or attention mechanisms for temporal context.
- Why unresolved: The ablation study shows multi-scale pyramids improve performance, but no comparison with adaptive sampling methods or learned sampling strategies is provided.
- What evidence would resolve it: Head-to-head comparisons between MuST's fixed-stride pyramid and methods using adaptive temporal sampling or learned attention mechanisms on the same surgical datasets.

## Limitations
- The paper lacks theoretical analysis of why specific architectural decisions (stride values, pyramid depth) were optimal
- Results are presented only on surgical datasets, raising questions about generalizability to other video understanding tasks
- Ablation studies are somewhat limited, focusing primarily on individual component removal rather than exploring the design space

## Confidence

**High**: The core mechanism of multi-scale temporal processing and its effectiveness for surgical phase recognition is well-supported by the ablation studies and comparison with baselines.

**Medium**: The specific architectural choices (number of pyramid levels, TCM design) are justified by empirical results but lack theoretical grounding for why these particular configurations work best.

**Medium**: The claims about long-term consistency improvements are supported by performance gains but could benefit from more qualitative analysis of prediction smoothness.

## Next Checks

1. Conduct an ablation study varying the number of scales in the temporal pyramid to determine if the current configuration is optimal or if fewer/more scales could achieve similar performance.

2. Test the model on non-surgical video datasets to evaluate generalizability of the multi-scale transformer approach beyond the surgical domain.

3. Perform a qualitative analysis of prediction sequences to verify that TCM actually produces smoother, more temporally consistent predictions compared to MTFE alone, beyond just improved metrics.