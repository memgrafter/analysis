---
ver: rpa2
title: A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models
arxiv_id: '2404.13940'
source_url: https://arxiv.org/abs/2404.13940
tags:
- user
- llms
- answer
- benchmark
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a user-centric benchmark for evaluating
  large language models (LLMs) based on real-world user needs across six intent categories:
  factual QA, professional problem solving, text assistance, advice seeking, creativity,
  and leisure. The benchmark is built on 1,846 authentic user cases from 712 participants
  across 23 countries, collected through a user study.'
---

# A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models

## Quick Facts
- arXiv ID: 2404.13940
- Source URL: https://arxiv.org/abs/2404.13940
- Reference count: 40
- Key outcome: Introduces a user-centric benchmark for evaluating LLMs based on real-world user needs across six intent categories, achieving Pearson correlations of 0.95 with real-world user satisfaction and 0.94 with pairwise human annotations.

## Executive Summary
This paper introduces a user-centric benchmark for evaluating large language models (LLMs) based on real-world user needs across six intent categories: factual QA, professional problem solving, text assistance, advice seeking, creativity, and leisure. The benchmark is built on 1,846 authentic user cases from 712 participants across 23 countries, collected through a user study. LLMs are evaluated using intent-aware criteria with GPT-4 as judge, achieving strong alignment with human preferences. The results provide actionable insights for users and developers, demonstrating that the benchmark effectively measures LLM performance in satisfying diverse real-world needs.

## Method Summary
The benchmark is constructed through a user study where participants report their recent LLM interactions, including conversation content, the LLM service used, and their intent. The collected cases are categorized into six user intents and evaluated using intent-aware criteria with GPT-4 as judge. The evaluation process includes chain-of-thought reasoning steps, scoring standards, and reference materials. The benchmark is then used to evaluate 10 LLM services, with results validated against real-world user satisfaction and pairwise human annotations.

## Key Results
- The benchmark achieves Pearson correlations of 0.95 with real-world user satisfaction and 0.94 with pairwise human annotations, demonstrating strong alignment with human preferences.
- GPT-4-based evaluations show minimal bias toward responses resembling its own style, with cross-validation efforts confirming the robustness of rankings.
- The benchmark reveals that GPT-4 dominates in professional problem solving and factual QA, while other models like ChatGPT and Claude show strengths in specific areas such as creativity and text assistance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The user-centric benchmark aligns strongly with real-world human preferences.
- Mechanism: By collecting authentic user interactions and evaluating LLM responses against human-defined intent criteria, the benchmark captures genuine user needs rather than abstract task performance.
- Core assumption: Real user interactions reflect the diversity and complexity of actual LLM usage scenarios.
- Evidence anchors:
  - [abstract] "Experiments demonstrate that benchmark results align closely with human preferences, as evidenced by Pearson correlations of 0.95 and 0.94 with real-world user experiences and pairwise annotations."
  - [section] "We show that benchmark scores align well with human preference in both real-world experience and pair-wise annotations, achieving Pearson correlations of 0.95 and 0.94, respectively."
- Break condition: If the collected user data is not representative of the broader user base or if the evaluation criteria do not accurately capture user intent satisfaction.

### Mechanism 2
- Claim: Intent-aware evaluation criteria improve the precision of LLM performance assessment.
- Mechanism: By tailoring evaluation criteria to specific user intents (e.g., factual QA, professional problem solving), the benchmark can more accurately measure how well LLMs satisfy diverse user needs.
- Core assumption: Different user intents require different aspects of LLM performance to be prioritized.
- Evidence anchors:
  - [section] "Based on the divided intents, the users who lack specific knowledge about the model capabilities can simplify their choice of proper service."
  - [section] "We evaluate each defined user intent with five selected criteria as described in Table 6."
- Break condition: If the intent categories are too broad or the criteria do not adequately differentiate performance across intents.

### Mechanism 3
- Claim: Multi-cultural data enhances the generalizability of the benchmark.
- Mechanism: By including user interactions from diverse cultural backgrounds, the benchmark can assess LLM performance in a wider range of contexts and languages.
- Core assumption: Cultural diversity in user interactions leads to a more comprehensive evaluation of LLM capabilities.
- Evidence anchors:
  - [section] "Their reported cases cover multiple cultural backgrounds, such as nations' traditional festivals, local points of interest, and pop culture across the globe."
  - [section] "This brings diversity in LLM evaluations, which is not included in simple translations of English-dominant content."
- Break condition: If the cultural diversity is superficial or if the evaluation criteria do not account for cultural nuances.

## Foundational Learning

- Concept: User Intent Taxonomy
  - Why needed here: Understanding user intent is crucial for designing relevant evaluation criteria and interpreting LLM performance.
  - Quick check question: What are the six user intent categories defined in the benchmark?

- Concept: Evaluation Methodology
  - Why needed here: The choice of evaluation method (e.g., GPT-4-as-Judge) impacts the reliability and validity of the benchmark results.
  - Quick check question: How does the intent-aware evaluation criteria improve the precision of LLM performance assessment?

- Concept: Data Collection and Ethics
  - Why needed here: Ensuring the authenticity and ethical handling of user data is essential for the credibility of the benchmark.
  - Quick check question: What measures were taken to ensure the quality and ethical handling of the user-reported data?

## Architecture Onboarding

- Component map: User study → Data collection → Intent categorization → Evaluation criteria design → LLM benchmarking → Result analysis
- Critical path: Data collection → Intent categorization → Evaluation criteria design → LLM benchmarking → Result analysis
- Design tradeoffs: Balancing the need for diverse user interactions with the practical constraints of data collection and evaluation.
- Failure signatures: Low Pearson correlation with human preferences, bias towards certain LLM services, or lack of cultural diversity in the data.
- First 3 experiments:
  1. Validate the intent categorization by having users label their own interactions.
  2. Test the evaluation criteria on a small set of LLM responses to ensure they are clear and effective.
  3. Conduct a pilot benchmark with a limited set of LLM services to refine the evaluation process.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic or cultural markers in the URS dataset indicate cross-cultural differences in LLM usage patterns?
- Basis in paper: [explicit] The paper notes the dataset includes "traditional events, such as Major Snow and the Spring Festival in the lunar calendar as well as popular entertainment information that are globally or locally recognized" across 23 countries.
- Why unresolved: While the paper identifies cultural diversity exists in the dataset, it doesn't analyze or quantify how cultural differences manifest in LLM interactions or whether certain intents show cultural variation.
- What evidence would resolve it: A comparative analysis of intent distributions, question types, or response preferences across different cultural regions within the dataset.

### Open Question 2
- Question: How do LLM performance rankings change when evaluated on domain-specific subsets of the URS dataset?
- Basis in paper: [explicit] The paper mentions that professional problem solving includes "domain knowledge, in-depth understanding, and reasoning in professional areas" and that current LLMs are "relatively weak" in subjective scenarios.
- Why unresolved: The benchmark provides overall rankings but doesn't examine whether certain LLMs excel in specific domains like medical, legal, or technical problem-solving that appear in the dataset.
- What evidence would resolve it: Performance breakdowns by professional domain types within the solve professional problem intent, showing whether different models dominate different domains.

### Open Question 3
- Question: What is the relationship between users' reported satisfaction levels and the specific intent-aware criteria scores in the benchmark?
- Basis in paper: [explicit] The paper reports a 0.95 Pearson correlation between average benchmark scores and user satisfaction, but doesn't examine correlations at the criteria level.
- Why unresolved: The study shows alignment at the intent level but doesn't reveal whether certain criteria (like factuality vs creativity) are more predictive of user satisfaction for different intent types.
- What evidence would resolve it: Correlation analysis between individual criterion scores and user satisfaction ratings, identifying which criteria most strongly drive user satisfaction for each intent type.

## Limitations

- The benchmark's reliance on GPT-4 as judge introduces potential bias toward responses that resemble GPT-4's own style, though cross-validation efforts show minimal impact on rankings.
- The 10 LLM services evaluated may not be fully representative of the broader ecosystem, potentially limiting generalizability.
- Cultural diversity claims are supported by data from 23 countries, but the depth of cultural representation across all intent categories remains unclear.

## Confidence

- High confidence in the correlation results (0.95 and 0.94) and overall benchmark methodology
- Medium confidence in the cultural diversity representation and its impact on evaluation
- Medium confidence in the generalizability to LLM services beyond those tested

## Next Checks

1. Conduct cross-cultural validation by having evaluators from different cultural backgrounds assess the same responses to verify cultural sensitivity.
2. Test the benchmark's performance consistency when using alternative LLM judges (e.g., Claude, Gemini) instead of GPT-4.
3. Expand the service evaluation to include newer or less common LLM services to assess benchmark adaptability and broader applicability.