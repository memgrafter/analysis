---
ver: rpa2
title: 'MaeFuse: Transferring Omni Features with Pretrained Masked Autoencoders for
  Infrared and Visible Image Fusion via Guided Training'
arxiv_id: '2404.11016'
source_url: https://arxiv.org/abs/2404.11016
tags:
- fusion
- image
- information
- training
- infrared
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MaeFuse, a novel method for infrared and visible
  image fusion that leverages pretrained Masked Autoencoders (MAE) to extract comprehensive
  omni features for fusion. Unlike existing approaches that rely on downstream tasks
  to obtain high-level visual information, MaeFuse directly utilizes the pretrained
  MAE encoder to capture both low-level reconstruction and high-level vision task
  features.
---

# MaeFuse: Transferring Omni Features with Pretrained Masked Autoencoders for Infrared and Visible Image Fusion via Guided Training

## Quick Facts
- arXiv ID: 2404.11016
- Source URL: https://arxiv.org/abs/2404.11016
- Reference count: 40
- Outperforms state-of-the-art methods on RoadScene dataset with CC of 0.653, SCD of 1.714, PSNR of 63.92, NAB/F of 0.028, and NLPD of 0.599

## Executive Summary
MaeFuse introduces a novel approach to infrared and visible image fusion by leveraging pretrained Masked Autoencoders (MAE) to extract comprehensive "omni features" that combine both high-level semantic and low-level texture information. Unlike existing methods that require downstream tasks to obtain visual information, MaeFuse directly utilizes the pretrained MAE encoder to capture rich features from both modalities. The method addresses the domain gap between different modal features and block effects caused by the MAE encoder through a guided training strategy that aligns the fusion layer's feature space with the encoder's.

## Method Summary
MaeFuse employs a pretrained MAE encoder to extract omni features from infrared and visible image pairs, then uses a fusion layer with Comparative Fusion Module (CFM) and Merging Fusion Module (MFM) to integrate cross-modal information. The method implements a two-stage guided training strategy: first aligning the fusion layer output with the encoder's feature space using mean fusion as a baseline, then refining with fusion loss functions including L1, gradient, and Laplacian terms. The pretrained MAE encoder and decoder are frozen, while only the fusion modules are trained. The approach effectively preserves both contour information through cross-attention and fine details through selective fusion mechanisms.

## Key Results
- Achieves superior performance compared to state-of-the-art methods across multiple public datasets
- Demonstrates strong quantitative results on RoadScene dataset: CC of 0.653, SCD of 1.714, PSNR of 63.92, NAB/F of 0.028, NLPD of 0.599
- Effectively integrates feature vectors from both infrared and visible modalities while preserving rich details inherent in each modal
- Addresses block effects caused by MAE encoder through guided training strategy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pretrained MAE encoder extracts both high-level semantic and low-level texture features, enabling comprehensive omni-feature fusion.
- **Mechanism**: MAE uses masked autoencoding with self-supervised learning on large-scale datasets, forcing the encoder to reconstruct masked patches from unmasked ones. This compels the model to capture both global semantic context and fine-grained texture details from limited visible regions.
- **Core assumption**: MAE pretrained weights contain generalizable visual priors that transfer well to infrared-visible fusion tasks.
- **Evidence anchors**:
  - [abstract]: "utilizes a pretrained encoder from Masked Autoencoders (MAE), which facilities the omni features extraction for low-level reconstruction and high-level vision tasks"
  - [section]: "MAE is based on this structure and conducts self-supervised training. Specifically, we randomly select some tokens to mask M = {mi | mi ∈ { 0, 1}, i = 1 , . . . , n}, and then use the unmasked parts to predict the masked areas. Since we need to recover the entire image from only 25% of the remaining regions, we must use a small amount of information to acquire semantic information to guide the recovery of the masked areas"
  - [corpus]: Weak evidence - corpus neighbors focus on MAE in different contexts (robotics, hyperspectral), no direct fusion comparison.
- **Break condition**: If MAE pretrained weights are overfit to natural images and lack representation for thermal patterns, transfer performance degrades.

### Mechanism 2
- **Claim**: Guided training strategy aligns fusion layer output with encoder feature space, preventing local optima traps.
- **Mechanism**: Two-stage training first aligns fusion layer output to encoder's mean feature domain, then refines with fusion loss. This establishes a proper initialization and prevents the fusion layer from diverging in the high-dimensional feature space.
- **Core assumption**: Mean fusion provides a reasonable anchor point for feature domain alignment.
- **Evidence anchors**:
  - [section]: "we use the mean of the two modalities as the baseline for training" and "Using the alignment function mentioned above, we can ensure that the current results are comparable to the mean fusion, thus aligning the fusion domain with the encoder's feature space"
  - [abstract]: "we further develop a guided training strategy. This strategy is meticulously crafted to ensure that the fusion layer seamlessly adjusts to the feature space of the encoder"
  - [corpus]: No direct corpus evidence for guided training in fusion.
- **Break condition**: If feature spaces are too dissimilar, mean alignment fails and fusion performance suffers.

### Mechanism 3
- **Claim**: Comparative and Merging Fusion Modules (CFM + MFM) effectively integrate cross-modal information while preserving details.
- **Mechanism**: CFM uses symmetric cross-attention for interactive learning, capturing contour information. MFM uses CFM output as guide to selectively fuse original features, preserving details that cross-attention alone might lose.
- **Core assumption**: Cross-attention can effectively match and merge complementary features from different modalities.
- **Evidence anchors**:
  - [section]: "the first part of the fusion layer is the Comparative Fusion Module (CFM), which primarily facilitates the interactive learning of features from two modalities through two symmetric cross-attention networks" and "we incorporated the Merging Fusion Module (MFM). Within this framework, the feature vector ΦD, produced by the CFM, acts as a guide for contour features, aiding in the re-fusion of the initial encoded features"
  - [abstract]: "The proposed method can facilitate the comprehensive integration of feature vectors from both infrared and visible modalities, thus preserving the rich details inherent in each modal"
  - [corpus]: No direct corpus evidence for CFM/MFM architecture.
- **Break condition**: If cross-attention fails to align features properly, the entire fusion structure breaks down.

## Foundational Learning

- **Concept**: Masked Autoencoders (MAE)
  - Why needed here: MAE provides pretrained omni-features (high-level semantics + low-level texture) essential for effective fusion without requiring large labeled datasets.
  - Quick check question: What percentage of image patches does MAE typically mask during training?

- **Concept**: Vision Transformer (ViT) architecture
  - Why needed here: ViT divides images into patches for processing, which creates block effects that MaeFuse must address through its guided training strategy.
  - Quick check question: How does ViT handle positional information for patch embeddings?

- **Concept**: Cross-attention mechanisms
  - Why needed here: Cross-attention enables the fusion layer to selectively integrate complementary information from infrared and visible modalities.
  - Quick check question: In cross-attention, what roles do the query, key, and value vectors play?

## Architecture Onboarding

- **Component map**: Input -> MAE Encoder (frozen) -> CFM -> MFM -> FFN -> MAE Decoder (frozen) -> Output
- **Critical path**: The fusion layer (CFM + MFM + FFN) is the critical path. It must successfully align with the MAE encoder's feature space and perform effective cross-modal fusion.
- **Design tradeoffs**: Using pretrained MAE provides strong features but requires careful alignment. Direct fusion without guided training leads to block effects and local optima. The two-stage training adds complexity but ensures convergence.
- **Failure signatures**: Block effects in output images, loss curves that plateau early, or fused images that resemble simple averaging of inputs indicate alignment or fusion layer failures.
- **First 3 experiments**:
  1. Train fusion layer with only L1 reconstruction loss (no guided training) to observe block effects and convergence issues.
  2. Implement two-stage training with mean alignment only (skip fusion loss stage) to verify domain alignment effectiveness.
  3. Test cross-attention vs simple concatenation in CFM to measure impact on contour preservation.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several key unanswered questions emerge from the methodology and results.

## Limitations

- The two-stage guided training strategy adds significant complexity compared to direct fusion approaches and may not generalize well to other fusion tasks.
- Reliance on pretrained MAE weights assumes sufficient transferability from natural images to thermal domains, which may not hold for all datasets.
- CFM and MFM module architectures are described conceptually but lack precise architectural specifications, potentially affecting reproducibility.

## Confidence

- Mechanism 1 (MAE omni-features): High confidence - well-supported by MAE literature and the paper's detailed explanation
- Mechanism 2 (Guided training): Medium confidence - novel approach with limited external validation
- Mechanism 3 (CFM/MFM integration): Low confidence - architectural details are underspecified

## Next Checks

1. Ablation study comparing direct fusion (no guided training) against the proposed two-stage approach to quantify the alignment benefit
2. Transferability test on datasets with different thermal characteristics to assess MAE weight generalization limits
3. Architectural variation study testing alternative cross-modal fusion mechanisms (e.g., concatenation vs cross-attention) in the CFM module