---
ver: rpa2
title: 'MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection'
arxiv_id: '2410.03010'
source_url: https://arxiv.org/abs/2410.03010
tags:
- modality
- missing
- modalities
- tokens
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of missing modalities in multimodal
  learning, where models suffer significant performance degradation when some input
  modalities are absent during testing. The proposed method, Masked Modality Projection
  (MMP), trains a single robust model by randomly masking subsets of modalities during
  training and learning to project available modality tokens to estimate the tokens
  for masked modalities.
---

# MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection

## Quick Facts
- **arXiv ID**: 2410.03010
- **Source URL**: https://arxiv.org/abs/2410.03010
- **Reference count**: 36
- **Primary result**: MMP significantly improves robustness to missing modalities in multimodal learning tasks without requiring separate models for each missing scenario

## Executive Summary
This paper addresses the critical challenge of missing modalities in multimodal learning systems, where models typically suffer significant performance degradation when some input modalities are absent during testing. The proposed Masked Modality Projection (MMP) method trains a single robust model by randomly masking subsets of modalities during training and learning to project available modality tokens to estimate the tokens for masked modalities. This approach enables effective compensation for missing modalities during inference without requiring separate training or adaptation for each modality combination.

MMP demonstrates substantial improvements across multiple tasks and datasets, including multimodal segmentation (MCubeS, NYUDv2, FMB datasets), classification (UPMC Food-101), and sentiment analysis (CMU-MOSI). The method achieves significant performance gains when modalities are missing while maintaining competitive performance when all modalities are present, making it a practical solution for real-world multimodal applications where sensor failures or data unavailability are common.

## Method Summary
MMP works by randomly masking subsets of modalities during training and using cross-attention between aggregated tokens of available modalities and the actual tokens of masked modalities to generate projected tokens. These projected tokens are then aligned with the actual tokens of the masked modalities using a smooth L1 loss. The method can be integrated into any existing multimodal architecture and handles varying numbers of tokens and embedding dimensions across modalities. During inference, when a modality is missing, the model uses the projected tokens generated from available modalities to maintain performance.

## Key Results
- On NYUDv2 segmentation, MMP achieves 52.04% mIoU when depth is missing (vs 29.79% for dropout baseline)
- On CMU-MOSI sentiment analysis, MMP improves accuracy by 5.8% and F1 score by 10.29% when text is missing
- MMP maintains competitive performance when all modalities are present while significantly improving robustness to missing modalities across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
The model learns to compensate for missing modalities by projecting available modality tokens to estimate missing modality tokens during training. During training, random subsets of modalities are masked and the model learns to generate projected tokens for the masked modalities using cross-attention between aggregated tokens of available modalities and the actual tokens of those modalities. This cross-attention mechanism captures meaningful relationships between available and missing modalities, enabling effective token estimation.

### Mechanism 2
Alignment loss ensures projected tokens closely match actual tokens, improving robustness. Smooth L1 loss is used to minimize the discrepancy between projected tokens and actual tokens during training, ensuring the model learns to generate accurate estimates. This loss function effectively balances the need for accurate projections while preventing overfitting to training data distributions.

### Mechanism 3
Using a single model for all missing modality scenarios is more efficient than training separate models for each combination. The MMP approach trains one model that can handle any missing modality scenario by learning to use available modalities to compensate, eliminating the need for multiple models or adaptation steps. This single-model approach reduces computational overhead and simplifies deployment in real-world applications.

## Foundational Learning

- **Cross-attention mechanism**: Why needed here: Cross-attention is used to capture relationships between available and missing modalities for token projection. Quick check question: How does cross-attention differ from self-attention, and why is it more suitable for this task?

- **Modality masking and dropout augmentation**: Why needed here: Random masking of modalities during training forces the model to learn robustness to missing inputs. Quick check question: What is the difference between modality masking and traditional dropout, and why is masking more effective for this task?

- **Token aggregation and embedding alignment**: Why needed here: Aggregated tokens provide compact representations that reduce computational complexity while maintaining information for cross-attention. Quick check question: Why are aggregated tokens used instead of all tokens in the cross-attention process, and how does this affect performance?

## Architecture Onboarding

- **Component map**: Input modalities → Embedding layers → Tokens (Ti) → Available modalities → Cross-attention → Aggregated tokens (Tj) → Masked modality i → Cross-attention with available aggregated tokens → Projected tokens (T'i) → Alignment loss between T'i and actual Ti → Projected tokens → Corresponding branch in network

- **Critical path**: 1. Token generation from input modalities, 2. Cross-attention between available and masked modality aggregated tokens, 3. Projection of missing modality tokens using available information, 4. Alignment loss optimization

- **Design tradeoffs**: Using cross-attention vs. linear projection: Cross-attention captures more complex relationships but is computationally more expensive. Number of aggregated tokens: More tokens capture more information but increase computational cost. Alignment loss weight: Higher weight ensures better projection accuracy but may overfit to training data.

- **Failure signatures**: Poor performance on specific missing modality combinations, high alignment loss values during training, model performance worse than modality dropout baseline, projected tokens showing low cosine similarity with actual tokens.

- **First 3 experiments**: 1. Baseline test: Train model without MMP and test with missing modalities to establish performance drop. 2. Ablation test: Add modality dropout augmentation and compare to MMP performance. 3. Alignment test: Train with and without alignment loss to measure its impact on projection quality.

## Open Questions the Paper Calls Out

### Open Question 1
How does the MMP approach perform when more than two modalities are missing simultaneously, and are there limits to the number of missing modalities it can effectively compensate for? The paper focuses on scenarios where a single modality is missing, but doesn't extensively explore cases with multiple simultaneous missing modalities. This question remains unresolved because the experiments primarily evaluate single-modality missing scenarios, leaving uncertainty about performance degradation when multiple modalities are absent.

### Open Question 2
What is the computational overhead of the MMP approach compared to standard multimodal models, and how does this impact real-time applications? While MMP is presented as efficient, no timing benchmarks or computational complexity analysis are provided to validate this claim. This question remains unresolved as the paper mentions "minimal change in the network" but doesn't provide quantitative analysis of computational complexity or inference time differences.

### Open Question 3
How does the performance of MMP vary with different choices of embedding dimensions and token counts across modalities, and are there optimal configurations for different task types? The paper acknowledges the method's flexibility but doesn't investigate how different architectural choices affect performance across tasks. This question remains unresolved as the paper mentions that MMP can handle varying token counts and embedding dimensions but doesn't explore the impact of these variations on performance.

## Limitations

- The method's effectiveness for scenarios with multiple simultaneous missing modalities remains unclear, as experiments primarily focus on single-modality missing cases
- The computational overhead and real-time performance impact of the cross-attention and alignment loss components needs more thorough analysis
- The sensitivity of MMP performance to architectural choices like embedding dimensions and token counts across different modality types is not fully explored

## Confidence

- **High confidence**: The general framework of using random modality masking and projection during training is well-established in the literature and the implementation details are clearly described
- **Medium confidence**: The claim that MMP significantly improves robustness across different tasks is supported by experimental results, though the absolute performance gains vary substantially across datasets
- **Low confidence**: The assertion that MMP is more efficient than training separate models needs more careful analysis, as the computational overhead of cross-attention and alignment loss during training may offset the benefits of using a single model

## Next Checks

1. **Cross-dataset generalization test**: Evaluate MMP performance on datasets with substantially different modality characteristics (e.g., medical imaging with missing MRI/CT, or satellite imagery with missing spectral bands) to verify the cross-attention mechanism generalizes beyond the tested scenarios.

2. **Computational overhead analysis**: Measure and compare the actual training time, memory usage, and inference latency of MMP against both baseline models and the multiple-model approach it claims to replace, accounting for all components including cross-attention and alignment loss.

3. **Failure mode characterization**: Systematically identify and analyze specific modality combinations where MMP underperforms relative to baselines, and investigate whether these failures stem from fundamental limitations in the projection mechanism or can be addressed through architectural modifications.