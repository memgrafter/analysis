---
ver: rpa2
title: 'PutnamBench: Evaluating Neural Theorem-Provers on the Putnam Mathematical
  Competition'
arxiv_id: '2407.11214'
source_url: https://arxiv.org/abs/2407.11214
tags:
- putnam
- proof
- lean
- problems
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The PutnamBench benchmark introduces 1692 formalizations of 640
  problems from the William Lowell Putnam Mathematical Competition, covering a broad
  range of undergraduate-level mathematics topics. All problems are formalized in
  Lean 4 and Isabelle, with a substantial subset also in Coq.
---

# PutnamBench: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition

## Quick Facts
- arXiv ID: 2407.11214
- Source URL: https://arxiv.org/abs/2407.11214
- Reference count: 40
- PutnamBench introduces 1692 formalizations of 640 Putnam competition problems across three proof assistants, with state-of-the-art methods solving only a handful

## Executive Summary
PutnamBench is a comprehensive benchmark for evaluating neural theorem-provers on undergraduate-level mathematical problems from the William Lowell Putnam Mathematical Competition. The benchmark includes formalizations in Lean 4, Isabelle, and Coq, covering a broad range of topics taught in undergraduate mathematics courses. When evaluated on this benchmark, current state-of-the-art neural and symbolic theorem-provers collectively solve at most a handful of problems, establishing PutnamBench as a challenging open problem for the neural theorem-proving community. The benchmark is available at https://github.com/trishullab/PutnamBench.

## Method Summary
The PutnamBench benchmark consists of 1692 formalizations of 640 Putnam competition problems across three formal proof languages (Lean 4, Isabelle, and Coq). For problems requiring numerical answers, factored solutions are provided alongside proofs. The evaluation framework uses a pass@n metric with a budget of n proof attempts per problem. The benchmark is evaluated using existing models including GPT-4, COPRA, ReProver, DSP, Sledgehammer, CoqHammer, and Tactician, with in-context learning and default hyperparameters. No training is performed on the benchmark itself.

## Key Results
- Current state-of-the-art neural and symbolic theorem-provers collectively solve at most a handful of the 640 PutnamBench problems
- The benchmark requires significant problem-solving ability and proficiency in a broad range of undergraduate mathematics topics
- GPT-4 demonstrates limitations in understanding mathematical assumptions and converting informal proofs to valid formal proofs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PutnamBench's difficulty stems from the gap between natural language proof intuition and formal proof execution.
- Mechanism: Neural theorem provers trained on informal mathematical reasoning struggle to bridge the gap to formal proof assistants because the informal-to-formal translation requires both deep mathematical understanding and mastery of proof assistant syntax and tactics.
- Core assumption: Current neural theorem provers can generate informal proofs but cannot reliably convert them into valid formal proofs in proof assistants.
- Evidence anchors:
  - [abstract] "These approaches can only solve a handful of the PutnamBench problems"
  - [section] "GPT-4 misunderstands that the hypothesis [Mul S], which gives an operation ⋆ and asserts it is a binary operation on S, also asserts associativity of the operation"
  - [corpus] Weak evidence - no corpus papers directly address informal-to-formal translation difficulties

### Mechanism 2
- Claim: The multi-language support in PutnamBench reveals language-specific challenges in neural theorem proving.
- Mechanism: Different proof assistants have different foundations, syntax, and available libraries, making it difficult for neural methods to generalize across languages without language-specific training or adaptation.
- Core assumption: Neural theorem provers need language-specific knowledge to effectively work in different proof assistants.
- Evidence anchors:
  - [abstract] "PUTNAM BENCH is the first mathematics-competition benchmark to include problems in all three languages"
  - [section] "Due to differences in the underlying foundations of each language, we found that formalizations in one language sometimes do not directly transfer to another"
  - [corpus] Weak evidence - corpus lacks papers specifically analyzing multi-language generalization in neural theorem proving

### Mechanism 3
- Claim: PutnamBench's focus on undergraduate-level topics creates challenges that high-school level benchmarks don't capture.
- Mechanism: Neural theorem provers trained primarily on high-school level problems struggle with undergraduate concepts like analysis, abstract algebra, and advanced number theory that require deeper mathematical knowledge and more sophisticated proof techniques.
- Core assumption: Current neural theorem provers have limited exposure to advanced undergraduate mathematical concepts during training.
- Evidence anchors:
  - [abstract] "PUTNAM BENCH requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses"
  - [section] "Success in the two competitions is correlated — top performers on the Putnam competition are often former IMO medalists as well"
  - [corpus] Weak evidence - corpus papers focus on competition problems but don't explicitly compare undergraduate vs high-school difficulty

## Foundational Learning

- Concept: Formal theorem proving in proof assistants
  - Why needed here: Understanding the mechanics of proof assistants (Lean, Isabelle, Coq) is essential for interpreting why neural methods struggle with PutnamBench
  - Quick check question: What are the three most popular formal proof frameworks mentioned in the paper?

- Concept: Neural theorem proving methodology
  - Why needed here: Understanding how neural methods approach theorem proving helps explain their limitations on challenging benchmarks
  - Quick check question: What is the primary metric used to evaluate theorem provers in the experiments?

- Concept: Mathematical competition problem structure
  - Why needed here: Understanding how Putnam problems differ from other competition problems (IMO, AMC) explains the benchmark's unique challenges
  - Quick check question: What percentage of Putnam problems require exhibiting a closed-form solution along with a proof?

## Architecture Onboarding

- Component map: PutnamBench consists of 1692 formalizations across three languages (Lean 4, Isabelle, Coq), with factored solutions for problems requiring numerical answers. The evaluation framework uses pass@n metric with budget of n proof attempts per problem.
- Critical path: Formalization → Problem selection from Putnam competition → Translation to proof assistant syntax → Neural prover attempt → Verification in proof assistant
- Design tradeoffs: Multi-language support increases benchmark coverage but creates maintenance complexity; factored solutions preserve problem difficulty but add evaluation complexity
- Failure signatures: Incorrect syntax (Lean 3 vs Lean 4), misunderstanding of mathematical assumptions, inability to generate appropriate witnesses for existential quantifiers, failure to use appropriate tactics for given mathematical domains
- First 3 experiments:
  1. Evaluate GPT-4 on a simple PutnamBench problem using pass@10 metric to establish baseline
  2. Compare performance of neural prover with and without retrieval-augmented lemmas on a representative problem
  3. Test symbolic prover (Sledgehammer) on problems involving binary operations to identify patterns in successful cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can neural theorem-provers be trained to automatically generate and prove new lemmas for Putnam-level problems, rather than just combining existing ones?
- Basis in paper: [inferred] The paper notes that current theorem-provers can "effectively stitch together standard proof steps well-represented in the training corpus, they often fail at synthesizing new lemmas and orchestrating these lemmas into intricate proofs."
- Why unresolved: Current neural theorem-provers lack the ability to creatively generate new mathematical concepts and integrate them into complex proofs.
- What evidence would resolve it: Success rates on PutnamBench problems significantly higher than current handful of solutions, demonstrating ability to generate and use novel lemmas.

### Open Question 2
- Question: How can theorem-provers better leverage existing mathematical libraries to solve PutnamBench problems?
- Basis in paper: [explicit] "Current methods often fail to leverage the deep knowledge available in mathematics repositories."
- Why unresolved: Current methods don't effectively utilize the vast mathematical knowledge encoded in formal libraries.
- What evidence would resolve it: Successful proofs on PutnamBench problems that require accessing and combining knowledge from multiple mathematical domains.

### Open Question 3
- Question: Can we develop theorem-provers that can handle the full range of mathematical topics in PutnamBench, including geometry and probability?
- Basis in paper: [explicit] "We found that while formalizing problems involving probabilities is possible, such formalizations often require heavy probability theory. Similarly, support for problems involving Euclidean geometry varies across languages."
- Why unresolved: Current formal libraries lack sufficient support for certain mathematical domains like geometry and probability.
- What evidence would resolve it: Successful proofs on PutnamBench geometry and probability problems, demonstrating comprehensive mathematical reasoning capabilities.

## Limitations
- The benchmark's difficulty may be partially attributed to evaluation methodology rather than inherent problem complexity
- The evaluation focuses on neural and symbolic theorem provers but does not explore hybrid approaches that combine neural guidance with symbolic reasoning
- The multi-language formalization, while comprehensive, may introduce inconsistencies in problem difficulty due to differences in available libraries and proof automation

## Confidence
- **High Confidence**: The claim that PutnamBench presents significant challenges to current neural and symbolic theorem provers is well-supported by experimental results showing minimal success rates across all methods tested.
- **Medium Confidence**: The claim that undergraduate-level topics create unique challenges compared to high-school level competitions is supported by the benchmark's focus on advanced mathematical concepts, though direct comparison data is limited.
- **Medium Confidence**: The claim that informal-to-formal translation represents a key bottleneck is supported by specific error examples but lacks systematic analysis of this particular failure mode.

## Next Checks
1. **Prompt Optimization Study**: Systematically vary temperature, system prompt content, and few-shot examples for GPT-4 across all three languages to determine if performance improves beyond the baseline pass@10 results reported.

2. **Cross-Language Difficulty Analysis**: Compare success rates of the same problems formalized in different languages to quantify the impact of language-specific libraries and proof automation on solver performance.

3. **Hybrid Method Evaluation**: Implement a hybrid approach combining neural guidance for proof strategy with symbolic methods for tactical execution, then evaluate on PutnamBench to determine if this addresses the informal-to-formal translation bottleneck.