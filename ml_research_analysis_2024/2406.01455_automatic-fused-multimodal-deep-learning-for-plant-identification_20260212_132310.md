---
ver: rpa2
title: Automatic Fused Multimodal Deep Learning for Plant Identification
arxiv_id: '2406.01455'
source_url: https://arxiv.org/abs/2406.01455
tags:
- plant
- fusion
- multimodal
- images
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of accurate plant classification
  using images from multiple plant organs (flowers, leaves, fruits, stems) by proposing
  an automatic multimodal deep learning approach with neural architecture search for
  optimal modality fusion. The method converts the PlantCLEF2015 dataset into a multimodal
  version and trains individual unimodal models for each organ, then uses a modified
  multimodal fusion architecture search (MFAS) algorithm to automatically determine
  the best fusion strategy.
---

# Automatic Fused Multimodal Deep Learning for Plant Identification

## Quick Facts
- arXiv ID: 2406.01455
- Source URL: https://arxiv.org/abs/2406.01455
- Authors: Alfreds Lapkovskis; Natalia Nefedova; Ali Beikmohammadi
- Reference count: 14
- Primary result: 82.61% accuracy on 979 plant classes using multimodal fusion architecture search

## Executive Summary
This paper presents an automatic multimodal deep learning approach for plant identification that fuses images from multiple plant organs (flowers, leaves, fruits, stems). The method converts the PlantCLEF2015 dataset into a multimodal version and uses neural architecture search to automatically determine the optimal fusion strategy across modalities. The proposed approach achieves state-of-the-art performance with 82.61% accuracy while demonstrating strong robustness to missing modalities through multimodal dropout, requiring minimal computational resources during inference.

## Method Summary
The method converts the unimodal PlantCLEF2015 dataset into a multimodal version by grouping images from the same observation and class, then trains individual unimodal models for each organ using MobileNetV3Small. A modified multimodal fusion architecture search (MFAS) algorithm is then employed to automatically discover the optimal fusion point across the unimodal networks, using a surrogate model to guide the search while sharing weights between architectures. Multimodal dropout is incorporated during training to improve robustness to missing modalities by randomly dropping entire modalities with a probability of 0.125.

## Key Results
- Achieves 82.61% accuracy on 979 plant classes, outperforming late fusion baselines by 10.33%
- Demonstrates strong robustness to missing modalities through multimodal dropout
- Provides an efficient, lightweight solution requiring minimal computational resources during inference
- Significantly outperforms state-of-the-art methods while maintaining competitive parameter count

## Why This Works (Mechanism)

### Mechanism 1
The MFAS algorithm automatically discovers an optimal fusion point across multiple modalities, outperforming manual fusion strategies. The algorithm iteratively trains fusion layers at different points in the unimodal network hierarchies, using a surrogate model to guide the search toward high-performing configurations while sharing weights between architectures to reduce computation. This approach is needed because the optimal fusion point varies across tasks and modalities, and manual selection often leads to suboptimal architectures.

### Mechanism 2
Multimodal dropout improves robustness to missing modalities by forcing the model to learn representations that do not overly depend on any single modality. During training, entire modalities are randomly dropped (replaced with zeros) with a probability of 0.125, encouraging the network to distribute information across modalities and preventing overfitting to the most frequent ones. This technique is needed because real-world scenarios often involve incomplete modality availability, and models must generalize well even when some organ images are missing.

### Mechanism 3
Converting a unimodal dataset into a multimodal one by combining plant organ images from the same observation enables the development of multimodal models without requiring new data collection. The preprocessing pipeline groups images by observation and class, filters out underrepresented organs and classes, then creates random combinations of available organs per class to generate fixed-input multimodal samples. This conversion is needed because there is a lack of large-scale multimodal plant datasets, and leveraging existing unimodal datasets provides a practical path to multimodal model development.

## Foundational Learning

- **Multimodal Fusion Architecture Search (MFAS)**: A neural architecture search method that automatically determines optimal fusion points across multiple modalities by iteratively training and evaluating different fusion layer configurations. Needed because manual fusion strategy selection is suboptimal and varies across tasks. Quick check: Verify the surrogate model can accurately rank fusion configurations during search.

- **Multimodal Dropout**: A regularization technique that randomly drops entire modalities during training with a specified probability (0.125 in this work) to improve robustness to missing modalities. Needed because real-world plant identification often involves incomplete data where some organ images may be unavailable. Quick check: Test model performance under varying levels of modality availability.

- **Dataset Conversion from Unimodal to Multimodal**: The process of restructuring a unimodal dataset (PlantCLEF2015) into a multimodal version by grouping images from the same observation and class. Needed because large-scale multimodal plant datasets are scarce, and this approach leverages existing data. Quick check: Verify that images from the same observation are consistent and suitable for multimodal fusion.

## Architecture Onboarding

### Component Map
Unimodal Models (MobileNetV3Small) -> MFAS Fusion Layer Search -> Multimodal Fusion Architecture -> Multimodal Dropout Training -> Final Classification

### Critical Path
The critical path is: Dataset Preprocessing -> Unimodal Model Training -> MFAS Algorithm Execution -> Multimodal Model Fine-tuning. Each stage must complete successfully for the final model to achieve optimal performance.

### Design Tradeoffs
The approach trades computational cost during training (due to MFAS search) for improved accuracy and robustness during inference. Using MobileNetV3Small provides a lightweight backbone but may limit the model's capacity compared to larger architectures. The multimodal dropout rate of 0.125 balances robustness with learning capacity.

### Failure Signatures
- Poor performance on underrepresented classes indicates class imbalance issues
- MFAS convergence failure suggests inadequate search space design or surrogate model limitations
- Robustness degradation under missing modalities indicates multimodal dropout rate is too low or unimodal models are too specialized

### First Experiments
1. Train unimodal models for each organ and evaluate individual organ classification accuracy
2. Run MFAS with a simplified search space to verify the architecture search process works correctly
3. Implement multimodal dropout during training and test robustness to randomly missing modalities

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed model scale with the number of plant classes beyond 979? The paper demonstrates effectiveness on 979 classes but does not explore performance at larger scales. Testing the model on larger datasets with thousands of plant classes and comparing accuracy, training time, and resource requirements would resolve this question.

### Open Question 2
What is the impact of using different backbone architectures (e.g., ResNet, Vision Transformers) instead of MobileNetV3Small for the unimodal models? Only MobileNetV3Small is evaluated, leaving uncertainty about whether other architectures could yield better performance or efficiency. Systematic comparison using various backbone architectures on the same dataset would resolve this question.

### Open Question 3
How does the proposed multimodal dropout technique compare to other robustness methods for handling missing modalities? Only multimodal dropout is evaluated for handling missing modalities, without benchmarking against alternative approaches. Comparative experiments between multimodal dropout, data augmentation strategies, and modality-specific models would resolve this question.

## Limitations

- The study lacks external validation on datasets beyond the converted PlantCLEF2015 data
- The MFAS algorithm's performance heavily depends on the quality of the surrogate model and search space design
- The multimodal dropout implementation may not generalize well to real-world scenarios with unpredictable missing patterns
- The dataset conversion process introduces potential biases from the assumption that images from the same observation are fully compatible

## Confidence

- **High Confidence**: The unimodal model training procedure using MobileNetV3Small and standard data augmentation techniques is well-established and reproducible
- **Medium Confidence**: The overall framework combining MFAS with multimodal dropout is conceptually sound, though specific implementation details require verification
- **Low Confidence**: The claimed superiority over state-of-the-art methods and robustness to missing modalities need independent validation on diverse datasets

## Next Checks

1. **External Dataset Validation**: Test the trained model on other plant identification datasets (e.g., iNaturalist, PlantVillage) to verify generalization beyond the converted PlantCLEF2015 data

2. **Ablation Study on MFAS Components**: Systematically disable individual components of the MFAS algorithm (surrogate model, weight sharing, temperature scheduling) to quantify their contribution to final performance

3. **Real-World Missing Modality Testing**: Design experiments where modalities are missing in non-random patterns reflecting actual field conditions (e.g., flowers only available in spring) to validate the multimodal dropout robustness claims