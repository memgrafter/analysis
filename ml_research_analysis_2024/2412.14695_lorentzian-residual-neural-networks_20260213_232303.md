---
ver: rpa2
title: Lorentzian Residual Neural Networks
arxiv_id: '2412.14695'
source_url: https://arxiv.org/abs/2412.14695
tags:
- hyperbolic
- lresnet
- space
- residual
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LResNet, a novel residual neural network
  for hyperbolic geometry that overcomes limitations of existing methods. LResNet
  uses a weighted Lorentzian centroid to perform residual connections directly in
  the Lorentz model, avoiding complex mappings to and from tangent spaces.
---

# Lorentzian Residual Neural Networks

## Quick Facts
- arXiv ID: 2412.14695
- Source URL: https://arxiv.org/abs/2412.14695
- Reference count: 40
- Primary result: LResNet achieves 10.9% improvement on graph tasks and 0.5% on vision tasks compared to state-of-the-art hyperbolic neural networks

## Executive Summary
This paper introduces LResNet, a novel residual neural network architecture for hyperbolic geometry that overcomes limitations of existing hyperbolic residual methods. LResNet performs residual connections directly in the Lorentz model using a weighted Lorentzian centroid, avoiding complex mappings to and from tangent spaces. This approach is over 2000x faster, numerically stable, commutative, and maintains clear geometric interpretation. The method generalizes previous approaches while maintaining expressiveness. Extensive experiments on graph and vision tasks demonstrate superior performance compared to both Euclidean and hyperbolic alternatives.

## Method Summary
LResNet implements residual connections directly on the Lorentz hyperboloid manifold using a weighted Lorentzian centroid operation. The method normalizes a weighted Euclidean sum of input and function output to project directly onto the hyperboloid, avoiding the need for exponential/logarithmic mappings between tangent and hyperbolic spaces. This approach is computationally efficient (O(n) complexity vs O(n²) for parallel transport), numerically stable due to guaranteed lower bounds on normalization denominators, and commutative by construction. The method can be applied to various hyperbolic neural network architectures including hyperbolic graph neural networks, graph Transformers, and convolutional networks.

## Key Results
- LResNet outperforms existing hyperbolic residual methods on 6 graph datasets with up to 10.9% improvement in F1-score
- Demonstrates superior performance on image classification (CIFAR-10/CIFAR-100) with 0.5% improvement
- Successfully addresses the graph over-smoothing problem in deep hyperbolic networks (4-64 layers)
- Shows better robustness to out-of-distribution samples with improved FPR95, AUROC, and AUPR metrics

## Why This Works (Mechanism)

### Mechanism 1
LResNet's weighted Lorentzian centroid operation avoids mapping errors by operating entirely within the Lorentz model, preserving hierarchical structure more faithfully than tangent space methods. By normalizing the Euclidean weighted sum directly onto the hyperboloid manifold, LResNet bypasses the need for exponential/logarithmic mappings between tangent and hyperbolic spaces, which introduce approximation errors especially for points far from the origin.

### Mechanism 2
LResNet is mathematically commutative, unlike parallel transport-based methods, enabling more flexible and expressive residual connections in hyperbolic neural networks. LResNet implements a generalized weighted sum where x + f(x) = f(x) + x by construction, as the normalization factor is symmetric in its arguments. This contrasts with parallel transport which is non-commutative due to the directional nature of geodesic transport.

### Mechanism 3
LResNet provides numerical stability by ensuring the denominator in the normalization is bounded away from zero, avoiding the floating-point issues that plague parallel transport and tangent space methods. Lemma 4.1 proves that the Lorentzian norm of the weighted sum is always greater than √(w_x² + w_y²), establishing a strict lower bound that prevents division by near-zero values.

## Foundational Learning

- **Hyperbolic geometry and Lorentz model**: Understanding the Lorentz (hyperboloid) model of hyperbolic geometry is essential as LResNet is built specifically on this model, requiring knowledge of its metric, tangent spaces, and geometric properties. Quick check: What distinguishes the Lorentz model from the Poincaré ball model in terms of computational implementation?

- **Riemannian geometry and parallel transport**: Previous hyperbolic residual methods rely on parallel transport operations, which LResNet eliminates. Understanding these methods is crucial for appreciating LResNet's advantages. Quick check: Why does parallel transport in hyperbolic space lead to non-commutativity in residual connections?

- **Residual connections and gradient flow**: LResNet extends the residual connection concept to hyperbolic spaces, requiring understanding of why residual connections help with vanishing gradients and over-smoothing. Quick check: How do residual connections help address the vanishing gradient problem in deep neural networks?

## Architecture Onboarding

- **Component map**: Input → Hyperbolic layer → Weighted sum → Lorentzian normalization → Output
- **Critical path**: The critical computational path is: input → hyperbolic layer → weighted sum → Lorentzian normalization → output. The normalization step is the key operation that ensures geometric validity and numerical stability.
- **Design tradeoffs**: LResNet trades off the geometric interpretability of parallel transport for computational efficiency and numerical stability. The method assumes that direct projection onto the hyperboloid preserves sufficient hierarchical information.
- **Failure signatures**: Common failure modes include NaN values from overflow in the normalization step with extremely large weights, poor performance when data hierarchy doesn't align with hyperbolic geometry assumptions, and suboptimal results if curvature parameter K is poorly chosen.
- **First 3 experiments**: 
  1. Implement LResNet on a simple graph with known hierarchical structure and compare against Euclidean residual connections on link prediction accuracy.
  2. Test numerical stability by running LResNet with varying weight magnitudes and measuring the frequency of NaN or Inf outputs.
  3. Evaluate commutativity empirically by comparing forward (x ⊕ f(x)) and backward (f(x) ⊕ x) residual operations on the same dataset.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text.

## Limitations

- Performance improvements on vision tasks are modest (only 0.5% gain on CIFAR-100) compared to substantial gains on graph tasks
- Numerical stability proof relies on specific conditions that may not hold for all weight configurations, particularly with extremely large or pathological values
- All experimental validation focuses on hierarchical datasets; performance on non-hierarchical data remains unexplored

## Confidence

- **High Confidence**: Numerical stability claims (rigorous proof with clear bounds), computational efficiency improvements (explicit O(n) vs O(n²) complexity), and commutativity property (explicit theorem and proof)
- **Medium Confidence**: Performance improvements on graph tasks (extensive experiments but limited to specific datasets), and the claim about addressing graph over-smoothing (logical connection but not extensively validated)
- **Low Confidence**: Vision task improvements (only 0.5% gain, limited dataset scope), and OOD detection robustness claims (limited experimental validation)

## Next Checks

1. Test LResNet with extreme weight magnitudes (10⁶ to 10⁻⁶) to verify numerical stability guarantees hold across orders of magnitude
2. Conduct ablation studies specifically testing whether commutativity property affects representational capacity on non-hierarchical graph datasets
3. Evaluate LResNet on additional vision datasets (ImageNet, MNIST) to assess generalizability beyond CIFAR experiments