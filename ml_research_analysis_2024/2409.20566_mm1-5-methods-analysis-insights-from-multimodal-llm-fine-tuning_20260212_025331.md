---
ver: rpa2
title: 'MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning'
arxiv_id: '2409.20566'
source_url: https://arxiv.org/abs/2409.20566
tags:
- arxiv
- image
- data
- preprint
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MM1.5, a family of multimodal large language
  models (MLLMs) designed to enhance capabilities in text-rich image understanding,
  visual referring and grounding, and multi-image reasoning. Building upon the MM1
  architecture, MM1.5 adopts a data-centric approach to model training, systematically
  exploring the impact of diverse data mixtures across the entire model training lifecycle.
---

# MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning

## Quick Facts
- arXiv ID: 2409.20566
- Source URL: https://arxiv.org/abs/2409.20566
- Reference count: 40
- Primary result: Data-centric fine-tuning approach yields strong performance in multimodal understanding even at small scales (1B-30B parameters)

## Executive Summary
MM1.5 is a family of multimodal large language models that advances text-rich image understanding, visual referring and grounding, and multi-image reasoning through systematic exploration of data mixtures across the entire training lifecycle. Building on the MM1 architecture, the models range from 1B to 30B parameters with both dense and MoE variants. The work demonstrates that careful data curation and training strategies can achieve strong performance even at small scales, while introducing specialized variants for video and mobile UI understanding. Extensive empirical studies provide insights into training processes that inform future MLLM development.

## Method Summary
MM1.5 employs a three-stage training approach: pre-training on 2B image-text pairs and 600M interleaved image-text documents, continual pre-training with high-resolution OCR data (45M examples), and supervised fine-tuning with carefully balanced data mixtures. Key innovations include dynamic high-resolution image splitting (nmin=4, nmax=9) with overview images, a C-Abstractor vision-language connector, and optimized sampling weights across capability-specific data categories. The approach systematically explores the impact of diverse data mixtures, achieving strong performance even at small scales through careful data curation rather than model scaling alone.

## Key Results
- Strong performance on text-rich understanding benchmarks achieved through high-resolution OCR data and dynamic image splitting
- Effective capability balancing via optimized SFT data mixture (80% single-image, 10% multi-image, 10% text-only)
- Successful scaling from 1B to 30B parameters with both dense and MoE variants
- Specialized MM1.5-Video and MM1.5-UI variants demonstrate targeted capability improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual pre-training with high-resolution OCR data improves text-rich image understanding
- Mechanism: Higher resolution input allows the model to see finer text details during training, which translates into better recognition and comprehension of text in images during inference
- Core assumption: The model can effectively utilize additional detail in high-resolution images when trained on OCR-focused datasets
- Evidence anchors:
  - [abstract]: "high-quality OCR data and synthetic captions for continual pre-training"
  - [section]: "We find that this high-resolution setup is essential for continual pre-training"
  - [corpus]: Weak—no direct citations in corpus, but implied by contrast with MM1 which didn't emphasize OCR in continual stage
- Break condition: If image resolution exceeds the model's effective token limit without corresponding gains, or if OCR data quality is poor

### Mechanism 2
- Claim: Dynamic image splitting enables efficient high-resolution processing while preserving spatial structure
- Mechanism: Images are split into sub-images based on aspect ratio and resolution constraints, minimizing padding and resolution loss while maintaining global-local context through an overview image
- Core assumption: The model can reconstruct spatial relationships from sub-image tokens and global overview without explicit positional indicators
- Evidence anchors:
  - [section]: "dynamic image splitting approach, which is common in the literature [71, 27, 46, 99, 165, 185]"
  - [section]: "Dynamic image splitting using a larger nmax is especially well suited for unusual aspect ratios"
  - [corpus]: Weak—general trends in vision-LLM literature support dynamic splitting, but no direct citations in corpus
- Break condition: If sub-image token count exceeds LLM context window or if spatial reconstruction degrades with high sub-image counts

### Mechanism 3
- Claim: Careful SFT data mixture balances diverse capabilities without catastrophic forgetting
- Mechanism: By categorizing SFT data into capability groups and tuning sampling weights, the model maintains general performance while enhancing specific skills like referring/grounding and multi-image reasoning
- Core assumption: Different data categories contribute independently to different capabilities, and sampling weights can be optimized without negative interference
- Evidence anchors:
  - [section]: "We first study the impact of different data categories in Section 3.2.1, followed by investigating how to best mix all the data in Section 3.2.2"
  - [section]: "We observe that adding refer&ground data significantly boosts referring and grounding performance"
  - [corpus]: No direct evidence—this is a novel experimental contribution of MM1.5
- Break condition: If certain data categories dominate training and suppress other capabilities, or if mixing ratios are poorly balanced

## Foundational Learning

- Concept: Vision-language connector design and its impact on multimodal integration
  - Why needed here: The paper evaluates different connectors implicitly through ablation studies; understanding their role is key to interpreting architectural choices
  - Quick check question: What is the role of the vision-language connector in aligning visual and textual features for the LLM?

- Concept: Mixture-of-experts (MoE) scaling in LLMs
  - Why needed here: MM1.5 introduces MoE variants; understanding how expert routing and load balancing work is essential for interpreting performance gains
  - Quick check question: How does top-k expert routing with load balancing loss affect model capacity and inference efficiency?

- Concept: High-resolution image encoding and position embedding interpolation
  - Why needed here: Dynamic splitting and resolution scaling are central to MM1.5's text-rich understanding; understanding how resolution affects token generation is critical
  - Quick check question: What is the relationship between image resolution, sub-image count, and effective token count in dynamic splitting?

## Architecture Onboarding

- Component map: Image encoder → Vision-language connector (C-Abstractor) → LLM decoder. Dynamic splitting module preprocesses high-res images into sub-images + overview. MoE routing in LLM FFN layers for MoE variants
- Critical path: Input image → dynamic splitting → sub-image encoding → token concatenation → connector → LLM → text output (with optional bounding boxes)
- Design tradeoffs: Static vs dynamic splitting (simplicity vs efficiency), dense vs MoE (parameter count vs compute), high-res vs context window (detail vs token limits)
- Failure signatures: Poor text-rich performance → check OCR data quality and resolution; degraded referring/grounding → check sub-image grid config; imbalanced capabilities → check SFT data mixture weights
- First 3 experiments:
  1. Validate dynamic splitting with a simple high-res image and inspect sub-image grid selection
  2. Test OCR continual pre-training with 1.3M synthetic captions vs baseline to confirm gains
  3. Ablate SFT mixture weights (wsingle, wmulti, wtext) to find optimal balance for target benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise impact of synthetic captions generated through self-training compared to OCR data for continual pre-training?
- Basis in paper: [explicit] Section 3.3 mentions that synthetic captions were investigated but results were inconclusive, with the authors deferring further study to future work
- Why unresolved: The paper states that while prior studies showed synthetic captions boost performance, their results did not find conclusive evidence. They only explored a limited set of synthetic caption datasets and did not investigate the quality, distribution, or style of the generated captions
- What evidence would resolve it: Systematic ablation studies varying the volume, quality, and style of synthetic captions, compared against a comprehensive set of OCR data, would clarify the impact

### Open Question 2
- Question: How do larger model scales (beyond 30B parameters) affect performance on text-rich benchmarks, particularly for tasks like DocVQA and InfoVQA?
- Basis in paper: [inferred] Section 4.1 mentions that performance improvements are greater with larger LLM backbones, but the study only went up to 30B parameters. It also notes that the 7B and 30B models appear to have plateaued
- Why unresolved: The paper does not explore model scales beyond 30B, and the plateau observed in larger models suggests potential limitations in data diversity or overfitting
- What evidence would resolve it: Training and evaluating MM1.5 models with scales exceeding 30B parameters, using diverse and high-quality datasets, would reveal the true scaling behavior and potential limitations

### Open Question 3
- Question: What is the optimal mixture of single-image, multi-image, and text-only data for SFT to achieve the best balance across all capabilities?
- Basis in paper: [explicit] Section 3.2.2 presents ablation studies on mixing ratios, but the authors acknowledge that enumerating all combinations would be computationally expensive. They settled on a mixture of 80% single-image, 10% multi-image, and 10% text-only data
- Why unresolved: The paper only explored a limited subset of possible mixing ratios due to computational constraints, and the impact of different ratios on various capabilities is not fully understood
- What evidence would resolve it: Conducting a comprehensive grid search or using optimization techniques to find the optimal mixing ratios for different model scales and capabilities would provide a definitive answer

## Limitations

- Heavy reliance on proprietary datasets and in-house synthetic generation pipelines that are not publicly available
- Performance gains may be data-dependent rather than methodology-transferable
- Limited evaluation of specialized variants (MM1.5-Video, MM1.5-UI) on narrow task sets
- No assessment of catastrophic forgetting when models are specialized versus maintaining general capabilities

## Confidence

- Confidence in core data mixture methodology: High (supported by extensive ablation studies across 35 benchmarks)
- Confidence in architectural innovations: Medium (builds on established techniques with MM1.5-specific optimizations)
- Confidence in claimed performance gains: Medium-Low (due to proprietary datasets and incomplete baseline specification)

## Next Checks

1. **Data Dependency Isolation**: Replicate the SFT performance gains using only publicly available datasets to determine whether the improvements stem from data quality or training methodology

2. **Dynamic Splitting Efficiency**: Systematically vary nmin and nmax parameters while measuring both text-rich understanding performance and computational overhead to validate the claimed efficiency gains

3. **Generalization to New Domains**: Test MM1.5 models on out-of-distribution multimodal tasks not included in the training data mixture to assess true capability transfer versus memorization of training patterns