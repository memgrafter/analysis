---
ver: rpa2
title: Towards Neural Scaling Laws for Time Series Foundation Models
arxiv_id: '2410.12360'
source_url: https://arxiv.org/abs/2410.12360
tags:
- scaling
- time
- data
- series
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the scaling behavior of time series foundation
  models (TSFMs) in both in-distribution (ID) and out-of-distribution (OOD) settings.
  We train and evaluate encoder-only and decoder-only Transformer architectures across
  varying model sizes, compute budgets, and dataset scales.
---

# Towards Neural Scaling Laws for Time Series Foundation Models

## Quick Facts
- arXiv ID: 2410.12360
- Source URL: https://arxiv.org/abs/2410.12360
- Reference count: 34
- Key outcome: TSFMs follow power-law scaling in both ID and OOD settings, with encoder-only models showing better ID scalability and MAPE improvements larger in OOD scenarios

## Executive Summary
This paper investigates neural scaling laws for time series foundation models (TSFMs) across in-distribution (ID) and out-of-distribution (OOD) settings. Through systematic experiments varying model size, compute budget, and dataset scale, the authors demonstrate that TSFMs exhibit power-law scaling behavior in both ID and OOD scenarios. The study compares encoder-only and decoder-only Transformer architectures, revealing that encoder-only models show slightly better scalability in ID settings, while decoder-only models exhibit comparable OOD performance. The findings provide practical design principles for scaling TSFMs, emphasizing the importance of dataset diversity, model architecture, and compute efficiency.

## Method Summary
The authors construct a balanced, high-quality time series corpus from ~17B time points across 39 datasets spanning 7 domains. They train encoder-only and decoder-only Transformer architectures with varying parameter counts (1K to 100M), dataset sizes (10M to 1B time points), and compute budgets. Models are evaluated on ID test sets (5% of training data) and OOD test sets from LSF and Monash datasets using metrics including NLL, MAPE, SMAPE, MASE, and CRPS. The study analyzes scaling behavior across model size, compute, and data dimensions, comparing different architectural choices and their impact on scalability.

## Key Results
- TSFMs exhibit power-law scaling behavior in both ID and OOD settings across five orders of magnitude in model size
- Encoder-only Transformers demonstrate slightly better scalability than decoder-only Transformers in ID data scenarios
- MAPE improvements are greater in OOD performance when scaling model size compared to ID performance
- Advanced TSFMs like Moirai and Chronos improve ID performance but compromise OOD scalability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural scaling laws apply to TSFMs in both ID and OOD settings, following power-law behavior across model parameters, compute, and dataset size.
- Mechanism: As model size, compute budget, or dataset size increases, TSFMs exhibit predictable performance improvements following a power-law relationship, where performance loss decreases proportionally to a scaling exponent.
- Core assumption: The relationship between model scale and performance is stable and predictable across different data distributions (ID vs OOD).
- Evidence anchors:
  - [abstract] "Our experiments reveal that the negative log-likelihood of TSFMs exhibits similar scaling behavior in both OOD and ID settings."
  - [section 3.1] "Both ID and OOD performance roughly follow power-law behavior over five orders of magnitude in model sizes."
  - [corpus] Weak - limited citations for scaling laws in TSFMs specifically

### Mechanism 2
- Claim: Encoder-only Transformers demonstrate better scalability than decoder-only Transformers, particularly in ID data.
- Mechanism: The bidirectional attention in encoder-only models allows for more efficient information processing and better parameter utilization, leading to improved scalability in in-distribution settings.
- Core assumption: The architectural differences between encoder-only and decoder-only models create meaningful differences in how they scale with increased parameters.
- Evidence anchors:
  - [abstract] "The encoder-only Transformers demonstrate better scalability than the decoder-only Transformers in ID data"
  - [section 3.2] "encoder-only Transformer shows a slight advantage, with a marginally higher power-law exponent on ID data"
  - [corpus] Weak - limited comparative studies between these architectures for TSFMs

### Mechanism 3
- Claim: Increasing model size yields greater improvements in OOD performance than ID performance when using MAPE as the metric.
- Mechanism: Larger models develop more generalizable representations that transfer better to unseen distributions, and MAPE's relative error measurement amplifies improvements in diverse OOD scenarios.
- Core assumption: Model capacity directly correlates with generalization ability across data distributions.
- Evidence anchors:
  - [section 3.1] "When evaluated using MAPE, the power-law for OOD scenario shows a bigger exponent value than ID scenario"
  - [section 3.1] "increasing model size yields greater improvements in OOD performance than ID performance"
  - [corpus] Weak - limited evidence on how different metrics interact with scaling laws

## Foundational Learning

- Concept: Power-law scaling relationships
  - Why needed here: The paper establishes that TSFM performance follows predictable power-law patterns as model size, compute, and data scale up, which is fundamental to understanding how to optimize resource allocation.
  - Quick check question: If a model's performance improves from MAPE 20 to 10 when doubling the parameter count, what exponent α would this represent in the power-law equation L(N) ∝ (Nc/N)^α?

- Concept: In-distribution vs Out-of-distribution generalization
  - Why needed here: The paper investigates how TSFMs perform on both ID data (seen during training) and OOD data (unseen), which is crucial for understanding real-world applicability.
  - Quick check question: If a model achieves NLL of 2.0 on ID data and 3.5 on OOD data with the same parameter count, what does this tell you about the model's generalization capability?

- Concept: Architectural differences in Transformers (encoder-only vs decoder-only)
  - Why needed here: The paper compares these two architectures to understand how architectural choices affect scaling behavior and performance.
  - Quick check question: How does the attention mechanism differ between encoder-only and decoder-only Transformers, and why might this impact their scalability?

## Architecture Onboarding

- Component map: Data preprocessing → Patch embedding (32-patch size, 32dm) → Rotary position encoding → Transformer layers (attention) → Mixture distribution prediction (512dm Student-T) → Loss calculation
- Critical path: Data preprocessing → Patch embedding → Positional encoding → Transformer layers → Mixture distribution prediction → Loss calculation and backpropagation
- Design tradeoffs: Encoder-only vs decoder-only architectures offer different attention patterns (bidirectional vs unidirectional), affecting scalability and performance. Mixture distributions (Student-T vs Gaussian) impact robustness to outliers but add complexity.
- Failure signatures: Unstable loss convergence during training (particularly with Gaussian mixtures), poor OOD generalization despite good ID performance, or failure to follow expected power-law scaling patterns.
- First 3 experiments:
  1. Train encoder-only and decoder-only Transformers with identical configurations on a subset of the pre-training data, comparing NLL and MAPE metrics to establish baseline scaling behavior.
  2. Vary model size systematically (e.g., 10K, 100K, 1M, 10M parameters) while keeping other factors constant to observe power-law relationships.
  3. Test models on both ID and OOD datasets to quantify generalization gaps and compare scaling exponents between distributions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do scaling laws for time series foundation models (TSFMs) change when extended to multivariate time series forecasting, considering the additional complexities of variable interactions and correlations?
- Basis in paper: [explicit] The paper discusses the extension of scaling laws to multivariate time series as a future research direction, highlighting challenges such as the scarcity of large and diverse multivariate datasets and the need to analyze the impact of variable count and correlation strength.
- Why unresolved: Multivariate time series forecasting introduces additional complexities not present in univariate scenarios, such as modeling variable interactions and correlations, which require different experimental setups and theoretical frameworks.
- What evidence would resolve it: Empirical studies demonstrating scaling laws for multivariate TSFMs with varying numbers of variables and correlation strengths, along with theoretical analyses explaining the observed scaling behaviors.

### Open Question 2
- Question: What is the optimal data mixing strategy for pre-training TSFMs to maximize performance across diverse time series domains, and how can this be determined analytically rather than through exhaustive empirical research?
- Basis in paper: [explicit] The paper suggests that investigating the impact of data mixing strategies on model performance is significant, noting the potential for performance bias or degradation due to different strategies. It proposes developing an analytical framework to balance experimental complexity.
- Why unresolved: Time series pre-training datasets often contain data from numerous sources, creating exponentially large possibilities for combinations. Determining the optimal strategy analytically is challenging due to the complexity of interactions between different data sources.
- What evidence would resolve it: A validated analytical framework that predicts the performance impact of different data mixing strategies, along with empirical results demonstrating improved model performance using the optimal strategy.

### Open Question 3
- Question: How do specific architectural modules in TSFMs, such as attention mechanisms and positional encodings, influence scalability, particularly in out-of-distribution (OOD) scenarios?
- Basis in paper: [explicit] The paper discusses the impact of model architecture on scalability, noting that certain modifications in advanced TSFMs like Moirai and Chronos improve in-distribution (ID) performance but compromise OOD scalability. It highlights the importance of module design in determining scalability.
- Why unresolved: While the paper identifies that architectural choices affect scalability, it does not provide a detailed analysis of how specific modules, such as attention mechanisms and positional encodings, individually contribute to scalability in OOD settings.
- What evidence would resolve it: Systematic ablation studies isolating the effects of specific architectural modules on scalability, along with theoretical analyses explaining their contributions to model performance in OOD scenarios.

## Limitations

- Dataset representativeness: The specific selection of 39 datasets across 7 domains may not capture all real-world time series variations, potentially limiting generalizability.
- Architectural specificity: The study doesn't explore hybrid architectures or attention mechanisms beyond standard Transformers, leaving uncertainty about whether findings extend to more complex architectures.
- Metric sensitivity: The differential scaling behavior between NLL and MAPE suggests scaling laws may be metric-dependent, but this isn't fully explored.

## Confidence

- High confidence: The existence of power-law scaling behavior in TSFMs across model parameters, compute, and dataset size.
- Medium confidence: The comparative scalability advantage of encoder-only over decoder-only architectures in ID settings.
- Medium confidence: The greater OOD improvements when scaling model size, particularly when measured with MAPE.

## Next Checks

1. **Cross-domain validation**: Test the scaling laws on a completely independent time series corpus from different domains (e.g., financial, biomedical, sensor data) to verify whether the observed power-law relationships hold across diverse data distributions.

2. **Architecture ablation study**: Systematically vary attention mechanisms (full vs sparse, local vs global) and positional encoding schemes to determine which architectural components most strongly influence scaling behavior and whether the encoder-only advantage persists under these variations.

3. **Metric robustness analysis**: Conduct controlled experiments varying evaluation metrics (e.g., testing both symmetric and asymmetric error measures) to determine whether the differential scaling behavior between ID and OOD performance is consistent across different performance metrics.