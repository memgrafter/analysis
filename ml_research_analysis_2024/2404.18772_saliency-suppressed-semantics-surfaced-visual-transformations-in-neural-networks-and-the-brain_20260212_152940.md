---
ver: rpa2
title: 'Saliency Suppressed, Semantics Surfaced: Visual Transformations in Neural
  Networks and the Brain'
arxiv_id: '2404.18772'
source_url: https://arxiv.org/abs/2404.18772
tags:
- saliency
- semantic
- visual
- image
- salient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how neural networks and the human brain encode
  visual information at different levels of abstraction. The authors use representational
  similarity analysis (RSA) to measure alignment between network features and image
  attributes - visual saliency (low-level) and semantic similarity (high-level).
---

# Saliency Suppressed, Semantics Surfaced: Visual Transformations in Neural Networks and the Brain

## Quick Facts
- **arXiv ID:** 2404.18772
- **Source URL:** https://arxiv.org/abs/2404.18772
- **Reference count:** 23
- **Primary result:** CLIP training enhances semantic encoding while suppressing saliency in neural networks, with semantic processing driving alignment with human brain representations.

## Executive Summary
This paper examines how neural networks and the human brain encode visual information at different levels of abstraction. Using representational similarity analysis (RSA), the authors measure alignment between network features and image attributes - visual saliency (low-level) and semantic similarity (high-level). They introduce a custom dataset with systematically manipulated saliency and semantic information to study causal effects. Key findings include that ResNets are more sensitive to saliency than Vision Transformers (ViTs), CLIP training enhances both saliency suppression and semantic encoding in both architectures, and semantic encoding strongly predicts alignment with brain activity while saliency suppression negatively predicts it. The results suggest that semantic processing drives alignment between AI and human vision, while saliency suppression is a non-brain-like strategy.

## Method Summary
The authors use representational similarity analysis (RSA) to compare neural network features with image attributes across different architectures and training objectives. They extract features from ResNets and Vision Transformers trained on ImageNet and CLIP objectives, then compute representational dissimilarity matrices (RDMs) for network activations, saliency maps, and caption embeddings. These RDMs are correlated to assess alignment between low-level saliency processing and high-level semantic encoding. The analysis is validated using a custom dataset with systematically manipulated saliency and semantic information, as well as brain activity data from the Natural Scenes Dataset (NSD) containing fMRI responses from human subjects viewing COCO images.

## Key Results
- CLIP training enhances semantic encoding in both ResNets and Vision Transformers, with CLIP-ResNets showing significantly higher semantic RSA scores than their ImageNet counterparts
- ResNets are more sensitive to visual saliency than ViTs, with CLIP training enhancing saliency suppression primarily in ResNets
- Semantic encoding strongly predicts alignment with brain activity (r = 0.83, p < .001), while saliency suppression negatively predicts brain alignment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CLIP training enhances semantic encoding while suppressing saliency, aligning AI visual processing more closely with human brain representations.
- **Mechanism:** CLIP training uses natural language supervision, forcing the model to associate images with rich semantic descriptions rather than just object categories. This encourages the network to prioritize semantic features over low-level saliency cues during visual processing.
- **Core assumption:** Semantic encoding is the primary driver of alignment between AI and human visual cortex, while saliency suppression is a non-brain-like strategy that paradoxically improves alignment by forcing the network to rely on higher-level features.
- **Evidence anchors:** [abstract] "CLIP also enhances semantic encoding in both architectures. Finally, we show that semantic encoding is a key factor in aligning AI with human visual perception, while saliency suppression is a non-brain-like strategy."

### Mechanism 2
- **Claim:** Visual Transformers (ViTs) are inherently less sensitive to saliency than Convolutional Neural Networks (ResNets) due to architectural differences in receptive field and feature processing.
- **Mechanism:** ViTs process images as sequences of patches, allowing them to capture long-range dependencies and global context. This architectural bias makes them less reliant on local contrast-based saliency features compared to ResNets, which process images through local convolutional filters optimized for detecting local features like edges and textures.
- **Core assumption:** The inductive bias of an architecture towards certain types of features (local vs. global) determines its sensitivity to saliency information.
- **Evidence anchors:** [section] "Regarding saliency, CLIP seems to have an effect primarily in ResNets... This suggests that suppression of saliency might be a common strategy employed by ResNets, which is enhanced by natural language supervision."

### Mechanism 3
- **Claim:** Representational Similarity Analysis (RSA) effectively quantifies alignment between neural network features and image attributes, revealing architectural and training objective differences in visual processing.
- **Mechanism:** RSA converts activations, saliency maps, and caption embeddings into representational dissimilarity matrices (RDMs) and measures their correlation. This allows for quantitative comparison of how different networks encode low-level (saliency) and high-level (semantic) information, revealing systematic differences based on architecture and training.
- **Core assumption:** RSA can capture meaningful differences in distributed representations that reflect actual processing strategies, and these differences correlate with human brain activity.
- **Evidence anchors:** [section] "We assess the alignment between neural representations and saliency/semantics by first converting activations, saliency maps, and caption embeddings into representational dissimilarity matrices (RDMs). We then quantify the degree of alignment using non-parametric regression."

## Foundational Learning

- **Concept: Representational Similarity Analysis (RSA)**
  - Why needed here: RSA is the core method for quantifying alignment between network features and image attributes, enabling the comparison of different architectures and training objectives.
  - Quick check question: How does RSA convert neural activations into a form that can be compared with saliency maps and semantic embeddings?

- **Concept: Visual Saliency**
  - Why needed here: Saliency represents low-level image features that are behaviorally distracting and have an established neural basis, making it a key attribute for understanding early visual processing.
  - Quick check question: What are the key components that define visual saliency in the context of this paper?

- **Concept: Semantic Similarity**
  - Why needed here: Semantic similarity represents high-level image content derived from captions, allowing the study of how networks encode abstract concepts and their alignment with human perception.
  - Quick check question: How is semantic similarity operationalized in this paper, and why is it important for understanding visual transformations?

## Architecture Onboarding

- **Component map:** COCO images -> Neural networks (ResNets/ViTs) -> Feature extraction -> RDMs -> RSA correlation -> Alignment scores
- **Critical path:** 1) Extract network features from different layers, 2) Compute RDMs for network features, saliency maps, and semantic embeddings, 3) Calculate RSA scores to measure alignment, 4) Analyze differences based on architecture and training, 5) Validate findings using controlled dataset and brain data
- **Design tradeoffs:** Using RSA provides quantitative metrics but may not capture all aspects of visual processing; the controlled dataset allows for causal analysis but may not fully represent real-world complexity; brain data from NSD provides validation but is limited to specific image types and viewing conditions
- **Failure signatures:** If RSA scores do not correlate with expected differences in processing strategies; if controlled dataset manipulations do not produce expected effects on network representations; if brain alignment patterns do not match those predicted by network analysis
- **First 3 experiments:** 1) Compute RSA scores for a simple ResNet and ViT on a small set of COCO images to verify basic functionality, 2) Create a minimal controlled dataset with one salient and one non-salient distractor to test the manipulation method, 3) Compare brain alignment patterns for a single network layer with high semantic RSA to verify the brain correlation method

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does CLIP training enhance saliency suppression in CNNs through the integration of language supervision or through exposure to more complex visual scenes?
- **Basis in paper:** [inferred] The paper notes that the control analysis using SWSL ResNet-50 (trained on 940M images) does not fully replicate CLIP's effects on saliency suppression, suggesting that dataset size and diversity alone do not account for the observed effects.
- **Why unresolved:** The paper cannot fully disentangle the effects of CLIP's language supervision from its use of a larger, more diverse dataset compared to standard ImageNet training.
- **What evidence would resolve it:** Training a CNN with language supervision on a dataset of ImageNet size and diversity, and comparing its saliency suppression effects to CLIP and SWSL ResNet-50, would help determine the relative contributions of language supervision and dataset complexity.

### Open Question 2
- **Question:** What are the neural mechanisms underlying the difference in saliency sensitivity between CNNs and Vision Transformers?
- **Basis in paper:** [explicit] The paper finds that ResNets are more sensitive to saliency than ViTs, and hypothesizes that this is due to architectural differences, such as ViTs having larger receptive fields and being better equipped to capture long-range dependencies.
- **Why unresolved:** The paper does not provide direct evidence linking the architectural differences to the observed differences in saliency sensitivity.
- **What evidence would resolve it:** Circuit-level analyses and formal links between saliency maps and CNN computations, as suggested by the paper, would provide insights into the neural mechanisms underlying the difference in saliency sensitivity.

### Open Question 3
- **Question:** How does saliency suppression in CNNs affect their generalization abilities and robustness to out-of-distribution images?
- **Basis in paper:** [inferred] The paper mentions that CLIP-trained models make non-human errors but are more robust to out-of-distribution images, potentially due to the enhancement of saliency suppression.
- **Why unresolved:** The paper does not directly investigate the relationship between saliency suppression and generalization abilities or robustness to out-of-distribution images.
- **What evidence would resolve it:** Comparing the performance of CNNs with and without saliency suppression on out-of-distribution image datasets would help determine the impact of saliency suppression on generalization and robustness.

## Limitations

- **Confounded Training Objectives:** The comparison between ImageNet and CLIP training is complicated by CLIP's use of a larger, more diverse dataset (WebImageText) compared to ImageNet, making it difficult to isolate the effects of language supervision from dataset differences.
- **Dataset Construction Uncertainty:** The custom dataset with manipulated saliency and semantic information is central to establishing causal relationships, but the paper provides limited detail on the exact methodology for creating these images and measuring saliency/semantic values.
- **Brain Data Specificity:** While the NSD dataset provides valuable validation, it consists of responses to COCO images viewed in specific experimental conditions, raising questions about generalizability to natural viewing scenarios.

## Confidence

**High Confidence:** The finding that CLIP training enhances semantic encoding in both ResNets and ViTs is well-supported by the RSA analysis showing consistent patterns across architectures. The correlation between semantic encoding and brain alignment (r = 0.83, p < .001) is robust and statistically significant.

**Medium Confidence:** The claim that ResNets are more sensitive to saliency than ViTs has strong evidence from the controlled dataset experiments, but the underlying mechanism could be influenced by factors beyond architectural differences, such as implementation details or training hyperparameters.

**Low Confidence:** The assertion that saliency suppression is a "non-brain-like strategy" that paradoxically improves alignment is counterintuitive and requires more direct evidence. While the data shows negative correlation between saliency suppression and brain alignment, the causal interpretation and implications for human visual processing are speculative.

## Next Checks

1. **Controlled Dataset Replication:** Replicate the saliency manipulation experiments using an independent dataset and methodology to verify that ResNets consistently show greater saliency sensitivity than ViTs across different image types and manipulation approaches.

2. **Ablation Study on CLIP Components:** Train a ResNet on ImageNet with additional data augmentation or regularization that matches CLIP's data diversity, but without language supervision, to isolate whether the enhanced semantic encoding is due to dataset size or the language objective.

3. **Cross-Subject Brain Consistency:** Analyze brain alignment patterns across individual subjects in the NSD dataset to determine whether the strong correlation between semantic encoding and brain scores holds consistently at the individual level, or if it emerges primarily at the group level.