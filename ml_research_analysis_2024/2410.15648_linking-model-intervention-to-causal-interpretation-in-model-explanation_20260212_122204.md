---
ver: rpa2
title: Linking Model Intervention to Causal Interpretation in Model Explanation
arxiv_id: '2410.15648'
source_url: https://arxiv.org/abs/2410.15648
tags:
- direct
- causal
- unobserved
- causes
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes conditions under which model intervention
  effects can provide causal interpretations of feature importance. The model intervention
  effect measures how predictions change when a feature is toggled between baseline
  and current values.
---

# Linking Model Intervention to Causal Interpretation in Model Explanation

## Quick Facts
- arXiv ID: 2410.15648
- Source URL: https://arxiv.org/abs/2410.15648
- Authors: Debo Cheng; Ziqi Xu; Jiuyong Li; Lin Liu; Kui Yu; Thuc Duy Le; Jixue Liu
- Reference count: 40
- Primary result: Model intervention effects (AMIE) can indicate causal relationships under conditions of no unobserved direct causes and exclusion of outcome descendants

## Executive Summary
This paper establishes conditions under which model intervention effects can provide causal interpretations of feature importance. The model intervention effect measures how predictions change when a feature is toggled between baseline and current values. While inherently associative, the paper shows that under certain conditions—specifically, when no post-outcome variables are included in the feature set and either (1) no unobserved variables exist or (2) unobserved variables have observed activators or proxies—non-zero model intervention effects indicate direct causal relationships. The work identifies three types of unobserved variables: activators (observed features triggering unobserved causes), proxies (observed features representing unobserved causes), and standalone unobserved causes. The latter prevents causal interpretation. Experiments on semi-synthetic datasets validate these conditions, showing consistency between model intervention effects and true direct causes ranging from 85-100% under appropriate conditions, while model accuracy drops significantly when standalone unobserved direct causes are present.

## Method Summary
The paper proposes using model intervention effects (AMIE) to identify causal relationships between features and outcomes. The method involves calculating AMIE by toggling each feature between baseline and current values, then comparing features with non-zero AMIEs to ground truth causal structures from known DAGs. The approach is validated using semi-synthetic datasets generated from Bayesian networks (Insurance and Water datasets) with 10,000 instances each. Two classification methods (Logistic Regression and Random Forest) are trained on these datasets, and AMIEs are calculated and compared to true direct causes from the DAGs. Independence tests are used to detect false linkages when unobserved variables are present.

## Key Results
- Under conditions of no unobserved direct causes and exclusion of outcome descendants, AMIE≠0 indicates direct causal relationships with 85-100% consistency
- When unobserved variables have activators or proxies, AMIE can still identify causal relationships through observed features
- Model accuracy drops significantly (to 70-75%) when standalone unobserved direct causes are present, making AMIE interpretation unreliable
- False positive linkages occur through inducing paths or when features are parents/proxies of unobserved causes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model intervention effect (AMIE) can indicate causal relationships under specific structural assumptions
- Mechanism: When a DAG contains no unobserved direct causes of Y and X excludes Y's descendants, AMIE≠0 ⇔ Xi is a direct cause of Y
- Core assumption: Assumptions 1 (faithfulness between data and causal DAG) and 2 (model consistency with data) hold, and no unobserved variables exist
- Evidence anchors:
  - [abstract]: "model intervention effect of a feature is inherently association. In this paper, we will study the conditions when an intuitive model intervention effect has a causal interpretation"
  - [section 3.1]: "Theorem 1 shows the conditions when the AMIE of a feature provides a causal attribution of the feature when the conditions in Theorem 1 are met"
  - [corpus]: Weak - corpus papers discuss causal effect identifiability but don't directly validate AMIE mechanism
- Break condition: Presence of unobserved direct causes or inclusion of Y's descendants in X

### Mechanism 2
- Claim: AMIE can still indicate causality when unobserved variables have activators or proxies
- Mechanism: If Xi is an activator or proxy of unobserved direct cause Ui, then AMIE(Xi, Y)≠0 even though Ui is unobserved
- Core assumption: Assumptions 1 and 2 hold, no standalone unobserved direct causes exist, and X excludes Y's descendants
- Evidence anchors:
  - [section 3.2]: "We call the observed variable Xi an activator of the unobserved direct cause since the effect of Ui on Y is activated or executed when Xi is intervened"
  - [section 3.2]: "Xi a proxy of the unobserved direct cause"
  - [corpus]: Weak - corpus focuses on general causal effect identifiability, not activator/proxy mechanisms
- Break condition: Unobserved variables exist without activators or proxies (standalone unobserved causes)

### Mechanism 3
- Claim: False positives in causal interpretation occur through specific structural configurations
- Mechanism: False positive linkages arise when Xj and Y form inducing paths or relaxed inducing paths, or when Xj is parent/proxy of unobserved causes
- Core assumption: Assumptions 1 and 2 hold, no standalone unobserved direct causes, X excludes Y's descendants
- Evidence anchors:
  - [section 3.2]: "Theorem 3 indicates that when there are unobserved direct causes in the feature set X, if we use the necessary condition stated in Theorem 1 to link non-zero AMIEs with direct causes, or activators or proxies of observed direct causes of Y, false positives may occur"
  - [section 3.2]: "Definition 2 (Inducing Paths and Relaxed Inducing Paths)"
  - [corpus]: Weak - corpus doesn't discuss inducing path mechanisms specifically
- Break condition: Independence tests detect Case 1 and 2 false positives; Case 3 remains possible

## Foundational Learning

- Concept: Causal DAGs and d-separation
  - Why needed here: The entire framework relies on understanding how conditional independencies map to causal structures
  - Quick check question: Given a DAG X→Y←Z, are X and Z independent conditioning on Y? (Answer: Yes, they form a collider)

- Concept: Faithfulness assumption
  - Why needed here: Links observed data dependencies to underlying causal structure, essential for Theorem 1
  - Quick check question: If data shows X⊥⊥Y|Z but DAG has path X→Z→Y, does faithfulness hold? (Answer: No)

- Concept: Intervention vs. observation
  - Why needed here: Distinguishes do-intervention (causal) from model intervention effect (AMIE, observational)
  - Quick check question: Does P(Y|do(X=x)) always equal P(Y|X=x)? (Answer: No, only under no confounding)

## Architecture Onboarding

- Component map: Trained ML model Y=f(X) -> AMIE calculation (baseline vs current) -> Independence testing -> Causal ranking
- Critical path: Model training → AMIE calculation → Independence testing → Causal ranking
- Design tradeoffs: Computational efficiency (AMIE vs Shapley) vs. causal interpretability vs. robustness to unobserved variables
- Failure signatures: Low model accuracy when standalone unobserved causes present; high false positive rate when inducing paths exist
- First 3 experiments:
  1. Generate DAG with no unobserved variables, calculate AMIE consistency with ground truth direct causes
  2. Add unobserved variables with activators/proxies, verify AMIE still captures causal relationships
  3. Create DAG with inducing paths, measure false positive rate and validate independence test detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum model accuracy threshold required for AMIE-based causal interpretation to be reliable?
- Basis in paper: [explicit] The paper states "It is not advisable to explain an inaccurate machine learning model with AMIE" and shows model accuracy drops significantly when standalone unobserved direct causes are present
- Why unresolved: The paper demonstrates the correlation between model accuracy and reliability of AMIE interpretation but doesn't establish a specific accuracy threshold below which AMIE interpretation becomes unreliable
- What evidence would resolve it: Systematic experiments varying model accuracy across different datasets and measuring the consistency between AMIE-identified features and true direct causes at different accuracy levels

### Open Question 2
- Question: How does the presence of measurement error in observed features affect the validity of AMIE-based causal interpretation?
- Basis in paper: [inferred] The paper assumes perfect faithfulness between data and causal DAG, and between model and data, but doesn't address measurement error in observed features
- Why unresolved: Measurement error is common in real-world applications and could introduce spurious associations that AMIE might misinterpret as causal relationships, yet this scenario isn't examined
- What evidence would resolve it: Controlled experiments introducing varying levels of measurement error in features and measuring how this affects AMIE's ability to correctly identify direct causes

### Open Question 3
- Question: Can AMIE-based causal interpretation be extended to temporal causal relationships where features at different time points interact?
- Basis in paper: [explicit] The paper focuses on static feature sets and doesn't consider temporal dynamics or time-series data
- Why unresolved: Many real-world applications involve temporal relationships where causes at different time points may have varying effects, but the current framework doesn't address this
- What evidence would resolve it: Development of an extended AMIE framework that can handle temporal features and validation on datasets with known temporal causal structures

## Limitations
- Theoretical framework relies heavily on faithfulness and model consistency assumptions that may not hold in real-world data
- Semi-synthetic datasets may not capture the complexity of real-world causal structures
- Practical detection and handling of standalone unobserved causes remains challenging

## Confidence
- High confidence in the theoretical framework connecting AMIE to causal interpretation under specified conditions
- Medium confidence in the practical applicability given assumptions about faithfulness and model consistency
- Medium confidence in the experimental results, as they rely on semi-synthetic data with known ground truth

## Next Checks
1. Apply the framework to real-world datasets with known causal structures to validate practical applicability beyond semi-synthetic settings
2. Test robustness to violations of faithfulness assumption by introducing data distributions that violate the faithfulness condition
3. Implement and evaluate the independence tests for false positive detection (Cases 1 and 2 of Theorem 3) on datasets with known inducing paths to verify detection rates