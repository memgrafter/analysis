---
ver: rpa2
title: Deep Learning for Generalised Planning with Background Knowledge
arxiv_id: '2410.07923'
source_url: https://arxiv.org/abs/2410.07923
tags:
- planning
- learning
- policy
- plan
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel approach to integrating background
  knowledge (BK) into deep learning models for planning. The core idea is to represent
  a generalized policy as a neural network that scores actions from a BK policy based
  on their likelihood of improving plan quality.
---

# Deep Learning for Generalised Planning with Background Knowledge

## Quick Facts
- arXiv ID: 2410.07923
- Source URL: https://arxiv.org/abs/2410.07923
- Reference count: 20
- Key outcome: Proposed method achieves over 50% of possible plan length improvement with respect to optimal plans using only small training data

## Executive Summary
This paper introduces a novel approach to integrating background knowledge into deep learning models for planning. The method represents a generalized policy as a neural network that scores actions from a Datalog-encoded background knowledge policy based on their likelihood of improving plan quality. Lifted Relational Neural Networks (LRNNs) are used to implement this approach, leveraging their ability to handle relational structures and incorporate Datalog rules. Experiments on four classical planning domains demonstrate significant improvements in plan quality over baseline policies while requiring minimal training data.

## Method Summary
The approach encodes background knowledge as Datalog rules representing a satisficing but suboptimal strategy. This policy guides the neural network to focus on improving plan quality rather than relearning domain mechanics. LRNNs, which are differentiable Datalog programs, take planning states as input and learn to score actions based on their potential to improve plan quality. Training data is generated from small tasks where optimal actions can be computed, and the learned policy is then applied to larger tasks. The method uses message passing between object embeddings to capture relational structure in planning states.

## Key Results
- Achieved over 50% of possible plan length improvement compared to optimal plans
- Training data generated in under 5 seconds from small tasks (less than 10000 states)
- Significant plan quality improvements across four classical planning domains (Blocksworld, Ferry, Rover, Satellite)
- Demonstrated efficiency and scalability through minimal training data requirements

## Why This Works (Mechanism)

### Mechanism 1
Background knowledge encoded as Datalog rules guides the neural network to focus learning on plan quality rather than relearning domain mechanics. The Datalog policy provides a satisficing but suboptimal strategy, which acts as a starting point. The neural network then learns to score actions from this policy based on their likelihood of improving plan quality, effectively fine-tuning the strategy. Core assumption: The Datalog rules encode a valid, cycle-free strategy that achieves goals without dead-ends.

### Mechanism 2
Lifted Relational Neural Networks (LRNNs) can effectively learn from the Datalog-encoded background knowledge and small training data to improve plan quality. LRNNs, being differentiable Datalog programs, can take relational structures like planning states as input and incorporate the Datalog rules as background knowledge. They learn to score actions based on their potential to improve plan quality. Core assumption: LRNNs can effectively learn from the structured input and background knowledge to optimize plan quality.

### Mechanism 3
Training data generated from small tasks is sufficient for the LRNN to learn effective plan quality optimization. The LRNN is trained on data from small tasks where optimal actions can be computed, learning to score actions based on their likelihood of improving plan quality. This learned scoring function is then applied to larger tasks. Core assumption: The patterns of plan quality improvement learned from small tasks generalize to larger tasks.

## Foundational Learning

- Concept: Datalog and its properties (stratification, P-completeness)
  - Why needed here: Datalog is used to encode the background knowledge policies, and understanding its properties is crucial for understanding the approach's limitations and potential.
  - Quick check question: What is the significance of Datalog being P-complete for fixed programs with stratified negation?

- Concept: Lifted Relational Neural Networks (LRNNs) and their architecture
  - Why needed here: LRNNs are the core neural architecture used in this approach, and understanding their structure and how they incorporate background knowledge is essential.
  - Quick check question: How do LRNNs differ from standard neural networks in terms of their input and computation structure?

- Concept: Planning domain representations (PDDL, lifted vs. ground forms)
  - Why needed here: The approach works with planning domains, and understanding their representation is necessary for implementing and applying the method.
  - Quick check question: What is the difference between a lifted planning task and a ground planning task, and why is the lifted form used here?

## Architecture Onboarding

- Component map: PDDL domain -> Datalog rules -> LRNN -> Training data generator -> Planner
- Critical path: 1. Define planning domain in PDDL 2. Encode background knowledge as Datalog rules 3. Generate training data from small tasks 4. Train LRNN on training data 5. Execute learned policy on larger tasks
- Design tradeoffs:
  - Expressiveness vs. efficiency of Datalog rules: More expressive rules may lead to better policies but could increase computational complexity
  - LRNN architecture: More layers and hidden units may improve performance but increase training time and risk overfitting
  - Training data size: More data may lead to better generalization but increases computational cost
- Failure signatures:
  - LRNN policy performs worse than baseline: May indicate issues with Datalog rules, LRNN architecture, or training data
  - LRNN policy fails to solve tasks: May indicate that Datalog rules do not encode a valid strategy
  - Long training times or overfitting: May indicate overly complex LRNN architecture or insufficient regularization
- First 3 experiments:
  1. Implement and test Datalog policy execution on small tasks to ensure it represents a valid strategy
  2. Train LRNN on small tasks and evaluate its performance on the same tasks to ensure it learns to improve over the baseline
  3. Test LRNN policy on larger tasks to evaluate its generalization ability and plan quality improvement

## Open Questions the Paper Calls Out

### Open Question 1
What are the precise theoretical conditions under which the proposed approach would fail to improve plan quality over the baseline BK policy? The paper discusses properties of BK policies but does not provide formal guarantees on when the learned policy will outperform the baseline. This remains unresolved because the paper relies on empirical results rather than theoretical analysis to demonstrate improvements, and the relationship between BK policy properties and learning success is not fully characterized.

### Open Question 2
How does the performance of the learned policies scale with the size of the training data and the complexity of the planning domain? The paper demonstrates effectiveness with small training data but does not systematically study scaling effects. This remains unresolved because the experiments focus on a fixed small training set size and do not explore the relationship between training data volume, domain complexity, and policy performance.

### Open Question 3
Can the proposed approach be extended to handle domains with continuous state spaces or action spaces? The paper focuses on discrete planning domains and does not address continuous spaces. This remains unresolved because the LRNN architecture and Datalog representation are designed for discrete structures, and it's unclear how they would handle continuous variables or actions.

### Open Question 4
How sensitive is the learned policy to the specific form and quality of the background knowledge provided? The paper states that BK policies are "satisficing but suboptimal" but does not explore sensitivity to different BK formulations. This remains unresolved because the experiments use a single BK formulation per domain and do not compare performance across different BK strategies or examine robustness to BK quality variations.

## Limitations
- Evaluation limited to four classical planning domains with relatively small state spaces
- Datalog background knowledge must satisfy specific properties (dead-end avoiding, goal achieving, cycle-free)
- Fixed LRNN architecture and training hyperparameters may not be optimal across all domains

## Confidence

**Confidence Levels:**
- High: The core mechanism of using Datalog rules to encode background knowledge and guide neural network learning
- Medium: The effectiveness of LRNNs in learning plan quality optimization from small training data
- Medium: The generalization ability from small training tasks to larger test tasks

## Next Checks

1. Test the approach on domains with larger state spaces and more complex dynamics to evaluate scalability and generalization
2. Perform ablation studies removing the background knowledge component to quantify its contribution to performance improvements
3. Evaluate the sensitivity of results to different LRNN architectures and training hyperparameters across multiple domains