---
ver: rpa2
title: Deep Domain Specialisation for single-model multi-domain learning to rank
arxiv_id: '2407.01069'
source_url: https://arxiv.org/abs/2407.01069
tags:
- domain
- domains
- performance
- deep
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Deep Domain Specialisation (DDS) as a method
  to consolidate multiple domain-specific ranking models into a single model. The
  core idea is to use a domain classification head (without gradient reversal) alongside
  the primary ranking task, forcing the model to learn domain-specific representations.
---

# Deep Domain Specialisation for single-model multi-domain learning to rank

## Quick Facts
- arXiv ID: 2407.01069
- Source URL: https://arxiv.org/abs/2407.01069
- Authors: Paul Missault; Abdelmaseeh Felfel
- Reference count: 11
- Key outcome: DDS consolidates models across UAE and Saudi Arabia stores with 0.51% NDCG@16 gain for UAE and 0.22% for Saudi Arabia in offline evaluation, while using fewer parameters than multi-headed approaches

## Executive Summary
This paper introduces Deep Domain Specialisation (DDS) as a method to consolidate multiple domain-specific ranking models into a single model. The approach uses a domain classification head without gradient reversal alongside the primary ranking task, forcing the model to learn domain-specific representations. Evaluated on search relevance ranking across two geographically different stores, DDS achieved modest offline improvements while using fewer parameters than multi-headed approaches. Online interleaving experiments showed significant sales improvements of 14.79% for UAE and 24.67% for Saudi Arabia compared to production models.

## Method Summary
DDS builds on a RankerFormer architecture with BERT text encoders, trunk MLP, listwise transformer, and final MLP for purchase probability scoring. The key innovation is adding a domain classification head trained without gradient reversal alongside the ranking loss. This forces the model to learn features that maximize domain discriminability rather than minimizing it (as in DDA). The approach was tested on consolidating search ranking models for UAE and Saudi Arabia retail stores, using NDCG@16 for offline evaluation and interleaving credit for online evaluation.

## Key Results
- DDS achieved 0.51% NDCG@16 gain over domain-specific baseline for UAE and 0.22% for Saudi Arabia in offline evaluation
- DDS used fewer parameters than multi-headed approaches (1 scoring head + 1 classification head vs N scoring heads)
- Online interleaving experiments showed DDS improved sales by 14.79% for UAE and 24.67% for Saudi Arabia compared to production models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DDS forces domain-specific representations by using a domain classification head without gradient reversal.
- Mechanism: The domain classifier is trained alongside the primary ranking task. Without gradient reversal, the network minimizes the classification loss, encouraging the model to learn features that maximize domain discriminability. This contrasts with DDA, where gradient reversal minimizes domain discriminability to force domain-agnostic features.
- Core assumption: The domain classification task does not interfere negatively with the primary ranking task and can be jointly optimized.
- Evidence anchors:
  - [abstract] "The core idea is to use a domain classification head (without gradient reversal) alongside the primary ranking task, forcing the model to learn domain-specific representations."
  - [section 3.4] "Deep Domain Specification (DDS) uses the same architecture from Section 3.3 but without the Gradient Reversal Layer. This causes the network to minimize it’s classification loss instead of maximising it."
  - [corpus] Weak - no direct evidence in corpus; corpus focuses on multi-domain models but not this specific mechanism.
- Break condition: If the domain classification head causes catastrophic interference with the ranking task, degrading overall performance.

### Mechanism 2
- Claim: Training on combined data from multiple domains improves model performance on each domain compared to single-domain models.
- Mechanism: By training on a larger, combined dataset, the consolidated model can learn richer representations that capture both domain-specific and shared patterns. This broader exposure can lead to better generalization within each domain than models trained on smaller, isolated datasets.
- Core assumption: The combined dataset is sufficiently large and diverse to improve learning without introducing harmful noise or domain conflicts.
- Evidence anchors:
  - [abstract] "DDS achieved a 0.51% NDCG@16 gain over the domain-specific baseline for UAE and 0.22% for Saudi Arabia in offline evaluation"
  - [section 5.1] "The two-headed and DDS models both increase performance compared to the baseline... DDS achieves this performance with the least model parameters."
  - [corpus] Weak - no direct evidence in corpus; general multi-domain learning literature supports but not specific to this claim.
- Break condition: If the domains are too dissimilar, combining data could dilute domain-specific signals and hurt performance.

### Mechanism 3
- Claim: DDS scales better to multiple domains than multi-headed approaches because it uses a single scoring head.
- Mechanism: Multi-headed models require N separate scoring heads for N domains, increasing model size and complexity. DDS uses one scoring head plus a domain classifier, keeping parameters constant regardless of the number of domains.
- Core assumption: A single scoring head can effectively handle multiple domains when domain-specific representations are learned.
- Evidence anchors:
  - [section 5.1] "DDS achieves this performance with the least model parameters: DDS uses 1 scoring head (+ 1 classification head that can be cut off after training) against the N scoring heads for an N-headed solution."
  - [section 7] "Comparing between DDS and the two-headed approach, DDS had better performance online and can be scaled easier to expand to additional stores."
  - [corpus] Weak - no direct evidence in corpus; scaling considerations are mentioned but not specific to DDS.
- Break condition: If adding domains requires significant architectural changes or if the single head becomes a bottleneck.

## Foundational Learning

- Concept: Gradient Reversal Layer (GRL)
  - Why needed here: Understanding GRL is key to distinguishing DDA (domain-agnostic) from DDS (domain-specific). It's central to how the two approaches differ architecturally.
  - Quick check question: What happens to gradients during backprop when a GRL is present versus absent?

- Concept: Listwise vs Pointwise Ranking
  - Why needed here: The model uses a listwise transformer component. Understanding this distinction is important for grasping how the model evaluates and ranks entire lists rather than individual items.
  - Quick check question: How does listwise ranking differ from pointwise ranking in terms of input and loss function?

- Concept: Domain Generalization vs Domain Specialization
  - Why needed here: The paper contrasts these two paradigms. Recognizing when to use each is crucial for applying the right technique to the problem.
  - Quick check question: When would you prefer domain generalization over domain specialization, and vice versa?

## Architecture Onboarding

- Component map: Text encoders → Trunk MLP → Listwise transformer → Final MLP → Output. For DDS, domain classification head is parallel to this path.
- Critical path: Text encoders → Trunk MLP → Listwise transformer → Final MLP → Output. For DDS, domain classification head is parallel to this path.
- Design tradeoffs:
  - DDS vs DDA: DDS learns domain-specific features (better in-domain performance) vs DDA learns domain-agnostic features (better out-of-domain generalization).
  - DDS vs Multi-headed: DDS uses fewer parameters and scales better but may be less flexible if domains are very different.
  - Adding domain classifier: Provides domain-specific learning but adds a small overhead and requires joint optimization.
- Failure signatures:
  - Performance degradation on one or both domains: Could indicate interference between ranking and domain classification tasks.
  - Domain classifier accuracy is very low: Suggests the model isn't learning meaningful domain-specific features.
  - No improvement over single-domain models: Might mean domains are too similar or dataset isn't large enough.
- First 3 experiments:
  1. Train DDS and DDA on a small combined dataset, compare domain classifier accuracy and ranking NDCG on each domain.
  2. Vary the weight of the domain classification loss in DDS to find the optimal balance between ranking and domain learning.
  3. Test DDS with 3+ domains (if available) to empirically verify the scaling advantage over multi-headed approaches.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DDS performance scale when consolidating more than two domains, particularly when domains have significantly different characteristics?
- Basis in paper: [inferred] The paper only tested DDS on two geographically different stores (UAE and Saudi Arabia) and mentions "scaling to multiple domains, more diverse domains is left for future work."
- Why unresolved: The paper explicitly states this as future work and did not test DDS with more than two domains or with domains that have very different characteristics.
- What evidence would resolve it: Experiments showing DDS performance when consolidating three or more domains with varying characteristics (e.g., different languages, product categories, or customer behaviors) compared to domain-specific baselines.

### Open Question 2
- Question: What specific aspects of the larger training dataset contribute to the performance improvements seen in consolidated models compared to domain-specific models?
- Basis in paper: [explicit] "By training on a larger dataset combined across stores, the consolidated models appear to leverage that data to perform better on each store."
- Why unresolved: The paper notes the performance improvement but does not investigate which specific data aspects (e.g., increased diversity, more training examples for rare queries) drive this improvement.
- What evidence would resolve it: Ablation studies isolating the effects of different dataset characteristics (size, diversity, query distribution) on consolidated model performance versus domain-specific models.

### Open Question 3
- Question: How does the domain classification head in DDS contribute to the model's ability to learn domain-specific representations, and could this be further optimized?
- Basis in paper: [inferred] While the paper describes the DDS architecture and its use of a domain classification head, it does not investigate the specific role or potential optimization of this component in learning domain-specific representations.
- Why unresolved: The paper focuses on comparing DDS performance to other models but does not analyze the internal mechanisms or optimization potential of the domain classification head.
- What evidence would resolve it: Analysis of the domain classification head's contribution through ablation studies, examination of learned representations, or experiments with alternative domain classification architectures or loss functions.

## Limitations
- Evaluation only on two domains limits scalability claims
- No ablation studies showing optimal weight for domain classification loss
- Lack of comparison against other domain adaptation techniques beyond DDA

## Confidence
- Claims about DDS effectiveness: **High confidence** for geographically separated retail stores, **Medium confidence** for generalization to other domain types
- Online sales improvement claims: **High confidence** for e-commerce context studied
- Scaling claims: **Medium confidence** due to limited domain testing

## Next Checks
1. Test DDS across 3+ domains with varying similarity levels to empirically verify the claimed scalability advantage over multi-headed approaches and identify the breaking point where domain conflicts overwhelm benefits.

2. Conduct an ablation study varying the weight of the domain classification loss in DDS to determine the optimal balance between ranking performance and domain discriminability, and test whether the loss can be reduced over time.

3. Evaluate DDS on domains with different underlying distributions (e.g., different product categories or user demographics) to assess whether the domain-specific learning mechanism breaks down when domains are too dissimilar.