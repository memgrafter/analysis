---
ver: rpa2
title: Atomic Fact Decomposition Helps Attributed Question Answering
arxiv_id: '2410.16708'
source_url: https://arxiv.org/abs/2410.16708
tags:
- atomic
- evidence
- fact
- retrieval
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of attributed question answering
  (AQA), where the goal is to provide both trustworthy answers and reliable attribution
  reports. Existing methods struggle with complex long-form answers and imprecise
  evidence attribution.
---

# Atomic Fact Decomposition Helps Attributed Question Answering

## Quick Facts
- arXiv ID: 2410.16708
- Source URL: https://arxiv.org/abs/2410.16708
- Reference count: 40
- Primary result: ARE framework improves attribution precision (Attr_p) by 68-72% over baselines while preserving answer intent

## Executive Summary
This paper addresses the challenge of attributed question answering (AQA), where systems must provide both trustworthy answers and reliable evidence attribution. Current methods struggle with complex long-form answers and imprecise evidence attribution, often missing relevant supporting evidence or including irrelevant information. The proposed ARE framework tackles these issues by decomposing answers into atomic facts at molecular and atomic levels, retrieving evidence for each atomic fact, and editing facts based on evidence verification. This approach significantly improves attribution precision while maintaining answer quality across four diverse datasets and multiple LLMs.

## Method Summary
The ARE framework introduces a novel approach to attributed question answering by implementing a three-stage pipeline: fact decomposition, evidence retrieval, and fact editing. First, it uses a proposed LLM-based molecular-to-atomic fact decomposition model to break down long-form answers into discrete atomic facts. Then, for each atomic fact, it retrieves relevant evidence from Wikipedia using both question-answer and fact-only queries. Finally, it edits each atomic fact based on the retrieved evidence, removing unsupported or inaccurate information. The framework introduces a strict Attr_p metric that evaluates both the precision and completeness of evidence attribution, measuring how well retrieved evidence supports each atomic fact without including irrelevant information.

## Key Results
- ARE achieves 68-72% improvement in attribution precision (Attr_p) over post-hoc retrieval baselines
- Maintains answer intent preservation while significantly improving evidence quality
- Outperforms state-of-the-art methods across four diverse datasets with multiple LLMs
- Demonstrates effectiveness across different answer lengths and complexity levels

## Why This Works (Mechanism)
The framework's success stems from its granular approach to evidence attribution. By decomposing answers into atomic facts, it enables precise retrieval of evidence for each individual fact rather than treating the answer as a monolithic unit. The dual-query retrieval strategy (question-answer and fact-only) captures both contextual and fact-specific evidence, while the fact editing stage ensures that each atomic fact is verified against its supporting evidence. This decomposition approach addresses the inherent limitation of post-hoc retrieval methods that struggle to attribute evidence precisely for complex, multi-fact answers.

## Foundational Learning

**Atomic Fact Decomposition**: Breaking down complex answers into discrete, verifiable units of information.
*Why needed*: Enables precise evidence retrieval and attribution for each component of an answer
*Quick check*: Can each fact be verified independently without requiring context from other facts?

**Molecular-to-Atomic Fact Extraction**: LLM-based model that identifies and extracts atomic facts from long-form answers.
*Why needed*: Transforms unstructured answers into structured, verifiable components
*Quick check*: Are extracted facts logically independent and complete?

**Strict Attribution Precision (Attr_p)**: Metric measuring both precision and completeness of evidence attribution.
*Why needed*: Provides rigorous evaluation of evidence quality beyond simple recall metrics
*Quick check*: Does retrieved evidence fully support the fact without including irrelevant information?

**Dual-Query Retrieval**: Simultaneous retrieval using both question-answer and fact-only queries.
*Why needed*: Captures both contextual and fact-specific evidence for comprehensive coverage
*Quick check*: Are both query types retrieving complementary, non-overlapping evidence?

## Architecture Onboarding

**Component Map**: Fact Decomposition -> Evidence Retrieval -> Fact Editing -> Attribution Evaluation

**Critical Path**: The molecular-to-atomic fact decomposition is the critical path, as all subsequent stages depend on the quality and completeness of the initial fact extraction.

**Design Tradeoffs**: The framework trades computational overhead (processing each atomic fact separately) for significantly improved attribution precision. This design choice prioritizes evidence quality over speed, which is appropriate for applications requiring high trustworthiness.

**Failure Signatures**: 
- Poor fact decomposition leads to incomplete or inaccurate evidence retrieval
- Inadequate retrieval results in unsupported facts or missing evidence
- Over-aggressive fact editing may remove valid information

**First 3 Experiments**:
1. Test ARE framework on simple single-fact answers to establish baseline performance
2. Evaluate fact decomposition quality independently of the full pipeline
3. Compare dual-query retrieval effectiveness against single-query approaches

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the scalability of human evaluation for ARE quality assessment, the framework's generalizability to non-Wikipedia domains, and the potential for automated metrics that can replace human annotations while maintaining evaluation rigor.

## Limitations
- Heavy reliance on human annotations for ARE quality assessment raises scalability concerns
- Performance depends substantially on the quality of the LLM-based fact decomposition model
- Strict Attr_p metric may be overly demanding and penalize legitimate but slightly imperfect attributions
- Focus on Wikipedia-based retrieval limits generalizability to domains with different evidence structures

## Confidence

**High Confidence**: The ARE framework's modular design and core methodology are well-established, with clear implementation details and reproducible components. The improvement in Attr_p scores (68-72% over baselines) is substantial and methodologically sound.

**Medium Confidence**: The attribution precision improvements, while impressive, depend heavily on the quality of the fact decomposition and evidence retrieval subsystems. The human evaluation results are promising but may not fully capture edge cases or domain-specific challenges.

**Low Confidence**: The generalizability of results to non-Wikipedia domains and the scalability of the human evaluation process for large-scale deployment remain uncertain.

## Next Checks
1. Test the ARE framework on non-Wikipedia domains (scientific literature, news articles, or domain-specific knowledge bases) to assess generalizability and identify domain-specific challenges.

2. Develop and validate automated metrics for ARE quality assessment that can scale to large datasets while maintaining evaluation rigor.

3. Evaluate the framework's performance under adversarial conditions, including answers with subtle misinformation, ambiguous phrasing, or evidence that partially supports multiple facts.