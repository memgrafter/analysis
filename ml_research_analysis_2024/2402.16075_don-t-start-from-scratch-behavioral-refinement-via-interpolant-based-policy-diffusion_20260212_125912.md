---
ver: rpa2
title: 'Don''t Start from Scratch: Behavioral Refinement via Interpolant-based Policy
  Diffusion'
arxiv_id: '2402.16075'
source_url: https://arxiv.org/abs/2402.16075
tags:
- source
- bridg
- learning
- diffusion
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving diffusion-based
  imitation learning by starting from a more informative source distribution than
  standard Gaussian noise. The key idea is to use stochastic interpolants to bridge
  arbitrary source and target policies, allowing for better initializations that accelerate
  learning and improve final policy quality.
---

# Don't Start from Scratch: Behavioral Refinement via Interpolant-based Policy Diffusion

## Quick Facts
- arXiv ID: 2402.16075
- Source URL: https://arxiv.org/abs/2402.16075
- Reference count: 40
- Primary result: BRIDGER outperforms state-of-the-art diffusion policies by using stochastic interpolants to bridge informative source policies with target policies, especially with few diffusion steps.

## Executive Summary
This paper addresses the challenge of improving diffusion-based imitation learning by starting from a more informative source distribution than standard Gaussian noise. The key insight is that using stochastic interpolants to bridge arbitrary source and target policies allows for better initializations that accelerate learning and improve final policy quality. The proposed BRIDGER method leverages this framework, demonstrating significant performance gains across challenging robot tasks including Franka Kitchen, Adroit, grasp generation, and real-world manipulation. The method shows particular advantages when using small numbers of diffusion steps, making it more practical for real-time applications.

## Method Summary
BRIDGER uses stochastic interpolants to bridge a source policy distribution with a target policy distribution over finite time. The method learns both velocity and score functions to reverse the forward SDE, allowing it to start from any arbitrary source policy (Gaussian, CVAE, or heuristic) rather than just Gaussian noise. The approach includes a decomposition of velocity into deterministic and stochastic components for improved training stability, and offers choice between Linear and Power3 interpolant functions depending on target distribution characteristics.

## Key Results
- BRIDGER outperforms state-of-the-art diffusion policies across multiple robot tasks (Franka Kitchen, Adroit, grasp generation)
- Using informative source policies (CVAE or heuristic) significantly improves performance compared to Gaussian baselines
- Power3 interpolant function shows superior performance on highly multi-modal distributions like grasp generation
- BRIDGER generalizes successfully to real-world manipulation tasks with high-dimensional observations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Starting diffusion from a more informative source policy improves final policy quality compared to starting from Gaussian noise.
- Mechanism: The stochastic interpolant framework bridges the source and target distributions over time; a better source reduces the distance the diffusion process must cover, leading to higher quality final policies.
- Core assumption: The forward SDE in Eq. 9 has bounded improvement rates such that improvements in the source distribution translate to improvements in the target.
- Evidence anchors: Theorem 1 and 2 formalize the bound: better source → better target up to additive factor.
- Break condition: If the source policy is poorly chosen or improvement bounds are violated, performance may degrade below the Gaussian baseline.

### Mechanism 2
- Claim: Power3 interpolant function outperforms Linear for highly multi-modal distributions.
- Mechanism: Power3 starts with larger steps that converge quickly to high-density areas of multi-modal targets, then fine-tunes, while Linear interpolates uniformly.
- Core assumption: The target distribution's modes are sufficiently separated that aggressive early movement is beneficial.
- Evidence anchors: Power3 significantly outperforms Linear in Grasp Generation where end-effector poses are highly multi-modal.
- Break condition: If target distribution is uni-modal, Power3 offers no advantage and may introduce instability.

### Mechanism 3
- Claim: Decomposing velocity b into v and score s components improves training stability.
- Mechanism: The decomposition allows separate learning of deterministic and stochastic parts, mitigating vanishing/exploding gradients.
- Core assumption: The score s can be reliably estimated and the decomposition is mathematically valid for the stochastic interpolant.
- Evidence anchors: Decomposition is suggested in standard stochastic interpolant literature and implemented for stability.
- Break condition: If score network fails to converge or decomposition introduces numerical instability.

## Foundational Learning

- **Stochastic interpolants**: Framework to bridge arbitrary densities over finite time; needed to enable starting from any source distribution rather than just Gaussian noise.
  - Quick check: What are the boundary conditions on the interpolant I(t, a0, a1, x) for it to satisfy Definition 1?

- **Score matching and DDPM**: Understanding denoising diffusion models and noise schedules helps grasp how noise schedules and denoising work in BRIDGER.
  - Quick check: How does the noise schedule γ(t) influence the variance of Gaussian latent noise during the diffusion process?

- **CVAEs as source policies**: CVAEs provide cheap-to-sample data-driven source policies; understanding their limitations explains empirical results.
  - Quick check: Why might a CVAE fail to fully capture the diversity of complex demonstration distributions?

## Architecture Onboarding

- **Component map**: Source policy sampler -> Interpolant function I(t,a0,a1,x) -> Velocity network bθ(t,a,x) -> Score network sη(t,a,x) -> Noise schedule γ(t) -> Diffusion coefficient ϵ(t)
- **Critical path**: Sample a0 ~ π0 → Apply SDE steps using b and s → Generate a1 ~ π1
- **Design tradeoffs**:
  - Source policy choice: heuristic (task-specific but brittle) vs. CVAE (data-driven but limited) vs. Gaussian (baseline, no prior)
  - Interpolant choice: Linear (stable, uniform) vs. Power3 (faster early convergence, better for multi-modal)
  - Noise schedule γ(t): larger d → more exploration but risk of over-dispersion
- **Failure signatures**:
  - Poor source policy → worse final performance than Gaussian
  - Inappropriate interpolant (e.g., Linear on multi-modal) → slower convergence or mode collapse
  - Unstable training → check Lipschitz constants of b and s
- **First 3 experiments**:
  1. Train BRIDGER with Gaussian source on Franka Kitchen; compare k=5 vs k=80 success rates
  2. Swap interpolant from Linear to Power3 on Grasp Generation; measure EMD and success rate
  3. Replace CVAE source with heuristic source on Adroit Hammer; compare performance across k values

## Open Questions the Paper Calls Out

1. How do different noise schedules (γ(t)) affect performance across diverse tasks and dataset sizes?
2. What is the impact of using source policies designed for different but related tasks (transfer learning)?
3. How does the choice of interpolant function interact with the characteristics of the target distribution (e.g., multi-modality, dimensionality)?

## Limitations
- Theoretical guarantees rely on bounded improvement rates in the forward SDE, but empirical validation is limited
- Power3 interpolant benefits are demonstrated primarily on a single multi-modal task (Grasp Generation)
- Real-world experiments are conducted on a single Franka robot with limited task diversity
- Comparison to Gaussian baselines may not fully capture cases where poor source policy choice degrades performance

## Confidence

- Theoretical framework and bounds: **Medium** - Sound in principle but limited empirical validation
- BRIDGER method efficacy: **High** - Strong empirical support across multiple tasks and baselines
- Power3 interpolant benefits: **Medium** - Well-supported for Grasp Generation but limited cross-task validation

## Next Checks

1. **Source Policy Sensitivity Analysis**: Systematically evaluate BRIDGER's performance across a spectrum of source policies ranging from excellent to deliberately poor choices, measuring the exact degradation point relative to Gaussian baseline to validate Theorem 1's practical bounds.

2. **Cross-Task Multi-Modality Test**: Apply Power3 interpolant to additional highly multi-modal tasks (e.g., cloth folding with multiple valid configurations) and uni-modal tasks to quantify the exact conditions under which Power3 provides measurable benefits over Linear.

3. **Real-World Robustness Study**: Deploy BRIDGER on multiple robot platforms (different kinematics, sensors) and tasks to assess generalization beyond the single Franka setup, measuring performance variance across hardware and task complexity.