---
ver: rpa2
title: 'CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models'
arxiv_id: '2412.10117'
source_url: https://arxiv.org/abs/2412.10117
tags:
- speech
- oice
- text
- speaker
- cosyv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CosyVoice 2 introduces a streaming speech synthesis model that
  achieves human-parity naturalness with minimal latency. It uses a unified framework
  for both streaming and non-streaming synthesis, employing a text-speech language
  model based on a pre-trained LLM and a chunk-aware causal flow matching model.
---

# CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models

## Quick Facts
- arXiv ID: 2412.10117
- Source URL: https://arxiv.org/abs/2412.10117
- Reference count: 40
- Human-parity streaming synthesis with 1.45% CER and 0.806 speaker similarity

## Executive Summary
CosyVoice 2 introduces a streaming speech synthesis model that achieves human-parity naturalness with minimal latency. It uses a unified framework for both streaming and non-streaming synthesis, employing a text-speech language model based on a pre-trained LLM and a chunk-aware causal flow matching model. The model introduces finite-scalar quantization for improved codebook utilization and supports fine-grained control over emotion, accent, and role style. Trained on a large-scale multilingual dataset, CosyVoice 2 achieves state-of-the-art performance while maintaining near-lossless quality in streaming mode.

## Method Summary
CosyVoice 2 employs a unified framework that supports both streaming and non-streaming synthesis through a text-speech language model based on a pre-trained LLM. The model uses finite-scalar quantization (FSQ) for speech tokenization, which improves codebook utilization compared to traditional vector quantization. For streaming synthesis, a chunk-aware causal flow matching model leverages past and future context through different masking strategies to balance latency and quality. The system also includes multi-speaker fine-tuning with reinforcement learning to improve speaker adaptation and control over various expressive parameters.

## Key Results
- Achieves 1.45% CER and 0.806 speaker similarity on Chinese test sets
- Maintains near-lossless quality in streaming mode compared to non-streaming
- Supports fine-grained control over emotion, accent, and role style with instruction-based synthesis

## Why This Works (Mechanism)

### Mechanism 1
Finite scalar quantization (FSQ) improves codebook utilization and preserves more semantic information compared to vector quantization (VQ). FSQ quantizes intermediate representations into bounded scalar values, allowing each dimension to be fully utilized in the (2K+1)-ary system. This contrasts with VQ, which underutilizes codebooks due to clustering constraints.

### Mechanism 2
A unified text-speech language model supports both streaming and non-streaming synthesis within a single framework. The LM is trained on hybrid sequences mixing text and speech tokens in a predefined ratio. During inference, streaming mode uses autoregressive generation with padding for missing text tokens, while non-streaming mode uses complete sequences.

### Mechanism 3
Chunk-aware causal flow matching enables lossless streaming synthesis by leveraging past and future context. The flow matching model uses four masks (non-causal, full-causal, chunk-M, chunk-2M) to balance latency and performance. Training samples are randomly masked, enabling implicit self-distillation where masks with more context teach those with less.

## Foundational Learning

- **Discrete speech tokenization and codebook utilization**: Understanding how speech is converted into tokens and how quantization methods affect information preservation is crucial for implementing FSQ. *Quick check: How does FSQ differ from VQ in terms of codebook utilization and information preservation?*

- **Causal and non-causal masking in attention mechanisms**: Chunk-aware flow matching relies on different masking strategies to balance latency and performance. *Quick check: What are the differences between non-causal, full-causal, chunk-M, and chunk-2M masks, and how do they affect latency and quality?*

- **Reinforcement learning for TTS model fine-tuning**: The paper mentions using reinforcement learning with ASR and speaker similarity rewards to improve model performance. *Quick check: How does the direct preference optimization (DPO) method work in the context of TTS model fine-tuning, and what are its advantages over traditional methods?*

## Architecture Onboarding

- **Component map**: Text → Text tokenizer → Text-speech LM → Speech tokens → Chunk-aware flow matching → Mel spectrogram → Vocoder → Waveform

- **Critical path**: The complete synthesis pipeline from text input through tokenization, language model generation, flow matching, and vocoder output

- **Design tradeoffs**: Streaming vs non-streaming support introduces complexity in sequence construction; FSQ vs VQ affects quantization quality; chunk-aware masks balance latency and quality

- **Failure signatures**: High WER/CER indicates text-speech alignment issues; low speaker similarity suggests embedding problems; high latency points to inefficient streaming implementation

- **First 3 experiments**:
  1. Implement and test FSQ-based speech tokenizer on a small dataset to verify codebook utilization and ASR accuracy
  2. Train a simple text-speech LM on a unified streaming/non-streaming dataset to evaluate performance in both modes
  3. Implement chunk-aware flow matching with different masking strategies and measure latency and quality trade-offs on a small dataset

## Open Questions the Paper Calls Out

- Can the unified streaming and non-streaming framework be extended to other neural speech synthesis models beyond autoregressive and flow matching approaches?
- What is the optimal chunk size for streaming synthesis that balances latency and quality across different hardware configurations?
- How does the performance of CosyVoice 2 scale with increasing model size, particularly for the text-speech language model component?
- Can the finite scalar quantization approach be adapted for other discrete representation learning tasks beyond speech synthesis?
- What is the impact of instruction complexity on synthesis quality and what are the practical limits of fine-grained control?

## Limitations

- Evaluation primarily focused on Chinese test sets with limited results for other languages
- Streaming performance claims based on "typical test cases" without detailed analysis of challenging scenarios
- Finite-scalar quantization lacks ablation studies to isolate its contribution to overall performance

## Confidence

**High Confidence:**
- The unified text-speech language model architecture is technically sound and well-documented
- The finite-scalar quantization approach improves codebook utilization compared to traditional vector quantization
- The chunk-aware flow matching framework can support both streaming and non-streaming synthesis

**Medium Confidence:**
- The model achieves human-parity naturalness in streaming mode, based on the NMOS scores and speaker similarity metrics
- The 1.45% CER on Chinese test sets represents state-of-the-art performance
- The multi-speaker fine-tuning with reinforcement learning effectively improves speaker adaptation

**Low Confidence:**
- The streaming latency claims of "minimal latency" without specific quantitative benchmarks
- The generalization performance across all four languages given the imbalanced training data distribution
- The long-term stability and robustness of the streaming synthesis in real-world deployment scenarios

## Next Checks

1. **Cross-lingual performance validation**: Conduct comprehensive evaluation on English, Japanese, and Korean test sets using the same metrics (CER, speaker similarity, NMOS) as Chinese evaluation to verify claimed multilingual capabilities.

2. **Streaming robustness testing**: Design a test suite that includes challenging streaming scenarios such as long-form content, rapid emotional shifts, and mixed language input to validate "minimal latency" and "near-lossless quality" claims.

3. **FSQ ablation study**: Implement a controlled experiment comparing FSQ against both traditional VQ and continuous token approaches on the same model architecture to isolate FSQ's specific contribution to performance.