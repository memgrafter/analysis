---
ver: rpa2
title: Cross-Task Multi-Branch Vision Transformer for Facial Expression and Mask Wearing
  Classification
arxiv_id: '2404.14606'
source_url: https://arxiv.org/abs/2404.14606
tags:
- facial
- attention
- vision
- recognition
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of facial expression recognition
  (FER) in the presence of masks, which has become a significant issue in the post-COVID-19
  era. The authors propose a unified multi-branch vision transformer model that performs
  both facial expression recognition and mask wearing classification.
---

# Cross-Task Multi-Branch Vision Transformer for Facial Expression and Mask Wearing Classification

## Quick Facts
- arXiv ID: 2404.14606
- Source URL: https://arxiv.org/abs/2404.14606
- Reference count: 40
- Key outcome: Unified multi-branch vision transformer model for facial expression recognition and mask wearing classification, achieving 77.02% accuracy on M-CK+ dataset

## Executive Summary
This paper addresses the challenge of facial expression recognition (FER) in the presence of masks, a significant issue in the post-COVID-19 era. The authors propose a unified multi-branch vision transformer model that performs both facial expression recognition and mask wearing classification. The model uses a dual-branch architecture to extract shared features and a cross-task fusion phase to exchange information between the two tasks using a cross attention module. The proposed framework reduces complexity compared to using separate networks for both tasks while achieving state-of-the-art performance on benchmark datasets.

## Method Summary
The method employs a cross-task multi-branch vision transformer with a dual-branch architecture for shared feature extraction. The model consists of two phases: Phase 1 uses a large branch (L-Branch) and small branch (S-Branch) to extract multi-scale features, while Phase 2 implements cross-task fusion with task-specific branches (E-branch for expression, M-branch for mask) connected through cross-additive-attention modules. The model is trained in two stages: first for a shared classifier, then for joint training of both task-specific classifiers. Datasets are prepared by converting to 3 channels, resizing to 224x224, and normalizing, with masks added using the AWFM method.

## Key Results
- Achieves 77.02% accuracy on M-CK+ dataset, outperforming existing methods
- Maintains low computational cost compared to separate networks for both tasks
- Demonstrates improved performance over state-of-the-art methods on both FER and mask classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-task fusion reduces model complexity by sharing low-level feature extraction across facial expression and mask classification tasks.
- Mechanism: A dual-branch vision transformer first extracts shared features using a large and small branch, then processes task-specific representations through cross-attention modules.
- Core assumption: Facial expression and mask classification share significant low-level visual features, making joint feature extraction beneficial.
- Evidence anchors:
  - [abstract] "Our approach extracts shared features for both tasks using a dual-branch architecture that obtains multi-scale feature representations."
  - [section] "The proposed framework reduces the overall complexity compared with using separate networks for both tasks by the simple yet effective cross-task fusion phase."
  - [corpus] Weak - no direct corpus evidence, but related multi-task learning papers exist.
- Break condition: If tasks diverge significantly in feature space or if mask occlusion fundamentally alters facial geometry, shared feature extraction becomes detrimental.

### Mechanism 2
- Claim: Cross-attention modules enable effective information exchange between facial expression and mask classification branches.
- Mechanism: The model uses cross-attention to fuse features from both branches, allowing each task to benefit from information relevant to the other task.
- Core assumption: Information from mask classification can improve facial expression recognition accuracy, and vice versa.
- Evidence anchors:
  - [abstract] "Furthermore, we propose a cross-task fusion phase that processes tokens for each task with separate branches, while exchanging information using a cross attention module."
  - [section] "The cross-additive-attention module replaces the dot-product attention with an additive attention module to enhance the aggregation capability of the feature fusion."
  - [corpus] Weak - no direct corpus evidence, but cross-attention is a known mechanism from CrossViT.
- Break condition: If cross-attention introduces noise or irrelevant information that degrades performance on either task.

### Mechanism 3
- Claim: Multi-scale feature extraction balances fine-grained detail capture with computational efficiency.
- Mechanism: The dual-branch architecture uses a large branch with bigger patches and more encoders for coarse features, and a small branch with finer patches for detailed features.
- Core assumption: Different patch sizes capture complementary information - large patches for overall structure, small patches for local details.
- Evidence anchors:
  - [section] "The dual branch architecture balances performance by incorporating fine-grained information from S-Branch, while having relatively small computational cost when processing L-Branch."
  - [section] "L-Branch: a large branch that utilizes bigger patch size (ùëÉùëô) with more transformer encoders, and (2) S-Branch: a small, complementary branch that operates at fine-grained patch size (ùëÉùë†) with smaller embedding dimensions."
  - [corpus] Weak - no direct corpus evidence, but multi-scale approaches are common in vision transformers.
- Break condition: If computational overhead of dual branches outweighs benefits, or if one branch consistently dominates the other.

## Foundational Learning

- Vision Transformer fundamentals:
  - Why needed here: The entire model architecture is based on vision transformers, requiring understanding of tokenization, self-attention, and multi-head attention mechanisms.
  - Quick check question: How does the vision transformer tokenize images and what role does positional encoding play?

- Multi-task learning principles:
  - Why needed here: The model performs two related classification tasks simultaneously, requiring knowledge of shared representation learning and task-specific adaptation.
  - Quick check question: What are the advantages and potential drawbacks of multi-task learning compared to training separate models for each task?

- Attention mechanisms:
  - Why needed here: Cross-attention and additive attention are central to the model's information fusion strategy, requiring understanding of different attention variants and their applications.
  - Quick check question: How does additive attention differ from dot-product attention, and when might one be preferred over the other?

## Architecture Onboarding

- Component map: Input ‚Üí Phase 1 tokenization and dual-branch encoding ‚Üí Phase 2 cross-attention fusion ‚Üí Classification
- Critical path: Input ‚Üí Phase 1 tokenization and dual-branch encoding ‚Üí Phase 2 cross-attention fusion ‚Üí Classification
- Design tradeoffs:
  - Shared vs. separate feature extractors: Tradeoff between parameter efficiency and task-specific optimization
  - Cross-attention vs. simple concatenation: More expressive fusion at cost of computational overhead
  - Additive vs. dot-product attention: Better aggregation capability vs. standard implementation
- Failure signatures:
  - Expression accuracy drops significantly when masks are present
  - Mask detection accuracy is much higher than expression accuracy
  - Model performs worse than single-task baselines
  - Training instability or slow convergence
- First 3 experiments:
  1. Ablation study: Train single-branch vs. dual-branch architecture to measure impact of multi-scale features
  2. Attention type comparison: Replace cross-additive-attention with cross-dot-product-attention to measure performance impact
  3. Task separation test: Train two separate networks vs. joint model to quantify complexity reduction benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the cross-task fusion module affect performance when applied to datasets with higher occlusion rates than M-CK+?
- Basis in paper: [explicit] The paper demonstrates improved performance on M-CK+ with added masks but does not explore scenarios with even higher occlusion levels.
- Why unresolved: The current experiments focus on datasets with moderate occlusion; the model's robustness to extreme occlusion is untested.
- What evidence would resolve it: Testing the model on datasets with severe occlusion (e.g., full-face masks or additional occlusions) and comparing performance metrics would clarify its limits.

### Open Question 2
- Question: Can the proposed architecture generalize to other multi-task learning scenarios beyond facial expression and mask detection?
- Basis in paper: [inferred] The model's dual-branch and cross-task fusion approach suggests potential for broader application, but this is not explicitly tested.
- Why unresolved: The paper focuses on two specific tasks; generalization to other domains requires further experimentation.
- What evidence would resolve it: Applying the architecture to different multi-task scenarios (e.g., object detection and action recognition) and evaluating performance would demonstrate its versatility.

### Open Question 3
- Question: What is the impact of varying the number of cross attention iterations (ùêø1 and ùêø2) on model performance and computational efficiency?
- Basis in paper: [explicit] The paper uses fixed iterations (3 for ùêø1 and configurable ùêø2) but does not explore the effects of varying these parameters.
- Why unresolved: The optimal number of iterations for balancing performance and efficiency is not determined.
- What evidence would resolve it: Conducting experiments with different iteration counts and analyzing trade-offs between accuracy and computational cost would identify optimal settings.

## Limitations
- Architecture details of the CrossViT-B base model are not fully specified
- Two-stage training procedure and cross-additive-attention mechanism implementation lack complete detail
- Performance claims are moderately supported but direct comparisons with some state-of-the-art methods are limited

## Confidence
- Architecture claims: Medium - core design is clear but specific implementation details are missing
- Performance claims: Medium - experimental results are presented but comparison baselines are limited
- Multi-task learning benefits: Low-Medium - theoretical advantages are described but not rigorously quantified against alternatives

## Next Checks
1. Ablation study comparing single-branch vs. dual-branch performance to isolate the benefit of multi-scale feature extraction
2. Implementation verification by reproducing the cross-additive-attention mechanism and testing its performance against standard cross-attention
3. Computational complexity analysis comparing the proposed joint model against separate networks for both tasks to quantify the claimed complexity reduction