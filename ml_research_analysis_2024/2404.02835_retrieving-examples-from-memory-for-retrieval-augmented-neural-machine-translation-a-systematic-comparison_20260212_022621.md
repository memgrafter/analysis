---
ver: rpa2
title: 'Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation:
  A Systematic Comparison'
arxiv_id: '2404.02835'
source_url: https://arxiv.org/abs/2404.02835
tags:
- translation
- retrieval
- examples
- bm25
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper examines how different retrieval techniques impact downstream
  translation quality across three RAMT architectures: NFA (autoregressive), TM-LevT
  (edit-based), and ICL with BLOOM (large language model). The study varies domain
  selection, filtering, and ranking strategies, focusing on their interplay with translation
  performance.'
---

# Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison

## Quick Facts
- arXiv ID: 2404.02835
- Source URL: https://arxiv.org/abs/2404.02835
- Reference count: 40
- The paper shows retrieval methods significantly impact translation quality across three RAMT architectures, with coverage-oriented retrieval and in-domain examples generally improving results.

## Executive Summary
This paper systematically examines how different retrieval techniques affect downstream translation quality in retrieval-augmented machine translation. The study evaluates three translation architectures—autoregressive (NFA), edit-based (TM-LevT), and in-context learning (ICL)—under various retrieval conditions including domain selection, filtering, and ranking strategies. Results show that retrieval choices significantly impact translation performance, with coverage-oriented methods like δ-LCS generally improving results. The study also finds that in-domain retrieval outperforms out-of-domain retrieval and that removing filters at inference simplifies the pipeline without hurting performance.

## Method Summary
The study evaluates three RAMT architectures (NFA, TM-LevT, ICL) using English→French and German→English language pairs across multiple domains from OPUS. The retrieval pipeline consists of three stages: domain selection, filtering (NGM and BM25), and ranking (edit distances with δ-LCS and LED). The authors systematically vary retrieval techniques, number of examples (k), and domain choices, measuring translation quality with BLEU and COMET scores, as well as coverage and relevance metrics. They test both contrastive and non-contrastive ranking approaches, and examine the impact of removing filters at inference time.

## Key Results
- Coverage-oriented retrieval (δ-LCS) generally improves translation results, especially when the closest match is poor
- In-domain retrieval is superior to out-of-domain retrieval across all architectures
- Removing filters at inference simplifies the pipeline without hurting performance
- Increasing the number of examples helps, particularly for ICL and TM-LevT architectures
- Retrieval methods significantly influence BLEU and COMET scores, with variance across architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different retrieval methods interact differently with translation architectures, affecting translation quality
- Mechanism: Retrieval techniques impact downstream translation by influencing the set of examples provided to the model. The interaction depends on how the model processes these examples—autoregressive models are more robust, while edit-based and in-context learning models are more sensitive
- Core assumption: The choice of retrieval method directly affects the quality and relevance of examples, which in turn impacts translation performance
- Evidence anchors:
  - [abstract] "Our experiments show that the choice of the retrieval technique impacts the translation scores, with variance across architectures."
  - [section 4.2] "We observe here a stronger impact of retrieval on the downstream scores, with a large gain over the baseline (for test-0.4 and en-de)."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.522, average citations=0.0." (Weak corpus evidence for retrieval-augmentation mechanisms specifically)
- Break condition: If the retrieval method consistently selects irrelevant or low-quality examples, or if the translation architecture cannot effectively utilize the retrieved examples

### Mechanism 2
- Claim: Coverage-oriented retrieval (e.g., δ-LCS) generally improves translation results, especially when the closest match is poor
- Mechanism: Coverage-oriented retrieval aims to maximize the proportion of query tokens covered by example tokens, which helps the model access more relevant information from the examples. This is particularly beneficial when the best match is not highly relevant
- Core assumption: Increasing coverage of the source query by retrieved examples leads to better translation quality, especially for models that rely on editing or in-context learning
- Evidence anchors:
  - [section 2.3] "We propose a smoothed version, namely δ-LCS, with a small non-zero deletion cost δ. δ-LCS thus performs a trade-off between coverage, relevance, and length."
  - [section 4.2.2] "0.1-LCS slightly outperforms LED in most conditions and metrics, with a very small difference between the contrastive and non-contrastive versions of the ranking."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.522, average citations=0.0." (Weak corpus evidence for coverage-oriented retrieval specifically)
- Break condition: If increasing coverage leads to retrieving overly long or irrelevant examples that the model cannot effectively utilize, or if the translation architecture does not benefit from high coverage

### Mechanism 3
- Claim: Increasing the number and diversity of examples generally has a positive effect on translation performance
- Mechanism: Providing more examples increases the likelihood of finding relevant information for the translation task. Diversity in examples helps cover different aspects of the source query and prevents the model from overfitting to a single example
- Core assumption: A larger and more diverse set of examples provides more information for the model to leverage, leading to better translation quality
- Evidence anchors:
  - [abstract] "We also discuss the effects of increasing the number and diversity of examples, which are mostly positive across the board."
  - [section 4.2.2] "Overall, we observe a gain (BLEU/COMET) when k increases. For ICL, this is already clear from the results in Table 6 where 3-shots clearly outperforms 1-shot."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.522, average citations=0.0." (Weak corpus evidence for diversity in examples specifically)
- Break condition: If increasing the number of examples leads to information overload or if the examples are not diverse enough to provide meaningful additional information

## Foundational Learning

- Concept: Information Retrieval (IR) and its components (domain selection, filtering, ranking)
  - Why needed here: Understanding IR is crucial for grasping how retrieval techniques work and how they impact downstream translation tasks
  - Quick check question: What are the three main steps in the retrieval pipeline for RAMT, and what is the purpose of each step?

- Concept: Translation Memory (TM) and its role in machine translation
  - Why needed here: TMs provide the examples that are retrieved and used by RAMT architectures. Understanding TMs is essential for understanding the context and purpose of retrieval-augmented translation
  - Quick check question: How do TMs traditionally help translators, and how is this concept adapted in RAMT?

- Concept: Edit distance and its variants (e.g., δ-LCS)
  - Why needed here: Edit distance is a key metric used for ranking retrieved examples. Understanding its variants helps in grasping how different retrieval techniques prioritize examples
  - Quick check question: How does δ-LCS differ from standard edit distance, and what is the trade-off it makes?

## Architecture Onboarding

- Component map:
  - Retrieval pipeline (domain selection → filtering → ranking → selection) -> Translation architectures (NFA, TM-LevT, ICL) -> Evaluation metrics (BLEU, COMET, copy rate)

- Critical path:
  1. Retrieve examples using a chosen retrieval technique
  2. Provide retrieved examples to the translation architecture
  3. Generate translation output
  4. Evaluate translation quality using BLEU, COMET, and copy rate

- Design tradeoffs:
  - Retrieval speed vs. quality: More complex retrieval techniques may yield better examples but at a higher computational cost
  - Number of examples vs. model performance: Increasing the number of examples generally improves performance but may also increase computational cost and complexity
  - Coverage vs. relevance: Coverage-oriented retrieval may retrieve longer examples that cover more of the source, but may also include irrelevant information

- Failure signatures:
  - Low BLEU/COMET scores: Indicates poor translation quality, possibly due to ineffective retrieval or model architecture
  - High copy rate with low BLEU/COMET: Suggests the model is relying too heavily on copying from examples without effectively translating
  - Slow retrieval times: May indicate inefficiencies in the retrieval pipeline, especially with complex filtering or ranking techniques

- First 3 experiments:
  1. Compare BLEU scores using different retrieval techniques (e.g., NGM vs. BM25) with a fixed translation architecture (e.g., NFA)
  2. Vary the number of retrieved examples (k) and observe the impact on BLEU/COMET scores for different architectures (e.g., ICL, TM-LevT)
  3. Test the effect of domain selection (in-domain vs. out-of-domain) on translation quality for a specific architecture (e.g., NFA)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the retrieval method impact translation quality differently for high-resource vs low-resource language pairs?
- Basis in paper: [inferred] The paper studies en-fr and de-en pairs but does not systematically compare retrieval effects across resource levels. De-en is noted as having fewer domains and less data
- Why unresolved: The experiments focus on translation performance but do not analyze how retrieval quality trade-offs scale with language pair data availability or domain density differences
- What evidence would resolve it: Systematic experiments comparing retrieval strategies across multiple high-resource and low-resource language pairs, with detailed per-domain and per-language analysis of retrieval quality vs BLEU gains

### Open Question 2
- Question: How do retrieval method choices interact with different NMT architecture types (autoregressive, edit-based, non-autoregressive) beyond the three tested?
- Basis in paper: [explicit] The paper tests three architectures (NFA, TM-LevT, ICL) and notes variance in retrieval sensitivity, but suggests broader exploration is needed
- Why unresolved: Only three architectures are tested, and the paper explicitly states the goal is not to compare them but to understand retrieval-translation interplay. The conclusions about architecture-specific retrieval needs are limited to these cases
- What evidence would resolve it: Experiments with additional NMT architectures (e.g., fully non-autoregressive, latent variable models) under identical retrieval conditions, measuring architecture-specific sensitivity to retrieval quality and diversity

### Open Question 3
- Question: What is the optimal balance between retrieval computational cost and translation quality improvement?
- Basis in paper: [explicit] The paper discusses computational complexity of retrieval methods (O(n) vs O(nℓ|q|)) and notes that removing filters increases latency, but does not quantify the trade-off
- Why unresolved: While latency vs quality trade-offs are mentioned, the paper does not measure or optimize for computational efficiency, leaving the practical deployment trade-off open
- What evidence would resolve it: Empirical studies measuring BLEU gains per unit of retrieval time/compute cost across different retrieval pipelines and domains, identifying Pareto-optimal configurations

## Limitations
- Weak corpus-based evidence with low citation counts and average neighbor FMR
- Limited to specific language pairs (en-fr, de-en) and domains, potentially limiting generalizability
- Relies solely on automatic metrics (BLEU, COMET) without human evaluation

## Confidence
- **High Confidence**: Domain selection significantly impacts translation quality; increasing number of examples generally improves performance
- **Medium Confidence**: Coverage-oriented retrieval (δ-LCS) improves results, particularly when closest matches are poor; autoregressively-trained models are more robust to retrieval quality variations
- **Low Confidence**: Removal of filters at inference simplifies pipeline without hurting performance; diversity effects on translation quality

## Next Checks
1. Cross-domain validation: Test retrieval techniques on additional language pairs and specialized domains (medical, legal) to assess generalizability of observed patterns

2. Human evaluation: Conduct human assessment of translation quality for best-performing retrieval-architecture combinations to validate automatic metric findings, particularly for coverage-oriented retrieval effects

3. Efficiency analysis: Measure computational costs and inference latency for different retrieval pipelines, especially comparing filtered vs. filter-free inference to verify practical benefits claimed