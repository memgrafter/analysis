---
ver: rpa2
title: 'RIRAG: Regulatory Information Retrieval and Answer Generation'
arxiv_id: '2409.05677'
source_url: https://arxiv.org/abs/2409.05677
tags:
- regulatory
- passages
- passage
- answer
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RIRAG (Regulatory Information Retrieval and
  Answer Generation), a system for automating regulatory compliance tasks. The authors
  address the challenge of interpreting complex, frequently updated regulatory documents
  by developing an automated question-passage generation framework and creating the
  ObliQA dataset, which contains 27,869 questions derived from Abu Dhabi Global Markets
  financial regulations.
---

# RIRAG: Regulatory Information Retrieval and Answer Generation

## Quick Facts
- arXiv ID: 2409.05677
- Source URL: https://arxiv.org/abs/2409.05677
- Authors: Tuba Gokhan; Kexin Wang; Iryna Gurevych; Ted Briscoe
- Reference count: 16
- Primary result: Automated regulatory compliance system with 69.1% recall@10 and novel RePASs evaluation metric

## Executive Summary
This paper introduces RIRAG, a system for automating regulatory compliance tasks through question-passage generation and answer synthesis. The authors address the challenge of interpreting complex, frequently updated regulatory documents by developing an automated framework and creating the ObliQA dataset with 27,869 questions from Abu Dhabi Global Markets financial regulations. They propose RePASs, a novel evaluation metric that measures answer quality based on entailment, contradiction avoidance, and obligation coverage. Their baseline system combines multiple retrieval models with GPT-4 answer generation, demonstrating that scaling retrievers with extensive training data and parameters significantly improves performance.

## Method Summary
The RIRAG system uses an automated question-passage generation framework to create the ObliQA dataset from ADGM regulatory documents. It employs multiple retrieval models (BM25, DRAGON+, SPLADEv2, ColBERTv2, NV-Embed-v2, BGE-EN-ICL) with rank fusion, followed by GPT-4 answer generation. The RePASs metric evaluates answers using NLI models to assess entailment, contradiction, and obligation coverage. The system is evaluated on recall@10, MAP@10, and RePASs scores, with results showing significant improvements through retriever scaling.

## Key Results
- Recall@10 reaches up to 69.1% and MAP@10 up to 53.9% using scaled retrievers
- RePASs metric demonstrates high alignment with expert judgments on gold standard data
- NV-Embed-v2 and BGE-EN-ICL models outperform BM25 by up to 4.9 points in recall@10 and 2.9 points in MAP@10
- ICL is important for adapting neural models to regulatory QA tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling retrievers with more training data and parameters significantly improves retrieval performance in regulatory contexts
- Mechanism: Larger models trained on extensive datasets learn richer semantic representations that better capture regulatory language patterns and relationships
- Core assumption: The increased capacity and diverse training data allow the model to better understand complex regulatory language and document structures
- Evidence anchors:
  - [section]: "We find the models of NV-Embed-v2 and BGE-EN-ICL, which are fine-tuned with more training data and more parameters, outperform BM25 significantly in all the settings. Compared with BM25, recall@10 and MAP@10 are improved by up to 4.9 points and up to 2.9 points, respectively."
  - [abstract]: "Experimental results show that scaling retrievers with extensive training data and parameters significantly improves retrieval performance, with recall@10 reaching up to 69.1% and MAP@10 up to 53.9%."
  - [corpus]: Weak evidence - no corpus support found for this specific scaling mechanism
- Break condition: When regulatory documents become so specialized that general training data no longer captures domain-specific terminology and relationships

### Mechanism 2
- Claim: In-context learning (ICL) adapts neural models to new regulatory QA tasks with minimal additional training
- Mechanism: Providing demonstration examples in the prompt allows the model to learn task-specific patterns without full fine-tuning
- Core assumption: The model's pre-training has captured sufficient general knowledge to adapt to new tasks through demonstration
- Evidence anchors:
  - [section]: "We find ICL is important for adapting the neural models to this new task, while the advantage of including more examples in the demonstration is very marginal."
  - [abstract]: "These answers must synthesize information from multiple sources, ensuring that all pertinent obligations are included to fully address the regulatory query."
  - [corpus]: Weak evidence - no corpus support found for ICL effectiveness in regulatory contexts
- Break condition: When the task requires knowledge or reasoning patterns not captured in the pre-training data, regardless of demonstration examples

### Mechanism 3
- Claim: The RePASs metric effectively evaluates regulatory answer quality by measuring entailment, contradiction avoidance, and obligation coverage
- Mechanism: By using NLI models to compare answer sentences with source passages, RePASs ensures answers are supported by evidence while capturing all obligations
- Core assumption: NLI models can accurately determine semantic relationships between regulatory text fragments at the sentence level
- Evidence anchors:
  - [section]: "The RePASs metric demonstrates high alignment with expert judgments on gold standard data" and "high scores obtained suggest that RePASs aligns well with expert judgements"
  - [abstract]: "We develop RePASs a novel reference-free metric aimed at evaluating the quality of generated answers in regulatory compliance contexts"
  - [corpus]: Weak evidence - no corpus support found for RePASs validation beyond the paper's own experiments
- Break condition: When regulatory language becomes too ambiguous or context-dependent for NLI models to make reliable semantic judgments

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: NLI is fundamental to RePASs evaluation and question-passage validation, determining whether answers are supported by source passages
  - Quick check question: Can you explain the difference between entailment, contradiction, and neutral relationships in NLI?

- Concept: Information Retrieval (IR) metrics
  - Why needed here: Understanding recall, precision, MAP, and rank fusion is essential for evaluating retrieval performance in RIRAG
  - Quick check question: What's the difference between recall@K and MAP@K, and when would you prioritize one over the other?

- Concept: Document-aware passage retrieval
  - Why needed here: RIRAG requires retrieving passages from long documents, necessitating understanding of how document context affects passage relevance
  - Quick check question: How does rank fusion between document and passage rankings improve retrieval in multi-document regulatory contexts?

## Architecture Onboarding

- Component map:
  - Data pipeline: Document standardization → JSON structuring → Question generation → NLI validation
  - Retrieval pipeline: Query processing → Multiple retriever models → Rank fusion → Top-10 passage selection
  - Generation pipeline: Filtered passages → GPT-4 answer generation → RePASs evaluation
  - Evaluation pipeline: Expert validation → RePASs scoring → Performance analysis

- Critical path: Question → Passage Retrieval (BM25/DRAGON+/SPLADE/ColBERTv2/NV-Embed-v2/BGE-EN-ICL) → Rank Fusion → Filtered Passages → GPT-4 Answer Generation → RePASs Evaluation

- Design tradeoffs:
  - Single-vector vs multi-vector retrievers: Simpler models vs better semantic understanding
  - Dense vs sparse retrieval: Semantic matching vs lexical precision
  - Rank fusion weights: Balancing document context vs passage relevance
  - Passage filtering threshold: Completeness vs conciseness

- Failure signatures:
  - Low recall@10: Retrieval models failing to capture regulatory language patterns
  - High contradiction scores: Answer generation not properly constrained by source passages
  - Low obligation coverage: Generation missing required regulatory obligations
  - Inconsistent RePASs scores: Metric sensitivity to answer phrasing variations

- First 3 experiments:
  1. Compare BM25-only vs neural retriever performance on a small validation set to establish baseline retrieval quality
  2. Test different rank fusion weights (α values) to optimize the balance between document and passage rankings
  3. Evaluate RePASs performance on synthetically generated answers vs human-written answers to validate the metric's reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RIRAG scale with increasing document complexity and length in regulatory corpora beyond the ADGM dataset?
- Basis in paper: [inferred] The paper mentions that regulatory documents are characterized by length, complexity, and frequent updates, and that the ObliQA dataset was created from ADGM documents specifically.
- Why unresolved: The evaluation was conducted on a single regulatory authority's document collection (ADGM), and the paper does not address how the system would perform on more complex or larger regulatory corpora.
- What evidence would resolve it: Experiments testing RIRAG on regulatory documents from multiple authorities with varying complexity levels, document lengths, and update frequencies would provide insights into scalability.

### Open Question 2
- Question: What is the impact of different NLI model choices on the RePASs metric's effectiveness and alignment with expert judgments?
- Basis in paper: [explicit] The paper uses specific NLI models (nli-deberta-v3-xsmall and microsoft/deberta-large-mnli) for entailment and contradiction detection, but does not explore alternative models.
- Why unresolved: The choice of NLI model could significantly affect the RePASs metric's performance, but the paper does not investigate how different NLI models might impact the results.
- What evidence would resolve it: Comparative experiments using different NLI models (e.g., BERT-based, RoBERTa-based, or multilingual models) to evaluate their impact on RePASs scores and alignment with expert judgments.

### Open Question 3
- Question: How does the RePASs metric perform on regulatory domains outside of financial services, such as healthcare or environmental regulations?
- Basis in paper: [inferred] The ObliQA dataset is focused on financial regulations from ADGM, and RePASs is evaluated on this specific domain.
- Why unresolved: Regulatory documents vary significantly across industries, and the paper does not address whether RePASs is generalizable to other regulatory domains with different terminology and compliance requirements.
- What evidence would resolve it: Testing RePASs on regulatory datasets from other domains (e.g., healthcare, environmental, or data protection regulations) and comparing its performance to expert evaluations in those domains would determine its generalizability.

## Limitations

- Limited baseline comparison: Only BM25 is used as a baseline, without comparison to other strong retrieval methods
- Domain specificity: Evaluation only on financial regulations from ADGM, limiting generalizability to other regulatory domains
- Metric validation: RePASs only validated on gold standard data without testing on adversarial cases or diverse domains

## Confidence

- **High** confidence: The overall system architecture combining retrieval and generation is sound and follows established best practices in the field.
- **Medium** confidence: Scaling retrievers improves performance, based on the limited baseline comparison and lack of ablation studies.
- **Low** confidence: ICL is sufficient for regulatory task adaptation, given the lack of quantitative evidence and domain-specific testing.
- **Medium** confidence: RePASs metric reliability, as it only validated on gold standard data without adversarial testing.

## Next Checks

1. **Ablation study on retrieval scaling**: Test whether adding more parameters or more training data contributes more to performance improvements by comparing models with similar parameter counts but different training data sizes, and vice versa.

2. **ICL effectiveness quantification**: Systematically vary the number of demonstration examples (0, 1, 3, 5, 10) in the prompt and measure the impact on answer quality to determine the true marginal benefit.

3. **RePASs metric robustness testing**: Create adversarial regulatory questions where small wording changes should or shouldn't affect the RePASs score, testing whether the metric is sensitive to superficial variations or captures genuine semantic differences.