---
ver: rpa2
title: 'Alignment of Diffusion Models: Fundamentals, Challenges, and Future'
arxiv_id: '2409.07253'
source_url: https://arxiv.org/abs/2409.07253
tags:
- diffusion
- alignment
- human
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews the emerging field of diffusion
  model alignment, which aims to improve the alignment of these models with human
  preferences. The work covers fundamental alignment techniques, including preference
  data modeling and algorithms like RLHF and DPO, as well as specific alignment techniques
  for diffusion models, such as RLHF-based fine-tuning, DPO adaptations, and test-time
  alignment strategies.
---

# Alignment of Diffusion Models: Fundamentals, Challenges, and Future

## Quick Facts
- arXiv ID: 2409.07253
- Source URL: https://arxiv.org/abs/2409.07253
- Reference count: 40
- Primary result: Comprehensive survey of diffusion model alignment techniques covering fundamentals, specific methods for diffusion models, benchmarks, and future directions

## Executive Summary
This survey provides a comprehensive overview of diffusion model alignment, which aims to improve these models' alignment with human preferences. The work covers fundamental alignment techniques like preference data modeling and algorithms including RLHF and DPO, as well as specific alignment techniques for diffusion models such as RLHF-based fine-tuning, DPO adaptations, and test-time alignment strategies. It discusses benchmarks, evaluation metrics, and future research directions while highlighting the unique challenges in aligning diffusion models, including their iterative nature and the complexity of visual content.

## Method Summary
The survey systematically reviews alignment techniques by first establishing fundamental concepts (preference data modeling, reward modeling via Bradley-Terry models, and optimization algorithms), then applying these to diffusion models through RLHF-based fine-tuning, DPO adaptations, and test-time alignment strategies. The reproduction plan involves collecting preference data, training reward models, and fine-tuning diffusion models using either RLHF or DPO approaches. The survey identifies several unknowns including specific implementation details of preference data collection and fine-tuning processes, and notes common failure modes such as overfitting to preference data and distributional shift.

## Key Results
- Diffusion model alignment faces unique challenges due to the iterative denoising process and visual content complexity
- Both training-based methods (RLHF, DPO) and test-time alignment strategies are viable approaches
- Self-alignment and pluralistic alignment represent promising future directions for capturing diverse human preferences
- Evaluation requires both automated metrics and human judgments to assess alignment quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models can be aligned with human preferences by treating the denoising process as a multi-step Markov decision process (MDP) and optimizing expected rewards.
- Mechanism: The denoising process is formulated as sequential decisions where at each step t, the model chooses how to denoise the current latent given the prompt and previous latent. RLHF algorithms like DDPO optimize this policy to maximize human preference rewards.
- Core assumption: Human preferences can be captured by a reward function that scores the final generated image, and this reward signal can guide the multi-step denoising decisions.
- Evidence anchors:
  - [abstract] "recent studies have investigated aligning diffusion models with human expectations and preferences"
  - [section 4.1] "To address the high computational overhead... researchers often formulate the denoising process as a multi-step Markov decision process (MDP)"
  - [corpus] Weak - the corpus doesn't provide specific evidence about MDP formulation for diffusion alignment
- Break condition: The reward signal becomes too sparse or noisy to provide meaningful gradient signals for the multi-step denoising process.

### Mechanism 2
- Claim: Direct Preference Optimization (DPO) can align diffusion models without requiring explicit reward model training by optimizing directly on pairwise preference data.
- Mechanism: DPO optimizes the log-likelihood ratio of preferred vs dis-preferred samples under a Bradley-Terry model framework, adapting the objective to work across the entire denoising trajectory.
- Core assumption: Human preferences can be modeled as pairwise comparisons that follow Bradley-Terry distributions, and these can be optimized directly without intermediate reward modeling.
- Evidence anchors:
  - [abstract] "covering advancements in fundamentals of alignment, alignment techniques of diffusion models"
  - [section 4.2] "Diffusion-DPO... adapts the original DPO objective (Eq. (6)) to the iterative nature of diffusion models"
  - [corpus] Weak - corpus doesn't discuss DPO adaptation for diffusion models specifically
- Break condition: Distributional shift occurs between the preference data distribution and the evolving policy distribution, causing optimization instability.

### Mechanism 3
- Claim: Test-time alignment strategies can modify the sampling trajectory during inference to align outputs with external preferences without model retraining.
- Mechanism: Methods like reward-guided decoding use external reward models to steer the denoising process at each step, optimizing the sampling trajectory directly for preference objectives.
- Core assumption: The sampling process can be modified during inference using gradient-based or derivative-free optimization methods without requiring full model updates.
- Evidence anchors:
  - [abstract] "covering advancements in fundamentals of alignment, alignment techniques of diffusion models"
  - [section 4.3] "explicit reward-guided strategies directly use an external reward function... to modify the sampling trajectory at each step"
  - [corpus] Weak - corpus doesn't provide evidence about test-time alignment methods for diffusion models
- Break condition: The computational overhead of test-time optimization becomes prohibitive or the reward function overfits to proxy metrics rather than true human preferences.

## Foundational Learning

- Concept: Markov Decision Process formulation for diffusion denoising
  - Why needed here: Understanding how to cast the iterative denoising process as sequential decision-making is crucial for applying RL techniques to diffusion model alignment
  - Quick check question: How does treating each denoising step as a decision point enable the application of policy gradient methods to diffusion models?

- Concept: Bradley-Terry preference modeling
  - Why needed here: This probabilistic model forms the theoretical foundation for converting pairwise human preferences into learnable reward signals
  - Quick check question: What is the relationship between the Bradley-Terry model and the logistic function used in preference optimization?

- Concept: KL regularization in policy optimization
  - Why needed here: KL regularization helps prevent reward hacking by constraining how much the aligned model can deviate from the original pre-trained model
  - Quick check question: How does KL regularization balance between following human preferences and maintaining model stability during alignment?

## Architecture Onboarding

- Component map: Pre-trained diffusion model -> Preference dataset collection -> Reward model training (RLHF) or direct optimization (DPO) -> Aligned model evaluation -> Iterative refinement

- Critical path: Pre-trained diffusion model → Preference dataset collection → Reward model training (RLHF) or direct optimization (DPO) → Aligned model evaluation → Iterative refinement

- Design tradeoffs:
  - RLHF vs DPO: RLHF offers more flexible reward functions but requires expensive reward model training and suffers from high variance; DPO is simpler but sensitive to distributional shift
  - Training-based vs test-time alignment: Training-based methods provide stronger alignment but require full fine-tuning; test-time methods are more flexible but computationally heavier at inference
  - Human vs AI feedback: Human feedback is more reliable but expensive; AI feedback is scalable but may inherit biases

- Failure signatures:
  - Over-optimization of reward metrics leading to reward hacking (images maximize proxy scores but lose quality)
  - Distributional shift causing preference model to become unreliable on new data
  - High variance in RL training causing instability and poor convergence
  - Memory overhead from storing intermediate denoising steps during optimization

- First 3 experiments:
  1. Implement basic Diffusion-DPO on a small preference dataset to verify the alignment pipeline works end-to-end
  2. Compare RLHF vs DPO on a held-out test set to measure which approach provides better alignment quality
  3. Test test-time alignment methods on a pre-trained model to validate inference-time optimization feasibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can self-alignment of diffusion models be achieved by leveraging the model's internal representations (e.g., attention maps or intermediate denoising steps) to generate a reward signal without external supervision?
- Basis in paper: [explicit] The paper discusses self-alignment as a future direction, suggesting that large diffusion models may already possess implicit knowledge of human preferences within their internal representations.
- Why unresolved: Current alignment methods heavily rely on external human or AI feedback. Extracting a reliable reward signal from internal model representations is a novel and unexplored approach for diffusion models.
- What evidence would resolve it: Demonstrating that a diffusion model can improve its alignment with human preferences by iteratively using its own internal representations as a reward signal, leading to measurable gains in alignment metrics compared to models trained with external feedback.

### Open Question 2
- Question: How can pluralistic alignment be effectively implemented for diffusion models to capture diverse human preferences in visual content, such as aesthetics, style, and cultural context, without collapsing to a single, averaged preference?
- Basis in paper: [explicit] The paper highlights the challenge of capturing diverse human preferences and proposes pluralistic alignment as a future direction, contrasting it with current methods that often enforce uniformity.
- Why unresolved: Visual content is inherently more subjective than text, making it difficult to model and optimize for multiple, potentially conflicting preference distributions. Current alignment frameworks are not designed for this complexity.
- What evidence would resolve it: Developing and validating a method that can generate a distribution of outputs reflecting diverse human preferences, with each output preferred by a distinct user group, and showing that this approach outperforms methods that optimize for a single, averaged preference.

### Open Question 3
- Question: What is the optimal balance between training-based alignment (e.g., RLHF, DPO) and test-time alignment strategies for diffusion models, and how can these approaches be effectively combined to achieve robust and efficient alignment?
- Basis in paper: [inferred] The paper discusses both training-based and test-time alignment techniques, highlighting their respective strengths and limitations. It suggests that combining these approaches could lead to more robust and versatile alignment systems.
- Why unresolved: While both approaches have been studied, their relative effectiveness and the best strategies for integration are not well understood. The optimal combination may depend on the specific application and the nature of human preferences.
- What evidence would resolve it: Empirical studies comparing different combinations of training-based and test-time alignment methods, demonstrating scenarios where each combination excels and providing guidelines for choosing the most appropriate strategy based on the alignment goals and computational constraints.

## Limitations
- The survey lacks empirical validation of proposed mechanisms, particularly regarding MDP formulations and test-time alignment effectiveness
- No quantitative comparisons between different alignment approaches are provided
- Computational constraints and real-world deployment scenarios are not addressed
- Discussion of failure modes is superficial without detailed analysis of specific observed cases

## Confidence
- **High confidence**: The survey accurately describes existing alignment techniques (RLHF, DPO, test-time alignment) and their general applicability to diffusion models
- **Medium confidence**: The formulation of diffusion denoising as an MDP process is theoretically sound but practical effectiveness remains to be demonstrated
- **Low confidence**: Claims about superiority of specific alignment variants and practical utility of test-time alignment methods lack supporting evidence

## Next Checks
1. Implement and compare RLHF and Diffusion-DPO on a standardized benchmark (e.g., LAION Aesthetic) with quantitative metrics for both alignment quality and computational efficiency.
2. Conduct controlled experiments testing the MDP formulation by varying the number of denoising steps and measuring how this affects alignment performance and stability.
3. Evaluate test-time alignment methods on real-world applications to measure the tradeoff between alignment quality improvements and computational overhead during inference.