---
ver: rpa2
title: 'Persuasiveness of Generated Free-Text Rationales in Subjective Decisions:
  A Case Study on Pairwise Argument Ranking'
arxiv_id: '2406.13905'
source_url: https://arxiv.org/abs/2406.13905
tags:
- argument
- rationales
- rationale
- persuasiveness
- persuasive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the persuasiveness of free-text rationales
  generated by nine large language models (LLMs) for pairwise argument ranking, a
  highly subjective task. The study evaluates the persuasiveness of these rationales
  using human and GPT-4-based assessments, finding that open-source models, particularly
  Llama2-70B-chat, generate more persuasive rationales than GPT models.
---

# Persuasiveness of Generated Free-Text Rationales in Subjective Decisions: A Case Study on Pairwise Argument Ranking

## Quick Facts
- arXiv ID: 2406.13905
- Source URL: https://arxiv.org/abs/2406.13905
- Reference count: 40
- Key outcome: Open-source LLMs, particularly Llama2-70B-chat, generate more persuasive rationales than GPT models for pairwise argument ranking.

## Executive Summary
This paper investigates the persuasiveness of free-text rationales generated by nine large language models (LLMs) for pairwise argument ranking, a highly subjective task. The study evaluates these rationales using both human and GPT-4-based assessments, finding that open-source models outperform GPT models in generating persuasive content. Key factors contributing to persuasiveness include contrast (refuting the alternative argument) and rationale length. The research also demonstrates that prompting models with specific persuasive parameters can enhance rationale persuasiveness, while self-refinement methods show limited improvement.

## Method Summary
The study evaluates nine LLMs on two argument ranking datasets (IBM-9k and IBM-30k), generating rationales for pairwise comparisons. Rationales undergo basic-form filtering and content evaluation for contrast and novelty features. Persuasiveness is assessed through human annotations and GPT-4 pairwise comparisons. The study then explores methods to enhance persuasiveness through explicit prompting with persuasive parameters and self-refinement techniques, comparing their effectiveness across different models.

## Key Results
- Open-source models, particularly Llama2-70B-chat, generate more persuasive rationales than GPT models for pairwise argument ranking.
- Contrast (refuting the alternative argument) and rationale length are the strongest predictors of persuasiveness.
- Prompting LLMs with explicit instructions to include contrastive and detailed justifications improves rationale persuasiveness more effectively than self-refinement methods.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive rationales (refuting the alternative argument) are the strongest predictor of persuasiveness in subjective tasks.
- Mechanism: By explicitly weakening the unchosen argument, the chosen argument appears relatively stronger, increasing acceptance likelihood.
- Core assumption: In pairwise subjective decisions, highlighting flaws in the alternative improves perceived quality of the chosen option.
- Evidence anchors:
  - [section 5.2] SHAP analysis shows contrast has the highest impact on average persuasive rank.
  - [section 5.2] In length-controlled clustering, contrast percentages are significantly higher in high-persuasive groups (ANOVA, p < 0.05).
  - [corpus] No strong corpus support found; cited evidence is from this study's own analysis.
- Break condition: If contrast becomes repetitive or introduces new bias, persuasiveness may decline.

### Mechanism 2
- Claim: Prompting LLMs with explicit instructions to include contrastive and detailed justifications improves rationale persuasiveness.
- Mechanism: Direct instructions guide the model to produce content that matches known persuasive features rather than relying on implicit reasoning.
- Core assumption: LLMs can follow explicit prompting guidelines to generate more persuasive rationales than relying on default reasoning.
- Evidence anchors:
  - [section 5.3] Llama2-7B-chat-persuasion-prompted achieved higher APR than both baseline and self-refined versions.
  - [section 5.3] Re-prompting with "2 sentences supporting + 2 sentences refuting" improved persuasiveness rankings.
  - [corpus] Weak corpus support; no external studies cited confirming this prompting method's effectiveness.
- Break condition: If prompt instructions conflict with the model's knowledge, quality may degrade.

### Mechanism 3
- Claim: Self-refinement methods (evaluate and refine) do not improve rationale persuasiveness compared to explicit prompting.
- Mechanism: Models tend to agree with their own prior outputs when asked to self-evaluate, limiting improvement potential.
- Core assumption: LLMs are biased toward affirming their initial reasoning when asked to evaluate themselves.
- Evidence anchors:
  - [section 5.3] Llama2-7B-chat-persuasion-refined did not improve over baseline; prompting method outperformed.
  - [section 5.3] "Evaluate and refine" did not improve persuasiveness compared to prompting with persuasive parameters.
  - [corpus] No external corpus evidence provided for self-refinement limitations.
- Break condition: If refinement prompts are more structured or adversarial, improvement might occur.

## Foundational Learning

- Concept: Pairwise ranking tasks
  - Why needed here: This study evaluates persuasiveness in pairwise argument ranking, requiring understanding of how subjective quality judgments work.
  - Quick check question: In a pairwise ranking setup, if two arguments support the same stance, what determines the "winner"?

- Concept: Free-text rationale generation and evaluation
  - Why needed here: The study's core task is generating and analyzing rationales, so understanding what makes a rationale valid, contrastive, and novel is essential.
  - Quick check question: What distinguishes a valid rationale from a repetitive one in this context?

- Concept: Persuasiveness measurement via pairwise comparison
  - Why needed here: Persuasiveness is assessed through pairwise comparisons rather than absolute scores due to task subjectivity.
  - Quick check question: Why might a single persuasiveness score be inadequate for this task?

## Architecture Onboarding

- Component map: Data collection -> Basic-form filtering -> Content evaluation (contrast/novelty) -> Persuasiveness ranking (human/GPT4) -> Analysis -> Improvement via prompting
- Critical path: Dataset creation -> Rationale generation (9 LLMs) -> Human evaluation (basic-form + content) -> Automatic persuasiveness ranking -> Improvement experiments
- Design tradeoffs: Manual human evaluation ensures quality but limits scale; automatic GPT4 evaluation enables large-scale analysis but may miss nuanced judgments.
- Failure signatures: Low inter-annotator reliability (e.g., Krippendorff's alpha for novelty was 0.31) indicates ambiguous evaluation criteria; basic-form failures suggest model limitations in generating coherent rationales.
- First 3 experiments:
  1. Generate rationales for a small, agreed-upon subset and manually validate basic-form compliance.
  2. Evaluate content features (contrast/novelty) on filtered rationales to confirm expected patterns.
  3. Run automatic persuasiveness ranking on a larger set to test scalability of the approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the persuasiveness of rationales generated by large language models (LLMs) vary across different subjective tasks beyond pairwise argument ranking?
- Basis in paper: [inferred] The paper focuses on pairwise argument ranking as a case study for analyzing the persuasiveness of LLM-generated rationales in subjective tasks. The authors suggest that expanding the study to other subjective tasks would provide a more comprehensive evaluation.
- Why unresolved: The paper only examines one specific subjective task (pairwise argument ranking) and does not explore how the findings generalize to other domains.
- What evidence would resolve it: Conducting similar analyses on a variety of subjective tasks, such as debate assistance, opinion summarization, or decision-making in ethical dilemmas, would provide insights into the generalizability of the results.

### Open Question 2
- Question: To what extent does the instruction tuning of LLMs influence their ability to generate persuasive rationales compared to other factors like model size or further tuning with advanced models?
- Basis in paper: [explicit] The paper highlights that instruction tuning is essential for creating effective rationales, as models without it failed to provide valid justifications. However, it also notes that scaling up parameters and further instruction tuning based on advanced models can enhance persuasiveness.
- Why unresolved: While the paper identifies the importance of instruction tuning, it does not quantify its relative impact compared to other factors like model size or additional tuning.
- What evidence would resolve it: Conducting controlled experiments that isolate the effects of instruction tuning, model size, and further tuning on the persuasiveness of rationales would clarify their relative contributions.

### Open Question 3
- Question: How does the presence of novelty in rationales influence their persuasiveness, especially when controlling for rationale length?
- Basis in paper: [explicit] The paper finds that novelty has a less pronounced impact on persuasiveness compared to contrast and length. However, it also suggests that novelty in lengthy rationales may act as a confounding factor.
- Why unresolved: The paper does not fully disentangle the effects of novelty from those of length, leaving the question of novelty's true impact open.
- What evidence would resolve it: Analyzing the persuasiveness of rationales with controlled lengths but varying degrees of novelty would help determine the independent effect of novelty on persuasiveness.

## Limitations

- The study's findings may not generalize beyond pairwise argument ranking to other subjective decision tasks.
- GPT-4 persuasiveness evaluation may not fully capture human judgment nuances, particularly for contrast and novelty features where inter-annotator agreement was modest.
- The finding that open-source models outperform GPT models in persuasiveness warrants caution as it may reflect task-specific characteristics rather than general superiority.

## Confidence

**High**: Contrast as strongest predictor of persuasiveness (supported by SHAP analysis and statistical validation)
**Medium**: Prompting effectiveness for enhancing persuasiveness (demonstrated in controlled experiments but limited to specific model versions)
**Medium**: Self-refinement limitations (shown within study's experimental framework but not externally validated)

## Next Checks

1. Replicate persuasiveness findings using human-only evaluations across multiple argument ranking datasets to verify GPT-4 proxy reliability
2. Test prompt engineering methods on additional open-source models (beyond Llama2) to assess generalizability of persuasiveness improvements
3. Conduct ablation studies removing contrast and length features to quantify their relative contributions to persuasiveness across different reasoning tasks