---
ver: rpa2
title: 'Sub-goal Distillation: A Method to Improve Small Language Agents'
arxiv_id: '2405.02749'
source_url: https://arxiv.org/abs/2405.02749
tags:
- task
- sub-goal
- sub-goals
- actions
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to transfer the performance of large
  language models (LLMs) to smaller language models for decision-making in complex
  interactive text environments. The approach involves a hierarchical agent with a
  planning module that generates sub-goals through knowledge distillation from an
  LLM, and an execution module that learns to accomplish these sub-goals using elementary
  actions.
---

# Sub-goal Distillation: A Method to Improve Small Language Agents

## Quick Facts
- arXiv ID: 2405.02749
- Source URL: https://arxiv.org/abs/2405.02749
- Authors: Maryam Hashemzadeh; Elias Stengel-Eskin; Sarath Chandar; Marc-Alexandre Cote
- Reference count: 40
- Primary result: Improves performance over standard imitation learning by 16.7% (absolute) on ScienceWorld benchmark

## Executive Summary
This paper proposes a method to transfer the performance of large language models (LLMs) to smaller language models for decision-making in complex interactive text environments. The approach involves a hierarchical agent with a planning module that generates sub-goals through knowledge distillation from an LLM, and an execution module that learns to accomplish these sub-goals using elementary actions. The method is evaluated on ScienceWorld, a challenging multi-task interactive text environment, where it outperforms standard imitation learning by 16.7% (absolute). The approach is also more cost-effective in terms of LLM interactions compared to other LLM-based methods.

## Method Summary
The method involves constructing a hierarchical agent comprising a planning module, which learns through knowledge distillation from an LLM to generate sub-goals, and an execution module, which learns to accomplish these sub-goals using elementary actions. The process begins by collecting expert trajectories from the ScienceWorld environment. These trajectories are then annotated with sub-goals using an LLM (ChatGPT) following a specific prompt format. Two FLAN-T5-LARGE models are fine-tuned - one for sub-goal generation and one for action generation - using the annotated data. During inference, the high-level policy generates sub-goals from task descriptions, which guide the low-level action generator to produce the next action. This hierarchical approach significantly reduces the number of LLM queries needed during inference, making the system more cost-effective while maintaining or improving performance.

## Key Results
- Hierarchical agent achieves 65.43% average score on ScienceWorld, surpassing baseline methods by 16.7% absolute
- Fixed sub-goal sequence eliminates need for real-time LLM access during inference, reducing costs
- Performance improvement is consistent across 30 tasks spanning 10 science domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sub-goal distillation reduces the number of LLM queries needed during inference.
- Mechanism: Instead of querying the LLM for every action, the system queries once per task to extract a sequence of sub-goals from an expert trajectory, then uses smaller fine-tuned models to generate actions aligned with those sub-goals.
- Core assumption: The sub-goal sequence extracted by the LLM can be reused across multiple variations of the same task.
- Evidence anchors:
  - [abstract] "Importantly, neither module relies on real-time access to an LLM during inference, significantly reducing the overall cost associated with LLM interactions to a fixed cost."
  - [section 3.2] "We assume access to a collection of expert trajectories. Then we prompt an LLM with two in-context examples... Given the two in-context examples and a new task description with its expert trajectory, the LLM is then instructed to generate a response."
- Break condition: If the expert trajectory is too different across task variations, the sub-goal sequence may not generalize, requiring additional LLM queries.

### Mechanism 2
- Claim: Hierarchical policies improve performance on long-horizon tasks by reducing action-level hallucinations.
- Mechanism: The high-level sub-goal generator provides coarse guidance, while the low-level action generator fills in fine-grained actions, constraining the search space and reducing drift from the intended path.
- Core assumption: The sub-goal generator produces coherent, task-relevant sub-goals that the action generator can reliably execute.
- Evidence anchors:
  - [abstract] "Our approach involves constructing a hierarchical agent comprising a planning module, which learns through Knowledge Distillation from an LLM to generate sub-goals, and an execution module, which learns to accomplish these sub-goals using elementary actions."
  - [section 3.3] "During inference, we first leverage the high-level policy to generate a sub-goal. This generated sub-goal is then fed into the action generator, allowing it to produce the next action aligned with the provided sub-goal."
- Break condition: If the sub-goal generator produces irrelevant or incorrect sub-goals, the action generator may follow a misguided trajectory.

### Mechanism 3
- Claim: Fine-tuning smaller models with distilled knowledge from an LLM enables cost-effective deployment without sacrificing performance.
- Mechanism: The system distills sub-goal-action pairs from an LLM into smaller language models (770M parameters), allowing the smaller models to replicate LLM-like planning performance at a fraction of the computational cost.
- Core assumption: The smaller models can capture the essential reasoning patterns from the LLM when trained on distilled data.
- Evidence anchors:
  - [abstract] "Our approach involves constructing a hierarchical agent comprising a planning module... and an execution module, which learns to accomplish these sub-goals using elementary actions."
  - [section 4.4] "Our approach demonstrates an overall performance of 65.43%, surpassing Swift-only by 16.71% (33.9% relative increase)."
- Break condition: If the smaller models cannot generalize the distilled patterns to new task instances, performance may degrade.

## Foundational Learning

- Concept: Knowledge Distillation (KD)
  - Why needed here: KD allows transferring reasoning capabilities from a large, expensive LLM to a smaller, deployable model, enabling cost-effective decision-making in complex environments.
  - Quick check question: What is the main difference between black-box KD and white-box KD, and which is used in this work?

- Concept: Hierarchical Planning
  - Why needed here: Breaking down complex tasks into sub-goals reduces the cognitive load on the low-level action generator and mitigates hallucinations by constraining the action space.
  - Quick check question: How does the sub-goal generator influence the action generator's behavior during inference?

- Concept: Imitation Learning
  - Why needed here: Imitation learning trains the smaller models to replicate expert behavior by learning from annotated sub-goal-action pairs, ensuring the agent mimics the demonstrated trajectory.
  - Quick check question: What type of data is used to train the action generator and sub-goal generator in this approach?

## Architecture Onboarding

- Component map:
  - Task description → Sub-goal Generator → Sub-goal
  - Sub-goal + History → Action Generator → Action
  - Action → Environment → Observation/Score
  - Repeat until task completion or max steps

- Critical path:
  1. Task description → Sub-goal Generator → Sub-goal
  2. Sub-goal + History → Action Generator → Action
  3. Action → Environment → Observation/Score
  4. Repeat until task completion or max steps

- Design tradeoffs:
  - Using sub-goals adds an extra inference step but reduces action-level hallucinations.
  - Fixed sub-goal sequence limits adaptability but improves efficiency.
  - Smaller models reduce cost but may underperform on highly novel tasks.

- Failure signatures:
  - Agent gets stuck cycling through the same sub-goal (sub-goal generator failure).
  - Agent performs irrelevant actions (sub-goal-action misalignment).
  - Performance drops on unseen task variations (generalization failure).

- First 3 experiments:
  1. Replace the sub-goal generator with random sub-goals to test its impact on performance.
  2. Vary the sub-goal generator size while keeping the action generator fixed to measure performance gains.
  3. Test the model on held-out task types to evaluate generalization.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the limitations discussed, several unresolved issues emerge regarding the approach's scalability, adaptability, and robustness to imperfect expert demonstrations.

## Limitations

- Reliance on access to expert trajectories for knowledge distillation, constraining the method's applicability
- Substantial upfront LLM interaction required for trajectory annotation, potentially limiting scalability to extremely large task spaces
- Demonstrated effectiveness on ScienceWorld does not guarantee generalization to more open-ended or less structured domains

## Confidence

**High Confidence**: The claim that sub-goal distillation reduces LLM queries during inference is well-supported by the architecture description and evaluation metrics showing fixed LLM interaction costs. The comparison showing 16.7% absolute performance improvement over standard imitation learning is directly demonstrated in the experimental results.

**Medium Confidence**: The assertion that hierarchical policies improve performance on long-horizon tasks by reducing action-level hallucinations is plausible given the mechanism, but the paper doesn't directly measure hallucination rates or provide ablation studies showing the specific contribution of the hierarchical structure versus other factors.

**Low Confidence**: The claim that this approach is "more cost-effective in terms of LLM interactions compared to other LLM-based methods" lacks direct comparison to specific alternative LLM-based approaches. The cost-effectiveness claim is relative to standard imitation learning rather than other LLM-based methods.

## Next Checks

1. **Generalization Test**: Evaluate the model on held-out task types not seen during training to assess whether the distilled knowledge transfers to novel scenarios, particularly testing whether sub-goal sequences from the LLM can handle task variations.

2. **Ablation Study**: Conduct an ablation where the sub-goal generator is replaced with random sub-goals or removed entirely to quantify the specific contribution of hierarchical planning to the 16.7% performance improvement.

3. **Scalability Analysis**: Measure the total LLM interaction costs (annotation + inference) across different numbers of tasks to determine the actual break-even point compared to methods that use LLM queries during inference, addressing the cost-effectiveness claim more directly.