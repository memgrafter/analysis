---
ver: rpa2
title: Towards Zero-Shot Annotation of the Built Environment with Vision-Language
  Models (Vision Paper)
arxiv_id: '2408.00932'
source_url: https://arxiv.org/abs/2408.00932
tags:
- features
- images
- urban
- image
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for zero-shot annotation of urban
  features in satellite imagery using vision-language models (VLMs). The approach
  segments satellite images, generates labeled visual prompts for each segment, and
  uses a VLM to identify and annotate specific features like stop lines and raised
  tables based on descriptive text prompts.
---

# Towards Zero-Shot Annotation of the Built Environment with Vision-Language Models (Vision Paper)

## Quick Facts
- arXiv ID: 2408.00932
- Source URL: https://arxiv.org/abs/2408.00932
- Reference count: 22
- Primary result: Pre-segmentation with set-of-mark prompting achieves up to 40% IoU accuracy vs nearly zero for direct prompting

## Executive Summary
This paper introduces a zero-shot approach for annotating urban features in satellite imagery using vision-language models. The method segments images, generates labeled visual prompts for each segment, and uses GPT-4o to identify features like stop lines and raised tables. Experiments show significant improvements over direct prompting, with 40% IoU accuracy compared to nearly zero for baseline approaches. While promising, results highlight challenges including segmentation quality, VLM comprehension of built environment features, and output variability.

## Method Summary
The approach combines image segmentation with vision-language model processing to annotate urban features without training data. First, SAM segments satellite images into candidate objects. These candidates are filtered using heuristics based on color and size, then each candidate is overlaid with visual marks and labels in three different strategies (no-context, in-context, combination). GPT-4o analyzes these marked candidates with textual prompts describing the target features, and the output is post-processed to extract bounding boxes. The entire pipeline requires no fine-tuning or labeled examples.

## Key Results
- Pre-segmentation with SoM-NC approach achieves 40% IoU accuracy vs nearly zero for direct prompting
- In-context strategy improves accuracy by 47.3% over no-context for stop lines
- VLM output variability remains a challenge even with zero temperature settings

## Why This Works (Mechanism)

### Mechanism 1
Pre-segmentation followed by set-of-mark prompting significantly improves VLM's ability to identify specific urban features in satellite imagery. SAM segments the satellite image into candidate objects, from which GPT-4o can select the appropriate feature of interest. This breaks down complex scene analysis into simpler object recognition tasks.

### Mechanism 2
Set-of-mark generation with different contexts (no-context, in-context, combination) improves VLM comprehension by providing visual grounding cues. By overlaying each segmented candidate with visual marks and labels, the VLM receives additional visual context that helps distinguish between similar-looking objects.

### Mechanism 3
The zero-shot capability of VLMs enables rapid annotation of diverse urban features without requiring retraining or fine-tuning. By using natural language prompts combined with visual segmentation, the system can identify and annotate any feature describable in text.

## Foundational Learning

- Concept: Intersection-over-Union (IoU) metric for evaluating object detection accuracy
  - Why needed here: The paper uses IoU to quantitatively measure how well the VLM's annotations match ground truth
  - Quick check question: If ground truth box is (0,0,10,10) and predicted box is (5,5,15,15), what is the IoU score?

- Concept: Vision-Language Models (VLMs) and their training characteristics
  - Why needed here: Understanding that VLMs are trained on internet images and text explains why they struggle with satellite imagery
  - Quick check question: Why might a VLM trained on internet images struggle with identifying stop lines in satellite imagery?

- Concept: Image segmentation and candidate filtering
  - Why needed here: The system relies on SAM to segment images and then applies heuristic filters
  - Quick check question: What might happen if the color filtering heuristic incorrectly excludes the target feature's color?

## Architecture Onboarding

- Component map: User Interface -> Image Segmentation (SAM) -> Candidate Filtering -> Set-of-Mark Generation (NC/IC/Comb) -> VLM Processing (GPT-4o) -> Post-processing -> Output annotations
- Critical path: User input → Image Segmentation → Candidate Filtering → Set-of-Mark Generation → VLM Processing → Post-processing → Output annotations
- Design tradeoffs: Zero-shot approach trades potential accuracy for rapid deployment and flexibility; segmentation-based approach trades computational overhead for improved VLM comprehension
- Failure signatures: Direct prompting produces nearly zero overlap with ground truth; noisy segmentation generates too many irrelevant candidates; VLM fails to select correct marked candidate; variable outputs from VLM
- First 3 experiments: 1) Compare IoU scores between direct prompting and SoM-NC approach on same test set; 2) Test impact of different candidate filtering thresholds on final annotation accuracy; 3) Evaluate performance across different urban feature types to identify generalizability limits

## Open Questions the Paper Calls Out

### Open Question 1
Can specialized segmentation models for satellite imagery significantly improve feature detection accuracy compared to general-purpose models like SAM? The paper only uses SAM and does not explore satellite-specific segmentation models.

### Open Question 2
How does the performance of VLMs vary across different cities and regions with diverse built environment features and standards? The paper only tests on images from unspecified locations and does not conduct cross-city experiments.

### Open Question 3
What is the impact of inconsistent VLM outputs on the overall reliability of the automated annotation process, and how can this variability be minimized? The paper identifies the issue but does not explore methods to reduce output variability.

## Limitations
- VLM output variability affects reproducibility and reliability even with zero temperature
- Segmentation quality heavily impacts final annotation accuracy
- Limited generalizability across diverse urban features beyond stop lines and raised tables
- System may struggle with features lacking distinctive visual markers

## Confidence

- **High confidence**: Pre-segmentation with SoM prompting significantly outperforms direct prompting (well-supported by quantitative IoU comparisons)
- **Medium confidence**: Generalizability claim across different cities and features (supported by limited evidence)
- **Low confidence**: Assertion that approach works for "any feature describable in text" (speculative given limited testing)

## Next Checks
1. Cross-city validation: Test the approach on satellite imagery from multiple cities with different urban planning styles
2. Feature diversity testing: Evaluate performance across broader range of urban features including traffic signs, pedestrian crossings, and building types
3. Segmentation ablation study: Systematically vary SAM segmentation parameters to quantify impact on final annotation accuracy