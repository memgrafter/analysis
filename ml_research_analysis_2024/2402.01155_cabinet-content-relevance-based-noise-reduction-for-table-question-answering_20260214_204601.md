---
ver: rpa2
title: 'CABINET: Content Relevance based Noise Reduction for Table Question Answering'
arxiv_id: '2402.01155'
source_url: https://arxiv.org/abs/2402.01155
tags:
- table
- relevance
- cabinet
- tokens
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CABINET, a method for improving table question
  answering by learning to weigh table tokens based on their relevance to the question.
  CABINET trains an unsupervised relevance scorer to assign low weights to irrelevant
  table tokens, allowing the QA model to focus on relevant content.
---

# CABINET: Content Relevance based Noise Reduction for Table Question Answering

## Quick Facts
- arXiv ID: 2402.01155
- Source URL: https://arxiv.org/abs/2402.01155
- Reference count: 40
- Establishes new state-of-the-art performance on WikiTQ (69.1% accuracy), FeTaQA (40.5 S-BLEU), and WikiSQL (89.5% accuracy)

## Executive Summary
CABINET introduces a novel approach to improve table question answering by learning to weigh table tokens based on their relevance to the question. The method trains an unsupervised relevance scorer to assign low weights to irrelevant table tokens, allowing the QA model to focus on relevant content. Additionally, CABINET employs a weakly-supervised cell highlighter to further boost weights of cells containing relevant information. This dual-weighting mechanism establishes new state-of-the-art performance across multiple table QA datasets while demonstrating improved robustness to noise and better performance on larger tables.

## Method Summary
CABINET operates by introducing a relevance weighting mechanism into table question answering models. The core innovation is an unsupervised relevance scorer that learns to assign weights to table tokens based on their importance to answering the given question. This scorer is trained without explicit relevance labels, instead learning to distinguish between relevant and irrelevant content through the QA task itself. The method also incorporates a weakly-supervised cell highlighting component that further refines the weighting by identifying specific cells that contain key information for answering questions. These weighted representations are then fed into standard table QA architectures, effectively filtering out noise and focusing the model's attention on the most pertinent information.

## Key Results
- Achieves 69.1% accuracy on WikiTQ dataset, establishing new state-of-the-art
- Reaches 40.5 S-BLEU on FeTaQA dataset, surpassing previous best results
- Obtains 89.5% accuracy on WikiSQL dataset, setting new benchmark
- Demonstrates improved robustness to noise and better performance on larger tables compared to baselines

## Why This Works (Mechanism)
CABINET works by addressing the fundamental challenge in table QA of information overload. Tables often contain many tokens that are irrelevant to answering a specific question, which can distract models and reduce performance. By learning to assign lower weights to irrelevant tokens and higher weights to relevant ones, CABINET effectively reduces the noise in the input representation. The unsupervised relevance scorer learns this weighting pattern through exposure to many table-question pairs, discovering which types of content tend to be relevant for different question types. The cell highlighting component adds an additional layer of refinement by identifying specific cells that contain key evidence, further sharpening the model's focus on the most important information.

## Foundational Learning

**Unsupervised learning**: Why needed - To train the relevance scorer without requiring explicit relevance annotations. Quick check - Verify the model can learn meaningful patterns from unlabeled data by testing on a held-out validation set.

**Weak supervision**: Why needed - To provide additional guidance for identifying relevant cells without full supervision. Quick check - Evaluate whether the weakly-supervised cell highlighter improves performance over purely unsupervised methods.

**Attention mechanisms**: Why needed - To implement the weighting of table tokens based on their relevance. Quick check - Examine the attention weight distributions to ensure they align with human intuition about relevance.

## Architecture Onboarding

**Component map**: Input table + question -> Unsupervised relevance scorer -> Weight assignment -> Weakly-supervised cell highlighter -> Refined weights -> Table QA model -> Answer

**Critical path**: The critical path flows from the input table and question through both the unsupervised relevance scorer and the cell highlighter to produce weighted table representations that are then processed by the table QA model. The quality of the relevance scoring directly impacts downstream performance.

**Design tradeoffs**: The method trades additional computational complexity for improved accuracy and robustness. The unsupervised approach avoids annotation costs but may require more training data to achieve good performance. The cell highlighting adds another layer of complexity but provides significant benefits in focusing the model.

**Failure signatures**: The model may fail when relevance patterns are too subtle for the unsupervised scorer to detect, when the cell highlighter makes incorrect assignments, or when the weighted representation loses important contextual information. Performance degradation is most likely on tables with complex structures or questions requiring cross-referencing between distant cells.

**First experiments**: 
1. Test the unsupervised relevance scorer in isolation to evaluate its ability to distinguish relevant from irrelevant tokens
2. Evaluate the cell highlighter's precision and recall in identifying relevant cells
3. Compare performance with and without the weighting mechanisms to quantify their individual contributions

## Open Questions the Paper Calls Out
None

## Limitations
- The approach's generalizability to domains beyond tested datasets is uncertain
- Computational overhead introduced by the additional weighting mechanisms is not extensively analyzed
- Performance sensitivity to hyperparameter choices for the weighting functions is not thoroughly investigated

## Confidence
- High confidence in the core methodology and reported performance improvements on tested datasets
- Medium confidence in claims about robustness to noise and performance on larger tables
- Medium confidence in the unsupervised nature of the relevance scoring

## Next Checks
1. Conduct ablation studies to isolate the impact of the unsupervised relevance scoring from the weakly-supervised cell highlighting on overall performance
2. Test the model's performance on tables from different domains (e.g., financial, scientific) to assess generalizability beyond current datasets
3. Evaluate the computational efficiency and memory requirements of CABINET compared to baseline models, particularly for very large tables