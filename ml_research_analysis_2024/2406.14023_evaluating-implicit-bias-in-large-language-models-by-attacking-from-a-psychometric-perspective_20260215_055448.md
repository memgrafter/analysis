---
ver: rpa2
title: Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric
  Perspective
arxiv_id: '2406.14023'
source_url: https://arxiv.org/abs/2406.14023
tags:
- bias
- llms
- attacks
- evaluation
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates implicit bias in large language models (LLMs)
  using a psychometric approach, introducing three attack methods: Disguise, Deception,
  and Teaching. By crafting prompts based on cognitive and social psychology principles,
  the study reveals that all three methods effectively elicit biased responses from
  LLMs, with Deception attacks being the most successful.'
---

# Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective

## Quick Facts
- **arXiv ID**: 2406.14023
- **Source URL**: https://arxiv.org/abs/2406.14023
- **Reference count**: 24
- **Primary result**: Psychometric-inspired attacks (Disguise, Deception, Teaching) successfully elicit biased responses from LLMs, with Deception attacks being most effective

## Executive Summary
This paper introduces a novel approach to evaluating implicit bias in large language models by leveraging principles from cognitive and social psychology. The authors develop three attack methods - Disguise, Deception, and Teaching - that exploit LLMs' instruction-following capabilities and cognitive processing patterns. By systematically attacking models like GPT-3.5, GPT-4, and GLM-3, the study reveals that these models can be manipulated to express biases across age, gender, race, and sexual orientation categories. The research demonstrates that Deception attacks are particularly effective, with models showing more bias in mild categories like age. The findings highlight the vulnerability of LLMs to psychometric manipulation and suggest the need for improved bias mitigation strategies.

## Method Summary
The study employs a psychometric-inspired approach to bias evaluation, using the CBBQ dataset containing 233 bias statements across four categories. The authors craft attack prompts based on three psychological principles: goal shifting (Disguise), cognition concordance (Deception), and imitation learning (Teaching). For discriminative tasks, they measure Attack Success Rate (ASR) - the proportion of successful attacks eliciting biased responses. For generative tasks, they measure Attack Failure Rate (AFR) - the proportion of outputs judged as unbiased by an LLM evaluator. The evaluation pipeline involves transforming bias statements into dialogues, constructing attack prompts, running them through target LLMs, and using GPT-3.5-turbo-1106 to classify responses as biased or unbiased.

## Key Results
- All three psychometric attack methods successfully elicited biased responses from tested LLMs, with Deception attacks showing the highest effectiveness
- GLM-3-turbo and GPT-4-1106-preview demonstrated stronger bias safety compared to other models
- Biases were more prevalent in mild categories like age compared to more severe categories like race and sexual orientation
- Cross-linguistic evaluation revealed that GPT-3.5-turbo-1106 showed more bias in English than Chinese responses

## Why This Works (Mechanism)

### Mechanism 1: Deception Attack Effectiveness
Deception attacks exploit cognitive concordance by presenting LLMs with conflicting cognitions - their safety training versus biased beliefs they're instructed to "firmly believe." When LLMs encounter Mental Deception (believing biased statements) or Memory Falsification (falsified memories), they experience cognitive conflict and relax safety constraints to maintain harmony between new and existing cognitions.

### Mechanism 2: Disguise Attack Through Viewpoint Contextualization
Disguise attacks embed biased content in conversational contexts, reducing model vigilance through scene activation and semantic subtlety. By asking LLMs to roleplay in online forum discussions, the conversational context activates relevant scene memories and makes bias expressions more subtle and harder to detect, reducing safety monitoring while maintaining task compliance.

### Mechanism 3: Teaching Attack Through Imitation Learning
Teaching attacks provide biased examples that LLMs imitate through in-context learning. When given several biased statements as examples (Destructive Indoctrination), LLMs use pattern recognition to mimic the examples. The similarity between examples makes them easier to learn, and the model transfers learned bias patterns to generate similar content.

## Foundational Learning

- **Concept**: Cognitive psychology principles (goal shifting, cognition concordance, imitation learning)
  - Why needed here: The attack methodology is explicitly designed around these psychological principles to exploit LLM behavior patterns.
  - Quick check question: Can you explain how each psychological principle maps to a specific attack method?

- **Concept**: Instruction-following capabilities and safety training tradeoffs
  - Why needed here: Understanding how LLMs balance task completion with safety constraints is crucial for predicting when attacks will succeed.
  - Quick check question: What happens when an LLM receives conflicting instructions - one to complete a task and another to maintain safety?

- **Concept**: In-context learning mechanisms
  - Why needed here: Teaching attacks rely on LLMs learning from few-shot examples, so understanding this mechanism is essential.
  - Quick check question: How does the number and similarity of examples affect the strength of in-context learning?

## Architecture Onboarding

- **Component map**: CBBQ dataset -> Prompt generation system -> LLM evaluation pipeline -> GPT-3.5-turbo-1106 bias detection -> Results storage

- **Critical path**:
  1. Load bias statements from CBBQ dataset
  2. Transform statements into attack prompts using three methods
  3. Send prompts to target LLMs
  4. Collect and classify responses (discriminative: ASR calculation, generative: AFR calculation)
  5. Analyze results across bias categories and models

- **Design tradeoffs**:
  - Manual vs. automatic prompt generation: Manual ensures quality but doesn't scale; automatic scales but may miss subtle requirements
  - Single vs. multiple evaluation models: Single model is consistent but may have blind spots; multiple models provide broader coverage but introduce variance
  - Bilingual vs. single-language evaluation: Bilingual provides cross-cultural insights but doubles complexity

- **Failure signatures**:
  - Low ASR across all attacks: LLMs have strong safety mechanisms that resist all attack types
  - High variance in ASR: Attack prompts may be inconsistently effective or LLMs may have inconsistent safety implementations
  - Unexpected bias transfer patterns: May indicate deeper issues with how LLMs internalize and generalize bias information

- **First 3 experiments**:
  1. Run Disguise attacks on a single bias category with GPT-3.5-turbo to verify basic functionality
  2. Compare Deception attack effectiveness (MD vs MF) on the same model to determine optimal approach
  3. Test Teaching attack across multiple bias categories to observe transfer patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different languages affect the effectiveness of psychometric attack methods on implicit bias?
- Basis in paper: The paper compares GPT-3.5-turbo-1106's bias performance in English and Chinese, showing more bias in English.
- Why unresolved: The study only tested two languages and found inconsistent results across models. It's unclear whether language differences are due to training data, instruction-following abilities, or cultural context.
- What evidence would resolve it: Systematic evaluation of attack effectiveness across multiple languages with controlled prompts and diverse cultural contexts.

### Open Question 2
- Question: Do specific vs. general mental deception approaches have different long-term effects on LLM bias exposure?
- Basis in paper: The paper compares specific bias beliefs ("young people don't want to work") versus general ones ("young people are lazy..."), finding both effective but general minds easier to generalize.
- Why unresolved: The study only tested immediate attack success rates. Long-term effects on model behavior or transferability to other bias types weren't examined.
- What evidence would resolve it: Longitudinal studies tracking how different deception approaches affect model responses across multiple sessions and bias categories.

### Open Question 3
- Question: What is the relationship between instruction-following capabilities and vulnerability to implicit bias attacks?
- Basis in paper: The paper notes that GPT-3.5-turbo-1106 shows more bias than GLM-3-turbo, possibly due to stronger instruction-following abilities in English.
- Why unresolved: The study correlates instruction-following strength with bias exposure but doesn't establish causation or determine whether this relationship holds across all attack types.
- What evidence would resolve it: Controlled experiments varying instruction complexity and measuring both compliance rates and bias exposure across multiple models and attack methods.

## Limitations
- The evaluation relies on a single bias detection model (GPT-3.5-turbo-1106) which may have its own biases and blind spots
- The CBBQ dataset contains only 233 entries, potentially limiting statistical power for robust conclusions
- Attack effectiveness results may be specific to the particular prompt engineering techniques used and may not generalize to other methodologies
- The study focuses on English and Chinese languages, potentially missing cultural nuances and bias patterns in other languages

## Confidence
- **High Confidence**: The general finding that LLMs can be manipulated to express biases through psychometric-inspired attacks, and that Deception attacks are more effective than Disguise attacks.
- **Medium Confidence**: The specific ASR/AFR percentages and model rankings, given potential limitations in the evaluation methodology and dataset size.
- **Low Confidence**: The cross-cultural generalizability of findings between English and Chinese evaluations, and the assumption that cognitive psychology principles map perfectly to LLM behavior.

## Next Checks
1. **Multi-Evaluator Validation**: Run the same bias evaluation using multiple independent bias detection models (not just GPT-3.5-turbo-1106) to verify the consistency and reliability of the bias judgments.

2. **Dataset Size Validation**: Expand the evaluation to include a larger sample from the CBBQ dataset (or additional bias datasets) to confirm that the observed patterns hold with greater statistical power.

3. **Alternative Attack Methodology**: Design and test a different set of attack prompts based on alternative psychological principles or prompt engineering techniques to determine if the observed effectiveness patterns are specific to the current methodology or represent broader LLM vulnerabilities.