---
ver: rpa2
title: Solving Continual Offline Reinforcement Learning with Decision Transformer
arxiv_id: '2401.08478'
source_url: https://arxiv.org/abs/2401.08478
tags:
- learning
- tasks
- offline
- mh-dt
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  continuous offline reinforcement learning (CORL) by introducing two Decision Transformer
  (DT)-based methods. The authors first demonstrate that DT has advantages in learning
  efficiency and zero-shot generalization but suffers from severe forgetting due to
  supervised parameter updates.
---

# Solving Continual Offline Reinforcement Learning with Decision Transformer

## Quick Facts
- arXiv ID: 2401.08478
- Source URL: https://arxiv.org/abs/2401.08478
- Reference count: 9
- This paper addresses catastrophic forgetting in continuous offline reinforcement learning using Decision Transformer-based methods

## Executive Summary
This paper tackles catastrophic forgetting in continuous offline reinforcement learning by proposing two Decision Transformer-based approaches. The authors demonstrate that while Decision Transformers offer advantages in learning efficiency and zero-shot generalization, they suffer from severe forgetting due to supervised parameter updates. They introduce Multi-Head Decision Transformer (MH-DT) with task-specific heads and shared components, and Low-Rank Adaptation Decision Transformer (LoRA-DT) that freezes most parameters while fine-tuning decisive MLP layers. Both methods significantly outperform state-of-the-art baselines on MuJoCo and Meta-World benchmarks.

## Method Summary
The paper introduces two Decision Transformer-based methods to address catastrophic forgetting in continual offline reinforcement learning. MH-DT uses multiple task-specific heads with shared transformer modules, implementing distillation and selective rehearsal to mitigate forgetting when replay buffers are available. LoRA-DT merges weights across tasks and fine-tunes only the decisive MLP layers using low-rank adaptation matrices, making it suitable for buffer-unavailable scenarios. Both methods build upon the Decision Transformer architecture, which processes trajectory sequences (state, action, reward-to-go) to predict actions through supervised learning.

## Key Results
- MH-DT achieves performance close to upper bound multi-task methods while mitigating catastrophic forgetting
- LoRA-DT demonstrates superior memory efficiency by freezing most parameters and only fine-tuning low-rank adapters
- Both methods outperform state-of-the-art baselines on MuJoCo and Meta-World benchmarks for continual offline RL

## Why This Works (Mechanism)

### Mechanism 1
DT's supervised parameter updates exacerbate catastrophic forgetting because all parameters change simultaneously during training on new task data. When dataset distribution shifts, DT updates all weights to fit new tasks, erasing previous knowledge. Actor-critic methods are less sensitive because they use temporal difference learning as an intermediate step before updating actors.

### Mechanism 2
MH-DT mitigates forgetting by separating task-specific and shared knowledge through multiple heads. The architecture splits parameters into shared transformer modules and task-specific heads. When training on new tasks, only current heads and shared modules update while previous heads remain frozen, preventing overwriting of previous task knowledge.

### Mechanism 3
LoRA-DT prevents forgetting by freezing most parameters and only fine-tuning low-rank adapters on MLP layers. This limits parameter changes to preserve previous task knowledge while adapting to new tasks. The approach assumes MLP layers are the most task-specific components in DT, so adapting only these layers provides sufficient plasticity while maintaining stability.

## Foundational Learning

- **Catastrophic forgetting in neural networks**: Understanding why standard training causes forgetting is essential to grasp why MH-DT and LoRA-DT are necessary. Quick check: What happens to previously learned parameters when all weights are updated during training on new data?

- **Decision Transformer architecture and sequence modeling**: MH-DT and LoRA-DT are both DT-based methods, so understanding how DT processes trajectories as sequences is crucial. Quick check: How does DT use trajectory sequences (state, action, reward-to-go) to predict actions?

- **Low-rank adaptation (LoRA) and parameter-efficient fine-tuning**: LoRA-DT specifically uses LoRA to adapt MLP layers while keeping most parameters frozen. Quick check: What is the advantage of using LoRA matrices instead of fine-tuning all parameters?

## Architecture Onboarding

- **Component map**: MH-DT: Multiple task-specific heads + shared transformer modules + distillation objective + selective rehearsal; LoRA-DT: Merged weights + LoRA adapters on MLP layers + separate LoRA matrices per task; Both: DT backbone with trajectory sequence processing

- **Critical path**: For MH-DT: Task head selection → Trajectory encoding → Shared module processing → Head-specific action prediction; For LoRA-DT: Trajectory encoding → Shared module processing → LoRA-adapted MLP → Action prediction

- **Design tradeoffs**: MH-DT: More memory for multiple heads but better task separation; distillation and rehearsal add complexity but improve learning; LoRA-DT: Very memory efficient but limited plasticity; rank selection is critical for performance

- **Failure signatures**: MH-DT: Poor performance on new tasks if shared modules don't capture common structure; forgetting if heads are updated incorrectly; LoRA-DT: Insufficient adaptation if rank is too low; instability if merge weight is inappropriate

- **First 3 experiments**: 1) Test single-task DT performance to establish baseline; 2) Implement MH-DT with two tasks to verify head separation works; 3) Test LoRA-DT with different ranks to find optimal trade-off between memory and performance

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal architecture for Decision Transformer-based methods in continual offline reinforcement learning? The paper acknowledges that selecting common knowledge between tasks in RL is non-trivial and focuses on two specific DT-based approaches without exploring the full design space or providing systematic comparisons of different architectural choices.

### Open Question 2
How does catastrophic forgetting in DT-based methods compare to other transformer architectures in continual learning settings? The paper establishes DT's forgetting problem but doesn't contextualize it within the broader landscape of transformer-based continual learning approaches like GATO or related architectures.

### Open Question 3
What is the relationship between task similarity and the effectiveness of knowledge sharing in DT-based continual learning? While the paper implements selective rehearsal based on similarity, it doesn't provide thorough analysis of how different similarity metrics or thresholds affect performance across diverse task sequences.

## Limitations

- The paper lacks ablation studies directly comparing DT with actor-critic methods to rigorously test claims about why DT exacerbates forgetting
- Claims about mechanism 1 (supervised updates causing forgetting) are plausible but not rigorously tested with controlled experiments
- The paper doesn't address potential negative transfer between tasks or provide theoretical bounds on performance

## Confidence

- Claims about MH-DT and LoRA-DT performance improvements: **Medium** confidence (empirical results show improvements but limited theoretical grounding)
- Claims about why DT specifically exacerbates forgetting compared to actor-critic methods: **Low** confidence (lacks direct comparison studies)
- Claims about mechanisms 2 and 3 following established continual learning patterns: **Medium** confidence (standard approaches but specific design choices lack justification)

## Next Checks

1. **Ablation study on update strategies**: Compare vanilla DT with partial parameter updates to isolate whether supervised learning per se causes forgetting versus the update strategy.

2. **Cross-dataset generalization**: Test MH-DT and LoRA-DT on unseen task combinations to verify that knowledge sharing generalizes beyond the specific task sequences used in training.

3. **Memory-accuracy tradeoff analysis**: Systematically vary the rank parameter in LoRA-DT and the number of heads in MH-DT to quantify the exact relationship between memory efficiency and performance degradation.