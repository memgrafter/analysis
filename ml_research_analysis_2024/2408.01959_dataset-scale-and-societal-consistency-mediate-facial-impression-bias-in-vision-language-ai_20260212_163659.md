---
ver: rpa2
title: Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language
  AI
arxiv_id: '2408.01959'
source_url: https://arxiv.org/abs/2408.01959
tags:
- clip
- photo
- facial
- biases
- someone
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether multimodal AI models learn human-like\
  \ facial impression biases. Analyzing 43 CLIP vision-language models and three text-to-image\
  \ generators, the authors find that CLIP models do learn societal facial impression\
  \ biases, with the degree of bias correlated with societal consistency (Spearman's\
  \ \u03C1 = .72-.76)."
---

# Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI

## Quick Facts
- arXiv ID: 2408.01959
- Source URL: https://arxiv.org/abs/2408.01959
- Authors: Robert Wolfe; Aayushi Dangol; Alexis Hiniker; Bill Howe
- Reference count: 22
- Primary result: CLIP models learn human-like facial impression biases strongly correlated with societal consistency (Spearman's ρ = .72-.76)

## Executive Summary
This study investigates whether multimodal AI models learn human-like facial impression biases by analyzing 43 CLIP vision-language models and three text-to-image generators. The authors find that CLIP models do learn societal facial impression biases, with the degree of bias correlated with societal consistency measured by inter-rater reliability. Dataset scale significantly predicts facial impression bias, particularly for unobservable traits like trustworthiness, with larger datasets (LAION-2B vs LAION-400M) showing increased human similarity. The study also reveals that CLIP models learn human-like associations between facial impression biases, with hierarchical clustering showing similar groupings to human data.

## Method Summary
The study analyzed 43 CLIP models (OpenAI CLIP, Scaling CLIP, FaceCLIP) and three text-to-image generators using the One Million Impressions (OMI) dataset containing 1,004 images rated by humans on 34 attributes. For each CLIP model, cosine similarities were computed between image and text embeddings for positive and negative attribute prompts. Spearman's correlation was used to measure the relationship between CLIP model associations and human ratings. The analysis examined effects of dataset size, model architecture, and inter-rater reliability on bias learning. Hierarchical clustering was applied to attribute correlation matrices to compare structural similarities between model and human data.

## Key Results
- CLIP models learn societal facial impression biases with strong correlation to human inter-rater reliability (Spearman's ρ = .72-.76)
- Dataset scale significantly predicts facial impression bias, with LAION-2B models showing increased human similarity for unobservable traits like trustworthiness (d = 1.33, p < .05)
- CLIP models learn human-like associations between facial impression biases, with hierarchical clustering revealing similar groupings to human data
- Text-to-image generators using CLIP as text encoder exhibit facial impression biases intersecting with racial biases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP models learn societal facial impression biases because the degree of bias learned is strongly correlated with societal consistency (inter-rater reliability).
- Mechanism: The pretraining process on web-scraped datasets captures cultural patterns where attributes that humans agree upon more consistently (higher IRR) are more strongly represented in the text-image pairs, leading CLIP to learn these associations.
- Core assumption: The pretraining dataset contains text captions that reflect societal biases and human agreement patterns.
- Evidence anchors:
  - [abstract] "the degree to which an attribute bias is learned by a CLIP model is strongly correlated with the inter-rater reliability (IRR) of human judgments of the attribute (Spearman'sρ = .73 for OpenAI models; ρ = .76 for FaceCLIP models; and ρ = .72 for Scaling models)"
  - [section] "A multiple linear regression predicting the similarity of CLIP bias to Human bias finds that the IRR of the attribute plays a larger role than any model-related variable, with t(912) = 25.47, p < .001"
- Break condition: If the pretraining dataset does not reflect human societal agreement patterns, or if the correlation between IRR and model bias is not causal but spurious.

### Mechanism 2
- Claim: Dataset scale significantly predicts facial impression bias, with larger datasets showing increased human similarity for unobservable traits.
- Mechanism: As dataset size increases, CLIP models are exposed to more diverse examples of text-image pairs, allowing them to capture increasingly subtle societal biases that are present in the data, including those for visually unobservable attributes.
- Core assumption: Larger datasets contain a more representative sample of societal biases, including subtle ones.
- Evidence anchors:
  - [abstract] "Dataset Scale is a significant predictor of facial impression bias in CLIP... Differences between models trained on LAION-2B (2.32 billion examples) and LAION-400M are mostly not significant, with the notable exception of unobservable attributes like trustworthiness (d = 1.33, p < .05) and sexuality (d = 1.14, p < .05)"
  - [section] "multiple linear regression finds that only Human IRR and Dataset Size are statistically significant predictors of model-human similarity"
- Break condition: If larger datasets do not contain proportionally more diverse societal representations, or if the relationship between scale and bias is non-linear in a way that diminishes returns.

### Mechanism 3
- Claim: CLIP models learn human-like associations between facial impression biases, with hierarchical clustering revealing similar groupings to human data.
- Mechanism: The multimodal embedding space learned by CLIP captures not just individual attribute associations but also the relationships between different attributes, mirroring the structure of human facial impression biases.
- Core assumption: The contrastive pretraining objective captures both individual attribute representations and their interrelationships in the embedding space.
- Evidence anchors:
  - [abstract] "CLIP models learn human-like associations between facial impression biases. Hierarchical clustering of CLIP and OMI attribute correlation matrices reveals similar groupings of traits"
  - [section] "computing the normalized Frobenius inner product of CLIP correlation matrices with the OMI matrix reveals increasing similarity as pretraining data size increases"
- Break condition: If the embedding space does not preserve the correlation structure of attributes, or if the hierarchical clustering method is not appropriate for comparing model and human data structures.

## Foundational Learning

- Concept: Inter-rater reliability (IRR)
  - Why needed here: IRR measures how consistently human raters agree on attribute judgments, which is crucial for understanding which biases CLIP models learn.
  - Quick check question: If two raters consistently disagree on whether someone is trustworthy, what would the IRR for trustworthiness be?

- Concept: Spearman's rho correlation
  - Why needed here: Spearman's rho is used to measure the monotonic relationship between CLIP model associations and human ratings.
  - Quick check question: If higher CLIP similarity scores always correspond to higher human ratings, what sign would Spearman's rho have?

- Concept: Hierarchical clustering
  - Why needed here: Hierarchical clustering is used to compare the structure of facial impression biases in CLIP models to those in human data.
  - Quick check question: If trustworthiness and attractiveness are often rated similarly by humans, would you expect them to cluster together in a hierarchical clustering?

## Architecture Onboarding

- Component map: CLIP model (image encoder + text encoder + projection head) -> OMI dataset (1,004 images with 34 attribute ratings) -> Cosine similarity calculations -> Model-human similarity metrics
- Critical path: Load OMI images -> Compute CLIP embeddings for images and text prompts -> Calculate cosine similarities -> Compare to human ratings -> Analyze correlations
- Design tradeoffs: Using a large, diverse OMI dataset provides robust human bias data but may introduce noise; using multiple CLIP model families allows for scale analysis but increases computational complexity.
- Failure signatures: Low model-human similarity scores indicate the model is not learning facial impression biases; inconsistent results across model families suggest methodological issues.
- First 3 experiments:
  1. Compute model-human similarity for a single attribute (e.g., "happy") across all CLIP models to validate the basic methodology.
  2. Analyze the correlation between IRR and model-human similarity for a subset of attributes to test the main hypothesis.
  3. Compare the hierarchical clustering of attribute correlations between a small and large dataset CLIP model to examine scale effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does dataset scale affect the emergence of facial impression biases for attributes beyond trustworthiness and sexuality in CLIP models?
- Basis in paper: [explicit] The paper finds that LAION-2B models show significant increases in human similarity for trustworthiness (d = 1.33) and sexuality (d = 1.14) compared to LAION-400M, but notes that most other attributes do not show significant differences between these dataset sizes.
- Why unresolved: The paper only explicitly analyzes trustworthiness and sexuality as observable attributes that show significant emergence at the largest dataset scale. The authors note that most other attributes do not show significant differences between LAION-400M and LAION-2B, but don't systematically investigate whether other subtle biases might emerge at larger scales.
- What evidence would resolve it: A systematic comparison of model-human similarity across multiple dataset sizes for a broader range of facial impression attributes, particularly focusing on attributes with lower inter-rater reliability.

### Open Question 2
- Question: How do facial impression biases in CLIP models compare to those in other vision-language models trained on different datasets or architectures?
- Basis in paper: [inferred] The paper only examines CLIP models and Stable Diffusion models that use CLIP as a text encoder. It doesn't compare CLIP's facial impression biases to those of other vision-language architectures or models trained on different datasets.
- Why unresolved: The paper focuses exclusively on CLIP and its variants, leaving open the question of whether the observed biases are specific to CLIP's architecture and training data, or whether they would be present in other vision-language models.
- What evidence would resolve it: Comparative studies of facial impression biases across different vision-language model families (e.g., Flamingo, BLIP, ALIGN) and different training datasets.

### Open Question 3
- Question: What is the causal relationship between dataset characteristics (e.g., language distribution, image quality) and the emergence of facial impression biases in CLIP models?
- Basis in paper: [inferred] The paper establishes that dataset scale predicts facial impression bias, but doesn't investigate which specific characteristics of larger datasets contribute to this effect. The authors note that larger datasets may better approximate societal perceptions, but don't examine what aspects of the data drive this.
- Why unresolved: While the paper demonstrates correlation between dataset size and bias emergence, it doesn't isolate which dataset characteristics (e.g., diversity of captions, quality of images, language patterns) are causally responsible for learning facial impression biases.
- What evidence would resolve it: Controlled experiments varying specific dataset characteristics (e.g., caption diversity, image quality thresholds, language distributions) while holding dataset size constant to identify which factors most strongly predict facial impression bias emergence.

## Limitations

- Analysis limited to 43 CLIP models and three text-to-image generators, potentially missing biases in other multimodal architectures
- OMI dataset primarily consists of American raters and images, which may not capture global diversity in facial impressions
- The study establishes correlation between dataset scale and bias emergence but doesn't isolate which specific dataset characteristics drive this relationship

## Confidence

- High confidence: Core claims about CLIP models learning societal facial impression biases, particularly regarding the strong correlation between model bias and human inter-rater reliability (ρ = .72-.76)
- Medium confidence: Dataset scale effects and hierarchical clustering results showing similar groupings between CLIP and human data
- Low confidence: Generalizability to other model architectures and cultural contexts beyond the American-focused dataset

## Next Checks

1. Replicate the analysis using a more diverse global dataset to assess whether findings generalize across cultures
2. Test additional multimodal models beyond CLIP (e.g., Flamingo, BLIP) to determine if findings are CLIP-specific or generalizable to the architecture
3. Conduct a controlled experiment varying dataset composition (rather than just scale) to isolate which types of web-scraped content most strongly influence bias learning