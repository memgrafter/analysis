---
ver: rpa2
title: 'Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language
  Models in Blocks World'
arxiv_id: '2404.00246'
source_url: https://arxiv.org/abs/2404.00246
tags:
- block
- agents
- agent
- tasks
- blocks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the collaborative capabilities of large language
  models (LLMs) in a blocks-world environment called COBLOCK, where two agents with
  complementary goals and skills work together to build a target structure. The authors
  design three increasingly challenging task types: independent tasks requiring minimal
  coordination, skill-dependent tasks where one agent needs the other''s resources,
  and goal-dependent tasks where agents'' goals depend on each other''s completion.'
---

# Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World

## Quick Facts
- arXiv ID: 2404.00246
- Source URL: https://arxiv.org/abs/2404.00246
- Reference count: 40
- Primary result: LLM agents improve collaboration with partner-state modeling and self-reflection in blocks-world tasks

## Executive Summary
This paper evaluates the collaborative capabilities of large language models in a blocks-world environment called COBLOCK, where two agents with complementary goals and skills work together to build target structures. The authors design three increasingly challenging task types and implement chain-of-thought prompting with partner-state modeling and self-reflection mechanisms to improve collaboration. Results show that while LLM agents have strong grounding abilities in blocks-world tasks, they struggle with collaboration without these enhancements. Adding partner-state modeling and self-reflection significantly improves task success rates and workload balance between agents.

## Method Summary
The study uses a blocks-world environment called COBLOCK where two agents collaborate to build target structures. Three task types are designed with increasing coordination requirements: independent tasks (minimal coordination), skill-dependent tasks (requiring resource sharing), and goal-dependent tasks (requiring sequential completion). LLM agents are prompted using chain-of-thought prompts with four reasoning steps: understanding world state, modeling partner state, reflecting on past actions, and predicting next actions. The evaluation compares GPT-3.5 and GPT-4 performance in both machine-machine and human-machine collaboration settings using task success rate, total steps, and workload balance as metrics.

## Key Results
- LLM agents have strong grounding abilities but struggle with collaboration without enhancement mechanisms
- Partner-state modeling and self-reflection significantly improve task success rates and workload balance
- Human-machine collaboration shows slightly higher success rates than machine-machine collaboration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM agents have strong grounding capacities in blocks world tasks.
- Mechanism: LLMs can parse XML structure descriptions and generate accurate textual descriptions and action sequences for single-agent tasks.
- Core assumption: The XML structure representation is sufficient for LLMs to understand the task without multimodal training.
- Evidence anchors:
  - [abstract] "LLM agents have strong grounding capacities"
  - [section] "According to Figure 5, both GPT-4 and GPT-3.5 agents successfully complete almost all three tasks"
- Break condition: If XML structure becomes too complex or abstract for LLMs to parse accurately.

### Mechanism 2
- Claim: Partner-state modeling and self-reflection improve collaborative performance.
- Mechanism: By modeling partner's state and reflecting on past actions, agents can better coordinate and correct errors in real-time.
- Core assumption: Intermediate reasoning steps help LLMs make better decisions in collaborative contexts.
- Evidence anchors:
  - [abstract] "Adding partner-state modeling and self-reflection significantly improves task success rates"
  - [section] "Both partner-state modeling and reflection enhance collaboration"
- Break condition: If partner-state modeling becomes too complex or self-reflection leads to over-correction.

### Mechanism 3
- Claim: Human-machine collaboration shows slightly higher success rates than machine-machine collaboration.
- Mechanism: Humans can identify and correct LLM errors, but may take on more responsibility when LLMs struggle.
- Core assumption: Humans have superior problem-solving abilities compared to LLMs in collaborative tasks.
- Evidence anchors:
  - [abstract] "human-machine collaboration has a slightly higher success rate than machine-machine collaboration"
  - [section] "Humans are smarter, but sometimes less collaborative"
- Break condition: If human participants become uncooperative or if LLM capabilities improve significantly.

## Foundational Learning

- Concept: XML structure representation
  - Why needed here: Provides a standardized way to represent blocks world structures for LLM parsing
  - Quick check question: Can you convert a textual description of a blocks structure into XML format?
- Concept: Chain-of-thought prompting
  - Why needed here: Enables LLMs to break down complex tasks into intermediate reasoning steps
  - Quick check question: What are the four steps in the chain-of-thought prompting used in this paper?
- Concept: Workload balance metrics
  - Why needed here: Quantifies how evenly tasks are distributed between agents in collaborative settings
  - Quick check question: How is the workload balance γ calculated in this paper?

## Architecture Onboarding

- Component map: LLM agent → Chain-of-thought prompting → Partner-state modeling → Self-reflection → Action selection → COBLOCK environment
- Critical path: Input (world state, goal) → Chain-of-thought reasoning → Output (action)
- Design tradeoffs: Complexity of chain-of-thought prompts vs. performance improvement; number of in-context examples vs. prompt size
- Failure signatures: Low task success rate; high workload imbalance; excessive communication steps
- First 3 experiments:
  1. Single-agent task generation (textual descriptions from XML)
  2. Single-agent task planning (action sequences from XML)
  3. Multi-agent collaboration task (human-machine or machine-machine)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multi-agent collaboration capabilities scale beyond two agents?
- Basis in paper: [explicit] The paper acknowledges this as a primary limitation, stating they focus on a two-agent setting which "may not fully capture the dynamics of multi-agent collaboration."
- Why unresolved: The current study was intentionally designed as a controlled two-agent environment to establish baseline collaboration metrics. Scaling to more agents would introduce additional complexity in communication patterns, coordination overhead, and potential conflicts.
- What evidence would resolve it: An expanded platform supporting 3+ agents with controlled experiments measuring success rates, communication efficiency, and task completion times across different group sizes.

### Open Question 2
- Question: How can partner-state modeling be generalized to domains beyond the blocks world?
- Basis in paper: [explicit] The paper notes their agent-state modeling "mainly focuses on information specific to the blocks world" and would need reconfiguration for other domains.
- Why unresolved: The current partner-state modeling is tailored to block structures, inventories, and 3D positioning. Real-world applications would require different state representations and modeling approaches.
- What evidence would resolve it: Implementation and evaluation of the partner-state modeling framework in different domains (e.g., collaborative writing, software development, or medical diagnosis) with domain-specific state representations.

### Open Question 3
- Question: How does memory impact long-term multi-agent collaboration?
- Basis in paper: [inferred] The paper mentions that "our design overlooks the aspect of memory, which is crucial for long-term collaboration" but doesn't explore this dimension.
- Why unresolved: The current experiments are limited to single-session tasks where memory between sessions isn't relevant. Real-world collaboration often involves ongoing relationships and shared context.
- What evidence would resolve it: Comparative studies of collaboration performance with and without memory mechanisms across multiple sessions, measuring how historical interactions affect future collaboration efficiency and success.

## Limitations

- The study focuses exclusively on a blocks-world environment, limiting generalizability to more complex real-world collaborative scenarios
- The partner-state modeling assumes perfect observability of partner states, which rarely occurs in practical collaborative settings with information asymmetry
- The human-machine collaboration evaluation involved only four participants, limiting statistical power and diversity of human collaboration styles

## Confidence

**High confidence**: The core finding that LLMs struggle with collaboration in blocks-world tasks without enhancement mechanisms is well-supported by the experimental results. The improvements from partner-state modeling and self-reflection are clearly demonstrated through task success rates and workload balance metrics.

**Medium confidence**: The claim that XML structure representation is sufficient for LLMs to understand blocks-world tasks is supported by the strong grounding performance, but this may not extend to more complex representations or domains requiring richer perceptual understanding.

**Medium confidence**: The finding that human-machine collaboration outperforms machine-machine collaboration is supported by the data, but the small sample size of human participants and potential for participants to overcompensate for LLM limitations reduces confidence in the generalizability of this result.

## Next Checks

1. **Generalization to alternative domains**: Replicate the collaboration evaluation framework using different task domains (e.g., text-based collaborative games, visual block manipulation, or sequential decision-making tasks) to test whether partner-state modeling and self-reflection improvements transfer beyond the blocks-world environment.

2. **Information asymmetry testing**: Modify the COBLOCK environment to include partial observability where agents cannot perfectly observe partner states, then evaluate whether the partner-state modeling mechanism still provides benefits or whether it introduces errors due to incorrect assumptions.

3. **Human collaborator diversity study**: Expand the human-machine collaboration evaluation to include 20+ diverse participants with varying backgrounds and collaboration styles, measuring both objective task performance and subjective measures of collaboration quality to better understand the factors that influence human-LLM teaming success.