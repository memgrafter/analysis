---
ver: rpa2
title: 'AGENT-CQ: Automatic Generation and Evaluation of Clarifying Questions for
  Conversational Search with LLMs'
arxiv_id: '2410.19692'
source_url: https://arxiv.org/abs/2410.19692
tags:
- questions
- question
- clarifying
- human
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AGENT-CQ introduces an end-to-end LLM-based framework for generating
  and evaluating clarifying questions in conversational search. It addresses scalability
  challenges of manual curation and rigidity of template-based approaches through
  two stages: automatic question generation and multi-dimensional evaluation.'
---

# AGENT-CQ: Automatic Generation and Evaluation of Clarifying Questions for Conversational Search with LLMs

## Quick Facts
- arXiv ID: 2410.19692
- Source URL: https://arxiv.org/abs/2410.19692
- Reference count: 40
- Key outcome: AGENT-CQ introduces an LLM-based framework for automatic generation and evaluation of clarifying questions in conversational search, showing significant improvements over human-created questions in clarity, usefulness, and retrieval effectiveness

## Executive Summary
AGENT-CQ addresses the challenge of creating clarifying questions for conversational search by introducing an end-to-end framework that automates both generation and evaluation. The framework tackles the scalability limitations of manual curation and the rigidity of template-based approaches by leveraging large language models in two stages: automatic question generation and multi-dimensional evaluation. Experiments on the ClariQ dataset demonstrate that AGENT-CQ-generated questions significantly outperform human-created ones in clarity, usefulness, and retrieval effectiveness, with NDCG@1 scores reaching 0.225 for BM25 and 0.312 for BERT.

## Method Summary
AGENT-CQ operates through two complementary stages. The generation stage uses temperature-based prompting strategies to create diverse clarifying questions from search queries, exploring different aspects through facet-based prompting. The evaluation stage (CrowdLLM) employs multiple LLM instances to simulate crowdsourced judgments across seven quality metrics, achieving strong inter-rater agreement and correlation with human assessments. The framework evaluates generated questions against human-created ones and incorporates them into conversational search systems, where they demonstrate superior performance in enhancing retrieval outcomes.

## Key Results
- Generated questions significantly outperform human-created ones in clarity, usefulness, and retrieval effectiveness
- GPT-Temp achieved NDCG@1 scores of 0.225 for BM25 and 0.312 for BERT, surpassing human-generated questions
- CrowdLLM achieved strong inter-rater agreement (ICC up to 0.97) and correlated well with human assessments

## Why This Works (Mechanism)
AGENT-CQ works by addressing the dual challenges of diversity and quality in clarifying questions. The temperature-based generation strategy creates varied questions that explore different facets of ambiguous queries, while the multi-LLM evaluation system provides comprehensive quality assessment across multiple dimensions. This combination allows the framework to generate questions that are both diverse and high-quality, leading to improved conversational search performance compared to human-created questions or simpler automated approaches.

## Foundational Learning

**Temperature-based prompting** - Controls randomness in LLM outputs to generate diverse responses
Why needed: Creates varied clarifying questions that explore different aspects of ambiguous queries
Quick check: Vary temperature values and observe diversity metrics in generated questions

**Multi-dimensional evaluation** - Assesses questions across multiple quality metrics simultaneously
Why needed: Ensures generated questions meet various criteria for effective conversational search
Quick check: Verify correlation between individual metric scores and overall question quality

**Facet-based prompting** - Directs LLMs to generate questions focusing on specific aspects of the query
Why needed: Ensures comprehensive coverage of potential user intents and information needs
Quick check: Compare coverage of generated questions against query facets identified by human annotators

## Architecture Onboarding

**Component map:** Query -> Generation LLM (with temperature variation) -> Generated Questions -> Evaluation LLM (CrowdLLM) -> Quality Scores -> Retrieval System

**Critical path:** Query → Generation (temperature-based) → Evaluation (multi-metric) → Retrieval effectiveness

**Design tradeoffs:** The framework trades computational cost of multiple LLM evaluations for improved question quality and diversity, versus simpler template-based approaches that are faster but less effective

**Failure signatures:** 
- Low diversity in generated questions (indicates insufficient temperature variation)
- Poor correlation between LLM evaluation and human judgments (suggests evaluation metric misalignment)
- Minimal improvement in retrieval effectiveness (indicates generated questions aren't capturing user intent)

**First experiments to run:**
1. Generate questions at different temperature settings and measure diversity metrics
2. Compare CrowdLLM evaluation scores against human judgments on the same question pairs
3. Test retrieval effectiveness of generated questions against human-created questions in a controlled search environment

## Open Questions the Paper Calls Out

The paper acknowledges that the strong performance of LLM-based evaluation relies on the assumption that multiple LLM assessors can accurately simulate human crowdsourced evaluations, though potential biases in LLM-generated judgments may not fully capture human reasoning nuances. The optimal temperature values appear dataset-specific and may not generalize across different conversational search contexts without recalibration. The evaluation framework's dependence on existing relevance datasets like MS MARCO may limit applicability to domains with different information needs or query distributions.

## Limitations

- The temperature-based generation strategy's optimal values appear dataset-specific and may not generalize across different conversational search contexts without recalibration
- The framework's dependence on existing relevance datasets like MS MARCO may limit applicability to domains with different information needs or query distributions
- Potential biases in LLM-generated judgments may not fully capture human reasoning nuances, despite strong correlation with human assessments

## Confidence

- High confidence: Retrieval effectiveness improvements using generated questions over human-created ones
- Medium confidence: Strong performance of LLM-based evaluation with high inter-rater agreement and correlation with human judgments
- Medium confidence: Effectiveness of temperature-based diversity strategy, though optimal values may be dataset-specific

## Next Checks

1. Conduct cross-dataset validation to assess whether the temperature-based generation strategy and evaluation framework generalize beyond the ClariQ dataset to diverse conversational search scenarios

2. Perform ablation studies comparing CrowdLLM against human crowdworkers on identical question pairs to quantify potential systematic biases in LLM-based evaluation

3. Test the framework's robustness with domain-specific knowledge bases and specialized corpora to evaluate performance outside general web search contexts