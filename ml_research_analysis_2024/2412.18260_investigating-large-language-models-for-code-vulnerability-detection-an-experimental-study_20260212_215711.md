---
ver: rpa2
title: 'Investigating Large Language Models for Code Vulnerability Detection: An Experimental
  Study'
arxiv_id: '2412.18260'
source_url: https://arxiv.org/abs/2412.18260
tags:
- uni00000013
- code
- uni00000018
- uni00000008
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the effectiveness of fine-tuning large\
  \ language models (LLMs) for code vulnerability detection (CVD). It systematically\
  \ evaluates four widely-used open-source LLMs\u2014Llama-2, CodeLlama, Llama-3,\
  \ and Llama-3.1\u2014against three graph-based models and two medium-size sequence\
  \ models across five CVD datasets."
---

# Investigating Large Language Models for Code Vulnerability Detection: An Experimental Study

## Quick Facts
- arXiv ID: 2412.18260
- Source URL: https://arxiv.org/abs/2412.18260
- Reference count: 40
- Large language models outperform other models on long code samples for vulnerability detection

## Executive Summary
This paper investigates the effectiveness of fine-tuning large language models (LLMs) for code vulnerability detection (CVD). The study systematically evaluates four widely-used open-source LLMs—Llama-2, CodeLlama, Llama-3, and Llama-3.1—against three graph-based models and two medium-size sequence models across five CVD datasets. Experiments reveal that LLMs perform exceptionally well on long code samples, significantly outperforming other models in this aspect. However, all models are highly sensitive to class imbalance in datasets, with better performance observed on balanced datasets.

## Method Summary
The study fine-tunes four LLMs (Llama-2, CodeLlama, Llama-3, and Llama-3.1) using LoRA with default hyperparameters on five CVD datasets. The datasets are preprocessed by filtering anomalies, formatting samples, and dividing by sequence length (short <512 tokens, long 512-1024 tokens). Models are evaluated using standard classification metrics including accuracy, precision, recall, F1-score, and FPR. The evaluation compares LLM performance against three graph-based models and two medium-size sequence models to assess effectiveness across different code sample lengths and dataset characteristics.

## Key Results
- LLMs significantly outperform other models on long code samples (≥512 tokens) for vulnerability detection
- Class imbalance severely degrades model performance, with better results on balanced datasets
- Positive sample ratio has greater impact on model performance than sample length

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning LLMs on CVD datasets yields strong performance on long code samples compared to graph-based and medium-size sequence models. LLMs possess larger parameter space and longer context windows, enabling them to capture long-distance token associations and complex vulnerability patterns that smaller models struggle with. Core assumption: Long code samples inherently contain more vulnerability-relevant context than short ones.

### Mechanism 2
Class imbalance in CVD datasets severely degrades model performance, especially for LLMs. When the proportion of positive (vulnerable) samples is low, the model is less exposed to vulnerability patterns during training, leading to poor recall and F1-score. Core assumption: Models learn vulnerability detection by generalizing from positive examples.

### Mechanism 3
Positive sample ratio has a greater impact on fine-tuning LLMs than sample length. The frequency of vulnerability patterns during training is more influential on model learning than the average length of code samples. Core assumption: Models can learn vulnerability patterns regardless of context length if given sufficient exposure.

## Foundational Learning

- **Concept**: Class imbalance and its effects on classification metrics
  - Why needed here: The study emphasizes that imbalanced datasets lead to misleadingly high accuracy but poor F1-score and recall
  - Quick check question: If a dataset has 95% negative samples and a model always predicts negative, what is the accuracy? What are the precision, recall, and F1-score?

- **Concept**: Tokenization and sequence length limitations in pre-trained models
  - Why needed here: The study divides datasets based on 512-token boundary due to model constraints, and investigates how sample length affects performance
  - Quick check question: What happens to a 600-token code sample when fed into CodeBERT which has a 512-token limit? How might this truncation affect vulnerability detection?

- **Concept**: Fine-tuning vs. instruction tuning vs. zero-shot prompting for LLMs
  - Why needed here: The study compares fine-tuning LLMs with other approaches mentioned in related works, highlighting the effectiveness of fine-tuning
  - Quick check question: What is the key difference between fine-tuning an LLM and using it in a zero-shot prompting setting? How might this difference affect performance on specialized tasks like CVD?

## Architecture Onboarding

- **Component map**: Data preprocessing pipeline -> Model zoo -> Training framework -> Evaluation suite
- **Critical path**: 1) Preprocess datasets (format, split by length, subsample if needed) 2) Convert code to appropriate input format 3) Fine-tune or train models on training set 4) Evaluate on test set using defined metrics 5) Analyze results focusing on class imbalance and sample length effects
- **Design tradeoffs**: Using LoRA for fine-tuning reduces computational cost but may limit adaptation compared to full fine-tuning; subsampling balances datasets but reduces overall sample size and may introduce bias
- **Failure signatures**: All metrics except accuracy are zero (indicating the model is predicting only the majority class); extremely low recall despite moderate precision (suggesting the model misses most positive samples)
- **First 3 experiments**: 1) Run the full model evaluation pipeline on the Devign dataset (balanced) to establish a performance baseline 2) Evaluate the same models on the DiverseVul dataset (highly imbalanced) to observe class imbalance effects 3) Compare LLM performance on short vs. long subsets of the same dataset to isolate length effects

## Open Questions the Paper Calls Out

### Open Question 1
How does class imbalance in CVD datasets affect model performance, and what strategies can mitigate its impact? The paper demonstrates that all models perform significantly better on the balanced Devign dataset compared to imbalanced datasets like DiverseVul, with performance improving when positive sample ratios reach 30%. Unresolved because the paper only suggests using more balanced datasets but does not explore specific techniques like oversampling, undersampling, or data augmentation.

### Open Question 2
What is the impact of sample length on the performance of fine-tuned LLMs for CVD, and how can models be optimized for long code sequences? The paper shows that sample length influences F1-score, with longer samples generally leading to lower scores, though less significantly than positive sample ratio. Unresolved because the study does not investigate architectural or training modifications to enhance LLM performance on long code sequences.

### Open Question 3
How do different fine-tuning techniques (e.g., LoRA, QLoRA, full fine-tuning) affect the performance and efficiency of LLMs for CVD? The paper uses LoRA for fine-tuning LLMs and analyzes its hyperparameters but does not compare it to other methods like QLoRA or full fine-tuning. Unresolved because the paper focuses on LoRA without exploring its trade-offs against other parameter-efficient fine-tuning methods or full fine-tuning.

## Limitations

- Narrow domain scope limits generalizability to enterprise codebases and different programming paradigms
- Evaluation lacks cost-sensitive analysis of false positives vs false negatives in real-world deployment
- Uses LoRA with default hyperparameters without exploring full parameter space or comparing to full fine-tuning

## Confidence

**High Confidence**: LLMs' superior performance on long code samples (≥512 tokens) is well-supported by systematic experiments across multiple datasets and model architectures; the negative impact of class imbalance on model performance is consistently observed; the finding that positive sample ratio has greater impact than sample length is reproducible

**Medium Confidence**: The specific threshold of 30% positive sample ratio for performance gains is based on observed patterns but may not generalize to all CVD scenarios; the comparative advantage of LLMs over graph-based models may be influenced by implementation choices in the baseline models

**Low Confidence**: The assertion that LLMs will continue to scale favorably with even longer code samples (beyond 1024 tokens) lacks empirical validation; the study does not explore whether different vulnerability types respond differently to model architectures or sample characteristics

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate the fine-tuned LLMs on code samples from enterprise systems, proprietary codebases, or different programming paradigms (functional, logic, or domain-specific languages) to assess whether the observed advantages transfer beyond GitHub-style imperative code.

2. **Cost-Benefit Analysis Implementation**: Develop a framework that weighs the security value of true positive detections against the operational cost of false positives, using real developer feedback on investigation time and patch implementation costs to validate whether high recall is always preferable.

3. **Parameter Sensitivity Study**: Conduct a systematic hyperparameter sweep for LoRA fine-tuning (rank, scaling factor, learning rate) and compare against full fine-tuning approaches to determine whether the observed LLM advantages are robust to training methodology choices or represent optimization artifacts.