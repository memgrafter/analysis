---
ver: rpa2
title: Contrastive Representation Learning for Dynamic Link Prediction in Temporal
  Networks
arxiv_id: '2408.12753'
source_url: https://arxiv.org/abs/2408.12753
tags:
- graph
- temporal
- time
- learning
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning expressive representations
  for temporal networks that encode both structural connectivity and temporal evolution.
  The authors propose a self-supervised method called teneNCE, which uses a recurrent
  message-passing neural network architecture to model information flow over time-respecting
  paths in temporal networks.
---

# Contrastive Representation Learning for Dynamic Link Prediction in Temporal Networks

## Quick Facts
- arXiv ID: 2408.12753
- Source URL: https://arxiv.org/abs/2408.12753
- Reference count: 40
- Primary result: Achieves 4% average improvement in dynamic link prediction accuracy over state-of-the-art models

## Executive Summary
This paper proposes teneNCE, a self-supervised method for learning expressive representations in temporal networks that encode both structural connectivity and temporal evolution. The method uses a recurrent message-passing neural network architecture combined with a contrastive training objective that integrates link prediction, graph reconstruction, and contrastive predictive coding (CPC) losses. The CPC component employs infoNCE losses at both local and global scales to capture long-range temporal dependencies. The proposed approach is evaluated on Enron, COLAB, and Facebook datasets, demonstrating superior performance in dynamic link prediction tasks compared to existing models.

## Method Summary
TeneNCE learns temporal network representations through a multi-task contrastive learning framework. The method discretizes temporal networks into static graph snapshots and processes them through an encoder (3-layer GCN) and a recurrent Update module (GGRU) that models information flow over time-respecting paths. The training objective combines three loss terms: a link prediction loss, a graph reconstruction loss, and a CPC loss implemented using infoNCE at local (node-level) and global (graph-level) scales. This combination encourages the model to capture both immediate structural patterns and longer-term temporal dependencies, resulting in improved performance for dynamic link prediction tasks.

## Key Results
- Achieves an average improvement of 4% in link prediction accuracy compared to state-of-the-art models
- Demonstrates effectiveness across multiple datasets (Enron, COLAB, Facebook) with consistent performance gains
- Ablation study confirms the contribution of each loss term, with CPC loss showing particular importance for capturing temporal dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The infoNCE loss at both local and global scales helps the model capture long-range temporal dependencies beyond immediate next-step prediction.
- Mechanism: The infoNCE loss contrasts the similarity of node (or graph) representations with their future states against negative samples from different nodes and time steps. This encourages the learned representations to align with future structural patterns, effectively regularizing the model to extract features spanning multiple time steps.
- Core assumption: Maximizing mutual information between current representations and future states via infoNCE leads to better generalization for future predictions.
- Evidence anchors:
  - [abstract] "The CPC objective is implemented using infoNCE losses at both local and global scales of the input graphs."
  - [section 3.3.5] "We employ the contrastive predictive coding (CPC) framework [40], which maximizes the mutual information between the context sequence and future features in a contrastive way."
  - [corpus] Weak - corpus neighbors discuss contrastive methods but do not specifically anchor to CPC/infoNCE in temporal networks.
- Break condition: If negative sampling is too noisy or the time horizon is too long, the mutual information estimation becomes unreliable, leading to degraded representations.

### Mechanism 2
- Claim: The recurrent message-passing architecture models information flow over time-respecting paths, capturing the true dynamics of temporal networks.
- Mechanism: The Update function, implemented as a GGRU, propagates node state information through the graph structure at each time step, allowing each node to aggregate information from its neighbors while maintaining temporal coherence through the recurrent state.
- Core assumption: Message passing over time-respecting paths captures the essential dynamics of temporal networks better than static graph approaches.
- Evidence anchors:
  - [section 3.3.3] "The Update function is responsible for capturing the information flow over time-respecting paths in the temporal graph."
  - [section 4.1] Experimental results show improved performance over static graph methods.
- Break condition: If the temporal discretization is too coarse or the message passing depth is insufficient, important temporal patterns may be lost or smoothed out.

## Foundational Learning

### Temporal Networks
- Why needed: The paper operates on temporal networks where edges appear and disappear over time, requiring specialized modeling approaches
- Quick check: Verify that the discretization process preserves key temporal patterns by examining temporal correlation coefficients

### Contrastive Predictive Coding (CPC)
- Why needed: CPC provides a framework for learning representations that capture future states through contrastive objectives
- Quick check: Confirm that the infoNCE loss is properly implemented with appropriate negative sampling strategies

### InfoNCE Loss
- Why needed: The specific contrastive objective used to maximize mutual information between current and future representations
- Quick check: Monitor the infoNCE loss during training to ensure it decreases appropriately while maintaining diversity in negative samples

## Architecture Onboarding

### Component Map
- Input Data -> Temporal Discretization -> Encoder (GCN) -> Update (GGRU) -> Decoder/LinkPredictor -> CPC Components (Local/Global) -> Combined Loss -> Model Output

### Critical Path
The critical computational path is: Input Snapshots → Encoder → Update (GGRU) → Decoder/LinkPredictor → Loss Computation → Parameter Updates

### Design Tradeoffs
- Discrete-time snapshots simplify processing but may lose fine-grained temporal information
- Combining multiple loss objectives improves representation quality but increases training complexity
- Using GNN-based encoders leverages structural information but may struggle with very large graphs

### Failure Signatures
- CPC loss not decreasing: May indicate poor negative sampling or overly long prediction horizons
- Reconstruction loss dominating: Suggests insufficient weight on predictive objectives
- Degraded performance on certain datasets: May indicate sensitivity to specific temporal network characteristics

### First Experiments
1. Train with only link prediction loss to establish baseline performance
2. Add reconstruction loss to evaluate its contribution to overall performance
3. Enable CPC loss and observe improvements in capturing temporal dependencies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the discrete-time snapshot approach compare to continuous-time modeling in terms of information loss and computational efficiency across different types of temporal networks?
- Basis in paper: The paper explicitly discusses this trade-off, stating "Despite losing some information during the discretization process, this approach allows us to handle a greater number of interactions in a single call to the representation model."
- Why unresolved: The paper only mentions the trade-off in general terms without providing quantitative comparisons or specific thresholds where one approach becomes preferable over the other.
- What evidence would resolve it: Empirical studies comparing both approaches on networks with varying densities and interaction frequencies, with metrics measuring both information retention and computational costs.

### Open Question 2
- Question: What is the optimal balance between local and global contrastive predictive coding losses for different types of temporal network dynamics?
- Basis in paper: The paper uses both local and global infoNCE losses but states "The findings highlight the effectiveness of teneNCE in modeling the historical data and generalizing to unseen new links, relative to other methods" without exploring how different weightings affect performance across network types.
- Why unresolved: The paper uses fixed hyperparameters (α and β) for all datasets without exploring how these should be tuned for different network characteristics.
- What evidence would resolve it: Systematic experiments varying the weights of local and global CPC losses across networks with different temporal correlation structures and structural hierarchies.

### Open Question 3
- Question: How does the proposed recurrent message-passing architecture scale to very large temporal networks with millions of nodes and edges?
- Basis in paper: The paper mentions "to execute this architecture on large-scale graphs by introducing a parallel computational framework" when discussing related work but does not evaluate their own method on large-scale datasets.
- Why unresolved: The experiments are limited to relatively small datasets (Enron, COLAB, Facebook) without testing scalability to larger networks.
- What evidence would resolve it: Performance benchmarks on large-scale temporal networks, including training time, memory usage, and prediction accuracy as a function of network size.

## Limitations
- The ablation study results show that removing CPC loss degrades performance, but the relative contribution of local vs. global infoNCE components is not separately analyzed
- The paper reports improvements over specific baselines but doesn't explore the full space of temporal network representation learning methods
- The Facebook dataset results show less consistent improvement compared to Enron and COLAB, suggesting potential dataset-specific limitations

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| General effectiveness of combining contrastive objectives with recurrent message passing for temporal link prediction | High confidence |
| Specific claim of 4% average improvement | Medium confidence |
| Mechanism that infoNCE losses capture long-range temporal dependencies | Medium confidence |

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary α and β weights across a broader range to understand their impact on final performance and stability

2. **Ablation of CPC components**: Separate the local and global infoNCE losses in the ablation study to quantify their individual contributions

3. **Temporal horizon evaluation**: Test the model's performance on longer time horizons beyond immediate next-step prediction to validate the claim about capturing long-range dependencies