---
ver: rpa2
title: Conversational Query Reformulation with the Guidance of Retrieved Documents
arxiv_id: '2407.12363'
source_url: https://arxiv.org/abs/2407.12363
tags:
- query
- documents
- guidecqr
- keywords
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GuideCQR improves conversational search by refining queries using
  key information from initially retrieved documents. The method first retrieves documents
  with a baseline query, then extracts keywords and generates expected answers from
  these documents.
---

# Conversational Query Reformulation with the Guidance of Retrieved Documents

## Quick Facts
- arXiv ID: 2407.12363
- Source URL: https://arxiv.org/abs/2407.12363
- Reference count: 34
- GuideCQR improves conversational search by refining queries using key information from initially retrieved documents

## Executive Summary
GuideCQR is a conversational query reformulation method that improves retrieval performance by leveraging information from initially retrieved documents. The framework extracts keywords and generates expected answers from these documents, then filters and combines them with the original query to create more retriever-friendly queries. GuideCQR achieves state-of-the-art performance across multiple datasets, demonstrating that retrieval signals from initially retrieved documents can effectively guide query reformulation.

## Method Summary
GuideCQR reformulates conversational queries by first retrieving documents using a baseline query, then extracting keywords and generating expected answers from these documents. The method applies a filtering stage based on cosine similarity thresholds to remove irrelevant information, and finally concatenates the filtered keywords and answers with the baseline query. The framework uses ANCE for initial retrieval, KeyBERT for keyword extraction, and RoBERTa-Base-Squad2 for answer generation, with dataset-specific filtering thresholds.

## Key Results
- GuideCQR achieves state-of-the-art performance across CAsT-19, CAsT-20, and QReCC datasets
- Improves retrieval performance even when applied to human-written queries or queries generated by other models
- Demonstrates effectiveness by improving MRR, NDCG@3, and Recall@10 metrics over baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GuideCQR improves retrieval by extracting keywords from initially retrieved documents that align with gold passages
- Mechanism: The framework retrieves documents using a baseline query, extracts keywords from these documents using KeyBERT, and augments the original query with these keywords to create more retriever-friendly queries
- Core assumption: Initially retrieved documents contain meaningful signals (keywords) that are relevant to the gold passage
- Evidence anchors:
  - [abstract] "GuideCQR extracts keywords and generates expected answers from the retrieved documents, then unifies them with the queries after filtering to add useful information that enhances the search process"
  - [section 3.2.1] "keywords from the initial documents play a critical role in forming retriever-friendly queries, as they capture the most relevant and significant terms within the document"
  - [corpus] Weak - corpus doesn't directly address keyword extraction mechanism
- Break condition: If initially retrieved documents are too irrelevant to the gold passage, extracted keywords would not align with what retrievers need, degrading performance

### Mechanism 2
- Claim: GuideCQR improves retrieval by generating expected answers from retrieved documents that capture user intent
- Mechanism: The framework generates concise answers from the retrieved documents using a QA model, then filters and appends these answers to the query to provide direct insights into user intent
- Core assumption: Retrieved documents contain passages that can serve as expected answers to the query, providing valuable context
- Evidence anchors:
  - [abstract] "GuideCQR extracts keywords and generates expected answers from the retrieved documents"
  - [section 3.2.2] "Guided documents often include gold answers to the query, serving as a valuable resource for efficiently reformulating the query"
  - [corpus] Weak - corpus doesn't directly address expected answer generation
- Break condition: If retrieved documents don't contain answers relevant to the query, generated answers would add noise rather than useful information

### Mechanism 3
- Claim: GuideCQR's filtering mechanism removes redundant information by measuring similarity between augmented content and both the current query and dialogue history
- Mechanism: The framework calculates a FilterScore based on cosine similarity between keywords/answers and the baseline query plus history, filtering out elements below a threshold
- Core assumption: Keywords and answers that are dissimilar to both the current query and dialogue history are likely irrelevant and should be removed
- Evidence anchors:
  - [section 3.3] "we introduce an additional filtering stage to more effectively remove irrelevant keywords and answers from the reformulated query"
  - [section 3.3] "we account for history queries through the HistoryScore, which allows us to traverse the entire dialogue and capture the global context from past to present"
  - [corpus] Weak - corpus doesn't directly address filtering mechanism
- Break condition: If the similarity threshold is set too high, useful keywords/answers might be filtered out; if too low, irrelevant content remains

## Foundational Learning

- Concept: Conversational Query Reformulation (CQR)
  - Why needed here: Understanding CQR is fundamental to grasping how GuideCQR improves conversational search by transforming context-dependent queries into standalone forms
  - Quick check question: What are the main challenges in conversational queries that CQR aims to address?

- Concept: Dense Retrieval vs Sparse Retrieval
  - Why needed here: GuideCQR uses dense retrieval (ANCE), so understanding the difference helps explain why the method focuses on query reformulation rather than traditional keyword matching
  - Quick check question: How does dense retrieval differ from sparse retrieval in terms of query representation?

- Concept: Document Reranking
  - Why needed here: GuideCQR uses a reranking step to improve the quality of initially retrieved documents, which is crucial for extracting useful signals
  - Quick check question: What is the purpose of reranking in information retrieval systems?

## Architecture Onboarding

- Component map:
  - Query Generation: LLM-based baseline query rewriting (OpenAI gpt3.5-turbo-16k)
  - Initial Retrieval: ANCE dense retriever with baseline query
  - Reranking: SentenceTransformer models for document reordering
  - Signal Extraction: KeyBERT for keywords, QA model for expected answers
  - Filtering: Cosine similarity-based filtering using multiple embedding models
  - Final Query Construction: Concatenation of baseline query with filtered keywords and answers

- Critical path: Qbaseline → Initial Retrieval → Reranking → Signal Extraction → Filtering → Qfinal

- Design tradeoffs:
  - Keyword span length vs. relevance: Longer spans capture more context but risk including irrelevant terms
  - Number of documents vs. computation: More documents provide more signals but increase processing time
  - FilterScore threshold vs. completeness: Higher thresholds reduce noise but may remove useful information

- Failure signatures:
  - Performance degradation when initial retrieval yields mostly irrelevant documents
  - Decreased performance when keyword span is too long or too many documents are used
  - Over-filtering when FilterScore threshold is set too high

- First 3 experiments:
  1. Test GuideCQR with varying keyword span lengths (5, 10, 15 tokens) on CAsT-19 to find optimal balance
  2. Compare performance using different numbers of initial guided documents (10, 100, 1000, 2000) to identify sweet spot
  3. Evaluate filtering effectiveness by comparing performance with and without the filtering stage on the validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of initial retrieved documents to use for keyword augmentation and answer generation?
- Basis in paper: [explicit] The paper mentions using 2,000 initial documents and selecting top-10 re-ranked documents, but notes that using too many documents may cause performance decline
- Why unresolved: The paper shows performance varies with document count but doesn't identify an optimal sweet spot, and the choice seems dataset-dependent
- What evidence would resolve it: Systematic experiments varying the number of initial documents (e.g., 500, 1000, 2000, 5000) across multiple datasets to identify performance plateaus or optimal ranges

### Open Question 2
- Question: How does GuideCQR's performance compare when using different dense retrieval models versus sparse retrieval models?
- Basis in paper: [explicit] The paper primarily uses ANCE dense retriever and provides some sparse retrieval results in appendix, but doesn't systematically compare different retriever types
- Why unresolved: The paper focuses on dense retrieval but doesn't explore how retrieval model choice impacts GuideCQR's effectiveness
- What evidence would resolve it: Comparative experiments using various dense (e.g., Contriever, COIL) and sparse (e.g., BM25, SPLADE) retrievers with GuideCQR to measure relative performance gains

### Open Question 3
- Question: What is the relationship between keyword span length and retrieval performance, and is there an optimal span length?
- Basis in paper: [explicit] The paper tests different span lengths (5, 10, 15, 20) but doesn't identify a clear optimal length, noting that too many signals can degrade performance
- Why unresolved: The paper shows performance varies with span length but doesn't establish why certain lengths work better or what factors determine optimal length
- What evidence would resolve it: Detailed analysis of keyword relevance distribution across span lengths and correlation studies between span length, query length, and retrieval performance across different query types

### Open Question 4
- Question: How does GuideCQR handle cases where the initially retrieved documents are largely irrelevant to the query?
- Basis in paper: [explicit] The paper mentions filtering based on similarity scores and discusses failure cases, but doesn't systematically analyze performance degradation in low-relevance scenarios
- Why unresolved: The paper shows GuideCQR can fail when irrelevant keywords are introduced, but doesn't explore the robustness threshold or recovery mechanisms
- What evidence would resolve it: Controlled experiments where initial retrieval relevance is artificially manipulated to study performance degradation patterns and identify failure thresholds

### Open Question 5
- Question: What is the computational trade-off between GuideCQR's query reformulation time and its performance gains?
- Basis in paper: [explicit] The paper mentions GuideCQR takes more time than conventional methods, particularly for keyword augmentation and answer generation, but doesn't quantify this trade-off
- Why unresolved: The paper acknowledges time inefficiency but doesn't provide timing measurements or analyze the cost-benefit relationship
- What evidence would resolve it: Detailed timing measurements of each GuideCQR component and performance analysis comparing query reformulation time versus retrieval accuracy improvements across different query types and datasets

## Limitations
- Performance depends heavily on the quality of initially retrieved documents
- Requires dataset-specific threshold tuning for the filtering mechanism
- Shows limited generalizability across different query generation approaches

## Confidence
- **High Confidence**: GuideCQR improves retrieval performance over baseline methods across multiple datasets (CAsT-19, CAsT-20, QReCC) with statistically significant gains in MRR, NDCG@3, and Recall@10
- **Medium Confidence**: The effectiveness of combining keywords and expected answers from retrieved documents, as the method shows variable performance improvements depending on the baseline query source
- **Medium Confidence**: The filtering mechanism's ability to remove redundant information, as optimal thresholds require dataset-specific tuning and may not generalize

## Next Checks
1. Apply GuideCQR to baseline queries from additional query generation methods beyond InfoCQR to assess robustness across different query reformulation approaches
2. Systematically remove the filtering stage and test with different keyword span lengths to quantify the contribution of each component to overall performance
3. Evaluate GuideCQR's performance when initial retrieval yields predominantly irrelevant documents to determine failure thresholds and identify scenarios where the method may degrade rather than improve retrieval quality