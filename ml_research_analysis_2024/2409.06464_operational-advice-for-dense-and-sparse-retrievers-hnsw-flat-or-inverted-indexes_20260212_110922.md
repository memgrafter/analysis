---
ver: rpa2
title: 'Operational Advice for Dense and Sparse Retrievers: HNSW, Flat, or Inverted
  Indexes?'
arxiv_id: '2409.06464'
source_url: https://arxiv.org/abs/2409.06464
tags:
- indexes
- hnsw
- retrieval
- flat
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides empirical guidance for choosing between HNSW
  and flat vector indexes for dense retrieval, as well as sparse retrieval methods,
  across different corpus sizes. The author uses the BEIR benchmark and the Lucene
  search library to compare effectiveness (nDCG@10), query evaluation performance
  (QPS), and indexing time for various configurations.
---

# Operational Advice for Dense and Sparse Retrievers: HNSW, Flat, or Inverted Indexes?

## Quick Facts
- arXiv ID: 2409.06464
- Source URL: https://arxiv.org/abs/2409.06464
- Authors: Jimmy Lin
- Reference count: 10
- Primary result: HNSW indexes offer significant query performance advantages for large corpora (>1M documents) but higher indexing costs; quantization provides substantial QPS improvements with minimal effectiveness loss for both index types.

## Executive Summary
This paper provides empirical guidance for choosing between HNSW and flat vector indexes for dense retrieval, as well as sparse retrieval methods, across different corpus sizes. Using the BEIR benchmark and Lucene search library, the author compares effectiveness (nDCG@10), query evaluation performance (QPS), and indexing time for various configurations. Key findings include that for small corpora (<100K documents), flat and HNSW indexes show negligible differences, while for large corpora (>1M documents), HNSW indexes offer significant query performance advantages but at higher indexing costs. Quantization provides substantial QPS improvements with minimal effectiveness loss for both index types. Dense and sparse retrieval models show comparable performance, with sparse methods being faster in some cases.

## Method Summary
The study evaluates dense retrieval (HNSW vs flat indexes) and sparse retrieval methods using the BEIR benchmark with 29 retrieval datasets ranging from 3.6K to 15M documents. Experiments use Lucene 9.9.1 with Anserini toolkit, comparing HNSW, flat, and inverted indexes with and without int8 quantization. The BGE model (bge-base-en-v1.5) is used for dense retrieval and SPLADE++ for sparse retrieval. Key metrics include nDCG@10 for effectiveness, queries per second (QPS) for efficiency, and indexing time. The study examines performance across corpus size categories: small (<100K documents), medium (100K-1M documents), and large (>1M documents).

## Key Results
- For small corpora (<100K documents), flat and HNSW indexes show negligible differences in performance.
- For large corpora (>1M documents), HNSW indexes offer significant query performance advantages but at higher indexing costs.
- Quantization provides substantial QPS improvements with minimal effectiveness loss for both index types.

## Why This Works (Mechanism)

### Mechanism 1
HNSW indexes outperform flat indexes in query performance for large corpora (>1M documents) despite higher indexing costs. HNSW builds a hierarchical graph structure that allows logarithmic-time search complexity compared to linear search in flat indexes, enabling faster query evaluation as corpus size grows. The core assumption is that the hierarchical graph traversal in HNSW remains efficient even as document vectors increase, and the graph construction cost is amortized over many queries. Evidence shows that for large corpora, flat indexes can be up to an order of magnitude slower than HNSW indexes. This advantage breaks down when query patterns are highly random or corpus size remains small (<100K documents), where the overhead of building HNSW graphs may not be justified.

### Mechanism 2
Quantization provides substantial QPS improvements with minimal effectiveness loss for both HNSW and flat indexes. Quantization reduces vector precision (e.g., from float32 to int8), decreasing memory bandwidth requirements and enabling more efficient SIMD operations during similarity calculations. The core assumption is that reduced precision in quantized vectors does not significantly alter the relative ranking of nearest neighbors for most queries. Evidence shows that quantization for HNSW indexes delivers large benefits in increased QPS, with effectiveness degradation comparable to non-quantized HNSW indexes. This mechanism may break down if the embedding model produces vectors where small differences are semantically meaningful, causing unacceptable retrieval quality degradation.

### Mechanism 3
Dense and sparse retrieval models show comparable performance, with sparse methods being faster in some cases. Dense models use approximate nearest neighbor search over continuous vectors, while sparse models use exact matching with inverted indexes; the choice depends on the tradeoff between semantic matching quality and computational efficiency. The core assumption is that both dense and sparse models can achieve similar retrieval effectiveness when properly optimized, and performance differences depend more on implementation details than fundamental model properties. Evidence shows that in terms of QPS, both approaches appear to be comparable. This breaks down when query distributions contain many rare terms or require exact phrase matching, where sparse retrieval may significantly outperform dense retrieval.

## Foundational Learning

- **Vector similarity measures (dot product, cosine similarity)**: Understanding how dense retrieval computes relevance scores is fundamental to grasping why different indexing strategies matter. Quick check: What is the relationship between dot product and cosine similarity when vectors are normalized?

- **Inverted index structure and postings lists**: Sparse retrieval relies on inverted indexes, and understanding their structure helps explain why they can be faster than dense retrieval in certain scenarios. Quick check: How does term frequency weighting in inverted indexes relate to the "fake words" trick used for learned sparse retrieval?

- **Graph-based nearest neighbor search (HNSW algorithm)**: HNSW is the primary alternative to flat indexes, and understanding its hierarchical structure explains its performance characteristics. Quick check: What role do the M parameter (maximum number of neighbors) and efSearch parameter play in HNSW index construction and query performance?

## Architecture Onboarding

- **Component map**: Document embedding → Index construction → Query encoding → Similarity search → Result ranking → Evaluation
- **Critical path**: Document embedding → Index construction → Query encoding → Similarity search → Result ranking → Evaluation
- **Design tradeoffs**:
  - Indexing time vs query performance: HNSW requires more indexing time but provides better query performance at scale
  - Memory usage vs accuracy: Quantization reduces memory usage but may slightly degrade retrieval quality
  - Exact vs approximate search: Flat indexes provide exact results but scale poorly; HNSW provides approximate results with better scalability
- **Failure signatures**:
  - High indexing times with poor query performance: HNSW parameters (M, efC) may be misconfigured
  - Significant effectiveness degradation after quantization: The embedding model may be sensitive to precision loss
  - Comparable performance between dense and sparse retrieval: The corpus may not benefit significantly from semantic matching
- **First 3 experiments**:
  1. Compare flat vs HNSW indexing times and query performance on a small corpus (<100K documents) to verify negligible differences
  2. Measure QPS improvement from quantization on both flat and HNSW indexes while tracking nDCG@10 degradation
  3. Benchmark dense retrieval (BGE) vs sparse retrieval (SPLADE) on a medium-sized corpus (100K-1M documents) to observe performance tradeoffs

## Open Questions the Paper Calls Out

**Open Question 1**: What is the minimum corpus size where HNSW indexes become consistently more efficient than flat indexes? The paper suggests HNSW indexes become beneficial for corpora larger than 1M documents but doesn't pinpoint the exact threshold where benefits become consistent. This remains unresolved because the paper uses informal categories without identifying a specific break-even point. A systematic study varying corpus sizes at finer granularity (e.g., 100K, 250K, 500K, 750K, 1M, 2M) with consistent experimental conditions would resolve this question.

**Open Question 2**: How do quantization effects vary across different embedding models beyond BGE? The paper only evaluates quantization effects on the BGE embedding model, noting that different models may behave differently. This remains unresolved because the findings about quantization's impact are specific to BGE and may not generalize to other models. Replicating the quantization experiments with multiple embedding models (e.g., E5, GTR, SBERT) using the same methodology would resolve this question.

**Open Question 3**: How do different HNSW implementations (beyond Lucene) compare in terms of effectiveness-efficiency tradeoffs? The paper acknowledges using only Lucene's HNSW implementation and notes that other implementations exist but are difficult to compare fairly. This remains unresolved because the findings about HNSW performance are specific to Lucene's implementation. A comprehensive comparison of HNSW implementations (Faiss, Pinecone, Weaviate, etc.) using identical embedding models and corpora would resolve this question.

## Limitations
- Conclusions are based on a single search library (Lucene) and specific model implementations, which may not generalize to other systems or models.
- The analysis covers only one specific HNSW configuration (M=16, efC=100, efSearch=1000) without exploring the parameter space exhaustively.
- The evaluation uses the BEIR benchmark, which may not represent all real-world retrieval scenarios, particularly those with different query distributions or domain-specific requirements.

## Confidence
- High confidence: Relative performance comparison between HNSW and flat indexes across different corpus sizes
- Medium confidence: Quantization effectiveness claims and their exact tradeoff points
- Medium confidence: Comparative claims between dense and sparse retrieval showing they are comparable

## Next Checks
1. Validate the HNSW vs flat performance findings on a different search library (e.g., FAISS or Milvus) to confirm the scalability conclusions are not Lucene-specific.
2. Conduct ablation studies on HNSW parameters (M, efC, efSearch) to identify optimal configurations for different corpus sizes and query patterns.
3. Test the dense vs sparse retrieval comparisons on domain-specific corpora (e.g., legal or biomedical) to assess whether the comparable performance holds for specialized vocabularies and query types.