---
ver: rpa2
title: Anatomy of Neural Language Models
arxiv_id: '2401.03797'
source_url: https://arxiv.org/abs/2401.03797
tags:
- language
- transformer
- input
- parameters
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This tutorial provides a comprehensive, self-contained mathematical
  framework explaining the main types of neural language models (LMs), including feedforward
  neural networks (FFNN), recurrent neural networks (RNN), and transformers. The tutorial
  covers autoregressive and autoencoding language modeling, transfer learning applications,
  and the use of transformers in computer vision and time series analysis.
---

# Anatomy of Neural Language Models

## Quick Facts
- arXiv ID: 2401.03797
- Source URL: https://arxiv.org/abs/2401.03797
- Authors: Majd Saleh; St√©phane Paquelet
- Reference count: 40
- Primary result: Derives and validates formulas for calculating trainable parameters in FFNN, RNN, and transformer LMs

## Executive Summary
This tutorial provides a comprehensive mathematical framework explaining neural language models including feedforward neural networks, recurrent neural networks, and transformers. The paper derives formulas for calculating the total number of trainable parameters as functions of hyperparameters and validates these formulas using TensorFlow's parameter counters and KerasNLP's GPT2 implementation. It also includes a from-scratch implementation of GPT2 and explores transformer applications in computer vision and time series analysis.

## Method Summary
The paper develops a self-contained mathematical framework for neural language models, deriving parameter counting formulas for various architectures. These formulas are implemented in Python and validated against TensorFlow's parameter counters using widely used models like BERT and GPT2. The tutorial includes concrete examples with clear graphical illustrations, strict mathematical notation, and a from-scratch implementation of transformer inference-pass equations with a GPT2 example. The framework covers autoregressive and autoencoding language modeling, transfer learning applications, and transformer extensions to computer vision and time series domains.

## Key Results
- Derived and validated formulas for calculating trainable parameters in FFNN, RNN, and transformer LMs
- Implemented a from-scratch GPT2 model demonstrating transformer inference-pass equations
- Explored transformer applications in computer vision (ViT) and time series analysis (TST) with focus on embedding preparation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The tutorial provides a self-contained comprehensive anatomy of neural language models in a strict mathematical framework.
- Mechanism: The paper derives formulas for the total number of trainable parameters of considered LMs as functions of their hyperparameters and validates these formulas using TensorFlow's parameter counters and KerasNLP's GPT2 implementation, showing identical results.
- Core assumption: The mathematical framework is complete and accurate enough to capture all trainable parameters in the considered architectures.
- Evidence anchors:
  - [abstract]: "We derived the formulas that give the total number of trainable parameters of the considered LMs as functions of their hyper parameters. These formulas have been implemented in Python and their results were compared to trainable parameters' counters of TensorFlow with examples on widely used models like BERT [12] and GPT2 [48]."
  - [section]: "We derived the formulas that give the total number of trainable parameters of the considered LMs as functions of their hyper parameters. These formulas have been implemented in Python and their results were compared to trainable parameters' counters of TensorFlow with examples on widely used models like BERT [12] and GPT2 [48]."
  - [corpus]: Weak evidence - corpus contains no direct evidence about parameter formula validation. Only general survey of transformer literature.

### Mechanism 2
- Claim: The tutorial explains neural language models in a detailed, simplified and unambiguous mathematical framework accompanied by clear graphical illustrations.
- Mechanism: The paper provides strict notation for scalars, vectors, matrices, functions, sets, and tuples to avoid ambiguity. It organizes figures systematically to show NN components, their dimensionality, and their packaging functions.
- Core assumption: The strict notation and figure organization enable clear understanding of complex mathematical concepts.
- Evidence anchors:
  - [abstract]: "We address the aforementioned problem in this tutorial where the objective is to explain neural LMs in a detailed, simplified and unambiguous mathematical framework accompanied by clear graphical illustrations."
  - [section]: "In order to avoid any ambiguity between scalars, vectors, matrices, function names, sets, tuples and sequences, we use a strict notation as described in Table 1... Unless otherwise stated, figures are read from bottom to up and/or from left to right. Most of figures describe components of neural networks (NNs) as in the example shown in Figure 1."
  - [corpus]: Weak evidence - corpus contains no direct evidence about the effectiveness of the notation system or figure organization in improving understanding.

### Mechanism 3
- Claim: The tutorial enables readers to understand how transformers work in computer vision and time series applications and compare this use with the original one in NLP.
- Mechanism: The paper explores concrete examples of transformers in CV (ViT) and TS (TST) focusing on how embeddings are prepared from raw data, and compares these applications to the original NLP use.
- Core assumption: Understanding the embedding preparation process in CV and TS applications provides insight into how transformers work in these domains compared to NLP.
- Evidence anchors:
  - [abstract]: "Finally, since transformers pretrained on language-modeling-like tasks have been widely adopted in computer vision and time series applications, we briefly explore some examples of such solutions in order to enable readers to understand how transformers work in the aforementioned domains and compare this use with the original one in NLP."
  - [section]: "We explore in this section the use of transformers in computer vision and time series analysis applications. We mainly focus of the preparation of input embeddings starting from raw data... Figure 25-a shows how ViT convert the input image ùë∞‚àà‚Ñõùêª√óùëä√óùê∂ into ùëõ patches... Figure 26 shows the TST Masking scheme..."
  - [corpus]: Weak evidence - corpus contains no direct evidence about the effectiveness of the CV and TS examples in helping readers understand transformer applications compared to NLP.

## Foundational Learning

- Concept: Word vector representations (one-hot vectors and dense real-valued embeddings)
  - Why needed here: Neural language models require numeric vector representations of words to process them mathematically.
  - Quick check question: What is the difference between one-hot vector representation and dense real-valued embedding representation of words?

- Concept: Language modeling types (autoregressive and autoencoding)
  - Why needed here: Understanding the two main types of language modeling is fundamental to grasping how different neural architectures approach language modeling tasks.
  - Quick check question: What is the key difference between autoregressive and autoencoding language modeling objectives?

- Concept: Transfer learning in NLP
  - Why needed here: Transfer learning is a crucial application of language modeling that enables pretrained models to be used in downstream tasks.
  - Quick check question: How does transfer learning work in the context of pretrained language models?

## Architecture Onboarding

- Component map: Input tokens ‚Üí one-hot vectors ‚Üí embeddings ‚Üí neural network processing (FFNN/RNN/transformer blocks) ‚Üí logits ‚Üí probability distribution ‚Üí output tokens
- Critical path: For autoregressive language modeling: input tokens ‚Üí one-hot vectors ‚Üí embeddings ‚Üí neural network processing ‚Üí logits ‚Üí probability distribution ‚Üí next token prediction
- Design tradeoffs: FFNNs offer simplicity but limited context window; RNNs support unlimited sequence length but suffer from vanishing gradients; transformers enable efficient parallel processing and capture long-range dependencies but require fixed-length inputs
- Failure signatures: Vanishing/exploding gradients in RNNs, limited context window in FFNNs, quadratic complexity in transformers with long sequences, overfitting in large models with limited data
- First 3 experiments:
  1. Implement and validate the parameter counting formulas for a simple FFNN language model.
  2. Implement the self-attention mechanism and verify it produces correct attention weights.
  3. Implement a complete transformer block and verify it can process a small sequence correctly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different positional encoding schemes (learned vs fixed sinusoidal) on the performance of transformer language models?
- Basis in paper: [explicit] The paper mentions that positional encodings might be trained from scratch or, alternatively, fixed sinusoidal formulas can be used. It also states that positional encodings aim at representing the position of the token regardless of what the token is.
- Why unresolved: The paper does not provide a detailed comparison or analysis of the performance differences between learned and fixed sinusoidal positional encodings in transformer language models.
- What evidence would resolve it: Experimental results comparing the performance of transformer language models using learned positional encodings versus fixed sinusoidal positional encodings on various NLP tasks.

### Open Question 2
- Question: How does the choice of tokenization method (rule-based, character-based, or data-based) affect the performance of transformer language models?
- Basis in paper: [explicit] The paper discusses different tokenization methods, including rule-based, character-based, and data-based tokenization. It also mentions that depending on the tokenization algorithm, the same sentence can be segmented in different ways.
- Why unresolved: The paper does not provide a detailed analysis of how different tokenization methods impact the performance of transformer language models.
- What evidence would resolve it: Experimental results comparing the performance of transformer language models using different tokenization methods on various NLP tasks.

### Open Question 3
- Question: What are the limitations of the current transformer architecture in handling long sequences, and how can these limitations be addressed?
- Basis in paper: [explicit] The paper discusses the limits of input sequence length for different types of language models, including transformers. It mentions that transformers are designed to receive fixed length sequences, but this fixed length can be quite long. It also mentions that the attention mechanism and parallel architecture of transformers enable efficient training with long sequences.
- Why unresolved: The paper does not provide a detailed analysis of the specific limitations of the current transformer architecture in handling long sequences and potential solutions to address these limitations.
- What evidence would resolve it: Experimental results and analysis of the performance of transformer language models on long sequences, as well as proposed solutions to address any identified limitations.

## Limitations

- The validation of parameter counting formulas relies on comparison with TensorFlow counters, but potential discrepancies in how different frameworks count parameters (e.g., shared vs. independent parameters) are not explicitly addressed
- The corpus analysis revealed limited direct evidence supporting the tutorial's effectiveness in achieving its stated objectives of clarity and understanding
- While the paper claims to provide a "self-contained" framework, the actual completeness of the mathematical treatment of all architectural components remains uncertain without detailed examination

## Confidence

- **High Confidence**: The derivation of parameter counting formulas as functions of hyperparameters - this is a well-defined mathematical problem with clear validation methodology
- **Medium Confidence**: The tutorial's ability to provide a comprehensive mathematical framework - while formulas are provided, the pedagogical effectiveness is not empirically validated
- **Low Confidence**: Claims about enabling readers to understand transformer applications in CV and TS domains compared to NLP - based on corpus evidence showing no direct support for this claim

## Next Checks

1. **Parameter Formula Verification**: Implement the parameter counting formulas for multiple architecture configurations (varying embedding sizes, hidden dimensions, number of layers) and systematically compare with TensorFlow and PyTorch parameter counters to identify any edge cases or discrepancies

2. **Pedagogical Effectiveness Study**: Conduct a small user study where participants with intermediate ML knowledge attempt to implement architectures based solely on the tutorial's mathematical framework, measuring implementation success rate and time-to-completion

3. **Cross-Framework Consistency**: Replicate the parameter counting validation using PyTorch implementations alongside TensorFlow to verify consistency across deep learning frameworks, particularly for models with parameter sharing or complex architectural patterns