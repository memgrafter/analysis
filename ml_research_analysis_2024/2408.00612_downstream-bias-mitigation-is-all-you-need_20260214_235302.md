---
ver: rpa2
title: Downstream bias mitigation is all you need
arxiv_id: '2408.00612'
source_url: https://arxiv.org/abs/2408.00612
tags:
- bias
- dataset
- downstream
- association
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the transfer of bias from pre-trained language
  models (LLMs) to downstream tasks through fine-tuning. The authors conduct controlled
  experiments on two classification tasks - occupation prediction from biographies
  and toxicity detection from Wikipedia comments - to examine whether interventions
  on pre-trained models affect downstream bias.
---

# Downstream bias mitigation is all you need

## Quick Facts
- arXiv ID: 2408.00612
- Source URL: https://arxiv.org/abs/2408.00612
- Authors: Arkadeep Baksi; Rahul Singh; Tarun Joshi
- Reference count: 12
- Key outcome: Most downstream bias in LLMs comes from fine-tuning dataset bias rather than pre-trained model bias, with scrubbing identity terms only effective when not pre-trained

## Executive Summary
This paper investigates whether upstream bias mitigation in pre-trained language models effectively reduces downstream bias after fine-tuning. Through controlled experiments on occupation prediction and toxicity detection tasks, the authors demonstrate that downstream bias is primarily determined by biases present in the fine-tuning dataset rather than the pre-trained model. The study reveals that common interventions like scrubbing identity terms or rebalancing training data have minimal impact on downstream bias when using pre-trained models. The authors propose using bias directions in word embedding spaces as a proxy to identify dataset bias, particularly for gender bias in biographies. The key finding is that dataset quality and context-specific bias mitigation during fine-tuning are more important than upstream interventions for reducing downstream harms in LLMs.

## Method Summary
The authors conduct controlled experiments using BERT-base on two classification tasks: occupation prediction from BIOS biographies and toxicity detection from Wikipedia comments. They systematically test interventions including no pre-training, weight perturbation, re-sampling, and scrubbing identity terms. Bias is measured using TPR gaps for pronouns in BIOS and FPR for identity terms in WIKI. The study compares downstream bias across different intervention conditions and analyzes the relationship between upstream model bias and dataset bias through regression analysis. A novel contribution is the use of bias directions in embedding spaces to identify proxy words encoding gender bias.

## Key Results
- Downstream bias is predominantly explained by biases present in the fine-tuning dataset rather than the pre-trained model
- Scrubbing identity terms from fine-tuning data only reduces downstream bias when models are trained from scratch without pre-training
- Bias directions in word embedding spaces can effectively identify proxy words encoding gender bias beyond explicit pronouns
- Interventions on pre-trained model weights show minimal impact on reducing downstream bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Downstream bias is primarily determined by biases present in the fine-tuning dataset rather than upstream biases from pre-trained models.
- Mechanism: The fine-tuning process learns patterns directly from the dataset, so biases encoded in the data dominate the model's behavior regardless of upstream bias mitigation.
- Core assumption: The fine-tuning dataset is large enough and representative enough that it overwrites any upstream biases during adaptation.
- Evidence anchors:
  - [abstract]: "most downstream bias is explained by biases present in the fine-tuning dataset rather than the pre-trained model"
  - [section 4.2]: "dataset bias during the fine-tuning stage plays a pivotal role in explaining the dependence between upstream and downstream bias"
  - [corpus]: Weak evidence - corpus neighbors discuss similar bias inheritance issues but don't directly support the primary claim
- Break condition: If the fine-tuning dataset is small or highly noisy, upstream biases may dominate the final model behavior.

### Mechanism 2
- Claim: Removing identity terms from the fine-tuning dataset only reduces downstream bias when the model is not pre-trained.
- Mechanism: Pre-trained models have already learned associations between identity terms and biased contexts from their pre-training corpora, so removing these terms during fine-tuning doesn't eliminate the learned associations.
- Core assumption: Pre-trained models have learned robust representations that persist through fine-tuning, even when identity terms are removed.
- Evidence anchors:
  - [section 4.3]: "BERT relies on its high-quality feature embeddings to learn proxy biases regarding identity terms based on their usage patterns in the pre-training corpora"
  - [section 4.3]: "when the upstream model is not pre-trained, the fine-tuned model lacks such preconceptions"
  - [corpus]: Weak evidence - corpus neighbors discuss bias mitigation but don't specifically address the pre-training condition
- Break condition: If the pre-training corpus doesn't contain biased associations with identity terms, scrubbing during fine-tuning would be effective regardless of pre-training status.

### Mechanism 3
- Claim: Bias directions in word embedding spaces can serve as proxies for protected attributes and help identify proxy words encoding gender bias.
- Mechanism: Gender bias manifests as a consistent direction in the embedding space that can be quantified by projecting word vectors onto this direction, allowing identification of proxy words beyond explicit pronouns.
- Core assumption: The embedding space contains meaningful semantic directions that correspond to gender and can be extracted through principal component analysis of gendered word pairs.
- Evidence anchors:
  - [section 4.4]: "we examined the efficacy of utilizing the bias directions inherent in the word embeddings to produce a proxy for the protected attribute being considered"
  - [section 4.4]: "we defined the femaleness score of each word as the magnitude of the projected word-vector on the gender direction"
  - [corpus]: Weak evidence - corpus neighbors discuss bias in embeddings but don't specifically address using embedding directions as proxies
- Break condition: If the embedding space is too context-dependent or if gender associations are not represented as a consistent direction, this proxy approach would fail.

## Foundational Learning

- Concept: True Positive Rate (TPR) and False Positive Rate (FPR) bias metrics
  - Why needed here: These metrics quantify downstream bias by measuring performance disparities across different groups (pronouns or identity terms)
  - Quick check question: If a classifier has TPR_bias = 0.5 for surgeons, what does this mean about its accuracy for she/her vs he/him biographies?

- Concept: Embedding space bias directions
  - Why needed here: Understanding how biases manifest as directions in embedding space is crucial for the proposed proxy word identification method
  - Quick check question: If we compute the top principal component of gender pair differences, what property should this direction have regarding gender bias?

- Concept: Controlled intervention methodology
  - Why needed here: The paper systematically tests different interventions (no pre-training, weight perturbation, re-sampling, scrubbing) to isolate the source of bias
  - Quick check question: Why is it important to test both pre-trained and randomly initialized models when studying bias transfer?

## Architecture Onboarding

- Component map: Pre-trained BERT model -> fine-tuning dataset -> bias measurement pipeline (upstream and downstream metrics) -> intervention experiments
- Critical path: Pre-trained model weights -> fine-tuning process -> dataset bias -> downstream bias outcome
- Design tradeoffs: Using pre-trained models provides better performance but may introduce upstream bias; training from scratch avoids upstream bias but requires more data and computation
- Failure signatures: If downstream bias persists despite upstream bias mitigation, the issue likely lies in the fine-tuning dataset; if scrubbing identity terms doesn't help, pre-training bias may be the culprit
- First 3 experiments:
  1. Fine-tune BERT-base on BIOS dataset with pronouns scrubbed, measure downstream TPR bias across occupations
  2. Train BERT from scratch on BIOS dataset with pronouns scrubbed, compare downstream bias to pre-trained version
  3. Compute embedding-based gender direction, identify and remove proxy words from BIOS dataset, measure downstream bias reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms in pre-trained language models cause them to resist interventions aimed at reducing downstream bias?
- Basis in paper: [explicit] The paper notes that "pre-trained models may exhibit resistance to interventions like scrubbing, re-balancing, and other straightforward modifications of the fine-tuning dataset" and discusses how BERT relies on "high-quality feature embeddings to learn proxy biases."
- Why unresolved: The paper identifies resistance but doesn't explain the underlying mechanisms. Understanding whether this is due to attention mechanisms, embedding space geometry, or other architectural features would help design better mitigation strategies.
- What evidence would resolve it: Detailed ablation studies examining which components of the model architecture (attention heads, embedding layers, etc.) contribute most to bias persistence, or controlled experiments varying architectural parameters while measuring intervention effectiveness.

### Open Question 2
- Question: How do different types of bias (gender, racial, intersectional) interact and transfer through the pre-training and fine-tuning pipeline?
- Basis in paper: [inferred] The paper primarily focuses on gender bias in the BIOS dataset and mentions identity terms in the WIKI dataset, but doesn't systematically explore how different bias types interact or whether they transfer differently.
- Why unresolved: The paper's controlled experiments focus on single bias types. Understanding whether intersectional biases compound differently or whether some bias types are more resistant to intervention than others would inform comprehensive bias mitigation strategies.
- What evidence would resolve it: Controlled experiments introducing multiple bias types simultaneously during pre-training and fine-tuning, measuring how they interact and whether mitigation strategies effective for one bias type work for others.

### Open Question 3
- Question: Can we develop reliable early indicators during pre-training that predict downstream bias severity without requiring full fine-tuning?
- Basis in paper: [explicit] The paper uses bias directions in embedding spaces as proxies and finds correlations between intrinsic and extrinsic metrics, but notes that "intrinsic bias metrics may be unsuitable for measuring model bias" and explores limitations of current approaches.
- Why unresolved: While the paper identifies some correlation between upstream and downstream bias, it doesn't establish a robust predictive framework. Developing such indicators would allow for early intervention during pre-training rather than post-hoc fine-tuning adjustments.
- What evidence would resolve it: Longitudinal studies tracking bias metrics throughout pre-training, identifying specific checkpoints or metrics that consistently predict downstream behavior across multiple tasks and bias types.

## Limitations
- Findings may not generalize beyond occupation prediction and toxicity detection tasks
- Controlled experiments operate in narrow settings that might not capture real-world deployment complexity
- Paper doesn't address temporal dynamics in bias or how patterns hold with evolving language use
- Embedding-based proxy identification assumes linear separability of gender bias that may not hold for intersectional biases

## Confidence
- **High confidence**: The primary finding that downstream bias is predominantly explained by fine-tuning dataset bias rather than upstream model bias
- **Medium confidence**: The claim that pre-training fundamentally prevents identity term scrubbing from reducing downstream bias
- **Medium confidence**: The effectiveness of bias direction proxies for identifying gender bias in datasets

## Next Checks
1. Test the core hypothesis on additional classification tasks (e.g., sentiment analysis, named entity recognition) to assess generalizability
2. Examine whether scrubbing intervention effectiveness varies systematically with pre-training corpus composition
3. Evaluate whether the embedding-based proxy identification method can detect and help mitigate intersectional biases beyond binary gender