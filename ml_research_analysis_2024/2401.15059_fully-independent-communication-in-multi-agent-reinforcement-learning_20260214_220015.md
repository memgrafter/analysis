---
ver: rpa2
title: Fully Independent Communication in Multi-Agent Reinforcement Learning
arxiv_id: '2401.15059'
source_url: https://arxiv.org/abs/2401.15059
tags:
- communication
- learning
- agents
- parameters
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies communication in multi-agent reinforcement
  learning (MARL) when agents do not share network parameters. The authors identify
  a key problem: when parameters are not shared, backpropagation cannot properly update
  communication networks because messages from other agents are not linked to the
  receiving agent''s parameters.'
---

# Fully Independent Communication in Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2401.15059
- Source URL: https://arxiv.org/abs/2401.15059
- Reference count: 40
- One-line primary result: Independent agents can learn communication by including their own message as input and detaching incoming messages during backpropagation, though performance lags behind parameter sharing.

## Executive Summary
This paper addresses a fundamental problem in multi-agent reinforcement learning (MARL): enabling communication among agents that do not share network parameters. When parameters are not shared, standard backpropagation fails to properly update communication networks because messages from other agents are not linked to the receiving agent's parameters. The authors propose a novel learning scheme where agents include their own message as input and detach incoming messages during backpropagation, enabling independent communication. Experiments in StarCraft micromanagement and predator-prey tasks demonstrate that this approach allows communication among independent agents, though with performance trade-offs compared to parameter sharing.

## Method Summary
The authors study independent communication in MARL by implementing a scheme where each agent maintains separate policy and communication networks. The key innovation is including each agent's own message as input to its policy network while detaching incoming messages from other agents during backpropagation. This keeps the computational graph intact for updating both policy and communication networks independently. The method is tested using Independent Deep Q-Learning (IQL) with epsilon-greedy exploration, RMSprop optimizer, and target networks, comparing configurations with and without parameter sharing across different network capacities.

## Key Results
- Independent agents can learn communication strategies using the proposed method, though performance lags behind parameter sharing
- Communication is not always necessary; increasing network capacity can sometimes replace the need for communication
- Network capacity must be considered alongside communication for efficient learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Independent agents can still learn communication by including their own message as input to the policy network, keeping the computational graph intact for backpropagation.
- Mechanism: By adding the agent's own message (ð‘”ð‘–(ð‘œð‘–; ðœ‡ð‘–)) as an input to its policy (ð‘“ð‘–), the gradient flows back through both policy and communication networks during updates. Incoming messages from others are detached to avoid multiple gradient updates.
- Core assumption: The agent's own message is a sufficient signal for the communication network to learn meaningful representations, even without direct gradient feedback from other agents' policies.
- Evidence anchors:
  - [abstract]: "We propose a new learning scheme where agents include their own message as input and detach incoming messages during backpropagation, enabling independent communication."
  - [section]: "Intuitively, this step solves the problem stated in Remark 1... we detach ð‘šâˆ’ð‘– from the computational graph, ensuring that all ðœƒð‘– âˆ§ ðœ‡ð‘–... are updated exactly once."
- Break condition: If the communication network's representation capacity is too small, or if the environment does not require coordination, the self-message feedback may be insufficient for meaningful communication learning.

### Mechanism 2
- Claim: Parameter sharing is not strictly required for successful MARL; independent agents can converge, albeit slower, when using the proposed communication scheme.
- Mechanism: Each agent maintains separate policy and communication networks. Updates propagate gradients only within each agent's networks, and self-message inclusion keeps communication gradients alive. This avoids the shared-parameter dependency but requires more samples to converge.
- Core assumption: Agents can learn from global reward signals and self-message feedback, even without direct inter-agent parameter linkage.
- Evidence anchors:
  - [abstract]: "Our results show that, despite the challenges, independent agents can still learn communication strategies following our method."
  - [section]: "When we study the effect of communication, it is possible to see that the impact of communication is also more evident when parameters are shared... However, despite this limitation... we can still see the improvements of communication in fully independent learners."
- Break condition: In highly complex coordination tasks, lack of parameter sharing may slow convergence so much that learning becomes impractical.

### Mechanism 3
- Claim: Communication is not always necessary; increasing network capacity can sometimes replace the need for communication.
- Mechanism: Larger policy network hidden layers can encode more information directly from observations, reducing the marginal benefit of adding communication. When the environment is simple, agents can learn optimal policies without message exchange.
- Core assumption: The information bottleneck created by small network capacity can be alleviated either by communication (external info sharing) or by increasing internal representational capacity.
- Evidence anchors:
  - [abstract]: "We observe that communication may not always be needed and that the chosen agent network sizes need to be considered when used together with communication in order to achieve efficient learning."
  - [section]: "Figures 5(a) and 5(b) show the performances of PS+IQL and PS+IQL+COMM in the 3s_vs_5z environment, respectively. In this case, we can see that communication does not seem to play an important role."
- Break condition: In tasks requiring tight coordination or information that individual observations cannot provide, communication will still be essential regardless of network size.

## Foundational Learning

- Concept: Backpropagation and computational graphs
  - Why needed here: Understanding how message inputs affect gradient flow in multi-agent settings is central to the proposed independent communication scheme.
  - Quick check question: What happens to the gradient of a message received from another agent if that agent's network parameters are not shared?

- Concept: Experience replay and target networks
  - Why needed here: Independent learners each maintain their own replay buffer and target network, so the student must know how this stabilizes learning without parameter sharing.
  - Quick check question: How does the target network update interval affect stability in independent Q-learning versus shared-parameter setups?

- Concept: GRU-based recurrent networks for partial observability
  - Why needed here: Agents operate on action-observation histories; knowing how GRUs process sequential data is essential to implement the policy and communication networks.
  - Quick check question: What is the effect of increasing GRU hidden size on sample efficiency in a partially observable multi-agent task?

## Architecture Onboarding

- Component map:
  Policy network (GRU) -> Q-value output
  Communication network (MLP) -> Message encoding
  Replay buffer (per-agent) -> (observation, action, reward, next observation) tuples
  Target networks (per-agent) -> Stable Q-value targets

- Critical path:
  1. Observe â†’ encode message (own network)
  2. Collect incoming messages, detach them
  3. Feed observation + own message + detached incoming messages into policy network
  4. Select action, store transition in replay buffer
  5. Sample minibatch, compute target Q-values using target networks
  6. Backpropagate through policy and communication networks

- Design tradeoffs:
  - Parameter sharing: Faster convergence, simpler communication gradients, but unrealistic for distributed deployment
  - No parameter sharing: More realistic deployment, slower convergence, requires self-message trick for gradient flow
  - Communication network size: Larger size can encode richer messages but increases computation; may offset need for communication in simple tasks

- Failure signatures:
  - No learning progress: Check if self-message is being passed correctly and if gradients are flowing through communication networks
  - High variance in returns: Verify that incoming messages are properly detached to avoid repeated gradient updates
  - Suboptimal performance vs. parameter sharing: Expect slower convergence; if gap is too large, check network capacities and exploration schedule

- First 3 experiments:
  1. Run NPS+IQL (no communication, no sharing) on 3s_vs_5z; confirm baseline performance
  2. Add communication network (NPS+IQL+COMM); verify that self-message trick allows gradients to flow and that win rate improves
  3. Vary GRU hidden size (32, 64, 128) with and without communication; observe if larger capacity reduces need for communication

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed learning scheme for independent communication without parameter sharing (NPS+IQL+COMM) maintain or improve performance when scaling to larger numbers of agents?
- Basis in paper: [explicit] The paper demonstrates the method works in 3s_vs_5z (8 agents total) and PredatorPrey (4 agents), but doesn't test scalability
- Why unresolved: The experiments only cover small-scale scenarios. Larger agent counts could reveal limitations in the gradient propagation approach or communication overhead
- What evidence would resolve it: Experiments testing the method with 10+ agents in complex environments, measuring performance degradation and communication efficiency

### Open Question 2
- Question: How does the proposed independent communication method compare to parameter sharing in terms of sample efficiency and convergence speed when network capacity is optimized?
- Basis in paper: [inferred] The paper shows communication helps in PredatorPrey but not 3s_vs_5z, and shows network capacity affects performance, but doesn't systematically compare sample efficiency between PS and NPS configurations
- Why unresolved: The paper focuses on final performance rather than learning dynamics. Sample efficiency could reveal different tradeoffs between parameter sharing and independent communication
- What evidence would resolve it: Learning curves comparing wall-clock time to reach specific performance thresholds for both configurations with matched network capacities

### Open Question 3
- Question: What are the fundamental limitations of the proposed gradient detachment approach when applied to more complex communication architectures (e.g., attention-based or graph neural network communication)?
- Basis in paper: [explicit] The paper proposes detaching incoming messages from the computational graph to solve the backpropagation problem, but only tests with simple linear transformation communication networks
- Why unresolved: The gradient detachment solution may not scale to architectures where message interactions are more complex or where global communication patterns are important
- What evidence would resolve it: Testing the method with attention-based communication modules or graph neural networks, measuring performance degradation or learning instability

## Limitations
- The proposed method shows performance gaps compared to parameter sharing, suggesting fundamental limitations in the gradient propagation approach
- Network capacity requirements must be carefully considered alongside communication, adding complexity to system design
- Experiments only cover small-scale scenarios (up to 8 agents), leaving scalability questions unanswered

## Confidence
- **High**: The core mechanism of including self-messages and detaching incoming messages is clearly described and theoretically sound.
- **Medium**: Empirical results demonstrate the feasibility of independent communication, but the extent of performance degradation compared to parameter sharing is not fully explained.
- **Medium**: The observation that network capacity can substitute for communication in simple tasks is supported, but the relationship between task complexity and this effect needs further exploration.

## Next Checks
1. Verify gradient flow through communication networks by logging parameter updates during training
2. Systematically test the effect of different GRU hidden sizes (32, 64, 128) on both communication-dependent and communication-independent tasks
3. Compare convergence speed and final performance against alternative communication schemes that use parameter sharing