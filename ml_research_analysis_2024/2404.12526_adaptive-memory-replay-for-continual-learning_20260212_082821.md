---
ver: rpa2
title: Adaptive Memory Replay for Continual Learning
arxiv_id: '2404.12526'
source_url: https://arxiv.org/abs/2404.12526
tags:
- data
- replay
- training
- learning
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses continual learning for foundation models,
  where the goal is to update large pre-trained models on new data without catastrophic
  forgetting of past tasks. The authors advocate for a setting with abundant memory
  (all past data stored) but limited computational resources.
---

# Adaptive Memory Replay for Continual Learning

## Quick Facts
- arXiv ID: 2404.12526
- Source URL: https://arxiv.org/abs/2404.12526
- Reference count: 40
- One-line primary result: Adaptive memory replay reduces catastrophic forgetting by up to 10% compared to uniform replay while maintaining training efficiency

## Executive Summary
This paper addresses continual learning for foundation models by proposing an adaptive memory replay method that dynamically selects past data for replay based on a multi-armed bandit formulation. The approach uses Boltzmann sampling to probabilistically select data clusters expected to cause the most forgetting given the current task. Experiments on vision and language pre-training tasks show the method reduces forgetting by up to 10% compared to simple uniform replay, while maintaining training efficiency.

## Method Summary
The method frames continual learning as a non-stationary multi-armed bandit problem where each cluster of past data is treated as an arm. For each batch of new task data, the algorithm estimates forgetting for each cluster, uses Boltzmann sampling to probabilistically select clusters with higher estimated forgetting, and fills a replay buffer with sampled data. To maintain constant computational budget, the algorithm removes an equal number of random data points from the new task batch before training on the combined batch.

## Key Results
- Reduces forgetting by up to 10% compared to uniform replay on vision and language tasks
- Maintains training efficiency by keeping the number of computed gradients constant
- Outperforms naive replay baselines while using the same computational resources
- Works across multiple domains including vision (DomainNet, Medical MNIST) and language (Huggingface datasets)

## Why This Works (Mechanism)

### Mechanism 1
The method reduces catastrophic forgetting by dynamically selecting past data that currently exhibits the highest forgetting given the new task data. The algorithm estimates forgetting for each cluster using a small number of evaluations and uses Boltzmann sampling to probabilistically select clusters with higher estimated forgetting. This ensures the most "at-risk" data points are replayed.

Core assumption: Data points within the same cluster have similar forgetting values.

### Mechanism 2
The method maintains training efficiency by keeping the number of computed gradients constant and compensating for the extra cost of replay selection. For each batch of new task data, the algorithm randomly removes |M(θj)| data points to compensate for the replay data added to the batch.

Core assumption: The computational cost of randomly removing data points from the new task batch is negligible compared to the cost of computing gradients.

### Mechanism 3
The method leverages a non-stationary multi-armed bandit formulation to adaptively select the most useful replay data over time. The algorithm treats each cluster of past data as an arm in a K-armed bandit problem, where the reward is the estimated forgetting of the data points in the cluster.

Core assumption: The forgetting values of data points in a cluster change in a predictable way as model parameters are updated.

## Foundational Learning

- Concept: Multi-armed bandit problem
  - Why needed here: The method uses a K-armed bandit formulation to adaptively select the most useful replay data over time
  - Quick check question: What is the difference between a stationary and a non-stationary bandit problem, and why is the non-stationary case relevant here?

- Concept: Catastrophic forgetting
  - Why needed here: The method aims to reduce catastrophic forgetting by dynamically selecting past data that currently exhibits the highest forgetting
  - Quick check question: What is catastrophic forgetting, and how does it affect the performance of machine learning models in continual learning scenarios?

- Concept: Experience replay
  - Why needed here: The method is a form of experience replay, where past data is stored and replayed during training to mitigate catastrophic forgetting
  - Quick check question: How does experience replay work in the context of continual learning, and what are the advantages and disadvantages of using it?

## Architecture Onboarding

- Component map: Pre-trained model (Vision MAE or Llama) -> Data storage for past tasks -> Cluster assignment for past data -> Forgetting estimation module -> Boltzmann sampling module -> Replay buffer -> Training loop with adaptive memory replay

- Critical path: 1. Preprocess new task data 2. Estimate forgetting for each cluster 3. Sample clusters using Boltzmann distribution 4. Fill replay buffer with sampled data 5. Remove random data points from new task batch 6. Train model on combined batch (new task + replay)

- Design tradeoffs:
  - Cluster granularity: Finer clusters may lead to more accurate forgetting estimates but increase computational overhead
  - Replay buffer size: Larger buffers may improve performance but increase memory usage and computational cost
  - Forgetting estimation frequency: More frequent estimates may lead to better adaptation but increase computational overhead

- Failure signatures:
  - High forgetting on past tasks: Indicates the method is not effectively selecting the most forgotten data points for replay
  - Low performance on new task: Indicates the method is not effectively learning from new task data or too much replay data is being used
  - High computational overhead: Indicates the method is not maintaining training efficiency as intended

- First 3 experiments:
  1. Run the method on a simple vision task (e.g., MNIST) with a small number of tasks and compare performance to naive fine-tuning and iid replay
  2. Vary the cluster granularity and measure the impact on performance and computational overhead
  3. Vary the replay buffer size and measure the impact on performance and memory usage

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of adaptive memory replay scale with the number of tasks in the sequence, particularly for very large task counts? The paper mentions that the number of learned tasks increases but doesn't provide results for very large task counts. Experiments showing performance metrics across task sequences of varying lengths, especially with 10+ tasks, would resolve this.

### Open Question 2
Would more sophisticated clustering techniques beyond simple task-based clustering significantly improve performance? The paper only uses naive task-based clustering and explicitly acknowledges this limitation without exploring alternatives. Experiments comparing adaptive memory replay with different clustering approaches on the same benchmark tasks would resolve this.

### Open Question 3
How sensitive is the adaptive memory replay method to the choice of temperature parameter (t) and forgetting mean update ratio (β)? The paper reports only one set of optimal hyperparameters without exploring the sensitivity or robustness of the method to different parameter choices. Ablation studies showing performance across a range of temperature and β values would resolve this.

## Limitations
- The method relies heavily on the assumption that data points within the same cluster have similar forgetting values, but this is weakly supported
- The clustering strategy of grouping all data from the same task into one cluster is simplistic and may not capture task complexity
- The computational efficiency claim assumes that removing random data points from new task batches has negligible cost, but this is not empirically validated

## Confidence
- High confidence: The basic premise that experience replay can reduce catastrophic forgetting is well-established
- Medium confidence: The specific implementation details and hyperparameter choices appear reasonable but are not fully specified
- Low confidence: The assumption about within-cluster forgetting similarity and the efficiency claims lack direct empirical support

## Next Checks
1. Run the method with varying cluster granularities (e.g., split tasks into multiple clusters based on difficulty or data properties) and measure the impact on performance to validate the within-cluster forgetting similarity assumption.

2. Measure actual training time per epoch with and without the adaptive replay method, accounting for all components (forgetting estimation, sampling, data removal) to verify the claimed efficiency benefits.

3. Implement the method on a simple, well-understood task like MNIST with a small number of tasks and compare performance against naive replay baselines to establish baseline effectiveness before scaling to complex vision and language tasks.