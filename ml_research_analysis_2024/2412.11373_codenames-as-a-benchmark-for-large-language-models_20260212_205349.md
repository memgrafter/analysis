---
ver: rpa2
title: Codenames as a Benchmark for Large Language Models
arxiv_id: '2412.11373'
source_url: https://arxiv.org/abs/2412.11373
tags:
- words
- codenames
- word
- team
- guesser
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Codenames, a popular word-based board game,
  as a benchmark for evaluating Large Language Models (LLMs). The game requires sophisticated
  language understanding, theory of mind, and epistemic reasoning.
---

# Codenames as a Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2412.11373
- Source URL: https://arxiv.org/abs/2412.11373
- Authors: Matthew Stephenson; Matthew Sidji; BenoÃ®t Ronval
- Reference count: 40
- Primary result: LLM agents demonstrate superior generalization and performance in Codenames compared to word-vector approaches, with o1-preview achieving best results

## Executive Summary
This paper introduces Codenames, a word-based board game, as a comprehensive benchmark for evaluating Large Language Models (LLMs). The game requires sophisticated language understanding, theory of mind, and epistemic reasoning, making it ideal for assessing multiple facets of cognitive intelligence. The authors evaluate state-of-the-art LLMs including GPT-4o, Gemini 1.5, Claude 3.5 Sonnet, and Llama 3.1 across different board setups and game versions (single team and two teams). Results demonstrate that while certain LLMs perform better overall, each model exhibits varying emergent behaviors and excels at specific roles. LLM agents show better generalization to different teammates compared to traditional word-vector approaches.

## Method Summary
The authors developed an automated Codenames AI framework to evaluate LLM and word-vector agents in both single-team (cooperative) and two-team (competitive) game versions. The evaluation used 25-word game boards with 100 trials per agent combination. LLM models were tested using standardized prompt templates for codemaster and guesser roles, while word-vector agents used Word2Vec, GloVe, and Combined approaches. Performance metrics included mean score, win-rate, loss percentage, and various behavioral metrics such as average clue number and stopping behavior. The framework allowed systematic comparison of different agent combinations and identification of emergent behaviors.

## Key Results
- o1-preview model achieved the highest overall performance across all evaluation metrics
- LLM agents demonstrated superior generalization when paired with different teammate types compared to word-vector approaches
- Each LLM model exhibited distinct emergent behaviors and excelled at specific roles (codemaster vs. guesser)
- Traditional word-vector agents showed significant performance degradation when paired with different techniques

## Why This Works (Mechanism)
Codenames serves as an effective benchmark because it requires simultaneous execution of multiple cognitive tasks: understanding semantic relationships between words, generating informative clues, predicting teammate behavior, and strategic reasoning about opponent moves. The game's structure naturally surfaces model capabilities in language understanding, theory of mind, and cooperative reasoning. The competitive and cooperative variants reveal different aspects of model intelligence, while the variability in board configurations tests generalization and adaptability.

## Foundational Learning
- **Epistemic reasoning**: Understanding what others know and can infer - needed to generate clues that teammates can interpret but opponents cannot; quick check: evaluate clue generation when opponent word proximity varies
- **Theory of mind**: Modeling teammate's knowledge and reasoning process - essential for creating effective clues and predicting guesses; quick check: analyze performance when paired with different agent types
- **Strategic reasoning**: Balancing clue informativeness against risk of opponent advantage - critical for competitive variants; quick check: compare single-team vs. two-team performance metrics
- **Language understanding**: Identifying semantic, associative, and phonetic relationships between words - fundamental for clue generation and interpretation; quick check: analyze clue quality across different word relationship types
- **Cooperative reasoning**: Coordinating with teammates to achieve shared goals - necessary for effective team play; quick check: evaluate performance across different teammate pairings
- **Risk assessment**: Evaluating trade-offs between clue precision and potential for errors - important for game strategy; quick check: analyze stopping behavior and guess accuracy

## Architecture Onboarding

### Component Map
LLM API -> Prompt Template Engine -> Game State Manager -> Performance Tracker -> Results Analyzer

### Critical Path
Game initialization -> Prompt generation -> LLM response processing -> Game state update -> Performance metric calculation

### Design Tradeoffs
Prompt engineering vs. model capabilities: Well-crafted prompts can significantly improve performance but may mask model limitations. Word-vector approaches vs. LLMs: Traditional methods are faster and more predictable but lack the reasoning capabilities of LLMs.

### Failure Signatures
- LLM generating invalid clues (multiple words, proper nouns, or unrelated terms)
- Guessers misinterpreting clues due to limited contextual understanding
- Word-vector agents failing to interpret LLM-generated clues due to vocabulary limitations
- Performance degradation when pairing agents with different underlying techniques

### First 3 Experiments
1. Single-team evaluation with GPT-4o as codemaster and guesser to establish baseline performance
2. Two-team competitive evaluation with different LLM models to identify role-specific strengths
3. Mixed-agent pairing experiments (LLM + word-vector) to test generalization capabilities

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LLM agents in Codenames compare when playing against human teammates versus other LLM agents? The paper only compares LLM performance against other LLM agents and word-vector agents, not human players. Direct comparison of LLM agents playing Codenames with human players versus other LLM agents, measuring win rates, clue quality, and player satisfaction would resolve this.

### Open Question 2
What specific linguistic reasoning capabilities (synonyms, antonyms, hyponyms, hypernyms) do different LLMs demonstrate most effectively in Codenames gameplay? While the paper notes the potential for such analysis, it doesn't provide detailed linguistic analysis of the specific reasoning patterns used by different LLMs. Detailed linguistic analysis of clue patterns and word associations made by different LLMs, categorized by type of linguistic relationship would resolve this.

### Open Question 3
How does the performance of LLM agents in Codenames vary across different board configurations and word pools? The paper only tests on standard Codenames boards without systematically varying board configurations or word pool characteristics. Controlled experiments testing LLM performance across various board configurations, including boards with specific word relationships, fictional words, or topic-specific word pools would resolve this.

## Limitations
- Game boards sourced from external datasets rather than systematically controlled for word similarity or semantic structure
- Evaluation conducted on publicly available boards, introducing potential selection bias
- Analysis primarily describes observed patterns without establishing underlying mechanisms or testing alternative explanations

## Confidence
- High confidence in relative performance rankings between LLM models and word-vector approaches
- Medium confidence in interpretation of emergent behaviors and role-specific advantages
- Medium confidence in framework's ability to predict real-world cooperative performance

## Next Checks
1. Conduct controlled experiments using systematically generated boards with varying semantic relationships to test whether performance differences persist across different word distributions
2. Implement ablation studies testing individual components of the prompt templates to quantify their impact on model performance
3. Extend evaluation to include mixed human-AI teams to validate the framework's ability to predict real-world cooperative performance