---
ver: rpa2
title: Learning Manipulation Tasks in Dynamic and Shared 3D Spaces
arxiv_id: '2404.17673'
source_url: https://arxiv.org/abs/2404.17673
tags:
- learning
- agent
- dynamic
- robot
- obstacles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning autonomous pick-and-place
  tasks in dynamic and shared 3D workspaces involving multiple collaborative robots
  and human operators. The authors propose a two-stage deep reinforcement learning
  framework, MLDEnv, that leverages a PointNet architecture for 3D scene segmentation
  and a Proximal Policy Optimization (PPO) algorithm for policy learning.
---

# Learning Manipulation Tasks in Dynamic and Shared 3D Spaces

## Quick Facts
- arXiv ID: 2404.17673
- Source URL: https://arxiv.org/abs/2404.17673
- Authors: Hariharan Arunachalam; Marc Hanheide; Sariah Mghames
- Reference count: 14
- Key outcome: Two-stage deep reinforcement learning framework (MLDEnv) with PointNet segmentation and PPO policy learning enables UR10 manipulators to learn pick-and-place tasks in dynamic shared spaces with moving human obstacles.

## Executive Summary
This paper addresses the challenge of learning autonomous pick-and-place tasks in dynamic 3D workspaces shared with human operators. The authors propose a two-stage deep reinforcement learning framework that first segments the environment using PointNet to identify objects, humans, and robots, then trains Proximal Policy Optimization (PPO) agents to perform manipulation tasks while avoiding obstacles. The system is evaluated in Gazebo with two UR10 manipulators navigating a shared workspace with a moving human agent.

## Method Summary
The MLDEnv framework combines 3D semantic segmentation with deep reinforcement learning to enable autonomous manipulation in shared dynamic spaces. PointNet processes depth camera point clouds to semantically segment objects, humans, and robots, while PPO learns individual policies for each UR10 manipulator. The agents receive state information about joint positions and obstacle distances, and are trained to place objects in designated boxes while avoiding collisions. The approach uses separate Markov Decision Processes for each robot rather than joint multi-agent learning.

## Key Results
- The agent positioned farther from the human learns the pick-and-place task more quickly than the agent closer to the human
- Learning performance measured through cumulative reward over 56,000 training timesteps in Gazebo simulation
- PointNet segmentation achieves near-perfect accuracy in identifying objects, humans, and robots in the shared workspace
- The two-stage architecture (PointNet + PPO) enables effective learning without requiring explicit coordination between robots

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage architecture (PointNet segmentation + PPO policy learning) enables effective learning in dynamic shared spaces by separating perception from control.
- Mechanism: PointNet provides robust 3D semantic segmentation of the environment, creating a structured state representation (object, human, robot classes) that PPO can use for safe policy learning. This separation allows perception errors to be isolated from policy learning, improving overall system stability.
- Core assumption: Accurate semantic segmentation is sufficient to create a safe state representation for RL policy learning in shared workspaces.
- Evidence anchors:
  - [abstract] "leverages first a stochastic actor-critic framework to train an agent's policy network, and second, a dynamic 3D Gym environment where both static and dynamic obstacles (e.g. human factors and robot mate) constitute the state space"
  - [section II.C] "We train a 3D point cloud-based segmentation framework, the PointNet [12], on a custom dataset to perform semantic segmentation of the scene"
  - [corpus] Weak evidence - no corpus papers directly address this specific two-stage architecture combination
- Break condition: If PointNet segmentation accuracy drops below ~0.8, the state representation becomes unreliable and policy learning performance degrades significantly.

### Mechanism 2
- Claim: PPO's advantage over optimization-based methods in this context stems from its ability to handle unknown dynamics and adapt through exploration.
- Mechanism: PPO learns a stochastic policy that can adapt to dynamic obstacles (moving humans) through exploration, while optimization methods require accurate system models that are difficult to obtain in shared dynamic environments.
- Core assumption: The environment dynamics are sufficiently complex that model-based optimization methods would struggle with convergence and adaptability.
- Evidence anchors:
  - [abstract] "The ability of a reinforcement learning (RL) framework to adapt the learned task to new and unknown dynamics in an end-to-end exploration fashion makes it an efficient approach"
  - [section II.A] "Implementation of conventional motion planners is not suitable for dynamic scenarios where real-time computational cost is high and convergence speed is of paramount importance"
  - [corpus] Moderate evidence - several related papers mention RL advantages in dynamic environments, but none directly compare to optimization methods in this specific setup
- Break condition: If the dynamic obstacle patterns become too complex or unpredictable, PPO may require prohibitively long training times or fail to converge.

### Mechanism 3
- Claim: Training separate MDPs for each robot (rather than a joint multi-agent approach) simplifies the learning problem while maintaining safety through individual obstacle avoidance.
- Mechanism: Each robot learns its own policy based on its local state (joint positions + obstacle distances), treating the other robot as a dynamic obstacle. This avoids the exponential state space complexity of joint multi-agent learning while still achieving collaborative behavior through shared workspace constraints.
- Core assumption: Individual obstacle avoidance policies are sufficient to achieve effective collaboration without explicit coordination.
- Evidence anchors:
  - [section II.D] "We model each robot with a separate MDP. Hence, in this work, we train two RL agents"
  - [section II.D] "The states of a single process are a combination of one agent or manipulator's joint positions and the surface point coordinates extracted from the clustered obstacles"
  - [corpus] Strong evidence - related paper [10] mentions limitations of coordinated approaches and this paper's approach addresses shared workspace awareness
- Break condition: If the robots need to perform coordinated actions (e.g., passing objects between them), the individual MDP approach may fail to learn effective coordination strategies.

## Foundational Learning

- Concept: 3D point cloud processing and segmentation
  - Why needed here: The environment state is represented as 3D point clouds from a depth camera, requiring semantic segmentation to identify obstacles, objects, and robots for safe navigation
  - Quick check question: How does PointNet architecture process unordered point clouds to produce consistent segmentation results?

- Concept: Markov Decision Processes and reinforcement learning
  - Why needed here: The pick-and-place task is modeled as an MDP where agents learn optimal policies through interaction with the dynamic environment, receiving rewards for task completion and obstacle avoidance
  - Quick check question: What are the state, action, and reward components in this MDP formulation, and how do they capture the shared workspace dynamics?

- Concept: Proximal Policy Optimization algorithm
  - Why needed here: PPO provides stable policy learning with good sample efficiency for this continuous control problem, handling the variable number of obstacles through LSTM embedding
  - Quick check question: How does PPO's clipped objective function maintain training stability compared to vanilla policy gradient methods?

## Architecture Onboarding

- Component map: Gazebo simulator -> PointNet segmentation -> DBSCAN clustering -> State representation -> PPO training -> Policy execution -> Gazebo feedback

- Critical path: Gazebo → PointNet segmentation → DBSCAN clustering → State representation → PPO training → Policy execution → Gazebo feedback

- Design tradeoffs:
  - Separate MDPs vs joint multi-agent learning: Simpler learning problem but potentially suboptimal coordination
  - PointNet vs more modern 3D segmentation: PointNet provides sufficient accuracy with simpler implementation
  - PPO vs other RL algorithms: Good balance of stability and sample efficiency for this continuous control problem

- Failure signatures:
  - PointNet segmentation accuracy below 0.8 indicates perception issues
  - PPO policy collapse (high variance or low reward) suggests learning instability
  - Distance to obstacles decreasing below safety threshold indicates control failure

- First 3 experiments:
  1. Train PointNet on custom dataset and verify segmentation accuracy exceeds 0.9 on validation set
  2. Implement PPO with a single robot in a static environment to verify basic policy learning works
  3. Add dynamic obstacle (moving human) and verify the agent learns to avoid while maintaining task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the PointNet segmentation accuracy translate to downstream task performance, and what is the quantitative impact of segmentation errors on the RL agent's learning efficiency?
- Basis in paper: [explicit] The paper mentions PointNet is trained to segment objects, humans, and robots with near-perfect accuracy, but does not report the impact of segmentation errors on RL performance or task success rate.
- Why unresolved: The authors do not report how segmentation errors affect the RL reward structure, obstacle avoidance, or policy convergence.
- What evidence would resolve it: Quantitative ablation study comparing RL performance with perfect vs. noisy segmentation, including success rate and cumulative reward degradation metrics.

### Open Question 2
- Question: What is the effect of the dynamic human movement pattern on the RL agent's policy convergence, and how does it compare to static obstacles or different human trajectories?
- Basis in paper: [explicit] The paper uses a human moving along predefined linear paths at varying speeds as a dynamic obstacle, but does not compare this to static obstacles or other movement patterns.
- Why unresolved: The paper does not analyze how different human movement patterns (e.g., circular, random, or stationary) affect the learning curve or policy robustness.
- What evidence would resolve it: Comparative experiments with different human movement patterns and their impact on RL convergence speed and policy generalization.

### Open Question 3
- Question: How does the reward function design, particularly the choice of weights (w1, w2, w3, w4) and thresholds (l1, l2), influence the trade-off between obstacle avoidance and task completion speed?
- Basis in paper: [explicit] The reward function uses weighted distances to obstacles and thresholds for safety, but the paper does not explore the sensitivity of these parameters or their impact on task performance.
- Why unresolved: The authors do not provide a sensitivity analysis of the reward function parameters or their effect on the agent's behavior (e.g., overly cautious vs. aggressive).
- What evidence would resolve it: Parameter sweep experiments showing the effect of varying weights and thresholds on task completion time, obstacle collisions, and cumulative reward.

## Limitations

- The paper does not provide quantitative performance metrics beyond cumulative reward curves, making it difficult to assess practical effectiveness
- No ablation studies are presented to isolate the contribution of individual components (PointNet vs PPO vs MDP formulation)
- The open-sourced code framework is mentioned but not directly linked, limiting reproducibility verification

## Confidence

- High confidence in the technical feasibility of the two-stage architecture combining PointNet segmentation with PPO policy learning
- Medium confidence in the claim that separate MDPs for each robot provide sufficient collaboration without explicit coordination
- Low confidence in the generalizability of results, as experiments are limited to a specific Gazebo simulation setup

## Next Checks

1. Implement the complete system and verify the cumulative reward curves match those reported in the paper across multiple random seeds
2. Conduct ablation studies removing PointNet segmentation or using alternative RL algorithms to quantify individual component contributions
3. Test the trained policies in more complex scenarios with multiple moving humans and varying obstacle configurations to assess robustness