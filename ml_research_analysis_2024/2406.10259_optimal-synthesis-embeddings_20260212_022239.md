---
ver: rpa2
title: Optimal synthesis embeddings
arxiv_id: '2406.10259'
source_url: https://arxiv.org/abs/2406.10259
tags:
- word
- problem
- sentence
- embedding
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for creating word embeddings
  that are equidistant from a set of given word vectors, termed Optimal Synthesis
  Embeddings (OSE). The method is based on the idea that a fair embedding representation
  for a set of words should be at the same distance from each of its constituent vectors,
  and this distance should be minimized.
---

# Optimal synthesis embeddings

## Quick Facts
- arXiv ID: 2406.10259
- Source URL: https://arxiv.org/abs/2406.10259
- Authors: Roberto Santana; Mauricio Romero Sicre
- Reference count: 27
- One-line primary result: OSE outperforms BOV in 30/50 probing tasks and 7/9 sentence classification problems

## Executive Summary
This paper introduces Optimal Synthesis Embeddings (OSE), a method for creating word embeddings that are equidistant from a set of given word vectors. The authors theoretically characterize the conditions for the existence of such representations and derive the solution, which involves projecting the normalized target vector onto the orthogonal complement of the span of difference vectors. The method is evaluated in data augmentation and sentence classification tasks, showing promising results compared to previous methods like Bag of Vectors (BOV).

## Method Summary
OSE computes a new word vector that is equidistant from a given set of word vectors by finding a non-zero vector in the orthogonal complement of the span of normalized difference vectors. The method minimizes the cosine distance to the original vectors among all equidistant vectors. OSE can be applied to both static and contextualized word representations and can create representations of sentences and sets of words that are not necessarily organized as a sequence. The method is evaluated using data augmentation, where OSE representations are added as new examples of a class, and sentence classification tasks, where OSE is used to create sentence embeddings from word embeddings.

## Key Results
- OSE outperforms BOV in 30 out of 50 probing tasks designed to capture simple linguistic features
- OSE produces the best absolute results in 7 out of 9 sentence classification problems when using a multi-layer perceptron classifier
- OSE excels in solving semantic similarity correlation tasks, outperforming BOV in 5 out of 10 problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimal synthesis embedding (OSE) is found by projecting the normalized target vector onto the orthogonal complement of the span of difference vectors.
- Mechanism: For a set of N non-zero vectors, the problem of finding a vector equidistant from all of them reduces to finding a non-zero vector in the orthogonal complement of the span of the normalized difference vectors. The OSE is the normalized projection of the normalized target vector onto this orthogonal complement.
- Core assumption: The distance function used (cosine similarity) is symmetric and zero only when normalized vectors are equal.
- Evidence anchors:
  - [abstract] "a fair embedding representation for a given set of words should satisfy that the new vector will be at the same distance of the vector representation of each of its constituents, and this distance should be minimized."
  - [section 4.3.1] "The embedding composition method can work with static and contextualized word representations, it can be applied to create representations of sentences and learn also representations of sets of words that are not necessarily organized as a sequence."
  - [corpus] Weak evidence; no direct mention of orthogonal projection or cosine similarity in neighbors.
- Break condition: If the span of the difference vectors has full dimension (equals n), then the orthogonal complement is empty, and no equidistant vector exists.

### Mechanism 2
- Claim: The OSE minimizes the distance to the original vectors among all equidistant vectors.
- Mechanism: The problem of finding the closest equidistant vector is formulated as minimizing the distance to the target vector subject to the equidistance constraints. The solution is unique and given by the normalized projection of the normalized target vector onto the orthogonal complement.
- Core assumption: The objective function is continuous and the feasible set (the unit sphere intersected with the orthogonal complement) is compact.
- Evidence anchors:
  - [abstract] "a fair embedding representation for a given set of words should satisfy that the new vector will be at the same distance of the vector representation of each of its constituents, and this distance should be minimized."
  - [section 4.3.1] "we also aim for a vector that encodes a meaning as close as possible to the ones of the original words."
  - [corpus] No direct evidence; this is a theoretical result from the paper.
- Break condition: If the orthogonal complement is empty (span of difference vectors has full dimension), then no equidistant vector exists, and the problem is infeasible.

### Mechanism 3
- Claim: OSE can be used for data augmentation by creating a synthetic word vector that is equidistant from a set of word vectors in the same class.
- Mechanism: The method selects k random words from the same class, constructs their OSE representation, and adds it as a new example of the class. The rationale is that a word vector equidistant from other k instances of the class is a good exemplar of the class.
- Core assumption: Words in the same class have similar semantic meanings, and an equidistant representation captures the class's semantic attributes.
- Evidence anchors:
  - [abstract] "We propose the use of OSE as a way to generate new examples from a given class."
  - [section 5.1] "The method, that we call OSE-Augmentation will take k random words of the same class from the train set, and construct the OSE representation, which is then added as a new example of the class."
  - [corpus] No direct evidence; this is an application of the OSE method.
- Break condition: If the k selected words are too dissimilar or the class is too heterogeneous, the OSE representation may not be a good exemplar of the class.

## Foundational Learning

- Concept: Cosine similarity and its relationship to Euclidean distance.
  - Why needed here: The OSE method is based on minimizing the cosine distance between the new vector and the original vectors.
  - Quick check question: Given two normalized vectors, what is the relationship between their cosine similarity and Euclidean distance?

- Concept: Orthogonal projection and orthogonal complement.
  - Why needed here: The OSE is found by projecting the normalized target vector onto the orthogonal complement of the span of difference vectors.
  - Quick check question: Given a vector and a subspace, how do you find the projection of the vector onto the orthogonal complement of the subspace?

- Concept: Constrained optimization and Lagrange multipliers.
  - Why needed here: The problem of finding the closest equidistant vector is formulated as a constrained optimization problem.
  - Quick check question: Given an objective function and equality constraints, how do you use Lagrange multipliers to find the optimal solution?

## Architecture Onboarding

- Component map:
  - OSE computation: Takes a set of word vectors and returns their OSE representation
  - Data augmentation: Uses OSE to generate new examples for a given class
  - Sentence representation: Uses OSE to create a sentence embedding from the word embeddings in the sentence
  - Classification: Uses the OSE representations for data augmentation or sentence classification

- Critical path:
  1. Compute OSE representation for a set of word vectors
  2. Use OSE for data augmentation or sentence classification
  3. Evaluate the performance of the OSE-based method

- Design tradeoffs:
  - Using OSE for data augmentation may improve classification performance but may also introduce noise if the selected words are too dissimilar
  - Using OSE for sentence representation assumes that all words in the sentence are equally relevant, which may not be true in practice

- Failure signatures:
  - If the span of the difference vectors has full dimension, the OSE computation will fail
  - If the selected words for data augmentation are too dissimilar, the augmented examples may not be good exemplars of the class
  - If the words in a sentence have very different semantic meanings, the OSE sentence representation may not capture the sentence's semantics well

- First 3 experiments:
  1. Implement the OSE computation and test it on a small set of word vectors
  2. Use OSE for data augmentation on a simple classification problem and compare the performance with the baseline
  3. Use OSE for sentence representation on a sentence classification task and compare the performance with the Bag of Vectors (BOV) method

## Open Questions the Paper Calls Out
- Question: How sensitive is the OSE method to the choice of the number of words (k) used for data augmentation, and what is the optimal k value across different classification tasks and embedding types?
  - Basis in paper: [inferred] The paper mentions that the number of words k is a parameter of the OSE-Augmentation method, and that results are presented for different k values (2, 5, 10, 15, 20). However, the optimal k value is not explicitly determined or discussed.
  - Why unresolved: The paper only provides results for different k values without determining the optimal value or discussing its impact on the method's performance. The optimal k value might depend on the specific task, dataset, and embedding type, and further investigation is needed to establish general guidelines.
  - What evidence would resolve it: A systematic analysis of the method's performance across a wide range of tasks, datasets, and embedding types, with varying k values, could help identify the optimal k value or a range of values that consistently yield good results. Additionally, theoretical analysis of the method's behavior with respect to k could provide insights into its sensitivity to this parameter.

- Question: How does the OSE method compare to other sentence embedding methods, such as those based on attention mechanisms or transformers, in terms of capturing complex semantic relationships and handling long-range dependencies?
  - Basis in paper: [inferred] The paper compares the OSE method to the Bag of Vectors (BOV) method and shows that OSE outperforms BOV on probing tasks designed to capture simple linguistic features. However, the paper does not compare OSE to more advanced sentence embedding methods, such as those based on attention mechanisms or transformers, which are known to be effective at capturing complex semantic relationships and handling long-range dependencies.
  - Why unresolved: The paper only provides a limited comparison of the OSE method to a simple baseline (BOV) and does not explore its performance relative to more advanced sentence embedding methods. This leaves open the question of whether OSE can compete with or complement these methods in terms of capturing complex semantic relationships and handling long-range dependencies.
  - What evidence would resolve it: A comprehensive evaluation of the OSE method against a wide range of sentence embedding methods, including those based on attention mechanisms and transformers, on tasks that require capturing complex semantic relationships and handling long-range dependencies, would provide insights into its relative performance and potential applications.

- Question: How can the OSE method be extended to handle multi-modal data, such as text and images, or to incorporate additional information, such as syntactic or semantic dependencies, to improve its performance on downstream tasks?
  - Basis in paper: [inferred] The paper focuses on the OSE method for creating word and sentence embeddings from textual data. However, it does not explore the potential of extending the method to handle multi-modal data or incorporate additional information, such as syntactic or semantic dependencies, which could potentially improve its performance on downstream tasks.
  - Why unresolved: The paper presents the OSE method in the context of text-based embeddings and does not discuss its potential for extension to multi-modal data or incorporation of additional information. This leaves open the question of whether and how the method can be adapted to handle these more complex scenarios.
  - What evidence would resolve it: Research exploring the extension of the OSE method to handle multi-modal data, such as text and images, or to incorporate additional information, such as syntactic or semantic dependencies, would provide insights into its potential for broader applications and improved performance on downstream tasks.

## Limitations
- The OSE method depends on the existence of an orthogonal complement for the span of difference vectors, which may not always exist when the span has full dimension.
- The method assumes that equidistance under cosine similarity is a meaningful criterion for semantic representation, which may not hold for all semantic relationships or languages.
- The paper provides limited empirical validation across diverse linguistic phenomena, focusing mainly on English datasets and probing tasks designed to capture simple linguistic features.

## Confidence
- **High confidence**: The mathematical derivation of OSE conditions and the projection-based solution for finding equidistant vectors.
- **Medium confidence**: The empirical evaluation showing OSE outperforms BOV in probing tasks and sentence classification.
- **Low confidence**: The assumption that equidistance implies semantic representativeness for data augmentation.

## Next Checks
1. **Edge case validation**: Systematically test OSE computation on synthetic word vector sets where the span of difference vectors approaches full dimension, quantifying the failure rate and measuring how close the method can get to equidistance in near-degenerate cases.
2. **Semantic fidelity check**: Create controlled experiments where word sets have known semantic relationships (synonyms, antonyms, hypernyms) and verify whether OSE equidistance preserves these relationships better than alternatives like BOV or weighted averaging.
3. **Cross-linguistic validation**: Apply OSE to non-English word embedding spaces (e.g., multilingual BERT, language-specific embeddings) to test whether the equidistance criterion holds cross-linguistically or if it's an artifact of Indo-European language structures.