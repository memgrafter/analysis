---
ver: rpa2
title: Towards Federated Learning with On-device Training and Communication in 8-bit
  Floating Point
arxiv_id: '2407.02610'
source_url: https://arxiv.org/abs/2407.02610
tags:
- quantization
- training
- qrand
- communication
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates 8-bit floating point (FP8) quantization
  in federated learning to reduce communication costs while maintaining model accuracy.
  The authors propose a method combining FP8 client training with a global FP32 server
  model, using stochastic quantization for communication and deterministic quantization
  during local training.
---

# Towards Federated Learning with On-device Training and Communication in 8-bit Floating Point

## Quick Facts
- arXiv ID: 2407.02610
- Source URL: https://arxiv.org/abs/2407.02610
- Authors: Bokun Wang; Axel Berg; Durmus Alp Emre Acar; Chuteng Zhou
- Reference count: 40
- Primary result: Achieves 2.9x communication reduction with maintained accuracy in federated learning

## Executive Summary
This work proposes a novel approach to federated learning using 8-bit floating point (FP8) quantization to significantly reduce communication costs while maintaining model accuracy. The method combines FP8 client training with a global FP32 server model, using stochastic quantization for communication and deterministic quantization during local training. An optional server-side optimization step minimizes mean squared error during model aggregation. The approach is validated through theoretical convergence analysis and empirical experiments on image classification and keyword spotting tasks, demonstrating consistent communication reductions of at least 2.9x compared to FP32 baseline while achieving similar accuracy.

## Method Summary
The proposed method implements a hybrid quantization strategy in federated learning, where clients perform local training using 8-bit floating point representation with deterministic quantization, while communication between clients and server employs stochastic quantization to maintain unbiased gradients. The server maintains a full-precision (FP32) global model and aggregates updates from clients using weighted averaging. An optional server-side optimization step can be applied to minimize the mean squared error between the quantized and full-precision models during aggregation. The approach balances the computational efficiency of low-precision training with the accuracy benefits of higher-precision global model maintenance.

## Key Results
- Achieves at least 2.9x reduction in communication compared to FP32 baseline
- Maintains similar model accuracy to full-precision federated learning
- Demonstrates effectiveness across multiple tasks including image classification and keyword spotting
- Provides theoretical convergence guarantees under certain assumptions

## Why This Works (Mechanism)
The effectiveness stems from combining the computational efficiency of FP8 during local training with the accuracy benefits of FP32 for global model maintenance. Stochastic quantization during communication ensures unbiased gradient updates, while deterministic quantization during training maintains computational efficiency. The optional server-side optimization further refines the aggregation process to minimize quantization errors.

## Foundational Learning
1. **Federated Learning**: Decentralized training where multiple clients collaborate without sharing raw data
   - Why needed: Framework for distributed model training while preserving privacy
   - Quick check: Understand client-server communication pattern

2. **Quantization**: Process of reducing numerical precision of model parameters
   - Why needed: Reduces memory footprint and communication bandwidth
   - Quick check: Compare FP32 vs FP8 storage requirements

3. **Stochastic vs Deterministic Quantization**: Different approaches to converting continuous values to discrete representations
   - Why needed: Affects bias and variance in gradient updates
   - Quick check: Understand impact on convergence properties

## Architecture Onboarding

**Component Map**: Clients -> Server -> Global Model
- Clients perform FP8 training with deterministic quantization
- Communication uses stochastic quantization
- Server maintains FP32 model and performs weighted aggregation

**Critical Path**: Local training → Stochastic quantization → Server aggregation → Global model update

**Design Tradeoffs**: 
- FP8 for efficiency vs FP32 for accuracy
- Stochastic quantization for unbiased updates vs deterministic for computational simplicity
- Optional server optimization for accuracy vs computational overhead

**Failure Signatures**:
- Accuracy degradation indicates quantization errors
- Communication overhead suggests inefficient quantization scheme
- Convergence issues may stem from biased gradient updates

**First Experiments**:
1. Compare FP8 client training with FP32 baseline on simple dataset
2. Test stochastic vs deterministic quantization impact on communication
3. Evaluate server-side optimization effectiveness on aggregation quality

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to larger models and more complex tasks unverified
- Computational overhead of server-side optimization not fully characterized
- Convergence analysis assumes ideal conditions that may not hold in practice
- Security implications of lower-precision communication not addressed

## Confidence

| Claim | Confidence |
|-------|------------|
| 2.9x communication reduction while maintaining accuracy | High |
| Theoretical convergence guarantees | Medium |
| Server-side optimization effectiveness | Low-Medium |

## Next Checks
1. Test the proposed approach on larger models (e.g., ResNet-50 or transformer-based architectures) to verify scalability and identify potential bottlenecks.
2. Conduct experiments with highly non-IID data distributions to evaluate the robustness of the quantization scheme under realistic federated learning conditions.
3. Perform a comprehensive security analysis to assess the vulnerability of 8-bit communication to adversarial attacks and propose mitigation strategies if necessary.