---
ver: rpa2
title: Reinforcement Learning from Delayed Observations via World Models
arxiv_id: '2403.12309'
source_url: https://arxiv.org/abs/2403.12309
tags:
- learning
- world
- delayed
- state
- delay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reinforcement learning under
  observation delays in partially observable environments. The core idea is to leverage
  world models, specifically Dreamer, to handle these delays by reducing delayed POMDPs
  to delayed MDPs in latent space.
---

# Reinforcement Learning from Delayed Observations via World Models

## Quick Facts
- arXiv ID: 2403.12309
- Source URL: https://arxiv.org/abs/2403.12309
- Authors: Armin Karamzade; Kyungmin Kim; Montek Kalsi; Roy Fox
- Reference count: 40
- Primary result: World models can handle observation delays by reducing delayed POMDPs to delayed MDPs in latent space

## Executive Summary
This paper addresses reinforcement learning under observation delays in partially observable environments by leveraging world models, specifically adapting the Dreamer architecture. The key insight is that world models can transform delayed partially observable Markov decision processes (POMDPs) into delayed Markov decision processes (MDPs) in latent space, where delays can be more effectively managed. Two main strategies are proposed: an Extended actor that conditions policy on an extended state including past actions, and a Latent actor that predicts the current latent state through imagination. The methods are evaluated on both vector and visual control tasks with delays ranging from 2 to 20 timesteps, demonstrating significant performance improvements over baseline approaches.

## Method Summary
The approach builds on Dreamer-V3 by modifying how delayed observations are handled. The world model is trained on undelayed data but stores subsequent actions in the experience replay buffer. Two main variants are introduced: the Extended actor, which conditions the policy on an extended state (delayed latent state plus action buffer), and the Latent actor, which predicts the current latent state by imagining forward d steps from the delayed latent state. A simpler Memoryless actor variant is also evaluated. The methods reduce delayed POMDPs to delayed MDPs in latent space by leveraging the congruence between interaction and imagination modes of the world model. The architecture modifications are minimal compared to the underlying Dreamer framework, requiring only changes to the policy input and imagination process.

## Key Results
- Extended and Latent actor variants significantly outperform naive approaches, with up to 250% improvement in policy value
- Methods demonstrate robustness to partial observability in visual delayed environments
- The delay-agnostic approach performs well without requiring knowledge of delay distribution
- Performance scales with delay length but shows degradation for very long delays (20 timesteps)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: World models can reduce delayed POMDPs to delayed MDPs in latent space by leveraging congruence between interaction and imagination modes.
- Mechanism: By defining a world model congruent with a POMDP, delayed observations can be handled by shifting the delay into the latent state update process. The model uses past observations to build a Markov latent state, and delayed actions are fed into the imagination mode with the delayed latent state.
- Core assumption: The world model's imagination and interaction modes are equivalent (congruent), and the latent state maintains Markov properties despite delayed observations.
- Evidence anchors: [abstract], [section 4.1] Proposition 3, [corpus]

### Mechanism 2
- Claim: Extended actor approach handles delays by conditioning policy on extended state (delayed latent state + action buffer).
- Mechanism: Instead of predicting the current state through imagination, the policy directly uses the extended state (mt-d, at-d, ..., at-1) as input. This provides all necessary information without requiring accurate state prediction through multiple imagination steps.
- Core assumption: The extended state contains sufficient information to make optimal decisions despite the delay.
- Evidence anchors: [section 4.3], [abstract], [corpus]

### Mechanism 3
- Claim: Latent actor approach handles delays by predicting the current latent state through imagination.
- Mechanism: The agent uses the world model's forward dynamics to imagine forward d steps from the delayed latent state mt-d to estimate the current latent state. Actions are then conditioned on this predicted state.
- Core assumption: The one-step prediction world model can accurately predict the current latent state through multiple imagination steps.
- Evidence anchors: [section 4.4], [abstract], [corpus]

## Foundational Learning

- Concept: POMDPs and DMDPs
  - Why needed here: The paper addresses delays in partially observable environments, requiring understanding of how delays transform POMDPs into structured POMDPs (delayed POMDPs) and how these can be reduced to MDPs.
  - Quick check question: What is the key difference between a POMDP and a DMDP in the context of delays?

- Concept: World models and latent state representations
  - Why needed here: The proposed methods rely on world models that learn latent state representations to handle delays. Understanding how these models compress observations and learn dynamics is crucial.
  - Quick check question: How does a world model's latent state differ from the raw observation space in terms of Markov properties?

- Concept: Actor-critic reinforcement learning
  - Why needed here: The paper adapts Dreamer's actor-critic framework to handle delays. Understanding the standard actor-critic approach is necessary to grasp the modifications proposed.
  - Quick check question: In standard actor-critic methods, what roles do the actor and critic play, and how are they typically trained?

## Architecture Onboarding

- Component map: World model (encoder-decoder + RSSM) -> Experience replay buffer (with action sequences) -> Extended/Latent actor (with modified policy input) -> Critic network
- Critical path: 1) Collect delayed observations and actions in replay buffer, 2) Train world model on undelayed data, 3) For Extended actor: use extended state (delayed latent + action buffer) as policy input, 4) For Latent actor: imagine forward d steps from delayed latent state to predict current state, 5) Train actor-critic using imagined trajectories
- Design tradeoffs: Extended actor is more robust but adds architectural complexity and memory requirements. Latent actor is simpler but suffers from error accumulation in imagination. Memoryless actor is simplest but may lose information.
- Failure signatures: Extended actor: poor performance with long delays due to large action buffers. Latent actor: performance degradation as delay increases due to accumulated prediction errors. Both: poor performance if world model fails to learn good latent representation.
- First 3 experiments:
  1. Run Dreamer-V3 on delayed version of a simple Gym environment (e.g., HalfCheetah-v4) with various delay values to establish baseline performance.
  2. Implement and test Extended actor variant on the same environment to verify improved performance over baseline.
  3. Implement and test Latent actor variant on the same environment to compare with Extended actor performance and identify error accumulation issues.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of extended actors scale with different action buffer sizes (d) and what is the optimal trade-off between buffer size and prediction accuracy?
- Basis in paper: [explicit] The paper discusses extended actors using action sequences and mentions the curse of dimensionality as the delay increases.
- Why unresolved: The paper only experiments with a fixed delay (d) for each task and does not explore varying buffer sizes systematically.
- What evidence would resolve it: Systematic experiments varying d and measuring performance, with a focus on the trade-off between buffer size and accuracy.

### Open Question 2
- Question: How does the performance of latent actors degrade with longer delays and what is the fundamental limit to their effectiveness?
- Basis in paper: [explicit] The paper notes that latent actors suffer from error accumulation over longer delays.
- Why unresolved: The paper does not provide a theoretical analysis of the error accumulation or its fundamental limits.
- What evidence would resolve it: Theoretical analysis of error propagation in latent actors, combined with experimental results for very long delays.

### Open Question 3
- Question: How does the performance of the delay-agnostic method compare to methods that know the delay distribution in advance, especially for complex and variable delay distributions?
- Basis in paper: [explicit] The paper highlights that the delay-agnostic method does not need to know the delay distribution beforehand and compares its performance to other methods.
- Why unresolved: The paper does not explore scenarios with complex or variable delay distributions.
- What evidence would resolve it: Experiments with varying and complex delay distributions, comparing the delay-agnostic method to methods that know the distribution in advance.

## Limitations

- Theoretical assumptions about world model congruence may not hold in practice, particularly for visual observations with long delays
- Limited evaluation of failure modes at higher delay values (>20 timesteps)
- Comparison with specialized delay-handling methods may not be entirely fair due to different optimization targets

## Confidence

- **High Confidence**: The empirical demonstration that both Extended and Latent actor variants outperform baseline approaches on standard control tasks with delays of 2-20 timesteps
- **Medium Confidence**: The theoretical claims about world model congruence and reduction of delayed POMDPs to MDPs in latent space
- **Low Confidence**: The claim of state-of-the-art performance in visual delayed environments, as this is a novel setting with limited baseline comparisons

## Next Checks

1. **Prediction Error Analysis**: Measure and report how prediction error in the latent state accumulates over multiple imagination steps for different delay values, particularly for visual observations, to quantify the practical limits of the Latent actor approach.

2. **Robustness to Delay Length**: Systematically evaluate performance across a wider range of delay values (including delays > 20 timesteps) to identify the breaking points for both Extended and Latent actor variants and understand their respective scalability limitations.

3. **Comparison with Specialized Methods**: Implement and evaluate the proposed methods against more recent specialized delay-handling approaches on both visual and non-visual tasks to establish a stronger empirical baseline for delayed reinforcement learning performance.