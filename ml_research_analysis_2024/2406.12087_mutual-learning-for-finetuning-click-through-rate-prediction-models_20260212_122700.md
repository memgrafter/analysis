---
ver: rpa2
title: Mutual Learning for Finetuning Click-Through Rate Prediction Models
arxiv_id: '2406.12087'
source_url: https://arxiv.org/abs/2406.12087
tags:
- mutual
- learning
- knowledge
- network
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates mutual learning as a finetuning strategy
  for click-through rate (CTR) prediction models. While knowledge distillation is
  commonly used for model compression, the authors argue that mutual learning is more
  suitable for CTR models, which typically have similar architectural complexity and
  performance levels.
---

# Mutual Learning for Finetuning Click-Through Rate Prediction Models

## Quick Facts
- **arXiv ID**: 2406.12087
- **Source URL**: https://arxiv.org/abs/2406.12087
- **Reference count**: 13
- **Key result**: Mutual learning applied to pretrained CTR models yields 0.25-0.66% relative improvement in AUC scores, while mutual learning from scratch fails to improve performance

## Executive Summary
This paper investigates mutual learning as a finetuning strategy for click-through rate (CTR) prediction models. While knowledge distillation is commonly used for model compression, the authors argue that mutual learning is more suitable for CTR models, which typically have similar architectural complexity and performance levels. The proposed method involves training multiple CTR models simultaneously, where each model's loss function includes both supervised learning loss and mutual learning loss computed as mean squared error between the models' predictions. The approach is tested on four CTR models (FiBiNET, DCN, PNN, and DeepFM) using Criteo and Avazu datasets. Key findings include that mutual learning from scratch does not improve performance, but when applied to pretrained models it yields 0.25-0.66% relative improvement in AUC scores. The improvement is greater when models have higher initial performance scores, and using four models in mutual learning provides optimal results with diminishing returns for more models.

## Method Summary
The method involves training multiple CTR models simultaneously using both supervised loss (binary cross-entropy) and mutual learning loss (mean squared error between model predictions). Four CTR models (FiBiNET, DCN, PNN, DeepFM) are trained on Criteo and Avazu datasets with 80%/10%/10% train/dev/test splits. The models use embedding size 16, batch size 4000, and are pretrained independently first. Mutual learning is then applied by fine-tuning these pretrained models together with the combined loss function. The study tests configurations with 2, 3, 4, and 5 model instances to determine the optimal number for mutual learning.

## Key Results
- Mutual learning from scratch fails to improve CTR model performance
- Pretrained models benefit from mutual learning with 0.25-0.66% relative improvement in AUC scores
- Using four models in mutual learning provides optimal results, with diminishing returns beyond this number
- Improvement magnitude correlates with initial model performance levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mutual learning from scratch fails because it constrains models from learning unique representations, preventing the emergence of diverse "expert" knowledge.
- Mechanism: When models are trained independently, random initialization and stochastic training dynamics cause each model to learn distinct feature representations. Mutual learning from scratch forces all models to align early, erasing these differences and preventing the accumulation of diverse knowledge that can later be transferred.
- Core assumption: Random initialization + training noise creates sufficient model diversity for mutual learning to be beneficial.
- Evidence anchors:
  - [abstract] "mutual learning from scratch does not improve performance"
  - [section] "Training from scratch mutually prevents instances from learning uniquely, because it constrains what instances learn distilling the knowledge from the very beginning of training"
  - [corpus] Weak - no direct supporting papers in neighbor set
- Break condition: If model architectures are highly constrained or regularization is so strong that all models converge to similar representations even when trained independently.

### Mechanism 2
- Claim: Pretrained models benefit from mutual learning because each model instance has learned unique knowledge during independent training, which can be transferred to other models.
- Mechanism: Each independently trained model captures different aspects of the data distribution. When these pretrained models are fine-tuned together, they exchange their unique insights through mutual loss, leading to performance improvements beyond what any single model could achieve alone.
- Core assumption: Different training runs on the same architecture produce meaningfully different representations.
- Evidence anchors:
  - [section] "each model instance learns in a unique way, depending on the initialization and randomness"
  - [section] "Distilling their unique knowledge to each other creates a combination of experts"
  - [corpus] Weak - no direct supporting papers in neighbor set
- Break condition: If models are regularized to the point where different training runs produce nearly identical representations.

### Mechanism 3
- Claim: The optimal number of models for mutual learning is four, beyond which performance plateaus.
- Mechanism: Each additional model contributes diminishing returns to the collective knowledge transfer. With four models, sufficient diversity is achieved while avoiding excessive computational cost and potential negative interference from too many conflicting signals.
- Core assumption: Each model contributes unique but overlapping knowledge, and four models capture most of the available diversity.
- Evidence anchors:
  - [section] "Using four models in mutual learning provides optimal results, with diminishing returns for more models"
  - [section] "we increased the number of instances to five, which, on average, almost did not make any impact on the results"
  - [corpus] Weak - no direct supporting papers in neighbor set
- Break condition: If model architectures are fundamentally different enough that more than four models continue to provide meaningful diversity.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Understanding the teacher-student framework helps contrast with mutual learning's collaborative approach
  - Quick check question: What's the key difference between knowledge distillation and mutual learning in terms of model relationships?

- Concept: Feature Interaction Modeling
  - Why needed here: CTR models rely heavily on capturing feature interactions, which is central to their architecture and performance
  - Quick check question: How do different CTR models (FM, DeepFM, DCN) approach feature interaction modeling differently?

- Concept: AUC and Relative Improvement Metrics
  - Why needed here: These are the primary evaluation metrics used to assess model performance improvements
  - Quick check question: Why is AUC preferred over accuracy for imbalanced CTR prediction tasks?

## Architecture Onboarding

- Component map:
  Embedding layer (size 16) → Feature interaction module → MLP layers → Sigmoid output
  Mutual learning: MSE loss between predictions + BCE loss with labels
  Models: FiBiNET, DCN, PNN, DeepFM

- Critical path:
  1. Pretrain models independently
  2. Fine-tune pretrained models with mutual learning
  3. Evaluate AUC improvement

- Design tradeoffs:
  - Mutual learning from scratch vs. fine-tuning: Fine-tuning yields 0.25-0.66% AUC improvement vs. slight degradation from scratch
  - Number of models: 4 optimal, diminishing returns beyond
  - Model diversity: Different architectures vs. same architecture with different seeds

- Failure signatures:
  - No improvement or degradation when mutual learning from scratch
  - Performance degradation after multiple epochs of mutual learning
  - Reduced improvement when using models with similar initial performance

- First 3 experiments:
  1. Train four instances of FiBiNET independently, record AUC scores
  2. Fine-tune same four instances with mutual learning for one epoch, compare AUC
  3. Replace one FiBiNET with DCN, repeat mutual learning, compare improvement patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of model instances for mutual learning in CTR prediction tasks beyond the four models tested?
- Basis in paper: [explicit] The authors tested with 2, 3, 4, and 5 model instances and found performance plateaus at four models
- Why unresolved: The study only tested up to five model instances. The relationship between model count and performance gains may continue beyond this point, and the optimal number could vary based on dataset size or model complexity
- What evidence would resolve it: Systematic testing with larger numbers of model instances (6, 8, 10, etc.) across multiple datasets and model architectures to determine if the performance plateau persists or if there's an optimal range that varies with task characteristics

### Open Question 2
- Question: Why does mutual learning from scratch fail to improve performance while mutual learning with pretrained models succeeds?
- Basis in paper: [explicit] The authors observed that mutual learning from scratch does not improve results, but when applied to pretrained models it yields 0.25-0.66% relative improvement in AUC scores
- Why unresolved: The authors provide a hypothesis about unique learning paths in pretrained models, but the underlying mechanisms remain unclear and untested
- What evidence would resolve it: Controlled experiments comparing knowledge transfer patterns, gradient distributions, and feature space alignments between mutual learning from scratch versus pretrained scenarios, potentially using techniques like representation similarity analysis

### Open Question 3
- Question: Does mutual learning between different CTR model architectures provide the same benefits as mutual learning between multiple instances of the same architecture?
- Basis in paper: [explicit] The authors found that mutual learning between different models decreased performance for the highest-scoring model but improved performance for lower-scoring models
- Why unresolved: The study only compared four specific model architectures (FiBiNET, DCN, PNN, DeepFM) and did not explore whether this pattern holds across broader architectural families or different model combinations
- What evidence would resolve it: Systematic testing of mutual learning across diverse model architecture combinations, including newer state-of-the-art models, to determine if certain architectural pairings are more beneficial than others

### Open Question 4
- Question: What is the theoretical limit of mutual learning improvements for CTR models, and how does this scale with dataset size and model complexity?
- Basis in paper: [inferred] The study achieved up to 0.66% relative improvement on the Avazu dataset, but the paper does not explore whether larger improvements are possible with different conditions
- Why unresolved: The experiments were limited to two datasets and four specific models, without exploring the relationship between mutual learning gains and factors like dataset size, feature dimensionality, or model capacity
- What evidence would resolve it: Comprehensive experiments varying dataset characteristics, model architectures, and training durations to map the theoretical maximum improvements achievable through mutual learning across different CTR prediction scenarios

## Limitations

- The evidence for all three proposed mechanisms is weak, with no direct supporting papers found in the neighbor set
- The optimal number of models (four) is based on empirical observation but lacks theoretical grounding
- The study only tested two datasets and four specific CTR models, limiting generalizability

## Confidence

**High**: Empirical findings about mutual learning from scratch vs. fine-tuning (0.25-0.66% AUC improvement)
**Medium**: Claim that pretrained models benefit from mutual learning
**Low**: Theoretical explanations for why mechanisms work

## Next Checks

1. Verify whether different training runs on the same CTR architecture produce meaningfully different representations by analyzing embedding similarity
2. Test whether mutual learning benefits persist when using identical architectures with different random seeds versus fundamentally different architectures
3. Investigate the point of diminishing returns beyond four models by systematically testing 6-8 model configurations