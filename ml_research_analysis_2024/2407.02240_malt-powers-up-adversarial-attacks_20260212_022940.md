---
ver: rpa2
title: MALT Powers Up Adversarial Attacks
arxiv_id: '2407.02240'
source_url: https://arxiv.org/abs/2407.02240
tags:
- adversarial
- attack
- malt
- autoattack
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MALT, a targeted adversarial attack method
  that improves over AutoAttack by better selecting target classes. The key idea is
  to re-order potential target classes using a score based on the difference in model
  outputs divided by the difference in gradient norms, inspired by linear classifier
  analysis.
---

# MALT Powers Up Adversarial Attacks

## Quick Facts
- arXiv ID: 2407.02240
- Source URL: https://arxiv.org/abs/2407.02240
- Authors: Odelia Melamed; Gilad Yehudai; Adi Shamir
- Reference count: 40
- Key outcome: MALT improves targeted adversarial attacks by 3.5-5x speedup while attacking more images than AutoAttack

## Executive Summary
MALT is a targeted adversarial attack method that improves upon AutoAttack by better selecting target classes. The key innovation is a targeting method that normalizes model output confidence by gradient norm differences, inspired by linear classifier analysis. This approach is grounded in the observation that neural networks exhibit "mesoscopic almost linearity" - gradients change slowly in adversarial directions at the scale where adversarial examples exist. MALT demonstrates superior performance on CIFAR-100 and ImageNet datasets, attacking more images while running significantly faster than existing methods.

## Method Summary
MALT introduces a novel targeting method for adversarial attacks that improves upon AutoAttack by better selecting target classes. The method computes a score for each potential target class based on the difference in model outputs divided by the difference in gradient norms, inspired by linear classifier analysis. This targeting module is then combined with a standard APGD attack algorithm. The system uses two hyperparameters: c (number of candidate classes to consider) and a (number of targets to actually attack). By limiting the number of gradient calculations needed while maintaining or improving attack success rate, MALT achieves 3.5-5x speedup over AutoAttack.

## Key Results
- MALT attacks more images than AutoAttack while running 3.5-5x faster on CIFAR-100 and ImageNet
- The method achieves 5x speedup by calculating scores for only top 100 classes by confidence and attacking only top 9 by the new score
- Theoretical analysis proves MALT's targeting method works for neural networks on low-dimensional manifolds
- Empirical results demonstrate neural networks exhibit "mesoscopic almost linearity" supporting MALT's approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MALT improves target selection by normalizing model output confidence with gradient norm differences, inspired by linear classifier analysis
- Mechanism: For each potential target class, MALT computes a score: (target logit - source logit) / (gradient difference norm). This prioritizes classes that are both far in output space and have large gradient changes, making them easier to attack with small perturbations
- Core assumption: Neural networks exhibit "mesoscopic almost linearity" - gradients change slowly in adversarial directions at the scale where adversarial examples exist
- Evidence anchors:
  - [abstract] "MALT normalizes the class's confidence by the norm of the row of the Jacobian corresponding to it"
  - [section 3.2] "The basic idea of MALT is that instead of sorting the target classes only by the confidence levels of the attacked model, MALT normalizes the class's confidence by the norm of the row of the Jacobian corresponding to it"
  - [corpus] Weak - neighbors focus on different attack types without discussing this specific targeting mechanism
- Break condition: If neural networks become highly non-linear at the mesoscopic scale, or if gradient norms change rapidly in adversarial directions, the normalization would become ineffective

### Mechanism 2
- Claim: The targeting method from linear classifiers generalizes to neural networks because they remain "almost linear" at the mesoscopic scale
- Mechanism: Theoretical analysis proves that for data on low-dimensional manifolds, neural networks maintain local linearity in directions orthogonal to the data subspace, preserving the optimal targeting order from linear analysis
- Core assumption: Data resides on a low-dimensional linear subspace and the network weights don't change significantly in directions orthogonal to this subspace during training
- Evidence anchors:
  - [section 4.1] "We prove that the network remains 'almost linear' at the mesoscopic scale around data points, deducing that the ordering provided by our targeting method is preserved in that scale"
  - [section 4.2] "Empirically, we demonstrate that models tend to be almost linear close to data points by showing that a linear approximation successfully predicts the model's output when taking random or adversarial steps away from a data point"
  - [corpus] Weak - neighbors don't discuss manifold assumptions or linear approximation behavior
- Break condition: If data doesn't lie on low-dimensional manifolds, or if the network weights change significantly in all directions during training

### Mechanism 3
- Claim: MALT's efficiency comes from reducing the number of gradient calculations needed while maintaining or improving attack success rate
- Mechanism: Instead of attacking all possible target classes, MALT calculates scores for only the top 100 classes by confidence and attacks only the top 9 by the new score, achieving 5x speedup over AutoAttack
- Core assumption: The best target classes are typically among those with highest initial confidence, so limiting candidates doesn't miss important targets
- Evidence anchors:
  - [section 3.2] "In all of our experiments, we used c = 100 and a = 9" and "we ran MALT with c = 1000 for the ImageNet test dataset... This attack reached a robust accuracy of 59.84%, exactly the same as c = 100"
  - [section 5] "MALT requires 1900 passes, while AutoAttack takes 9727 passes, which is more than five times faster"
  - [corpus] Weak - neighbors don't discuss computational efficiency or candidate selection strategies
- Break condition: If optimal target classes are consistently found among low-confidence classes, or if the best targets vary significantly between models

## Foundational Learning

- Concept: Linear classifier analysis and gradient-based optimization
  - Why needed here: Understanding how MALT's targeting method is inspired by linear classifiers requires familiarity with how gradients and output differences relate to perturbation effectiveness in linear models
  - Quick check question: In a linear classifier, why does dividing the logit difference by the gradient norm difference give the optimal target class?

- Concept: Neural network architecture and backpropagation
  - Why needed here: Implementing MALT requires understanding how to compute gradients for specific output classes and how to modify targeting strategies without changing the core attack algorithm
  - Quick check question: How do you compute the gradient of a neural network's output with respect to its input for a specific target class?

- Concept: Adversarial attack evaluation metrics and benchmarks
  - Why needed here: Interpreting MALT's results requires understanding robust accuracy, attack success rates, and how different attacks are compared on standard benchmarks
  - Quick check question: What's the difference between clean accuracy and robust accuracy in adversarial attack evaluation?

## Architecture Onboarding

- Component map: Input image -> Compute model outputs and gradients -> Calculate MALT scores for top c classes -> Sort by score -> Run APGD attack on top a targets -> Return first successful adversarial example
- Critical path: Input image → Compute model outputs and gradients → Calculate MALT scores for top c classes → Sort by score → Run APGD attack on top a targets → Return first successful adversarial example
- Design tradeoffs: MALT trades some potential target class coverage (by limiting to top c candidates) for significant computational speedup (5x faster than AutoAttack). The choice of c=100 and a=9 balances coverage and efficiency.
- Failure signatures: If MALT performs similarly to naive targeting, it suggests the "almost linearity" assumption doesn't hold. If MALT is significantly slower than AutoAttack, it indicates inefficient score computation or poor candidate selection.
- First 3 experiments:
  1. Run MALT with default parameters on a small subset of CIFAR-100 and verify it attacks more images than AutoAttack
  2. Test MALT with different values of c (e.g., 50, 100, 200) to find the optimal tradeoff between coverage and speed
  3. Compare MALT's performance on different model architectures (CNNs vs transformers) to understand generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dimensionality of the input space affect the mesoscopic almost linearity property observed in neural networks?
- Basis in paper: [explicit] The paper mentions that mesoscopic almost linearity means α = o(d(1)), where d is the input dimension, and notes that the average value of α is nearly the same for CIFAR100 (d=3072) and ImageNet (d=150528) despite the large difference in input dimensions.
- Why unresolved: While the paper shows that α remains small for both CIFAR100 and ImageNet, it doesn't provide a comprehensive analysis of how α scales with input dimension across a wider range of datasets or how this property might be affected by architectural choices.
- What evidence would resolve it: Systematic experiments varying input dimensions across multiple datasets and architectures, showing how α scales with d and whether this scaling affects the effectiveness of MALT.

### Open Question 2
- Question: Can the targeting method in MALT be further improved by incorporating second-order information about the network's curvature?
- Basis in paper: [explicit] The paper concludes by suggesting that it would be interesting to understand whether there exists a better targeting method to find adversarial classes, e.g., using a second-order approximation of the network.
- Why unresolved: The paper only explores first-order targeting methods and does not investigate whether second-order methods could provide additional benefits in selecting target classes.
- What evidence would resolve it: Implementation and testing of MALT variants that incorporate second-order information (e.g., Hessian-based targeting) and comparison of their performance against the current first-order MALT method.

### Open Question 3
- Question: How does the effectiveness of MALT change when applied to non-robust (standard) models compared to robust models?
- Basis in paper: [inferred] The paper primarily focuses on robust models from the RobustBench benchmark and mentions that MALT is five times faster than AutoAttack on SOTA robust models, but doesn't explicitly test MALT on non-robust models.
- Why unresolved: The paper's empirical results are limited to robust models, leaving open the question of how MALT performs on standard, non-robust models where adversarial examples are typically easier to find.
- What evidence would resolve it: Comparative experiments applying MALT to both robust and non-robust versions of the same architectures on the same datasets, measuring success rates and runtime improvements.

## Limitations

- The "mesoscopic almost linearity" assumption, while theoretically justified, remains empirically validated only for CIFAR-100 and ImageNet datasets
- The computational efficiency gains are based on specific hyperparameter choices (c=100, a=9) that may not generalize to other datasets or model architectures
- The method primarily focuses on robust models from RobustBench, with limited testing on standard, non-robust models where adversarial examples are typically easier to find

## Confidence

- **High Confidence**: MALT's targeting mechanism is correctly implemented and produces measurable improvements over baseline methods on tested datasets
- **Medium Confidence**: The "mesoscopic almost linearity" assumption holds sufficiently well for CIFAR-100 and ImageNet to justify MALT's approach
- **Medium Confidence**: The computational efficiency gains are real and reproducible under similar hardware conditions

## Next Checks

1. **Cross-architecture validation**: Test MALT on transformer-based vision models (like ViT) and architectures trained with different objectives (like self-supervised learning) to determine if the targeting method generalizes beyond standard CNNs.

2. **Scale sensitivity analysis**: Systematically vary the hyperparameters c and a across a wider range (e.g., c ∈ [50, 200], a ∈ [5, 15]) to map the efficiency-robustness tradeoff and identify optimal configurations for different computational budgets.

3. **Manifold structure validation**: Conduct experiments to verify the low-dimensional manifold assumption by measuring the intrinsic dimensionality of the data manifold and testing whether gradient changes in adversarial directions remain slow at different perturbation scales.