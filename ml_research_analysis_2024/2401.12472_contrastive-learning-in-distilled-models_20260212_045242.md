---
ver: rpa2
title: Contrastive Learning in Distilled Models
arxiv_id: '2401.12472'
source_url: https://arxiv.org/abs/2401.12472
tags:
- learning
- contrastive
- bert
- spearman
- distilbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work applied SimCSE-style contrastive learning to DistilBERT
  to improve performance on Semantic Textual Similarity (STS) tasks while maintaining
  a lightweight model suitable for edge deployment. The approach involved using standard
  dropout twice during encoding to generate positive pairs (x, x+), treating other
  sentences in the same mini-batch as negative samples, and optimizing a contrastive
  loss.
---

# Contrastive Learning in Distilled Models

## Quick Facts
- arXiv ID: 2401.12472
- Source URL: https://arxiv.org/abs/2401.12472
- Reference count: 24
- Primary result: DistilFACE achieves 72.1 average Spearman correlation on STS tasks (34.2% improvement over BERT base)

## Executive Summary
This work combines knowledge distillation with contrastive learning to create DistilFACE, a lightweight model that significantly outperforms BERT base on Semantic Textual Similarity tasks while being 1.64× smaller and 2.95× faster. The approach uses SimCSE-style contrastive learning with DistilBERT as the base architecture, applying dropout twice during encoding to generate positive pairs and treating other batch samples as negative samples. Extensive hyperparameter tuning across pooling strategies, learning rates, batch sizes, and temperature values was conducted to optimize performance.

## Method Summary
The method applies SimCSE-style contrastive learning to DistilBERT by using standard dropout twice during encoding to generate positive pairs (x, x+), while treating other sentences in the same mini-batch as negative samples. The model is trained on Wiki 1M using a contrastive loss function with temperature scaling, and various pooling strategies are explored. The approach maintains the lightweight nature of DistilBERT while achieving significant performance gains on STS benchmarks.

## Key Results
- DistilFACE achieves 72.1 average Spearman correlation on STS tasks
- 34.2% improvement over BERT base with default parameters
- 1.64× smaller and 2.95× faster than BERT base
- Effective at batch sizes of 128 with temperature 0.05

## Why This Works (Mechanism)

### Mechanism 1
Applying dropout twice during encoding generates semantically similar but non-identical positive pairs (x, x+). Dropout masks applied independently during two forward passes produce embeddings with shared semantic structure but minor perturbations, satisfying contrastive learning's requirement for positive pairs.

### Mechanism 2
Using other sentences in the same mini-batch as negative samples provides sufficient contrastive signal. The contrastive loss function maximizes similarity between positive pairs while minimizing similarity between all other sentence pairs in the batch, effectively treating them as negatives.

### Mechanism 3
DistilBERT architecture combined with contrastive learning achieves both efficiency and performance gains. Knowledge distillation creates a lightweight base model that retains BERT's language understanding capabilities while being smaller and faster; contrastive learning then enhances semantic similarity performance without significantly increasing model size.

## Foundational Learning

- **Contrastive learning objective and mutual information maximization**: The entire model training depends on understanding how positive and negative pairs drive representation learning through similarity maximization/minimization. Quick check: How does the contrastive loss function mathematically ensure that positive pairs are pulled together while negative pairs are pushed apart?

- **Knowledge distillation and teacher-student model relationships**: DistilBERT serves as the student model that must effectively learn from BERT's pre-training while being suitable for contrastive fine-tuning. Quick check: What specific architectural changes in DistilBERT enable the 40% size reduction while retaining 97% of BERT's language understanding?

- **Spearman correlation as evaluation metric for semantic similarity**: Model performance is measured using Spearman correlation on STS tasks, requiring understanding of rank-based correlation vs other metrics. Quick check: Why might Spearman correlation be preferred over Pearson correlation for evaluating semantic textual similarity tasks?

## Architecture Onboarding

- **Component map**: DistilBERT encoder → Dropout application (twice) → Pooling layer → Contrastive loss computation → Parameter updates
- **Critical path**: Input sentences → Dual dropout encoding → Cosine similarity calculation → Contrastive loss → Gradient updates to DistilBERT parameters
- **Design tradeoffs**: Larger batch sizes improve negative sample diversity but increase memory usage; different pooling strategies affect representation quality vs computational efficiency
- **Failure signatures**: Performance plateaus at low Spearman correlation → Check pooling strategy and temperature hyperparameter; Training instability → Verify temperature scaling and batch size appropriateness; Memory errors during training → Consider Automatic Mixed Precision or reduce batch size
- **First 3 experiments**:
  1. Baseline evaluation: Run DistilBERT on STS tasks with default pooling to establish performance baseline
  2. Simple contrastive learning: Implement SimCSE-style contrastive learning with average pooling and standard hyperparameters
  3. Pooling method comparison: Test different pooling strategies (last layer, second-to-last, concatenation of last four) while keeping other hyperparameters constant

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of DistilFACE scale with increasing batch sizes beyond 256, and what are the computational trade-offs? The paper tested batch sizes of 64, 128, and 256, with 128 achieving the best average Spearman correlation, but did not explore larger batch sizes.

### Open Question 2
What is the impact of using different dropout rates in the SimCSE-style contrastive learning approach on the performance of DistilFACE? The paper mentions using standard dropout twice but does not explore the effect of varying dropout rates.

### Open Question 3
How does the performance of DistilFACE compare to other state-of-the-art models on tasks beyond Semantic Textual Similarity (STS), such as Named Entity Recognition (NER) or Sentiment Analysis? The paper focuses on evaluating DistilFACE's performance on STS tasks but does not explore its effectiveness on other NLP tasks.

## Limitations

- Specific pooling layer configuration for the final DistilFACE model is not clearly specified
- Impact of quantization on model performance mentioned but not thoroughly explored
- Temperature hyperparameter sensitivity analysis is limited

## Confidence

**High Confidence**: The core methodology of applying dropout twice to generate positive pairs and using batch samples as negatives is clearly described and theoretically sound.

**Medium Confidence**: The architectural modifications and efficiency claims are supported by measurements, but exact implementation details for achieving specific metrics are not fully transparent.

**Low Confidence**: The impact of quantization on model performance is mentioned but not thoroughly explored, with only supplementary results available.

## Next Checks

1. **Pooling Strategy Verification**: Systematically test all three pooling strategies mentioned (last layer, second-to-last layer, concatenation of last four layers) to determine which configuration achieves the reported 72.1 Spearman correlation.

2. **Temperature Sensitivity Analysis**: Conduct experiments varying the temperature hyperparameter (τ) across a range (e.g., 0.01 to 0.1) to understand its impact on convergence and final performance.

3. **Batch Size Scaling Study**: Evaluate model performance across different batch sizes (32, 64, 128, 256) to verify efficiency gains and identify the optimal batch size that balances negative sample quality with computational constraints.