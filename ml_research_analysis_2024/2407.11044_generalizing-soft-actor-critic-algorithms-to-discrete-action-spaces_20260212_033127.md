---
ver: rpa2
title: Generalizing soft actor-critic algorithms to discrete action spaces
arxiv_id: '2407.11044'
source_url: https://arxiv.org/abs/2407.11044
tags:
- policy
- learning
- sac-bbf
- action
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a discrete variant of the Soft Actor-Critic
  (SAC) algorithm and applies it to the Atari 100K benchmark. The method enables off-policy
  learning with policy heads for discrete action spaces, integrating SAC into the
  state-of-the-art Rainbow-based BBF algorithm.
---

# Generalizing soft actor-critic algorithms to discrete action spaces

## Quick Facts
- arXiv ID: 2407.11044
- Source URL: https://arxiv.org/abs/2407.11044
- Authors: Le Zhang; Yong Gu; Xin Zhao; Yanshuo Zhang; Shu Zhao; Yifei Jin; Xinxin Wu
- Reference count: 39
- Primary result: Discrete SAC variant achieves super-human performance on Atari 100K with IQM of 1.088

## Executive Summary
This paper extends Soft Actor-Critic (SAC) to discrete action spaces and integrates it with the Rainbow-based BBF algorithm. The resulting SAC-BBF achieves state-of-the-art performance on the Atari 100K benchmark, demonstrating super-human performance with minimal sample complexity. The key innovations include a variance reduction technique for discrete SAC and the integration of SAC principles into BBF's architecture.

## Method Summary
The authors propose a discrete variant of SAC that uses policy heads for discrete action spaces, integrating it into the BBF algorithm framework. The method employs variance reduction in policy gradients, target network smoothing for action selection, and an annealing schedule for the entropy bonus parameter β. The approach is evaluated on the Atari 100K benchmark, which consists of 26 games with a total of 400K frames.

## Key Results
- SAC-BBF achieves an IQM score of 1.088 on Atari 100K, surpassing the previous state-of-the-art of 1.045
- The method uses only a replay ratio of 2 while maintaining super-human performance
- SAC-BBF demonstrates faster training time compared to BBF while maintaining or improving performance

## Why This Works (Mechanism)

### Mechanism 1
Variance reduction in the policy gradient estimator is critical for discrete SAC to perform well. The discrete SAC policy gradient uses a baseline term `E[Q(st, a') * π(a'|st)]` to reduce variance without changing the expected gradient value. This avoids the high variance of naïvely sampling actions for Q-value estimation in discrete spaces.

### Mechanism 2
Using target networks for action selection during training preserves exploration while maintaining stability. Instead of using online parameters for both policy evaluation and action selection, SAC-BBF samples from the target policy distribution `π_θ_targ(·|s)`. This smooths the action selection process and reduces overfitting to recent online updates.

### Mechanism 3
Annealing the entropy bonus β to zero encourages deterministic final policies while maintaining exploration during training. β starts at 0.01 and linearly decreases to 0 over F=40K steps, after which it stays at 0. This transitions the agent from exploratory to exploitation-focused behavior.

## Foundational Learning

- **Policy gradient variance reduction**
  - Why needed here: In discrete action spaces, sampling Q-values directly leads to high variance; baseline subtraction stabilizes learning.
  - Quick check question: Why does subtracting `E[Q * π]` from Q in the policy gradient not change the expected gradient value?

- **Target network smoothing**
  - Why needed here: Using online parameters for action selection causes instability; target networks provide a moving average that smooths updates.
  - Quick check question: What would happen to training stability if we removed the target network and used online parameters for action selection?

- **Entropy regularization schedule**
  - Why needed here: Early entropy encourages exploration; late entropy removal allows exploitation. Fixed entropy is suboptimal.
  - Quick check question: What is the effect of keeping β fixed at 0.01 throughout training versus annealing it to zero?

## Architecture Onboarding

- **Component map:** Online encoder → conv. transition model → online projection θ → policy head → Q-function head; Target encoder → augmented transitions → target projection θ → target Q-network; Predictor θ module for SPR loss computation; Cosine similarity loss for latent state prediction

- **Critical path:** State → encoder → transition model → projection → policy head (sampling) → environment → reward → replay buffer → Q-network update → policy update (with variance reduction)

- **Design tradeoffs:** Adding policy head increases model capacity but adds training overhead; variance reduction term improves stability at slight computational cost; sampling during evaluation vs greedy action selection trades off consistency for robustness

- **Failure signatures:** High variance in policy gradients → unstable training or poor final performance; target network too stale → slow learning or poor exploration; β annealing too fast → premature convergence to suboptimal deterministic policy

- **First 3 experiments:**
  1. Compare SAC-BBF with and without variance reduction on a small Atari subset
  2. Test greedy vs sampling evaluation strategies on a held-out set
  3. Vary β annealing schedule (fast vs slow) and measure impact on IQM

## Open Questions the Paper Calls Out

### Open Question 1
How does the variance reduction technique in SAC-BBF compare to other variance reduction methods used in reinforcement learning? The paper highlights that variance reduction is "the single most important trick for successfully adapting SAC to discrete domains" but doesn't compare this technique to other methods like baseline subtraction or control variates.

### Open Question 2
What is the impact of the replay ratio (RR) on the performance of SAC-BBF beyond the tested values of 2 and 4? The paper mentions that "SAC-BBF exhibits replay-ratio scaling capabilities" but only tests up to RR 4.

### Open Question 3
How does the annealing schedule for the entropy bonus parameter β affect the final policy's performance and exploration-exploitation trade-off? The paper describes a linear annealing schedule for β but doesn't investigate whether this is optimal or how different schedules might impact learning.

## Limitations
- Variance reduction mechanism lacks direct empirical validation against alternatives
- Experimental design doesn't isolate contribution of individual components
- Limited exploration of replay ratio scaling beyond RR 4

## Confidence
- **High confidence** in overall methodology and experimental setup
- **Medium confidence** in variance reduction mechanism's specific implementation
- **Medium confidence** in claimed superiority over state-of-the-art methods

## Next Checks
1. Conduct an ablation study to isolate the impact of the variance reduction term on training stability and final performance
2. Test the sensitivity of results to different β annealing schedules and target network update frequencies
3. Compare the discrete SAC variant against other discrete RL algorithms on the same benchmark to validate relative performance gains