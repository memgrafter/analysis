---
ver: rpa2
title: Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning
arxiv_id: '2402.04009'
source_url: https://arxiv.org/abs/2402.04009
tags:
- last
- pretrained
- methods
- finetuning
- peft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LAST proposes low-rank self-attention side-tuning for parameter-efficient
  fine-tuning of vision transformers. The method disentangles the trainable side-network
  from the frozen backbone by freezing both parameters and outputs, using only low-rank
  attention modules without feed-forward networks.
---

# Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning

## Quick Facts
- arXiv ID: 2402.04009
- Source URL: https://arxiv.org/abs/2402.04009
- Authors: Ningyuan Tang; Minghao Fu; Ke Zhu; Jianxin Wu
- Reference count: 40
- One-line primary result: Achieves 76.5% average accuracy on VTAB-1K with only 0.66M parameters, 75% less memory than full finetuning

## Executive Summary
LAST (Low-rank Attention Side-Tuning) is a parameter-efficient fine-tuning method for vision transformers that dramatically reduces GPU memory usage and training time while maintaining strong performance. By freezing both parameters and outputs of a pretrained ViT backbone and adding a small trainable side-network composed only of low-rank self-attention modules, LAST achieves 76.5% average accuracy on VTAB-1K with just 0.66M parameters. The method uses 1.33GB GPU memory (75% less than full fine-tuning) and trains 60% faster, making it practical for adapting very large models like ViT-g on single GPUs.

## Method Summary
LAST works by freezing a pretrained ViT backbone and adding a small trainable side-network that receives intermediate feature maps from the frozen model. The side-network consists solely of low-rank self-attention (LSA) modules without feed-forward networks, drastically reducing parameters. Each LSA block contains multiple LSA modules with configurable gap and stack factors. A key innovation is bias correction, which subtracts intermediate features to ensure the final representation cleanly combines the frozen backbone's output with the side-network's task-specific contribution. This disentanglement means gradients are only computed for the small side-network, enabling dramatic memory savings.

## Key Results
- Achieves 76.5% average accuracy on VTAB-1K benchmark with only 0.66M trainable parameters
- Uses 1.33GB GPU memory (75% less than full fine-tuning) and trains 60% faster
- Outperforms state-of-the-art PEFT methods including LoRA, Adapter, and Side-Tuning
- Successfully adapts very large models like ViT-g (1167M parameters) on single GPU

## Why This Works (Mechanism)

### Mechanism 1
Disentangling the trainable side-network from the frozen backbone eliminates the need to compute and store gradients for the large pretrained model, drastically reducing GPU memory usage. LAST freezes both the parameters and outputs of the pretrained model, treating it as a standalone feature extractor. The side-network operates independently, receiving only intermediate features from the frozen backbone. This separation means gradients are only computed for the small side-network, not the massive pretrained model.

### Mechanism 2
Low-rank self-attention (LSA) modules are sufficient for capturing task-specific variations in the side-network, eliminating the need for large feed-forward networks (FFNs). LSA projects input tokens to a very low dimensionality (e.g., r=16 when d=768) for the query, key, and value computations, performs self-attention in this low-dimensional space, then projects back to the original dimensionality. This drastically reduces the number of parameters and computations compared to full-rank attention with FFNs.

### Mechanism 3
Correcting the LSA bias by subtracting intermediate features ensures the final representation is the sum of the frozen backbone's output and the side-network's task-specific contribution. Without correction, the residual connections in LSA cause the final representation to include all intermediate features from the backbone instead of just the final one. Subtracting the sum of all but the last intermediate features corrects this bias.

## Foundational Learning

- **Vision Transformers (ViTs)**: Understand the architecture components including patch embedding, multi-head self-attention, and feed-forward networks. Why needed: LAST is designed specifically for adapting ViT-based models, and understanding their components is crucial for grasping how LSA modules fit into the architecture. Quick check: What are the two main components of a ViT Transformer block, and how do they process the input tokens?

- **Parameter-efficient fine-tuning (PEFT) methods**: Know the trade-offs between methods like LoRA, adapters, and prompt tuning regarding memory usage and training speed. Why needed: LAST is a PEFT method, and understanding the limitations of existing methods is key to appreciating its contributions. Quick check: Why do most PEFT methods still require significant GPU memory during fine-tuning, even though they only update a small number of parameters?

- **Side-tuning framework**: Understand how side-tuning avoids catastrophic forgetting while potentially being less parameter-efficient than other methods. Why needed: LAST is a side-tuning method, and understanding this framework is essential for grasping its design choices. Quick check: How does side-tuning avoid catastrophic forgetting when adapting a pretrained model to a new task?

## Architecture Onboarding

- **Component map**: Pretrained ViT backbone (frozen) -> Intermediate features -> LSA blocks (trainable) -> Bias-corrected final representation -> Classification head

- **Critical path**: Input image → ViT backbone (frozen) → Intermediate features → LSA blocks (trainable) → Bias-corrected final representation → Classification head

- **Design tradeoffs**: LSA dimensionality (r) vs. representation power: Lower r reduces parameters and memory but may limit the side-network's ability to capture complex task-specific variations. Gap factor (g) and stack factor (T) vs. parameter efficiency: Smaller g and larger T increase the number of parameters and memory usage but may improve adaptation performance. Bias correction vs. simplicity: Bias correction ensures clean separation of contributions but adds a step to the computation.

- **Failure signatures**: Poor adaptation performance may indicate that the LSA dimensionality is too low, the gap/stack factors are not well-tuned, or the bias correction is not applied correctly. High GPU memory usage may indicate that the LSA dimensionality is too high, the gap/stack factors are too small, or the bias correction is not implemented efficiently. Slow training speed may indicate that the LSA dimensionality is too high, the gap/stack factors are too small, or the bias correction is computationally expensive.

- **First 3 experiments**: 1) Verify the memory and speed benefits: Compare LAST's GPU memory usage and training speed to full fine-tuning and LoRA on a small dataset with moderate-sized ViT. 2) Ablate LSA dimensionality: Test different values of r (e.g., 8, 16, 32) and measure the impact on adaptation performance and memory usage. 3) Verify the importance of bias correction: Compare LAST with and without bias correction on a few downstream tasks and measure the impact on adaptation performance.

## Open Questions the Paper Calls Out

### Open Question 1
What is the precise mathematical formulation of the bias correction term in LAST and how does it affect the optimization landscape? The paper states that the bias correction term is "m-1 sum from i=0 to m-1 of zi" and explains its necessity but doesn't provide a detailed mathematical analysis of its effects. A formal mathematical proof of the bias correction's effect on gradient flow and convergence, along with controlled experiments varying the correction term, would resolve this.

### Open Question 2
How does LAST's performance scale with increasingly larger backbone models beyond ViT-g, and what are the theoretical limits of its efficiency gains? The paper demonstrates LAST works on ViT-g (1167M parameters) but doesn't explore scaling to even larger models or establish theoretical bounds on efficiency improvements. Experiments with larger backbones (e.g., ViT-h or beyond) and theoretical analysis of computational complexity and memory scaling would address this.

### Open Question 3
What is the fundamental reason why low-rank self-attention without feed-forward networks is sufficient for PEFT tasks, and can this principle be generalized to other architectures? The paper claims that "we can throw away large FFN" and that "low-rank self-attention is sufficient" but doesn't provide theoretical justification or explore applications to other architectures. Theoretical analysis of the representation requirements for PEFT tasks and experiments applying the low-rank attention principle to other architectures (e.g., ResNets, MLPs) would resolve this.

## Limitations
- Relies on frozen backbone features being sufficiently rich and general for downstream tasks, which may not hold for significant domain shifts
- Low-rank assumption for self-attention may not capture all task-specific variations, particularly for complex visual reasoning tasks
- Performance depends on specific hyperparameter choices (g=2, T=2, r=16) that may not generalize optimally to all tasks

## Confidence

- **High confidence**: The memory efficiency claims (75% reduction) and training speed improvements (60% faster) are well-supported by the methodology and implementation details.
- **Medium confidence**: The 76.5% average accuracy on VTAB-1K is impressive but relies on specific hyperparameter choices that may not generalize optimally to all tasks.
- **Medium confidence**: The claim that FFNs are unnecessary for the side-network is well-justified through ablation studies, though the exact threshold where this holds may vary.

## Next Checks

1. **Cross-domain robustness test**: Evaluate LAST on domain-shifted datasets (e.g., DomainNet, WILDS) to assess performance when pretraining and target domains have significant distribution shift.

2. **LSA dimensionality sensitivity**: Conduct a more extensive ablation study varying r from 8 to 64 while keeping other parameters fixed to precisely map the trade-off between parameter efficiency and accuracy.

3. **Comparison with recent PEFT methods**: Benchmark LAST against newer approaches like LoRA++ and AdaLoRA on the same hardware configuration to validate the claimed memory and speed advantages under direct competition.