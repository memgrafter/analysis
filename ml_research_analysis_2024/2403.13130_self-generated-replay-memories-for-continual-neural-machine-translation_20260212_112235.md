---
ver: rpa2
title: Self-generated Replay Memories for Continual Neural Machine Translation
arxiv_id: '2403.13130'
source_url: https://arxiv.org/abs/2403.13130
tags:
- training
- language
- data
- translation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  continual learning for neural machine translation (NMT). The authors propose SG-Rep,
  a method that uses self-generated pseudo-parallel sentences to populate a replay
  memory, enabling the model to maintain performance across multiple language pairs
  learned incrementally.
---

# Self-generated Replay Memories for Continual Neural Machine Translation

## Quick Facts
- arXiv ID: 2403.13130
- Source URL: https://arxiv.org/abs/2403.13130
- Reference count: 32
- Primary result: SG-Rep outperforms strong baselines including EWC, A-GEM, and LAMOL on continual NMT tasks

## Executive Summary
This paper addresses catastrophic forgetting in continual learning for neural machine translation by proposing SG-Rep, a method that uses self-generated pseudo-parallel sentences to populate a replay memory. The approach leverages the generative capabilities of encoder-decoder Transformers to produce synthetic training samples that are filtered and translated back to maintain performance across multiple language pairs learned incrementally. Experiments on IWSLT17 and UNPC datasets demonstrate that SG-Rep achieves higher BLEU scores than strong baselines while effectively mitigating forgetting, particularly in scenarios with low token overlap between language pairs.

## Method Summary
SG-Rep generates synthetic training samples using the model's own generative ability, filters them based on quality criteria, translates them back to the source language, and populates a replay buffer using reservoir sampling. The method trains incrementally on streams of language pairs, using the replay buffer during subsequent training to preserve knowledge of previous pairs. This approach avoids explicit memorization of training data while maintaining diversity in the replay buffer through controlled generation of pseudo-samples.

## Key Results
- SG-Rep outperforms EWC, A-GEM, and LAMOL baselines on IWSLT17 and UNPC datasets
- Particularly effective in low token overlap scenarios between language pairs
- Increasing self-generated samples improves replay buffer diversity and overall performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Self-generated pseudo-parallel sentences mitigate catastrophic forgetting without explicit memorization of training data.
- **Mechanism**: The model uses its own generative ability to produce synthetic training samples that are filtered and translated back to populate a replay memory. This memory is then used during subsequent training to preserve knowledge of previous language pairs.
- **Core assumption**: The model's own generative outputs, when properly filtered, are sufficiently diverse and representative to prevent forgetting.
- **Evidence anchors**:
  - [abstract]: "We empirically demonstrate that our approach can counteract catastrophic forgetting without requiring explicit memorization of training data."
  - [section 3.2]: "We generate n pseudo sentences in language lt iteratively by top-k sampling with k equal to vocabulary size. At each step, we process the sentences by 1) eliminating duplicates and 2) filtering out low-quality ones."
  - [corpus]: Weak evidence. No direct citations found, but related works like LAMOL use similar self-generation strategies.
- **Break condition**: If the filtering process removes too many samples or the generated samples are too similar to each other, diversity in the replay buffer may be insufficient to prevent forgetting.

### Mechanism 2
- **Claim**: Low sub-word token overlap between language pairs reduces the effectiveness of regularization-based methods like EWC.
- **Mechanism**: In the CILL setting, the model must learn distinct target distributions that often exhibit minimal overlap, even with a shared sub-word vocabulary. This makes it difficult for EWC to effectively protect important weights.
- **Core assumption**: The effectiveness of EWC is tied to the degree of overlap between tasks.
- **Evidence anchors**:
  - [section 5.2]: "We attribute this outcome to the inherent characteristics of this CL scenario, where the model must learn distinct target distributions that often exhibit minimal overlap, despite the utilization of a shared sub-word vocabulary."
  - [section 5.2]: "The findings demonstrate that while there is a higher overlap percentage between experiences that share the same target language, even for larger values of k, the overlap remains relatively low."
  - [corpus]: Weak evidence. No direct citations found, but the paper cites Lesort et al., 2019 and van de Ven and Tolias, 2019, who discuss EWC's limitations in low-overlap scenarios.
- **Break condition**: If language pairs have high token overlap, regularization-based methods may become more effective.

### Mechanism 3
- **Claim**: Increasing the number of self-generated samples improves the diversity of the replay buffer, leading to better performance.
- **Mechanism**: By generating a larger number of pseudo-samples and using reservoir sampling to populate the replay buffer, the model ensures a more diverse set of training examples for future experiences.
- **Core assumption**: Diversity in the replay buffer is beneficial for preventing forgetting.
- **Evidence anchors**:
  - [section 5.3]: "By maintaining a fixed memory size and increasing the quantity of generated pseudo-samples, we obtain a larger initial population for reservoir sampling, leading to increased diversity within the memory."
  - [section 5.3]: "This diversity proved to be beneficial for the overall performance."
  - [corpus]: Weak evidence. No direct citations found, but the paper's experimental results support this claim.
- **Break condition**: If the generated samples become too repetitive or the filtering process removes too many diverse samples, increasing the number of generated samples may not improve diversity.

## Foundational Learning

- **Concept**: Catastrophic Forgetting (CF)
  - **Why needed here**: CF is the core problem that SG-Rep aims to solve. Understanding CF is essential for grasping the motivation behind the proposed method.
  - **Quick check question**: What is catastrophic forgetting, and why is it a problem in continual learning?

- **Concept**: Continual Learning (CL)
  - **Why needed here**: SG-Rep is a method for continual learning in neural machine translation. Understanding CL concepts is crucial for understanding the context and challenges of the proposed approach.
  - **Quick check question**: What are the main challenges in continual learning, and how do different approaches (architecture-based, regularization-based, data-based) address them?

- **Concept**: Neural Machine Translation (NMT)
  - **Why needed here**: SG-Rep is specifically designed for NMT systems. Understanding NMT concepts is essential for understanding the specific challenges and opportunities in this domain.
  - **Quick check question**: What are the key components of an NMT system, and how do they contribute to the overall translation process?

## Architecture Onboarding

- **Component map**: Encoder-Decoder Transformer model -> SentencePiece tokenizer (32k tokens) -> Replay buffer -> Reservoir sampling mechanism

- **Critical path**: 
  1. Train the model on the current experience
  2. Generate pseudo-samples using the model's own generative ability
  3. Filter and translate the pseudo-samples
  4. Populate the replay buffer using reservoir sampling
  5. Use the replay buffer during training on subsequent experiences

- **Design tradeoffs**:
  - Memory size vs. diversity: Larger replay buffers can store more diverse samples, but require more memory.
  - Number of generated samples vs. computational cost: Generating more samples increases diversity but also increases computational overhead.
  - Filtering criteria vs. sample quality: Stricter filtering criteria improve sample quality but may reduce diversity.

- **Failure signatures**:
  - High self-BLEU scores of generated samples: Indicates low diversity and potential overfitting to the model's own outputs.
  - Rapid decrease in BLEU scores for previous language pairs: Suggests insufficient preservation of knowledge in the replay buffer.
  - Long training times: May indicate inefficiencies in the self-generation or filtering process.

- **First 3 experiments**:
  1. Train the model incrementally on a stream of experiences with varying memory sizes (e.g., 5%, 10%, 20% of training data).
  2. Vary the number of self-generated samples (e.g., 100k, 180k, 250k) while keeping the memory size fixed.
  3. Compare the performance of SG-Rep with other continual learning baselines (e.g., EWC, A-GEM, Replay) on the same stream of experiences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SG-Rep perform in scenarios with extremely low token overlap between language pairs?
- Basis in paper: [inferred] The paper shows SG-Rep performs well in IWSLT17 experiments where token overlap is relatively low, but doesn't test with minimal overlap.
- Why unresolved: The paper only demonstrates SG-Rep's effectiveness in moderately low overlap scenarios, not extreme cases.
- What evidence would resolve it: Experiments testing SG-Rep with language pairs having <5% token overlap would clarify its limits.

### Open Question 2
- Question: What is the optimal ratio of self-generated samples to real training data for different memory buffer sizes?
- Basis in paper: [explicit] The paper experiments with varying numbers of generated samples (100K-250K) but doesn't systematically study the optimal ratio.
- Why unresolved: The experiments show that more generated samples help up to a point, but the relationship between sample ratio, memory size, and performance isn't fully characterized.
- What evidence would resolve it: A comprehensive ablation study varying both memory size and self-generated sample ratio would identify optimal combinations.

### Open Question 3
- Question: How does SG-Rep's performance degrade with increasing number of language pairs in the stream?
- Basis in paper: [inferred] The paper tests up to 8 language pairs but doesn't systematically explore scaling to larger numbers of languages.
- Why unresolved: The paper shows good performance with moderate numbers of languages but doesn't establish limits on scalability.
- What evidence would resolve it: Experiments scaling from 4 to 20+ language pairs would reveal SG-Rep's performance trends and potential bottlenecks.

## Limitations

- SG-Rep's effectiveness in extremely low token overlap scenarios remains unproven
- The optimal ratio of self-generated samples to real training data for different memory sizes is not characterized
- Scalability to large numbers of language pairs (>8) has not been thoroughly tested

## Confidence

- **High Confidence**: The empirical results showing SG-Rep outperforming baselines (EWC, A-GEM, LAMOL) on IWSLT17 and UNPC datasets are well-supported by the experimental data presented.
- **Medium Confidence**: The theoretical justification for why self-generated samples effectively prevent forgetting is reasonable but could benefit from more rigorous analysis of the diversity and quality metrics.
- **Low Confidence**: The scalability claims to other language families and the long-term stability of the approach across many more language pairs remain unproven.

## Next Checks

1. **Diversity Analysis**: Conduct a detailed analysis of the diversity of generated samples by computing self-BLEU scores between generated sentences and original training data, comparing this across different language pairs and generation parameters.
2. **Cross-Domain Testing**: Test SG-Rep on a more diverse set of language families (e.g., including Asian languages with different scripts, agglutinative languages) to validate generalizability claims beyond the European language pairs used in experiments.
3. **Long-term Stability**: Evaluate the approach's performance after learning 10+ language pairs to assess whether the replay buffer remains effective and whether the model maintains performance across all previously learned pairs without degradation.