---
ver: rpa2
title: 'Multilingual transformer and BERTopic for short text topic modeling: The case
  of Serbian'
arxiv_id: '2402.03067'
source_url: https://arxiv.org/abs/2402.03067
tags:
- bertopic
- topic
- topics
- text
- preprocessing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates BERTopic, a neural topic modeling approach,
  on short text in Serbian, a morphologically rich language. The authors compare BERTopic
  with multilingual embedding models (distiluse-base-multilingual-cased-v2, paraphrase-multilingual-MiniLM-L12-v2,
  paraphrase-multilingual-mpnet-base-v2) on two preprocessing levels: partial (minimal)
  and full (including lemmatization).'
---

# Multilingual transformer and BERTopic for short text topic modeling: The case of Serbian

## Quick Facts
- **arXiv ID**: 2402.03067
- **Source URL**: https://arxiv.org/abs/2402.03067
- **Reference count**: 24
- **Primary result**: BERTopic with multilingual transformer embeddings and minimal preprocessing outperforms traditional methods on short Serbian text topic modeling

## Executive Summary
This study evaluates BERTopic, a neural topic modeling approach, on short text in Serbian, a morphologically rich language. The authors compare BERTopic with multilingual embedding models (distiluse-base-multilingual-cased-v2, paraphrase-multilingual-MiniLM-L12-v2, paraphrase-multilingual-mpnet-base-v2) on two preprocessing levels: partial (minimal) and full (including lemmatization). Performance is benchmarked against LDA and NMF on fully preprocessed text. Using NPMI-based topic coherence (TC) and topic diversity (TD) scores, BERTopic with paraphrase-multilingual-mpnet-base-v2 on partial preprocessing achieved TC of -0.133 and TD of 0.896, outperforming traditional methods in some metrics. Manual inspection shows BERTopic yields diverse, interpretable topics even with minimal preprocessing. When topic number is unconstrained, BERTopic reveals novel subtopics not captured by LDA/NMF. The findings suggest BERTopic is effective for morphologically rich, low-resource languages without extensive preprocessing.

## Method Summary
The study compares BERTopic with multilingual transformer embeddings against traditional LDA and NMF topic modeling approaches on Serbian short texts. Two preprocessing levels are evaluated: partial preprocessing (minimal tokenization and cleaning) and full preprocessing (including lemmatization). Three multilingual embedding models are tested: distiluse-base-multilingual-cased-v2, paraphrase-multilingual-MiniLM-L12-v2, and paraphrase-multilingual-mpnet-base-v2. Performance is measured using NPMI-based topic coherence and topic diversity scores. Manual inspection validates the quality and interpretability of generated topics. The experiments use football forum discussion data as the test corpus.

## Key Results
- BERTopic with paraphrase-multilingual-mpnet-base-v2 and partial preprocessing achieved TC of -0.133 and TD of 0.896
- BERTopic outperformed traditional LDA/NMF methods in topic diversity while maintaining competitive coherence
- Minimal preprocessing was sufficient for BERTopic to produce interpretable topics in morphologically rich Serbian
- Unconstrained topic modeling revealed novel subtopics not captured by traditional methods

## Why This Works (Mechanism)
BERTopic leverages transformer-based embeddings to capture semantic relationships in short texts, which is particularly effective for morphologically rich languages where traditional bag-of-words approaches struggle with inflectional variations. The neural embedding models can encode morphological variations as semantically similar representations, reducing the need for extensive preprocessing like lemmatization. This semantic understanding enables BERTopic to identify meaningful topics even with minimal preprocessing, while traditional methods require morphological normalization to achieve comparable results.

## Foundational Learning
**Topic Modeling**: A technique for discovering abstract themes in document collections; needed to understand the core problem being solved and evaluation metrics.
*Why needed*: Provides context for evaluating BERTopic's effectiveness against established methods.
*Quick check*: Can you explain the difference between LDA and NMF topic modeling approaches?

**Morphological Richness**: Languages with extensive inflectional systems where words change form based on grammatical context; critical for understanding preprocessing requirements.
*Why needed*: Explains why Serbian presents unique challenges for topic modeling and why BERTopic's approach is advantageous.
*Quick check*: How does morphological richness affect bag-of-words approaches to topic modeling?

**Transformer Embeddings**: Neural network representations that capture semantic relationships between words; essential for understanding BERTopic's core mechanism.
*Why needed*: Explains how BERTopic achieves semantic understanding without extensive preprocessing.
*Quick check*: What advantage do transformer embeddings have over traditional word embeddings for morphologically rich languages?

**NPMI Coherence**: Normalized Pointwise Mutual Information used to measure topic quality by evaluating word co-occurrence patterns; the primary evaluation metric.
*Why needed*: Understanding this metric is crucial for interpreting the study's quantitative results.
*Quick check*: How does NPMI coherence differ from other topic coherence metrics like UMass or UCI?

## Architecture Onboarding
**Component Map**: Data Preprocessing -> Multilingual Embedding Model -> BERTopic Clustering -> Topic Quality Evaluation
**Critical Path**: Raw Text → Minimal Preprocessing → Transformer Embedding → HDBSCAN Clustering → C-TF-IDF Refinement → NPMI Evaluation
**Design Tradeoffs**: BERTopic sacrifices some interpretability transparency compared to LDA but gains semantic robustness and reduced preprocessing needs; multilingual embeddings add computational overhead but enable cross-linguistic applicability.
**Failure Signatures**: Poor topic coherence with morphologically rich languages suggests embedding model inadequacy; overly generic topics indicate suboptimal clustering parameters; preprocessing requirements reveal whether semantic understanding is occurring.
**3 First Experiments**:
1. Compare BERTopic with different multilingual embedding models on the same dataset to isolate embedding quality effects
2. Vary preprocessing levels systematically to quantify the relationship between morphological normalization and topic quality
3. Test unconstrained versus fixed-topic-number configurations to understand subtopic discovery capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on automated NPMI metrics that may not fully capture semantic quality for morphologically complex languages
- Manual inspection of topics lacks systematic annotation protocols to strengthen reliability claims
- Single-domain evaluation (football forum discussions) limits generalizability to other short-text domains in Serbian

## Confidence
- **Medium**: BERTopic outperforms traditional methods (LDA/NMF) on short texts in morphologically rich languages
- **High**: Minimal preprocessing suffices for BERTopic to produce interpretable topics
- **Medium**: BERTopic reveals novel subtopics when unconstrained topic numbers are allowed

## Next Checks
1. Evaluate BERTopic across multiple Serbian text domains (news, social media, academic abstracts) to assess domain robustness and identify whether morphological richness consistently reduces preprocessing requirements.

2. Implement a blind human annotation study where multiple annotators rate topic coherence and interpretability for BERTopic outputs versus LDA/NMF across different preprocessing levels, using standardized rubrics.

3. Conduct cross-linguistic validation by applying the same experimental setup to other morphologically rich languages (e.g., Russian, Finnish) to determine whether the Serbian-specific findings generalize to the broader class of morphologically complex languages.