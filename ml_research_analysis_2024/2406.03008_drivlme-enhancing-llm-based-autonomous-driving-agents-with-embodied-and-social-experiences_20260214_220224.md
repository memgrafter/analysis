---
ver: rpa2
title: 'DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social
  Experiences'
arxiv_id: '2406.03008'
source_url: https://arxiv.org/abs/2406.03008
tags:
- dialogue
- driving
- language
- agent
- drivlme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DriVLMe, a video-language-model-based autonomous
  driving agent that integrates both embodied experiences from a simulated environment
  and social experiences from real human dialogue. The core method involves fine-tuning
  a large language model with multimodal inputs, including visual perception and dialogue
  history, to facilitate natural communication and navigation in complex driving scenarios.
---

# DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences

## Quick Facts
- arXiv ID: 2406.03008
- Source URL: https://arxiv.org/abs/2406.03008
- Reference count: 40
- Key outcome: Introduces DriVLMe, a video-language-model-based autonomous driving agent that integrates embodied and social experiences, demonstrating competitive performance on open-loop benchmarks and closed-loop human studies, but revealing limitations in inference time, imbalanced training data, limited visual understanding, multi-turn interactions, and handling unexpected situations.

## Executive Summary
This paper introduces DriVLMe, a video-language-model-based autonomous driving agent that integrates both embodied experiences from a simulated environment and social experiences from real human dialogue. The core method involves fine-tuning a large language model with multimodal inputs, including visual perception and dialogue history, to facilitate natural communication and navigation in complex driving scenarios. The model demonstrates competitive performance on open-loop benchmarks (SDN and BDD-X) and closed-loop human studies in the CARLA simulator, significantly outperforming previous baselines in generating dialogue responses and physical actions. However, the study also reveals several limitations, including long inference times, imbalanced training data, limited visual understanding, challenges with multi-turn interactions, and difficulties in handling unexpected situations like environmental dynamics and task changes.

## Method Summary
DriVLMe fine-tunes a large language model (Vicuna-7B + LoRA) with multimodal inputs including video representations, dialogue history, and route planning. The training involves domain video instruction tuning, social instruction tuning, and embodied instruction tuning stages. The model processes egocentric RGB images, dialogue history, action history, and map knowledge to generate natural dialogue responses and physical actions for autonomous driving. Evaluation is performed on SDN and BDD-X datasets for open-loop benchmarks and in CARLA simulator for closed-loop human studies.

## Key Results
- Outperforms previous baselines on SDN benchmark for NfD and RfN tasks
- Demonstrates competitive performance on open-loop benchmarks (SDN and BDD-X)
- Significantly improves dialogue response generation and physical action selection in closed-loop human studies
- Reveals limitations in inference time, imbalanced training data, limited visual understanding, and handling unexpected situations

## Why This Works (Mechanism)
DriVLMe leverages the power of large language models to integrate embodied experiences from simulated driving environments with social experiences from human dialogue. By fine-tuning the LLM with multimodal inputs, including video representations and dialogue history, the model learns to generate contextually appropriate responses and actions. The embodied instruction tuning stage enables the model to verbalize its experiences, while the social instruction tuning stage enhances its ability to communicate naturally with human instructors. This integration of embodied and social experiences allows DriVLMe to navigate complex driving scenarios and engage in meaningful dialogue with humans.

## Foundational Learning
- Large Language Models (LLMs): Why needed - to generate natural language responses and actions. Quick check - evaluate model's performance on language generation metrics (CIDEr, BERTScore, METEOR).
- Embodied Experiences: Why needed - to learn from simulated driving environments and verbalize experiences. Quick check - assess model's ability to generate appropriate actions in closed-loop experiments.
- Social Experiences: Why needed - to facilitate natural communication with human instructors. Quick check - evaluate model's performance on dialogue generation tasks (RfN).
- Multimodal Inputs: Why needed - to process visual perception and dialogue history for decision-making. Quick check - analyze model's performance on open-loop benchmarks with multimodal inputs.
- Route Planning: Why needed - to navigate from start to destination. Quick check - verify model's ability to generate valid routes using shortest path algorithm on graph-structured map.

## Architecture Onboarding
- Component Map: Egocentric RGB Images -> CLIP Encoder -> Linear Projection Layer -> Video Representations -> LLM (Vicuna-7B + LoRA) -> Dialogue Responses and Physical Actions
- Critical Path: Multimodal inputs (visual perception, dialogue history, action history, map knowledge) -> LLM fine-tuning -> Natural dialogue generation and physical action selection
- Design Tradeoffs: Integration of embodied and social experiences enhances communication and navigation capabilities but introduces challenges in inference time, imbalanced training data, and handling unexpected situations.
- Failure Signatures: Long inference times, oversimplified language generation, inability to initiate dialogue, absence of situated Theory of Mind, struggles with unexpected situations and world dynamics.
- First Experiments:
  1. Evaluate model's performance on SDN benchmark for NfD and RfN tasks with multimodal inputs.
  2. Conduct closed-loop experiments in CARLA simulator with human subjects to assess dialogue generation and physical action selection.
  3. Analyze action distribution in training data and explore data augmentation strategies to address imbalanced training data.

## Open Questions the Paper Calls Out
1. How can the inference time of DriVLMe be reduced to meet real-time requirements without significantly sacrificing performance?
2. How can DriVLMe be improved to better handle unexpected situations and world dynamics in autonomous driving scenarios?
3. How can DriVLMe be enhanced to generate more natural and contextually appropriate language, including initiating dialogue and expressing a theory of mind?

## Limitations
- Long inference time exceeding the interval between two decision points
- Imbalanced training data leading to model biases towards frequent actions
- Limited visual understanding due to low image input resolution
- Challenges in handling unexpected situations and world dynamics
- Absence of situated Theory of Mind and inability to initiate dialogue

## Confidence
High confidence in the core technical contributions and model's performance in controlled environments. Medium confidence in generalizability to real-world scenarios due to reliance on simulated data and identified limitations.

## Next Checks
1. Conduct real-world driving tests to evaluate performance in diverse and unpredictable traffic conditions.
2. Investigate data augmentation strategies to address imbalanced training data and improve performance on less frequent actions and scenarios.
3. Explore techniques to enhance visual understanding, such as integrating additional sensor modalities or improving image resolution, to assess impact on decision-making and dialogue generation capabilities.