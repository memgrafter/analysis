---
ver: rpa2
title: 'SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers'
arxiv_id: '2407.09413'
source_url: https://arxiv.org/abs/2407.09413
tags:
- question
- figures
- tables
- questions
- spiqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPIQA introduces a large-scale dataset for multimodal question
  answering on scientific papers, addressing the gap in existing QA datasets that
  focus only on text and overlook figures and tables. The dataset comprises 270K question-answer
  pairs across 26K computer science papers, covering figures, tables, and full-text
  contexts.
---

# SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers

## Quick Facts
- **arXiv ID**: 2407.09413
- **Source URL**: https://arxiv.org/abs/2407.09413
- **Reference count**: 40
- **Primary result**: Large-scale multimodal QA dataset (270K pairs across 26K CS papers) showing fine-tuning improves performance by up to 28 L3Score points

## Executive Summary
SPIQA introduces a comprehensive dataset for multimodal question answering on scientific papers, addressing the gap in existing QA datasets that focus only on text while overlooking figures and tables. The dataset comprises 270K question-answer pairs across 26K computer science papers, covering figures, tables, and full-text contexts. Three tasks—direct QA with figures/tables, full paper, and Chain-of-Thought (CoT) QA—evaluate model comprehension and reasoning. Experiments with 12 models, including GPT-4o, Claude-3, and fine-tuned LLaVA/InstructBLIP, show that fine-tuning significantly improves performance (e.g., 28-point L3Score gain). GPT-4o and Claude-3 achieve state-of-the-art results, while models struggle with complex visualizations and tables. The proposed LLMLogScore (L3Score) metric improves evaluation by leveraging LLM confidence in semantic matching.

## Method Summary
SPIQA is built through a pipeline starting with PDF/TeX collection of 26K computer science papers, followed by figure/table extraction using PDFFigures 2.0. QA pairs are automatically generated using Gemini 1.5 Pro, with manual filtering applied only to the evaluation set. Three tasks are defined: direct QA with figures/tables, full paper, and Chain-of-Thought QA. Models are evaluated using traditional metrics (METEOR, ROUGE-L, CIDEr, BERTScore) plus the proposed L3Score metric. Fine-tuning experiments use LLaVA-1.5 and InstructBLIP with LoRA (rank 32), AdamW optimizer, cosine scheduler, learning rate 1e-5, and batch size 32.

## Key Results
- Fine-tuned InstructBLIP and LLaVA-1.5 achieve massive improvements of 28 and 26 L3Score points respectively over zero-shot models
- GPT-4o and Claude-3 achieve state-of-the-art results with L3Scores of 4.09 and 3.79 on test-A
- High-resolution images are essential for accurate figure understanding, with significant performance degradation at lower resolutions
- Chain-of-Thought QA improves performance by 6.70, 1.73, and 2.98 L3Score for GPT-4 Vision across test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SPIQA enables improved performance through fine-tuning multimodal models on task-specific data
- Mechanism: Models like InstructBLIP and LLaVA achieve significant L3Score gains (28 and 26 points respectively) when fine-tuned on the SPIQA training set compared to zero-shot performance
- Core assumption: The automatically generated QAs in SPIQA are of sufficient quality to provide effective supervision for fine-tuning
- Evidence anchors:
  - [abstract]: "We further explore the upper bounds of performance enhancement with additional textual information, highlighting its promising potential for future research"
  - [section 5.2]: "The fine-tuned InstructBLIP and LLaVA 1.5 obtain a massive improvement of 28 and 26 point L3Score on average over three test sets compared to corresponding zero-shot models"
- Break condition: If the automatically generated QAs contain too many errors or inconsistencies, the fine-tuning process may not converge properly or could learn incorrect patterns

### Mechanism 2
- Claim: L3Score provides more accurate evaluation by incorporating LLM confidence through log-likelihood probabilities
- Mechanism: Instead of using fixed scales, L3Score uses the probability distribution of "yes" vs "no" responses from the LLM to determine semantic similarity between candidate and ground truth answers
- Core assumption: The probability distribution from the LLM's log-likelihood scores correlates with human judgment of answer quality
- Evidence anchors:
  - [section 4]: "We alleviate the necessity of a predefined scale range and detailed interpretation of every point in the scale by proposing LLMLogScore (L3Score), which directly uses the log-likelihood probabilities"
  - [section 5.2]: "Traditional metrics like ROUGE-L and BERTScore fail to evaluate such responses, scoring them as 0"
- Break condition: If the LLM used for scoring is biased or has systematic errors in evaluating semantic similarity, the L3Score will inherit these biases

### Mechanism 3
- Claim: Chain-of-Thought (CoT) QA improves performance by breaking down the reasoning process into retrieval and answering steps
- Mechanism: By first identifying relevant figures and tables before answering, models can focus their attention and reasoning on the most pertinent information
- Core assumption: The step-by-step reasoning process allows models to better ground their answers in visual evidence
- Evidence anchors:
  - [abstract]: "Additionally, we propose a Chain-of-Thought (CoT) evaluation strategy with in-context retrieval that allows fine-grained, step-by-step assessment and improves model performance"
  - [section 5.2]: "For instance, GPT-4 Vision shows an increase of 6.70, 1.73, and 2.98 in L3Score when using CoT prompts compared to direct QA"
- Break condition: If the retrieval step is inaccurate, the subsequent answering step will be working with incorrect or irrelevant information, potentially leading to worse performance than direct QA

## Foundational Learning

- **Concept**: Multimodal understanding
  - Why needed here: The dataset requires models to process both visual elements (figures, tables) and textual content simultaneously
  - Quick check question: Can you explain how a model would process an image of a chart and its caption together to answer a question?

- **Concept**: Long-context reasoning
  - Why needed here: Scientific papers are typically 8-10 pages long, requiring models to maintain and reason over extended contexts
  - Quick check question: What challenges arise when processing full scientific papers versus just abstracts?

- **Concept**: Semantic similarity evaluation
  - Why needed here: Free-form answers require evaluation methods that go beyond exact string matching
  - Quick check question: How would you determine if two different answers to the same question are semantically equivalent?

## Architecture Onboarding

- **Component map**: PDF/TeX collection -> figure/table extraction -> QA generation -> filtering -> model evaluation
- **Critical path**: Data collection and preprocessing -> QA generation using Gemini 1.5 Pro -> manual filtering of evaluation set -> model evaluation on three tasks -> fine-tuning and re-evaluation
- **Design tradeoffs**:
  - Quality vs scale: Automatic QA generation enables large dataset but may introduce noise
  - Resolution vs cost: Full-resolution images provide better performance but increase computational cost
  - Closed vs open models: Closed models achieve better performance but lack transparency
- **Failure signatures**:
  - Low L3Score with high traditional metric scores indicates semantic mismatch
  - Performance degradation when captions are removed suggests caption understanding is critical
  - Open-source models struggling with tables indicates table parsing limitations
- **First 3 experiments**:
  1. Run zero-shot evaluation on test-A with Gemini Pro Vision to establish baseline
  2. Fine-tune InstructBLIP on training set and evaluate on test-A to measure improvement
  3. Test CoT QA setup with Gemini 1.5 Flash to compare against direct QA performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the SPIQA dataset be extended to include other scientific domains beyond computer science?
- Basis in paper: Explicit
- Why unresolved: The paper acknowledges that SPIQA is restricted to computer science papers and that models trained on this dataset may exhibit biases towards specific topics within this domain. Extending the dataset to other scientific fields is identified as a future prospect but is not addressed in the current work.
- What evidence would resolve it: Research demonstrating the feasibility and benefits of expanding SPIQA to other scientific domains, such as biology, physics, or chemistry, with corresponding improvements in model performance on those domains.

### Open Question 2
- Question: What are the limitations of using LLMLogScore (L3Score) for evaluating free-form answers, and how can these limitations be addressed?
- Basis in paper: Inferred
- Why unresolved: While the paper proposes L3Score as an improved metric for free-form QA evaluation, it acknowledges that the absolute L3Score value varies depending on the chosen LLM. The paper recommends using the same LLM consistently across evaluations but does not address other potential limitations or how to overcome them.
- What evidence would resolve it: Comparative studies between L3Score and other evaluation metrics, including human evaluation, to identify strengths, weaknesses, and potential improvements for L3Score.

### Open Question 3
- Question: How can the performance of open-source models on table comprehension be improved, given their current struggles with this task?
- Basis in paper: Explicit
- Why unresolved: The paper reports that open-source models perform poorly on tables compared to closed-source models, even when using the full paper text. This suggests a significant gap in the ability of open-source models to understand and reason about tabular data.
- What evidence would resolve it: Research exploring techniques such as specialized training on table data, architectural modifications for better table understanding, or hybrid approaches combining text and visual table representations to improve open-source model performance on tables.

## Limitations

- Dataset quality uncertainty: Automatic QA generation may introduce noise, with only evaluation set manually verified
- Domain specificity: Dataset restricted to computer science papers, limiting generalizability to other scientific domains
- Evaluation dependency: State-of-the-art performance relies on closed models (GPT-4o, Claude-3) that are not openly available

## Confidence

**High Confidence** (Level 3 or above):
- Fine-tuning multimodal models on SPIQA significantly improves performance across all three tasks
- L3Score provides more reliable evaluation than traditional metrics for free-form answers
- High-resolution images are essential for accurate figure understanding
- Captions are critical for comprehending figures and tables

**Medium Confidence** (Level 2):
- Chain-of-Thought QA strategy improves model performance through step-by-step reasoning
- Current open-weight models struggle with complex tables and visualizations
- Full-text context provides meaningful performance improvements over figures/tables alone

**Low Confidence** (Level 1):
- The specific L3Score threshold of 2.5 accurately separates acceptable from unacceptable answers across all question types
- The performance gap between closed and open models will remain consistent as open models continue to evolve

## Next Checks

1. **Manual Quality Assessment**: Conduct human evaluation of a random sample of 100 training set QAs to quantify the noise level in automatically generated data and its correlation with model performance during fine-tuning.

2. **Cross-Domain Generalization**: Evaluate fine-tuned models on SPIQA-style questions from non-CS scientific papers (e.g., from arXiv categories like physics, biology, or medicine) to assess domain transferability and identify domain-specific challenges.

3. **L3Score Calibration**: Perform controlled experiments comparing L3Score against human judgments across different answer types (short vs. long, figure-based vs. table-based) to validate whether the log-likelihood probability approach consistently aligns with human evaluation standards.