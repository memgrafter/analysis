---
ver: rpa2
title: Guarantee Regions for Local Explanations
arxiv_id: '2402.12737'
source_url: https://arxiv.org/abs/2402.12737
tags:
- anchor
- local
- points
- boxes
- region
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of defining guarantee regions
  for local explanations, particularly for local surrogate models like LIME, which
  may not reliably extrapolate to the local region around a point of interest. The
  authors propose an anchor-based algorithm to identify regions where local explanations
  are guaranteed to be correct by explicitly describing intervals along which input
  features can be trusted.
---

# Guarantee Regions for Local Explanations

## Quick Facts
- arXiv ID: 2402.12737
- Source URL: https://arxiv.org/abs/2402.12737
- Authors: Marton Havasi; Sonali Parbhoo; Finale Doshi-Velez
- Reference count: 15
- Primary result: Proposed anchor-based algorithm finds explanations with larger guarantee regions that better cover the data manifold compared to baselines, and can identify misleading local explanations with significantly poorer guarantee regions

## Executive Summary
This paper addresses the challenge of defining guarantee regions for local explanations, particularly for local surrogate models like LIME, which may not reliably extrapolate to the local region around a point of interest. The authors propose an anchor-based algorithm to identify regions where local explanations are guaranteed to be correct by explicitly describing intervals along which input features can be trusted. Their method produces an interpretable feature-aligned box where the prediction of the local surrogate model is guaranteed to match the predictive model. The primary results show that their algorithm can find explanations with larger guarantee regions that better cover the data manifold compared to existing baselines, and can identify misleading local explanations with significantly poorer guarantee regions.

## Method Summary
The paper proposes a divide-and-conquer algorithm called FindAnchor that incrementally builds anchor boxes by solving subproblems for subsets of features and merging them. The algorithm uses a subroutine FindMB to find maximum boxes given positive and negative points, along with a statistical testing framework to control confidence levels across multiple tests. The method generates local surrogate models (logistic regression or decision trees) for test points by sampling from Gaussian distributions around anchor points, and evaluates the resulting anchor boxes against baseline methods (radial explanations and greedy anchors) on 5 tabular datasets.

## Key Results
- Algorithm consistently results in larger guarantee regions than baseline methods
- Anchor boxes better cover the local data cluster compared to baselines
- Method can identify misleading local explanations by comparing anchor box sizes along features claimed to have zero importance
- Proposed method outperforms radial explanations and greedy anchors in terms of guarantee region size and cluster coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The anchor box can identify when a local surrogate model is misleading by comparing the size of the guaranteed region along a feature claimed to have zero importance.
- Mechanism: When a surrogate model claims a feature has zero weight, the anchor box size along that feature will be small if the original model actually depends on it (dishonest case), but large if it does not (honest case).
- Core assumption: The size of the anchor box along a feature is proportional to how reliably the surrogate can extrapolate along that feature.
- Evidence anchors:
  - [abstract] "We demonstrate that our algorithm can be used to find explanations with larger guarantee regions that better cover the data manifold compared to existing baselines."
  - [section 4.1] "In Figure 2, we see that the size of the anchor box is significantly smaller in the dishonest case than the honest ones on all datasets."
  - [corpus] Weak - no direct corpus support found for this specific comparison mechanism.

### Mechanism 2
- Claim: The divide-and-conquer strategy enables tractable computation of large anchor boxes in high dimensions by recursively building up from one-dimensional solutions.
- Mechanism: The algorithm starts with anchor boxes for individual features, then merges them dimension by dimension, restricting the search space at each step based on previously found solutions.
- Core assumption: The anchor box for a set of features is nested within the anchor box for any subset of those features (nestedness property).
- Evidence anchors:
  - [section 3.2] "Our solution will adapt and use as a subroutine an efficient algorithm for the maximum-box problem... To apply it to our anchor box problem, we make three modifications to FindMB."
  - [section 3.2] "The challenge to use FindMB to find the new anchor box is that in our setting, we do not have a fixed, finite set of positive and negative points."
  - [corpus] Weak - no direct corpus support found for this specific divide-and-conquer approach.

### Mechanism 3
- Claim: The statistical guarantee ensures the surrogate model is faithful within the resulting anchor box with high confidence.
- Mechanism: The algorithm uses statistical tests at each merge step with carefully chosen significance levels that sum to no more than the desired overall confidence level δ.
- Core assumption: The statistical tests correctly control the type I error rate across multiple hypothesis tests.
- Evidence anchors:
  - [section 3.2] "To ensure that the levels for all our significance tests sum to no more than our desired level δ, that is, P i δi ≤ δ, we have each successive test have significance level δi = δ i log2(i+1) P∞ j=1 1 j log2 (j+1)."
  - [section 3.2] "This change is needed to bound the run time when the search tree is very large."
  - [corpus] Weak - no direct corpus support found for this specific multiple testing correction approach.

## Foundational Learning

- Concept: Purity of anchor boxes
  - Why needed here: The algorithm needs to ensure that the anchor box contains mostly points where the surrogate model is faithful to the original model
  - Quick check question: What does it mean for an anchor box to have purity ρ, and how is it formally defined?

- Concept: Statistical hypothesis testing with multiple comparisons
  - Why needed here: The algorithm performs multiple statistical tests during the merge process and needs to control the overall error rate
  - Quick check question: How does the algorithm ensure that the sum of significance levels across all tests doesn't exceed the desired confidence level δ?

- Concept: Exponential complexity in high dimensions
  - Why needed here: Understanding why the naive approach of finding the maximum anchor box is intractable helps justify the divide-and-conquer approach
  - Quick check question: According to Theorem 3.2, how many function evaluations are needed in the worst case to identify the maximum anchor box up to a factor r?

## Architecture Onboarding

- Component map:
  - FindAnchor: Main recursive function that builds up anchor boxes dimension by dimension
  - SolveRestricted: Function that finds anchor boxes for a subset of dimensions using FindMB
  - FindMB: Subroutine that finds maximum boxes given positive and negative points
  - Statistical testing framework: Controls confidence levels across multiple tests

- Critical path:
  1. Start with FindAnchor called on all dimensions
  2. Recursively split feature set and solve for subsets
  3. Merge solutions dimension by dimension using SolveRestricted
  4. Use FindMB with statistical tests to find valid anchor boxes
  5. Return the final anchor box

- Design tradeoffs:
  - Computational cost vs. guarantee quality: More iterations in FindMB and more positive points N improve results but increase runtime
  - Dimension complexity: The divide-and-conquer approach trades off optimality for tractability in high dimensions
  - Statistical confidence vs. box size: Higher confidence requirements (lower δ) may result in smaller anchor boxes

- Failure signatures:
  - Very small anchor boxes along features claimed to have zero importance may indicate a dishonest surrogate model
  - Anchor boxes that don't extend to the bounding box may indicate regions where the surrogate fails to extrapolate
  - Runtime that grows exponentially with dimension may indicate violation of the nestedness property

- First 3 experiments:
  1. Test the honesty detection mechanism by creating a surrogate that masks an important feature and verifying the anchor box is small along that feature
  2. Compare the divide-and-conquer approach against a naive greedy approach on a low-dimensional dataset to verify it finds larger boxes
  3. Verify the statistical guarantees by running multiple trials and checking the empirical purity of the resulting anchor boxes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the divide-and-conquer algorithm be adapted to find optimal anchor boxes when the nestedness property does not hold?
- Basis in paper: [explicit] The authors state that the algorithm finds the largest anchor box in the limit when the nestedness property holds, but that it still leads to a good solution when the property does not hold, though no formal guarantees can be made.
- Why unresolved: The paper does not provide a method or proof for finding optimal anchor boxes when the nestedness property is violated.
- What evidence would resolve it: A proof that the algorithm can find optimal anchor boxes without the nestedness property, or an algorithm that guarantees optimality without relying on nestedness.

### Open Question 2
- Question: How does the performance of the anchor box method scale with the number of dimensions in the input space?
- Basis in paper: [inferred] The paper shows that the number of function evaluations needed to identify the maximum anchor box grows exponentially with the number of dimensions, making it intractable for high-dimensional data.
- Why unresolved: The paper does not provide empirical results on how the anchor box method performs as the number of dimensions increases beyond the datasets used in the experiments.
- What evidence would resolve it: Experimental results showing the performance of the anchor box method on datasets with varying numbers of dimensions, particularly high-dimensional datasets.

### Open Question 3
- Question: Can the anchor box method be extended to handle non-axis-aligned local explanations?
- Basis in paper: [explicit] The paper focuses on finding axis-aligned anchor boxes, but the authors mention that the method could be adapted to handle other types of explanations.
- Why unresolved: The paper does not provide a method or proof for extending the anchor box method to handle non-axis-aligned explanations.
- What evidence would resolve it: A method for extending the anchor box method to handle non-axis-aligned explanations, along with experimental results showing its performance compared to the axis-aligned method.

## Limitations
- The nestedness property assumed in the divide-and-conquer approach may not hold for non-linear surrogates or when D ≥ 3, potentially leading to suboptimal anchor boxes.
- The statistical testing framework requires careful calibration of significance levels across multiple tests, and violations of the uniform distribution assumption for test points could invalidate confidence guarantees.
- The method's performance depends heavily on the quality of the surrogate model's local training data and the choice of anchor points.

## Confidence
- High: The anchor box provides interpretable guarantee regions for local explanations
- Medium: The divide-and-conquer approach efficiently finds large anchor boxes in high dimensions
- Medium: The statistical framework provides valid confidence guarantees for the resulting anchor boxes

## Next Checks
1. Test the nestedness property assumption by comparing anchor boxes found by the divide-and-conquer approach against those found by exhaustive search on small-dimensional datasets.
2. Validate the statistical confidence guarantees by running multiple trials and empirically measuring the purity of the resulting anchor boxes.
3. Evaluate the method's robustness to violations of the uniform distribution assumption by testing on datasets with highly non-uniform feature distributions.