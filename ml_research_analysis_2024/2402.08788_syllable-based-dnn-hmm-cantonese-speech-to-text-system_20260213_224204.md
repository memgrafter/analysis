---
ver: rpa2
title: Syllable based DNN-HMM Cantonese Speech to Text System
arxiv_id: '2402.08788'
source_url: https://arxiv.org/abs/2402.08788
tags:
- speech
- training
- system
- modeling
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a Cantonese speech-to-text system that uses
  syllable-based acoustic modeling with deep neural networks and hidden Markov models.
  The system improves on conventional initial-final syllable models by splitting finals
  into nucleus and coda units, better reflecting Cantonese intra-syllable variations.
---

# Syllable based DNN-HMM Cantonese Speech to Text System

## Quick Facts
- arXiv ID: 2402.08788
- Source URL: https://arxiv.org/abs/2402.08788
- Reference count: 0
- Primary result: ONC-based syllable acoustic modeling with I-vector adaptation achieves 9.66% WER and 1.39 RTF

## Executive Summary
This work presents a Cantonese speech-to-text system that uses syllable-based acoustic modeling with deep neural networks and hidden Markov models. The system improves on conventional initial-final syllable models by splitting finals into nucleus and coda units, better reflecting Cantonese intra-syllable variations. It is trained using the Kaldi toolkit with stochastic gradient descent and GPU acceleration, and incorporates I-vector speaker adaptation. Experiments show the onset-nucleus-coda (ONC) model with I-vector adaptation achieves a word error rate of 9.66% and a real-time factor of 1.39, outperforming the baseline initial-final model. The improvements are attributed to the finer acoustic unit structure and speaker adaptation.

## Method Summary
The system uses a hybrid DNN-HMM architecture trained on the CUCorpora Cantonese speech dataset with 68 training speakers and 12 test speakers. It implements onset-nucleus-coda (ONC) syllable modeling where finals are split into nucleus and coda units, reducing the phone set size from 53 to 24. I-vector speaker adaptation with 100-dimensional vectors is applied by concatenating I-vectors with GMM-SAT features. The DNN uses 4 hidden layers with p-norm nonlinearity and 2,268 output units. Training uses Kaldi toolkit with stochastic gradient descent and GPU acceleration. Decoding employs lattice-based rescoring with 4-gram and RNN language models.

## Key Results
- ONC-based syllable acoustic modeling with I-vector adaptation achieves 9.66% WER
- Real-time factor of 1.39 demonstrates efficient decoding
- ONC model with I-vectors outperforms baseline initial-final model

## Why This Works (Mechanism)

### Mechanism 1
Splitting finals into nucleus and coda (ONC scheme) reduces intra-syllable variation modeling error. Cantonese finals contain both vowel nuclei and consonant codas that vary contextually; by modeling them separately, the system can train each elementary phoneme from more diverse acoustic contexts, reducing confusion from sound mergers (e.g., "baat" vs "baak"). Core assumption: Cantonese finals are not acoustically distinctive across all characters; splitting them allows cleaner training signals for each sub-unit. Evidence anchors: [abstract] "splitting finals into nucleus and coda units, better reflecting Cantonese intra-syllable variations"; [section] "The ONC scheme can halve the size of the phone set for modeling the finals, which is from 53 to 24". Break condition: If Cantonese finals remain highly distinctive across all syllables, the added complexity of ONC offers no benefit.

### Mechanism 2
Adding 100-dimensional I-vectors as speaker-adapted features improves recognition accuracy. I-vectors capture per-speaker acoustic characteristics; concatenating them with GMM-SAT features provides speaker-normalized input to DNN, reducing mismatch between training and test speakers. Core assumption: Speaker variability is a major source of error; modeling it explicitly improves DNN generalization. Evidence anchors: [abstract] "incorporates I-vector speaker adaptation" and "ONC-based syllable acoustic modeling with I-vector based DNN-HMM achieves the best performance"; [section] "Both training and decoding use one I-vector per speaker". Break condition: If test speakers are not represented in training data, I-vectors cannot adapt effectively.

### Mechanism 3
Lattice-based rescoring with RNNLM improves WER by capturing long-distance dependencies. First-pass decoding uses n-gram LM; rescoring with RNNLM refines hypotheses by modeling context beyond fixed n-gram windows. Core assumption: Conventional n-grams fail on long-distance dependencies; RNNLMs can represent them better. Evidence anchors: [abstract] "lattice-based rescoring" and "LM-RNNLM of size 39.6M is trained"; [section] "Type 1 errors are mostly due the AM dealing with multiple identical or similar pronunciations so that recognizer has to rely on LM to choose the correct one". Break condition: If RNNLM training data is too small or domain-mismatched, rescoring may not help.

## Foundational Learning

- **Concept**: Syllable-based acoustic modeling
  - Why needed here: Cantonese is monosyllabic; modeling at syllable level aligns with phonological structure and improves generalization over phone-level modeling.
  - Quick check question: How many base syllables are there in Cantonese, and how are they divided into initials and finals?

- **Concept**: Hidden Markov Models for temporal alignment
  - Why needed here: DNNs output frame-level posteriors; HMMs provide temporal smoothing and state transitions to handle speech dynamics.
  - Quick check question: What is the state topology (left-right, self-loop) used for each phone in the baseline system?

- **Concept**: Speaker adaptation via I-vectors
  - Why needed here: Speaker variability degrades recognition; I-vectors offer low-dimensional speaker embeddings to normalize acoustic features.
  - Quick check question: What is the dimensionality of the I-vector extractor used in this system?

## Architecture Onboarding

- **Component map**: MFCC feature extraction → CMVN + delta/delta-delta → LDA+MLLT → GMM-SAT with fMLLR → optional I-vector → spliced 15-frame input → 4 hidden-layer p-norm DNN → softmax output (2,268 senones) → WFST decoding with LM2 → lattice rescoring with LM4 and LM-RNNLM
- **Critical path**: Feature extraction → speaker adaptation (I-vector) → DNN training → decoding with first-pass LM → lattice rescoring
- **Design tradeoffs**: ONC reduces phone set size but increases modeling complexity; I-vectors improve accuracy at cost of extra feature computation and slight RTF loss; lattice rescoring adds WER gains but increases latency
- **Failure signatures**: High WER on same speaker → I-vector extractor or GMM-SAT mismatch; WER degrades on new speakers → insufficient speaker coverage in training; RTF too high → large beam width or max_active_states; lattice rescoring ineffective → RNNLM domain mismatch
- **First 3 experiments**:
  1. Train baseline IF-based DNN-HMM without I-vectors; measure WER and RTF
  2. Add I-vectors to IF-based system; tune beam width and max_active_states to balance WER vs RTF
  3. Switch to ONC-based units with I-vectors; compare WER/RTF to previous runs and analyze error type shifts

## Open Questions the Paper Calls Out

### Open Question 1
Does the ONC-based model's improved performance stem primarily from better modeling of intra-syllable variations, or are there other contributing factors? Basis in paper: [explicit] The paper attributes improvements to the finer acoustic unit structure and speaker adaptation, but does not isolate the impact of intra-syllable variation modeling. Why unresolved: The paper combines ONC modeling with I-vector speaker adaptation, making it difficult to attribute performance gains to either factor alone. What evidence would resolve it: Conducting experiments comparing ONC without speaker adaptation to IF with speaker adaptation would isolate the impact of intra-syllable variation modeling.

### Open Question 2
How does the ONC model perform on spontaneous speech or speech with significant dialectal variations? Basis in paper: [inferred] The paper uses a carefully transcribed corpus, but does not evaluate performance on more challenging speech data. Why unresolved: The paper focuses on a specific dataset and does not explore performance on diverse speech conditions. What evidence would resolve it: Testing the ONC model on spontaneous speech datasets or datasets with known dialectal variations would provide insights into its robustness.

### Open Question 3
Can the ONC model be further improved by incorporating contextual information beyond the current triphone context? Basis in paper: [inferred] The paper uses a triphone context, but does not explore the potential benefits of incorporating longer-range dependencies. Why unresolved: The paper does not investigate the impact of using more complex context modeling techniques. What evidence would resolve it: Experimenting with different context modeling approaches, such as using longer-range dependencies or neural network-based context modeling, would reveal the potential for further improvements.

## Limitations
- Evaluation based on single proprietary dataset with limited speaker diversity
- Specific benefits of ONC scheme not fully quantified independently from I-vector adaptation
- No analysis of performance on spontaneous speech or dialectal variations

## Confidence

- **High Confidence**: Standard methodology using Kaldi toolkit with SGD and GPU acceleration; well-established ONC modeling and I-vector adaptation techniques
- **Medium Confidence**: Reported WER of 9.66% and RTF of 1.39 specific to CUCorpora dataset; improvement claims over baseline supported but could benefit from independent validation
- **Low Confidence**: Specific benefits of ONC scheme over initial-final models not fully isolated; I-vector effectiveness across diverse speaker conditions not thoroughly explored; RNNLM rescoring contribution not clearly quantified

## Next Checks

1. **Corpus Diversity Analysis**: Conduct experiments using multiple Cantonese speech corpora (e.g., HKUST, HKCanCor) to validate whether the ONC-based system maintains its performance advantage across different datasets and speaker populations.

2. **Ablation Study for Adaptation Techniques**: Perform systematic ablation studies removing I-vector adaptation and lattice-based rescoring separately to quantify their individual contributions to the final WER improvement, and test the system's performance on unseen speakers.

3. **Error Analysis Across Tonal Variations**: Analyze the system's performance on different Cantonese tonal patterns and finals to determine whether the ONC scheme specifically helps with tonal confusion or other phonetic challenges in Cantonese speech.