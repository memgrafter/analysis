---
ver: rpa2
title: 'Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning'
arxiv_id: '2411.05193'
source_url: https://arxiv.org/abs/2411.05193
tags:
- language
- learning
- offline
- policy
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Q-learning via Supervised Fine-tuning (Q-SFT),
  a novel offline RL algorithm for fine-tuning large language models (LLMs) and vision-language
  models (VLMs) on multi-turn tasks. The key idea is to cast Q-learning as a weighted
  cross-entropy loss over token probabilities, enabling the model to learn Q-values
  without reinitializing weights or adding new heads.
---

# Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning
## Quick Facts
- arXiv ID: 2411.05193
- Source URL: https://arxiv.org/abs/2411.05193
- Reference count: 14
- Proposes a novel offline RL algorithm for fine-tuning LLMs/VLMs on multi-turn tasks using Q-learning via supervised fine-tuning

## Executive Summary
This paper introduces Q-SFT, a novel offline RL algorithm that enables large language models to learn Q-values through supervised fine-tuning without reinitializing weights or adding new heads. By casting Q-learning as a weighted cross-entropy loss over token probabilities, the method leverages pretrained likelihoods while addressing traditional value-based RL instability. The approach shows strong empirical performance across diverse tasks including language games, web-based interactions, vision-based navigation, and robotic manipulation, demonstrating effectiveness in learning from suboptimal data and achieving competitive results with larger models while requiring less hyperparameter tuning.

## Method Summary
Q-SFT transforms Q-learning into a supervised fine-tuning problem by computing weighted cross-entropy losses where token probabilities are weighted by estimated Q-values. This allows the model to learn value functions directly from behavior data without the instability typically associated with value-based RL methods. The approach maintains the original model architecture while effectively learning from suboptimal demonstrations, combining the benefits of maximum likelihood pretraining with reinforcement learning objectives. The method is theoretically grounded and designed to work with multi-turn tasks where traditional supervised fine-tuning struggles due to the sequential decision-making nature of the problems.

## Key Results
- Outperforms supervised fine-tuning and other value-based RL methods on diverse tasks including Chess, Wordle, Twenty Questions, WebShop, ALFWorld, and robotic manipulation
- Demonstrates effectiveness in learning from suboptimal data and benefits significantly from pretraining
- Achieves competitive performance with larger models while requiring less hyperparameter tuning
- Shows strong empirical performance across language games, interactive web tasks, vision-based navigation, and robotic applications

## Why This Works (Mechanism)
Q-SFT works by reframing the Q-learning problem as a supervised learning task where the model learns to predict action values through weighted token probabilities. The key insight is that by using the existing transformer architecture and pretraining as a foundation, the method can learn value functions without the instability of traditional value-based RL approaches. The weighted cross-entropy loss effectively captures the long-term value of actions while maintaining the probabilistic outputs that LLMs naturally produce. This approach bridges the gap between the stable optimization of supervised learning and the goal-directed behavior of reinforcement learning.

## Foundational Learning
- **Q-learning fundamentals**: Understanding temporal difference learning and value function estimation is crucial for grasping how Q-SFT adapts these concepts to language models. Quick check: Verify understanding of Bellman equation and its application to token-level decisions.
- **Transformer architecture**: Knowledge of self-attention mechanisms and how LLMs generate probabilities over token sequences is essential for understanding how Q-values can be represented. Quick check: Confirm understanding of how token probabilities relate to action selection.
- **Supervised fine-tuning**: Familiarity with how pretrained models are adapted to specific tasks through cross-entropy loss provides context for Q-SFT's approach. Quick check: Review standard SFT loss formulation and its limitations for sequential decision tasks.
- **Offline RL**: Understanding the challenges of learning from fixed datasets without environment interaction is key to appreciating Q-SFT's contribution. Quick check: Compare offline vs online RL approaches and their respective challenges.

## Architecture Onboarding
- **Component map**: Pretrained LLM -> Q-SFT weighted loss computation -> Fine-tuned LLM with learned Q-values
- **Critical path**: Input sequence → Model forward pass → Q-value estimation → Weighted cross-entropy loss → Parameter update
- **Design tradeoffs**: Uses existing architecture (no new heads) vs potential limitations in expressiveness; stable supervised learning optimization vs potential loss of exploration benefits; offline learning vs inability to adapt to new situations
- **Failure signatures**: Poor performance on long-horizon tasks, inability to escape local optima from suboptimal data, overfitting to specific behavior patterns in training data
- **First experiments**: 1) Test on simple sequential decision tasks with known optimal policies, 2) Compare learning curves with standard SFT on multi-turn tasks, 3) Evaluate robustness to varying quality of demonstration data

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, focusing instead on presenting the method and empirical results. However, implicit questions include how the method scales to more complex real-world tasks and whether the approach can be extended to continuous action spaces.

## Limitations
- Generalizability across different domains and task types remains uncertain
- Computational efficiency compared to other RL approaches is not well-established
- Limited empirical validation on a specific set of tasks and models
- No detailed analysis of performance in real-world applications with varying data quality
- Potential issues with overfitting and robustness to noisy or adversarial data are not addressed

## Confidence
- High confidence: Theoretical framework and core idea of weighted cross-entropy loss are well-founded
- Medium confidence: Empirical results are promising but generalizability to other tasks is uncertain
- Low confidence: Computational efficiency and scalability to larger models are not well-established

## Next Checks
1. Evaluate Q-SFT on a broader range of tasks with different characteristics and complexities to assess generalizability
2. Conduct detailed analysis of computational efficiency compared to other RL approaches, including training time and resource usage
3. Investigate robustness to noisy or adversarial data and assess performance in real-world applications with varying data quality