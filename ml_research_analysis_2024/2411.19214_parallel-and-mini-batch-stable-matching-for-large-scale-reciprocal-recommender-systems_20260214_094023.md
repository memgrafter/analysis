---
ver: rpa2
title: Parallel and Mini-Batch Stable Matching for Large-Scale Reciprocal Recommender
  Systems
arxiv_id: '2411.19214'
source_url: https://arxiv.org/abs/2411.19214
tags:
- matching
- ipfp
- mini-batch
- size
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to improve the computational efficiency
  and memory usage of stable matching algorithms for large-scale reciprocal recommender
  systems. The core idea is to use parallel and mini-batch computations to solve the
  transferable utility matching problem, which is formulated as an entropy-regularized
  optimal transport problem.
---

# Parallel and Mini-Batch Stable Matching for Large-Scale Reciprocal Recommender Systems

## Quick Facts
- arXiv ID: 2411.19214
- Source URL: https://arxiv.org/abs/2411.19214
- Authors: Kento Nakada; Kazuki Kawamura; Ryosuke Furukawa
- Reference count: 40
- Key outcome: Proposed parallel and mini-batch IPFP methods can process computations for up to a million users while maintaining matching probabilities equivalent to conventional IPFP.

## Executive Summary
This paper addresses the computational challenges of stable matching in large-scale reciprocal recommender systems by proposing parallel and mini-batch implementations of the Iterative Proportional Fitting Procedure (IPFP). The authors reformulate the transferable utility matching problem as an entropy-regularized optimal transport problem, enabling efficient solution via IPFP. Their approach leverages GPU acceleration and factor vector representations to achieve both speed and memory efficiency, demonstrating scalability to datasets with up to one million users while maintaining the same matching quality as traditional methods.

## Method Summary
The method reformulates the stable matching problem as an entropy-regularized optimal transport problem, which can be solved using the Iterative Proportional Fitting Procedure (IPFP). The paper introduces two key innovations: parallel IPFP using GPU-accelerated matrix-vector products for batch updates, and mini-batch IPFP that computes updates in smaller chunks using factor vectors to reduce memory consumption. The mini-batch approach maintains the same matching probabilities as batch IPFP while enabling scalability to much larger datasets by avoiding the need to store full preference matrices in memory.

## Key Results
- Mini-batch IPFP maintains matching probabilities equivalent to batch IPFP while reducing memory usage from O(nm) to O(n + m) where n and m are the number of users
- GPU acceleration of batch IPFP provides significant speedups compared to CPU implementation, with computation time per iteration decreasing as problem size increases
- The method successfully scales to datasets with up to one million users, demonstrating practical applicability for large-scale reciprocal recommender systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The entropy-regularized optimal transport formulation allows the transferable utility matching problem to be solved via iterative proportional fitting (IPFP), which converges to a stable matching state.
- Mechanism: By modeling the matching as an optimal transport problem, the IPFP algorithm performs coordinate descent on the scaling vectors (u, v), iteratively updating them using matrix-vector products until convergence.
- Core assumption: The random utilities follow i.i.d. type-I extreme value distributions with scale parameter β.
- Evidence anchors:
  - [abstract] "The core idea is to use parallel and mini-batch computations to solve the transferable utility matching problem, which is formulated as an entropy-regularized optimal transport problem."
  - [section] "Equation (2) is an OT problem with entropy regularization. We adopt the IPFP proposed in [9], a coordinate decent method to solve (2)."
  - [corpus] Weak - related papers focus on fairness and bias in reciprocal recommenders, not on OT formulation.
- Break condition: If the utility distributions deviate significantly from the assumed extreme value distribution, the equivalence between the stable matching and the OT problem may break.

### Mechanism 2
- Claim: Parallel computation of the IPFP updates using matrix-vector products significantly speeds up convergence.
- Mechanism: The IPFP updates can be rewritten as matrix-vector multiplications (u ← √(n + s² - s) where s = 1/2 Av, v ← √(m + s² - s) where s = 1/2 Aᵀu), which can be efficiently computed using parallel processing on GPUs.
- Core assumption: The preference score matrix A can fit into GPU memory for batch updates.
- Evidence anchors:
  - [section] "Equation (7) can be further expressed in matrix-vector arithmetic... Because Equation (7) only involves the matrix-vector product, it can be efficiently computed through parallel computation."
  - [section] "Figure 5 presents the average computation time per iteration for each method. The GPU implementation of IPFP is faster than the CPU implementation for both batch and mini-batch."
  - [corpus] Weak - related papers discuss scalability but not GPU parallelization specifically.
- Break condition: If the preference matrix is too large to fit in GPU memory, batch updates become infeasible, necessitating mini-batch approaches.

### Mechanism 3
- Claim: Mini-batch IPFP reduces memory consumption by computing matrix-vector products in smaller chunks while maintaining the same matching probabilities as batch IPFP.
- Mechanism: The user sets are partitioned into mini-batches, and the IPFP updates are performed sequentially on each mini-batch, computing the matrix-vector products online using factor vectors rather than precomputing the full preference matrix.
- Core assumption: The user preferences can be accurately represented using factor vectors (e.g., from matrix factorization) and computed on-the-fly.
- Evidence anchors:
  - [section] "To alleviate the memory space limitation, we further propose a memory-efficient update method, mini-batch IPFP... user sets can be substituted for the IPFP algorithm to be executed online."
  - [section] "Figure 6 presents the computation time and memory usage of mini-batch IPFP for large datasets with different batch sizes... memory usage remains linear scaling regardless of batch size."
  - [corpus] Weak - related papers discuss memory-efficient algorithms but not specifically for mini-batch OT.
- Break condition: If the factor vectors cannot accurately approximate the full preference matrix, the matching probabilities may deviate from the batch IPFP results.

## Foundational Learning

- Concept: Iterative Proportional Fitting (IPFP) algorithm
  - Why needed here: IPFP is the core optimization method used to solve the entropy-regularized optimal transport problem formulation of the stable matching problem.
  - Quick check question: What are the update equations for the scaling vectors u and v in IPFP, and how do they relate to the preference matrix A?

- Concept: Entropy-regularized optimal transport
  - Why needed here: The stable matching problem is reformulated as an entropy-regularized optimal transport problem, which allows the use of efficient OT algorithms like IPFP.
  - Quick check question: How does the entropy regularization term in the objective function affect the resulting matching distribution?

- Concept: Matrix factorization for preference representation
  - Why needed here: Mini-batch IPFP relies on representing user preferences using factor vectors, which can be computed on-the-fly to avoid storing the full preference matrix.
  - Quick check question: How are the unilateral preferences p_x,y and q_y,x computed from the factor vectors f_x, g_y, k_x, and l_y?

## Architecture Onboarding

- Component map: Input factor matrices F, K, G, L -> IPFP algorithm (batch or mini-batch) -> Output stable matching probability matrix μ
- Critical path:
  1. Compute preference matrix A (batch) or factor matrices (mini-batch)
  2. Initialize scaling vectors u, v
  3. Iterate IPFP updates until convergence or max iterations
  4. Compute stable matching probabilities from final scaling vectors
- Design tradeoffs:
  - Batch IPFP: Faster convergence, higher memory usage
  - Mini-batch IPFP: Slower convergence, lower memory usage, scalable to larger datasets
  - GPU acceleration: Faster computation, limited by GPU memory
- Failure signatures:
  - Out-of-memory errors: Preference matrix too large for available memory
  - Slow convergence: Poor initialization of scaling vectors or ill-conditioned preference matrix
  - Inaccurate matching: Factor vectors cannot accurately approximate preferences
- First 3 experiments:
  1. Verify that batch IPFP on CPU produces the same results as the reference implementation for small datasets.
  2. Compare the computation time and memory usage of batch vs. mini-batch IPFP for increasing dataset sizes.
  3. Test the scalability of mini-batch IPFP on GPU for very large datasets (e.g., 1 million users) and verify the matching probabilities against batch IPFP.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal batch size for mini-batch IPFP that balances computational efficiency and memory usage across different hardware configurations?
- Basis in paper: [inferred] The paper experiments with batch sizes of {1, 10, 100} but notes that optimal selection depends on hardware constraints and data dimensions.
- Why unresolved: The paper demonstrates that memory usage remains linear regardless of batch size due to JAX's memory processing, but does not establish a general rule for optimal batch size selection across different scenarios.
- What evidence would resolve it: Systematic experiments comparing various batch sizes (e.g., 1, 10, 50, 100, 500) across different hardware configurations and problem dimensions, measuring both memory usage and computation time to identify optimal trade-offs.

### Open Question 2
- Question: How does the performance of the proposed mini-batch IPFP method compare to other approximation techniques for large-scale stable matching problems, such as low-rank Sinkhorn factorization?
- Basis in paper: [explicit] The conclusion mentions that future work plans to apply acceleration techniques developed in optimal transport problems, including low-rank Sinkhorn factorization.
- Why unresolved: The paper focuses on parallel and mini-batch computations but does not compare its performance against other approximation methods that have been developed for optimal transport problems.
- What evidence would resolve it: Direct experimental comparisons between mini-batch IPFP and other approximation techniques (e.g., low-rank Sinkhorn factorization) on the same datasets, measuring both accuracy and computational efficiency.

### Open Question 3
- Question: What is the impact of early stopping criteria on the computational efficiency of mini-batch IPFP, and how can it be effectively implemented for reciprocal recommender systems?
- Basis in paper: [inferred] The conclusion mentions that practical applications could benefit from early stopping criteria to reduce overall computation time, but this is not explored in the experiments.
- Why unresolved: While the paper demonstrates the basic functionality of mini-batch IPFP, it does not investigate how early stopping criteria might affect both computation time and the quality of the matching results.
- What evidence would resolve it: Experiments implementing various early stopping criteria (e.g., based on stability of rankings, convergence thresholds) and measuring their impact on both computation time and matching quality across different problem sizes and dimensions.

## Limitations

- The assumption that user preferences follow i.i.d. type-I extreme value distributions may not hold in real-world scenarios, potentially affecting the equivalence between stable matching and the OT formulation
- Limited validation on real-world datasets beyond the Libimseti dating platform data
- The scalability claims are primarily based on synthetic data experiments
- The convergence guarantees for mini-batch IPFP in the presence of factor approximation errors are not rigorously established

## Confidence

- High confidence in the parallel batch IPFP implementation and its GPU acceleration benefits
- Medium confidence in the memory efficiency claims of mini-batch IPFP, as real-world performance may vary based on preference matrix structure
- Low confidence in the general applicability of the extreme value distribution assumption across different domains

## Next Checks

1. Test the stability of matching results when user preferences deviate from the assumed extreme value distribution by introducing controlled perturbations in synthetic experiments
2. Evaluate the mini-batch IPFP on additional real-world datasets from different domains (e.g., professional networking, housing allocation) to assess generalizability
3. Conduct a rigorous convergence analysis of mini-batch IPFP, comparing the number of iterations required to reach a stable state versus batch IPFP across various dataset sizes and factor vector qualities