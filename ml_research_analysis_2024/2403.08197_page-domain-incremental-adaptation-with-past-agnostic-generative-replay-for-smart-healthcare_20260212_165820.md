---
ver: rpa2
title: 'PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for
  Smart Healthcare'
arxiv_id: '2403.08197'
source_url: https://arxiv.org/abs/2403.08197
tags:
- data
- page
- training
- synthetic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes PAGE, a domain-incremental adaptation strategy
  with past-agnostic generative replay for smart healthcare applications using wearable
  medical sensors (WMSs). The key challenge addressed is catastrophic forgetting in
  machine learning models when adapting to new data domains without access to previously
  learned data due to privacy and storage constraints.
---

# PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for Smart Healthcare

## Quick Facts
- arXiv ID: 2403.08197
- Source URL: https://arxiv.org/abs/2403.08197
- Reference count: 40
- The paper proposes PAGE, a domain-incremental adaptation strategy with past-agnostic generative replay for smart healthcare applications using wearable medical sensors (WMSs).

## Executive Summary
This paper introduces PAGE, a domain-incremental adaptation strategy that addresses catastrophic forgetting in machine learning models for smart healthcare applications using wearable medical sensors. The key innovation is using synthetic data generation to retain knowledge from previous domains without storing any past data, combined with extended inductive conformal prediction (EICP) for interpretable predictions with statistical guarantees. Experiments on three disease datasets demonstrate PAGE's competitive performance against state-of-the-art methods while requiring no preserved data from prior domains, enabling significant clinical workload reduction through interpretable predictions.

## Method Summary
PAGE is a domain-incremental adaptation strategy that mitigates catastrophic forgetting without storing any past data. It uses a synthetic data generation (SDG) module based on Gaussian mixture models (GMM) to generate synthetic data representing previous domains during new domain adaptation. The model is updated using both real data from new domains and the synthetic data. PAGE incorporates extended inductive conformal prediction (EICP) to provide confidence scores and credibility values for each prediction, offering statistical guarantees and interpretability. The method is designed for privacy preservation and scalability, as it does not require storing raw data or distilled knowledge from learned domains.

## Key Results
- PAGE achieves highly competitive performance against state-of-the-art methods in domain-incremental adaptation for disease detection using WMSs
- The method successfully mitigates catastrophic forgetting without requiring any preserved data from previous domains
- EICP integration provides interpretable predictions with statistical guarantees, potentially reducing clinical workload by up to 75%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PAGE mitigates catastrophic forgetting without storing any past data.
- Mechanism: It uses synthetic data generation based on probability density estimation (GMM) to replay knowledge from previous domains during new domain adaptation.
- Core assumption: The GMM can accurately model the joint distribution of previous domain data and generate synthetic samples that retain the learned knowledge.
- Evidence anchors:
  - [abstract] "When adapting to a new domain, it exploits real data from the new distribution and the current model to generate synthetic data that retain the learned knowledge of previous domains."
  - [section] "When adapting to new domains, PAGE does not rely on replaying raw preserved data or exploiting distilled information from learned domains to mitigate CF. Instead, it takes advantage of real data from new domains to generate synthetic data that retain learned knowledge for replay."
  - [corpus] No direct corpus evidence found.
- Break condition: If the GMM fails to capture the true data distribution, the synthetic data will not represent the previous domains accurately, leading to catastrophic forgetting.

### Mechanism 2
- Claim: PAGE provides statistical guarantees and model interpretability through Extended Inductive Conformal Prediction (EICP).
- Mechanism: EICP extends ICP by incorporating a data selection module that chooses data instances with high training loss values for the calibration set, improving the model's ability to measure non-conformity.
- Core assumption: Data instances with higher training loss values are more informative for calibrating non-conformity in new data predictions.
- Evidence anchors:
  - [abstract] "In addition, we incorporate an extended inductive conformal prediction (EICP) method into PAGE to produce a confidence score and a credibility value for each detection result."
  - [section] "We take inspiration from a framework called clustering training losses for label error detection (CTRL) to construct the DS module. We look at the average training loss of each training data instance to evaluate how suitable the instance is for selection in the extended calibration set."
  - [corpus] No direct corpus evidence found.
- Break condition: If the training loss values do not correlate with the importance of data for non-conformity calibration, the EICP will not improve prediction interpretability.

### Mechanism 3
- Claim: PAGE is highly scalable and preserves patient privacy by not storing any past data.
- Mechanism: By generating synthetic data on-the-fly and not requiring any data preservation, PAGE avoids the storage overhead and privacy concerns associated with storing patient data.
- Core assumption: Synthetic data generated by PAGE are sufficient to retain the knowledge of previous domains without the need for actual data storage.
- Evidence anchors:
  - [abstract] "PAGE has very low memory storage consumption since it does not store data or information from learned domains. Hence, PAGE is highly scalable to multi-domain disease detection while, at the same time, preserving patient privacy."
  - [section] "Preserving raw data or distilled knowledge from all learned domains places a constraint on a system's scalability owing to storage limitations."
  - [corpus] No direct corpus evidence found.
- Break condition: If the synthetic data generation fails to retain sufficient knowledge of previous domains, PAGE will not be able to adapt to new domains effectively, undermining its scalability and privacy benefits.

## Foundational Learning

- Concept: Catastrophic Forgetting (CF)
  - Why needed here: CF is the main problem that PAGE aims to solve in domain-incremental learning scenarios.
  - Quick check question: What happens to a machine learning model's performance on previous tasks when it is fine-tuned on new data?

- Concept: Generative Replay
  - Why needed here: Generative replay is the core method used by PAGE to mitigate CF by generating synthetic data that represent previous domains.
  - Quick check question: How does generative replay help a model retain knowledge of previous domains while adapting to new ones?

- Concept: Inductive Conformal Prediction (ICP)
  - Why needed here: ICP is extended by PAGE to provide statistical guarantees and model interpretability for disease detection results.
  - Quick check question: What is the main difference between inductive conformal prediction and transductive conformal prediction?

## Architecture Onboarding

- Component map:
  SDG Module -> Model Update -> EICP Module

- Critical path:
  1. Generate synthetic data using SDG module.
  2. Update model using real and synthetic data.
  3. Provide predictions with EICP.

- Design tradeoffs:
  - Memory vs. Performance: PAGE trades off some performance for scalability and privacy by not storing any past data.
  - Synthetic Data Quality: The quality of synthetic data generated by GMM directly impacts PAGE's ability to retain knowledge of previous domains.

- Failure signatures:
  - Performance degradation on previous domains: Indicates that the synthetic data generation is not accurately representing the previous domains.
  - High uncertainty in predictions: Suggests that the EICP module is not effectively calibrating non-conformity.

- First 3 experiments:
  1. Test PAGE's ability to retain knowledge of previous domains without catastrophic forgetting.
  2. Evaluate the quality of synthetic data generated by the SDG module.
  3. Assess the effectiveness of the EICP module in providing statistical guarantees and model interpretability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would PAGE perform on class-incremental learning scenarios compared to its current domain-incremental focus?
- Basis in paper: [explicit] The paper states "the current version of PAGE only targets domain-incremental adaptation scenarios" and mentions future work to expand to class-incremental and task-incremental scenarios.
- Why unresolved: The paper only evaluates PAGE on domain-incremental adaptation, leaving its performance on class-incremental learning unexplored.
- What evidence would resolve it: Experimental results comparing PAGE's performance on class-incremental learning tasks against existing methods, using metrics like average accuracy, BWT, and memory usage.

### Open Question 2
- Question: What is the optimal range for selecting data instances based on average training loss values for the extended calibration set in EICP?
- Basis in paper: [explicit] The paper conducts an ablation study on data selection ranges (plower, pupper) but acknowledges uncertainty about the optimal range.
- Why unresolved: The ablation study shows that different ranges affect performance, but doesn't identify a universally optimal range across all datasets.
- What evidence would resolve it: Systematic evaluation of EICP performance across multiple datasets with varying plower and pupper values to identify optimal ranges for different scenarios.

### Open Question 3
- Question: How would PAGE's performance change if applied to image classification or natural language processing tasks instead of tabular wearable sensor data?
- Basis in paper: [explicit] The paper states "it will be interesting to apply PAGE to natural language processing and image classification tasks" and acknowledges it was only tested on tabular data.
- Why unresolved: The paper only evaluates PAGE on tabular data from wearable sensors, not on image or text data which have different characteristics.
- What evidence would resolve it: Experimental results showing PAGE's performance on image and text datasets, comparing it to state-of-the-art methods for those modalities, and analyzing any necessary architectural modifications.

## Limitations
- The paper's claims are based on experiments with only three disease datasets, which may not represent the diversity of real-world smart healthcare applications
- The synthetic data generation process relies heavily on the assumption that GMM can accurately model the joint distribution of previous domain data
- The claim of up to 75% reduction in clinical workload through interpretable predictions lacks direct user study evidence or expert validation

## Confidence

- **High Confidence**: The core mechanism of using synthetic data generation to mitigate catastrophic forgetting in domain-incremental learning scenarios is well-supported by the paper's experimental results and aligns with established concepts in continual learning.
- **Medium Confidence**: The effectiveness of the EICP module in providing statistical guarantees and model interpretability is demonstrated through the paper's experiments, but the specific implementation details and hyperparameters are not fully specified, which may impact reproducibility.
- **Low Confidence**: The claim that PAGE can achieve up to 75% reduction in clinical workload through interpretable predictions is based on the assumption that the confidence scores and credibility values provided by EICP are sufficient for clinicians to make informed decisions. However, the paper does not provide direct evidence or user studies to support this claim.

## Next Checks

1. Evaluate Synthetic Data Quality: Conduct a thorough analysis of the synthetic data generated by the SDG module, comparing its distribution and characteristics to the real data from previous domains. Assess the impact of synthetic data quality on PAGE's ability to retain knowledge of previous domains and mitigate catastrophic forgetting.

2. Assess EICP Interpretability: Perform a user study or expert evaluation to determine the effectiveness of the confidence scores and credibility values provided by the EICP module in aiding clinicians' decision-making processes. Investigate whether these interpretability features can indeed lead to a significant reduction in clinical workload.

3. Test Scalability and Privacy Benefits: Design experiments to measure the actual memory storage savings and privacy benefits of PAGE compared to baseline methods that store raw data or distilled knowledge from previous domains. Evaluate PAGE's performance as the number of domains and data volume increase, assessing its scalability and ability to preserve patient privacy.