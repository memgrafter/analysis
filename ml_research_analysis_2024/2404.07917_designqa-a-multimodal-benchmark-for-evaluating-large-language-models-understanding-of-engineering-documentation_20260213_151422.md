---
ver: rpa2
title: 'DesignQA: A Multimodal Benchmark for Evaluating Large Language Models'' Understanding
  of Engineering Documentation'
arxiv_id: '2404.07917'
source_url: https://arxiv.org/abs/2404.07917
tags:
- rule
- questions
- document
- benchmark
- engineering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces DesignQA, a novel benchmark for evaluating
  multimodal large language models (MLLMs) on understanding and applying engineering
  requirements in technical documentation. The benchmark consists of 1,451 questions
  based on real-world Formula SAE competition rules and MIT Motorsports data, testing
  models' abilities across rule extraction, comprehension, and compliance tasks.
---

# DesignQA: A Multimodal Benchmark for Evaluating Large Language Models' Understanding of Engineering Documentation

## Quick Facts
- arXiv ID: 2404.07917
- Source URL: https://arxiv.org/abs/2404.07917
- Authors: Anna C. Doris; Daniele Grandi; Ryan Tomich; Md Ferdous Alam; Mohammadmehdi Ataei; Hyunmin Cheong; Faez Ahmed
- Reference count: 40
- Primary result: GPT-4o outperforms other MLLMs on DesignQA but still struggles with reliable rule extraction, technical component recognition, and engineering drawing analysis

## Executive Summary
This study introduces DesignQA, a novel benchmark for evaluating multimodal large language models (MLLMs) on understanding and applying engineering requirements in technical documentation. The benchmark consists of 1,451 questions based on real-world Formula SAE competition rules and MIT Motorsports data, testing models' abilities across rule extraction, comprehension, and compliance tasks. When evaluated against state-of-the-art models including GPT-4o, GPT-4, Gemini-1.0, Claude-Opus, and LLaVA-1.5, results showed significant limitations in MLLMs' current capabilities to accurately interpret complex technical documents, particularly in referencing relevant requirements and analyzing technical images. Even the best-performing model (GPT-4o) struggled with reliable rule extraction and technical component recognition, highlighting the need for further advancements in AI models to better handle the multifaceted nature of engineering design tasks.

## Method Summary
The DesignQA benchmark was created by extracting text from the 2024 Formula SAE Rules document and processing CAD models from MIT Motorsports into multi-view, close-up, and engineering drawing images. Question-answer pairs were generated across three segments: Rule Extraction (identifying relevant rules), Rule Comprehension (understanding rules), and Rule Compliance (applying rules to engineering designs). The benchmark was evaluated using state-of-the-art MLLMs with and without retrieval-augmented generation (RAG) systems, comparing performance using automatic metrics including F1-score, accuracy, BLEU, ROUGE, and Similarity scores across six benchmark subsets.

## Key Results
- GPT-4o-AllRules achieved the highest overall performance but still showed significant limitations in technical component recognition and rule extraction
- RAG-based approaches consistently underperformed compared to models with full context access, demonstrating the ineffectiveness of simple keyword-based retrieval for engineering documentation
- All models struggled with analyzing engineering drawings, particularly in computing accurate dimensions from scale bars and combining multiple dimensions to answer compliance questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4o-AllRules outperforms other models because it has access to the complete rule document in its context window, allowing it to reliably extract specific rules and perform better on rule compliance tasks.
- Mechanism: By providing the entire 70,091-token rule document in the context window, GPT-4o can perform in-context learning and reference the full document for each question without relying on retrieval-augmented generation, which introduces errors and incomplete context.
- Core assumption: The model's context window is sufficiently large to handle the full document without truncation, and the model can effectively search and retrieve relevant information from within this large context.
- Evidence anchors:
  - [abstract]: "Of the models tested, GPT-4o-AllRules performs the best across almost all metrics in the benchmark... GPT-4o-AllRules and GPT-4-AllRules, in all but one subset case, perform better than their corresponding -RAG models, demonstrating that the simple LlamaIndex RAG implementation was generally ineffective at providing relevant rule information to the models."
  - [section]: "The extracted text from the FSAE rule document PDF is roughly 70,091 tokens in length. As GPT-4o and GPT-4 have 128,000 token context windows and Claude-Opus has a 200,000 token context window, these models can ingest the whole rule text in the prompt."
- Break condition: If the model's context window is exceeded or if the model cannot effectively search within large contexts, performance would degrade to levels similar to RAG-based approaches.

### Mechanism 2
- Claim: MLLMs struggle with visual component recognition in CAD images because they lack specific training on technical engineering components and their relationships.
- Mechanism: The models fail to identify highlighted components in CAD images because these technical components (like "front hoop" or "impact attenuator") are not commonly represented in their training data, and the models cannot effectively correlate visual features with the technical terminology used in engineering documentation.
- Core assumption: The visual training data for MLLMs does not include sufficient representation of technical engineering components and their visual characteristics, and the models cannot transfer general object recognition skills to specialized engineering components.
- Evidence anchors:
  - [abstract]: "The MLLMs tested, while promising, struggle to reliably retrieve relevant rules from the Formula SAE documentation, face challenges in recognizing technical components in CAD images, and encounter difficulty in analyzing engineering drawings."
  - [section]: "We observed that GPT-4o-AllRules frequently neglects to report rules that are mentioned by rules that include the search term... Without good component understanding, MLLMs will struggle to answer Rule Compliance questions that pertain to those components."
- Break condition: If the models receive extensive fine-tuning on engineering CAD data or if visual component recognition is improved through specialized training, performance on component identification would improve significantly.

### Mechanism 3
- Claim: RAG systems with simple keyword matching are ineffective for engineering documentation because they cannot understand the semantic relationships between questions and relevant document sections.
- Mechanism: The simple RAG implementation using LlamaIndex with cosine similarity on embedded chunks fails to retrieve contextually relevant portions of the rule document, often missing critical information needed to answer questions correctly.
- Core assumption: Keyword-based retrieval cannot capture the nuanced relationships between engineering questions and the relevant rules, and the embedding space does not adequately represent the technical language used in engineering documentation.
- Evidence anchors:
  - [abstract]: "GPT-4o-AllRules and GPT-4-AllRules, in all but one subset case, perform better than their corresponding -RAG models, demonstrating that the simple LlamaIndex RAG implementation was generally ineffective at providing relevant rule information to the models."
  - [section]: "We saw that GPT-4o-AllRules and GPT-4-AllRules, given all the context, almost always performed better than their respective -RAG models, which were given the selected rule document pages through simple retrieval."
- Break condition: If more sophisticated RAG methods are implemented that understand semantic relationships and technical context, or if models have sufficient context windows to avoid RAG entirely, performance would improve.

## Foundational Learning

- Concept: Context window limitations and their impact on model performance
  - Why needed here: Understanding why some models perform better with full document access versus RAG-based approaches is crucial for designing effective evaluation frameworks
  - Quick check question: Why did GPT-4o-AllRules outperform GPT-4o-RAG on most benchmark subsets?

- Concept: Multimodal learning and visual recognition challenges
  - Why needed here: The benchmark reveals significant gaps in MLLMs' ability to recognize and understand technical engineering components in visual data
  - Quick check question: What specific visual recognition challenges did models face when identifying highlighted components in CAD images?

- Concept: Evaluation metric selection and interpretation
  - Why needed here: The benchmark uses multiple evaluation metrics (F1-score, accuracy, BLEU, ROUGE, Similarity) that require understanding for proper interpretation of results
  - Quick check question: How does F1-Bag of Characters differ from F1-Bag of Words, and why was it chosen for the Definition subset?

## Architecture Onboarding

- Component map: PDF text extraction -> CAD image processing -> Question generation -> Model evaluation -> Performance analysis
- Critical path: 1. Load FSAE rule document and extract text 2. Process CAD models into multi-view, close-up, and engineering drawing images 3. Generate question-answer pairs for each benchmark subset 4. Evaluate models using appropriate metrics for each subset 5. Analyze results and identify failure patterns
- Design tradeoffs:
  - Full context vs RAG: Using full context provides better performance but is more expensive; RAG is cheaper but less effective
  - Automated vs manual QA generation: Automated generation enables scale but may miss nuance; manual generation ensures quality but is resource-intensive
  - Single document vs multiple documents: Focusing on one document allows deep evaluation but may limit generalizability
- Failure signatures:
  - RAG failures: Models reporting rules as "not found" when they exist in the document
  - Visual recognition failures: Incorrect identification of technical components despite clear visual highlighting
  - Dimension calculation failures: Errors in computing dimensions from scale bars or combining multiple dimensions
- First 3 experiments:
  1. Test GPT-4o with and without the full rule document to quantify the performance gap between full context and RAG approaches
  2. Evaluate LLaVA-1.5 with GuaranteedRAG (context guaranteed to contain relevant information) to isolate model capability from RAG effectiveness
  3. Compare model performance on direct-dimensioned versus scale-bar-dimensioned engineering drawings to understand dimension interpretation challenges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MLLMs on DesignQA scale with increased model size or more sophisticated RAG implementations?
- Basis in paper: [explicit] The paper mentions that GPT-4o-AllRules outperforms GPT-4o-RAG, and that even the best performer (GPT-4o) does not achieve perfect scores, indicating room for improvement in both model capabilities and RAG systems.
- Why unresolved: The paper only evaluates a limited set of state-of-the-art models and a simple RAG implementation. It does not explore the impact of larger model sizes or more advanced RAG techniques on performance.
- What evidence would resolve it: Systematic evaluation of MLLMs of varying sizes and with different RAG architectures (e.g., end-to-end trained RAG, advanced chunking strategies) on DesignQA would provide insights into the scalability of performance.

### Open Question 2
- Question: What are the specific challenges MLLMs face in understanding and applying dimensional constraints from engineering drawings?
- Basis in paper: [explicit] The paper highlights that models struggle with tasks involving engineering drawings, particularly in computing accurate dimensions using scale bars and combining multiple dimensions to answer questions.
- Why unresolved: The paper identifies these challenges but does not delve into the specific technical reasons behind the models' difficulties with dimensional constraints.
- What evidence would resolve it: Detailed analysis of model outputs and error patterns on Dimension questions, coupled with investigations into the models' visual reasoning capabilities, would shed light on the specific challenges in understanding dimensional constraints.

### Open Question 3
- Question: How does the mention type of vehicle components in the rule document affect MLLMs' ability to identify and reason about these components?
- Basis in paper: [explicit] The paper tracks how components are mentioned in the rule document (definition, multi-mention, no-mention) and observes trends in model performance based on these mention types.
- Why unresolved: The paper identifies the trend but does not provide a comprehensive explanation for why these mention types affect model performance differently.
- What evidence would resolve it: Further investigation into the relationship between component mention types and model performance, including analysis of the models' internal representations and attention patterns, would help understand the underlying reasons for these differences.

## Limitations

- The benchmark's focus on Formula SAE automotive regulations may not generalize to other engineering domains
- The simple RAG implementation using LlamaIndex may underestimate MLLMs' true capabilities when paired with more sophisticated retrieval mechanisms
- The evaluation relies on a single rule document and specific CAD models, potentially limiting the diversity of engineering documentation challenges represented

## Confidence

- High Confidence: The observation that GPT-4o-AllRules outperforms RAG-based approaches due to full context access is strongly supported by the empirical evidence
- Medium Confidence: The claim about MLLMs' struggles with technical component recognition in CAD images is supported by the benchmark results but may be partially attributable to the specific CAD models used
- Medium Confidence: The assertion that simple keyword-based RAG is ineffective for engineering documentation is supported by the performance gap between AllRules and RAG models

## Next Checks

1. **Cross-domain validation**: Test DesignQA models on engineering documentation from other domains (aerospace, biomedical, civil engineering) to assess the benchmark's generalizability and identify domain-specific versus universal limitations

2. **Advanced RAG comparison**: Implement and evaluate state-of-the-art RAG systems (e.g., hybrid retrieval, learned retrievers) against the simple LlamaIndex approach to determine if retrieval methodology significantly impacts MLLM performance on engineering tasks

3. **Fine-tuning impact study**: Fine-tune selected MLLMs on a diverse corpus of engineering CAD data and documentation, then re-evaluate their performance on DesignQA to quantify the potential improvements from domain-specific adaptation