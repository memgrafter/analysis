---
ver: rpa2
title: Static Pruning in Dense Retrieval using Matrix Decomposition
arxiv_id: '2412.09983'
source_url: https://arxiv.org/abs/2412.09983
tags:
- retrieval
- pruning
- embeddings
- dense
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a static pruning method for dense retrieval
  using Principal Component Analysis (PCA) to reduce embedding dimensionality without
  relying on query-specific information or adding computational overhead at inference
  time. The method computes a transformation matrix offline using PCA on document
  embeddings, then applies this matrix to both document and query embeddings to enable
  efficient retrieval with reduced dimensions.
---

# Static Pruning in Dense Retrieval using Matrix Decomposition

## Quick Facts
- arXiv ID: 2412.09983
- Source URL: https://arxiv.org/abs/2412.09983
- Reference count: 22
- Primary result: PCA-based static pruning reduces embedding dimensionality by 50% with up to 5% NDCG@10 reduction

## Executive Summary
This paper proposes a static pruning method for dense retrieval using Principal Component Analysis (PCA) to reduce embedding dimensionality without relying on query-specific information or adding computational overhead at inference time. The method computes a transformation matrix offline using PCA on document embeddings, then applies this matrix to both document and query embeddings to enable efficient retrieval with reduced dimensions. Experiments show the approach can reduce embedding dimensionality by over 50% with up to 5% reduction in NDCG@10 across multiple dense retrieval models (TAS-B, Contriever, ANCE) and query sets (DL 19, DL 20, DL HARD, MS MARCO Dev Small, BEIR TREC COVID). The method is robust to the number of embeddings used for PCA computation and performs well even when applied out-of-domain.

## Method Summary
The method uses PCA to compute a transformation matrix offline from document embeddings, which captures the principal components of the embedding space. This matrix is then applied to both document and query embeddings to project them into a lower-dimensional space while preserving the most important variance. At inference time, queries are transformed using this precomputed matrix and compared against the reduced document embeddings using efficient nearest neighbor search (FAISS). The approach is query-independent and executed offline, enabling significant efficiency gains during inference without requiring additional per-query processing.

## Key Results
- Reduces embedding dimensionality by over 50% with up to 5% reduction in NDCG@10
- Achieves 2√ó space efficiency (O(mn) vs O(dn)) and O(d/m) query processing speedup
- Shows robust performance across different dense retrieval models (TAS-B, Contriever, ANCE)
- Performs well out-of-domain and with varying numbers of embeddings for PCA computation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PCA-based dimension pruning reduces embedding dimensionality without requiring query-specific computation.
- Mechanism: The method computes a transformation matrix offline using eigendecomposition of the covariance matrix of document embeddings. This matrix captures the principal components of the embedding space, allowing documents and queries to be projected into a lower-dimensional space where the most important variance is preserved.
- Core assumption: The principal components of document embeddings also capture the important dimensions for query embedding transformations.
- Evidence anchors:
  - [abstract]: "This approach is query-independent and can be executed offline, leading to a significant boost in dense retrieval efficiency with a negligible impact on the system effectiveness."
  - [section]: "To reduce the query processing time and/or the embedding index size, we propose an approach that reduces the dimensionality of ùê∑ by applying PCA."
  - [corpus]: Weak evidence - related papers focus on different pruning approaches but don't directly address static PCA-based methods.
- Break condition: If the principal components of document embeddings don't align with the dimensions important for query relevance, effectiveness will degrade significantly.

### Mechanism 2
- Claim: The method achieves significant space efficiency (2√ó reduction) with minimal effectiveness loss (up to 5% NDCG@10 reduction).
- Mechanism: By pruning the least important dimensions based on eigenvalue ordering, the method reduces both the embedding index size (O(mn) vs O(dn)) and query processing time (O(d/m) speedup) while preserving the most informative dimensions.
- Core assumption: The top m principal components capture sufficient information for effective retrieval, even when m << d.
- Evidence anchors:
  - [abstract]: "Our experiments show that our proposed method reduces the dimensionality of document representations by over 50% with up to a 5% reduction in NDCG@10, for different dense retrieval models."
  - [section]: "At inference time, the query processing now has an average time complexity of ùëÇ (ùëëùëö + ùëöùëõ), due to query transformation and dot product computation over ÀÜùê∑"
  - [corpus]: Weak evidence - no direct corpus support for these specific efficiency claims.
- Break condition: If the pruned dimensions contain critical information for distinguishing relevant documents, effectiveness will degrade beyond acceptable thresholds.

### Mechanism 3
- Claim: The method is robust to the number of embeddings used for PCA computation and performs well out-of-domain.
- Mechanism: The transformation matrix computed on in-domain embeddings can be applied to different corpora without significant effectiveness loss, and the method shows stability even with varying numbers of documents for PCA computation.
- Core assumption: The principal components learned from one corpus generalize well to other corpora and are not highly sensitive to the specific number of training documents.
- Evidence anchors:
  - [section]: "Surprisingly, the performance on all metrics is on par with that of the in-domain pruning. Even at a 50% cutoff, TAS-B shows no significant degradation across most benchmarks"
  - [section]: "All encoders show remarkable stability, with almost no significant differences from the baseline, even when the decomposition is performed on only 103 documents"
  - [corpus]: Weak evidence - related papers don't address out-of-domain generalization for static PCA pruning.
- Break condition: If the embedding distributions differ significantly between corpora, or if too few documents are used for PCA, the method will fail to generalize.

## Foundational Learning

- Concept: Principal Component Analysis (PCA) and eigendecomposition
  - Why needed here: The method relies on computing the eigendecomposition of the covariance matrix of document embeddings to identify principal components for dimensionality reduction.
  - Quick check question: What property of eigenvalues determines the ordering of principal components?

- Concept: Dense retrieval and embedding-based similarity
  - Why needed here: Understanding how dense retrievers use document and query embeddings to compute relevance scores is crucial for understanding why dimensionality reduction works.
  - Quick check question: How does reducing embedding dimensionality affect the dot product similarity computation?

- Concept: Computational complexity analysis
  - Why needed here: The paper claims specific time and space complexity improvements that require understanding of algorithmic complexity analysis.
  - Quick check question: How does reducing dimensionality from d to m affect the space complexity of storing document embeddings?

## Architecture Onboarding

- Component map:
  Document embedding matrix (D ‚àà R‚Åø√óùëë) -> PCA transformation matrix (W ‚àà Rùëë√óùëë) -> Pruned embedding matrix (DÃÇ ‚àà R‚Åø√óùëö) -> FAISS index for efficient nearest neighbor search

- Critical path:
  1. Compute PCA transformation matrix offline using document embeddings
  2. Apply transformation to create pruned document embeddings
  3. Build FAISS index with pruned embeddings
  4. At query time: transform query embedding, perform nearest neighbor search

- Design tradeoffs:
  - Space vs effectiveness: More aggressive pruning saves space but may reduce retrieval quality
  - In-domain vs out-of-domain: Computing PCA on different corpora affects generalization
  - Number of embeddings for PCA: More embeddings may improve stability but increase computation

- Failure signatures:
  - Significant effectiveness degradation (>5% NDCG@10) indicates pruning too aggressively
  - Out-of-domain PCA failure suggests corpus distribution mismatch
  - Inconsistent results across different numbers of PCA embeddings indicates instability

- First 3 experiments:
  1. Baseline comparison: Run retrieval with full vs pruned embeddings on same query set
  2. Cutoff sensitivity: Test different pruning percentages (25%, 50%, 75%) to find sweet spot
  3. Cross-corpus validation: Apply PCA from one corpus to another to test out-of-domain robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed PCA-based static pruning method compare to other existing static pruning methods in terms of retrieval effectiveness and efficiency?
- Basis in paper: [inferred] The paper discusses other static pruning methods like Acquavio et al. [1] and Chang et al. [4], but does not directly compare the proposed PCA-based method to these approaches.
- Why unresolved: The paper focuses on evaluating the proposed PCA-based method against baselines using the same bi-encoder models without pruning. It does not provide a direct comparison with other static pruning techniques.
- What evidence would resolve it: Experimental results comparing the proposed PCA-based method with other static pruning methods (e.g., Acquavio et al., Chang et al.) on the same datasets and using the same evaluation metrics would provide a direct comparison and resolve this question.

### Open Question 2
- Question: What is the impact of using different PCA-based dimensionality reduction techniques (e.g., kernel PCA, sparse PCA) on the retrieval effectiveness and efficiency?
- Basis in paper: [inferred] The paper uses standard PCA for dimensionality reduction, but does not explore other PCA variants or compare their performance.
- Why unresolved: The paper only presents results using standard PCA and does not investigate the potential benefits or drawbacks of using alternative PCA techniques for this specific task.
- What evidence would resolve it: Experimental results comparing the proposed method using different PCA variants (e.g., kernel PCA, sparse PCA) on the same datasets and using the same evaluation metrics would provide insights into the impact of different dimensionality reduction techniques and resolve this question.

### Open Question 3
- Question: How does the proposed PCA-based static pruning method perform on different types of corpora (e.g., web pages, scientific articles, social media posts) with varying characteristics?
- Basis in paper: [inferred] The paper evaluates the method on passage collections from MS MARCO and BEIR TREC COVID, but does not explore its performance on other types of corpora.
- Why unresolved: The paper focuses on passage retrieval tasks and does not investigate how the method generalizes to other types of text corpora with different characteristics (e.g., length, structure, language style).
- What evidence would resolve it: Experimental results applying the proposed method to different types of corpora (e.g., web pages, scientific articles, social media posts) and evaluating its performance on relevant tasks would provide insights into its generalizability and resolve this question.

## Limitations
- Lack of corpus support for the specific PCA-based static pruning approach
- Uncertainty about method's robustness to different embedding distributions
- Limited validation of out-of-domain generalization beyond specific model and corpus combinations

## Confidence
- **High confidence**: The core mechanism of using PCA for dimensionality reduction and the theoretical efficiency gains (2√ó space reduction, O(d/m) speedup) are well-established and mathematically sound.
- **Medium confidence**: The effectiveness claims (up to 5% NDCG@10 reduction) are supported by extensive experiments across multiple models and datasets, but the lack of external corpus validation introduces uncertainty.
- **Medium confidence**: The robustness claims regarding out-of-domain generalization and sensitivity to the number of PCA embeddings are based on the presented experiments but lack broader validation.

## Next Checks
1. **External Corpus Validation**: Apply the PCA pruning method to a different dense retrieval model (e.g., SBERT) and corpus (e.g., Wikipedia) not used in the original experiments to verify the out-of-domain generalization claims.

2. **Embedding Distribution Analysis**: Conduct experiments varying document length and content characteristics to assess how different embedding distributions affect the method's effectiveness and whether the principal components learned from one distribution transfer to others.

3. **Ablation Study on PCA Parameters**: Systematically vary the number of documents used for PCA computation (beyond the tested range of 10^3 to 10^5) and test different PCA variants (e.g., with and without whitening) to better understand the method's sensitivity to these parameters.