---
ver: rpa2
title: 'TACOMORE: Leveraging the Potential of LLMs in Corpus-based Discourse Analysis
  with Prompt Engineering'
arxiv_id: '2412.10139'
source_url: https://arxiv.org/abs/2412.10139
tags:
- keyword
- collocate
- virus
- reason
- theme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TACOMORE is a prompting framework designed to improve LLM performance\
  \ in corpus-based discourse analysis by addressing concerns of bias, hallucination,\
  \ and irreproducibility. The framework specifies four principles\u2014Task, Context,\
  \ Model, and Reproducibility\u2014and five fundamental elements\u2014Role Description,\
  \ Task Definition, Task Procedures, Contextual Information, and Output Format."
---

# TACOMORE: Leveraging the Potential of LLMs in Corpus-based Discourse Analysis with Prompt Engineering

## Quick Facts
- arXiv ID: 2412.10139
- Source URL: https://arxiv.org/abs/2412.10139
- Reference count: 40
- Key outcome: TACOMORE framework improves LLM performance in corpus-based discourse analysis by addressing bias, hallucination, and irreproducibility through structured prompting principles and elements.

## Executive Summary
TACOMORE is a prompting framework designed to enhance Large Language Model (LLM) performance in corpus-based discourse analysis by addressing concerns of bias, hallucination, and irreproducibility. The framework specifies four principles—Task, Context, Model, and Reproducibility—and five fundamental elements—Role Description, Task Definition, Task Procedures, Contextual Information, and Output Format. Experiments on GPT-4o, Gemini-1.5-Pro, and Gemini-1.5-Flash using three tasks (keyword, collocate, and concordance analysis) showed that TACOMORE significantly improved LLM performance in Accuracy, Ethicality, Reasoning, and Reproducibility compared to baseline prompts. The method successfully enabled LLMs to conduct discourse analysis tasks with human-like understanding and reasoning, achieving results closer to human analysis.

## Method Summary
TACOMORE improves LLM performance by breaking down complex corpus tasks into explicit, sequential steps that mimic human reasoning processes. The framework uses four principles (Task, Context, Model, Reproducibility) and five fundamental elements (Role Description, Task Definition, Task Procedures, Contextual Information, Output Format) to create structured prompts. The method was tested on three LLMs using an open corpus of COVID-19 research article abstracts, evaluating performance across keyword, collocate, and concordance analysis tasks with metrics for Accuracy, Ethicality, Reasoning, and Reproducibility.

## Key Results
- TACOMORE significantly improved LLM performance across all four evaluation metrics (Accuracy, Ethicality, Reasoning, Reproducibility) compared to baseline prompts
- The framework enabled LLMs to achieve results closer to human analysis in corpus-based discourse tasks
- Each TACOMORE element incrementally improved performance, with the framework showing consistent effectiveness across different LLM models (GPT-4o, Gemini-1.5-Pro, Gemini-1.5-Flash)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TACOMORE improves LLM performance by breaking down complex corpus tasks into explicit, sequential steps that mimic human reasoning processes.
- Mechanism: By decomposing tasks into Step 1 (keyword labeling/meaning), Step 2 (theme definition), and Step 3 (keyword-theme assignment), LLMs can follow a structured reasoning chain rather than attempting holistic understanding, reducing cognitive load and error propagation.
- Core assumption: LLMs benefit from step-by-step procedural guidance in complex linguistic tasks more than from holistic instructions.
- Evidence anchors:
  - [abstract]: "we could split the task into several steps as human would do when conducting the same task"
  - [section 3.3]: "Give clear and orderly instructions to activate the thinking process of LLMs"
- Break condition: If task decomposition is too granular or misses logical connections between steps, LLMs may lose context and produce fragmented outputs.

### Mechanism 2
- Claim: Contextual information provided to LLMs aligns their understanding with human analysts, enabling accurate interpretation of keywords and collocates.
- Mechanism: Including concordance lines and original text in prompts gives LLMs access to the same contextual information human linguists use, allowing them to interpret semantic nuances and avoid surface-level categorization.
- Core assumption: LLMs require the same contextual information as humans to perform discourse analysis tasks accurately.
- Evidence anchors:
  - [abstract]: "contextual information mostly refers to the co-text around a target word or phrase"
  - [section 4.2]: "it is necessary to provide as much necessary contextual information as one can to the LLMs"
- Break condition: If contextual information is incomplete or contains noise, LLMs may still hallucinate or misinterpret meanings despite having access to context.

### Mechanism 3
- Claim: Strict output formatting requirements ensure reproducibility and completeness of LLM outputs across trials.
- Mechanism: By specifying exact output formats with delimiters and requiring exhaustive keyword inclusion, TACOMORE forces LLMs to produce consistent, complete results that can be reliably compared across runs.
- Core assumption: LLMs can be constrained to produce stable outputs through precise formatting instructions.
- Evidence anchors:
  - [abstract]: "define the reproducibility of task results as meaningful, reasonable, and similar output every time"
  - [section 4.3]: "we significantly improve the repeatability of LLMs' outputs with TACOMORE"
- Break condition: If formatting instructions are ambiguous or LLMs prioritize creative generation over compliance, reproducibility may still vary.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Enables LLMs to break down complex linguistic analysis into manageable reasoning steps, mirroring human analytical processes
  - Quick check question: What are the three main steps in keyword analysis task decomposition according to TACOMORE?

- Concept: Context-aware prompting
  - Why needed here: Corpus linguistics tasks require understanding of surrounding text to interpret meaning correctly
  - Quick check question: What type of information does TACOMORE include to align LLM understanding with human analysts?

- Concept: Reproducibility constraints in LLM outputs
  - Why needed here: LLMs naturally produce variable outputs; strict formatting requirements help achieve consistent results
  - Quick check question: What element in TACOMORE ensures all keywords are included in the final output?

## Architecture Onboarding

- Component map:
  - Corpus construction pipeline (CDC COVID-19 abstracts)
  - TACOMORE framework (4 principles × 5 elements)
  - Task-specific prompt templates (keyword, collocate, concordance analysis)
  - Evaluation metrics (Accuracy, Ethicality, Reasoning, Reproducibility)
  - LLM APIs (GPT-4o, Gemini-1.5-Pro, Gemini-1.5-Flash)

- Critical path:
  1. Build corpus → 2. Define task → 3. Create TACOMORE prompt → 4. Run LLM → 5. Evaluate output → 6. Refine prompt if needed

- Design tradeoffs:
  - Input length vs. model capability (Gemini-1.5-Pro/Fast vs GPT-4o)
  - Granularity of task decomposition vs. LLM comprehension limits
  - Strict formatting vs. creative flexibility in outputs

- Failure signatures:
  - Missing keywords in output → insufficient output format specification
  - Inconsistent theme categorization → inadequate context provision
  - Incorrect part-of-speech labeling → insufficient task procedure detail

- First 3 experiments:
  1. Baseline prompt vs. TACOMORE prompt for keyword analysis on small keyword set
  2. Single task type (keyword analysis) with varying context lengths
  3. Different LLM models with identical TACOMORE prompts to test model-specific performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TACOMORE perform on other qualitative analysis tasks in corpus linguistics beyond keywords, collocates, and concordances?
- Basis in paper: [inferred] The paper focuses on three representative discourse analysis tasks but acknowledges that discourse analysis is only a part of corpus linguistics methodology
- Why unresolved: The study deliberately narrowed scope to three tasks to maintain manageable scale, leaving broader applicability unexplored
- What evidence would resolve it: Systematic testing of TACOMORE across diverse corpus linguistics tasks (topic modeling, sentiment analysis, discourse parsing) with comparative evaluation against baseline prompts

### Open Question 2
- Question: What is the optimal balance between prompt specificity and flexibility in TACOMORE for different LLM capabilities?
- Basis in paper: [explicit] The paper shows that each TACOMORE element incrementally improves performance, but doesn't explore whether excessive specificity might constrain LLM reasoning
- Why unresolved: The study adds all elements systematically without testing whether certain tasks might benefit from more flexible prompts
- What evidence would resolve it: Comparative experiments testing TACOMORE with varying levels of detail across different task complexities and LLM model sizes

### Open Question 3
- Question: How does TACOMORE's performance compare to fine-tuning approaches for corpus-based discourse analysis?
- Basis in paper: [explicit] The paper mentions fine-tuning as an alternative technique that could be explored in future work
- Why unresolved: The study focuses exclusively on prompt engineering without comparing against model adaptation approaches
- What evidence would resolve it: Direct performance comparison between TACOMORE prompts and fine-tuned models on identical discourse analysis tasks with established metrics

### Open Question 4
- Question: How does contextual information size affect TACOMORE's performance across different LLM models?
- Basis in paper: [inferred] The study shows contextual information improves performance but uses fixed amounts without exploring the relationship between context length and model capabilities
- Why unresolved: The paper uses predetermined context sizes without testing whether larger models can handle more extensive context effectively
- What evidence would resolve it: Systematic testing of TACOMORE with varying context lengths across different LLM models to identify optimal context-model pairings

### Open Question 5
- Question: What is the relationship between TACOMORE's reproducibility improvements and the underlying variance in different LLM deployment environments?
- Basis in paper: [explicit] The paper defines reproducibility as "meaningful, reasonable, and similar output" but doesn't quantify how much variance remains across trials
- Why unresolved: The study acknowledges inherent LLM variance but doesn't measure or characterize the remaining variability after applying TACOMORE
- What evidence would resolve it: Statistical analysis of output variance across multiple trials and deployment environments with and without TACOMORE to quantify improvement in reproducibility

## Limitations

- The framework's effectiveness depends heavily on the quality and comprehensiveness of contextual information provided, with no public access to the original COVID-19 corpus used in experiments
- Evaluation metrics (Accuracy, Ethicality, Reasoning, Reproducibility) are not fully specified, making independent verification of performance improvements difficult
- The study assumes LLMs can be constrained to produce stable, reproducible outputs through formatting requirements alone, without addressing inherent model variability

## Confidence

**High Confidence**: The claim that structured prompt engineering improves LLM performance in linguistic tasks is well-supported. The framework's four principles and five elements are clearly defined and logically connected to established prompt engineering practices.

**Medium Confidence**: The assertion that TACOMORE significantly improves Accuracy, Ethicality, Reasoning, and Reproducibility compared to baseline prompts. While experimental results are reported, the lack of detailed evaluation methodology and public corpus limits independent verification.

**Low Confidence**: The claim that TACOMORE enables LLMs to achieve "human-like understanding and reasoning" in discourse analysis. This represents a strong claim about LLM capabilities that would require more extensive human benchmark comparisons and cross-corpus validation.

## Next Checks

1. **Replicate with open corpus**: Test TACOMORE on a publicly available corpus (e.g., PubMed COVID-19 dataset) to verify generalizability and assess whether performance improvements hold without access to the original dataset.

2. **Independent metric validation**: Develop and apply standardized evaluation criteria for the four metrics to assess whether improvements are consistent across different evaluators and evaluation frameworks.

3. **Cross-model stress test**: Apply TACOMORE prompts to a broader range of LLM architectures (including open-source models of varying sizes) to determine whether the framework's effectiveness is model-specific or more universally applicable.