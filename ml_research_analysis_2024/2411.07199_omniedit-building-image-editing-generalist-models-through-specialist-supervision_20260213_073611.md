---
ver: rpa2
title: 'OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision'
arxiv_id: '2411.07199'
source_url: https://arxiv.org/abs/2411.07199
tags:
- image
- editing
- object
- conference
- omni
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OMNI-EDIT addresses the limitations of existing instruction-based
  image editing models, which suffer from biased synthetic data, poor data quality
  control, and limited support for varying resolutions and aspect ratios. The paper
  introduces a specialist-to-generalist learning framework that trains a generalist
  model using supervision from seven specialist models, each handling a specific editing
  task.
---

# OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision

## Quick Facts
- arXiv ID: 2411.07199
- Source URL: https://arxiv.org/abs/2411.07199
- Authors: Cong Wei; Zheyang Xiong; Weiming Ren; Xinrun Du; Ge Zhang; Wenhu Chen
- Reference count: 31
- One-line primary result: OMNI-EDIT achieves state-of-the-art performance on instruction-based image editing across seven tasks and varying aspect ratios through specialist-to-generalist supervision.

## Executive Summary
OMNI-EDIT addresses key limitations in instruction-based image editing models by introducing a specialist-to-generalist learning framework. The model overcomes challenges with biased synthetic data, poor data quality control, and limited support for varying resolutions and aspect ratios. By leveraging seven specialist models for specific editing tasks and implementing importance sampling using large multimodal models, OMNI-EDIT achieves superior performance across diverse editing scenarios.

## Method Summary
OMNI-EDIT trains a generalist image editing model using supervision from seven specialist models, each handling a distinct editing task (object swap, object addition, object removal, attribute modification, background swap, environment change, and style transfer). The training data is curated from LAION-5B and OpenImageV6 with diverse aspect ratios and high resolutions. Importance sampling using a distilled LMM (InternVL2) filters high-quality examples based on scores from GPT-4o. The EditNet architecture enables interaction between control and original branches through intermediate representations, improving editing success rates. The model is trained on 1.2M image-editing pairs and evaluated on 62 manually collected test images with 434 edits.

## Key Results
- OMNI-EDIT outperforms existing models on both automatic and human evaluations
- Achieves higher perceptual quality and semantic consistency scores across diverse editing tasks
- Successfully handles images with varying aspect ratios and resolutions beyond standard square formats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specialist-to-generalist supervision enables broader editing capabilities by leveraging task-specific expertise from multiple specialists.
- Mechanism: The generalist model OMNI-EDIT is trained using supervision signals from seven specialist models, each handling a distinct editing task (object swap, object addition, object removal, attribute modification, background swap, environment change, and style transfer). This ensemble approach ensures comprehensive task coverage that a single synthetic data pipeline cannot achieve.
- Core assumption: The specialist models can generate high-quality demonstrations that accurately represent their respective editing tasks.
- Evidence anchors:
  - [abstract] "OMNI-EDIT is trained by utilizing the supervision from seven different specialist models to ensure task coverage."
  - [section 3.2] Details the seven specialist models and their specific roles in generating training data.
  - [corpus] Weak - only general references to specialist-generalist frameworks without direct evidence for image editing applications.
- Break condition: If specialist models fail to generate accurate demonstrations or if the generalist cannot effectively learn from multiple supervision signals.

### Mechanism 2
- Claim: Importance sampling using large multimodal models (LMMs) improves data quality by filtering out low-quality synthetic examples.
- Mechanism: Instead of using simple CLIP-score filtering, OMNI-EDIT employs GPT-4o to assign quality scores to synthesized samples, retaining only those with scores â‰¥ 9. Due to computational costs, GPT-4o's scoring ability is distilled into a smaller model (InternVL2) for large-scale filtering.
- Core assumption: LMMs can reliably distinguish high-quality from low-quality image editing pairs.
- Evidence anchors:
  - [abstract] "we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality."
  - [section 3.3] Explains the distillation process from GPT-4o to InternVL2 for efficient scoring.
  - [corpus] Weak - general references to importance sampling but not specific to multimodal model-based quality control for image editing.
- Break condition: If the distilled scoring model fails to maintain the accuracy of GPT-4o's evaluations or if the filtering threshold removes too many valid examples.

### Mechanism 3
- Claim: EditNet architecture enhances editing success by allowing interaction between control and original branches through intermediate representations.
- Mechanism: EditNet introduces control branch DIT blocks that interact with original DIT tokens and editing prompts, allowing adaptive adjustment of representations based on editing instructions. This differs from ControlNet by updating text representations and enabling intermediate representation interaction.
- Core assumption: Allowing interaction between control and original branches improves the model's ability to understand and execute editing tasks.
- Evidence anchors:
  - [abstract] "we propose a new editing architecture called EditNet to greatly boost the editing success rate"
  - [section 4] Detailed comparison between EditNet and ControlNet, highlighting the advantages of intermediate representation interaction.
  - [corpus] Weak - general references to diffusion-transformer architectures but not specific evidence for EditNet's effectiveness.
- Break condition: If the additional complexity of EditNet does not translate to improved editing performance or if it introduces training instability.

## Foundational Learning

- Concept: Diffusion models and their denoising process
  - Why needed here: OMNI-EDIT builds upon diffusion models as its base architecture, requiring understanding of how they reverse the diffusion process to generate images.
  - Quick check question: How does a diffusion model reconstruct the original data from noisy latent representations?

- Concept: Supervised learning with paired data
  - Why needed here: The instruction-based image editing task is formulated as supervised learning using pairs of source images, edited images, and corresponding instructions.
  - Quick check question: What is the objective function for training a diffusion model on instruction-based image editing pairs?

- Concept: Importance sampling and weighted loss functions
  - Why needed here: OMNI-EDIT uses importance sampling to weight training examples based on their quality scores, requiring understanding of how to incorporate these weights into the loss function.
  - Quick check question: How does importance sampling modify the standard maximum likelihood training objective?

## Architecture Onboarding

- Component map: Specialist models -> Importance scoring (GPT-4o distilled to InternVL2) -> Filtered dataset -> EditNet (on SD3 Medium) -> Generalist model
- Critical path: Specialist model generation -> Importance scoring -> EditNet training -> Evaluation
- Design tradeoffs:
  - EditNet vs ControlNet: EditNet updates text representations and enables intermediate interaction but adds complexity
  - Specialist models vs single pipeline: Multiple specialists ensure coverage but increase implementation complexity
  - LMM scoring vs CLIP-score: Higher quality filtering but increased computational cost
- Failure signatures:
  - Poor editing performance may indicate specialist model failures or ineffective distillation of scoring function
  - Training instability may result from EditNet's complex interactions between branches
  - Low data quality despite LMM scoring may indicate threshold misconfiguration
- First 3 experiments:
  1. Verify specialist models can generate accurate demonstrations for their respective tasks
  2. Test distilled scoring model (InternVL2) against GPT-4o on a validation set
  3. Compare EditNet against ControlNet baseline on a subset of editing tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between specialist model diversity and generalist model performance in the specialist-to-generalist learning framework?
- Basis in paper: [explicit] The paper uses seven specialist models for different editing tasks and claims that this ensures task coverage.
- Why unresolved: The paper does not explore whether using more or fewer specialist models would improve or degrade OMNI-EDIT's performance, or if certain tasks benefit more from specialization than others.
- What evidence would resolve it: Comparative studies showing OMNI-EDIT's performance with different numbers of specialist models, or ablation studies demonstrating which tasks benefit most from specialization.

### Open Question 2
- Question: How does the importance sampling threshold (score >= 9) affect the quality and diversity of the training data?
- Basis in paper: [explicit] The paper uses a threshold of 9 for filtering data based on LMM scores but does not explore alternative thresholds.
- Why unresolved: The choice of threshold appears arbitrary and could significantly impact both the quality of the final model and the diversity of training examples.
- What evidence would resolve it: Experiments comparing OMNI-EDIT trained with different importance sampling thresholds to determine the optimal balance between quality and diversity.

### Open Question 3
- Question: What is the long-term impact of EditNet's architecture on the preservation of original image details compared to other approaches?
- Basis in paper: [inferred] The paper claims EditNet preserves original image details better than ControlNet and channel-wise concatenation methods, but only shows short-term comparisons.
- Why unresolved: The paper does not investigate whether EditNet's advantage in preserving details persists across many editing iterations or with different types of source images.
- What evidence would resolve it: Longitudinal studies tracking detail preservation across multiple editing operations, or systematic tests across diverse image categories.

## Limitations

- The effectiveness of the specialist-to-generalist framework depends heavily on the quality and diversity of the specialist models, which are not directly evaluated.
- The importance sampling methodology using LMMs introduces computational overhead, and the specific quality threshold (score >= 9) may be arbitrary.
- The relative contribution of each architectural component (EditNet, specialist supervision, importance sampling) to overall performance is not quantified through ablation studies.

## Confidence

**High Confidence**: The model achieves superior performance on the proposed evaluation metrics (VIEScore, human evaluation) compared to baseline models. The use of varying aspect ratios and high-resolution images in training is well-documented and justified.

**Medium Confidence**: The specialist-to-generalist supervision framework is theoretically sound and the paper provides reasonable evidence for its effectiveness. The importance sampling methodology using LMMs is innovative, though the specific implementation details and quality thresholds are not fully transparent.

**Low Confidence**: The relative contribution of each component (EditNet architecture, specialist supervision, importance sampling) to the overall performance is not clearly quantified. The generalizability of the model to editing tasks beyond the seven specified ones remains unclear.

## Next Checks

1. **Ablation Study**: Conduct an ablation study to quantify the individual contributions of EditNet architecture, specialist supervision, and importance sampling to overall performance. This would help isolate which components drive the improvements.

2. **Generalization Test**: Evaluate OMNI-EDIT on a broader range of editing tasks beyond the seven specified ones to assess its true generalist capabilities and identify potential limitations.

3. **Specialist Model Analysis**: Perform a detailed analysis of the specialist models' performance and quality, including error analysis to understand failure modes and limitations in the specialist-to-generalist supervision framework.