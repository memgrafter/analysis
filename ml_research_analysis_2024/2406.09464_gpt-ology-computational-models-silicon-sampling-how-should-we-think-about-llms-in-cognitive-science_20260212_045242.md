---
ver: rpa2
title: 'GPT-ology, Computational Models, Silicon Sampling: How should we think about
  LLMs in Cognitive Science?'
arxiv_id: '2406.09464'
source_url: https://arxiv.org/abs/2406.09464
tags:
- llms
- language
- these
- about
- cognitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews emerging research paradigms for using Large
  Language Models (LLMs) in cognitive science, including "GPT-ology" (studying LLM
  capabilities), using LLMs as computational models of human cognition, and "silicon
  sampling" (simulating human behavior). The author identifies several challenges
  to scientific inference, including anthropomorphism, reliability issues, prompt
  sensitivity, lack of model transparency, and concerns about training data and reproducibility.
---

# GPT-ology, Computational Models, Silicon Sampling: How should we think about LLMs in Cognitive Science?

## Quick Facts
- arXiv ID: 2406.09464
- Source URL: https://arxiv.org/abs/2406.09464
- Authors: Desmond C. Ong
- Reference count: 5
- Key outcome: Reviews methodological challenges in using LLMs for cognitive science research, identifying anthropomorphism, reliability issues, prompt sensitivity, and concerns about training data and reproducibility as major obstacles to scientific inference.

## Executive Summary
This paper examines three emerging research paradigms for using Large Language Models (LLMs) in cognitive science: GPT-ology (studying LLM capabilities), using LLMs as computational models of human cognition, and "silicon sampling" (simulating human behavior). The author identifies significant methodological challenges that limit scientific inference, including anthropomorphism, reliability issues, prompt sensitivity, lack of model transparency, and concerns about training data and reproducibility. A central concern is that many current studies make inferences based on transient LLM capabilities that may become obsolete as models are updated. The paper calls for more generalizable, lasting knowledge in cognitive science research using LLMs.

## Method Summary
This is a conceptual review paper that synthesizes existing research paradigms and methodological challenges in using Large Language Models for cognitive science. The author analyzes three approaches: GPT-ology (studying LLM capabilities), using LLMs as computational models of human cognition, and silicon sampling (simulating human behavior). The paper draws on observations from published studies and identifies theoretical concerns rather than presenting original empirical data. The analysis focuses on identifying limitations and proposing directions for more robust scientific inference when using LLMs in cognitive research.

## Key Results
- Current LLM research in cognitive science faces significant methodological challenges including anthropomorphism, reliability issues, and prompt sensitivity
- Many studies make inferences based on transient LLM capabilities that may become obsolete as models are updated
- Key outstanding issues include which LLMs to use, handling different model performances, and managing proprietary models that are regularly updated

## Why This Works (Mechanism)
This conceptual analysis works by systematically identifying and categorizing the methodological challenges in using LLMs for cognitive science research. The paper examines how different research paradigms (GPT-ology, computational modeling, silicon sampling) each face unique challenges, and how these challenges interact to limit scientific inference. The mechanism involves highlighting the temporal fragility of LLM-based findings and the difficulty of drawing generalizable conclusions from models that are rapidly evolving. The analysis is structured around the central problem that current research often produces findings that may be obsolete as models are updated, rather than lasting scientific knowledge.

## Foundational Learning
- LLM capability transience: Why needed - Understanding that current LLM behaviors may not persist across model updates is crucial for evaluating the longevity of research findings. Quick check - Compare identical prompts across different model versions to verify result stability.
- Anthropomorphism in LLM interpretation: Why needed - Researchers must recognize when they're attributing human-like understanding to models that may simply be pattern matching. Quick check - Test whether model responses persist when prompts are rephrased in non-anthropomorphic terms.
- Prompt sensitivity and reproducibility: Why needed - Small changes in prompt formulation can dramatically alter model outputs, threatening research replicability. Quick check - Systematically vary prompt wording while keeping semantic content constant to measure output variability.
- Model transparency limitations: Why needed - Understanding that proprietary models lack transparency about their training data and architecture is essential for interpreting results. Quick check - Compare results across open-source and proprietary models to identify systematic differences.

## Architecture Onboarding
Component map: Research question -> LLM selection -> Prompt design -> Output analysis -> Scientific inference
Critical path: The most time-sensitive steps are prompt design and output analysis, as these directly impact the validity of scientific inferences and must be carefully controlled to ensure reproducibility.
Design tradeoffs: Using proprietary models offers access to state-of-the-art capabilities but limits transparency and control over updates; open-source models provide more transparency but may have lower performance. Researchers must balance these tradeoffs based on their specific research questions and the need for reproducibility.
Failure signatures: When scientific inferences are based on transient LLM capabilities, findings may be invalidated by model updates. When anthropomorphism is not controlled, researchers may attribute human-like reasoning to pattern matching. When prompt sensitivity is not addressed, results may not be reproducible.
3 first experiments: 1) Test the same cognitive task across multiple LLM versions to quantify result stability over time. 2) Systematically vary prompts for identical tasks to measure sensitivity effects. 3) Compare silicon sampling outputs to actual human responses on identical tasks to assess ecological validity.

## Open Questions the Paper Calls Out
The paper identifies several key open questions: Which specific LLMs should researchers use for cognitive science studies? What inferences should be drawn when different models produce different results on the same task? How should researchers handle the fact that proprietary models are regularly updated by companies, potentially invalidating previous findings? The paper also questions the validity of using synthetic human responses generated by models trained on internet text to represent real human behavior, and calls for more research on making inferences that remain valid across model updates rather than being tied to specific model versions.

## Limitations
- Rapidly evolving LLM capabilities mean many current observations may become outdated quickly
- The paper lacks empirical data quantifying the extent of anthropomorphism and reliability issues across different tasks and model versions
- Insufficient evidence is provided for the ecological validity of "silicon sampling" as a method for simulating human behavior

## Confidence
- High confidence: The identification of anthropomorphism as a concern in LLM research
- Medium confidence: The challenges related to prompt sensitivity and reproducibility
- Low confidence: The specific recommendations for generalizable knowledge generation across model updates

## Next Checks
1. Conduct systematic replication studies using multiple LLM versions (e.g., GPT-3.5, GPT-4, Claude) on identical cognitive tasks to quantify the stability of results across model updates
2. Design controlled experiments comparing silicon sampling outputs to actual human responses on identical tasks, measuring ecological validity
3. Develop and test standardized protocols for documenting prompt variations and their effects on model outputs to address reproducibility concerns