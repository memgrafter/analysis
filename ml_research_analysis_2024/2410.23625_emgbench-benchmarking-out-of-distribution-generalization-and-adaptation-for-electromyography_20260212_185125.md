---
ver: rpa2
title: 'EMGBench: Benchmarking Out-of-Distribution Generalization and Adaptation for
  Electromyography'
arxiv_id: '2410.23625'
source_url: https://arxiv.org/abs/2410.23625
tags:
- data
- dataset
- classification
- datasets
- gesture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first benchmark for evaluating out-of-distribution
  performance of EMG classification algorithms. The authors present a standardized
  codebase for testing generalization and adaptation tasks across nine EMG datasets,
  including a new dataset collected with a high-density wearable EMG sensor.
---

# EMGBench: Benchmarking Out-of-Distribution Generalization and Adaptation for Electromyography

## Quick Facts
- **arXiv ID**: 2410.23625
- **Source URL**: https://arxiv.org/abs/2410.23625
- **Authors**: Jehan Yang; Maxwell Soh; Vivianna Lieu; Douglas J Weber; Zackory Erickson
- **Reference count**: 40
- **Primary result**: First benchmark for evaluating out-of-distribution performance of EMG classification algorithms

## Executive Summary
This paper introduces EMGBench, the first benchmark for evaluating out-of-distribution generalization and adaptation of electromyography (EMG) classification algorithms. The authors present a standardized codebase for testing generalization and adaptation tasks across nine EMG datasets, including a new dataset collected with a high-density wearable EMG sensor. Their results demonstrate that fine-tuning pretrained models on small amounts of subject-specific data significantly improves classification accuracy, with some datasets achieving over 95% accuracy using only 5% of a subject's data. The benchmark enables standardized comparison of EMG classification methods and provides insights into practical deployment challenges like inter-subject variability and temporal shifts between data collection sessions.

## Method Summary
The authors created EMGBench to evaluate EMG gesture classification across nine datasets using leave-one-subject-out cross-validation (LOSO-CV) for generalization and few-shot fine-tuning for adaptation. They converted time-series EMG data into 2D activity maps (heatmaps, spectrograms, continuous wavelet transforms) to enable use of image classification architectures. Pretrained models (ResNet18, EfficientNet, ViT, EfficientViT) were fine-tuned on subject-specific data ranging from 5% to 100% of available samples. The framework tested four preprocessing methods and compared against domain generalization techniques (IRM, CORAL).

## Key Results
- LOSO-CV accuracy ranged from 58.9% to 97.8% across nine datasets
- Fine-tuning pretrained models with 5% of subject-specific data achieved over 93% accuracy for some datasets
- Raw heatmaps, spectrograms, and continuous wavelet transforms all performed well as preprocessing methods
- ViT and EfficientNet models showed comparable performance to ResNet18 for EMG classification
- Fine-tuning consistently improved performance over training from scratch

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning pretrained models on small amounts of subject-specific data significantly improves classification accuracy across diverse EMG datasets.
- **Mechanism:** Pretrained models learn generalizable EMG gesture features across subjects. Fine-tuning adapts these features to individual subject variations (muscle morphology, electrode placement, etc.) without overfitting.
- **Core assumption:** EMG gesture patterns are sufficiently similar across subjects to allow transfer learning.
- **Evidence anchors:**
  - [abstract]: "fine-tuning pretrained models on small amounts of subject-specific data significantly improves classification accuracy, with some datasets achieving over 95% accuracy using only 5% of a subject's data"
  - [section 5.3]: "Our findings indicate that for many datasets, performance increases with more fine-tuning data from the participant. However, for datasets such as the Myo Dataset and FlexWear-HD, performance reaches 93.1% for 7 gestures and 94.2% for 10 gestures, respectively, with just 5% of a subject's initial data"
  - [corpus]: Weak evidence for transfer learning in EMG; needs specific EMG domain studies
- **Break condition:** If subject-specific EMG patterns are too dissimilar across individuals, fine-tuning won't effectively transfer learned features.

### Mechanism 2
- **Claim:** Converting time-series EMG data into 2D activity maps enables effective use of image classification architectures for gesture recognition.
- **Mechanism:** EMG signals contain spatial patterns across electrodes and temporal patterns within gestures. Converting to 2D heatmaps preserves both dimensions, allowing CNNs and transformers to learn spatial-temporal feature representations.
- **Core assumption:** EMG gesture patterns are visually distinguishable when represented as 2D activity maps.
- **Evidence anchors:**
  - [section 4.1]: "By converting raw time series data or other manual features over time to heatmaps, we are able to create spatiotemporal patterns from time-series data, which can be given as input into 2D CNNs"
  - [section 5.1]: "Raw heatmaps, spectrograms, and CWTs all perform well for most datasets, with different datasets having different preprocessing methods that work the best out of the four"
  - [corpus]: Limited evidence of EMG-to-image conversion success; needs more domain-specific validation
- **Break condition:** If EMG patterns are not visually distinguishable in 2D representations, image classifiers won't effectively learn gesture features.

### Mechanism 3
- **Claim:** Leave-one-subject-out cross-validation (LOSO-CV) provides realistic assessment of EMG classifier generalization to new users.
- **Mechanism:** LOSO-CV tests whether models can classify gestures for subjects entirely excluded from training, simulating real-world deployment scenarios where new users must use pre-trained systems.
- **Core assumption:** EMG gesture patterns exhibit sufficient consistency across subjects to enable classification without subject-specific training data.
- **Evidence anchors:**
  - [section 2.3]: "LOSO-CV introduces variability stemming from inter-individual differences in body size, muscle morphology [25], and differences in skin impedance and adipose tissue distribution [27]"
  - [section 4.3]: "In task 1, we use LOSO-CV to assess how well an EMG gesture classifier and interface may work for a new user whose data was not included in training"
  - [corpus]: Weak evidence of LOSO-CV effectiveness in EMG domain; needs more validation studies
- **Break condition:** If inter-subject EMG variability is too high, LOSO-CV performance will be unacceptably low for practical deployment.

## Foundational Learning

- **Concept: EMG signal characteristics**
  - Why needed here: Understanding EMG signal properties (frequency range, spatial distribution, non-stationarity) is crucial for selecting appropriate preprocessing methods and model architectures
  - Quick check question: What frequency range contains the most relevant information for EMG gesture classification?

- **Concept: Transfer learning principles**
  - Why needed here: The approach relies on pretrained models learning generalizable features that can be adapted to new subjects with minimal data
  - Quick check question: Why might pretrained image classifiers work well for EMG data when converted to 2D activity maps?

- **Concept: Cross-validation strategies**
  - Why needed here: Different data splitting approaches (random, chronological, leave-one-subject-out) have different implications for assessing real-world generalization
  - Quick check question: How does leave-one-subject-out cross-validation differ from random train-test splits in terms of what it measures?

## Architecture Onboarding

- **Component map:** Raw EMG data → Preprocessing → Pretrained model → Fine-tuning → Evaluation
- **Critical path:** Raw EMG data → Preprocessing → Pretrained model → Fine-tuning → Evaluation
- **Design tradeoffs:**
  - Preprocessing method: Raw heatmaps preserve temporal information but may be noisy; spectrograms capture frequency content but lose phase information
  - Model architecture: CNNs offer local feature learning with fewer parameters; transformers capture global relationships but require more data
  - Fine-tuning amount: More data improves accuracy but reduces the practical benefit of transfer learning
- **Failure signatures:**
  - Low LOSO-CV accuracy: Model fails to generalize across subjects
  - High variance in subject-specific performance: Model overfits to training subjects
  - Minimal improvement from fine-tuning: Pretrained features aren't transferable to new subjects
- **First 3 experiments:**
  1. Test LOSO-CV performance on a single dataset with raw heatmap preprocessing and ResNet18
  2. Evaluate fine-tuning performance using 5% of left-out subject data on the same dataset
  3. Compare different preprocessing methods (raw, RMS, spectrogram, CWT) for the same dataset and model

## Open Questions the Paper Calls Out

- **Open Question 1:** How does EMG classification performance vary across different electrode materials and configurations beyond what was tested in this benchmark?
  - Basis in paper: [explicit] The paper discusses various electrode configurations (dry vs wet, individual vs array) and their potential impact on classification accuracy
  - Why unresolved: The benchmark only tested a subset of available electrode configurations, leaving open the question of optimal hardware for different use cases
  - What evidence would resolve it: Systematic testing of EMG classification performance across a wider range of electrode types, materials, and placements

- **Open Question 2:** What is the minimum amount of data needed from a new user to achieve acceptable EMG classification accuracy in real-world scenarios?
  - Basis in paper: [explicit] The paper shows that 5% of a subject's data can achieve over 93% accuracy for some datasets, but this may not reflect real-world deployment
  - Why unresolved: Real-world conditions involve more variability than controlled laboratory settings, and the paper doesn't test generalization to spontaneous gesture execution
  - What evidence would resolve it: Testing EMG classification with varying amounts of user data in uncontrolled, real-world environments

- **Open Question 3:** How does the inclusion of transition data between gestures affect EMG classification accuracy in practical control interfaces?
  - Basis in paper: [explicit] The paper shows decreased accuracy when including transition data in classification, but doesn't explore strategies to improve performance
  - Why unresolved: Transition data is common in natural gesture sequences but was excluded from most tested datasets, leaving open how to handle this in practice
  - What evidence would resolve it: Development and testing of EMG classification methods that explicitly model gesture transitions

## Limitations

- Transfer learning effectiveness across EMG datasets has limited validation - while the paper demonstrates success on nine datasets, the underlying assumption that EMG gesture features are sufficiently generalizable across different sensor types, electrode placements, and experimental protocols remains uncertain
- The conversion of time-series EMG data to 2D activity maps assumes that spatial-temporal patterns are visually distinguishable, but this has limited empirical validation in the EMG domain
- LOSO-CV provides a proxy for real-world generalization but doesn't account for temporal drift, environmental changes, or long-term adaptation effects that occur in actual deployment

## Confidence

- **High confidence**: LOSO-CV methodology and its role in measuring inter-subject generalization; the benchmark framework and dataset collection procedures
- **Medium confidence**: Fine-tuning effectiveness with small subject-specific datasets; the assumption that ImageNet-pretrained models transfer well to EMG patterns
- **Low confidence**: Claims about visual distinguishability of EMG patterns in 2D representations; generalization across radically different EMG acquisition systems

## Next Checks

1. Test transfer learning effectiveness by pretraining on one EMG dataset and fine-tuning on a completely different dataset with different sensor specifications and gesture vocabularies
2. Validate the 2D activity map conversion approach by comparing classification performance using the same models trained directly on raw time-series data (with appropriate architectures like RNNs)
3. Assess temporal generalization by splitting datasets chronologically rather than by subject to evaluate model robustness to temporal drift and session-to-session variations