---
ver: rpa2
title: Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks
arxiv_id: '2403.04814'
source_url: https://arxiv.org/abs/2403.04814
tags:
- arxiv
- code
- llms
- syntax-aware
- safim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAFIM, a new benchmark for evaluating large
  language models (LLMs) on syntax-aware code fill-in-the-middle (FIM) tasks. SAFIM
  includes 17,720 examples from multiple programming languages, sourced from recent
  code submissions to minimize data contamination.
---

# Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks

## Quick Facts
- **arXiv ID**: 2403.04814
- **Source URL**: https://arxiv.org/abs/2403.04814
- **Reference count**: 40
- **Key outcome**: Introduces SAFIM benchmark with 17,720 examples to evaluate LLMs on syntax-aware code FIM tasks, showing pretraining method and data quality often outweigh model size.

## Executive Summary
This paper introduces SAFIM, a new benchmark for evaluating large language models (LLMs) on syntax-aware code fill-in-the-middle (FIM) tasks. The benchmark includes 17,720 examples from multiple programming languages, sourced from recent code submissions to minimize data contamination. The authors evaluate 15 LLMs using various prompt designs and introduce a syntax-aware truncation algorithm for post-processing outputs. Their comprehensive evaluation shows that FIM pretraining not only enhances FIM proficiency but also improves left-to-right (L2R) inference using LLMs.

## Method Summary
The authors created SAFIM by collecting code from Codeforces and GitHub, filtering for recent submissions after April 2022 to minimize contamination. They generated FIM tasks by identifying code blocks and conditional expressions using AST parsing, creating prefix and suffix contexts with missing middle segments. The evaluation used five prompt designs (L2R, PSM, SPM, IPF, 1S) and implemented a syntax-aware truncation algorithm to post-process model outputs. They evaluated 15 LLMs across four programming languages using execution-based tests when available, falling back to syntactic matching otherwise.

## Key Results
- SAFIM achieves an average pass@1 rate of 40.9% across all evaluated models
- DeepSeekCoder-33B performs best with 69.0% pass@1 rate
- Pretraining method and data quality are more important than sheer model size
- FIM pretraining improves both FIM and L2R performance
- Repository-level information in pretraining data improves API function call completion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Syntax-aware truncation enables fair evaluation of non-FIM models by removing extraneous code beyond the targeted completion structure.
- Mechanism: The truncation algorithm iteratively removes trailing code until the remaining segment fits the AST as the correct structure type (block or expression) and the context aligns with the original code's AST.
- Core assumption: Model outputs contain valid completions followed by extraneous code, and AST alignment is sufficient to detect correct boundaries.
- Evidence anchors:
  - [abstract] "Our approach unveils the true capabilities of non-FIM-trained models, allowing for a fair comparison with FIM-trained models."
  - [section] "Syntax-Aware Truncation. In SAFIM, we introduce a syntax-aware truncation algorithm, replacing the conventional regex-based heuristics."
- Break condition: If models learn to generate bounded completions or if AST alignment fails due to subtle structural mismatches.

### Mechanism 2
- Claim: FIM pretraining improves both FIM and L2R performance by providing richer contextual understanding during training.
- Mechanism: FIM training exposes models to bidirectional context (prefix and suffix), enhancing their ability to understand code relationships and generate coherent completions in both FIM and L2R scenarios.
- Core assumption: Bidirectional context during training translates to better performance on both task types.
- Evidence anchors:
  - [abstract] "Our findings suggest that FIM pretraining not only improves LLMs’ performance in FIM inference but also enhances their performance in classical Left-to-Right (L2R) inference scenarios."
  - [section] "FIM Pretraining Boosts Both FIM and L2R Performance. Pretraining LLMs with a FIM objective enhances their performance not only in FIM but also in left-to-right (L2R) generation."
- Break condition: If the bidirectional context advantage diminishes for simpler code patterns or if L2R performance plateaus regardless of FIM training.

### Mechanism 3
- Claim: Repository-level information in pretraining data improves API function call completion performance.
- Mechanism: Including repository context (issues, commit messages, topological ordering) provides models with better understanding of API usage patterns and dependencies, leading to more accurate API completions.
- Core assumption: API usage patterns and dependencies are learnable from repository-level context and improve completion accuracy.
- Evidence anchors:
  - [abstract] "We also observe that pretraining methods and data quality often outweigh the sheer model size—smaller models with sophisticated pretraining paradigms often outperform larger models."
  - [section] "For API function call completion, repository-level information is key. StarCoder and DeepSeekCoder, which excel in this task, both incorporate repository context into their pretraining data."
- Break condition: If API patterns become too diverse or if repository context introduces noise that outweighs the benefits.

## Foundational Learning

- **Abstract Syntax Trees (ASTs)**: ASTs provide the structural representation needed for syntax-aware truncation and task generation. *Quick check: Can you explain how an AST represents code structure differently from plain text?*

- **Bidirectional context modeling**: FIM pretraining relies on understanding both prefix and suffix context to generate middle segments. *Quick check: How does bidirectional context differ from traditional L2R language modeling?*

- **Execution-based evaluation**: SAFIM uses execution tests to validate completions, requiring understanding of code execution environments. *Quick check: What are the advantages and limitations of execution-based evaluation compared to syntactic matching?*

## Architecture Onboarding

- **Component map**: Corpus collection (Codeforces + GitHub) -> AST parsing and task generation -> Prompt engineering (L2R, PSM, SPM, IPF, 1S) -> Syntax-aware truncation algorithm -> Execution-based evaluation framework -> Result aggregation and analysis

- **Critical path**: Corpus → AST parsing → Task generation → Model inference → Post-processing → Evaluation → Analysis

- **Design tradeoffs**: 
  - Execution-based vs. syntactic evaluation: Execution is more reliable but limited by test availability and external dependencies
  - Prompt diversity vs. standardization: Multiple prompts enable fair comparison but increase complexity
  - Truncation aggressiveness vs. correctness: Aggressive truncation may remove valid code; conservative truncation may retain extraneous content

- **Failure signatures**:
  - High compilation error rates indicate issues with truncation or model generation
  - Performance gaps across prompts suggest prompt sensitivity
  - Inconsistent results across programming languages may indicate PL-specific challenges

- **First 3 experiments**:
  1. Implement syntax-aware truncation on a small subset and verify it correctly identifies completion boundaries
  2. Test multiple prompts on one model to identify the most effective prompt type
  3. Compare execution-based and syntactic evaluation on a mixed dataset to understand their relative strengths

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the pretraining data distribution (e.g., programming languages, project types) influence model performance on different SAFIM task categories?
  - Basis: The paper notes pretraining methods and data influence task-specific performance
  - Why unresolved: Analysis only compares different model families rather than controlled experiments varying pretraining data distributions
  - What evidence would resolve it: Training identical model architectures with different pretraining data distributions and measuring performance on each SAFIM category

- **Open Question 2**: Does the syntax-aware truncation algorithm have diminishing returns as model size increases?
  - Basis: The paper shows syntax-aware truncation significantly improves non-FIM models but mentions this effect might be less pronounced for FIM models
  - Why unresolved: The paper doesn't analyze truncation effectiveness across different model sizes systematically
  - What evidence would resolve it: Comparing pass@1 rates with and without truncation across a wide range of model sizes

- **Open Question 3**: How does the syntax-aware truncation algorithm affect model performance on other code generation benchmarks?
  - Basis: The paper demonstrates syntax-aware truncation improves FIM output quality and enables fair comparison for non-FIM models on SAFIM
  - Why unresolved: The paper only evaluates truncation on SAFIM and doesn't test its effectiveness on other benchmarks
  - What evidence would resolve it: Applying syntax-aware truncation to model outputs on benchmarks like HumanEval or MBPP and measuring performance changes

## Limitations

- The syntax-aware truncation algorithm relies heavily on AST alignment which may not always correctly identify completion boundaries in complex code structures
- The evaluation framework's dependence on execution-based tests introduces constraints, as 18.9% of examples had to be excluded due to compilation failures or external dependencies
- The study focuses on four programming languages, limiting generalizability to other languages or domains

## Confidence

- **High confidence**: The effectiveness of syntax-aware truncation for fair evaluation across model types, supported by direct experimental evidence
- **Medium confidence**: The superiority of FIM pretraining for both FIM and L2R tasks, based on comparative results but lacking ablation studies on pretraining data quality
- **Low confidence**: The claim that pretraining method and data quality consistently outweigh model size, as this conclusion relies heavily on cherry-picked comparisons without comprehensive statistical analysis across all model pairs

## Next Checks

1. **Truncation Algorithm Validation**: Implement the syntax-aware truncation on a held-out test set and measure precision/recall of boundary detection compared to human-annotated completion boundaries.

2. **Pretraining Data Quality Ablation**: Systematically vary the quality and composition of pretraining data (repository-level information, execution-based feedback) while controlling for model size to isolate their effects on specific task performance.

3. **Cross-Language Generalization**: Evaluate the best-performing models on SAFIM-style tasks in additional programming languages not included in the original benchmark to assess the generalizability of findings beyond the four studied languages.