---
ver: rpa2
title: 'ReCycle: Fast and Efficient Long Time Series Forecasting with Residual Cyclic
  Transformers'
arxiv_id: '2405.03429'
source_url: https://arxiv.org/abs/2405.03429
tags:
- time
- recycle
- series
- forecasting
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ReCycle, a method for fast and energy-efficient
  long time series forecasting using Transformer-based architectures. The core idea
  is to address the computational complexity of the attention mechanism by leveraging
  primary cycle compression (PCC) and residual learning.
---

# ReCycle: Fast and Efficient Long Time Series Forecasting with Residual Cyclic Transformers

## Quick Facts
- arXiv ID: 2405.03429
- Source URL: https://arxiv.org/abs/2405.03429
- Reference count: 17
- Primary result: ReCycle improves long time series forecasting accuracy while reducing training time and energy consumption by more than an order of magnitude compared to state-of-the-art Transformer models.

## Executive Summary
ReCycle introduces a novel approach for long time series forecasting that addresses the computational complexity of Transformer-based architectures. The method combines Primary Cycle Compression (PCC) with residual learning from Recent Historic Profiles (RHP) to transform univariate time series into multivariate representations that are more efficient for attention mechanisms. By learning residuals rather than absolute values, ReCycle focuses on modulations rather than predominant patterns, enabling faster training and lower energy consumption. Experiments across five diverse datasets demonstrate significant improvements in both forecasting accuracy and computational efficiency, making the approach suitable for deployment on low-power edge computing devices.

## Method Summary
ReCycle tackles long time series forecasting by first applying Primary Cycle Compression (PCC) to convert univariate time series into multivariate representations based on primary cycles (typically 24-hour periods). The method then extracts Recent Historic Profiles (RHP) from the last 3 days of data, categorized by day type (weekday, Saturday, holiday), and computes residuals by subtracting these profiles from the original series. These residuals are then used as input to encoder-decoder Transformer architectures. The approach leverages the fact that residuals capture modulations rather than predominant patterns, making them more amenable to efficient attention mechanisms. The entire pipeline is designed to reduce the computational burden while maintaining or improving forecasting accuracy.

## Key Results
- ReCycle achieves significant improvements in forecasting accuracy (MSE and MAPE) across five diverse time series datasets
- Training time and energy consumption are reduced by more than an order of magnitude compared to state-of-the-art Transformer models
- The approach is effective for deployment on low-performance, low-power, and edge computing devices
- ReCycle demonstrates particular effectiveness for long time series forecasting tasks

## Why This Works (Mechanism)
ReCycle works by addressing two key challenges in Transformer-based time series forecasting: the computational complexity of attention mechanisms and the difficulty of modeling long-range dependencies. The Primary Cycle Compression converts univariate time series into multivariate representations by extracting cycles (e.g., daily patterns), which reduces the sequence length that attention mechanisms must process. By learning residuals from Recent Historic Profiles rather than absolute values, the model focuses on capturing modulations and anomalies rather than predominant patterns, which are already captured by the RHP. This decomposition allows the Transformer to operate more efficiently on the residuals while maintaining accuracy.

## Foundational Learning
1. **Primary Cycle Compression (PCC)**
   - Why needed: Reduces sequence length by converting univariate to multivariate representations based on repeating patterns
   - Quick check: Verify that cycle extraction preserves temporal dependencies while reducing dimensionality

2. **Recent Historic Profiles (RHP)**
   - Why needed: Captures predominant patterns from recent history to enable residual learning
   - Quick check: Ensure RHP smoothing effectively represents baseline patterns without overfitting noise

3. **Residual Learning**
   - Why needed: Focuses model attention on modulations and anomalies rather than predominant patterns
   - Quick check: Validate that residuals are sufficiently informative for forecasting while being computationally efficient

4. **Transformer Attention Mechanism**
   - Why needed: Standard architecture for sequence modeling, but computationally expensive for long sequences
   - Quick check: Confirm that attention efficiency gains from PCC are maintained during training

5. **Multivariate Time Series Representation**
   - Why needed: Enables more efficient processing by the attention mechanism compared to univariate series
   - Quick check: Verify that multivariate representation preserves all necessary temporal information

## Architecture Onboarding

**Component Map:** Raw Time Series -> PCC (24-hour cycles) -> RHP Extraction (3 days) -> Residual Calculation -> Transformer Encoder-Decoder -> Forecast

**Critical Path:** The most critical components are the PCC transformation and residual calculation, as errors in these steps propagate through the entire pipeline. The Transformer architecture itself is relatively standard but must be adapted to handle multivariate input from PCC.

**Design Tradeoffs:** The method trades some model complexity (additional preprocessing steps) for computational efficiency. The choice of cycle length (24 hours) is application-specific and may need adjustment for different domains. The 3-day RHP window is a balance between capturing sufficient context and computational efficiency.

**Failure Signatures:** Poor performance on datasets without clear cyclical patterns, high residual variance indicating insufficient RHP smoothing, or computational overhead from inefficient PCC implementation.

**Three First Experiments:**
1. Implement PCC with varying cycle lengths (12h, 24h, 48h) on a single dataset to determine optimal cycle duration
2. Compare forecasting accuracy using raw time series vs. residuals vs. combined approach to validate residual learning effectiveness
3. Benchmark training time and energy consumption against a standard Transformer baseline using identical hardware configurations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ReCycle compare to other non-Transformer architectures like NHiTS when applied to different types of time series datasets?
- Basis in paper: The paper mentions that NHiTS yields low training times and energy consumption, but ReCycle-based Transformer approaches provide even lower values.
- Why unresolved: The paper does not provide a direct comparison between ReCycle and NHiTS across multiple datasets in terms of forecasting accuracy.
- What evidence would resolve it: Conducting experiments to compare the forecasting accuracy of ReCycle-based Transformer models and NHiTS across a variety of time series datasets would provide the necessary evidence.

### Open Question 2
- Question: What is the impact of different primary cycle lengths on the performance of ReCycle?
- Basis in paper: The paper mentions that D, the length of the primary cycle, is an application-specific parameter.
- Why unresolved: The paper does not explore the effect of varying the primary cycle length on the model's performance.
- What evidence would resolve it: Running experiments with different primary cycle lengths for various datasets would help determine the optimal cycle length for ReCycle.

### Open Question 3
- Question: How does ReCycle handle missing values in time series data, and what is the impact on its performance?
- Basis in paper: The paper mentions that missing values are filled with the average of neighboring values, but does not discuss how ReCycle specifically handles missing data.
- Why unresolved: The paper does not provide details on the robustness of ReCycle when dealing with missing data.
- What evidence would resolve it: Analyzing the performance of ReCycle on datasets with varying degrees of missing data would clarify its robustness and effectiveness in handling such scenarios.

## Limitations
- Claims about energy consumption improvements rely on reported computational metrics that are not independently verified
- The approach may be less effective for non-cyclical time series patterns or datasets with irregular temporal resolutions
- Claims about edge deployment feasibility are theoretical extrapolations rather than demonstrated through actual deployments

## Confidence

**High Confidence:** The methodological description of PCC and RHP-based residual learning is detailed and reproducible. The core innovation of converting univariate to multivariate time series via cycle compression and learning residuals from recent historic profiles is well-defined.

**Medium Confidence:** The experimental results demonstrating improved accuracy and efficiency are plausible given the described methodology, but cannot be independently verified without exact model configurations and energy measurement protocols.

**Low Confidence:** Claims about deployment on edge computing devices are theoretical extrapolations from reported energy metrics rather than demonstrated through actual edge deployments.

## Next Checks

1. Replicate the energy consumption measurements using the same hardware configurations and measurement methodology to verify the claimed "order of magnitude" improvements across all five datasets.

2. Test the ReCycle approach on non-cyclical time series datasets (e.g., financial data with irregular patterns) to evaluate its robustness beyond the reported datasets.

3. Implement and compare against additional Transformer-based baselines (e.g., Informer, Autoformer) with standardized hyperparameter tuning to validate the claimed superiority over "state-of-the-art" models.