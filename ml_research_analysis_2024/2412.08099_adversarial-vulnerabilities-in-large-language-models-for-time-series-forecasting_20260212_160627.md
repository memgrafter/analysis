---
ver: rpa2
title: Adversarial Vulnerabilities in Large Language Models for Time Series Forecasting
arxiv_id: '2412.08099'
source_url: https://arxiv.org/abs/2412.08099
tags:
- forecasting
- time
- series
- adversarial
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the robustness of large language models
  (LLMs) for time series forecasting against adversarial attacks. The authors propose
  a targeted adversarial attack framework using directional gradient approximation
  (DGA), a gradient-free optimization method, to generate imperceptible perturbations
  that significantly degrade forecasting accuracy.
---

# Adversarial Vulnerabilities in Large Language Models for Time Series Forecasting

## Quick Facts
- **arXiv ID**: 2412.08099
- **Source URL**: https://arxiv.org/abs/2412.08099
- **Reference count**: 12
- **Primary result**: Adversarial attacks cause up to 80% larger MAE increases than Gaussian noise for LLM-based time series forecasting

## Executive Summary
This paper investigates the robustness of large language models (LLMs) for time series forecasting against adversarial attacks. The authors propose a targeted adversarial attack framework using Directional Gradient Approximation (DGA), a gradient-free optimization method, to generate imperceptible perturbations that significantly degrade forecasting accuracy. Experiments across multiple datasets and six LLM architectures show that adversarial attacks cause much larger performance degradation than random noise, with MAE increases up to 80% higher than Gaussian noise under equivalent perturbation levels. The results highlight critical vulnerabilities of LLMs in time series forecasting and underscore the need for robust defense mechanisms.

## Method Summary
The paper proposes a black-box adversarial attack framework for LLM-based time series forecasting using Directional Gradient Approximation (DGA), a gradient-free optimization method. The attack generates minimal perturbations (constrained by ℓ1-norm) that push forecasts toward random walk behavior. The framework is evaluated on six LLM architectures (GPT-3.5, GPT-4, LLaMa, Mistral, TimeGPT, and TimeLLM) across five real-world datasets, comparing adversarial performance against Gaussian white noise (GWN) baselines using MAE and MSE metrics.

## Key Results
- Adversarial attacks using DGA cause up to 80% larger MAE increases than Gaussian white noise under equivalent perturbation levels
- The attack consistently degrades performance across all six tested LLM architectures and five diverse datasets
- Black-box adversarial perturbations successfully push LLM predictions toward random walk behavior, disrupting temporal dependencies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-free optimization can generate effective adversarial perturbations for LLMs in time series forecasting despite black-box access.
- Mechanism: Directional Gradient Approximation (DGA) approximates the direction from normal predictions to targeted anomalous outputs using small random signals, then applies ℓ1-constrained perturbations to push forecasts toward random walk behavior.
- Core assumption: Even without model gradients, small signal perturbations can approximate the adversarial direction sufficiently to degrade forecasting performance.
- Evidence anchors:
  - [abstract] "By employing both gradient-free and black-box optimization methods, we generate minimal yet highly effective perturbations"
  - [section 4] "We propose a gradient-free optimization approach, referred to as targeted attack with Directional Gradient Approximation (DGA)"
  - [corpus] Weak evidence - corpus focuses on general LLM security rather than gradient-free time series attacks
- Break condition: If the approximated gradient direction is orthogonal to the true adversarial direction, perturbations may fail to degrade performance significantly.

### Mechanism 2
- Claim: LLMs for time series forecasting are more vulnerable to targeted adversarial attacks than random noise of equivalent magnitude.
- Mechanism: The proposed attack specifically manipulates the model's ability to maintain temporal dependencies, causing predictions to deviate significantly from both ground truth and normal predictions.
- Core assumption: LLMs rely heavily on temporal structure, and disrupting this structure through adversarial perturbations causes disproportionate performance degradation.
- Evidence anchors:
  - [abstract] "adversarial attacks cause much larger performance degradation than random noise, with MAE increases up to 80% higher than Gaussian noise under equivalent perturbation levels"
  - [section 5.5] "The proposed DGA method is designed to mislead the forecasting model, causing its predictions to resemble a random walk"
  - [corpus] Moderate evidence - related work discusses general LLM vulnerabilities but not specific to time series
- Break condition: If the model has strong regularization or denoising capabilities that can filter out adversarial perturbations.

### Mechanism 3
- Claim: Black-box adversarial attacks on LLM-based forecasting models are effective across different model architectures and datasets.
- Mechanism: The attack framework works by perturbing input time series sequences to generate adversarial examples that consistently degrade performance across diverse LLM architectures (GPT-3.5, GPT-4, LLaMa, Mistral, TimeGPT, TimeLLM) and datasets.
- Core assumption: The fundamental vulnerability exploited by the attack is common across different LLM implementations and time series domains.
- Evidence anchors:
  - [abstract] "Experiments across multiple datasets and six LLM architectures... show that adversarial attacks cause much larger performance degradation than random noise"
  - [section 5.4] "the proposed adversarial perturbations introduced significantly greater disruptions than GWN, clearly impacting the predictions and demonstrating the precision of the attack"
  - [corpus] Limited evidence - corpus papers focus on general LLM security rather than cross-architecture time series attacks
- Break condition: If certain architectures have inherent robustness mechanisms that make them resistant to this specific attack pattern.

## Foundational Learning

- Concept: Time series forecasting fundamentals (stationarity, temporal dependencies, autocorrelation)
  - Why needed here: Understanding how LLMs model temporal patterns is crucial for designing effective adversarial perturbations
  - Quick check question: What is the difference between white noise and a random walk in time series analysis?

- Concept: Gradient-free optimization methods and their applications
  - Why needed here: The attack uses Directional Gradient Approximation, a gradient-free approach, requiring understanding of alternative optimization strategies
  - Quick check question: How does finite difference approximation estimate gradients when analytical gradients are unavailable?

- Concept: Adversarial machine learning concepts (adversarial examples, threat models, attack transferability)
  - Why needed here: The paper's attack framework relies on established adversarial ML principles adapted for time series and black-box scenarios
  - Quick check question: What distinguishes white-box from black-box adversarial attacks?

## Architecture Onboarding

- Component map: Input time series → DGA perturbation generation → Adversarial example creation → Model prediction → Performance evaluation → Comparison with baseline (GWN)
- Critical path: Input time series → DGA perturbation generation → Adversarial example creation → Model prediction → Performance evaluation → Comparison with baseline (GWN)
- Design tradeoffs: Imperceptible perturbations vs. effective attack (ℓ1-norm constraint), black-box access vs. attack precision, computational efficiency vs. attack strength
- Failure signatures: Minimal performance degradation compared to GWN, failure to maintain temporal structure in perturbed inputs, model resistance to perturbation magnitude
- First 3 experiments:
  1. Implement DGA on a simple LLM time series model with synthetic data to verify basic attack functionality
  2. Compare attack effectiveness across different perturbation magnitudes to identify optimal trade-off point
  3. Test attack transferability by evaluating whether perturbations effective on one LLM architecture work on others

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would adversarial training be in improving LLM robustness for time series forecasting?
- Basis in paper: explicit
- Why unresolved: The paper discusses the high computational cost and impracticality of adversarial training for LLMs due to their pre-training on large datasets and the iterative nature of adversarial training.
- What evidence would resolve it: Empirical studies comparing the robustness of LLMs with and without adversarial training on time series forecasting tasks.

### Open Question 2
- Question: Can preprocessing-based defense methods effectively mitigate adversarial attacks on LLM-based time series forecasting models?
- Basis in paper: explicit
- Why unresolved: The paper suggests preprocessing-based methods like filter-based defenses and machine learning-based anomaly detection as practical alternatives but does not provide empirical results.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of preprocessing-based defense methods against adversarial attacks on LLM-based time series forecasting models.

### Open Question 3
- Question: How do different types of adversarial perturbations (e.g., noise-based, geometric transformations) affect the performance of LLM-based time series forecasting models?
- Basis in paper: explicit
- Why unresolved: The paper focuses on gradient-free optimization methods and Gaussian White Noise (GWN) but does not explore other types of adversarial perturbations.
- What evidence would resolve it: Comparative studies evaluating the impact of various adversarial perturbation techniques on LLM-based time series forecasting models.

## Limitations

- The experimental setup tests only one specific attack magnitude (2% perturbation scale) without exploring sensitivity to different perturbation levels
- The evaluation focuses exclusively on MAE and MSE metrics, neglecting other potentially relevant measures like directional accuracy or temporal structure preservation
- The paper claims cross-architecture effectiveness but only tests six specific LLM implementations without exploring architectural diversity comprehensively

## Confidence

- **High confidence**: The empirical observation that DGA-generated perturbations outperform random noise of equivalent magnitude (MAE increases up to 80% higher)
- **Medium confidence**: The general claim that LLMs for time series forecasting are vulnerable to black-box adversarial attacks, based on results across multiple datasets
- **Low confidence**: The assertion that these vulnerabilities are fundamental to LLM time series forecasting rather than specific to the tested architectures or attack parameters

## Next Checks

1. **Attack Magnitude Sensitivity**: Systematically vary perturbation scales (e.g., 0.5%, 1%, 2%, 5%, 10%) to determine whether the 2:1 performance degradation advantage holds across different attack strengths, and identify the point of diminishing returns where random noise becomes comparably effective.

2. **Temporal Structure Analysis**: Beyond aggregate error metrics, analyze how adversarial perturbations specifically affect temporal dependencies by examining autocorrelation preservation, trend-following behavior, and short-term vs. long-term forecasting accuracy separately.

3. **Architecture Transferability Test**: Generate perturbations on one LLM architecture (e.g., GPT-4) and evaluate their effectiveness against all other tested architectures to quantify the actual transferability of the attack, which would validate or challenge the paper's cross-architecture vulnerability claims.