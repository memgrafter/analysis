---
ver: rpa2
title: 'The Mismeasure of Man and Models: Evaluating Allocational Harms in Large Language
  Models'
arxiv_id: '2408.01285'
source_url: https://arxiv.org/abs/2408.01285
tags:
- bias
- group
- rabbi
- essay
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses bias in LLM-based decision-making, specifically
  in allocation tasks like hiring and grading. Existing bias metrics fail to reliably
  capture group disparities in allocation outcomes because they focus on average prediction
  gaps rather than actual selection rates.
---

# The Mismeasure of Man and Models: Evaluating Allocational Harms in Large Language Models

## Quick Facts
- arXiv ID: 2408.01285
- Source URL: https://arxiv.org/abs/2408.01285
- Reference count: 37
- Primary result: RABBI metric shows strong correlation (r ≥ 0.86 for resume screening, r ≥ 0.70 for essay grading) with allocation gaps in LLM-based hiring and grading decisions

## Executive Summary
This paper addresses the critical gap between traditional bias metrics and actual allocational harms in LLM-based decision-making. Existing bias metrics focus on average score differences rather than selection outcomes, making them poor predictors of real-world allocation disparities. The authors introduce RABBI (Rank-Allocational-Based Bias Index), a model-agnostic metric that measures allocational bias by comparing pairwise candidate scores across demographic groups. Through experiments on resume screening and essay grading with ten LLMs, RABBI demonstrates superior correlation with allocation gaps compared to traditional metrics, making it more effective for model selection in fair allocation contexts.

## Method Summary
The study evaluates RABBI on two allocation tasks: resume screening for 4 job positions with 8 demographic groups, and essay grading with 10 country groups. The method involves generating candidate scores using either pointwise (normalized model probabilities weighted by relevance) or pairwise ranking approaches, then computing bias as the difference in proportions of pairs where one group is preferred over another. The metric's predictive validity is assessed through Pearson correlation with allocation gaps (demographic parity and equal opportunity gaps), while its utility for model selection is evaluated using NDCG to compare model rankings against ideal fairness rankings.

## Key Results
- RABBI shows strong correlation with allocation gaps (r ≥ 0.86 for resume screening, r ≥ 0.70 for essay grading)
- RABBI outperforms traditional metrics (average score gap, Jensen-Shannon Divergence, Earth Mover's Distance) in predicting allocation disparities
- RABBI consistently ranks models closer to ideal fairness rankings with NDCG@10 ≥ 0.95 on both tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RABBI measures allocational bias by comparing pairwise candidate scores across groups.
- Mechanism: It computes the proportion of pairs where a candidate from group A is preferred over group B minus the proportion where B is preferred over A.
- Core assumption: Higher scores indicate better candidate fitness, and allocation decisions depend on relative ranking.
- Evidence anchors:
  - [abstract] "RABBI uses pointwise or pairwise ranking methods to generate scores and computes bias as the difference in proportions of pairs preferring one group over another."
  - [section 3.1] "We assess bias by comparing the scores between all possible candidate pairs in ξAB = A×B."
  - [corpus] Found 25 related papers, average neighbor FMR=0.511, showing moderate relevance to bias in LLMs.
- Break condition: If scores do not reliably reflect candidate quality or if relative ranking is not used in allocation decisions.

### Mechanism 2
- Claim: RABBI correlates strongly with allocation gaps in decision outcomes.
- Mechanism: By using relative pairwise comparisons, RABBI captures the probability that a candidate from one group will be selected over another in limited-resource allocation scenarios.
- Core assumption: Selection outcomes in allocation tasks depend on relative candidate scores rather than absolute score differences.
- Evidence anchors:
  - [abstract] "RABBI strongly correlates with allocation gaps (r ≥ 0.86 for resume screening, r ≥ 0.70 for essay grading)."
  - [section 5.1] "RABBI exhibits the strongest correlation across both tasks and evaluation settings with a correlation of ≥ 0.86 for resume screening and ≥ 0.7 for essay grading."
  - [corpus] Related work on bias in LLMs and allocational harms supports the need for metrics like RABBI.
- Break condition: If allocation decisions do not follow relative ranking or if score distributions are too skewed.

### Mechanism 3
- Claim: RABBI is more effective than traditional metrics for model selection in fair allocation.
- Mechanism: RABBI rankings align better with ideal rankings based on actual allocation gaps, leading to better model selection for fairness.
- Core assumption: Model selection for fairness should prioritize minimizing allocation disparities rather than average score differences.
- Evidence anchors:
  - [abstract] "RABBI also performs better in model selection for fairness, consistently ranking models closer to ideal rankings based on allocation gaps."
  - [section 5.2] "RABBI consistently performs better than all the baseline metrics under the same evaluation setting with an average NDCG@10 ≥ 0.95 on both tasks."
  - [corpus] Limited direct evidence in corpus, but moderate FMR suggests some relevance to model selection.
- Break condition: If fairness criteria other than allocation gaps are prioritized or if ranking stability is low.

## Foundational Learning

- Concept: Pairwise ranking and relative comparisons
  - Why needed here: RABBI fundamentally relies on comparing pairs of candidates across groups to assess allocational bias.
  - Quick check question: If you have two groups A and B, how does RABBI compute the bias score between them?

- Concept: Correlation and predictive validity
  - Why needed here: Evaluating RABBI requires understanding how well it predicts actual allocation gaps in decision outcomes.
  - Quick check question: What does a high Pearson correlation between RABBI scores and allocation gaps indicate about the metric's effectiveness?

- Concept: Normalized discounted cumulative gain (NDCG)
  - Why needed here: NDCG is used to evaluate the utility of RABBI for model selection by comparing model rankings to ideal rankings.
  - Quick check question: In the context of model selection for fairness, what does a high NDCG score indicate about a bias metric?

## Architecture Onboarding

- Component map: Scoring function -> Pairwise comparison -> Bias calculation
- Critical path: Generate candidate scores → Compare all pairs across groups → Calculate proportion differences → Output bias score
- Design tradeoffs: Pointwise scoring is faster but may miss relative comparisons; pairwise scoring is more accurate for relative comparisons but computationally expensive
- Failure signatures: Low correlation with allocation gaps, high sensitivity to score distribution skew, or inconsistent rankings across groups
- First 3 experiments:
  1. Implement RABBI with pointwise scoring on the resume screening task and compare correlation with allocation gaps
  2. Implement RABBI with pairwise scoring on the essay grading task and compare correlation with allocation gaps
  3. Use RABBI to rank models for fairness and compare the ranking to ideal rankings based on allocation gaps using NDCG

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does RABBI maintain its superior performance when evaluated on allocation tasks with continuous rather than binary outcomes?
- Basis in paper: [explicit] The paper evaluates RABBI on binary classification tasks (resume screening with Yes/No labels and essay grading with pass/fail thresholds). The authors note that their metric could be extended to continuous outcomes but do not provide experimental validation.
- Why unresolved: The current experiments only test binary outcomes, leaving uncertainty about RABBI's performance on tasks with continuous prediction scores.
- What evidence would resolve it: Experiments applying RABBI to allocation tasks with continuous outcomes (e.g., salary predictions, probability scores) showing consistent correlation with allocation gaps across different distributions.

### Open Question 2
- Question: How does RABBI perform when comparing groups with more than two categories or when the reference group is not predefined?
- Basis in paper: [explicit] RABBI is formulated for pairwise comparisons between a protected group and a reference group. The paper only evaluates comparisons against a single reference group (White males for resumes, L1 English speakers for essays).
- Why unresolved: Real-world allocation scenarios often involve multiple protected groups and may require comparisons between any group pairs, not just against a reference.
- What evidence would resolve it: Experiments extending RABBI to multi-group comparisons (e.g., computing bias between all possible group pairs) and showing consistent performance across different reference group choices.

### Open Question 3
- Question: Is RABBI robust to variations in selection quota k across different allocation contexts?
- Basis in paper: [explicit] The paper shows RABBI's correlation with allocation gaps varies with k, and while it generally performs well, the optimal k value differs between tasks. The authors do not investigate why certain k values yield better performance.
- Why unresolved: The paper only tests k values from 1 to 5, and the relationship between k and RABBI's effectiveness is not fully characterized.
- What evidence would resolve it: A systematic analysis of RABBI's performance across a wider range of k values and allocation contexts, identifying patterns or thresholds where the metric's predictive validity changes.

## Limitations
- Evaluation relies on synthetic datasets with predefined quotas and limited demographic categories (8 groups for resumes, 10 for essays)
- Temperature and generation parameters for LLM inference are unspecified, potentially affecting reproducibility
- Focuses exclusively on two allocation tasks, limiting generalizability to diverse decision-making contexts
- Computational expense of pairwise ranking presents practical deployment challenges

## Confidence
- High confidence: The core mechanism of RABBI and its strong correlation with allocation gaps in tested scenarios
- Medium confidence: RABBI's superiority over traditional metrics for model selection in fairness contexts
- Low confidence: Generalizability across diverse allocation scenarios, LLM architectures, and real-world deployment conditions

## Next Checks
1. Apply RABBI to allocation tasks outside tested domains (college admissions, loan approvals) with different demographic structures and resource constraints
2. Evaluate RABBI's sensitivity to scoring function variations, including different normalization techniques and handling of edge cases
3. Implement RABBI in a live allocation system with actual candidates and measure its predictive validity for selection outcomes in operational contexts