---
ver: rpa2
title: On Policy Evaluation Algorithms in Distributional Reinforcement Learning
arxiv_id: '2407.14175'
source_url: https://arxiv.org/abs/2407.14175
tags:
- algorithms
- which
- distributions
- approximation
- distributional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel class of algorithms for distributional
  dynamic programming in reinforcement learning, applicable to Markov decision processes
  with arbitrary probabilistic reward mechanisms, including heavy-tailed and unbounded
  distributions. The proposed algorithms approximate return distributions by iteratively
  applying the distributional Bellman operator with varying projection steps that
  map distributions to finitely supported ones.
---

# On Policy Evaluation Algorithms in Distributional Reinforcement Learning

## Quick Facts
- arXiv ID: 2407.14175
- Source URL: https://arxiv.org/abs/2407.14175
- Authors: Julian Gerstenberg; Ralph Neininger; Denis Spiegel
- Reference count: 10
- Key outcome: Novel distributional dynamic programming algorithms for arbitrary probabilistic reward mechanisms in MDPs, with error bounds and simulation experiments showing improved performance over Monte Carlo methods

## Executive Summary
This paper introduces a novel class of algorithms for distributional dynamic programming in reinforcement learning, applicable to Markov decision processes with arbitrary probabilistic reward mechanisms, including heavy-tailed and unbounded distributions. The proposed algorithms approximate return distributions by iteratively applying the distributional Bellman operator with varying projection steps that map distributions to finitely supported ones. The key innovation is allowing projection sizes to increase with each iteration, enabling efficient approximation of complex distributions. Theoretical analysis provides error bounds in Wasserstein and Kolmogorov-Smirnov distances, as well as density approximation bounds. The algorithms are shown to outperform Monte Carlo methods in simulation experiments, with particular effectiveness demonstrated for both light-tailed and heavy-tailed reward distributions.

## Method Summary
The paper proposes distributional dynamic programming algorithms that iteratively apply the distributional Bellman operator combined with parameterized projections that map arbitrary distributions to finitely supported ones. The projection size grows with each iteration, controlled by parameter θ, to avoid exponential blowup in complexity. Three parameter algorithms are introduced: plain parameter algorithm (PPA), adaptive plain parameter algorithm (ADP), and quantile-spline parameter algorithm (QSP). The framework provides theoretical error bounds in Wasserstein and Kolmogorov-Smirnov distances, and density approximation bounds for distributions with PDFs.

## Key Results
- Algorithms handle arbitrary reward distributions including continuous, unbounded, and heavy-tailed distributions
- Error bounds established in Wasserstein and Kolmogorov-Smirnov distances, plus density approximation bounds
- Simulation experiments demonstrate superior performance compared to Monte Carlo methods for both light-tailed and heavy-tailed reward distributions
- Framework extends distributional reinforcement learning beyond finitely supported rewards to practical finance and insurance applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm approximates return distributions by iteratively applying the distributional Bellman operator with projections whose size grows each iteration, enabling efficient handling of complex reward distributions.
- Mechanism: At each iteration, the distributional Bellman operator produces an intricate mixture of continuous and/or unbounded distributions. By projecting this mixture to a finitely supported distribution whose support size increases with each iteration, the algorithm avoids the exponential blowup in complexity that would occur if using a fixed-size projection. This controlled growth allows efficient approximation of complex distributions while maintaining computational tractability.
- Core assumption: The reward distributions have cumulative distribution functions (CDFs) and quantile functions (QFs) that can be evaluated efficiently.
- Evidence anchors:
  - [abstract] "The proposed distributional dynamic programming algorithms are suitable for underlying Markov decision processes (MDPs) having an arbitrary probabilistic reward mechanism, including continuous reward distributions with unbounded support being potentially heavy-tailed."
  - [section 2] "Our approach is to focus on a special class of (parameterised) projections Π( ·, ξ) in which Π( µ, ξ) depends on µ only by a controllable number of evaluations of its CDF."
  - [section 3] "The DDP algorithms we are proposing in Section 3 also operate by iterations of the DBO together with projection steps that map arbitrary distributions to finitely supported ones. To take up an idea of Devroye and Neininger (2002), where fixed-points of related recursive distributional equations were approximated, we allow varying projections, i.e. projections which depend on the update step. In particular, as Devroye and Neininger (2002), we allow the sizes of the finitely supported representations to increase with each iteration."
- Break condition: If reward distributions have CDFs or QFs that cannot be evaluated efficiently, the algorithm cannot be applied as designed.

### Mechanism 2
- Claim: The algorithm provides error bounds in Wasserstein and Kolmogorov-Smirnov distances, as well as density approximation bounds for return distributions with probability density functions.
- Mechanism: By using analysis metrics such as Wasserstein distances and leveraging the properties of the distributional Bellman operator, the algorithm can quantify the quality of the approximations. For distributions with PDFs, the algorithm can approximate these densities with uniform error bounds, providing a measure of how close the approximation is to the true distribution.
- Core assumption: The return distributions have PDFs with certain regularity properties (e.g., bounded or Hölder continuous).
- Evidence anchors:
  - [abstract] "Theoretical analysis provides error bounds in Wasserstein and Kolmogorov-Smirnov distances, as well as density approximation bounds."
  - [section 4.1] "Bounds on projection errors yield bounds on the accumulated projection error" and "By investigating the trade-off between time complexity and approximation quality, we make a case for constructing DDP algorithms such that PE( k, d) decays exponentially with some rate θ ∈ [γc, 1)"
  - [section 6.1] "Lemma 20 Let µ, ν ∈ P(R). (a) If Fν is ϱ-Hölder with constantM, then for every β ∈ (0, ∞) ks(µ, ν) ≤ a−a · M 1−a · wβ(µ, ν)a(1∨β) with a = ϱ ϱ + β"
- Break condition: If return distributions do not have PDFs or if the PDFs lack the required regularity properties, the density approximation bounds cannot be applied.

### Mechanism 3
- Claim: The algorithm is universally applicable to MDPs with arbitrary reward distributions, including heavy-tailed and unbounded distributions, and outperforms Monte Carlo methods in simulation experiments.
- Mechanism: By not requiring reward distributions to be finitely supported, the algorithm can handle a wide range of MDPs, including those with continuous, unbounded, and heavy-tailed reward distributions. The use of quantile-spline discretizations and the ability to bound quantiles of convolutions enable the algorithm to approximate the return distributions accurately. The simulation experiments demonstrate that the algorithm outperforms Monte Carlo methods in terms of approximation quality.
- Core assumption: The MDPs have finite state and action spaces, and the reward distributions can be evaluated efficiently.
- Evidence anchors:
  - [abstract] "The work extends the applicability of distributional reinforcement learning beyond previously studied cases with finitely supported rewards, making it suitable for practical applications in finance and insurance where such distributions commonly arise."
  - [section 5.4] "We compare three DDP algorithms to approximate (estimate) η∗: The two blackbox algorithms of Section 5 with parameter algorithms ADP (Section 5.2) and QSP (Section 5.3) and hyper-parameter choices as in (18) and (19) and initial approximation η(0) = [δ0 : ¯s ∈ S]. We let both algorithms run up to time tmax = 45 seconds (on a standard notebook) and return the last completely calculated approximation η(n) before that time."
  - [section 5.4] "Each procedure returns some approximation (estimation) η(n). We report ¯d(η(n), η∗) for d = ks, w1, ℓ2 in Figures 3 and 4. For mdp (i) it is seen that both ADP and QSP outperform (MC, MC2) significantly for any analysis metric."
- Break condition: If the MDP has a large state or action space, the algorithm may not be practical due to computational complexity.

## Foundational Learning

- Concept: Distributional Reinforcement Learning (DRL)
  - Why needed here: DRL is the foundation for understanding the problem of approximating return distributions in MDPs. The paper builds upon and extends existing DRL techniques to handle more general reward distributions.
  - Quick check question: What is the main difference between traditional RL and DRL in terms of the objective function?

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: MDPs provide the framework for modeling sequential decision-making problems with probabilistic transitions and rewards. Understanding the components of MDPs (states, actions, transition probabilities, reward mechanisms) is crucial for grasping the problem setting and the proposed algorithms.
  - Quick check question: In an MDP, what is the relationship between the state-action-state transition probabilities and the reward mechanism?

- Concept: Probability Metrics and Distribution Approximation
  - Why needed here: The paper uses various probability metrics (e.g., Wasserstein, Kolmogorov-Smirnov) to quantify the quality of the approximations. Understanding these metrics and their properties is essential for interpreting the theoretical results and the simulation experiments.
  - Quick check question: What is the key difference between the Wasserstein distance and the Kolmogorov-Smirnov distance in terms of what aspect of the distributions they measure?

## Architecture Onboarding

- Component map:
  - MDP Components: states (S), actions (A), transition probabilities (p), policy (π), discount factor (γ), reward mechanism (r)
  - Distributional Bellman Operator (T): maps return distributions to return distributions
  - Parameterized Projections (Π): map arbitrary distributions to finitely supported ones
  - Parameter Algorithms (A): determine the parameters for the projections
  - Analysis Metrics: Wasserstein distances (wβ), Kolmogorov-Smirnov distance (ks), Cramér distance (ℓ2)

- Critical path:
  1. Initialize approximation η(0) with finitely supported distributions
  2. For each iteration n:
     a. Calculate parameters ξ(n) using parameter algorithm A
     b. Apply distributional Bellman operator T to η(n-1)
     c. Project the result using Π with parameters ξ(n) to obtain η(n)
  3. Return the final approximation η(n) or use it for decision-making

- Design tradeoffs:
  - Fixed vs. varying projection sizes: Using a fixed projection size can lead to exponential blowup in complexity, while varying sizes (increasing with each iteration) maintain tractability but require careful parameter selection.
  - Projection quality vs. computational cost: Higher-quality projections (e.g., with more support points) provide better approximations but increase computational cost.
  - Universality vs. efficiency: The proposed algorithms are universally applicable but may not be the most efficient for specific classes of MDPs with known properties.

- Failure signatures:
  - Poor approximation quality: If the accumulated projection errors do not decay sufficiently fast, the final approximation may be far from the true return distribution.
  - High computational cost: If the MDP has a large state or action space, the algorithm may become computationally infeasible.
  - Inappropriate parameter choices: If the parameter algorithm A is not well-suited to the specific MDP, the projections may not capture the essential features of the return distributions.

- First 3 experiments:
  1. Implement the plain parameter algorithm (PPA) with a simple MDP (e.g., a 2-state chain with normal reward distributions) and verify that the approximation error decreases as the number of iterations increases.
  2. Compare the PPA with the adaptive plain parameter algorithm (ADP) on an MDP with heavy-tailed reward distributions (e.g., Cauchy distributions) and observe the difference in approximation quality.
  3. Implement the quantile-spline parameter algorithm (QSP) and test its performance on an MDP with a mix of light-tailed and heavy-tailed reward distributions, comparing the results with Monte Carlo estimation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal growth rate for the size function M(k) in practice, particularly for heavy-tailed distributions?
- Basis in paper: [explicit] The paper suggests θ ≈ γ+1/2 as a heuristic for the exponential growth rate, but acknowledges this is not rigorously proven and that the case θ = γc remains open.
- Why unresolved: The paper provides theoretical bounds and experimental evidence suggesting exponential growth is preferable to polynomial, but does not rigorously prove the optimal rate, particularly for heavy-tailed distributions where the heuristics may fail.
- What evidence would resolve it: Comprehensive empirical studies comparing different growth rates across a wide range of MDPs with varying reward distributions, particularly heavy-tailed ones, would provide evidence for the optimal choice. A theoretical proof establishing the optimal rate for different classes of distributions would be ideal.

### Open Question 2
- Question: How do the proposed DDP algorithms perform in high-dimensional state-action spaces?
- Basis in paper: [inferred] The paper explicitly states that the algorithms are "reasonably applicable for small finite state-action spaces" and suggests that for larger spaces, methods based on Markov chains or temporal difference learning may be more appropriate.
- Why unresolved: The paper focuses on the theoretical development and analysis of the algorithms for small state-action spaces, but does not provide empirical results or theoretical analysis for high-dimensional settings.
- What evidence would resolve it: Empirical studies applying the algorithms to MDPs with large state-action spaces, along with a theoretical analysis of the space-time complexity and approximation error in high-dimensional settings, would address this question.

### Open Question 3
- Question: Can the parameter algorithms (ADP and QSP) be extended to work with continuous state spaces?
- Basis in paper: [inferred] The paper assumes finite state and action spaces throughout, and the algorithms rely on explicit evaluation of cumulative distribution functions and quantile functions, which may not be feasible for continuous state spaces.
- Why unresolved: The paper does not discuss the extension of the algorithms to continuous state spaces, which is a common scenario in many practical applications.
- What evidence would resolve it: A theoretical analysis of the challenges and potential solutions for extending the algorithms to continuous state spaces, along with empirical results demonstrating the performance of such extensions, would provide insights into this question.

## Limitations
- The framework assumes efficient evaluation of CDFs and quantile functions, which may not be feasible for complex or unknown distributions
- Exponential growth in projection size creates a fundamental tradeoff between approximation quality and computational feasibility
- Simulation experiments are limited to relatively small MDPs with known reward distributions, with limited analysis of scalability

## Confidence
- High: Theoretical error bounds and algorithmic framework based on established probability theory
- Medium: Simulation results demonstrate effectiveness on specific test cases but don't exhaustively explore parameter space
- Low: Practical applicability to real-world problems with unknown or complex reward distributions remains to be validated

## Next Checks
1. Test the algorithms on larger MDPs with hundreds of states to assess computational scalability and identify practical limits on state space size
2. Implement the algorithms with approximate CDF/quantile evaluation methods for cases where exact evaluation is infeasible, and measure the impact on approximation quality
3. Apply the framework to a real-world finance or insurance problem with heavy-tailed reward distributions, comparing the results to domain-specific benchmarks or expert knowledge