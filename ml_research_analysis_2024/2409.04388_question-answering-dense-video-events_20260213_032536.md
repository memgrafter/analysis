---
ver: rpa2
title: Question-Answering Dense Video Events
arxiv_id: '2409.04388'
source_url: https://arxiv.org/abs/2409.04388
tags:
- video
- event
- events
- dense
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel task called question-answering dense
  video events, which challenges MLLMs to understand and reason about multiple events
  in long videos. To support this task, the authors create DeVE-QA, a dataset containing
  78K questions about 26K events across 10.6K videos.
---

# Question-Answering Dense Video Events

## Quick Facts
- arXiv ID: 2409.04388
- Source URL: https://arxiv.org/abs/2409.04388
- Authors: Hangyu Qin; Junbin Xiao; Angela Yao
- Reference count: 40
- Key outcome: DeVi achieves 4.8% and 2.1% increases in GQA accuracy on DeVE-QA and NExT-GQA, outperforming state-of-the-art MLLMs by 6.9% and 8.0%

## Executive Summary
This paper introduces DeVi, a training-free MLLM approach for question-answering dense video events that challenges models to understand and reason about multiple events in long videos. The method employs hierarchical dense event captioning at multiple temporal scales, temporal event memory for contextualization, and self-consistency checking to ensure grounded answers. DeVi demonstrates significant improvements over existing methods, achieving 4.8% and 2.1% increases in GQA accuracy on DeVE-QA and NExT-GQA datasets respectively.

## Method Summary
DeVi is a training-free MLLM approach that processes long videos through hierarchical dense event captioning, temporal event memory, and self-consistency checking. The method segments videos into 15s, 35s, and 65s clips for multi-scale event detection, then uses MLLMs to generate captions at each level. These captions are contextualized using an LLM that incorporates information from all other captions and questions. The system stores visual features and captions in a memory module for retrieval during question answering, with a self-consistency checking mechanism that verifies answers against visual evidence using cosine similarity.

## Key Results
- DeVi achieves 4.8% and 2.1% increases in GQA accuracy on DeVE-QA and NExT-GQA datasets respectively
- Outperforms state-of-the-art MLLMs by 6.9% and 8.0% compared to second-best models
- Shows strong performance on medium (60-120s) and long (120s+) videos
- Maintains effectiveness even when event density increases from 1/2 to dense configurations

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical dense event captioning
- **Claim:** Improves detection of events at multiple temporal scales
- **Mechanism:** Segments videos into short (15s), medium (35s), and long (65s) clips, using MLLMs to caption each separately
- **Core assumption:** Events vary in duration and complexity, requiring different temporal resolutions
- **Evidence anchors:** Abstract mentions "detect the dense events at multiple temporal scales"; section 4.2 describes applying MLLMs at multiple scales
- **Break condition:** If events are uniformly distributed or single resolution suffices

### Mechanism 2: Temporal event contextualizing and memorizing
- **Claim:** Captures long-range dependencies between events
- **Mechanism:** LLM refines captions by incorporating contextual information from all captions and questions, stores features in memory module
- **Core assumption:** Individual captions lack sufficient context for questions requiring understanding relationships between distant events
- **Evidence anchors:** Abstract mentions "capture long-term event dependency"; section 4.3 discusses lack of contextual information
- **Break condition:** If events are independent or memory module becomes impractical

### Mechanism 3: Self-consistency checking
- **Claim:** Ensures answers are grounded in correct video content
- **Mechanism:** Calculates cosine similarity between predicted answers and visual features; resubmits answers with dynamic verification if similarity is low
- **Core assumption:** MLLMs can generate plausible but ungrounded answers requiring verification against visual evidence
- **Evidence anchors:** Abstract mentions "anchor or rectify answers with regard to grounded event moments"; section 4.4 describes consistency checking
- **Break condition:** If threshold is set too high (unnecessary iterations) or too low (failing to catch hallucinations)

## Foundational Learning

- **Concept: Temporal segmentation and hierarchical processing**
  - Why needed here: Long videos contain events at multiple time scales that cannot be captured by uniform sampling
  - Quick check question: What are the three segment lengths used in DeVi and why are they different?

- **Concept: Cross-modal similarity for verification**
  - Why needed here: Ensures generated answers are supported by visual content rather than LLM hallucinations
  - Quick check question: How is the consistency score between an answer and video segment computed?

- **Concept: In-context learning with examples**
  - Why needed here: LLMs need examples to understand contextual caption refinement and synopsis generation
  - Quick check question: What type of examples are provided to the LLM before actual caption refinement?

## Architecture Onboarding

- **Component map:** Raw video → Visual encoder (CLIP) → Hierarchical segmentation → MLLM captioning → Memory (visual features + captions) → LLM reasoning with consistency checking → Answer + temporal span
- **Critical path:** Video → Hierarchical captioning → Memory storage → LLM reasoning with consistency checking → Answer
- **Design tradeoffs:** More hierarchy levels → better detection but higher computational cost; lower consistency threshold → fewer iterations but risk of ungrounded answers; larger LLM backbone → better reasoning but slower inference
- **Failure signatures:** Consistently low IoP scores → problems with temporal grounding; high QA accuracy but low GQA accuracy → good reasoning but poor grounding; long inference times → memory or consistency checking inefficiencies
- **First 3 experiments:** 1) Compare single-level vs. three-level hierarchical captioning on DeVE-QA subset; 2) Test different consistency thresholds (0.3, 0.4, 0.5) to find optimal balance; 3) Evaluate impact of removing temporal contextualization on QA accuracy for long vs. short videos

## Open Questions the Paper Calls Out

### Open Question 1: Hierarchical layer scalability
- **Question:** How does performance scale with different numbers of hierarchical layers and segment lengths?
- **Basis in paper:** Paper identifies optimal parameters (15s, 35s, 65s) but doesn't systematically analyze performance across different configurations
- **Why unresolved:** Paper identifies optimal setup for their specific data but lacks comprehensive ablation study
- **What evidence would resolve it:** Systematic ablation study varying number of layers (2-5) and segment lengths (5s-60s) across multiple datasets

### Open Question 2: Computational efficiency of self-consistency checking
- **Question:** What is the impact on computational efficiency and how can it be optimized for real-time applications?
- **Basis in paper:** Paper notes self-consistency checking accounts for nearly half of runtime but doesn't explore optimization strategies
- **Why unresolved:** Identifies computational overhead but doesn't test optimization strategies
- **What evidence would resolve it:** Experiments comparing variants with different iteration limits, parallel processing, and dynamic thresholds on same hardware

### Open Question 3: Cross-domain generalization
- **Question:** How well does DeVi generalize to domains outside ActivityNet-Caption videos?
- **Basis in paper:** Paper demonstrates effectiveness on curated datasets but doesn't evaluate on surveillance footage, sports broadcasts, or user-generated content
- **Why unresolved:** Shows performance on specific video types but doesn't address generalization to different visual characteristics
- **What evidence would resolve it:** Cross-domain evaluation on surveillance footage, sports broadcasts, and user-generated content with quantitative comparisons

## Limitations
- The specific temporal scales (15s, 35s, 65s) were chosen without comprehensive ablation studies to verify optimality
- Cross-modal similarity verification may not capture semantic alignment perfectly for abstract concepts
- Dataset construction methodology lacks discussion of annotation verification and potential bias
- Self-consistency checking adds significant computational overhead that may limit real-time applications

## Confidence

**High Confidence:**
- Hierarchical dense event captioning improves event detection compared to single-scale approaches
- Temporal event memory module helps capture contextual relationships between events
- Self-consistency checking mechanism reduces hallucinations compared to approaches without verification

**Medium Confidence:**
- Specific improvement percentages (4.8% and 2.1%) are reliable but may vary with different evaluation settings
- Superiority over existing MLLMs (6.9% and 8.0% improvements) holds for tested models but may not generalize to all architectures

## Next Checks
1. **Ablation study on temporal scales:** Test hierarchical captioning with different segment lengths (e.g., 10s/25s/50s or 20s/40s/80s) to verify current choices are optimal
2. **Cross-dataset generalization:** Evaluate DeVi on NExT-GQA and other video question-answering datasets with different characteristics to confirm improvements aren't dataset-specific
3. **Memory module efficiency:** Analyze computational overhead and memory requirements as video length increases, particularly for videos longer than those in DeVE-QA dataset, to identify scalability limitations