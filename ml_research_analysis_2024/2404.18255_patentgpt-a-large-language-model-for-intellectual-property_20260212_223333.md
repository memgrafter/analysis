---
ver: rpa2
title: 'PatentGPT: A Large Language Model for Intellectual Property'
arxiv_id: '2404.18255'
source_url: https://arxiv.org/abs/2404.18255
tags:
- patent
- arxiv
- data
- patentgpt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PatentGPT, a series of large language models
  trained on 240B tokens of intellectual property (IP)-oriented data to address the
  lack of specialized models in the IP domain. The training pipeline includes data
  preprocessing (filtering, deduplication, rewriting, synthesis), multilingual pretraining,
  two-stage pretraining, and alignment via SFT and RLHF.
---

# PatentGPT: A Large Language Model for Intellectual Property

## Quick Facts
- arXiv ID: 2404.18255
- Source URL: https://arxiv.org/abs/2404.18255
- Reference count: 40
- PatentGPT models outperform GPT-4 on IP benchmarks and match human experts on China Patent Agent Qualification Examination

## Executive Summary
This work introduces PatentGPT, a series of large language models specifically trained for intellectual property tasks. The models are pretrained on 240B tokens of IP-oriented data and demonstrate superior performance on IP benchmarks compared to general-purpose LLMs like GPT-4. PatentGPT-1.0-MoE achieves a score of 65 on the 2019 China Patent Agent Qualification Examination, matching human expert performance. The work validates the effectiveness of domain-specific pretraining and the cost-efficiency of SMoE architecture for long-text IP applications.

## Method Summary
The PatentGPT models were developed through a comprehensive training pipeline including data preprocessing (filtering, deduplication, rewriting, synthesis), multilingual pretraining, and two-stage pretraining. The first stage consumes 226B tokens with shorter context, while the second stage extends context to 16k and uses 20B additional tokens. Supervised fine-tuning with 43k instructions and reinforcement learning from human feedback (100k preference data) were applied for alignment. The PatentGPT-1.0-MoE variant uses a sparse mixture-of-experts architecture with shared attention heads to reduce memory consumption for long documents.

## Key Results
- PatentGPT models outperform GPT-4 on the MOZIP IP benchmark
- PatentGPT-1.0-MoE achieves a score of 65 on the 2019 China Patent Agent Qualification Examination, matching human expert performance
- SMoE architecture demonstrates better cost-performance ratio on long-text IP tasks while maintaining GPT-4-level performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pretraining on 240B tokens of IP-oriented data improves model performance on IP tasks beyond general LLMs
- Mechanism: Pretraining on curated IP data (patents, file wrappers, litigation, etc.) injects specialized knowledge that general pretraining lacks, enabling the model to understand legal concepts, technical terminology, and domain-specific reasoning patterns
- Core assumption: The quality and diversity of the IP dataset is sufficient to teach the model specialized IP knowledge
- Evidence anchors:
  - [abstract]: "By evaluating them on the open-source IP-oriented benchmark MOZIP, our domain-specific LLMs outperforms GPT-4"
  - [section]: "Pretraining on over 240 billion tokens of IP-oriented data, our PatentGPT models exhibit performance in the IP domain that surpasses that of GPT-4"
- Break condition: If the IP dataset contains significant noise, bias, or lacks coverage of key IP subdomains, the pretraining will not effectively teach the specialized knowledge needed

### Mechanism 2
- Claim: The two-stage pretraining process with long-context extension improves the model's ability to handle long patent documents and complex IP tasks
- Mechanism: Stage 1 injects basic IP knowledge with shorter context, then Stage 2 extends context length and focuses on specialized IP tasks like patent drafting, comparison, and classification using longer documents
- Core assumption: The model can effectively learn to handle longer contexts without catastrophic forgetting of Stage 1 knowledge
- Evidence anchors:
  - [abstract]: "PatentGPT-1.0-MoE model, with SMoE architecture, demonstrates better cost-performance on long-text tasks while maintaining GPT-4-level performance"
  - [section]: "Long context extending: As expected to reduce computation resources, we trained PatentGPT-0.5 and PatentGPT-1.0-MoE in a shorter context length in stage 1, and then extended them to 16k during stage 2"
- Break condition: If the context extension causes significant performance degradation or the model fails to generalize from shorter to longer contexts

### Mechanism 3
- Claim: The SMoE architecture provides better cost-performance for long-text IP tasks compared to dense models
- Mechanism: SMoE selectively activates experts for different inputs, reducing computation while maintaining performance, and sharing attention heads reduces KV cache memory for long sequences
- Core assumption: The expert routing and shared attention mechanisms work effectively for the types of long patent documents encountered in IP tasks
- Evidence anchors:
  - [abstract]: "the PatentGPT model, which utilizes the SMoE architecture, achieves performance comparable to that of GPT-4 in the IP domain and demonstrates a better cost-performance ratio on long-text tasks"
  - [section]: "SMoE LLMs can significantly reduce the memory consumption of the key-value Cache (KV cache) when processing long texts"
- Break condition: If expert routing becomes imbalanced (some experts rarely used) or the shared attention degrades performance on specialized IP reasoning

## Foundational Learning

- Concept: Tokenization and vocabulary design for multilingual IP text
  - Why needed here: IP documents contain specialized terminology in both English and Chinese that require appropriate tokenization to compress context efficiently and maintain semantic meaning
  - Quick check question: Why did the authors merge a new tokenizer with LLaMA2's tokenizer rather than using either one alone?

- Concept: Mixture-of-Experts (MoE) architecture fundamentals
  - Why needed here: Understanding how MoE selectively activates different "expert" subnetworks allows optimization of computation while maintaining performance on specialized tasks
  - Quick check question: How does sharing attention heads between experts in SMoE reduce memory consumption for long sequences?

- Concept: Reinforcement Learning from Human Feedback (RLHF) alignment
  - Why needed here: Aligning the model with human preferences ensures the IP-specific outputs are not only accurate but also match what IP professionals expect in terms of format, style, and completeness
  - Quick check question: What role does the reward model play in the PPO-based RLHF training process described?

## Architecture Onboarding

- Component map: Tokenizer (55,296 vocab) -> Base Model (LLaMA2/Mixtral) -> Two-Stage Pretraining (Stage 1: 226B tokens, 4Kâ†’2K context; Stage 2: 20B tokens, 16K context) -> SFT Alignment (30k general + 13k IP instructions) -> RLHF Alignment (100k human preference data) -> Evaluation (PatentBench, MOZIP, MMLU, C-Eval)
- Critical path: Data preprocessing -> Pretraining (2 stages) -> SFT -> RLHF -> Evaluation
- Design tradeoffs: Dense vs. MoE architecture (performance vs. inference efficiency), multilingual tokenization (coverage vs. complexity), two-stage training (computational cost vs. specialization)
- Failure signatures: Poor performance on IPBench suggests issues with pretraining data quality or alignment; high inference latency suggests architectural inefficiency; low scores on general benchmarks suggest over-specialization
- First 3 experiments:
  1. Evaluate model performance on a small subset of PatentBench tasks to verify the basic IP capabilities are working
  2. Test context length handling by feeding progressively longer patent documents and measuring performance degradation
  3. Compare inference latency and memory usage of dense vs. MoE variants on long-text patent comparison tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multilingual alignment process specifically impact the model's performance in patent-related tasks across different languages?
- Basis in paper: [explicit] The paper mentions that the new tokenizer was created to improve token compression rate for text in Chinese and the IP domain, and that the multilingual alignment process ensures enhancement of Chinese contextual understanding and generation capabilities
- Why unresolved: While the paper states that the multilingual alignment process was implemented, it does not provide quantitative data on how this specifically impacts performance on patent-related tasks across different languages
- What evidence would resolve it: Comparative performance metrics of the PatentGPT models on patent-related tasks in English versus Chinese, both with and without the multilingual alignment process, would clarify the impact of this feature

### Open Question 2
- Question: What is the long-term impact of the two-stage pretraining process on the model's ability to handle diverse and complex IP-related tasks?
- Basis in paper: [explicit] The paper describes a two-stage pretraining process where the first stage involves consuming 226B tokens, and the second stage involves 20B tokens, with different proportions of data categories used in each stage to inject IP-oriented knowledge and develop specific abilities
- Why unresolved: The paper does not provide longitudinal studies or long-term performance data to show how the two-stage pretraining process affects the model's adaptability and proficiency in handling increasingly complex and diverse IP-related tasks over time
- What evidence would resolve it: Longitudinal studies tracking the performance of the PatentGPT models on a wide range of IP-related tasks over an extended period, comparing models trained with and without the two-stage process, would provide insights into the long-term benefits or drawbacks of this approach

### Open Question 3
- Question: How does the SMoE architecture contribute to cost-performance ratio improvements specifically in long-text IP tasks compared to other sparse architectures?
- Basis in paper: [explicit] The paper mentions that the PatentGPT model with SMoE architecture shows a better cost-performance ratio on long-text tasks compared to dense LLMs and highlights the memory consumption benefits of SMoE for long-document applications
- Why unresolved: While the paper indicates that SMoE architecture is beneficial for long-text tasks, it does not provide a detailed comparison with other sparse architectures or quantify the specific improvements in cost-performance ratio
- What evidence would resolve it: Detailed comparative analysis of the PatentGPT-1.0-MoE model against other sparse architectures (e.g., Switch Transformers, GShard) in terms of memory usage, inference speed, and cost-performance ratio on long-text IP tasks would clarify the unique contributions of the SMoE architecture

## Limitations

- Dataset composition and quality remain unclear, making it difficult to assess whether pretraining data adequately represents real-world IP tasks
- Evaluation methodology lacks transparency in test conditions and metrics, particularly for human expert comparisons
- The exact mechanisms by which IP data translates to superior performance are not fully validated through ablation studies

## Confidence

- **High Confidence**: The technical architecture and training pipeline (two-stage pretraining, SFT, RLHF) are well-documented and represent standard approaches in LLM development
- **Medium Confidence**: The reported benchmark results are promising but lack detailed methodological transparency
- **Low Confidence**: The specific contributions of different data sources and the long-term generalization capabilities remain uncertain

## Next Checks

1. **Ablation Study on Data Sources**: Systematically remove different categories of IP data from pretraining and measure performance degradation on MOZIP tasks to identify which data types contribute most to the performance gains

2. **Human Evaluation Protocol**: Conduct blind evaluations where IP professionals assess outputs from PatentGPT and GPT-4 on identical tasks, rating accuracy, completeness, and format adherence to establish whether the performance differences are practically meaningful

3. **Generalization Stress Test**: Evaluate PatentGPT models on out-of-distribution IP tasks not represented in the training data (e.g., emerging IP domains like AI-generated content rights) to assess whether the model has learned transferable IP reasoning capabilities or merely memorized training patterns