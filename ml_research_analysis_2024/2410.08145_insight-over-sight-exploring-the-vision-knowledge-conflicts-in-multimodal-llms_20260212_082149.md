---
ver: rpa2
title: 'Insight Over Sight: Exploring the Vision-Knowledge Conflicts in Multimodal
  LLMs'
arxiv_id: '2410.08145'
source_url: https://arxiv.org/abs/2410.08145
tags:
- knowledge
- mllms
- visual
- framework
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a diagnostic framework to systematically\
  \ evaluate commonsense-level vision-knowledge conflicts in Multimodal Large Language\
  \ Models (MLLMs), where visual inputs contradict the model\u2019s parametric commonsense\
  \ knowledge. An automated pipeline generates counter-commonsense inputs by extracting\
  \ knowledge components from image captions, constructing conflicting queries, and\
  \ synthesizing images and QA pairs, augmented by human-in-the-loop quality control."
---

# Insight Over Sight: Exploring the Vision-Knowledge Conflicts in Multimodal LLMs

## Quick Facts
- arXiv ID: 2410.08145
- Source URL: https://arxiv.org/abs/2410.08145
- Reference count: 40
- One-line primary result: MLLMs over-rely on parametric knowledge in about 20% of cases when visual inputs contradict commonsense knowledge

## Executive Summary
This paper introduces a systematic framework to diagnose and evaluate vision-knowledge conflicts in Multimodal Large Language Models (MLLMs), where visual inputs contradict the model's parametric commonsense knowledge. The authors develop an automated pipeline that generates counter-commonsense inputs by extracting knowledge components from image captions, constructing conflicting queries, and synthesizing images and QA pairs with human-in-the-loop quality control. The resulting CONFLICT VIS benchmark contains 374 images and 1,122 QA pairs spanning two conflict targets and three question types. Experiments with nine representative MLLMs reveal that models over-rely on parametric knowledge for approximately 20% of queries, especially for Yes-No and action-related questions, with memorization ratios up to 43.6% for Claude-3.5-Sonnet.

## Method Summary
The authors developed an automated framework augmented with human-in-the-loop quality control to generate counter-commonsense inputs that evaluate vision-knowledge conflicts in MLLMs. The framework extracts Subject, Action, and Place phrases from image captions, computes Normalized Pointwise Mutual Information (NPMI) scores to identify strongly co-occurring context pairs, and constructs counter-commonsense triplets by selecting targets with low NPMI scores. This process generates corresponding images and QA pairs, validated by human experts. The resulting CONFLICT VIS benchmark was used to evaluate nine representative MLLMs across two conflict targets (counter-commonsense actions and places) and three question types (Yes-No, Multiple-Choice, and Open-Ended), measuring accuracy and Memorization Ratio (MR) to quantify reliance on parametric knowledge.

## Key Results
- MLLMs over-rely on parametric knowledge for approximately 20% of queries when visual inputs contradict commonsense knowledge
- Memorization ratios reach up to 43.6% for Claude-3.5-Sonnet, with Yes-No and action-related questions showing the highest over-reliance
- Existing improvement methods and the proposed "Focus-on-Vision" prompting improve performance but cannot entirely resolve the vision-knowledge conflict

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The automated framework reliably generates counter-commonsense images by constructing triplets of low co-occurring concepts.
- Mechanism: The framework extracts Subject, Action, and Place phrases from a caption corpus, computes NPMI scores to identify strongly co-occurring context pairs, and then selects targets with low NPMI scores to construct counter-commonsense triplets.
- Core assumption: High NPMI scores indicate strong co-occurrence, and low NPMI scores between a target and context pair indicate a counter-commonsense relationship.
- Evidence anchors:
  - [abstract]: "To study this issue, we introduce an automated framework, augmented with human-in-the-loop quality control, to generate inputs designed to simulate and evaluate these conflicts in MLLMs."
  - [section]: "Our framework includes three stages, as depicted in Figure 2... Our framework first groups context components by the target category, e.g., ContextAction consists of (Subject, Place) pairs. These groupings help to develop the focus of the questions in the next section. We omit ContextSubject to prevent ambiguity and mitigate potential ethical concerns related to subject identity. Next, our framework enumerate all the context combinations within the each group and computes their Normalized Pointwise Mutual Information (NPMI) scores."
  - [corpus]: Weak - the paper relies on NPMI as a proxy for co-occurrence, but does not provide empirical validation that low NPMI always indicates a counter-commonsense relationship.

### Mechanism 2
- Claim: MLLMs exhibit a tendency to over-rely on parametric knowledge when confronted with vision-knowledge conflicts, especially for Yes-No and action-related questions.
- Mechanism: The framework evaluates MLLMs on the CONFLICT VIS benchmark, comparing their predictions with and without the image input. The Memorization Ratio (MR) metric quantifies the extent to which the model adheres to its parametric knowledge in the presence of conflicting information.
- Core assumption: The model's predictions without the image input reflect its parametric knowledge, and a higher MR indicates greater reliance on this knowledge.
- Evidence anchors:
  - [abstract]: "Our results indicate an evident over-reliance on parametric knowledge for approximately 20% of all queries, especially among Yes-No and action-related problems."
  - [section]: "Figure 5 illustrates the MR across different question types. Notably, the MR values for Yes-No questions are significantly higher than the other two question types across all MLLMs, indicating that MLLMs tend to overly rely on parametric knowledge when answering Yes-No questions."
  - [corpus]: Weak - the paper does not provide evidence that the model's predictions without the image input accurately reflect its parametric knowledge.

### Mechanism 3
- Claim: The Focus-on-Vision (FoV) prompting strategy improves MLLM performance on vision-knowledge conflicts by explicitly instructing the model to prioritize visual information.
- Mechanism: The FoV prompting technique is integrated into the current MLLMs using the template: [textual query] Please focus on the visual information.
- Core assumption: Explicitly instructing the model to prioritize visual information will lead to improved performance on vision-knowledge conflicts.
- Evidence anchors:
  - [abstract]: "Drawing on these observations, we assess several existing improvement methods to enhance the impact of visual context in answer generation. Interestingly, although Chain-of-Thought prompting improves the reasoning abilities, it guides MLLMs to utilize parametric knowledge more during rationalization, often resulting in contradictory conclusions or refusals. In response, we propose 'Focus-on-Vision' (FoV) prompting to directly instruct MLLMs to prioritize visual information, which markedly improves the model's performance."
  - [section]: "Table 3 presents the results of the evaluated improvement methods... In contrast, our Focus-on-Vision (FoV) prompting mitigates this issue by explicitly directing the model's attention to the visual input and consistently boosts model performance."
  - [corpus]: Weak - the paper does not provide evidence that FoV prompting consistently improves performance across all MLLMs or all types of vision-knowledge conflicts.

## Foundational Learning

- Concept: Normalized Pointwise Mutual Information (NPMI)
  - Why needed here: NPMI is used to identify strongly co-occurring context pairs and targets with low co-occurrence, which are essential for constructing counter-commonsense triplets.
  - Quick check question: How does NPMI differ from raw Pointwise Mutual Information (PMI), and why is this difference important for the framework?

- Concept: Memorization Ratio (MR)
  - Why needed here: MR is used to quantify the extent to which MLLMs rely on parametric knowledge when confronted with vision-knowledge conflicts.
  - Quick check question: How is MR calculated, and what does a high MR value indicate about the model's behavior?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT prompting is used as a baseline improvement method to evaluate the effectiveness of FoV prompting.
  - Quick check question: How does CoT prompting work, and why might it lead to increased reliance on parametric knowledge during rationalization?

## Architecture Onboarding

- Component map: Knowledge Component Extraction -> Counter-commonsense Query Construction -> Image and QA Pair Synthesis -> Human-in-the-loop Quality Control
- Critical path: The framework generates counter-commonsense inputs by extracting knowledge components, constructing counter-commonsense queries, and synthesizing images and QA pairs, with human-in-the-loop quality control at each stage.
- Design tradeoffs: The framework prioritizes automation and scalability over manual curation, which may lead to some errors in the generated data.
- Failure signatures: Errors in the generated data may arise from inaccurate extraction of knowledge components, incorrect identification of co-occurrence patterns, or poor quality of generated images.
- First 3 experiments:
  1. Evaluate the accuracy of the knowledge component extraction module by comparing the extracted phrases to a manually annotated dataset.
  2. Assess the effectiveness of the NPMI-based co-occurrence identification by analyzing the correlation between NPMI scores and human judgments of co-occurrence.
  3. Test the quality of the generated images by conducting a human evaluation study to assess their alignment with the text prompts and overall quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the vision-knowledge conflict be entirely eliminated through improved prompting strategies?
- Basis in paper: [explicit] The paper evaluates several improvement methods, including VCD, PAI, VR, and the proposed FoV prompting, but concludes that "none of the approaches can entirely resolve the vision-knowledge conflict, especially for open-source models."
- Why unresolved: While FoV prompting improves performance, it only reduces the error rate rather than eliminating the conflict entirely. The paper suggests that the problem remains significant, particularly for open-source models.
- What evidence would resolve it: Comparative experiments testing whether newer or more sophisticated prompting strategies (e.g., dynamic prompting, adaptive context weighting) can achieve near-zero memorization ratios across all question types and conflict targets.

### Open Question 2
- Question: What is the fundamental cause of MLLMs' over-reliance on parametric knowledge when visual information contradicts it?
- Basis in paper: [inferred] The paper analyzes failure cases and finds that models "underutilize visual information" and "assign greater weight to the textual input than to the visual context." It hypothesizes that this may stem from training data bias, where models are predominantly aligned with image-text pairs conforming to commonsense expectations.
- Why unresolved: The analysis is limited to input-output relevancy and attention patterns. The paper does not conduct controlled experiments comparing models trained with and without counter-commonsense datasets to investigate behavior differences.
- What evidence would resolve it: Controlled experiments training models with curated counter-commonsense datasets versus standard datasets, measuring performance differences on CONFLICT VIS and analyzing whether exposure to conflicting scenarios reduces knowledge over-reliance.

### Open Question 3
- Question: How does the severity of vision-knowledge conflicts vary across different model architectures and training paradigms?
- Basis in paper: [explicit] The paper benchmarks nine MLLMs from five model families (LLaVA, BLIP-2, Qwen-VL, GPT-4o, Claude-3.5-Sonnet) with varying architectures and parameter sizes, finding that commercial models perform better than open-source counterparts, particularly on open-ended questions.
- Why unresolved: While the paper shows performance differences, it does not systematically analyze how specific architectural choices (e.g., frozen image encoders, instruction tuning, multimodal reasoning capabilities) or training paradigms (e.g., supervised fine-tuning vs. alignment) affect conflict resolution.
- What evidence would resolve it: Detailed ablation studies comparing models with different architectural components and training approaches, correlating these factors with performance metrics on CONFLICT VIS to identify which design choices most effectively mitigate vision-knowledge conflicts.

## Limitations

- The benchmark covers only two conflict targets (actions and places) and three question types, potentially missing other important dimensions of vision-knowledge conflicts.
- The evaluation focuses on commonsense-level conflicts, which may not generalize to more complex reasoning scenarios.
- The paper does not explore whether the over-reliance on parametric knowledge is due to the models' training data composition or architectural limitations.

## Confidence

- High: The empirical finding that MLLMs over-rely on parametric knowledge for approximately 20% of queries is well-supported by the benchmark results across nine different models.
- Medium: The claim that Yes-No and action-related questions show the highest memorization ratios is supported by the data but could benefit from additional analysis of why these question types are particularly susceptible.
- Medium: The effectiveness of Focus-on-Vision prompting is demonstrated, but the paper does not provide evidence that this approach generalizes across all MLLMs or all types of vision-knowledge conflicts.

## Next Checks

1. **Validate NPMI correlation**: Conduct a human evaluation study to assess whether low NPMI scores between knowledge components actually correspond to counter-commonsense relationships, and quantify the accuracy of the automated framework's triplet generation.

2. **Test cross-modal attention patterns**: Use input-output relevancy scores and attention heatmaps to systematically analyze whether image tokens are consistently less attended than textual tokens across all evaluated MLLMs, controlling for model architecture differences.

3. **Evaluate generalization**: Test the CONFLICT VIS benchmark and improvement methods on additional MLLM architectures not included in the original study (e.g., Gemini, Mistral) to assess whether the observed patterns of over-reliance on parametric knowledge are consistent across model families.