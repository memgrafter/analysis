---
ver: rpa2
title: Dynamic Q&A of Clinical Documents with Large Language Models
arxiv_id: '2401.10733'
source_url: https://arxiv.org/abs/2401.10733
tags:
- language
- information
- clinical
- retrieval
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of large language models (LLMs) to
  develop a conversational interface for querying clinical notes. The approach leverages
  Retrieval Augmented Generation (RAG) to enable users to ask questions in natural
  language and receive answers extracted from clinical documents.
---

# Dynamic Q&A of Clinical Documents with Large Language Models

## Quick Facts
- arXiv ID: 2401.10733
- Source URL: https://arxiv.org/abs/2401.10733
- Authors: Ran Elgedawy; Ioana Danciu; Maria Mahbub; Sudarshan Srinivasan
- Reference count: 40
- Primary result: Developed conversational interface for querying clinical notes using RAG with LLMs; achieved 80% accuracy with Wizard Vicuna and sentence transformers; 48x latency reduction via quantization

## Executive Summary
This paper explores using large language models (LLMs) with Retrieval Augmented Generation (RAG) to enable natural language querying of clinical notes. The approach retrieves relevant document chunks using embeddings, then generates answers using an LLM. Experiments with different embedding models and LLMs showed Wizard Vicuna paired with sentence transformers achieved highest accuracy (80%). Model quantization improved latency by approximately 48 times. While promising, challenges remain around hallucinations and need for broader evaluation across diverse clinical cases.

## Method Summary
The method uses Langchain framework to implement RAG for clinical document querying. Clinical notes from MIMIC-IV dataset are preprocessed, split into chunks, and embedded using various models. A vector database stores these embeddings for retrieval. When users ask questions, the system retrieves relevant chunks using cosine similarity, then passes query plus context to an LLM for answer generation. The study evaluated combinations of 6 embedding models with 10 LLMs, comparing RAG against fine-tuning approaches. Model optimization via quantization was applied to reduce inference latency.

## Key Results
- Wizard Vicuna with sentence transformers achieved highest accuracy (80%) among tested combinations
- Quantization improved inference latency by approximately 48 times while maintaining accuracy
- RAG approach outperformed fine-tuning for this clinical QA task based on limited evaluation
- System successfully enables conversational querying of clinical notes in natural language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG enables retrieval of specific clinical facts without requiring expensive fine-tuning.
- Mechanism: RAG first retrieves relevant chunks from a vector store indexed by embeddings, then passes those chunks plus the query to an LLM for final answer generation.
- Core assumption: Clinical notes are long and contain discrete facts; retrieval can isolate relevant context for accurate answering.
- Evidence anchors:
  - [abstract] "RAG to enable users to ask questions in natural language and receive answers extracted from clinical documents"
  - [section] "This approach combines the benefits of both retrieval-based and generation-based methods, allowing for more contextually relevant and informative responses in conversational systems"
  - [corpus] Weak evidence—related papers focus on clinical note processing but not on retrieval-augmented question answering.
- Break condition: Retrieval fails to return relevant chunks (e.g., embeddings poorly capture semantic meaning, chunk boundaries break context).

### Mechanism 2
- Claim: Sentence transformers capture semantic similarity better than TF-IDF for clinical text.
- Mechanism: Embedding models map both queries and clinical note chunks into vector space where cosine similarity retrieves semantically related content.
- Core assumption: Clinical language has domain-specific semantics that traditional keyword-based retrieval misses.
- Evidence anchors:
  - [section] "Embedding models (EMs) serve as powerful tools in natural language processing by representing words or phrases in a continuous vector space where the geometric distances between vectors capture semantic relationships between words"
  - [section] "We evaluated combinations of semantic embedding models... on their ability to encode both queries and documents for optimal information retrieval"
  - [corpus] No direct support—related papers discuss NLP for clinical notes but not embedding model comparisons.
- Break condition: Embedding model underfits or overfits to the training domain, reducing retrieval accuracy.

### Mechanism 3
- Claim: Model quantization preserves accuracy while drastically reducing inference latency.
- Mechanism: Post-training quantization reduces numerical precision (e.g., 32-bit → 8-bit) of LLM weights, lowering memory usage and speeding computation.
- Core assumption: Quantized models retain semantic understanding needed for clinical QA while being deployable on consumer GPUs.
- Evidence anchors:
  - [section] "We used model optimization techniques like weight quantization to reduce latency by approximately 48 times"
  - [section] "We achieved significant reductions in latency, paving the way for more agile and deployable solutions"
  - [corpus] No evidence—related works do not discuss quantization for clinical LLMs.
- Break condition: Quantization degrades answer quality beyond acceptable thresholds (e.g., hallucinations increase).

## Foundational Learning

- Concept: Vector embeddings and cosine similarity
  - Why needed here: RAG relies on embeddings to match queries with relevant note chunks; understanding this underpins the retrieval step.
  - Quick check question: If two clinical notes have embeddings [0.8, 0.1] and [0.9, 0.05], which is more similar to query embedding [0.85, 0.02]?

- Concept: Large language model inference and context limits
  - Why needed here: LLMs must process retrieved context plus query within fixed token limits; understanding context window constraints is essential for chunking strategy.
  - Quick check question: If an LLM has a 4096-token context window and each retrieved chunk is 512 tokens, what is the maximum number of chunks that can be included with a 200-token query?

- Concept: Model quantization and its trade-offs
  - Why needed here: Quantization is used to reduce latency and memory footprint; understanding its impact on accuracy is critical for deployment decisions.
  - Quick check question: If a model's 32-bit weights are quantized to 8-bit, by what factor does memory usage decrease (ignoring overhead)?

## Architecture Onboarding

- Component map: Document Store -> Retriever -> LLM -> Output
- Critical path: Ingestion → Embedding → Vector Store → Retrieval → LLM → Output
- Design tradeoffs:
  - Chunk size vs. context preservation (smaller = more precise but risk losing context)
  - Embedding model choice (semantic quality vs. speed)
  - Quantization level (speed vs. accuracy)
  - LLM size (accuracy vs. latency and cost)
- Failure signatures:
  - Low recall: retrieval returns irrelevant or too few chunks
  - High latency: model too large or unoptimized
  - Hallucinations: LLM fabricates facts not in retrieved context
  - Memory errors: chunking too large for GPU context window
- First 3 experiments:
  1. Swap sentence transformers for a simpler embedding model (e.g., all-MiniLM-L6-v2) and measure retrieval accuracy.
  2. Compare inference latency with and without quantization on a fixed query set.
  3. Vary chunk overlap percentage (e.g., 0%, 10%, 20%) and observe impact on answer correctness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between retrieval augmented generation (RAG) and domain-specific fine-tuning for clinical question answering tasks?
- Basis in paper: [explicit] The authors explicitly state that "RAG-based approach confirms the latter as the most robust and effective method for tackling the intricate challenges of information extraction using LLMs in our particular application" and found fine-tuning suboptimal
- Why unresolved: The paper only tested one fine-tuning approach (QLoRA) on a relatively small dataset (1,250 records). Other fine-tuning methods or larger datasets might yield different results.
- What evidence would resolve it: Systematic comparison of multiple fine-tuning approaches (different algorithms, hyperparameters, dataset sizes) against RAG across various clinical question answering benchmarks.

### Open Question 2
- Question: How do different embedding model choices affect the accuracy and efficiency of clinical document retrieval in RAG systems?
- Basis in paper: [explicit] The authors tested 6 different embedding models paired with 10 language models and found that "Wizard Vicuna achieved the top accuracy (80%) when paired with the sentence transformers embedding model"
- Why unresolved: The study only tested a limited set of embedding models. Other recent embedding models might perform differently, and the optimal embedding choice may vary by clinical document type or query complexity.
- What evidence would resolve it: Comprehensive benchmarking of a wider range of embedding models across diverse clinical document types and query scenarios, measuring both accuracy and computational efficiency.

### Open Question 3
- Question: What is the impact of quantization on model accuracy versus inference speed for large clinical language models?
- Basis in paper: [explicit] The authors found that quantization improved speed by approximately 48x while maintaining 100% accuracy for their specific use case
- Why unresolved: The study only tested one quantization approach on one model. Different quantization strategies or models might show different accuracy-speed tradeoffs.
- What evidence would resolve it: Systematic evaluation of multiple quantization techniques across various model architectures and sizes, measuring the full spectrum of accuracy degradation versus speed improvements.

### Open Question 4
- Question: How can model hallucinations in clinical question answering systems be reliably detected and mitigated?
- Basis in paper: [explicit] The authors note that "challenges such as model hallucinations...remain" and observed hallucination in their fine-tuned model
- Why unresolved: The paper only briefly mentions hallucinations as a challenge without testing specific detection or mitigation strategies.
- What evidence would resolve it: Development and validation of hallucination detection methods (e.g., confidence scoring, fact-checking mechanisms) and mitigation techniques (e.g., prompt engineering, retrieval quality filtering) for clinical RAG systems.

## Limitations

- Small evaluation set (only 5 question-answer pairs) limits generalizability to diverse clinical scenarios
- Use of GPT-4 as reference standard introduces potential circularity in accuracy evaluation
- Lack of detailed hallucination metrics and false positive/negative rates for clinical safety assessment
- Evaluation lacks diversity in clinical note types and patient populations

## Confidence

- **High confidence**: The RAG approach successfully enables conversational querying of clinical notes, and quantization significantly reduces inference latency.
- **Medium confidence**: Sentence transformers and Wizard Vicuna provide optimal performance for retrieval and generation, respectively, based on the limited evaluation set.
- **Low confidence**: The system's robustness to diverse clinical scenarios, hallucination rates, and clinical safety in real-world deployment.

## Next Checks

1. Expand the evaluation set to include at least 50 diverse clinical question-answer pairs across different note types (discharge summaries, progress notes, radiology reports) and medical conditions.

2. Conduct a human evaluation study with clinical experts to assess the accuracy, relevance, and safety of generated answers, specifically measuring hallucination rates and false positives/negatives.

3. Test the system on out-of-distribution clinical notes (e.g., rare diseases, complex cases) to evaluate robustness and identify failure modes not captured in the initial evaluation.