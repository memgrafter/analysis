---
ver: rpa2
title: Self-Supervised Denoiser Framework
arxiv_id: '2411.19593'
source_url: https://arxiv.org/abs/2411.19593
tags:
- image
- sinogram
- data
- reconstructed
- noise2inverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Self-supervised Denoiser Framework (SDF)
  for enhancing images reconstructed from undersampled tomographic data in industrial
  CT settings. SDF trains an image denoiser in the sinogram space by predicting one
  sinogram subset from another, eliminating the need for ground-truth image data.
---

# Self-Supervised Denoiser Framework

## Quick Facts
- arXiv ID: 2411.19593
- Source URL: https://arxiv.org/abs/2411.19593
- Authors: Emilien Valat; Andreas Hauptmann; Ozan Öktem
- Reference count: 29
- Key outcome: SDF achieves superior PSNR compared to analytical and self-supervised frameworks in both 2D fan-beam and 3D cone-beam CT settings, and serves as effective pre-training for few-shot supervised training.

## Executive Summary
The Self-Supervised Denoiser Framework (SDF) addresses the challenge of enhancing images reconstructed from undersampled tomographic data in industrial CT settings without requiring ground-truth image data. The framework trains an image denoiser in the sinogram space by predicting one sinogram subset from another, leveraging the abundant sinogram data modality. SDF demonstrates significant improvements in image quality (up to 20dB PSNR) compared to traditional analytical methods and other self-supervised approaches, while also serving as an effective pre-training technique for few-shot supervised learning when limited high-quality image data is available.

## Method Summary
SDF divides sinogram data into disjoint subsets and trains an image denoiser to predict one subset from another through the image reconstruction operator, which serves as a latent space. The framework uses angular subsets for 2D fan-beam CT and combines angular and orbital subsets for 3D cone-beam CT. The method trains using gradient descent on sinogram pairs without requiring ground-truth images, and can be fine-tuned with a small number of supervised examples for further improvement.

## Key Results
- SDF achieves up to 20dB PSNR improvement over FBP reconstruction in Mode1 sinograms
- Outperforms Noise2Inverse by 3.9dB while requiring no paired noisy-clean data
- Demonstrates 1.4-3.6dB improvement when fine-tuned with few supervised examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SDF trains image denoisers in sinogram space by predicting one subset from another without ground-truth images
- Mechanism: The framework divides sinograms into disjoint subsets and trains denoisers to predict one subset from another through the image reconstruction operator
- Core assumption: The reconstruction operator and image denoiser together form an effective auto-encoder with image space as latent space
- Evidence anchors: Abstract states SDF doesn't require ground-truth image data; Section 3.1 describes the sinogram subset division approach
- Break condition: Method fails if reconstruction operator is not invertible or subsets are too disjoint to allow meaningful prediction

### Mechanism 2
- Claim: SDF provides superior PSNR compared to analytical and self-supervised frameworks in both 2D and 3D CT settings
- Mechanism: By leveraging abundant sinogram data and training on highly sampled data, SDF learns to denoise undersampled reconstructions more effectively
- Core assumption: Sinogram space contains sufficient information for effective denoising without paired clean-noisy image data
- Evidence anchors: Abstract claims better PSNR than other frameworks; Section 4.3 shows 20dB improvement over FBP and 3.9dB over Noise2Inverse
- Break condition: Method fails with too high undersampling rates or insufficient sinogram information

### Mechanism 3
- Claim: SDF serves as effective pre-training for few-shot supervised training of high-quality image denoisers
- Mechanism: Self-supervised pre-training captures generalizable features that transfer well to supervised fine-tuning tasks
- Core assumption: Pre-trained network parameters provide better starting point than random initialization for supervised tasks
- Evidence anchors: Abstract mentions pre-training effectiveness; Section 4.4 reports 1.4dB and 3.6dB improvements for Mode1 and Mode2
- Break condition: Method fails if supervised task differs significantly from pre-training or few-shot examples don't represent target distribution

## Foundational Learning

- Concept: Inverse problems in tomography
  - Why needed here: CT image reconstruction from sinograms is fundamentally an inverse problem requiring understanding of ill-posedness and regularization
  - Quick check question: Why is reconstructing images from sinograms considered "ill-posed," and what role does regularization play?

- Concept: Self-supervised learning
  - Why needed here: SDF is a self-supervised framework that trains without ground-truth images, requiring understanding of self-supervision advantages
  - Quick check question: What is the key difference between self-supervised and supervised learning, and why might self-supervised be advantageous in CT imaging?

- Concept: Convolutional neural networks for image processing
  - Why needed here: The paper uses CNN-based image denoiser (Λθ) as core component, requiring understanding of how CNNs process images
  - Quick check question: How do convolutional layers help extract spatial features from images, and why are they useful for image denoising?

## Architecture Onboarding

- Component map: Sinogram data (Y) → Sinogram subsets (Y1, Y2, ..., YM) → Reconstruction operators (A†i) → Image space (X) → Image denoiser (Λθ) → Output images → Forward operator (A) → Sinogram generation → Loss function (ℓj) → Training objective → Optimization algorithm (Adam) → Parameter updates

- Critical path: Sinogram → Sinogram subset division → Reconstruction → Denoising → Image output
  - The most critical path is the ability to accurately reconstruct images from sinogram subsets and then denoise them effectively

- Design tradeoffs:
  - Angular subset size vs. reconstruction quality: Smaller subsets may lead to more severe artifacts but provide more training examples
  - Network complexity vs. training efficiency: Simpler networks train faster but may not capture complex denoising patterns
  - Self-supervised vs. supervised training: Self-supervised avoids needing ground truth but may not perform as well as supervised methods when ground truth is available

- Failure signatures:
  - Poor PSNR values compared to baseline methods
  - Failure to converge during training (loss plateaus or diverges)
  - Overfitting to training data (good training performance but poor test performance)
  - Inability to handle different undersampling rates or noise levels

- First 3 experiments:
  1. Implement basic SDF framework with simple angular subset division and evaluate on small subset of 2DeteCT dataset with Mode1 sinograms
  2. Compare SDF performance against traditional FBP reconstruction on same dataset
  3. Test SDF's ability to handle different levels of angular undersampling by varying number of subsets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SDF performance vary with different sinogram subset divisions (e.g., non-uniform or adaptive splitting strategies)?
- Basis in paper: [inferred] Paper mentions dividing sinogram space into disjoint sets but doesn't explore optimal subset divisions beyond uniform angular subsampling
- Why unresolved: Current experiments use uniform angular and orbital subsets, leaving open question of whether adaptive or problem-specific subset divisions could improve performance
- What evidence would resolve it: Comparative experiments testing SDF with various subset division strategies (adaptive, non-uniform, or application-specific) on same datasets

### Open Question 2
- Question: Can SDF be extended to handle more complex undersampling patterns, such as random or dynamic view selection?
- Basis in paper: [explicit] Paper focuses on uniform angular subsampling and orbital sparsity but doesn't address random or dynamic sampling patterns
- Why unresolved: Industrial CT often involves non-uniform sampling due to practical constraints, and paper doesn't explore SDF's robustness to such scenarios
- What evidence would resolve it: Experiments applying SDF to sinograms with random or dynamically selected views and comparing results to uniform subsampling

### Open Question 3
- Question: How does SDF perform when integrated with other post-processing tasks, such as segmentation or classification, in industrial CT workflows?
- Basis in paper: [explicit] Paper mentions SDF could be interfaced with post-processing operators but doesn't test this integration
- Why unresolved: While SDF improves image quality, its compatibility with downstream tasks like segmentation or classification remains unexplored
- What evidence would resolve it: Experiments combining SDF-enhanced reconstructions with segmentation or classification networks and evaluating task-specific performance metrics

## Limitations
- Results primarily validated on synthetic experiments with controlled undersampling rather than truly real-world industrial CT data
- Comparison against analytical methods like FBP may be overly favorable given FBP's known limitations
- Limited evidence for few-shot fine-tuning effectiveness, showing only modest improvements that may not justify additional complexity

## Confidence
- Mechanism 1: Medium confidence - theoretical framework is sound but optimal subset division strategies not fully explored
- Mechanism 2: Medium confidence - results compelling on test datasets but comparisons may favor SDF
- Mechanism 3: Low confidence - claims of effectiveness supported by modest improvements that may not justify complexity

## Next Checks
1. Test SDF on truly real-world industrial CT data with natural variations in material density, geometry complexity, and measurement noise to assess generalization beyond controlled synthetic datasets

2. Compare SDF against modern deep learning reconstruction methods (learned iterative schemes) rather than just traditional analytical approaches to establish competitive position in current state-of-the-art

3. Evaluate method's robustness to different undersampling patterns and noise characteristics that better reflect industrial CT scanning constraints, including sparse angular sampling and limited detector resolution