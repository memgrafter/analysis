---
ver: rpa2
title: Towards a Benchmark for Causal Business Process Reasoning with LLMs
arxiv_id: '2406.05506'
source_url: https://arxiv.org/abs/2406.05506
tags:
- process
- questions
- llms
- causal
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a benchmark for evaluating LLMs' ability to
  reason about Causally-augmented Business Processes (BPCs), which incorporate both
  temporal and causal execution dependencies. The core method involves creating template
  situations representing fundamental causal patterns, generating deductive rules,
  and formulating questions with ground truth answers.
---

# Towards a Benchmark for Causal Business Process Reasoning with LLMs

## Quick Facts
- arXiv ID: 2406.05506
- Source URL: https://arxiv.org/abs/2406.05506
- Authors: Fabiana Fournier; Lior Limonad; Inna Skarbovsky
- Reference count: 24
- Primary result: Introduced a benchmark for evaluating LLMs' ability to reason about Causally-augmented Business Processes (BPCs), tested on prototype dataset showing varying accuracy across different LLMs

## Executive Summary
This paper introduces a benchmark for evaluating LLMs' ability to reason about Causally-augmented Business Processes (BPCs), which incorporate both temporal and causal execution dependencies. The benchmark systematically tests LLMs by decomposing complex business processes into fundamental causal patterns, generating deductive rules, and formulating questions with ground truth answers. The benchmark serves dual purposes: testing existing LLM performance and training LLMs to improve their causal reasoning capabilities. Evaluation on a prototype dataset showed varying accuracy across different LLMs, with commercial models generally outperforming open-source ones.

## Method Summary
The benchmark creates template situations representing three fundamental causal patterns (confounder, collider, mediator), generates deductive rules for each pattern, and formulates questions with ground truth answers. These templates are then populated with domain-specific instances using LLMs. The benchmark is designed for both testing LLM performance and training LLMs to improve their reasoning capabilities. Evaluation involves prompting LLMs with BPC situation descriptions and questions, then comparing their answers to ground truth answers to calculate accuracy.

## Key Results
- The benchmark systematically tests LLMs' causal reasoning by decomposing complex business processes into fundamental causal patterns
- Evaluation on a prototype dataset showed varying accuracy across different LLMs, with commercial models generally outperforming open-source ones
- The benchmark can be used both for testing existing LLM performance and training LLMs to improve their causal reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark systematically tests LLMs' causal reasoning by decomposing complex business processes into fundamental causal patterns.
- Mechanism: The benchmark creates template situations representing three fundamental causal patterns (confounder, collider, mediator), generates deductive rules for each pattern, and formulates questions with ground truth answers. These templates are then populated with domain-specific instances using LLMs.
- Core assumption: LLMs can be trained to reason about causal relationships when provided with structured deductive rules and ground truth answers.
- Evidence anchors:
  - [abstract] "The core of the benchmark comprises a set of BP^C related situations, a set of questions about these situations, and a set of deductive rules employed to systematically resolve the ground truth answers to these questions."
  - [section] "From a process perspective, a BPC may be represented in the basic form of a graph, with nodes designating activities and edges as causal execution dependencies among these activities. Three fundamental patterns, or 'junctions' [17], can be composed to characterize any causal network."
- Break condition: The benchmark would fail if LLMs cannot generalize from template situations to novel domain-specific instances, or if the deductive rules are insufficient to capture the complexity of real-world causal relationships.

### Mechanism 2
- Claim: The benchmark can be used both for testing existing LLM performance and training LLMs to improve their causal reasoning capabilities.
- Mechanism: The benchmark serves a dual purpose - it can quantify an LLM's ability to reason about BPCs (testing mode) and can be used as training data to adapt an LLM's ability for this task (training mode).
- Core assumption: LLMs can improve their reasoning capabilities through exposure to structured training data that includes causal relationships and deductive rules.
- Evidence anchors:
  - [abstract] "Our benchmark, accessible at https://huggingface.co/datasets/ibm/BPC, can be used in one of two possible modalities: testing the performance of any target LLM and training an LLM to advance its capability to reason about BP^C."
  - [section] "The benchmark may also be further extended to more concrete domains and particular aspects of reasoning as the example given in this paper."
- Break condition: The benchmark would fail if LLMs show no improvement after training on the benchmark data, or if the training process leads to overfitting on specific patterns rather than general causal reasoning ability.

### Mechanism 3
- Claim: The benchmark provides a standardized method for comparing different LLMs' performance on causal reasoning tasks.
- Mechanism: By using a consistent set of template questions and situations across different domains and LLMs, the benchmark enables objective numerical comparison of LLM performance.
- Core assumption: Standardized evaluation using the same benchmark allows for meaningful comparison between different LLMs' capabilities.
- Evidence anchors:
  - [abstract] "Our results highlight the importance of producing an objective numeric scale partitioned by different perspectives to compare and assess various LLM performances."
  - [section] "We evaluated two open-source and three commercial LLMs on a small subset of the situations and questions. Our results highlight the importance of producing an objective numeric scale partitioned by different perspectives to compare and assess various LLM performances."
- Break condition: The benchmark would fail if the results are inconsistent across different runs or if the benchmark does not capture the full range of causal reasoning abilities needed for real-world business process applications.

## Foundational Learning

- **Business Process Management (BPM)**: Why needed here: Understanding BPM is crucial for grasping the context of causal business processes and the importance of reasoning about them. Quick check question: What are the key stages in the Business Process Management lifecycle?
- **Causal reasoning in AI**: Why needed here: The benchmark specifically tests LLMs' ability to reason about causal relationships, which is distinct from other forms of reasoning. Quick check question: How does causal reasoning differ from correlational reasoning in AI applications?
- **Large Language Models (LLMs)**: Why needed here: The benchmark is designed to test and train LLMs, so understanding their capabilities and limitations is essential. Quick check question: What are the main limitations of LLMs when it comes to reasoning tasks?

## Architecture Onboarding

- **Component map**: Template situations (confounder, collider, mediator patterns) -> Deductive rules -> Template questions -> Domain-specific instantiations (using LLM) -> Target LLM evaluation
- **Critical path**: 1) Define template situations and deductive rules, 2) Generate template questions, 3) Populate domain-specific instances using an LLM, 4) Test target LLMs on the benchmark, 5) Optionally use the benchmark for training
- **Design tradeoffs**: The benchmark balances comprehensiveness (covering all fundamental causal patterns) with practicality (manageable number of questions and domains). It also trades off between testing existing LLM capabilities and providing training data for improvement.
- **Failure signatures**: Inconsistent results across different LLM runs, poor generalization from template to domain-specific questions, or LLMs showing no improvement after training on the benchmark data.
- **First 3 experiments**:
  1. Test the benchmark with a simple LLM on a single domain to verify the basic functionality.
  2. Expand to multiple domains while keeping the same LLM to test generalization.
  3. Compare results across different LLMs to validate the benchmark's ability to differentiate performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of LLMs in reasoning about Causally-augmented Business Processes (BPC) scale with the size and diversity of the training dataset?
- Basis in paper: [explicit] The paper mentions that the benchmark can be used for both testing and training LLMs, and that training an LLM with the questions and answers can measure improvement in its ability to reason about BPC.
- Why unresolved: The paper does not provide experimental results on how the size and diversity of the training dataset affect LLM accuracy in BPC reasoning.
- What evidence would resolve it: Experiments varying the size and diversity of the training dataset, measuring LLM accuracy on a held-out test set, and analyzing the relationship between dataset characteristics and model performance.

### Open Question 2
- Question: To what extent do the emergent reasoning abilities of LLMs in BPC reasoning reflect genuine logical reasoning versus statistical pattern matching?
- Basis in paper: [explicit] The paper acknowledges the debate about whether LLMs possess genuine reasoning abilities or achieve predictive accuracy through training on large datasets, and states that it leaves this philosophical question beyond the scope of the study.
- Why unresolved: The paper does not attempt to distinguish between genuine logical reasoning and statistical pattern matching in LLM performance on BPC reasoning tasks.
- What evidence would resolve it: Comparative experiments between LLM performance on BPC reasoning tasks and performance of rule-based systems using the same deductive rules, analyzing differences in accuracy and reasoning patterns.

### Open Question 3
- Question: How does the inclusion of causal execution dependencies in business process descriptions affect the accuracy of LLM reasoning compared to descriptions with only temporal dependencies?
- Basis in paper: [inferred] The paper introduces BPCs as business processes extended with causal execution dependencies, implying that these dependencies are crucial for reasoning about processes, but does not directly compare LLM performance on BPCs versus processes with only temporal dependencies.
- Why unresolved: The paper does not provide experimental results comparing LLM accuracy on BPCs versus traditional business processes.
- What evidence would resolve it: Experiments comparing LLM accuracy on reasoning tasks for BPCs and traditional business processes, controlling for other factors such as process complexity and domain.

## Limitations

- Limited Benchmark Coverage: The current benchmark is based on a relatively small dataset of 56 situations across 7 domains with only 6 questions per domain, which may not fully capture the complexity and diversity of real-world business processes.
- LLM Performance Variability: Results are based on a small subset of the benchmark and may not be representative of LLM performance across all domains and question types. The non-deterministic nature of LLMs introduces variability in results.
- Generalization to Real-World Applications: The translation of benchmark results to practical business process interventions and improvements is not explicitly validated, leaving a gap between benchmark performance and real-world applicability.

## Confidence

- **Mechanism 1 (Systematic Testing of Causal Reasoning)**: High confidence
- **Mechanism 2 (Dual Purpose - Testing and Training)**: Medium confidence
- **Mechanism 3 (Standardized Comparison Method)**: Medium confidence

## Next Checks

1. **Benchmark Extension and Validation**: Extend the benchmark to include a larger and more diverse set of business process domains and causal patterns. Validate the extended benchmark by testing it with a wider range of LLMs and assessing its ability to differentiate performance across different models and scenarios.

2. **Training Effectiveness Evaluation**: Conduct experiments to evaluate the effectiveness of the benchmark as a training tool. Compare the causal reasoning performance of LLMs trained on the benchmark data against those without such training, using both the benchmark and real-world business process scenarios.

3. **Real-World Application Assessment**: Design and execute a study to assess the practical applicability of the benchmark results. This could involve collaborating with business process experts to evaluate the usefulness of LLM predictions in actual process intervention and improvement scenarios, and correlating benchmark performance with real-world effectiveness.