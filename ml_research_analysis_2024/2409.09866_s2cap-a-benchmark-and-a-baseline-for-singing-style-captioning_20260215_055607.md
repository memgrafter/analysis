---
ver: rpa2
title: 'S2Cap: A Benchmark and a Baseline for Singing Style Captioning'
arxiv_id: '2409.09866'
source_url: https://arxiv.org/abs/2409.09866
tags:
- style
- audio
- captioning
- singing
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces singing style captioning as a novel task to
  generate textual descriptions of vocal and musical characteristics from singing
  voices. To support this task, the authors present S2Cap, a dataset containing 71,215
  captions derived from 12,105 music tracks, annotated with diverse attributes including
  vocal tone, tempo, timbre, genre, and emotional expression.
---

# S2Cap: A Benchmark and a Baseline for Singing Style Captioning

## Quick Facts
- arXiv ID: 2409.09866
- Source URL: https://arxiv.org/abs/2409.09866
- Authors: Hyunjong Ok; Jaeho Lee
- Reference count: 40
- Key outcome: The paper introduces singing style captioning as a novel task to generate textual descriptions of vocal and musical characteristics from singing voices.

## Executive Summary
This paper introduces singing style captioning as a novel task to generate textual descriptions of vocal and musical characteristics from singing voices. To support this task, the authors present S2Cap, a dataset containing 71,215 captions derived from 12,105 music tracks, annotated with diverse attributes including vocal tone, tempo, timbre, genre, and emotional expression. A strong baseline method is established using a pretrained AST audio encoder paired with a BART text decoder, enhanced by two novel techniques: CRESCENDO, which aligns the audio and text embedding spaces through similarity learning, and demixing supervision, which focuses the model on vocal tracks by leveraging vocal demixing models. Experimental results show that the proposed approach achieves state-of-the-art performance, with BLEU-4 scores of 28.6, ROUGE-L of 48.7, and SPIDEr of 79.1 on the S2Cap test set.

## Method Summary
The method uses a pretrained AST audio encoder combined with a BART text decoder as a baseline for singing style captioning. Two novel techniques enhance performance: CRESCENDO, which aligns audio and text embedding spaces through positive-pair similarity learning, and demixing supervision, which focuses the model on vocal characteristics using vocal demixing models like Demucs. The model is trained on mel-spectrograms (16 kHz, 128-dimensional) with SpecAugment data augmentation, using cross-entropy loss with teacher forcing. Training occurs in two stages: first optimizing CRESCENDO alignment for 10 epochs, then full fine-tuning for 20 epochs.

## Key Results
- Achieves BLEU-4 score of 28.6, ROUGE-L of 48.7, and SPIDEr of 79.1 on the S2Cap test set
- Demonstrates that CRESCENDO alignment and demixing supervision significantly improve caption quality over baseline AST+BART
- Establishes S2Cap as a benchmark dataset with 71,215 captions from 12,105 music tracks covering diverse singing styles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The AST audio encoder paired with BART decoder works effectively for singing style captioning because AST's self-supervised training on large-scale audio data provides rich musical feature extraction, while BART's text generation capability is strong for caption output.
- Mechanism: AST learns to encode mel-spectrogram representations into high-level features that capture pitch, timbre, tempo, and other musical attributes. BART then maps these features to descriptive text. The CRESCENDO technique aligns the embedding spaces between the two pretrained models, mitigating misalignment issues.
- Core assumption: The audio features extracted by AST are sufficiently rich to support detailed singing style descriptions, and the embedding space alignment via CRESCENDO is effective.
- Evidence anchors:
  - [abstract]: "A strong baseline method is established using a pretrained AST audio encoder paired with a BART text decoder, enhanced by two novel techniques: CRESCENDO"
  - [section]: "We use SpecAugment [40] for data augmentation, opting for a 16 kHz sampling rate and 128-dimensional mel-spectrograms for feature extraction. Our architecture combines the AST 3 encoder with a BART-base 4 decoder."
- Break condition: If the AST encoder fails to capture nuanced vocal and musical characteristics, or if CRESCENDO fails to properly align the embedding spaces, the system will underperform.

### Mechanism 2
- Claim: CRESCENDO improves performance by synchronizing the embedding spaces of the audio encoder and text decoder through positive-pair similarity learning.
- Mechanism: CRESCENDO performs mean pooling on AST audio embeddings to match the sequence length of text embeddings, then optimizes the cosine similarity between the aligned audio embeddings and BART text embeddings, with a stop-gradient operation to prevent affecting the BART encoder.
- Core assumption: The embedding space misalignment between independently pretrained AST and BART is significant enough to warrant explicit alignment, and positive-pair similarity learning is an effective method for this.
- Evidence anchors:
  - [abstract]: "CRESCENDO, which aligns the audio and text embedding spaces through similarity learning"
  - [section]: "In our setting, we feed the audio from S2Cap data into the AST encoder while the associated captions are sent to the BART encoder. We optimize the loss based on their cosine similarity."
- Break condition: If the embedding spaces are not actually misaligned or if the similarity learning introduces noise, performance could degrade.

### Mechanism 3
- Claim: Demixing supervision improves caption accuracy by forcing the model to focus on vocal characteristics rather than musical accompaniment.
- Mechanism: The model uses a vocal demixing algorithm (Demucs) to separate vocals from the original audio, then applies KL divergence loss between the embeddings of the demixed vocal audio and the original song, encouraging the model to extract vocal-specific features.
- Core assumption: Vocal characteristics are the primary focus for singing style captioning, and the demixing process is accurate enough to provide meaningful supervision.
- Evidence anchors:
  - [abstract]: "demixing supervision, which focuses the model on vocal tracks by leveraging vocal demixing models"
  - [section]: "We implement this supervision by incorporating an auxiliary loss function based on the Kullback-Leibler (KL) divergence between the embeddings of the demixed vocal audio and the original song."
- Break condition: If the demixing model produces poor vocal separation or if musical accompaniment is relevant to style description, this mechanism could harm performance.

## Foundational Learning

- Concept: Audio-Text Representation Alignment
  - Why needed here: The pretrained AST and BART models were trained independently, so their embedding spaces may not be compatible. Without alignment, the model cannot effectively translate audio features into descriptive text.
  - Quick check question: What would happen if you skipped CRESCENDO and directly fine-tuned the AST+BART model on S2Cap?

- Concept: Self-Supervised Audio Feature Learning
  - Why needed here: AST was pretrained using self-supervised learning on large-scale audio data, allowing it to extract meaningful musical features without explicit labels. This pretraining is crucial for capturing the diverse vocal and musical attributes in singing style captioning.
  - Quick check question: How would performance change if you used a randomly initialized audio encoder instead of pretrained AST?

- Concept: Vocal Source Separation
  - Why needed here: Singing style is primarily determined by vocal characteristics, but music tracks contain both vocals and accompaniment. Separating these allows the model to focus on the relevant signal.
  - Quick check question: Why might the KL divergence loss between demixed vocals and original audio encourage vocal-focused feature extraction?

## Architecture Onboarding

- Component map: AST encoder → CRESCENDO alignment layer → BART decoder with demixing supervision
- Critical path: AST feature extraction → CRESCENDO embedding alignment → BART caption generation. The demixing supervision runs in parallel but doesn't block the main inference path.
- Design tradeoffs: Using pretrained models enables quick deployment but limits architectural flexibility. CRESCENDO adds training complexity but addresses alignment issues. Demixing adds computational overhead and dependency on external models but improves focus on vocals.
- Failure signatures: Low BLEU/ROUGE scores indicate poor caption quality. High KL divergence loss without caption improvement suggests demixing supervision isn't helping. Degradation after CRESCENDO training indicates alignment issues.
- First 3 experiments:
  1. Train baseline AST+BART without CRESCENDO or demixing to establish performance floor.
  2. Add CRESCENDO to test embedding alignment benefits while keeping demixing off.
  3. Add demixing supervision to test vocal focus benefits while keeping CRESCENDO from experiment 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CRESCENDO technique specifically impact the model's ability to generalize to singing styles not present in the training data?
- Basis in paper: [explicit] The paper mentions that CRESCENDO aims to resolve potential misalignment between the audio encoder and text decoder by performing positive-pair similarity learning.
- Why unresolved: The paper demonstrates improved performance metrics with CRESCENDO but does not explicitly test the model's generalization to unseen singing styles.
- What evidence would resolve it: Experimental results showing model performance on a dataset of singing styles not included in the training set would clarify the generalization capabilities.

### Open Question 2
- Question: What is the impact of the demixing supervision technique on the model's performance when dealing with complex musical arrangements where vocals are heavily intertwined with the accompaniment?
- Basis in paper: [explicit] The paper describes using demixing supervision to enhance vocal characteristics by leveraging the results of vocal demixing algorithms.
- Why unresolved: While the paper shows improved metrics with demixing supervision, it does not address scenarios with complex musical arrangements.
- What evidence would resolve it: Comparative analysis of model performance on tracks with varying levels of vocal-accompaniment complexity would provide insights into the robustness of the demixing supervision.

### Open Question 3
- Question: How does the inclusion of demographic information (e.g., age, gender) in the dataset influence the model's ability to generate culturally or contextually accurate singing style captions?
- Basis in paper: [explicit] The dataset includes demographic information such as age and gender as part of the metadata used for caption generation.
- Why unresolved: The paper does not explore the influence of demographic information on the accuracy or cultural relevance of the generated captions.
- What evidence would resolve it: An analysis comparing caption accuracy and cultural relevance with and without demographic information would clarify its impact on the model's outputs.

## Limitations

- Dataset scale and diversity limitations: While S2Cap contains 71,215 captions from 12,105 tracks, this represents a relatively modest dataset for pretraining large transformer models. The dataset may lack coverage of rare singing styles, regional variations, or niche vocal techniques.
- Transferability concerns: The paper establishes S2Cap as a benchmark for singing style captioning, but it remains unclear how well models trained on this dataset generalize to singing in languages other than English or to singing styles from different cultural traditions.
- Dependency on external models: Both CRESCENDO and demixing supervision rely on pretrained models (AST, BART, Demucs) that were not developed specifically for singing style captioning, introducing uncertainty about whether observed improvements stem from the novel techniques or the underlying model capabilities.

## Confidence

- High confidence: The dataset creation methodology and basic experimental results are well-documented and reproducible.
- Medium confidence: The effectiveness of CRESCENDO and demixing supervision is demonstrated through ablation studies, but improvements may be partially attributable to hyperparameter tuning.
- Low confidence: Claims about the general applicability of singing style captioning to downstream tasks are speculative and not empirically validated in the paper.

## Next Checks

1. **Cross-dataset validation**: Test the trained S2Cap models on an independent singing style dataset or on subsets of S2Cap with styles underrepresented in the training data to assess generalization and identify potential biases in the learned representations.

2. **Ablation of pretraining**: Compare the full CRESCENDO + demixing approach against a version that uses randomly initialized encoders rather than pretrained AST/BART to isolate the contribution of the proposed techniques from the benefits of pretraining on large-scale data.

3. **Human evaluation of caption quality**: Conduct a human study where music professionals or trained listeners rate the quality, accuracy, and usefulness of the generated captions compared to reference annotations, particularly for nuanced style descriptors that automated metrics may not capture well.