---
ver: rpa2
title: 'Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing
  Knowledge Conflicts with Source Citations'
arxiv_id: '2410.04241'
source_url: https://arxiv.org/abs/2410.04241
tags:
- document
- answer
- language
- conflicting
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel task: question answering with source
  citation in ambiguous settings, where multiple valid answers exist. To address this,
  the authors create five new datasets with citation metadata across various ambiguous
  settings (e.g., distractors, paraphrasing), including the first ambiguous multi-hop
  QA dataset.'
---

# Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations

## Quick Facts
- arXiv ID: 2410.04241
- Source URL: https://arxiv.org/abs/2410.04241
- Authors: Sagi Shaier; Ari Kobren; Philip Ogren
- Reference count: 20
- Primary result: Introduces novel task of QA with source citation in ambiguous settings and creates five new datasets with evaluation metrics

## Executive Summary
This paper introduces a novel task: question answering with source citation in ambiguous settings where multiple valid answers exist. To address this, the authors create five new datasets with citation metadata across various ambiguous settings including the first ambiguous multi-hop QA dataset. They propose two new evaluation metrics: Acc_K for measuring diverse correct answer generation and Citation Accuracy (A_C) for assessing accurate citation generation. Several strong baselines are established using rule-based, prompting, and fine-tuning approaches over five large language models. Results show that while prompting methods improve performance, all models struggle to generate multiple correct answers and cite sources accurately, especially in multi-hop settings.

## Method Summary
The paper establishes several strong baselines using rule-based, prompting, and fine-tuning approaches over five large language models (Llama-2-7B Chat, Llama-2-13B Chat, Llama-2-70B Chat, MPT-7B, and Falcon-7B Instruct). The fine-tuning is done using LoRA with specific hyperparameters. The paper creates five novel datasets with citation metadata by augmenting existing reading comprehension datasets. The proposed evaluation metrics are Acc_K (measuring diverse correct answer generation) and Citation Accuracy (A_C, assessing accurate citation generation). The prompting strategies include conflict-aware basic prompting, few-shot conflict-aware CoT prompting, and zero-shot CoT prompting, while the rule-based approach uses document splitting.

## Key Results
- Conflict-aware prompting improves model performance by explicitly acknowledging conflicting information in context
- Finetuning on conflict-augmented datasets significantly improves model performance on ambiguous QA tasks
- Document split method simplifies citation generation but limits multi-hop reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conflict-aware prompting improves model performance by explicitly acknowledging conflicting information in the context.
- **Mechanism:** The prompt structure emphasizes the existence of conflicting information and its corresponding citations, enabling models to develop a more nuanced understanding of ambiguous contexts.
- **Core assumption:** Models can better handle conflicting information when explicitly prompted to do so.
- **Evidence anchors:** [abstract] "we propose the novel task of QA with source citation in ambiguous settings, where multiple valid answers exist"; [section] "This conflict-aware (C.A) prompting design emphasizes the existence of conflicting information and its corresponding citations, enabling models to develop a more nuanced understanding of ambiguous contexts."
- **Break condition:** If the model lacks sufficient reasoning capabilities or if the conflicting information is too subtle to be recognized even with explicit prompting.

### Mechanism 2
- **Claim:** Finetuning on conflict-augmented datasets significantly improves model performance on ambiguous QA tasks.
- **Mechanism:** By training on datasets specifically designed with conflicting information and source citations, models learn to generate multiple answers and cite sources accurately.
- **Core assumption:** Exposure to conflict-augmented data during training improves model ability to handle ambiguity and citation generation.
- **Evidence anchors:** [abstract] "several strong baselines using rule-based, prompting, and finetuning approaches over five large language models"; [section] "We fine-tune each model on each of the following datasets: AmbigQA-Cite, DisentQA-DupliCite, and Conflicting HotPotQA-Cite"
- **Break condition:** If the finetuning dataset is too small or not representative of real-world ambiguity, or if the model's architecture cannot effectively incorporate the learned patterns.

### Mechanism 3
- **Claim:** Document split method simplifies citation generation by processing each document individually, but limits multi-hop reasoning capabilities.
- **Mechanism:** By splitting the context into individual documents based on citation tokens and processing them sequentially, the model can easily generate citations. However, this approach prevents the model from reasoning across multiple documents simultaneously.
- **Core assumption:** Processing documents individually simplifies citation generation but at the cost of complex reasoning.
- **Evidence anchors:** [section] "We split the context into individual articles based on the citation tokens, and process them sequentially, following a strict rule: each article is processed one at a time"; [section] "This approach makes citations trivial, as we can generate one response per document and evaluate them separately to identify correct citations. However, this rule-based approach also has a limitation."
- **Break condition:** If the question requires reasoning across multiple documents or if the document structure is too complex for simple splitting.

## Foundational Learning

- **Concept:** Understanding of ambiguous QA settings
  - Why needed here: The task involves questions with multiple valid answers, requiring models to generate diverse responses and cite sources accurately.
  - Quick check question: What is the difference between ambiguous questions and ambiguous contexts in QA tasks?

- **Concept:** Knowledge of citation generation techniques
  - Why needed here: The task requires models to not only generate answers but also provide accurate citations for each conflicting information.
  - Quick check question: How do existing citation generation methods differ from those proposed in this work?

- **Concept:** Familiarity with multi-hop reasoning
  - Why needed here: The work introduces the first ambiguous multi-hop QA dataset, requiring models to reason across multiple documents to answer questions.
  - Quick check question: What is multi-hop reasoning and why is it important in complex QA tasks?

## Architecture Onboarding

- **Component map:** Input question and context -> Conflict-aware prompting module -> Citation generation module -> Multi-hop reasoning module -> Evaluation metrics (Acc_K and A_C)
- **Critical path:** 1. Input question and contexts; 2. Apply conflict-aware prompting; 3. Generate multiple answers with citations; 4. Evaluate using Acc_K and A_C metrics
- **Design tradeoffs:** Conflict-aware prompting vs. zero-shot CoT (explicit prompting may be more effective but requires manual design); Document split vs. full context processing (simplifies citation generation but limits complex reasoning); Finetuning vs. prompting (finishing may be more effective but requires additional computational resources)
- **Failure signatures:** Low Acc_K scores indicate inability to generate diverse correct answers; Low A_C scores suggest poor citation generation; Poor performance on multi-hop datasets indicates limited reasoning capabilities
- **First 3 experiments:** 1. Test zero-shot baseline on AmbigQA-Cite dataset to establish performance baseline; 2. Apply conflict-aware basic prompting with 3-shot examples on DisentQA-DupliCite dataset; 3. Finetune Llama-7B on Conflicting HotPotQA-Cite (no distractors) and evaluate performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on QA with source citation in ambiguous settings when using a more diverse range of ambiguity types beyond the three flavors (paraphrasing, distractors, and duplications) tested in this work?
- Basis in paper: [inferred] The paper discusses the need for creating more diverse datasets that capture different aspects of ambiguity beyond the three flavors they experimented with.
- Why unresolved: The authors explicitly state that future work should explore creating a more diverse range of datasets to capture different aspects of ambiguity. Their current experiments are limited to paraphrasing, distractors, and duplications.
- What evidence would resolve it: Experiments using new datasets that incorporate additional types of ambiguity (e.g., temporal ambiguity, logical contradictions, semantic ambiguity) would provide evidence for how well LLMs handle these variations.

### Open Question 2
- Question: Does pretraining on conflicting data and citations improve LLMs' ability to resolve knowledge conflicts and provide trustworthy answers compared to finetuning approaches?
- Basis in paper: [explicit] The authors suggest pretraining models on conflicting data and citations as a potential direction for future research to potentially improve their ability to resolve knowledge conflicts.
- Why unresolved: The paper focuses on finetuning approaches and does not explore pretraining on conflicting data. This remains an untested hypothesis.
- What evidence would resolve it: Direct comparison of models pretrained on conflicting data versus models finetuned on the same data would provide evidence for the effectiveness of pretraining in this context.

### Open Question 3
- Question: How do larger LLMs (beyond the 70B parameter model tested) perform on the task of QA with source citation in ambiguous settings, particularly in multi-hop reasoning scenarios?
- Basis in paper: [inferred] The paper evaluates models up to 70B parameters and notes that even these larger models struggle with generating multiple correct answers and citing sources accurately, especially in multi-hop settings.
- Why unresolved: The authors acknowledge the limitations of the 70B parameter models and suggest that larger models might perform better, but this remains untested.
- What evidence would resolve it: Experiments using even larger LLMs (e.g., 100B+ parameters) on the same datasets would provide evidence for whether scale improves performance in this task.

## Limitations
- The document split method, while simplifying citation generation, fundamentally limits the models' ability to perform multi-hop reasoning across multiple documents simultaneously
- The evaluation metrics, while novel, may not comprehensively capture all aspects of citation quality and answer diversity in practical applications
- The relatively small scale of the conflict-augmented datasets used for finetuning may not fully capture the complexity of real-world ambiguous QA scenarios

## Confidence
**High Confidence:** The core task formulation of QA with source citation in ambiguous settings is clearly defined and justified. The creation of five new datasets with citation metadata is well-documented and reproducible.

**Medium Confidence:** The proposed evaluation metrics (Acc_K and A_C) are theoretically sound but may require further validation on diverse datasets to establish their robustness. The effectiveness of conflict-aware prompting is supported by experimental results but could benefit from ablation studies.

**Low Confidence:** The scalability of the proposed approaches to larger, more complex multi-hop reasoning tasks remains uncertain. The paper's results suggest that current models struggle significantly with these scenarios, indicating potential fundamental limitations in the approach.

## Next Checks
1. **Ablation Study on Prompting Strategy:** Conduct an ablation study comparing conflict-aware prompting against zero-shot CoT prompting across all five datasets to quantify the specific contribution of conflict-awareness to performance improvements.

2. **Cross-Dataset Generalization Test:** Evaluate the finetuned models on non-ambiguous versions of the datasets (where applicable) to assess whether the conflict-augmentation approach negatively impacts performance when no ambiguity exists.

3. **Citation Quality Analysis:** Perform a detailed qualitative analysis of the generated citations, examining not just the accuracy metric but also the relevance and completeness of the cited sources in relation to the generated answers.