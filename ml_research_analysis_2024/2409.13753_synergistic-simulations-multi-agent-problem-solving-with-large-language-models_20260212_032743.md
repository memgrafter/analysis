---
ver: rpa2
title: 'Synergistic Simulations: Multi-Agent Problem Solving with Large Language Models'
arxiv_id: '2409.13753'
source_url: https://arxiv.org/abs/2409.13753
tags:
- agents
- environment
- agent
- moderator
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents a multi-agent framework using large language
  models (LLMs) to simulate collaborative problem solving in two domains: a shared
  apartment environment and an online coding environment. The method involves creating
  independent agents with unique identities and histories, interacting through a moderator
  in a turn-based event loop, and using an environment representation for agents to
  manipulate.'
---

# Synergistic Simulations: Multi-Agent Problem Solving with Large Language Models

## Quick Facts
- arXiv ID: 2409.13753
- Source URL: https://arxiv.org/abs/2409.13753
- Authors: Asher Sprigler; Alexander Drobek; Keagan Weinstock; Wendpanga Tapsoba; Gavin Childress; Andy Dao; Lucas Gral
- Reference count: 7
- Primary result: Multi-agent framework using LLMs shows promise for collaborative problem solving but struggles with detailed tasks due to context limitations

## Executive Summary
This paper presents a multi-agent framework for collaborative problem solving using large language models in two distinct environments: a shared apartment scenario and an online coding environment. The system uses independent agents with unique identities interacting through a moderator in a turn-based event loop, with environment-mediated interaction and observation-based memory systems. While the framework successfully demonstrated basic collaboration on broad tasks like expense tracking, it struggled with detailed tasks like baking a cake and failed to solve Leetcode problems in the coding environment. The authors identify key challenges including context limitations, unrestricted code editing, and lack of real-time validation, suggesting future improvements through parallel LLMs, fine-tuning, and more sophisticated observation mechanisms.

## Method Summary
The method involves creating independent agents with unique identities and histories, interacting through a moderator in a turn-based event loop, and using an environment representation for agents to manipulate. Agents generate observations after each action with timestamps and importance values, which are retrieved based on recency, importance, and relevance scoring using embeddings. The framework was tested in two environments: a shared apartment with roommates managing tasks like expense tracking and baking, and an online coding environment where agents attempted to solve Leetcode problems through collaborative code editing and chatroom interaction.

## Key Results
- Agents successfully collaborated on broad tasks like expense tracking in the apartment environment
- Agents struggled with specific, detailed tasks like baking a cake due to limited context
- In the coding environment, agents failed to solve Leetcode problems and did not use the chatroom for social interaction
- Single LLMs outperformed the multi-agent approach in coding tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Independent agents with unique identities can collaborate by leveraging their distinct perspectives and histories.
- Mechanism: Agents are differentiated by their unique histories (lists of messages with roles and content), allowing them to maintain separate identities while sharing a single LLM instance. This enables parallel exploration of solutions through turn-based interactions.
- Core assumption: Distinct agent identities and histories provide sufficient diversity in problem-solving approaches to produce synergistic outcomes.
- Evidence anchors:
  - [section] "All independent agents exist in parallel on top of a single LLM instance, differentiated by their unique histories."
  - [abstract] "This paper aims to integrate both aforementioned topics (agents & world interaction) into a single simulation where multiple agents can work together to solve a problem, modeling how groups of humans can often solve problems better than individuals."
- Break condition: If agent histories become too similar or if the LLM's context window limits meaningful differentiation, the diversity advantage disappears.

### Mechanism 2
- Claim: Environment-mediated interaction allows agents to coordinate without direct communication.
- Mechanism: A moderator agent facilitates all agent-environment and agent-agent interactions through a structured event loop, where agents receive environment summaries and take turns acting. This indirect communication pattern enables coordinated problem-solving.
- Core assumption: Mediated interaction can effectively replace direct agent-to-agent communication for collaborative problem-solving.
- Evidence anchors:
  - [section] "To facilitate communication between the agents, each individual has a 'turn' to generate a message for a target agent, formatted as JSON with entries for 'message' and 'to.'"
  - [section] "This model highlights the potential of mediated communication in complex agent-based systems, facilitating intricate interactions within shared spaces without direct agent-to-agent communication."
- Break condition: If the moderator cannot adequately represent the environment state or if turn-based communication creates bottlenecks that prevent timely coordination.

### Mechanism 3
- Claim: Observation-based memory systems extend agent reasoning beyond context window limitations.
- Mechanism: Agents generate observations after each action with timestamps and importance values, which are then retrieved based on recency, importance, and relevance scoring using embeddings. This creates a long-term memory system.
- Core assumption: Short-term observations with weighted retrieval can effectively simulate long-term memory for agents operating within context window constraints.
- Evidence anchors:
  - [section] "Observations are short statements about the environment that agents generate after every action. Generated observations include a timestamp, and an importance-value."
  - [section] "To determine the relevance of observations before adding them to the history, every observation is given a retrieval-importance value by adding each given observation's recency value, importance value, and relevance value."
- Break condition: If the observation retrieval mechanism fails to surface relevant information or if the importance scoring becomes unreliable.

## Foundational Learning

- Concept: Multi-agent systems and distributed problem-solving
  - Why needed here: Understanding how multiple agents can work together versus a single agent is central to evaluating the paper's claims about synergy.
  - Quick check question: What are the key differences between centralized and distributed problem-solving approaches in AI systems?

- Concept: Context window limitations in large language models
  - Why needed here: The paper explicitly discusses context limitations as a challenge, and understanding these constraints is crucial for evaluating the observation mechanism.
  - Quick check question: How do context window limitations affect an LLM's ability to maintain state across extended interactions?

- Concept: Monte Carlo Tree Search and planning algorithms
  - Why needed here: The related work section mentions MCTS, and understanding these algorithms helps contextualize the paper's contribution to agent-based planning.
  - Quick check question: How does MCTS use reward estimation to guide search in complex problem spaces?

## Architecture Onboarding

- Component map:
  LLM instance -> Agent module -> Moderator module -> Environment module -> Observation system

- Critical path:
  1. Agent formulates action based on environment summary
  2. Moderator validates and executes action
  3. Environment state updates
  4. Observations generated and stored
  5. Next agent takes turn
  6. Loop continues until goal completion

- Design tradeoffs:
  - Single LLM instance vs. parallel instances (memory efficiency vs. throughput)
  - Turn-based vs. parallel agent execution (simplicity vs. speed)
  - Moderator-mediated vs. direct agent communication (control vs. flexibility)
  - Observation-based memory vs. larger context windows (scalability vs. complexity)

- Failure signatures:
  - Agents produce repetitive or contradictory actions
  - Event loop stalls due to invalid function calls
  - Observation retrieval fails to surface relevant information
  - Moderator cannot adequately represent complex environment states

- First 3 experiments:
  1. Test basic multi-agent interaction with simple turn-taking and moderator mediation using a minimal environment
  2. Implement observation system and test retrieval effectiveness with varying importance and recency parameters
  3. Add environment functions and test agent ability to discover and use appropriate functions for goal completion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-agent LLM collaboration outperform single LLM solutions in problem-solving tasks?
- Basis in paper: [inferred] The authors observed that single LLMs outperformed multi-agent approaches in coding tasks and note this as a key question.
- Why unresolved: The paper's experiments showed mixed results with agents failing to solve Leetcode problems, but the framework was not optimized or tested with different LLM configurations.
- What evidence would resolve it: Comparative experiments testing single vs. multi-agent approaches on the same problems with optimized agent configurations and larger context windows.

### Open Question 2
- Question: How does context window size affect multi-agent LLM collaboration effectiveness?
- Basis in paper: [explicit] The authors note that agents struggled with detailed tasks like baking a cake due to limited context and mention this as a key challenge.
- Why unresolved: The experiments were conducted with Mistral7B which has a relatively small context window (approximately 4k tokens), limiting the agents' ability to maintain detailed task information.
- What evidence would resolve it: Experiments using LLMs with larger context windows (e.g., Code Llama with 16k context) to determine if increased context improves multi-agent problem-solving performance.

### Open Question 3
- Question: What is the optimal method for agents to interact with and modify code in collaborative programming tasks?
- Basis in paper: [explicit] The authors discuss that unrestricted code editing led to agents producing unworkable solutions and suggest implementing step-by-step code generation instead.
- Why unresolved: The current implementation allowed agents to modify any part of the codebase, leading to runtime errors and inefficient solutions that were not properly validated.
- What evidence would resolve it: Comparative experiments testing different interaction methods (e.g., restricted editing zones, step-by-step validation, real-time code execution feedback) to determine which produces the most reliable collaborative coding results.

## Limitations

- Context window limitations prevented effective handling of detailed tasks like baking a cake
- Lack of real-time code validation led to agents producing and editing unworkable code solutions
- Single LLM instance approach may have limited the diversity of agent perspectives and problem-solving approaches

## Confidence

**Medium confidence**: The claim that independent agents with unique identities can collaborate through mediated interaction is supported by the successful expense tracking in the apartment scenario, but contradicted by the coding environment results.

**Low confidence**: The assertion that observation-based memory systems can effectively extend agent reasoning beyond context window limitations is questionable, as the paper doesn't provide sufficient evidence of meaningful performance improvement.

**Medium confidence**: The claim about the potential of mediated communication in complex agent-based systems is partially supported, with success in simple tasks but significant limitations in complex coding scenarios.

## Next Checks

1. Implement parallel LLM instances: Test whether using multiple LLM instances per agent (instead of a single shared instance) improves performance, particularly in the coding environment where the single LLM approach failed.

2. Add real-time code validation: Implement a code execution and validation system to prevent agents from producing and editing unworkable code, then re-test the coding environment tasks.

3. Evaluate observation system effectiveness: Conduct controlled experiments comparing agent performance with and without the observation memory system across both environments, measuring the impact on task completion and solution quality.