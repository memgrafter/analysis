---
ver: rpa2
title: 'Mixture of Parrots: Experts improve memorization more than reasoning'
arxiv_id: '2410.19034'
source_url: https://arxiv.org/abs/2410.19034
tags:
- parameters
- arxiv
- number
- dense
- moes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes the theoretical and empirical differences between
  dense transformers and mixture-of-experts (MoE) models in terms of memorization
  versus reasoning capabilities. Theoretical analysis shows that dense models require
  fewer parameters than MoEs for reasoning tasks (e.g., graph connectivity), but MoEs
  can achieve better memorization with fewer active parameters due to their ability
  to route tokens to different experts.
---

# Mixture of Parrots: Experts improve memorization more than reasoning

## Quick Facts
- **arXiv ID:** 2410.19034
- **Source URL:** https://arxiv.org/abs/2410.19034
- **Reference count:** 40
- **Primary result:** MoEs excel at memorization tasks but dense models outperform them on reasoning tasks due to width requirements

## Executive Summary
This paper analyzes the theoretical and empirical differences between dense transformers and mixture-of-experts (MoE) models in terms of memorization versus reasoning capabilities. The authors demonstrate that while MoEs can achieve better memorization with fewer active parameters due to efficient routing, dense models require fewer total parameters than MoEs for reasoning tasks like graph connectivity. Synthetic experiments validate these findings: MoEs match dense model performance on memorization tasks but require more active parameters for reasoning tasks. When pre-trained on real-world data, MoEs excel at knowledge-intensive tasks while dense models perform better on reasoning tasks, with MoEs also showing higher generalization gaps on reasoning tasks.

## Method Summary
The paper compares dense transformers and MoE models on both synthetic and real-world tasks. Synthetic experiments involve shortest path graphs and phone-book lookup tasks, testing models with varying hidden dimensions (256-4096) and number of experts (8-64). Pre-trained models use datasets including Fineweb-edu, Cosmopedia, Wikipedia, Proof-Pile2, OpenMathInstruct, and MetaMathQA, with training for 60k steps on 63B tokens. The MoE models use top-2 token-choice routing without token dropping. Downstream evaluation includes world knowledge benchmarks, commonsense reasoning, and math problems.

## Key Results
- MoEs match dense model performance on memorization tasks (phone-book lookup) with fewer active parameters
- Dense models require fewer total parameters than MoEs for reasoning tasks (graph connectivity)
- MoEs show higher generalization gaps on reasoning tasks compared to dense models
- Pre-trained MoEs excel at knowledge-intensive tasks while dense models perform better on reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing the number of experts in an MoE improves memorization performance by enabling efficient routing of tokens to specialized experts.
- **Mechanism:** Each expert in the MoE specializes in memorizing specific patterns from the training data. The routing function assigns tokens to the expert most capable of handling them, effectively partitioning the memorization task across experts.
- **Core assumption:** The routing function can accurately identify which expert should handle each token based on learned patterns.
- **Evidence anchors:**
  - [abstract] "MoEs can achieve better memorization with fewer active parameters due to their ability to route tokens to different experts"
  - [section 4.2] "No matter the number of active parameters, MoEs match the performance of dense transformers with the same number of total parameters"
  - [corpus] Weak - neighboring papers focus on scaling laws and architectural improvements rather than memorization mechanisms specifically
- **Break condition:** If the routing function becomes too coarse or fails to differentiate between token patterns, memorization performance will degrade despite having many experts.

### Mechanism 2
- **Claim:** MoEs require a critical hidden size to solve reasoning tasks, and simply adding more experts cannot compensate for insufficient width.
- **Mechanism:** Graph reasoning tasks require sufficient model capacity to maintain and process complex relationships. A sparse transformer with insufficient width cannot maintain the necessary information flow between tokens regardless of how many experts are added.
- **Core assumption:** The reasoning task requires maintaining global information that cannot be effectively partitioned across experts.
- **Evidence anchors:**
  - [abstract] "dense models require fewer parameters than MoEs for reasoning tasks (e.g., graph connectivity)"
  - [section 3.2] "There exist graph problems that cannot be solved by any number of experts of a certain width; however, the same task can be easily solved by a dense model with a slightly larger width"
  - [corpus] Weak - neighboring papers discuss expressive power but don't directly address the width requirement for reasoning
- **Break condition:** If the reasoning task can be decomposed into smaller subproblems that can be solved independently by different experts, then increasing experts might help despite insufficient width.

### Mechanism 3
- **Claim:** MoEs exhibit higher generalization gaps on reasoning tasks compared to dense models, indicating potential overfitting to memorized patterns.
- **Mechanism:** MoEs can more effectively memorize training data through their routing mechanism, but this memorization capability may lead to poorer generalization on reasoning tasks that require understanding underlying patterns rather than recalling specific examples.
- **Core assumption:** The routing mechanism that benefits memorization also makes it easier to memorize rather than learn generalizable reasoning patterns.
- **Evidence anchors:**
  - [abstract] "MoEs also show higher generalization gaps on reasoning tasks, indicating potential overfitting to memorized patterns"
  - [section 5.2] "On math tasks, MoEs display a higher train-test gap than dense models, suggestive of memorization"
  - [corpus] Weak - neighboring papers don't discuss generalization gaps specifically, focusing instead on architectural improvements
- **Break condition:** If the training data contains sufficient diversity and the routing mechanism is regularized to prevent over-specialization, the generalization gap may not appear or could be reduced.

## Foundational Learning

- **Concept: Graph connectivity and reasoning tasks**
  - Why needed here: Understanding why dense models outperform MoEs on reasoning tasks requires grasping the nature of graph problems and why they demand global information processing
  - Quick check question: Why can't a sparse model with many experts solve a graph connectivity problem if each expert only needs to process a subset of the graph?

- **Concept: Memorization vs generalization in neural networks**
  - Why needed here: The paper's central finding depends on distinguishing between a model's ability to memorize training data versus its ability to generalize to new reasoning problems
  - Quick check question: What's the key difference between a model that memorizes phone numbers versus one that learns how to find shortest paths?

- **Concept: Computational complexity and parameter efficiency**
  - Why needed here: The paper's analysis of active vs total parameters requires understanding how computational cost scales differently in dense versus sparse architectures
  - Quick check question: Why does an MoE with the same total parameters as a dense model potentially have much lower computational cost during inference?

## Architecture Onboarding

- **Component map:** Token embedding -> Router/gating module -> Expert modules (specialized MLPs) -> Aggregation layer -> Standard transformer layers (residual connections, normalization)

- **Critical path:**
  1. Token embedding → Router → Expert selection
  2. Selected experts process tokens in parallel
  3. Expert outputs combined and passed to next layer
  4. Process repeats through all layers

- **Design tradeoffs:**
  - More experts improve memorization but increase memory usage and routing complexity
  - Fewer experts reduce memory overhead but may not capture sufficient specialization
  - Top-k routing (typically top-2) balances computational efficiency with routing flexibility

- **Failure signatures:**
  - High variance in expert utilization (some experts get most tokens, others get none)
  - Degradation in reasoning task performance as experts increase
  - Training instability or exploding/vanishing gradients in deep MoE stacks

- **First 3 experiments:**
  1. Implement a simple MoE with 2 experts and verify routing works correctly on synthetic data
  2. Compare memorization performance on phonebook task between MoE and dense model with equal total parameters
  3. Test reasoning performance on shortest path task and observe how performance changes with increasing experts at fixed active parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the memorization advantage of MoEs over dense transformers persist at scale (e.g., billions of parameters)?
- Basis in paper: [explicit] The paper notes that their pre-trained models have up to ≤ 2.1B parameters, but recognizes that large-scale MoEs like Mixtral and DeepSeek-V2 have orders of magnitude more parameters.
- Why unresolved: The paper only tests models up to 2.1B parameters and hypothesizes that results would still be meaningful at larger scales due to strong theoretical underpinnings, but does not empirically verify this.
- What evidence would resolve it: Pre-training and evaluating MoEs and dense transformers with billions of parameters on the same tasks (world knowledge, commonsense, and math) as the paper would show whether the memorization advantage persists at scale.

### Open Question 2
- Question: How do different routing mechanisms (e.g., top-2 vs. top-1) affect the performance of MoEs on reasoning vs. memorization tasks?
- Basis in paper: [explicit] The paper uses top-2 token-choice routing without token dropping and notes that it leaves the study of MoEs trained with other routing mechanisms for future work.
- Why unresolved: The paper only tests one routing mechanism, so it is unclear how other routing strategies might impact the tradeoff between reasoning and memorization capabilities.
- What evidence would resolve it: Pre-training and evaluating MoEs with various routing mechanisms (e.g., top-1, top-k, expert-choice) on the same tasks would reveal how routing affects performance on reasoning vs. memorization.

### Open Question 3
- Question: Can architectures with reduced width dependence (e.g., sub-quadratic attention variants) achieve the best of both worlds for reasoning and memorization tasks?
- Basis in paper: [explicit] The paper discusses the limitations of MoEs for reasoning tasks and suggests that sub-quadratic attention variants or other architectures with reduced width dependence could provide an alternative to achieve both reasoning and memorization capabilities.
- Why unresolved: The paper does not empirically test these alternative architectures, so it is unclear whether they can indeed achieve the desired tradeoff.
- What evidence would resolve it: Pre-training and evaluating architectures with reduced width dependence (e.g., linear attention, Monarch mixers) on the same tasks would show whether they can outperform both dense transformers and MoEs on reasoning and memorization tasks.

## Limitations

- The empirical validation relies heavily on synthetic reasoning tasks whose complexity may not fully capture real-world reasoning problems
- The analysis of generalization gaps focuses on train-test splits but doesn't account for potential distribution shifts or out-of-distribution testing
- The paper doesn't investigate how different routing mechanisms might affect the memorization-reasoning tradeoff

## Confidence

- **High confidence:** The theoretical analysis showing dense models can be more parameter-efficient than MoEs for reasoning tasks (graph connectivity)
- **Medium confidence:** The empirical finding that MoEs excel at memorization while dense models perform better on reasoning
- **Medium confidence:** The claim about higher generalization gaps in MoEs for reasoning tasks

## Next Checks

1. **Distribution shift validation:** Test both MoE and dense models on reasoning tasks with significant distribution shifts (e.g., graph connectivity on graphs with different degree distributions than training data) to verify the generalization gap findings hold under realistic conditions.

2. **Routing mechanism ablation:** Systematically compare different routing strategies (top-1, top-2, learned gating) across both memorization and reasoning tasks to determine if the observed differences are routing-mechanism specific or inherent to the MoE architecture.

3. **Scaling behavior analysis:** Evaluate how the memorization-reasoning tradeoff evolves as model scale increases (both in terms of total parameters and active parameters) to determine if the current findings hold at larger scales typical of production models.