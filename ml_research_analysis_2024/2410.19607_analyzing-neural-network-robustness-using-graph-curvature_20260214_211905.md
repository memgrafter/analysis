---
ver: rpa2
title: Analyzing Neural Network Robustness Using Graph Curvature
arxiv_id: '2410.19607'
source_url: https://arxiv.org/abs/2410.19607
tags:
- neural
- curvature
- graph
- robust
- edges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using graph Ricci curvature to analyze neural
  network robustness. The authors define neural Ricci curvature by constructing a
  neural data graph where edges are weighted based on network weights and input activations.
---

# Analyzing Neural Network Robustness Using Graph Curvature

## Quick Facts
- arXiv ID: 2410.19607
- Source URL: https://arxiv.org/abs/2410.19607
- Authors: Shuhang Tan; Jayson Sia; Paul Bogdan; Radoslav Ivanov
- Reference count: 27
- Primary result: More robust neural networks have neural data graphs with fewer negative-Ollivier-Ricci curvature edges

## Executive Summary
This paper proposes using graph Ricci curvature to analyze neural network robustness by constructing neural data graphs where edges are weighted based on network weights and input activations. The authors define neural Ricci curvature and show that more robust examples (those resistant to adversarial perturbations) tend to have neural data graphs with fewer negative-curvature edges. Experiments on MNIST demonstrate that as perturbation tolerance ε increases, the average area under the curvature distribution curve decreases across different network architectures and training methods. This suggests that minimizing negative-curvature edges could be an effective approach for training more robust neural networks.

## Method Summary
The method constructs neural data graphs from trained MNIST networks by creating weighted graphs where edges are determined by ReLU activations and normalized weights. Neural Ricci curvature is calculated using the Ollivier-Ricci curvature formula with Wasserstein distance between node distributions. The authors evaluate robustness by generating adversarial examples at different perturbation levels (ε = 0.03 to 0.2) and measuring whether examples remain correctly classified. They then analyze the distribution of neural Ricci curvature values across robust and non-robust examples, finding that more robust examples have fewer negative-curvature edges.

## Key Results
- More robust examples have neural data graphs with fewer negative-NRC edges
- As perturbation tolerance ε increases, the average area under the CDF curve decreases
- The trend holds across different network architectures (fully-connected and CNN) and training methods (cross-entropy, weight decay, adversarial training)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative-Ollivier-Ricci curvature edges in neural data graphs correspond to bottleneck edges that carry disproportionate information flow and thus increase vulnerability to adversarial perturbations.
- Mechanism: When an edge has negative curvature, it means the Wasserstein distance between neighboring node distributions exceeds the graph distance, indicating that all paths between those nodes must pass through this edge. In neural networks, this manifests as edges where ReLU activations are consistently active and weight magnitudes are high, creating critical information transport paths.
- Core assumption: The neural data graph construction properly captures both the network architecture and input-dependent activation patterns to reveal true information bottlenecks.
- Evidence anchors:
  - [abstract] "We define the notion of neural Ricci curvature and use it to identify bottleneck NN edges that are heavily used to 'transport data' to the NN outputs."
  - [section] "if an edge e between u and v is a bottleneck, then all paths from u's neighbors to v's neighbors have to go through e; in this case, κ(u, v) < 0 since W1 > d (u, v)"
- Break condition: If the normalization in Algorithm 1 fails to properly handle mixed-sign weights or if ReLU phases don't accurately reflect critical information flow, the curvature metric will misidentify bottlenecks.

### Mechanism 2
- Claim: Adversarial perturbations primarily exploit bottleneck edges, causing misclassification when these critical paths are disturbed.
- Mechanism: Robust examples have neural data graphs with fewer negative-curvature edges, meaning their information flow is distributed across multiple paths rather than concentrated through single critical edges. This distribution makes the network less sensitive to localized perturbations.
- Core assumption: The relationship between edge curvature and robustness is causal rather than merely correlational, and that reducing negative-curvature edges during training will improve robustness.
- Evidence anchors:
  - [abstract] "We show that neural data graphs corresponding to more robust examples (i.e., examples which are correctly classified even for an adversarial perturbation) indeed have fewer negative-NRC edges."
  - [section] "the CDF grows faster for examples which are less ε-robust, i.e., those examples have more negatve-NRC edges, on average, than the graphs corresponding to more ε-robust examples."
- Break condition: If adversarial perturbations can bypass identified bottlenecks through alternative paths, or if the network's decision boundary depends on features distributed across many edges rather than concentrated paths.

### Mechanism 3
- Claim: The neural data graph construction method (Algorithm 1) effectively handles the mixed-sign weight challenge by normalizing weights while preserving information flow direction.
- Mechanism: By setting negative weights to zero and proportionally scaling positive weights to maintain the same sum, the algorithm ensures that edges contributing negatively to the output don't artificially inflate curvature measurements while preserving the relative importance of positive contributions.
- Core assumption: The assumption that sum ≥ 0 (line 2 in Algorithm 1) always holds for ReLU-activated networks, ensuring valid normalization.
- Evidence anchors:
  - [section] "Algorithm 1 only applies when nl1,j(x) > 0 since otherwise the ReLU would be its zero phase and all outgoing edges would be removed."
  - [section] "Algorithm 1 normalizes the weights such that negative weights are reset to 0 and positive weights are normalized so that overall sum remains the same."
- Break condition: If the ReLU activation pattern creates situations where the sum of weighted inputs becomes negative for active neurons, violating the algorithm's precondition.

## Foundational Learning

- Concept: Graph curvature theory (specifically Ollivier-Ricci curvature)
  - Why needed here: Provides the mathematical foundation for identifying bottleneck edges in both traditional graphs and neural data graphs
  - Quick check question: What does it mean geometrically when W1(mu, mv) > d(u, v) for adjacent nodes u and v in a graph?

- Concept: Wasserstein distance and optimal transport theory
  - Why needed here: The curvature calculation relies on measuring the "work" required to transform one probability distribution into another across graph edges
  - Quick check question: How does the Wasserstein distance between uniform distributions over node neighbors relate to the presence of bottleneck edges?

- Concept: Neural network activation patterns and their input-dependence
  - Why needed here: The neural data graph construction must account for which neurons are active for specific inputs, affecting edge weights and curvature calculations
  - Quick check question: Why does the neural data graph remove edges when nl,i(x) = 0, and how does this relate to information flow bottlenecks?

## Architecture Onboarding

- Component map: Input example -> Neural data graph construction (Algorithm 1) -> NRC calculation for all edges -> Statistical analysis (CDF/AUC) -> Robustness correlation assessment
- Critical path: Input example → Neural data graph construction (Algorithm 1) → NRC calculation for all edges → Statistical analysis (CDF/AUC) → Robustness correlation assessment
- Design tradeoffs: The method trades computational efficiency (curvature calculation is expensive) for geometric insight into network vulnerabilities. The normalization approach balances simplicity with the need to handle mixed-sign weights.
- Failure signatures: (1) No correlation between NRC and robustness across different architectures, (2) Negative-curvature edges not corresponding to known critical paths in the network, (3) Curvature calculations producing identical values across all examples regardless of robustness.
- First 3 experiments:
  1. Verify NRC calculation on a simple 2-layer network with known bottleneck structure
  2. Test correlation between NRC distribution and adversarial example generation success rate
  3. Compare NRC-based bottleneck identification with gradient-based saliency methods on toy networks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does neural Ricci curvature behave across different datasets beyond MNIST?
- Basis in paper: [explicit] The paper states "In future work, we will perform an evaluation over multiple datasets and robust training methods"
- Why unresolved: The evaluation is limited to MNIST, and the authors acknowledge the need for broader testing
- What evidence would resolve it: Experimental results showing NRC patterns across diverse datasets (CIFAR, ImageNet, etc.) with varying network architectures

### Open Question 2
- Question: Can minimizing negative-NRC edges during training improve adversarial robustness more effectively than existing methods?
- Basis in paper: [explicit] The paper states "These results will serve as the basis for an alternative method of robust training, by minimizing the number of bottleneck edges"
- Why unresolved: The paper only presents correlation between negative-NRC edges and non-robust examples, but does not demonstrate that minimizing these edges during training improves robustness
- What evidence would resolve it: Experimental comparison of a training method that explicitly minimizes negative-NRC edges versus standard adversarial training approaches

### Open Question 3
- Question: How does neural Ricci curvature scale with deeper networks and different layer types (e.g., attention layers)?
- Basis in paper: [inferred] The paper notes that CNN results were less pronounced due to few edges per node, suggesting depth and layer type affect curvature analysis
- Why unresolved: The evaluation focused on relatively shallow networks, and the CNN analysis was limited
- What evidence would resolve it: Systematic study of NRC across networks of varying depth and architectures, including transformers and other modern architectures

## Limitations

- The theoretical link between negative Ricci curvature and adversarial vulnerability relies on strong assumptions about information flow that may not generalize to all network architectures
- The normalization procedure in Algorithm 1 assumes sum ≥ 0, which may not always hold in practice
- The computational cost of curvature calculations limits scalability to larger networks and datasets

## Confidence

- **High Confidence**: The empirical observation that more robust examples have neural data graphs with fewer negative-curvature edges is well-supported by the experimental results across multiple network architectures and training methods.
- **Medium Confidence**: The theoretical mechanism connecting negative curvature to bottleneck edges and adversarial vulnerability is plausible but relies on strong assumptions about information flow that may not generalize to all network architectures.
- **Medium Confidence**: The proposed method for normalizing mixed-sign weights is mathematically sound, but its effectiveness in practice depends on the specific weight distributions encountered during training.

## Next Checks

1. Test the correlation between neural Ricci curvature and robustness on a different dataset (e.g., CIFAR-10) with convolutional architectures to verify generalization beyond MNIST fully-connected networks.
2. Implement ablation studies where negative-curvature edges are specifically targeted during training (using the curvature-based regularization approach) to determine if this directly improves adversarial robustness.
3. Compare the curvature-based bottleneck identification with gradient-based saliency methods on the same networks to assess whether the geometric approach provides complementary insights about vulnerability.