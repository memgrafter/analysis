---
ver: rpa2
title: 'NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition'
arxiv_id: '2405.07609'
source_url: https://arxiv.org/abs/2405.07609
tags:
- noise
- training
- real
- noisy
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces NOISE BENCH, a benchmark for evaluating
  the impact of real label noise on Named Entity Recognition (NER) models. The benchmark
  consists of clean CoNLL-03 data corrupted with six types of real noise: expert errors,
  crowdsourcing errors, distant supervision errors, weak supervision errors, and LLM-generated
  errors.'
---

# NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition

## Quick Facts
- **arXiv ID**: 2405.07609
- **Source URL**: https://arxiv.org/abs/2405.07609
- **Reference count**: 27
- **Primary result**: Introduces NOISE BENCH benchmark showing real label noise is significantly more challenging than simulated noise for NER models

## Executive Summary
This paper introduces NOISE BENCH, a benchmark for evaluating the impact of real label noise on Named Entity Recognition (NER) models. The benchmark consists of clean CoNLL-03 data corrupted with six types of real noise: expert errors, crowdsourcing errors, distant supervision errors, weak supervision errors, and LLM-generated errors. The authors conduct experiments comparing real noise to simulated noise and find that real noise is significantly more challenging for current models. They also evaluate state-of-the-art noise-robust learning approaches on NOISE BENCH and show that they fall far short of their theoretical upper bounds. The key findings are that real noise causes immediate memorization in models, while simulated noise shows delayed memorization, and that no single current approach works best for all types of real noise.

## Method Summary
The authors create NOISE BENCH by corrupting clean CoNLL-03 data with six types of real noise: expert errors, crowdsourcing errors, distant supervision errors, weak supervision errors, and LLM-generated errors. They fine-tune XLM-RoBERTa-Large using FLERT approach with learning rate 5e-6, batch size 32, and 10 epochs. Performance is evaluated on clean test splits using micro-averaged token-level and entity-level F1 scores. The benchmark also includes simulated noise variants (uniform noise and oracle class-dependent noise) for comparison. Various noise-robust learning approaches are implemented and evaluated, including Confident Learning, CrossWeigh, L2R, Co-regularization, BOND, and MSR.

## Key Results
- Real noise causes immediate memorization in models during training, while simulated noise shows delayed memorization
- No single noise-robust learning approach works best across all types of real noise
- Approaches identifying clean subsets of labels show the highest potential for improving performance under real noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Real noise is immediately memorized during training, while simulated noise shows delayed memorization.
- Mechanism: Real noise contains plausible patterns that the model learns to recognize early, causing it to start overfitting from the beginning. Simulated noise lacks these coherent patterns, so the model first learns general features before overfitting.
- Core assumption: The plausibility of noise patterns affects the timing of memorization in the learning process.
- Evidence anchors:
  - [abstract] "We further find that during training, real noise is memorized immediately, whereas memorization of simulated noise is delayed."
  - [section 3.4.1] "With real noise, this does not happen and the model starts fitting the noisy labels from the beginning (see Figure 3a, 3b, 3c)."
  - [corpus] Weak corpus evidence; the claim is primarily supported by the paper's internal experiments.
- Break condition: If noise patterns become uniformly random or lack plausible structure, the immediate memorization mechanism would not hold.

### Mechanism 2
- Claim: No single current noise-robust learning approach works best for all types of real noise.
- Mechanism: Different types of real noise have distinct characteristics (e.g., missing mentions vs. incorrect types), and each noise-robust approach is optimized for specific noise patterns. Therefore, a method effective for one noise type may not generalize to others.
- Core assumption: The diversity of real noise types necessitates a diverse set of noise-robust approaches.
- Evidence anchors:
  - [abstract] "We further find that during training, real noise is memorized immediately, whereas memorization of simulated noise is delayed."
  - [section 4.2] "Evidently, there is no single best approach for all noise types. For each noise type, at least one noise-robust approach outperforms the baseline, however on average most of them are comparable to it."
  - [corpus] Weak corpus evidence; the claim is primarily supported by the paper's internal experiments.
- Break condition: If a universal noise-robust approach is developed that can handle all types of real noise, this mechanism would be invalidated.

### Mechanism 3
- Claim: Approaches that identify a clean subset of labels have the highest potential for improving performance under real noise.
- Mechanism: By filtering out noisy labels and training only on clean data, these approaches avoid the pitfalls of overfitting to noise. The oracle subset experiment shows that this strategy can achieve the best performance among all evaluated methods.
- Core assumption: Clean data is more valuable for training than noisy data, and the ability to identify it accurately is crucial.
- Evidence anchors:
  - [section 4.2] "The upper bound of training only the clean subset of each noisy split (see 'Oracle subset' in Table 3) achieves the best scores of all upper bounds."
  - [table 3] "Oracle subset - 90.31±0.28 91.83±0.33 85.95±0.59 83.07±0.59 81.13±1.25 75.70±1.10 85.99"
  - [corpus] Weak corpus evidence; the claim is primarily supported by the paper's internal experiments.
- Break condition: If the cost of identifying clean data becomes prohibitively high or if the clean subset is too small to be useful, this mechanism would be less effective.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: Understanding NER is fundamental to grasping the task and challenges addressed by the paper.
  - Quick check question: What are the main entity types typically recognized in NER tasks, and how are they labeled in datasets like CoNLL-03?

- Concept: Label Noise
  - Why needed here: Label noise is the central problem the paper addresses, and understanding its types and effects is crucial.
  - Quick check question: What are the different types of label noise mentioned in the paper, and how do they affect model performance?

- Concept: Noise-Robust Learning
  - Why needed here: The paper evaluates various noise-robust learning approaches, so understanding these methods is essential.
  - Quick check question: What are the main strategies used by noise-robust learning approaches to handle label noise, and how do they differ?

## Architecture Onboarding

- Component map:
  Data Preparation -> Model Training -> Evaluation -> Noise-Robust Approaches

- Critical path:
  1. Prepare the noisy training data variants.
  2. Fine-tune the baseline model on each noisy variant.
  3. Evaluate the model on the clean test set.
  4. Analyze memorization patterns and compare real vs. simulated noise.
  5. Implement and evaluate noise-robust learning approaches.
  6. Compare results and identify the most effective strategies.

- Design tradeoffs:
  - Using a larger transformer model (XLM-RoBERTa-Large) provides better performance but requires more computational resources.
  - Simulated noise is easier to generate but may not accurately reflect the challenges of real noise.
  - Noise-robust approaches can improve performance but may add complexity and computational overhead.

- Failure signatures:
  - Poor performance on the clean test set indicates that the model is overfitting to the noise in the training data.
  - Similar performance on real and simulated noise suggests that the simulated noise is not challenging enough.
  - Inconsistent results across different noise types indicate that the noise-robust approach may not be generalizable.

- First 3 experiments:
  1. Fine-tune the baseline model on each noisy variant of the training data and evaluate on the clean test set.
  2. Generate simulated noise variants and compare their performance to real noise.
  3. Implement and evaluate a simple noise-robust approach (e.g., Confident Learning) on the noisy training data.

## Open Questions the Paper Calls Out
- How does the composition of the noise (types of errors, error rates) in real-world datasets affect the efficacy of noise-robust learning approaches, and can we develop adaptive methods that adjust to different noise compositions?
- What is the theoretical limit of noise-robust learning approaches when applied to real noise, and how close can practical methods get to this limit under realistic assumptions?

## Limitations
- The findings are primarily based on experiments with CoNLL-03 data and may not generalize to other NER datasets or tasks.
- Real noise is harder to simulate accurately, which could affect the validity of comparisons between real and simulated noise.
- The noise-robust learning approaches evaluated are limited to a specific set of methods, and there may be other approaches that perform better on NOISE BENCH.

## Confidence
- High: The finding that real noise is more challenging than simulated noise for current models, supported by direct experimental evidence.
- Medium: The observation that no single noise-robust approach works best for all types of real noise, based on comparative results across multiple approaches.
- Medium: The claim that approaches identifying clean subsets have the highest potential, inferred from oracle subset experiments.

## Next Checks
1. Replicate the experiments on additional NER datasets (e.g., OntoNotes, WNUT) to assess generalizability of the findings across different domains and languages.
2. Test a broader range of noise-robust learning approaches, including recently proposed methods like Meta-Weight-Net or DivideMix, to determine if better performance can be achieved.
3. Conduct ablation studies to quantify the impact of specific noise characteristics (e.g., error rate, noise type distribution) on model performance and memorization patterns.