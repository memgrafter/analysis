---
ver: rpa2
title: 'Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval
  Model'
arxiv_id: '2409.17745'
source_url: https://arxiv.org/abs/2409.17745
tags:
- examples
- few-shot
- retrieval
- training
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple non-parametric ranking model based
  on in-context learning. Unlike standard supervised models, it uses a frozen LLM
  and provides pairwise preference examples from similar training queries as additional
  context.
---

# Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model

## Quick Facts
- **arXiv ID:** 2409.17745
- **Source URL:** https://arxiv.org/abs/2409.17745
- **Reference count:** 39
- **Primary result:** Simple few-shot prompting with LLM significantly improves pairwise ranking over zero-shot baselines without requiring training

## Executive Summary
This paper proposes a non-parametric ranking model that leverages in-context learning with large language models (LLMs) for pairwise document ranking. The approach uses examples of preferences from similar queries in a training set as additional context, enabling the LLM to make better pairwise comparisons without parameter updates. The method demonstrates significant improvements over zero-shot baselines in both in-domain and out-of-domain settings, achieving close performance to supervised models while avoiding complex training pipelines. Experiments on TREC DL and BEIR datasets show consistent gains, particularly with semantic similarity-based example selection.

## Method Summary
The approach uses few-shot prompting for pairwise document ranking by providing the LLM with examples of query-document preferences from similar training queries. For each input query, the system retrieves k similar queries from a training set using either lexical (BM25) or semantic (BERT) similarity. Each example consists of a query, a relevant document, and a hard negative document. The LLM processes the current query-document pairs along with these examples to generate preference scores, which are aggregated to produce a final ranking. The method requires no parameter updates and only needs a frozen LLM and an available training set with relevance judgments.

## Key Results
- Few-shot prompting improves MAP@100 by up to 0.0859 and nDCG@10 by up to 0.0631 over zero-shot baselines on TREC DL datasets
- Semantic similarity (BERT) outperforms lexical similarity (BM25) for in-domain tasks, while lexical similarity is more effective for out-of-domain generalization
- The method achieves performance close to supervised models while requiring only unlabeled data and avoiding training complexity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Localized few-shot examples improve ranking effectiveness by providing semantically related context that helps the LLM make better pairwise comparisons.
- **Mechanism:** The LLM generates preference scores based on both the current query-document pair and the additional examples from similar queries in the training set. These examples serve as a form of contextual augmentation that guides the LLM's reasoning.
- **Core assumption:** The LLM can effectively process and leverage the semantic similarity between the input query and example queries to improve preference prediction accuracy.
- **Evidence anchors:** [abstract] "examples of preferences for similar queries from a training set"; [section 3.2] "we input a set of queries (from an available training set) that are related to the current query in terms of an abstract similarity measure"
- **Break condition:** If the similarity function between queries produces unrelated examples, the additional context could confuse the LLM and degrade performance.

### Mechanism 2
- **Claim:** Using both relevant documents and hard negatives as examples provides contrastive learning signals that improve preference estimation.
- **Mechanism:** Each example consists of a query, a relevant document, and a hard negative document. The LLM learns to distinguish between relevant and non-relevant documents through these contrastive pairs, improving its ability to rank documents for the current query.
- **Core assumption:** The LLM can generalize from the contrastive examples to make better pairwise comparisons for unseen queries.
- **Evidence anchors:** [section 3.2] "we sample a non-relevant document as a hard negative from ranks m to M of a BM25 retrieved list"; [section 3.2] "The triple ⟨Q′, RQ′, NQ′⟩ constitutes a single example that we input to an LLM"
- **Break condition:** If the hard negatives are too easy or too difficult, the contrastive signal may be ineffective or misleading.

### Mechanism 3
- **Claim:** Semantic similarity-based example selection is more effective than lexical similarity for in-domain tasks.
- **Mechanism:** When examples are selected based on semantic similarity (BERT embeddings), they capture deeper information needs that align better with the input query, leading to more effective context for the LLM.
- **Core assumption:** Semantic similarity better captures information need similarity than lexical overlap, especially for complex queries.
- **Evidence anchors:** [section 4.2] "we employ two methodologies for the neighbourhood selection function... BM25 (sparse BoW) and BERT (dense)"; [section 5.1] "Our method also improves on a static baseline... We further explore the effects of using both lexical and semantic similarity scoring functions"
- **Break condition:** In out-of-domain scenarios, semantic similarity may not translate well, making lexical approaches more robust despite their limitations.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed here: This paper's approach fundamentally relies on ICL, where the LLM uses example query-document pairs as context to improve its ranking decisions without parameter updates.
  - Quick check question: What is the key difference between in-context learning and traditional fine-tuning in terms of model parameters?

- **Concept: Pairwise Ranking**
  - Why needed here: The core methodology involves comparing document pairs for a given query, requiring understanding of pairwise ranking concepts and their advantages over pointwise approaches.
  - Quick check question: Why might pairwise ranking be more effective than pointwise ranking for document ranking tasks?

- **Concept: Contrastive Learning**
  - Why needed here: The approach uses relevant documents and hard negatives to create contrastive examples, which is a fundamental concept in modern representation learning and retrieval.
  - Quick check question: What is the purpose of including hard negative examples in contrastive learning frameworks?

## Architecture Onboarding

- **Component map:** Query processing pipeline → Similarity-based example retrieval → Prompt construction → LLM inference → Preference score aggregation; First-stage retriever (BM25/Contriever) → Few-shot ranker → Final ranking output; Training set indexing → Query similarity computation → Example selection → Prompt template filling

- **Critical path:** 1. Input query and document pairs; 2. Retrieve similar queries from training set using similarity function; 3. Construct few-shot examples with relevant documents and hard negatives; 4. Generate prompt with examples and current query-document pairs; 5. LLM inference to get preference scores; 6. Aggregate scores across all document pairs; 7. Output final ranked list

- **Design tradeoffs:**
  - Number of examples (k): More examples provide better context but increase computational cost and risk of "lost-in-the-middle" effects
  - Similarity function choice: Semantic similarity (BERT) vs lexical (BM25) - trade-off between capturing deeper meaning vs robustness to domain shifts
  - Example selection strategy: Localized (similar queries) vs static examples - trade-off between relevance and computational overhead

- **Failure signatures:**
  - Poor performance improvement over zero-shot baseline → Check example relevance and similarity function effectiveness
  - Degradation in performance with few-shot examples → Check if examples are introducing noise or confusion
  - High computational cost with minimal gains → Optimize number of examples or similarity computation efficiency

- **First 3 experiments:**
  1. Zero-shot vs 1-shot comparison with lexical similarity to establish baseline effectiveness
  2. Semantic vs lexical similarity comparison on in-domain data to identify optimal similarity function
  3. In-domain vs out-of-domain evaluation to test generalization capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do larger language models (e.g., LLaMa-70B, GPT-3.5, or GPT-4) perform compared to the 7B models used in this study when applying few-shot pairwise ranking prompting?
- **Basis in paper:** [explicit] The paper explicitly states they "mainly focus on the open-source lightweight LLMs (≤7B)" and notes that "whether the few-shot performance gains are much higher with larger LLMs" is yet to be investigated.
- **Why unresolved:** The study only tested models with 7B parameters or less, leaving the performance characteristics of larger models unknown.
- **What evidence would resolve it:** Direct experiments comparing few-shot PRP performance across a range of model sizes (7B, 13B, 30B, 70B, etc.) on the same datasets would show whether performance gains scale with model size.

### Open Question 2
- **Question:** What is the optimal number of few-shot examples to use for different types of queries (e.g., long vs. short, specific vs. general)?
- **Basis in paper:** [inferred] The paper shows that MAP@100 improves monotonically with increasing k while nDCG@10 plateaus beyond k=1, but doesn't investigate per-query optimization of example count.
- **Why unresolved:** The study uses a fixed k value across all queries and doesn't explore adaptive selection of example count based on query characteristics.
- **What evidence would resolve it:** Experiments that vary k per query based on query features (length, specificity, etc.) and measure the resulting retrieval performance would identify optimal example counts for different query types.

### Open Question 3
- **Question:** How does the effectiveness of few-shot PRP compare when using different first-stage retrieval methods (e.g., dense vs. sparse) across various domains?
- **Basis in paper:** [explicit] The paper provides supplementary results showing Contriever + few-shot PRP performance, but notes that effectiveness varies significantly between in-domain and out-of-domain settings.
- **Why unresolved:** While the paper shows some cross-domain performance differences, it doesn't systematically compare how first-stage retrieval method choice interacts with domain characteristics.
- **What evidence would resolve it:** Comprehensive experiments testing few-shot PRP with multiple first-stage retrievers (BM25, Contriever, SPLADE, etc.) across diverse domain combinations would reveal optimal retriever pairings for different scenarios.

## Limitations

- Limited out-of-domain generalization, with gains primarily from lexical similarity methods rather than semantic approaches
- Effectiveness heavily depends on availability of similar queries in the training set
- Substantial LLM inference overhead may limit practical deployment due to computational costs

## Confidence

- High confidence in in-domain effectiveness claims (TREC DL results with statistical significance)
- Medium confidence in out-of-domain generalization claims (BEIR results show limited improvement)
- Medium confidence in semantic vs lexical similarity findings (in-domain advantage but opposite out-of-domain trend)

## Next Checks

1. **Generalization stress test:** Evaluate the few-shot approach on a broader set of out-of-domain datasets to better understand the limits of semantic similarity-based example selection and identify when lexical approaches outperform.

2. **Prompt sensitivity analysis:** Systematically vary prompt format, number of examples, and example selection criteria to quantify their impact on ranking effectiveness and identify optimal configurations.

3. **Efficiency benchmarking:** Measure computational overhead per query and compare against supervised models to determine practical deployment thresholds and identify opportunities for optimization.