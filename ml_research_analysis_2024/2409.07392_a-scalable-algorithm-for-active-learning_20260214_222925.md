---
ver: rpa2
title: A Scalable Algorithm for Active Learning
arxiv_id: '2409.07392'
source_url: https://arxiv.org/abs/2409.07392
tags:
- number
- step
- algorithm
- learning
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Approx-FIRAL is a GPU-accelerated active learning algorithm that
  scales to large datasets while maintaining accuracy. It builds on the FIRAL algorithm
  by replacing exact solutions with iterative methods, randomized trace estimators,
  and block diagonal approximations.
---

# A Scalable Algorithm for Active Learning

## Quick Facts
- arXiv ID: 2409.07392
- Source URL: https://arxiv.org/abs/2409.07392
- Authors: Youguang Chen; Zheyu Wen; George Biros
- Reference count: 24
- Approx-FIRAL achieves 10-12x speedup over Exact-FIRAL on 12 GPUs while maintaining accuracy

## Executive Summary
Approx-FIRAL is a GPU-accelerated active learning algorithm that scales to large datasets while maintaining the accuracy of the original FIRAL algorithm. It replaces exact solutions with iterative methods, randomized trace estimators, and block diagonal approximations to reduce computational and storage complexity. The algorithm achieves significant speedups on multi-GPU systems while preserving classification accuracy across MNIST, CIFAR-10, Caltech-101, ImageNet-1k, and synthetic datasets.

## Method Summary
Approx-FIRAL builds on the FIRAL algorithm by introducing matrix-free operations, randomized trace estimators using Hutchinson's method, preconditioned conjugate gradient solvers, and block diagonal approximations. The algorithm uses Kronecker product structures in Fisher information matrices, Rademacher random vectors for trace estimation, and Sherman-Morrison-like formulas for efficient matrix updates. GPU acceleration is achieved through CuPy and MPI-based multi-GPU communication.

## Key Results
- Reduces storage complexity from O(c²d² + nc²d) to O(n(d + c) + cd²)
- Reduces computational complexity from O(c³(nd² + bd³ + bn)) to O(bncd²)
- Achieves 10-12x speedup on 12 GPUs compared to Exact-FIRAL
- Maintains accuracy on datasets with up to three million points and 1000 classes

## Why This Works (Mechanism)

### Mechanism 1
Randomized trace estimators reduce computational complexity by avoiding explicit matrix formation. The algorithm uses Hutchinson's trace estimator with Rademacher random vectors to approximate gradients without forming large matrices. The trace estimator with sufficient random vectors provides an accurate approximation of the true gradient.

### Mechanism 2
Block diagonal preconditioners accelerate conjugate gradient convergence. The algorithm uses block diagonal approximations of the Fisher information matrix as preconditioners for the CG solver, improving the conditioning of the matrix. The block diagonal structure captures the essential spectral properties needed for effective preconditioning.

### Mechanism 3
Sherman-Morrison-like formulas enable efficient updates to block diagonal matrix inverses. The algorithm uses a lemma showing that when adding a low-rank update to a block diagonal matrix, the inverse can be computed efficiently using element-wise operations rather than full matrix inversion.

## Foundational Learning

- Concept: Kronecker product structure in Fisher information matrices
  - Why needed here: Enables efficient matrix-free operations
  - Quick check question: What is the Kronecker product form of the Fisher information matrix for a single data point?

- Concept: Randomized linear algebra techniques
  - Why needed here: Randomized trace estimators and matrix-free operations are essential for scaling
  - Quick check question: How does Hutchinson's trace estimator approximate the trace of a matrix using random vectors?

- Concept: Preconditioning in iterative methods
  - Why needed here: Preconditioners dramatically improve the convergence rate of conjugate gradient solvers
  - Quick check question: What is the mathematical relationship between a preconditioner M and the original matrix A in the context of CG?

## Architecture Onboarding

- Component map: Data preprocessing -> RELAX step -> ROUND step -> GPU parallelization -> Evaluation
- Critical path: Load data -> Initialize sets -> RELAX step -> ROUND step -> Train logistic regression -> Evaluate accuracy
- Design tradeoffs: Single precision vs double precision for memory efficiency; block diagonal vs full matrix for computational cost; fixed vs adaptive CG tolerance for implementation simplicity
- Failure signatures: Poor accuracy despite correct implementation (insufficient Rademacher vectors or CG iterations); memory errors on large datasets (check block diagonal preconditioner); slow convergence (verify preconditioner quality); GPU communication bottlenecks (monitor MPI timings)
- First 3 experiments: Run on MNIST with default parameters; profile memory usage on ImageNet-1k with varying classes; test strong scaling on 1, 2, 4, 8, and 12 GPUs

## Open Questions the Paper Calls Out

### Open Question 1
How can FIRAL-like algorithms be extended to handle dynamic feature embeddings that change as new labeled data is introduced? This is identified as a "more fundamental limitation" since the current FIRAL theory assumes fixed feature embeddings, and no theoretical framework exists for handling evolving embeddings with theoretical guarantees.

### Open Question 2
What is the impact of replacing exact eigenvalue solvers with iterative methods on the theoretical convergence guarantees of Approx-FIRAL? The paper notes that deriving precise error bounds requires detailed estimates of the approximation error when considering iterative alternatives.

### Open Question 3
How would Approx-FIRAL perform on non-NVIDIA GPU architectures, particularly for CPU implementations using NumPy? The paper states that testing has been limited to NVIDIA GPUs, and while CuPy supports AMD GPUs and CPU implementations are theoretically possible, these alternatives have not yet been carried out.

## Limitations
- Lack of comprehensive ablation studies on key design choices like number of Rademacher vectors and block diagonal approximation
- Performance with very high-dimensional features (>10,000 dimensions) or extremely large numbers of classes (>1000) remains untested
- Limited testing to NVIDIA GPUs with 12 GPU maximum, making generalization to larger GPU clusters uncertain

## Confidence
- High confidence: Computational complexity improvements and memory reduction claims are mathematically sound and verified through timing experiments
- Medium confidence: Accuracy preservation claim across all tested datasets is supported, but lack of ablation studies reduces confidence in understanding failure modes
- Medium confidence: GPU scaling results are impressive but limited to specific hardware configurations, making generalization uncertain

## Next Checks
1. Ablation on Trace Estimator Accuracy: Run Approx-FIRAL with varying numbers of Rademacher vectors (1, 2, 4, 8, 16) on MNIST and CIFAR-10 to quantify the relationship between trace estimation accuracy and final classification performance.

2. Block Diagonal vs Full Matrix Comparison: For small-scale datasets (e.g., MNIST with 1000 points), implement and compare Approx-FIRAL against a variant using full Fisher information matrices to measure the accuracy tradeoff of the block diagonal approximation.

3. High-Dimensional Feature Robustness: Test Approx-FIRAL on datasets with high-dimensional features (e.g., raw pixel data instead of learned embeddings) to evaluate whether the Kronecker product structure assumptions break down in the absence of good feature representations.