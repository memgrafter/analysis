---
ver: rpa2
title: The Large Language Model GreekLegalRoBERTa
arxiv_id: '2410.12852'
source_url: https://arxiv.org/abs/2410.12852
tags:
- legal
- greek
- language
- https
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces four versions of GreekLegalRoBERTa, large
  language models trained on Greek legal and non-legal text, which outperform existing
  models (GreekBERT, GreekLegalBERT, and GreekLegalBERT-v2) on Greek named entity
  recognition and multi-class legal topic classification tasks. The models were trained
  using RoBERTa architecture with dynamic masking and achieved improvements of up
  to 1.4 F1 points on named entity recognition and 0.89 F1 points on legal topic classification.
---

# The Large Language Model GreekLegalRoBERTa

## Quick Facts
- arXiv ID: 2410.12852
- Source URL: https://arxiv.org/abs/2410.12852
- Authors: Vasileios Saketos; Despina-Athanasia Pantazi; Manolis Koubarakis
- Reference count: 39
- Key outcome: Four versions of GreekLegalRoBERTa outperform existing models on Greek NER and legal topic classification tasks, achieving up to 1.4 F1 points improvement on NER

## Executive Summary
This paper introduces GreekLegalRoBERTa, a series of four transformer-based language models trained on Greek legal and non-legal text using RoBERTa architecture. The models are evaluated on Greek named entity recognition and multi-class legal topic classification tasks, demonstrating performance improvements over existing Greek language models including GreekBERT and GreekLegalBERT. The work addresses the challenge of processing Greek legal documents, which are primarily in unstructured text format, by developing specialized models that can handle both domain-specific legal terminology and general language patterns.

## Method Summary
The authors trained four versions of GreekLegalRoBERTa using RoBERTa architecture with dynamic masking and a 50,264 vocabulary size. The models were pretrained on different combinations of Greek legal and non-legal text corpora totaling 37.03GB, including Nomothesia platform data, Greek Parliament Proceedings, European Parliament Proceedings Parallel Corpus, European Union legislation, Raptarchis dataset, Wikipedia, and OSCAR dataset. The models were then fine-tuned on GreekLegalNER for named entity recognition and GreekLegalCode for legal topic classification, with evaluation using micro and weighted average F1 scores for NER and precision, recall, and F1 scores for classification tasks.

## Key Results
- GreekLegalRoBERTa-v4 achieved up to 1.4 F1 points improvement on Greek named entity recognition compared to previous models
- Models trained on combined legal and non-legal text showed better overall performance than those trained on legal text only
- The 50k vocabulary size provided better tokenization for Greek legal terminology compared to 35k vocabulary used in previous models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GreekLegalRoBERTa models outperform GreekBERT and GreekLegalBERT on Greek legal tasks because RoBERTa's pretraining uses dynamic masking instead of static masking, avoiding the need to duplicate training data 10 times and reducing memory requirements while maintaining or improving performance.
- Mechanism: Dynamic masking generates a new masking pattern each time a sequence is fed to the model, allowing the model to see more diverse training instances within the same computational budget. This increased diversity in masked language modeling improves the model's ability to generalize to downstream tasks like named entity recognition and legal topic classification.
- Core assumption: Dynamic masking provides sufficient variation in training signals to match or exceed the benefits of static masking with data duplication, without the increased memory overhead.
- Evidence anchors:
  - [abstract] The models were trained using RoBERTa architecture with dynamic masking and achieved improvements of up to 1.4 F1 points on named entity recognition.
  - [section] "RoBERTa uses dynamic masking. In dynamic masking, the masking pattern is generated every time we feed a sequence to the model. In the original RoBERTa paper, it was concluded that dynamic masking performs slightly better than static masking."
- Break condition: If dynamic masking fails to provide sufficient variation in training signals, or if the computational savings are negated by other factors (e.g., very large batch sizes), the performance advantage may disappear.

### Mechanism 2
- Claim: GreekLegalRoBERTa models achieve better performance on legal tasks because they are trained on a larger and more diverse corpus of Greek legal and non-legal text compared to previous models, allowing them to capture both domain-specific legal terminology and general language patterns.
- Mechanism: The pretraining corpus for GreekLegalRoBERTa includes the Nomothesia platform data (5GB), Greek Parliament Proceedings (2.7GB), Eurparl (0.38GB), Eurlex (0.41GB), Raptarchis (0.22GB), Wikipedia (1.73GB), and OSCAR (27GB), totaling 37.03GB. This diverse corpus allows the model to learn both legal-specific and general language representations, improving performance on downstream tasks that require understanding both legal context and general language.
- Core assumption: The additional non-legal text in the pretraining corpus provides useful linguistic patterns that transfer to legal tasks, rather than diluting the legal-specific knowledge.
- Evidence anchors:
  - [abstract] The models were trained on Greek legal and nonlegal text.
  - [section] "GreekLegalRoBERTa-v4: Last but not least, we utilize all the datasets we discussed in the previous models. In this model, we aim to ascertain whether incorporating both legal and non-legal contexts results in improved performance on legal tasks."
- Break condition: If the non-legal text introduces noise or conflicting patterns that interfere with legal task performance, or if the legal tasks are too specialized to benefit from general language patterns, the performance advantage may be reduced.

### Mechanism 3
- Claim: GreekLegalRoBERTa models show improved performance because they use a larger vocabulary size (50k) compared to previous Greek models (35k), allowing them to better represent the complexity of Greek legal terminology and subword units.
- Mechanism: The larger vocabulary size (50,264 vs 35,000) allows for more granular subword tokenization, which is particularly important for morphologically rich languages like Greek and for capturing the specialized terminology used in legal documents. This improved tokenization leads to better representation of legal concepts and improved performance on named entity recognition and classification tasks.
- Core assumption: The increased vocabulary size provides meaningful granularity improvements without causing overfitting or increased computational overhead that negates the benefits.
- Evidence anchors:
  - [section] "To train our encoder, we utilize a vocabulary of size 50 264. BPE training was done in the same dataset used to pretrain our model."
  - [section] "This can occur because the datasets used during pretraining contain more instances of Facilities and Locations."
- Break condition: If the increased vocabulary size leads to overfitting on the training corpus, or if the computational overhead of larger vocabulary handling negates the performance benefits, the advantage may disappear.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The models are based on RoBERTa, which is a variant of BERT built on the Transformer architecture. Understanding how self-attention works is crucial for understanding how these models process input text and generate representations.
  - Quick check question: How does the multi-head attention mechanism in Transformers allow the model to capture different types of relationships between words in a sentence?

- Concept: Masked Language Modeling (MLM) pretraining objective
  - Why needed here: Both BERT and RoBERTa use MLM as their primary pretraining objective, where the model learns to predict masked words in a sentence. Understanding this objective is key to understanding how these models acquire their language understanding capabilities.
  - Quick check question: What is the difference between static masking (used in BERT) and dynamic masking (used in RoBERTa), and how might this affect the model's learning?

- Concept: Named Entity Recognition (NER) and multi-class classification
  - Why needed here: The paper evaluates the models on two downstream tasks - NER and multi-class legal topic classification. Understanding these tasks and their evaluation metrics (F1 score, precision, recall) is essential for interpreting the results.
  - Quick check question: What is the difference between micro, macro, and weighted averaging in calculating F1 scores for NER tasks with multiple entity types?

## Architecture Onboarding

- Component map:
  - Pretraining: GreekLegalRoBERTa models use RoBERTa architecture with dynamic masking, trained on a large corpus of Greek legal and non-legal text
  - Tokenization: Byte-Pair Encoding (BPE) with vocabulary size of 50,264
  - Downstream tasks: Named Entity Recognition and multi-class legal topic classification
  - Evaluation: F1 scores (micro, macro, weighted) for NER; precision, recall, F1 for volume, chapter, and subject classification in legal topic classification

- Critical path: Pretraining → Fine-tuning → Evaluation
  - Pretraining: Train on large corpus using dynamic masking and MLM objective
  - Fine-tuning: Adapt pretrained model to specific downstream tasks using labeled data
  - Evaluation: Measure performance using appropriate metrics for each task

- Design tradeoffs:
  - Static vs dynamic masking: Dynamic masking reduces memory requirements but may provide less consistent training signals
  - Legal vs non-legal pretraining data: Including non-legal data may improve general language understanding but could dilute legal-specific knowledge
  - Vocabulary size: Larger vocabulary captures more linguistic detail but increases computational overhead

- Failure signatures:
  - Poor performance on legal tasks despite good general language understanding: May indicate insufficient legal-specific training data
  - Overfitting on pretraining data: May indicate need for regularization or data augmentation
  - Inconsistent results across different random seeds: May indicate need for more stable training procedures

- First 3 experiments:
  1. Reproduce baseline results: Fine-tune GreekLegalRoBERTa-v2 on GreekLegalNER and GreekLegalCode datasets using the same hyperparameters reported in the paper to verify performance claims
  2. Ablation study on pretraining data: Train a version of GreekLegalRoBERTa using only legal text (like GreekLegalBERT-v2) to measure the impact of including non-legal data
  3. Vocabulary size impact: Train a version with smaller vocabulary (35k like GreekBERT) to quantify the benefit of the larger vocabulary size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would incorporating additional Greek legal document sources beyond those used in GreekLegalRoBERTa-v4 (Nomothesia, Greek Parliament Proceedings, Europarl, Eurolex, Raptarchis, Wikipedia, and OSCAR) further improve performance on Greek legal NLP tasks?
- Basis in paper: [inferred] The paper shows that GreekLegalRoBERTa-v4, which incorporates the most diverse training data including both legal and non-legal sources, performs better than models trained on fewer sources. The authors suggest that combining legal and non-legal contexts improves performance.
- Why unresolved: The paper does not test whether additional legal document sources beyond those used in GreekLegalRoBERTa-v4 could further improve performance. The current model uses a wide range of sources but there may be other relevant Greek legal document repositories that were not included.
- What evidence would resolve it: Training a new version of GreekLegalRoBERTa on additional Greek legal document sources beyond those used in v4 and comparing its performance to v4 on the GreekLegalNER and GreekLegalCode tasks would provide evidence. If performance improves, it would suggest that more diverse legal sources can further enhance the model.

### Open Question 2
- Question: How would the performance of GreekLegalRoBERTa models compare to other transformer-based architectures (e.g., Longformer, BigBird) for handling longer Greek legal documents?
- Basis in paper: [inferred] The paper uses RoBERTa, which has a maximum sequence length of 512 tokens. Greek legal documents can be quite lengthy, and the authors mention that the attention mechanism's computational complexity grows quadratically with sequence length. They don't explore other architectures designed for longer sequences.
- Why unresolved: The paper only evaluates RoBERTa-based models and doesn't compare them to other transformer architectures that might be better suited for handling longer documents, such as Longformer or BigBird, which use sparse attention mechanisms.
- What evidence would resolve it: Training Longformer or BigBird models on the same Greek legal datasets and comparing their performance to GreekLegalRoBERTa on tasks involving longer legal documents would provide evidence. If these models outperform GreekLegalRoBERTa on such tasks, it would suggest that architectures with longer context windows are more effective for Greek legal NLP.

### Open Question 3
- Question: What is the impact of using different tokenization strategies (e.g., SentencePiece, WordPiece) on the performance of GreekLegalRoBERTa models for Greek legal NLP tasks?
- Basis in paper: [explicit] The paper mentions using Byte-Pair Encoding (BPE) with a vocabulary size of 50,264 for tokenizing the text. The authors note that the tokenizer inadequately represents the expanded vocabulary when incorporating both legal and non-legal contexts, leading to suboptimal performance.
- Why unresolved: The paper only uses BPE for tokenization and doesn't explore how other tokenization strategies might affect performance, especially given the unique challenges of tokenizing Greek legal text which combines formal legal language with more common vocabulary.
- What evidence would resolve it: Training GreekLegalRoBERTa models using different tokenization strategies like SentencePiece or WordPiece and comparing their performance to the BPE-based models on GreekLegalNER and GreekLegalCode tasks would provide evidence. If alternative tokenization strategies lead to better performance, it would suggest that the choice of tokenizer has a significant impact on model effectiveness for Greek legal NLP.

## Limitations

- The evaluation is limited to specific Greek legal NLP tasks and doesn't test general language understanding capabilities
- Performance improvements are relatively small (up to 1.4 F1 points) and may not be practically significant
- The paper doesn't address potential overfitting to the specific legal datasets used for evaluation

## Confidence

**High Confidence**: The architectural improvements (RoBERTa with dynamic masking, larger vocabulary size) are well-established in the literature and their benefits are theoretically sound. The claim that dynamic masking reduces memory requirements while maintaining performance is directly supported by the original RoBERTa paper and the authors' results.

**Medium Confidence**: The claim that combining legal and non-legal pretraining data improves legal task performance has some support but shows inconsistencies. While GreekLegalRoBERTa-v4 generally performs well, the expectation that larger pretraining datasets would lead to better performance was not consistently met, suggesting complex interactions between vocabulary coverage and domain adaptation.

**Low Confidence**: The claim that GreekLegalRoBERTa is "better" than previous models in general is not fully supported, as improvements are task-specific and the evaluation doesn't cover a broad range of Greek NLP tasks. The performance differences, while statistically significant, are relatively small (1.4 F1 points maximum), making it unclear whether these improvements are practically meaningful.

## Next Checks

1. **Cross-task generalization evaluation**: Test GreekLegalRoBERTa models on non-legal Greek NLP tasks (sentiment analysis, question answering, machine translation) to verify whether the legal domain training provides general language understanding benefits or creates domain-specific limitations.

2. **Pretraining data ablation with matched vocabulary**: Train versions of GreekLegalRoBERTa using only legal text but with the same 50k vocabulary size as GreekLegalRoBERTa-v4 to isolate the effects of vocabulary size from pretraining data composition on downstream task performance.

3. **Long-range dependency analysis**: Evaluate model performance on legal documents with varying lengths (beyond the 512-token limit used in training) to assess whether the models can effectively handle the longer documents typical in legal contexts, which may require different architectural considerations or windowing strategies.