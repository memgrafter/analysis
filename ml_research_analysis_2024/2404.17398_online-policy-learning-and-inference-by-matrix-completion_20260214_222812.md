---
ver: rpa2
title: Online Policy Learning and Inference by Matrix Completion
arxiv_id: '2404.17398'
source_url: https://arxiv.org/abs/2404.17398
tags: []
core_contribution: "This paper addresses the problem of online decision-making when\
  \ features are sparse and orthogonal to historical ones, formulated as a matrix\
  \ completion bandit (MCB). The authors propose a policy learning procedure combining\
  \ an \u03B5-greedy policy for decision-making with an online gradient descent algorithm\
  \ for bandit parameter estimation."
---

# Online Policy Learning and Inference by Matrix Completion

## Quick Facts
- arXiv ID: 2404.17398
- Source URL: https://arxiv.org/abs/2404.17398
- Authors: Congyuan Duan; Jingyang Li; Dong Xia
- Reference count: 40
- This paper addresses online decision-making with sparse, orthogonal features through matrix completion bandits

## Executive Summary
This paper tackles the challenge of online decision-making when features are sparse and orthogonal to historical ones by formulating it as a matrix completion bandit (MCB) problem. The authors propose an algorithm combining ε-greedy exploration with online gradient descent for parameter estimation, along with a two-phase design to balance learning accuracy and regret performance. For policy inference, they develop an inverse propensity weighting (IPW) based debiasing method with established asymptotic normality. The methods are validated on San Francisco parking pricing data, demonstrating superior performance compared to benchmark policies.

## Method Summary
The paper introduces a matrix completion bandit framework where expected rewards under each arm are characterized by unknown low-rank matrices. The approach combines ε-greedy exploration with online gradient descent to learn these parameters efficiently. A two-phase design is employed where faster decaying exploration probability yields smaller regret but less accurate policy learning. For inference, an online IPW debiasing method corrects for adaptive sampling, enabling valid statistical inference even with non-i.i.d. data. The algorithm is applied to real-world parking pricing data, revealing practical insights and outperforming benchmark approaches.

## Key Results
- The proposed MCB framework successfully handles sparse, orthogonal features through low-rank matrix completion
- The ε-greedy online gradient descent algorithm achieves efficient parameter learning with controlled regret
- IPW-based debiasing enables valid statistical inference in adaptive sampling settings
- Real-world application to SFpark parking pricing demonstrates practical utility and superior performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The online gradient descent algorithm with ε-greedy exploration can learn low-rank matrix parameters efficiently even when covariates are sparse and orthogonal to historical ones.
- Mechanism: The algorithm updates reward matrix estimates via gradient descent using observed rewards and feature matrices. ε-greedy exploration balances learning accuracy and regret by exploring suboptimal arms with probability ε_t and exploiting the best estimate with probability 1 - ε_t.
- Core assumption: Underlying reward matrices are low-rank with sub-Gaussian noise and incoherent initial estimates.
- Evidence anchors: [abstract] "The ε-greedy bandit and the online gradient descent algorithm are explored." [section] "We propose an online gradient descent algorithm with an ε-greedy bandit strategy to learn the MCB parameters."
- Break condition: Algorithm may not converge if initial estimates are too far from true matrices or if exploration probability decays too quickly.

### Mechanism 2
- Claim: Inverse propensity weighting (IPW) can debias estimators constructed from adaptively collected data in matrix completion bandits.
- Mechanism: IPW corrects for different sampling probabilities in non-i.i.d. data by weighting each observation by the inverse of its selection probability, reducing estimator bias and enabling valid statistical inference.
- Core assumption: Arm optimality sets can be accurately identified from historical data.
- Evidence anchors: [abstract] "For policy inference, we develop an online debiasing method based on inverse propensity weighting and establish its asymptotic normality." [section] "The crucial ingredient is to include the inverse propensity weight (IPW)."
- Break condition: IPW variance may become too large for valid inference if exploration probability is too small or if arm optimality sets are not well-defined.

### Mechanism 3
- Claim: The matrix completion bandit formulation allows for collaborative filtering policy learning when features are sparse and orthogonal to historical ones.
- Mechanism: By assuming low-dimensional latent features, the problem is formulated as a matrix completion bandit where expected reward under each arm is characterized by an unknown low-rank matrix learned from observed noisy entries.
- Core assumption: Reward matrices are low-rank and observed entries are noisy versions of true entries.
- Evidence anchors: [abstract] "We formulate the problem as a matrix completion bandit (MCB), where the expected reward under each arm is characterized by an unknown low-rank matrix." [section] "The fundamental assumption is that both matrices M0 and M1 are low-rank."
- Break condition: Algorithm may not converge to true parameters if reward matrices are not low-rank or if noise is too large.

## Foundational Learning

- Concept: Martingale techniques for online learning and inference
  - Why needed here: Observations arrive sequentially and depend on historical data, making them neither independent nor identically distributed. Martingale techniques enable valid statistical inference in this setting.
  - Quick check question: What is the difference between a martingale and a submartingale? How are they used in online learning?

- Concept: Low-rank matrix completion and estimation
  - Why needed here: The matrix completion bandit formulation relies on low-rank reward matrix assumptions. Knowledge of matrix completion techniques is essential for understanding algorithm convergence and statistical properties.
  - Quick check question: What are the key assumptions in low-rank matrix completion? How do they differ from traditional matrix completion?

- Concept: Inverse propensity weighting and debiasing
  - Why needed here: IPW-based debiasing corrects for different sampling probabilities in non-i.i.d. data. Understanding IPW is crucial for valid statistical inference in matrix completion bandits.
  - Quick check question: What is the intuition behind inverse propensity weighting? How does it differ from reweighting by the marginal probability?

## Architecture Onboarding

- Component map: Data (context, action, reward tuples) -> Algorithm (online gradient descent with ε-greedy) -> Estimation (low-rank matrix estimation) -> Inference (IPW debiasing and asymptotic normality) -> Evaluation (regret analysis and real data application)

- Critical path: 1) Initialize low-rank estimates of reward matrices 2) At each time step, observe context and choose action via ε-greedy 3) Update estimates via online gradient descent 4) Debias estimates via IPW 5) Perform inference on optimal policy

- Design tradeoffs: Exploration vs. exploitation (balancing learning accuracy and regret performance), computational efficiency vs. statistical accuracy (online vs. offline estimation), bias vs. variance (IPW debiasing reduces bias but may increase variance)

- Failure signatures: Algorithm not converging (initial estimates too far from true matrices, exploration probability decaying too quickly), invalid inference (exploration probability too small, arm optimality sets not well-defined), poor regret performance (insufficient exploration, reward matrices not low-rank)

- First 3 experiments: 1) Synthetic data with known low-rank matrices and varying noise levels to evaluate convergence and statistical accuracy 2) Synthetic data with sparse and orthogonal contexts to evaluate performance compared to traditional contextual bandits 3) Real data application (e.g., SFpark parking pricing) to evaluate practical performance and policy implications

## Open Questions the Paper Calls Out
None

## Limitations
- The algorithm relies heavily on the low-rank assumption for reward matrices, which may not hold in all practical settings
- The two-phase design introduces complexity in balancing exploration and exploitation, with optimal hyperparameter tuning not fully characterized
- The asymptotic normality of the IPW estimator depends on correct specification of arm optimality sets, which may be challenging to identify in practice

## Confidence

**High confidence**: Matrix completion bandit formulation and online gradient descent for parameter estimation are well-established techniques. Asymptotic normality of IPW estimator under stated assumptions is theoretically sound.

**Medium confidence**: Two-phase design for balancing policy learning accuracy and regret performance is novel but requires careful tuning. Practical performance on real data is promising but may not generalize to all settings.

**Low confidence**: Paper does not fully address computational complexity of the algorithm or provide guidelines for hyperparameter tuning in practice.

## Next Checks
1. Conduct thorough sensitivity analysis of algorithm's performance to exploration probability decay rate and other hyperparameters
2. Evaluate algorithm's performance on synthetic data with non-low-rank reward matrices to assess robustness of low-rank assumption
3. Apply algorithm to additional real-world datasets to validate practical utility and generalizability beyond SFpark parking pricing application