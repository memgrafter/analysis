---
ver: rpa2
title: A Closer Look at the Limitations of Instruction Tuning
arxiv_id: '2402.05119'
source_url: https://arxiv.org/abs/2402.05119
tags:
- llama-2
- just-eval-instruct1k
- alpaca
- instruction
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates limitations of instruction tuning (IT)
  for aligning large language models (LLMs) to follow open-domain instructions. Through
  extensive experiments across various LLMs, datasets, and training paradigms, the
  authors find that IT does not enhance knowledge or skills in models - LoRA fine-tuning
  only learns response initiation while full-parameter fine-tuning leads to knowledge
  degradation and increased hallucination.
---

# A Closer Look at the Limitations of Instruction Tuning

## Quick Facts
- **arXiv ID**: 2402.05119
- **Source URL**: https://arxiv.org/abs/2402.05119
- **Reference count**: 40
- **Key outcome**: Instruction tuning does not enhance knowledge or skills in models - LoRA fine-tuning only learns response initiation while full-parameter fine-tuning leads to knowledge degradation and increased hallucination.

## Executive Summary
This paper investigates the fundamental limitations of instruction tuning (IT) for aligning large language models (LLMs) to follow open-domain instructions. Through extensive experiments across various LLMs, datasets, and training paradigms, the authors find that IT does not enhance knowledge or skills in models. LoRA fine-tuning learns only response initiation and style tokens while relying on pre-trained knowledge, whereas full-parameter fine-tuning leads to knowledge degradation and increased hallucination. The study also reveals that pattern copying from IT datasets often hurts performance, and various proposed methods to improve IT do not lead to performance improvements over simple LoRA fine-tuned models.

## Method Summary
The study uses LLaMa-2 models (7B, 13B, 70B), Mistral-v0.1 7B, and Phi-1.5 1.3B fine-tuned with various IT datasets including Alpaca52k, MedInstruct 52k, LIMA 1k, databricks-dolly 15k, and Tulu-V2-Mix 326k. Models are fine-tuned using either LoRA or full-parameter fine-tuning for 3 epochs with learning rate 5e-5 and batch size 32. Evaluation uses the just-eval-instruct 1k dataset with both expert human evaluation and GPT-4-based multi-aspect evaluation covering helpfulness, clarity, factuality, depth, and engagement.

## Key Results
- LoRA fine-tuned models consistently outperform full-parameter fine-tuned models across multiple evaluation metrics
- Full-parameter fine-tuning increases hallucination by borrowing tokens from conceptually similar IT instances
- Pattern copying from IT datasets derived from knowledgeable sources leads to decline in response quality
- Various proposed improvement methods (NEFTune, dataset filtering, WizardLM) do not improve performance over simple LoRA fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA fine-tuning preserves pre-trained knowledge better than full-parameter fine-tuning
- Mechanism: LFT only modifies low-rank matrices approximating weight updates, leaving most pre-trained parameters unchanged, resulting in minimal token distribution shift
- Core assumption: Majority of model's factual knowledge resides in pre-trained parameters
- Evidence anchors: Abstract findings, token distribution analysis section
- Break condition: If LoRA matrices capture substantial semantic shifts or if pre-trained knowledge is not localized in specific parameter subsets

### Mechanism 2
- Claim: SFT increases hallucination by borrowing tokens from conceptually similar IT instances
- Mechanism: Causal analysis shows novel tokens in hallucinated responses often originate from IT instances describing similar concepts
- Core assumption: IT dataset contains overlapping semantic concepts that model incorrectly associates during generation
- Evidence anchors: Abstract findings, causal analysis section
- Break condition: If semantic similarity does not correlate with hallucination instances or attention mechanism prevents cross-instance borrowing

### Mechanism 3
- Claim: Pattern copying from IT datasets degrades response quality
- Mechanism: SFT causes model to mimic response styles from IT dataset, leading to verbose responses and hallucinations when knowledge is insufficient
- Core assumption: IT dataset stylistic characteristics directly influence model response style
- Evidence anchors: Abstract findings, style imitation analysis
- Break condition: If response style is not influenced by IT dataset patterns or simplification does not reduce hallucinations

## Foundational Learning

- **Token distribution shift analysis**
  - Why needed here: Critical for diagnosing knowledge retention and hallucination
  - Quick check question: What does high KL divergence between base and fine-tuned model token distributions indicate?

- **Causal analysis in model responses**
  - Why needed here: Essential for tracing hallucinated tokens to specific IT instances
  - Quick check question: How can semantic similarity between hallucinated phrases and IT dataset instances help determine causal relationships?

- **Pattern copying and style imitation**
  - Why needed here: Important for understanding response quality degradation
  - Quick check question: What is the difference between tone imitation and style imitation in instruction tuning?

## Architecture Onboarding

- **Component map**: Base pre-trained LLM → LoRA or SFT fine-tuning → Evaluation on instruction-response pairs → Token distribution and hallucination analysis
- **Critical path**: Pre-training → Fine-tuning (LoRA/SFT) → Response generation → Evaluation (human/GPT-4) → Analysis (token distribution, causal, pattern copying)
- **Design tradeoffs**: LoRA preserves knowledge but may limit adaptation; SFT enables adaptation but risks knowledge degradation and hallucinations; dataset complexity affects response quality
- **Failure signatures**: High KL divergence indicates knowledge degradation; hallucinations traceable to IT dataset suggest causal borrowing; verbose responses with factual errors indicate pattern copying issues
- **First 3 experiments**:
  1. Compare token distribution shifts between LoRA and SFT fine-tuned models on same IT dataset
  2. Perform causal analysis on hallucinated responses to trace token origins in IT dataset
  3. Simplify IT dataset responses and evaluate hallucination reduction in SFT models

## Open Questions the Paper Calls Out

# Open Question 1
- Question: How does LoRA fine-tuning learn to initiate responses while relying primarily on pre-trained knowledge?
- Basis in paper: Explicit
- Why unresolved: Paper demonstrates this behavior but does not explain mechanism
- What evidence would resolve it: Analysis of LoRA weight matrices to understand response initiation facilitation

# Open Question 2
- Question: Why does full-parameter fine-tuning lead to knowledge degradation and increased hallucination?
- Basis in paper: Explicit
- Why unresolved: Paper shows correlation but does not explain underlying causes
- What evidence would resolve it: Investigation of how full-parameter fine-tuning affects attention mechanisms and knowledge representation

# Open Question 3
- Question: How can we develop instruction tuning methods that enhance knowledge without causing degradation or hallucination?
- Basis in paper: Explicit
- Why unresolved: Paper demonstrates existing methods fail but does not propose solutions
- What evidence would resolve it: Development and evaluation of new instruction tuning methods that successfully enhance knowledge

## Limitations

- Causal analysis of hallucination sources lacks validation through ablation studies or alternative attribution methods
- Pattern copying mechanism is primarily observational without systematic quantification of stylistic influence
- Evidence for mechanisms remains largely indirect, relying on distributional analysis rather than behavioral validation

## Confidence

- **High confidence**: LoRA fine-tuning performs comparably or better than full-parameter fine-tuning on instruction-following tasks
- **Medium confidence**: Mechanism of LoRA preserving pre-trained knowledge through minimal parameter modification
- **Low confidence**: Causal attribution of hallucinations to concept borrowing from similar IT instances, and pattern copying hypothesis

## Next Checks

1. Conduct ablation studies comparing fine-tuning on semantically similar vs. dissimilar IT instances to test concept borrowing hypothesis for hallucinations
2. Perform systematic analysis of response style similarity between IT datasets and fine-tuned models using linguistic feature extraction to quantify pattern copying effects
3. Test whether observed knowledge preservation in LoRA models holds when fine-tuning on datasets with different knowledge distributions than pre-training corpus