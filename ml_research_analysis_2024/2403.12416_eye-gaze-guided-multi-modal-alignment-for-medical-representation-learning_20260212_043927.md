---
ver: rpa2
title: Eye-gaze Guided Multi-modal Alignment for Medical Representation Learning
arxiv_id: '2403.12416'
source_url: https://arxiv.org/abs/2403.12416
tags:
- data
- eye-gaze
- image
- text
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Eye-gaze Guided Multi-modal Alignment
  (EGMA) framework to enhance alignment of medical visual and textual features using
  radiologists' eye-gaze data. The framework processes synchronized eye-gaze, image,
  and transcript data to generate attention heatmaps that reflect radiologists' diagnostic
  focus.
---

# Eye-gaze Guided Multi-modal Alignment for Medical Representation Learning

## Quick Facts
- arXiv ID: 2403.12416
- Source URL: https://arxiv.org/abs/2403.12416
- Reference count: 40
- Primary result: Eye-gaze guided framework improves medical image-text alignment with 3.9% improvement in image-to-text retrieval and 19.75% in text-to-image retrieval

## Executive Summary
This paper introduces the Eye-gaze Guided Multi-modal Alignment (EGMA) framework to enhance alignment of medical visual and textual features using radiologists' eye-gaze data. The framework processes synchronized eye-gaze, image, and transcript data to generate attention heatmaps that reflect radiologists' diagnostic focus. It introduces two novel modules: Eye-gaze Guided Fine-grained (EGF) alignment for optimizing local feature relationships between image patches and text tokens, and Eye-gaze Guided cross-model Mapping (EGM) for bidirectional feature mapping between modalities. EGMA was evaluated on four medical datasets, achieving state-of-the-art performance in supervised and zero-shot classification tasks.

## Method Summary
EGMA processes synchronized eye-gaze, image, and transcript data to generate attention heatmaps reflecting radiologists' diagnostic focus. The framework includes two key modules: EGF alignment optimizes local feature relationships between image patches and text tokens using gaze-weighted similarity matrices, while EGM enables bidirectional feature mapping between modalities guided by eye-gaze data. The method uses Swin Transformer for image encoding and BioClinicalBERT for text encoding, with pre-training on 50 epochs followed by fine-tuning on 30 epochs for classification tasks.

## Key Results
- Achieved state-of-the-art performance in supervised and zero-shot classification tasks
- 3.9% improvement in image-to-text retrieval and 19.75% in text-to-image retrieval
- Small amounts of eye-gaze data (5% of training set) significantly improved model performance
- Superior generalization and feature representation capabilities demonstrated through better clustering in t-SNE visualizations

## Why This Works (Mechanism)

### Mechanism 1
Eye-gaze data provides localized, radiologist-specific attention cues that align naturally with diagnostic text, improving cross-modal alignment. By converting synchronized gaze data into attention heatmaps and using these to weight similarity matrices between image patches and text tokens, the model learns to focus on diagnostically relevant image regions in alignment with associated textual descriptions. Core assumption: Radiologists' eye-gaze patterns during diagnosis are meaningful indicators of diagnostic attention that correlate with text semantics.

### Mechanism 2
Fine-grained patch-to-sentence similarity matrices weighted by gaze data improve local feature alignment more effectively than global contrastive loss alone. The model computes fine-grained similarities between individual image patches and text tokens, then applies gaze-derived weights to emphasize semantically related pairs, refining alignment beyond global instance-level similarity. Core assumption: Local feature relationships between image patches and text tokens are more informative for medical diagnosis than global features alone.

### Mechanism 3
Bidirectional cross-modal mapping guided by eye-gaze data improves the model's ability to map between modalities even for unseen samples. The model uses gaze-weighted similarity matrices to map text features to image feature space and vice versa, creating bidirectional alignment that enhances zero-shot retrieval performance. Core assumption: The gaze-guided cross-mapping creates more robust feature space that generalizes to unseen samples.

## Foundational Learning

- **Multi-modal contrastive learning**: Framework builds upon contrastive learning principles to align image and text features, extending them with gaze-guided refinement. Why needed: Core objective is to align image and text features in shared embedding space. Quick check: What is the core objective of contrastive learning in multi-modal settings?

- **Vision Transformers (ViT)**: Image encoder uses ViT-based architecture (Swin Transformer) to extract patch-level features that can be aligned with text tokens. Why needed: ViT provides patch-level features suitable for fine-grained alignment with text tokens. Quick check: How does a Vision Transformer differ from a CNN in processing image patches?

- **Attention mechanisms and heatmaps**: Framework converts eye-gaze data into attention heatmaps that weight alignment between image regions and text tokens. Why needed: Eye-gaze data must be transformed into usable format for attention-based alignment. Quick check: How can eye-gaze data be transformed into format usable for attention-based alignment?

## Architecture Onboarding

- **Component map**: Image encoder (Swin Transformer) → Patch features → EGF module → EGM module → Contrastive loss → Parameter updates
  Text encoder (BioClinicalBERT) → Sentence features → EGF module → EGM module → Contrastive loss → Parameter updates
  Gaze processing module → Attention heatmaps → EGF module → EGM module → Contrastive loss → Parameter updates

- **Critical path**: Image/Text encoding → Gaze processing → EGF alignment → EGM mapping → Loss computation → Parameter updates

- **Design tradeoffs**: Using eye-gaze data vs. manual annotations (eye-gaze is cheaper but potentially noisier); Fine-grained vs. global alignment (fine-grained is more precise but computationally heavier); Bidirectional mapping vs. unidirectional (bidirectional provides better alignment but requires more computation)

- **Failure signatures**: Poor performance on zero-shot tasks may indicate ineffective gaze-guided alignment; Degraded performance with small gaze data proportions suggests over-reliance on gaze signals; Failure to improve over baseline on certain datasets may indicate domain-specific limitations

- **First 3 experiments**: 1) Baseline vs. EGF-only: Test if fine-grained alignment improves performance without cross-mapping; 2) EGF-only vs. EGM-only: Compare effectiveness of fine-grained alignment vs. cross-mapping; 3) Varying gaze data proportions: Test performance with 1%, 5%, 10%, and 50% of gaze data to find optimal ratio

## Open Questions the Paper Calls Out
None

## Limitations
- Dependency on synchronized eye-gaze data requiring specialized equipment and potential quality consistency issues across radiologists and modalities
- Computational overhead from EGF and EGM modules may challenge deployment in resource-constrained clinical settings
- Effectiveness may be limited to radiology domains where gaze patterns correlate strongly with diagnostic focus

## Confidence
**High Confidence**: Eye-gaze guided multi-modal alignment improves medical representation learning, well-supported by experimental results across four datasets.
**Medium Confidence**: Small amounts of eye-gaze data (5%) can provide significant improvements, supported by ablation studies but optimal proportion may vary.
**Low Confidence**: EGMA provides superior generalization capabilities compared to existing methods, primarily supported by qualitative t-SNE visualizations rather than quantitative evidence.

## Next Checks
1. **Cross-domain generalization test**: Evaluate EGMA on non-radiology medical imaging datasets (histopathology, dermatology) to verify if eye-gaze guided alignment provides similar benefits across different medical specialties and imaging modalities.

2. **Gaze data quality sensitivity analysis**: Systematically vary quality and resolution of eye-gaze data (lower sampling rates, simulated noisy gaze patterns) to determine minimum viable gaze data quality required for meaningful improvements.

3. **Clinical deployment feasibility study**: Measure computational overhead and inference latency of EGMA compared to baseline methods, and assess whether performance gains justify additional computational costs in real-world clinical settings with resource constraints.