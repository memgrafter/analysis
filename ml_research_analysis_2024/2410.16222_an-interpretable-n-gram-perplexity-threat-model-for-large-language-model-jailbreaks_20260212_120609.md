---
ver: rpa2
title: An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks
arxiv_id: '2410.16222'
source_url: https://arxiv.org/abs/2410.16222
tags:
- attacks
- uni00000015
- uni00000014
- uni00000045
- perplexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an interpretable N-gram language model (LM)
  perplexity threat model for benchmarking jailbreak attacks on large language models
  (LLMs). By constructing an N-gram LM on 1 trillion tokens, the proposed threat model
  enables LLM-agnostic, nonparametric, and interpretable evaluation of attack fluency.
---

# An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks

## Quick Facts
- arXiv ID: 2410.16222
- Source URL: https://arxiv.org/abs/2410.16222
- Authors: Valentyn Boreiko; Alexander Panfilov; Vaclav Voracek; Matthias Hein; Jonas Geiting
- Reference count: 40
- Key outcome: Adaptive discrete optimization-based attacks (PRS, GCG) significantly outperform recent LLM-based attacks under N-gram perplexity constraints, achieving higher attack success rates despite lower perplexity.

## Executive Summary
This work introduces an interpretable N-gram language model (LM) perplexity threat model for benchmarking jailbreak attacks on large language models (LLMs). By constructing an N-gram LM on 1 trillion tokens, the proposed threat model enables LLM-agnostic, nonparametric, and interpretable evaluation of attack fluency. Popular jailbreak attacks are adapted to this model, and their performance is benchmarked on open-source models across various sizes and generations. The results show that adaptive discrete optimization-based attacks significantly outperform recent LLM-based attacks when constrained by N-gram perplexity, achieving higher attack success rates despite lower perplexity. Additionally, the interpretability of the N-gram LM allows for fine-grained analysis of attacks and models, revealing that effective attacks exploit infrequent bigrams, either absent from real-world text or specific to certain domains like Reddit or code datasets.

## Method Summary
The method constructs a bigram LM from 1 trillion tokens of the Dolma dataset, then builds a perplexity filter with threshold γ0.999 = 38,276 for 99.9% true positive rate. Five popular jailbreak attacks (GCG, PRS, AutoDan, BEAST, PAIR) are adapted to incorporate this N-gram perplexity constraint. The attacks are evaluated on open-source models (Llama2-7b, Llama2-13b, Llama3-8b, Llama3.1-8b, Llama3.2-1b, Llama3.2-3b, Gemma-7b, Gemma2-2b, Starling-7b-α, Vicuna-13b) using a Llama2-13B judge model, measuring Attack Success Rate (ASR) and total FLOPs.

## Key Results
- Adaptive attacks (PRS, GCG) achieve 3.8× higher ASR than LLM-based attacks under N-gram PPL constraint
- Attacks exploiting infrequent bigrams (Reddit, code domains) show highest ASR
- N-gram PPL filter generalizes well to morphologically simple languages (0.0-37.7% rejection) but poorly to complex ones (51.7-68.7% rejection)

## Why This Works (Mechanism)

### Mechanism 1
The N-gram LM perplexity filter effectively discriminates natural text from adversarial jailbreak inputs by computing sequence probability using frequency counts from a large corpus (1T tokens) with Laplacian smoothing. Low-probability N-grams indicate unnatural text, raising perplexity above the chosen threshold. Core assumption: Text used in benign prompts follows the distribution learned from the training corpus, while adversarial jailbreaks introduce rare or unseen N-grams.

### Mechanism 2
Adaptive attacks optimized against the N-gram LM filter maintain higher ASR than non-adaptive attacks by incorporating the filter into their optimization loop, rejecting candidates that exceed the perplexity threshold. This forces them to find jailbreaks that are both fluent and harmful. Core assumption: The filter is fixed and known to the attacker, allowing optimization to avoid high-perplexity regions while still finding successful jailbreaks.

### Mechanism 3
Interpretability of the N-gram LM enables fine-grained analysis of attack success and model robustness by tracing each N-gram's contribution to perplexity back to its frequency in the training corpus. This reveals which linguistic patterns attacks exploit (e.g., rare Reddit or code-specific bigrams). Core assumption: The training corpus accurately represents the distribution of text the models were trained on, so bigram frequencies reflect model exposure.

## Foundational Learning

- Concept: N-gram language models and perplexity calculation
  - Why needed here: The threat model is built on N-gram perplexity; understanding how it measures text fluency is essential to grasp the filter's operation.
  - Quick check question: Given a bigram LM with counts C("the cat")=50 and C("the")=200, what is P("cat"|"the") with Laplacian smoothing?

- Concept: Threat model formalization in security
  - Why needed here: The paper defines a specific threat model (chat scenario with N-gram PPL constraint) that shapes how attacks are evaluated and compared.
  - Quick check question: In the described threat model, what three constraints must an attacker satisfy to succeed?

- Concept: Adaptive optimization under constraints
  - Why needed here: Adaptive attacks must optimize for both jailbreak success and perplexity compliance, requiring knowledge of constraint integration.
  - Quick check question: In adaptive PRS, what happens when a sampled substitution fails the N-gram PPL filter?

## Architecture Onboarding

- Component map: Training corpus → N-gram LM construction → Perplexity filter (threshold γ) → Threat model T → Adaptive attacks (PRS, GCG, etc.) → Judge model → ASR evaluation
- Critical path: 1) Build bigram LM on 1T tokens, 2) Select γ0.999 for 99.9% TPR on validation data, 3) Implement filter in attack optimization loop, 4) Evaluate ASR with judge model
- Design tradeoffs: N-gram depth (2 vs 3+) affects sparsity and separation; window size W balances separation vs computation; filter threshold trades utility vs robustness
- Failure signatures: ASR ≈ 0 despite successful baseline attacks → filter too strict; ASR unchanged with filter → filter ineffective or attacks bypass it; Judge disagreements → evaluation inconsistency
- First 3 experiments: 1) Measure perplexity distribution on benign prompts vs known adversarial suffixes; verify separation, 2) Run adaptive PRS with N-gram filter enabled; record ASR and FLOPs vs baseline, 3) Perform dataset attribution on successful jailbreaks; identify which corpus domains contribute most frequent bigrams

## Open Questions the Paper Calls Out

### Open Question 1
How can the computational cost of adaptive attacks against LLM-based perplexity defenses (self-perplexity) be reduced while maintaining attack success rates? The paper notes that self-perplexity attacks require significantly higher FLOPs (2.34 × 10^17 vs. 5.17 × 10^16 for N-gram) while achieving the same ASR, but does not propose specific solutions to reduce this computational burden.

### Open Question 2
What specific features or patterns in jailbreak attacks allow them to transfer effectively across different model architectures and safety training procedures, particularly when adaptive attacks are used? While the paper demonstrates successful transfer, it only speculates that adaptive attacks might discover "spurious features that generalize well" without identifying the specific characteristics of these features or why they transfer across models with different training regimes.

### Open Question 3
How does the choice of tokenizer affect the effectiveness of N-gram-based perplexity filters and jailbreak attacks across different languages, particularly for morphologically complex languages? The paper identifies that English-centric tokenizers severely affect language modeling performance for morphologically rich languages but does not explore how different tokenization strategies would affect filter performance or attack success.

## Limitations
- Limited to 300 malicious queries from HarmBench, excluding copyright-related behaviors
- Focuses on open-source models (1B-70B parameters), uncertain performance on frontier models
- FLOPs metric may not fully capture practical constraints attackers face in real-world scenarios

## Confidence
- High confidence: Interpretability of N-gram LM contributions and mechanism of adaptive attacks incorporating filter into optimization loops
- Medium confidence: Comparative performance of adaptive discrete optimization attacks versus LLM-based attacks under N-gram perplexity constraint
- Low confidence: Generalization of findings to frontier models, alternative threat models, and attacks that might bypass the N-gram filter through semantic rather than syntactic manipulation

## Next Checks
1. Conduct systematic comparison between Dolma's bigram distribution and actual training corpora of evaluated models to quantify attribution reliability
2. Evaluate adaptive attacks against a held-out set of 100 novel malicious prompts not seen during development, measuring both ASR and perplexity distributions
3. Systematically relax the N-gram PPL threshold from γ0.999 downward in 10% increments, measuring resulting ASR and identifying point where filter effectiveness breaks down