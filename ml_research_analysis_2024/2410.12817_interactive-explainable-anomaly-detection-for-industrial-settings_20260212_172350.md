---
ver: rpa2
title: Interactive Explainable Anomaly Detection for Industrial Settings
arxiv_id: '2410.12817'
source_url: https://arxiv.org/abs/2410.12817
tags:
- dataset
- anomaly
- learning
- interactive
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of explainable anomaly detection
  in industrial settings, focusing on visual defect recognition in RGB images. The
  authors propose a method that combines CNN-based classification models with an enhanced
  model-agnostic explanation algorithm (InvRISE) and an interactive interface for
  human feedback.
---

# Interactive Explainable Anomaly Detection for Industrial Settings

## Quick Facts
- arXiv ID: 2410.12817
- Source URL: https://arxiv.org/abs/2410.12817
- Authors: Daniel Gramelt; Timon Höfer; Ute Schmid
- Reference count: 40
- Primary result: NearCAIPI framework with interactive explainability outperforms random addition and active learning baselines on industrial anomaly detection tasks

## Executive Summary
This work addresses the challenge of explainable anomaly detection in industrial settings, focusing on visual defect recognition in RGB images. The authors propose a method that combines CNN-based classification models with an enhanced model-agnostic explanation algorithm (InvRISE) and an interactive interface for human feedback. The key innovation is the NearCAIPI framework, which improves AI through user interaction by incorporating the concept of near hits and misses. The approach was tested on two datasets: a newly introduced dataset of welding seams and a casting product dataset. Results show that the proposed method outperforms random addition and active learning baselines, with NearCAIPI achieving the highest accuracy and F1 scores. The work demonstrates how interactive explainability can increase model trustworthiness and performance in industrial quality assurance applications.

## Method Summary
The method combines CNN-based classification models with an enhanced explanation algorithm (InvRISE) and an interactive interface for human feedback. The core innovation is the NearCAIPI framework, which extends the CAIPI algorithm by using RISE instead of LIME, introducing additional user feedback for correct predictions with wrong explanations, and incorporating near hits and misses. The process involves training a CNN on industrial images, generating explanations using InvRISE, presenting predictions and explanations to human experts for correction, generating refutations (augmented versions of corrected instances), incorporating near hits and misses using feature embeddings, and retraining iteratively. The approach was tested on two datasets: a newly introduced welding seam dataset and a casting product dataset, using metrics such as accuracy, F1-score, Matthews Correlation Coefficient, Dice coefficient, and Jaccard index.

## Key Results
- NearCAIPI framework achieved the highest accuracy and F1 scores compared to random addition, active learning, and CAIPI baselines
- The InvRISE explanation method successfully highlighted anomaly-relevant pixels rather than normal object features
- Incorporating near hits and misses improved the model's ability to learn from human corrections systematically

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The InvRISE explanation method works better than standard RISE for industrial anomaly detection by inverting the saliency computation to focus on anomaly-relevant pixels rather than normal object features.
- Mechanism: Instead of masking pixels and measuring confidence drop for the target class (as RISE does), InvRISE measures confidence increase for the non-target class when pixels are masked. This shift aligns the explanation with the industrial defect detection task where anomalies are defined by absence of normal patterns rather than presence of specific features.
- Core assumption: Industrial anomaly detection models learn to identify anomalies through negative evidence (what's missing from normal patterns) rather than positive evidence of defect features.
- Evidence anchors: [abstract] "this introduces new problems, e.g. while blackening the pixel area of a dog in the image would result in not classifying the image as a dog anymore, in our scenario, by blackening the area of the scratches on a welding seam, we would still expect the model to classify the image as an anomaly"

### Mechanism 2
- Claim: The NearCAIPI framework improves model performance by incorporating human feedback on both the initial instance and its near hits/misses, creating a richer training signal.
- Mechanism: When a human corrects a prediction or explanation, the system finds similar instances (near hits/misses) using feature embeddings and presents them for correction as well. This expands the correction scope beyond the immediate instance, capturing systematic patterns in the model's errors.
- Core assumption: Errors in anomaly detection are often systematic and affect groups of similar instances, not just isolated cases.
- Evidence anchors: [abstract] "we modified the CAIPI algorithm by using RISE [24] instead of LIME [27], introducing additional user feedback in case of correct prediction but wrong explanation [34], and incorporating near hits and misses [8] into the CAIPI algorithm"

### Mechanism 3
- Claim: The interactive pipeline increases model trustworthiness by providing explanations that are both locally accurate and globally consistent with human understanding of anomalies.
- Mechanism: By allowing human experts to correct both predictions and explanations, the system iteratively refines its understanding of what constitutes relevant anomaly evidence. The combination of InvRISE explanations and human correction creates a feedback loop that aligns the model's reasoning with domain expertise.
- Core assumption: Human experts can reliably identify when an explanation correctly highlights the relevant anomaly features, even when the overall prediction is correct.
- Evidence anchors: [abstract] "the further development of a model-agnostic explanation algorithm for black-box classifiers...we demonstrate how we can establish an interactive interface that allows users to further correct the model's output"

## Foundational Learning

- Concept: Feature embedding similarity and cosine distance
  - Why needed here: The near hits/misses mechanism relies on finding similar instances using feature embeddings from the second-to-last layer of the neural network
  - Quick check question: If two images have feature embeddings of [0.5, 0.2, 0.8] and [0.4, 0.3, 0.7], what is their cosine similarity?

- Concept: Interactive machine learning and active learning
  - Why needed here: The NearCAIPI framework builds on CAIPI's interactive learning approach, extending it with near hits/misses
  - Quick check question: How does active learning differ from passive learning in terms of which data points are selected for labeling?

- Concept: Anomaly detection as binary classification vs. segmentation
  - Why needed here: The paper chooses binary classification over segmentation for anomaly detection, which influences the explanation method design
  - Quick check question: What are the advantages and disadvantages of treating anomaly detection as a binary classification problem versus a segmentation problem?

## Architecture Onboarding

- Component map:
  - CNN backbone (ResNet variants) for classification
  - InvRISE explanation module for anomaly-relevant saliency maps
  - Feature extraction module (second-to-last layer) for similarity computation
  - Interactive interface for human feedback collection
  - Training data augmentation module (refutations generation)
  - Near hits/misses selection module using cosine similarity

- Critical path: Image → CNN prediction → InvRISE explanation → Human correction → Refutations generation → Training data augmentation → Retraining

- Design tradeoffs: Binary classification vs. segmentation (simplicity and generalization vs. precise localization), InvRISE vs. standard RISE (task-specific vs. general applicability), near hits/misses vs. random sampling (systematic error correction vs. exploration)

- Failure signatures: Explanations highlighting normal features instead of anomalies, near hits/misses not being perceptually similar, human feedback not improving performance over iterations, feature embedding space not capturing anomaly-relevant similarity

- First 3 experiments:
  1. Compare InvRISE explanations against ground truth annotations on the welding dataset using Dice and Jaccard coefficients
  2. Test near hits/misses selection by manually verifying if the top-5 nearest neighbors are actually similar in anomaly-relevant features
  3. Run a small-scale interactive session with a domain expert to validate the correction workflow and measure explanation accuracy improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of NearCAIPI compare to other interactive learning methods in real-world industrial settings beyond the controlled datasets used in the paper?
- Basis in paper: [explicit] The paper compares NearCAIPI to random addition, active learning, and CAIPI baselines on two datasets (welding seams and casting products) but does not address real-world deployment.
- Why unresolved: The experiments are conducted in a simulated environment with labeled interactive datasets, which may not reflect the complexities and noise of actual industrial quality assurance processes.
- What evidence would resolve it: A field study deploying NearCAIPI in an actual production line with human experts interacting with the system in real-time, measuring performance, trust, and usability metrics.

### Open Question 2
- Question: What is the impact of the near hits and misses component on the model's ability to generalize to unseen defect types or variations in the welding seam dataset?
- Basis in paper: [explicit] The paper introduces near hits and misses as an extension to CAIPI and shows performance improvements on the datasets used, but does not investigate generalization to new defect types.
- Why unresolved: The evaluation focuses on improving performance on known defect types within the dataset, without testing the model's robustness to novel or rare anomalies that may appear in industrial settings.
- What evidence would resolve it: Experiments testing NearCAIPI's performance on datasets with previously unseen defect types or in cross-domain scenarios where the model is applied to different industrial processes.

### Open Question 3
- Question: How does the time and cognitive load required for human experts to interact with the NearCAIPI system compare to traditional manual inspection methods?
- Basis in paper: [inferred] The paper emphasizes the importance of human-in-the-loop interaction for increasing trust and performance but does not quantify the time or effort required from human experts.
- Why unresolved: While the paper discusses the benefits of interactive learning, it does not provide a cost-benefit analysis comparing the interactive process to existing quality assurance methods in terms of time, resources, and expert effort.
- What evidence would resolve it: A comparative study measuring the time taken for experts to interact with NearCAIPI versus conducting manual inspections, including subjective assessments of cognitive load and user satisfaction.

## Limitations

- The evaluation relies heavily on simulated human feedback rather than actual domain expert interaction
- The new welding dataset is relatively small (413 images) and may not capture the full complexity of industrial settings
- The casting product dataset details are not fully described, particularly regarding defect diversity

## Confidence

- **Medium** confidence in the core mechanisms (InvRISE explanation method, near hits/misses extension, interactive correction workflow)
- **Low** confidence in the evaluation (simulated feedback, limited baseline comparison, no ablation studies)

## Next Checks

1. **Ground truth explanation validation**: Manually annotate a subset of welding dataset images with pixel-level anomaly regions and compute Dice/Jaccard scores for InvRISE explanations against these annotations.

2. **Real expert interaction**: Conduct a small-scale study with 2-3 domain experts performing the interactive correction workflow on actual industrial images, measuring both performance improvement and user satisfaction.

3. **Feature embedding quality check**: Select 20 random images from the welding dataset and manually verify whether the top-5 nearest neighbors retrieved by cosine similarity on feature embeddings are perceptually similar in terms of defect patterns.