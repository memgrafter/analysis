---
ver: rpa2
title: 'Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of
  the Missing AANNs'
arxiv_id: '2403.19827'
source_url: https://arxiv.org/abs/2403.19827
tags:
- aann
- language
- construction
- which
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how language models learn rare grammatical
  constructions, specifically the English Article+Adjective+Numeral+Noun (AANN) construction
  (e.g., "a beautiful five days"). The authors trained transformer models on a human-scale
  corpus (100M words) with systematically manipulated data, comparing learning of
  AANNs against counterfactual constructions.
---

# Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs

## Quick Facts
- arXiv ID: 2403.19827
- Source URL: https://arxiv.org/abs/2403.19827
- Authors: Kanishka Misra; Kyle Mahowald
- Reference count: 26
- Primary result: Models learn rare AANN constructions through generalization from more common related constructions rather than pure memorization

## Executive Summary
This study investigates how language models learn rare grammatical constructions, specifically the English Article+Adjective+Numeral+Noun (AANN) construction (e.g., "a beautiful five days"). The authors trained transformer models on a human-scale corpus (100M words) with systematically manipulated data, comparing learning of AANNs against counterfactual constructions. They found that models learn AANNs better than shuffled variants (ANAN, NAAN) and that this learning occurs through generalization from related constructions (e.g., "a few days"). Models trained without any AANN exposure still achieved 54% accuracy versus 70% with AANNs, demonstrating significant generalization.

## Method Summary
The authors created a counterfactual corpus from the BabyLM corpus (100M words) by systematically removing or replacing AANN constructions with shuffled variants (ANAN, NAAN). They trained transformer language models (OPT architecture, 12 layers, 12 heads, 97M parameters) on these manipulated corpora and tested acceptability judgments using SLOR scores. The study manipulated three factors: (1) presence/absence of AANNs, (2) variability of slot fillers in AANNs, and (3) frequency balance between adjectives and numerals following articles.

## Key Results
- Models learn AANNs significantly better than shuffled variants (ANAN, NAAN), indicating abstract pattern learning rather than surface memorization
- Models trained without AANN exposure achieved 54% accuracy versus 70% with AANNs, demonstrating significant zero-shot generalization
- Models exposed to AANNs with higher variability in slot fillers showed better generalization to novel instances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models learn rare AANN constructions through generalization from more frequent related constructions rather than pure memorization.
- Mechanism: The model abstracts structural patterns from common constructions like "a few days" and "measure noun phrases as singular" and applies them to learn AANN patterns.
- Core assumption: Related constructions share enough abstract structural similarity with AANNs to enable transfer learning.
- Evidence anchors:
  - [abstract]: "models learn AANNs better than shuffled variants (ANAN, NAAN) and that this learning occurs through generalization from related constructions"
  - [section]: "Models trained without any AANN exposure still achieved 54% accuracy versus 70% with AANNs, demonstrating significant generalization"
  - [corpus]: Evidence from Table 2 shows "A few/couple/dozen/etc. NNS" occurs 55,226 times vs 2,301 AANN instances
- Break condition: If the model only learns through memorization, performance on AANNs should drop to near-chance levels when AANNs are removed from training data.

### Mechanism 2
- Claim: The variability of slot fillers in training data affects the model's ability to generalize to novel AANN instances.
- Mechanism: When models see AANNs with diverse adjectives, numerals, and nouns in training, they learn that these slots are flexible rather than fixed.
- Core assumption: Higher variability in training examples signals to the model that slots are open and generalizable.
- Evidence anchors:
  - [abstract]: "models learn better when exposed to AANNs with more variability in slot fillers"
  - [section]: "LMs that observed AANN s with more variability on the adjective, numeral, and noun slots to show better generalization"
  - [corpus]: Corpus analysis shows restricted vs. variable AANN distributions (section 5)
- Break condition: If models only learn surface patterns, variability in training data should not affect generalization performance.

### Mechanism 3
- Claim: The frequency balance between adjectives and numerals following articles affects AANN learning.
- Mechanism: Models learn from the statistical tendency that adjectives follow articles more frequently than numerals, and this pattern influences AANN acceptability judgments.
- Core assumption: Frequency statistics of word sequences provide cues about grammatical acceptability.
- Evidence anchors:
  - [abstract]: "balancing the frequency of adjectives and numerals following an article has the greatest effect"
  - [section]: "adjectives are ≈14.6 times more likely to follow indefinite articles in the BabyLM corpus than are numerals"
  - [corpus]: Corpus statistics show 613,985 "a/an/another (JJ|JJR|JJS)" vs 42,111 "a/an/another CD"
- Break condition: If frequency balance doesn't matter, equalizing adjective/numeral frequencies should not affect model performance.

## Foundational Learning

- Concept: Generalization vs. memorization distinction
  - Why needed here: The paper's central question is whether models learn AANNs through abstract generalization or rote memorization of examples
  - Quick check question: If a model only memorizes, would it know that "a beautiful five days" is grammatical but "a five beautiful days" is not?

- Concept: Counterfactual corpus manipulation
  - Why needed here: The methodology relies on creating controlled corpora where specific constructions are removed or replaced to isolate learning mechanisms
  - Quick check question: How does removing AANNs while keeping other data constant help determine if models truly generalize?

- Concept: Acceptability judgment testing
  - Why needed here: The paper uses acceptability tests (SLOR scores) to measure whether models have learned grammatical patterns rather than just surface statistics
  - Quick check question: Why use SLOR instead of raw probabilities to compare model performance across different corpus conditions?

## Architecture Onboarding

- Component map: BabyLM corpus → POS tagging → AANN detection → Corpus manipulation → OPT model (12L, 12H, 97M) → Acceptability testing → Analysis
- Critical path: Corpus preprocessing → POS tagging → AANN detection → Corpus manipulation → Model training → Acceptability testing → Analysis
- Design tradeoffs: Human-scale corpus (100M words) chosen for tractability vs. larger corpora that might enable more memorization
- Failure signatures: If models fail to generalize beyond chance levels when AANNs are removed, suggests memorization rather than abstraction
- First 3 experiments:
  1. Train on full BabyLM → Test on AANN acceptability (baseline)
  2. Train on BabyLM without AANNs → Test on AANN acceptability (zero-shot generalization)
  3. Train on BabyLM with ANAN/NAAN replacements → Test on AANN acceptability (counterfactual comparison)

## Open Questions the Paper Calls Out
None

## Limitations
- The study doesn't definitively prove the cognitive mechanism of generalization, only demonstrates statistical learning differences
- The specific nature of abstract patterns being learned is not fully characterized
- Results may not generalize to other rare grammatical constructions or different languages

## Confidence
- Medium: Claims about generalization occurring through abstraction from related constructions - evidence shows statistical learning differences but doesn't definitively prove cognitive mechanism
- High: Claims about variability in training data affecting generalization performance - follows directly from experimental design and results
- Medium: Claims about frequency balance effects - correlation demonstrated but causal mechanism requires more exploration

## Next Checks
1. Ablation studies varying the degree of similarity between "related constructions" and AANNs to determine which structural features are most important for transfer learning
2. Probing experiments to identify whether models encode abstract AANN templates or learn through distributed representations of related patterns
3. Cross-linguistic validation testing whether the generalization mechanism holds for AANN constructions in languages with different article systems or word order patterns