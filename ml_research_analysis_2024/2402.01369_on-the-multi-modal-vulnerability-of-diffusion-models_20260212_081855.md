---
ver: rpa2
title: On the Multi-modal Vulnerability of Diffusion Models
arxiv_id: '2402.01369'
source_url: https://arxiv.org/abs/2402.01369
tags:
- image
- diffusion
- text
- cheating
- suffix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MMP-Attack, the first method to exploit misalignment
  between text and image feature spaces in diffusion models. By leveraging multi-modal
  priors, it optimizes a suffix appended to the original prompt to manipulate the
  model into generating a target object while removing the original object.
---

# On the Multi-modal Vulnerability of Diffusion Models

## Quick Facts
- arXiv ID: 2402.01369
- Source URL: https://arxiv.org/abs/2402.01369
- Reference count: 40
- First method to exploit misalignment between text and image feature spaces in diffusion models

## Executive Summary
This paper identifies a fundamental vulnerability in diffusion-based text-to-image models arising from the misalignment between text and image feature spaces. The authors propose MMP-Attack, a generation-free method that appends optimized cheating suffixes to prompts to manipulate the model into generating target objects while removing original ones. The attack leverages multi-modal priors and gradient-based optimization to exploit the discrepancy between how text and image features are structured in CLIP's embedding space. Results show the attack achieves over 81.8% success rate on Stable Diffusion v1.4 and v2.1 with just 4 tokens, and demonstrates strong transferability to commercial models like DALL-E 3.

## Method Summary
MMP-Attack exploits the misalignment between text and image feature spaces in diffusion models by optimizing cheating suffixes that are appended to original prompts. The method uses gradient-based optimization with Straight-Through Estimation (STE) to find suffix tokens that maximize similarity to target object embeddings in both text and image spaces while minimizing similarity to original object embeddings. The attack optimizes for both text modality (using CLIP text embeddings) and image modality (using reference images and CLIP image embeddings) simultaneously, with a weighting parameter λ to balance the two. The approach is generation-free, meaning it doesn't require actually generating images during the optimization process, making it more efficient than prior black-box attacks.

## Key Results
- MMP-Attack achieves over 81.8% success rate on Stable Diffusion v1.4 and v2.1 with only 4 tokens
- The attack shows strong transferability to commercial models like DALL-E 3 and Imagine Art
- Outperforms prior works in both attack capability and efficiency while maintaining robustness to filtering systems
- Demonstrates the vulnerability exists across different diffusion model architectures (SD v1.4 and v2.1)

## Why This Works (Mechanism)
The attack works by exploiting the fundamental misalignment between text and image feature spaces in diffusion models. While text features in CLIP's embedding space are chaotic and not well-clustered by subject, image features naturally cluster by the objects they depict. By optimizing suffixes that simultaneously maximize target similarity and minimize original similarity in both spaces, the attack can manipulate the model's understanding of what should be generated. The gradient-based optimization with STE allows efficient search through the token space to find combinations that exploit this vulnerability.

## Foundational Learning
- **CLIP embedding spaces**: Why needed - forms the foundation for both text and image feature alignment; Quick check - visualize text vs image embeddings to confirm clustering patterns
- **Diffusion model architecture**: Why needed - understanding how text conditioning works in the denoising process; Quick check - trace how text embeddings influence the generation process
- **Gradient-based optimization with STE**: Why needed - enables efficient search through discrete token space; Quick check - verify STE implementation correctly estimates gradients for discrete tokens
- **Multi-modal alignment**: Why needed - the core vulnerability being exploited; Quick check - measure cosine similarity differences between text and image spaces for same concepts
- **Token embedding space**: Why needed - the optimization operates in this discrete space; Quick check - examine token distribution and vocabulary coverage

## Architecture Onboarding

**Component Map**: Original Prompt → Optimized Suffix (MMP-Attack) → CLIP Text Encoder + Image Encoder → Cosine Similarity Losses → Optimized Cheating Suffix

**Critical Path**: The optimization pipeline that transforms an original prompt into an effective cheating suffix by minimizing losses across both text and image modalities

**Design Tradeoffs**: Generation-free approach trades off some attack precision for significant computational efficiency compared to black-box attacks that require image generation

**Failure Signatures**: Poor attack performance typically indicates issues with either the initialization method (EOS, Random, Synonym) or improper balancing of the multi-modal loss weighting (λ parameter)

**3 First Experiments**: 
1. Verify the clustering difference between text and image features in CLIP space
2. Test the optimization with different initialization methods (EOS, Random, Synonym)
3. Evaluate the attack success rate with varying λ values for multi-modal weighting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the robustness misalignment between text and image feature spaces manifest when using different diffusion model architectures beyond Stable Diffusion v1.4 and v2.1?
- Basis in paper: The paper observes significant differences between text and image feature spaces in Stable Diffusion models, noting that text features are chaotic while image features cluster by subject. They suggest this indicates potential misalignment in robustness between modalities.
- Why unresolved: The study only examined Stable Diffusion v1.4 and v2.1 models. Different architectures (e.g., DALL-E, Imagen) might exhibit different degrees or patterns of misalignment.
- What evidence would resolve it: Systematic visualization and comparison of text and image feature spaces across multiple diffusion model architectures, showing whether the misalignment pattern generalizes or varies significantly between models.

### Open Question 2
- Question: What is the fundamental reason that the cheating suffix optimization succeeds in targeted attacks but fails when individual tokens from the suffix are used separately?
- Basis in paper: The paper notes that when using individual tokens from the optimized suffix (like rwby, migration, reed, mone) separately, the model generates images of cars, but when all tokens are used together, it generates birds.
- Why unresolved: The paper observes this phenomenon but does not explain the underlying mechanism. It could be related to token interactions, semantic composition, or optimization dynamics.
- What evidence would resolve it: Detailed analysis of token embeddings, their interactions, and how the model processes concatenated versus individual tokens, potentially through ablation studies or feature visualization techniques.

### Open Question 3
- Question: How does the performance of MMP-Attack scale with the length of the cheating suffix, and is there an optimal length beyond which additional tokens degrade performance?
- Basis in paper: The paper uses a fixed suffix length of 4 tokens and shows strong performance, but does not explore how performance changes with different suffix lengths or whether there's a point of diminishing returns.
- Why unresolved: The choice of 4 tokens appears arbitrary, and the paper doesn't investigate whether shorter or longer suffixes might be more effective or efficient for different attack scenarios.
- What evidence would resolve it: Systematic experiments varying the suffix length from 1 to N tokens (where N is large enough to show saturation), measuring attack success rate, naturalness of generated images, and computational efficiency across different category pairs and model versions.

## Limitations
- Relies on white-box access assumptions for the target diffusion model
- Evaluation focuses primarily on Stable Diffusion models with limited testing on other architectures
- Effectiveness against sophisticated filtering and content moderation systems remains unclear
- Requires optimization for each specific target category, limiting scalability

## Confidence

**High confidence** in the attack methodology and optimization framework, as it follows established gradient-based attack principles
**Medium confidence** in the quantitative results, given the comprehensive evaluation across multiple models and metrics
**Medium confidence** in the transferability claims, as the evaluation includes only two commercial models

## Next Checks
1. Test the attack's effectiveness against larger, more diverse object categories and real-world images beyond the COCO dataset
2. Evaluate the attack's robustness against various filtering mechanisms, including adversarial training and detection systems
3. Investigate the attack's performance on other diffusion model architectures and commercial implementations not covered in the current study