---
ver: rpa2
title: 'Toward industrial use of continual learning : new metrics proposal for class
  incremental learning'
arxiv_id: '2404.06972'
source_url: https://arxiv.org/abs/2404.06972
tags:
- learning
- performance
- methods
- metric
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates performance metrics for class incremental
  learning (CIL), focusing on the commonly used mean task accuracy (ACC). Through
  experiments on CIFAR10/100 datasets, the authors demonstrate that ACC can be misleading
  due to large variations in class accuracies across tasks.
---

# Toward industrial use of continual learning : new metrics proposal for class incremental learning

## Quick Facts
- arXiv ID: 2404.06972
- Source URL: https://arxiv.org/abs/2404.06972
- Reference count: 25
- This paper proposes new metrics (MICA and WAMICA) for class incremental learning that address the limitations of mean task accuracy and provide more reliable evaluation for industrial applications.

## Executive Summary
This paper investigates the limitations of the commonly used mean task accuracy (ACC) metric in class incremental learning (CIL). Through experiments on CIFAR10/100 datasets, the authors demonstrate that ACC can be misleading due to large variations in class accuracies across tasks. They propose two new metrics - Minimum Incremental Class Accuracy (MICA) and Weighted Average of Minimum Incremental Class Accuracy (WAMICA) - that provide more reliable evaluation for industrial applications by considering performance variations across tasks and offering lower-bound guarantees.

## Method Summary
The authors propose two new evaluation metrics for class incremental learning. MICA calculates the minimum average accuracy across all classes, providing a lower bound guarantee on performance. WAMICA extends this by weighting the minimum accuracies based on class distribution across tasks, creating an average that accounts for performance variations. These metrics are tested against traditional ACC on multiple CIL methods using CIFAR10/100 datasets, revealing that ACC can mask poor performance on certain tasks while MICA and WAMICA provide more consistent and reliable evaluation.

## Key Results
- ACC can be misleading in CIL evaluation due to large variations in class accuracies across tasks
- MICA provides a lower bound guarantee on performance, making it more suitable for industrial applications
- WAMICA accounts for performance variations across tasks while enabling easier comparison of different CIL methods
- Some seemingly high-performing methods actually have poor and similar performance when evaluated with MICA/WAMICA, with Gdumb often outperforming more sophisticated approaches

## Why This Works (Mechanism)
The proposed metrics work by addressing the fundamental limitation of ACC - its inability to capture performance variations across tasks. MICA ensures that no class performs below a certain threshold, while WAMICA balances this guarantee with practical considerations of class distribution. This mechanism provides a more realistic assessment of CIL methods' capabilities, particularly important for industrial applications where consistent performance across all classes is crucial.

## Foundational Learning
1. Class Incremental Learning (CIL): A continual learning scenario where new classes are introduced sequentially while maintaining performance on previous classes
   - Why needed: Forms the basis of the evaluation problem being addressed
   - Quick check: Verify understanding of catastrophic forgetting and its impact on CIL

2. Evaluation Metrics in Machine Learning: Standardized methods for measuring model performance
   - Why needed: Understanding traditional metrics helps appreciate the limitations being addressed
   - Quick check: Review mean accuracy, precision, recall, and F1-score concepts

3. Statistical Guarantees: Mathematical bounds on performance metrics
   - Why needed: Essential for understanding the lower-bound guarantee provided by MICA
   - Quick check: Examine concepts of confidence intervals and worst-case analysis

## Architecture Onboarding
Component Map: Dataset -> CIL Method -> Evaluation Metrics (ACC, MICA, WAMICA) -> Performance Analysis

Critical Path: Data preparation -> Model training with incremental classes -> Performance evaluation using multiple metrics -> Comparative analysis

Design Tradeoffs:
- MICA provides strong guarantees but may be overly conservative
- WAMICA balances guarantees with practical considerations but is more complex
- ACC is simple but can be misleading

Failure Signatures:
- Large discrepancies between ACC and MICA/WAMICA values
- Consistently low MICA scores despite high ACC
- Similar WAMICA scores across diverse methods

First Experiments:
1. Compare ACC vs MICA/WAMICA on a simple CIL baseline (e.g., Fine-tuning)
2. Test metrics on a balanced vs imbalanced class distribution
3. Evaluate metrics across different dataset sizes and complexities

## Open Questions the Paper Calls Out
None

## Limitations
- Findings may not generalize to larger, more complex datasets beyond CIFAR10/100
- MICA may be overly conservative for some industrial applications where average performance is more critical
- Unclear how well metrics translate to other continual learning settings like domain or task incremental learning

## Confidence
High confidence in: The empirical demonstration that ACC can be misleading due to task-specific performance variations; the mathematical soundness of MICA as a lower-bound metric; the experimental setup and methodology.

Medium confidence in: The practical utility of MICA and WAMICA in industrial settings; the generalizability of findings to other datasets and learning scenarios; the claim that Gdumb often outperforms more sophisticated methods.

Low confidence in: The long-term stability of MICA and WAMICA as evaluation metrics as continual learning methods evolve; the potential for these metrics to capture all relevant aspects of continual learning performance in real-world applications.

## Next Checks
1. Test the proposed metrics (MICA and WAMICA) on larger-scale datasets like ImageNet or domain-specific industrial datasets to verify if the findings about ACC hold in more complex scenarios.

2. Conduct ablation studies comparing MICA/WAMICA against other potential metrics (e.g., worst-case accuracy, variance-aware metrics) to validate that the proposed metrics are indeed the most suitable for industrial applications.

3. Implement a real-world industrial case study using the proposed metrics to evaluate a continual learning system, measuring not just accuracy but also practical considerations like training time, memory constraints, and deployment feasibility.