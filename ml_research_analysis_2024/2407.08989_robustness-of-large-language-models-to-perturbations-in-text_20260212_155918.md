---
ver: rpa2
title: Robustness of Large Language Models to Perturbations in Text
arxiv_id: '2407.08989'
source_url: https://arxiv.org/abs/2407.08989
tags:
- text
- llms
- language
- arxiv
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the robustness of large language models
  (LLMs) to text perturbations and their performance on downstream tasks like Lexical
  Semantic Change (LSC) detection and Grammar Error Correction (GEC). The authors
  systematically introduce varying levels of noise into diverse datasets and evaluate
  the resilience of LLMs against these corruptions.
---

# Robustness of Large Language Models to Perturbations in Text

## Quick Facts
- **arXiv ID**: 2407.08989
- **Source URL**: https://arxiv.org/abs/2407.08989
- **Authors**: Ayush Singh; Navpreet Singh; Shubham Vatsal
- **Reference count**: 18
- **Primary result**: Generative LLMs show remarkable robustness to noisy perturbations in text, achieving state-of-the-art performance on grammar error correction and lexical semantic change detection with minimal prompting.

## Executive Summary
This paper investigates the robustness of large language models to text perturbations and their performance on downstream tasks like Lexical Semantic Change (LSC) detection and Grammar Error Correction (GEC). The authors systematically introduce varying levels of noise into diverse datasets and evaluate the resilience of LLMs against these corruptions. Contrary to popular belief, the results show that generative LLMs are remarkably robust to noisy perturbations in text, unlike pre-trained models like BERT or RoBERTa. The study also achieves state-of-the-art performance on benchmark tasks of GEC and LSC using minimal prompting. Additionally, the authors release a human-annotated dataset comparing LLM vs. human-corrected outputs, along with the code to reproduce their results.

## Method Summary
The study artificially introduces various types of noise (OCR errors, spelling mistakes, word swaps, synonym/antonym swaps) into datasets like IMDB movie reviews, JFLEG, and BEA-19. The authors evaluate LLMs using cosine similarity between clean and corrupted text embeddings and prompt-based approaches for LEC tasks. They compare performance across different LLM architectures and release a human-annotated preference dataset for evaluating correction quality.

## Key Results
- Generative LLMs maintain high cosine similarity between clean and corrupted text embeddings, demonstrating robustness to semantic-preserving noise
- With minimal prompting, LLMs achieve state-of-the-art performance on GEC benchmark tasks
- Open-source models like LLaMa show significant performance gaps compared to proprietary models like GPT on unsupervised LEC tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs maintain similar semantic encodings for text and its semantically-preserving corruptions
- Mechanism: LLMs learn robust embeddings through training on mixed clean/noisy text, allowing them to extract semantic meaning despite morphological variations
- Core assumption: The corrupted text remains semantically similar enough that cosine similarity of embeddings remains high
- Evidence anchors:
  - [abstract] "generative LLMs are quiet robust to noisy perturbations in text"
  - [section] "LLMs have been trained on a mixture of clean and noisy text which allows them to incorporate robustness towards minor irregularities in text"
  - [corpus] Weak evidence - only 5 related papers found, average FMR 0.45, no citations
- Break condition: When corruptions change meaning beyond semantic similarity threshold (e.g., synonym/antonym swaps that flip meaning)

### Mechanism 2
- Claim: LLMs achieve state-of-the-art performance on grammar error correction through minimal prompting
- Mechanism: Zero-shot prompting with task-specific instructions allows LLMs to correct errors without task-specific fine-tuning
- Core assumption: The prompt format effectively communicates the correction task to the LLM
- Evidence anchors:
  - [abstract] "With minimal prompting, LLMs achieve a new state-of-the-art on the benchmark tasks of Grammar Error Correction (GEC)"
  - [section] "We evaluate LLMs for LEC task by prompting them to correct the errors in the dataset"
  - [corpus] Weak evidence - no direct supporting papers found in corpus
- Break condition: When prompt format fails to convey task requirements or when errors require domain-specific knowledge

### Mechanism 3
- Claim: LLMs' robustness to noise differs from traditional models like BERT/RoBERTa
- Mechanism: Autoregressive training allows LLMs to better handle semantic-preserving noise compared to masked language models
- Core assumption: Training objectives fundamentally affect noise robustness
- Evidence anchors:
  - [abstract] "contrary to popular beliefs, generative LLMs are quiet robust to noisy perturbations in text. This is a departure from pre-trained models like BERT or RoBERTa"
  - [section] "contrary to popular beliefs, generative LLMs are quiet robust to noisy perturbations in text. This is a departure from pre-trained models like BERT or RoBERTa whose performance has been shown to be sensitive to deteriorating noisy text"
  - [corpus] Weak evidence - related work focuses on robustness generally but not this specific comparison
- Break condition: When noise levels exceed what can be handled by autoregressive training alone

## Foundational Learning

- Concept: Cosine similarity as distance metric for semantic comparison
  - Why needed here: Used to quantify similarity between clean and corrupted text embeddings
  - Quick check question: What does a cosine similarity of 0.95 indicate about two text embeddings?

- Concept: Semantic-preserving perturbations
  - Why needed here: Defines the scope of noise that should not affect meaning
  - Quick check question: Why would a synonym swap sometimes fail to be semantic-preserving?

- Concept: Zero-shot prompting methodology
  - Why needed here: Core technique for evaluating LLM performance without fine-tuning
  - Quick check question: What are the key components of an effective zero-shot prompt for error correction?

## Architecture Onboarding

- Component map: Text perturbation generators (OCR errors, spelling mistakes, etc.) -> LLM embedding API (text-embedding-ada-002 for GPT) -> Cosine similarity calculator -> Error correction prompt formatter -> Evaluation metrics (GLEU, ERRANT, human preference)

- Critical path:
  1. Generate corrupted text variants
  2. Obtain embeddings for clean and corrupted versions
  3. Calculate cosine similarity
  4. Evaluate on downstream tasks
  5. Compare to baselines

- Design tradeoffs:
  - Trade model size vs. inference speed (GPT-4 vs LLaMa-3)
  - Trade perturbation severity vs. semantic preservation
  - Trade evaluation metric strictness vs. practical relevance

- Failure signatures:
  - Sudden drops in cosine similarity indicating semantic drift
  - Performance gaps between GPT and open-source models
  - Human preference data contradicting automated metrics

- First 3 experiments:
  1. Compare cosine similarity across perturbation types (OCR vs spelling vs synonym)
  2. Test different prompting strategies on LEC performance
  3. Evaluate human preference vs automated metrics on BEA-19 dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the robustness of LLMs to noise change when processing longer documents or multi-paragraph text compared to single sentences?
- Basis in paper: [explicit] The authors mention aiming to expand their study to build and study LEC on longer passages and documents rather than just sentence-level corrections.
- Why unresolved: The current study only evaluates LLMs on sentence-level tasks (JFLEG and BEA-19 datasets), which may not fully represent real-world scenarios where longer documents are common.
- What evidence would resolve it: Experimental results comparing LLM performance on noisy text across different document lengths, showing how robustness scales with input size.

### Open Question 2
- Question: To what extent does the inclusion of machine translation as part of standard LEC practices improve overall text normalization and correction quality?
- Basis in paper: [explicit] The authors aim to include machine translation as part of standard LEC practices in future work.
- Why unresolved: The current study focuses on monolingual correction tasks, but many real-world scenarios involve text in multiple languages or mixed-language content.
- What evidence would resolve it: Comparative studies showing the performance of LEC systems with and without integrated translation capabilities on multilingual datasets.

### Open Question 3
- Question: How can open-source models like LLaMa be improved to match the performance of proprietary models like GPT on unsupervised LEC tasks?
- Basis in paper: [explicit] The authors note that LLaMa performed poorly compared to GPT and plan to improve open-source models for unsupervised LEC.
- Why unresolved: While the paper identifies that LLaMa struggles with short sentences and semantic preservation, it does not provide concrete solutions for bridging this performance gap.
- What evidence would resolve it: Development and evaluation of enhanced open-source models specifically optimized for unsupervised LEC, demonstrating improved performance on benchmark tasks.

## Limitations

- Perturbation Realism Gap: The controlled perturbations may not fully capture real-world noise patterns that LLMs encounter during training
- Human Preference Validation: The methodology for collecting and evaluating human preferences isn't detailed, making verification difficult
- Benchmark Scope Limitation: Results may not extend to other downstream applications like question answering or summarization where semantic preservation requirements differ

## Confidence

**LLM Robustness to Semantic-Preserving Noise** (Medium Confidence): Supported by quantitative evidence but influenced by embedding model choice and perturbation parameters.

**State-of-the-Art GEC Performance** (Low Confidence): Results depend heavily on specific prompt formulations and may be affected by evaluation protocol differences.

**Human Preference Dataset Utility** (Low Confidence): Valuable dataset but reliability uncertain without transparent annotation guidelines and quality metrics.

## Next Checks

1. **Perturbation Type Generalization Test**: Replicate cosine similarity experiments using broader noise types including natural language variation from social media, code-switching, and domain-specific terminology.

2. **Prompt Ablation Study**: Systematically vary prompt structure, instruction clarity, and example inclusion in LEC task to identify performance drivers.

3. **Cross-Task Robustness Analysis**: Apply perturbation methodology to other downstream tasks like question answering or sentiment analysis to test general robustness claims.