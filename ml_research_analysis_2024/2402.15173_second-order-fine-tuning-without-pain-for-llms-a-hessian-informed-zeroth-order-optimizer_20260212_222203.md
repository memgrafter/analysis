---
ver: rpa2
title: Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order
  Optimizer
arxiv_id: '2402.15173'
source_url: https://arxiv.org/abs/2402.15173
tags:
- hizoo
- mezo
- hessian
- learning
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory inefficiency of fine-tuning large
  language models using traditional first-order optimizers, which require backpropagation.
  The authors propose HiZOO, a zeroth-order optimizer that incorporates diagonal Hessian
  information to improve convergence speed and accuracy.
---

# Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer

## Quick Facts
- arXiv ID: 2402.15173
- Source URL: https://arxiv.org/abs/2402.15173
- Authors: Yanjun Zhao; Sizhe Dang; Haishan Ye; Guang Dai; Yi Qian; Ivor W. Tsang
- Reference count: 40
- Primary result: HiZOO achieves 8× speedup and 1.55% accuracy improvement on SST2 task compared to MeZO

## Executive Summary
This paper introduces HiZOO, a zeroth-order optimizer that incorporates diagonal Hessian information to address the memory inefficiency of fine-tuning large language models. By estimating the diagonal Hessian with one additional forward pass per step, HiZOO acts as a preconditioner that adjusts parameter updates based on their curvatures, effectively handling heterogeneous curvatures across parameters. Experiments across various models (350M to 66B parameters) demonstrate significant reductions in training steps and improvements in accuracy, with the HiZOO-L variant offering memory-efficient performance at 10% of MeZO's memory usage.

## Method Summary
HiZOO is a zeroth-order optimizer that estimates diagonal Hessian information to improve convergence speed and accuracy in LLM fine-tuning. The method uses three forward passes per step: one for the base loss, and two with parameter perturbations in opposite directions to estimate the diagonal Hessian. This estimated Hessian acts as a preconditioner, scaling updates according to parameter curvatures. The HiZOO-L variant implements a low-rank approximation of the Hessian to reduce memory consumption while maintaining performance. The approach is particularly effective for optimizing non-differentiable objectives like F1 score, where it outperforms MeZO by 6.5% on average.

## Key Results
- HiZOO achieves 8× speedup and 1.55% absolute accuracy improvement on SST2 task compared to MeZO
- Reduces total number of forward passes required for convergence across multiple tasks and model scales
- HiZOO-L variant reduces Hessian memory usage to 10% of MeZO while maintaining performance
- Particularly effective for non-differentiable objectives, improving F1 score optimization by 6.5% on average

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HiZOO estimates diagonal Hessian with one additional forward pass per step to act as a preconditioner
- Mechanism: The diagonal Hessian matrix is estimated using a zeroth-order oracle approach. By computing L(θ + μΣ^{1/2}u; B), L(θ - μΣ^{1/2}u; B), and L(θ; B), the difference captures second-order curvature information. This estimated diagonal Hessian scales parameter updates according to their respective curvatures, addressing heterogeneous curvatures across dimensions
- Core assumption: The diagonal Hessian captures sufficient curvature information to improve convergence without requiring full Hessian computation
- Evidence anchors:
  - [abstract]: "HiZOO estimates the diagonal Hessian with one additional forward pass per step, acting as a pre-conditioner to adjust parameter updates based on their curvatures."
  - [section 3.3]: Provides mathematical derivation showing how diagonal Hessian is estimated using Taylor expansion and zeroth-order oracles
- Break condition: If the diagonal Hessian fails to capture the curvature heterogeneity effectively, the preconditioner would not improve convergence

### Mechanism 2
- Claim: HiZOO reduces total number of forward passes required for model convergence compared to MeZO
- Mechanism: While HiZOO requires one additional forward pass per step for Hessian estimation, this investment reduces the total number of steps needed for convergence. The preconditioner helps navigate the loss landscape more efficiently, leading to faster convergence overall
- Core assumption: The reduction in steps outweighs the additional computational cost per step
- Evidence anchors:
  - [abstract]: "HiZOO significantly reduces the number of training steps and improves model accuracy. For example, on the SST2 task, HiZOO achieves an 8× speedup and 1.55% absolute accuracy improvement compared to MeZO."
  - [section 4.4]: "HiZOO reduces total number of forward passes required for convergence. For example, HiZOO achieves a 8× and 4× speedup on SST2 and MNLI tasks."
- Break condition: If the curvature heterogeneity is not significant, the additional forward pass per step may not justify the reduced total steps

### Mechanism 3
- Claim: HiZOO-L reduces Hessian memory usage to 10% of MeZO while maintaining performance
- Mechanism: HiZOO-L implements a low-rank approximation of the Hessian matrix using two low-rank matrices R and C. The diagonal Hessian is stored as ˆΣ^{-1} = (R * C)/(1^T_p * R), where k=1. This reduces memory from O(d) to O(d) with a smaller constant factor
- Core assumption: The low-rank approximation preserves sufficient information for effective preconditioning
- Evidence anchors:
  - [abstract]: "We also propose HiZOO-L, a memory-efficient variant that reduces Hessian memory usage to 10% of MeZO while maintaining performance."
  - [section 3.3]: "To further reduce Hessian memory consumption, we propose HiZOO-L to maintain it in a low-rank subspace, motivated by Adafactor (Shazeer & Stern, 2018)."
- Break condition: If the low-rank approximation loses critical curvature information, performance would degrade

## Foundational Learning

- Concept: Zeroth-order optimization (ZOO)
  - Why needed here: HiZOO builds upon MeZO, which uses ZOO to estimate gradients using only forward passes, avoiding backpropagation memory costs
  - Quick check question: How does ZOO estimate gradients without backpropagation, and what are the key trade-offs compared to first-order methods?

- Concept: Second-order optimization and Hessian matrix
  - Why needed here: HiZOO incorporates diagonal Hessian information to provide curvature-aware updates, addressing the heterogeneous curvature problem in LLM fine-tuning
  - Quick check question: What information does the Hessian matrix capture about the loss landscape, and why is it valuable for optimization?

- Concept: Preconditioning in optimization
  - Why needed here: The diagonal Hessian acts as a preconditioner that scales parameter updates according to their curvatures, helping navigate the loss landscape more effectively
  - Quick check question: How does preconditioning affect the optimization trajectory, and what are the benefits for handling ill-conditioned problems?

## Architecture Onboarding

- Component map: Forward pass engine -> Perturbation module -> Hessian estimator -> Update rule -> Memory manager
- Critical path:
  1. Sample batch and random seed
  2. Compute base loss L(θ; B)
  3. Perturb parameters forward and compute L(θ + μΣ^{1/2}u; B) and L(θ - μΣ^{1/2}u; B)
  4. Estimate diagonal Hessian from three loss values
  5. Update parameters using preconditioned gradient direction
  6. Repeat for T steps
- Design tradeoffs:
  - Memory vs. performance: Full Hessian requires O(d^2) memory, diagonal Hessian requires O(d), low-rank approximation further reduces memory
  - Accuracy vs. computation: More Hessian estimation samples (n > 1) increase accuracy but add computational overhead
  - Convergence speed vs. per-step cost: Additional forward pass per step reduces total steps needed
- Failure signatures:
  - Poor convergence: Indicates inadequate Hessian estimation or inappropriate learning rate
  - Memory overflow: Suggests Hessian storage is too large for available resources
  - Gradient explosion: May occur with aggressive Hessian scaling or inappropriate EMA smoothing
- First 3 experiments:
  1. Compare convergence speed and accuracy on SST-2 task between MeZO, HiZOO, and HiZOO-L using RoBERTa-large
  2. Measure memory usage across different model sizes (OPT family) for each variant
  3. Test sensitivity to Hessian smoothing parameter α_t by varying it across multiple downstream tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of perturbation scale µ affect the trade-off between estimation accuracy and computational overhead in HiZOO?
- Basis in paper: [explicit] The paper mentions that µ is a hyperparameter in Algorithm 1 and that "too large αt values may impede convergence or even cause training to fail due to gradient explosion," suggesting that µ might have similar effects
- Why unresolved: The paper does not provide systematic experiments exploring how different values of µ impact the convergence speed, final accuracy, or memory usage of HiZOO across various model scales and tasks
- What evidence would resolve it: A comprehensive ablation study varying µ across multiple orders of magnitude, reporting convergence speed, final accuracy, and memory usage for each configuration on different model sizes and tasks

### Open Question 2
- Question: Can the diagonal Hessian estimation be further improved by using more sophisticated smoothing techniques beyond exponential moving average?
- Basis in paper: [explicit] The paper states "Due to the presence of noise in the calculation of the Hessian, we utilize exponential moving average (EMA) to denoise the diagonal Hessian estimation."
- Why unresolved: The paper only explores one smoothing method (EMA) and does not compare it against alternatives like low-pass filters, Kalman filtering, or other statistical denoising techniques that might better handle the noise characteristics in the Hessian estimation
- What evidence would resolve it: Comparative experiments implementing and evaluating multiple smoothing techniques on the same benchmarks, measuring both the quality of Hessian estimation (e.g., via spectral analysis) and downstream fine-tuning performance

### Open Question 3
- Question: How does HiZOO's performance scale when applied to multimodal models or models with non-standard architectures?
- Basis in paper: [inferred] The paper focuses exclusively on standard transformer-based language models (RoBERTa, OPT, Phi-2, Llama3) and does not address multimodal architectures or models with unique structural features
- Why unresolved: The heterogeneous curvature handling mechanism of HiZOO is demonstrated only on standard language models, and its effectiveness on architectures with different parameter distributions or training dynamics remains unknown
- What evidence would resolve it: Empirical results applying HiZOO to multimodal models (e.g., CLIP, Flamingo) and models with architectural variations (e.g., MoE models, models with layer normalization variants), comparing performance against baseline methods

## Limitations
- The paper focuses exclusively on standard transformer-based language models, with unknown performance on multimodal or non-standard architectures
- Limited exploration of the perturbation scale hyperparameter µ and its impact across different model scales and tasks
- The low-rank approximation in HiZOO-L, while effective, may lose critical curvature information for certain loss landscapes

## Confidence
- **High confidence**: Memory efficiency improvements demonstrated through systematic comparison across multiple model scales; convergence speed improvements verified on standard NLP benchmarks
- **Medium confidence**: Hessian estimation accuracy claims supported by theoretical analysis and empirical convergence; low-rank approximation effectiveness shown but with limited ablation studies
- **Low confidence**: Generalization to arbitrary non-differentiable objectives; performance stability under extreme hyperparameter variations

## Next Checks
1. **Cross-domain robustness test**: Apply HiZOO to vision transformer fine-tuning tasks (e.g., ImageNet classification) to verify performance beyond NLP domains
2. **Extreme scale validation**: Test HiZOO on models exceeding 100B parameters to identify any scaling limitations in the Hessian estimation or low-rank approximation
3. **Non-differentiable objective ablation**: Systematically compare HiZOO against specialized methods on multiple non-differentiable metrics (e.g., BLEU, ROUGE, F1 across different tasks) to quantify generalization benefits