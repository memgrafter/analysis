---
ver: rpa2
title: Self-Explainable Graph Transformer for Link Sign Prediction
arxiv_id: '2408.08754'
source_url: https://arxiv.org/abs/2408.08754
tags:
- graph
- signed
- node
- negative
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SE-SGformer, a self-explainable signed graph
  transformer for link sign prediction. The method combines a novel signed random
  walk encoding with a Transformer architecture to capture multi-hop neighbor information,
  and uses a discriminator that identifies K-nearest positive and K-farthest negative
  neighbors to make explainable predictions.
---

# Self-Explainable Graph Transformer for Link Sign Prediction

## Quick Facts
- arXiv ID: 2408.08754
- Source URL: https://arxiv.org/abs/2408.08754
- Reference count: 20
- Key outcome: 2.2% higher prediction accuracy and 73.1% higher explainability accuracy compared to state-of-the-art baselines on real-world datasets

## Executive Summary
This paper introduces SE-SGformer, a self-explainable signed graph transformer for link sign prediction. The method combines a novel signed random walk encoding with a Transformer architecture to capture multi-hop neighbor information, and uses a discriminator that identifies K-nearest positive and K-farthest negative neighbors to make explainable predictions. The model is theoretically shown to be more expressive than SGCN and achieves 2.2% higher prediction accuracy and 73.1% higher explainability accuracy compared to state-of-the-art baselines on real-world datasets.

## Method Summary
SE-SGformer uses a Transformer encoder with three types of positional encodings: centrality encoding (based on node degrees), adjacency matrix encoding (normalized graph structure), and signed random walk encoding (multiple random walks capturing multi-hop relationships). The model generates a diffusion matrix using signed random walk with restart, then for each node, identifies K-nearest positive and K-farthest negative neighbors based on their distances in the embedding space. Edge sign predictions are made by comparing median distances to these neighbor sets, providing both predictions and explanations.

## Key Results
- Achieves 2.2% higher prediction accuracy compared to state-of-the-art baselines
- Demonstrates 73.1% higher explainability accuracy (Precision@K) than baselines
- Theoretically proves greater expressive power than SGCN using signed random walk encoding

## Why This Works (Mechanism)

### Mechanism 1
Signed random walk encoding captures multi-hop neighbor relationships better than shortest path encoding by exploring multiple paths between nodes and aggregating signed information along each path. This provides richer structural information than single-path approaches.

### Mechanism 2
The K-nearest positive and K-farthest negative neighbor selection provides self-explainable predictions by using distance comparisons to representative neighbors. If a node is closer to a target's K-nearest positive neighbors than its K-farthest negative neighbors, predict positive edge.

### Mechanism 3
Centrality encoding improves node representation by incorporating degree information through learnable embeddings. Nodes with higher degrees have different roles in signed graphs and should be weighted differently in attention mechanisms.

## Foundational Learning

- **Signed graphs and balance theory**: Understanding that signed graphs have both positive and negative edges, and that balance theory governs how these edges interact. Quick check: What does it mean for a signed graph to be balanced, and why is this important for link sign prediction?

- **Graph attention mechanisms and positional encodings**: The model uses Transformer architecture with custom positional encodings to capture graph structure. Quick check: How do positional encodings differ between sequential data (like text) and graph data?

- **Random walks and diffusion processes on graphs**: The signed random walk encoding and signed random walk with restart (SRWR) algorithm are core components. Quick check: What information does a random walk capture that direct neighborhood aggregation might miss?

## Architecture Onboarding

- **Component map**: Input (signed adjacency matrix, node features) -> Encoding module (Transformer with centrality, adjacency, random walk encodings) -> Explainable prediction module (K-nearest/farthest neighbor discriminator) -> Output (edge sign prediction + K neighbors)

- **Critical path**: 1) Compute centrality encoding, 2) Compute adjacency matrix encoding, 3) Compute signed random walk encoding, 4) Transformer layers with attention, 5) Generate diffusion matrix S using SRWR, 6) Find K-nearest positive and K-farthest negative neighbors, 7) Predict edge signs based on distance comparisons

- **Design tradeoffs**: Random walk length vs. computational cost, K value vs. explanation quality, number of Transformer layers vs. overfitting

- **Failure signatures**: Poor prediction accuracy despite good training loss (overfitting or poor encoding), high variance across runs (unstable initialization), low explanation quality despite good prediction (K-neighbor selection misaligned)

- **First 3 experiments**: 1) Ablation study removing each encoding type to measure impact on prediction accuracy, 2) Vary K value to find optimal tradeoff between explanation quality and prediction accuracy, 3) Compare signed random walk encoding vs. shortest path encoding on a simple synthetic signed graph with known ground truth

## Open Questions the Paper Calls Out

### Open Question 1
How does signed random walk encoding's performance compare to other positional encodings (shortest path, Laplacian) across different signed graph topologies? The paper only compares to shortest path encoding.

### Open Question 2
What is the impact of balance attenuation factors (β and γ) in the SRWR algorithm on explainability and prediction accuracy? The paper introduces these factors but doesn't investigate their effects.

### Open Question 3
How does explainability performance change when using different distance metrics (cosine similarity, Manhattan distance) instead of Euclidean distance? The paper only uses Euclidean distance.

## Limitations

- Theoretical claims about expressiveness are asserted but not fully proven, with only comparison to SGCN mentioned
- Explanation quality metric (Precision@K) is novel and its correlation with true explainability remains untested
- Implementation details for key components like signed random walk encoding are underspecified

## Confidence

- **High**: The overall architecture combining Transformer with signed random walk encoding is sound and builds on established methods
- **Medium**: The 2.2% prediction accuracy improvement claim, as it depends heavily on implementation details of baselines and proper hyperparameter tuning
- **Low**: The 73.1% explanation accuracy improvement, given the novelty of the evaluation metric and lack of comparison to other explanation methods

## Next Checks

1. Replicate the theoretical analysis by formally proving the expressiveness claim comparing signed random walk encoding to shortest path encoding under specific graph classes
2. Conduct ablation studies varying K and random walk parameters to establish sensitivity and optimal configurations
3. Test the explanation method on a synthetic signed graph with known ground truth to validate whether K-nearest/farthest neighbors actually capture causal factors for edge formation