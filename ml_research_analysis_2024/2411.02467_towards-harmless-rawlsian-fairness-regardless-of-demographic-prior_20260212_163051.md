---
ver: rpa2
title: Towards Harmless Rawlsian Fairness Regardless of Demographic Prior
arxiv_id: '2411.02467'
source_url: https://arxiv.org/abs/2411.02467
tags:
- uni00000013
- fairness
- utility
- vfair
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VFair, a method for achieving harmless Rawlsian
  fairness in machine learning without requiring prior demographic information. The
  core idea is to minimize the variance of training losses, which encourages similar
  performance across all potential group partitions in the data.
---

# Towards Harmless Rawlsian Fairness Regardless of Demographic Prior

## Quick Facts
- arXiv ID: 2411.02467
- Source URL: https://arxiv.org/abs/2411.02467
- Reference count: 40
- Key outcome: VFair achieves significant fairness improvements in regression tasks without demographic priors by minimizing variance of training losses

## Executive Summary
This paper addresses the challenge of achieving Rawlsian fairness in machine learning without requiring demographic prior information. The authors propose VFair, a method that minimizes the variance of training losses to encourage similar performance across all potential group partitions in the data. By operating at both the loss and gradient levels with a dynamic update strategy, VFair achieves up to 20% reduction in variance and group disparity metrics while maintaining overall utility. The method shows particular effectiveness in regression tasks where continuous utility metrics are used.

## Method Summary
VFair is a fairness method that minimizes the variance of training losses inside the optimal set of empirical losses, operating without demographic prior information. The method employs a dynamic update strategy that combines primary gradient descent (minimizing expected loss) with secondary gradient descent (minimizing variance of losses) using an adaptive weighting coefficient λ. This coefficient is calculated through two components: λ1 prevents gradient conflict and λ2 ensures non-negative weights for all examples. The method is implemented with mini-batch updates and uses exponential moving average (EMA) to maintain global mean loss information.

## Key Results
- VFair achieves up to 20% reduction in variance and group disparity metrics in regression tasks
- The method maintains overall utility while improving fairness, demonstrating harmless fairness improvements
- For classification tasks, improvements are limited due to quantized utility measurements, though VFair still outperforms baselines
- VFair is particularly effective when using continuous utility metrics like prediction error rather than accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing variance of training losses improves Rawlsian fairness without demographic priors by encouraging uniform performance across all possible data partitions.
- Mechanism: The VFair method penalizes the variance of individual example losses inside the optimal set of empirical losses. This forces the model to achieve similar losses for all examples, approximating a Dirac delta distribution where losses are concentrated at zero. This variance minimization acts as a proxy for group fairness when demographic information is unavailable.
- Core assumption: The variance of individual example losses is a good proxy for group utility disparity across arbitrary partitions of the data.
- Evidence anchors:
  - [abstract]: "To this end, we propose a simple but effective method named VFair to minimize the variance of training losses inside the optimal set of empirical losses."
  - [section 3.2]: "We advocate that minimizing the variance of prediction losses is a straightforward yet effective fairness proxy."
  - [corpus]: Weak evidence - the corpus neighbors discuss fairness without demographics but don't specifically address variance minimization as a mechanism.
- Break condition: If the variance of losses does not correlate with group utility disparity, or if the model converges to a uniform classifier that sacrifices overall utility.

### Mechanism 2
- Claim: Dynamic gradient updates with adaptive weighting coefficients maintain overall utility while improving fairness.
- Mechanism: VFair uses a dynamic coefficient λ that combines primary and secondary gradients. λ is calculated to ensure the combined gradient doesn't harm the descent of the primary objective (minimizing expected loss) while simultaneously minimizing variance. This is achieved through two components: λ1 prevents gradient conflict, and λ2 ensures non-negative weights for all examples.
- Core assumption: The combined gradient update scheme can simultaneously minimize variance and maintain the primary objective without explicit constraints.
- Evidence anchors:
  - [section 3.4]: "We consider the following gradient update scheme, θt+1 ← θt − γt[λt∇μ(θt) + ∇σ(θt)]" and the derivation of λ using λ1 and λ2.
  - [section 3.4]: "Theorem 2. Given the objective of Eq. 2, the combined gradient derived by the update scheme of Eq. 5 can be expressed with an example-reweighting form."
  - [corpus]: Weak evidence - the corpus neighbors discuss fairness methods but don't specifically address this dynamic gradient update mechanism.
- Break condition: If the dynamic λ calculation fails to maintain the balance between variance minimization and utility preservation, or if the mini-batch strategy introduces instability.

### Mechanism 3
- Claim: The difference between regression and classification tasks explains why VFair achieves better fairness improvements in regression.
- Mechanism: Regression tasks use continuous utility metrics (like prediction error) that are more consistent with loss values, while classification tasks use quantized metrics (like accuracy) that don't capture the nuanced differences in loss distributions. VFair's variance minimization is more effective when the utility metric aligns with the loss space.
- Core assumption: Continuous utility metrics provide a better signal for variance minimization than quantized metrics.
- Evidence anchors:
  - [section 4.3]: "We observe that on UCI Adult, the earned fairness for each fairness method is still limited while on CelebA, VFair yields superior performance. A reasonable explanation is that VFair has the opportunity to discover better solutions in a relatively larger solution space, where more diverse minima can be examined through fairness criteria."
  - [section 4.3]: "From quantized to continuous. For scenarios where prediction error (the difference between prediction and true label) is desired in classification, e.g., assessing whether a model overestimates or underestimates, VFair should be more applicable."
  - [corpus]: Weak evidence - the corpus neighbors don't specifically address the difference between regression and classification in the context of fairness without demographics.
- Break condition: If classification tasks can be reformulated to use continuous utility metrics, or if the quantized nature of accuracy doesn't significantly impact the effectiveness of variance minimization.

## Foundational Learning

- Concept: Rawlsian fairness and its connection to group utility disparity
  - Why needed here: The paper builds on Rawlsian fairness principles to develop a method that works without demographic information. Understanding this philosophical foundation is crucial for grasping why minimizing variance of losses serves as a fairness proxy.
  - Quick check question: What is the core principle of Rawlsian fairness, and how does it relate to group utility disparity in machine learning?

- Concept: Variance as a statistical measure and its relationship to distribution concentration
- Why needed here: The method's core mechanism relies on minimizing the variance of training losses to approximate a Dirac delta distribution. Understanding variance and its properties is essential for comprehending why this approach works.
  - Quick check question: How does minimizing variance of a distribution relate to concentrating the distribution around its mean?

- Concept: Dynamic optimization and gradient-based learning
  - Why needed here: VFair employs a dynamic update strategy that operates at both the loss and gradient levels. Understanding how gradients are used in optimization and how dynamic adjustments can be made is crucial for implementing and extending this method.
  - Quick check question: How do dynamic gradient updates differ from standard gradient descent, and what are the potential benefits and challenges?

## Architecture Onboarding

- Component map:
  Input -> Primary objective (minimize expected loss μ) + Secondary objective (minimize variance σ) -> Dynamic coefficient λ -> Model parameters

- Critical path:
  1. Compute losses for current batch
  2. Update global mean loss using EMA
  3. Calculate variance of losses
  4. Compute primary and secondary gradients
  5. Calculate dynamic λ using λ1 and λ2
  6. Update model parameters using combined gradient

- Design tradeoffs:
  - Variance minimization vs. overall utility: The method must balance fairness improvement with utility preservation
  - Global vs. local information: Using EMA for global mean loss vs. computing variance on mini-batches
  - Computational cost: VFair requires approximately twice the computation time of ERM

- Failure signatures:
  - Model converges to uniform classifier (all predictions near decision boundary)
  - Significant drop in overall utility despite variance minimization
  - Instability in training due to mini-batch variance calculations

- First 3 experiments:
  1. Implement basic VFair on a simple regression task (e.g., UCI Adult converted to regression) and compare fairness metrics with ERM
  2. Test VFair on a classification task with continuous utility metric (e.g., prediction error instead of accuracy) to verify the claim about quantized vs. continuous metrics
  3. Implement ablation study by removing λ1 or λ2 components to understand their individual contributions to the method's effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does VFair perform on non-IID data distributions where training and test data may have different demographic compositions?
- Basis in paper: [inferred] The paper mentions in the conclusion that future work will involve identifying challenges when applying VFair to non-IID data predictions.
- Why unresolved: The current experiments focus on IID data settings where training and test splits maintain similar distributions. Real-world deployment often involves shifts in demographic compositions between training and test data.
- What evidence would resolve it: Experiments comparing VFair's performance on train-test splits with deliberately introduced demographic distribution shifts, measuring both utility and fairness metrics under these conditions.

### Open Question 2
- Question: What is the theoretical upper bound on fairness improvement achievable by VFair when using quantized utility metrics like accuracy?
- Basis in paper: [explicit] Section 4.3 discusses the limitation that accuracy-based metrics provide limited improvements without compromising utility due to quantization effects.
- Why unresolved: While the paper demonstrates empirical limitations, it doesn't provide a formal theoretical analysis of the fundamental limits imposed by quantized metrics on fairness improvement.
- What evidence would resolve it: A mathematical proof establishing the relationship between quantization granularity of utility metrics and the maximum achievable fairness improvement, potentially through information-theoretic bounds.

### Open Question 3
- Question: How does the choice of loss function (e.g., cross-entropy vs MSE) affect VFair's ability to achieve harmless Rawlsian fairness in classification tasks?
- Basis in paper: [explicit] Section 4.3 shows that using prediction error (continuous metric) rather than accuracy (quantized metric) enables better fairness improvements in classification tasks.
- Why unresolved: The paper demonstrates empirical differences but doesn't systematically explore how different loss functions interact with VFair's variance minimization approach across various classification scenarios.
- What evidence would resolve it: Controlled experiments comparing VFair's performance across multiple loss functions (cross-entropy, MSE, hinge loss, etc.) on diverse classification datasets, analyzing the relationship between loss function properties and fairness-utility trade-offs.

## Limitations
- The method's effectiveness is dataset-dependent, with significantly better results in regression tasks compared to classification
- VFair requires approximately 2x the computational cost of standard empirical risk minimization
- The assumption that variance of losses serves as an effective proxy for group utility disparity across arbitrary partitions remains unverified

## Confidence
- High confidence: The variance minimization mechanism and dynamic gradient update framework are well-defined and mathematically rigorous
- Medium confidence: The claim that variance of losses serves as an effective proxy for group utility disparity without demographics
- Medium confidence: The explanation for superior performance in regression vs. classification tasks based on continuous vs. quantized utility metrics
- Low confidence: The method's effectiveness across diverse real-world scenarios without demographic information

## Next Checks
1. Conduct ablation studies to isolate the contribution of λ1 vs λ2 components in the dynamic update mechanism
2. Test VFair on classification tasks with continuous utility metrics (prediction error) to validate the claimed advantage over quantized metrics
3. Implement synthetic experiments with known group partitions to verify that variance minimization actually correlates with group utility disparity