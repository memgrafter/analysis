---
ver: rpa2
title: 'Medical Adaptation of Large Language and Vision-Language Models: Are We Making
  Progress?'
arxiv_id: '2411.04118'
source_url: https://arxiv.org/abs/2411.04118
tags:
- medical
- prompt
- mmlu
- show
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether domain-adaptive pretraining (DAPT)
  on biomedical corpora meaningfully improves medical question-answering performance
  compared to general-domain large language and vision-language models. The authors
  conduct rigorous head-to-head comparisons of seven medical LLMs and two medical
  VLMs against their base models across 13 textual and 8 visual medical QA datasets.
---

# Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?

## Quick Facts
- arXiv ID: 2411.04118
- Source URL: https://arxiv.org/abs/2411.04118
- Reference count: 40
- Primary result: Medical DAPT shows minimal benefits over general-domain models when prompts are properly optimized and statistical significance is considered

## Executive Summary
This paper challenges the prevailing assumption that domain-adaptive pretraining on biomedical corpora significantly improves medical question-answering performance. Through rigorous head-to-head comparisons across 13 textual and 8 visual medical QA datasets, the authors demonstrate that medical large language models (LLMs) and vision-language models (VLMs) fail to consistently outperform their general-domain counterparts when prompts are separately optimized for each model and statistical significance is properly accounted for. Medical LLMs only showed statistically significant improvements in 12.1% of cases, while VLMs improved in just 6.3% of cases.

The study reveals a critical flaw in previous medical AI evaluations: the use of fixed prompts optimized only for medical models, which artificially inflated their apparent performance. By employing separate prompt optimization for each model type and incorporating statistical uncertainty analysis, the authors provide a more accurate assessment of medical DAPT's actual value. Their findings suggest that state-of-the-art general-domain models may already possess substantial medical reasoning capabilities when appropriately prompted, raising questions about the cost-effectiveness of medical-specific adaptation approaches.

## Method Summary
The authors conducted systematic comparisons between seven medical LLMs and their base general-domain models, plus two medical VLMs against their base versions. They evaluated all models across 13 textual and 8 visual medical QA datasets using prompts separately optimized for each model through an iterative process. Performance was measured using standard accuracy metrics, with statistical significance testing applied to determine whether observed differences were meaningful. The study controlled for prompt bias by ensuring that prompts were not fixed but rather tailored to each model's characteristics, addressing a key limitation in prior medical AI evaluations.

## Key Results
- Medical LLMs showed statistically significant improvements over base models in only 12.1% of test cases
- Medical VLMs demonstrated significant improvements in just 6.3% of comparisons
- Previous evaluations likely overestimated medical DAPT benefits by using fixed prompts optimized only for medical models
- General-domain models achieved comparable or superior performance when provided with appropriately optimized prompts

## Why This Works (Mechanism)
The study's findings suggest that general-domain LLMs and VLMs possess substantial latent medical reasoning capabilities that can be unlocked through appropriate prompting, without requiring expensive domain-specific pretraining. This mechanism appears to work because large-scale pretraining on diverse web data already exposes models to substantial medical terminology and reasoning patterns, which can be effectively activated through well-crafted prompts. The failure of medical DAPT to consistently improve performance indicates that the additional biomedical corpus exposure during adaptation may provide diminishing returns compared to the original pretraining, or that the adaptation process may not effectively transfer the specialized medical knowledge.

## Foundational Learning
- **Statistical significance testing in ML evaluation**: Needed to distinguish true performance differences from random variation; quick check: verify p-values and confidence intervals are reported for all comparisons
- **Prompt optimization methodology**: Critical for fair model comparison; quick check: confirm separate optimization processes were documented for each model type
- **Medical QA dataset diversity**: Ensures results generalize across different medical domains; quick check: verify dataset sources span multiple medical specialties and question types
- **Vision-language model evaluation**: Requires different metrics and considerations than text-only models; quick check: confirm visual reasoning tasks are appropriately weighted in overall analysis

## Architecture Onboarding
**Component Map**: Data preprocessing -> Prompt optimization -> Model inference -> Statistical analysis -> Performance comparison
**Critical Path**: Prompt optimization is the bottleneck, as it must be performed separately for each model to ensure fair comparison
**Design Tradeoffs**: Separate prompt optimization ensures fairness but increases computational cost and complexity compared to fixed-prompt approaches
**Failure Signatures**: Overestimated medical model performance when fixed prompts are used; false significance when statistical testing is omitted
**First Experiments**: 1) Replicate prompt optimization process with different random seeds, 2) Test model performance on out-of-distribution medical questions, 3) Evaluate resource consumption differences between medical and base models

## Open Questions the Paper Calls Out
None

## Limitations
- Prompt optimization methodology details remain unclear, raising questions about potential human bias in prompt selection
- The 13 textual and 8 visual datasets may not fully represent the diversity of medical reasoning tasks
- Statistical significance testing assumes independence between test samples that may not hold in medical contexts

## Confidence
**High Confidence**: Previous medical LLM evaluations likely overestimated DAPT benefits due to fixed prompt bias
**Medium Confidence**: State-of-the-art general-domain models possess strong medical reasoning capabilities when appropriately prompted
**Low Confidence**: Medical DAPT may not be worth the computational investment without economic analysis of training costs versus performance gains

## Next Checks
1. Cross-domain generalization test: Evaluate the same model comparisons across additional medical specialties not represented in current datasets
2. Real-world clinical workflow validation: Test model performance in simulated clinical decision support scenarios with time constraints
3. Cost-benefit analysis: Quantify computational resources required for medical DAPT versus marginal performance improvements observed