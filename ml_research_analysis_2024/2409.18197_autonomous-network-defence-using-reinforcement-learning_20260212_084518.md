---
ver: rpa2
title: Autonomous Network Defence using Reinforcement Learning
arxiv_id: '2409.18197'
source_url: https://arxiv.org/abs/2409.18197
tags:
- agent
- network
- agents
- against
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the application of intelligent agents to
  autonomously defend a computer network. Utilising the recently proposed CAGE Challenge
  scenario, and CybORG (an autonomous network defence environment), the authors develop
  a hierarchical RL agent that can defend against multiple APT adversaries over varying
  lengths of time and overcomes the performance limitations of training against a
  single adversary.
---

# Autonomous Network Defence using Reinforcement Learning

## Quick Facts
- arXiv ID: 2409.18197
- Source URL: https://arxiv.org/abs/2409.18197
- Authors: Myles Foley; Chris Hicks; Kate Highnam; Vasilios Mavroudis
- Reference count: 16
- Key outcome: Hierarchical RL agents with specialized sub-agents outperform individual components in autonomous network defense

## Executive Summary
This paper investigates autonomous network defense using reinforcement learning agents in the CAGE Challenge environment. The authors develop a hierarchical RL approach where a controller agent selects between specialized sub-agents trained against specific APT adversaries (B_lineAgent and RedMeanderAgent). Their approach leverages PPO with curiosity-driven exploration to achieve superior performance compared to individual agents, demonstrating generalization across multiple adversary types and episode lengths.

## Method Summary
The method employs a hierarchical RL architecture where a controller agent selects between two specialized sub-agents (B-line-defence and Meander-defence) based on the current state observation. Each sub-agent is trained using PPO with Intrinsic Curiosity Module (ICM) against their respective adversary types. The controller learns to identify which adversary is active and chooses the optimal sub-agent accordingly. Training occurs in the CybORG environment simulating a network with 13 hosts across 3 subnets, with agents receiving 52-bit state vectors and standard defense action sets.

## Key Results
- Hierarchical RL agent outperforms individual sub-agents against both B_lineAgent and RedMeanderAgent adversaries
- Curiosity-driven exploration significantly improves performance, particularly for the B-line-defence agent
- The approach generalizes across episode lengths (30, 50, and 100 timesteps) and multiple adversary types
- Open-source implementation provided to support further research in autonomous network defense

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical RL with specialized sub-agents outperforms any single agent because no single policy can generalize across both specialized and exploratory adversaries
- Mechanism: The controller agent selects the best sub-agent at each timestep based on the current state, allowing each sub-agent to leverage policies optimized for a specific adversary type
- Core assumption: The state space contains distinguishable features that indicate which adversary is active, enabling correct controller selection
- Evidence anchors:
  - [abstract]: "Surprisingly, they show that their hierarchy of specialised agents outperforms any of its individual components and provides for a more generalised defensive capability."
  - [section 3.3]: "In this way the controller should identify the adversary and choose the agent trained to deal with the threat and mitigate it."
  - [corpus]: Weak - no direct corpus evidence supporting this mechanism; the related papers focus on communication and entity-based approaches rather than hierarchical selection
- Break condition: If the state space becomes too noisy or if the adversaries share indistinguishable behavioral patterns, the controller may fail to select the optimal sub-agent consistently

### Mechanism 2
- Claim: Curiosity-driven exploration helps agents learn more robust policies by encouraging discovery of states that would otherwise be under-explored
- Mechanism: The Intrinsic Curiosity Module (ICM) provides an intrinsic reward for taking actions that lead to unpredictable outcomes, pushing the agent to explore uncertain regions of the state space
- Core assumption: The environment contains stochastic elements (e.g., failed actions, random adversary behavior) that benefit from curiosity-driven exploration
- Evidence anchors:
  - [section 3.2]: "Curiosity promotes exploration in an environment via an intrinsic reward... ICM also reduces noise in this process by using only the relevant information in the state space."
  - [section 5]: "Additionally, our experiments with curiosity, a technique which encourages greater generalisation [10], substantially improves the performance of the B-line-defence agent even against the B_lineAgent itself."
  - [corpus]: Weak - corpus papers do not discuss curiosity mechanisms specifically; they focus on communication and entity-based approaches
- Break condition: If the environment becomes deterministic or if intrinsic rewards dominate extrinsic rewards, curiosity may lead to suboptimal policies

### Mechanism 3
- Claim: PPO with clipping function balances exploration and exploitation by preventing large policy updates that could destabilize training
- Mechanism: The clipped objective function in PPO constrains the policy update magnitude, ensuring stable learning while still allowing progress toward optimal policies
- Core assumption: The network defense environment requires stable learning due to its complexity and the presence of multiple adversaries
- Evidence anchors:
  - [section 3.1]: "PPO also introduces a clipping function in gradient descent. This allows the model to balance large updates which lead to local optima and smaller updates which increase training time."
  - [abstract]: PPO is mentioned as the base algorithm used for all sub-agents
  - [corpus]: Weak - corpus papers do not discuss PPO specifically; they focus on different RL approaches
- Break condition: If the clipping parameter is set too aggressively, the agent may fail to make meaningful policy improvements

## Foundational Learning

- Concept: Reinforcement Learning fundamentals (states, actions, rewards, policies)
  - Why needed here: The entire architecture relies on RL agents learning optimal defensive policies through interaction with the CybORG environment
  - Quick check question: What is the difference between the policy π and the value function V in RL?

- Concept: Multi-agent RL and adversarial environments
  - Why needed here: The defense problem involves multiple agents (defender vs. attackers) with competing objectives, requiring understanding of zero-sum and non-zero-sum game dynamics
  - Quick check question: How does the presence of an adversary change the reward structure compared to a single-agent RL problem?

- Concept: Hierarchical RL and meta-policies
  - Why needed here: The controller acts as a meta-policy selecting among specialized sub-agents, requiring understanding of hierarchical decision-making structures
  - Quick check question: What are the benefits and drawbacks of using a hierarchical approach versus training a single agent on all adversary types?

## Architecture Onboarding

- Component map: Controller agent → Sub-agents (b-line-defence, meander-defence) → CybORG environment; each component trained separately then integrated
- Critical path: State observation → Controller selection → Sub-agent action → Environment update → Reward → Policy update
- Design tradeoffs: Specialized sub-agents provide better performance against specific adversaries but require more training time and careful controller design; unified agent would be simpler but likely underperform
- Failure signatures: Controller consistently selects wrong sub-agent (suggests poor state representation or insufficient training diversity); sub-agents fail to converge (suggests exploration issues or reward sparsity)
- First 3 experiments:
  1. Train b-line-defence agent against B_lineAgent only and evaluate performance metrics (reward, server uptime)
  2. Train meander-defence agent against RedMeanderAgent only and compare with b-line-defence agent on both adversaries
  3. Implement controller with random selection strategy, then evaluate against both adversaries to establish baseline for hierarchical performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hierarchical RL agents be optimized to adapt dynamically to unknown adversaries beyond the trained scenarios?
- Basis in paper: [explicit] The paper notes that their hierarchical agent was trained against specific adversaries and performed well, but does not explore adaptation to completely novel attack strategies
- Why unresolved: The experiments only tested against two specific red agents, limiting generalizability to other potential adversaries
- What evidence would resolve it: Demonstrating the agent's performance against a wider variety of unseen adversarial strategies would show its adaptability

### Open Question 2
- Question: What are the specific contributions of intrinsic curiosity in improving the generalization of defense agents in diverse network environments?
- Basis in paper: [explicit] The paper mentions that curiosity helps in overcoming randomness and promotes exploration, but does not quantify its impact on generalization across different network setups
- Why unresolved: The paper shows improved performance with curiosity but does not explore its effects in varied environments beyond the CAGE Challenge setup
- What evidence would resolve it: Comparative studies showing curiosity-driven agents' performance in multiple distinct network environments would clarify its role

### Open Question 3
- Question: How can the performance of hierarchical RL agents be formally quantified and ensured in real-world network defense scenarios?
- Basis in paper: [inferred] The paper suggests that their hierarchical approach outperforms individual sub-agents but does not provide a formal framework for quantifying generalization or robustness in real-world settings
- Why unresolved: The results are based on simulated environments, and the paper does not address how these findings translate to real-world complexities
- What evidence would resolve it: Developing a formal framework and testing the agent in real-world network environments would provide insights into its practical applicability

## Limitations
- The approach shows performance degradation against untrained adversaries (SleepAgent), suggesting limited robustness to novel attack patterns
- Specific PPO hyperparameters and curiosity module configurations are not fully specified, making exact reproduction challenging
- Results are based on simulated environments without validation in real-world network defense scenarios

## Confidence
- **High**: PPO with clipping function provides stable learning (well-established in RL literature)
- **Medium**: Curiosity-driven exploration improves policy robustness (supported by internal results but limited external validation)
- **Medium**: Hierarchical structure outperforms individual agents (results show this but mechanism explanation could be stronger)

## Next Checks
1. **Ablation study**: Test the controller agent with random selection versus learned selection to quantify the performance gain from hierarchical decision-making
2. **Generalization stress test**: Evaluate the trained agents against additional adversary types not seen during training to assess true generalization capability
3. **Hyperparameter sensitivity analysis**: Systematically vary PPO and curiosity parameters to identify which configurations most significantly impact defensive performance