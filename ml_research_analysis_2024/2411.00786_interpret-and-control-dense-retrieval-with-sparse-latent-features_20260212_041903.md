---
ver: rpa2
title: Interpret and Control Dense Retrieval with Sparse Latent Features
arxiv_id: '2411.00786'
source_url: https://arxiv.org/abs/2411.00786
tags:
- retrieval
- features
- sparse
- latent
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of interpretability and controllability
  in dense retrieval models by introducing sparse autoencoders to extract sparse latent
  features from dense embeddings. The key innovation is a retrieval-oriented contrastive
  loss that ensures these latent features remain effective for retrieval tasks while
  enabling interpretability.
---

# Interpret and Control Dense Retrieval with Sparse Latent Features

## Quick Facts
- arXiv ID: 2411.00786
- Source URL: https://arxiv.org/abs/2411.00786
- Reference count: 40
- Primary result: Sparse autoencoders with retrieval-oriented contrastive loss extract interpretable features that maintain retrieval accuracy while enabling controllable behavior

## Executive Summary
This paper addresses the lack of interpretability and controllability in dense retrieval models by introducing sparse autoencoders to extract sparse latent features from dense embeddings. The key innovation is a retrieval-oriented contrastive loss that ensures these latent features remain effective for retrieval tasks while enabling interpretability. Experimental results show that both the learned sparse features and their reconstructed embeddings retain nearly the same retrieval accuracy as original dense vectors (e.g., MRR of 0.3455 vs 0.3605 on MSMARCO with K=128 features). The sparse features capture meaningful concepts and enable controllable retrieval behavior through targeted feature manipulation, allowing users to prioritize documents from specific perspectives by amplifying relevant latent features.

## Method Summary
The method trains a k-sparse autoencoder on dense retrieval embeddings using a combination of MSE reconstruction loss and KL divergence contrastive loss. The encoder maps dense embeddings to sparse latent space using TopK activation, while the decoder reconstructs dense embeddings from these sparse features. The contrastive loss ensures the reconstructed embeddings maintain the same relevance relationships as the original dense embeddings. The sparse latent features are interpreted using Neuron to Graph (N2G) methodology, which identifies semantically meaningful token patterns. Controllability is achieved by amplifying specific feature dimensions to bias retrieval toward documents containing those concepts.

## Key Results
- Sparse latent features with K=128 active dimensions maintain nearly identical retrieval performance to original dense vectors (MRR 0.3455 vs 0.3605 on MSMARCO)
- N2G interpretation reveals semantically meaningful concepts captured by individual sparse features
- Feature amplification experiments demonstrate controllable retrieval behavior with measurable improvements in precision and recall metrics
- The approach generalizes across different embedding models (BGE and MiniCPM) while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-oriented contrastive loss ensures sparse features remain effective for retrieval while preserving interpretability
- Mechanism: The contrastive loss (KLD) aligns the distribution of reconstructed query-document pairs with original embeddings, maintaining relevance relationships while MSE handles point-wise reconstruction accuracy
- Core assumption: Preserving the relative positioning between queries and documents is more important than perfect individual point reconstruction for retrieval tasks
- Evidence anchors:
  - [abstract] "Our key contribution is the development of a retrieval-oriented contrastive loss, which ensures the sparse latent features remain effective for retrieval tasks"
  - [section 2] "we employ contrastive learning via Kullbackâ€“Leibler divergence (KLD) to ensure that the distribution of reconstructed query and document embedding aligns with the original"
  - [corpus] Weak evidence - no directly comparable papers found in corpus

### Mechanism 2
- Claim: Sparse latent features capture semantically meaningful concepts through activation patterns
- Mechanism: The k-sparse autoencoder uses TopK activation to enforce sparsity, creating distinct features that can be interpreted via Neuron to Graph (N2G) approach which identifies token patterns that strongly activate each feature
- Core assumption: Sparse activation patterns in high-dimensional space correspond to coherent semantic concepts rather than random noise
- Evidence anchors:
  - [section 3] "For each feature, we create a training set of 512 samples by selecting the highest-activating documents... N2G to construct trie representations for each feature"
  - [section 4.2] "N2G provides an automated approach to interpret the behavior of individual neurons by converting their activations into graph-based representations"
  - [corpus] Moderate evidence - "Decoding Dense Embeddings: Sparse Autoencoders for Interpreting and Discretizing Dense Retrieval" shows similar approach

### Mechanism 3
- Claim: Manipulating sparse latent features enables controllable retrieval behavior
- Mechanism: By amplifying specific feature dimensions associated with query-relevant concepts, the reconstructed embeddings can be biased toward documents containing those concepts, as demonstrated through quantitative improvement in retrieval metrics
- Core assumption: Feature amplification in the sparse latent space translates predictably to semantic shifts in the reconstructed embedding space
- Evidence anchors:
  - [section 4.3] "we observe a clear trend of improvement in both MRR and P@10 as the amplification of relevant sparse latent features increases"
  - [section 4.3] "amplifying the targeted feature dimension effectively biases the retrieval results towards the corresponding perspective"
  - [corpus] Weak evidence - no directly comparable controllability studies found in corpus

## Foundational Learning

- Concept: Kullback-Leibler divergence for contrastive learning
  - Why needed here: KL divergence measures how one probability distribution diverges from another, crucial for ensuring reconstructed embeddings maintain the same relevance relationships as original dense embeddings
  - Quick check question: What distinguishes KL divergence from MSE in the context of embedding reconstruction?

- Concept: Sparse autoencoder architecture with TopK activation
  - Why needed here: TopK activation enforces exact sparsity by keeping only the k largest values active, creating interpretable features that can be manipulated for controllable retrieval
  - Quick check question: How does TopK activation differ from L1 regularization for inducing sparsity?

- Concept: Neuron to Graph (N2G) interpretation methodology
  - Why needed here: N2G converts neuron activation patterns into interpretable graph structures by identifying relevant tokens and their relationships, enabling semantic understanding of sparse features
  - Quick check question: What is the purpose of token pruning and augmentation in the N2G approach?

## Architecture Onboarding

- Component map:
  Encoder -> Sparse Latent Space (TopK activation) -> Decoder -> Reconstructed Embeddings
  Loss Function: MSE (reconstruction) + KLD (contrastive)
  N2G Interpreter: Extracts semantic meaning from sparse feature activations
  Manipulation Module: Amplifies specific features for controllable retrieval

- Critical path:
  1. Train sparse autoencoder with retrieval-oriented contrastive loss
  2. Evaluate reconstruction accuracy and retrieval performance
  3. Interpret sparse features using N2G
  4. Validate controllability through feature manipulation experiments

- Design tradeoffs:
  - Sparsity level (K): Higher K improves retrieval accuracy but reduces interpretability; lower K increases interpretability but may hurt retrieval
  - Loss weighting: Balance between MSE and KLD affects reconstruction fidelity vs retrieval effectiveness
  - Feature dimensionality: Higher dimensions capture more concepts but increase computational cost

- Failure signatures:
  - MSE too high: Reconstruction is inaccurate, features lose semantic meaning
  - Retrieval performance drops significantly: Contrastive loss not preserving relevance relationships
  - Interpreted features are nonsensical: N2G failing to capture meaningful patterns
  - Manipulation has no effect: Features not capturing semantically relevant information

- First 3 experiments:
  1. Train with only MSE loss, compare retrieval performance to full model
  2. Vary K (sparsity level) and measure trade-off between reconstruction accuracy and interpretability
  3. Perform feature manipulation on controlled binary perspective queries and measure retrieval bias

## Open Questions the Paper Calls Out
1. How do the sparse latent features generalize to other dense retrieval models beyond BGE and MiniCPM?
2. Can the relationship between sparse latent features and retrieval outcomes be established as causal rather than merely correlational?
3. What is the optimal number of active features (K) for balancing reconstruction accuracy and computational efficiency across different retrieval tasks?

## Limitations
- Unknown encoder/decoder architectures may affect reproducibility of results
- Optimal balance between MSE and KL divergence loss components not explicitly specified
- Generalization performance to other retrieval benchmarks and embedding models remains uncertain

## Confidence

- **High confidence**: The core mechanism of using retrieval-oriented contrastive loss to preserve query-document relationships while inducing sparsity is well-supported by both theoretical reasoning and experimental evidence. The trade-off between MRR performance and feature sparsity is clearly demonstrated.

- **Medium confidence**: The interpretability claims via N2G interpretation are convincing but depend heavily on the quality of the interpretation methodology. The semantic coherence of extracted features may vary across different domains or embedding models.

- **Medium confidence**: The controllability experiments show promising directional trends, but the quantitative improvements are relatively modest. The practical utility of feature amplification for real-world retrieval tasks requires further validation.

## Next Checks

1. **Architecture sensitivity analysis**: Systematically vary the encoder/decoder layer sizes and activation functions to determine how sensitive the retrieval performance and interpretability are to these architectural choices.

2. **Cross-dataset generalization**: Evaluate the learned sparse features and their controllability properties on datasets with different characteristics (e.g., long documents, specialized domains) to assess robustness.

3. **Ablation of loss components**: Train models with only MSE loss, only KL divergence loss, and various combinations to precisely quantify the contribution of each component to both retrieval performance and interpretability.