---
ver: rpa2
title: 'Exploring the Stability Gap in Continual Learning: The Role of the Classification
  Head'
arxiv_id: '2411.04723'
source_url: https://arxiv.org/abs/2411.04723
tags:
- task
- learning
- tasks
- stability
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the stability gap in continual learning
  by disentangling the contributions of the backbone and classification head. The
  authors introduce an oracle nearest-mean classifier (NMC) that uses all available
  training data to compute class prototypes, revealing that most of the stability
  gap stems from the linear classification head rather than insufficient representation
  learning in the backbone.
---

# Exploring the Stability Gap in Continual Learning: The Role of the Classification Head

## Quick Facts
- arXiv ID: 2411.04723
- Source URL: https://arxiv.org/abs/2411.04723
- Reference count: 40
- Primary result: NMC reduces stability gap by 2-4x compared to linear heads in continual learning

## Executive Summary
This paper investigates the stability gap in continual learning by disentangling the contributions of the backbone and classification head. The authors introduce an oracle nearest-mean classifier (NMC) that uses all available training data to compute class prototypes, revealing that most of the stability gap stems from the linear classification head rather than insufficient representation learning in the backbone. They demonstrate that using NMC with exemplars significantly improves both stability and final accuracy compared to standard fine-tuning across multiple benchmarks (CIFAR100, ImageNet100, CUB-200, FGVC Aircrafts), while also reducing task-recency bias.

## Method Summary
The paper introduces an oracle nearest-mean classifier (NMC) that computes class prototypes using all available training data and evaluates continual learning performance using both task-agnostic and task-aware metrics. The method is compared against standard linear head approaches across multiple image classification benchmarks with 5 or 10 task splits. The authors use ResNet18 backbone (also MobileNetV2, ResNet50, EfficientNet-B4, VGG11 in appendix) with 100 epochs per task, SGD optimizer with linearly decaying learning rate, batch size 128, and 2000 exemplar memory buffer (10 exemplars per class for fine-grained datasets).

## Key Results
- NMC improves final performance and significantly enhances training stability compared to linear heads
- Most of the stability gap (2-4x) is attributed to the linear classification head rather than backbone representation issues
- NMC reduces task-recency bias by treating all tasks equally in the prototype space
- Results hold across different memory buffer sizes, pre-trained models, and various CNN architectures

## Why This Works (Mechanism)

### Mechanism 1
The linear classification head is the primary source of the stability gap, not the backbone representations. During continual learning, the linear head learns task-specific weight updates that destabilize performance on previous tasks. NMC avoids this by using fixed class prototypes in the representation space, decoupling classification from task-specific weight updates.

### Mechanism 2
NMC reduces task-recency bias by treating all tasks equally in the prototype space. Linear heads trained on imbalanced task data (current task much larger than exemplar buffer) develop preference for recent task classes. NMC's prototype-based classification treats all classes symmetrically regardless of when they were learned.

### Mechanism 3
Oracle NMC demonstrates that classification head instability accounts for 2-4x more of the stability gap than backbone representation issues. Oracle NMC uses all available data to compute optimal prototypes, showing what performance is possible with perfect classification given the learned representations.

## Foundational Learning

- **Concept: Continual Learning Stability Gap**
  - Why needed here: Understanding what causes stability gaps is fundamental to why NMC helps
  - Quick check question: What is the difference between gradual forgetting and stability gap phenomenon?

- **Concept: Prototype-based Classification**
  - Why needed here: NMC's core mechanism relies on computing and using class prototypes
  - Quick check question: How does NMC compute class prototypes and what data does it use?

- **Concept: Task-agnostic vs Task-aware Accuracy**
  - Why needed here: The paper distinguishes between these evaluation metrics and their implications
  - Quick check question: What is the key difference between task-agnostic and task-aware accuracy evaluation?

## Architecture Onboarding

- **Component map:**
  - Backbone (Î˜) -> Linear Head (g) or NMC Classifier
  - Exemplar Buffer -> Prototype Calculator
  - Backbone -> Feature Extractor
  - NMC Classifier -> Computes distances to class prototypes

- **Critical path:**
  1. Backbone processes input to generate features
  2. For NMC: compute distances to class prototypes and assign label
  3. For linear head: compute dot product with weights and apply softmax
  4. During training: update backbone weights and either linear head weights or exemplar buffer

- **Design tradeoffs:**
  - Memory vs Stability: Larger exemplar buffers improve NMC prototype quality but increase memory usage
  - Computational cost: NMC requires computing distances to all class prototypes at inference
  - Backbone vs Head: NMC isolates backbone representation quality from classification head instability

- **Failure signatures:**
  - High LTB (Latest Task Prediction Bias): Exemplar buffer too small or unrepresentative
  - Degraded oracle NMC performance: Backbone representations not task-agnostic
  - NMC instability: Backbone features not discriminative enough for prototype separation

- **First 3 experiments:**
  1. Compare linear head vs NMC on CIFAR100 with 2000 exemplar buffer, measuring task 1 accuracy throughout training
  2. Measure stability gap reduction when switching from linear head to NMC on ImageNet100
  3. Test NMC with varying exemplar buffer sizes (100, 500, 1000, 2000) on CIFAR100 to find stability-memory tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the stability gap phenomenon manifest in continual learning approaches that use architectural methods like parameter isolation or network expansion?
- Basis in paper: [inferred] The paper mentions that the stability gap occurs even with joint incremental training and full replay, but does not investigate architectural methods.
- Why unresolved: The paper focuses on regularization-based and replay-based methods, leaving the impact of architectural approaches on the stability gap unexplored.

### Open Question 2
- Question: What is the relationship between the stability gap and the quality of learned representations in the backbone network?
- Basis in paper: [explicit] The paper demonstrates that the linear head is primarily responsible for the stability gap, but notes that the remaining portion could be attributed to insufficient representation learning.
- Why unresolved: The paper provides an upper bound on the stability gap caused by the backbone but does not quantify or characterize this contribution in detail.

### Open Question 3
- Question: How does the stability gap evolve when the number of classes per task changes or when task boundaries become less distinct?
- Basis in paper: [inferred] The paper evaluates fixed task splits with equal class distributions but does not explore variable task granularities or blurred task boundaries.
- Why unresolved: The experiments focus on discrete task boundaries with fixed class allocations, not examining scenarios where task distinctions are less clear.

### Open Question 4
- Question: What is the impact of the stability gap on more complex tasks beyond image classification, such as object detection, segmentation, or sequential decision-making?
- Basis in paper: [explicit] The paper limits its investigation to image classification benchmarks and mentions that the stability gap has been observed in LLM pre-training but does not explore other computer vision tasks.
- Why unresolved: The analysis is confined to classification accuracy metrics, leaving the effects on task-specific performance measures unexplored.

## Limitations

- The corpus evidence for the proposed mechanisms is weak, with no direct citations supporting the specific claims about NMC's superiority over linear heads in continual learning stability
- The paper does not explore alternative non-parametric classifiers beyond NMC, leaving open the possibility that other approaches might yield similar or better results
- The stability gap attribution to classification head vs backbone relies on oracle NMC performance, which may not generalize to all task complexities and dataset characteristics

## Confidence

- **High Confidence:** The empirical results showing NMC outperforming linear heads across multiple benchmarks and architectures are well-supported by the presented experiments
- **Medium Confidence:** The claim that classification head instability accounts for 2-4x more of the stability gap than backbone issues is supported by oracle NMC experiments but lacks external validation
- **Low Confidence:** The mechanism explaining why NMC reduces task-recency bias is primarily theoretical with limited empirical evidence beyond the observed performance improvements

## Next Checks

1. Conduct ablation studies comparing NMC against other non-parametric classifiers (e.g., nearest-neighbor, radial basis function classifiers) to verify that the benefits are specific to the prototype-based approach
2. Test the robustness of NMC's stability improvements across different task ordering strategies and more complex task boundaries to ensure the results generalize beyond the current experimental setup
3. Investigate the impact of backbone architecture choices (e.g., transformer-based models vs CNNs) on the relative contributions of classification head vs representation learning to the stability gap