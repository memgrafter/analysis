---
ver: rpa2
title: 'VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision'
arxiv_id: '2412.14446'
source_url: https://arxiv.org/abs/2412.14446
tags:
- driving
- vehicle
- action
- vlm-ad
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VLM-AD, a method that enhances end-to-end autonomous
  driving models by leveraging vision-language models (VLMs) as auxiliary teachers.
  The approach addresses the limitation of existing E2E models that rely solely on
  trajectory supervision without capturing underlying reasoning processes.
---

# VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision

## Quick Facts
- arXiv ID: 2412.14446
- Source URL: https://arxiv.org/abs/2412.14446
- Reference count: 40
- VLM-AD achieves 14.6% and 33.3% reduction in L2 planning error, 38.7% and 57.4% reduction in collision rate when integrated with UniAD and VAD on nuScenes

## Executive Summary
VLM-AD introduces a novel approach to enhance end-to-end autonomous driving models by leveraging vision-language models (VLMs) as auxiliary teachers. The method addresses the limitation of existing E2E models that rely solely on trajectory supervision without capturing underlying reasoning processes. By automatically generating reasoning-based text annotations and structured action labels through VLMs, VLM-AD provides supplementary supervision that improves planning accuracy, reduces collisions, and enhances overall driving performance. The approach distills VLM knowledge into existing E2E models through two auxiliary tasks - text feature alignment and structured action classification - without requiring VLMs during inference.

## Method Summary
VLM-AD enhances end-to-end autonomous driving by using vision-language models to generate reasoning-based text annotations and structured action labels as supplementary supervision. The method processes front-view images with projected trajectories through VLMs (specifically GPT-4o) to create both unstructured freeform text and structured action categories. These annotations are then encoded into feature representations that align with the model's ego feature through knowledge distillation. Two auxiliary heads are integrated into existing E2E models - one for text feature alignment using MHCA blocks and another for structured action classification. The approach is trained jointly with the primary planning task using weighted loss balancing, achieving significant improvements in planning accuracy and safety without requiring VLMs during inference.

## Key Results
- 14.6% and 33.3% reduction in L2 planning error when integrated with UniAD and VAD on nuScenes
- 38.7% and 57.4% reduction in collision rate under closed-loop evaluation
- Enhanced route completion and driving scores compared to baseline E2E methods
- Significant improvements in safety metrics while maintaining or improving planning accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLM-generated reasoning annotations provide supplementary supervision that captures underlying driving decision rationale
- Mechanism: VLMs process front-view images with projected trajectories to generate freeform text describing current status, future actions, and reasoning, then encode these annotations into feature representations that align with the model's ego feature through knowledge distillation
- Core assumption: VLMs can accurately interpret driving scenarios and generate meaningful reasoning that correlates with optimal driving decisions
- Evidence anchors:
  - [abstract] "leverages vision-language models (VLMs) as teachers to enhance training by providing additional supervision that incorporates unstructured reasoning information"
  - [section 3.1] "We employ GPT-4o... to automatically annotate our dataset. GPT-4o can interpret the scenario, generate suitable reasoning-based responses"
  - [corpus] Weak - no direct corpus evidence of VLM accuracy for driving reasoning
- Break condition: VLM reasoning becomes inconsistent with actual optimal driving behavior or fails to capture relevant environmental context

### Mechanism 2
- Claim: Structured action labels from VLMs provide interpretable action predictions that improve planning accuracy
- Mechanism: VLMs select actions from predefined categories (control, turn, lane) based on scene interpretation, which are then converted to one-hot labels and supervised through action classification heads using cross-entropy loss
- Core assumption: VLM-selected actions from predefined categories accurately represent optimal driving decisions for given scenarios
- Evidence anchors:
  - [section 3.1] "we create three distinct action sets and prompt the VLM to select answers from these predefined options"
  - [section 4.4] "Our action text head correctly outputs the control action 'go straight' for all three cases"
  - [corpus] Weak - no corpus evidence of VLM action selection accuracy
- Break condition: VLM action selections become biased toward certain categories or fail to represent nuanced driving scenarios

### Mechanism 3
- Claim: Feature alignment through knowledge distillation with temperature scaling balances smoothness and sharpness of teacher-student features
- Mechanism: Output features from auxiliary heads are normalized with different temperature parameters compared to VLM text features, then aligned using cross-entropy loss to encourage rich representation learning
- Core assumption: Temperature scaling effectively controls the trade-off between smooth and sharp feature distributions for optimal knowledge transfer
- Evidence anchors:
  - [section 3.2] "we adopt a similar strategy to normalize the text and output features with different temperature parameters... producing feature distributions rather than raw feature values"
  - [section 4.3] "MSE loss performs slightly better than UniAD, by minimizing the Euclidean distance between features, driving outputs toward their mean that causes information loss"
  - [corpus] Weak - no corpus evidence of temperature scaling effectiveness
- Break condition: Temperature parameters become poorly tuned, causing either overly smooth features that lose discriminative information or overly sharp features that overfit to VLM annotations

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Transfers reasoning capabilities from VLMs to E2E models without requiring VLMs at inference time
  - Quick check question: How does temperature scaling in knowledge distillation affect the smoothness of feature distributions?

- Concept: Multi-task Learning
  - Why needed here: Enables joint training of planning task with auxiliary text alignment and action classification tasks to learn richer representations
  - Quick check question: What is the role of weighted loss balancing (λ1, λ2) in multi-task learning for this approach?

- Concept: Vision-Language Models
  - Why needed here: Provides reasoning capabilities by integrating language and vision for multimodal understanding of driving scenarios
  - Quick check question: How do VLMs process visual inputs differently from traditional computer vision models?

## Architecture Onboarding

- Component map: Front-end multi-view camera input → Trajectory projection onto front-view image → VLM annotation generation (reasoning + action labels) → E2E model with ego feature extraction → Auxiliary heads (text feature alignment + structured action classification) → Planning output + auxiliary predictions
- Critical path: Visual input → VLM annotation generation → Feature encoding → Auxiliary head processing → Loss computation → Parameter updates
- Design tradeoffs: Single front-view image vs. full 360-degree view (computational efficiency vs. comprehensive scene understanding), freeform reasoning vs. structured actions (richness of information vs. interpretability), VLM annotation quality vs. annotation speed
- Failure signatures: Degraded planning performance despite VLM supervision, VLM annotation inconsistencies with ground truth, temperature parameter sensitivity causing training instability, action classification heads producing incorrect predictions
- First 3 experiments:
  1. Test VLM annotation quality on a small subset of nuScenes data by comparing VLM-generated reasoning with human annotations
  2. Evaluate different temperature parameter settings (τs, τt) on a validation set to find optimal balance for feature alignment
  3. Compare planning performance with different combinations of auxiliary tasks (text alignment only, action classification only, both) to identify most effective supervision strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of VLM-generated annotations vary across different weather and lighting conditions, and what is the optimal temperature parameter setting for each condition?
- Basis in paper: [inferred] The paper mentions that in a rainy day scenario (Fig. 13), the VLM successfully identified the red traffic light and predicted future movement, but also shows an imperfect annotation example (Fig. 16) where the pedestrian traffic light was mistakenly considered in rainy conditions.
- Why unresolved: The paper only provides limited examples of annotations under different weather conditions and doesn't systematically evaluate how VLM annotation quality varies across diverse environmental conditions or optimize temperature parameters for each scenario.
- What evidence would resolve it: A comprehensive study testing VLM annotation accuracy across a wide range of weather conditions (rain, fog, night, snow, bright sunlight) with temperature parameter optimization for each condition, along with quantitative accuracy metrics for each scenario.

### Open Question 2
- Question: What is the impact of using structured action annotations (Q2) versus freeform reasoning annotations (Q1) on long-term driving performance and safety in closed-loop simulation beyond the nuScenes dataset?
- Basis in paper: [explicit] The paper shows that Q1 yields better results than Q2 in open-loop planning on nuScenes (Tab. 1), and states "Q1 yields better results than Q2, verifying the value of supervising the driving model through rich reasoning information."
- Why unresolved: The paper only evaluates these approaches on nuScenes open-loop planning and doesn't test long-term closed-loop performance or safety in more diverse driving environments and simulators.
- What evidence would resolve it: Comparative evaluation of both approaches in closed-loop simulation across multiple datasets and driving simulators (CARLA, LGSVL, etc.) measuring not just planning error but also safety metrics like collision rates, rule violations, and passenger comfort over extended driving sessions.

### Open Question 3
- Question: How does the VLM-AD framework perform when integrated with end-to-end driving models trained on different datasets or in different geographic regions?
- Basis in paper: [inferred] The paper only tests VLM-AD on nuScenes dataset and with UniAD and VAD models, stating "We use the nuScenes dataset for open-loop planning evaluation" and validating with two specific methods.
- Why unresolved: The paper doesn't explore generalization to other datasets (Waymo, Argoverse, etc.) or different geographic driving styles (European vs. American vs. Asian driving patterns) which could significantly impact the effectiveness of VLM-based reasoning.
- What evidence would resolve it: Cross-dataset and cross-regional evaluation showing performance metrics when VLM-AD is applied to models trained on different driving datasets from various geographic regions, including comparison of reasoning quality and planning performance across these different contexts.

## Limitations

- The approach depends heavily on the quality and consistency of VLM-generated annotations, but the paper does not provide direct evidence that GPT-4o's reasoning aligns with human expert driving decisions.
- The temperature scaling parameters (τs=0.1, τt=0.04) are specified but their sensitivity to dataset variations is not explored.
- The action classification relies on predefined categories that may not capture all driving scenarios, potentially limiting generalization to edge cases.

## Confidence

- Confidence in VLM annotation quality: Medium - weak corpus evidence of VLM accuracy for driving reasoning
- Confidence in temperature scaling effectiveness: Medium - limited exploration of parameter sensitivity
- Confidence in action classification accuracy: Medium - no corpus evidence of VLM action selection accuracy
- Confidence in generalization: Medium - only tested on nuScenes with two specific E2E methods

## Next Checks

1. **Cross-dataset generalization test**: Evaluate VLM-AD performance on a different autonomous driving dataset (e.g., Waymo Open Dataset or Argoverse) to assess whether the improvements generalize beyond nuScenes.

2. **Ablation study on VLM annotation quality**: Conduct a systematic evaluation comparing VLM-generated annotations with human expert annotations across diverse driving scenarios to quantify annotation consistency and identify failure modes.

3. **Real-world closed-loop deployment validation**: Test the trained VLM-AD models in real autonomous vehicles under varying weather conditions and traffic densities to verify that simulation performance translates to practical driving safety and reliability.