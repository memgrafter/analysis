---
ver: rpa2
title: 'TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced CTR
  Prediction at Kuaishou'
arxiv_id: '2407.16357'
source_url: https://arxiv.org/abs/2407.16357
tags:
- user
- behaviors
- cluster
- behavior
- kuaishou
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of modeling extremely long user
  behavior sequences (up to 10^6 items) for click-through rate prediction in large-scale
  recommendation systems. The authors propose TWIN-V2, an enhancement of TWIN, which
  employs a divide-and-conquer approach using hierarchical clustering to compress
  life-cycle behaviors into manageable clusters.
---

# TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced CTR Prediction at Kuaishou

## Quick Facts
- arXiv ID: 2407.16357
- Source URL: https://arxiv.org/abs/2407.16357
- Reference count: 35
- Primary result: Hierarchical clustering + cluster-aware target attention improves CTR prediction on ultra-long sequences (up to 10^6 items)

## Executive Summary
This paper addresses the challenge of modeling extremely long user behavior sequences for click-through rate prediction in large-scale recommendation systems. The authors propose TWIN-V2, an enhancement of TWIN, which employs a divide-and-conquer approach using hierarchical clustering to compress life-cycle behaviors into manageable clusters. The core innovation is a cluster-aware target attention mechanism that reweights attention scores based on cluster sizes, allowing for more accurate and diverse modeling of user interests. Extensive experiments on a multi-billion-scale industrial dataset and online A/B tests demonstrate TWIN-V2's effectiveness, achieving a 0.16% improvement in AUC and 0.33% in GAUC over TWIN, with even greater gains for highly active users.

## Method Summary
TWIN-V2 addresses ultra-long user behavior sequences (up to 10^6 items) through hierarchical clustering that groups similar items into clusters based on k-means clustering over item embeddings, recursively splitting until cluster sizes fall below a threshold. Each cluster is represented by a virtual item combining average numerical features and centroid-proximal categorical features. The core innovation is cluster-aware target attention, which reweights attention scores by adding the logarithm of cluster size to reflect user preference strength. The method also splits behavior features into inherent (cached, precomputed) and cross (simplified projection) parts to reduce computational complexity. The two-stage approach (GSU and ESU) uses these mechanisms for both short-term and life-cycle behavior modeling.

## Key Results
- 0.16% improvement in AUC and 0.33% in GAUC over TWIN baseline
- Successfully deployed at Kuaishou serving hundreds of millions of daily active users
- Greater performance gains observed for highly active users with longer behavior sequences
- Achieves more diverse recommendation results compared to TWIN

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical clustering reduces the effective length of user behavior sequences by grouping similar items, enabling life-cycle modeling within computational limits. Items are grouped into clusters based on k-means clustering over item embeddings, recursively splitting until cluster sizes fall below a threshold. Each cluster is then represented by a virtual item combining average numerical features and centroid-proximal categorical features.

Core assumption: Similar user behaviors (e.g., multiple videos from the same topic) can be aggregated without losing discriminative signal for CTR prediction.

### Mechanism 2
Cluster-aware target attention reweights attention scores by cluster size to reflect the strength of user preference. The relevance score between a target item and a cluster is adjusted by adding the logarithm of the cluster size, so larger clusters (more user interactions) receive higher attention weights during both GSU and ESU stages.

Core assumption: The number of items in a cluster is proportional to the user's interest strength in that cluster's topic.

### Mechanism 3
Splitting behavior features into inherent and cross parts reduces computational complexity while preserving essential information for target attention. Inherent features (e.g., video ID, author) are shared across users and cached; cross features (e.g., play time, interaction context) are projected using simplified diagonal block matrices, reducing projection from full matrix multiplication to lightweight per-feature scaling.

Core assumption: Inherent features dominate the attention computation and can be precomputed, while cross features contribute linearly and sparsely.

## Foundational Learning

- **Concept: Hierarchical clustering and k-means algorithm**
  - Why needed here: To compress ultra-long behavior sequences into manageable clusters without losing topical coherence.
  - Quick check question: What happens to cluster quality if the maximum cluster size ùõæ is set too large or too small?

- **Concept: Target attention mechanism in CTR prediction**
  - Why needed here: To measure the relevance between user historical behaviors (clustered) and the target item, enabling personalized ranking.
  - Quick check question: How does the Softmax(ùú∂ ‚Ä≤) change when cluster sizes vary significantly?

- **Concept: Feature engineering for categorical vs. numerical features**
  - Why needed here: To create meaningful virtual items for clusters by appropriately aggregating different feature types.
  - Quick check question: Why is the closest item to the centroid chosen for categorical features instead of averaging?

## Architecture Onboarding

- **Component map**: Offline: Hierarchical clustering runner ‚Üí item grouping by play completion ratio ‚Üí recursive k-means clustering ‚Üí cluster representation extraction ‚Üí embedding server update
  Online: Request handler ‚Üí GSU (cluster-aware target attention) ‚Üí top-100 cluster retrieval ‚Üí ESU (cluster-aware target attention) ‚Üí CTR prediction ‚Üí model parameter update via nearline trainer

- **Critical path**: Request ‚Üí offline-prepared cluster features ‚Üí GSU scoring ‚Üí ESU aggregation ‚Üí CTR model ‚Üí response

- **Design tradeoffs**:
  - Cluster size vs. representation fidelity: Smaller clusters preserve detail but increase sequence length; larger clusters compress more but may blur distinctions.
  - Update frequency vs. freshness: More frequent clustering updates capture recent behavior changes but increase computational load.
  - Caching inherent features vs. memory usage: Precomputing speeds up inference but requires significant storage for large vocabularies.

- **Failure signatures**:
  - Performance degradation if cluster quality drops (e.g., items in a cluster become topically diverse).
  - Latency spikes if hierarchical clustering is not precomputed or if embedding server sync lags.
  - Accuracy drop if cross feature projections oversimplify interactions.

- **First 3 experiments**:
  1. Vary ùõæ (max cluster size) and measure impact on CTR accuracy and clustering runtime.
  2. Disable cluster size reweighting in attention and compare AUC/GAUC to baseline.
  3. Replace hierarchical clustering with uniform sampling of behaviors and measure trade-off between sequence length and prediction performance.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of TWIN-V2 scale with varying values of the maximum cluster size (Œ≥) and the group number (M) in the hierarchical clustering approach? The paper mentions that Œ≥ is set to 20 and M is set to 5 in their practices, but does not explore the impact of varying these hyperparameters. Conducting experiments with different values of Œ≥ and M, and comparing the resulting performance metrics (e.g., AUC, GAUC) would provide insights into the optimal settings for these hyperparameters.

### Open Question 2
What is the impact of the hierarchical clustering method on the diversity of the recommended results, and how does it compare to other clustering techniques? The paper mentions that TWIN-V2 achieves more diverse recommendation results compared to TWIN, but does not provide a detailed analysis of the impact of the hierarchical clustering method on diversity. Conducting experiments with different clustering techniques (e.g., k-means, DBSCAN) and measuring the diversity of the recommended results using metrics such as topic coverage or novelty would provide insights into the effectiveness of the hierarchical clustering method.

### Open Question 3
How does the cluster-aware target attention mechanism perform in scenarios with rapidly changing user interests or short-term trends? The paper focuses on modeling long-term user interests, but does not address the performance of the model in scenarios with rapidly changing user interests or short-term trends. Conducting experiments with datasets that exhibit rapidly changing user interests or short-term trends, and comparing the performance of TWIN-V2 with other models that incorporate short-term interest modeling would provide insights into the effectiveness of the cluster-aware target attention mechanism in such scenarios.

## Limitations
- Core assumption that cluster size is proportional to user interest strength lacks empirical validation
- Performance gains, while statistically significant, are modest (0.16% AUC, 0.33% GAUC)
- No detailed ablation studies on hyperparameter sensitivity (Œ≥, Œ¥)
- No discussion of how clustering handles temporal dynamics in user behavior sequences

## Confidence

- **High Confidence**: The hierarchical clustering approach effectively compresses long user behavior sequences into manageable clusters, as demonstrated by the significant reduction in sequence length and the ability to model life-cycle behaviors within computational limits.

- **Medium Confidence**: The claim that splitting behavior features into inherent and cross parts reduces computational complexity while preserving essential information is supported by the theoretical analysis and the reported performance improvements.

- **Low Confidence**: The assertion that the number of items in a cluster is proportional to the user's interest strength is not empirically validated. The paper does not provide evidence that cluster size is a reliable indicator of user interest.

## Next Checks
1. Conduct experiments to systematically vary the maximum cluster size (Œ≥) and measure its impact on both clustering runtime and CTR prediction accuracy.

2. Compare the performance of TWIN-V2 with and without the cluster size reweighting in the attention mechanism to isolate the contribution of the size-based reweighting.

3. Design experiments to assess how well the hierarchical clustering approach handles temporal dynamics in user behavior sequences by evaluating performance on sequences with varying degrees of temporal recency and diversity.