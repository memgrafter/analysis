---
ver: rpa2
title: Black-Box Forgetting
arxiv_id: '2411.00409'
source_url: https://arxiv.org/abs/2411.00409
tags:
- classes
- forgetting
- latent
- ours
- black-box
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the novel Black-Box Forgetting problem, where
  the goal is to selectively forget specific classes in a black-box pre-trained vision-language
  model (like CLIP) without access to its internal parameters or gradients. The authors
  propose a method that optimizes input text prompts using derivative-free optimization
  (CMA-ES) to reduce classification accuracy for specified classes while maintaining
  accuracy for others.
---

# Black-Box Forgetting

## Quick Facts
- arXiv ID: 2411.00409
- Source URL: https://arxiv.org/abs/2411.00409
- Authors: Yusuke Kuwana; Yuta Goto; Takashi Shibata; Go Irie
- Reference count: 15
- Key outcome: Selective forgetting of classes in black-box vision-language models using derivative-free optimization and Latent Context Sharing

## Executive Summary
This paper introduces the novel Black-Box Forgetting problem, where the goal is to selectively forget specific classes in a black-box pre-trained vision-language model (like CLIP) without access to its internal parameters or gradients. The authors propose a method that optimizes input text prompts using derivative-free optimization (CMA-ES) to reduce classification accuracy for specified classes while maintaining accuracy for others. To address the challenge of high-dimensional optimization, they introduce Latent Context Sharing (LCS), which parameterizes each context in the prompt with shared low-dimensional latent components, significantly improving optimization efficiency and effectiveness.

## Method Summary
The method involves optimizing input text prompts for a black-box vision-language model using derivative-free optimization (CMA-ES). The key innovation is Latent Context Sharing (LCS), which reduces the dimensionality of the optimization space by parameterizing each prompt token with shared low-dimensional latent components. The optimization objective combines entropy maximization for classes to be forgotten with cross-entropy minimization for classes to be memorized. The approach is evaluated on four benchmark datasets (CIFAR-10, CIFAR-100, CUB-200-2011, ImageNet30) in a few-shot setting, demonstrating superior performance compared to baselines and comparable results to white-box methods.

## Key Results
- LCS significantly reduces optimization dimensionality while maintaining performance, achieving stability even with minimal latent contexts (m=1)
- The proposed method outperforms reasonable baselines (BBT, CBBT) on four benchmark datasets
- Performance is comparable to white-box methods like CoOp, demonstrating the effectiveness of black-box forgetting
- The method successfully achieves selective forgetting while maintaining accuracy on memorized classes across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Derivative-free optimization with CMA-ES is effective for black-box prompt tuning when high-dimensional optimization is avoided.
- **Mechanism:** Instead of directly optimizing high-dimensional token embeddings, LCS parameterizes each token with low-dimensional latent components (shared + unique), reducing the optimization space while preserving contextual dependencies.
- **Core assumption:** The shared latent components capture semantic correlations between prompt tokens, making optimization more efficient.
- **Evidence anchors:**
  - [abstract] "To avoid difficult high-dimensional optimization while ensuring high forgetting performance, we propose Latent Context Sharing (LCS), which introduces common low-dimensional latent components among multiple tokens for the prompt."
  - [section 3.1] "Despite the number of parameters prepared for BBT and LCS is the same, LCS is possible to significantly reduce the number of optimization dimensions compared to BBT."
- **Break condition:** If semantic correlations between prompt tokens are weak or non-existent, shared components would provide no optimization benefit.

### Mechanism 2
- **Claim:** Entropy maximization for forgotten classes combined with cross-entropy minimization for memorized classes achieves selective forgetting.
- **Mechanism:** For classes to be forgotten, maximizing entropy makes confidence scores uniform (random), while minimizing cross-entropy for memorized classes preserves their accuracy.
- **Core assumption:** The black-box model's confidence scores are meaningful and can be manipulated through prompt optimization to achieve the desired forgetting behavior.
- **Evidence anchors:**
  - [abstract] "Given that information on the model is unavailable, we optimize the input prompt to decrease the accuracy of specified classes through derivative-free optimization."
  - [section 3.2] "We maximize the entropy of confidence for each image of the classes to be forgotten" and "The cross-entropy loss is used for the classes to be memorized to maintain the classification accuracy."
- **Break condition:** If the black-box model doesn't provide meaningful confidence scores or if confidence scores are not sensitive to prompt changes.

### Mechanism 3
- **Claim:** Shared latent components provide better optimization efficiency than independent optimization of all latent contexts.
- **Mechanism:** LCS decomposes each latent context into shared components (common across tokens) and unique components, allowing more efficient optimization by reducing dimensionality and capturing token dependencies.
- **Core assumption:** The optimization landscape is smoother when shared components are introduced, making derivative-free optimization more effective.
- **Evidence anchors:**
  - [section 3.1] "Despite the number of parameters prepared for BBT and LCS is the same, LCS is possible to significantly reduce the number of optimization dimensions compared to BBT."
  - [section 4.3.1] "While BBT suffers a sharp drop in accuracy with decreasing m, our method shows only a small decrease from m = 16 even when m = 1."
- **Break condition:** If the optimization landscape is too rugged or if shared components introduce harmful correlations between unrelated tokens.

## Foundational Learning

- **Concept: Derivative-free optimization (CMA-ES)**
  - Why needed here: The black-box model doesn't provide gradients, making gradient-based optimization impossible.
  - Quick check question: What optimization method would you use if you can only evaluate function values but not gradients?

- **Concept: Vision-language models (VLMs) like CLIP**
  - Why needed here: The method builds on CLIP's zero-shot classification capability and its text encoder's learnable prompt tokens.
  - Quick check question: How does CLIP perform zero-shot classification without task-specific fine-tuning?

- **Concept: Entropy and cross-entropy loss functions**
  - Why needed here: Entropy maximization is used to forget classes, while cross-entropy is used to maintain accuracy for memorized classes.
  - Quick check question: What happens to the confidence distribution when you maximize entropy?

## Architecture Onboarding

- **Component map:** Image → CLIP → Confidence scores → Loss computation → CMA-ES sampling → Prompt update → Repeat until convergence

- **Critical path:** Image → CLIP → Confidence scores → Loss computation → CMA-ES sampling → Prompt update → Repeat until convergence

- **Design tradeoffs:**
  - Dimensionality reduction vs. representation power: LCS trades some representation capacity for optimization efficiency
  - Shared vs. unique components: Shared components capture token dependencies but may introduce unwanted correlations
  - CMA-ES population size vs. computational cost: Larger populations explore better but are more expensive

- **Failure signatures:**
  - No forgetting progress: Check if entropy maximization is working (confidence distributions should become flatter)
  - Accuracy drop in memorized classes: Check cross-entropy loss and ensure proper weighting
  - Slow convergence: Check dimensionality of latent contexts and CMA-ES parameters

- **First 3 experiments:**
  1. Verify basic setup works: Run on CIFAR-10 with 1-2 classes to forget, check if forgetting occurs
  2. Test LCS effectiveness: Compare with BBT baseline using same number of dimensions
  3. Sensitivity analysis: Vary number of latent contexts and check performance stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the black-box forgetting method perform when the model has no access to context embeddings and only final classification results are available?
- Basis in paper: [explicit] The paper acknowledges this limitation in section 5, stating "There should be models in the real world with a higher level of 'black boxness,' i.e., models in which even access to contextual embeddedness is prohibited."
- Why unresolved: The current method assumes access to context embeddings, which is a common black-box setting but not the most restrictive scenario possible.
- What evidence would resolve it: Experimental results comparing the proposed method with a truly black-box scenario where only final classification outputs are accessible, and demonstrating whether performance degrades significantly or if alternative approaches are needed.

### Open Question 2
- Question: Can the LCS method be extended to handle cases where some classes have training samples available while others do not?
- Basis in paper: [explicit] Section A.4 shows preliminary experiments combining few-shot and zero-shot approaches, finding that combining them outperforms either approach alone.
- Why unresolved: The paper only explores this combination in one specific scenario (half classes with samples, half without) and doesn't provide a systematic framework for handling arbitrary distributions of available samples.
- What evidence would resolve it: A comprehensive study testing various ratios and distributions of available samples across classes, and developing a principled method for automatically determining when to use few-shot vs zero-shot approaches for different classes.

### Open Question 3
- Question: What is the relationship between the dimensionality of shared vs unique latent contexts and the forgetting performance across different types of datasets?
- Basis in paper: [explicit] Section 4.3.2 shows sensitivity analysis on CIFAR-10, CIFAR-100, and CUB-200-2011, finding that performance is robust across a wide range of ratios.
- Why unresolved: The analysis is limited to three datasets and doesn't explore whether the optimal ratio varies systematically with dataset characteristics like number of classes, class similarity, or image complexity.
- What evidence would resolve it: Experiments on a diverse set of datasets varying in these characteristics, with analysis to identify patterns in optimal ratio selection, potentially leading to a heuristic or adaptive method for setting these dimensions.

## Limitations

- The method relies on meaningful and sensitive confidence scores from the black-box model, which may not hold for all models
- Only tested on relatively small benchmark datasets (CIFAR-10, CIFAR-100, CUB-200-2011, ImageNet30), not validated on larger-scale vision datasets
- Computationally expensive derivative-free optimization (CMA-ES) limits scalability compared to gradient-based methods
- Assumes access to context embeddings, which may not be available in more restrictive black-box scenarios

## Confidence

**High Confidence Claims:**
- LCS reduces optimization dimensionality compared to BBT while maintaining parameter count - well-supported by quantitative comparisons
- The proposed method achieves selective forgetting on benchmark datasets - demonstrated across four different datasets
- Performance comparable to white-box methods like CoOp - explicitly shown in experimental results

**Medium Confidence Claims:**
- Shared latent components capture semantic correlations between prompt tokens - supported by optimization efficiency gains but lacks direct analysis
- Entropy maximization effectively achieves forgetting - demonstrated empirically but mechanism could benefit from deeper analysis
- Derivative-free optimization is viable for black-box prompt tuning - proven effective but computational cost needs more analysis

**Low Confidence Claims:**
- The method generalizes to arbitrary black-box models beyond CLIP - only tested on CLIP architecture
- The approach scales to larger datasets and more complex forgetting scenarios - limited to relatively small datasets
- Shared components always provide optimization benefits - effectiveness depends on prompt structure not fully explored

## Next Checks

1. **Generalization Test:** Apply the black-box forgetting method to a different VLM architecture (e.g., OpenCLIP, OpenCLIP-ViT-L-14) to verify that the approach works beyond the specific CLIP model used in experiments. Measure forgetting performance and optimization efficiency on CIFAR-10.

2. **Ablation on Shared Components:** Create an ablation study comparing LCS with different configurations: (a) all unique components (no sharing), (b) all shared components, (c) varying ratios of shared vs unique components. Analyze which configuration performs best and under what conditions.

3. **Confidence Score Analysis:** For a representative experiment, visualize the confidence score distributions for forgotten vs memorized classes before and after optimization. Calculate entropy values and verify that entropy maximization is actually driving the forgetting behavior as claimed.