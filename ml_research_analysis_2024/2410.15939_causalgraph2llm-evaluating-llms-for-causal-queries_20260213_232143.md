---
ver: rpa2
title: 'CausalGraph2LLM: Evaluating LLMs for Causal Queries'
arxiv_id: '2410.15939'
source_url: https://arxiv.org/abs/2410.15939
tags:
- causal
- graph
- node
- llms
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CausalGraph2LLM, a benchmark to evaluate
  how well large language models (LLMs) encode and reason about causal directed acyclic
  graphs (DAGs). The authors categorize causal queries into graph-level and node-level
  types and test multiple encoding formats (JSON, adjacency matrix, GraphML, etc.)
  across several open-source and proprietary LLMs.
---

# CausalGraph2LLM: Evaluating LLMs for Causal Queries

## Quick Facts
- arXiv ID: 2410.15939
- Source URL: https://arxiv.org/abs/2410.15939
- Reference count: 40
- Primary result: Encoding format choice can cause up to 60% variance in LLM performance on causal graph queries

## Executive Summary
This paper introduces CausalGraph2LLM, a benchmark to evaluate how well large language models encode and reason about causal directed acyclic graphs (DAGs). The authors categorize causal queries into graph-level and node-level types and test multiple encoding formats across several open-source and proprietary LLMs. They find that model performance varies significantly—up to 60%—depending on the encoding used, with some formats aligning better with specific query types. LLMs also show sensitivity to contextual knowledge in variable names, sometimes introducing biases or overestimation errors. Node-level queries generally yield higher accuracy than graph-level ones, and downstream causal intervention tasks are affected by encoding choice. The results highlight the need for careful encoding selection, fine-tuning, and bias mitigation when deploying LLMs for causal reasoning tasks.

## Method Summary
The CausalGraph2LLM benchmark evaluates LLMs on causal graph encoding and reasoning tasks using synthetic and real-world DAGs (e.g., Insurance, Alarm datasets). Graphs are encoded in seven formats (JSON, adjacency matrix, GraphML, GraphViz, single/multi-node) and paired with prompt templates for graph-level (source, sink, mediator, confounder) and node-level (parent, child) queries. LLMs are tested zero-shot across open-source and proprietary models, with performance measured using F1 scores for graph-level and binary accuracy for node-level queries. The benchmark also explores the impact of contextual variable names and evaluates downstream intervention tasks.

## Key Results
- Encoding format choice causes up to 60% variance in LLM performance on causal queries
- Node-level queries (parent, child) achieve higher accuracy than graph-level queries (source, sink, mediator, confounder)
- Contextual variable names improve performance but introduce bias from pretraining knowledge
- Fine-tuning on specific encodings (e.g., adjacency matrix) can improve performance by 10-20%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can encode causal graphs as text and answer causal queries when the graph is represented in a compatible format.
- Mechanism: The LLM parses the graph representation (e.g., JSON, adjacency matrix) and uses its internal knowledge to infer causal relationships between variables. This allows it to answer queries like "Which nodes are sources?" or "Which nodes are mediators?"
- Core assumption: The model has been exposed to graph-like structures during pretraining, so it can interpret textual graph representations without explicit training on causal graphs.
- Evidence anchors:
  - [abstract] "These tasks necessitate the LLMs to encode the causal graph effectively for subsequent downstream tasks."
  - [section] "By introducing the benchmark, we aim to shed light on the strengths and limitations of these models in encoding causal graphs."
  - [corpus] Weak evidence. No corpus paper directly validates this encoding ability.
- Break condition: If the encoding format is incompatible with the model's pretraining distribution, performance drops sharply.

### Mechanism 2
- Claim: The encoding format strongly influences LLM performance on causal queries.
- Mechanism: Different encodings (JSON, adjacency matrix, GraphML, etc.) present causal relationships in varying levels of abstraction. Some formats may align better with the model's internal representation of graphs, leading to higher accuracy.
- Core assumption: The LLM's pretraining data included diverse graph-like structures, so encoding choice affects its ability to interpret and reason about them.
- Evidence anchors:
  - [abstract] "Our findings reveal that while LLMs show promise in this domain, they are highly sensitive to the encoding used."
  - [section] "We observe up to a 60% variation in performance across different graph encodings."
  - [corpus] Weak evidence. No corpus paper directly addresses encoding sensitivity.
- Break condition: If the encoding is ambiguous or overly complex, the model may fail to extract correct causal relationships.

### Mechanism 3
- Claim: Contextual variable names in causal graphs improve LLM performance by leveraging pretraining knowledge.
- Mechanism: When variable names are semantically meaningful (e.g., "heart rate", "blood pressure"), the LLM can draw on its pretraining knowledge to infer plausible causal relationships, leading to higher accuracy.
- Core assumption: The model's pretraining data included domain-specific knowledge that can be activated by contextual variable names.
- Evidence anchors:
  - [abstract] "We observe that LLMs can often display biases when presented with contextual information about a causal graph, potentially stemming from their parametric memory."
  - [section] "We specifically consider two popular causal DAGs - Insurance and Alarm. These graphs were presented in two formats: one set featured contextual causal knowledge with semantically meaningful labels, and the other set consisted of the same graphs labeled with random identifiers."
  - [corpus] Weak evidence. No corpus paper directly validates this contextual bias.
- Break condition: If the contextual knowledge contradicts the actual graph structure, the model may produce overconfident but incorrect answers.

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs)
  - Why needed here: Causal graphs are represented as DAGs, so understanding their structure is essential for interpreting and reasoning about causal relationships.
  - Quick check question: In a DAG, can a node have a directed path back to itself?

- Concept: Causal relationships (parent, child, mediator, confounder)
  - Why needed here: These are the fundamental relationships that LLMs must identify and reason about when answering causal queries.
  - Quick check question: If node A causes node B, and node B causes node C, what is the relationship between A and C?

- Concept: Graph encoding formats (JSON, adjacency matrix, GraphML, etc.)
  - Why needed here: LLMs receive causal graphs as text, so understanding how different encodings represent the same graph is crucial for interpreting model performance.
  - Quick check question: Which encoding explicitly lists edges as ordered pairs?

## Architecture Onboarding

- Component map:
  - Causal graph generation (synthetic + real-world datasets) -> Graph encoding (JSON, adjacency, GraphML, GraphViz, single/multi node) -> LLM inference (prompting with encoded graph + query) -> Evaluation (F1 score for graph-level queries, binary accuracy for node-level queries)
- Critical path:
  - Generate causal graph → Encode graph → Prompt LLM → Evaluate answer
- Design tradeoffs:
  - Encoding complexity vs. LLM interpretability: Simpler encodings may be easier for the model to parse but may lose information.
  - Contextual vs. non-contextual variables: Contextual names improve performance but introduce bias; non-contextual names are neutral but harder for the model.
  - Graph density vs. model performance: Denser graphs are more challenging for the model to process.
- Failure signatures:
  - Low performance across all encodings: The model may not have sufficient pretraining on graph-like structures.
  - High variance across encodings: The model is highly sensitive to how the graph is represented.
  - Overconfidence with contextual variables: The model may rely too heavily on its pretraining knowledge rather than the actual graph structure.
- First 3 experiments:
  1. Test LLM performance on a simple 3-node causal graph using all 7 encodings to identify the best-performing format.
  2. Compare performance on contextual vs. non-contextual versions of the same graph to quantify the impact of variable names.
  3. Evaluate node-level vs. graph-level query performance to understand the model's strengths and weaknesses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different encoding strategies affect the accuracy of causal reasoning in LLMs, and can adaptive encoding improve performance?
- Basis in paper: [explicit] The paper shows that encoding strategies like JSON, adjacency matrix, and GraphML yield significantly different performance, with deviations up to 60%.
- Why unresolved: The study focuses on benchmarking performance but does not explore adaptive or dynamic encoding strategies that could optimize results for specific tasks.
- What evidence would resolve it: Comparative experiments testing adaptive encoding methods tailored to specific query types or graph structures.

### Open Question 2
- Question: How can biases from pretraining data, such as over-reliance on semantically meaningful variable names, be mitigated in causal reasoning tasks?
- Basis in paper: [explicit] The authors observe that LLMs perform better with contextual knowledge but also introduce biases, leading to overestimation or underestimation of causal relationships.
- Why unresolved: While the study identifies these biases, it does not propose methods to mitigate them or test the robustness of LLMs on less conventional causal graphs.
- What evidence would resolve it: Experiments evaluating bias mitigation techniques, such as fine-tuning on diverse datasets or using de-biasing strategies.

### Open Question 3
- Question: How does the complexity of causal graphs, such as node count and edge density, impact the performance of LLMs, and are there thresholds where performance degrades significantly?
- Basis in paper: [inferred] The authors note that performance decreases with increasing graph complexity but do not explore specific thresholds or structural factors contributing to this decline.
- Why unresolved: The study uses synthetic graphs with varying complexity but does not analyze the relationship between structural factors and performance in detail.
- What evidence would resolve it: Detailed analysis of performance trends across graphs with systematically varied node counts, edge densities, and structural properties.

## Limitations

- The benchmark primarily uses predefined DAG structures rather than requiring models to infer causality from observational data, limiting generalizability to real-world causal discovery
- Potential hallucination risks when LLMs generate causal explanations beyond what the graph encoding specifies are not addressed
- Prompt engineering influence is not systematically controlled across different LLM families, making it difficult to isolate encoding effects from prompt sensitivity

## Confidence

- **High confidence**: That encoding format significantly impacts LLM performance on causal queries (60% variance observed)
- **Medium confidence**: That contextual variable names introduce measurable bias in LLM causal reasoning
- **Low confidence**: That current zero-shot prompting is sufficient for reliable causal graph encoding in production systems

## Next Checks

1. Test whether fine-tuning LLMs on graph-encoding pairs improves robustness across formats and reduces sensitivity to encoding choice
2. Evaluate model performance on causally ambiguous graphs where multiple DAGs are consistent with the data to test whether LLMs can handle uncertainty appropriately
3. Compare zero-shot prompting results with few-shot examples to determine whether demonstration examples improve encoding consistency and reduce format sensitivity