---
ver: rpa2
title: 'CulturalBench: A Robust, Diverse, and Challenging Cultural Benchmark by Human-AI
  CulturalTeaming'
arxiv_id: '2410.02677'
source_url: https://arxiv.org/abs/2410.02677
tags:
- cultural
- question
- questions
- bench
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CulturalBench, a benchmark designed to evaluate
  large language models' (LLMs) cultural knowledge across 45 global regions, including
  underrepresented ones like Bangladesh, Zimbabwe, and Peru. The benchmark was constructed
  using a human-AI collaborative red-teaming pipeline, where humans provided culturally
  relevant scenarios and an AI assistant helped formulate challenging multiple-choice
  questions.
---

# CulturalBench: A Robust, Diverse, and Challenging Cultural Benchmark by Human-AI CulturalTeaming

## Quick Facts
- arXiv ID: 2410.02677
- Source URL: https://arxiv.org/abs/2410.02677
- Reference count: 40
- Models achieve only 28.7-61.5% accuracy on hard version, compared to 92.4% human performance

## Executive Summary
This paper introduces CulturalBench, a benchmark designed to evaluate large language models' (LLMs) cultural knowledge across 45 global regions, including underrepresented ones like Bangladesh, Zimbabwe, and Peru. The benchmark was constructed using a human-AI collaborative red-teaming pipeline, where humans provided culturally relevant scenarios and an AI assistant helped formulate challenging multiple-choice questions. Each question was verified by five independent annotators, resulting in 1,696 high-quality questions covering 17 diverse cultural topics. The benchmark includes two versions: an easy version with multiple-choice questions and a hard version with binary true/false questions for each option. Evaluation on 29 models showed that the best-performing model (OpenAI o1) achieved only 61.4% accuracy on the hard version, significantly lower than human performance (92.4%). The hard version proved more challenging as models could not rely on heuristics like option similarity to country names.

## Method Summary
The paper introduces CulturalBench, a benchmark to evaluate LLMs' cultural knowledge across 45 global regions. The pipeline consists of three stages: (1) Human-AI red-teaming data collection where humans provide culturally relevant scenarios and an AI assistant helps formulate multiple-choice questions, (2) Human quality check with five independent annotators per question, and (3) Majority vote filtering to construct two benchmark versions. The easy version presents multiple-choice questions while the hard version presents binary true/false questions for each option. The dataset includes 1,696 high-quality, human-written and human-verified questions covering 17 diverse cultural topics spanning 45 global regions.

## Key Results
- Best-performing model (OpenAI o1) achieved only 61.4% accuracy on the hard version, significantly lower than human performance (92.4%)
- Models struggle more with questions having multiple correct answers, with a 28.7% performance gap between single-mode (49.6%) and multi-mode (20.9%) questions
- Models perform worse on questions related to North Africa, South America, and the Middle East regions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-AI collaborative red-teaming improves cultural question quality and diversity
- Mechanism: The pipeline combines human cultural knowledge with AI writing assistance and iterative verification to produce challenging, culturally nuanced questions
- Core assumption: Humans provide authentic cultural scenarios while AI assists in question formulation and revision, creating higher quality questions than either could alone
- Evidence anchors:
  - [abstract] "We introduce CulturalBench: a set of 1,696 human-written and human-verified questions to assess LMs' cultural knowledge"
  - [section] "The pipeline consists of three parts as shown in Fig. 1 – (1) Red-teaming Data Collection (2) Human Quality Check (3) Filtering"
  - [corpus] Found 25 related papers with average FMR=0.415, showing this approach addresses a gap in multicultural evaluation
- Break condition: If AI assistance leads to homogenization of questions or humans provide culturally stereotypical scenarios without AI intervention

### Mechanism 2
- Claim: Binary True/False format more effectively evaluates cultural knowledge than multiple-choice
- Mechanism: Binary format prevents models from using heuristics like option similarity to country names, forcing genuine cultural understanding
- Core assumption: Models can guess correct answers in multiple-choice format by exploiting superficial patterns rather than demonstrating actual cultural knowledge
- Evidence anchors:
  - [abstract] "the hard version of CULTURAL BENCH is challenging even for the best-performing frontier LMs, ranging from 28.7% to 61.5% in accuracy"
  - [section] "By using a simple heuristic of choosing the option with highest cosine similarity with the country name... we attain 40.4% accuracy"
  - [corpus] No direct corpus evidence, but this represents a novel methodological contribution
- Break condition: If models develop sophisticated heuristics that work across both formats or if binary format introduces new guessing strategies

### Mechanism 3
- Claim: Models struggle more with questions having multiple correct answers due to mode-seeking behavior
- Mechanism: LLMs tend to converge on single "correct" answers even when cultural diversity requires acknowledging multiple valid responses
- Core assumption: Cultural knowledge often involves multiple acceptable answers, but models are trained to predict single best answers
- Evidence anchors:
  - [abstract] "LMs often struggle with tricky questions that have multiple correct answers (e.g., What utensils do the Chinese usually use?)"
  - [section] "the average across all models shown in Fig. 2 is 49.6% on Single-mode questions and 20.9% on Multi-mode questions, revealing a significant gap of 28.7%"
  - [corpus] No direct corpus evidence, but this reveals important limitations in current evaluation approaches
- Break condition: If models can be trained to better handle multi-label cultural questions or if evaluation methodology needs refinement

## Foundational Learning

- Concept: Cultural knowledge representation in language models
  - Why needed here: Understanding how cultural knowledge is encoded and retrieved in LLMs is crucial for evaluating their multicultural capabilities
  - Quick check question: Why might a model perform well on cultural questions about North America but poorly on questions about East Europe?

- Concept: Human-AI collaboration in data annotation
  - Why needed here: The pipeline relies on effective collaboration between humans and AI to produce high-quality cultural questions
  - Quick check question: What are the potential risks of relying too heavily on AI assistance in cultural question generation?

- Concept: Evaluation methodology design
  - Why needed here: Choosing between multiple-choice and binary formats significantly impacts the validity of cultural knowledge assessment
  - Quick check question: How might a model exploit superficial patterns in multiple-choice questions without understanding cultural content?

## Architecture Onboarding

- Component map: Human scenario collection -> AI question formulation -> Human revision -> Quality check (5 annotators) -> Majority vote filtering -> Benchmark construction
- Critical path: Question formulation → LLM verification → Human revision → Quality check → Majority vote filtering → Benchmark construction
- Design tradeoffs: Multiple-choice format is easier to implement but less rigorous; binary format is more challenging but requires more computational resources; human-only vs human-AI collaboration affects both quality and scalability
- Failure signatures: Questions without majority votes indicate cultural ambiguity or poor question design; models achieving high scores on multiple-choice but low on binary suggests format exploitation; regional performance gaps reveal training data biases
- First 3 experiments:
  1. Test whether AI assistance improves question quality by comparing questions generated with and without AI help
  2. Validate that binary format prevents heuristic exploitation by testing both formats on the same model
  3. Analyze performance differences between single-mode and multi-mode questions to confirm mode-seeking behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific revision strategies from the LLM hints were most effective at increasing question difficulty?
- Basis in paper: [explicit] The paper mentions that LLM hints provided revision strategies like "Negate the Question" but doesn't analyze which were most effective.
- Why unresolved: The paper only shows that LLM hints helped users make more revisions and spend more time, but doesn't analyze which specific hint strategies were most effective at creating harder questions.
- What evidence would resolve it: Analysis of question difficulty metrics (e.g., accuracy drop on tested models) correlated with specific revision strategies used, showing which strategies most effectively increased question difficulty.

### Open Question 2
- Question: How does the performance gap between single-mode and multi-mode questions vary across different model families and sizes?
- Basis in paper: [explicit] The paper shows a 28.7% performance gap between single-mode and multi-mode questions overall, but doesn't break this down by model family/size.
- Why unresolved: The analysis only shows aggregate performance differences, not how different model architectures handle the two question types differently.
- What evidence would resolve it: Detailed performance breakdown showing accuracy differences between single-mode and multi-mode questions for each model family (GPT, Llama, Mistral, etc.) and size category.

### Open Question 3
- Question: What cultural topics are most underrepresented in the current CulturalBench dataset despite having significant global populations?
- Basis in paper: [explicit] The paper mentions recruiting from Prolific with at least 10 active annotators per region, but doesn't analyze which cultural topics emerged most frequently or least frequently.
- Why unresolved: The paper provides topic distribution examples for specific countries but doesn't analyze which cultural topics across all 45 regions are systematically underrepresented.
- What evidence would resolve it: Topic frequency analysis across all regions showing which cultural topics (e.g., specific religious practices, local political systems, regional celebrations) appear least frequently despite having significant global populations.

## Limitations

- The benchmark's focus on English-language questions may not fully capture cultural knowledge in native languages or multilingual contexts
- Limited coverage of cultural topics in Oceania and Central Asia regions may skew results
- The binary true/false format, while preventing certain heuristics, may introduce new guessing strategies

## Confidence

- High confidence: The methodology for creating challenging cultural questions and the finding that models perform significantly worse on the hard version compared to the easy version
- Medium confidence: The claim about mode-seeking behavior in models, as this requires further investigation into whether the effect is consistent across different question types
- Medium confidence: The regional performance gaps, as they may reflect both model biases and question difficulty variations

## Next Checks

1. Test whether the hard version prevents heuristic exploitation across different model families by comparing performance gaps between easy and hard versions for each model
2. Validate the mode-seeking behavior finding by analyzing model responses to multi-answer questions across different cultural domains
3. Investigate whether the regional performance gaps persist when controlling for question difficulty and cultural specificity metrics