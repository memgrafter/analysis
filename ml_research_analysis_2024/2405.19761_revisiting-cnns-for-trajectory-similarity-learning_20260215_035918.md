---
ver: rpa2
title: Revisiting CNNs for Trajectory Similarity Learning
arxiv_id: '2405.19761'
source_url: https://arxiv.org/abs/2405.19761
tags:
- trajectory
- convtraj
- similarity
- trajectories
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that trajectory similarity learning should focus
  on local similarity rather than long-term global dependency. It proposes ConvTraj,
  a simple CNN-based framework that combines 1D and 2D convolutions to capture sequential
  and geo-distribution features of trajectories.
---

# Revisiting CNNs for Trajectory Similarity Learning

## Quick Facts
- arXiv ID: 2405.19761
- Source URL: https://arxiv.org/abs/2405.19761
- Authors: Zhihao Chang; Linzhu Yu; Huan Li; Sai Wu; Gang Chen; Dongxiang Zhang
- Reference count: 40
- Key outcome: ConvTraj achieves state-of-the-art accuracy for similarity retrieval with 240x training and 2.16x inference speedup on Porto dataset

## Executive Summary
This paper proposes ConvTraj, a CNN-based framework for trajectory similarity learning that focuses on local similarity rather than long-term global dependency. The method combines 1D and 2D convolutions to capture sequential and geo-distribution features of trajectories respectively. Through theoretical analyses and extensive experiments on three real-world datasets, ConvTraj demonstrates superior performance in similarity retrieval tasks with significant speed improvements over existing methods.

## Method Summary
ConvTraj is a simple CNN-based framework that processes trajectories through dual representations: 1D sequences and 2D binary images. The 1D branch uses residual blocks with 1D convolutions and max-pooling to capture sequential features, while the 2D branch employs 2D convolutions on binary trajectory images to encode geographic distribution. Features from both branches are concatenated and passed through an MLP to produce the final embedding. The model is trained using a combination of triplet loss and MSE loss with a simple top-k triplet selection strategy.

## Key Results
- Achieves state-of-the-art accuracy for similarity retrieval on four commonly used similarity measurements
- Training speed increased by at least 240x compared to existing methods
- Inference speed improved by 2.16x on Porto dataset with 1.6 million trajectories
- Outperforms baselines including t2vec, TrjSR, NeuTraj, TrajGAT, and TrajCL on Geolife and Porto datasets

## Why This Works (Mechanism)

### Mechanism 1
1D convolution preserves trajectory distance bounds effectively by creating transformed sequences where DFD is bounded by both original DFD and a modified version incorporating kernel coefficients. The coupling achieving minimum DFD in original space can be applied to transformed sequences with bounded error. If convolution kernel has very large or oscillating coefficients, the error term could become large enough to break the distance preservation guarantee.

### Mechanism 2
Max-pooling preserves trajectory similarity with minimal information loss by reducing sequence length while maintaining DFD bounds within a margin defined by local extrema differences. The maximum values in local windows preserve critical distance information needed for similarity. If trajectories have very similar local patterns but different global structures, max-pooling might lose distinguishing features.

### Mechanism 3
2D convolution ensures large distances for trajectories in distant geographic areas by creating embeddings where geographically separated trajectories have large Euclidean distances due to non-overlapping convolution responses. When trajectories are geographically separated beyond a threshold, their binary image representations will produce non-overlapping convolution outputs. If trajectories are geographically close but follow very different paths, this mechanism might not capture the similarity well.

## Foundational Learning

- Concept: Discrete Frechet Distance and its properties
  - Why needed here: The paper's theoretical analysis and design decisions are built around preserving DFD bounds through various transformations
  - Quick check question: What is the key property of DFD that makes it suitable for trajectory similarity and how does it differ from DTW?

- Concept: Convolutional neural network operations and their mathematical properties
  - Why needed here: Understanding how 1D and 2D convolutions transform input data is crucial for grasping the theoretical bounds and design choices
  - Quick check question: How does a 1D convolution with stride 1 and padding 1 transform a sequence of length n with a kernel of size 3?

- Concept: Trajectory representation and preprocessing
  - Why needed here: The paper converts trajectories to both 1D sequences and 2D binary images, requiring understanding of this dual representation
  - Quick check question: How does the min-max normalization and MLP transformation in 1D preprocessing affect the trajectory representation?

## Architecture Onboarding

- Component map: Input preprocessing -> 1D branch -> 2D branch -> Fusion -> Output embedding
- Critical path: Input ‚Üí 1D branch ‚Üí 2D branch ‚Üí Fusion ‚Üí Output embedding
- Design tradeoffs:
  - Simplicity vs. expressiveness: Using standard CNN operations rather than complex attention mechanisms
  - Speed vs. accuracy: The 240x speedup comes from simpler operations but may miss some complex patterns
  - Local vs. global: Focusing on local similarity rather than long-range dependencies
- Failure signatures:
  - Poor performance on datasets with very long trajectories might indicate 1D convolution limitations
  - Degraded results on geographically clustered trajectories might show 2D convolution limitations
  - High variance in results could indicate triplet selection sensitivity
- First 3 experiments:
  1. Validate that 1D convolution preserves DFD bounds on a small dataset with known properties
  2. Test the effect of removing 2D convolution to isolate its contribution
  3. Experiment with different kernel sizes in the 1D convolution to understand sensitivity to hyperparameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ConvTraj perform on irregularly sampled trajectories compared to regularly sampled ones?
- Basis in paper: The paper mentions "non-uniform sampling rates" as a limitation of traditional measures but does not evaluate ConvTraj's performance on irregularly sampled trajectories.
- Why unresolved: The paper focuses on datasets with regular sampling and does not provide experimental results for irregular sampling scenarios.
- What evidence would resolve it: Experiments comparing ConvTraj's performance on regularly vs. irregularly sampled trajectories from the same dataset, or a dedicated dataset with known irregular sampling.

### Open Question 2
- Question: What is the impact of different grid sizes (Œ¥) on ConvTraj's performance for 2D convolution?
- Basis in paper: The paper states "grid width ùõø = 250m when generating binary images" but does not explore the sensitivity of performance to different grid sizes.
- Why unresolved: The paper uses a fixed grid size without investigating how varying this parameter affects accuracy and efficiency.
- What evidence would resolve it: A systematic evaluation of ConvTraj's performance across multiple grid sizes (e.g., 100m, 250m, 500m, 1000m) on the same datasets.

### Open Question 3
- Question: How does ConvTraj's performance scale with trajectory length beyond the tested datasets?
- Basis in paper: The paper tests on datasets with max trajectory lengths up to 7579 points but does not evaluate performance on extremely long trajectories or provide theoretical scaling analysis.
- Why unresolved: The paper does not include experiments with trajectories significantly longer than those in the tested datasets, nor does it discuss computational complexity for very long sequences.
- What evidence would resolve it: Experiments on datasets with trajectories of varying lengths (e.g., 10K, 100K points) and computational complexity analysis for long trajectories.

## Limitations
- Theoretical analysis relies heavily on assumptions about kernel coefficients and trajectory distributions
- 240x speedup claim based on comparison with single GPU implementation on specific hardware (NVIDIA Tesla V100)
- Evaluation focuses on four similarity measures but doesn't extensively test edge cases like very long trajectories or highly irregular patterns

## Confidence
- High confidence: The core claim that 1D convolution preserves DFD bounds (Theorem 5.1) is well-supported by mathematical proof
- Medium confidence: The empirical performance claims (240x speedup, state-of-the-art accuracy) are based on extensive experiments but depend on specific hardware and implementation details
- Medium confidence: The claim that local similarity is sufficient for trajectory learning is supported by experiments but not rigorously proven

## Next Checks
1. **Ablation study**: Remove the 2D convolution branch and retrain to quantify its specific contribution to accuracy improvements
2. **Robustness test**: Evaluate on trajectories with varying lengths (short vs. very long) to identify potential failure modes of the 1D convolution approach
3. **Hardware independence verification**: Replicate the speedup experiments on different GPU architectures (e.g., RTX 3080, A100) to confirm the 240x improvement is consistent across hardware