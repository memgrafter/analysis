---
ver: rpa2
title: On the Surprising Effectiveness of Attention Transfer for Vision Transformers
arxiv_id: '2411.09702'
source_url: https://arxiv.org/abs/2411.09702
tags:
- attention
- transfer
- maps
- distillation
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether pre-training in vision transformers
  (ViTs) works by learning useful features or by teaching models how to route information
  between tokens. The authors propose a method called attention transfer, which isolates
  the role of attention patterns by transferring only these patterns from a pre-trained
  teacher to a student model.
---

# On the Surprising Effectiveness of Attention Transfer for Vision Transformers

## Quick Facts
- arXiv ID: 2411.09702
- Source URL: https://arxiv.org/abs/2411.09702
- Authors: Alexander C. Li; Yuandong Tian; Beidi Chen; Deepak Pathak; Xinlei Chen
- Reference count: 40
- Primary result: Attention transfer can match full fine-tuning performance on ImageNet-1K by transferring only attention patterns from pre-trained models

## Executive Summary
This paper challenges the conventional view that vision transformer pre-training effectiveness comes primarily from feature learning by proposing attention transfer as an alternative. The authors demonstrate that transferring only attention patterns from a pre-trained teacher to a student model can achieve performance comparable to full fine-tuning on ImageNet-1K, reaching up to 85.7% accuracy. The method reveals that attention patterns play a crucial role in guiding how models route information between tokens, which may be as important as learning useful features themselves. The approach offers a parameter-efficient alternative to fine-tuning, particularly valuable when pre-training data differs from downstream data.

## Method Summary
The authors propose two attention transfer methods: Attention Copy, which directly copies attention maps from a pre-trained teacher to a student, and Attention Distillation, which distills attention patterns during training through a regularization loss. These methods isolate the role of attention patterns by transferring them separately from feature representations. The approach is evaluated on ImageNet-1K using various pre-trained models and shows that attention transfer can match full fine-tuning performance while being more parameter-efficient. The method is also tested in ensemble settings, showing complementary benefits when combined with fine-tuning.

## Key Results
- Attention transfer achieves up to 85.7% accuracy on ImageNet-1K, matching full fine-tuning performance
- The method shows strong scaling properties, improving with larger model sizes
- Attention transfer provides complementary benefits when ensembled with fine-tuned models
- Performance degrades significantly with dataset shifts, revealing a key limitation compared to fine-tuning

## Why This Works (Mechanism)
Attention transfer works by leveraging the attention patterns learned during pre-training to guide information routing in the student model. These patterns encode how tokens should attend to each other for effective information flow, which is critical for vision transformers that rely heavily on self-attention mechanisms. The method suggests that pre-training teaches models not just what features to learn, but how to route information between tokens effectively. The attention patterns act as a form of inductive bias that helps the student model learn more efficiently, even without access to the original features.

## Foundational Learning

**Self-attention mechanisms**: Why needed - Understanding how vision transformers route information between tokens is fundamental to grasping why attention transfer works. Quick check - Review how multi-head self-attention computes weighted combinations of token representations.

**Vision transformer architecture**: Why needed - The paper's findings are specific to ViTs and their reliance on attention patterns for information flow. Quick check - Understand the difference between ViTs and convolutional networks in terms of information routing.

**Contrastive learning**: Why needed - Many pre-trained models used in the study are trained with contrastive objectives, which may influence the attention patterns learned. Quick check - Review how contrastive objectives shape representation learning.

**Parameter-efficient fine-tuning**: Why needed - Attention transfer is positioned as an alternative to methods like LoRA and adapters. Quick check - Compare different parameter-efficient fine-tuning approaches and their tradeoffs.

## Architecture Onboarding

**Component map**: Pre-trained teacher model -> Attention map extraction -> Student model training (with attention regularization) -> Fine-tuned model (baseline comparison)

**Critical path**: The most important component is the attention regularization during student training, which ensures the student learns to replicate the teacher's attention patterns while adapting to the downstream task.

**Design tradeoffs**: The method trades off some robustness to distribution shifts for parameter efficiency and potentially faster adaptation. It requires storing attention maps from the teacher, adding computational overhead.

**Failure signatures**: Performance degradation occurs when pre-training and downstream datasets differ significantly. The method may underperform when fine-grained feature adaptation is crucial for the task.

**First experiments**: 1) Compare attention transfer with full fine-tuning on ImageNet-1K using different pre-trained models. 2) Test ensemble performance combining attention transfer with fine-tuning. 3) Evaluate performance degradation with synthetic dataset shifts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which pre-trained attention patterns enable feature learning in ViTs?
- Basis in paper: The paper finds that transferring only attention patterns from a pre-trained teacher can match fine-tuning accuracy, suggesting attention patterns provide crucial guidance for feature learning. However, the exact mechanism remains unclear.
- Why unresolved: While the paper provides empirical evidence that attention transfer works, it does not fully explain why or how the attention patterns guide feature learning. The CKA analysis shows dissimilarity between attention transfer models and fine-tuned models, suggesting different learning pathways.
- What evidence would resolve it: Detailed analysis of intermediate representations, such as probing what specific attention patterns the student learns and how they relate to feature learning. Experiments ablating different aspects of attention maps (e.g., focusing on specific head types or spatial patterns) could reveal the key mechanisms.

### Open Question 2
- Question: Can attention transfer methods be extended to work effectively with dataset shifts or out-of-distribution data?
- Basis in paper: The paper explicitly shows that attention transfer performance degrades significantly when the pre-training dataset differs from the downstream dataset, and it loses some out-of-distribution robustness compared to fine-tuning.
- Why unresolved: The paper demonstrates the limitation but does not provide solutions or explore potential modifications to make attention transfer more robust to distribution shifts.
- What evidence would resolve it: Experiments testing attention transfer with various dataset shifts, including synthetic shifts and natural distribution shifts. Investigation of methods to adapt attention patterns during transfer or to combine attention transfer with other techniques (e.g., domain adaptation) could provide solutions.

### Open Question 3
- Question: What is the optimal strategy for transferring attention patterns across different model sizes or architectures?
- Basis in paper: The paper shows that attention transfer scales well with model size and works across different pre-training methods. However, it does not explore optimal strategies for transferring between models of different sizes or architectures.
- Why unresolved: While the paper demonstrates the effectiveness of attention transfer within the same model architecture, it does not investigate how to best leverage attention patterns when transferring to a larger or smaller model, or to a different architecture altogether.
- What evidence would resolve it: Experiments transferring attention patterns from a smaller model to a larger one, or from a ViT to a different architecture (e.g., ConvNeXt). Analysis of how to adapt attention patterns to account for differences in model capacity or architecture could reveal optimal strategies.

## Limitations
- Results are limited to ImageNet-1K and ImageNet-22K, limiting generalizability to other domains
- Performance degrades significantly with dataset shifts, reducing robustness compared to fine-tuning
- Computational overhead of storing and transferring attention maps was not thoroughly analyzed

## Confidence
- High Confidence: The core finding that attention transfer can achieve competitive performance with full fine-tuning on ImageNet-1K
- Medium Confidence: Claims about attention transfer providing complementary information to fine-tuning in ensemble settings
- Medium Confidence: Conclusions about the role of attention patterns versus feature learning in pre-training effectiveness

## Next Checks
1. Test attention transfer across diverse computer vision tasks beyond ImageNet classification, including object detection and semantic segmentation
2. Evaluate performance degradation when applying attention transfer to domain-shifted datasets with varying degrees of distribution shift
3. Conduct ablation studies comparing attention transfer with other parameter-efficient fine-tuning methods like LoRA or adapter layers