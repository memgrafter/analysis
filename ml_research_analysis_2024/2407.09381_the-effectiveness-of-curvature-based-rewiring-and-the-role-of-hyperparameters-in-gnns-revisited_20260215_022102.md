---
ver: rpa2
title: The Effectiveness of Curvature-Based Rewiring and the Role of Hyperparameters
  in GNNs Revisited
arxiv_id: '2407.09381'
source_url: https://arxiv.org/abs/2407.09381
tags:
- graph
- curvature
- rewiring
- edges
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work reevaluates the effectiveness of curvature-based rewiring
  in Graph Neural Networks (GNNs) using real-world datasets. While synthetic datasets
  show oversquashing occurs, this study finds that most edges selected during curvature-based
  rewiring in benchmark datasets do not satisfy theoretical conditions for oversquashing.
---

# The Effectiveness of Curvature-Based Rewiring and the Role of Hyperparameters in GNNs Revisited

## Quick Facts
- **arXiv ID:** 2407.09381
- **Source URL:** https://arxiv.org/abs/2407.09381
- **Reference count:** 40
- **Primary result:** Curvature-based rewiring shows no measurable improvements compared to no rewiring in real-world datasets, with performance gains attributed to hyperparameter optimization rather than the rewiring method itself.

## Executive Summary
This study reevaluates the effectiveness of curvature-based rewiring in Graph Neural Networks (GNNs) using real-world datasets. While synthetic datasets show oversquashing occurs, this study finds that most edges selected during curvature-based rewiring in benchmark datasets do not satisfy theoretical conditions for oversquashing. The analysis reveals that state-of-the-art accuracies are outliers from hyperparameter sweeps rather than consistent performance gains. Results show no measurable improvements from different curvature measures compared to no rewiring. The study highlights the importance of considering hyperparameter dependency when evaluating rewiring methods and questions the effectiveness of curvature-based rewiring for large graph datasets.

## Method Summary
The study evaluates curvature-based rewiring using the Stochastic Discrete Ricci Flow (SDRF) algorithm on real-world graph datasets. It computes six different curvature measures (Balanced Forman, JLc, and Augmented Forman with various cycle contributions) for all edges, then applies SDRF to select edges for rewiring. The research conducts extensive hyperparameter sweeps across learning rate, layer depth, width, dropout, weight decay, and rewiring-specific parameters (C+ and τ). GCN models are trained on both original and rewired graphs, with performance compared across different curvature measures and rewiring extents.

## Key Results
- SDRF-selected edges rarely satisfy theoretical conditions for oversquashing in benchmark datasets
- State-of-the-art accuracies from curvature-based rewiring are outliers from hyperparameter sweeps
- Different curvature measures select largely non-overlapping sets of edges for rewiring
- No measurable performance improvements from any curvature measure compared to no rewiring

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Edges selected for rewiring during SDRF rarely satisfy the theoretical conditions for oversquashing in real-world datasets.
- Mechanism: The SDRF algorithm selects edges based on their negative curvature values, but the curvature-based criterion δ < 1/♯△ & δ < 1/γmax is seldom satisfied in benchmark datasets.
- Core assumption: Theorem 4's conditions accurately identify edges that cause oversquashing in message passing.
- Evidence anchors:
  - "From our results in Table 1, we see that these conditions on δ are seldom satisfied by the graphs in the datasets."
  - "In the second column of Table 1, we display the number of edges that satisfy the actual modified condition required."
- Break condition: If future work identifies a broader set of edges that cause oversquashing, or if synthetic datasets with real-world-like structures are developed.

### Mechanism 2
- Claim: State-of-the-art accuracies from curvature-based rewiring are outliers from hyperparameter sweeps rather than consistent performance improvements.
- Mechanism: Performance gains attributed to rewiring are actually due to finding optimal hyperparameter configurations during random search, not the rewiring itself.
- Core assumption: Hyperparameter tuning has a significant impact on GNN performance, potentially masking or mimicking the effects of rewiring.
- Evidence anchors:
  - "The analysis reveals that state-of-the-art accuracies are outliers from hyperparameter sweeps rather than consistent performance gains."
  - "When looking at the average of the top 10% results we see that AF c based rewiring does perform better than other variants while having a smaller standard deviation, but does not advance the performance with respect to no rewiring."
- Break condition: If future studies control for hyperparameters more rigorously or use different optimization methods that reveal consistent benefits from rewiring.

### Mechanism 3
- Claim: Different curvature measures select largely non-overlapping sets of edges for rewiring.
- Mechanism: The mathematical formulations of different curvature measures lead to different edge rankings, resulting in different rewiring decisions even on the same graph.
- Core assumption: The specific mathematical formulation of a curvature measure directly determines which edges are selected for rewiring.
- Evidence anchors:
  - "The results shown in Table 2 indicate a large discrepancy between the edges selected for rewiring depending on different curvature measures."
  - "This is also interesting to note in the context of Table 2 which showed that the chosen edges to be rewired do not overlap from one curvature to another."
- Break condition: If future research develops curvature measures that select more similar sets of edges, or if the impact of specific edge selections is better understood.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: The paper analyzes how graph topology affects information propagation in GNNs through message passing.
  - Quick check question: What is the difference between the input graph and the computational graph in the context of graph rewiring?

- Concept: Discrete graph curvature measures (Forman, Jost-Liu, etc.)
  - Why needed here: These curvature measures are used to identify bottlenecks and guide the rewiring process.
  - Quick check question: How does the Balanced Forman curvature mathematically capture the presence of tree-like structures around an edge?

- Concept: Hyperparameter tuning and its impact on model performance
  - Why needed here: The paper argues that performance gains attributed to rewiring are actually due to hyperparameter optimization.
  - Quick check question: Why is it problematic to compare the performance of different rewiring methods without controlling for hyperparameter effects?

## Architecture Onboarding

- Component map: Data preprocessing -> Curvature computation -> SDRF edge selection -> GNN training -> Evaluation
- Critical path: Curvature computation → SDRF edge selection → GNN training → Performance evaluation
- Design tradeoffs:
  - Computational cost vs. accuracy: More complex curvature measures (like BFc with four-cycle contributions) are more expensive but may not yield better results
  - Rewiring extent vs. information preservation: Aggressive rewiring may improve information flow but could disrupt important structural patterns
- Failure signatures:
  - Performance degrades significantly after rewiring: Indicates that the original graph structure was important for the task
  - No consistent improvement across hyperparameter sweeps: Suggests that rewiring is not addressing the actual bottleneck
- First 3 experiments:
  1. Compute curvature distributions for a small benchmark dataset (like Cora) using all six measures and visualize them to understand their behavior
  2. Run SDRF with different curvature measures on a small dataset and compare which edges are selected to verify the non-overlapping selection claim
  3. Perform a small hyperparameter sweep (10-20 configurations) on Cora with and without rewiring using one curvature measure to observe the distribution of results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed limitations of curvature-based rewiring extend to other graph rewiring algorithms beyond SDRF?
- Basis in paper: The authors note that "It would be interesting to extend this investigation to other rewiring algorithms to validate similar claims" and focused specifically on SDRF due to its simplicity.
- Why unresolved: The study only tested one rewiring algorithm (SDRF) while acknowledging there are others like BORF, FoSR, and G-RLEF that might behave differently.
- What evidence would resolve it: Testing multiple rewiring algorithms (SDRF, BORF, FoSR, etc.) on the same benchmark datasets and comparing their edge selection patterns and performance outcomes.

### Open Question 2
- Question: Can real-world benchmark datasets be constructed that genuinely suffer from severe information bottlenecks where curvature-based rewiring would be effective?
- Basis in paper: The authors state "Future work could involve developing real-world benchmark datasets that do suffer from bottlenecks to test whether the robust performance gains observed in synthetic data can be replicated."
- Why unresolved: Current benchmark datasets appear to lack sufficient negatively curved edges that would indicate bottleneck conditions, as shown in Table 1.
- What evidence would resolve it: Creating and validating new datasets with intentionally engineered bottleneck structures, then testing if curvature-based rewiring shows measurable improvements on these datasets compared to standard ones.

### Open Question 3
- Question: Would relaxing the theoretical bound in Theorem 4 (from δ · |#△| ≤ 1 to δ · |#△| ≤ R with R > 1) make more edges eligible for rewiring and improve performance?
- Basis in paper: The authors mention "The theoretical analysis of the SDRF bounds indicates that the current bound, δ · |#△| ≤ 1, could potentially be replaced by a more general bound, δ · |#△| ≤ R with R > 1, as alluded to in Remark 15 of [18]."
- Why unresolved: The current strict bound excludes many edges from being considered bottlenecks, but relaxing it would further weaken theoretical guarantees.
- What evidence would resolve it: Implementing a modified SDRF algorithm with relaxed bounds, testing it on benchmark datasets, and comparing both the number of eligible edges and resulting performance improvements against the original SDRF implementation.

## Limitations
- Performance gains from curvature-based rewiring are artifacts of hyperparameter optimization rather than the rewiring method itself
- Current benchmark datasets lack sufficient bottleneck structures where curvature-based rewiring would be theoretically effective
- The study only tested one rewiring algorithm (SDRF), limiting generalizability to other methods

## Confidence

**High confidence:** The claim that SDRF-selected edges rarely satisfy theoretical oversquashing conditions (supported by Table 1 data)

**Medium confidence:** The assertion that performance gains are due to hyperparameter optimization rather than rewiring (based on outlier analysis in hyperparameter sweeps)

**Medium confidence:** Different curvature measures select largely non-overlapping edges (supported by Table 2)

## Next Checks

1. Conduct controlled experiments varying only the rewiring method while keeping all hyperparameters fixed to isolate rewiring effects
2. Test curvature-based rewiring on synthetic graphs specifically designed to exhibit oversquashing to validate the theoretical conditions
3. Implement alternative hyperparameter optimization strategies (Bayesian optimization) to verify if the outlier pattern persists across different search methods