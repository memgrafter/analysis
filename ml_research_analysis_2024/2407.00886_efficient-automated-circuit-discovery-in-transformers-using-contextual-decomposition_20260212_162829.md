---
ver: rpa2
title: Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition
arxiv_id: '2407.00886'
source_url: https://arxiv.org/abs/2407.00886
tags:
- circuits
- circuit
- cd-t
- nodes
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CD-T (Contextual Decomposition for Transformers),
  a method to efficiently discover interpretable computational subgraphs (circuits)
  in large language models. CD-T uses mathematical decomposition principles to isolate
  contributions of model features, enabling automated circuit discovery without approximations,
  training, or manual examples.
---

# Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition

## Quick Facts
- **arXiv ID**: 2407.00886
- **Source URL**: https://arxiv.org/abs/2407.00886
- **Reference count**: 24
- **Primary result**: CD-T discovers interpretable circuits with 97% ROC AUC, reducing runtime from hours to seconds compared to existing methods

## Executive Summary
This paper introduces CD-T (Contextual Decomposition for Transformers), a method that efficiently discovers interpretable computational subgraphs (circuits) in large language models. CD-T uses mathematical decomposition principles to isolate contributions of model features without approximations, training, or manual examples. The method supports arbitrary granularity levels and was evaluated on three standard tasks (IOI, Greater-than, Docstring), demonstrating superior performance over existing methods (ACDC, EAP) with 97% ROC AUC while reducing runtime from hours to seconds.

## Method Summary
CD-T employs contextual decomposition equations to recursively split node activations into relevant (β) and irrelevant (γ) parts, propagating these through the transformer's computational graph to measure each node's contribution to task-relevant outputs. The algorithm uses a relevance metric based on the ratio of β to γ magnitudes to identify important nodes, then iteratively selects and prunes nodes to converge on minimal yet complete circuits. The method computes exact node contributions without approximations, enabling efficient circuit discovery that outperforms existing approaches on standard benchmarks.

## Key Results
- CD-T achieves 97% ROC AUC in circuit recovery, outperforming ACDC and EAP baselines
- Runtime reduced from hours to seconds while maintaining or improving circuit quality
- CD-T circuits are 80% more faithful than random circuits of up to 60% of the model size
- CD-T perfectly replicates model behavior using fewer nodes than baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CD-T computes exact node contributions without approximations, enabling efficient circuit discovery.
- **Mechanism**: CD-T uses contextual decomposition equations to recursively split node activations into relevant (β) and irrelevant (γ) parts, propagating these through the transformer's computational graph to measure each node's contribution to task-relevant outputs.
- **Core assumption**: The decomposition equations accurately isolate contributions of model features without introducing approximation errors that plague activation patching methods.
- **Break condition**: If the decomposition equations fail to accurately capture feature contributions, the circuit discovery would become unreliable and potentially miss important nodes.

### Mechanism 2
- **Claim**: The relevance metric based on the ratio of β to γ magnitudes effectively identifies important nodes for circuit discovery.
- **Mechanism**: CD-T measures node relevance using R(s,T) = Σ ∥βt∥₁/∥γt∥₁, which captures the magnitude of a node's contribution relative to its irrelevant counterpart.
- **Core assumption**: Nodes with larger β/γ ratios are more important for the task, and this metric works regardless of whether contributions are positive or negative.
- **Break condition**: If the metric fails to distinguish truly important nodes from noise, the algorithm would select irrelevant nodes and produce ineffective circuits.

### Mechanism 3
- **Claim**: Recursive pruning of least-relevant nodes efficiently converges to minimal yet complete circuits.
- **Mechanism**: The algorithm iteratively selects nodes with highest relevance, adds them to the circuit, then prunes away nodes that don't improve performance, repeating until convergence.
- **Core assumption**: Greedy selection of highest-relevance nodes and iterative pruning will converge to an optimal or near-optimal circuit representation.
- **Break condition**: If the pruning heuristic removes critical nodes or the convergence criterion is too lenient, the resulting circuit may be incomplete or suboptimal.

## Foundational Learning

- **Concept**: Contextual Decomposition (CD) for neural networks
  - **Why needed here**: CD-T builds directly on CD principles, extending them from LSTMs/CNNs to transformers, so understanding the original CD framework is essential.
  - **Quick check question**: What are the β and γ components in contextual decomposition, and how are they initialized in the original CD framework?

- **Concept**: Transformer attention mechanism and computational graph structure
  - **Why needed here**: CD-T operates by decomposing activations through the transformer's computational graph, requiring understanding of how attention layers process information.
  - **Quick check question**: How do query, key, and value functions interact in the attention mechanism to produce output vectors?

- **Concept**: Mechanistic interpretability and circuit discovery methodology
  - **Why needed here**: CD-T is specifically designed for automated circuit discovery, so understanding what circuits are and how they're evaluated is crucial.
  - **Quick check question**: What is the difference between node-level and edge-level circuit discovery approaches?

## Architecture Onboarding

- **Component map**: Input → CD-T decomposition propagation → Relevance calculation → Node selection → Pruning → Circuit output → Faithfulness evaluation
- **Critical path**: Input → CD-T decomposition propagation → Relevance calculation → Node selection → Pruning → Circuit output → Faithfulness evaluation
- **Design tradeoffs**: 
  - Exact decomposition vs. computational efficiency (CD-T trades some runtime for accuracy)
  - Granularity level (node-level vs. sequence-position level) affects runtime and circuit quality
  - Mean-ablation vs. zero-ablation for defining irrelevant components
- **Failure signatures**:
  - Runtime increasing exponentially with model size (indicates inefficient decomposition)
  - Faithfulness plateauing below 1 (suggests missing critical nodes)
  - ROC AUC not improving with iterations (indicates poor relevance metric or decomposition)
- **First 3 experiments**:
  1. Run CD-T on IOI task with default settings and verify runtime reduction compared to ACDC
  2. Test different relevance metrics (magnitude-only vs. signed) on Greater-than task
  3. Compare mean-ablation vs. zero-ablation decomposition initialization on Docstring task

## Open Questions the Paper Calls Out
- The authors acknowledge that their methods can be applied to construct circuits comprised of any of a neural network's internal components, but they only discussed circuits built with purely attention heads for comparability with baselines. MLPs and other components represent a promising direction for future work.

## Limitations
- The paper only tests CD-T on GPT2-small and a 4-layer attention-only transformer, not on larger state-of-the-art models
- Runtime improvements are demonstrated but the computational complexity analysis is incomplete, particularly regarding how CD-T scales with model size
- The greedy selection and pruning algorithm may get stuck in local optima, though this isn't explored in the experiments

## Confidence
- **High confidence**: Runtime reduction claims (directly measured and compared to baselines)
- **Medium confidence**: Faithfulness metrics (well-defined but rely on specific evaluation protocols)
- **Medium confidence**: ROC AUC improvements (statistically significant but depend on the quality of baseline circuits)
- **Low confidence**: Claims about CD-T's generality across different transformer architectures (only tested on two specific models)

## Next Checks
1. Implement ablation studies testing alternative relevance metrics (e.g., signed contributions, magnitude-only) to verify the proposed β/γ ratio is optimal
2. Test CD-T on additional transformer architectures beyond GPT2-small and the 4-layer attention-only model to validate generality claims
3. Conduct runtime scaling experiments with increasingly large models to characterize the computational complexity and identify potential bottlenecks