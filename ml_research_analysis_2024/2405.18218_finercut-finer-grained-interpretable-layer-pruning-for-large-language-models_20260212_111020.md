---
ver: rpa2
title: 'FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models'
arxiv_id: '2405.18218'
source_url: https://arxiv.org/abs/2405.18218
tags:
- pruning
- layer
- layers
- performance
- pruned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying over-parameterized
  transformer networks, which, while achieving state-of-the-art performance, require
  significant computational resources and raise environmental concerns. To tackle
  this, the authors propose FinerCut, a fine-grained layer pruning method that treats
  individual self-attention and feed-forward network layers within transformer blocks
  as separate pruning candidates.
---

# FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models

## Quick Facts
- arXiv ID: 2405.18218
- Source URL: https://arxiv.org/abs/2405.18218
- Reference count: 40
- Primary result: 25-30% layer reduction with minimal performance degradation on Llama3 models

## Executive Summary
This paper addresses the computational burden of deploying over-parameterized transformer networks by introducing FinerCut, a fine-grained layer pruning method that treats individual self-attention and feed-forward network layers within transformer blocks as separate pruning candidates. The method iteratively removes layers whose absence minimally impacts model output, measured by changes in the predictive distribution. Evaluated across 9 benchmarks, FinerCut achieves 90% performance of Llama3-8B with 25% layers removed, and 95% performance of Llama3-70B with 30% layers removed, without fine-tuning or reconstruction. The approach provides interpretability by revealing which types of layers can be pruned while maintaining performance.

## Method Summary
FinerCut implements an iterative layer pruning algorithm that considers all self-attention and feed-forward network (FFN) layers within transformer blocks as individual pruning candidates, rather than pruning entire blocks. The algorithm starts with all layers active and iteratively removes the layer whose absence causes the smallest increase in a distance metric (angular, Euclidean, or Jensen-Shannon divergence) between the original and pruned model outputs. This greedy approach approximates optimal pruning with O(L²) complexity instead of O(2^(2L)) for exhaustive search. The method is task-agnostic, requiring only forward passes without gradient-based computations, and provides interpretability by revealing layer types and positions that can be removed while preserving performance.

## Key Results
- Retains 90% performance of Llama3-8B with 25% layers removed, and 95% performance of Llama3-70B with 30% layers removed
- 42% (34 out of 80) of self-attention layers in Llama3-70B can be removed while preserving 99% of performance
- Pruned self-attention layers are usually in consecutive transformer blocks, often at deeper layers

## Why This Works (Mechanism)

### Mechanism 1
- Iterative evaluation of individual layer removal enables globally optimal pruning under the metric q by removing the layer whose absence causes the smallest increase in the distance metric q between original and pruned model outputs. This greedy step-by-step removal uses forward-pass evaluation only.
- Core assumption: The distance metric q accurately reflects downstream task performance.
- Break condition: If the metric q becomes saturated (e.g., JS divergence near 0) before reaching the desired pruning ratio, the algorithm may terminate early.

### Mechanism 2
- Pruning at the sub-block level (attention vs. FFN) uncovers more redundancy than block-level pruning by treating self-attention and FFN layers as separate candidates, allowing selective removal of only the redundant component within a block.
- Core assumption: Attention and FFN layers contribute independently to model output and are not tightly coupled in importance.
- Break condition: If a pruned FFN causes downstream degradation that was previously masked by the co-located attention, the independence assumption fails.

### Mechanism 3
- The model learns to assign redundancy to deeper self-attention layers, relying more heavily on FFNs for transformation while attention layers become more "routing" or "residual" in function, allowing them to be removed without loss.
- Core assumption: Later-stage representations are less dependent on cross-token interaction than on internal feature transformation.
- Break condition: If later-layer attention removal degrades generation quality more than earlier-layer removal, the depth-redundancy hypothesis is invalid.

## Foundational Learning

- Concept: Distance metrics for comparing model outputs (cosine, Euclidean, JS divergence)
  - Why needed here: The pruning algorithm uses q to quantify how much the output distribution shifts when a layer is removed
  - Quick check question: If two vectors have the same orientation but different norms, which metric will treat them as identical?
    - Answer: Cosine/angular distance will treat them as identical; Euclidean and JS divergence will not

- Concept: Transformer decoder architecture (self-attention + FFN within each block)
  - Why needed here: Pruning targets are selected from these two layer types, so understanding their roles is critical
  - Quick check question: In the residual connection of a transformer block, what happens if you remove the attention but keep the FFN?
    - Answer: The output of the block becomes just the FFN output plus the original input (attention contribution is gone)

- Concept: Greedy combinatorial optimization
  - Why needed here: The iterative pruning algorithm is a greedy search over 2L binary variables
  - Quick check question: What is the time complexity of brute-force search over all possible layer subsets?
    - Answer: O(2^(2L)), which is infeasible for modern LLMs

## Architecture Onboarding

- Component map: Embedding layer → L decoder blocks (each: Attn → Add → FFN → Add) → Head layer
- Critical path: Token generation path: embedding → successive self-attention and FFN → logits
- Design tradeoffs: Fine-grained pruning increases search flexibility but requires repeated forward passes; coarse pruning is faster but less optimal
- Failure signatures: Sudden performance collapse when removing a single critical layer; degraded perplexity on WikiText2 while QA tasks remain stable
- First 3 experiments:
  1. Run iterative pruning on Llama3-8B with 10% target ratio using JS divergence; inspect pruned layer types
  2. Compare performance when using only transformer-block candidates vs. separate attention/FFN candidates
  3. Test the effect of limiting pruning candidates to the last 60% of layers; measure speedup vs. performance trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FINER CUT compare to other pruning methods when applied to larger or smaller models than Llama3-70B and Llama3-8B?
- Basis in paper: The paper demonstrates that FINER CUT outperforms other methods on Llama3-70B and Llama3-8B, and mentions that larger models are easier pruning targets. However, it does not explore a wider range of model sizes.
- Why unresolved: The study is limited to a specific set of models and does not provide comprehensive analysis across different model sizes or architectures.
- What evidence would resolve it: Testing FINER CUT on a broader range of model sizes and architectures, including both larger and smaller models, and comparing the results with other pruning methods.

### Open Question 2
- Question: Can the observed preference for pruning self-attention layers over FFN layers in deeper decoder layers be generalized to other transformer-based architectures?
- Basis in paper: The paper observes a preference for pruning self-attention layers, especially at deeper consecutive decoder layers in Llama3 models, and suggests this could inspire future efficient LLM architecture designs.
- Why unresolved: The study is limited to Llama3 models, and it is unclear whether this observation holds true for other transformer-based architectures or if it is specific to the Llama3 architecture.
- What evidence would resolve it: Applying FINER CUT to a variety of transformer-based architectures and analyzing the types and locations of pruned layers to determine if the preference for pruning self-attention layers is consistent across different models.

### Open Question 3
- Question: What is the impact of different distance metrics on the pruning performance of FINER CUT, and is there a metric that consistently outperforms others across various tasks and models?
- Basis in paper: The paper explores three distance metrics (Euclidean, angular, and statistical) and finds that JS divergence performs better on perplexity results, but all three have similar performance on zero/few-shot tasks.
- Why unresolved: The study does not conduct an exhaustive comparison of all possible distance metrics, and it is unclear if there exists a metric that consistently outperforms others across all tasks and models.
- What evidence would resolve it: Systematically testing a wide range of distance metrics on various tasks and models to identify if there is a metric that consistently provides the best pruning performance across different scenarios.

## Limitations

- The computational cost of the iterative pruning process (O(L²) forward passes) becomes prohibitive for very large models with hundreds of layers
- The independence assumption between self-attention and FFN layers within blocks is not rigorously validated
- The claim that deeper self-attention layers are more redundant needs more systematic investigation across different model families

## Confidence

**High Confidence**: The empirical results showing 25-30% layer reduction with minimal performance degradation are well-supported by the experimental data.

**Medium Confidence**: The task-agnostic nature of the method and its applicability to different model architectures. While results are shown on multiple Llama3 variants and Mixtral, broader validation across different model families would strengthen this claim.

**Low Confidence**: The theoretical justification for why self-attention layers become more redundant in deeper layers. The paper observes this pattern but provides limited mechanistic explanation.

## Next Checks

1. **Metric Correlation Validation**: Systematically test the correlation between the three distance metrics (angular