---
ver: rpa2
title: 'On Explaining Unfairness: An Overview'
arxiv_id: '2402.10762'
source_url: https://arxiv.org/abs/2402.10762
tags:
- fairness
- explanations
- group
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive taxonomy of fairness and explanation
  methods, and explores the intersection of the two fields. It identifies three key
  directions for using explanations to address fairness: (1) enhancing fairness metrics,
  (2) understanding the causes of unfairness, and (3) designing methods to mitigate
  unfairness.'
---

# On Explaining Unfairness: An Overview

## Quick Facts
- **arXiv ID**: 2402.10762
- **Source URL**: https://arxiv.org/abs/2402.10762
- **Reference count**: 40
- **Primary result**: Comprehensive taxonomy of fairness and explanation methods, with 31 approaches analyzed for explaining unfairness.

## Executive Summary
This paper presents a comprehensive survey and taxonomy of fairness and explanation methods in AI, focusing on how explanations can be used to address unfairness. The authors identify three key directions: enhancing fairness metrics, understanding causes of unfairness, and designing mitigation methods. Through analysis of 31 existing approaches, they find that most methods use post-processing techniques, are black-box and model-agnostic, and focus on group fairness using counterfactual explanations. The paper highlights gaps in current research, including the need for dynamic fairness criteria, bridging individual and group-level fairness, and extending work to more complex ML tasks beyond classification and ranking.

## Method Summary
This is a survey paper that presents two comprehensive taxonomies - one for fairness approaches and one for explanation methods. The authors analyze 31 existing approaches for explaining unfairness, categorizing them based on methodology, objectives, and characteristics such as model access (white-box vs. black-box), coverage (global vs. local), and type (feature-based, example-based, approximation-based). Rather than proposing new methods, the paper synthesizes existing research to provide a structured framework for understanding how explanation methods can be applied to fairness analysis.

## Key Results
- Most existing approaches use post-processing methods, are black-box and model-agnostic
- Counterfactual explanations are the primary method for quantifying unfairness and suggesting recourse
- Current research is limited to classification and ranking tasks, with significant gaps in extending to more complex ML applications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The paper’s comprehensive taxonomy clarifies how explanation methods can be applied to fairness analysis.
- **Mechanism:** By mapping explanation methods onto a taxonomy with dimensions like model access, coverage, and type, the authors provide a structured framework for identifying which techniques can reveal sources of unfairness. This enables practitioners to select appropriate methods for specific fairness objectives.
- **Core assumption:** A clear taxonomy enables more effective method selection for fairness-related explanations.
- **Evidence anchors:**
  - [abstract] states that the authors present “two comprehensive taxonomies, each representing one of the two complementary fields of study: fairness and explanations.”
  - [section] describes how the taxonomy distinguishes explanation methods based on model access, coverage, and type, and notes that “most of these methods are black-box, model-agnostic, and geared towards addressing global explanations.”
- **Break condition:** The taxonomy becomes less useful if it does not cover emerging fairness definitions or explanation techniques, or if practitioners lack understanding of how to map their problem to the taxonomy’s dimensions.

### Mechanism 2
- **Claim:** Counterfactual explanations are effective for quantifying unfairness and suggesting actionable recourse.
- **Mechanism:** Counterfactual explanations generate alternative scenarios where the outcome differs from the original while minimizing distance between inputs. This allows measurement of “burden” for individuals or groups and identification of minimal changes required for fair outcomes.
- **Core assumption:** The distance metric and optimization process used to generate counterfactuals are valid and interpretable for fairness analysis.
- **Evidence anchors:**
  - [section] explains the formal definition of counterfactuals and how they are used to calculate burden for groups.
  - [section] describes how FACTS uses counterfactuals to detect bias among subgroups by ranking subspaces based on recourse effectiveness.
- **Break condition:** If the distance metric is not meaningful in the context of fairness or if optimization fails to generate valid counterfactuals, the method loses effectiveness.

### Mechanism 3
- **Claim:** Shapley values can decompose model disparity into feature contributions, aiding in fairness explanation.
- **Mechanism:** By calculating Shapley values for each feature, the approach quantifies how much each feature contributes to the model’s fairness metric. This decomposition helps identify which features drive unfairness and can be extended to consider causal paths linking sensitive attributes to outcomes.
- **Core assumption:** Shapley values accurately capture the marginal contribution of each feature to fairness, and the underlying model is stable enough for such decomposition.
- **Evidence anchors:**
  - [section] describes how Shapley-based approaches use global Shapley values to explain feature contributions to parity fairness.
  - [section] mentions FACTS as an approach that decomposes model disparity into contributions of causal paths.
- **Break condition:** If the model is unstable or the fairness metric is not additive over features, Shapley values may provide misleading attributions.

## Foundational Learning

- **Concept:** Counterfactual explanations
  - **Why needed here:** They are a primary method for generating actionable fairness explanations, allowing measurement of burden and identification of minimal changes required for fair outcomes.
  - **Quick check question:** How do counterfactual explanations differ from traditional feature importance methods in the context of fairness?

- **Concept:** Shapley values
  - **Why needed here:** They provide a principled way to decompose model disparity into feature contributions, which is essential for understanding and mitigating unfairness.
  - **Quick check question:** What is the key difference between using Shapley values for accuracy vs. fairness explanations?

- **Concept:** Taxonomy-based method selection
  - **Why needed here:** The taxonomy provides a structured way to select the most appropriate explanation method for a given fairness objective, ensuring that the chosen method aligns with the problem’s requirements.
  - **Quick check question:** How does the taxonomy help in choosing between local and global explanations for fairness?

## Architecture Onboarding

- **Component map:** Taxonomy Engine -> Counterfactual Generator -> Shapley Value Calculator -> Causal Path Analyzer -> Explanation Synthesizer

- **Critical path:**
  1. Define fairness objective (e.g., group fairness, individual fairness)
  2. Use taxonomy to select appropriate explanation method(s)
  3. Generate explanations (e.g., counterfactuals, Shapley values)
  4. Analyze explanations to identify sources of unfairness
  5. Synthesize findings into actionable recommendations

- **Design tradeoffs:**
  - Granularity vs. interpretability: More granular explanations may be harder to interpret than aggregated ones
  - Computational cost vs. accuracy: More sophisticated methods may be more accurate but computationally expensive
  - Black-box vs. white-box access: Black-box methods are more widely applicable but may be less precise than white-box methods

- **Failure signatures:**
  - Inconsistent explanations across methods: Indicates potential issues with the underlying model or fairness metric
  - High computational cost with limited actionable insights: Suggests the need for simpler methods or better-defined fairness objectives
  - Lack of interpretability: Indicates the need for better visualization or summarization of explanations

- **First 3 experiments:**
  1. Apply the taxonomy to a simple classification task with known bias, and verify that the selected method(s) correctly identify the source(s) of unfairness
  2. Generate counterfactual explanations for a group of individuals and calculate the burden metric, then compare it to the actual fairness metric of the model
  3. Decompose model disparity using Shapley values and verify that the identified feature contributions align with domain knowledge of potential biases

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we develop dynamic fairness metrics that adapt to evolving data distributions and societal norms while maintaining robustness and interpretability?
- **Basis in paper:** [explicit] The authors identify a need for dynamic fairness criteria that adapt to evolving data distributions and societal norms, stating “There is also room for exploration into dynamic fairness criteria that adapt to evolving data distributions and societal norms.”
- **Why unresolved:** Dynamic fairness metrics require balancing adaptability with stability, addressing the tension between responsiveness to change and maintaining consistent evaluation standards.
- **What evidence would resolve it:** Development and validation of dynamic fairness metrics that demonstrate improved performance in adapting to data shifts while maintaining interpretability and robustness compared to static metrics.

### Open Question 2
- **Question:** What are the most effective methods for bridging the gap between individual and group-level fairness explanations, and how can hybrid models balance these perspectives?
- **Basis in paper:** [explicit] The authors note that “Most approaches tend to focus either on individual or group levels of fairness. Therefore, future directions in this field could involve bridging the gap between individual and group levels, thus offering a more comprehensive view of fairness.”
- **Why unresolved:** Bridging individual and group fairness requires reconciling potentially conflicting objectives at different levels of analysis.
- **What evidence would resolve it:** Development of hybrid explanation models that successfully integrate individual and group fairness perspectives, demonstrated through case studies showing improved fairness outcomes and stakeholder understanding.

### Open Question 3
- **Question:** How can we extend fairness explanation methods beyond classification and ranking tasks to more complex ML applications like clustering, regression, and graph-based learning?
- **Basis in paper:** [explicit] The authors state that “Current research is limited in its coverage of various ML tasks... there is a significant gap in extending such work to more complex tasks like multiclass classification, regression, clustering, representation learning, unsupervised learning and applications involving graphs and knowledge graphs.”
- **Why unresolved:** Complex ML tasks present unique challenges for fairness explanations due to their different output structures and evaluation criteria.
- **What evidence would resolve it:** Development and validation of fairness explanation methods that successfully address complex ML tasks, with empirical studies demonstrating their effectiveness across different task types and data modalities.

## Limitations
- **Completeness of taxonomy:** The taxonomy may not cover emerging fairness definitions and explanation techniques as the field rapidly evolves
- **Generalizability across tasks:** Findings are primarily based on classification and ranking tasks, with limited validation for more complex ML applications
- **Depth of evaluation:** The analysis of 31 approaches provides breadth but limited depth in evaluating each method’s effectiveness

## Confidence
- **Taxonomy completeness:** Medium confidence - rapidly evolving field may outpace current categorization
- **Generalizability:** Medium confidence - limited validation beyond classification and ranking tasks
- **Empirical validation:** Low confidence - paper is a survey without experimental validation of the framework

## Next Checks
1. **Taxonomy Validation:** Apply the presented taxonomies to a new set of fairness and explanation methods to verify their comprehensiveness and applicability.
2. **Method Comparison:** Conduct a controlled experiment comparing the effectiveness of different explanation methods (e.g., counterfactuals vs. Shapley values) in identifying and mitigating unfairness in a real-world dataset.
3. **Cross-Domain Applicability:** Test the applicability of the findings to ML tasks beyond classification and ranking, such as reinforcement learning or unsupervised learning, to assess the generalizability of the proposed framework.