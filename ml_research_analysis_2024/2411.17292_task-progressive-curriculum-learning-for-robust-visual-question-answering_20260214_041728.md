---
ver: rpa2
title: Task Progressive Curriculum Learning for Robust Visual Question Answering
arxiv_id: '2411.17292'
source_url: https://arxiv.org/abs/2411.17292
tags:
- learning
- tpcl
- curriculum
- training
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Task Progressive Curriculum Learning (TPCL),
  a novel training strategy for robust Visual Question Answering (VQA) that achieves
  state-of-the-art performance on out-of-distribution benchmarks. The key innovation
  is decomposing the VQA problem into smaller tasks based on question types and training
  progressively from harder to easier tasks using a novel distributional-based difficulty
  measurer based on Optimal Transport.
---

# Task Progressive Curriculum Learning for Robust Visual Question Answering

## Quick Facts
- arXiv ID: 2411.17292
- Source URL: https://arxiv.org/abs/2411.17292
- Authors: Ahmed Akl; Abdelwahed Khamis; Zhe Wang; Ali Cheraghian; Sara Khalifa; Kewen Wang
- Reference count: 40
- Primary result: TPCL achieves 77.23% accuracy on VQA-CP v2, outperforming state-of-the-art by >5%

## Executive Summary
This paper introduces Task Progressive Curriculum Learning (TPCL), a novel training strategy for robust Visual Question Answering (VQA) that achieves state-of-the-art performance on out-of-distribution benchmarks. The key innovation is decomposing the VQA problem into smaller tasks based on question types and training progressively from harder to easier tasks using a novel distributional-based difficulty measurer based on Optimal Transport. TPCL demonstrates significant improvements over existing approaches: it achieves 77.23% accuracy on VQA-CP v2 and 76.15% on VQA-CP v1, outperforming the previous state-of-the-art by more than 5% and 7% respectively. The method is model-agnostic and can boost baseline backbone performance by up to 28.5%. Notably, TPCL accomplishes this without requiring data augmentation or explicit debiasing mechanisms, showing that curriculum learning alone is sufficient for robust VQA. The approach also performs well in in-distribution settings and low-data regimes.

## Method Summary
TPCL breaks the VQA problem into 65 tasks based on question types, then uses Optimal Transport-based distributional difficulty measurement to rank tasks from hardest to easiest. The method employs a step-wise pacing function to progressively introduce easier tasks as training progresses. Two variants are proposed: TPCLDyn↑ (dynamic difficulty) and TPCLFix↑ (fixed linguistic difficulty). The approach trains for 30 epochs with 6 training iterations and 5 consolidation iterations each, using Faster-RCNN for image features and GloVe+GRU for question embeddings. The method achieves robust OOD performance without explicit debiasing mechanisms or data augmentation.

## Key Results
- TPCLDyn↑ achieves 77.23% accuracy on VQA-CP v2, surpassing previous state-of-the-art by >5%
- TPCLDyn↑ achieves 76.15% accuracy on VQA-CP v1, surpassing previous state-of-the-art by >7%
- The method is model-agnostic, boosting baseline backbone performance by up to 28.5%
- TPCL maintains strong in-distribution performance on VQA v2 while excelling at OOD generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task decomposition into question-type-specific sub-tasks reduces the label space complexity and makes the learning problem more tractable.
- Mechanism: By grouping all questions of the same type (e.g., "how many") together, each sub-task has a smaller, more coherent label distribution, which reduces the model's search space and improves generalization.
- Core assumption: Question type is a strong semantic predictor of answer distribution, and questions of the same type share underlying reasoning patterns.
- Evidence anchors: [abstract] "Our proposed approach, Task Progressive Curriculum Learning (TPCL), breaks the main VQA problem into smaller, easier tasks based on the question type." [section 3.1] "For a set of question types τ ∈ [T], we reorganise the dataset into a group of T VQA sub-tasks {Dτ }T τ=1 where task Dτ ⊂ D is the data subset whose questions belong to type τ."

### Mechanism 2
- Claim: Dynamic distributional difficulty measurement based on Optimal Transport (OT) better captures task hardness than instantaneous loss-based metrics.
- Mechanism: Instead of averaging per-sample losses, TPCL computes a histogram of losses for each task and measures distributional divergence (via OT) across training iterations. Tasks whose loss distributions change less are considered easier because the model memorizes them quickly.
- Core assumption: Stable loss distributions indicate task memorability, and OT is robust to distribution shifts where traditional metrics like KL divergence fail.
- Evidence anchors: [section 3.1.1.A] "We create a distribution of scores for each question type. Then, we track the distributional divergence across iterations... Based on our observations of the distributions shifts during the training, we base our divergence on Optimal Transport [22]." [section 4.4] "Table 4 summarises the findings. The results clearly show that utilising the loss distribution metric offers superior performance compared to a mean-based metric across all baseline models."

### Mechanism 3
- Claim: Training from harder to easier tasks in a dynamic curriculum enables out-of-distribution generalization by forcing the model to focus on challenging, bias-prone samples early.
- Mechanism: The pacing function gradually exposes the model to easier tasks after it has been trained on harder ones. This prevents early overfitting to language priors and encourages robust reasoning.
- Core assumption: Harder tasks are more representative of OOD scenarios and expose model biases that easier tasks do not.
- Evidence anchors: [abstract] "Then, it progressively trains the model on a (carefully crafted) sequence of tasks... it achieves state-of-the-art on VQA-CP v2, VQA-CP v1 and VQA v2 datasets." [section 3.1.2] "The pacing function determines the rate at which new training tasks are introduced to a model during learning... We use a standard step pacing function [43] that adds a fraction of the training data (every B iterations) as..."

## Foundational Learning

- Concept: Optimal Transport (Wasserstein Distance)
  - Why needed here: To measure distributional divergence between loss histograms when distributions do not overlap perfectly.
  - Quick check question: What is the key advantage of OT over KL divergence when comparing two histograms that are horizontally shifted?

- Concept: Multi-Task Learning (MTL) decomposition
  - Why needed here: To understand how grouping by question type reduces label space complexity and improves training efficiency.
  - Quick check question: How does reducing the answer space per task help the model learn more robust features?

- Concept: Curriculum Learning pacing strategies
  - Why needed here: To control the rate at which tasks are introduced during training and prevent catastrophic forgetting.
  - Quick check question: What is the difference between step-wise and continuous pacing functions in curriculum learning?

## Architecture Onboarding

- Component map: Data preprocessing (Faster-RCNN + GloVe+GRU) -> Backbone (SAN/UpDn/LXMERT) -> TPCL wrapper (OT-based difficulty measurer + pacing function + task scheduler) -> Training loop
- Critical path: Data split → difficulty score computation (OT) → task ordering → mini-batch training on current curriculum → repeat
- Design tradeoffs:
  - Fixed vs dynamic curriculum: Fixed is simpler but less adaptive; dynamic requires OT computation overhead but adapts to model state
  - Histogram bin size: Larger bins reduce noise but lose granularity; smaller bins are precise but noisy
  - Consolidation window length (B): Longer windows smooth difficulty estimates but slow adaptation
- Failure signatures:
  - No performance gain: Difficulty measurer not working (OT scores flat), pacing too conservative
  - Degraded ID performance: Curriculum too hard-biased, model never sees easy data
  - Training instability: Binning or OT computation unstable, causing erratic task ordering
- First 3 experiments:
  1. Run TPCLDyn↑ on SAN backbone with default parameters; verify task ordering changes over iterations
  2. Replace OT with mean loss; compare performance drop to validate distributional difficulty
  3. Swap dynamic for fixed curriculum; measure impact on OOD vs ID performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the task-based curriculum design scale to more complex question taxonomies beyond the 65 types used in this work?
- Basis in paper: [explicit] The paper demonstrates effectiveness with 65 question types but does not explore scalability to larger or more granular taxonomies
- Why unresolved: The paper focuses on the 65-type categorization without investigating whether the approach would maintain effectiveness with thousands of question types or more complex hierarchical taxonomies
- What evidence would resolve it: Experiments testing TPCL with expanded question taxonomies (e.g., 200+ types) or hierarchical question structures, measuring performance degradation or maintenance of gains

### Open Question 2
- Question: What is the theoretical justification for using Optimal Transport versus other distributional divergence measures (KL divergence, JS divergence) in the difficulty measurement?
- Basis in paper: [explicit] The paper justifies OT based on horizontal distribution shifts but doesn't provide theoretical analysis comparing it to alternatives
- Why unresolved: While the paper demonstrates OT works empirically, it doesn't prove theoretically why OT is superior or under what conditions other measures might fail
- What evidence would resolve it: Formal analysis of distribution shift patterns in VQA training, mathematical proofs showing when OT provides advantages over other divergence measures, or comprehensive ablation studies

### Open Question 3
- Question: What is the computational overhead of TPCL compared to standard training, and how does it scale with dataset size and number of question types?
- Basis in paper: [explicit] The paper mentions "negligible computational overhead" for OT but doesn't provide comprehensive timing or scaling analysis
- Why unresolved: While the paper notes OT takes 0.9-1.2 milliseconds per iteration, it doesn't analyze the total training time increase or scaling behavior
- What evidence would resolve it: Detailed computational complexity analysis, timing comparisons across different dataset sizes and question type counts, and GPU memory usage comparisons

## Limitations
- The Optimal Transport-based difficulty measurer is novel and lacks extensive validation across different domains
- The method's effectiveness on other OOD benchmarks beyond VQA-CP v1/v2 remains untested
- Computational overhead of OT computation and multiple consolidation iterations is not quantified

## Confidence
- **High confidence**: VQA-CP v2 performance claims (77.23%) and general OOD improvements over baselines
- **Medium confidence**: Claims about TPCL's effectiveness in low-data regimes and without explicit debiasing mechanisms
- **Low confidence**: Claims about distributional difficulty measurement being superior to alternatives in general cases

## Next Checks
1. **Sensitivity analysis**: Systematically vary histogram bin sizes (M=50, 100, 200) and consolidation window lengths (B=3, 5, 7) to determine optimal hyperparameters
2. **Cross-benchmark validation**: Evaluate TPCL on other OOD VQA benchmarks (e.g., GQA-OOD, VQA-CE) to test generalizability
3. **Task-level analysis**: Correlate task difficulty scores with known dataset biases and analyze whether harder tasks truly correspond to less biased samples