---
ver: rpa2
title: Boosting the Transferability of Adversarial Examples via Local Mixup and Adaptive
  Step Size
arxiv_id: '2401.13205'
source_url: https://arxiv.org/abs/2401.13205
tags:
- adversarial
- examples
- step
- size
- fsuc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses improving the transferability of black-box
  adversarial examples by designing enhanced input diversity and adaptive step sizes.
  The proposed Input-Diversity-based Adaptive Attack (IDAA) framework introduces a
  local mixup module that randomly mixes regions of transformed adversarial images
  to strengthen input diversity.
---

# Boosting the Transferability of Adversarial Examples via Local Mixup and Adaptive Step Size

## Quick Facts
- arXiv ID: 2401.13205
- Source URL: https://arxiv.org/abs/2401.13205
- Reference count: 40
- Primary result: IDAA achieves superior transferability compared to state-of-the-art baselines on ImageNet

## Executive Summary
This paper introduces Input-Diversity-based Adaptive Attack (IDAA), a framework that significantly improves the transferability of black-box adversarial examples through enhanced input diversity and adaptive optimization. The key innovations include local mixup of transformed image regions, tanh projection for perturbation optimization, and second-order momentum for dynamic step size adjustment. Extensive experiments demonstrate IDAA's superiority over state-of-the-art methods across both naturally trained and adversarially trained models.

## Method Summary
The IDAA framework generates adversarial examples by first applying multiple transformations (flip, perspective, rotate, etc.) to create diverse input variants. These transformed images are then processed through a local mixup module that randomly combines regions from different transformed examples to create composite images with enhanced input diversity. Perturbations are optimized in tanh space to avoid clipping artifacts, and step sizes are dynamically adjusted using second-order momentum that accounts for gradient variance across spatial locations. The framework is designed to be compatible with existing input-diversity-based attacks like DIM, TIM, and SIT.

## Key Results
- IDAA outperforms state-of-the-art transferability attacks on ImageNet validation set
- Significant improvements in fooling success rates against both naturally trained and adversarially trained models
- Local mixup and adaptive step size mechanisms contribute to enhanced cross-model transferability

## Why This Works (Mechanism)

### Mechanism 1: Local Mixup for Enhanced Input Diversity
- Claim: Mixing transformed image regions boosts input diversity more effectively than applying transformations alone
- Mechanism: Random region mixing creates combined spatial features that surrogate models haven't seen during training
- Core assumption: Different image regions carry distinct classification weights, so combining them increases cross-model transferability
- Evidence anchors: Weak - no corpus papers directly confirm local mixup improves transferability; most focus on global transformations

### Mechanism 2: Tanh Projection for Precise Optimization
- Claim: Projecting perturbations into tanh space eliminates clipping artifacts that degrade optimization
- Mechanism: Tanh transformation maps perturbations to valid pixel ranges without explicit clipping, preventing optimization plateaus
- Core assumption: Clipping operations cause optimization to get stuck in flat regions that slow convergence
- Evidence anchors: Weak - only indirect support from general adversarial training literature on avoiding clipping

### Mechanism 3: Adaptive Step Sizes with Second-Order Momentum
- Claim: Second-order momentum dynamically adjusts step sizes for different image regions
- Mechanism: Variance-aware step size adaptation accounts for gradient stability differences across spatial locations
- Core assumption: Different image regions have distinct impacts on model classification, requiring region-specific optimization
- Evidence anchors: Moderate - related to adaptive learning rates in optimization, but specific to adversarial perturbations

## Foundational Learning

- Concept: Adversarial example transferability
  - Why needed here: The entire framework builds on improving cross-model attack success rates
  - Quick check question: What's the difference between white-box and black-box adversarial attacks?

- Concept: Input diversity techniques
  - Why needed here: Local mixup extends traditional transformation-based diversity methods
  - Quick check question: How do random resizing and perspective transformations affect model robustness?

- Concept: Gradient-based optimization with constraints
  - Why needed here: Tanh projection and adaptive step sizes both modify the optimization process
  - Quick check question: Why does clipping cause optimization to get stuck in flat regions?

## Architecture Onboarding

- Component map: Image transformations -> Local mixup processor -> Tanh projection engine -> Adaptive optimizer -> Loss function
- Critical path: 1. Generate N transformed variants, 2. Apply local mixup to create B'₁, 3. Compute gradients and update w in tanh space, 4. Generate final adversarial example
- Design tradeoffs: Group size vs. computation time, step size magnitude vs. stability, mixup region size vs. feature preservation
- Failure signatures: Low fooling rates despite high targeted success rates, very slow convergence, computational bottleneck
- First 3 experiments: 1. Baseline comparison: Run IDAA vs. DIM/TIM with identical settings on RN-50 → DN-121 transfer, 2. Mixup ablation: Test local mixup with 0, 1, 3, 5 mixup operations while keeping other components constant, 3. Step size sweep: Evaluate IDAA performance with step sizes 0.2, 0.5, 1.0, 1.5 to find optimal value

## Open Questions the Paper Calls Out

- How does IDAA perform when integrating with other input diversity techniques beyond DIM, TIM, and SIT?
- What is the impact of varying mixup region size and number of mixups on transferability against different model types?
- How does tanh projection and adaptive step size affect multi-target attacks?
- What is the computational overhead compared to baseline attacks and how does it scale?

## Limitations

- Local mixup mechanism lacks theoretical grounding and may be dataset-specific to natural scene images
- Computational overhead from multiple transformations and mixups is not thoroughly quantified
- Effectiveness may degrade on structured or synthetic data beyond natural images

## Confidence

- **High confidence**: Overall framework design and experimental methodology are sound with comprehensive model coverage
- **Medium confidence**: Improvement claims are supported by extensive experiments, though component contributions are difficult to isolate
- **Low confidence**: Theoretical justification for local mixup and tanh projection relies on empirical observations rather than formal proofs

## Next Checks

1. Component ablation study: Systematically disable each innovation to quantify individual contributions to performance improvement
2. Computational overhead analysis: Measure exact wall-clock time difference between IDAA and baselines across different hardware
3. Dataset generalization test: Evaluate IDAA on non-ImageNet datasets to verify local mixup approach generalizes beyond natural scene images