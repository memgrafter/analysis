---
ver: rpa2
title: Retrieval-augmented generation in multilingual settings
arxiv_id: '2407.01463'
source_url: https://arxiv.org/abs/2407.01463
tags:
- multilingual
- language
- languages
- retrieval
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates multilingual retrieval-augmented generation
  (mRAG) to enable knowledge-intensive question answering in 13 languages. It builds
  a zero-shot mRAG baseline pipeline using off-the-shelf multilingual retrievers,
  rerankers, and LLMs, demonstrating that retrieval consistently improves performance
  across all languages.
---

# Retrieval-augmented generation in multilingual settings

## Quick Facts
- arXiv ID: 2407.01463
- Source URL: https://arxiv.org/abs/2407.01463
- Reference count: 9
- One-line primary result: Zero-shot mRAG pipeline with off-the-shelf multilingual components improves QA performance across 13 languages when using explicit language instructions in translated prompts

## Executive Summary
This work presents a comprehensive investigation of multilingual retrieval-augmented generation (mRAG) for open-domain question answering across 13 languages. The study builds a zero-shot mRAG baseline pipeline using off-the-shelf multilingual retrievers, rerankers, and large language models (LLMs), demonstrating that retrieval consistently improves performance across all languages. Key findings include the importance of explicit language instructions in prompts, the need for task-specific prompt engineering including prompt translation, and the necessity of adjusting evaluation metrics to handle cross-lingual named entity variations. The study identifies critical limitations including code-switching in non-Latin scripts, occasional irrelevant retrieval, and LLM generation issues with mixed-language prompts.

## Method Summary
The study constructs a zero-shot mRAG pipeline using the BERGEN framework with BGE-m3 as the multilingual retriever and reranker, and Command-R-35B as the multilingual LLM. Wikipedia passages in 13 languages are processed into 100-word chunks with article titles prepended and embedded using BGE-m3. The pipeline retrieves top-50 documents, reranks to top-5, and generates responses with explicit language instructions. Two datasets are evaluated: MKQA (cross-lingual questions with English answers) and XOR-TyDi QA (questions in 7 typologically diverse languages with answers in same language). The system uses translated prompts and measures performance using character 3-gram recall and Correct Language Rate (CLR) metrics.

## Key Results
- Retrieval consistently improves performance across all 13 languages, not just English
- Explicit language instructions in translated prompts are essential for generating responses in the user's language
- Standard evaluation metrics need adjustment to handle named entity transliteration variations across languages
- Code-switching in non-Latin scripts and occasional irrelevant retrieval remain key limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval improves performance across all 13 languages, not just English.
- Mechanism: Off-the-shelf multilingual retrievers (BGE-m3) can map user queries in any supported language to relevant documents, even when query and document languages differ.
- Core assumption: The retriever was trained on multilingual data including all considered languages and can generalize to cross-lingual retrieval.
- Evidence anchors:
  - [abstract]: "retrieval consistently improves performance across all languages"
  - [section]: "Our key findings can be summarized as follows: • Retrieval: recent off-the-shelf multilingual retrievers and rerankers perform reasonably well in both cases when queries and documents are in the same or different language"
  - [corpus]: Weak - corpus only lists related work, no direct retriever evaluation.
- Break condition: If the retriever's training data omitted a target language or if domain shift exists between training data and target documents.

### Mechanism 2
- Claim: Explicit language instructions in prompts are essential for generating responses in the user's language.
- Mechanism: LLMs default to English unless explicitly told otherwise; adding instructions like "Answer in {UL}" and translating prompts into user languages forces generation in the correct language.
- Core assumption: LLMs can follow language instructions when properly prompted, even if primarily trained on English.
- Evidence anchors:
  - [section]: "models sometimes reply in English even for non-English user queries... requires a strong multilingually pretrained and tuned LLM, coupled with advanced prompting, e.g. translating prompts into user languages and instructing the LLM to generate responses in the user language"
  - [section]: "Both models sometimes reply in English instead of the user language and it gets maximally addressed by explicitly specifying an instruction to generate response in the user language and translating the system prompt into the user language."
  - [corpus]: Weak - related works discuss language preference but not prompt engineering specifics.
- Break condition: If LLM's instruction following capability degrades with mixed-language prompts or if translation quality of prompts is poor.

### Mechanism 3
- Claim: Standard evaluation metrics need adjustment for multilingual RAG due to variations in named entity spelling across languages.
- Mechanism: Character 3-gram recall captures semantic similarity even when named entities are transliterated differently between languages.
- Core assumption: Transliteration differences are the primary source of mismatch between ground truth and model output in cross-lingual settings.
- Evidence anchors:
  - [section]: "models generate a transliteration of named entities in other languages different from the one contained in the ground-truth label... we propose to evaluate recall on character n-gram level"
  - [section]: "evaluation metrics need adjustment to take into account the zero-shot scenario, e.g. variations in spelling named entities in cross-lingual settings"
  - [corpus]: Weak - related works mention quality issues but not specific metric adjustments.
- Break condition: If evaluation requires semantic matching beyond named entity transliteration or if other linguistic variations dominate.

## Foundational Learning

- Concept: Cross-lingual retrieval fundamentals
  - Why needed here: Understanding how queries in one language can retrieve relevant documents in another is central to mRAG performance
  - Quick check question: Can a retriever trained on English queries and German documents retrieve relevant German documents for French queries?

- Concept: Prompt engineering for multilingual generation
  - Why needed here: LLMs need explicit instructions to generate in non-English languages, and prompt translation is required for effectiveness
  - Quick check question: What happens if you give an English-centric LLM a prompt in French without language instructions?

- Concept: Evaluation metric design for multilingual settings
  - Why needed here: Standard metrics fail when named entities are transliterated differently across languages
  - Quick check question: How would you evaluate a response that says "Sofia Kovalevskaya" when the ground truth is "Sofya Kovalevskaya"?

## Architecture Onboarding

- Component map:
  User query → Query processor (optional) → Multilingual retriever (BGE-m3) → Reranker (BGE-m3) → Context + Query → Multilingual LLM (Command-R-35B) → Response
  Evaluation: Character 3-gram recall + Correct language rate

- Critical path:
  1. Retrieve relevant documents from multilingual Wikipedia
  2. Rerank top-50 to top-5
  3. Generate response with explicit language instruction in translated prompt

- Design tradeoffs:
  - English-centric vs multilingual LLM: English-centric models may perform better on English but fail on non-English generation
  - Retrieval language choice: English retrieval works for MKQA but user language retrieval better for XOR-TyDi QA
  - Prompt translation overhead: Ensures correct language but requires language expertise and dynamic selection

- Failure signatures:
  - Response in wrong language → Prompt engineering issue
  - Missing relevant documents → Retriever performance problem
  - Named entity mismatches → Need for character-level evaluation
  - Code-switching in non-Latin scripts → LLM generation issue

- First 3 experiments:
  1. Compare retrieval from English Wikipedia vs user language Wikipedia for a single non-English language
  2. Test Command-R-35B with default prompt vs translated prompt with language instruction for non-English queries
  3. Evaluate character 3-gram recall vs exact match for responses with transliterated named entities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of mRAG systems degrade as the number of supported languages increases, and is there an optimal number of languages for balancing performance and computational cost?
- Basis in paper: [inferred] The paper mentions testing 13 languages and discusses the "curse of multilinguality" affecting model performance.
- Why unresolved: The paper does not systematically test performance degradation with increasing language count or explore optimal language combinations.
- What evidence would resolve it: Controlled experiments varying the number of supported languages and measuring performance trade-offs, including computational efficiency metrics.

### Open Question 2
- Question: What specific architectural modifications to retrieval systems would improve cross-lingual retrieval quality when the query and document languages have significantly different linguistic structures (e.g., Japanese vs. French)?
- Basis in paper: [explicit] The paper identifies "irrelevant retrieval" as a key limitation and discusses cross-lingual retrieval challenges.
- Why unresolved: The paper relies on existing BGE-m3 models without exploring specialized architectures for linguistically distant language pairs.
- What evidence would resolve it: Comparative studies of retrieval architectures (e.g., language-specific encoders, contrastive learning approaches) across linguistically diverse language pairs.

### Open Question 3
- Question: How can evaluation metrics be improved to better handle semantic equivalence across languages while maintaining objectivity in zero-shot mRAG settings?
- Basis in paper: [explicit] The paper discusses limitations of current evaluation metrics, particularly for named entity variations and code-switching.
- Why unresolved: The proposed character 3-gram metric is a partial solution but doesn't address broader semantic equivalence challenges.
- What evidence would resolve it: Development and validation of multilingual semantic similarity metrics that can be applied without labeled data or reference translations.

### Open Question 4
- Question: What are the most effective strategies for prompt engineering in mRAG that minimize the need for language-specific prompt translation while maintaining high performance across all target languages?
- Basis in paper: [explicit] The paper shows that translating system prompts improves performance but is "less convenient in practice" and explores mixed-language prompt approaches.
- Why unresolved: The paper only tests explicit translation vs. English-only prompts without exploring more sophisticated multilingual prompting strategies.
- What evidence would resolve it: Comparative evaluation of prompt engineering techniques (e.g., few-shot examples in multiple languages, language-agnostic instructions) across diverse language pairs.

## Limitations
- Code-switching in non-Latin scripts presents significant challenges for both retrieval and generation components
- The evaluation framework relies heavily on Wikipedia-based knowledge sources, limiting generalizability to other domains
- Reliance on off-the-shelf models without fine-tuning may limit performance improvements beyond the base model capabilities

## Confidence

- **High Confidence**: The core finding that retrieval improves performance across all 13 languages is well-supported by direct experimental evidence and consistent with the retriever's multilingual training objectives.

- **Medium Confidence**: The claim about explicit language instructions being essential for correct language generation is supported by experimental observations, but the paper does not conduct ablation studies to quantify the exact contribution of each instruction component.

- **Medium Confidence**: The proposed character 3-gram evaluation metric addresses a real problem with named entity transliteration, but the paper does not validate whether this metric captures all relevant aspects of multilingual response quality.

## Next Checks

1. **Cross-lingual Retrieval Robustness Test**: Systematically evaluate BGE-m3 retrieval performance across language pairs where query and document languages differ, measuring recall@K for each combination to identify specific language pairs where retrieval breaks down.

2. **Prompt Engineering Ablation**: Conduct controlled experiments removing individual prompt components (language instruction, prompt translation, system prompt content) to quantify their relative contributions to correct language generation performance.

3. **Metric Validation Study**: Compare character 3-gram recall against human judgments of response quality for a subset of cross-lingual examples to verify that the metric accurately captures semantic similarity beyond named entity transliteration.