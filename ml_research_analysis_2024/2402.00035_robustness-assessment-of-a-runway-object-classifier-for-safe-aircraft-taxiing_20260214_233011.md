---
ver: rpa2
title: Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing
arxiv_id: '2402.00035'
source_url: https://arxiv.org/abs/2402.00035
tags:
- queries
- verification
- brightness
- noise
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This case study uses formal verification to assess a runway object
  classifier's robustness to image perturbations. The authors develop an incremental
  verification algorithm that reduces the number of verification queries by nearly
  60% through leveraging the monotonicity of robustness properties.
---

# Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing

## Quick Facts
- arXiv ID: 2402.00035
- Source URL: https://arxiv.org/abs/2402.00035
- Reference count: 29
- The classifier is considerably more vulnerable to noise than to brightness or contrast perturbations

## Executive Summary
This paper presents a formal verification approach to assess the robustness of a runway object classifier for aircraft taxiing applications. The authors develop an incremental verification algorithm that leverages the monotonicity of robustness properties to reduce verification query count by nearly 60%. They apply this method to evaluate robustness to noise, brightness, and contrast perturbations, finding that the classifier shows significantly higher vulnerability to noise perturbations compared to brightness and contrast perturbations. This finding is reassuring for outdoor aviation applications where brightness and contrast variations are more common and less predictable.

## Method Summary
The authors use Marabou, a DNN verification tool, to assess the robustness of a runway object classifier to various image perturbations. They develop an incremental verification algorithm that exploits the monotonicity of robustness properties to reduce the number of verification queries from O(m²) to O(m), where m is the number of perturbation parameters. Brightness and contrast perturbations are encoded as noise robustness queries by adding an input layer that models these transformations as linear combinations of the original inputs. The method is applied to a DNN with 8000 ReLU neurons trained on 32x32 pixel runway object images.

## Key Results
- The incremental verification algorithm reduces verification query count by approximately 60% compared to exhaustive search
- The classifier shows similar robustness to contrast and brightness perturbations, with UNSAT rates of 83.3% and 91.1% respectively
- The classifier is significantly more vulnerable to noise perturbations, with only 11.1% UNSAT rate for noise (ε=0.3)
- Most UNSAT results occur for small perturbations, with minimal UNSAT for noise perturbations above ε=0.3

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incremental verification reduces the number of verification queries by exploiting monotonicity of perturbation robustness.
- Mechanism: The algorithm leverages the property that if a network is robust to smaller perturbations, it remains robust to larger ones. It uses binary search to find transition points in robustness, reducing queries from O(m²) to O(m).
- Core assumption: Robustness is monotonic with respect to perturbation magnitude.
- Evidence anchors:
  - [abstract]: "we propose a method that leverages the monotonicity of these robustness properties"
  - [section 4.2]: "If there exists an adversarial example for parameters β′, ϵ′ or γ′, it then also constitutes a counterexample for a query with parameters β, ϵ or γ respectively"
  - [corpus]: Weak evidence; no directly relevant papers found
- Break condition: If robustness is non-monotonic due to network structure or training artifacts, the algorithm would fail to reduce queries effectively.

### Mechanism 2
- Claim: Encoding brightness and contrast perturbations as noise robustness queries enables reuse of existing noise verification tools.
- Mechanism: The paper transforms brightness and contrast perturbations into equivalent noise perturbation problems by adding an input layer that models the perturbation as a linear combination of original inputs and perturbation parameters.
- Core assumption: The verification tools can handle the modified input space with additional linear operations.
- Evidence anchors:
  - [section 4.1]: "We now show how to encode the brightness and contrast properties... into verification queries that assess robustness to noise perturbations over a modified input space"
  - [corpus]: Weak evidence; no directly relevant papers found
- Break condition: If the verifier cannot handle the increased dimensionality or complexity of the modified network, the encoding would fail.

### Mechanism 3
- Claim: The DNN classifier shows higher robustness to brightness and contrast perturbations than to noise perturbations.
- Mechanism: The evaluation results demonstrate that the classifier maintains higher UNSAT (no adversarial examples found) rates for brightness and contrast perturbations compared to noise perturbations.
- Core assumption: The perturbation types are independent and have distinct effects on the classifier's robustness.
- Evidence anchors:
  - [abstract]: "our results provide an indication of the level of robustness achieved by the DNN classifier... and indicate that it is considerably more vulnerable to noise than to brightness or contrast perturbations"
  - [section 5]: "Overall, the results indicate that the classifier shows similar robustness to contrast and brightness perturbations. However, it is significantly more sensitive to noise perturbations"
  - [corpus]: Weak evidence; no directly relevant papers found
- Break condition: If the perturbation models are inaccurate or if other perturbation types (e.g., rotation, scale) significantly affect robustness, the conclusion would not hold.

## Foundational Learning

- Concept: Deep Neural Networks (DNNs)
  - Why needed here: Understanding DNN architecture is crucial for comprehending how perturbations affect the classifier and how formal verification is applied.
  - Quick check question: What are the main components of a DNN and how do they process input data?

- Concept: Formal Verification
  - Why needed here: Formal verification provides rigorous assurances about the DNN's robustness by proving the absence of certain mispredictions for infinite sets of inputs.
  - Quick check question: How does formal verification differ from testing in assessing DNN robustness?

- Concept: Adversarial Examples
  - Why needed here: Adversarial examples are small input perturbations that lead to incorrect DNN outputs, which is the core problem the paper addresses.
  - Quick check question: What are adversarial examples and why are they a concern for safety-critical applications?

## Architecture Onboarding

- Component map:
  Runway object classifier DNN (N1) -> Incremental verification algorithm -> Verification backend (Marabou DNN verifier) -> Perturbation encoding module (brightness, contrast, noise)

- Critical path:
  1. Select correctly classified test image
  2. Encode brightness/contrast perturbations as noise robustness queries
  3. Apply incremental verification algorithm to reduce number of queries
  4. Execute verification queries using Marabou backend
  5. Analyze results to assess robustness

- Design tradeoffs:
  - Using incremental verification reduces computational cost but requires the assumption of monotonicity.
  - Encoding perturbations as noise robustness queries enables reuse of existing tools but may limit the types of perturbations that can be verified.
  - Focusing on brightness, contrast, and noise perturbations covers common real-world scenarios but may miss other important perturbation types.

- Failure signatures:
  - High percentage of UNKNOWN results (timeouts, memoryouts)
  - Non-monotonic robustness behavior (algorithm fails to reduce queries)
  - Low UNSAT rates for all perturbation types (classifier is highly vulnerable)

- First 3 experiments:
  1. Verify robustness of N1 to small noise perturbations (ε=0.05) around a correctly classified image.
  2. Encode brightness perturbation (β=0.1) as a noise robustness query and verify N1's robustness.
  3. Apply the incremental verification algorithm to find the minimal noise perturbation (ε) for which N1 becomes vulnerable to misclassification.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the primary sources of noise in images captured during aircraft taxiing operations?
- Basis in paper: [explicit] The paper states that noise in images comes from various sources, including the camera's sensor (e.g., impulse noise, thermal noise), associated electronics (e.g., shot noise), and operating/environmental conditions (e.g., low-light conditions, scenery's colors).
- Why unresolved: While the paper lists potential sources of noise, it does not provide a detailed analysis of their relative contributions or frequency in the context of aircraft taxiing.
- What evidence would resolve it: A study quantifying the prevalence and impact of different noise sources in taxiing images, potentially through controlled experiments or analysis of real-world data.

### Open Question 2
- Question: How does the incremental verification algorithm's performance scale with the size of the neural network?
- Basis in paper: [inferred] The paper mentions that the incremental verification algorithm reduces the number of verification queries by nearly 60%, but it does not discuss how this reduction scales with the size of the network.
- Why unresolved: The paper only evaluates the algorithm on a single network, and it is unclear how the reduction in queries would change for larger or more complex networks.
- What evidence would resolve it: Experiments evaluating the algorithm's performance on networks of varying sizes and complexities, reporting the reduction in queries and computational time.

### Open Question 3
- Question: How does the classifier's performance change when evaluating simultaneous brightness and noise perturbations?
- Basis in paper: [explicit] The paper mentions that they aspire to verify the classifier's robustness to simultaneous brightness and noise perturbations in the future, but they do not present results for this scenario.
- Why unresolved: The paper only evaluates the classifier's performance under individual perturbation types, and it is unclear how the classifier would handle the combined effect of brightness and noise.
- What evidence would resolve it: Experiments evaluating the classifier's performance under various combinations of brightness and noise perturbations, reporting the accuracy and robustness metrics.

## Limitations
- The incremental verification algorithm relies on the assumption of monotonicity in perturbation robustness, which may not hold for all DNN architectures or training procedures.
- The encoding of brightness and contrast perturbations as noise robustness queries may not capture all aspects of these transformations, potentially leading to incomplete robustness assessment.
- The evaluation only considers three types of perturbations (noise, brightness, contrast), potentially missing other important vulnerabilities.

## Confidence
- **High Confidence:** The methodology for formal verification using Marabou and the general approach to encoding perturbations are well-established and clearly explained.
- **Medium Confidence:** The claim about higher robustness to brightness and contrast compared to noise perturbations is supported by experimental results, but the generalizability to other DNNs and perturbation types is uncertain.
- **Low Confidence:** The effectiveness of the incremental verification algorithm in reducing query count relies heavily on the monotonicity assumption, which may not always hold.

## Next Checks
1. Test the incremental verification algorithm on DNNs with known non-monotonic robustness properties to assess its performance under violated assumptions.
2. Compare the robustness assessment results when using the proposed perturbation encoding versus direct verification of brightness and contrast perturbations (if supported by the verifier).
3. Evaluate the classifier's robustness to additional perturbation types (e.g., rotation, scale) to determine if noise, brightness, and contrast perturbations cover the most critical vulnerabilities.