---
ver: rpa2
title: 'MaFeRw: Query Rewriting with Multi-Aspect Feedbacks for Retrieval-Augmented
  Large Language Models'
arxiv_id: '2408.17072'
source_url: https://arxiv.org/abs/2408.17072
tags:
- query
- reward
- rewriter
- generation
- rewriting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of query rewriting in retrieval-augmented
  generation (RAG) systems for multi-turn dialogues, where current queries often contain
  ellipses and ambiguous references. The proposed method, MaFeRw, improves RAG performance
  by integrating multi-aspect feedback from both the retrieval process and generated
  results.
---

# MaFeRw: Query Rewriting with Multi-Aspect Feedbacks for Retrieval-Augmented Large Language Models

## Quick Facts
- **arXiv ID**: 2408.17072
- **Source URL**: https://arxiv.org/abs/2408.17072
- **Reference count**: 6
- **Primary result**: Multi-aspect dense rewards improve query rewriting stability and RAG performance in multi-turn dialogues

## Executive Summary
MaFeRw addresses the challenge of query rewriting in retrieval-augmented generation systems for multi-turn dialogues, where queries often contain ellipses and ambiguous references. The method improves RAG performance by integrating dense feedback from multiple sources: similarity with gold documents, ranking metrics, generation quality (ROUGE), and manual rewrite similarity. By training reward models to approximate these metrics and using PPO for optimization, MaFeRw achieves superior generation metrics and more stable training compared to baselines on conversational RAG datasets.

## Method Summary
The approach initializes a T5-based rewriter using supervised training on manual rewrite data, then trains three reward models to approximate key metrics: document similarity, ranking performance, and generation ROUGE. These reward models provide dense feedback during PPO optimization, where the final reward function combines the three reward scores with ROUGE similarity between manual and model rewrites. This multi-aspect feedback design enables more stable and efficient training compared to single-reward approaches, while the reward model approximation avoids the computational cost of full RAG inference during each RL step.

## Key Results
- Superior ROUGE-1, ROUGE-L, BLEU, and METEOR scores compared to baselines on QReCC and TopiOCQA datasets
- Improved MRR for retrieval performance
- More stable training curves with reduced variance compared to single-reward approaches
- Better generalization to multi-document QA tasks (WSDM@24) than baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-aspect dense rewards provide more stable and informative feedback than single sparse rewards during query rewriting.
- Mechanism: The rewriter receives dense reward signals from multiple sources: similarity between rewritten query and gold document, ranking metric between retrieved documents and ground truth, ROUGE between generation and ground truth, and ROUGE between manual and model rewrites. These dense signals give the rewriter consistent feedback at every training step, reducing variance and instability compared to sparse generation-only rewards.
- Core assumption: User intent is sufficiently reflected in the gold document, retrieved documents, and ground truth, so that these can serve as reliable feedback sources for query rewriting.
- Evidence anchors:
  - [abstract]: "We find that user's needs are also reflected in the gold document, retrieved documents and ground truth. Therefore, by feeding back these multi-aspect dense rewards to query rewriting, more stable and satisfactory responses can be achieved."
  - [section 1]: "We find that user information needs are also reflected in gold documents, retrieved documents, and ground truth, which allows for the design of multi-aspect rewards to provide more dense feedback signals."
  - [corpus]: Weak evidence; related papers discuss ranking feedback and diverse multi-query rewriting, but none explicitly validate dense vs sparse reward stability.
- Break condition: If user intent is not sufficiently present in gold documents or ground truth (e.g., ambiguous or incomplete ground truth), the dense rewards become noisy and may mislead training.

### Mechanism 2
- Claim: Using reward models instead of full RAG inference during RL training significantly improves training efficiency while preserving feedback quality.
- Mechanism: Instead of running the complete RAG pipeline for every RL step, reward models are trained to approximate the three metrics (document similarity, ranking, and generation ROUGE). These lightweight models provide fast feedback scores that guide the rewriter, avoiding the computational cost of full RAG inference.
- Core assumption: Reward models trained on paired data can accurately approximate the true metric values and maintain the relative ranking of rewrites.
- Evidence anchors:
  - [abstract]: "Inspired by RLAIF, we train three kinds of reward models for the above metrics to achieve more efficient training."
  - [section 2]: "Reward models were employed to evaluate rewritten queries, effectively decoupling the evaluation from RAG process and enabling more efficient training."
  - [corpus]: Weak evidence; related papers mention ranking feedback and diverse query rewriting but do not validate the efficiency gains of using reward models.
- Break condition: If reward models overfit or fail to generalize, they may give misleading feedback, causing the rewriter to optimize for wrong objectives.

### Mechanism 3
- Claim: Combining reward model scores with the ROUGE metric between manual and model rewrites provides a balanced feedback signal that captures both retrieval effectiveness and generation quality.
- Mechanism: The final reward function aggregates three reward model scores (document similarity, ranking, and generation ROUGE) with the ROUGE similarity between manual and model rewrites. This combination ensures the rewriter improves both retrieval performance and generation relevance, guided by human-level rewrite quality.
- Core assumption: Manual rewrites are consistently better than model rewrites, so their ROUGE similarity is a reliable proxy for rewrite quality.
- Evidence anchors:
  - [abstract]: "Additionally, we use ROUGE scores of model-rewritten queries and manual-rewritten queries as the fourth feedback to measure the rewriter's performance."
  - [section 3]: "Considering the obvious improvement of manual rewriting in the generation, ROUGE between manual rewrites and model rewrites is used as another feedback."
  - [corpus]: Weak evidence; related papers discuss diverse query rewriting and ranking feedback but do not validate the specific combination of manual-model ROUGE in reward functions.
- Break condition: If manual rewrites are not consistently better (e.g., due to domain shift or annotation noise), this feedback term may bias the rewriter away from optimal solutions.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF) and Reward Model Training
  - Why needed here: The approach relies on training reward models to approximate human-aligned metrics and using PPO to optimize the rewriter. Understanding RLHF principles is essential to grasp how dense rewards are constructed and used.
  - Quick check question: What is the main difference between sparse and dense rewards in RL, and why does MaFeRw prefer dense rewards?

- Concept: Query Rewriting in Multi-Turn Dialogues
  - Why needed here: The problem involves resolving ellipses and ambiguous references in conversational contexts. Knowledge of context-based rewriting techniques and their limitations is crucial.
  - Quick check question: Why do traditional context-based query rewriting methods provide minimal enhancement in downstream RAG tasks?

- Concept: Dense Retrieval and Embedding Similarity
  - Why needed here: The method uses cosine similarity between query and document embeddings as part of the reward signal. Understanding dense retrieval and embedding-based similarity metrics is key to grasping the feedback design.
  - Quick check question: How does cosine similarity between dense vectors help evaluate the quality of a rewritten query in RAG?

## Architecture Onboarding

- Component map: T5-rewriter -> Reward models (3) -> PPO optimizer -> RAG system
- Critical path: 1. Initialize rewriter with supervised T5 training on manual rewrites. 2. Train three reward models on paired data generated from metric comparisons. 3. Use PPO to optimize rewriter with aggregated reward from reward models + manual-model ROUGE.
- Design tradeoffs:
  - Efficiency vs. accuracy: Reward models speed up training but may introduce approximation error.
  - Complexity vs. stability: Multi-aspect rewards improve stability but increase hyperparameter tuning burden.
  - Generalization vs. task specificity: Manual rewrite ROUGE helps but may not generalize to domains with different rewrite styles.
- Failure signatures:
  - Rewriter overfits to reward model artifacts rather than true user intent.
  - Training instability due to poorly balanced reward weights.
  - Reward models fail to generalize, giving noisy feedback.
- First 3 experiments:
  1. Train reward models and evaluate their accuracy on held-out pairs to ensure they approximate true metrics.
  2. Run ablation studies removing each reward component to quantify its contribution to final performance.
  3. Compare training curves (ROUGE and MRR over iterations) between single-reward and multi-reward setups to verify stability gains.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the limitations section highlights areas for future research including testing on diverse RAG tasks beyond conversational QA and exploring different dense retrieval and LLM architectures.

## Limitations

- Limited validation to conversational QA datasets (QReCC, TopiOCQA) with minimal testing on other RAG applications
- Reliance on availability of gold documents, retrieved documents, and ground truths as feedback sources
- Significant computational requirements for reward model training and PPO optimization
- Potential bias from reward model approximations if they fail to generalize

## Confidence

- High confidence in the core mechanism of using dense rewards for stable training
- Medium confidence in the efficiency gains from reward models vs. full RAG inference
- Medium confidence in the generalizability across different RAG tasks beyond conversational QA

## Next Checks

1. Conduct ablation studies on each reward component across multiple RAG task types (not just conversational QA) to quantify their domain-specific contributions and identify when certain feedback signals may be harmful.

2. Test reward model generalization by evaluating their performance on out-of-distribution queries and measuring how approximation error impacts final RAG quality, particularly in domains with different query rewriting patterns.

3. Implement a sensitivity analysis of reward weights (λ1, λ2, λ3, μ) across different datasets to determine whether a universal weighting scheme exists or if task-specific tuning is required for optimal performance.