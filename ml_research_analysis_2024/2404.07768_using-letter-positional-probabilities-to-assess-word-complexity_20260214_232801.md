---
ver: rpa2
title: Using Letter Positional Probabilities to Assess Word Complexity
arxiv_id: '2404.07768'
source_url: https://arxiv.org/abs/2404.07768
tags:
- words
- word
- variables
- complexity
- complex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of assessing word complexity using
  letter positional probabilities (LPPs). The core idea is that certain letter positions
  are more likely in simple vs.
---

# Using Letter Positional Probabilities to Assess Word Complexity

## Quick Facts
- arXiv ID: 2404.07768
- Source URL: https://arxiv.org/abs/2404.07768
- Authors: Michael Dalvean
- Reference count: 40
- One-line primary result: Letter positional probabilities (LPPs) can classify word complexity with up to 97% accuracy by capturing orthographic patterns that distinguish simple from complex words.

## Executive Summary
This paper introduces a novel approach to word complexity assessment using letter positional probabilities (LPPs). The method analyzes how letters are distributed across positions in simple versus complex words, then uses these positional patterns as features for classification. By focusing on binary indicators of letter presence at each position (22 positions × 26 letters), the approach creates a structured representation that captures latent orthographic complexity. The technique demonstrates strong performance, achieving 97% accuracy in classifying extreme complexity words and successfully scoring words from an ESL program across four difficulty levels.

## Method Summary
The method involves extracting letter positional probabilities by one-hot encoding each letter at every position for words up to 22 letters long. Statistical analysis (t-tests with Bonferroni correction) identifies significant LPPs that differentiate simple from complex words. Classifiers (random forest with SMOTE balancing) are trained using these features. The process includes corpus construction from simple (children's books) and complex (academic/GRE) sources, filtering nulls, feature selection through statistical significance testing, and validation through cross-validation scoring before applying to a dictionary of 128,511 English words.

## Key Results
- 84 LPPs in the first 6 positions were significant at p < .001, showing clear distributional differences between simple and complex words
- A classifier using 66 common LPPs from two datasets achieved 70% accuracy on a third dataset
- Final classifier combining extreme words from three datasets achieved 97% accuracy and successfully scored four levels of English word groups from an ESL program

## Why This Works (Mechanism)

### Mechanism 1
Letter positional probabilities capture latent complexity because they reflect phonotactic and orthographic patterns tied to word difficulty. By decomposing each word into binary indicators for letter-in-position (22 positions × 26 letters), the method creates 572 predictors that statistically encode how likely a letter is in a given position for simple vs. complex word classes. The distribution of letters in specific positions is significantly different between simple (children's books) and complex (academic texts) words, with 84 LPPs in the first 6 positions significant at p < .001.

### Mechanism 2
LPPs are predictive across datasets because they tap into stable orthographic regularities. Experiment 2 validates that the same 66 LPPs significant in Experiment 1 remain significant in a synthetic dataset built from extreme AoA, frequency, and concreteness values. This suggests the same orthographic cues generalize beyond the original picture-book/academic split, with 85 variables significant at Bp < .001 in the first 6 positions, and 66 overlapping the Experiment 1 set.

### Mechanism 3
LPPs explain the syllable-length link in complexity because vowel-consonant patterns drive both orthographic and phonological complexity. The paper notes that in same-length word pairs, complex words have more syllables; this follows from vowel-consonant positional biases captured by LPPs. High LC words are significantly more likely to start with a vowel, while simple words start with consonants, and syllable counts rise with LC while controlling for length.

## Foundational Learning

- Concept: Binary one-hot encoding of letter-position features
  - Why needed here: Transforms raw text into 572 structured predictors so statistical tests can detect positional biases
  - Quick check question: How many binary features result from encoding a 22-letter word alphabet?
    - Answer: 22 × 26 = 572

- Concept: Bonferroni correction for multiple comparisons
  - Why needed here: Prevents false positives when testing hundreds of LPPs; p-values are multiplied by the number of tests (468 or 572)
  - Quick check question: If p = .0001 and 468 tests are run, what is the corrected Bp?
    - Answer: 0.0001 × 468 = 0.0468

- Concept: SMOTE oversampling for imbalanced binary classification
  - Why needed here: Simple and complex word classes are imbalanced; SMOTE synthetically balances training folds to avoid bias
  - Quick check question: What happens if SMOTE is omitted with 7,574 simple vs. 10,212 complex words?
    - Answer: Classifier may over-predict the majority class, reducing sensitivity

## Architecture Onboarding

- Component map: Corpus construction -> Cleaning/preprocessing -> One-hot encoding (572 features) -> Statistical filtering (t-tests + Bonferroni) -> Feature subset selection -> RandomForest classifier (n_estimators=100, max_depth=None, min_samples_split=5) -> Cross-validation scoring -> Dictionary scoring
- Critical path: Corpus construction → filtering nulls → 66-variable selection → 97% accuracy model → score dictionary
- Design tradeoffs: Using all 403 non-null variables yields 78% accuracy vs. 70% with only 66; choosing 66 for generality trades off raw performance for cross-dataset stability
- Failure signatures: If vocabulary shifts (e.g., domain-specific jargon), the 66-variable model accuracy drops sharply; if class imbalance is severe, SMOTE becomes essential
- First 3 experiments:
  1. Reproduce Experiment 1: Build binary LPP matrix from Children's Picture Books Lexicon vs. GRE/academic lists; run t-tests with Bonferroni correction; verify 84 significant variables in first 6 positions
  2. Replicate Experiment 2: Generate synthetic simple/complex sets from AoA/frequency/concreteness extremes; confirm 66 common significant LPPs; test classifier on third dataset
  3. Run Experiment 3: Using only the 66 common LPPs, train RandomForest on held-out folds; measure 70% accuracy and compare to 66 random variables baseline

## Open Questions the Paper Calls Out

### Open Question 1
Can LPPs predict word complexity across languages beyond English, and what cross-linguistic patterns exist? The authors note their study focuses on English but suggest LPPs could capture "latent complexity" - a general phenomenon. This remains unresolved as the paper only analyzes English datasets and doesn't test cross-linguistic generalizability. Testing LPPs on word complexity classification across multiple languages with different orthographic systems would resolve this.

### Open Question 2
How do LPPs interact with morphological complexity (e.g., affixation patterns) in determining overall word complexity? The authors explicitly exclude morphological analysis, focusing only on "atomic" letter positions, and note previous work used word endings. This remains unresolved as the study isolates LPPs from morphological features rather than examining their interaction. Comparative analysis of classification accuracy using LPPs alone vs. LPPs combined with morphological features would resolve this.

### Open Question 3
What is the developmental trajectory of LPPs in early literacy acquisition, and how does this relate to reading errors? The authors note that reading/phonological errors occur more frequently with vowels than consonants, and observe vowel/consonant patterns in their complexity findings. This remains unresolved as the study doesn't examine how children's use of LPPs changes with literacy development or connect to error patterns. Longitudinal studies tracking LPPs in children's writing samples alongside error rates across developmental stages would resolve this.

## Limitations
- The method relies on potentially non-representative training corpora (children's picture books vs. academic/GRE words), limiting generalizability
- Cross-dataset stability is moderate (70% accuracy) compared to within-dataset performance (97%), suggesting some overfitting
- The approach doesn't account for morphological complexity, which may be a significant factor in word difficulty

## Confidence
- High confidence: The statistical methodology (Bonferroni-corrected t-tests, SMOTE balancing, RandomForest validation) is sound and well-documented
- Medium confidence: The claim that LPPs capture fundamental orthographic-phonological complexity relationships, based on vowel-consonant patterns
- Medium confidence: Cross-dataset generalizability, given 70% accuracy on a third dataset but 97% on extreme-word combinations

## Next Checks
1. Test the 66-LPP classifier on a diverse corpus spanning multiple genres (fiction, news, technical writing) to assess true domain robustness
2. Compare LPP-based complexity scores against human judgments of word difficulty across varying education levels and language backgrounds
3. Validate whether LPPs maintain predictive power when applied to morphologically complex words (compounds, inflections) that may violate simple letter-position patterns