---
ver: rpa2
title: 'DAM: Dynamic Adapter Merging for Continual Video QA Learning'
arxiv_id: '2403.08755'
source_url: https://arxiv.org/abs/2403.08755
tags:
- learning
- adapter
- merging
- continual
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for continual video question-answering
  (VidQA) learning. The key idea is to use dynamic adapter merging, where dataset-specific
  adapters are trained sequentially while freezing a pretrained video-language backbone.
---

# DAM: Dynamic Adapter Merging for Continual Video QA Learning

## Quick Facts
- arXiv ID: 2403.08755
- Source URL: https://arxiv.org/abs/2403.08755
- Authors: Feng Cheng; Ziyang Wang; Yi-Lin Sung; Yan-Bo Lin; Mohit Bansal; Gedas Bertasius
- Reference count: 15
- Primary result: Proposed DAM method outperforms prior state-of-the-art continual learning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA datasets

## Executive Summary
This paper introduces Dynamic Adapter Merging (DAM), a novel approach for continual video question-answering (VidQA) learning. The method addresses catastrophic forgetting by training dataset-specific adapters sequentially while keeping a pretrained video-language backbone frozen. During inference, a non-parametric router estimates the relevance of each adapter to a given video-question input, and adapters are dynamically merged based on these probabilities. DAM achieves significant performance improvements over existing continual learning methods while maintaining parameter efficiency.

## Method Summary
DAM employs a pretrained video-language backbone (CLIP ViT-L/14 + DeBERTa-V2-XL) with dataset-specific adapters inserted after each self-attention and feed-forward network layer. Adapters are trained sequentially on arriving datasets while the backbone remains frozen. The non-parametric router computes adapter relevance probabilities using cosine similarity to dataset centroids. During inference, top-k adapters are dynamically merged through weighted averaging based on router probabilities, producing a model tailored to each test sample. Continual initialization further improves adaptation by initializing new adapter weights with previous adapter weights.

## Key Results
- DAM outperforms prior state-of-the-art continual learning approaches by 9.1% on average
- DAM exhibits 1.9% less forgetting compared to baseline methods across 6 VidQA datasets
- The non-parametric router with cosine similarity achieves superior performance despite its simplicity

## Why This Works (Mechanism)

### Mechanism 1
Dynamic adapter merging enables knowledge sharing across similar dataset domains, improving performance when the router makes incorrect predictions. By merging multiple adapters weighted by their router-predicted relevance rather than selecting a single adapter, the model incorporates knowledge from multiple adapters, including those associated with the correct domain even if the router is partially inaccurate.

### Mechanism 2
The non-parametric router based on cosine similarity to dataset centroids provides effective dataset relevance estimation for adapter selection. The router computes the cosine similarity between the test sample's feature representation and each dataset's centroid, then converts these similarities to probabilities using softmax with temperature scaling.

### Mechanism 3
Continual initialization of adapters (initializing new adapter weights with previous adapter weights) reduces interference and improves adaptation efficiency. This places new adapters in a parameter space closer to previously learned solutions, reducing optimization difficulty when adapting to new datasets.

## Foundational Learning

- **Concept: Adapter-based parameter-efficient fine-tuning**
  - Why needed here: Adapters provide a parameter-efficient way to learn dataset-specific transformations while keeping the large pretrained backbone frozen, enabling continual learning without catastrophic forgetting
  - Quick check question: What percentage of total parameters do the adapters typically represent in this framework?

- **Concept: Catastrophic forgetting and continual learning**
  - Why needed here: The paper addresses the challenge of learning from sequentially arriving datasets without losing previously acquired knowledge, which is the fundamental problem of catastrophic forgetting
  - Quick check question: What are the two main strategies mentioned in the paper for preventing catastrophic forgetting?

- **Concept: Model merging and parameter space averaging**
  - Why needed here: The dynamic adapter merging scheme draws inspiration from model merging literature, using weighted averaging of parameters to combine knowledge from multiple domain-specific models
  - Quick check question: How does the dynamic merging scheme differ from traditional model merging approaches?

## Architecture Onboarding

- **Component map**: Frozen pretrained video-language backbone -> Dataset-specific adapters -> Non-parametric router -> Dynamic adapter merging module

- **Critical path**: During inference, the test sample flows through the frozen backbone → router computes adapter probabilities → top-k adapters are merged → merged adapter + backbone produce final prediction

- **Design tradeoffs**:
  - Adapter merging vs. single adapter selection: Merging provides robustness to router errors but adds computational overhead
  - Router complexity vs. performance: Simple non-parametric router performs better than complex learned routers
  - Number of adapters to merge (top-k): More adapters provide more robustness but may introduce noise

- **Failure signatures**:
  - Poor router accuracy indicates difficulty distinguishing between similar domains
  - Degradation when merging too many dissimilar adapters
  - Performance drops when adapter initialization doesn't match new dataset characteristics

- **First 3 experiments**:
  1. Verify that individual adapters trained on separate datasets outperform a single shared adapter
  2. Test router accuracy on dataset identification task before evaluating full system
  3. Compare performance of top-1 adapter selection vs. top-2 merging on a validation set

## Open Questions the Paper Calls Out

The paper identifies three key limitations: 1) The need to assess DAM's effectiveness across a more extensive domain spectrum involving a substantial number of domains (e.g., 100+), 2) The potential for more advanced adapter merging techniques beyond simple weighted averaging to further improve knowledge sharing across domains, and 3) The impact of different adapter architectures (number of layers, adapter size, downsampling rate) on DAM's performance and parameter efficiency.

## Limitations
- Evaluation assumes unknown dataset identity during inference, which may not reflect practical scenarios
- Router effectiveness relies on clear feature space separation between datasets, which may not hold for similar domains
- Adapter architecture specifics and hyperparameters are not fully detailed, potentially affecting reproducibility

## Confidence

- **High confidence**: Adapter-based approach effectively prevents catastrophic forgetting through parameter-efficient fine-tuning while keeping backbone frozen
- **Medium confidence**: Dynamic adapter merging provides robustness to router errors and facilitates knowledge sharing, though optimal number of adapters to merge not systematically studied
- **Low confidence**: Claim that continual initialization consistently improves performance across all dataset orders lacks comprehensive ablation studies

## Next Checks

1. Conduct sensitivity analysis on the temperature hyperparameter τ in the router function and the top-k parameter for adapter merging to determine optimal values
2. Evaluate performance when dataset identity is known during inference to establish an upper bound and assess the cost of the unknown-domain assumption
3. Test the method on datasets with smaller domain gaps to measure router accuracy degradation and its impact on overall performance