---
ver: rpa2
title: Language Models Encode Numbers Using Digit Representations in Base 10
arxiv_id: '2410.11781'
source_url: https://arxiv.org/abs/2410.11781
tags:
- numbers
- digit
- number
- representations
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how large language models (LLMs) represent
  numbers internally, motivated by the observation that LLM errors on numerical tasks
  (e.g., addition, comparison) are scattered across individual digits rather than
  being normally distributed around the correct numeric value. The authors propose
  that LLMs use digit-wise circular representations in base 10 rather than linear
  value representations.
---

# Language Models Encode Numbers Using Digit Representations in Base 10
## Quick Facts
- arXiv ID: 2410.11781
- Source URL: https://arxiv.org/abs/2410.11781
- Reference count: 12
- Primary result: Large language models use digit-wise circular representations in base 10 rather than linear value representations for numbers

## Executive Summary
This paper investigates how large language models internally represent numbers, motivated by the observation that LLM errors on numerical tasks are scattered across individual digits rather than being normally distributed around correct values. The authors propose that LLMs use digit-wise circular representations in base 10, and through probing experiments on Llama 3 8B and Mistral 7B, demonstrate that circular digit-wise probes can accurately reconstruct digit values with up to 91-92% accuracy. Causal interventions further support this hypothesis, showing that modifying digit representations successfully alters encoded numbers in approximately 15-50% of cases, while linear interventions achieve less than 1% success.

## Method Summary
The authors use circular probing to extract digit representations from LLM hidden states, training probes to predict digit values from representations at various layers. They employ causal interventions by flipping digit representations through orthogonal projection and scaling operations, measuring success rates when modified outputs match intended digit changes. The experiments involve multi-operand addition tasks (7 operands, numbers 0-999) and number comparison tasks, with probing datasets covering numbers 0-2000. PCA visualization is used to analyze the structure of hidden representations, and error patterns are examined to distinguish between digit-wise and value-based representations.

## Key Results
- Circular digit-wise probes achieve 91-92% accuracy in reconstructing number values in base 10, while linear probes fail to accurately recover values
- Causal interventions modifying digit representations succeed in 15-50% of cases depending on digit position, while linear interventions achieve less than 1% success
- Error patterns on numerical tasks show digit-wise scattering rather than value-based distribution around correct answers

## Why This Works (Mechanism)
The paper's findings suggest that LLMs develop digit-wise circular representations as a more robust alternative to linear value representations for numerical processing. This mechanism appears to emerge from the need to handle the discrete nature of digits while maintaining some continuity for arithmetic operations. The circular representation allows the model to maintain proximity between similar digits (e.g., 9 and 0) while keeping them distinct from non-adjacent digits, which may be particularly useful for tasks like carrying in addition or borrowing in subtraction. The success of causal interventions confirms that these digit representations are not merely correlated with numerical processing but are causally significant in determining output.

## Foundational Learning
- **Base-n number systems**: Understanding how numbers can be represented in different bases (binary, decimal, etc.) is crucial for interpreting the probing experiments across bases 2-14 and 1000, 2000.
- **Circular embeddings**: The concept of using periodic/circular representations for categorical data with inherent ordering is essential for understanding why digit-wise circular probes outperform linear ones.
- **Causal intervention methodology**: Knowledge of how to perform and interpret causal interventions in neural networks is necessary to understand the flipping experiments and their implications.
- **Probing techniques**: Understanding how linear and non-linear probes can extract information from hidden representations is fundamental to the experimental approach.
- **Error analysis patterns**: Recognizing the difference between errors that are scattered across digits versus those distributed around numeric values is key to interpreting the empirical findings.

## Architecture Onboarding
- **Component map**: Tokenization -> Transformer layers (Llama 3 8B/Mistral 7B) -> Hidden representations -> Circular probes / Causal interventions -> Output predictions
- **Critical path**: The path from hidden representations at layer 3 through the circular probe intervention to the final output is critical, as this is where digit representations are both most accessible and causally significant.
- **Design tradeoffs**: The choice between digit-wise circular representations versus value-based linear representations involves tradeoffs between noise robustness (favoring digit-wise) and computational efficiency for certain operations (potentially favoring value-based).
- **Failure signatures**: When digit-wise circular probes fail to reconstruct values accurately, it may indicate that digit representations are either not present in the targeted layer or are obscured by other representations. When causal interventions fail, it suggests either insufficient scaling or that digit representations are not causally primary at that layer.
- **3 first experiments**: 1) Train circular probes at different layers to identify where digit representations first emerge, 2) Compare probe accuracy between base 10 and other bases to confirm the base-10 specificity, 3) Vary the scaling factor in causal interventions to find optimal values for different digit positions.

## Open Questions the Paper Calls Out
- **Open Question 1**: How does the base-10 digit-wise representation extend to fractions, negative numbers, and other non-natural numbers? The authors acknowledge their focus on natural numbers leaves open whether digit-wise circular representation applies to other numeric types.
- **Open Question 2**: Are digit-wise circular representations the only way numbers are encoded in LLMs, or do multiple redundant representations exist? The authors state their work doesn't conclusively rule out superposition of multiple representations.
- **Open Question 3**: What causes LLMs to develop base-10 digit-wise representations rather than value-based representations? The authors suggest possible reasons (noise robustness, training data frequency) but don't provide empirical evidence.

## Limitations
- The study is limited to two specific models (Llama 3 8B and Mistral 7B), which may not generalize across different architectures or scales
- The research only examines natural numbers (0-999), leaving open questions about representation of fractions, negative numbers, and large numbers
- The paper doesn't explore whether digit representations are learned during pre-training or fine-tuning, or whether this is an emergent property of transformer architectures

## Confidence
- **High confidence**: Empirical findings showing digit-wise circular probes outperform linear probes in reconstructing number values
- **Medium confidence**: Causal intervention results showing significant but incomplete success rates (15-50% depending on digit position)
- **Medium confidence**: Claims about digit-wise representations being more robust to noise, which would benefit from additional controlled experiments

## Next Checks
1. **Architecture generalization**: Test the digit-wise representation hypothesis across multiple model architectures (GPT, BERT, PaLM) and scales to determine if this is a universal property of transformer-based LLMs.
2. **Temporal dynamics**: Analyze how digit representations evolve across layers using PCA and probing accuracy to identify when and where these representations emerge during the forward pass.
3. **Robustness quantification**: Systematically vary noise levels in input representations and measure the impact on digit reconstruction accuracy versus value-based reconstruction to directly compare robustness claims.