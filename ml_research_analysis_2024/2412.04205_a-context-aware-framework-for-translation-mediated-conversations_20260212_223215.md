---
ver: rpa2
title: A Context-aware Framework for Translation-mediated Conversations
arxiv_id: '2412.04205'
source_url: https://arxiv.org/abs/2412.04205
tags:
- context
- translation
- language
- tower
- chat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses translation quality in bilingual, multi-turn
  conversations, where context loss and ambiguity degrade accuracy. The authors propose
  a context-aware framework for large language models that incorporates bilingual
  conversational context during both training (via context-augmented instruction finetuning)
  and inference (via quality-aware decoding with context-aware metrics).
---

# A Context-aware Framework for Translation-mediated Conversations

## Quick Facts
- arXiv ID: 2412.04205
- Source URL: https://arxiv.org/abs/2412.04205
- Reference count: 40
- Primary result: TOWER CHAT model achieves up to 4+ CHR F and 1+ COMET gains over strong baselines in bilingual conversational translation

## Executive Summary
This paper addresses the challenge of translation quality in bilingual, multi-turn conversations where context loss and ambiguity degrade accuracy. The authors propose a context-aware framework for large language models that incorporates bilingual conversational context during both training (via context-augmented instruction finetuning) and inference (via quality-aware decoding with context-aware metrics). Evaluated on customer chat and user-assistant datasets across five language pairs, their TOWER CHAT model consistently outperforms strong baselines like GPT-4o and TOWER INSTRUCT, achieving up to 4+ CHR F and 1+ COMET gains. Context-aware decoding further improves consistency and accuracy in discourse phenomena.

## Method Summary
The framework combines context-augmented instruction finetuning with quality-aware decoding. During training, bilingual conversational context from previous turns is incorporated into instruction prompts to teach the model to attend to discourse-level elements. At inference, the model generates multiple candidate translations and uses Minimum Bayes Risk decoding with context-aware metrics (like CONTEXT COMET) to select the translation that best fits the conversational context. The approach is evaluated on customer chat and user-assistant datasets across five language pairs (English â†” German, French, Portuguese, Korean, Dutch).

## Key Results
- TOWER CHAT outperforms GPT-4o and TOWER INSTRUCT by up to 4+ CHR F and 1+ COMET points
- Context-aware decoding resolves discourse phenomena and improves translation consistency
- Selective context usage based on ambiguity prediction yields targeted improvements
- Gains are most pronounced for low-quality translations and in settings requiring cross-lingual context

## Why This Works (Mechanism)

### Mechanism 1
Context-augmented instruction finetuning improves model performance by explicitly conditioning translation outputs on bilingual conversational context during training. During training, each instance is formatted with a context-augmented prompt that includes previous bilingual exchanges. The model learns to attend to both contextual cues and language-specific discourse elements such as pronoun references, formality, and continuity. This shifts the model's learned distribution toward context-aware outputs.

### Mechanism 2
Quality-aware decoding with context-aware metrics improves contextual accuracy by reranking candidate translations based on how well they fit the preceding conversation. At inference, the model generates a set of candidate translations. A context-aware metric (e.g., CONTEXT COMET) scores each candidate by comparing it against both the source and the bilingual context. The candidate with the highest score is selected, favoring translations that resolve ambiguities consistent with the conversation history.

### Mechanism 3
Context is most beneficial when the translation task involves resolving semantic ambiguity that depends on previous conversational turns. The model's P-CXMI and likelihood difference metrics show that translations improve when the reference or hypothesis requires context for accurate interpretation. This suggests that context usage is not uniformly beneficial but targeted to ambiguous cases.

## Foundational Learning

- Concept: Context in machine translation
  - Why needed here: Understanding how context extends beyond the current sentence is critical to designing models that can resolve discourse-level ambiguities like pronoun references, formality, and continuity in conversations.
  - Quick check question: What is the difference between sentence-level and document-level context in translation, and why does conversational context pose unique challenges?

- Concept: Instruction finetuning for LLMs
  - Why needed here: The framework relies on finetuning LLMs with specially formatted context-augmented prompts, so understanding how instruction finetuning shapes model behavior is essential.
  - Quick check question: How does instruction finetuning differ from standard finetuning, and what role do prompt templates play in shaping model outputs?

- Concept: Quality-aware decoding and MBR
  - Why needed here: The inference strategy uses Minimum Bayes Risk decoding with context-aware metrics to select the best translation candidate, so familiarity with MBR and its variants is necessary.
  - Quick check question: How does MBR decoding with a quality metric differ from greedy decoding, and what are the computational trade-offs?

## Architecture Onboarding

- Component map:
  - Bilingual conversational datasets (WMT24 Chat, BCONTRAST) -> Context-augmented instruction finetuning -> TOWER CHAT model -> Epsilon sampling candidate generation -> Context-aware MBR decoding -> Selected translation output

- Critical path:
  1. Prepare context-augmented training instances
  2. Finetune model on context-aware prompts
  3. Generate candidates at inference (epsilon sampling)
  4. Rerank candidates using context-aware metric
  5. Select best translation

- Design tradeoffs:
  - Including full bilingual context vs. English-only context: bilingual context preserves language-specific nuances but adds complexity
  - Context window size: longer context may improve accuracy but increases computational cost and risk of irrelevant information
  - Number of candidates in MBR: more candidates improve reranking quality but increase latency

- Failure signatures:
  - Model fails to use context: MUDA F1 scores do not improve with context-aware prompts
  - Context-aware reranking degrades quality: CHR F or COMET scores drop after QAD
  - High latency: MBR decoding with many candidates is too slow for real-time use

- First 3 experiments:
  1. Compare CHR F/MUDA F1 between context-augmented and standard prompts on a small dev set
  2. Evaluate impact of context window size (2, 6, 10, full) on COMET scores for each language pair
  3. Measure latency and quality trade-offs for different numbers of MBR candidates (10, 50, 100)

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of TOWER CHAT vary across different conversational domains beyond customer support and personal assistant tasks? The paper states that the framework is designed to generalize to a wide range of LLM-driven interactions, including open-ended dialogue, but only evaluates on task-oriented domains (customer chat and user-assistant interactions).

### Open Question 2
What is the optimal strategy for dynamically selecting the number of context turns to include during inference to balance translation quality and computational efficiency? While the paper identifies variation in optimal context length across language pairs, it does not provide a concrete method for dynamically determining when and how much context to include.

### Open Question 3
How do different quality-aware decoding strategies (e.g., MBR with alternative metrics vs. quality estimation reranking) compare in terms of computational efficiency and translation quality for chat translation? The paper demonstrates the effectiveness of MBR with context-aware metrics but does not compare it to other decoding strategies that might offer better efficiency.

## Limitations

- Context-augmented instruction finetuning approach shows promising results, but the exact prompt formatting and context selection criteria are not fully specified, limiting reproducibility
- Quality-aware decoding with context-aware metrics improves contextual accuracy, but the computational overhead of MBR decoding may impact real-time deployment
- The selective context usage mechanism based on ambiguity prediction is novel but relies on metrics (P-CXMI, likelihood difference) that may not generalize across all conversational domains

## Confidence

- **High**: Claims about overall performance improvements (CHR F, COMET gains) and qualitative error analysis showing context-aware decoding resolves discourse phenomena
- **Medium**: Claims about mechanism effectiveness (context-augmented finetuning learning context usage) and selective context application based on ambiguity
- **Low**: Claims about the specific impact of bilingual vs. English-only context and the generalizability of context-aware metrics across all language pairs

## Next Checks

1. Reproduce context-augmented prompt formatting: Create a small-scale experiment comparing context-augmented vs. standard prompts on a dev set to verify the impact on MUDA F1 scores.

2. Test context window sensitivity: Evaluate COMET scores across different context window sizes (2, 6, 10 turns) for each language pair to determine optimal context length and identify potential overfitting.

3. Measure computational overhead: Benchmark MBR decoding latency with different candidate pool sizes (10, 50, 100) and assess whether distillation techniques like SFT on QAD outputs can reduce computational cost without quality loss.