---
ver: rpa2
title: Intent Classification for Bank Chatbots through LLM Fine-Tuning
arxiv_id: '2410.04925'
source_url: https://arxiv.org/abs/2410.04925
tags:
- intent
- accuracy
- instruct
- these
- ne-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compared fine-tuned SlovakBERT with multilingual generative
  models (Llama 8b instruct and Gemma 7b instruct) for intent classification in banking
  chatbots. Fine-tuning SlovakBERT achieved 77.2% in-scope accuracy with 6.3% out-of-scope
  false positive rate, outperforming all other approaches including banking-tailored
  BERT (68.5% accuracy), pre-trained generative models (65.5-69.5% accuracy), and
  fine-tuned generative models (70.6-75.1% accuracy).
---

# Intent Classification for Bank Chatbots through LLM Fine-Tuning

## Quick Facts
- arXiv ID: 2410.04925
- Source URL: https://arxiv.org/abs/2410.04925
- Reference count: 6
- Fine-tuned SlovakBERT achieves 77.2% in-scope accuracy, outperforming multilingual generative models for banking intent classification

## Executive Summary
This study evaluates intent classification approaches for Slovak banking chatbots by comparing fine-tuned SlovakBERT against multilingual generative models. The research demonstrates that SlovakBERT fine-tuned on domain-specific data achieves superior accuracy (77.2%) compared to banking-tailored BERT (68.5%), pre-trained generative models (65.5-69.5%), and fine-tuned generative models (70.6-75.1%). The work highlights the importance of language-specific pre-training for domains where linguistic nuances significantly impact classification performance.

## Method Summary
The study fine-tunes multiple models including SlovakBERT, banking-tailored BERT, Gemma 7b instruct, and Llama3 8b instruct using LoRA parameter-efficient training on a proprietary Slovak banking dataset. Two training datasets are evaluated: a "simple" set with 10-20 examples per intent and a "generated" set with 20-500 examples per intent. Models are evaluated on in-scope accuracy and out-of-scope false positive rate using a test set of approximately 300 samples. Text preprocessing includes lowercasing and removing diacritics and punctuation.

## Key Results
- SlovakBERT fine-tuned on "generated" dataset achieves 77.2% in-scope accuracy with 6.3% out-of-scope false positive rate
- Banking-tailored BERT achieves 68.5% accuracy, while fine-tuned Gemma 7b and Llama3 8b achieve 70.6% and 75.1% respectively
- The best model improves 10% over the proprietary baseline (67% accuracy) without deep learning
- Fine-tuning on the "generated" dataset with more examples per intent provides optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SlovakBERT achieves higher in-scope accuracy because it is pre-trained on Slovak language data, preserving linguistic nuances that multilingual models lose during cross-lingual training.
- Mechanism: The model's embeddings capture Slovak-specific morphology and syntax patterns that directly improve intent discrimination in banking queries.
- Core assumption: Banking intents are linguistically distinct in Slovak, and this distinction is captured by SlovakBERT's pre-training but diluted in multilingual models.
- Evidence anchors:
  - [abstract] "SlovakBERT outperforms the other models in terms of in-scope accuracy"
  - [section 2.3.1] "Since the data at hand is in the Slovak language, the choice of a model with Slovak understanding was inevitable. Therefore, we have opted for a model named SlovakBERT"
- Break condition: If banking intents are expressed in highly standardized forms, linguistic nuances become less critical and multilingual models catch up.

### Mechanism 2
- Claim: Fine-tuning on the "generated" dataset with more examples per intent improves model performance by increasing the diversity of training patterns.
- Mechanism: More training samples reduce overfitting to specific phrasings and improve generalization across different ways users might express the same intent.
- Core assumption: The "generated" dataset, despite increased repetition, provides sufficient semantic variety to improve classification accuracy.
- Evidence anchors:
  - [section 2.3.1] "The results showed that the optimal performance of the model is achieved when training on the 'generated' dataset"
  - [section 2.3.1] "we have access to a comprehensive set of intents, each accompanied by corresponding user query examples. We consider two sets of training data: a 'simple' set, providing 10 to 20 examples for each intent, and a 'generated' set, which encompasses 20 to 500 examples per intent"
- Break condition: If the "generated" dataset contains noisy or unrepresentative examples, the increased volume could degrade rather than improve performance.

### Mechanism 3
- Claim: The baseline proprietary model's 67% accuracy sets a realistic lower bound for banking intent classification without deep learning, making the 77.2% improvement significant.
- Mechanism: The proprietary model likely uses rule-based or simpler ML approaches that struggle with linguistic variation, while fine-tuned SlovakBERT captures complex patterns through its deep architecture.
- Core assumption: The baseline model represents a practical implementation of traditional intent classification approaches in banking chatbots.
- Evidence anchors:
  - [section 2] "Our baseline proprietary intent classification model, which does not leverage any deep learning framework, achieves a 67% accuracy on a real-world test dataset"
  - [section 3] "The best model produced from these experiments had an average accuracy of 77.2% with a standard deviation of 0.012, representing an increase of 10% from the baseline model's accuracy"
- Break condition: If the baseline model's evaluation methodology differs significantly from the deep learning models, the comparison may not be valid.

## Foundational Learning

- Concept: Fine-tuning vs. Pre-training
  - Why needed here: Understanding when to fine-tune an existing model versus using it pre-trained is crucial for choosing the right approach for banking intent classification
  - Quick check question: What's the difference between adapting a pre-trained model's weights for a specific task versus using it as-is?

- Concept: Intent Classification vs. Generation
  - Why needed here: The study contrasts classification approaches (mapping queries to predefined intents) with generative approaches (creating responses), which is fundamental to understanding the methodology
  - Quick check question: Why would a banking chatbot prefer predetermined responses over generated ones?

- Concept: Language-specific vs. Multilingual Models
  - Why needed here: The performance difference between SlovakBERT and multilingual models demonstrates why language-specific pre-training matters for certain applications
  - Quick check question: How does training on a specific language versus multiple languages affect a model's performance on tasks in that language?

## Architecture Onboarding

- Component map: Data preprocessing → Model selection (SlovakBERT, multilingual LLMs) → Fine-tuning pipeline → Evaluation (in-scope accuracy, out-of-scope false positive rate)
- Critical path: Text preprocessing (lowercasing, diacritic/punctuation removal) → SlovakBERT fine-tuning → Evaluation on real-world test set
- Design tradeoffs: SlovakBERT offers superior accuracy but requires Slovak language expertise; multilingual models offer broader applicability but lower performance; fine-tuning requires computational resources but improves accuracy
- Failure signatures: Low in-scope accuracy suggests insufficient training data or poor preprocessing; high out-of-scope false positive rate indicates the model is overconfident in classifying unknown intents
- First 3 experiments:
  1. Compare SlovakBERT with and without text preprocessing to validate the importance of lowercasing and diacritic removal
  2. Test SlovakBERT fine-tuning with different dataset ratios (simple vs. generated) to confirm optimal training data mix
  3. Evaluate the baseline proprietary model on the same test set to establish the performance floor for traditional approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would training SlovakBERT with the "simple" dataset (10-20 examples per intent) instead of the "generated" dataset (20-500 examples per intent) yield better performance, considering the repetition of phrases in the generated set might lead to overfitting?
- Basis in paper: [explicit] The paper states that "the optimal performance of the model is achieved when training on the 'generated' dataset," but doesn't explore whether the simpler dataset might be better due to less repetition.
- Why unresolved: The study only tested the "generated" dataset as optimal and didn't conduct comparative experiments with the "simple" dataset to validate if the increased repetition actually improves or harms performance.
- What evidence would resolve it: Conduct experiments training SlovakBERT on both datasets separately and compare their accuracy and false positive rates on the test set.

### Open Question 2
- Question: Would fine-tuning only a subset of SlovakBERT layers (such as one quarter or one half) while keeping the rest frozen provide better accuracy than fine-tuning the entire model, especially considering computational efficiency?
- Basis in paper: [explicit] The paper states that "the average improvement achieved by these adjustments to the model's training process is statistically insignificant" but chose not to implement layer-specific fine-tuning "to keep the pipeline as simple as possible."
- Why unresolved: The study dismissed layer-specific fine-tuning based on statistical insignificance in early experiments but didn't thoroughly explore whether selective layer tuning could optimize both performance and computational resources.
- What evidence would resolve it: Systematically test different layer subsets (one quarter, one half, three quarters) with statistical significance testing to determine if any configuration provides meaningful performance gains or efficiency improvements.

### Open Question 3
- Question: How would domain-specific fine-tuning of multilingual generative models like Gemma 7b instruct and Llama3 8b instruct on banking-specific data affect their performance compared to the current results?
- Basis in paper: [inferred] The paper notes that "neither Gemma 7b instruct nor Llama3 8b instruct models outperformed the capabilities of the fine-tuned SlovakBERT model," suggesting that additional domain-specific fine-tuning might improve their performance.
- Why unresolved: The study only used the general "simple" dataset for fine-tuning the generative models and didn't explore whether additional fine-tuning on banking-specific corpora would improve their accuracy and reduce false positive rates.
- What evidence would resolve it: Fine-tune the generative models on a dataset specifically curated with banking industry terminology and compare their performance metrics against the current results and SlovakBERT baseline.

## Limitations
- Proprietary dataset prevents independent verification and limits reproducibility across different banking contexts
- Findings are specific to Slovak language banking queries and may not generalize to other languages or domains
- Evaluation focuses only on in-scope accuracy and out-of-scope false positive rate, omitting precision, recall, and F1-score metrics
- Computational efficiency and inference costs are not addressed, which are critical for real-world deployment

## Confidence

**High confidence**: The core finding that SlovakBERT fine-tuning achieves 77.2% in-scope accuracy represents a robust result supported by systematic comparison with multiple baselines and careful experimental design.

**Medium confidence**: The relative performance ranking of different models appears consistent across experiments, but specific accuracy values may be influenced by dataset characteristics and preprocessing choices that aren't fully specified.

**Low confidence**: The claim that linguistic nuances preserved by SlovakBERT directly cause the performance improvement lacks direct evidence, though the correlation between Slovak pre-training and higher accuracy is demonstrated.

## Next Checks
1. **Cross-linguistic validation**: Test whether SlovakBERT's advantage extends to other languages by replicating the experiment with comparable language-specific models (e.g., FinBERT for English banking queries) versus multilingual alternatives.

2. **Dataset quality audit**: Analyze the "generated" dataset for potential biases or artifacts that could artificially inflate performance, examining whether increased training examples introduce semantic redundancy or genuinely capture diverse user query patterns.

3. **Computational cost analysis**: Measure inference latency and resource requirements for SlovakBERT versus fine-tuned generative models under realistic deployment conditions to reveal whether accuracy advantage comes with prohibitive computational costs.