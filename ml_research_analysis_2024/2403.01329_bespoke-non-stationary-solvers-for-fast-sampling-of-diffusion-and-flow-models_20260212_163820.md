---
ver: rpa2
title: Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models
arxiv_id: '2403.01329'
source_url: https://arxiv.org/abs/2403.01329
tags:
- solvers
- solver
- diffusion
- psnr
- family
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver
  distillation approach to improve sample efficiency of Diffusion and Flow models.
  BNS solvers are based on a family of non-stationary solvers that provably subsumes
  existing numerical ODE solvers and consequently demonstrate considerable improvement
  in sample approximation (PSNR) over these baselines.
---

# Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models

## Quick Facts
- arXiv ID: 2403.01329
- Source URL: https://arxiv.org/abs/2403.01329
- Reference count: 40
- This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver distillation approach to improve sample efficiency of Diffusion and Flow models.

## Executive Summary
This paper presents Bespoke Non-Stationary (BNS) Solvers, a novel approach to improve sample efficiency for diffusion and flow model sampling. BNS solvers leverage a family of non-stationary solvers that provably subsume existing numerical ODE solvers, enabling optimized sampling trajectories that significantly improve sample approximation quality. The method demonstrates substantial improvements over baseline solvers across multiple tasks including class-conditional image generation, text-to-image generation, and text-to-audio generation, while requiring minimal parameter optimization and training time compared to model distillation approaches.

## Method Summary
BNS solvers optimize a small set of parameters (<200) to learn an improved sampling trajectory for a given pre-trained diffusion or flow model. The method generates synthetic training data from the pre-trained model, then optimizes the NS solver parameters to minimize PSNR loss between generated samples and ground truth images. A key innovation is the use of scale-time transformations to precondition the velocity field, improving convergence. The approach requires separate optimization for each target number of function evaluations (NFE), but achieves two orders of magnitude faster training compared to model distillation while maintaining sample diversity.

## Key Results
- BNS solver achieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64
- Compared to model distillation, BNS solvers benefit from tiny parameter space (<200 parameters) and fast optimization (two orders of magnitude faster)
- BNS solvers nearly close the gap from standard distillation methods such as Progressive Distillation in the low-medium NFE regime
- Significant improvement in sample approximation (PSNR) demonstrated across conditional image generation, text-to-image generation, and text-to-audio generation

## Why This Works (Mechanism)

### Mechanism 1
BNS solvers subsume all previous numerical ODE solvers used for diffusion/flow sampling by allowing arbitrary linear combinations of all previous points and velocities. The parameter space of NS solvers is expressive enough to approximate any convergent sampling trajectory for a given VF. The core assumption is that optimization can find good parameter combinations without getting stuck in local minima.

### Mechanism 2
BNS optimization is orders of magnitude faster than model distillation due to the tiny parameter count (<200) versus millions for model distillation, and the direct PSNR loss objective over a small synthetic dataset. The core assumption is that PSNR on the validation set correlates strongly with perceptual quality on unseen data.

### Mechanism 3
Preconditioning the VF via scale-time transformation improves convergence and sample quality by transforming the sampling ODE into a simpler trajectory in transformed space. The core assumption is that higher-variance source distributions yield smoother trajectories that are easier to approximate numerically.

## Foundational Learning

- **Ordinary Differential Equations and numerical solvers (Euler, Midpoint, Runge-Kutta, Multistep)**
  - Why needed here: Sampling diffusion/flow models reduces to solving an ODE; understanding solver families and their convergence properties is essential to appreciate why NS solvers can subsume them
  - Quick check question: What is the update rule for a 2nd-order Runge-Kutta (Midpoint) solver?

- **Scale-Time (ST) transformations and their effect on ODEs**
  - Why needed here: ST transformations allow reparameterizing the sampling trajectory to a simpler form; this is how exponential integrators and preconditioning work
  - Quick check question: Given a VF ut(x), write the transformed VF under ST (sr, tr)

- **Diffusion and Flow-Matching model parametrizations (ϵ-prediction, x-prediction, velocity field prediction)**
  - Why needed here: The form of the VF ut(x) depends on the model parametrization and scheduler; BNS solvers are agnostic to this but need to know how to sample the VF
  - Quick check question: How does the VF ut(x) differ between ϵ-prediction and x-prediction models?

## Architecture Onboarding

- **Component map:** Pre-trained diffusion/flow model -> Synthetic dataset generator -> NS solver parameterization -> PSNR loss function -> Adam optimizer -> Validation

- **Critical path:** Generate training pairs -> Initialize θ from baseline solver -> Run NS sampling -> Compute PSNR loss -> Backprop -> Update θ -> Validate -> Repeat

- **Design tradeoffs:** Small parameter space enables fast training but risks underfitting; synthetic dataset avoids data privacy issues but may not cover all modes; fixed NFE per solver requires separate training for each NFE target

- **Failure signatures:** Validation PSNR plateaus early indicates stuck in local minimum; PSNR good but FID poor suggests loss function mismatch with perceptual quality; instability during training indicates bad preconditioning or too high learning rate

- **First 3 experiments:**
  1. Train BNS solver for NFE=8 on ImageNet-64 ϵ-VP model with RK-Euler initialization, σ0=1, learning rate 5e-4; report PSNR/FID
  2. Vary σ0 in {0.5, 1, 5, 10} and compare convergence curves and final PSNR
  3. Compare BNS solver initialized from Midpoint vs Euler on same task and measure improvement

## Open Questions the Paper Calls Out

- **Can a single BNS solver handle different NFEs without degrading performance?**
  - Question: Can a single BNS solver handle different NFEs without degrading performance?
  - Basis in paper: The authors mention this as an "interesting future research question" in the conclusions
  - Why unresolved: Current BNS implementation requires separate optimization for each NFE target
  - What evidence would resolve it: Demonstration of a single BNS solver architecture that maintains comparable PSNR/FID across a range of NFEs (e.g., 8-32) without re-optimization

- **What is the theoretical limit of NFE reduction achievable with BNS solvers?**
  - Question: What is the theoretical limit of NFE reduction achievable with BNS solvers?
  - Basis in paper: The paper shows BNS achieves 5-10dB improvement over baselines but doesn't reach the 1-4 NFE regime
  - Why unresolved: The paper demonstrates practical improvements but doesn't analyze theoretical convergence bounds
  - What evidence would resolve it: Mathematical analysis of BNS convergence rates combined with empirical tests showing maximum achievable NFE reduction

- **Can conditional guidance be incorporated into the BNS solver optimization?**
  - Question: Can conditional guidance be incorporated into the BNS solver optimization?
  - Basis in paper: The authors identify this as an "interesting future work" in the conclusions
  - Why unresolved: Current BNS formulation optimizes unconditional solvers, missing potential performance gains from guidance-aware optimization
  - What evidence would resolve it: Modified BNS framework that jointly optimizes solver parameters with guidance scale parameters

## Limitations

- BNS solvers require separate optimization for each target NFE, limiting flexibility
- Performance sensitivity to the synthetic training dataset composition and size is not fully characterized
- The method has only been validated on specific model architectures (ϵ-prediction and flow-matching) and may not generalize to all diffusion models

## Confidence

- Mechanism 1 (NS solver expressiveness): Medium - Theoretical claim is well-supported but empirical validation is limited to specific models
- Mechanism 2 (training speedup): High - Parameter count difference is substantial and training efficiency gains are clearly demonstrated
- Mechanism 3 (preconditioning benefits): Low - Benefits are shown but optimal σ0 values appear highly model-dependent with limited theoretical guidance

## Next Checks

1. Test BNS solvers on a diffusion model with a different architecture (e.g., denoising diffusion implicit models) to assess generalizability
2. Conduct an ablation study on the synthetic training dataset size and composition to determine minimum requirements
3. Evaluate the performance impact of BNS solvers across different guidance scales and classifier-free guidance settings to test robustness