---
ver: rpa2
title: Learning to Correct for QA Reasoning with Black-box LLMs
arxiv_id: '2406.18695'
source_url: https://arxiv.org/abs/2406.18695
tags:
- cobb
- reasoning
- adaptation
- black-box
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving reasoning capabilities
  of large language models (LLMs) in a black-box setting, without access to internal
  model details. The authors propose COBB, a novel framework that uses a trained adaptation
  model to map imperfect reasonings from the original black-box LLM to correct or
  improved reasonings.
---

# Learning to Correct for QA Reasoning with Black-box LLMs

## Quick Facts
- arXiv ID: 2406.18695
- Source URL: https://arxiv.org/abs/2406.18695
- Reference count: 25
- This paper proposes COBB, a framework that improves reasoning capabilities of black-box LLMs by training an adaptation model to correct imperfect reasonings.

## Executive Summary
This paper addresses the challenge of improving reasoning capabilities of large language models (LLMs) in a black-box setting, without access to internal model details. The authors propose COBB, a novel framework that uses a trained adaptation model to map imperfect reasonings from the original black-box LLM to correct or improved reasonings. COBB constructs an effective training dataset by subsampling representative reasoning pairs via a genetic algorithm that minimizes statistical divergence. The adaptation model is trained by contrasting the likelihoods of correct and incorrect reasonings. Experiments on four QA benchmarks show that COBB significantly improves reasoning accuracy compared to baselines.

## Method Summary
COBB works by first collecting multiple chain-of-thought reasonings from a black-box LLM for each QA sample, then labeling them as correct or incorrect using ground-truth answers. A genetic algorithm is used to subsample representative pairs of correct/incorrect reasonings that minimize statistical divergence from the full set. The adaptation model, initialized with an open-source LLM (Mistral-7B-inst-v2), is then trained using the constructed dataset with an ORPO objective to increase/decrease likelihoods of positive/negative reasonings. This adaptation model can then be used to improve the reasoning of the original black-box LLM.

## Key Results
- COBB achieves average accuracy improvements of 6.2% compared to the original black-box LLM (gpt-3.5-turbo)
- COBB outperforms previous state-of-the-art adaptation methods by 2.2% on average
- The framework demonstrates good transferability, successfully adapting both API-based and open-source LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The genetic algorithm-based dataset construction selects representative pairs that minimize statistical divergence between the sampled subset and the entire reasoning pair collection.
- **Mechanism:** The genetic algorithm iteratively samples subsets of reasoning pairs (positive/negative) and evaluates their representativeness using 2-Wasserstein distance between the distributions of likelihood differences. Better subsets are kept and mutated to generate new candidates.
- **Core assumption:** The likelihood differences of reasoning pairs can be modeled as samples from normal distributions, making 2-Wasserstein distance an appropriate metric.
- **Evidence anchors:**
  - [abstract] "we formulated the dataset construction as an optimization problem that minimizes the statistical divergence between the sampled subset and the entire collection, and solved it via a genetic algorithm"
  - [section 3.2] "we formulated an optimization problem that minimizes the statistical divergence between the sampled subset and the entire collection, and solved it via a genetic algorithm"
  - [corpus] Weak evidence - the corpus contains no papers specifically about genetic algorithms for dataset construction in LLM adaptation contexts
- **Break condition:** If the likelihood differences are not normally distributed, the 2-Wasserstein distance becomes a poor proxy for representativeness, and the genetic algorithm may converge to suboptimal subsets.

### Mechanism 2
- **Claim:** Contrasting likelihoods of positive and negative reasonings during training improves the adaptation model's ability to distinguish correct from incorrect reasoning.
- **Mechanism:** The adaptation model is trained using Odds Ratio Preference Optimization (ORPO), which maximizes the likelihood of correct reasoning while minimizing the likelihood of incorrect reasoning for the same input.
- **Core assumption:** The adaptation model can effectively learn to discriminate between correct and incorrect reasonings when trained on contrasting pairs.
- **Evidence anchors:**
  - [abstract] "We then train the adaptation model over the sampled pairs by contrasting the likelihoods of correct and incorrect reasonings"
  - [section 3.3] "we propose to further use the negative reasoning yn to lower its likelihood in the output space of πθ, while simultaneously increasing the likelihood of the positive reasoning yp"
  - [corpus] Weak evidence - while ORPO is mentioned in the corpus, there's no direct evidence of its effectiveness specifically for LLM reasoning correction
- **Break condition:** If the negative reasonings are too similar to positive ones or if the adaptation model lacks sufficient capacity to model the contrast, the training signal becomes too weak to improve performance.

### Mechanism 3
- **Claim:** Initializing the adaptation model with a smaller open-source LLM provides a good starting point for learning the correction mapping while being more efficient than fine-tuning the black-box LLM directly.
- **Mechanism:** The adaptation model πθ is initialized with Mistral-7B-inst-v2 and fine-tuned on the constructed dataset, leveraging pre-existing knowledge while adapting to the specific reasoning correction task.
- **Core assumption:** The pre-trained knowledge in the open-source LLM is relevant and transferable to the reasoning correction task for the black-box LLM.
- **Evidence anchors:**
  - [abstract] "the adaptation model is initialized with a relatively small open-source LLM and adapted over a collection of sub-sampled training pairs"
  - [section 3.1] "our goal is to obtain an adaptation model πθ, that can generate the adapted output... initialized with a relatively small open-source LLM"
  - [corpus] Weak evidence - the corpus contains no papers about initializing adaptation models with open-source LLMs for black-box correction tasks
- **Break condition:** If the open-source LLM's pre-training is too dissimilar from the black-box LLM's capabilities, the initialization may hinder rather than help the adaptation process.

## Foundational Learning

- **Concept: Statistical divergence and 2-Wasserstein distance**
  - Why needed here: The genetic algorithm uses 2-Wasserstein distance to measure how well a subset of reasoning pairs represents the entire collection, which is central to the dataset construction method
  - Quick check question: How does 2-Wasserstein distance differ from KL divergence when comparing two distributions of samples?

- **Concept: Contrastive learning and preference optimization**
  - Why needed here: The adaptation model is trained by contrasting the likelihoods of correct and incorrect reasonings, which requires understanding contrastive learning principles
  - Quick check question: What is the key difference between supervised fine-tuning and contrastive learning in the context of model adaptation?

- **Concept: Genetic algorithms and optimization**
  - Why needed here: The dataset construction uses a genetic algorithm to find optimal subsets of reasoning pairs, requiring understanding of evolutionary optimization techniques
  - Quick check question: How does a genetic algorithm balance exploration and exploitation when searching for optimal solutions?

## Architecture Onboarding

- **Component map:** Black-box LLM → Data collection pipeline → Genetic algorithm module → Adaptation model → Inference pipeline
- **Critical path:** Question → Black-box LLM reasoning generation → Correctness labeling → Genetic algorithm selection → Adaptation model training → Improved reasoning generation
- **Design tradeoffs:**
  - Using smaller open-source LLM vs. larger one for initialization (capacity vs. efficiency)
  - Number of reasoning samples K per question (coverage vs. cost)
  - Genetic algorithm iterations T (optimization quality vs. computation time)
  - Contrastive coefficient λ (training stability vs. correction effectiveness)
- **Failure signatures:**
  - Poor performance improvement: Likely issues with dataset construction or insufficient model capacity
  - Training instability: Contrastive coefficient λ may be too high
  - Slow convergence: Genetic algorithm parameters may need adjustment
  - Overfitting: Training dataset may be too small or lack diversity
- **First 3 experiments:**
  1. Verify dataset construction: Compare 2-Wasserstein distances for random vs. genetic algorithm sampling on a small dataset
  2. Test contrastive training: Train adaptation model with and without contrastive objective on a simple QA task
  3. Evaluate transferability: Train on one black-box LLM and test adaptation on another LLM with same task

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The framework's effectiveness is primarily demonstrated on QA benchmarks and may not generalize to other reasoning tasks.
- The genetic algorithm approach for dataset construction lacks comparison with simpler sampling methods to validate its superiority.
- Implementation details for critical components like the genetic algorithm remain underspecified.

## Confidence

### Major Uncertainties and Limitations
- **Low Confidence Claims:**
  - The effectiveness of the genetic algorithm for dataset construction in this specific context - while the paper claims it minimizes statistical divergence, there's no direct comparison with simpler sampling methods to validate this approach.
  - The generalizability of COBB across different reasoning tasks and model architectures - the experiments focus on specific QA benchmarks and adaptation from GPT-3.5-turbo, with limited exploration of other domains.

- **Medium Confidence Claims:**
  - The improvement in reasoning accuracy (6.2% and 2.2% gains) is measured on four specific benchmarks but may not translate to other reasoning tasks or more complex reasoning scenarios.
  - The assumption that contrasting likelihoods during training effectively teaches the adaptation model to distinguish correct from incorrect reasoning needs further validation across different model scales and reasoning types.

### Confidence Labels for Major Claim Clusters
1. **Framework Design and Implementation** (Medium): The overall architecture of COBB is clearly specified, but implementation details for critical components like the genetic algorithm remain underspecified.

2. **Experimental Results** (Medium): Results are well-documented on the tested benchmarks, but the scope is limited to specific QA tasks and model combinations.

3. **Theoretical Mechanisms** (Low): The paper provides theoretical justification for the genetic algorithm and contrastive learning approach, but lacks empirical validation of these specific mechanisms.

## Next Checks
1. **Dataset Construction Validation**: Implement and test alternative sampling methods (random, stratified, active learning-based) alongside the genetic algorithm approach to empirically validate its superiority in minimizing statistical divergence and improving adaptation performance.

2. **Mechanism Ablation Study**: Conduct controlled experiments to isolate the impact of each component (genetic algorithm, contrastive training, open-source initialization) by systematically removing or replacing them with simpler alternatives.

3. **Cross-Domain Transferability Test**: Evaluate COBB's performance on non-QA reasoning tasks (e.g., code generation, mathematical proof, logical reasoning) and with different model pairs (e.g., Claude → Llama, GPT-4 → Mistral) to assess true generalizability beyond the reported benchmarks.