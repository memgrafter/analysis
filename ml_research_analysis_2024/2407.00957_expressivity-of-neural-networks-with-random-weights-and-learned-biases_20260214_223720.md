---
ver: rpa2
title: Expressivity of Neural Networks with Random Weights and Learned Biases
arxiv_id: '2407.00957'
source_url: https://arxiv.org/abs/2407.00957
tags:
- networks
- neural
- learning
- network
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper establishes that neural networks with fixed random weights
  and learned biases can universally approximate continuous functions and dynamical
  systems. It provides theoretical proofs for both feedforward and recurrent architectures,
  showing that bias learning alone suffices for universal approximation with high
  probability.
---

# Expressivity of Neural Networks with Random Weights and Learned Biases

## Quick Facts
- arXiv ID: 2407.00957
- Source URL: https://arxiv.org/abs/2407.00957
- Reference count: 40
- Primary result: Neural networks with fixed random weights can universally approximate continuous functions and dynamical systems when only biases are learned.

## Executive Summary
This paper establishes that neural networks with fixed random weights and learned biases can achieve universal approximation for both feedforward and recurrent architectures. The authors prove that by learning only the bias parameters, networks can approximate any continuous function or dynamical system with high probability, provided the hidden layers are sufficiently wide. Empirical results validate these theoretical findings across multiple domains including multi-task learning, dynamical system forecasting, and auto-regressive modeling. The work demonstrates that learning-relevant changes in neural activity can be achieved without modifying synaptic weights, offering insights for both neuroscience and AI.

## Method Summary
The method involves initializing neural networks with random weights that are then frozen, while only the bias parameters are optimized. For feedforward networks, this means learning input and output layer biases while keeping all hidden weights fixed. For recurrent networks, both input and recurrent weights are frozen while biases are learned. The theoretical framework proves universal approximation under the condition of parameter-bounding activation functions. Empirical validation includes training networks on multiple image classification tasks (MNIST, KMNIST, Fashion MNIST, etc.) using shared random weights but task-specific bias vectors, as well as training recurrent networks to generate autonomous and non-autonomous dynamical systems.

## Key Results
- Theoretical proof of universal approximation for feedforward networks with fixed random weights and learned biases
- Extension of results to recurrent networks for dynamical system forecasting
- Demonstration of multi-task learning capabilities with distinct functional clusters emerging in hidden units
- Empirical validation showing performance comparable to fully-trained networks on simple tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Universal approximation is achieved by finding a subnetwork within a randomly initialized, fixed-weight network whose bias parameters match those of a fully trained network.
- Mechanism: A randomly initialized network is first constructed with very wide hidden layers. A smaller, fully trained network is then identified. The random network's hidden layer is then made wide enough such that, with high probability, a subnetwork exists whose parameters closely match those of the fully trained network. Bias learning is then used to activate this subnetwork.
- Core assumption: The activation function is "γ-parameter bounding," meaning universal approximation is possible even when individual parameters are bounded.
- Evidence anchors:
  - [abstract] "We provide theoretical and numerical evidence demonstrating that feedforward neural networks with fixed random weights can be trained to perform multiple tasks by learning biases only."
  - [section] "Proposition 1. The ReLU and the Heaviside step function are γ-parameter bounding activations for any γ > 0."
  - [corpus] Weak evidence. Corpus papers discuss random feature models and shallow networks but do not directly address parameter-bounding activations.
- Break condition: If the activation function is not parameter bounding, or if the hidden layer is not wide enough to find the matching subnetwork.

### Mechanism 2
- Claim: Bias learning shapes the effective connectivity matrix (Jacobian) in recurrent neural networks, allowing them to generate specific dynamical systems.
- Mechanism: Fixed random recurrent weights are initialized. Biases are then learned to shape the Jacobian matrix, which involves the derivative of the activation and the recurrent weight matrix. This allows the network to generate desired oscillations or dynamical patterns.
- Core assumption: The distribution of the initial recurrent weights affects the network's ability to learn the desired dynamics through bias learning alone.
- Evidence anchors:
  - [abstract] "We further show that an equivalent result holds for recurrent neural networks predicting dynamical system trajectories."
  - [section] "We found that bias learning required a large enough gain (at least g = 1) and failed for g < 1... In contrast, fully-trained networks with the same number of training parameters... were not sensitive to the value of the gain at initialization."
  - [corpus] Weak evidence. Corpus papers discuss random neural networks and dynamical systems but do not specifically address the relationship between initial weight distribution and bias learning in RNNs.
- Break condition: If the initial recurrent weights are not distributed appropriately, bias learning may not be able to shape the Jacobian sufficiently to generate the desired dynamics.

### Mechanism 3
- Claim: Bias learning leads to task-specific functional organization in multi-task learning scenarios.
- Mechanism: When a single random-weight network is trained on multiple tasks using only bias learning, distinct functional clusters of hidden units emerge. Each cluster is specialized to particular tasks, with some units being used for many or all tasks.
- Core assumption: The variance of a hidden unit's activation across the test set for each task (Task Variance, TV) reflects the extent to which a given hidden unit contributes to the given task.
- Evidence anchors:
  - [abstract] "We showcase the expressivity of bias-learned networks in auto-regressive modelling, multi-task learning, dynamic pattern generation, and dynamical system forecasting."
  - [section] "We found that distinct functional clusters of hidden units emerged... Most units reflected strong task specialization, i.e., they were only used for specific tasks."
  - [corpus] Weak evidence. Corpus papers discuss multi-task learning and neural network architectures but do not specifically address the emergence of task-specific functional organization through bias learning.
- Break condition: If the hidden layer is not wide enough, or if the tasks are too dissimilar, bias learning may not be able to create distinct functional clusters.

## Foundational Learning

- Concept: Universal Approximation Theorem
  - Why needed here: The paper's theoretical results build upon the universal approximation theorem, which states that neural networks can approximate any continuous function on compact sets.
  - Quick check question: Can you explain the universal approximation theorem in your own words and its relevance to this paper?

- Concept: Parameter-bounding Activations
  - Why needed here: The paper introduces the concept of parameter-bounding activations, which are crucial for the theoretical results on bias learning.
  - Quick check question: What is a parameter-bounding activation, and why is it important for bias learning?

- Concept: Dynamical Systems and RNNs
  - Why needed here: The paper extends the bias learning results to recurrent neural networks and their ability to approximate dynamical systems.
  - Quick check question: How do RNNs approximate dynamical systems, and what role does bias learning play in this process?

## Architecture Onboarding

- Component map: Input layer -> Hidden layer(s) with fixed random weights and learnable biases -> Output layer
- Critical path:
  1. Initialize network with random weights
  2. Freeze weights
  3. Train biases to match target function or dynamics
  4. Evaluate performance
- Design tradeoffs:
  - Wider hidden layers allow for better approximation but increase computational cost
  - Choice of activation function affects the network's ability to learn through bias adjustment
  - Initial weight distribution can impact the effectiveness of bias learning in RNNs
- Failure signatures:
  - Poor performance on training data
  - Lack of task-specific functional organization in multi-task learning
  - Inability to generate desired dynamical patterns in RNNs
- First 3 experiments:
  1. Train a single-hidden-layer network on a simple function (e.g., sine wave) using only bias learning and evaluate the approximation error.
  2. Train a network on a multi-task learning problem (e.g., image classification on multiple datasets) using only bias learning and analyze the emergence of task-specific functional clusters.
  3. Train an RNN on a simple dynamical system (e.g., oscillator) using only bias learning and examine the eigenvalue spectrum of the Jacobian matrix.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal edge length γ for parameter-bounding activations in universal approximation?
- Basis in paper: [explicit] The paper discusses parameter-bounding activations and mentions that γ should be strictly positive, but doesn't specify an optimal value.
- Why unresolved: The paper notes that γ affects hidden layer scaling and suggests future work to explore the relationship between γ and network performance.
- What evidence would resolve it: Empirical studies comparing network performance across different γ values while controlling for other parameters.

### Open Question 2
- Question: Can the convergence results for dynamical systems be extended to Lp convergence over trajectories?
- Basis in paper: [explicit] The paper states that current convergence results are point-wise and finite-time, suggesting Lp convergence as a future direction.
- Why unresolved: The proof technique for point-wise convergence may not directly extend to Lp convergence, requiring new mathematical approaches.
- What evidence would resolve it: A formal proof demonstrating Lp convergence or a counterexample showing its impossibility.

### Open Question 3
- Question: How does the initial distribution of recurrent weights affect the expressivity of bias-learning RNNs?
- Basis in paper: [explicit] The paper shows that bias learning fails for recurrent weights with gain g < 1, while fully-trained networks are not sensitive to initial weight gain.
- Why unresolved: The paper only explores a limited range of weight distributions and gains, leaving the full relationship unclear.
- What evidence would resolve it: Systematic experiments varying weight distributions and gains, or theoretical analysis of how weight statistics interact with bias learning.

## Limitations
- The theoretical results depend critically on parameter-bounding activation functions, limiting general applicability
- RNN experiments show sensitivity to initial weight distribution, with failure to learn dynamics when gain parameters fall below threshold values
- Performance on KMNIST significantly lags behind other tasks, suggesting potential limitations for more challenging problems

## Confidence
- High confidence in the theoretical framework for feedforward networks and the core mechanism of bias-based approximation
- Medium confidence in the RNN results due to sensitivity to initialization parameters and limited exploration of different dynamical system complexities
- Medium confidence in multi-task learning findings given the strong dependence on task similarity and hidden layer width

## Next Checks
1. Test the universality claims with activation functions beyond ReLU and Heaviside to determine the breadth of parameter-bounding applicability
2. Systematically vary initial weight distributions in RNNs across multiple dynamical systems to map the failure boundary for bias learning
3. Evaluate bias-only learning on more challenging computer vision tasks (e.g., CIFAR-10) to assess scalability limitations and identify failure modes