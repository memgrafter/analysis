---
ver: rpa2
title: 'OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model'
arxiv_id: '2411.07238'
source_url: https://arxiv.org/abs/2411.07238
tags:
- thai
- language
- o-net
- openthaigpt
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpenThaiGPT 1.5 is a Thai language large language model built on
  Qwen v2.5, finetuned on over 2 million Thai instruction pairs to improve performance
  on Thai-specific tasks. The model uses LoRA-based finetuning on 8x H100 GPUs, incorporating
  curated Thai datasets, synthetic data generation, and safety alignment via DPO.
---

# OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model

## Quick Facts
- arXiv ID: 2411.07238
- Source URL: https://arxiv.org/abs/2411.07238
- Reference count: 16
- Primary result: State-of-the-art Thai language model achieving 76.73% (72B) on custom Thai benchmarks

## Executive Summary
OpenThaiGPT 1.5 is an advanced Thai language model built on Qwen v2.5, finetuned on over 2 million Thai instruction pairs. The model achieves state-of-the-art performance among open-source Thai models on custom benchmarks while maintaining multi-turn conversation capability and bilingual (Thai-English) functionality. The development employed LoRA-based finetuning on 8x H100 GPUs, incorporating curated Thai datasets, synthetic data generation, and safety alignment through DPO.

## Method Summary
The model was developed through LoRA-based finetuning of Qwen v2.5 (7B, 14B, 72B parameters) using over 2 million Thai instruction pairs. Training employed the NeMo framework with 8x H100 GPUs, learning rate 1e-4, and global batch size 32. The dataset included Thai Wiki summaries, QA pairs, proprietary datasets, synthetic data from Llama 3.1 405B, English data for bilingual capability, and safety alignment data. Safety alignment was performed using DPO with approximately 5,000 records.

## Key Results
- Achieved 65.78% (7B), 71.51% (14B), and 76.73% (72B) on OpenThaiGPT Evaluation Dataset
- Outperformed other open-source Thai models on Thai Exam Benchmark and M3Exam
- Preserved multi-turn conversation capability with ~30% of finetuning data being conversational

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finetuning on Thai-specific instruction pairs improves model performance on Thai language tasks.
- Mechanism: The model leverages a large dataset of over 2 million Thai instruction pairs to adapt the base Qwen v2.5 model to the nuances of Thai language and culture.
- Core assumption: The quality and diversity of the Thai instruction pairs are sufficient to capture the linguistic and cultural nuances required for effective Thai language modeling.
- Evidence anchors:
  - [abstract]: "OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5, finetuned on over 2,000,000 Thai instruction pairs."
  - [section]: "The model underwent extensive finetuning on a diverse dataset of over 2,000,000 Thai instruction pairs."
  - [corpus]: Weak - The corpus contains related Thai language models but lacks specific evidence on the effectiveness of finetuning on Thai instruction pairs.
- Break condition: If the Thai instruction pairs are not diverse or high-quality enough, the model may not effectively capture the nuances of the Thai language and culture.

### Mechanism 2
- Claim: Multi-turn conversation support enhances the model's ability to generate coherent responses across multiple interactions.
- Mechanism: The finetuning dataset includes approximately 30% multi-turn conversations, enabling the model to maintain context and generate more natural dialogues.
- Core assumption: The inclusion of multi-turn conversations in the finetuning dataset is sufficient to preserve and enhance the model's multi-turn conversation capability.
- Evidence anchors:
  - [section]: "OpenThaiGPT 1.5 preserved multi-turn conversation capability by having around 30% of the instruction finetuning dataset consist of multi-turn conversations."
  - [corpus]: Weak - The corpus contains related Thai language models but lacks specific evidence on the effectiveness of multi-turn conversation support.
- Break condition: If the multi-turn conversations in the finetuning dataset are not representative or diverse enough, the model may struggle to maintain context and generate coherent responses in real-world multi-turn interactions.

### Mechanism 3
- Claim: The use of LoRA-based finetuning allows for efficient adaptation of the base model to Thai language tasks.
- Mechanism: The model uses LoRA (Low-Rank Adaptation) technique to finetune the base Qwen v2.5 model, which reduces the number of trainable parameters and computational resources required.
- Core assumption: LoRA is an effective and efficient method for adapting large language models to specific tasks or languages.
- Evidence anchors:
  - [section]: "The 7B and 72B proposed models were trained using the LoRa technique."
  - [corpus]: Weak - The corpus contains related Thai language models but lacks specific evidence on the effectiveness of LoRA-based finetuning.
- Break condition: If LoRA is not suitable for the specific task or language, or if the rank of the LoRA adapters is not optimized, the finetuning process may not effectively adapt the base model to Thai language tasks.

## Foundational Learning

- Concept: Transfer learning
  - Why needed here: Understanding how the base Qwen v2.5 model can be adapted to Thai language tasks through finetuning.
  - Quick check question: What are the advantages and limitations of using transfer learning for adapting large language models to specific languages or tasks?

- Concept: Multi-turn conversation modeling
  - Why needed here: Understanding how the model maintains context and generates coherent responses across multiple interactions.
  - Quick check question: What are the key challenges in modeling multi-turn conversations, and how can they be addressed in large language models?

- Concept: Retrieval Augmented Generation (RAG)
  - Why needed here: Understanding how the model incorporates external knowledge sources during response generation.
  - Quick check question: How does RAG enhance the model's ability to provide accurate and up-to-date information, and what are the potential limitations of this approach?

## Architecture Onboarding

- Component map: Qwen v2.5 (7B/14B/72B) -> LoRA-based finetuning on Thai instruction pairs -> NeMo framework training -> Safety alignment via DPO -> Multi-turn conversation support -> RAG compatibility

- Critical path:
  1. Prepare the finetuning dataset by curating and processing Thai instruction pairs.
  2. Set up the NeMo framework and configure the LoRA-based finetuning process.
  3. Train the model on the finetuning dataset using 8x H100 GPUs.
  4. Evaluate the model's performance on the custom benchmarks.
  5. Deploy the model for inference and fine-tune as needed.

- Design tradeoffs:
  - Model size: The 7B model is more computationally efficient but may have lower performance compared to the 72B model.
  - Dataset size and diversity: A larger and more diverse finetuning dataset can improve the model's performance but may require more computational resources.
  - Finetuning technique: LoRA allows for efficient adaptation but may not be as effective as full finetuning in some cases.

- Failure signatures:
  - Poor performance on Thai language tasks: Indicates issues with the finetuning dataset or the LoRA-based finetuning process.
  - Inability to maintain context in multi-turn conversations: Suggests problems with the multi-turn conversation support in the finetuning dataset.
  - Slow inference or high memory usage: May indicate issues with the model architecture or deployment configuration.

- First 3 experiments:
  1. Evaluate the model's performance on a subset of the Thai instruction pairs to assess the effectiveness of the finetuning process.
  2. Test the model's ability to maintain context and generate coherent responses in multi-turn conversations.
  3. Measure the model's inference speed and memory usage to optimize the deployment configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OpenThaiGPT 1.5 compare to GPT-4o on Thai-specific benchmarks when controlling for model size?
- Basis in paper: [inferred] The paper compares OpenThaiGPT 1.5 (72B) against GPT-4o and other models on Thai Exam Benchmark and M3Exam, but does not provide direct size-controlled comparisons.
- Why unresolved: The paper shows absolute performance numbers but doesn't provide normalized comparisons that account for parameter differences between models.
- What evidence would resolve it: A comprehensive benchmark table showing performance per parameter count or relative improvement over base model performance across different sizes.

### Open Question 2
- Question: What is the impact of the synthetic data generation process on the model's ability to handle novel or edge-case Thai language queries?
- Basis in paper: [explicit] The paper describes using Llama 3.1 405B to generate synthetic data and LLM-as-a-judge for quality control, but doesn't evaluate its effectiveness.
- Why unresolved: While the paper describes the synthetic data methodology, it doesn't provide specific evaluation of how well this data improves handling of rare or complex Thai language scenarios.
- What evidence would resolve it: Ablation studies comparing model performance with and without synthetic data on specialized Thai language tasks or edge-case queries.

### Open Question 3
- Question: How does the model's performance degrade when processing mixed Thai-English code-switching text compared to monolingual Thai text?
- Basis in paper: [explicit] The paper mentions mixing 20% English data to prevent catastrophic forgetting, but doesn't evaluate code-switching performance.
- Why unresolved: The paper claims English-Thai bilingual capability but doesn't provide quantitative assessment of code-switching scenarios, which are common in real-world Thai usage.
- What evidence would resolve it: Benchmark results comparing performance on mixed Thai-English text versus pure Thai text across multiple task types.

## Limitations
- Evaluation relies on custom benchmarks without comparison to established Thai language benchmarks
- Proprietary datasets (>30) are not fully disclosed, limiting reproducibility and bias analysis
- Synthetic data generation process uses Llama 3.1 405B with unspecified prompting strategies

## Confidence
- High confidence: Technical feasibility of LoRA-based finetuning approach and basic functionality
- Medium confidence: State-of-the-art performance claims due to reliance on custom benchmarks without external validation
- Low confidence: Precise composition and quality of training data, particularly proprietary datasets and synthetic data generation

## Next Checks
1. Evaluate OpenThaiGPT 1.5 on established Thai language benchmarks like ThaiGLUE or ThaiMBench (when available) to verify claimed state-of-the-art performance
2. Conduct detailed analysis of training data distribution, focusing on proprietary versus public datasets and potential biases
3. Analyze synthetic data quality generated by Llama 3.1 405B for linguistic quality and alignment with authentic Thai language patterns