---
ver: rpa2
title: Test-time Adversarial Defense with Opposite Adversarial Path and High Attack
  Time Cost
arxiv_id: '2410.16805'
source_url: https://arxiv.org/abs/2410.16805
tags:
- adversarial
- diffusion
- defense
- attack
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a test-time adversarial defense method that
  uses excessive denoising along the opposite adversarial path (OAP). The key idea
  is to push the input image further toward the opposite adversarial direction, improving
  robustness without compromising natural accuracy.
---

# Test-time Adversarial Defense with Opposite Adversarial Path and High Attack Time Cost

## Quick Facts
- **arXiv ID:** 2410.16805
- **Source URL:** https://arxiv.org/abs/2410.16805
- **Authors:** Cheng-Han Yeh; Kuanchun Yu; Chun-Shien Lu
- **Reference count:** 25
- **Primary result:** Proposes a test-time adversarial defense using excessive denoising along the opposite adversarial path (OAP) with diffusion models, achieving high clean and robust accuracy while significantly increasing attacker computational cost.

## Executive Summary
This paper introduces a novel test-time adversarial defense method that combines excessive denoising along the opposite adversarial path with diffusion-based purification. The key innovation is using a purifier trained on data pairs that iteratively apply the opposite of adversarial perturbations, pushing denoised images further from decision boundaries. The method can be plugged into any pre-trained model and introduces a double diffusion path cleaning approach that increases computational cost for attackers, thereby enhancing robustness against adaptive attacks. Experimental results demonstrate superior or comparable performance to state-of-the-art defenses on CIFAR-10, CIFAR-100, and ImageNet datasets.

## Method Summary
The proposed defense integrates excessive denoising along the opposite adversarial path (OAP) with diffusion-based purification. It trains a purifier using data pairs where each pair consists of an adversarial example and its counterpart generated by iteratively applying the opposite of the adversarial perturbation. This pushes the denoised image further along the OAP, improving robustness without sacrificing clean accuracy. The method also introduces a double diffusion path cleaning approach where two diffusion paths process the input - one with the adversarial image and another with a color-transferred version using excessively denoised clean images. Attackers must defeat both paths, significantly increasing their computational cost and time.

## Key Results
- Outperforms or matches state-of-the-art defenses on CIFAR-10, CIFAR-100, and ImageNet datasets
- Achieves high clean accuracy while maintaining strong robust accuracy against various attack methods
- Significantly increases computational cost for attackers, particularly for adaptive attacks
- The double diffusion path approach effectively doubles the attack time cost by requiring attackers to defeat two separate purification paths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Excessive denoising along the opposite adversarial path pushes the input image further toward the opposite adversarial direction, improving robustness without compromising natural accuracy.
- Mechanism: The purifier moves data along OAP by training on pairs {(xadv, xK)} where xK is generated by iteratively applying the opposite of the adversarial perturbation. This pushes denoised images further from decision boundaries.
- Core assumption: Moving input images further along the opposite adversarial direction places them in regions with lower loss, improving classification accuracy.
- Evidence anchors:
  - [abstract] "Different from prior arts, the key idea is excessive denoising or purification by integrating the opposite adversarial direction with reverse diffusion to push the input image further toward the opposite adversarial direction."
  - [section] "Starting from the clean image x, ground-truth label y, parameter ϕ and loss function L of classifier f, we can generate a new reference point xK for training by: xk = Πxk−1+S(xk−1 − α sign(∇xk−1 L(xk−1, y, ϕ))), (1) for 1 ≤ k ≤ K, where x0 = x."
- Break condition: Excessive denoising may move images too far from original, losing perceptual similarity and causing clean image misclassification.

### Mechanism 2
- Claim: Integrating OAP with reverse diffusion provides stronger adversarial defense by combining directions from score-based diffusion model and baseline purifier.
- Mechanism: The method modifies reverse diffusion by adding a term that moves images toward the opposite adversarial direction (xK) in each step, incorporating the gradient of log probability of xK given the current noisy image.
- Core assumption: Combining diffusion model's denoising direction with OAP direction results in more effective purification than either method alone.
- Evidence anchors:
  - [abstract] "Different from prior arts, the key idea is excessive denoising or purification by integrating the opposite adversarial direction with reverse diffusion to push the input image further toward the opposite adversarial direction."
  - [section] "Different from previous works, if y in the third term of Eq. (5) is replaced with the new reference point xK, as described in Eq. (1) of Sec. 3.1, then the term becomes ∇xt−1 log pϕ(xK|xt−1) and represents how to move along the direction to xK given xt−1."
- Break condition: Combination of directions may cause instability in diffusion process or OAP direction may not align well with true adversarial direction.

### Mechanism 3
- Claim: Double diffusion path cleaning approach increases computational cost for attackers, enhancing robustness against adaptive attacks.
- Mechanism: Uses two diffusion paths - one with adversarial image and another with color-transferred version using excessively denoised target images from clean training data. Attackers must defeat both paths.
- Core assumption: Maintaining two relatively clean diffusion paths results in more purified output, and forcing attackers to defeat both paths significantly increases computational cost.
- Evidence anchors:
  - [abstract] "It also introduces a double diffusion path cleaning approach to increase the computational cost for attackers, thereby enhancing robustness against adaptive attacks."
  - [section] "Conceptually, the idea of generating the input to path p2 that guides path p1 is to transfer pixel values from the source image (adversarial image) to the other target image (clean/purified image)... Based on the above test and observations, we now describe the proposed method for cleaning the diffusion path with adversarial images as input."
- Break condition: Color transfer may be ineffective or attacker may find way to bypass one path, reducing increased computational cost benefits.

## Foundational Learning

- Concept: Diffusion Models
  - Why needed here: The proposed method is based on diffusion models, which are used for denoising and generating data by learning the reverse process of adding Gaussian noise. Understanding diffusion models is crucial for understanding how the OAP prior is integrated into the reverse diffusion process.
  - Quick check question: How does a diffusion model generate data, and what is the role of the forward and reverse processes in this generation?

- Concept: Adversarial Attacks and Defenses
  - Why needed here: The paper deals with adversarial attacks and defenses, specifically test-time adversarial defenses. Understanding different types of adversarial attacks (white-box, black-box) and defense mechanisms (training-time, test-time) is essential for grasping the motivation and approach of the proposed method.
  - Quick check question: What is the difference between training-time and test-time adversarial defenses, and why might a test-time defense be preferred in certain scenarios?

- Concept: AutoAttack and its variants
  - Why needed here: The paper evaluates the proposed method against AutoAttack and its variants, including the pitfalls of using AutoAttack (Rand) for diffusion-based defense methods. Understanding AutoAttack and its components (APGD-CE, APGD-DLR, FAB, Square Attack) is important for interpreting the experimental results.
  - Quick check question: What are the different components of AutoAttack, and how does the "Rand" variant differ from the "Standard" variant?

## Architecture Onboarding

- Component map: Pre-trained classifier -> Purifier (baseline or with OAP) -> Diffusion model -> Color transfer module (for double path) -> AutoAttack evaluator

- Critical path:
  1. Input image is processed by the purifier (either baseline purifier alone or in combination with diffusion model)
  2. Purified image is fed into pre-trained classifier for prediction
  3. For double diffusion path cleaning, input image also processed through second diffusion path with color-transferred version
  4. Outputs from both paths are combined to produce final purified image

- Design tradeoffs:
  - Increased robustness vs. computational cost: Double diffusion path cleaning increases computational cost for both defense and attacker, potentially limiting practical applications
  - Clean accuracy vs. robust accuracy: Method aims to improve robust accuracy without compromising clean accuracy, but tradeoff may exist
  - Complexity of implementation: Method involves integrating multiple components (purifier, diffusion model, color transfer), increasing implementation complexity

- Failure signatures:
  - Decreased clean accuracy: Excessive denoising may move images too far from original, causing clean image misclassification
  - Instability in diffusion process: Combination of directions from diffusion model and OAP may cause instability, failing to improve robustness
  - Ineffective color transfer: If color transfer in double diffusion path is not effective, increased computational cost may not translate to improved robustness

- First 3 experiments:
  1. Implement baseline purifier (gθ) and test performance on CIFAR-10 with pre-trained classifier, comparing to no-defense baseline
  2. Integrate OAP prior with pre-trained diffusion model and test performance on CIFAR-10, comparing to baseline purifier alone
  3. Implement double diffusion path cleaning approach and test performance on CIFAR-10, comparing to previous methods in terms of both robustness and computational cost

## Open Questions the Paper Calls Out
None

## Limitations
- **Adaptive Attack Evaluation**: Limited evaluation of true adaptive attacks where attackers optimize against the specific double diffusion path mechanism
- **Computational Overhead**: Significant increase in computational cost for both defense and attacker, with unclear trade-off analysis for practical applications
- **Generalizability**: Unclear how well method scales to larger, more complex models used in real-world applications due to computational constraints

## Confidence

- **High Confidence**: Core mechanism of excessive denoising along opposite adversarial path is well-supported by theoretical framework and experimental results
- **Medium Confidence**: Claim of increased attack time cost based on evaluation against AutoAttack and variants, but true effectiveness against fully adaptive attackers is uncertain
- **Low Confidence**: Generalizability to larger models and trade-off between robustness and computational overhead are not fully explored

## Next Checks

1. **Adaptive Attack Evaluation**: Conduct thorough evaluation against adaptive attackers specifically designed to optimize against the double diffusion path cleaning approach to assess true robustness in realistic scenarios.

2. **Computational Overhead Analysis**: Perform detailed analysis of computational overhead introduced by double diffusion path approach, comparing trade-off between increased robustness and computational cost across different model sizes and datasets.

3. **Scalability to Larger Models**: Evaluate method on larger, more complex models (e.g., real-world applications) to assess scalability and generalizability, investigating potential modifications to reduce computational overhead for larger models.