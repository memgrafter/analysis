---
ver: rpa2
title: Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning
arxiv_id: '2410.16105'
source_url: https://arxiv.org/abs/2410.16105
tags:
- mgdl
- sgdl
- grade
- function
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to address the spectral bias
  of deep neural networks (DNNs) by decomposing high-frequency functions into a sum-composition
  of low-frequency components, each represented by a shallow neural network (SNN).
  The method leverages the Multi-Grade Deep Learning (MGDL) model, which trains SNNs
  incrementally, with each grade learning from the residue of the previous grade.
---

# Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning

## Quick Facts
- arXiv ID: 2410.16105
- Source URL: https://arxiv.org/abs/2410.16105
- Authors: Ronglong Fang; Yuesheng Xu
- Reference count: 40
- Primary result: MGDL effectively learns high-frequency components, reducing TeRSE by factors of 592-7,058 and achieving 2.35-3.93 dB higher PSNR than SGDL

## Executive Summary
This paper proposes Multi-Grade Deep Learning (MGDL) to address spectral bias in deep neural networks by decomposing high-frequency functions into sum-compositions of low-frequency components, each represented by a shallow neural network (SNN). The method trains SNNs incrementally, with each grade learning from the residue of the previous grade. Four experiments on synthetic, manifold, colored images, and MNIST datasets demonstrate that MGDL significantly improves approximation accuracy and image reconstruction quality compared to traditional single-grade deep learning models.

## Method Summary
MGDL addresses spectral bias by decomposing high-frequency functions into sum-compositions of low-frequency components, each represented by an SNN. The model trains SNNs incrementally, grade by grade, with each grade learning from the residual error of the previous grade. This approach allows MGDL to progressively capture high-frequency components while avoiding the spectral bias that affects traditional deep neural networks. The method is validated through four experiments: synthetic 1D functions, 2D manifold data, colored image regression, and MNIST classification with radial frequency noise.

## Key Results
- MGDL reduces test relative squared error (TeRSE) by factors of 592 to 7,058 compared to single-grade deep learning (SGDL) on synthetic data
- MGDL achieves 2.35 to 3.93 dB higher peak signal-to-noise ratio (PSNR) values in image reconstruction tasks
- The method demonstrates consistent performance across synthetic, manifold, colored images, and MNIST datasets with high-frequency features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-frequency functions can be decomposed into a sum-composition of low-frequency components, each represented by an SNN
- Mechanism: By splitting a complex high-frequency function into simpler low-frequency sub-functions, each SNN in MGDL learns only low-frequency information. The composition of these low-frequency outputs progressively builds up the high-frequency components
- Core assumption: A high-frequency function can be accurately represented as a sum-composition of low-frequency functions (supported by the Jacobiâ€“Anger identity)
- Evidence anchors:
  - [abstract] "composition of low frequency functions can effectively approximate a high-frequency function"
  - [section] "composition of two low frequency functions may lead to a high-frequency function"
  - [corpus] No direct evidence for this specific mechanism, but related papers on spectral bias and frequency decomposition support the general idea
- Break Condition: If the decomposition into low-frequency components fails to capture essential high-frequency features, or if the composition does not preserve accuracy

### Mechanism 2
- Claim: MGDL mitigates spectral bias by training incrementally, with each grade learning from the residue of the previous grade
- Mechanism: Each grade in MGDL focuses on learning the remaining high-frequency components not captured by previous grades. By freezing the parameters of previous SNNs and using them as features, the model avoids re-learning low-frequency information
- Core assumption: Residual learning allows the network to progressively refine the approximation, focusing on increasingly higher frequencies
- Evidence anchors:
  - [abstract] "a current grade learning from the residue of the previous grade"
  - [section] "residual error sequence (see, Theorem 1 in Appendix A)"
  - [corpus] No direct evidence for this specific incremental training mechanism, but residual learning is a known technique in deep learning
- Break Condition: If the residual learning process fails to converge, or if the accumulation of errors across grades degrades performance

### Mechanism 3
- Claim: MGDL is more scalable and easier to implement than traditional DNNs for high-dimensional data with high-frequency features
- Mechanism: MGDL avoids training a deep DNN end-to-end, instead training several SNNs sequentially. This reduces the computational complexity and makes the training process more manageable
- Core assumption: Training SNNs is easier than training deep DNNs, and the sequential training of SNNs is more efficient
- Evidence anchors:
  - [abstract] "easy to implement and not limited by the input dimension"
  - [section] "training an SNN is notably easier than training a DNN"
  - [corpus] No direct evidence for this specific scalability claim, but SNNs are generally simpler to train than deep DNNs
- Break Condition: If the sequential training of SNNs becomes inefficient for very high-dimensional data, or if the memory requirements for storing intermediate features become prohibitive

## Foundational Learning

- Concept: Spectral bias in DNNs
  - Why needed here: Understanding spectral bias is crucial for recognizing the problem that MGDL addresses
  - Quick check question: What is spectral bias, and how does it affect the learning of high-frequency functions in DNNs?

- Concept: Function decomposition
  - Why needed here: Decomposing a high-frequency function into low-frequency components is the core idea behind MGDL
  - Quick check question: How can a high-frequency function be decomposed into a sum-composition of low-frequency functions?

- Concept: Residual learning
  - Why needed here: Residual learning is the mechanism by which MGDL progressively refines the approximation
  - Quick check question: How does residual learning work, and why is it effective for mitigating spectral bias?

## Architecture Onboarding

- Component map: Input -> Grade 1 (SNN) -> Grade 2 (SNN) -> ... -> Grade n (SNN) -> Output (sum of all grades)
- Critical path:
  1. Initialize first grade with input data
  2. Train SNN to minimize loss on input data
  3. Compute residual (difference between target and output of current grade)
  4. Use residual as input to next grade
  5. Repeat steps 2-4 for all grades
  6. Sum outputs of all grades for final prediction
- Design tradeoffs:
  - More grades allow for finer approximation of high-frequency components but increase training time
  - Deeper SNNs within each grade can capture more complex low-frequency functions but may reintroduce spectral bias
  - Choice of activation function and optimization algorithm can affect the convergence and accuracy of each grade
- Failure signatures:
  - Slow decrease in loss during training of early grades
  - Overfitting in later grades (validation loss increases while training loss decreases)
  - Poor generalization to test data (high TeRSE)
- First 3 experiments:
  1. Synthetic data with known high-frequency components: Compare TeRSE of MGDL and SGDL
  2. Manifold data: Evaluate accuracy of MGDL and SGDL in learning functions on lower-dimensional manifolds
  3. Colored images: Compare PSNR of reconstructed images using MGDL and SGDL

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical foundation that guarantees MGDL can effectively decompose high-frequency functions into low-frequency components?
- Basis in paper: [inferred] The paper demonstrates empirical success of MGDL in learning high-frequency features but lacks mathematical proof of why this decomposition works
- Why unresolved: The paper focuses on experimental validation rather than theoretical analysis of the spectral bias mitigation mechanism
- What evidence would resolve it: A mathematical proof showing that MGDL's grade-by-grade learning approach guarantees convergence to the optimal decomposition of high-frequency functions into low-frequency components

### Open Question 2
- Question: How does MGDL perform on real-world datasets with complex, non-periodic high-frequency patterns compared to synthetic datasets?
- Basis in paper: [explicit] The paper tests MGDL on synthetic, manifold, colored images, and MNIST data, but doesn't explore more complex real-world scenarios
- Why unresolved: The experiments are limited to controlled datasets, and the performance on more challenging real-world data remains unknown
- What evidence would resolve it: Testing MGDL on diverse real-world datasets like medical imaging, satellite imagery, or audio signals with complex high-frequency patterns

### Open Question 3
- Question: What is the optimal number of grades in MGDL for different types of high-frequency functions, and how does it scale with input dimensionality?
- Basis in paper: [explicit] The paper uses a fixed number of grades for each experiment but doesn't explore how to determine the optimal number of grades
- Why unresolved: The paper doesn't provide a systematic method for determining the number of grades needed for different functions or input dimensions
- What evidence would resolve it: A study analyzing the relationship between the number of grades, function complexity, and input dimensionality to establish guidelines for optimal grade selection

### Open Question 4
- Question: How does MGDL compare to other spectral bias mitigation techniques like Fourier feature mapping or adaptive activation functions in terms of accuracy and computational efficiency?
- Basis in paper: [explicit] The paper mentions other methods like Fourier features and adaptive activation functions but doesn't compare MGDL's performance against them
- Why unresolved: The paper focuses on demonstrating MGDL's effectiveness without benchmarking it against alternative approaches
- What evidence would resolve it: A comprehensive comparison of MGDL with other spectral bias mitigation techniques on the same datasets, measuring both accuracy and computational efficiency

## Limitations

- Limited empirical evidence on high-dimensional data beyond tested datasets, with scalability claims remaining largely theoretical
- Exact network architectures for SGDL baselines and specific initialization strategies beyond basic learning rate decay schedule are unclear
- Performance on real-world datasets with complex, non-periodic high-frequency patterns remains unexplored

## Confidence

- Mechanism 1 (Function Decomposition): Medium
- Mechanism 2 (Residual Learning): High
- Mechanism 3 (Scalability): Medium
- Overall Results: Medium

## Next Checks

1. Test MGDL on real-world datasets with varying frequency distributions to validate generalization beyond synthetic and image data
2. Conduct ablation studies to quantify the contribution of each grade to the final performance
3. Compare computational complexity and memory requirements of MGDL versus traditional DNNs for high-dimensional inputs