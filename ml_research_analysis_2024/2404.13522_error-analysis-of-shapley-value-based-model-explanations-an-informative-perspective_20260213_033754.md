---
ver: rpa2
title: 'Error Analysis of Shapley Value-Based Model Explanations: An Informative Perspective'
arxiv_id: '2404.13522'
source_url: https://arxiv.org/abs/2404.13522
tags:
- bias
- feature
- distribution
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified error analysis framework for Shapley
  value-based model explanations, decomposing explanation errors into observation
  bias and structural bias. The authors demonstrate that observation bias arises from
  data sparsity while structural bias stems from distributional assumptions.
---

# Error Analysis of Shapley Value-Based Model Explanations: An Informative Perspective

## Quick Facts
- arXiv ID: 2404.13522
- Source URL: https://arxiv.org/abs/2404.13522
- Authors: Ningsheng Zhao; Jia Yuan Yu; Krzysztof Dzieciolowski; Trang Bui
- Reference count: 35
- One-line primary result: Introduces unified error analysis framework decomposing Shapley value attribution errors into observation bias and structural bias, demonstrating trade-offs between over-informative and under-informative explanations

## Executive Summary
This paper presents a novel error theoretical analysis framework for Shapley value-based model explanations (SVAs) that decomposes explanation errors into observation bias and structural bias. The framework provides insights into why different SVA methods exhibit varying levels of informativeness, with some methods producing over-informative attributions due to data sparsity while others produce under-informative attributions due to unrealistic distributional assumptions. The authors validate their framework through experiments on Bike Sharing and Census Income datasets, demonstrating that commonly used methods like TreeSHAP and baseline assumption methods tend to be under-informative by ignoring feature dependencies.

## Method Summary
The paper develops a unified error analysis framework that decomposes SVA errors into two orthogonal components: observation bias arising from finite sample estimation and structural bias from distributional assumptions. The framework introduces concepts of over-informative explanations (high observation bias) and under-informative explanations (high structural bias). To quantify structural bias, the authors propose an OOD detection-based measurement tool that compares distribution drift between real and hybrid samples generated under different distributional assumptions. The framework is applied to analyze existing SVA methods including data smoothing approaches (kernel-based, model-based, TreeSHAP) and distributional assumption methods (baseline, marginal, product of marginals, uniform).

## Key Results
- TreeSHAP and other assumption-based methods produce under-informative attributions by ignoring feature dependencies, as evidenced by high OOD rates (0.99 for baseline, 0.88 for product of marginals)
- Data smoothing methods like kernel-based approaches can be over-informative under data sparsity conditions
- The observation bias-structural bias trade-off explains why no single SVA method is optimal across all scenarios
- Under-informativeness audit reveals that correlated features (Temperature vs Feeling_Temperature, Hours_per_week vs Minutes_per_week) receive disproportionately different SHAP scores under distributional assumption methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The decomposition of explanation error into observation bias and structural bias provides a complete framework for understanding SVA errors.
- Mechanism: The framework separates errors arising from finite sample estimation (observation bias) from errors due to distributional assumptions (structural bias), allowing systematic analysis of both sources.
- Core assumption: All SVA estimation errors can be captured by these two orthogonal components.
- Evidence anchors:
  - [abstract] "the explanation errors of SVAs are decomposed into two components: observation bias and structural bias"
  - [section] "we propose a novel error theoretical analysis framework, in which the explanation errors of SVAs are decomposed into two components: observation bias and structural bias"
  - [corpus] Weak evidence - corpus neighbors discuss Shapley values but not this specific decomposition framework
- Break condition: If estimation errors arise from sources not captured by finite sample effects or distributional assumptions (e.g., algorithmic approximation errors in the Shapley computation itself).

### Mechanism 2
- Claim: The observation bias-structural bias trade-off explains why no single SVA method can be optimal for all scenarios.
- Mechanism: Methods that reduce observation bias (like data smoothing) increase structural bias, while methods that reduce structural bias (like distributional assumptions) increase observation bias, creating an inherent trade-off.
- Core assumption: There exists a fundamental trade-off between reducing data sparsity effects and maintaining realistic distributional assumptions.
- Evidence anchors:
  - [abstract] "we further clarify the underlying causes of these two biases and demonstrate that there is a trade-off between them"
  - [section] "there is typically a trade-off between observation bias and structural bias in estimating the conditional RF using a finite explaining set"
  - [corpus] Weak evidence - corpus neighbors don't discuss this specific trade-off concept
- Break condition: If new methods can simultaneously reduce both biases without trade-offs, or if the relationship is not monotonic.

### Mechanism 3
- Claim: OOD detection provides a practical measurement tool for quantifying distribution drift and structural bias.
- Mechanism: By training a classifier to distinguish between real samples and hybrid samples generated from assumed distributions, the framework can measure how much distributional assumptions cause distribution drift.
- Core assumption: OOD detection scores can reliably quantify the extent of distribution drift between true and assumed distributions.
- Evidence anchors:
  - [abstract] "We propose a measurement tool to quantify such a distribution drift"
  - [section] "we propose comparing the distribution drift by examining the distributions of OOD scores C calculated on Dp(X) and Dq(X)"
  - [corpus] Weak evidence - corpus neighbors discuss Shapley values but not OOD detection for measuring distribution drift
- Break condition: If OOD detection fails to capture the relevant aspects of distribution drift, or if the relationship between OOD scores and structural bias is not consistent.

## Foundational Learning

- Concept: Shapley value attribution and cooperative game theory
  - Why needed here: The entire framework builds on Shapley values as a method for feature attribution in ML models
  - Quick check question: What is the fundamental property that makes Shapley values unique among attribution methods?

- Concept: Conditional expectation and removal-based explanations
  - Why needed here: The framework focuses on informative SVAs that capture informational dependencies through conditional expectations
  - Quick check question: How does the conditional expectation fS(xS) = E[f(X)|XS = xS] differ from unconditional expectations in feature attribution?

- Concept: Distribution drift and OOD detection
  - Why needed here: The framework uses distribution drift as a key concept for measuring structural bias in assumption-based methods
  - Quick check question: What distinguishes an in-distribution sample from an out-of-distribution sample in the context of distribution drift?

## Architecture Onboarding

- Component map:
  - Error decomposition module (observation bias + structural bias)
  - Data smoothing methods (kernel-based, model-based, TreeSHAP)
  - Distributional assumption methods (baseline, marginal, product of marginals, uniform)
  - OOD detection system (classifier training and evaluation)
  - Experimental validation pipeline (dataset loading, model training, SVA computation)

- Critical path: OOD detection → Distribution drift measurement → Under-informativeness audit → Over-informativeness audit

- Design tradeoffs: Trade-off between reducing observation bias (data smoothing) and structural bias (distributional assumptions) means no single method is optimal; choice depends on data characteristics and application requirements

- Failure signatures: High OOD rates indicate severe distribution drift; high observation bias indicates data sparsity issues; high structural bias indicates unrealistic assumptions

- First 3 experiments:
  1. Reproduce the OOD rate calculations on the Bike Sharing dataset using different removal distributions
  2. Implement the under-informativeness audit comparing correlated features' attribution scores
  3. Test observation bias sensitivity by varying explaining set sizes for different SVA methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the observation bias change with different smoothing techniques beyond those studied, such as Gaussian processes or diffusion-based methods?
- Basis in paper: [explicit] The paper analyzes empirical conditional RF, kernel-based approaches, and model-based approaches, but does not explore all possible smoothing techniques.
- Why unresolved: The paper focuses on a limited set of smoothing methods and does not provide a comprehensive comparison across all smoothing techniques.
- What evidence would resolve it: Experimental results comparing observation bias across a wide range of smoothing techniques on various datasets.

### Open Question 2
- Question: Can the error analysis framework be extended to handle time-series or sequential data where feature dependencies evolve over time?
- Basis in paper: [inferred] The current framework assumes static feature dependencies and does not address temporal dynamics in data.
- Why unresolved: The paper does not discuss how the error decomposition would work for time-dependent features or how structural bias might manifest in temporal contexts.
- What evidence would resolve it: A theoretical extension of the framework to temporal settings with experimental validation on time-series datasets.

### Open Question 3
- Question: What is the optimal trade-off between observation bias and structural bias for different types of machine learning models (e.g., neural networks vs. decision trees)?
- Basis in paper: [explicit] The paper demonstrates a trade-off between observation bias and structural bias but does not provide model-specific optimal points.
- Why unresolved: The paper shows the existence of a trade-off but does not investigate how this trade-off varies across different model architectures or data distributions.
- What evidence would resolve it: Empirical studies measuring both biases across multiple model types and identifying conditions under which one bias dominates over the other.

### Open Question 4
- Question: How does the distribution drift measurement tool perform when the true data distribution is multimodal or contains complex dependencies?
- Basis in paper: [inferred] The paper validates the OOD detection method on two datasets but does not test its robustness to complex, multimodal distributions.
- Why unresolved: The current experiments use relatively simple datasets, and the paper does not address scenarios with complex feature interactions or multimodal distributions.
- What evidence would resolve it: Testing the OOD measurement tool on synthetic datasets with known complex distributions and comparing results to ground truth distribution drift.

## Limitations

- The error decomposition framework assumes all SVA estimation errors can be captured by observation and structural bias components, potentially missing approximation errors in Shapley value computation
- The OOD detection approach relies on classifier quality and may not capture all relevant aspects of distribution differences
- Experimental validation is limited to two datasets, which may not generalize across different data distributions or model types

## Confidence

- High confidence: The theoretical decomposition of SVA errors into observation and structural biases is well-grounded and provides a useful analytical framework
- Medium confidence: The empirical demonstration that distributional assumption methods tend to be under-informative due to distribution drift is convincing but could benefit from additional datasets
- Medium confidence: The observation bias-structural bias trade-off is theoretically sound but requires more extensive empirical validation across diverse scenarios

## Next Checks

1. Test the OOD detection methodology on additional datasets with known distribution drift characteristics to validate the measurement tool's sensitivity and specificity
2. Investigate whether the observation bias-structural bias trade-off holds for different model types (neural networks, random forests) beyond xgBoost
3. Develop and evaluate hybrid methods that explicitly balance observation and structural biases to determine if intermediate approaches can outperform pure strategies