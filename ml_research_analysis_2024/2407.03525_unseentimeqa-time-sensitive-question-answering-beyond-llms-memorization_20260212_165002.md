---
ver: rpa2
title: 'UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs'' Memorization'
arxiv_id: '2407.03525'
source_url: https://arxiv.org/abs/2407.03525
tags:
- location
- package
- truck
- airplane
- minutes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UnSeenTimeQA, a data contamination-free time-sensitive
  question-answering benchmark designed to evaluate LLMs' temporal reasoning abilities
  without relying on real-world facts. The benchmark uses synthetically generated
  event scenarios and includes parallel event execution to address limitations in
  existing benchmarks.
---

# UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization
## Quick Facts
- arXiv ID: 2407.03525
- Source URL: https://arxiv.org/abs/2407.03525
- Authors: Md Nayem Uddin; Amir Saeidi; Divij Handa; Agastya Seth; Tran Cao Son; Eduardo Blanco; Steven R. Corman; Chitta Baral
- Reference count: 40
- Primary result: Introduces UnSeenTimeQA benchmark showing LLMs struggle with temporal reasoning beyond memorization

## Executive Summary
UnSeenTimeQA is a novel benchmark designed to evaluate large language models' temporal reasoning capabilities without relying on real-world fact recall. The benchmark uses synthetically generated logistics planning scenarios to avoid data contamination issues common in existing time-sensitive question-answering benchmarks. By including parallel event execution scenarios and varying difficulty levels from easy to hard, the benchmark reveals significant performance gaps between current LLMs and human reasoning abilities, particularly for complex temporal reasoning tasks involving concurrent events.

## Method Summary
The benchmark generates synthetic logistics scenarios where events occur over time, creating questions that test temporal reasoning rather than fact recall. Five LLMs (Gemma-2-9B, Gemma-2-27B, Llama-3.1-8B, Llama-3.1-70B, GPT-4o) are evaluated using zero-shot chain-of-thought prompting on 10,800 questions across four difficulty levels and three question types. The synthetic data generation framework creates contamination-free evaluation by generating logistics planning problems rather than using real-world temporal data. Questions range from easy (with timestamps and durations) to hard (duration-only, requiring reasoning about parallel and serial event execution).

## Key Results
- GPT-4o achieves 49.77% accuracy on hard-serial questions and 42.85% on hard-parallel questions
- Human performance significantly exceeds LLM performance with 93.33% accuracy on easy questions and 84.44% on hard-parallel questions
- LLMs show significant performance drops as difficulty increases, particularly struggling with parallel event reasoning and long-range temporal dependencies
- Error analysis reveals models struggle with missing events in reasoning chains (60% for hard-serial) and parallel event reasoning (70% for hard-parallel)

## Why This Works (Mechanism)
### Mechanism 1
- Claim: UnSeenTimeQA avoids data contamination by using synthetically generated event scenarios rather than real-world facts
- Mechanism: By generating questions from synthetic logistics planning problems instead of real-world temporal data, the benchmark ensures no overlap with LLM pre-training data that includes web-sourced information
- Core assumption: Synthetic facts cannot be "memorized" by LLMs during pre-training because they don't exist in the training corpus
- Evidence anchors: [abstract] "It differs from existing TSQA benchmarks by avoiding web-searchable queries grounded in the real world. We present a series of time-sensitive event scenarios based on synthetically generated facts."

### Mechanism 2
- Claim: The benchmark evaluates genuine temporal reasoning rather than fact recall by withholding timestamps and durations
- Mechanism: Hard difficulty questions provide only event durations without start/end times, forcing models to reason about temporal relationships rather than retrieve memorized temporal facts
- Core assumption: LLMs cannot infer event timing relationships from duration information alone without engaging in temporal reasoning
- Evidence anchors: [section] "The hard category is further divided into Serial and Parallel event execution types. So, there are four variations: Easy (Serial), Medium (Serial), Hard (Serial), and Hard (Parallel)."

### Mechanism 3
- Claim: Parallel event execution capability provides novel temporal reasoning challenge not present in existing benchmarks
- Mechanism: By including scenarios where multiple events occur simultaneously (like loading two packages onto a truck), the benchmark tests models' ability to handle concurrent temporal dependencies
- Core assumption: Existing TSQA benchmarks' sequential event focus leaves a gap in evaluating parallel temporal reasoning capabilities
- Evidence anchors: [abstract] "Additionally, existing TSQA benchmarks (Suzgun et al., 2023) fall short by treating events as sequential occurrences. They ignore the scenarios where multiple events happen concurrently"

## Foundational Learning
- Concept: Temporal reasoning vs fact recall distinction
  - Why needed here: The benchmark's core innovation is distinguishing between memorized temporal facts and genuine reasoning about time
  - Quick check question: Can you explain why "Where did Messi play in 2010?" tests fact recall while "Where is package p2 2 hours after 8:13 PM?" tests temporal reasoning?

- Concept: Synthetic data generation for benchmarking
  - Why needed here: Understanding how synthetic scenarios can provide contamination-free evaluation is crucial for interpreting the benchmark's value
  - Quick check question: How does generating logistics planning problems differ from using real-world temporal data in terms of evaluation validity?

- Concept: Parallel vs sequential event processing
  - Why needed here: The parallel event capability is the novel dimension this benchmark adds to temporal reasoning evaluation
  - Quick check question: What's the key difference in reasoning required when events can occur simultaneously versus only sequentially?

## Architecture Onboarding
- Component map: Synthetic event scenario generation → Difficulty level assignment → Question type categorization → Model evaluation → Error analysis
- Critical path: Synthetic event scenario generation → Question creation with temporal constraints → Model evaluation with zero-shot prompting → Performance analysis by difficulty level
- Design tradeoffs: Synthetic vs real-world data (contamination-free but less ecologically valid), controlled vs diverse scenarios (reproducible but potentially limited), manual vs automated validation (accurate but resource-intensive)
- Failure signatures: Performance plateaus across difficulty levels (suggesting memorization rather than reasoning), inconsistent accuracy across similar difficulty levels (suggesting dataset quality issues), error patterns dominated by single failure modes (suggesting narrow reasoning capabilities)
- First 3 experiments:
  1. Replicate easy difficulty evaluation with zero-shot prompting to establish baseline performance
  2. Test medium difficulty with few-shot prompting to assess impact of additional examples
  3. Compare hard serial vs hard parallel performance to validate the novel parallel event dimension

## Open Questions the Paper Calls Out
- Can the data generation framework be extended to incorporate longer temporal durations beyond 24 hours while maintaining the contamination-free property?
- What alternative metrics could better capture the complexity of parallel time-sensitive questions beyond the depth metric?
- How would the introduction of unanswerable time-sensitive questions affect the benchmark's ability to evaluate LLM reasoning capabilities?

## Limitations
- The synthetic nature of the benchmark may not fully capture the complexity of real-world temporal reasoning scenarios, potentially limiting ecological validity
- Performance differences between models could be influenced by factors beyond temporal reasoning capability, such as prompt format sensitivity or chain-of-thought execution
- The dataset size (10,800 questions) may not be sufficient to establish statistically robust conclusions about model capabilities across all difficulty levels

## Confidence
- **High confidence** in the claim that UnSeenTimeQA provides contamination-free evaluation through synthetic data generation
- **Medium confidence** in the claim that parallel event execution represents a novel temporal reasoning challenge
- **Medium confidence** in the claim that current LLMs struggle with temporal reasoning beyond memorization

## Next Checks
1. Conduct inter-rater reliability testing on a subset of UnSeenTimeQA questions to verify human benchmark accuracy
2. Perform ablation studies on prompt structure to isolate the impact of chain-of-thought reasoning on temporal reasoning performance
3. Evaluate models on a subset of questions using both zero-shot and few-shot prompting to assess transfer learning capabilities and memorization patterns