---
ver: rpa2
title: Do Pre-trained Vision-Language Models Encode Object States?
arxiv_id: '2409.10488'
source_url: https://arxiv.org/abs/2409.10488
tags:
- object
- states
- state
- vlms
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether vision-language models (VLMs) can
  recognize object states, a crucial aspect of physical commonsense reasoning. While
  VLMs excel at object recognition, they consistently struggle to accurately distinguish
  object states, such as whether an apple is whole or sliced.
---

# Do Pre-trained Vision-Language Models Encode Object States?

## Quick Facts
- **arXiv ID**: 2409.10488
- **Source URL**: https://arxiv.org/abs/2409.10488
- **Reference count**: 18
- **Primary result**: VLMs show 30% accuracy drop in object state recognition compared to object recognition

## Executive Summary
This paper investigates whether vision-language models (VLMs) can effectively recognize object states, a fundamental aspect of physical commonsense reasoning. Despite their strong performance on object recognition tasks, VLMs consistently struggle to distinguish between different states of the same object, such as identifying whether an apple is whole or sliced. The authors introduce ChangeIt-Frames, a novel dataset containing 96 object states, and systematically evaluate nine open-source VLMs across contrastive and generative architectures. The results reveal a significant gap in state recognition capabilities, with models showing approximately 30% lower accuracy compared to object recognition tasks. This persistent performance gap suggests that current VLMs lack essential capabilities for understanding object states, highlighting the need for architectural improvements and enhanced training objectives to better capture physical commonsense.

## Method Summary
The authors created ChangeIt-Frames, a dataset featuring images depicting 96 distinct object states across various categories. They evaluated nine open-source VLMs, including both contrastive models (like CLIP) and generative models (like Flamingo). The evaluation compared model performance on object recognition versus state recognition tasks using the same image set. The study systematically tested whether object-centric representations or larger model sizes could improve state recognition performance. Models were assessed on their ability to correctly identify both the object (e.g., "apple") and its state (e.g., "sliced").

## Key Results
- VLMs demonstrate approximately 30% lower accuracy on state recognition compared to object recognition
- The performance gap persists across both contrastive and generative model architectures
- Larger model sizes and object-centric representations do not substantially improve state recognition performance

## Why This Works (Mechanism)
The observed performance gap between object and state recognition likely stems from how VLMs learn visual concepts during pre-training. While object recognition relies on identifying consistent visual patterns across instances, state recognition requires understanding subtle visual cues and transformations that indicate physical changes. The models appear to learn object-level features effectively but struggle to encode the nuanced visual information necessary for distinguishing states. This suggests that current training objectives and datasets may not adequately capture the rich visual semantics required for state recognition, pointing to a fundamental limitation in how VLMs encode physical commonsense knowledge.

## Foundational Learning
- **Object recognition**: Identifying visual patterns that correspond to object categories
  - Why needed: Serves as baseline for comparison with state recognition performance
  - Quick check: VLMs achieve high accuracy (>90%) on standard object recognition benchmarks

- **State representation**: Encoding visual information that distinguishes between different physical conditions of objects
  - Why needed: Essential for understanding physical commonsense and object transformations
  - Quick check: Models show significant performance degradation when asked to distinguish states

- **Contrastive learning**: Training method that learns representations by comparing similar and dissimilar examples
  - Why needed: Common training approach for VLMs that may influence how states are encoded
  - Quick check: Performance varies across contrastive vs generative architectures

## Architecture Onboarding

**Component Map**: Image Encoder -> Text Encoder -> Joint Embedding Space -> Classification Head

**Critical Path**: Image input → Feature extraction → State representation learning → Classification output

**Design Tradeoffs**: The paper implicitly compares contrastive models (CLIP) vs generative models (Flamingo), revealing that neither architecture effectively handles state recognition despite different design philosophies. This suggests the limitation is fundamental rather than architectural.

**Failure Signatures**: Consistent 30% accuracy drop when transitioning from object to state recognition, regardless of model size or architecture type. Models often confuse visually similar states (e.g., "whole" vs "halved" fruits).

**3 First Experiments**:
1. Evaluate a VLM on both object and state recognition using the same ChangeIt-Frames dataset
2. Test whether fine-tuning on state-specific examples improves performance
3. Compare performance across different state types (e.g., spatial arrangements vs physical transformations)

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation focuses on static images rather than dynamic state changes
- Dataset covers only 96 object states, potentially limiting generalizability
- Does not explore whether fine-tuning or additional training objectives could improve performance

## Confidence
- **High confidence**: VLMs show significantly worse performance on object state recognition compared to object recognition
- **High confidence**: The performance gap persists across different model architectures (contrastive and generative)
- **Medium confidence**: Object-centric representations and larger model sizes do not substantially improve state recognition

## Next Checks
1. Test whether fine-tuning VLMs on the ChangeIt-Frames dataset improves state recognition accuracy, particularly for models that show the largest gaps between object and state performance
2. Extend evaluation to include dynamic state changes (before/after pairs) rather than static images to assess temporal reasoning capabilities
3. Investigate whether combining VLMs with explicit physical reasoning modules or knowledge graphs can bridge the state recognition gap identified in this study