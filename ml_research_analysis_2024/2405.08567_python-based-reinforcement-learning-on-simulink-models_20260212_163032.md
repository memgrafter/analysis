---
ver: rpa2
title: Python-Based Reinforcement Learning on Simulink Models
arxiv_id: '2405.08567'
source_url: https://arxiv.org/abs/2405.08567
tags:
- simulink
- system
- self
- python
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for training Reinforcement Learning
  (RL) agents using Python and Simulink models, demonstrated on the Quanser Aero 2
  dual-rotor helicopter system. The approach involves generating C-code from Simulink,
  compiling it into a DLL, and creating a custom Gymnasium environment in Python to
  interface with the model.
---

# Python-Based Reinforcement Learning on Simulink Models

## Quick Facts
- arXiv ID: 2405.08567
- Source URL: https://arxiv.org/abs/2405.08567
- Reference count: 11
- Trained PPO agent achieves -64.87 reward (4.6° mean average deviation) on Quanser Aero 2 balancing task

## Executive Summary
This paper presents a framework for training Reinforcement Learning agents using Python and Simulink models, demonstrated on the Quanser Aero 2 dual-rotor helicopter system. The approach involves generating C-code from Simulink, compiling it into a DLL, and creating a custom Gymnasium environment in Python to interface with the model. Using Stable Baselines3's Proximal Policy Optimization (PPO) agent with default parameters, the framework achieves a maximum reward of -64.87 (mean average deviation of 4.6°) in a 1-DOF balancing task, surpassing previous efforts using MATLAB's RL Toolbox. The trained policies are successfully transferred to the real system, highlighting the potential of combining Simulink with Python for RL research and applications.

## Method Summary
The framework generates C-code from a Simulink model of the Quanser Aero 2 system, which is then compiled into a DLL. Python's ctypes library interfaces with this DLL, exposing functions for model initialization, stepping, and termination. A custom Gymnasium environment is constructed to wrap these DLL calls, implementing standard RL interface methods. The environment defines appropriate action and observation spaces, and includes reward calculation based on angle deviation from the target. Stable Baselines3's PPO agent is then trained on this environment for 500,000 steps, using default hyperparameters. The trained policies are evaluated in both simulation and on the real hardware using the Quanser Python API and HIL card.

## Key Results
- Achieved maximum reward of -64.87 (4.6° mean average deviation) on 1-DOF balancing task
- Outperformed previous MATLAB RL Toolbox results (-77.93 reward) by 16.8%
- Successfully transferred trained policies to real Quanser Aero 2 system

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simulink-generated C code compiled into a DLL can be directly called from Python using ctypes, enabling RL training without reimplementing system dynamics.
- Mechanism: Simulink Coder exports the model as C code. Compiling this code into a DLL exposes functions for initialization, stepping, and termination. Python's ctypes library binds to these functions, allowing direct model execution within a Gymnasium environment.
- Core assumption: The Simulink model's API (initialize, step, terminate) maps cleanly to Python method calls without serialization overhead.
- Evidence anchors:
  - [section] "The DLL is interfaced with Python using ctypes, exposing the following essential functions and data structures: MODELNAME initialize, MODELNAME step, MODELNAME terminate"
  - [section] "A custom Gymnasium [10] environment is constructed to interface with the Simulink model, implementing the following key methods"
- Break condition: If the Simulink model uses unsupported features (e.g., variable-step solvers) that prevent C-code generation, or if the compiled DLL is incompatible with the Python ctypes interface.

### Mechanism 2
- Claim: Using Stable Baselines3's PPO with default parameters outperforms MATLAB's RL Toolbox PPO on the same balancing task.
- Mechanism: PPO's clipped surrogate objective and adaptive learning rate provide stable policy updates. Default hyperparameters in SB3 are tuned for general performance, while MATLAB's RL Toolbox defaults may be suboptimal for this specific system.
- Core assumption: The default SB3 PPO configuration is broadly applicable without task-specific tuning.
- Evidence anchors:
  - [abstract] "Using Stable Baselines3's Proximal Policy Optimization (PPO) agent with default parameters, the framework achieves a maximum reward of -64.87... surpassing previous efforts using MATLAB's RL Toolbox"
  - [section] "This is a notable increase in performance compared to prior work from [9], where a maximum reward of -77.93... was reached"
- Break condition: If the task complexity increases beyond the default hyperparameter range, requiring manual tuning for convergence.

### Mechanism 3
- Claim: Policies trained in the Simulink simulation transfer effectively to the real Quanser Aero 2 system.
- Mechanism: The Simulink model accurately captures the system dynamics, allowing the RL agent to learn control policies that generalize to physical hardware. The Quanser Python API and HIL card enable seamless deployment.
- Core assumption: The simulation model sufficiently approximates real-world dynamics, including noise and nonlinearities.
- Evidence anchors:
  - [abstract] "The trained policies are successfully transferred to the real system"
  - [section] "Fig. 3 shows the evaluation of the agent trained on the simulated model in a real world scenario alongside its performance in the simulation"
- Break condition: If sim-to-real mismatch (e.g., unmodeled friction, sensor noise) causes significant performance degradation on the real system.

## Foundational Learning

- Concept: Reinforcement Learning fundamentals (states, actions, rewards, policy optimization)
  - Why needed here: The paper implements PPO to learn a control policy for the Aero 2 balancing task.
  - Quick check question: What are the state and action spaces for the Aero 2 1-DOF balancing task?

- Concept: Gymnasium environment interface (init, step, reset, render methods)
  - Why needed here: The custom environment wraps the Simulink model to interface with SB3.
  - Quick check question: How does the step method in the custom environment handle the Simulink model's sample time?

- Concept: C-code generation and DLL compilation from Simulink
  - Why needed here: Enables integration of the Simulink model with Python-based RL training.
  - Quick check question: What Simulink Coder functions must be exposed in the DLL for Python integration?

## Architecture Onboarding

- Component map:
  - Simulink model (Aero 2 dynamics) -> Simulink Coder (C-code generation) -> DLL (compiled C-code) -> Python ctypes (DLL interface) -> Gymnasium environment (RL interface) -> Stable Baselines3 (RL agent) -> Quanser Python API (real system deployment)

- Critical path:
  1. Design Simulink model
  2. Generate C-code with Simulink Coder
  3. Compile DLL
  4. Create Python Gymnasium environment with ctypes
  5. Train RL agent with SB3
  6. Deploy policy to real system

- Design tradeoffs:
  - Fixed-step vs. variable-step solver in Simulink (affects C-code generation)
  - DLL interface complexity vs. direct Python implementation
  - Simulation fidelity vs. training speed

- Failure signatures:
  - DLL loading errors (missing dependencies, architecture mismatch)
  - Gym environment crashes during step (invalid C-code interface)
  - RL training instability (poor reward signal or state representation)

- First 3 experiments:
  1. Verify DLL functionality: Create a minimal Python script that loads the DLL and calls initialize, step, and terminate functions.
  2. Test Gymnasium environment: Implement a basic environment that interacts with the DLL and returns random actions and rewards.
  3. Train on simplified task: Modify the reward function to a simple quadratic cost and verify PPO training converges in simulation.

## Open Questions the Paper Calls Out

- Open Question 1: What is the optimal set of hyperparameters for Stable Baselines3's PPO agent to maximize performance on the Quanser Aero 2 system?
  - Basis in paper: [explicit] The paper mentions that default parameters of SB3's PPO implementation outperformed the fine-tuned PPO agent based on MATLAB's RL Toolbox, but suggests that further fine-tuning could lead to increased performance.
  - Why unresolved: The paper only used default parameters and achieved good results, but did not explore the full hyperparameter space.
  - What evidence would resolve it: Systematic hyperparameter optimization experiments comparing different configurations and their impact on performance metrics.

- Open Question 2: How does the performance of RL-trained policies compare to traditional control methods (e.g., PID, LQR, MPC) for the Quanser Aero 2 system?
  - Basis in paper: [explicit] The paper states "planned areas of research include the comparison of the quality of the learned policies balancing capabilities to default controllers such as linear-quadratic regulators or model predictive control."
  - Why unresolved: The paper focuses on RL performance but does not provide a direct comparison with traditional control methods.
  - What evidence would resolve it: Benchmark experiments comparing RL policies against established control algorithms in terms of performance, stability, and robustness.

- Open Question 3: What is the impact of expanding the state space to include the actual tilt angle Θ on the RL agent's performance?
  - Basis in paper: [explicit] The paper mentions "expansion of the state space, by adding the actual tilt Θ" as a potential way to improve performance for the real system.
  - Why unresolved: The current state representation only includes the error and angular velocity, and the effect of adding Θ is not explored.
  - What evidence would resolve it: Experiments comparing agent performance with and without the additional state variable in both simulation and real-world settings.

## Limitations
- Limited generalizability: Framework demonstrated only on Quanser Aero 2 1-DOF system without exploration of parameter sensitivity or alternative control tasks
- Performance comparison lacks ablation studies: Claims of superiority over MATLAB's RL Toolbox not isolated from implementation differences
- Sim-to-real transfer metrics incomplete: Quantitative performance degradation between simulation and hardware not characterized

## Confidence

- Technical feasibility: High
  - Interfacing Simulink-generated DLLs with Python through ctypes is well-established practice

- Performance comparison claims: Medium
  - Limited benchmarking against MATLAB's RL Toolbox without deeper analysis of contributing factors

- Generality to other systems: Low
  - Framework demonstrated only on single application without exploration of broader applicability

## Next Checks
1. Replicate the training pipeline on a different Simulink model (e.g., mass-spring-damper system) to verify framework portability
2. Conduct an ablation study comparing SB3 PPO with MATLAB's PPO implementation on identical reward functions and hyperparameters
3. Measure quantitative performance metrics (settling time, overshoot) during sim-to-real transfer to characterize degradation patterns