---
ver: rpa2
title: Learning in Hybrid Active Inference Models
arxiv_id: '2409.01066'
source_url: https://arxiv.org/abs/2409.01066
tags:
- discrete
- continuous
- control
- inference
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of learning discrete abstractions
  for decision-making in continuous control tasks, which is challenging due to high-dimensional
  state spaces. The authors propose a hierarchical hybrid active inference agent that
  combines a discrete high-level planner with a continuous low-level controller.
---

# Learning in Hybrid Active Inference Models

## Quick Facts
- arXiv ID: 2409.01066
- Source URL: https://arxiv.org/abs/2409.01066
- Reference count: 40
- One-line primary result: Hybrid active inference agent learns discrete abstractions for continuous control, outperforming model-free RL in sample efficiency on sparse Mountain Car task.

## Executive Summary
This work introduces a hierarchical hybrid active inference agent that learns discrete abstractions from continuous dynamics to enable efficient decision-making in high-dimensional control tasks. The agent combines a recurrent switching linear dynamical system (rSLDS) for discovering meaningful discrete states, a discrete planner using active inference with information-theoretic exploration bonuses, and a continuous LQR controller for low-level execution. Applied to the sparse Continuous Mountain Car task, the method demonstrates fast system identification and successful planning through abstract sub-goals, achieving comparable performance to model-based algorithms while being more sample-efficient than model-free RL baselines.

## Method Summary
The method employs a hierarchical architecture where an rSLDS learns discrete state representations from continuous observations and control inputs through piecewise linear decomposition. These discrete states form the basis for a POMDP-based active inference planner that operates at a temporally-abstracted level, updating only when mode switches are detected. The planner incorporates Dirichlet priors and information-gain exploration bonuses to efficiently explore the discrete state space. For each discrete state transition, a precomputed LQR controller provides optimal continuous control within the corresponding linear regime. The discrete actions are mapped to continuous control priors through a link function, enabling seamless integration between the discrete and continuous components.

## Key Results
- Hybrid agent outperforms model-free RL baselines in sample efficiency on sparse Continuous Mountain Car task
- Achieves comparable state-space coverage to model-based algorithms with exploration enhancements
- Demonstrates fast system identification through effective discrete state discovery via rSLDS

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The rSLDS learns discrete state representations that align with meaningful behavioral modes in continuous dynamics, enabling hierarchical abstraction.
- Mechanism: rSLDS uses softmax regression over continuous latents and control inputs to predict discrete states, then learns linear dynamics conditioned on these discrete states. This piecewise linear decomposition creates polyhedral partitions of the state space that map naturally to subgoals.
- Core assumption: The underlying continuous dynamics can be approximated by a piecewise linear model with discrete latent states that switch based on continuous conditions.
- Evidence anchors:
  - [abstract]: "recurrent switching linear dynamical systems (rSLDS) which implement end-to-end learning of meaningful discrete representations via the piecewise linear decomposition of complex continuous dynamics"
  - [section 3.1]: "The discrete latent states zt ∈{1,2,...,K} are generated as a function of the continuous latentsxt ∈RM and the control input ut ∈RN (specified by some controller) via a softmax regression model"
- Break condition: If the true dynamics are not piecewise linear or cannot be well-approximated by a finite number of linear regimes, the rSLDS will fail to discover meaningful discrete abstractions.

### Mechanism 2
- Claim: The hierarchical structure allows temporal abstraction by lifting discrete planning to a coarser timescale than continuous control.
- Mechanism: The discrete planner only updates when the rSLDS detects a mode switch, creating temporally-extended actions. This separation enables efficient planning in discrete space while the continuous controller handles low-level execution within each mode.
- Core assumption: Mode switches in the rSLDS are sufficiently infrequent and meaningful to justify temporal abstraction in planning.
- Evidence anchors:
  - [abstract]: "specify temporally-abstracted sub-goals in a method reminiscent of the options framework"
  - [section 3.3]: "the discrete planner is only triggered when the system switches into a new mode... discrete actions are temporally abstracted and decoupled from continuous clock-time"
- Break condition: If mode switches occur too frequently or the rSLDS fails to learn stable discrete states, the temporal abstraction will break down and the planner will update too often.

### Mechanism 3
- Claim: Information-theoretic exploration bonuses in discrete space enable efficient system identification in sparse reward tasks.
- Mechanism: The discrete planner uses Dirichlet priors over transitions and incorporates state information gain, which has closed-form solutions in discrete space. This drives exploration of different discrete modes, which correspond to different regions of the continuous state space.
- Core assumption: Information gain in discrete state transitions correlates with exploration of the underlying continuous state space.
- Evidence anchors:
  - [abstract]: "lift the exploration into discrete space allowing us to exploit information-theoretic exploration bonuses"
  - [section 3.3]: "we maintain Dirichlet priors over the transition parameters B, facilitating directed exploration"
- Break condition: If the discrete state space becomes too large or the exploration bonuses don't correlate with meaningful exploration of the continuous space, the system identification will be inefficient.

## Foundational Learning

- Concept: Active Inference framework and free energy minimization
  - Why needed here: The entire hierarchical decision-making process is framed as minimizing expected free energy, which balances utility and information gain
  - Quick check question: What are the two main components of the expected free energy functional and what do they represent?

- Concept: Switching linear dynamical systems and piecewise linear approximations
  - Why needed here: The rSLDS forms the core mechanism for learning discrete abstractions from continuous dynamics through piecewise linear decomposition
  - Quick check question: How does the rSLDS use softmax regression to connect continuous latents to discrete states?

- Concept: Linear Quadratic Regulator (LQR) and optimal control for continuous subsystems
  - Why needed here: Each discrete mode corresponds to a linear system that can be controlled optimally using LQR, allowing efficient closed-loop control within each mode
  - Quick check question: What is the role of the control prior in the LQR formulation within each discrete mode?

## Architecture Onboarding

- Component map:
  - rSLDS module: Learns discrete states from continuous observations and control inputs
  - Discrete planner: POMDP-based active inference planner operating on discrete states
  - Continuous controller: Set of LQR controllers, one per discrete state transition
  - Link function: Maps discrete actions to continuous control priors

- Critical path:
  1. rSLDS observes current state and control input, infers current discrete state
  2. Discrete planner (if mode changed) selects action based on POMDP inference
  3. Action is mapped to control prior for target discrete state
  4. Continuous LQR controller executes control to reach target region
  5. Process repeats when rSLDS detects new mode

- Design tradeoffs:
  - Fixed polyhedral partition vs. adaptive discretization: The rSLDS provides fixed partitions but may not adapt to changing task requirements
  - Offline LQR computation vs. online optimization: Pre-computing LQR solutions speeds up control but may be suboptimal for changing environments
  - Temporal abstraction frequency: Balancing between too frequent updates (losing abstraction benefits) and too infrequent updates (poor responsiveness)

- Failure signatures:
  - Poor mode discovery: rSLDS fails to learn meaningful discrete states, leading to ineffective planning
  - Mode switching instability: Discrete states change too frequently, breaking temporal abstraction
  - Control failure: LQR controllers cannot stabilize within discrete regions due to poor linearization

- First 3 experiments:
  1. Test rSLDS on simple synthetic piecewise linear system to verify discrete state discovery
  2. Implement pure continuous active inference controller on Mountain Car without hierarchy to establish baseline
  3. Test discrete planner with pre-defined discrete states (no rSLDS) to isolate planner performance from state discovery

## Open Questions the Paper Calls Out

- Question: How does the performance of the hybrid hierarchical active inference agent scale with increasing state space dimensionality compared to traditional grid-based discretization methods?
- Basis in paper: [inferred] The paper discusses the curse of dimensionality associated with grid-based discretization and suggests their method mitigates this issue, but does not provide explicit comparisons across varying dimensionalities.
- Why unresolved: The paper only demonstrates results on a specific task (Continuous Mountain Car) without exploring how the method performs as state space dimensionality increases.
- What evidence would resolve it: Empirical results showing performance (e.g., sample efficiency, success rate) across tasks with varying state space dimensions, compared to both grid-based methods and other hierarchical approaches.

- Question: What is the impact of the fixed goal state assumption in the LQR controller on the overall optimality of the system, and how could this be improved?
- Basis in paper: [explicit] The authors acknowledge using a fixed goal state for the LQR controller, accepting "a good deal of sub-optimality," and suggest more involved methods could be used in future.
- Why unresolved: The paper does not explore alternative approaches to handling control input constraints or the trade-offs between computational efficiency and optimality.
- What evidence would resolve it: Comparative analysis of system performance using different LQR constraint handling methods (e.g., reachability regions) versus the fixed goal state approach, including both optimality metrics and computational costs.

- Question: How robust is the system identification process to noisy observations or perturbations in the underlying dynamics?
- Basis in paper: [inferred] The paper demonstrates successful system identification in a controlled environment but does not address how the method performs under realistic conditions with noise or dynamic changes.
- Why unresolved: The evaluation focuses on a single, relatively clean control task without exploring the method's resilience to real-world conditions.
- What evidence would resolve it: Empirical results showing system identification accuracy and planning success rates under varying levels of observation noise and dynamic perturbations, compared to both model-free and other model-based approaches.

## Limitations
- Performance in more complex continuous control tasks beyond Mountain Car remains untested, particularly those with high-dimensional state spaces or non-linear dynamics
- The assumption of sufficiently sparse mode switches for temporal abstraction may not hold in all domains
- Exploration bonuses may not scale effectively to tasks with complex reward structures or larger discrete state spaces

## Confidence

**High Confidence**: The core mechanisms of rSLDS for piecewise linear decomposition and the integration of discrete and continuous components are well-established and clearly specified.

**Medium Confidence**: The hierarchical structure's effectiveness in temporal abstraction and the specific implementation of information-theoretic exploration bonuses in the active inference framework are plausible but require empirical validation in varied settings.

**Low Confidence**: Generalization to more complex continuous control tasks beyond Mountain Car, particularly those with high-dimensional state spaces or non-linear dynamics that don't fit the piecewise linear assumption, remains an open question.

## Next Checks

1. **Cross-task generalization**: Apply the method to other continuous control benchmarks (e.g., Half-Cheetah, Hopper) to test whether the rSLDS can discover meaningful abstractions in higher-dimensional state spaces.

2. **Baselines comparison**: Compare against other hierarchical RL approaches (options framework, feudal networks) and model-based RL algorithms with explicit exploration strategies to isolate the contribution of the hybrid active inference architecture.

3. **Ablation study**: Systematically disable components (rSLDS, exploration bonuses, temporal abstraction) to quantify their individual contributions to sample efficiency and performance, particularly focusing on the rSLDS's state discovery quality.