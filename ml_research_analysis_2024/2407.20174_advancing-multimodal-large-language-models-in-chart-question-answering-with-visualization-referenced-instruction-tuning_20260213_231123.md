---
ver: rpa2
title: Advancing Multimodal Large Language Models in Chart Question Answering with
  Visualization-Referenced Instruction Tuning
arxiv_id: '2407.20174'
source_url: https://arxiv.org/abs/2407.20174
tags:
- chart
- data
- visual
- mllms
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving multimodal large
  language models (MLLMs) for chart question answering (CQA) tasks. Existing MLLMs
  struggle with understanding fine-grained visual encodings and reasoning over charts.
---

# Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning

## Quick Facts
- **arXiv ID**: 2407.20174
- **Source URL**: https://arxiv.org/abs/2407.20174
- **Reference count**: 40
- **Key outcome**: Achieves 93.6% accuracy on ChartQA and 78.6% on chart-to-table tasks, outperforming state-of-the-art models while using significantly less training data.

## Executive Summary
This paper addresses the challenge of improving multimodal large language models (MLLMs) for chart question answering (CQA) tasks. Current MLLMs struggle with understanding fine-grained visual encodings and reasoning over charts. The authors propose a visualization-referenced instruction tuning approach that includes a novel data engine for filtering and generating high-quality chart data, and a new MLLM architecture with unfrozen vision encoders and a mixture-of-resolution adaptation strategy. Their method outperforms state-of-the-art CQA models on established benchmarks, achieving 93.6% accuracy on ChartQA and 78.6% on chart-to-table tasks, while using significantly less training data. They also contribute a new benchmark for evaluating MLLM performance on real-world chart tasks.

## Method Summary
The paper proposes visualization-referenced instruction tuning for MLLMs to improve chart question answering. The approach includes a data engine that filters existing datasets and generates new chart-QA pairs to correct distribution biases, an MLLM architecture with unfrozen vision encoders and a mixture-of-resolution adaptation strategy, and training on the combined dataset using AdamW optimizer with cosine decay learning rate scheduler. The method aims to better align training data with practical QA tasks and visual encodings while improving fine-grained recognition through encoder adaptation and resolution mixing.

## Key Results
- Achieves 93.6% accuracy on ChartQA benchmark, outperforming state-of-the-art models
- Reaches 91.8% accuracy on chart-to-table tasks with significant data efficiency
- Demonstrates 46.15% average accuracy on a new real-world chart task benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visualization-referenced instruction tuning improves MLLM chart understanding by aligning training data distribution with real-world chart and QA distributions.
- Mechanism: The data engine filters existing datasets and generates new chart-QA pairs guided by the chart-task space, correcting biases in chart types and QA complexity that limit current MLLMs.
- Core assumption: Current CQA datasets are distributionally biased relative to real-world charts, leading to poor model generalization.
- Evidence anchors:
  - [abstract] states the approach "effectively filter diverse and high-quality data" and "generate unavailable chart tasks" to "better align with practical QA tasks and visual encodings."
  - [section] describes how Beagle dataset diversity contrasts with ChartQA, and that ChartQA over-represents data retrieval QAs.
  - [corpus] provides context of recent works (ChartMind, ChartQAPro) that also focus on improving CQA benchmarks, suggesting ongoing efforts to address distribution bias.
- Break condition: If the generated data does not reflect real-world chart complexity or if filtering removes too much useful data, the distribution alignment benefit disappears.

### Mechanism 2
- Claim: Unfreezing the vision encoder allows adaptation to chart-specific visual features, improving fine-grained recognition over frozen CLIP.
- Mechanism: By training vision encoder parameters (instead of freezing them), the model learns chart-specific visual encodings like axis inversions, color mappings, and stacked area charts that CLIP pre-training did not capture well.
- Core assumption: Pre-trained CLIP encoders are not optimized for the unique visual patterns of charts, leading to recognition errors.
- Evidence anchors:
  - [abstract] mentions "incorporating a mixture-of-resolution adaptation strategy for enhanced fine-grained recognition" and "unfrozen vision encoder."
  - [section] reports that "CLIP performs much worse in visualization images" and that unfreezing improves chart recognition ability.
  - [corpus] shows related work on improving vision encoders for charts, supporting the importance of encoder adaptation.
- Break condition: If unfreezing introduces overfitting to synthetic data or if the adaptation strategy does not generalize to real-world charts, performance may degrade.

### Mechanism 3
- Claim: Mixing high-resolution and normal-resolution features via vision encoder mixture improves recognition of fine-grained chart elements.
- Mechanism: Combining ConvNeXt (high-res) and CLIP-ViT (normal-res) with adapter fusion captures both coarse and fine visual details, helping tasks like determining ranges or character distributions.
- Core assumption: High-resolution features are necessary for recognizing small chart elements (e.g., bubble sizes, text annotations) that are critical for certain QA tasks.
- Evidence anchors:
  - [abstract] describes using "a mixture-of-resolution adaptation strategy" and mixing vision encoders.
  - [section] notes that current MLLMs "still exhibit subpar performance in specific tasks" like find anomalies, linked to difficulty recognizing small visual points.
  - [corpus] provides examples of recent works (ChartGaze) focusing on attention refinement for chart regions, implying the importance of resolution.
- Break condition: If the adapter fusion does not effectively combine features or if the computational cost outweighs benefits, the strategy may fail.

## Foundational Learning

- Concept: Multimodal large language models (MLLMs) combine vision encoders with LLMs to process images and text jointly.
  - Why needed here: Understanding how MLLMs are trained on chart-QA pairs is essential to grasp the paper's contributions.
  - Quick check question: What are the three main components of a typical MLLM architecture?
- Concept: Instruction tuning adapts pre-trained models to follow natural language instructions, improving task-specific performance.
  - Why needed here: The paper's data engine generates instruction-formatted chart-QA pairs for tuning MLLMs.
  - Quick check question: How does instruction tuning differ from standard fine-tuning?
- Concept: Data distribution alignment ensures training data reflects the diversity and complexity of real-world scenarios.
  - Why needed here: The paper identifies biased distributions in existing CQA datasets and corrects them via filtering and generation.
  - Quick check question: Why is stratified sampling used in the data filtering process?

## Architecture Onboarding

- Component map: Vision encoder (CLIP-ViT + ConvNeXt mixture) -> Projector (MLP) -> LLM (Vicuna) -> Output. Data engine generates training data. Benchmark evaluates performance.
- Critical path: Generate/filter chart-QA data -> Train MLLM with unfrozen vision encoder and resolution mixture -> Evaluate on benchmarks.
- Design tradeoffs: Unfreezing vision encoder improves chart recognition but increases training complexity; high-resolution features improve fine-grained tasks but add computational cost.
- Failure signatures: Poor performance on compositional/visual questions indicates distribution bias; recognition errors suggest vision encoder limitations; high computational cost may indicate inefficient resolution mixture.
- First 3 experiments:
  1. Evaluate baseline MLLM on ChartQA to confirm distribution bias.
  2. Train with filtered data only to test impact of data quality.
  3. Train with generated data only to test impact of data diversity.

## Open Questions the Paper Calls Out

- Question: What is the optimal balance between data filtering and data generation in the proposed data engine to maximize MLLM performance on CQA tasks while minimizing computational cost?
  - Basis in paper: [explicit] The paper discusses a two-stage data engine (filtering-then-generation) but does not provide specific guidelines on the optimal balance between these stages.
  - Why unresolved: The paper demonstrates the effectiveness of the data engine but does not explore the impact of varying the proportions of filtered versus generated data on model performance and computational efficiency.
  - What evidence would resolve it: A systematic study varying the ratio of filtered to generated data, measuring both model performance and computational costs, would help determine the optimal balance.

- Question: How do different types of visual encodings (e.g., color, shape, size) impact the performance of MLLMs on CQA tasks, and how can the data engine be optimized to better represent these encodings?
  - Basis in paper: [inferred] The paper mentions the importance of fine-grained visual encodings but does not specifically investigate the impact of different encoding types on model performance.
  - Why unresolved: The paper highlights the need for considering fine-grained chart features but does not provide a detailed analysis of how different visual encodings affect MLLM performance.
  - What evidence would resolve it: A study systematically varying the presence and complexity of different visual encodings in the training data and evaluating their impact on model performance would provide insights into optimizing the data engine.

- Question: What are the limitations of current MLLMs in handling complex reasoning tasks involving charts, and how can these limitations be addressed through architectural improvements or training strategies?
  - Basis in paper: [explicit] The paper identifies that MLLMs struggle with certain complex reasoning tasks, such as finding anomalies and determining ranges, but does not provide a comprehensive analysis of the underlying reasons for these limitations.
  - Why unresolved: While the paper acknowledges the limitations of MLLMs in complex reasoning tasks, it does not delve into the specific architectural or training-related factors contributing to these limitations.
  - What evidence would resolve it: A detailed investigation into the model's reasoning process, potentially using techniques like attention analysis or error analysis, could reveal the specific challenges faced by MLLMs in complex reasoning tasks and guide improvements.

## Limitations

- The data engine's effectiveness depends heavily on the quality of probing classifiers and LLM-based generation pipeline, neither of which are fully specified
- Reliance on synthetic data generation introduces uncertainty about whether the model learns genuine chart understanding versus pattern matching to generated examples
- Computational costs and potential overfitting to synthetic data remain concerns with the unfrozen vision encoder and resolution mixture strategy

## Confidence

- **High confidence**: The identification of distribution bias in existing CQA datasets and the effectiveness of the data filtering approach.
- **Medium confidence**: The overall performance improvements on benchmarks, though the specific contribution of each architectural component (unfrozen encoder, resolution mixture) is difficult to isolate.
- **Medium confidence**: The new benchmark provides valuable evaluation coverage, but the relatively low average accuracy (46.15%) suggests significant room for improvement.

## Next Checks

1. **Ablation study**: Remove the resolution mixture component and retrain to quantify its specific contribution to performance gains.
2. **Real-world deployment test**: Evaluate the model on charts from actual business reports or academic papers not seen during training to assess generalization beyond curated benchmarks.
3. **Computational efficiency analysis**: Measure training time and inference latency with the unfrozen vision encoder and resolution mixture to determine practical deployment viability.