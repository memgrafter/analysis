---
ver: rpa2
title: 'GLASS: Guided Latent Slot Diffusion for Object-Centric Learning'
arxiv_id: '2407.17929'
source_url: https://arxiv.org/abs/2407.17929
tags:
- glass
- object
- image
- slot
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GLASS addresses the part-whole hierarchy problem in object-centric
  learning by learning slot attention in the space of generated images from a pre-trained
  diffusion model. The key innovation is using the diffusion decoder to generate semantic
  masks from captions, which guide slots to bind to objects rather than object parts.
---

# GLASS: Guided Latent Slot Diffusion for Object-Centric Learning

## Quick Facts
- arXiv ID: 2407.17929
- Source URL: https://arxiv.org/abs/2407.17929
- Authors: Krishnakant Singh; Simone Schaub-Meyer; Stefan Roth
- Reference count: 40
- Key result: +35% (VOC) and +10% (COCO) relative improvement in mIoU over previous state-of-the-art object discovery methods

## Executive Summary
GLASS addresses the part-whole hierarchy problem in object-centric learning by learning slot attention in the space of generated images from a pre-trained diffusion model. The key innovation is using the diffusion decoder to generate semantic masks from captions, which guide slots to bind to objects rather than object parts. GLASS achieves approximately +35% (VOC) and +10% (COCO) relative improvement in mIoU over previous state-of-the-art object discovery methods, establishing new SOTA FID scores for conditional image generation among slot-attention-based methods. The method requires only a BLIP-2 caption generator in addition to the pre-trained diffusion decoder, making it computationally efficient while outperforming both weakly-supervised and language-based segmentation models.

## Method Summary
GLASS learns slot attention in the space of generated images by leveraging a pre-trained diffusion model to create semantic masks from captions. The method first generates synthetic training data by creating captions for real images using BLIP-2, then generating images with Stable Diffusion conditioned on these captions plus extracted class labels. A slot attention module is trained on this synthetic data using a DINOv2 encoder, with a guidance loss that matches predicted slots to semantic masks extracted from the diffusion model's cross-attention maps. During evaluation on real test images, the learned slots can be used for object discovery metrics (mIoU, mBOi, mBOc) and conditional generation (FID).

## Key Results
- GLASS achieves +35% (VOC) and +10% (COCO) relative improvement in mIoU over previous state-of-the-art object discovery methods
- Establishes new SOTA FID scores for conditional image generation among slot-attention-based methods
- Requires only a BLIP-2 caption generator in addition to pre-trained diffusion decoder, making it computationally efficient

## Why This Works (Mechanism)
GLASS works by addressing the fundamental issue of part-whole hierarchy in object-centric learning. Traditional slot attention models often bind to object parts rather than whole objects because they learn from real images where objects may be occluded or partially visible. By training in the space of generated images where objects are complete and well-separated, and using the diffusion decoder's cross-attention maps as semantic guidance, GLASS ensures slots learn to bind to entire objects rather than their constituent parts. The guidance loss explicitly encourages this behavior by matching predicted slots to the semantic masks generated from captions.

## Foundational Learning
- **Slot Attention**: Why needed - to discover and represent objects as independent entities; Quick check - slots should bind to coherent object regions rather than parts
- **Diffusion Models**: Why needed - to generate complete, well-separated objects for training; Quick check - generated images should have clear object boundaries
- **Cross-Attention Maps**: Why needed - to extract semantic information from diffusion decoder; Quick check - maps should highlight relevant object regions
- **Semantic Masks**: Why needed - to guide slot attention toward whole objects; Quick check - masks should align with object boundaries
- **Guidance Loss**: Why needed - to enforce correct slot-object binding; Quick check - loss should decrease as slots align with objects

## Architecture Onboarding
Component Map: Real Images -> Caption Generator (BLIP-2) -> Caption + Class Labels -> Diffusion Model (Stable Diffusion) -> Synthetic Images -> Slot Attention Module -> Semantic Masks -> Guidance Loss -> Trained Model -> Real Test Images

Critical Path: Caption generation → Image synthesis → Semantic mask extraction → Slot attention training → Real image evaluation

Design Tradeoffs: Uses pre-trained diffusion model rather than training from scratch (computational efficiency vs. potential domain mismatch)

Failure Signatures: Slots binding to object parts instead of wholes; Poor semantic mask quality leading to misaligned guidance

First Experiments:
1. Generate synthetic data with BLIP-2 and Stable Diffusion, verify semantic mask quality
2. Train slot attention module on synthetic data, monitor guidance loss convergence
3. Evaluate on VOC test set, check mIoU and qualitative slot visualizations

## Open Questions the Paper Calls Out
- How does the quality of the guidance signal (semantic masks) from the diffusion model scale with the complexity and diversity of real-world datasets?
- Can GLASS be extended to handle instance-level segmentation rather than semantic class segmentation?
- How does the performance of GLASS compare to state-of-the-art weakly supervised and language-based segmentation methods on datasets with fewer training examples?

## Limitations
- Performance depends heavily on quality of pre-trained diffusion models and caption generators
- Evaluation limited to VOC and COCO datasets, may not generalize to more complex scenes
- Synthetic data generation may introduce biases favoring certain object types or scene configurations

## Confidence
- High Confidence: Relative improvements in mIoU scores (+35% on VOC, +10% on COCO) with extensive ablation studies
- Medium Confidence: FID score improvements for conditional image generation, subjective visual quality assessments
- Low Confidence: Assumption that caption-conditioned diffusion generation consistently produces images with one-to-one object-caption entity correspondence

## Next Checks
1. Evaluate GLASS on out-of-distribution datasets (ADE20K, LVIS) to test generalization beyond VOC and COCO
2. Conduct controlled experiments varying the quality of the caption generator (using different models like CLIP, BLIP-1, or human annotations)
3. Implement quantitative comparison between cross-attention-based semantic masks and alternative approaches (segmentation heads, saliency maps)