---
ver: rpa2
title: 'TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular
  and Textual Data'
arxiv_id: '2401.13223'
source_url: https://arxiv.org/abs/2401.13223
tags:
- question
- answer
- table
- pipeline
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of question answering (QA)
  over hybrid tabular and textual data, which requires discrete reasoning capabilities.
  The authors abstract a Step-wise Pipeline for tabular and textual QA, consisting
  of three key steps: Extractor, Reasoner, and Executor.'
---

# TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data

## Quick Facts
- arXiv ID: 2401.13223
- Source URL: https://arxiv.org/abs/2401.13223
- Authors: Fengbin Zhu, Ziyang Liu, Fuli Feng, Chao Wang, Moxin Li, Tat-Seng Chua
- Reference count: 25
- Primary result: TAT-LLM (7B) outperforms GPT-4 on FinQA, TAT-QA, and TAT-DQA benchmarks

## Executive Summary
This paper addresses the challenge of question answering over hybrid tabular and textual data that requires discrete reasoning capabilities. The authors propose a Step-wise Pipeline consisting of Extractor, Reasoner, and Executor components, and demonstrate its effectiveness on GPT-4. To address cost, latency, and security concerns with online LLMs, they develop TAT-LLM by fine-tuning LLaMA 2 on automatically generated training data following this pipeline. Experimental results show that TAT-LLM outperforms both previous fine-tuned models and large-scale LLMs like GPT-4 on three benchmark datasets.

## Method Summary
The authors abstract a Step-wise Pipeline for tabular and textual QA, decomposing the task into three key steps: Extractor identifies relevant information from context, Reasoner generates mathematical equations or logic rules, and Executor derives final answers. They initially validate this pipeline on GPT-4, demonstrating promising results. To create a specialized model, they fine-tune LLaMA 2 (7B, 13B, 70B) on automatically generated training data following the pipeline structure. They also add an External Executor component to handle arithmetic and logic execution, addressing reliability issues with in-model calculations. The model is evaluated on FinQA, TAT-QA, and TAT-DQA benchmarks using Exact Match and F1 metrics.

## Key Results
- TAT-LLM outperforms all baseline models including previous fine-tuned models and GPT-4 on FinQA, TAT-QA, and TAT-DQA benchmarks
- The smallest TAT-LLM (7B) model achieves significant improvements over GPT-4 on all three datasets
- Adding an External Executor component significantly improves final answer accuracy, particularly for arithmetic-heavy questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing QA into Extractor, Reasoner, and Executor steps improves discrete reasoning accuracy
- Mechanism: Breaking complex reasoning into specialized subtasks allows targeted optimization of each capability
- Core assumption: Complex reasoning tasks can be effectively decomposed into these three sequential steps
- Evidence anchors: The paper validates the pipeline on GPT-4 and shows performance improvements when fine-tuning follows this structure

### Mechanism 2
- Claim: Fine-tuning LLaMA 2 on automatically generated training data following the Step-wise Pipeline outperforms GPT-4
- Mechanism: Smaller models trained on task-specific data with structured intermediate outputs can match or exceed larger general models
- Core assumption: Task-specific fine-tuning with structured intermediate outputs provides better performance than few-shot prompting of general models
- Evidence anchors: Experimental results show TAT-LLM outperforms GPT-4 on all three benchmark datasets

### Mechanism 3
- Claim: Adding an External Executor to handle arithmetic and logic execution significantly improves final answer accuracy
- Mechanism: Offloading calculation tasks to a reliable external executor prevents errors from propagating through the reasoning chain
- Core assumption: LLMs are less reliable at arithmetic execution than at generating equations/logic rules
- Evidence anchors: The paper observes that trained models struggle with execution, leading to the addition of External Executor component

## Foundational Learning

- Concept: Step-wise decomposition of reasoning tasks
  - Why needed here: Discrete reasoning over tabular and textual data requires multiple distinct capabilities (extraction, reasoning, execution) that benefit from specialized handling
  - Quick check question: Can you identify which step would handle "extracting the values 11,386 and 10,353 from the table"?

- Concept: Structured intermediate outputs
  - Why needed here: Formatting model outputs as structured tables enables automatic evaluation and further processing
  - Quick check question: What format does the model output use to represent each step's result?

- Concept: Automatic data generation from expert annotations
  - Why needed here: Large-scale training data is needed for fine-tuning, but manual annotation is expensive
  - Quick check question: How does the paper generate training data from existing expert-annotated datasets?

## Architecture Onboarding

- Component map: Instruction template → LLaMA 2 model → External Executor → Answer formatter
- Critical path: Input (table, text, question) → Extractor → Reasoner → Executor → External Executor → Output
- Design tradeoffs: Model size vs. performance vs. computational cost; structured vs. free-form outputs; in-model vs. external execution
- Failure signatures: Incorrect evidence extraction, malformed equations, calculation errors, format violations
- First 3 experiments:
  1. Validate Step-wise Pipeline on GPT-4 with the instruction template on FinQA dataset
  2. Fine-tune LLaMA 2 (7B) following Step-wise Pipeline without External Executor on combined training data
  3. Add External Executor to previous model and compare performance on arithmetic-heavy questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TAT-LLM scale with the size of the training data, beyond what was tested in the paper?
- Basis in paper: The paper mentions training TAT-LLM on a combination of datasets and observing performance improvements, but does not explore scaling the training data size beyond this combination
- Why unresolved: The paper does not provide information on the relationship between training data size and model performance
- What evidence would resolve it: Conducting experiments with varying sizes of training data and analyzing the corresponding performance changes

### Open Question 2
- Question: How does the performance of TAT-LLM compare to other specialized models for different domains, such as healthcare or legal documents?
- Basis in paper: The paper focuses on the finance domain and does not compare TAT-LLM's performance to models specialized in other domains
- Why unresolved: The paper does not explore the generalizability of TAT-LLM to other domains
- What evidence would resolve it: Conducting experiments with TAT-LLM on datasets from different domains and comparing its performance to specialized models in those domains

### Open Question 3
- Question: How does the performance of TAT-LLM vary with different types of numerical reasoning tasks, such as statistical analysis or predictive modeling?
- Basis in paper: The paper focuses on discrete reasoning tasks, such as arithmetic calculations and comparisons, but does not explore other types of numerical reasoning tasks
- Why unresolved: The paper does not provide information on TAT-LLM's performance on a broader range of numerical reasoning tasks
- What evidence would resolve it: Conducting experiments with TAT-LLM on datasets involving different types of numerical reasoning tasks and analyzing its performance on these tasks

## Limitations

- The evaluation focuses heavily on synthetic or semi-synthetic datasets without clear validation on truly open-domain hybrid tabular-textual data
- The External Executor component is described but its implementation details and error rates are not fully specified
- The automatic data generation process relies on existing expert annotations but the quality and coverage of generated training data is unclear

## Confidence

- **High Confidence**: The Step-wise Pipeline architecture is well-defined and its individual components are clearly specified
- **Medium Confidence**: The claim that TAT-LLM outperforms GPT-4 on the three benchmark datasets, though the evaluation methodology appears sound
- **Low Confidence**: The scalability and generalization of the approach to real-world scenarios beyond the evaluated benchmarks

## Next Checks

1. **Generalization Test**: Evaluate TAT-LLM on a held-out dataset of real-world tabular-textual reasoning problems not used in training or fine-tuning, including questions that require cross-referencing multiple tables and text passages.

2. **Error Analysis**: Conduct detailed error analysis on the External Executor component, measuring its accuracy on different types of arithmetic and logical operations, and quantifying how execution errors propagate to final answer quality.

3. **Cost-Benefit Analysis**: Compare the total computational cost (fine-tuning time, inference latency, and API costs) of TAT-LLM versus few-shot GPT-4 prompting across the three benchmarks, including both training and deployment scenarios.