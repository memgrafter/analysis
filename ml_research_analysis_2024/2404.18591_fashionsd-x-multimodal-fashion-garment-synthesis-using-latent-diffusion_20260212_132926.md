---
ver: rpa2
title: 'FashionSD-X: Multimodal Fashion Garment Synthesis using Latent Diffusion'
arxiv_id: '2404.18591'
source_url: https://arxiv.org/abs/2404.18591
tags:
- diffusion
- arxiv
- stable
- fashion
- sketch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FashionSD-X, a novel pipeline using latent
  diffusion models to generate high-quality fashion garment images conditioned on
  multimodal inputs such as text and sketches. The approach employs Stable Diffusion
  LoRA fine-tuning and ControlNet to enhance generation fidelity.
---

# FashionSD-X: Multimodal Fashion Garment Synthesis using Latent Diffusion

## Quick Facts
- arXiv ID: 2404.18591
- Source URL: https://arxiv.org/abs/2404.18591
- Authors: Abhishek Kumar Singh; Ioannis Patras
- Reference count: 39
- Primary result: FashionSD-X outperforms vanilla Stable Diffusion in multimodal fashion garment synthesis using LoRA fine-tuning and ControlNet.

## Executive Summary
This paper proposes FashionSD-X, a novel pipeline using latent diffusion models to generate high-quality fashion garment images conditioned on multimodal inputs such as text and sketches. The approach employs Stable Diffusion LoRA fine-tuning and ControlNet to enhance generation fidelity. Two datasets, Multimodal Dress Code and VITON-HD, were extended by extracting garment sketches using adaptive thresholding and converted into Hugging Face format. Quantitative evaluation using FID, KID, and CLIP Score demonstrated that FashionSD-X outperforms vanilla Stable Diffusion in realism and coherence. A novel sketch similarity metric was introduced to measure adherence of generated images to input sketches. User studies with fashion design students confirmed the superior quality of generated garments. The results underscore the potential of diffusion models to revolutionize fashion design workflows.

## Method Summary
FashionSD-X extends two fashion datasets by extracting garment sketches using adaptive thresholding and converting them into Hugging Face format. The method involves fine-tuning Stable Diffusion with LoRA on the extended datasets for 15-10 epochs respectively, with batch size 4-2 and learning rate 1e-4. ControlNet is trained on extracted sketches for 5 epochs with batch size 4 and learning rate 1e-5. The LoRA fine-tuned model is then combined with ControlNet for sketch-guided generation. Quantitative evaluation is performed using FID, KID, CLIP Score, structural similarity (SSIM), and a novel sketch similarity metric (MSE between extracted and input sketches).

## Key Results
- FashionSD-X outperforms vanilla Stable Diffusion in FID, KID, and CLIP Score metrics for multimodal fashion garment synthesis.
- User studies with fashion design students confirm the superior quality of generated garments.
- The VITON-HD dataset, with fewer images and less diversity, resulted in inferior model performance compared to the Dress Code dataset.

## Why This Works (Mechanism)
FashionSD-X leverages the power of latent diffusion models to generate high-quality fashion garment images conditioned on multimodal inputs. The use of LoRA fine-tuning allows for efficient adaptation of the pre-trained Stable Diffusion model to the fashion domain, while ControlNet provides precise control over the generation process using sketches. The combination of these techniques enables the model to produce realistic and coherent fashion garments that closely adhere to the input sketches and text descriptions.

## Foundational Learning
- **Latent Diffusion Models**: Generative models that operate in the latent space of an autoencoder, enabling efficient and high-quality image synthesis. Why needed: Latent diffusion models provide a powerful framework for generating realistic images while being computationally efficient.
- **LoRA Fine-tuning**: A parameter-efficient fine-tuning technique that adapts pre-trained models to new tasks by injecting low-rank adaptation matrices. Why needed: LoRA allows for efficient adaptation of Stable Diffusion to the fashion domain without the need for full fine-tuning.
- **ControlNet**: A conditioning mechanism that enables precise control over the generation process using additional input modalities, such as sketches. Why needed: ControlNet allows the model to generate fashion garments that closely adhere to the input sketches, enhancing the control and fidelity of the generated images.
- **Hugging Face Dataset Format**: A standardized format for storing and loading datasets, facilitating easy integration with machine learning frameworks. Why needed: Converting the fashion datasets into Hugging Face format enables seamless integration with the Stable Diffusion and ControlNet models.
- **Adaptive Thresholding**: An image processing technique that automatically adjusts the threshold value based on the local image characteristics, enabling effective sketch extraction. Why needed: Adaptive thresholding is used to extract garment sketches from the fashion images, providing the necessary input for the ControlNet conditioning.
- **FID, KID, and CLIP Score**: Quantitative metrics used to evaluate the realism, diversity, and semantic coherence of generated images. Why needed: These metrics provide objective measures of the generated fashion garments' quality and adherence to the input conditions.

## Architecture Onboarding

### Component Map
Stable Diffusion -> LoRA Fine-tuning -> ControlNet -> Sketch-Guided Generation

### Critical Path
The critical path for FashionSD-X involves extending the fashion datasets with sketches, fine-tuning Stable Diffusion using LoRA, training ControlNet on the extracted sketches, and combining the LoRA fine-tuned model with ControlNet for sketch-guided generation.

### Design Tradeoffs
- LoRA fine-tuning vs. full fine-tuning: LoRA provides a parameter-efficient alternative to full fine-tuning, reducing computational costs while maintaining performance.
- ControlNet conditioning vs. direct sketch input: ControlNet offers more precise control over the generation process compared to directly inputting sketches into the model.

### Failure Signatures
- Overfitting: If the model is trained for too many epochs or the dataset is imbalanced, it may overfit to specific fashion garments or styles, resulting in poor generalization.
- Poor sketch adherence: If the ControlNet conditioning is not properly tuned, the generated images may not closely adhere to the input sketches, leading to a mismatch between the intended and generated garments.

### First Experiments
1. Fine-tune Stable Diffusion with LoRA on the Dress Code dataset for 15 epochs, using a batch size of 4 and learning rate of 1e-4.
2. Train ControlNet on the extracted sketches from the Dress Code dataset for 5 epochs, using a batch size of 4 and learning rate of 1e-5.
3. Combine the LoRA fine-tuned model with ControlNet and generate fashion garments using both text prompts and sketches as input.

## Open Questions the Paper Calls Out
- How does the proposed sketch similarity metric correlate with human perception of sketch adherence in generated images?
- What is the impact of dataset size and diversity on the performance of the FashionSD-X model?
- How does the FashionSD-X model perform on fashion garment generation tasks outside the scope of the training datasets?

## Limitations
- The exact implementation details of the novel sketch similarity metric are not fully specified.
- Specific hyperparameters for ControlNet training are not clearly stated.
- The generalizability of the approach to other fashion datasets beyond Dress Code and VITON-HD is not demonstrated.

## Confidence
- High confidence: The core methodology of using LoRA fine-tuning and ControlNet for multimodal fashion synthesis is well-established and the quantitative results are promising.
- Medium confidence: The sketch similarity metric and user study results provide additional validation, but the lack of implementation details and specific hyperparameters introduces some uncertainty.
- Low confidence: The long-term impact and generalizability of FashionSD-X to other fashion datasets and real-world applications are yet to be determined.

## Next Checks
1. Conduct ablation studies to assess the impact of specific hyperparameters on the performance of FashionSD-X, such as the number of LoRA and ControlNet training epochs, learning rates, and batch sizes.
2. Evaluate the generalizability of FashionSD-X by testing its performance on additional fashion datasets beyond Dress Code and VITON-HD, such as DeepFashion or ModaNet.
3. Perform a comprehensive user study with a larger and more diverse group of fashion design professionals to validate the practical utility and user experience of FashionSD-X in real-world design workflows.