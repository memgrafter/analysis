---
ver: rpa2
title: Generating Music with Structure Using Self-Similarity as Attention
arxiv_id: '2406.15647'
source_url: https://arxiv.org/abs/2406.15647
tags:
- music
- piece
- structure
- attention
- sing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating music with long-term
  structural coherence using deep learning. It proposes a novel attention mechanism
  that leverages self-similarity matrices (SSMs) as input to guide the generation
  of music with desired structural patterns.
---

# Generating Music with Structure Using Self-Similarity as Attention
## Quick Facts
- arXiv ID: 2406.15647
- Source URL: https://arxiv.org/abs/2406.15647
- Authors: Sophia Hager; Kathleen Hablutzel; Katherine M. Kinnaird
- Reference count: 7
- Key outcome: SING (Similarity Incentivized Neural Generator) with SSM-based attention significantly outperforms LSTM alone in replicating musical structure, with human evaluation showing higher interest and likeability scores.

## Executive Summary
This paper addresses the challenge of generating music with long-term structural coherence using deep learning. The authors propose a novel attention mechanism that leverages self-similarity matrices (SSMs) as input to guide the generation of music with desired structural patterns. They introduce SING, a system that combines an LSTM layer with their proposed SSM-based attention layer, trained on the MAESTRO dataset using variable-length batching. The results show that SING significantly outperforms a basic LSTM model in replicating the structure of input SSMs, with human evaluation indicating higher perceived interest and likeability in the generated music.

## Method Summary
The paper proposes SING (Similarity Incentivized Neural Generator), a two-layer network combining an LSTM with an SSM-based attention mechanism. The system is trained on the MAESTRO dataset using variable-length batching to handle pieces of varying lengths. The attention layer uses the input SSM itself as attention weights, applying sparsemax normalization to retrieve weighted sums of past timesteps. This approach aims to preserve long-term structural information during music generation, with the model trained to minimize both binary cross-entropy loss and SSM mean squared error.

## Key Results
- SING achieved a mean squared error of 1.57 on SSM replication, compared to 1.96 for LSTM and random noise
- Human evaluation showed SING-generated music was perceived as more interesting (4.25 vs 3.70) and likeable (4.13 vs 3.73) than the comparison model
- The variable-length batching method effectively balanced batch sizes while minimizing edits to each piece

## Why This Works (Mechanism)
### Mechanism 1
- Claim: The proposed attention layer uses the SSM itself as attention weights, directly encoding structural similarity from the template piece into the generation process.
- Mechanism: For each timestep t, the network retrieves row t of the input SSM, applies sparsemax normalization, and computes a weighted sum of past timesteps using these normalized similarities as weights. This weighted sum is then combined with the LSTM output via a linear transformation.
- Core assumption: Self-similarity matrices provide a reliable and interpretable representation of musical structure that can be directly transferred to guide generation.
- Evidence anchors:
  - [abstract] "propose an attention layer that uses a novel approach applying user-supplied self-similarity matrices to previous time steps"
  - [section 3.2.3] "The key idea of SING is to use a SSM to find the attention weights for each timestep"
  - [corpus] No direct corpus evidence; weak external validation
- Break condition: If the SSM fails to capture meaningful structure (e.g., in highly atonal or non-repetitive music), the attention weights become arbitrary and degrade generation quality.

### Mechanism 2
- Claim: Variable-length batching allows training on pieces of diverse lengths without excessive truncation or padding, preserving long-term structural information.
- Mechanism: Pieces are assigned to one of 16 standard lengths based on log-distance from preselected lengths. This minimizes length adjustments while enabling efficient batching.
- Core assumption: Standard batching with fixed lengths harms structure preservation; this method better maintains structural fidelity.
- Evidence anchors:
  - [section 3.1.2] "Our proposed variable batching method balances the size of each batch...while also minimizing the amount of edits applied to each piece"
  - [section 4.1] "The variable-length batches used to train SING present a data processing solution for musical datasets with large variation in piece length"
  - [corpus] No direct corpus evidence; weak external validation
- Break condition: If the standard length selection poorly represents the dataset distribution, excessive truncation or padding still occurs.

### Mechanism 3
- Claim: Combining LSTM outputs with SSM-based attention vectors allows the model to generate sequences that follow the template structure while maintaining local coherence.
- Mechanism: The LSTM predicts next-step probabilities; the attention layer computes a similarity-weighted sum from the SSM; these are concatenated and linearly transformed to produce the final output distribution.
- Core assumption: The LSTM layer provides short-term coherence while the SSM attention enforces long-term structure.
- Evidence anchors:
  - [section 3.2.3] "we apply a linear transformation to a and the LSTM output vector z to get the final distribution vector"
  - [section 4.3] "SING scored significantly higher than the comparison model on interest and likeability"
  - [corpus] No direct corpus evidence; weak external validation
- Break condition: If the LSTM predictions are too noisy, the SSM attention cannot correct them, leading to structural mismatches.

## Foundational Learning
- Concept: Self-similarity matrices (SSMs)
  - Why needed here: SSMs encode pairwise similarity between timesteps, capturing repeated patterns and structure in music. They serve as the template for guiding generation.
  - Quick check question: What does a high value in the SSM indicate about two timesteps?

- Concept: Attention mechanisms in sequence models
  - Why needed here: Attention allows the model to focus on relevant past timesteps rather than treating all equally, enabling long-term structure preservation.
  - Quick check question: How does sparsemax differ from softmax in attention weight computation?

- Concept: Variable-length batching
  - Why needed here: Traditional fixed-length batching forces truncation or padding, potentially destroying long-term structure. This method preserves it.
  - Quick check question: Why is log-distance used instead of linear distance when assigning pieces to standard lengths?

## Architecture Onboarding
- Component map: Input MIDI sequence → Piano roll representation → SSM computation → LSTM layer (128 units) → SSM-based attention layer with sparsemax → Linear transformation → Output probability distribution

- Critical path:
  1. Preprocess MIDI → piano roll → SSM
  2. Train LSTM + attention using variable-length batches
  3. During generation, apply SSM attention at each timestep
  4. Sample from output distribution (top-50, max 3 notes)

- Design tradeoffs:
  - LSTM vs. Transformer: LSTMs are faster for long sequences but less expressive
  - Fixed vs. variable batching: Fixed is simpler but harms structure; variable is complex but preserves it
  - Binary vs. velocity-based piano roll: Binary simplifies training but loses dynamic information

- Failure signatures:
  - SSM MSE high → attention not aligning with template structure
  - Generated music lacks repetition → LSTM not learning local patterns
  - Training diverges → learning rate too high or batch size too large

- First 3 experiments:
  1. Train SING on synthetic SSMs (like Figure 3) to verify attention replication
  2. Compare variable-length batching vs. fixed-length on a small dataset
  3. Ablate attention layer to confirm its contribution to structure replication

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the proposed SSM-based attention mechanism scale to longer musical pieces, such as full-length classical compositions or extended jazz improvisations?
- Basis in paper: [explicit] The authors mention that with more computational power, the variable-length batching method could enable training on music even longer than 700 beats, for the replication of structure on extremely long scales.
- Why unresolved: The current implementation and evaluation only demonstrate the mechanism's effectiveness on pieces up to 700 beats (approximately 3 minutes). The paper acknowledges the need for more computational resources to scale the approach to longer compositions.
- What evidence would resolve it: A demonstration of the SSM-based attention mechanism's effectiveness on full-length classical compositions or extended jazz improvisations, showing that it can maintain structural coherence over much longer timescales than the current 700-beat limit.

### Open Question 2
- Question: How would the performance of the SSM-based attention mechanism compare to that of Transformer-based models when both are applied to music generation tasks?
- Basis in paper: [explicit] The authors suggest that Transformers can often achieve state-of-the-art results in generation and propose it would be interesting to create a system that uses Transformers as a generative model for their proposed attention layer.
- Why unresolved: The current implementation uses an LSTM as the generative model. While the authors speculate that combining their attention mechanism with Transformers could potentially address limitations seen in the user evaluation, they have not yet implemented or tested this combination.
- What evidence would resolve it: A direct comparison between the SSM-based attention mechanism combined with both LSTM and Transformer generative models, evaluating and comparing their performance in terms of musical quality, structural coherence, and computational efficiency.

### Open Question 3
- Question: How does the SSM-based attention mechanism perform when applied to music genres other than classical, such as pop, rock, or electronic music?
- Basis in paper: [explicit] The authors note that the MAESTRO dataset used for training consists of classical music, and they acknowledge that the model is limited by the reliance on this specific dataset.
- Why unresolved: The evaluation and discussion in the paper focus exclusively on the mechanism's performance with classical music. The authors suggest that training on beat-annotated or non-binarized data from other genres could improve the quality of the output music, but they have not tested this hypothesis.
- What evidence would resolve it: An evaluation of the SSM-based attention mechanism's performance when trained on and applied to music datasets from genres other than classical, such as pop, rock, or electronic music. This would demonstrate the mechanism's versatility and potential for cross-genre application.

## Limitations
- The variable-length batching method, while described, lacks empirical validation against standard batching on the same task
- Human evaluation results are promising but based on a single comparison model without reporting statistical significance for all dimensions
- The SSM-based attention mechanism's effectiveness is only demonstrated on pieces up to 700 beats, with scalability to longer compositions remaining untested

## Confidence
- **High**: SSM-based attention can replicate template structure better than LSTM alone (Section 4.3, quantitative MSE results)
- **Medium**: Variable-length batching preserves long-term structure better than fixed batching (Section 3.1.2, claimed but not empirically validated)
- **Low**: SING-generated music is more interesting and likeable than comparison model (Section 4.3, human evaluation without full statistical analysis)

## Next Checks
1. **Ablation study**: Train and evaluate three models—LSTM only, LSTM + fixed SSM attention weights, LSTM + proposed sparsemax SSM attention—to isolate the effect of the sparsemax normalization.

2. **Batching comparison**: Train identical models using fixed-length batching (truncate/pad to 4096 timesteps) vs. variable-length batching to quantify structural preservation differences.

3. **Statistical validation**: Re-analyze human evaluation data with proper statistical tests (e.g., paired t-tests) to confirm significance of differences in interest, proficiency, expressiveness, and likeability scores.