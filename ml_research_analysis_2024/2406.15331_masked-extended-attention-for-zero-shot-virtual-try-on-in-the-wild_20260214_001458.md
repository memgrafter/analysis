---
ver: rpa2
title: Masked Extended Attention for Zero-Shot Virtual Try-On In The Wild
arxiv_id: '2406.15331'
source_url: https://arxiv.org/abs/2406.15331
tags:
- image
- garment
- attention
- diffusion
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MaX4Zero, a zero-shot, training-free method
  for virtual try-on in the wild, which aims to replace a garment in an image with
  one from another while preserving person and garment characteristics. The method
  leverages a diffusion model with extended attention to transfer image information
  from reference to target images.
---

# Masked Extended Attention for Zero-Shot Virtual Try-On In The Wild

## Quick Facts
- arXiv ID: 2406.15331
- Source URL: https://arxiv.org/abs/2406.15331
- Authors: Nadav Orzech; Yotam Nitzan; Ulysse Mizrahi; Dov Danon; Amit H. Bermano
- Reference count: 19
- Primary result: Introduces MaX4Zero, a zero-shot, training-free method for virtual try-on that achieves LPIPS of 0.643 and DreamSim of 0.518 for garment identity preservation

## Executive Summary
This paper presents MaX4Zero, a novel zero-shot method for virtual try-on that transfers garments between images without requiring training or fine-tuning. The method addresses two major challenges: preventing texture sticking and avoiding background leakage from the reference image. By combining initial garment warping using deep features with a Masked Extended Attention mechanism in a diffusion model, MaX4Zero achieves state-of-the-art results in preserving both garment identity and person characteristics while maintaining background integrity.

## Method Summary
MaX4Zero operates in two stages: initial registration and consistent inpainting. First, it warps the reference garment to match the target person using deep features extracted from the diffusion model's transformer blocks, followed by double-mask inpainting to fill gaps. Then, it applies Masked Extended Attention (MEA) in the U-Net decoder to transfer fine details from the reference to the target while preventing background leakage through careful masking. The method leverages a pre-trained diffusion model (Stable Diffusion) without any fine-tuning, making it truly zero-shot.

## Key Results
- LPIPS score of 0.643 for garment identity preservation
- DreamSim score of 0.518 for garment identity preservation
- FID score of 66.27 for overall image fidelity
- Superior performance compared to state-of-the-art methods (LaDI-VTON, StableVITON, IP-Adapter, AnyDoor)
- User study with 57 participants confirming better garment attribute preservation and image fidelity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Masked Extended Attention (MEA) enables zero-shot garment transfer without catastrophic forgetting.
- **Mechanism**: MEA replaces standard self-attention with an expanded attention block that includes keys/values from the reference garment, while queries remain fixed to the target. A masking layer selectively allows only garment-relevant information to propagate, preventing background leakage.
- **Core assumption**: The diffusion prior already encodes rich garment knowledge; no fine-tuning is needed if attention is properly redirected.
- **Evidence anchors**:
  - [abstract] "The method employs extended attention to transfer image information from reference to target images, overcoming two significant challenges."
  - [section 3.3] Formal MEA definition shows query-key-value expansion with masking to block background leakage.
  - [corpus] No strong direct evidenceâ€”this is the paper's primary claim.
- **Break condition**: If the diffusion prior lacks garment-specific knowledge, or if the mask fails to isolate the garment region, MEA will leak background or fail to preserve identity.

### Mechanism 2
- **Claim**: Initial registration warps the reference garment to match the target, mitigating texture sticking.
- **Mechanism**: Deep features extracted from the diffusion model's transformer blocks are used to compute a bi-linear correspondence matrix. Moving-Least-Squares (MLS) deformation warps the reference garment onto the target figure, aligning spatial features before fine detail transfer.
- **Core assumption**: Texture sticking arises because spatial features in the reference are misaligned semantically; warping resolves this by grounding the reference spatially.
- **Evidence anchors**:
  - [abstract] "We first initially warp the reference garment over the target human using deep features, alleviating 'texture sticking'."
  - [section 3.2] Detailed explanation of deep-feature-based warping and fringe inpainting.
  - [corpus] No direct corpus support; based on internal observations.
- **Break condition**: If the correspondence matrix is inaccurate (e.g., due to pose mismatch), warping will distort the garment and reduce realism.

### Mechanism 3
- **Claim**: Double-mask inpainting fills fringe gaps without altering the background.
- **Mechanism**: A dilated mask predicts the diffused noise at each timestep, but only the thin mask is applied to update the image. This ensures that small fringe regions are filled using the diffusion model's learned priors while preserving the surrounding background.
- **Core assumption**: Small mask inpainting is prone to background alteration; using a dilated mask for noise prediction but a thin mask for application balances fidelity and context.
- **Evidence anchors**:
  - [section 3.2] Explicit description of the double-mask strategy and its rationale.
  - [abstract] Mentions "initial registration" and "fringe assignment" as part of the pipeline.
  - [corpus] No corpus corroboration; derived from paper description.
- **Break condition**: If the dilated mask is too large, it may predict irrelevant background content; if too small, it may fail to provide sufficient context for inpainting.

## Foundational Learning

- **Concept: Diffusion models and denoising U-Net architecture**
  - Why needed here: MaX4Zero relies on a pre-trained diffusion model (Stable Diffusion) and manipulates its attention layers for garment transfer. Understanding the forward and reverse processes, latent space encoding, and cross-attention vs. self-attention is essential.
  - Quick check question: What is the difference between cross-attention and self-attention in a diffusion U-Net, and how does each affect the generation process?

- **Concept: Extended and masked attention mechanisms**
  - Why needed here: MEA is the core innovation; engineers must understand how to expand attention blocks to include cross-image information and how masking controls data flow. This includes familiarity with softmax attention maps and feature projection.
  - Quick check question: How does masking the attention map in MEA prevent background leakage from the reference image?

- **Concept: Deep feature-based image alignment and MLS deformation**
  - Why needed here: The initial registration step depends on extracting deep features and computing correspondences for non-rigid warping. Knowledge of feature extraction from transformer blocks and bilinear correspondence matrices is required.
  - Quick check question: Why does the paper use deep features from the diffusion model's transformer blocks for warping instead of pre-trained vision backbones?

## Architecture Onboarding

- **Component map**: Target person image -> Deep feature extraction -> Correspondence matrix -> MLS warping -> Double-mask inpainting -> MEA layers -> Generated image

- **Critical path**:
  1. Extract deep features from both images
  2. Warp garment using correspondence matrix
  3. Fill fringes with double-mask inpainting
  4. Apply MEA-based inpainting for fine details
  5. Generate final image

- **Design tradeoffs**:
  - Zero-shot vs. fine-tuned: Avoids catastrophic forgetting but relies heavily on the diffusion prior's generality
  - Masking granularity: Coarse masks risk leakage; fine masks risk missing background; MEA balances this
  - Attention scale: Higher MEA guidance increases detail transfer but risks over-propagation; lower guidance preserves context but may lose identity

- **Failure signatures**:
  - Texture sticking: Misaligned garment features appear in the output
  - Background leakage: Reference background shows through in generated garment
  - Identity loss: Person's body or posture is altered during inpainting
  - Fringes visible: Gaps between warped garment and target figure remain unfilled

- **First 3 experiments**:
  1. **Ablation: Initial registration only** - Warp garment without MEA, observe texture sticking and background leakage
  2. **Ablation: MEA only** - Apply MEA without registration, observe misaligned features and contextual errors
  3. **Full pipeline test** - Run complete MaX4Zero pipeline on in-the-wild images, compare against baselines (LaDI-VTON, StableVITON, IP-Adapter, AnyDoor) using LPIPS, DreamSim, and FID

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MaX4Zero scale with the size and diversity of the dataset used for the Initial Registration stage?
- Basis in paper: [inferred] The paper mentions using datasets like DressCode and VITON-HD for evaluation, but does not explore the impact of dataset size or diversity on the performance of the Initial Registration stage.
- Why unresolved: The paper focuses on demonstrating the effectiveness of MaX4Zero compared to other methods, but does not investigate how the quality of the Initial Registration stage is affected by the characteristics of the dataset used.
- What evidence would resolve it: Conducting experiments with varying sizes and diversities of datasets for the Initial Registration stage and measuring the impact on the overall performance of MaX4Zero.

### Open Question 2
- Question: Can the Masked Extended Attention (MEA) mechanism be further improved by incorporating additional information from the reference image, such as semantic segmentation or pose estimation?
- Basis in paper: [explicit] The paper introduces MEA as a key component for transferring fine details from the reference image, but does not explore the potential benefits of incorporating additional information.
- Why unresolved: While MEA is effective in transferring details, it relies solely on the attention mechanism to determine relevant information. Incorporating additional cues could potentially enhance the quality of the transferred details.
- What evidence would resolve it: Implementing variations of MEA that incorporate semantic segmentation or pose estimation information and comparing their performance against the original MEA mechanism.

### Open Question 3
- Question: How does the performance of MaX4Zero compare to other zero-shot methods for image editing tasks beyond virtual try-on?
- Basis in paper: [inferred] The paper focuses on the specific task of virtual try-on and compares MaX4Zero to other methods in this domain. However, it does not explore how it performs on other image editing tasks.
- Why unresolved: While MaX4Zero is designed for virtual try-on, the underlying principles of using extended attention and masking could potentially be applied to other image editing tasks. Evaluating its performance in these domains would provide insights into its generalizability.
- What evidence would resolve it: Applying MaX4Zero to other image editing tasks, such as object removal, style transfer, or image completion, and comparing its performance to other zero-shot methods in those domains.

## Limitations
- Reliance on Stable Diffusion's prior knowledge without fine-tuning may restrict performance on highly novel garment styles or extreme poses
- Zero-shot nature means generalization depends entirely on the base diffusion model's training data distribution
- Paper does not provide detailed ablation studies on double-mask inpainting strategy parameters

## Confidence
- **High Confidence**: The core MEA mechanism for preventing background leakage (supported by formal definition and visual examples)
- **Medium Confidence**: Initial registration using deep features for warping (method described but effectiveness depends on feature quality)
- **Medium Confidence**: Quantitative results (LPIPS, DreamSim, FID scores provided, but user study methodology could be more detailed)

## Next Checks
1. **Ablation Study**: Compare MaX4Zero with and without the initial registration step on challenging pose variations to quantify the contribution of deep feature warping
2. **Cross-Dataset Evaluation**: Test the method on garments and poses not present in the training data of Stable Diffusion to assess true zero-shot capability
3. **Mask Sensitivity Analysis**: Systematically vary the dilated mask size in the double-mask inpainting strategy to identify optimal parameters and failure modes