---
ver: rpa2
title: Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions
arxiv_id: '2410.13788'
source_url: https://arxiv.org/abs/2410.13788
tags:
- clarifying
- answer
- question
- questions
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to teach large language models to ask
  clarifying questions when faced with ambiguous user queries. Current LLMs often
  presuppose a single interpretation, frustrating users.
---

# Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions

## Quick Facts
- arXiv ID: 2410.13788
- Source URL: https://arxiv.org/abs/2410.13788
- Authors: Michael J. Q. Zhang; W. Bradley Knox; Eunsol Choi
- Reference count: 35
- Primary result: Double-turn preference training improves F1 score by 5% and helps models learn when to ask clarifying questions

## Executive Summary
This paper addresses a key limitation in large language models: their tendency to presuppose single interpretations of ambiguous user queries rather than asking clarifying questions. The authors attribute this problem to preference data labeling that only considers single-turn interactions, making it difficult for annotators to assess the long-term utility of clarifying questions. Their solution involves assigning preference labels by simulating future conversation turns, allowing models to learn clarifying questions that can be tailored to different user interpretations. The approach is evaluated on open-domain QA datasets with multiple annotations, demonstrating both improved answer accuracy and better judgment about when clarification is necessary.

## Method Summary
The authors propose a double-turn preference labeling approach where they simulate future conversation turns to assign preference labels for training. First, they generate clarification questions using GPT-4, creating paired datasets of queries, clarification questions, clarifying answers, and gold answers. They then train a clarification question generation model using supervised fine-tuning on this synthetic dataset. Finally, they apply preference optimization using double-turn preferences, comparing simulated outcomes after user responses to generate training labels. This contrasts with standard single-turn preference methods where models are trained only on direct answers without considering the value of clarification questions.

## Key Results
- Double-turn preference training achieves a 5% improvement in F1 score compared to standard single-turn preference methods
- The approach enables models to judiciously decide when to ask clarifying questions, improving accuracy by 3% over existing methods
- Training with double-turn preferences consistently improves QA performance across three base LLMs and can help recover correct answers even for unambiguous queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-turn preference labeling causes LLMs to presuppose interpretations because annotators can't evaluate the utility of clarifying questions without seeing future turns.
- Mechanism: Standard preference data collection only considers single-turn interactions, making it difficult for annotators to assess whether a clarifying question would lead to a better outcome in subsequent turns. This leads to a bias where complete but presumptuous answers are preferred over incomplete clarifying questions.
- Core assumption: Annotators lack sufficient context to evaluate the long-term utility of clarifying questions when only shown single-turn interactions.
- Evidence anchors:
  - [abstract] "We speculate this is caused by current preference data labeling practice, where LLM responses are evaluated only on their prior contexts."
  - [section 3.1] "In standard preference data collection, annotators are given a conversation history and are tasked with ranking options for the next assistant turn (Bai et al., 2022; Wang et al., 2024). These annotation schemes only consider preferences over single-turns of interaction, making it difficult for annotators to assess the utility of a clarifying question."
  - [corpus] Weak - related papers discuss conversational follow-up queries and user satisfaction but don't directly address the single-turn preference labeling problem.
- Break condition: If annotators can accurately predict future conversation outcomes from single-turn preferences alone.

### Mechanism 2
- Claim: Double-turn preference labeling allows models to learn clarifying questions that are tailored to different user interpretations by simulating future conversation turns.
- Mechanism: By simulating user responses to clarifying questions and observing the subsequent LLM responses, the system can assign preference labels based on whether the completed interaction successfully fulfilled the request. This allows models to learn which clarifying questions lead to better outcomes for different user interpretations.
- Core assumption: Simulating future conversation turns accurately reflects how real users would respond to clarifying questions.
- Evidence anchors:
  - [abstract] "To address this, we assign preference labels by simulating their expected outcomes in future turns. This allows LLMs to learn to ask clarifying questions when it can generate responses that are tailored to each user interpretation in future turns."
  - [section 3.1] "In our annotation scheme (depicted in Figure 2), annotators are provided an input query x with several candidate clarifying question q and direct-answer responses ŷ. They respond to each clarifying question by providing the clarifying answer that corresponds to their interpretation."
  - [corpus] Moderate - related papers discuss asking clarifying questions for preference elicitation but don't specifically address the double-turn simulation approach.
- Break condition: If the simulated user responses don't accurately represent real user behavior.

### Mechanism 3
- Claim: Training with double-turn preferences improves model performance on both ambiguous and unambiguous queries by helping models recover correct answers through clarification.
- Mechanism: The double-turn training approach teaches models to judiciously determine when clarification is necessary, leading to better performance on ambiguous queries while also improving performance on unambiguous queries by helping models recover correct answers that might otherwise be missed.
- Core assumption: Models can learn to distinguish between queries that need clarification and those that don't through double-turn preference training.
- Evidence anchors:
  - [abstract] "We further demonstrate that our method can be used to train models to judiciously determine when to ask clarifying questions, directly answering the question when clarification is unnecessary."
  - [section 5] "Our experimental results show that training systems with double-turn preferences outperforms training with standard preferences annotation methods (Zhu et al., 2023), resulting in consistent 4-5% improvement in F1 score over three base LLMs."
  - [corpus] Weak - related papers discuss conversation length impact on user satisfaction but don't specifically address how clarification helps with unambiguous queries.
- Break condition: If the model fails to learn when clarification is actually necessary versus unnecessary.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper builds on RLHF training pipelines, modifying them to incorporate double-turn preferences instead of single-turn preferences.
  - Quick check question: What is the main difference between single-turn and double-turn preference labeling in RLHF?

- Concept: Preference optimization methods (e.g., DPO)
  - Why needed here: The paper uses Direct Preference Optimization (DPO) to train models using the double-turn preference labels generated through simulation.
  - Quick check question: How does DPO differ from traditional RLHF approaches in terms of preference label usage?

- Concept: User simulation for preference generation
  - Why needed here: The paper uses simulated user interactions to generate preference labels for training, as collecting human preferences for every possible clarifying question would be too expensive.
  - Quick check question: What are the potential risks of using simulated users instead of real human annotators for preference generation?

## Architecture Onboarding

- Component map: Base LLM → SFT training on clarifying questions → User simulator model → Double-turn preference generation → DPO fine-tuning → Clarify-or-answer model
- Critical path: User query → Model generates response (clarifying question or direct answer) → If clarifying question, user simulator generates clarifying answer → Model generates final answer → Preference labels generated based on final answer quality
- Design tradeoffs: Double-turn training improves performance but increases computational cost due to the need to simulate additional conversation turns; separate models for clarification and answering versus joint models
- Failure signatures: Model always asks clarifying questions even when unnecessary; model fails to generate useful clarifying questions; simulated user responses don't accurately represent real user behavior
- First 3 experiments:
  1. Compare single-turn vs. double-turn preference training on a small dataset to verify the core mechanism
  2. Test different methods for ranking clarifying questions (likelihood, match, RM) to find the most effective approach
  3. Evaluate the impact of using human-annotated vs. model-predicted answer sets for generating training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multi-turn interactions perform compared to the single or double-turn interactions studied in this paper?
- Basis in paper: [inferred] The authors note that they only consider one or two turn interactions and suggest that future work might explore extending their framework to accommodate general multi-turn interactions.
- Why unresolved: The paper explicitly limits its scope to one or two turn interactions, leaving the performance of multi-turn interactions unexplored.
- What evidence would resolve it: Conducting experiments with multi-turn interactions and comparing their performance metrics (efficiency and effectiveness) against the single and double-turn interactions studied in this paper.

### Open Question 2
- Question: How would incorporating dialogue acts beyond clarification and direct answers impact the performance of the model?
- Basis in paper: [explicit] The authors mention that their systems do not model dialogue acts outside of predicting a single answer or asking a clarifying question, suggesting that other dialogue acts might be more appropriate in certain situations.
- Why unresolved: The paper focuses on two specific dialogue acts, leaving the impact of incorporating other dialogue acts unexplored.
- What evidence would resolve it: Experimenting with models that incorporate a wider range of dialogue acts and evaluating their performance on the same metrics used in this paper.

### Open Question 3
- Question: How does the performance of the model vary when trained on datasets with different levels of ambiguity?
- Basis in paper: [inferred] The authors use datasets with varying levels of ambiguity (NQ-Open and AmbigQA) and observe differences in performance, suggesting that the level of ambiguity in the training data might impact the model's ability to handle ambiguity.
- Why unresolved: The paper does not explicitly study the impact of training data ambiguity on model performance.
- What evidence would resolve it: Training models on datasets with varying levels of ambiguity and comparing their performance on both ambiguous and unambiguous queries.

## Limitations
- The approach relies heavily on synthetic data generation, which may introduce biases not fully characterized
- The paper doesn't address potential user fatigue from clarification questions or provide analysis of when clarification becomes counterproductive
- The evaluation focuses on answer accuracy but doesn't measure user satisfaction or interaction efficiency beyond simple turn counts

## Confidence
- Confidence: Medium - The paper demonstrates measurable improvements (5% F1 gain for double-turn preferences) but relies heavily on synthetic data generation
- Confidence: Medium - The paper doesn't address potential user fatigue from clarification questions or provide analysis of when clarification becomes counterproductive
- Confidence: Low - The paper doesn't explore edge cases such as malicious or nonsensical user responses to clarifying questions

## Next Checks
1. **Real User Validation**: Conduct a user study comparing responses from the double-turn trained model against single-turn and baseline models, measuring not just answer accuracy but user satisfaction, perceived helpfulness, and interaction efficiency.

2. **Stress Testing with Adversarial Inputs**: Evaluate model behavior when given malformed or intentionally confusing user responses to clarifying questions, testing robustness and safety boundaries of the clarification approach.

3. **Generalization Across Domains**: Test the double-turn preference training approach on non-QA tasks such as task-oriented dialogue or recommendation systems to assess whether the benefits extend beyond open-domain QA scenarios.