---
ver: rpa2
title: 'In-Context Former: Lightning-fast Compressing Context for Large Language Model'
arxiv_id: '2406.13618'
source_url: https://arxiv.org/abs/2406.13618
tags:
- context
- ic-former
- tokens
- digest
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces In-Context Former (IC-Former), a method to\
  \ compress long input contexts for large language models (LLMs) to reduce high inference\
  \ costs. Unlike previous methods that rely on the LLM\u2019s self-attention mechanism,\
  \ IC-Former uses a cross-attention mechanism and learnable digest tokens to condense\
  \ information from contextual embeddings."
---

# In-Context Former: Lightning-fast Compressing Context for Large Language Model

## Quick Facts
- **arXiv ID**: 2406.13618
- **Source URL**: https://arxiv.org/abs/2406.13618
- **Reference count**: 19
- **Primary result**: IC-Former achieves 68-112x faster compression with linear time complexity while maintaining >90% baseline performance

## Executive Summary
This paper introduces In-Context Former (IC-Former), a method to compress long input contexts for large language models (LLMs) by replacing expensive self-attention mechanisms with efficient cross-attention. Unlike previous methods that rely on the LLM's self-attention for compression, IC-Former uses learnable digest tokens and cross-attention to directly condense information from contextual embeddings. The approach achieves linear time complexity O(kn), significantly reducing compression costs from 1/32 of the floating-point operations compared to baselines. Experiments demonstrate that IC-Former improves processing speed by 68 to 112 times while maintaining over 90% of baseline performance on evaluation metrics.

## Method Summary
IC-Former compresses long contexts by using cross-attention and learnable digest tokens to condense information from contextual embeddings. The model consists of 3 transformer layers with cross-attention blocks, where digest tokens act as queries and concatenated context+digest tokens serve as keys/values. This architecture avoids the quadratic complexity of self-attention while still allowing digest tokens to extract relevant information. The model is pretrained on context reconstruction tasks using a subset of the Pile dataset, then fine-tuned on instruction data (PwC dataset) to ensure digest vectors correctly respond to prompts. A divide-and-conquer strategy handles contexts exceeding compression limits.

## Key Results
- Achieves linear time complexity O(kn) compared to quadratic complexity of baseline methods
- Requires only 1/32 of the floating-point operations during compression
- Improves processing speed by 68 to 112 times while maintaining over 90% of baseline performance
- Effectively reduces compression costs, making real-time compression scenarios feasible

## Why This Works (Mechanism)

### Mechanism 1
IC-Former achieves linear time complexity by replacing self-attention with cross-attention and removing context-context interactions. Cross-attention uses digest tokens as queries and concatenates context + digest tokens as keys/values, avoiding the quadratic complexity of self-attention while allowing digest tokens to extract information from context. Core assumption: word embeddings already contain sufficient semantic information for direct cross-attention extraction. Evidence: [abstract] states the use of cross-attention to condense information from contextual word embeddings. Break condition: if semantic information in embeddings is insufficient or highly context-dependent, cross-attention alone may miss crucial context-context relationships.

### Mechanism 2
Digest tokens learn to attend to consecutive context tokens in a sequential manner, enabling neighborhood information aggregation. Each digest token focuses on 3-5 consecutive context tokens following a backslash pattern in attention maps, aggregating local information into digest vectors while maintaining sequential order. Core assumption: adjacent tokens in natural language carry related semantic information that can be aggregated effectively. Evidence: [section] describes digest tokens attending to 3-5 consecutive context tokens in sequential order. Break condition: if context has long-range dependencies or non-local semantic relationships, local aggregation may miss critical information.

### Mechanism 3
Layer-wise semantic diversification allows digest tokens to capture increasingly abstract and diverse semantic information through multiple attention layers. First layer focuses on function words (prepositions, articles), second layer extends to content words (verbs, nouns), and third layer captures broader context across more grammatical categories. Core assumption: hierarchical semantic abstraction emerges naturally through stacked attention layers. Evidence: [section] describes layer-wise focus progression from function words to broader context. Break condition: if the number of layers is too small (as in IC-Former with only 3 layers), the model may not achieve sufficient semantic abstraction for complex contexts.

## Foundational Learning

- **Cross-attention mechanism**
  - Why needed here: Cross-attention allows digest tokens to directly query context embeddings without computing expensive context-context interactions, achieving linear complexity.
  - Quick check question: How does cross-attention differ from self-attention in terms of computational complexity and information flow?

- **Rotary Position Embedding (RoPE)**
  - Why needed here: RoPE encodes relative positional relationships within context tokens, compensating for the lack of sequential information in pure cross-attention.
  - Quick check question: What problem does RoPE solve in the IC-Former architecture, and how does it work mathematically?

- **Autoencoding pre-training**
  - Why needed here: Autoencoding tasks force the model to attend to every token in context, ensuring comprehensive information preservation during compression.
  - Quick check question: Why is autoencoding pre-training critical for IC-Former's performance, and what would happen without it?

## Architecture Onboarding

- **Component map**: Context embeddings → Digest tokens + context concatenation → Cross-attention layers → Digest vectors → LLM input
- **Critical path**: The cross-attention computation is the performance bottleneck
- **Design tradeoffs**: Linear complexity vs. quadratic complexity tradeoff. IC-Former sacrifices some context-context interaction capability for significant speed improvements. Also, smaller model size (607M parameters) vs. potentially reduced capacity compared to larger compressors.
- **Failure signatures**: Poor reconstruction quality (low BLEU scores), attention maps showing random or non-sequential patterns, significant performance degradation on downstream tasks, high cross-entropy loss between reconstructed and original context.
- **First 3 experiments**:
  1. Run IC-Former on a small context (100 tokens → 25 digest vectors) and visualize attention maps to verify sequential aggregation pattern.
  2. Test reconstruction quality by comparing original vs. reconstructed context using BLEU and cross-entropy metrics.
  3. Measure compression time and memory usage compared to ICAE baseline on contexts of varying lengths (512, 1024 tokens).

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of IC-Former scale when applied to larger language models (e.g., beyond Llama2-7b-chat)? The paper only tested on Llama2-7b-chat and future work will involve experiments on larger-scale models to explore further potential.

- **Open Question 2**: What is the impact of using IC-Former on longer contexts (>512 tokens) beyond the tested range? Resource constraints prevented testing longer contexts, though the method is theoretically capable of handling them.

- **Open Question 3**: Can IC-Former achieve performance parity with ICAE on downstream tasks without sacrificing its efficiency advantages? The authors acknowledge that despite significantly outperforming the baseline in efficiency, it has not surpassed the baseline's performance in downstream tasks.

## Limitations
- Model's reliance on local token aggregation (3-5 consecutive tokens) may fail on contexts requiring long-range dependencies
- Evaluation primarily uses a single dataset (PwC) and specific LLM (Llama2-7b-chat), limiting generalizability
- 3-layer architecture may not achieve sufficient semantic abstraction for complex contexts compared to deeper models

## Confidence
- **High Confidence**: The core claim about achieving linear time complexity O(kn) through cross-attention replacement of self-attention is well-supported by mathematical formulation and experimental timing data.
- **Medium Confidence**: The sequential aggregation mechanism and layer-wise semantic diversification patterns are observed in attention maps but require broader validation across different contexts and domains.
- **Low Confidence**: The assertion that the method maintains "over 90% of the baseline performance" lacks specificity about which baseline is used and how performance is measured across different tasks.

## Next Checks
1. **Cross-Domain Generalization Test**: Evaluate IC-Former on three additional diverse datasets (technical documentation, legal contracts, and medical literature) to verify if sequential aggregation pattern and layer-wise semantic diversification hold across domains.

2. **Long-Range Dependency Assessment**: Design a synthetic benchmark with controlled long-range dependencies to quantify information loss from the local aggregation approach compared to full self-attention compression methods.

3. **Ablation on Layer Depth**: Systematically vary the number of transformer layers (1, 3, 6, 9) while measuring both compression quality and processing speed to determine the optimal tradeoff point.