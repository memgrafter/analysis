---
ver: rpa2
title: Towards Spoken Language Understanding via Multi-level Multi-grained Contrastive
  Learning
arxiv_id: '2405.20852'
source_url: https://arxiv.org/abs/2405.20852
tags:
- slot
- intent
- learning
- contrastive
- proc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MMCL, a multi-level multi-grained contrastive
  learning framework for spoken language understanding (SLU). The method addresses
  the problem of insufficient exploration of inherent relationships between intents
  and slots in joint multi-intent detection and slot filling.
---

# Towards Spoken Language Understanding via Multi-level Multi-grained Contrastive Learning

## Quick Facts
- arXiv ID: 2405.20852
- Source URL: https://arxiv.org/abs/2405.20852
- Reference count: 40
- Improves multi-intent detection and slot filling accuracy by 2.6% on MixATIS over previous best models

## Executive Summary
This paper proposes MMCL, a multi-level multi-grained contrastive learning framework for joint multi-intent detection and slot filling in spoken language understanding. The framework addresses the problem of insufficient exploration of inherent relationships between intents and slots by applying contrastive learning at three hierarchical levels - utterance, slot, and word - with both coarse and fine granularity. MMCL enables mutual guidance between the two SLU subtasks by aligning semantically similar units in representation space. The method incorporates self-distillation to improve model robustness and uses margin-based similarity instead of cosine similarity. Experiments on MixATIS and MixSNIPS datasets show state-of-the-art performance with significant improvements over previous models.

## Method Summary
The framework consists of a shared self-attentive encoder that produces utterance representations, followed by three-level contrastive learning with both coarse and fine granularity at the utterance level. The contrastive learning operates at utterance level (comparing whole utterances and individual tokens), slot level (comparing tokens with same slot labels), and word level (comparing words with same slot suffixes). A multi-intent detection decoder with token-level voting and global graph interaction layer handles intent detection and slot filling, while self-distillation minimizes KL divergence between current and previous predictions to improve robustness.

## Key Results
- Achieves 2.6% overall accuracy improvement on MixATIS over previous best models
- Shows consistent improvements across both intent accuracy and slot F1 metrics
- Demonstrates effectiveness of multi-level multi-grained contrastive learning for SLU tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-level contrastive learning enables mutual guidance between intent detection and slot filling by aligning representations of semantically similar utterances, slots, and words.
- Mechanism: The framework applies contrastive learning at three hierarchical levels (utterance, slot, word) using margin-based similarity. At each level, semantically related units (e.g., tokens with same slot label, utterances with same intents) are pulled closer in representation space while unrelated ones are pushed apart. This alignment allows the shared utterance representation to carry cross-task cues—slots inform intent detection and intents guide slot filling.
- Core assumption: Semantically similar utterances, slots, and words share related intent and slot labels, and contrastive alignment can effectively encode these relationships in the representation space.

### Mechanism 2
- Claim: Multi-grained contrastive learning at the utterance level captures both coarse global semantics and fine-grained local slot structures, improving overall accuracy.
- Mechanism: At utterance level, the framework performs coarse-grained contrastive learning (comparing whole utterances) and fine-grained contrastive learning (comparing individual tokens within utterances). Coarse-grained learning helps align entire utterances with similar intent and slot patterns, while fine-grained learning refines the alignment of individual token representations, enabling the model to learn both sentence-level and word-level dependencies.
- Core assumption: Combining coarse and fine-grained contrastive learning at the utterance level yields better semantic alignment than using either alone.

### Mechanism 3
- Claim: Self-distillation via KL divergence minimizes overconfident predictions and improves model robustness.
- Mechanism: The model minimizes KL divergence between current predictions and previous predictions for both intent and slot outputs. This regularizes the model by preventing it from becoming overconfident, especially in the presence of label noise or ambiguous examples.
- Core assumption: Distillation between successive predictions smooths decision boundaries and improves generalization.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To learn representations where semantically similar utterances, slots, and words are close in feature space, enabling mutual task guidance.
  - Quick check question: In contrastive learning, what do we want to do with positive pairs versus negative pairs in the representation space?

- Concept: Margin-based similarity vs cosine similarity
  - Why needed here: To avoid pushing semantically similar pairs apart due to scale inconsistencies in cosine similarity.
  - Quick check question: Why might cosine similarity alone be insufficient for contrastive learning in multi-intent SLU?

- Concept: Self-distillation
  - Why needed here: To reduce overconfident predictions and improve robustness, especially when label noise exists.
  - Quick check question: What is the main purpose of minimizing KL divergence between current and previous predictions in self-distillation?

## Architecture Onboarding

- Component map:
  Self-attentive encoder (BiLSTM + self-attention) → shared utterance representations
  → Contrastive learning layer (3 levels × 2 granularities + margin-based similarity)
  → Multi-intent detection decoder (token-level multi-label intent + voting)
  → Global graph interaction layer (intent-slot-slot graph propagation)
  → Self-distillation layer (KL divergence regularization)
  → Final predictions

- Critical path: Encoder → Contrastive learning → Decoder + Graph → Self-distillation → Final predictions

- Design tradeoffs:
  - Using margin-based similarity adds computation but improves semantic alignment quality.
  - Multi-grained learning increases model complexity but yields better semantic capture.
  - Self-distillation adds training overhead but improves robustness and generalization.

- Failure signatures:
  - Intent accuracy drops significantly when utterance-level contrastive learning is removed.
  - Slot F1 degrades when slot-level or word-level contrastive learning is disabled.
  - Overall performance declines if self-distillation is omitted, especially on noisy data.

- First 3 experiments:
  1. Remove utterance-level contrastive learning and measure drop in intent accuracy and overall performance.
  2. Remove slot-level contrastive learning and measure drop in slot F1 and overall performance.
  3. Remove self-distillation and measure change in robustness and performance on noisy data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the margin-based similarity metric compare to other contrastive learning objectives (like InfoNCE) in the specific context of SLU tasks?
- Basis in paper: [explicit] The paper states that margin-based similarity was adopted because it "outperforms simple cosine similarity" as reported in [25], but does not directly compare it to other contrastive learning objectives.
- Why unresolved: The paper only mentions superiority over cosine similarity, not other contrastive learning objectives like InfoNCE.
- What evidence would resolve it: Direct comparison of margin-based similarity with InfoNCE and other contrastive learning objectives on the same SLU tasks.

### Open Question 2
- Question: What is the impact of using a pre-trained language model (like RoBERTa) versus the self-attentive encoder on the effectiveness of the contrastive learning framework?
- Basis in paper: [explicit] The paper conducts experiments using RoBERTa and finds it brings "remarkable improvements" but does not analyze how this affects the contrastive learning framework specifically.
- Why unresolved: The paper shows RoBERTa improves performance but doesn't isolate how this affects the contrastive learning components.
- What evidence would resolve it: Ablation studies comparing contrastive learning performance with and without pre-trained encoders.

### Open Question 3
- Question: How sensitive is the model's performance to the choice of hyperparameters for the contrastive learning components (temperature, number of negative pairs, etc.)?
- Basis in paper: [inferred] The paper mentions specific hyperparameter values (temperature τ=2) but does not provide sensitivity analysis.
- Why unresolved: No hyperparameter sensitivity analysis is provided in the paper.
- What evidence would resolve it: Comprehensive sensitivity analysis showing how different hyperparameter values affect model performance.

## Limitations
- Implementation complexity with multiple hyperparameters across three hierarchical levels that are not fully specified
- Computational overhead from three-level contrastive learning with coarse and fine granularity may limit practical deployment
- Limited evaluation scope to only two benchmark datasets (MixATIS and MixSNIPS)

## Confidence
- High confidence: Core mechanism of multi-level contrastive learning for capturing hierarchical relationships
- Medium confidence: Specific implementation details and hyperparameter choices
- Medium confidence: Generalizability of results beyond the two evaluated datasets

## Next Checks
- Validation Check 1: Ablation on Contrastive Learning Levels - Remove each contrastive learning level independently and measure impact on intent accuracy and slot F1 scores
- Validation Check 2: Cross-Dataset Generalization - Evaluate on additional SLU datasets with different characteristics to test generalizability
- Validation Check 3: Robustness to Label Noise - Introduce varying levels of label noise and measure how self-distillation affects model robustness compared to baseline