---
ver: rpa2
title: 'Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies'
arxiv_id: '2407.13623'
source_url: https://arxiv.org/abs/2407.13623
tags:
- vocabulary
- size
- parameters
- training
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates the impact of vocabulary size on the scaling
  laws of large language models (LLMs), a factor previously overlooked in research.
  It proposes three approaches to predict the compute-optimal vocabulary size: IsoFLOPs
  analysis, derivative estimation, and parametric fit of the loss function.'
---

# Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies

## Quick Facts
- arXiv ID: 2407.13623
- Source URL: https://arxiv.org/abs/2407.13623
- Reference count: 40
- Larger language models require proportionally larger vocabularies for optimal performance

## Executive Summary
This paper challenges the conventional approach of fixing vocabulary size in large language model (LLM) pre-training. Through theoretical analysis and empirical validation, the authors demonstrate that optimal vocabulary size should scale with model parameters rather than remaining constant. They show that most existing LLMs, including Llama2-70B, use vocabulary sizes that are significantly smaller than optimal for their parameter counts. The research introduces three methods for predicting compute-optimal vocabulary sizes: IsoFLOPs analysis, derivative estimation, and parametric fitting of the loss function. These methods consistently indicate that larger models benefit from larger vocabularies, leading to improved downstream performance when implemented correctly.

## Method Summary
The authors propose three approaches to predict optimal vocabulary size. First, IsoFLOPs analysis examines vocabulary sizes with constant computational budget. Second, derivative estimation calculates the gradient of the loss function with respect to vocabulary size to identify the point of diminishing returns. Third, they fit a parametric model to the loss function that explicitly includes vocabulary size as a variable. Empirical validation uses encoder-decoder models with 3B parameters trained on CC-100 and C4 datasets, varying vocabulary sizes from 32K to 64K. Downstream performance is measured on ARC-Challenge, Winogrande, and Hellaswag tasks.

## Key Results
- Larger models consistently benefit from larger vocabularies, with optimal size scaling approximately linearly with parameter count
- Most existing LLMs use vocabulary sizes far below optimal (Llama2-70B should use ~216K vs current 32K)
- Empirical validation shows consistent downstream performance improvements when using predicted optimal vocabulary sizes
- Increasing vocabulary from 32K to 43K improved ARC-Challenge performance from 29.1 to 32.0 with same computational budget

## Why This Works (Mechanism)
The mechanism behind this finding relates to the information-theoretic efficiency of tokenization. Larger models have greater capacity to learn and utilize more fine-grained subword representations. When vocabulary is too small, the model must use longer sequences to represent the same information, creating inefficiencies in both computation and representation. Conversely, when vocabulary is appropriately sized for the model scale, each token carries more information, allowing the model to achieve better compression of the training data and more efficient parameter utilization.

## Foundational Learning

### Scaling Laws
**Why needed**: Understanding how model performance scales with parameters, data, and compute is essential for efficient LLM development.
**Quick check**: Verify that model loss follows the established power-law relationship with training tokens and parameters.

### Tokenization and Vocabulary Design
**Why needed**: Tokenization directly impacts how information is represented and processed by the model.
**Quick check**: Confirm that subword tokenization methods (BPE, WordPiece) can generate vocabularies at the required scales.

### Compute-Optimal Training
**Why needed**: Determining the optimal allocation of computational resources across model size, data, and vocabulary is critical for efficiency.
**Quick check**: Validate that the total compute budget (FLOPs) remains constant when varying vocabulary size in IsoFLOPs analysis.

## Architecture Onboarding

### Component Map
Data -> Tokenizer -> Vocabulary -> Model Parameters -> Loss Function -> Downstream Tasks

### Critical Path
The critical path is: Tokenizer selection → Vocabulary construction → Model architecture design → Training configuration → Downstream evaluation. The vocabulary size decision early in this pipeline has cascading effects on all subsequent components.

### Design Tradeoffs
**Memory vs. Efficiency**: Larger vocabularies require more memory but can reduce sequence length and improve representation efficiency.
**Computational Cost**: More vocabulary tokens increase per-token computation but may reduce total token count.
**Generalization vs. Specificity**: Larger vocabularies can capture more specific patterns but may overfit to training data distribution.

### Failure Signatures
**Under-tokenization**: Too small vocabulary leads to long sequences, high perplexity, and poor downstream performance.
**Over-tokenization**: Excessively large vocabulary causes data sparsity, increased memory usage, and potential overfitting.
**Suboptimal Scaling**: Vocabulary size not matching model scale results in inefficient parameter utilization.

### First Experiments
1. Train identical models with varying vocabulary sizes (32K, 43K, 64K) while keeping all other hyperparameters constant
2. Measure the relationship between vocabulary size and sequence length for a fixed text corpus
3. Evaluate downstream task performance as a function of vocabulary size for a fixed model scale

## Open Questions the Paper Calls Out
None

## Limitations
- Results primarily validated on encoder-decoder architectures, limiting generalizability to decoder-only models
- Only three downstream tasks evaluated, potentially missing task-specific vocabulary sensitivities
- Computational costs of larger vocabularies (memory, training time) not fully characterized
- Training data distribution effects on optimal vocabulary size not explored

## Confidence
**High confidence**: Core finding that larger models benefit from larger vocabularies is well-supported
**Medium confidence**: Optimal vocabulary size predictions for very large models rely on extrapolation
**Medium confidence**: Relationship between perplexity improvements and downstream performance not fully characterized

## Next Checks
1. Validate scaling relationship across different model architectures (decoder-only, decoder-encoder, encoder-only)
2. Extend downstream evaluation to broader range of tasks including multilingual benchmarks and code generation
3. Investigate interaction between vocabulary size and tokenization quality across different methods (BPE, WordPiece, SentencePiece)