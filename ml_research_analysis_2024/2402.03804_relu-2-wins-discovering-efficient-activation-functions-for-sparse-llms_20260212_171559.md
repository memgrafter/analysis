---
ver: rpa2
title: 'ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs'
arxiv_id: '2402.03804'
source_url: https://arxiv.org/abs/2402.03804
tags:
- activation
- sparsity
- llms
- neurons
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the sparse activation of large language models
  (LLMs) with different activation functions. The authors propose a more general definition
  of neuron activation based on neuron output magnitudes rather than zero activation
  values.
---

# ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs

## Quick Facts
- arXiv ID: 2402.03804
- Source URL: https://arxiv.org/abs/2402.03804
- Reference count: 30
- Primary result: ReLU² activation achieves best sparsity-performance tradeoff in LLMs, reducing 56% computational cost with <1% performance loss

## Executive Summary
This paper proposes a novel approach to sparse activation in Large Language Models (LLMs) by redefining neuron activation based on output magnitudes rather than exact zero values. Through extensive experiments on 1.3B parameter models using different activation functions (ReLU, SwiGLU, ReGLU, and ReLU²), the authors demonstrate that ReLU² achieves the optimal balance between sparsity and performance. The key insight is that neuron output magnitudes follow a long-tailed distribution, allowing for effective magnitude-based thresholding without significant performance degradation.

The study examines sparsity from three critical perspectives: performance-sparsity trade-off, neuron activation predictivity, and hardware affinity. ReLU² consistently outperforms other activation functions across all three metrics, showing the highest predictivity of neuron activation patterns and better hardware affinity for efficient sparse computation. The paper also introduces the CETT (Cumulative Errors of Tail Truncation) metric to quantify the impact of magnitude-based truncation on model performance.

## Method Summary
The authors train four 1.3B parameter models from scratch using different activation functions (SwiGLU, ReLU, ReGLU, ReLU²) on 100B tokens of mixed natural language data. They evaluate performance on multiple benchmarks and analyze sparsity using a neuron decomposition approach that computes output magnitudes for individual neurons. The CETT metric guides the selection of optimal magnitude thresholds, while a lightweight neural network predictor (6% of FFN size) evaluates activation pattern predictivity. Hardware affinity is assessed through reuse ratios and co-activation gaps between neuron pairs.

## Key Results
- ReLU² achieves comparable performance to SwiGLU with up to 56% computational cost reduction (<1% performance loss)
- ReLU² enables 92% reduction in I/O overhead for feed-forward networks through effective sparsity utilization
- ReLU² demonstrates highest neuron activation predictivity and best hardware affinity among all tested activation functions

## Why This Works (Mechanism)

### Mechanism 1: Long-tailed magnitude distribution
ReLU² leverages a long-tailed distribution of neuron output magnitudes where small magnitudes can be truncated without significant performance loss. The core assumption is that small-magnitude neuron outputs contribute negligibly to final FFN outputs. Evidence shows models using ReLU² achieve performance degradation <0.1% at sparsity ratios close to 90%.

### Mechanism 2: Enhanced hardware affinity
ReLU² creates more pronounced differences between highly co-activated and lowly co-activated neuron pairs, enabling more efficient memory address allocation and reuse. The mechanism assumes co-activation frequency differences can be exploited for hardware optimization. Evidence shows ReLU² models have higher co-activation frequency gaps than other activation functions.

### Mechanism 3: Superior predictivity
ReLU² creates more predictable activation patterns that can be accurately predicted by lightweight neural network predictors. The core assumption is that ReLU² activation patterns are more predictable than other functions. Evidence shows ReLU² achieves highest prediction recall and sparsity among all activation functions.

## Foundational Learning

- Concept: Neuron output magnitude distribution
  - Why needed here: Understanding long-tailed distribution is crucial for defining new activation sparsity metric and explaining ReLU²'s effectiveness
  - Quick check question: What's the difference between activation value sparsity (zero-based) and output magnitude sparsity (threshold-based)?

- Concept: Computational relationships between tokens and neurons
  - Why needed here: Fundamental to understanding how activation functions affect hardware efficiency and I/O overhead
  - Quick check question: How do reuse ratios and co-activation gaps contribute to hardware affinity in sparse LLM inference?

- Concept: Activation function variants and mathematical properties
  - Why needed here: Essential for interpreting experimental results and design choices
  - Quick check question: What are key mathematical differences between ReLU² and other activation functions that might contribute to superior performance?

## Architecture Onboarding

- Component map: Neuron decomposition module -> Threshold finding module -> Predictor construction module -> Hardware affinity analysis module -> Evaluation module
- Critical path: 1) Decompose FFN into neurons and compute output magnitudes 2) Calculate CETT to quantify sparsity impact 3) Find optimal magnitude thresholds 4) Build predictors and measure predictivity 5) Analyze hardware affinity through reuse ratios and co-activation gaps 6) Evaluate overall performance across all three aspects
- Design tradeoffs: Precision vs. sparsity (higher thresholds increase sparsity but may impact performance), predictor size vs. accuracy (larger predictors improve accuracy but increase overhead), threshold adaptation vs. consistency (adaptive thresholds optimize for each model but complicate implementation)
- Failure signatures: Performance degradation exceeding 1% at target sparsity levels, predictor recall dropping below 80% for any activation function, hardware affinity metrics not showing clear advantages for ReLU²
- First 3 experiments: 1) Reproduce CETT vs. sparsity ratio curves for all activation functions 2) Implement and compare predictor performance across activation functions using same architecture 3) Measure hardware affinity metrics (reuse ratios and co-activation gaps) for each activation function under controlled conditions

## Open Questions the Paper Calls Out

### Open Question 1: Optimal magnitude thresholds
What is the optimal magnitude threshold for different activation functions and model sizes to maximize trade-off between sparsity and performance? The paper uses fixed CETT upper bound of 0.2 but doesn't systematically explore how optimal threshold varies across different conditions.

### Open Question 2: Emergence of hot-activated neurons
How does emergence of hot-activated neurons vary with model scale and activation function, and what architectural factors influence this phenomenon? The paper observes hot-activated neurons in 7B models but not in 1B models, suggesting scale-dependent emergence without mechanistic explanation.

### Open Question 3: Hardware-specific optimizations
What are most effective hardware-specific optimizations for leveraging neuron activation predictivity and computational relationships in sparse LLMs? The paper discusses computational relationships but doesn't explore hardware-specific optimizations or implement specific hardware optimizations.

## Limitations

- Experimental scope limited to four activation functions without exploring potentially relevant variants like GELU or PReLU
- Predictor performance evaluation uses fixed two-layer architecture without exploring more complex predictors
- Study focuses on English-centric data and may not generalize to multilingual or specialized domain models
- Core claims rest on several significant uncertainties regarding generalization across different model architectures

## Confidence

- High Confidence: ReLU² achieves comparable performance to SwiGLU while enabling higher sparsity levels; CETT-based threshold finding method is clearly described and reproducible
- Medium Confidence: Hardware affinity claims regarding co-activation gaps and reuse ratios show consistent patterns but depend on specific hardware assumptions; predictivity results are robust within tested predictor architecture but may not generalize
- Low Confidence: Long-tailed distribution mechanism explanation lacks rigorous statistical validation; claim that small-magnitude outputs contribute negligibly to final results is assumed rather than proven

## Next Checks

1. Scale Validation: Reproduce key experiments (sparsity-performance tradeoff, predictivity, hardware affinity) on 7B parameter model to verify ReLU² advantages scale with model size and test whether long-tailed magnitude distribution persists

2. Generalization Test: Evaluate CETT-based threshold finding method and predictivity metrics across additional activation functions (GELU, PReLU, SiLU) to determine whether ReLU² advantages are specific to its mathematical properties

3. Hardware Architecture Validation: Implement co-activation gap analysis on at least two different hardware architectures (A100 GPUs and simulated sparse accelerator) to verify hardware affinity advantages are architecture-independent and not artifacts of specific memory access patterns