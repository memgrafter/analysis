---
ver: rpa2
title: 'DC is all you need: describing ReLU from a signal processing standpoint'
arxiv_id: '2407.16556'
source_url: https://arxiv.org/abs/2407.16556
tags:
- relu
- frequency
- signal
- component
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a rigorous frequency-domain analysis of the
  ReLU activation function using its Taylor expansion. The authors demonstrate that
  ReLU introduces higher-frequency oscillations and a DC component whose amplitude
  is modulated by the input signal's frequency content.
---

# DC is all you need: describing ReLU from a signal processing standpoint

## Quick Facts
- arXiv ID: 2407.16556
- Source URL: https://arxiv.org/abs/2407.16556
- Authors: Christodoulos Kechris; Jonathan Dan; Jose Miranda; David Atienza
- Reference count: 10
- Primary result: ReLU activation introduces a frequency-modulated DC component that enables effective neural network classification with minimal training

## Executive Summary
This paper presents a rigorous frequency-domain analysis of the ReLU activation function using Taylor expansion, demonstrating that ReLU introduces higher-frequency oscillations and a DC component whose amplitude is modulated by input signal frequency content. The authors show that this DC component enables neural networks to converge to solutions close to initial random weights, allowing effective classification without extensive training. The work is validated through simulations and a real-world heart rate extraction CNN, providing new insights into ReLU's role in feature extraction and network convergence behavior.

## Method Summary
The authors derive ReLU's frequency-domain behavior using Taylor expansion around g(t)=0, showing that ReLU introduces a DC component proportional to sqrt(2)/4 * sqrt(sum of squared input amplitudes). They validate this model through simulations with sinusoidal inputs and a heart rate extraction CNN. The method involves analyzing how ReLU transforms frequency content, measuring training loss and weight distance from initialization, and classifying sinusoidal signals based on their frequency-dependent DC signatures.

## Key Results
- ReLU introduces higher-frequency oscillations and a DC component whose amplitude is modulated by input signal frequency content
- The DC component enables neural networks to converge to weight configurations close to initial random weights
- CNNs can classify signals based on frequency-dependent DC signatures without extensive weight training
- Heart rate extraction CNN demonstrates practical application of the frequency-domain ReLU analysis

## Why This Works (Mechanism)

### Mechanism 1
ReLU introduces a DC component whose amplitude is proportional to the root-sum-of-squares of input amplitudes across all frequencies. By rewriting ReLU(x) = x + |x|/2 and expanding |x| using Taylor series around g(t)=0, the DC term emerges as sqrt(2)/4 * sqrt(sum of squared input amplitudes). This DC depends on the full frequency content of the input. The Taylor expansion requires |g(t)| < 1 for convergence, which may fail for signals with large amplitude differences across frequencies.

### Mechanism 2
CNNs leverage the frequency-dependent DC component to classify signals by converging to weight configurations close to random initialization. The DC component acts as a frequency fingerprint - different input frequencies produce distinct DC values when passed through random convolutional filters. Global average pooling extracts this DC, and the network learns to classify based on these DC signatures without extensive weight updates. This mechanism may fail if random initialization produces similar DC responses for distinct frequencies.

### Mechanism 3
Multiple ReLU layers do not introduce new frequencies beyond those from convolutional operations that reintroduce negative values. Since y(t) = relu(relu(x(t))) = relu(x(t)), repeated ReLU applications are idempotent. New frequencies arise only when convolution operations create negative values that subsequent ReLUs can act upon differently. This requires convolutional layers between ReLUs to produce outputs spanning both positive and negative ranges.

## Foundational Learning

- **Fourier Transform and frequency domain analysis**: The entire analysis relies on characterizing ReLU's effect in frequency domain using Fourier transforms of input signals decomposed into sinusoidal components. Quick check: If x(t) = cos(2πf₁t) + 0.5cos(2πf₂t), what are the frequency components of x²(t) after applying trigonometric identities?

- **Taylor series expansion around a non-zero point**: The DC component derivation requires expanding sqrt(1 + g(t)) using Taylor series around g(t) = 0 to isolate the constant term. Quick check: What is the first non-zero term in the Taylor expansion of sqrt(1 + ε) around ε = 0?

- **Global average pooling as DC extraction**: The paper uses global average pooling to extract the DC component introduced by ReLU for classification purposes. Quick check: For a signal with values [1, 2, 3, 4], what is the DC component extracted by global average pooling?

## Architecture Onboarding

- **Component map**: Input signal → Convolutional layer (random weights) → ReLU activation → DC component generation → Global average pooling → Classification
- **Critical path**: 1) Input signal passes through convolution with random weights, 2) ReLU activation generates frequency-dependent DC component, 3) Global average pooling extracts DC values, 4) Classification based on DC signatures
- **Design tradeoffs**: Random initialization vs trained weights (random suffices due to DC's frequency discrimination capability, but may fail if frequencies are too similar), kernel size (larger kernels provide more frequency resolution but increase computational cost), pooling strategy (average pooling extracts DC but discards other information)
- **Failure signatures**: Poor classification accuracy when input frequencies are too close together, DC values becoming indistinguishable across different frequency classes, training loss not decreasing when attempting to learn beyond DC-based features
- **First 3 experiments**: 1) Generate synthetic signals with single frequency components at f1=3Hz, f2=5Hz, f3=10Hz, pass through random convolution → ReLU → global average pooling, verify distinct DC outputs, 2) Vary convolutional kernel size from 2 to 16 samples, measure DC discrimination accuracy across frequency classes, 3) Test classification performance with frequencies closer together (e.g., 4Hz, 5Hz, 6Hz) to find breaking point of random initialization strategy

## Open Questions the Paper Calls Out

### Open Question 1
How does the frequency content of the input signal affect the magnitude and distribution of the DC component introduced by ReLU across multiple layers? The paper derives that the DC component is modulated by the amplitude of all oscillations present in the input signal (eq. 8) and shows this behavior in experiments with single-component sinusoidal signals. The interaction between multiple ReLU layers and how their DC components combine or compete across deep networks remains unclear. Systematic experiments varying input signal frequency content and measuring DC component evolution through multiple ReLU layers would resolve this.

### Open Question 2
What is the precise relationship between ReLU-induced DC components and the network's tendency to converge to solutions close to initial random weights? The paper states that "the DC helps to converge to a weight configuration that is close to the initial random weights" and demonstrates this through training loss and weight distance experiments. The mechanism linking DC components to weight initialization proximity is demonstrated empirically but not theoretically explained. Comparative analysis of convergence trajectories with and without DC components would resolve this.

### Open Question 3
How does the Taylor expansion approximation of ReLU's frequency response behave for non-sinusoidal input signals with complex spectral content? The paper derives the Taylor expansion around g(t)=0 and validates it for multi-frequency sinusoidal inputs, but notes it's valid "so long as g(t) is close to zero." The approximation's validity and accuracy for real-world signals with arbitrary spectral content, transients, and noise remains untested. Comprehensive testing across diverse signal types with quantitative error analysis would resolve this.

## Limitations
- Taylor expansion requires strict constraints on input signal amplitudes to ensure convergence, limiting applicability to real-world noisy signals
- Claims about random initialization sufficiency rely heavily on idealized frequency separation that may not hold in practical scenarios
- Mathematical derivations assume continuous-time signals and may not fully capture discrete implementation effects in digital neural networks

## Confidence
- **Medium confidence** in core claims about ReLU's frequency-domain behavior
- **Low confidence** in practical implications for neural network training convergence
- **Low confidence** in discrete-time implementation effects on theoretical predictions

## Next Checks
1. Apply the frequency analysis framework to discrete-time signals with varying sampling rates and quantization levels to assess the impact of digital implementation on theoretical predictions
2. Systematically vary the distance between input signal frequencies and measure classification accuracy and weight convergence behavior to quantify breaking point of random initialization strategy
3. Test the frequency-dependent DC behavior across different CNN architectures (varying depth, kernel sizes, pooling strategies) to determine critical architectural choices for the proposed mechanism to function effectively