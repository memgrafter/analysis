---
ver: rpa2
title: 'MobileAIBench: Benchmarking LLMs and LMMs for On-Device Use Cases'
arxiv_id: '2406.10290'
source_url: https://arxiv.org/abs/2406.10290
tags:
- mobile
- arxiv
- tasks
- llms
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MobileAIBench, a benchmarking framework for
  evaluating large language models (LLMs) and large multimodal models (LMMs) on mobile
  devices. The framework addresses the challenge of deploying these models on resource-constrained
  mobile hardware by assessing their performance across different sizes, quantization
  levels, and tasks.
---

# MobileAIBench: Benchmarking LLMs and LMMs for On-Device Use Cases

## Quick Facts
- arXiv ID: 2406.10290
- Source URL: https://arxiv.org/abs/2406.10290
- Reference count: 40
- Introduces MobileAIBench framework for evaluating LLMs and LMMs on mobile devices, measuring performance across quantization levels and tasks

## Executive Summary
MobileAIBench is a benchmarking framework designed to evaluate large language models (LLMs) and large multimodal models (LMMs) on resource-constrained mobile devices. The framework addresses the growing need for on-device AI deployment by providing tools to measure both performance and efficiency metrics across various model sizes, quantization levels, and task types. MobileAIBench includes a desktop evaluation library and an iOS app for on-device measurements of latency and hardware utilization.

The study reveals critical insights into mobile deployment challenges, showing that smaller models have lower latency and faster token processing, but memory usage remains high even for small models. Multimodal tasks are significantly more resource-intensive than text-based tasks. The framework enables systematic evaluation of quantization strategies and their impact on model performance, highlighting the trade-offs between model size, quantization, and deployment feasibility on mobile hardware.

## Method Summary
MobileAIBench provides a comprehensive framework for benchmarking LLMs and LMMs on mobile devices. The framework includes a desktop evaluation pipeline and an iOS app for on-device measurements. Models are quantized to various levels (16-bit, 8-bit, 4-bit, 3-bit) using standard techniques, then evaluated on diverse tasks including NLP benchmarks (Databricks-dolly, HotpotQA, CNN/Daily Mail), multimodal tasks (VQA-v2, VizWiz, GQA), and trust & safety tasks (TruthfulQA, Do-Not-Answer). Performance is measured using effectiveness metrics (Exact Match, F1 Score, accuracy), efficiency metrics (Time-to-First-Token, Input Token Per Second, Total Time), and utilization metrics (CPU usage, RAM usage, Battery Drain Rate). The iOS app enables real-time measurement of latency and hardware utilization on actual mobile devices.

## Key Results
- Smaller quantized models (4-bit) show lower latency and faster token processing on iPhone 14, but memory usage remains high even for small models
- Multimodal tasks consume significantly more resources than text-based tasks, creating deployment challenges
- 3-bit quantization causes significant performance drops, while 4-bit quantization maintains consistent accuracy across tasks
- MobileAIBench framework successfully measures trade-offs between model size, quantization level, and task-specific performance

## Why This Works (Mechanism)
The framework works by providing standardized evaluation procedures that isolate model performance from infrastructure variability. By using quantization techniques, models are compressed to fit mobile memory constraints while maintaining acceptable accuracy. The iOS app directly measures hardware utilization on real devices, capturing true mobile deployment conditions including thermal throttling and battery constraints that cloud-based evaluations miss.

## Foundational Learning
- **Model quantization**: Reducing precision of model weights to decrease memory footprint and computational requirements. Needed to fit large models on mobile devices with limited memory. Quick check: Verify quantized model size is 75% smaller than original.

- **Latency measurement**: Timing model inference from input to output generation. Critical for mobile user experience where delays are unacceptable. Quick check: Ensure Time-to-First-Token is under 2 seconds for interactive applications.

- **Hardware utilization monitoring**: Tracking CPU, RAM, and battery usage during model inference. Essential for understanding mobile deployment feasibility and user experience. Quick check: Confirm RAM usage stays below 70% of device capacity.

- **Multimodal processing**: Handling both text and visual inputs in a single model. Significantly more complex than text-only processing due to additional data types and computational requirements. Quick check: Verify image preprocessing time is less than 500ms.

- **Tokenization efficiency**: Converting text to numerical tokens and back. Directly impacts inference speed and memory usage. Quick check: Measure Input Token Per Second is above 20 for responsive applications.

- **Cross-device benchmarking**: Evaluating models across different hardware configurations. Necessary because mobile device capabilities vary widely. Quick check: Run same model on at least 3 different device classes.

## Architecture Onboarding

**Component Map**: MobileAIBench iOS App -> Model Inference Engine -> Quantized Models -> Task Datasets -> Performance Metrics

**Critical Path**: User Input → iOS App → Model Loading → Inference → Hardware Monitoring → Metrics Collection → Results Display

**Design Tradeoffs**: The framework prioritizes real-device measurements over simulated environments, sacrificing some experimental control for deployment authenticity. This means results are more representative but harder to reproduce exactly across different hardware generations.

**Failure Signatures**: Memory overflow crashes when quantized models still exceed device RAM, thermal throttling causing latency spikes, battery drain rates exceeding 10% per minute indicating inefficient inference, and accuracy drops below 80% suggesting quantization damage.

**3 First Experiments**:
1. Run 4-bit LLaMA-7B on iPhone 14 with text-only tasks to establish baseline latency and memory usage
2. Compare 4-bit vs 8-bit quantization on same model to measure accuracy-latency trade-off
3. Execute multimodal task with 4-bit LMM to identify image processing bottlenecks

## Open Questions the Paper Calls Out
- How does quantization level beyond 3-bit affect the performance of LLMs and LMMs on mobile devices? The paper only evaluates up to 3-bit for LMMs and 4-bit for LLMs, leaving the impact of further quantization unexplored.
- What is the impact of different quantization methods (e.g., QLoRA, GPTQ, AWQ) on the performance and efficiency of LLMs and LMMs on mobile devices? The paper uses a single quantization method without exploring alternatives.
- How do different mobile hardware architectures (e.g., ARM vs x86, different GPU capabilities) affect the performance and efficiency of quantized LLMs and LMMs? Experiments are limited to iPhone 14, preventing generalization to other hardware.
- What is the optimal balance between model size, quantization level, and task-specific performance for mobile deployment? The paper presents individual findings but lacks a unified framework for practitioners.

## Limitations
- Model architecture details are not fully specified, creating uncertainty about exact reproduction of baseline results
- Exact prompt templates and evaluation procedures for each task are not fully detailed, particularly for trust & safety tasks
- Results are based on iPhone 14 hardware only, limiting generalizability to other mobile devices with different capabilities

## Confidence
- **High Confidence**: Core methodology for measuring latency, memory usage, and hardware utilization on mobile devices is clearly specified and reproducible
- **Medium Confidence**: Comparative performance rankings between model sizes and quantization levels are reproducible, but absolute values may vary based on implementation details
- **Low Confidence**: Trust and safety task evaluations rely on subjective judgment criteria not fully detailed, making consistent evaluation difficult

## Next Checks
1. Quantization Consistency Verification: Replicate 4-bit quantization for LLaMA-7B using both the paper's framework and alternative tools (GPTQ, AWQ) to quantify variance in memory and latency measurements
2. Cross-Device Performance Scaling: Run the same benchmark suite on at least three different mobile devices spanning different performance tiers to establish hardware sensitivity patterns
3. Multimodal Task Bottleneck Analysis: Instrument the iOS app to separately measure image preprocessing, model inference, and post-processing time for multimodal tasks to identify specific bottlenecks