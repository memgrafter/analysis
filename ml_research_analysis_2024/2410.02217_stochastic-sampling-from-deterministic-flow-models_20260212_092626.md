---
ver: rpa2
title: Stochastic Sampling from Deterministic Flow Models
arxiv_id: '2410.02217'
source_url: https://arxiv.org/abs/2410.02217
tags:
- flow
- deterministic
- diffusion
- equation
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to construct stochastic samplers
  for deterministic flow models by converting their underlying ODEs into families
  of SDEs that share the same marginal distributions. The key contribution is Theorem
  1, which provides a general recipe to create infinitely many SDEs from a given ODE
  or SDE.
---

# Stochastic Sampling from Deterministic Flow Models

## Quick Facts
- arXiv ID: 2410.02217
- Source URL: https://arxiv.org/abs/2410.02217
- Authors: Saurabh Singh; Ian Fischer
- Reference count: 40
- Primary result: Non-singular stochastic samplers outperform deterministic ones in both toy Gaussian setup (better variance estimation) and large-scale ImageNet generation (improved FID scores)

## Executive Summary
This paper introduces a method to construct stochastic samplers for deterministic flow models by converting their underlying ODEs into families of SDEs that share the same marginal distributions. The key contribution is Theorem 1, which provides a general recipe to create infinitely many SDEs from a given ODE or SDE. The method allows generating stochastic samples from fixed deterministic flow models without retraining, enabling control over diversity and robustness to discretization errors. Experiments show that stochastic samplers outperform deterministic ones in both a toy Gaussian setup (better variance estimation) and large-scale ImageNet generation (improved FID scores). The approach also supports classifier-free guidance and provides an additional knob for controlling generation diversity.

## Method Summary
The method converts deterministic flow models (trained as ODEs) into stochastic samplers by constructing families of SDEs with identical marginal distributions. Using Theorem 1, the authors derive alternative drift and diffusion coefficients that preserve the probability flow while introducing stochasticity. The approach works with any flow model by leveraging score estimation techniques, particularly for Gaussian flows where the score can be computed analytically from the flow field. The NonSingular sampler, with its smooth diffusion coefficient, demonstrates superior robustness to discretization errors compared to singular alternatives.

## Key Results
- Non-singular stochastic samplers achieve better FID scores and are more robust to discretization than deterministic samplers
- Stochastic samplers provide better variance estimation, with deterministic methods showing increasing bias as discretization steps decrease
- The stochasticity parameter α allows principled control over diversity and sample quality metrics without retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing stochasticity via a family of SDEs with the same marginal distributions as the original ODE reduces bias in variance estimation.
- Mechanism: The theorem constructs a family of SDEs parameterized by γ_t and G̃ that share the same marginal distributions as the original deterministic ODE. By introducing time-dependent diffusion terms, the stochastic sampler can explore the state space more thoroughly, mitigating discretization bias.
- Core assumption: The marginal distributions p_t(x) remain unchanged across the SDE family, allowing stochastic sampling without retraining the flow model.
- Evidence anchors:
  - [abstract]: "Our method provides additional degrees of freedom that help alleviate the issues with the deterministic samplers and empirically outperforms them."
  - [section]: "In Figure 4 we study the effect of diffusion coefficient scale α on the NonSingular sampler... Increased stochasticity with increasing diffusion coefficient scale (α > 0) helps mitigate this bias at the cost of increased variance."
  - [corpus]: Weak evidence - no direct mention of bias mitigation in related papers.

### Mechanism 2
- Claim: Non-singular diffusion terms improve robustness to discretization errors compared to singular ones.
- Mechanism: Singular diffusion terms (e.g., g(t) = √(2t/(1-t))) cause instability near boundaries, while non-singular alternatives (e.g., g(t) = √t) maintain numerical stability throughout the sampling interval.
- Core assumption: The flow model's accuracy and differentiability are maintained across the entire time interval [0,1].
- Evidence anchors:
  - [section]: "While mean estimates are accurate for all methods, Deterministic gets increasingly biased for variance estimates as the number of sampling steps is decreased. Stochastic samplers perform consistently well at various discretization levels for t = 0."
  - [section]: "Since Constant also has a singularity, but only in the drift term f, we conclude that the instability is primarily due to the singularity in Singular's diffusion term."
  - [corpus]: Weak evidence - related papers focus on convergence analysis but not discretization robustness.

### Mechanism 3
- Claim: The stochasticity parameter α provides a principled way to trade bias for variance in sample quality metrics like FID.
- Mechanism: By scaling the diffusion coefficient, practitioners can explore a spectrum of stochastic samplers that balance deterministic accuracy with stochastic exploration, optimizing for metrics like FID without retraining.
- Core assumption: The FID metric and similar quality measures respond smoothly to changes in the stochasticity level.
- Evidence anchors:
  - [section]: "Non-singular samplers work well over a broad range of α. Plots of FID for each sampler as the diffusion coefficient scale α is increased."
  - [section]: "The addition of a parameter α to control the strength of the stochasticity while keeping the marginal distribution p_t unchanged... permits principled post-training optimization of the metrics like FID."
  - [corpus]: No direct evidence in related papers about FID optimization via stochasticity scaling.

## Foundational Learning

- Concept: Fokker-Planck-Kolmogorov (FPK) equation
  - Why needed here: The FPK equation describes how the probability density evolves under an SDE, and the theorem relies on showing that multiple SDEs can have identical FPK evolution.
  - Quick check question: How does the FPK equation change when we replace the drift f and diffusion G with f̄ and Ḡ as defined in Theorem 1?

- Concept: Score matching and denoising score matching
  - Why needed here: The score function ∇_x ln p_t(x) is required for the stochastic samplers, and for Gaussian flows it can be derived from the flow model using score matching principles.
  - Quick check question: Given a Gaussian flow with x_t = (1-t)x_0 + tx_1 and x_1 ~ N(μ_1, σ_1^2 I), what is the expression for the score function ∇_x ln p_t(x)?

- Concept: Euler-Maruyama discretization
  - Why needed here: The experiments compare different SDEs using Euler-Maruyama discretization, and the robustness of samplers depends on how well this discretization handles the SDE coefficients.
  - Quick check question: For the SDE dx = f(x,t)dt + g(t)dW_t, what is the Euler-Maruyama update rule for a timestep of size Δt?

## Architecture Onboarding

- Component map:
  Flow model -> Score estimator -> Sampler selector -> Discretization engine -> Evaluation module

- Critical path:
  1. Load trained flow model v(x,t)
  2. Compute score function using Equation (13)
  3. Select sampler (e.g., NonSingular) and set α
  4. Initialize with x_1 ~ N(0,I)
  5. Run Euler-Maruyama backward in time
  6. Evaluate samples with FID or visual inspection

- Design tradeoffs:
  - Non-singular vs. singular diffusion: Non-singular is more stable but may explore less aggressively
  - Fixed vs. adaptive timestep: Fixed is simpler but adaptive could handle stiff regions better
  - Score estimation accuracy vs. computational cost: More accurate scores improve sampling but increase overhead

- Failure signatures:
  - Diverging trajectories: Likely due to singular coefficients or too large α
  - Poor FID that doesn't improve with α: Score estimation may be inaccurate
  - Visual artifacts in samples: Discretization error or inappropriate g̃(t) choice

- First 3 experiments:
  1. Toy Gaussian setup: Compare Deterministic, Singular, Constant, NonSingular, and ZeroEnds at α=1 with 50 and 500 steps to reproduce Figure 2 and 3
  2. Image quality sweep: Fix NonSingular sampler and sweep α from 0 to 1 in steps of 0.1 to reproduce Figure 5
  3. Discretization robustness: Fix α at optimal value and vary number of steps from 50 to 500 to reproduce Figure 6

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on accurate score estimation from the underlying flow model
- Empirical validation constrained to relatively clean synthetic data and standard image datasets
- Smaller FID improvements on ImageNet compared to dramatic improvements in simpler setups

## Confidence
- Medium: The theoretical foundation appears sound and toy experiments convincingly demonstrate improved variance estimation and robustness to discretization. However, ImageNet results show smaller improvements, and the diversity improvements lack quantitative metrics beyond FID.

## Next Checks
1. Test on out-of-distribution data to assess robustness beyond the training distribution
2. Compare against alternative stochastic samplers that don't require flow model retraining
3. Conduct ablation studies on the score estimation accuracy to quantify its impact on final sample quality