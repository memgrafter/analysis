---
ver: rpa2
title: 'Tower: An Open Multilingual Large Language Model for Translation-Related Tasks'
arxiv_id: '2402.17733'
source_url: https://arxiv.org/abs/2402.17733
tags:
- tower
- translation
- language
- open
- instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TOWER introduces a multilingual large language model specialized\
  \ for translation-related tasks through continued pretraining on multilingual data\
  \ and instruction tuning. The model, TowerInstruct, achieves state-of-the-art performance\
  \ among open models on translation quality (e.g., 88.16 COMET-22 on FLORES-200 en\u2192\
  xx) and competitive results with GPT-4 on multiple translation-related tasks, including\
  \ automatic post-editing and named entity recognition."
---

# Tower: An Open Multilingual Large Language Model for Translation-Related Tasks

## Quick Facts
- arXiv ID: 2402.17733
- Source URL: https://arxiv.org/abs/2402.17733
- Reference count: 40
- Primary result: TowerInstruct achieves state-of-the-art performance among open models on translation quality (88.16 COMET-22 on FLORES-200 en→xx) and competitive results with GPT-4 on multiple translation-related tasks.

## Executive Summary
TOWER introduces a multilingual large language model specialized for translation-related tasks through continued pretraining on multilingual data and instruction tuning. The model, TowerInstruct, achieves state-of-the-art performance among open models on translation quality (e.g., 88.16 COMET-22 on FLORES-200 en→xx) and competitive results with GPT-4 on multiple translation-related tasks, including automatic post-editing and named entity recognition. The approach highlights the effectiveness of parallel data during continued pretraining and diverse instruction data for specialization.

## Method Summary
The method involves continued pretraining of LLaMA-2 on a multilingual corpus (2/3 monolingual, 1/3 parallel data) followed by supervised fine-tuning on a curated dataset called TOWER BLOCKS. The model is trained using Megatron-LLM for pretraining and Axolotl with DeepSpeed for fine-tuning, with specific focus on translation quality metrics like COMET-22, BLEURT, and chrF, as well as task-specific metrics for automatic post-editing, named entity recognition, and grammatical error correction.

## Key Results
- TowerInstruct achieves 88.16 COMET-22 on FLORES-200 en→xx, outperforming other open models
- The model shows competitive performance with GPT-4 on multiple translation-related tasks
- Mixing monolingual and parallel data during pretraining yields better results than using either alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel data during continued pretraining improves translation quality more than monolingual data alone.
- Mechanism: Including parallel sentences provides explicit alignment signals between languages, complementing the implicit cross-lingual learning from monolingual data.
- Core assumption: The model can effectively leverage parallel sentence pairs to improve translation quality beyond what monolingual data provides.
- Evidence anchors:
  - [abstract] "importantly, while Xu et al. (2024a) employ a dataset exclusively composed by monolingual data, our approach includes parallel data as an additional cross-lingual signal."
  - [section 4] "Mixing monolingual and parallel data achieves the highest quality, outperforming both monolingual only and parallel only data."
- Break condition: If the parallel data quality is poor or the alignment between sentences is weak, the benefit may be diminished or reversed.

### Mechanism 2
- Claim: Diverse instruction data improves performance on translation-related tasks.
- Mechanism: By including various tasks like paraphrasing, dialog, and coding alongside translation tasks, the model learns to generalize better and handle a wider range of instructions.
- Core assumption: The model can effectively transfer knowledge from diverse tasks to improve performance on translation-related tasks.
- Evidence anchors:
  - [abstract] "we curate a dataset to specialize LLMs for translation-related tasks, TOWER BLOCKS (§2.2)."
  - [section 2.2] "We build TOWER BLOCKS prioritizing data diversity and quality."
- Break condition: If the diversity of tasks is too high, the model may struggle to specialize in translation-related tasks.

### Mechanism 3
- Claim: Supervised finetuning on high-quality instruction data improves performance on translation-related tasks.
- Mechanism: By finetuning on carefully curated instruction data, the model learns to follow instructions and perform specific tasks related to translation workflows.
- Core assumption: The model can effectively learn from instruction data to improve performance on specific tasks.
- Evidence anchors:
  - [abstract] "we perform supervised finetuning to obtain an instruction-following model tailored for the field of translation, TOWER INSTRUCT (§2.3)."
  - [section 2.3] "As a final step, we obtain TOWER INSTRUCT by finetuning TOWER BASE on TOWER BLOCKS."
- Break condition: If the instruction data quality is poor or the instructions are not clear, the model may not learn effectively.

## Foundational Learning

- Concept: Continued pretraining
  - Why needed here: LLaMA-2's limited exposure to non-English data restricts its multilingual capabilities. Continued pretraining on a multilingual corpus extends its knowledge of other languages.
  - Quick check question: What is the purpose of continued pretraining in this context?

- Concept: Instruction tuning
  - Why needed here: General-purpose LLMs may not perform well on specific tasks without instruction-following capabilities. Instruction tuning enables the model to understand and execute instructions related to translation workflows.
  - Quick check question: How does instruction tuning improve the model's performance on translation-related tasks?

- Concept: Data curation
  - Why needed here: High-quality and diverse data is essential for training effective models. Careful curation of instruction data ensures that the model learns from reliable and representative examples.
  - Quick check question: Why is data curation important for training translation-related models?

## Architecture Onboarding

- Component map: LLaMA-2 -> Multilingual corpus -> TOWER BLOCKS -> TOWER INSTRUCT
- Critical path: 1. Continued pretraining on multilingual corpus 2. Supervised finetuning on TOWER BLOCKS 3. Evaluation on translation-related tasks
- Design tradeoffs:
  - Parallel vs. monolingual data: Parallel data provides explicit alignment but may be harder to obtain and curate.
  - Task diversity vs. specialization: Diverse tasks improve generalization but may reduce specialization.
- Failure signatures:
  - Poor translation quality: Insufficient multilingual data or poor instruction data quality.
  - Inability to follow instructions: Inadequate instruction tuning or unclear instructions.
- First 3 experiments:
  1. Evaluate the impact of parallel data on translation quality.
  2. Assess the effect of task diversity on instruction-following capabilities.
  3. Measure the performance gain from supervised finetuning on translation-related tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of parallel data during continued pretraining affect the performance of TOWER models on other multilingual tasks beyond translation, such as cross-lingual transfer learning or multilingual dialogue systems?
- Basis in paper: [explicit] The paper discusses the importance of parallel data during continued pretraining for improving translation quality (§4), but does not explore its impact on other multilingual tasks.
- Why unresolved: The authors focus primarily on translation-related tasks and do not investigate the broader implications of parallel data on other multilingual applications.
- What evidence would resolve it: Experiments comparing TOWER models with and without parallel data on a variety of multilingual tasks, such as cross-lingual transfer learning benchmarks or multilingual dialogue datasets, would provide insights into the generalizability of the approach.

### Open Question 2
- Question: What is the optimal balance between task diversity and data quality in the TOWER BLOCKS dataset to maximize performance on translation-related tasks while minimizing interference between tasks?
- Basis in paper: [explicit] The paper emphasizes the importance of data diversity and quality in TOWER BLOCKS (§2.2), but also notes that introducing general-purpose instructions recovers translation quality (§4). This suggests a complex interplay between task diversity and data quality.
- Why unresolved: The authors do not provide a systematic analysis of how different ratios of task diversity and data quality affect model performance.
- What evidence would resolve it: Ablation studies varying the proportion of different tasks and the quality of data within each task, followed by evaluation on translation-related tasks, would help determine the optimal balance.

### Open Question 3
- Question: How does the scale of the TOWER model (7B vs. 13B) impact its performance on translation-related tasks, and are there diminishing returns to increasing model size?
- Basis in paper: [explicit] The paper compares the performance of TOWER INSTRUCT 7B and 13B on various tasks (§3), but does not provide a comprehensive analysis of the scaling effects.
- Why unresolved: The authors do not investigate the relationship between model size and performance in detail, nor do they explore the possibility of diminishing returns.
- What evidence would resolve it: Training and evaluating TOWER models of various sizes on a range of translation-related tasks, followed by analysis of the scaling laws, would provide insights into the optimal model size for different applications.

## Limitations

- Data composition uncertainty: The exact distribution of monolingual vs. parallel data across languages remains unspecified
- Evaluation scope limitations: Primary evaluation focuses on English-centric tasks, with limited exploration of truly low-resource language pairs
- Instruction data composition: Specific tasks, templates, and quality control measures for TOWER BLOCKS are not fully detailed

## Confidence

- High confidence: Continued pretraining on multilingual data improves translation quality beyond base LLaMA-2
- Medium confidence: Mixing monolingual and parallel data yields better results than either alone
- Medium confidence: TOWER INSTRUCT achieves competitive results with GPT-4 on translation-related tasks

## Next Checks

1. Ablation study on language representation: Replicate pretraining with varying proportions of monolingual vs. parallel data for specific language families to determine whether improvements are uniform across language types

2. Cross-lingual transfer validation: Test the model on truly low-resource language pairs that were unlikely to be well-represented in the pretraining data to assess limits of cross-lingual transfer capabilities

3. Instruction following robustness test: Evaluate TOWER INSTRUCT on out-of-distribution translation instructions to determine whether instruction tuning generalizes beyond the curated TOWER BLOCKS dataset