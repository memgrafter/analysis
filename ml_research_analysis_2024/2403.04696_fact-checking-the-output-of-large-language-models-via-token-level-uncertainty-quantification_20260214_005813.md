---
ver: rpa2
title: Fact-Checking the Output of Large Language Models via Token-Level Uncertainty
  Quantification
arxiv_id: '2403.04696'
source_url: https://arxiv.org/abs/2403.04696
tags:
- uncertainty
- claims
- probability
- claim
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a fact-checking pipeline for large language
  models (LLMs) that uses token-level uncertainty quantification to detect hallucinations.
  The core method, Claim-Conditioned Probability (CCP), measures the uncertainty of
  atomic claims by focusing on claim uncertainty while ignoring irrelevant surface
  form and claim order uncertainty.
---

# Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification

## Quick Facts
- arXiv ID: 2403.04696
- Source URL: https://arxiv.org/abs/2403.04696
- Reference count: 37
- Primary result: CCP method outperforms baselines for fact-checking across 7 LLMs and 4 languages

## Executive Summary
This paper introduces a fact-checking pipeline for large language models that uses token-level uncertainty quantification to detect hallucinations. The core method, Claim-Conditioned Probability (CCP), measures the uncertainty of atomic claims by focusing on claim uncertainty while ignoring irrelevant surface form and claim order uncertainty. CCP achieves ROC-AUC scores of 0.66-0.71 compared to 0.53-0.64 for the best baseline on English biography data with FactScore annotations.

## Method Summary
The CCP method calculates token-level uncertainty by conditioning on claim type and semantic equivalence, then aggregates these scores to the claim level. The pipeline uses NLI models to filter semantically equivalent token alternatives, extracts atomic claims from generated text, maps claims back to original tokens, and applies uncertainty quantification for factuality classification. The method adds only 3-8% computational overhead over standard LLM inference.

## Key Results
- CCP achieves ROC-AUC scores of 0.66-0.71 vs 0.53-0.64 for baselines on English biography data
- Strong improvements demonstrated across 7 LLMs (Vicuna, Mistral, Jais, GPT-3.5-turbo, Yi, Vikhr) and 4 languages (English, Chinese, Arabic, Russian)
- Human evaluation shows CCP performs competitively with fact-checking tools using external knowledge sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CCP focuses uncertainty measurement on claim uncertainty by filtering out surface form and claim type/order uncertainty.
- Mechanism: CCP conditions probability on claim type and meaning equivalence, normalizing token probabilities to ignore semantically equivalent alternatives.
- Core assumption: Fact-checking cares about factual content, not surface variation or claim ordering.
- Evidence anchors: Abstract states CCP "measures only the uncertainty of a particular claim value expressed by the model." Section 4.2.1 notes relevant uncertainty for fact-checking. Weak corpus citation but method aligns with ablation studies.

### Mechanism 2
- Claim: CCP outperforms traditional token-level uncertainty metrics (max probability, perplexity, entropy) in detecting factual errors.
- Mechanism: By aggregating CCP scores at the claim level and ignoring irrelevant uncertainty sources, CCP produces more discriminative uncertainty scores for fact-checking.
- Core assumption: Fact-checking performance correlates with claim-level uncertainty scores when properly conditioned.
- Evidence anchors: Abstract shows strong improvements for CCP compared to baselines. Section 6.2 demonstrates CCP outperforms all other UQ techniques except one case. Moderate corpus support from ablation studies.

### Mechanism 3
- Claim: CCP provides competitive fact-checking performance without requiring external knowledge sources.
- Mechanism: CCP uses internal LLM token distributions and NLI-based semantic filtering to detect uncertain claims, avoiding the need for knowledge databases.
- Core assumption: Internal model uncertainty captures factual reliability signals sufficient for practical fact-checking.
- Evidence anchors: Abstract states human evaluation reveals CCP is competitive with external knowledge-based tools. Section 6.3 shows human annotation performance slightly higher than FactScore labels. Strong corpus support from direct human evaluation comparison.

## Foundational Learning

- Concept: Token-level uncertainty quantification
  - Why needed here: Fact-checking requires granular uncertainty scores that map back to specific claims in generated text.
  - Quick check question: What's the difference between token-level and sequence-level uncertainty in autoregressive models?

- Concept: Natural Language Inference (NLI) for semantic equivalence
  - Why needed here: CCP uses NLI to group semantically equivalent token alternatives and filter out surface form uncertainty.
  - Quick check question: How does NLI classification help distinguish between meaning-preserving and meaning-changing token substitutions?

- Concept: Claim decomposition and matching
  - Why needed here: The pipeline needs to map atomic claims back to generated text to calculate and apply uncertainty scores.
  - Quick check question: What challenges arise when matching extracted claims to original generated text tokens?

## Architecture Onboarding

- Component map: LLM → Token logits → CCP calculation → Claim-level aggregation → Factuality classification
- Critical path: Token probability extraction → NLI semantic filtering → CCP aggregation → Claim matching → Factuality prediction
- Design tradeoffs: CCP trades computational overhead (NLI model calls) for improved fact-checking accuracy vs. simpler baselines
- Failure signatures: High false positives when NLI model over-classifies as neutral; degraded performance when claim matching fails
- First 3 experiments:
  1. Compare CCP vs. max probability on a small set of biographies with known hallucinations
  2. Test NLI model sensitivity by varying K (number of alternatives) in CCP calculation
  3. Evaluate impact of functional word handling by running CCP with and without stopword filtering

## Open Questions the Paper Calls Out

- The paper doesn't explicitly call out open questions in the text, but several limitations are noted including the focus on biography generation, lack of CCP calibration analysis, and evaluation against only one knowledge source (Wikipedia).

## Limitations

- Task specificity: Evaluation focuses exclusively on biography generation, which has structured claim types that may not generalize to open-ended text generation.
- Claim matching complexity: The pipeline requires mapping atomic claims back to original generated text tokens, but the matching algorithm is not fully specified.
- NLI model dependence: CCP relies on NLI models for semantic filtering, but the paper doesn't report NLI model accuracy or robustness across languages.

## Confidence

- CCP outperforms baselines for fact-checking: High (consistent ROC-AUC improvements across 7 LLMs and 4 languages, human evaluation confirms competitive performance)
- CCP adds minimal computational overhead: High (quantified 3-8% additional cost over standard LLM inference)
- CCP works across multiple languages: Medium (consistent improvements shown but lacks error analysis for language-specific phenomena)

## Next Checks

1. Apply CCP to non-biography generation tasks (e.g., news summarization, technical documentation) to assess generalization beyond structured claim types.

2. Systematically vary NLI model quality (using different pretrained models, fine-tuned versions) and measure impact on CCP performance to quantify dependency on NLI quality.

3. Introduce controlled errors in the claim-to-text matching process and measure how CCP performance degrades to identify whether matching accuracy is a bottleneck for deployment.