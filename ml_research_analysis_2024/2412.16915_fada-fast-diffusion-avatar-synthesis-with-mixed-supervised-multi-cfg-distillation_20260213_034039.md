---
ver: rpa2
title: 'FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation'
arxiv_id: '2412.16915'
source_url: https://arxiv.org/abs/2412.16915
tags:
- distillation
- arxiv
- diffusion
- teacher
- talking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FADA addresses the slow inference speed of diffusion-based audio-driven
  talking avatar methods by proposing a mixed-supervised multi-CFG distillation framework.
  The approach combines adaptive mixed-supervised distillation to leverage data of
  varying quality with learnable token-based multi-CFG distillation to reduce inference
  steps while maintaining audio-image correlation.
---

# FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation

## Quick Facts
- **arXiv ID**: 2412.16915
- **Source URL**: https://arxiv.org/abs/2412.16915
- **Reference count**: 40
- **Primary result**: 4.17-12.5× speedup over diffusion baselines while maintaining video quality and audio-visual synchronization

## Executive Summary
FADA addresses the slow inference speed of diffusion-based audio-driven talking avatar methods by proposing a mixed-supervised multi-CFG distillation framework. The approach combines adaptive mixed-supervised distillation to leverage data of varying quality with learnable token-based multi-CFG distillation to reduce inference steps while maintaining audio-image correlation. The method achieves 4.17-12.5× speedup compared to state-of-the-art diffusion models while generating videos with comparable quality, as validated across multiple datasets.

## Method Summary
FADA proposes a mixed-supervised multi-CFG distillation framework for fast audio-driven talking avatar synthesis. The method uses adaptive mixed-supervised distillation that combines teacher-supervised loss with ground-truth loss, leveraging both high-quality (160 hours) and moderate-quality (1300 hours) training data. A learnable token-based multi-CFG distillation technique is introduced to reduce inference steps from 25-50 to just 6 DDIM steps while maintaining audio-image correlation. The student model is trained to mimic multi-CFG inference through injected learnable CFG tokens in cross-attention layers, achieving significant speedup without quality degradation.

## Key Results
- Achieves 4.17-12.5× speedup compared to diffusion baselines
- Maintains comparable video quality with IQA, FVD, and FID metrics
- Preserves audio-visual synchronization (Sync-D) while reducing inference steps
- Demonstrates effectiveness across CelebV-HQ, HDTF, and Loopy openset test sets

## Why This Works (Mechanism)
The mixed-supervised distillation framework works by combining high-quality data for precise teacher supervision with moderate-quality data to improve student model robustness and generalization. The adaptive loss weighting ensures optimal use of both data types based on their quality characteristics. The learnable token-based multi-CFG distillation reduces inference steps by training the student to capture the essential information from multiple CFG scales through a single set of learnable tokens, which are injected into the cross-attention layers to mimic the effects of multi-CFG inference without requiring multiple forward passes.

## Foundational Learning

**Diffusion Probabilistic Models**: Generative models that denoise data through iterative steps, essential for understanding the inference bottleneck FADA addresses.
- *Why needed*: FADA's core innovation is accelerating diffusion-based avatar synthesis by reducing inference steps.
- *Quick check*: Verify understanding of how diffusion models work through forward and reverse processes.

**Knowledge Distillation**: Technique where a smaller student model learns from a larger teacher model, fundamental to FADA's approach.
- *Why needed*: FADA uses mixed-supervised distillation to transfer knowledge from teacher to student while leveraging data of varying quality.
- *Quick check*: Confirm understanding of teacher-student training dynamics and loss functions.

**CFG (Classifier-Free Guidance) Scale**: Controls the trade-off between fidelity and diversity in diffusion models, critical for FADA's multi-CFG distillation.
- *Why needed*: FADA's multi-CFG approach uses different CFG scales to capture different aspects of the audio-image correlation.
- *Quick check*: Understand how varying CFG scales affects generation quality and diversity.

## Architecture Onboarding

**Component Map**: Dataset (A,B) -> Teacher Model (ϵ-prediction DDPM) -> Mixed-Supervised Loss (Adaptive Weighting) -> Student Model (Learnable CFG Tokens) -> 6-step DDIM Inference

**Critical Path**: High-quality dataset A pre-trains teacher model → Mixed-supervised distillation with dataset B → Learnable token-based multi-CFG distillation → 6-step DDIM inference

**Design Tradeoffs**: 
- Mixed-supervised approach balances quality (high-quality data) with robustness (moderate-quality data) at the cost of increased training complexity
- Learnable tokens reduce inference steps but require careful training to capture multi-CFG information effectively
- Adaptive loss weighting optimizes data utilization but introduces hyperparameter sensitivity

**Failure Signatures**:
- Significant quality degradation indicates improper teacher-student knowledge transfer or ineffective learnable token training
- Poor audio-visual synchronization suggests learnable tokens fail to capture audio-reference correlation
- Inconsistent performance across datasets may indicate overfitting to specific data distributions

**3 First Experiments**:
1. Validate teacher model quality on high-quality dataset A using ϵ-prediction DDPM loss and standard metrics
2. Test mixed-supervised distillation with adaptive loss weighting on a small subset to verify quality improvement
3. Implement learnable token-based multi-CFG distillation and measure impact on FVD and Sync-D metrics

## Open Questions the Paper Calls Out

**Open Question 1**: What is the optimal adaptive weight strategy for the mixed-supervised loss that maximizes performance across diverse data distributions?
- *Basis in paper*: The paper discusses an adaptive method for balancing teacher-supervised and ground-truth-supervised losses based on the ratio R, but notes that hyperparameters are relatively robust across different data distributions.
- *Why unresolved*: While the paper provides a specific adaptive strategy with hyperparameters, it does not explore the full parameter space or alternative adaptive strategies that might yield better performance.
- *What evidence would resolve it*: Systematic ablation studies varying the adaptive strategy parameters and comparing against other adaptive methods on diverse datasets would identify the optimal approach.

**Open Question 2**: How does the proposed multi-CFG distillation with learnable tokens generalize to other multimodal diffusion tasks beyond audio-driven talking avatar synthesis?
- *Basis in paper*: The paper mentions that the adaptive mixed-supervised distillation can extend to tasks like text-to-image or text-to-video, but does not explore this extension.
- *Why unresolved*: The paper focuses exclusively on the audio-driven talking avatar task and does not investigate the applicability of the multi-CFG distillation approach to other multimodal diffusion tasks.
- *What evidence would resolve it*: Applying the multi-CFG distillation framework to text-to-image and text-to-video tasks and comparing performance against existing distillation methods would demonstrate generalizability.

**Open Question 3**: What is the impact of different data filtering thresholds on the final model performance when using the proposed mixed-supervised distillation approach?
- *Basis in paper*: The paper describes strict data filtering for the teacher model and relaxed filtering for the student model during distillation, but does not systematically explore how different filtering thresholds affect performance.
- *Why unresolved*: While the paper demonstrates that using moderate-quality data during distillation improves robustness, it does not provide a comprehensive analysis of how varying the quality thresholds affects the final model's performance.
- *What evidence would resolve it*: Conducting experiments with different combinations of data filtering thresholds for both teacher and student models while measuring performance metrics would reveal the optimal filtering strategy.

## Limitations
- Specific filtering thresholds for dataset quality differentiation are not provided, critical for reproducing the mixed-supervised training regime
- Implementation details of the dual-Unet architecture components (reference net, audio attention, temporal attention) lack sufficient technical specificity
- Comparison is limited to diffusion-based approaches without benchmarking against non-diffusion methods like GAN-based systems
- Reliance on large-scale, carefully curated datasets (160 hours for high-quality, 1300 hours for moderate-quality) presents practical barriers for independent replication

## Confidence

**High confidence**: The core methodology of mixed-supervised distillation and learnable token-based multi-CFG reduction is clearly articulated and logically sound. The claimed speedup factors and the general framework for combining teacher-supervised and ground-truth losses are well-specified.

**Medium confidence**: The quantitative results showing comparable quality metrics (IQA, FVD, Sync-D) at reduced inference steps are supported by the methodology, though the exact evaluation protocols and implementation details could affect reproducibility.

**Low confidence**: The qualitative claims about superior performance on the Loopy openset test set and the specific filtering criteria for dataset curation lack sufficient detail for independent verification.

## Next Checks
1. Implement the mixed-supervised distillation framework with the adaptive loss weighting scheme and verify that moderate-quality data improves student model performance without degrading video quality or audio-visual synchronization.
2. Validate the learnable token-based multi-CFG distillation by testing different CFG scales and measuring the impact on FVD and Sync-D metrics to ensure the tokens properly capture audio-reference correlation.
3. Reproduce the 6-step DDIM inference pipeline and compare the generated videos against ground truth using the specified metrics (IQA, FVD, FID, E-FID, Sync-D, NFE-D) to confirm the claimed 4.17-12.5× speedup while maintaining quality.