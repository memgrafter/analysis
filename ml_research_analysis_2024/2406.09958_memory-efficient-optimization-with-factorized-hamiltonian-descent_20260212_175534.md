---
ver: rpa2
title: Memory-Efficient Optimization with Factorized Hamiltonian Descent
arxiv_id: '2406.09958'
source_url: https://arxiv.org/abs/2406.09958
tags:
- optimization
- momentum
- hamiltonian
- arxiv
- trace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel class of memory-efficient optimizers,
  H-Fac, based on Hamiltonian dynamics. The key idea is to factorize both the momentum
  and scaling parameter estimators using rank-1 parameterization, reducing memory
  costs to a sublinear level.
---

# Memory-Efficient Optimization with Factorized Hamiltonian Descent

## Quick Facts
- arXiv ID: 2406.09958
- Source URL: https://arxiv.org/abs/2406.09958
- Reference count: 40
- Primary result: H-Fac achieves 75.79% top-1 accuracy on ResNet50 and 78.35% on ViT-S/16 for ImageNet1K with memory efficiency comparable to vanilla SGD

## Executive Summary
This paper introduces H-Fac, a novel class of memory-efficient optimizers that leverage Hamiltonian dynamics and rank-1 parameterization to reduce memory costs from O(mn) to O(m+n) while maintaining competitive performance. The method factorizes both momentum and scaling parameter estimators, providing theoretical convergence guarantees through a Hamiltonian framework. Empirically, H-Fac demonstrates strong performance across various architectures including ResNets, Vision Transformers, and language models, achieving results comparable to or better than baselines like AdamW and Adafactor while offering significant memory savings.

## Method Summary
H-Fac is based on Hamiltonian dynamics and uses rank-1 parameterization for both momentum and scaling parameter estimators. The optimizer maintains two rank-1 vectors u and v instead of a full m×n momentum matrix, significantly reducing memory from O(mn) to O(m+n). The method derives from an ordinary differential equation that solves a minimization problem with constrained factors, providing theoretical convergence guarantees. The parameter update combines normalized momentum factorization, clipped normalized gradients, and decoupled weight decay. The approach includes corrected terms essential for convergence and normalizes momentum by second-moment estimators to address magnitude issues and improve stability.

## Key Results
- H-Fac achieves 75.79% top-1 accuracy on ResNet50 and 78.35% on ViT-S/16 for ImageNet1K
- Memory efficiency comparable to vanilla SGD without momentum while maintaining competitive performance
- Outperforms signFSGD and matches or exceeds AdamW/Adafactor baselines across tested architectures
- Demonstrates scalability to large language models including LLaMA-60M and LLaMA-130M

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factorizing momentum estimators using rank-1 parameterization reduces memory from O(mn) to O(m+n) while maintaining comparable performance to full-rank methods.
- Mechanism: The algorithm maintains two rank-1 vectors u (size m) and v (size n) instead of a full m×n momentum matrix. The model update uses sign(βû1ᵀn + G) + sign(β1ᵐvᵀ + G) where û and v̂ are corrected momentum estimates.
- Core assumption: The row and column mean statistics of gradients are sufficient to capture essential momentum information for convergence.
- Evidence anchors:
  - [abstract] "By employing a rank-1 parameterization for both momentum and scaling parameter estimators, H-Fac reduces memory costs to a sublinear level"
  - [section] "Compared with algorithms that maintain a full rank momentum matrix M ∈ Rm×n, this algorithm employs two rank-one momentum vectors u ∈ Rm and v ∈ Rn, which significantly reduces the memory cost from O(mn) to O(m+n)"
  - [corpus] Weak evidence - related works focus on low-rank gradient approximation but don't directly validate the rank-1 momentum factorization claim
- Break condition: If gradient distributions are highly non-uniform or have significant variance between rows/columns, the rank-1 approximation may lose critical directional information.

### Mechanism 2
- Claim: The Hamiltonian framework provides theoretical convergence guarantees for the factorized optimizers.
- Mechanism: The optimization trajectory follows a Hamiltonian H(W,M,r,s) that monotonically decreases along the ODE trajectory, ensuring convergence to local optima via LaSalle's invariance principle.
- Core assumption: The Hamiltonian function properly captures the optimization dynamics and its derivative is negative definite along trajectories.
- Evidence anchors:
  - [abstract] "We develop our algorithms based on principles derived from Hamiltonian dynamics, providing robust theoretical underpinnings in optimization dynamics and convergence guarantees"
  - [section] "The Hamiltonian H(.) is an augmented function of the original objective f(.) and it satisfies minM H(W,M) = L(W) ∀W, meaning that minimizing H(W,M) will reduce to minimizing f(W)"
  - [corpus] Moderate evidence - related work "Hamiltonian Descent Algorithms for Optimization: Accelerated Rates via Randomized Integration Time" supports the general framework
- Break condition: If the Hamiltonian is not properly constructed or if the discretization introduces significant errors, the theoretical guarantees may not hold.

### Mechanism 3
- Claim: Normalizing factorized momentum by second-moment estimators addresses magnitude issues and improves stability.
- Mechanism: The H-Fac update combines three elements: normalized momentum factorization 0.5*(ϕterm + ψterm), clipped normalized gradient, and decoupled weight decay. The momentum terms are scaled by row/column means of second moments.
- Core assumption: The second-moment statistics provide appropriate normalization that preserves update direction while controlling magnitude.
- Evidence anchors:
  - [abstract] "By employing rank-1 parameterization for both momentum and scaling parameter estimators, H-Fac reduces memory costs to a sublinear level while maintaining competitive performance"
  - [section] "Our model parameters update can be seen as an accumulator of normalized gradients, where both the momentum and the current gradient are rescaled by their corresponding cumulative second-moment information"
  - [corpus] Weak evidence - related works like "Memory-Efficient LLM Training with Online Subspace Descent" focus on gradient subspace projection rather than momentum normalization
- Break condition: If the second-moment estimates become unstable or if the normalization factor becomes too small/large, the update may become unstable or vanish.

## Foundational Learning

- Concept: Hamiltonian mechanics and Lagrangian dynamics
  - Why needed here: The paper builds the optimizer design on Hamiltonian principles, using energy functions to ensure convergence
  - Quick check question: What property must a Hamiltonian function satisfy to guarantee convergence in optimization contexts?

- Concept: Ordinary differential equations and discretization methods
  - Why needed here: The continuous-time Hamiltonian dynamics are discretized using Euler method to create practical algorithms
  - Quick check question: How does the choice of discretization method affect the theoretical guarantees of the Hamiltonian-based optimizer?

- Concept: Low-rank matrix factorization and its trade-offs
  - Why needed here: The memory efficiency comes from factorizing moment estimators into rank-1 components, requiring understanding of when this approximation is valid
  - Quick check question: Under what conditions does rank-1 factorization of gradient statistics preserve sufficient information for optimization?

## Architecture Onboarding

- Component map:
  - Input: Model parameters W (matrix), gradient ∇f(W)
  - Momentum state: u, v vectors (rank-1 momentum factors)
  - Second-moment state: r, s vectors (row/column scaling factors)
  - Output: Updated parameters W'
  - Core loop: Gradient computation → momentum update → scaling update → parameter update

- Critical path:
  1. Compute gradient G = ∇f(W)
  2. Update momentum factors: u = β1*u_prev + (1-β1)*G*1n/n, v = β1*v_prev + (1-β1)*Gᵀ*1m/m
  3. Update scaling factors: r = β2*r_prev + (1-β2)*(G²)*1n, s = β2*s_prev + (1-β2)*(Gᵀ)²*1m
  4. Compute normalized momentum terms with corrections
  5. Apply parameter update with clipping and weight decay

- Design tradeoffs:
  - Memory vs. Accuracy: Rank-1 factorization reduces memory but may lose some gradient information
  - Stability vs. Speed: Second-moment normalization improves stability but adds computational overhead
  - Simplicity vs. Performance: The corrected terms are essential for convergence but add complexity

- Failure signatures:
  - Training instability: Check if scaling factors r, s become zero or NaN
  - Poor convergence: Verify that momentum corrections are being applied correctly
  - Memory leaks: Ensure rank-1 vectors are properly allocated and not growing

- First 3 experiments:
  1. Implement basic H-Fac on a small CNN (e.g., CIFAR-10) and compare convergence to SGD
  2. Test memory usage of H-Fac vs. Adam on a large model to verify sublinear scaling
  3. Validate the corrected terms by comparing with and without them on ResNet architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of H-Fac compare to AdamW and Adafactor on modern convolution-free architectures like Vision Transformers when trained on large-scale datasets like ImageNet-21K?
- Basis in paper: [explicit] The paper mentions that the authors aim to extend the scalability of their algorithms to encompass modern convolution-free architectures, but the experiments only show results on ImageNet-1K with ViT-S/16 and ViT-B/32.
- Why unresolved: The paper does not provide experimental results on larger-scale datasets or deeper Vision Transformer architectures.
- What evidence would resolve it: Running H-Fac on ImageNet-21K with larger ViT models (e.g., ViT-L/16) and comparing performance metrics (accuracy, convergence speed, memory usage) to AdamW and Adafactor would provide definitive evidence.

### Open Question 2
- Question: What is the theoretical convergence rate of H-Fac under non-convex optimization settings, and how does it compare to other adaptive optimizers?
- Basis in paper: [explicit] The paper mentions that the algorithms provide "robust theoretical underpinnings in optimization dynamics and convergence guarantees," but does not specify the exact convergence rates.
- Why unresolved: The paper provides theoretical analysis showing convergence to local optima but does not quantify the convergence rate or compare it to other methods.
- What evidence would resolve it: Deriving explicit convergence rate bounds for H-Fac under standard non-convex optimization assumptions and comparing them to known bounds for Adam, Adafactor, and other adaptive methods would resolve this question.

### Open Question 3
- Question: How does the double-β scheme in LionFactor affect the optimization dynamics and memory efficiency compared to the original Lion optimizer?
- Basis in paper: [explicit] The paper mentions that LionFactor incorporates a double-β scheme similar to Lion, but notes that it "makes a lot of sense to explore more efficient algorithms to factorize the momentum in Lion optimizer."
- Why unresolved: The paper only provides preliminary experimental results showing LionFactor performs better than signFSGD on ResNet architectures, but does not provide a detailed analysis of how the double-β scheme affects the optimization dynamics or memory efficiency.
- What evidence would resolve it: Conducting a comprehensive study comparing the optimization trajectories, convergence rates, and memory usage of LionFactor and Lion on various architectures and datasets would provide evidence to resolve this question.

## Limitations

- The theoretical guarantees rely on continuous-time Hamiltonian formulation, but practical implementation uses Euler discretization which may not preserve all convergence properties
- The rank-1 approximation assumption is strong and lacks empirical validation of when this factorization fails
- Performance comparisons focus on specific architectures without exploring a broader range of tasks where memory-efficiency benefits would be most critical

## Confidence

**High Confidence**: The memory-efficiency claims are well-supported by the O(m+n) vs O(mn) complexity analysis and empirical memory measurements showing comparable usage to SGD. The basic mechanism of rank-1 factorization is mathematically sound.

**Medium Confidence**: The convergence guarantees from the Hamiltonian framework are theoretically valid but may not fully transfer to the discrete implementation. The performance claims on ImageNet are supported but the margins over baselines are modest, suggesting the method works but may not be dramatically superior.

**Low Confidence**: The claim that the corrected terms are "essential for convergence" lacks ablation studies showing what happens without them. The robustness of the second-moment normalization across diverse gradient distributions is not thoroughly validated.

## Next Checks

1. **Factorization Quality Assessment**: Systematically evaluate the rank-1 approximation error across different layers and model architectures during training, measuring how well the row/column means capture the full gradient statistics and identifying scenarios where the approximation breaks down.

2. **Hyperparameter Sensitivity Analysis**: Conduct comprehensive experiments varying β1, β2, learning rates, and weight decay to understand the stability boundaries of H-Fac, particularly focusing on when the second-moment normalization becomes problematic or when the corrected terms are most critical.

3. **Memory-Accuracy Pareto Frontier**: Compare H-Fac against other memory-efficient optimizers (Adafactor, Memory-efficient Adam) across a range of model sizes and batch sizes to quantify the actual memory savings at equivalent accuracy levels, testing whether the sublinear scaling translates to practical advantages on very large models.