---
ver: rpa2
title: Towards Explainability in Legal Outcome Prediction Models
arxiv_id: '2403.16852'
source_url: https://arxiv.org/abs/2403.16852
tags:
- precedent
- legal
- case
- cases
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how legal outcome prediction models use
  precedent by proposing a method based on influence functions to identify which training
  cases most affect model predictions. The authors develop a taxonomy of legal precedent
  (applied vs.
---

# Towards Explainability in Legal Outcome Prediction Models

## Quick Facts
- arXiv ID: 2403.16852
- Source URL: https://arxiv.org/abs/2403.16852
- Reference count: 40
- Models show strongest correlation with applied-positive precedent but weak alignment with other types

## Executive Summary
This paper investigates how legal outcome prediction models use precedent by proposing a method based on influence functions to identify which training cases most affect model predictions. The authors develop a taxonomy of legal precedent (applied vs. distinguished, positive vs. negative) and apply it to the ECtHR corpus. Their experiments reveal that models show strongest correlation with applied-positive precedent but weak or negative correlations with other types, indicating current models fail to emulate the full spectrum of precedential reasoning used by human judges. Notably, higher F1 scores do not correlate with better alignment with precedent, suggesting model performance is a poor proxy for legal reasoning quality.

## Method Summary
The authors propose using influence functions to identify which training cases affect model predictions for specific test cases. They develop a taxonomy of legal precedent based on whether cases are applied or distinguished and whether the precedent is positive or negative. The method computes influence scores using second-order derivatives and correlates these with the precedent taxonomy categories. Two neural models (Simple and Joint) are trained on the ECtHR corpus with LEGAL-BERT and BERT encoders. The influence scores are then mapped to the four taxonomy categories and Spearman's ρ is calculated to measure alignment with human precedential reasoning.

## Key Results
- Models show strongest correlation with applied-positive precedent (Spearman's ρ up to 0.18)
- Weak or negative correlations with applied-negative, distinguished-positive, and distinguished-negative precedent
- Higher F1 scores do not correlate with better alignment with precedent
- Simple model shows negative correlation with distinguished precedent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Influence functions approximate how much a training case affects a model's prediction without full retraining.
- Mechanism: The method computes a small perturbation ε in the loss weight for a training case z, then uses the chain rule and second-order derivatives to estimate the parameter change dθ⋆z/dε. This change is multiplied by the gradient of the test case loss to yield the influence score ι(z,z′).
- Core assumption: The optimization objective is twice differentiable and the Hessian ∇²θLR(θ⋆) is invertible.
- Evidence anchors:
  - [section] "Influence score... ι(z,z′) = −∇θℓz′(θ⋆)⊤∇²θLR(θ⋆)⁻¹∇θℓz(θ⋆)"
  - [section] "Because Θ is assumed to be compact and L(θ) + αR(θ) is a continuous function of θ the argmin given in Eq. (3) is well-defined."
- Break condition: If the Hessian is ill-conditioned or non-invertible, the approximation fails.

### Mechanism 2
- Claim: Precedent types are recoverable from citation patterns and case outcomes in the ECtHR corpus.
- Mechanism: By extracting case citations using regex and comparing positive/negative outcomes between citing and cited cases, the taxonomy labels each precedent relationship as applied/distinguished and positive/negative.
- Core assumption: Citation format in ECtHR cases is regular and outcome labels are consistent.
- Evidence anchors:
  - [section] "Because precedent is binding law... the relationship between any two cases can simultaneously be both positive and negative, though it can not be both applied and distinguished."
  - [section] "We use regular expressions for this purpose because citations in the ECtHR corpus follow a regular form."
- Break condition: Irregular citation formats or missing outcome labels would break the taxonomy extraction.

### Mechanism 3
- Claim: Model alignment with human precedential reasoning can be measured by correlating influence scores with the taxonomy.
- Mechanism: For each test case, compute influence scores for all training cases, map those to the four taxonomy categories, then calculate Spearman's ρ between influence scores and precedent labels (cited vs claimed, per-case vs per-article).
- Core assumption: Influence scores reflect actual reliance on precedent cases in the model's reasoning.
- Evidence anchors:
  - [section] "We compute the correlation between c and s and report Spearman's ρ."
  - [section] "We find that influence scores correlate more strongly with applied precedent than distinguished precedent."
- Break condition: If influence scores do not correspond to actual model reasoning, correlations will be misleading.

## Foundational Learning

- Concept: Influence functions in machine learning
  - Why needed here: To estimate how individual training cases affect predictions without retraining.
  - Quick check question: What is the mathematical form of the influence score used in this paper?

- Concept: Spearman's rank correlation
  - Why needed here: To measure monotonic relationships between influence scores and precedent categories without assuming linearity.
  - Quick check question: Why might Spearman's ρ be preferred over Pearson's r in this context?

- Concept: Legal precedent taxonomy (applied/distinguished, positive/negative)
  - Why needed here: To categorize how human judges use past cases and compare with model behavior.
  - Quick check question: What distinguishes applied-positive from distinguished-negative precedent?

## Architecture Onboarding

- Component map:
  Data ingestion -> citation extraction -> taxonomy labeling -> model training -> influence score computation -> correlation analysis

- Critical path:
  1. Build citation network from ECtHR corpus
  2. Train legal outcome prediction models
  3. Compute influence scores for all train-test pairs
  4. Map influence scores to precedent taxonomy
  5. Calculate and interpret correlations

- Design tradeoffs:
  - Per-case vs per-article scope: broader coverage vs finer granularity
  - Cited vs claimed precedent: narrower relevance vs more comprehensive
  - Simple vs Joint model: binary vs multi-class outcome modeling

- Failure signatures:
  - Near-zero or negative correlations suggest models don't use precedent like humans
  - High F1 but low correlation indicates performance ≠ legal reasoning quality
  - Ill-conditioned Hessian in influence computation

- First 3 experiments:
  1. Compute influence scores for a small subset and verify against manual retraining.
  2. Compare per-case vs per-article correlations for a single Article.
  3. Test model-based vs ground-truth precedent correlations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do neural models of legal outcome prediction fail to properly utilize negative and distinguished precedent, even though these are important forms of precedential reasoning used by human judges?
- Basis in paper: [explicit] The authors state "the models learn to predict outcomes reasonably well, their use of precedent is unlike that of human judges" and find "negative correlation with distinguished precedent" under the Simple model.
- Why unresolved: The paper identifies this as a key finding but does not investigate the underlying reasons for this failure. Is it due to the training data, model architecture, or something else?
- What evidence would resolve it: Experiments testing whether this is due to (1) training data quality (e.g., training on different datasets), (2) model architecture (e.g., testing different model types), or (3) optimization methods (e.g., different training procedures).

### Open Question 2
- Question: Would training models on legal briefs instead of case transcripts improve their ability to utilize the full spectrum of precedential reasoning?
- Basis in paper: [inferred] The authors note that "facts in current datasets contain superficial indicators of the outcome—they are written as post-hoc justification" and suggest "Using datasets built around legal briefs, instead of cases, could be a step towards mitigating this issue."
- Why unresolved: This is a suggestion for future work but has not been tested. Would legal briefs contain richer precedent information?
- What evidence would resolve it: Training and testing models on a dataset of legal briefs and comparing their precedent usage to models trained on case transcripts.

### Open Question 3
- Question: Would incorporating precedent retrieval into the model architecture induce more human-like precedential reasoning?
- Basis in paper: [inferred] The authors state "To induce precedential reasoning, a natural step would be to incorporate precedent retrieval as part of the model architecture."
- Why unresolved: This is proposed as a future direction but not implemented. Would explicitly retrieving precedent cases help models reason more like judges?
- What evidence would resolve it: Training models with an explicit precedent retrieval component and measuring their alignment with human judges' precedential reasoning.

## Limitations

- Influence function approximations may not accurately capture true model reliance on precedent cases
- Citation extraction relies on regex patterns that may miss nuanced legal citations
- Taxonomy depends on consistent outcome labeling across cases which may not always hold

## Confidence

- High confidence: Mathematical framework of influence functions is well-established
- Medium confidence: Citation extraction and taxonomy labeling methodology
- Medium confidence: Core finding about models' limited use of precedential reasoning

## Next Checks

1. Validate influence score approximations by computing exact influence on a small subset of cases through full retraining, comparing results to the approximation method.
2. Cross-validate the citation extraction and taxonomy labeling by manually annotating a random sample of 100 precedent relationships from the ECtHR corpus.
3. Test whether incorporating explicit precedent attention mechanisms in the model architecture improves correlation with applied-positive precedent while maintaining competitive F1 scores.