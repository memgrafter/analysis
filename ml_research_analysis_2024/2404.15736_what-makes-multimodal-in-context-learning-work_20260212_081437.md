---
ver: rpa2
title: What Makes Multimodal In-Context Learning Work?
arxiv_id: '2404.15736'
source_url: https://arxiv.org/abs/2404.15736
tags:
- rices
- image
- m-icl
- demonstrations
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates how different modalities
  (text vs. image) affect Multimodal In-Context Learning (M-ICL) performance in large
  multimodal models.
---

# What Makes Multimodal In-Context Learning Work?

## Quick Facts
- arXiv ID: 2404.15736
- Source URL: https://arxiv.org/abs/2404.15736
- Authors: Folco Bertini Baldassini; Mustafa Shukor; Matthieu Cord; Laure Soulier; Benjamin Piwowarski
- Reference count: 40
- Primary result: M-ICL performance is predominantly text-driven, with images having minimal impact except in image-to-text tasks

## Executive Summary
This study systematically investigates how different modalities (text vs. image) affect Multimodal In-Context Learning (M-ICL) performance in large multimodal models. Using IDEFICS and OpenFlamingo, the authors evaluate M-ICL across diverse vision-language tasks by manipulating context demonstrations through modality removal, randomization, and similarity-based retrieval (RICES). Key findings include: (1) M-ICL is predominantly text-driven, with images having minimal impact except in image-to-text tasks like captioning and classification; (2) RICES does not significantly outperform a simple majority-voting baseline for classification tasks; (3) The model exhibits recency bias, tending to copy the last demonstration's response when demonstrations are similar. These results highlight fundamental limitations of current M-ICL approaches and suggest that performance gains primarily stem from retrieving contextually relevant responses rather than genuine multimodal learning.

## Method Summary
The authors conducted systematic experiments with two large multimodal models (IDEFICS and OpenFlamingo) across various vision-language tasks. They manipulated context demonstrations through three main approaches: (1) Modality removal - systematically removing text or image components from demonstrations to isolate their individual contributions; (2) Randomization - randomizing demonstration order to test for recency bias; (3) RICES retrieval - using a similarity-based retrieval method (RICES) that selects demonstrations based on multimodal embedding similarity, compared against a simple majority-voting baseline. The experiments covered tasks including image captioning, classification, and visual reasoning, allowing the authors to assess how different modalities contribute to M-ICL performance under various conditions.

## Key Results
- M-ICL performance is predominantly text-driven, with images having minimal impact except in image-to-text tasks
- RICES retrieval method does not significantly outperform simple majority-voting baseline for classification tasks
- Models exhibit recency bias, copying the last demonstration's response when demonstrations are similar

## Why This Works (Mechanism)
The mechanism behind M-ICL performance appears to rely heavily on text-based pattern matching rather than genuine multimodal integration. When demonstrations are presented, the model seems to primarily extract textual patterns and associations, using images mainly as supplementary context for tasks where visual information is directly relevant to the output format. The lack of significant performance gains from RICES suggests that the model's in-context learning is not effectively leveraging the semantic richness of multimodal embeddings, instead relying on surface-level text similarity. The recency bias indicates that when demonstrations are semantically similar, the model defaults to copying the most recent example rather than synthesizing information across multiple examples, suggesting limitations in the model's ability to integrate multiple demonstrations effectively.

## Foundational Learning
1. **In-Context Learning (ICL)**: The ability of models to learn from examples provided in the prompt without parameter updates
   - Why needed: Forms the foundation for understanding how models can adapt to new tasks without fine-tuning
   - Quick check: Can the model perform a task after seeing 1-3 examples in the prompt?

2. **Multimodal Embeddings**: Vector representations that capture both visual and textual information in a shared space
   - Why needed: Essential for understanding how models process and relate different modalities
   - Quick check: Can embeddings from different modalities be meaningfully compared or combined?

3. **Similarity-based Retrieval**: Methods for selecting relevant examples based on similarity in embedding space
   - Why needed: Core to understanding how demonstration selection affects M-ICL performance
   - Quick check: Does retrieval improve performance compared to random demonstration selection?

## Architecture Onboarding
**Component Map**: Input Image/Text → Encoder → Shared Embedding Space → Attention Mechanism → Output Generation
**Critical Path**: Image/Text → Encoder → Attention → Output
**Design Tradeoffs**: Joint embedding vs. separate encoders; modality fusion strategies; retrieval vs. random demonstration selection
**Failure Signatures**: Text-dominance despite multimodal input; recency bias; retrieval methods not outperforming simple baselines
**3 First Experiments**:
1. Ablation study: Remove text vs. remove images from demonstrations
2. Randomization test: Randomize demonstration order and measure performance drop
3. Retrieval comparison: Compare RICES vs. majority voting baseline

## Open Questions the Paper Calls Out
None

## Limitations
- M-ICL appears predominantly text-driven rather than genuinely multimodal
- RICES retrieval does not outperform simple majority voting for classification tasks
- Models exhibit recency bias, copying last demonstration when demonstrations are similar

## Confidence
- **High Confidence**: M-ICL is predominantly text-driven (systematic ablation studies across tasks and models)
- **Medium Confidence**: RICES doesn't outperform majority voting (reliable for tested classification tasks but may not generalize)
- **Medium Confidence**: Recency bias observed consistently (could benefit from additional probing experiments)

## Next Checks
1. Test whether text-dominance holds for more complex multimodal reasoning tasks requiring deeper visual-textual integration
2. Probe different model architectures (separate encoders vs. joint embeddings) to determine if text-dominance is fundamental or architecture-specific
3. Conduct human evaluation of similarity judgments to validate whether improving alignment could enhance retrieval performance beyond majority voting