---
ver: rpa2
title: 'DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning'
arxiv_id: '2401.09068'
source_url: https://arxiv.org/abs/2401.09068
tags:
- pruning
- weights
- dtmm
- each
- convolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DTMM is a library for efficient deployment of ML models on resource-constrained
  MCUs. It addresses the limitations of structured and unstructured pruning by introducing
  filterlet-based pruning, a compact storage structure (FWCS), and a novel convolution
  operator with SIMD acceleration.
---

# DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning

## Quick Facts
- arXiv ID: 2401.09068
- Source URL: https://arxiv.org/abs/2401.09068
- Reference count: 40
- Key outcome: Achieves up to 42.8% model size reduction, 27.7% latency reduction, and maintains high accuracy compared to state-of-the-art methods on VGG-11, ResNet-12, and YOLO models.

## Executive Summary
DTMM is a library designed for efficient deployment of ML models on resource-constrained MCUs. It addresses the limitations of existing pruning methods by introducing filterlet-based pruning, a compact storage structure (FWCS), and a novel convolution operator with SIMD acceleration. The approach achieves significant improvements in model size and latency while maintaining accuracy, making it suitable for extremely weak IoT devices.

## Method Summary
DTMM employs filterlet-based pruning to achieve finer granularity than structured pruning while avoiding the indexing overhead of unstructured pruning. The method uses a compact FWCS storage structure and a novel convolution operator with SIMD acceleration. A pruning strategy scheduler finds an optimal pruning plan that minimizes latency within accuracy and memory constraints by estimating filterlet importance through loss gradient and formulating a constrained optimization problem.

## Key Results
- Achieves up to 42.8% model size reduction compared to state-of-the-art methods
- Reduces latency by up to 27.7% while maintaining high accuracy
- Demonstrates significant performance gains on VGG-11, ResNet-12, and YOLO models across CIFAR-10, VWW, and FDDB datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filterlet pruning achieves finer granularity than structured pruning while avoiding indexing overhead of unstructured pruning
- Mechanism: Filterlets group all weights at the same position across all channels in a filter. Since these weights are contiguous in memory, DTMM can store them compactly in FWCS without per-weight indexing, enabling SIMD acceleration
- Core assumption: On MCUs, weights are stored in channel-major order, so filterlet weights remain contiguous after pruning
- Evidence anchors: [abstract] "We choose to group an entire line of weights at the same position across all channels in the filter as an atomic pruning unit"; [section III-A] "we group all the weights at the same position across all channels to form a filterlet"; [corpus] Weak - related papers discuss TinyML and model compression but do not mention filterlet-based pruning or FWCS storage
- Break condition: If MCU storage order changes or filterlet grouping breaks contiguity, SIMD acceleration and compact storage fail

### Mechanism 2
- Claim: DTMM's convolution operator achieves latency reduction through SIMD data-level parallelism and instruction-level parallelism
- Mechanism: Filterlet contiguity allows loading multiple weights and feature values into SIMD registers in one instruction. Instruction reordering overlaps memory loads and MAC operations to keep ALU busy, eliminating idle cycles
- Core assumption: MCU SIMD units can execute loads and MACs in parallel, and filterlet weights can be reused across multiple MACs
- Evidence anchors: [section III-B] "Weights in filterlet can be operated by SIMD" and "we propose to maintain the value of a register...to alternately load values"; [section III-B] "the ALU is periodically idle as the memory unit needs to be loaded with two new values for each MAC computation"; [corpus] Weak - no corpus evidence on SIMD parallelism in TinyML inference engines
- Break condition: If SIMD lanes are fewer than filterlet size or instruction-level overlap is limited by pipeline constraints, latency gains diminish

### Mechanism 3
- Claim: DTMM's pruning strategy scheduler finds an optimal pruning plan that minimizes latency within accuracy and memory constraints
- Mechanism: The scheduler estimates importance of filterlets via loss gradient (Taylor expansion), then formulates a constrained optimization problem to minimize total layer latency while bounding accuracy loss (∆Lmax) and model size
- Core assumption: Loss gradient accurately predicts accuracy impact of pruning individual filterlets, and latency can be modeled as a function of filterlet counts
- Evidence anchors: [section III-C] "we employ a method inspired by previous works...Taylor expansion is used to measure the change of ∆L"; [section III-C] "we propose to perform a regression on the parameters...f(Ni, Hi, Wi, Ci, αi) = ˆTi"; [corpus] Weak - related papers do not discuss pruning strategy optimization with latency modeling
- Break condition: If loss gradient estimation is inaccurate or latency model underfits, scheduler may choose suboptimal pruning that violates constraints

## Foundational Learning

- Concept: Memory layout and storage order on MCUs
  - Why needed here: DTMM's efficiency relies on channel-major order contiguity for filterlet grouping and SIMD loading
  - Quick check question: In a 3-channel filter with kernel size 2x2, how many contiguous weights form one filterlet?

- Concept: SIMD (Single Instruction, Multiple Data) operations
  - Why needed here: DTMM uses SIMD MAC to compute multiple dot products in parallel, critical for inference speedup
  - Quick check question: If a SIMD lane number is 8 and weights are 8-bit, how many weights can be processed simultaneously?

- Concept: Constrained optimization and regression modeling
  - Why needed here: Pruning strategy scheduler uses regression to estimate layer latency and solves an optimization problem under accuracy/memory constraints
  - Quick check question: What is the objective function minimized by the pruning strategy scheduler?

## Architecture Onboarding

- Component map: DTMM Pruner -> FWCS Storage -> DTMM Convolution Operator -> Integration Layer
- Critical path:
  1. Scheduler computes pruning strategy (αi per layer)
  2. Pruner applies strategy, builds FWCS arrays (Arr, cPtr, fIdx, size)
  3. Model quantized and stored in FWCS format
  4. On-device runtime loads FWCS model and invokes DTMM operator for pruned layers
- Design tradeoffs:
  - Fine-grained pruning (filterlets) vs. indexing overhead: filterlets reduce overhead vs. unstructured pruning but add some vs. structured
  - SIMD acceleration vs. complexity: requires contiguous filterlet weights and instruction reordering logic
  - Accuracy preservation vs. compression: scheduler must balance ∆Lmax against model size
- Failure signatures:
  - Accuracy loss > 0.5%: pruning strategy too aggressive or loss gradient estimation poor
  - Runtime memory > 256 KB: indexing overhead higher than expected or model too large
  - Latency increase vs. baseline: SIMD lanes insufficient, instruction reordering ineffective, or filterlet contiguity broken
- First 3 experiments:
  1. Verify channel-major storage order and filterlet contiguity on target MCU
  2. Measure SIMD MAC throughput with filterlet-sized vectors vs. scalar
  3. Test scheduler's loss gradient estimation by pruning a single filterlet and measuring actual accuracy drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DTMM's performance scale with increasingly complex models (e.g., ResNet-50, Inception) on resource-constrained MCUs?
- Basis in paper: [explicit] The paper evaluates DTMM on VGG-11, ResNet-12, and YOLO models, demonstrating performance gains compared to state-of-the-art methods
- Why unresolved: The paper does not explore the performance of DTMM on more complex models, which could provide insights into its scalability and limitations on MCUs with limited resources
- What evidence would resolve it: Experimental results comparing DTMM's performance on complex models (e.g., ResNet-50, Inception) with other pruning methods, focusing on model size, latency, and accuracy trade-offs

### Open Question 2
- Question: How does DTMM's pruning strategy scheduler adapt to dynamic resource constraints on MCUs, such as varying battery levels or real-time processing requirements?
- Basis in paper: [explicit] The paper presents a pruning strategy scheduler that formulates the search for the optimal strategy as an optimization problem to minimize latency with accuracy and memory constraints
- Why unresolved: The paper does not address how the pruning strategy scheduler adapts to dynamic resource constraints on MCUs, which could impact the performance and usability of DTMM in real-world applications
- What evidence would resolve it: Experimental results demonstrating the adaptability of DTMM's pruning strategy scheduler to dynamic resource constraints on MCUs, including varying battery levels and real-time processing requirements

### Open Question 3
- Question: How does DTMM's performance compare to other model compression techniques, such as knowledge distillation and low-rank factorization, when deployed on MCUs?
- Basis in paper: [explicit] The paper focuses on DTMM's performance compared to structured and unstructured pruning methods (CHIP and PatDNN), but does not explore other model compression techniques
- Why unresolved: The paper does not provide a comprehensive comparison of DTMM with other model compression techniques, which could help identify its strengths and weaknesses in the context of MCUs
- What evidence would resolve it: Experimental results comparing DTMM's performance with other model compression techniques (e.g., knowledge distillation, low-rank factorization) on MCUs, focusing on model size, latency, and accuracy trade-offs

## Limitations

- The filterlet-based pruning mechanism relies on specific memory layout assumptions that may not generalize across all MCU architectures
- The claimed performance improvements (42.8% size reduction, 27.7% latency reduction) require verification through controlled experiments
- The pruning strategy scheduler's optimization formulation and FWCS storage layout depend on empirical results not fully detailed in the abstract

## Confidence

- **High confidence**: The general approach of combining fine-grained pruning with compact storage and SIMD acceleration
- **Medium confidence**: The specific implementation details, particularly around the pruning strategy scheduler's optimization formulation and the exact FWCS storage layout
- **Low confidence**: The claimed performance improvements (42.8% size reduction, 27.7% latency reduction) without access to actual experimental setup and baseline comparisons

## Next Checks

1. Verify the actual memory layout of weights on target MCU platforms to confirm filterlet contiguity assumptions hold across different hardware
2. Implement the pruning strategy scheduler and test its loss gradient estimation accuracy by comparing predicted vs. actual accuracy degradation for various pruning percentages
3. Benchmark the DTMM convolution operator against existing TinyML inference engines on the same hardware to validate claimed latency improvements under controlled conditions