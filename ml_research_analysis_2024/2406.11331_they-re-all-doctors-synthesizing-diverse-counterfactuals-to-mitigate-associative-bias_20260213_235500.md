---
ver: rpa2
title: 'They''re All Doctors: Synthesizing Diverse Counterfactuals to Mitigate Associative
  Bias'
arxiv_id: '2406.11331'
source_url: https://arxiv.org/abs/2406.11331
tags:
- clip
- images
- image
- arxiv
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of bias in vision-language models
  (VLMs) like CLIP, which can exhibit unwanted biases when deployed in applications
  such as text-to-image retrieval or classification tasks. The authors propose a novel
  framework to generate synthetic counterfactual images to create a diverse and balanced
  dataset for fine-tuning CLIP.
---

# They're All Doctors: Synthesizing Diverse Counterfactuals to Mitigate Associative Bias

## Quick Facts
- arXiv ID: 2406.11331
- Source URL: https://arxiv.org/abs/2406.11331
- Authors: Salma Abdel Magid; Jui-Hsien Wang; Kushal Kafle; Hanspeter Pfister
- Reference count: 40
- Key outcome: Fine-tuned CLIP model (CFα) improves fairness metrics by 40-66% for image retrieval while maintaining downstream accuracy

## Executive Summary
This paper addresses bias in vision-language models like CLIP by proposing a novel framework to generate synthetic counterfactual images for fine-tuning. The approach creates diverse counterfactuals by placing humans with different visual appearances in the same contextual settings, teaching the model to associate professions with context rather than sensitive attributes. The authors demonstrate significant improvements in fairness metrics while maintaining compatibility with original CLIP models through weight interpolation techniques.

## Method Summary
The method involves generating synthetic counterfactual images to create a balanced dataset for fine-tuning CLIP. First, neutral captions are generated for each visual concept (e.g., professions) using an LLM to avoid associative biases. Base images are then created using text-to-image models with decorators controlling visual attributes. Human body masks are computed using segmentation models, and counterfactual images are generated by inpainting masked regions with diverse human appearances. CLIP is fine-tuned on this counterfactual dataset using a combined loss function (CLIP loss + counterfactual loss), with weight interpolation techniques controlling the accuracy-fairness tradeoff.

## Key Results
- CFα improves MaxSkew, MinSkew, and NDKL fairness metrics by 40-66% for image retrieval tasks
- The model maintains similar performance in downstream tasks (FlickrR@5, ImageNet1KAcc)
- Weight ensembling allows users to control the accuracy-fairness tradeoff in a plug-n-play fashion
- The approach effectively disentangles profession-related concepts from sensitive attributes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CLIP's bias arises from spurious correlations between professions and visual attributes like skin color or gender, learned during pretraining on imbalanced datasets.
- **Mechanism:** By generating synthetic counterfactual images where humans with diverse visual appearances appear in the same context, the fine-tuned model learns to associate professions with contextual cues (background, attire, props) rather than sensitive attributes.
- **Core assumption:** Contextual cues are sufficient and more robust for identifying professions than visual attributes of humans.
- **Evidence anchors:** [abstract] "CLIP trained on such datasets learns to disentangle the human appearance from the context of an image..."; [section 4.1] "Our algorithm contains four key steps..."
- **Break condition:** If the generative models used to create counterfactuals themselves exhibit strong biases that cannot be mitigated through prompting or if contextual cues are not sufficient to disambiguate professions.

### Mechanism 2
- **Claim:** Fine-tuning CLIP on the synthetic counterfactual dataset with an additional contrastive loss term encourages the model to bring embeddings of counterfactuals originating from the same base image closer in the latent space.
- **Mechanism:** The counterfactual loss term explicitly contrasts counterfactual images of the same base image, supplementing the original CLIP loss. This encourages the model to learn representations where profession-related concepts are disentangled from sensitive attributes.
- **Core assumption:** The counterfactual loss term is effective in encouraging the model to focus on contextual cues rather than sensitive attributes.
- **Evidence anchors:** [abstract] "We use an additional constrastive loss term, similar to [51] and [37], between each counterfactual image pair..."; [section 4.2] "To achieve this, we use an additional constrastive loss term..."
- **Break condition:** If the counterfactual loss term is not properly weighted or if the batch size is too small to effectively contrast counterfactuals.

### Mechanism 3
- **Claim:** The weight ensembling technique (WiSE-FT) allows for controlling the accuracy-fairness tradeoff by linearly blending the weights of the original CLIP model and the fine-tuned model.
- **Mechanism:** By varying the interpolation parameter alpha, users can control the tradeoff between fairness metrics (MaxSkew, MinSkew, NDKL) and downstream accuracy (FlickrR@5, ImageNet1KAcc).
- **Core assumption:** The weight ensembling technique is effective in controlling the accuracy-fairness tradeoff without causing catastrophic forgetting.
- **Evidence anchors:** [abstract] "We show that, by design, our model retains maximal compatibility with the original CLIP models..."; [section 4.3] "To control the accuracy-fairness trade-off, one can simply ensemble the model weights..."
- **Break condition:** If the weight ensembling technique causes catastrophic forgetting or if the tradeoff is not smooth and controllable.

## Foundational Learning

- **Concept:** Contrastive Learning
  - **Why needed here:** The paper uses contrastive learning to train CLIP on the synthetic counterfactual dataset. Understanding contrastive learning is crucial for grasping how the model learns to associate professions with contextual cues rather than sensitive attributes.
  - **Quick check question:** What is the objective of contrastive learning, and how does it differ from supervised learning?

- **Concept:** Vision-Language Models (VLMs)
  - **Why needed here:** The paper focuses on debiasing CLIP, a popular VLM. Understanding VLMs is essential for comprehending the problem of bias in these models and the proposed solution.
  - **Quick check question:** What are the key components of a VLM, and how do they work together to align visual and textual representations?

- **Concept:** Fairness Metrics
  - **Why needed here:** The paper evaluates the debiased model using fairness metrics like MaxSkew, MinSkew, and NDKL. Understanding these metrics is necessary for assessing the effectiveness of the proposed approach.
  - **Quick check question:** What do MaxSkew, MinSkew, and NDKL measure, and how do they quantify bias in VLMs?

## Architecture Onboarding

- **Component map:** LLM (e.g., LLaMA 70B) -> Text-to-image model (e.g., SDXL Turbo) -> Segmentation model (e.g., SegFormer) -> Inpainting model (e.g., SDXL) -> CLIP model
- **Critical path:** LLM-generated captions → base image generation → human body segmentation → counterfactual image generation → CLIP fine-tuning with counterfactual loss
- **Design tradeoffs:** Using synthetic data ensures privacy and diversity but may introduce biases from the generative models. The choice of decorators affects the diversity of counterfactuals. The weighting of the counterfactual loss term impacts the effectiveness of bias mitigation.
- **Failure signatures:** If the generative models exhibit strong biases, the counterfactuals may not be diverse enough. If the segmentation is inaccurate, the counterfactuals may not effectively isolate contextual cues. If the counterfactual loss term is not properly weighted, the model may not learn to disentangle professions from sensitive attributes.
- **First 3 experiments:**
  1. Generate a small set of counterfactuals for a single profession and visually inspect their diversity and quality.
  2. Fine-tune CLIP on the synthetic dataset for a single epoch and evaluate the fairness metrics on a small validation set.
  3. Vary the weighting of the counterfactual loss term and observe its impact on the fairness metrics and downstream accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we generate more equitable and representative synthetic images that mitigate biases from LLMs and text-to-image models?
- **Basis in paper:** [explicit] The authors acknowledge that synthetic counterfactuals have their own biases stemming from the LLM and T2I models used in their pipeline, and suggest that future work should investigate interpolating across diverse characteristics like facial hair or skin tone to generate more equitable and representative images.
- **Why unresolved:** The paper focuses on demonstrating the effectiveness of their bias mitigation framework using existing T2I models, but does not delve into methods for generating more diverse and representative synthetic data. The issue of biases in generative models is a complex one that requires further investigation.
- **What evidence would resolve it:** Developing and evaluating techniques for generating synthetic images with a wider range of diverse characteristics, such as facial features, body types, and skin tones, while minimizing biases. This could involve exploring different interpolation methods, data augmentation techniques, or novel architectures for T2I models.

### Open Question 2
- **Question:** Can we develop evaluation metrics for fairness that go beyond fixed taxonomies of sensitive attributes and better capture the full spectrum of human diversity?
- **Basis in paper:** [explicit] The authors acknowledge the limitations of current evaluation practices that rely on existing gender and race crowdsourced annotations, and suggest that future work could generate more diverse counterfactuals and group them with unsupervised clustering to create fairness groups based on observable characteristics rather than socially constructed categories.
- **Why unresolved:** The paper uses existing fairness metrics (MaxSkew, MinSkew, NDKL) that rely on fixed taxonomies of sensitive attributes. However, these metrics may not fully capture the nuances of human diversity and could perpetuate biases in their own way.
- **What evidence would resolve it:** Developing and validating new evaluation metrics for fairness that are more inclusive and representative of human diversity. This could involve exploring unsupervised clustering techniques, using more granular or fluid categories for sensitive attributes, or incorporating additional dimensions of diversity beyond race and gender.

### Open Question 3
- **Question:** How can we ensure that achieving numerical balance in datasets does not inadvertently reinforce stereotypes or establish spurious correlations?
- **Basis in paper:** [explicit] The authors acknowledge that achieving numerical balance in datasets does not eliminate the risk of reinforcing stereotypes or establishing spurious correlations, and suggest that their method can begin to address these issues during the development of evaluation datasets.
- **Why unresolved:** The paper demonstrates the effectiveness of their bias mitigation framework in improving fairness metrics, but does not fully address the potential for unintended consequences of dataset balancing. This is a complex issue that requires careful consideration of the interplay between data representation and model learning.
- **What evidence would resolve it:** Conducting comprehensive studies to analyze the potential for unintended biases and spurious correlations in balanced datasets. This could involve examining the learned representations of the model, analyzing the output for stereotypical associations, and comparing the performance of the model on diverse subpopulations. Additionally, developing techniques for detecting and mitigating these unintended biases could be explored.

## Limitations
- The approach relies heavily on the quality and diversity of synthetic counterfactual images, which may not fully capture real-world complexity
- Effectiveness is contingent on generative models producing sufficiently diverse and unbiased counterfactuals
- Framework's performance may vary across different domains and professions beyond evaluated ones
- Potential for unintended biases or spurious correlations despite numerical balance

## Confidence

**High Confidence:** The core mechanism of using synthetic counterfactual images to disentangle sensitive attributes from contextual cues is well-supported by the experimental results. The improvement in fairness metrics (MaxSkew, MinSkew, NDKL) by 40-66% is a strong indicator of the approach's effectiveness.

**Medium Confidence:** The weight ensembling technique (WiSE-FT) for controlling the accuracy-fairness tradeoff is supported by the results, but the smoothness and controllability of the tradeoff may depend on the specific implementation and hyperparameters.

**Low Confidence:** The generalizability of the approach to other domains and professions beyond the evaluated ones is uncertain, as the framework is tailored to the specific dataset and use case.

## Next Checks

1. **Diversity Analysis of Counterfactuals:** Conduct a thorough analysis of the diversity and quality of the generated counterfactual images across different professions and sensitive attributes to ensure they adequately represent the desired diversity.

2. **Generalizability Across Domains:** Evaluate the framework's performance on a broader range of professions and domains to assess its generalizability and identify any domain-specific limitations.

3. **Robustness to Generative Model Biases:** Investigate the impact of potential biases in the generative models (e.g., SDXL Turbo) on the quality and diversity of counterfactuals, and explore techniques to mitigate these biases.