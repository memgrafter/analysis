---
ver: rpa2
title: Weak-to-Strong Reasoning
arxiv_id: '2407.13647'
source_url: https://arxiv.org/abs/2407.13647
tags:
- weak
- reasoning
- strong
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a weak-to-strong learning framework that
  enables large language models (LLMs) to improve their reasoning capabilities by
  learning from weaker models, without relying on ground truth data or stronger model
  supervision. The method employs a two-stage process: first, it uses a selective
  fine-tuning approach that combines weak model outputs with in-context learning data,
  filtering for answer consistency to generate high-quality training data.'
---

# Weak-to-Strong Reasoning

## Quick Facts
- arXiv ID: 2407.13647
- Source URL: https://arxiv.org/abs/2407.13647
- Authors: Yuqing Yang; Yan Ma; Pengfei Liu
- Reference count: 40
- Primary result: Weak-to-strong learning framework improves reasoning capabilities without ground truth supervision, achieving up to 26.99 point gains on GSM8K

## Executive Summary
This paper introduces a weak-to-strong learning framework that enables large language models to improve their reasoning capabilities by learning from weaker models, without relying on ground truth data or stronger model supervision. The method employs a two-stage process: first, it uses a selective fine-tuning approach that combines weak model outputs with in-context learning data, filtering for answer consistency to generate high-quality training data. Second, it uses preference optimization on contrastive samples identified by the strong model itself to refine its reasoning and avoid weak model errors. Experiments on GSM8K, MATH, and OlympicArena datasets demonstrate significant performance gains over naive fine-tuning, with the strongest model achieving improvements of up to 26.99 points on GSM8K and surpassing strong ceiling performance on difficult MATH problems.

## Method Summary
The framework operates in two stages. Stage I performs supervised fine-tuning using selective data from weak model outputs and in-context learning (ICL) from the strong model, filtered by answer consistency. The weak model (m) is fine-tuned on gold solutions, then generates solutions for new questions, while the strong model (M) uses ICL to generate solutions. Only questions where both models produce the same final answer are kept for training. Stage II applies preference optimization using contrastive samples where the strong model's confidence indicates correct answers, allowing the strong model to learn from weak model errors. The method uses LoRA for fine-tuning and DPO/ORPO for preference optimization, with hyperparameters tuned per dataset.

## Key Results
- GSM8K: Weak-ICL fine-tuning achieves 26.99 point improvement over naive fine-tuning, reaching 73.02% accuracy
- MATH: Model surpasses strong ceiling performance, achieving 40.63% accuracy on the hardest problems (level 5)
- OlympicArena: Significant improvements across multiple reasoning tasks with limited data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective data curation using answer consistency between weak model outputs and in-context learning samples from the strong model improves strong model performance by filtering out weak model errors.
- Mechanism: When weak model (m) and strong model (M) generate the same answer to the same question, the answer is more likely to be correct. This consistency filtering creates higher-quality training data (ˆDweak and ˆDicl) that avoids propagating weak model mistakes.
- Core assumption: Agreement between two models using different data representations (weak data vs. in-context learning) indicates higher likelihood of correctness in open-ended reasoning tasks.
- Evidence anchors:
  - [abstract] "This framework begins with supervised fine-tuning on a selective small but high-quality dataset, followed by preference optimization on contrastive samples identified by the strong model itself."
  - [section 3.1.3] "when two models, employing distinct data representations, converge on the same answer in an open-ended task, it is indicative of a higher likelihood of accuracy"
- Break condition: If the weak model's errors are systematic and the strong model's in-context learning is poor, answer consistency filtering may select incorrect answers consistently.

### Mechanism 2
- Claim: Progressive refinement through iterative training stages allows the strong model to bootstrap its own reasoning capabilities without ground truth supervision.
- Mechanism: Stage I uses answer consistency to create high-quality training data, producing Mplus. Stage II uses Mplus's confidence in its own answers to construct preference optimization samples, enabling learning from weak model errors.
- Core assumption: A model with enhanced reasoning capabilities can reliably identify its own correct answers through sampling and confidence estimation, even without ground truth.
- Evidence anchors:
  - [abstract] "This framework begins with supervised fine-tuning on a selective small but high-quality dataset, followed by preference optimization on contrastive samples identified by the strong model itself."
  - [section 3.2] "the current strong model with enhanced reasoning capabilities identifies the most likely correct answers based on its confidence"
- Break condition: If the strong model's confidence estimation is unreliable or systematically biased, preference optimization may reinforce incorrect patterns.

### Mechanism 3
- Claim: Learning from weak model errors through preference optimization enables the strong model to surpass strong ceiling performance on challenging problems.
- Mechanism: By constructing contrastive samples where the weak model gives incorrect answers and the strong model gives correct ones, preference optimization teaches the strong model to avoid weak model mistakes.
- Core assumption: Preference optimization can effectively learn from constructed contrastive samples even without ground truth, as long as the strong model can identify correct answers with sufficient confidence.
- Evidence anchors:
  - [abstract] "Our proposed preference optimization phase enables the strong model to learn from errors made by the weak supervisor, ultimately surpassing the strong model fine-tuned on gold-standard solutions"
  - [section 4.4] "our method for constructing positive and negative samples effectively enhances the strong model's math reasoning capabilities"
- Break condition: If the weak model makes systematic errors that the strong model cannot distinguish from correct answers, preference optimization may reinforce these errors.

## Foundational Learning

- Concept: Weak-to-Strong Generalization
  - Why needed here: The paper addresses scenarios where models exceed human capabilities, making traditional supervision impossible. Understanding weak-to-strong generalization is essential for grasping why weaker models can effectively supervise stronger ones.
  - Quick check question: What is the key difference between weak-to-strong generalization and traditional distillation methods?

- Concept: Preference Optimization (DPO/ORPO)
  - Why needed here: Stage II relies on preference optimization to learn from contrastive samples constructed by the strong model itself. Understanding how preference optimization works without ground truth is crucial.
  - Quick check question: How does preference optimization differ from supervised fine-tuning when ground truth labels are unavailable?

- Concept: In-Context Learning (ICL)
  - Why needed here: The method uses ICL from the strong model as one source of high-quality data, and understanding how ICL can activate latent capabilities is important for the framework.
  - Quick check question: What role does in-context learning play in the weak-ICL fine-tuning approach, and why is it combined with weak model outputs?

## Architecture Onboarding

- Component map: Weak Model (m) -> Weak Data Generation -> Consistency Filter -> Stage I SFT -> Stage II Preference Optimization -> Final Strong Model
- Critical path: Weak model fine-tuning → weak data generation → ICL data generation → consistency filtering → Stage I SFT → confidence-based sampling → Stage II preference optimization → final strong model
- Design tradeoffs:
  - Data quality vs. quantity: Selective filtering reduces training data but improves quality
  - Model size assumption: Method assumes strong model has latent capabilities to be unlocked
  - Confidence threshold tuning: Requires balancing false positives vs. false negatives in answer selection
- Failure signatures:
  - Stage I underperformance: Indicates poor answer consistency between weak and strong models
  - Stage II degradation: Suggests confidence estimation is unreliable or preference optimization is overfitting
  - No improvement over weak floor: Implies weak model is too weak to provide useful supervision
- First 3 experiments:
  1. Implement basic weak-ICL fine-tuning on GSM8K with Llama2-7b as weak model and Llama2-70b as strong model
  2. Test answer consistency filtering by comparing weak model outputs vs. strong model ICL outputs on validation set
  3. Implement Stage II preference optimization using constructed contrastive samples from Mplus confidence estimation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the weak-to-strong learning framework perform on reasoning tasks beyond mathematical problems, such as logical reasoning or commonsense reasoning?
- Basis in paper: [inferred] The paper focuses on mathematical reasoning tasks (GSM8K and MATH) and suggests the need for further exploration of other reasoning domains.
- Why unresolved: The paper only provides experimental results on mathematical reasoning datasets and does not explore the framework's applicability to other types of reasoning tasks.
- What evidence would resolve it: Experiments applying the weak-to-strong learning framework to logical reasoning, commonsense reasoning, or other reasoning tasks, and comparing the results to those on mathematical reasoning.

### Open Question 2
- Question: What is the impact of using different weak models on the performance of the strong model in the weak-to-strong learning framework?
- Basis in paper: [explicit] The paper uses three different weak models (Llama2-7b, Gemma-2b, and Mistral-7b) and shows that the performance of the strong model improves as the weak model's performance improves.
- Why unresolved: While the paper demonstrates a correlation between weak model performance and strong model performance, it does not explore the reasons behind this correlation or the optimal characteristics of a weak model for weak-to-strong learning.
- What evidence would resolve it: A systematic study comparing the performance of the strong model when trained with weak models of varying sizes, architectures, and capabilities, and analyzing the factors that contribute to the weak model's effectiveness in guiding the strong model.

### Open Question 3
- Question: How does the weak-to-strong learning framework scale to larger and more complex reasoning tasks, such as those found in scientific or legal domains?
- Basis in paper: [inferred] The paper mentions the potential for applying the framework to more complex reasoning tasks but does not provide experimental results on such tasks.
- Why unresolved: The paper's experiments are limited to relatively simple mathematical reasoning tasks, and it is unclear how the framework would perform on more complex reasoning tasks that require domain-specific knowledge and reasoning skills.
- What evidence would resolve it: Experiments applying the weak-to-strong learning framework to reasoning tasks in scientific, legal, or other complex domains, and evaluating the framework's ability to handle the increased complexity and domain-specific requirements.

## Limitations

- The framework's effectiveness depends heavily on the relative capabilities of weak and strong models, potentially limiting scalability to extreme capability gaps
- The method requires the strong model to have latent reasoning capabilities that can be activated through in-context learning, which may not hold for all model architectures
- Confidence-based sampling in Stage II assumes reliable confidence estimation, but model confidence can be miscalibrated, especially on challenging problems

## Confidence

**High Confidence**: The selective data curation approach using answer consistency is well-justified and the experimental results on GSM8K and MATH demonstrate clear improvements over baseline methods. The mathematical formulation of both stages is sound.

**Medium Confidence**: The claim that the method can "surpass strong ceiling performance" on challenging MATH problems needs careful interpretation - while the numbers show improvement, the absolute performance on the most difficult problems remains low, and the comparison to gold-standard fine-tuning may not be apples-to-apples given different data access.

**Low Confidence**: The assertion that this framework is directly applicable to "superintelligent AI development" overstates the current results. The experiments focus on reasoning tasks with relatively modest capability gaps between weak and strong models.

## Next Checks

1. **Ablation on consistency threshold**: Systematically vary the answer consistency filtering threshold to quantify its impact on final performance and determine the optimal balance between data quality and quantity.

2. **Confidence calibration analysis**: Evaluate the strong model's confidence estimation accuracy by comparing confidence scores against actual correctness rates across different difficulty levels to identify potential miscalibration issues.

3. **Generalization across capability gaps**: Test the framework with wider capability gaps (e.g., Llama2-7b supervising Llama3-70b) to assess scalability and identify at what point the method breaks down.