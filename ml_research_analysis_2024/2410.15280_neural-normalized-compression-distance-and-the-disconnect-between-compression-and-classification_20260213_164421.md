---
ver: rpa2
title: Neural Normalized Compression Distance and the Disconnect Between Compression
  and Classification
arxiv_id: '2410.15280'
source_url: https://arxiv.org/abs/2410.15280
tags:
- compression
- neural
- classification
- compressors
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between compression performance
  and classification accuracy using a novel approach called Neural Normalized Compression
  Distance (Neural NCD). The authors propose using large language models (LLMs) as
  neural compressors in combination with arithmetic coding to approximate Kolmogorov
  complexity, which is then used as a distance metric for k-nearest neighbors (kNN)
  classification.
---

# Neural Normalized Compression Distance and the Disconnect Between Compression and Classification

## Quick Facts
- arXiv ID: 2410.15280
- Source URL: https://arxiv.org/abs/2410.15280
- Authors: John Hurwitz; Charles Nicholas; Edward Raff
- Reference count: 31
- Primary result: Compression rate alone does not reliably predict classification accuracy when using neural compressors

## Executive Summary
This paper investigates the relationship between compression performance and classification accuracy using a novel approach called Neural Normalized Compression Distance (Neural NCD). The authors propose using large language models (LLMs) as neural compressors in combination with arithmetic coding to approximate Kolmogorov complexity, which is then used as a distance metric for k-nearest neighbors (kNN) classification. They compare this approach to traditional compressors like gzip, zstd, and lzma across three text classification datasets (AGNews, 20News, and DBpedia) in a few-shot learning setting.

The key finding is that compression rate alone does not reliably predict classification accuracy when using neural compressors. Despite achieving significantly better compression rates, neural NCD sometimes underperforms, matches, or outperforms traditional compressors depending on the dataset. These results challenge the conventional wisdom that better compression necessarily leads to better classification performance.

## Method Summary
The method uses LLMs as probabilistic predictors combined with arithmetic coding to approximate Kolmogorov complexity through compression. For each sequence pair, the concatenated sequence probability distribution is computed using the LLM, then arithmetic coding is applied to get compressed length. NCD is calculated using these compressed lengths, and kNN classification uses NCD as the distance metric. The approach is compared against traditional compressors (gzip, zstd, lzma) and against using model latent representations with Euclidean distance. Experiments are conducted on three text classification datasets in few-shot settings with n={5,10,50,100} samples per class.

## Key Results
- Neural compressors achieve significantly better compression rates than traditional compressors across all datasets
- Neural NCD performance varies unpredictably across datasets - outperforming, matching, or underperforming traditional compressors despite consistent compression advantages
- Different neural architectures (RWKV, GPT-2, OPT) show similar performance patterns across datasets despite varying compression rates
- The effectiveness of Neural NCD versus latent representation approaches is model-dependent, with some models benefiting from NCD while others benefit from direct latent representation comparison

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compression rate alone does not predict classification accuracy when using neural compressors
- Mechanism: Neural compressors like RWKV, GPT-2, and OPT achieve significantly better compression rates than traditional compressors (gzip, zstd, lzma), but their classification accuracy varies unpredictably across datasets. The hypothesis that better compression leads to better NCD-based classification is contradicted by empirical results showing that neural NCD outperforms traditional compressors on AGNews, matches them on 20News, and underperforms on DBpedia despite consistent compression advantages.
- Core assumption: Compression rate reflects the quality of NCD distance metric for classification
- Evidence anchors:
  - [abstract] "compression rate alone does not reliably predict classification accuracy when using neural compressors"
  - [section 4.1] "Despite the RWKV neural compressor achieving superior compression rates than traditional compressors on each dataset, Neural NCD yields varying relative performance"
  - [corpus] No direct evidence in corpus about compression-accuracy relationship; corpus papers focus on unrelated applications
- Break condition: If compression algorithms with similar rates show consistent classification performance across datasets

### Mechanism 2
- Claim: Different neural network architectures yield similar NCD performance despite varying compression rates
- Mechanism: When comparing RWKV, GPT-2, and OPT models of similar size (125-169M parameters), all three achieve comparable compression rates but show similar performance patterns across datasets - outperforming gzip on AGNews and under-performing on DBpedia. This suggests architectural differences affect compression quality in ways not captured by simple rate metrics.
- Core assumption: Compression rate captures all relevant information about NCD quality
- Evidence anchors:
  - [section 4.2] "all neural models outperform gzip in the same way on AGNews, and underperform it in the same way on DBpedia"
  - [section 4.2] "Table 2 shows compression rates across models are similar"
  - [corpus] No corpus evidence about architectural effects on compression-based classification
- Break condition: If different architectures show divergent performance patterns despite similar compression rates

### Mechanism 3
- Claim: The effectiveness of Neural NCD versus latent representation approaches is model-dependent
- Mechanism: For some models (GPT-2), Neural NCD outperforms using the model's latent representations with Euclidean distance, while for others (OPT), latent representations significantly outperform Neural NCD. This indicates that the quality and usefulness of distance metrics in latent space varies across model architectures.
- Core assumption: Neural compressors and their latent representations should provide similar classification performance
- Evidence anchors:
  - [section 4.3] "depending on the choice of neural network model used for neural compression, Neural NCD can either outperform or underperform the Euclidean distance-based approach"
  - [section 4.3] "For example, a Neural NCD approach with GPT-2 small outperforms using the model's latent representations, however with OPT 125M, latent representations drastically outperform a Neural NCD approach"
  - [corpus] No corpus evidence about comparing NCD vs latent representation approaches
- Break condition: If all models show consistent performance whether using Neural NCD or latent representations

## Foundational Learning

- Concept: Kolmogorov complexity and its practical approximation via compression algorithms
  - Why needed here: The paper's theoretical foundation relies on Kolmogorov complexity as the ideal measure of information content, with NCD providing a practical approximation
  - Quick check question: Why is Kolmogorov complexity uncomputable, and how does NCD provide a practical workaround?

- Concept: Normalized Compression Distance (NCD) calculation and properties
  - Why needed here: NCD is the core distance metric used for kNN classification in this work, requiring understanding of how it's computed and its theoretical properties
  - Quick check question: What is the formula for NCD, and why does it require concatenating sequences for calculation?

- Concept: Arithmetic coding and its application to neural compression
  - Why needed here: The neural compression approach uses arithmetic coding with language models to perform lossless compression, which is essential for understanding the experimental setup
  - Quick check question: How does arithmetic coding work with probabilistic models like language models to achieve lossless compression?

## Architecture Onboarding

- Component map:
  - Large Language Models (LLMs) as probabilistic predictors
  - Arithmetic coding engine for lossless compression
  - NCD calculator for pairwise distance computation
  - kNN classifier using NCD as distance metric
  - Data pipeline for few-shot text classification

- Critical path:
  1. Load pretrained LLM
  2. For each sequence pair, compute concatenated sequence probability distribution
  3. Apply arithmetic coding to get compressed length
  4. Calculate NCD using compressed lengths
  5. Build distance matrix for kNN classification
  6. Evaluate classification accuracy

- Design tradeoffs:
  - Model size vs computational cost: Larger models achieve better compression but increase O(n²) complexity
  - Few-shot setting vs full dataset: Subsampling enables experimentation but may miss dataset-wide patterns
  - Traditional vs neural compressors: Traditional compressors are faster but less effective; neural compressors are slower but achieve better compression

- Failure signatures:
  - Inconsistent performance across datasets despite compression rate improvements
  - Similar compression rates yielding different classification accuracies
  - Neural NCD performing worse than simple Euclidean distance on latent representations

- First 3 experiments:
  1. Replicate main result: Compare RWKV 169M NCD performance against gzip on AGNews with 5-shot setting
  2. Vary model architecture: Test GPT-2 small and OPT 125M on same dataset/parameter setting
  3. Compare distance metrics: Evaluate Neural NCD vs Euclidean distance on latent representations for a single model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural features of neural network models contribute to better or worse performance in NCD-based classification, beyond overall compression rates?
- Basis in paper: [explicit] The paper notes that "The internal workings of each compression algorithm are important in determining NCD quality" and mentions that "Differences in neural network architectures among neural compressors may yield different abilities to identify these long range similarities"
- Why unresolved: The paper observes varying NCD performance across different neural architectures but does not isolate which specific architectural features (attention mechanisms, recurrence patterns, layer depth, etc.) are responsible for these differences.
- What evidence would resolve it: Systematic ablation studies comparing different architectural components (e.g., comparing RWKV vs transformer with same parameters, or varying attention mechanisms) while controlling for compression rate would help identify which features matter most.

### Open Question 2
- Question: How does the performance of NCD-based classification scale with sequence length and dataset size, and at what point does the O(n²) computational complexity become prohibitive?
- Basis in paper: [inferred] The paper mentions "The O(n²) computational complexity of pairwise NCD calculation for neural compression is a limiting factor in the computational cost of this method, scaling with LLM model size and size of training and test sets" and subsamples only 100 test samples due to computational constraints.
- Why unresolved: The experiments were limited by computational constraints, preventing exploration of how performance scales with larger datasets and longer sequences, which would be necessary for practical applications.
- What evidence would resolve it: Experiments with varying dataset sizes (beyond the few-shot setting), different sequence lengths, and analysis of the point at which computational costs outweigh classification benefits would provide clarity.

### Open Question 3
- Question: Why do some neural models show better NCD performance than their latent representations while others show the opposite, and what does this reveal about the relationship between compression and meaningful feature extraction?
- Basis in paper: [explicit] The paper directly observes that "depending on the choice of neural network model used for neural compression, Neural NCD can either outperform or underperform the Euclidean distance-based approach on sequence latent representations"
- Why unresolved: The paper identifies this phenomenon but does not explain the underlying reasons why certain architectures benefit from the NCD approach while others benefit from direct latent representation comparison.
- What evidence would resolve it: Analysis of what information is preserved or lost in the compression process versus latent representations, perhaps through interpretability techniques or by measuring how well each method captures class-relevant features, would help explain these differences.

## Limitations

- Experimental scope limited to three text classification datasets with relatively small class counts (2-14 classes)
- All neural compressors tested are relatively small models (125-169M parameters), leaving uncertainty about whether results hold for larger, more capable models
- Comparison focuses on few-shot settings, which may not reflect performance in data-rich scenarios where neural models typically excel

## Confidence

*High Confidence Claims:*
- Neural compressors achieve significantly better compression rates than traditional compressors across all datasets
- The relationship between compression rate and classification accuracy is inconsistent and dataset-dependent when using neural compressors

*Medium Confidence Claims:*
- Different neural architectures yield similar performance patterns despite compression rate variations
- The effectiveness of Neural NCD versus latent representation approaches varies by model architecture

*Low Confidence Claims:*
- Whether these findings generalize to larger models, more complex datasets, or full-data scenarios

## Next Checks

1. Test larger LLM architectures (1B+ parameters) to determine if compression-accuracy disconnect persists with more capable models
2. Evaluate performance on multi-label classification tasks and datasets with 50+ classes to test generalizability
3. Compare Neural NCD performance in few-shot vs full-data regimes to identify the boundary where neural compression advantages emerge