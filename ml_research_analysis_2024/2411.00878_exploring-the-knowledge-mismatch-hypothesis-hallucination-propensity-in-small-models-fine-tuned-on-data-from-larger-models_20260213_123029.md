---
ver: rpa2
title: 'Exploring the Knowledge Mismatch Hypothesis: Hallucination Propensity in Small
  Models Fine-tuned on Data from Larger Models'
arxiv_id: '2411.00878'
source_url: https://arxiv.org/abs/2411.00878
tags:
- data
- hallucination
- generated
- small
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tests the hypothesis that fine-tuning a small language
  model on data generated by a larger model leads to increased hallucination. The
  authors fine-tune LLaMA 7B (small) and 13B (large) models to abstain when unsure,
  using data from the TriviaQA dataset.
---

# Exploring the Knowledge Mismatch Hypothesis: Hallucination Propensity in Small Models Fine-tuned on Data from Larger Models

## Quick Facts
- arXiv ID: 2411.00878
- Source URL: https://arxiv.org/abs/2411.00878
- Reference count: 22
- Small models fine-tuned on data from larger models show significantly increased hallucination rates

## Executive Summary
This paper investigates the hypothesis that fine-tuning a small language model on data generated by a larger model leads to increased hallucination. The authors fine-tune LLaMA 7B and 13B models to abstain when unsure, using data from the TriviaQA dataset. They compare performance when the small model is fine-tuned on its own data versus data from the large model. Results show the small model fine-tuned on large model data produces significantly more wrong answers (average 125% increase, median 107% increase) compared to the model fine-tuned on its own data, confirming the hypothesis. This suggests knowledge mismatch between the fine-tuning data and the model's internal knowledge leads to increased hallucination.

## Method Summary
The study uses LLaMA 7B (small) and 13B (large) models with parameter-efficient fine-tuning (PEFT) on the TriviaQA dataset. The large model generates fine-tuning data, which is then used to fine-tune the small model. This process is repeated with the small model generating its own fine-tuning data. Both fine-tuned models are evaluated on an unseen test set, measuring wrong answer rates to quantify hallucination. The key comparison is between wrong answer rates when the small model is fine-tuned on its own data versus data from the large model.

## Key Results
- Small model fine-tuned on large model data produces 125% more wrong answers on average
- Median increase in wrong answers is 107%
- The large model's generated data contains fewer "I don't know" responses compared to the small model's data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a small model on data from a larger model introduces knowledge mismatch that increases hallucination
- Mechanism: The fine-tuning data contains knowledge beyond the small model's learned knowledge, causing it to guess rather than abstain when unsure
- Core assumption: Large model's generated data contains knowledge not present in small model's parameters
- Evidence anchors:
  - [abstract] "This suggests that fine-tuning on data generated by larger models can lead to knowledge mismatch and increased hallucination in smaller models."
  - [section] "This mismatch trains the model to guess answers for questions similar to the one that was newly introduced"
- Break condition: If small model's knowledge base overlaps significantly with large model's generated data

### Mechanism 2
- Claim: Fine-tuning on data from larger model reduces tendency to say "I don't know"
- Mechanism: Large model generates fewer "I don't know" responses, teaching small model to provide answers more frequently
- Core assumption: Large model's generated data has lower proportion of "I don't know" responses
- Evidence anchors:
  - [section] "There are fewer 'I don't know' responses in the responses of the model fine-tuned on data generated by a larger model"
  - [abstract] "Results show that the small model fine-tuned on data from the large model produces significantly more wrong answers"
- Break condition: If large model's data contains similar or higher proportion of "I don't know" responses

### Mechanism 3
- Claim: Fine-tuning on mismatched data teaches model to prioritize parametric knowledge over input evidence
- Mechanism: Model learns to rely on internal knowledge rather than input context when faced with unfamiliar questions
- Core assumption: Fine-tuning reinforces model's tendency to use parametric knowledge
- Evidence anchors:
  - [section] "This leads to a mismatch" when fine-tuning teaches answers already known by model
  - [abstract] "Mismatch between the knowledge that is fed to the model to fine-tune it and the knowledge that is already present"
- Break condition: If fine-tuning process explicitly teaches verification against input evidence

## Foundational Learning

- **Knowledge distillation**: Understanding how knowledge transfer between models affects recipient behavior is crucial for interpreting results. Quick check: How does knowledge distillation typically affect smaller models trained on larger model data?
- **Parameter-efficient fine-tuning methods**: The study uses PEFT rather than full fine-tuning, affecting model adaptation. Quick check: What are key differences between PEFT and full fine-tuning in terms of behavioral modifications?
- **Hallucination detection and measurement**: Study measures hallucination by counting wrong answers. Quick check: What are common methods for measuring hallucination in language models and how do they differ in sensitivity?

## Architecture Onboarding

- **Component map**: LLaMA 7B (small model) -> LLaMA 13B (large model) -> TriviaQA dataset -> PEFT implementation -> Evaluation pipeline
- **Critical path**: 1) Generate fine-tuning data using large model, 2) Generate alternative fine-tuning data using small model, 3) Fine-tune small model on both datasets separately, 4) Evaluate both fine-tuned models on unseen test data, 5) Compare wrong answer rates
- **Design tradeoffs**: Using PEFT vs full fine-tuning (computational efficiency vs potential performance differences), using TriviaQA vs other datasets (size and quality), using LLaMA 7B/13B vs other sizes (balance between cost and effect visibility)
- **Failure signatures**: Similar wrong answer rates between both fine-tuned models (hypothesis may not hold), fewer wrong answers in large-model-fine-tuned version (confounding factors), no significant difference in "I don't know" responses (abstention mechanism not primary driver)
- **First 3 experiments**: 1) Reproduce core experiment with LLaMA 7B fine-tuned on LLaMA 7B vs LLaMA 13B data, 2) Vary fine-tuning epochs (1-5) to observe knowledge mismatch effect changes, 3) Test with different model pairs (e.g., LLaMA 7B vs LLaMA 33B) to see if effect scales with size differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does knowledge mismatch hypothesis apply to larger models fine-tuned on data from even larger models?
- Basis in paper: [inferred] Paper only tests hypothesis on small models fine-tuned on larger models
- Why unresolved: Only tested on LLaMA 7B and 13B models
- What evidence would resolve it: Experiments with different model size combinations (e.g., 13B vs 30B)

### Open Question 2
- Question: Does type of fine-tuning data affect degree of knowledge mismatch and hallucination?
- Basis in paper: [inferred] Uses general QA dataset but different data types might lead to different mismatch levels
- Why unresolved: Paper doesn't explore how domain or type of fine-tuning data influences hypothesis
- What evidence would resolve it: Experiments with different data types (medical, legal, technical) and comparison of hallucination levels

### Open Question 3
- Question: Can techniques like RLHF or RLAIF mitigate knowledge mismatch and reduce hallucination?
- Basis in paper: [explicit] Paper mentions RLHF and RLAIF as hallucination reduction techniques but doesn't explore their effectiveness
- Why unresolved: Paper focuses on knowledge mismatch hypothesis without investigating RLHF/RLAIF effectiveness
- What evidence would resolve it: Experiments applying RLHF or RLAIF to models fine-tuned on larger model data and measuring impact on hallucination

## Limitations
- Limited to LLaMA 7B and 13B models, unclear if results generalize to other model sizes or architectures
- Single dataset evaluation using TriviaQA, results might differ with other question-answering datasets
- Uses PEFT rather than full fine-tuning, which may limit how model adapts to fine-tuning data

## Confidence
- **Primary hypothesis (7B fine-tuned on 13B data hallucinates more)**: High confidence
- **Knowledge mismatch mechanism**: Medium confidence
- **Abstention rate reduction**: Medium confidence

## Next Checks
1. Repeat experiment using a different QA dataset (e.g., Natural Questions or SQuAD) to verify knowledge mismatch effect persists across different knowledge distributions
2. Conduct same experiment using full fine-tuning instead of PEFT to determine if knowledge mismatch effects are specific to parameter-efficient methods
3. Test hypothesis with more extreme model size difference (e.g., LLaMA 7B vs LLaMA 33B or larger) to determine if hallucination increase scales with knowledge gap