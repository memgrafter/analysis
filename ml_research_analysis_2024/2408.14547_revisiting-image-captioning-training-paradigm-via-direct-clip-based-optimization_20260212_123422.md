---
ver: rpa2
title: Revisiting Image Captioning Training Paradigm via Direct CLIP-based Optimization
arxiv_id: '2408.14547'
source_url: https://arxiv.org/abs/2408.14547
tags:
- dico
- scst
- image
- captioning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DiCO, a new training paradigm for image captioning
  that directly optimizes CLIP-based metrics like CLIP-S and PAC-S without reinforcement
  learning. The key idea is to distill a reward model from a learnable evaluator by
  solving a weighted classification problem inside the captioner, preventing model
  collapse and maintaining fluency.
---

# Revisiting Image Captioning Training Paradigm via Direct CLIP-based Optimization

## Quick Facts
- arXiv ID: 2408.14547
- Source URL: https://arxiv.org/abs/2408.14547
- Reference count: 40
- Key outcome: DiCO achieves 79.8 CIDEr, 0.869 PAC-S, and 0.837 CLIP-S on COCO, outperforming SCST and other CLIP-based optimization approaches.

## Executive Summary
This paper introduces DiCO, a novel training paradigm for image captioning that directly optimizes CLIP-based metrics (CLIP-S and PAC-S) without reinforcement learning. The key innovation is distilling a reward model from a learnable evaluator by solving a weighted classification problem inside the captioner, which prevents model collapse and maintains caption fluency. DiCO demonstrates improved stability and higher quality captions compared to existing methods, achieving state-of-the-art results on modern metrics while maintaining competitive performance on traditional ones.

## Method Summary
DiCO fine-tunes a Transformer-based captioner by distilling a reward model from CLIP-based evaluators (CLIP-S or PAC-S). The method generates multiple captions per image, computes evaluator scores, and uses weighted classification to mimic pairwise quality relations based on a Bradley-Terry model. A KL divergence term with the pre-trained model prevents overoptimization and maintains fluency. The approach optimizes the weighted classification loss directly without reinforcement learning, showing improved stability compared to SCST and better alignment with human judgment metrics.

## Key Results
- Achieves 79.8 CIDEr, 0.869 PAC-S, and 0.837 CLIP-S on COCO test set
- Outperforms SCST and other CLIP-based optimization methods
- Shows improved stability and prevents model collapse during fine-tuning
- Maintains competitive performance on traditional metrics while excelling on modern CLIP-based metrics

## Why This Works (Mechanism)

### Mechanism 1
DiCO prevents model collapse by distilling a reward model from a CLIP-based evaluator using weighted classification. Instead of RL, it compares a winner caption against losers using quality distances learned from the evaluator. This prevents overfitting to single high-reward outputs. Core assumption: the evaluator reliably ranks caption quality and aligns with human preferences. Evidence: abstract and section 3 claims, but corpus lacks direct supporting papers. Break condition: if evaluator doesn't align with human preferences, distilled reward model becomes inaccurate.

### Mechanism 2
The KL divergence term prevents the fine-tuned model from deviating too far from the pre-trained model, mitigating reward hacking. This ensures the model doesn't stray from the distribution where the reward model is accurate. Core assumption: pre-trained model has reasonable fluent captions. Evidence: section 3 mentions KL penalty and abstract claims fluency maintenance, but corpus lacks specific regularization papers. Break condition: if pre-trained model is biased or has poor fluency, KL penalty won't prevent low-quality captions.

### Mechanism 3
Direct optimization of CLIP-based metrics leads to more descriptive captions than traditional metrics like CIDEr. CLIP-based metrics measure image-caption embedding similarity, capturing semantic richness. Core assumption: CLIP-based metrics better align with human judgment than text-only metrics. Evidence: abstract claims significant human alignment, but corpus lacks direct comparisons. Break condition: if CLIP-based metrics don't reflect quality or visual embeddings are noisy, optimization may produce poor captions.

## Foundational Learning

- **Concept**: CLIP (Contrastive Language-Image Pre-training)
  - Why needed here: DiCO relies on CLIP-based metrics for evaluation and optimization. Understanding CLIP is crucial for grasping the evaluation process.
  - Quick check question: What is the key idea behind CLIP and how does it enable zero-shot image classification?

- **Concept**: Reinforcement Learning (RL) and Proximal Policy Optimization (PPO)
  - Why needed here: DiCO is derived from PPO's optimal solution, though it doesn't use RL during fine-tuning. Understanding RL foundations is necessary for grasping the theoretical basis.
  - Quick check question: What is the main difference between policy gradient methods and value-based methods in RL?

- **Concept**: Transformer architecture for image captioning
  - Why needed here: DiCO applies to standard Transformer models for captioning. Understanding the architecture is essential for implementation and debugging.
  - Quick check question: What are the key components of a Transformer-based image captioning model and how do they interact?

## Architecture Onboarding

- **Component map**: Image → Visual features → Captioner → Caption → CLIP embeddings → CLIP-based score → Reward model → Captioner update
- **Critical path**: Visual features from CLIP backbone flow through captioner to generate captions, which are evaluated using CLIP-based metrics, distilled into reward model, and used to update captioner parameters.
- **Design tradeoffs**: Using pre-trained CLIP backbone vs. training from scratch, direct optimization of CLIP metrics vs. traditional metrics, including KL regularization vs. allowing more freedom.
- **Failure signatures**: Model collapse (repetitive captions), reward hacking (optimizing metric without quality), over-regularization (too similar to pre-trained model).
- **First 3 experiments**: 1) Train baseline Transformer with XE loss on COCO, 2) Fine-tune baseline with DiCO using CLIP-S reward, 3) Fine-tune baseline with DiCO using PAC-S reward.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implicit questions arise from the work:

1. How does DiCO's performance scale when trained with larger datasets beyond COCO, particularly on diverse image domains?
2. What is the impact of different reward model architectures on DiCO's performance?
3. How does DiCO perform in multilingual image captioning tasks?

## Limitations

- The exact implementation details of the learnable evaluator for reward distillation remain unclear
- Claims about preventing model collapse lack thorough ablation studies on the KL regularization component
- The assertion that CLIP-based metrics are significantly aligned with human judgment is not empirically validated with human studies

## Confidence

- **High confidence**: The core technical contribution of using weighted classification to distill reward models is well-defined and reproducible
- **Medium confidence**: Claims about preventing model collapse and maintaining fluency are supported by experimental results but need more rigorous validation
- **Low confidence**: The assertion about CLIP-based metrics aligning with human judgment lacks empirical human study validation

## Next Checks

1. **Ablation study on KL regularization**: Remove the KL divergence term and retrain DiCO to determine whether prevention of model collapse is primarily due to this regularization or the weighted classification approach itself.

2. **Human evaluation comparison**: Conduct human studies comparing captions generated by DiCO versus SCST and baseline models to validate whether improved CLIP-S/PAC-S scores correlate with human preferences.

3. **Evaluator dependency test**: Replace the CLIP-based evaluator with a different metric (e.g., CIDEr or SPICE) and retrain DiCO to determine whether stability benefits are specific to contrastive metrics or generalizable to other reward functions.