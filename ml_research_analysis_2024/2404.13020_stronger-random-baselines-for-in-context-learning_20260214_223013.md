---
ver: rpa2
title: Stronger Random Baselines for In-Context Learning
arxiv_id: '2404.13020'
source_url: https://arxiv.org/abs/2404.13020
tags:
- random
- baseline
- validation
- maximum
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "In in-context learning (ICL) evaluation, the standard random baseline\u2014\
  expected accuracy of guessing labels uniformly at random\u2014does not account for\
  \ small datasets and validation set reuse. The authors propose a stronger baseline:\
  \ expected maximum accuracy across multiple random classifiers, which explicitly\
  \ depends on the number of examples (n), number of labels (m), and number of evaluated\
  \ prompts (t)."
---

# Stronger Random Baselines for In-Context Learning

## Quick Facts
- arXiv ID: 2404.13020
- Source URL: https://arxiv.org/abs/2404.13020
- Reference count: 40
- Primary result: Standard random baseline underestimates chance performance when multiple prompts are evaluated; maximum random baseline is a better predictor of held-out performance.

## Executive Summary
The paper addresses a critical gap in in-context learning (ICL) evaluation by proposing a stronger random baseline that accounts for validation set reuse and small dataset sizes. While the standard random baseline (1/m for m labels) has been widely used, it fails to capture the reality that practitioners evaluate multiple prompts and select the best one. The authors derive the expected maximum accuracy across multiple random classifiers as a function of dataset size (n), number of labels (m), and number of prompt evaluations (t), providing a more accurate benchmark for ICL performance.

Through extensive experiments with six quantized 7B-parameter language models on 16 BIG-bench Lite tasks, the authors demonstrate that over 20% of results exceeding the standard baseline do not exceed the stronger baseline. They show that comparing maximum validation accuracy to the maximum random baseline is a better predictor of held-out test performance than the standard baseline, reducing unnecessary test set evaluations. This stronger baseline provides a drop-in replacement for the standard baseline and better contextualizes ICL performance.

## Method Summary
The method evaluates ICL performance by comparing maximum validation accuracy against both standard random baseline (1/m) and maximum random baseline (expected maximum of t binomial random variables). The authors use six quantized 7B-parameter language models and 16 BIG-bench Lite tasks, evaluating 200 prompts per model-dataset-shot combination. The maximum random baseline is computed using the binomial distribution's order statistic, capturing the effect of selecting the best prompt from multiple evaluations. The approach addresses the common practice of validation set reuse and provides a more accurate benchmark for ICL performance.

## Key Results
- Over 20% of results exceeding the standard baseline did not exceed the stronger maximum random baseline
- The maximum random baseline is a better predictor of held-out test performance than the standard baseline
- For small datasets (< 200 examples), the maximum random baseline can be 10% or more higher than the standard baseline
- The stronger baseline provides a drop-in replacement that better contextualizes ICL performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The expected maximum random accuracy increases with more validation set evaluations because each evaluation is an independent chance to get lucky.
- Mechanism: Each random classifier's accuracy is a binomial random variable. When taking the maximum across t independent classifiers, the order statistic's expectation increases because the probability of observing at least one high-accuracy draw grows with t.
- Core assumption: All random classifiers are independent and identically distributed.
- Evidence anchors:
  - [abstract]: "We account for the common practice of validation set reuse and existing small datasets with a stronger random baseline: the expected maximum accuracy across multiple random classifiers."
  - [section 3]: "Let h1, . . ., ht be classifiers that guess answers independently and uniformly at random, with corresponding independent numbers of correct guesses X1, . . ., Xt."
  - [corpus]: Weak evidence; corpus contains related but not directly anchoring papers.

### Mechanism 2
- Claim: The maximum random baseline provides a better predictor of held-out accuracy because it models the selection process used in practice.
- Mechanism: Practitioners pick the best prompt based on validation accuracy. The maximum random baseline models the expected best performance among t random classifiers, capturing the effect of selecting the highest-scoring candidate from a pool.
- Core assumption: The validation process is equivalent to selecting the best of t random draws.
- Evidence anchors:
  - [abstract]: "When held-out test sets are available, this stronger baseline is also a better predictor of held-out performance than the standard baseline."
  - [section 5.2]: "We find that comparing maximum validation accuracy to the maximum random baseline, rather than the standard random baseline, is more indicative of whether held-out accuracy exceeds random chance."
  - [corpus]: Weak evidence; no direct corpus support for this selection mechanism claim.

### Mechanism 3
- Claim: Small datasets lead to higher variance in random classifier accuracy, making the maximum random baseline substantially higher than the standard baseline.
- Mechanism: For small n, the binomial distribution has high variance relative to its mean. The expected maximum of t such distributions is therefore much higher than the mean of a single draw.
- Core assumption: The binomial variance formula holds and dominates the behavior for small n.
- Evidence anchors:
  - [abstract]: "For larger datasets, the variance of this distribution is tightly concentrated around the expectation, but variance can be considerably higher for the small datasets typically used in the ICL setting."
  - [section 4]: "When there are fewer than several hundred examples in the validation set, even a few evaluations of the validation set yields a maximum random baseline at least 10% higher than the standard random baseline."
  - [corpus]: Weak evidence; corpus does not provide variance-focused support.

## Foundational Learning

- Concept: Order statistics of discrete distributions
  - Why needed here: The paper relies on computing the expected maximum of t binomial random variables, which is an order statistic problem.
  - Quick check question: What is the probability mass function of the maximum of t independent binomial random variables?

- Concept: Binomial distribution properties
  - Why needed here: The random classifier's accuracy is modeled as a binomial random variable, so understanding its mean, variance, and CDF is essential.
  - Quick check question: For a binomial(n, p) distribution, what is the probability of getting exactly k correct answers?

- Concept: p-value interpretation in distributional context
  - Why needed here: The paper uses the CDF of the random performance distribution to compute p-values, not just a point estimate.
  - Quick check question: How does the p-value against the maximum random baseline differ from the standard baseline?

## Architecture Onboarding

- Component map: Binomial PMF/CDF calculator -> Order statistic expectation calculator -> Maximum random baseline output
- Critical path: (1) Input: n (validation examples), m (labels), t (prompt evaluations). (2) Compute p = 1/m. (3) For each k in 0..n, compute P(Xmax = k) using the binomial CDF and PMF. (4) Compute E[Xmax] = sum(k * P(Xmax=k)). (5) Return E[Xmax]/n as the baseline.
- Design tradeoffs: Precomputing binomial coefficients vs computing on the fly; using log-space to avoid overflow for large n; caching CDF values if computing many baselines with same n, p.
- Failure signatures: (1) n <= 0 or m <= 0 → invalid input. (2) t < 1 → degenerate case. (3) Floating point underflow/overflow for very large n or t. (4) Incorrect CDF computation leading to negative probabilities.
- First 3 experiments:
  1. Compute the standard random baseline and maximum random baseline for n=100, m=2, t=1,10,100 and verify the max baseline increases with t.
  2. For a given validation accuracy (e.g., 0.6), compute p-values against both baselines for varying t and confirm the max baseline yields higher p-values.
  3. Simulate t random classifiers for n=50, m=5, t=20, compute their accuracies, take the max, repeat many times, and compare the empirical mean to the closed-form expectation.

## Open Questions the Paper Calls Out
The paper explicitly limits its analysis to in-context learning with language models for multiple choice tasks and states this baseline applies to "any classification setting with evaluation set reuse" without testing other task types. The authors suggest future work could explore extensions to settings where each example has a different number of possible labels, which would require using the Poisson binomial distribution instead of the binomial distribution.

## Limitations
- The experiments use quantized 4-bit models, which may perform differently from full-precision models, potentially limiting generalizability.
- The selection of 200 prompts per model-task-shot combination may not capture the full diversity of possible in-context learning demonstrations.
- The results are based on BIG-bench Lite tasks which may not represent all ICL evaluation scenarios.

## Confidence

**High Confidence**: The mathematical derivation of the maximum random baseline as an order statistic of binomial distributions is correct and well-established. The empirical observation that the standard baseline underestimates random performance when multiple prompts are evaluated is reproducible and mathematically inevitable.

**Medium Confidence**: The claim that the maximum random baseline better predicts held-out performance depends on the specific selection mechanism used in practice. While the paper demonstrates this on their experimental setup, different selection strategies (e.g., cross-validation, ensemble methods) may not benefit equally from this stronger baseline.

**Low Confidence**: The generalizability of the 20% figure (proportion of results exceeding standard but not maximum baseline) to other model families, task distributions, or evaluation protocols remains uncertain without broader experimental validation.

## Next Checks

1. **Order Statistic Verification**: For n=20, m=3, t=5, compute the maximum random baseline using the closed-form formula and compare it to 10,000 Monte Carlo simulations of t random classifiers to verify the analytical result.

2. **Selection Mechanism Sensitivity**: Repeat the experiments using a different prompt selection strategy (e.g., top-10% based on validation accuracy rather than maximum) and assess whether the maximum random baseline still provides better predictive power for held-out performance.

3. **Dataset Size Sensitivity**: Systematically vary n from 10 to 500 examples while keeping m=2 and t=20 fixed, and plot the ratio of maximum random baseline to standard baseline to identify the crossover point where the two baselines converge.