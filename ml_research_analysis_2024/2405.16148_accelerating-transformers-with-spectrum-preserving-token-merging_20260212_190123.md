---
ver: rpa2
title: Accelerating Transformers with Spectrum-Preserving Token Merging
arxiv_id: '2405.16148'
source_url: https://arxiv.org/abs/2405.16148
tags:
- pitome
- tokens
- token
- merging
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PiToMe, a novel token merging method for
  accelerating Vision Transformers (ViTs) by prioritizing the preservation of informative
  tokens. PiToMe uses an energy score to identify and protect low-energy, informative
  tokens while merging high-energy, redundant tokens.
---

# Accelerating Transformers with Spectrum-Preserving Token Merging

## Quick Facts
- arXiv ID: 2405.16148
- Source URL: https://arxiv.org/abs/2405.16148
- Reference count: 40
- Primary result: 40-60% FLOPs reduction with only 0.5% accuracy drop on ImageNet-1k

## Executive Summary
This paper introduces PiToMe, a novel token merging method for accelerating Vision Transformers (ViTs) by prioritizing the preservation of informative tokens. PiToMe uses an energy score to identify and protect low-energy, informative tokens while merging high-energy, redundant tokens. This approach is shown to be more effective than previous methods, achieving a 0.5% average performance drop on image classification compared to 2.6% for baselines, and saving 40-60% FLOPs. Theoretical analysis demonstrates that PiToMe preserves spectral properties of the original token space under mild conditions.

## Method Summary
PiToMe introduces an energy-based token merging approach that prioritizes preserving informative tokens while reducing computational cost. The method constructs a token graph using cosine similarity, calculates energy scores to identify high-energy (redundant) and low-energy (informative) tokens, and uses an ordered bipartite soft matching algorithm to merge tokens. The energy score function includes an adaptive margin that decreases with layer depth, allowing more tokens to be considered similar in deeper layers where token space becomes sparser. This approach is theoretically shown to preserve spectral properties of the original token space while achieving significant computational savings.

## Key Results
- Achieves 40-60% FLOPs reduction on ViT models
- Maintains only 0.5% average accuracy drop on ImageNet-1k classification
- Outperforms baselines by 2.1% average on ImageNet-1k (0.5% vs 2.6% drop)
- Demonstrates effectiveness across multiple tasks: image classification, image-text retrieval, VQA, and text classification

## Why This Works (Mechanism)

### Mechanism 1
Energy-based merging preserves spectral properties of the original token space. The energy score identifies large clusters of similar tokens (high energy) as merge candidates while preserving isolated or small-cluster tokens (low energy). This prioritization aligns with spectral graph theory, where the graph's eigenvalues and eigenvectors capture structural properties. The core assumption is that the energy score function accurately captures redundancy in token representations.

### Mechanism 2
Ordered energy-based bipartite soft matching improves token pairing quality. Tokens are sorted by energy score and split into two sets A and B using odd/even indices. This ensures tokens with similar energy (likely representing the same object) are distributed across sets, increasing the probability of finding optimal merge pairs. The core assumption is that tokens representing the same object have similar energy scores.

### Mechanism 3
Adaptive margin in energy score function improves performance across layers. The margin parameter m decreases as layers deepen (m = 0.9 - 0.9 × li/l), allowing more tokens to be considered similar in deeper layers where token space becomes sparser. The core assumption is that token space becomes sparser in deeper layers, requiring adjusted similarity thresholds.

## Foundational Learning

- **Spectral graph theory and graph coarsening**: Why needed here: The paper's theoretical guarantees rely on preserving spectral properties of the token graph through the merging process. Understanding eigenvalues, eigenvectors, and graph Laplacians is essential to grasp why PITOME maintains these properties. Quick check question: What is the relationship between the eigenvalues of a graph's Laplacian and its structural properties?

- **Bipartite Soft Matching (BSM) algorithm**: Why needed here: PITOME builds upon BSM but improves it with energy-based token prioritization. Understanding the original BSM algorithm is crucial to appreciate the novel contributions. Quick check question: How does BSM divide tokens into sets and determine which tokens to merge?

- **Cosine similarity and attention mechanisms in transformers**: Why needed here: The energy score function uses cosine similarity between token embeddings, and the merging process involves attention mechanisms. Understanding these concepts is essential for implementing and debugging the algorithm. Quick check question: How is cosine similarity calculated between two vectors, and why is it commonly used in transformer architectures?

## Architecture Onboarding

- **Component map**: Token embeddings -> Token graph construction -> Energy score calculation -> Token prioritization -> Bipartite matching -> Merging -> Merged token embeddings
- **Critical path**: Token graph construction → Energy score calculation → Token prioritization → Bipartite matching → Merging
- **Design tradeoffs**: Energy score computation adds overhead but improves merging quality; Adaptive margin parameter requires tuning but improves cross-layer performance; Token prioritization vs. random splitting (used in baseline BSM methods)
- **Failure signatures**: Poor performance if energy score function doesn't accurately capture token redundancy; Memory issues if token graph becomes too dense for large input sizes; Suboptimal merging if bipartite matching doesn't find good token pairs
- **First 3 experiments**: 1) Implement energy score calculation and verify it produces expected values for simple test cases; 2) Test token prioritization by comparing energy-sorted tokens against random selection; 3) Benchmark bipartite matching performance with and without energy-based ordering

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored: making the energy score function differentiable for end-to-end optimization, extending the approach to non-vision transformer architectures, and analyzing scalability for very large input sizes.

## Limitations
- The energy score function's computational overhead may be significant for very large input sizes
- The theoretical spectral preservation guarantees rely on assumptions that may not hold in all practical scenarios
- Limited evaluation on non-vision transformer architectures (BERT, LLaVA) despite claims of broader applicability

## Confidence
**High Confidence**: The energy-based merging approach effectively reduces FLOPs by 40-60% while maintaining accuracy; The theoretical framework connecting token merging to spectral graph theory is sound; The bipartite soft matching algorithm with energy-based token prioritization improves over baseline random splitting approaches.

**Medium Confidence**: The energy score function accurately captures token redundancy and redundancy-to-informativeness trade-offs; The adaptive margin parameter effectively adjusts similarity thresholds across layers; The spectral preservation guarantees hold under the claimed "mild conditions".

**Low Confidence**: The method generalizes equally well to all transformer architectures; The energy score calculation overhead is negligible compared to overall computation time; The bipartite matching quality improvement is solely due to energy-based token ordering.

## Next Checks
1. **Spectral Property Validation**: Implement spectral analysis tools to verify that the eigenvalues and eigenvectors of the token graph before and after merging remain similar. This would provide empirical validation of the theoretical spectral preservation claims.

2. **Energy Score Ablation Study**: Systematically test different energy score functions and margin parameter values across multiple layers and datasets to identify the optimal configuration and validate the adaptive margin strategy's effectiveness.

3. **Cross-Architecture Generalization**: Apply PITOME to transformer architectures beyond ViTs (such as BERT for text classification or LLaVA for VQA) to assess whether the energy-based merging approach generalizes across different token embedding spaces and attention mechanisms.