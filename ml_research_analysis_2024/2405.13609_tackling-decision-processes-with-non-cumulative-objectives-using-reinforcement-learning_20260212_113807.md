---
ver: rpa2
title: Tackling Decision Processes with Non-Cumulative Objectives using Reinforcement
  Learning
arxiv_id: '2405.13609'
source_url: https://arxiv.org/abs/2405.13609
tags:
- learning
- reinforcement
- training
- agent
- ncmdps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of solving non-cumulative Markov
  decision processes (NCMDPs), where the objective is to maximize the expected value
  of an arbitrary function of rewards rather than their sum. The authors introduce
  a general mapping that transforms NCMDPs into standard MDPs with adapted states
  and rewards, enabling the use of existing MDP solvers like reinforcement learning
  and dynamic programming without modification.
---

# Tackling Decision Processes with Non-Cumulative Objectives using Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.13609
- Source URL: https://arxiv.org/abs/2405.13609
- Reference count: 26
- Authors: Maximilian Nägele; Jan Olle; Thomas Fösel; Remmy Zen; Florian Marquardt
- Key outcome: Introduces a general mapping that transforms non-cumulative Markov decision processes (NCMDPs) into standard MDPs with adapted states and rewards, enabling the use of existing MDP solvers like reinforcement learning and dynamic programming without modification.

## Executive Summary
This paper addresses the challenge of solving non-cumulative Markov decision processes (NCMDPs), where the objective is to maximize the expected value of an arbitrary function of rewards rather than their sum. The authors introduce a general mapping that transforms NCMDPs into standard MDPs with adapted states and rewards, enabling the use of existing MDP solvers like reinforcement learning and dynamic programming without modification. The method is demonstrated across diverse applications including classical control (lunar lander), portfolio optimization (maximizing Sharpe ratio), and discrete optimization problems (quantum circuit optimization). Results show improvements in both final performance and training efficiency compared to standard MDP approaches.

## Method Summary
The method transforms any NCMDP into a standard MDP by augmenting the state space with additional information about reward history. The state space is extended with a vector ht that captures all necessary information about past rewards to compute the non-cumulative objective function in a Markovian way. The reward function is then redefined as the difference between consecutive values of the non-cumulative objective. By construction, the return of the transformed MDP equals the non-cumulative objective of the original NCMDP for any policy, preserving the optimal policy. The approach works with any standard MDP solver without requiring modifications to the algorithm.

## Key Results
- The method achieved up to 1.78× better performance than standard PPO in quantum error correction tasks
- In portfolio optimization, the approach significantly outperformed baseline algorithms in maximizing Sharpe ratio
- The transformed MDP approach showed improvements in both final performance and training efficiency compared to standard MDP approaches across all tested applications
- The method requires minimal implementation effort and is applicable to both deterministic and stochastic environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed method can transform any NCMDP into a standard MDP by augmenting the state space with additional information about reward history.
- Mechanism: The method constructs a corresponding MDP where the state space is extended with a vector ht that captures all necessary information about past rewards to compute the non-cumulative objective function in a Markovian way. The reward function is then redefined as the difference between consecutive values of the non-cumulative objective.
- Core assumption: The non-cumulative objective function f can be computed recursively using information that can be stored in a state vector of constant size.
- Evidence anchors:
  - [abstract] "We introduce a general mapping that transforms NCMDPs into standard MDPs with adapted states and rewards"
  - [section] "Definition 1 Given an NCMDP... we define a corresponding MDP... with functions ρ and u... such that rt = f (˜r0, . . . ,˜rt) − f (˜r0, . . . ,˜rt−1) = ρ(ht, ˜rt)"
  - [corpus] Weak evidence - corpus lacks papers specifically discussing NCMDP to MDP transformation methods
- Break condition: If the non-cumulative objective requires storing the entire history of rewards (making ht grow with trajectory length), the state space becomes intractable for practical RL algorithms.

### Mechanism 2
- Claim: The transformed MDP preserves the optimal policy of the original NCMDP.
- Mechanism: By construction, the return of the transformed MDP equals the non-cumulative objective of the original NCMDP for any policy. Therefore, the optimal policy that maximizes the MDP return also maximizes the NCMDP objective.
- Core assumption: The mapping between trajectories of the NCMDP and MDP is consistent and preserves the probability distribution over trajectories.
- Evidence anchors:
  - [abstract] "enabling the use of existing MDP solvers like reinforcement learning and dynamic programming without modification"
  - [section] "Theorem 1... the expected return of M is equal to the expected return of ˜M"
  - [corpus] Weak evidence - corpus lacks papers discussing theoretical guarantees for NCMDP transformations
- Break condition: If the state augmentation or reward transformation introduces errors in computing the non-cumulative objective, the policy optimality guarantee breaks.

### Mechanism 3
- Claim: The method works with any standard MDP solver without requiring modifications to the algorithm.
- Mechanism: The transformation operates purely at the environment level, modifying states and rewards while keeping the same action space and transition dynamics. Standard MDP solvers treat this as a regular MDP problem.
- Core assumption: MDP solvers are agnostic to the specific form of the reward function and only require Markovian states and rewards.
- Evidence anchors:
  - [abstract] "This allows all techniques developed to find optimal policies for MDPs... to be directly applied to the larger class of NCMDPs"
  - [section] "By mapping the NCMDP to an MDP, all guarantees available for MDP solvers... directly apply also to NCMDPs"
  - [corpus] Weak evidence - corpus lacks papers specifically discussing black-box compatibility of NCMDP transformations
- Break condition: If the MDP solver has built-in assumptions about reward structure (e.g., cumulative nature), it may not work correctly with the transformed rewards.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and their optimal policies
  - Why needed here: Understanding the standard MDP framework is essential to grasp how the proposed method extends it to NCMDPs
  - Quick check question: What is the optimal policy in an MDP trying to maximize, and how is it typically computed?

- Concept: Reinforcement Learning algorithms (e.g., PPO, Q-learning)
  - Why needed here: The method relies on using existing RL algorithms on the transformed MDP, so understanding how they work is crucial
  - Quick check question: How does the policy gradient theorem enable policy-based RL methods to find optimal policies?

- Concept: Non-cumulative objectives and their properties
  - Why needed here: The entire method is designed to handle objectives that cannot be expressed as sums of rewards, so understanding their characteristics is key
  - Quick check question: What makes an objective "non-cumulative," and why can't standard MDP methods handle them directly?

## Architecture Onboarding

- Component map: State augmentation function u -> Reward transformation function ρ -> MDP solver
- Critical path: The critical path is the computation of the adapted state and reward at each time step, which must be done before the MDP solver can make its decision
- Design tradeoffs: The main tradeoff is between the complexity of the state augmentation (which affects learning efficiency) and the generality of the method (which allows handling any non-cumulative objective)
- Failure signatures: Common failure modes include state explosion (if ht grows with trajectory length), incorrect implementation of u or ρ (leading to wrong objectives), and MDP solver incompatibility with the transformed rewards
- First 3 experiments:
  1. Implement the transformation for a simple non-cumulative objective (e.g., maximizing the maximum reward) on a grid world environment and verify that the optimal policy is found
  2. Compare the performance of the transformed MDP approach with a baseline that approximates the non-cumulative objective using cumulative rewards on a portfolio optimization task
  3. Test the method with a more complex non-cumulative objective (e.g., Sharpe ratio) on a continuous control task like Lunar Lander

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed NCMDP-to-MDP mapping maintain constant-sized state information for all possible objective functions, not just the restricted class mentioned in Section B.2?
- Basis in paper: [explicit] The paper states "In Section B.1, we provide a necessary and sufficient condition for objectives f with constant size additional state information ht" and "While Equation (10) provides a complete categorization of functions with constant size ht, in practice it may be difficult to check whether a given function satisfies this property."
- Why unresolved: The authors provide only a sufficient condition that is easier to verify but covers a smaller class of functions. They acknowledge that the complete characterization (Equation 10) is difficult to verify in practice.
- What evidence would resolve it: A constructive classification showing which objective functions beyond those in Table 1 can maintain constant-sized state information, or counterexamples demonstrating limitations of the approach.

### Open Question 2
- Question: How does the performance of the NCMDP approach compare to specialized algorithms for specific non-cumulative objectives (like the max objective) across diverse environments?
- Basis in paper: [explicit] The authors state "The question remains how to find the functions u and ρ given a new objective f" and note that their method "may result in a significantly increased state space, which might pose challenges for tabular reinforcement learning methods and dynamic programming techniques."
- Why unresolved: While the paper demonstrates improvements over standard MDP approaches, it doesn't provide comprehensive comparisons against specialized algorithms for individual non-cumulative objectives across multiple environments.
- What evidence would resolve it: Systematic empirical studies comparing the proposed method against specialized algorithms (e.g., Quah and Quek 2006, Veviurko et al. 2024) across multiple environments and non-cumulative objectives.

### Open Question 3
- Question: What is the impact of trajectory length on the effectiveness of the MAXPPO algorithm for discrete optimization problems?
- Basis in paper: [explicit] The authors note "When applying MAXPPO to discrete optimization problems with long trajectories in the many hundreds of steps, we empirically observed an initially slow learning speed."
- Why unresolved: The paper only mentions this observation briefly without providing detailed analysis of how trajectory length affects performance, or potential solutions to the slow learning issue.
- What evidence would resolve it: Empirical studies varying trajectory lengths systematically, analysis of variance reduction effects across different trajectory lengths, and evaluation of proposed solutions like dynamically adjusting trajectory length during training.

## Limitations
- The state augmentation requires storing reward history information in a fixed-size vector, which may not be possible for all objective functions
- The experimental validation relies on relatively small-scale problems, with untested performance on larger, more complex problems
- The approach may result in significantly increased state space, potentially posing challenges for tabular reinforcement learning methods and dynamic programming techniques

## Confidence
- High: The theoretical foundation for transforming NCMDPs to MDPs appears sound, with formal guarantees provided
- Medium: Empirical evidence shows positive results but covers a limited range of problem types and scales
- Low: No formal characterization of which objective functions admit compact state representations

## Next Checks
1. **State complexity analysis**: For each application domain (quantum optimization, portfolio management, control), formally analyze the size of the augmented state vector ht and determine whether it remains tractable as problem scale increases.

2. **Generalization to new objectives**: Implement the transformation for additional non-cumulative objectives not covered in the paper (e.g., minimizing drawdown in finance, maximizing minimum reward in control) to test the method's generality.

3. **Scaling experiments**: Test the approach on larger quantum codes (beyond 5 qubits) and with finer-grained financial data (individual stocks rather than sectors) to evaluate performance degradation with problem complexity.