---
ver: rpa2
title: Improving Pseudo Labels with Global-Local Denoising Framework for Cross-lingual
  Named Entity Recognition
arxiv_id: '2406.01213'
source_url: https://arxiv.org/abs/2406.01213
tags:
- language
- target
- data
- entity
- pseudo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GLoDe, a global-local denoising framework for
  cross-lingual named entity recognition. The method denoises pseudo labels for target
  language data by leveraging global and local distribution information in the semantic
  space, along with an auxiliary masked language model task to capture target language-specific
  features.
---

# Improving Pseudo Labels with Global-Local Denoising Framework for Cross-lingual Named Entity Recognition

## Quick Facts
- **arXiv ID**: 2406.01213
- **Source URL**: https://arxiv.org/abs/2406.01213
- **Reference count**: 28
- **Key outcome**: GLoDe achieves 0.41 F1 improvement on CoNLL and 1.89 F1 improvement on WikiAnn over previous state-of-the-art methods

## Executive Summary
This paper introduces GLoDe, a global-local denoising framework for cross-lingual named entity recognition that addresses the limitations of existing pseudo-label-based methods. The framework progressively denoises pseudo labels by leveraging both global semantic information through prototype-based clustering and local neighborhood distributions, while also incorporating an auxiliary masked language model task to capture target language-specific features. Experiments on CoNLL and WikiAnn datasets with six diverse target languages demonstrate significant improvements over previous state-of-the-art approaches, validating the effectiveness of combining global and local denoising strategies with target language-specific feature learning.

## Method Summary
GLoDe reformulates cross-lingual NER as a span classification problem and employs a progressive denoising strategy to refine pseudo labels for unlabeled target language data. The method uses prototypes to represent global entity type distributions and leverages local neighbor distributions for context-aware refinement. An auxiliary masked language model task is trained on target language data to capture language-specific features that complement language-agnostic alignment approaches. The framework iteratively improves pseudo labels across training epochs, combining decisions from both global and local levels with class-specific similarity thresholds to reduce misclassification.

## Key Results
- Achieves 0.41 average F1 improvement on CoNLL dataset compared to previous state-of-the-art
- Achieves 1.89 average F1 improvement on WikiAnn dataset compared to previous state-of-the-art
- Demonstrates consistent performance gains across six diverse target languages (German, Spanish, Dutch, Arabic, Hindi, Chinese)
- Shows that combining global and local denoising strategies yields superior results compared to using either approach alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Progressive denoising using both global and local semantic information significantly improves pseudo label quality.
- **Mechanism**: Uses prototypes for global distribution representation and neighbor type distributions for local context, combining both to iteratively refine pseudo labels.
- **Core assumption**: Misclassified samples are more similar to incorrect types' prototypes but can be corrected using local neighborhood information.
- **Evidence anchors**: [abstract] introduces progressive denoising strategy; [section] describes prototype-based refinement and neighbor distribution leverage.
- **Break condition**: Fails if semantic space representation is poor or prototypes/neighbor distributions are unreliable.

### Mechanism 2
- **Claim**: Auxiliary MLM task improves model's understanding of target language-specific features.
- **Mechanism**: Trains MLM on unlabeled target language data alongside main NER task to capture language-specific patterns.
- **Core assumption**: Language-specific features contain valuable information not captured by language-agnostic alignment.
- **Evidence anchors**: [abstract] argues target language-specific features are important; [section] describes MLM task on target language data.
- **Break condition**: Fails if target language data is too limited or too different from source language.

### Mechanism 3
- **Claim**: Multiple-decision scheme with dynamic thresholds mitigates misclassification in pseudo label denoising.
- **Mechanism**: Considers multiple types whose similarity exceeds class-specific thresholds rather than only the most similar type.
- **Core assumption**: Many misclassified samples are most similar to incorrect types' prototypes, and multiple-type consideration can help identify correct type.
- **Evidence anchors**: [abstract] describes similarity thresholds for potential classifications; [section] explains multiple-decision scheme.
- **Break condition**: Fails if thresholds are not well-calibrated or semantic space is not well-structured.

## Foundational Learning

- **Concept**: Span-based Named Entity Recognition
  - **Why needed here**: Reformulates NER as span classification problem fundamental to understanding model architecture and loss functions.
  - **Quick check question**: What are the advantages of treating NER as a span classification problem rather than a sequence labeling problem?

- **Concept**: Prototype-based clustering
  - **Why needed here**: Global-level denoising relies on prototypes as cluster centroids for each entity type.
  - **Quick check question**: How does using prototypes as cluster centroids help in identifying and correcting misclassified samples?

- **Concept**: Masked Language Model (MLM) pre-training
  - **Why needed here**: Auxiliary MLM task captures target language-specific features.
  - **Quick check question**: What types of language-specific patterns and distributions can an MLM task help a model learn that might be beneficial for NER?

## Architecture Onboarding

- **Component map**: PLM -> Span Classifier -> Global-level Denoising Module -> Local-level Denoising Module -> Pseudo Label Refinement, simultaneously with MLM Head -> MLM Task

- **Critical path**:
  1. Input sentences processed by PLM to get token representations
  2. Span representations fed to classifier for NER predictions
  3. Span representations used by denoising modules to refine pseudo labels
  4. Refined pseudo labels update model in next iteration
  5. MLM task performed on target language data simultaneously

- **Design tradeoffs**:
  - Using both global and local information increases computational complexity but provides more comprehensive denoising
  - Multiple-decision scheme with dynamic thresholds is more complex than single-type assignment but reduces misclassification
  - Auxiliary MLM task adds training overhead but captures valuable target language-specific features

- **Failure signatures**:
  - Pseudo labels don't improve over training epochs indicates denoising mechanism not working
  - Target language performance doesn't improve despite good source language performance suggests MLM task not capturing useful features
  - Significant performance degradation on similar languages indicates over-correction by global-local denoising

- **First 3 experiments**:
  1. Train baseline model without denoising or auxiliary tasks to establish performance floor
  2. Add global-level denoising mechanism only to evaluate standalone effectiveness
  3. Add local-level denoising mechanism only to evaluate standalone effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does GLoDe performance compare to using only global or only local denoising information?
- **Basis in paper**: [explicit] Figure 3 shows combining both yields superior results, but doesn't provide quantitative comparison of individual components.
- **Why unresolved**: Paper shows combined approach is beneficial but lacks specific performance differences between individual strategies.
- **What evidence would resolve it**: Table showing F1 scores for each target language when using only global, only local, and both global&local denoising strategies.

### Open Question 2
- **Question**: How does GLoDe performance vary with different values of hyperparameter K in local similarity score computation?
- **Basis in paper**: [explicit] K is set to 300 for neighbor retrieval but impact of varying K is not explored.
- **Why unresolved**: Paper doesn't investigate how choice of K affects quality of pseudo labels and overall performance.
- **What evidence would resolve it**: Experiments with different K values (100, 300, 500, 1000) and corresponding F1 scores on each target language.

### Open Question 3
- **Question**: How does GLoDe performance compare to other cross-lingual NER methods using large language models like ChatGPT?
- **Basis in paper**: [inferred] Appendix C mentions potential of using LLMs for cross-lingual NER but no direct comparison provided.
- **Why unresolved**: Paper doesn't explore GLoDe performance compared to methods utilizing in-context learning ability of LLMs.
- **What evidence would resolve it**: Experiments comparing F1 scores of GLoDe with methods using LLMs like ChatGPT on same datasets.

## Limitations

- **Semantic Space Quality**: Effectiveness depends critically on quality of semantic space where prototypes and neighbor distributions are computed; poor representations may introduce noise rather than remove it.
- **Threshold Sensitivity**: Uses dynamic class-specific thresholds but lacks sensitivity analysis showing how performance varies with different threshold settings.
- **Computational Overhead**: Multiple-decision scheme with prototype computation and neighbor retrieval introduces substantial computational overhead compared to baseline methods.

## Confidence

- **High Confidence Claims**: GLoDe outperforms previous state-of-the-art on both CoNLL and WikiAnn datasets; auxiliary MLM task contributes positively to target language understanding; progressive denoising improves pseudo label quality.
- **Medium Confidence Claims**: Global and local denoising mechanisms are primary drivers of improvement; multiple-decision scheme with dynamic thresholds is essential for mitigating misclassification; framework generalizes well across tested languages.
- **Low Confidence Claims**: Approach would scale effectively to dozens of target languages; denoising mechanism would be equally effective for all entity types; framework would maintain performance advantages on significantly different datasets.

## Next Checks

1. **Ablation Study on Denoising Components**: Systematically disable global denoising, local denoising, and MLM auxiliary task in separate experiments to quantify individual contributions and validate whether improvements are additive or synergistic.

2. **Threshold Sensitivity Analysis**: Conduct experiments varying similarity thresholds across a wide range to identify optimal settings and determine whether current thresholds are near-optimal or if performance could be improved through better threshold selection.

3. **Cross-Dataset Generalization Test**: Evaluate GLoDe on a third, independent cross-lingual NER dataset (such as WNUT or KBP) that was not used in original experiments to test whether framework's advantages generalize beyond CoNLL and WikiAnn datasets.