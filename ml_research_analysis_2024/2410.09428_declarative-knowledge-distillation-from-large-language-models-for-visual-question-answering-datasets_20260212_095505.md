---
ver: rpa2
title: Declarative Knowledge Distillation from Large Language Models for Visual Question
  Answering Datasets
arxiv_id: '2410.09428'
source_url: https://arxiv.org/abs/2410.09428
tags:
- rules
- question
- reasoning
- theory
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for declarative knowledge distillation
  from Large Language Models (LLMs) to generate Answer-Set Programming (ASP) rules
  for Visual Question Answering (VQA). The approach uses LLMs to extend an initial
  ASP theory based on examples from VQA datasets, with feedback from an ASP solver
  to correct rules.
---

# Declarative Knowledge Distillation from Large Language Models for Visual Question Answering Datasets

## Quick Facts
- arXiv ID: 2410.09428
- Source URL: https://arxiv.org/abs/2410.09428
- Authors: Thomas Eiter; Jan Hadl; Nelson Higuera; Johannes Oetsch
- Reference count: 18
- Primary result: Knowledge distillation from LLMs can effectively generate ASP rules for VQA tasks

## Executive Summary
This paper presents a novel approach for declarative knowledge distillation from Large Language Models (LLMs) to generate Answer-Set Programming (ASP) rules for Visual Question Answering (VQA) tasks. The method uses LLMs to extend an initial ASP theory based on examples from VQA datasets, with feedback from an ASP solver to correct rules. The approach was evaluated on CLEVR and GQA datasets, demonstrating that GPT-4 can effectively produce ASP rules to handle new reasoning operations, achieving up to 99.97% accuracy on GQA and 100% on CLEVR for certain predicates.

## Method Summary
The method involves using LLMs to extend an initial ASP theory by generating rules based on examples from VQA datasets. When the current ASP theory cannot handle a specific question, the LLM is prompted with the question, scene, and expected answer to generate new rules. The ASP solver validates these rules, and if incorrect, the LLM is prompted again to correct them. The approach also explores batch processing of examples to reduce the number of LLM calls and regression testing to ensure new rules don't break previously solvable questions.

## Key Results
- Achieved up to 99.97% accuracy on GQA and 100% on CLEVR for certain predicates
- Demonstrated effective rule generation for new reasoning operations
- Batch processing can reduce LLM calls while maintaining accuracy
- Regression testing prevents degradation of previously solvable questions

## Why This Works (Mechanism)

### Mechanism 1
LLMs can understand and generate ASP rules that extend an incomplete theory to handle new reasoning operations. The method prompts an LLM with examples of questions, scenes, and expected answers that the current ASP theory cannot handle. The LLM then generates new ASP rules to address the missing reasoning steps. The ASP solver validates these rules, and if incorrect, the LLM is prompted again to correct them.

### Mechanism 2
Batch processing of examples can reduce the number of LLM calls while still producing effective rules. Instead of presenting individual question/scene/answer tuples to the LLM, the method groups examples into batches. The LLM is then prompted to generate rules that can handle all examples in the batch simultaneously.

### Mechanism 3
Regression testing ensures that newly generated rules do not break the ability to answer previously solvable questions. After generating new rules, the method tests the updated ASP theory on all previously seen examples. Only if the theory passes all regression tests are the new rules kept.

## Foundational Learning

- Concept: Answer-Set Programming (ASP)
  - Why needed here: ASP is the logical framework used to represent the reasoning component of the VQA system.
  - Quick check question: What is the difference between a rule with negation as failure and a rule with classical negation in ASP?

- Concept: Visual Question Answering (VQA)
  - Why needed here: VQA is the task that the method is designed to solve.
  - Quick check question: What are the main components of a typical VQA system, and how do they interact?

- Concept: Large Language Models (LLMs)
  - Why needed here: LLMs are the core component used to generate new ASP rules.
  - Quick check question: What are the main challenges in using LLMs for tasks that require precise syntax, such as generating ASP rules?

## Architecture Onboarding

- Component map:
  LLM -> ASP Solver -> VQA System -> Dataset

- Critical path:
  1. Identify a question/scene/answer tuple that the current ASP theory cannot handle
  2. Prompt the LLM with the tuple to generate new rules
  3. Validate the generated rules using the ASP solver
  4. If the rules are correct, test them using regression testing
  5. If the rules pass regression testing, add them to the ASP theory

- Design tradeoffs:
  - Batch size vs. accuracy: Larger batch sizes reduce the number of LLM calls but may lead to less accurate rules
  - Number of retries vs. performance: More retries can improve the quality of the generated rules but increase the computational cost
  - Mending vs. no mending: Mending can improve the quality of the rules but requires additional LLM calls

- Failure signatures:
  - Syntax errors in the generated ASP rules
  - Incorrect answers from the ASP solver when using the generated rules
  - Regression testing failures

- First 3 experiments:
  1. Remove all rules that mention a specific predicate from the complete ASP theory and attempt to restore it using the knowledge distillation method
  2. Randomly remove a percentage of rules from the complete ASP theory and attempt to restore it using the knowledge distillation method
  3. Test the batch processing variant with different batch sizes and compare the results to the single example approach

## Open Questions the Paper Calls Out
1. Can knowledge distillation from LLMs be effectively applied to other neuro-symbolic tasks beyond VQA, such as semantic parsing or logical reasoning puzzles?
2. How does the performance of knowledge distillation scale with the size and complexity of the ASP theory, especially when dealing with large-scale, real-world datasets?
3. Can the knowledge distillation process be made more efficient by using smaller, specialized LLMs or by incorporating techniques from inductive logic programming?

## Limitations
- Reliance on LLMs introduces uncertainties about generalizability to more complex tasks
- Batch processing requires careful tuning of batch size parameters
- Regression testing may become computationally expensive for larger theories
- Current evaluation is limited to synthetic datasets (CLEVR and GQA)

## Confidence

**High Confidence**: The core mechanism of using LLMs to generate ASP rules and validate them through an ASP solver is well-demonstrated and supported by experimental results showing high accuracy on both CLEVR and GQA datasets.

**Medium Confidence**: The batch processing approach and its effectiveness in reducing computational costs while maintaining accuracy.

**Low Confidence**: The approach's scalability to more complex VQA tasks and its ability to handle real-world scenarios with noisy data or incomplete information.

## Next Checks
1. Apply the knowledge distillation method to a more complex VQA dataset with diverse question types and real-world images to evaluate its performance beyond synthetic datasets.
2. Measure the computational cost and accuracy degradation when scaling the approach to larger ASP theories and more extensive question-answer pairs, particularly focusing on the batch processing mechanism.
3. Test the method's resilience to noisy data, incomplete information, and adversarial examples to assess its practical applicability in real-world scenarios.