---
ver: rpa2
title: 'Value-Based Rationales Improve Social Experience: A Multiagent Simulation
  Study'
arxiv_id: '2408.02117'
source_url: https://arxiv.org/abs/2408.02117
tags:
- agents
- agent
- values
- exanna
- rationales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Exanna is a framework enabling agents to incorporate values in
  decision making and rationale generation. It allows agents to selectively share
  information aligned with their own and others' values when justifying norm-deviating
  actions.
---

# Value-Based Rationales Improve Social Experience: A Multiagent Simulation Study

## Quick Facts
- arXiv ID: 2408.02117
- Source URL: https://arxiv.org/abs/2408.02117
- Reference count: 40
- Agents using value-based rationales showed 60% conflict resolution vs 58% baseline in pandemic simulation

## Executive Summary
This paper introduces Exanna, a framework enabling agents to incorporate values in decision making and rationale generation. The framework allows agents to selectively share information aligned with their own and others' values when justifying norm-deviating actions. In a multiagent pandemic simulation, Exanna agents demonstrated improved social experience and privacy retention compared to baseline approaches.

The study shows that value-aligned rationales prove more effective than sharing all information or just rules, with agents learning to deviate from personal goals to improve cooperation. The framework represents a significant step toward more nuanced and context-aware agent communication strategies.

## Method Summary
The Exanna framework was evaluated through a multiagent pandemic simulation where agents needed to justify norm-deviating actions to other agents. The simulation environment included agents with varying values, social norms, and privacy preferences. Agents could choose to share complete information, rule-based explanations, or value-aligned rationales when explaining their actions.

The evaluation compared three conditions: baseline agents sharing complete information, agents sharing only rule-based explanations, and Exanna agents sharing value-aligned rationales. Metrics included conflict resolution rates, social experience scores, privacy retention, and flexibility measures. The simulation ran for multiple iterations with different agent configurations.

## Key Results
- Exanna agents achieved 60% conflict resolution rate compared to 58% baseline
- Social experience improved from 0.59-0.62 (baseline) to 0.70 with value-aligned rationales
- Privacy retention increased from near 0 to 0.25 with value-based sharing approach
- Agent flexibility improved from 0.09-0.10 to 0.12 when using value-aligned rationales

## Why This Works (Mechanism)
Exanna works by enabling agents to selectively share information based on value alignment between communicating parties. When an agent needs to justify a norm-deviating action, it evaluates which pieces of information would be most persuasive to the recipient based on their known values. This targeted communication strategy reduces information overload while maximizing the likelihood of successful persuasion.

The framework leverages the observation that humans respond more favorably to rationales that connect to their existing values and beliefs. By encoding this principle into agent behavior, Exanna agents can achieve better cooperation outcomes while maintaining privacy and avoiding unnecessary information disclosure.

## Foundational Learning

**Value Hierarchies** - Agents must understand the relative importance of different values to prioritize which information to share. Quick check: Can the agent correctly rank values when presented with conflicting scenarios?

**Social Norm Compliance** - Agents need to recognize when their actions deviate from established norms and why this requires justification. Quick check: Does the agent identify norm violations before attempting to justify them?

**Information Selectivity** - Agents must learn which pieces of information are most relevant to specific recipients based on their values. Quick check: Can the agent tailor explanations differently for recipients with different value profiles?

**Persuasion Metrics** - Agents need quantitative measures of how well their rationales are received by others. Quick check: Does the agent adjust its communication strategy based on feedback?

**Privacy-Preserving Communication** - Agents must balance transparency needs with privacy preservation. Quick check: Does the agent maintain appropriate information boundaries while still achieving cooperation?

## Architecture Onboarding

**Component Map**: Value Engine -> Rationalizer -> Communicator -> Feedback Analyzer -> Learning Module

**Critical Path**: When an agent considers a norm-deviating action, the Value Engine identifies relevant values, the Rationalizer constructs value-aligned explanations, the Communicator delivers the rationale, the Feedback Analyzer evaluates reception, and the Learning Module updates future behavior.

**Design Tradeoffs**: The framework trades computational complexity for more effective communication. Value alignment checking adds overhead but improves outcomes. Privacy preservation reduces information sharing but maintains trust.

**Failure Signatures**: Poor value alignment detection leads to ineffective rationales. Overly restrictive privacy settings prevent necessary transparency. Insufficient learning from feedback causes repeated communication failures.

**3 First Experiments**:
1. Test value alignment accuracy by having agents generate rationales for recipients with known value profiles
2. Evaluate privacy retention by measuring information disclosure across different communication strategies
3. Assess learning effectiveness by tracking how quickly agents improve their rationale generation over time

## Open Questions the Paper Calls Out
None

## Limitations
- Results limited to pandemic simulation scenario with fixed social norms and value hierarchies
- Only 60% conflict resolution shows modest improvement over 58% baseline, questioning practical significance
- Evaluation metrics rely on internal simulation measures not validated against human judgments
- Framework assumes perfect knowledge of others' values, which may not hold in real-world applications

## Confidence
- **High confidence**: The Exanna framework's technical implementation and simulation mechanics
- **Medium confidence**: The relative effectiveness of value-based rationales versus alternative information-sharing strategies
- **Low confidence**: The real-world applicability and generalizability of results to human-agent interactions

## Next Checks
1. Conduct human subject studies to validate social experience and privacy metrics against subjective user perceptions
2. Test the framework across multiple domains beyond pandemic scenarios, including business negotiations and collaborative creative tasks
3. Evaluate robustness when agents have incomplete or noisy information about others' values and norm compliance histories