---
ver: rpa2
title: Impact of Label Noise on Learning Complex Features
arxiv_id: '2411.04569'
source_url: https://arxiv.org/abs/2411.04569
tags:
- features
- noisy
- labels
- feature
- feature-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how noisy label pre-training can mitigate
  simplicity bias in neural networks, where models tend to rely on simpler features
  even when complex features are available. The authors propose a two-stage training
  approach: first, pre-training with noisy labels, then fine-tuning on clean labels.'
---

# Impact of Label Noise on Learning Complex Features

## Quick Facts
- arXiv ID: 2411.04569
- Source URL: https://arxiv.org/abs/2411.04569
- Authors: Rahul Vashisht; P. Krishna Kumar; Harsha Vardhan Govind; Harish G. Ramaswamy
- Reference count: 40
- Primary result: Noisy label pre-training shifts neural networks away from simplicity bias, enabling learning of more complex and diverse features

## Executive Summary
This paper investigates how noisy label pre-training can mitigate simplicity bias in neural networks, where models tend to rely on simpler features even when complex features are available. The authors propose a two-stage training approach: first, pre-training with noisy labels, then fine-tuning on clean labels. Experiments on synthetic and real datasets (slab data, dominoes, and waterbirds) show that noisy pre-training enables models to learn more diverse and complex features, improving out-of-group generalization. Metrics like randomized shuffle accuracy and Gram matrix visualizations confirm that models shift focus from simple to complex features. The findings challenge the notion that SGD inherently favors simple features and suggest that noisy pre-training can guide models toward more robust, diverse decision boundaries.

## Method Summary
The paper proposes a two-stage training procedure where neural networks are first pre-trained on data with noisy labels, then fine-tuned on clean labels. During pre-training, a fraction of labels are randomly corrupted, forcing the model to find alternative minima that require using more complex features to fit the noisy data. After pre-training converges, the model is fine-tuned on the original clean labels. This approach is evaluated across synthetic slab data, Dominoes datasets (MNIST-FMNIST), and Waterbirds datasets, with performance measured through randomized shuffle accuracy, Gram matrix visualizations, and in-group/out-of-group generalization accuracy.

## Key Results
- Noisy pre-training significantly reduces reliance on simple features as measured by randomized shuffle accuracy across multiple datasets
- Models pre-trained with noisy labels show improved out-of-group generalization compared to standard training
- Gram matrix visualizations confirm that pre-training with noisy labels leads to more diverse feature learning patterns
- Ensembling models trained with different noise patterns further improves robustness and accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Noisy pre-training shifts SGD into local minima that are not solely dependent on simple features, thereby forcing the model to utilize a broader set of features.
- Mechanism: During noisy pre-training, the label corruption prevents the optimizer from finding the trivial, low-complexity minima that rely only on simple features. Instead, the model is pushed into alternative minima that require leveraging more complex features to fit the noisy labels. This new initialization, when fine-tuned on clean data, retains the ability to use diverse features.
- Core assumption: SGD with noisy labels converges to a different basin of attraction than standard SGD, and the resulting model parameters are sufficiently distinct to alter feature dependence patterns.
- Evidence anchors:
  - [abstract] "pre-training with noisy labels encourages gradient descent to find alternate minima that do not solely depend upon simple features, rather learns more complex and broader set of features"
  - [section] "pre-training promotes learning complex functions and diverse features in the presence of noise"
  - [corpus] Weak - related papers discuss label noise and generalization but not the specific minima-shifting mechanism.
- Break condition: If the noisy pre-training does not change the loss landscape enough to escape simple-feature minima, the model will revert to simplicity bias during fine-tuning.

### Mechanism 2
- Claim: Noisy pre-training creates models that are "trapped" in complex decision boundaries, preventing them from reverting to simple-feature-only solutions even with clean labels.
- Mechanism: The model first learns to fit noisy labels by using a complex decision boundary that incorporates multiple features. During subsequent fine-tuning on clean data, the intrinsic regularization of SGD is insufficient to collapse this complex boundary back into a simple one, so the model retains its diverse feature usage.
- Core assumption: Once a model finds a complex minimum during noisy pre-training, the SGD dynamics on clean data do not have enough "pull" to revert to simpler minima.
- Evidence anchors:
  - [abstract] "The model, however, remains trapped in this minima and is unable to revert to a simpler decision boundary despite the intrinsic regularization properties of SGD"
  - [section] "We show that pretraining promotes learning complex functions and diverse features in the presence of noise"
  - [corpus] Weak - no direct corpus evidence for the "trap" effect; inferred from the paper's explanation.
- Break condition: If the clean-data fine-tuning phase has a strong enough learning rate or regularization to override the initial complex minimum, the model may revert to simplicity bias.

### Mechanism 3
- Claim: Noisy pre-training increases the diversity of learned decision boundaries, and ensembling these diverse models improves robustness.
- Mechanism: Different instances of noisy pre-training (with different noise patterns) lead the model to converge to different local minima, each relying on a different subset of complex features. Aggregating predictions from these diverse models results in more robust and accurate classification.
- Core assumption: The loss landscape contains many equivalent-accuracy minima that use different feature combinations, and noisy pre-training is sufficient to explore this diversity.
- Evidence anchors:
  - [section] "With different label noise, we observe that the decision boundaries learned are varying across each feature" and "Different noisy datasets, D and D′, result in distinct decision boundaries"
  - [section] "This is in contrast to the standard data-agnostic initializations, such as Xavier-Glorot, that converge to similar decision boundaries"
  - [corpus] Weak - related work on label noise and generalization does not explicitly discuss diversity of minima.
- Break condition: If the noise level is too low, the model may not escape the simple-feature basin; if too high, it may not learn useful features at all.

## Foundational Learning

- Concept: Simplicity bias in neural networks
  - Why needed here: The paper's central claim is that noisy pre-training mitigates simplicity bias, so understanding what simplicity bias is and why it occurs is essential to grasp the contribution.
  - Quick check question: What is simplicity bias, and how does it manifest in the training of overparameterized neural networks?

- Concept: Implicit regularization of SGD
  - Why needed here: The paper contrasts the effect of noisy pre-training with the intrinsic regularization properties of SGD, arguing that noisy pre-training can overcome SGD's tendency toward simple solutions.
  - Quick check question: How does SGD's implicit regularization lead to a preference for simple features, and why might this be insufficient to prevent shortcut learning?

- Concept: Label noise as a regularizer
  - Why needed here: The paper uses noisy labels not just as data corruption but as a tool to guide the learning process toward more complex, diverse features.
  - Quick check question: In what ways can label noise influence the optimization trajectory and the types of minima that SGD converges to?

## Architecture Onboarding

- Component map:
  - Data pipeline: Synthetic slab data -> Dominoes (MNIST-FMNIST) -> Waterbirds datasets
  - Model: Multi-layer perceptrons (2-4 layers) with ReLU activations
  - Training loop: Noisy pre-training -> Fine-tuning on clean labels -> Evaluation
  - Evaluation: Randomized shuffle accuracy -> Gram matrix visualization -> In-group/out-group accuracy

- Critical path:
  1. Generate or load dataset
  2. Apply label noise (randomly flip a fraction of labels)
  3. Pre-train model on noisy data to convergence
  4. Fine-tune pre-trained model on clean labels
  5. Evaluate feature dependence and generalization

- Design tradeoffs:
  - Noise level: Too little noise may not escape simple minima; too much may prevent learning.
  - Pre-training duration: Must be long enough to find a complex minimum but not so long as to overfit noise.
  - Model capacity: Overparameterized models are necessary to have the flexibility to learn complex features.

- Failure signatures:
  - If randomized shuffle accuracy for simple features remains high after pre-training, the model is still relying on simple features.
  - If in-group accuracy is high but out-group accuracy is low, the model is overfitting to spurious correlations.
  - If training accuracy drops significantly after pre-training, the model may have overfit to noise.

- First 3 experiments:
  1. Reproduce the slab data experiment: Train a 2-layer MLP on 4D slab data with and without noisy pre-training; compare feature dependence via shuffle accuracy.
  2. Run the Dominoes experiment: Use 3- and 4-layer MLPs on MNIST-FMNIST; measure top/bottom shuffle accuracy before and after noisy pre-training.
  3. Visualize Gram matrices: For a trained model, plot the diagonal of W₁ᵀW₁ to observe which input pixels the first layer relies on; compare standard vs. noisy pre-training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the magnitude of label noise during pre-training affect the diversity of learned features and decision boundaries?
- Basis in paper: [explicit] The paper states that different levels of label noise during pre-training lead to varying decision boundaries, but does not systematically study the effect of noise magnitude.
- Why unresolved: The experiments only use 10% label noise and show that it leads to diverse features, but the relationship between noise level and feature diversity is not explored.
- What evidence would resolve it: Experiments varying the percentage of corrupted labels during pre-training and measuring resulting feature diversity and generalization performance.

### Open Question 2
- Question: Can the observed benefits of noisy pre-training be explained by theoretical analysis of the loss landscape?
- Basis in paper: [inferred] The paper mentions that different noisy pre-training runs lead to distinct local minima with similar accuracy, suggesting a rich loss landscape, but does not provide theoretical justification.
- Why unresolved: The authors conjecture about the nature of local minima but do not provide rigorous theoretical analysis of why noisy pre-training leads to more diverse solutions.
- What evidence would resolve it: Theoretical analysis of the loss landscape of overparameterized networks showing that noisy pre-training leads to different basins of attraction.

### Open Question 3
- Question: How does the proposed method compare to other techniques for mitigating simplicity bias like feature decorrelation or information bottleneck regularization?
- Basis in paper: [explicit] The paper mentions these alternative approaches in the introduction but does not compare them to noisy pre-training.
- Why unresolved: The experiments only compare noisy pre-training to standard training, leaving the relative effectiveness of different simplicity bias mitigation techniques unknown.
- What evidence would resolve it: Empirical comparison of noisy pre-training with feature decorrelation and information bottleneck regularization on the same datasets, measuring feature diversity and out-of-distribution generalization.

## Limitations

- The paper's central claim about shifting SGD into fundamentally different minima relies heavily on empirical observations rather than theoretical guarantees
- The specific noise corruption rate used in experiments is sometimes underspecified, which could affect reproducibility
- The paper does not systematically explore how varying noise levels affects the benefits of pre-training

## Confidence

- **High Confidence**: The empirical demonstration that noisy pre-training reduces reliance on simple features (using randomized shuffle accuracy and Gram matrix visualizations) is well-supported by the experiments across multiple datasets.
- **Medium Confidence**: The claim that noisy pre-training leads to more diverse decision boundaries is supported by the Gram matrix analysis and ensemble performance, but the mechanism for how this diversity arises is not fully explained.
- **Low Confidence**: The theoretical mechanism explaining why SGD with noisy labels converges to different minima and why these minima persist during clean-data fine-tuning is largely speculative and lacks rigorous proof.

## Next Checks

1. **Systematic Noise Level Analysis**: Reproduce the main experiments while varying the noise level in pre-training (e.g., 5%, 10%, 20%, 30%) to identify the optimal range and determine whether the effect persists across different noise intensities.

2. **Theoretical Bounds on Feature Dependence**: Derive analytical bounds on how much label noise can shift the optimization trajectory away from simple-feature minima, potentially using PAC-Bayes or information-theoretic approaches to quantify the minimum noise level needed for meaningful feature diversity.

3. **Architecture Transferability**: Test whether the noisy pre-training benefits transfer to more complex architectures (ResNets, Transformers) and different dataset types (natural images, tabular data) to establish the generality of the approach beyond the specific experimental setup.