---
ver: rpa2
title: Towards Generating Informative Textual Description for Neurons in Language
  Models
arxiv_id: '2401.16731'
source_url: https://arxiv.org/abs/2401.16731
tags:
- descriptors
- neurons
- neuron
- language
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an unsupervised framework to generate textual
  descriptors for neurons in language models. The key idea is to leverage generative
  language models to discover candidate descriptors present in a dataset, then assign
  these descriptors to neurons based on their activation patterns.
---

# Towards Generating Informative Textual Description for Neurons in Language Models

## Quick Facts
- arXiv ID: 2401.16731
- Source URL: https://arxiv.org/abs/2401.16731
- Authors: Shrayani Mondal; Rishabh Garodia; Arbaaz Qureshi; Taesung Lee; Youngja Park
- Reference count: 3
- One-line primary result: Proposed unsupervised framework generates useful data-specific descriptors for neurons with 75% precision@2 and 50% recall@2

## Executive Summary
This paper introduces an unsupervised framework for generating informative textual descriptions of individual neurons in language models. The approach leverages generative language models to discover candidate descriptors from a dataset, then assigns these descriptors to neurons based on their activation patterns. Experiments on BERT using Amazon reviews demonstrate the framework can produce useful, data-specific descriptors with minimal human involvement, while also showing high consistency across disjoint datasets from the same distribution.

## Method Summary
The framework operates by first using generative language models (FLAN-T5 XXL and Pythia) to discover candidate descriptors from input sentences in the dataset. These descriptors are then clustered to create a concise set of representative concepts. For each neuron, the method identifies the top-k percentile of sentences that maximally activate it, and assigns descriptors that appear frequently above a threshold in this exemplar set. The process is validated by running it on two disjoint subsets of the data and measuring consistency via Jaccard similarity between the descriptor sets.

## Key Results
- Achieved 75% precision@2 and 50% recall@2 in correctly tagging neurons with appropriate descriptors
- Demonstrated high consistency with Jaccard similarity of 0.95 between calibration and validation sets
- Generated useful, data-specific descriptors with minimal human involvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework uses generative language models to discover candidate descriptors from a dataset, reducing manual labeling effort.
- Mechanism: Generative LLMs are prompted with sentences from the dataset to produce descriptors, which are then clustered to form a concise set.
- Core assumption: Generative LLMs can reliably produce semantically meaningful descriptors for input sentences when given appropriate prompts.
- Evidence anchors:
  - [abstract] "leverage the potential of generative language models to discover human-interpretable descriptors present in a dataset"
  - [section] "For each di ∈ D , we ask an instruction fine-tuned LLM G... to obtain the descriptors of di."
  - [corpus] "Average neighbor FMR=0.421" (suggests moderate correlation in related works)

### Mechanism 2
- Claim: The method assigns descriptors to neurons by analyzing top-k percentile sentence activations, ensuring that only frequently occurring descriptors in highly activating sentences are assigned.
- Mechanism: After obtaining sentence-level activations, the top k% sentences for each neuron are selected as an exemplar set. Descriptors that appear frequently above a threshold t in this set are assigned to the neuron.
- Core assumption: Neurons encode patterns that are consistently present in sentences that highly activate them.
- Evidence anchors:
  - [abstract] "use an unsupervised approach to explain neurons with these descriptors"
  - [section] "we compute the descriptors Cni of neuron ni as follows... f (c) is the percentage frequency of c in CEi and t is a composition threshold"
  - [corpus] No direct evidence; moderate FMR suggests some similarity but not strong.

### Mechanism 3
- Claim: Consistency across disjoint datasets from the same distribution ensures that the descriptors are not spurious and that neurons reliably encode the same concepts.
- Mechanism: The same process is run on two disjoint subsets of the data. Jaccard similarity between descriptor sets for the same neuron is computed to measure consistency.
- Core assumption: If a neuron encodes a real concept, it should activate on similar types of sentences across different subsets of the same distribution.
- Evidence anchors:
  - [abstract] "The approach also demonstrates high consistency when tested on two disjoint datasets from the same distribution"
  - [section] "We split this dataset into two subsets of equal sizes — calibration set and validation set... This provides a 50-50 split of reviews and maintains equal distribution for each descriptor"
  - [corpus] "Average neighbor FMR=0.421" (weak but some similarity in related work)

## Foundational Learning

- Concept: Clustering of semantically similar descriptors
  - Why needed here: To reduce the redundancy caused by generative LLMs producing multiple variations of the same concept.
  - Quick check question: What algorithm is used to cluster descriptors and why is it appropriate for this task?

- Concept: Neuron activation analysis and exemplar set construction
  - Why needed here: To link the semantic content of sentences to the neurons that respond to them.
  - Quick check question: How is the top-k percentile of sentences selected and why is k=1 used in the experiments?

- Concept: Jaccard similarity for consistency evaluation
  - Why needed here: To quantify how consistently descriptors are assigned to neurons across different data subsets.
  - Quick check question: What does a high Jaccard similarity indicate about the quality of the descriptors?

## Architecture Onboarding

- Component map:
  Generative LLMs (FlanT5 XXL, Pythia) → Descriptor generation
  Clustering module → Descriptor deduplication
  BERT model → Sentence encoding and neuron activations
  Top-k selector → Exemplar set construction
  Frequency threshold filter → Descriptor assignment to neurons
  Consistency checker → Jaccard similarity between calibration and validation sets

- Critical path:
  1. Input dataset → Generative LLM prompts → Raw descriptors
  2. Raw descriptors → Clustering → Candidate descriptor set
  3. Dataset + candidate descriptors → BERT activations → Exemplar sets
  4. Exemplar sets + frequency threshold → Neuron descriptor mapping
  5. Two data subsets → Consistency evaluation via Jaccard similarity

- Design tradeoffs:
  - Using top-1% vs. top-5% for exemplar sets: More focused but risk of noise vs. more stable but potentially diluted descriptors.
  - Frequency threshold vs. top-K descriptors: Automatic thresholding adapts to descriptor prevalence but can be sensitive to threshold choice; top-K is simpler but may include less relevant descriptors.

- Failure signatures:
  - Low precision/recall in descriptor assignment: Likely issues with descriptor generation or threshold tuning.
  - Low Jaccard similarity: Descriptors are spurious or dataset-specific.
  - Clustering produces too few clusters: Descriptor generation may be too narrow or clustering parameters need tuning.

- First 3 experiments:
  1. Run descriptor generation on a small sample of sentences and manually inspect the quality and diversity of outputs.
  2. Vary k (e.g., 0.5%, 1%, 2%) and observe the impact on exemplar set size and descriptor assignment quality.
  3. Vary the frequency threshold t and measure the effect on precision@K and Jaccard similarity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the discovered descriptors generalize across different domains and languages beyond the Amazon reviews dataset used in this study?
- Basis in paper: [explicit] The paper mentions evaluating on BERT using the Amazon reviews dataset and shows high consistency when tested on two disjoint datasets from the same distribution. It also notes potential extensions to explore other datasets or a mix of datasets.
- Why unresolved: The study is limited to one specific domain (product reviews) and language (English). It's unclear how well the descriptor discovery process would work on different types of text data or non-English languages.
- What evidence would resolve it: Evaluating the framework on diverse datasets from different domains (e.g., medical, legal, news) and languages to see if the same descriptors are discovered and if they remain meaningful.

### Open Question 2
- Question: What is the impact of fine-tuning the base BERT model on the discovered neuron descriptors?
- Basis in paper: [explicit] The paper notes analyzing the effect of model fine-tuning on the generated descriptors as a potential extension of the work. It uses BERT without fine-tuning in the experiments.
- Why unresolved: Fine-tuning can significantly change a model's behavior and what information it encodes. It's unknown if fine-tuning BERT on different tasks would lead to discovering different descriptors.
- What evidence would resolve it: Fine-tuning BERT on various downstream tasks (e.g., sentiment analysis, question answering) and then applying the descriptor discovery process to see if the resulting descriptors change.

### Open Question 3
- Question: How do the discovered descriptors compare to human judgments of what concepts neurons encode?
- Basis in paper: [explicit] The paper relies on ChatGPT and manual annotation to evaluate the discovered descriptors. It notes that getting ground truth descriptors for neurons is labor-intensive.
- Why unresolved: While the paper shows high agreement between ChatGPT and manual annotations, it's unclear how well these automatically discovered descriptors align with human intuitions about what neurons represent.
- What evidence would resolve it: Conducting a large-scale human study where participants are asked to judge whether the discovered descriptors accurately describe what they believe certain neurons encode.

## Limitations

- The framework's reliance on generative language models for descriptor discovery introduces potential for hallucinations or overly generic outputs, which could propagate through the clustering and assignment pipeline.
- The method focuses on the [CLS] token's activation pattern, potentially missing neuron-specific information from other layers or attention heads.
- The quantitative evaluation uses human annotators but the inter-annotator agreement or the specific annotation protocol is not reported, making it difficult to assess the reliability of the precision and recall metrics.

## Confidence

- **High Confidence**: The overall framework design and the use of generative LLMs for descriptor discovery is methodologically sound. The consistency evaluation across disjoint datasets is a strong validation of the approach's reliability.
- **Medium Confidence**: The reported precision@2 (75%) and recall@2 (50%) are promising but the evaluation methodology's details are limited, making it difficult to fully assess the metrics' robustness. The Jaccard similarity of 0.95 suggests high consistency, but the specific conditions and potential edge cases are not explored.
- **Low Confidence**: The exact prompt templates for the generative models and the clustering parameters are not fully specified, which are critical for faithful reproduction and understanding the method's sensitivity to these choices.

## Next Checks

1. **Descriptor Quality Validation**: Manually inspect a sample of descriptors generated by the LLM for a subset of reviews to assess their semantic quality, relevance to the input sentences, and diversity. This will help identify if the generative model is producing coherent and meaningful descriptors or if it's prone to hallucinations or generic outputs.

2. **Parameter Sensitivity Analysis**: Systematically vary the top-k percentile (e.g., 0.5%, 1%, 2%) and the frequency threshold (t) to observe their impact on descriptor assignment quality (precision@K), consistency (Jaccard similarity), and the number of descriptors assigned per neuron. This will help understand the robustness of the method to these critical parameters and identify optimal settings for different datasets or neuron types.

3. **Alternative Evaluation Protocol**: Design and implement an alternative evaluation protocol, such as using a held-out set of reviews with known properties or using a different similarity metric for consistency evaluation, to cross-validate the reported precision, recall, and Jaccard similarity. This will help assess the reliability and generalizability of the evaluation metrics and ensure that the method's performance is not an artifact of the specific evaluation setup.