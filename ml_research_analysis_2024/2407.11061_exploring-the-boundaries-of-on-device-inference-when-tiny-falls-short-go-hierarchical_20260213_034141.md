---
ver: rpa2
title: 'Exploring the Boundaries of On-Device Inference: When Tiny Falls Short, Go
  Hierarchical'
arxiv_id: '2407.11061'
source_url: https://arxiv.org/abs/2407.11061
tags:
- inference
- uni00000048
- latency
- energy
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates hierarchical inference (HI) as a method
  to enhance on-device inference in resource-constrained edge ML systems. The study
  compares HI against standalone on-device inference across five devices and three
  image classification datasets, measuring accuracy, latency, and energy consumption.
---

# Exploring the Boundaries of On-Device Inference: When Tiny Falls Short, Go Hierarchical

## Quick Facts
- arXiv ID: 2407.11061
- Source URL: https://arxiv.org/abs/2407.11061
- Reference count: 40
- Primary result: Hierarchical Inference (HI) with smaller on-device models achieves up to 73% latency reduction and 77% energy savings compared to larger standalone models while meeting accuracy requirements

## Executive Summary
This paper investigates hierarchical inference (HI) as a method to enhance on-device inference in resource-constrained edge ML systems. The study compares HI against standalone on-device inference across five devices and three image classification datasets, measuring accuracy, latency, and energy consumption. Results show that HI systems with smaller on-device models can meet accuracy requirements while significantly reducing latency (up to 73%) and energy consumption (up to 77%) compared to larger on-device models. However, HI introduces a fixed overhead due to on-device inference for all samples. To address this, the paper proposes a hybrid approach combining Early Exit with HI (EE-HI), which further reduces latency by up to 59.7% and energy consumption by up to 60.4% compared to HI. The findings demonstrate that HI and EE-HI can enable more efficient ML in IoT systems, balancing accuracy, latency, and energy consumption.

## Method Summary
The study evaluates hierarchical inference (HI) and Early Exit with HI (EE-HI) systems across five edge devices (Arduino Nano, ESP32, Coral Micro, Raspberry Pi 4B, Jetson Orin Nano) using three image classification datasets (MNIST, CIFAR-10, ImageNet-1K). HI systems use binary logistic regression on softmax outputs to decide whether to accept on-device inference or offload to a remote server. EE-HI extends this by incorporating early exit branches in the on-device model, allowing inference to stop early for confident predictions. The evaluation measures accuracy, latency, and energy consumption against quality of service (QoS) requirements, comparing performance against standalone on-device and remote inference baselines.

## Key Results
- HI systems with smaller on-device models achieve up to 73% latency reduction and 77% energy savings compared to larger standalone models
- EE-HI further reduces latency by up to 59.7% and energy consumption by up to 60.4% compared to HI
- The proposed approach meets accuracy requirements (≥90% for CIFAR-10 and ImageNet-1K) while significantly improving efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical Inference (HI) reduces energy and latency by offloading only complex samples.
- Mechanism: The on-device model first processes all inputs; a binary logistic regression (LR) decision module analyzes softmax outputs to determine whether to accept the inference locally (simple samples) or offload to a remote server (complex samples).
- Core assumption: The binary LR can accurately differentiate correct from incorrect on-device inferences, minimizing false negatives.
- Evidence anchors:
  - [abstract] "HI systems with smaller on-device models can meet accuracy requirements while significantly reducing latency (up to 73%) and energy consumption (up to 77%) compared to larger on-device models."
  - [section] "For the HI decision-making, we use a binary LR algorithm for each dataset... If the image is set to be offloaded, this is transmitted via the local WiFi network..."
  - [corpus] Weak evidence; no directly comparable hierarchical offloading studies found.
- Break condition: If the binary LR's F1 score falls below ~0.8, false negatives increase, forcing offloading of many correctly classified samples, eroding latency and energy gains.

### Mechanism 2
- Claim: Using a smaller on-device model in HI can outperform a larger standalone model.
- Mechanism: The HI system achieves higher accuracy by selectively offloading only the samples that would be misclassified by the smaller model, avoiding the full latency/energy cost of running a larger model on every sample.
- Core assumption: Remote inference latency/energy is low enough to make selective offloading more efficient than running a larger model locally.
- Evidence anchors:
  - [abstract] "HI systems with smaller on-device models can meet accuracy requirements while significantly reducing latency (up to 73%) and energy consumption (up to 77%) compared to larger on-device models."
  - [section] "From (2), we infer that while the on-device model's accuracy may fall below the accuracy QoS requirement, AccHI can still meet the requirement if the remote model's accuracy is significantly higher than the QoS requirement and ηFN is sufficiently small..."
  - [corpus] No direct comparable evidence; this is an original contribution.
- Break condition: If remote offloading latency/energy is too high relative to the performance gap between small and large models, the HI system loses its advantage.

### Mechanism 3
- Claim: Early Exit with HI (EE-HI) further reduces latency and energy by allowing early termination of on-device inference.
- Mechanism: The on-device model includes early exit branches; if a sample's confidence exceeds a threshold at an intermediate layer, inference stops early and the result is accepted. Otherwise, processing continues until the final layer, after which the HI decision module determines if offloading is needed.
- Core assumption: Early exit branches can maintain accuracy while reducing average computation depth.
- Evidence anchors:
  - [abstract] "To address this, we propose a hybrid system called Early Exit with HI (EE-HI) and demonstrate that, compared to HI, EE-HI reduces the latency up to 59.7% and lowers the device's energy consumption up to 60.4%."
  - [section] "We design and train the base models with early exit branches, referred to as EE models, following the pioneering BranchyNet approach... If the confidence level (maximum softmax value) for classification exceeds the threshold, the system accepts the output and proceeds to the next sample."
  - [corpus] No direct early-exit HI studies found; evidence is original to this paper.
- Break condition: If confidence thresholds are set too low, accuracy drops; if too high, early exit rarely triggers, negating benefits.

## Foundational Learning

- Concept: TinyML model quantization (INT8)
  - Why needed here: Quantized models reduce latency and energy consumption with minimal accuracy loss, crucial for resource-constrained edge devices.
  - Quick check question: What is the typical accuracy drop when converting a FP32 model to INT8 for image classification on MCUs?

- Concept: Early exit mechanisms in DNNs
  - Why needed here: Early exits allow inference to stop before reaching the final layer for confident predictions, reducing computation and improving latency/energy efficiency.
  - Quick check question: How does the placement of early exit branches affect the trade-off between accuracy and computational savings?

- Concept: Hierarchical inference decision algorithms
  - Why needed here: The binary LR decision module must accurately predict when local inference is likely incorrect to avoid unnecessary offloading while maintaining accuracy.
  - Quick check question: What metrics (e.g., F1 score) should be used to evaluate the performance of the hierarchical inference decision algorithm?

## Architecture Onboarding

- Component map:
  On-device model -> Binary logistic regression decision module -> WiFi/Bluetooth communication module -> Remote server with larger DL model -> Power/energy measurement system (Voltech PM1000+)

- Critical path:
  1. Sample input → On-device model inference
  2. Softmax outputs → Binary LR decision
  3. If accepted: return result
  4. If rejected: offload to server
  5. Server inference → Return result

- Design tradeoffs:
  - Model size vs. accuracy vs. latency/energy
  - Early exit branch placement vs. accuracy vs. computation savings
  - Confidence thresholds vs. early termination rate vs. accuracy
  - Remote server capability vs. offloading cost vs. system performance

- Failure signatures:
  - High false negative rate in binary LR → unnecessary offloading, higher latency/energy
  - Poor early exit branch placement → minimal computation savings or accuracy drop
  - Inadequate communication bandwidth → offloading becomes bottleneck
  - Remote server unavailability → system fallback degrades to slower on-device inference

- First 3 experiments:
  1. Measure baseline latency and energy for on-device ResNet-8 and ResNet-56 on Coral Micro and Raspberry Pi with CIFAR-10.
  2. Implement binary LR decision module, evaluate F1 score on validation set, and measure impact on HI accuracy/latency/energy.
  3. Add early exit branches to ResNet-8, tune confidence thresholds, and compare EE-HI performance against baseline HI.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different HI decision algorithms (beyond binary logistic regression) compare in terms of false positive and false negative rates across diverse edge ML workloads?
- Basis in paper: [explicit] "While binary LR is an effective HI offloading decision algorithm, it produces false positives (incorrect local inference samples that are not offloaded) and false negatives (correct local inference samples that are offloaded). In our HI systems, binary LR achieved an F1 score of 0.86 for ResNet-8 on CIFAR-10 and 0.83 for ResNet-18 on ImageNet-1K."
- Why unresolved: The paper only evaluates one HI decision algorithm (binary logistic regression) and acknowledges its limitations without comparing alternatives like confidence thresholding, ensemble methods, or learned policies.
- What evidence would resolve it: Empirical comparison of multiple HI decision algorithms across varied datasets, hardware configurations, and network conditions measuring accuracy, latency, energy consumption, and classification of false positives/negatives.

### Open Question 2
- Question: What is the optimal trade-off between early exit threshold values and hierarchical inference offloading decisions in the EE-HI framework for different accuracy-latency requirements?
- Basis in paper: [explicit] "However, with appropriate threshold selection, EE models can provide significant improvements in latency and energy consumption while maintaining reasonable accuracy. Introducing more branches in the model increases flexibility in the accuracy-latency trade-off... The optimal thresholds, where the dotted lines intersect the QoS requirement limit, are provided in Table IX for EE-HI."
- Why unresolved: The paper uses brute-force threshold optimization but doesn't provide analytical frameworks or adaptive mechanisms for dynamic threshold selection based on runtime conditions or model confidence distributions.
- What evidence would resolve it: Analytical models or reinforcement learning approaches that adaptively tune early exit and HI thresholds based on input characteristics, model uncertainty, and QoS constraints.

### Open Question 3
- Question: How does device mobility and network interference affect the performance and reliability of hierarchical inference systems compared to static deployments?
- Basis in paper: [explicit] "Future work will focus on studying the impact of device mobility and network interference on offloading times and the overall efficiency of HI systems."
- Why unresolved: The current study uses stable WiFi in controlled conditions, but real-world edge deployments involve variable network quality, latency jitter, and intermittent connectivity that could significantly impact HI system performance.
- What evidence would resolve it: Measurements of HI system performance under controlled network degradation scenarios (varying packet loss, latency jitter, bandwidth constraints) and real-world mobility traces showing impact on accuracy, latency, and energy consumption.

## Limitations

- The study focuses exclusively on image classification tasks, limiting applicability to other ML domains such as NLP or time-series analysis.
- Experimental results may not generalize to other hardware platforms or newer architectures with different latency-energy trade-offs.
- The hierarchical inference approach assumes reliable network connectivity with predictable latency, which may not hold in real-world IoT deployments with intermittent connectivity.

## Confidence

**High Confidence**: The empirical demonstration that EE-HI reduces latency by up to 59.7% and energy consumption by up to 60.4% compared to HI, as this is based on direct experimental measurements across multiple devices and datasets.

**Medium Confidence**: The claim that smaller on-device models in HI systems can outperform larger standalone models, as this relies on the assumption that remote offloading latency remains consistently low enough to justify selective offloading.

**Low Confidence**: The generalizability of findings to non-image classification tasks and real-world deployment scenarios with variable network conditions, as these were not explicitly tested in the study.

## Next Checks

1. **Network Variability Test**: Implement a controlled experiment where network latency and packet loss are artificially varied to assess how HI and EE-HI performance degrades under realistic IoT connectivity conditions. Measure the break-even point where hierarchical approaches lose their advantage over larger on-device models.

2. **Cross-Domain Generalization**: Apply the hierarchical inference framework to a non-image classification task such as keyword spotting or anomaly detection in sensor data. Compare performance against the established image classification results to evaluate domain transferability.

3. **Energy-Aware Threshold Optimization**: Develop and test an adaptive threshold mechanism for early exits and HI decisions that dynamically adjusts based on current battery levels or energy harvesting availability, rather than using static thresholds optimized for average-case performance.