---
ver: rpa2
title: 'Zero-to-Strong Generalization: Eliciting Strong Capabilities of Large Language
  Models Iteratively without Gold Labels'
arxiv_id: '2409.12425'
source_url: https://arxiv.org/abs/2409.12425
tags:
- tasks
- labels
- classification
- more
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces zero-to-strong generalization, a framework
  that iteratively elicits strong capabilities of large language models without gold
  labels or weak supervisors. The method works by prompting LLMs to annotate unlabeled
  data, filtering high-confidence labels, and using these as demonstrations for subsequent
  iterations.
---

# Zero-to-Strong Generalization: Eliciting Strong Capabilities of Large Language Models Iteratively without Gold Labels

## Quick Facts
- **arXiv ID**: 2409.12425
- **Source URL**: https://arxiv.org/abs/2409.12425
- **Reference count**: 31
- **Primary result**: Zero-to-strong generalization framework elicits strong LLM capabilities without gold labels through iterative demonstration refinement

## Executive Summary
This paper introduces zero-to-strong generalization, a framework that iteratively elicits strong capabilities of large language models without requiring gold labels or weak supervisors. The method works by prompting LLMs to annotate unlabeled data, filtering high-confidence labels, and using these as demonstrations for subsequent iterations. Through extensive experiments on 17 classification tasks, 2 extreme-label classification tasks, and 2 reasoning tasks, the framework consistently outperforms or matches few-shot learning with gold labels, especially for stronger models. Performance improves with each iteration as more confident and accurate demonstrations are selected.

## Method Summary
The framework iteratively prompts LLMs to annotate unlabeled data with initial random or invalid demonstrations, then filters high-confidence responses to select the best demonstrations for the next iteration. This creates a positive feedback loop where better demonstrations lead to better predictions. The method works for both in-context learning and fine-tuning with LoRA, and shows effectiveness across various model sizes. Confidence scores are used to select top-k demonstrations, with the assumption that higher confidence correlates with better accuracy.

## Key Results
- Iterative demonstration refinement consistently outperforms few-shot learning with gold labels
- Performance improves progressively with each iteration
- Stronger models show greater benefit from the iterative process
- Framework works for both in-context learning and fine-tuning with LoRA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative selection of high-confidence predictions progressively refines demonstration quality
- Mechanism: The framework iteratively prompts LLMs to annotate unlabeled data, filters responses by confidence, and uses top-k confident samples as demonstrations for the next iteration
- Core assumption: Confidence scores correlate with prediction accuracy
- Evidence anchors:
  - [abstract]: "We iteratively prompt LLMs to annotate unlabeled data and retain high-quality labels by filtering"
  - [section]: "We then select a new set of demonstrations based on confidence levels and prompt the LLMs again, repeating this process iteratively"
  - [corpus]: Weak correlation observed in corpus neighbors for uncertainty quantification methods
- Break condition: If confidence no longer correlates with accuracy, the iterative improvement stalls

### Mechanism 2
- Claim: Random or invalid initial demonstrations are sufficient to bootstrap the iterative process
- Mechanism: Starting with random labels or invalid reasoning paths, the LLM can still generate responses that, when filtered by confidence, yield useful demonstrations for subsequent iterations
- Core assumption: LLMs have sufficient pre-trained knowledge to bootstrap from poor initial demonstrations
- Evidence anchors:
  - [abstract]: "Inspired by this, we initially prompt LLMs with random or invalid demonstrations to label the data"
  - [section]: "Previous works have demonstrated that random labels... or invalid reasoning paths... can also yield good performance"
  - [corpus]: Missing specific evidence for bootstrap effectiveness from random demonstrations
- Break condition: If initial demonstrations are too far from correct format or semantic meaning

### Mechanism 3
- Claim: The method generalizes across model sizes and fine-tuning settings
- Mechanism: The iterative demonstration refinement process works for both in-context learning and fine-tuning, and performance improves with stronger models
- Core assumption: The iterative refinement process is model-agnostic
- Evidence anchors:
  - [abstract]: "Our analysis indicates that this paradigm is effective for both in-context learning and fine-tuning, and for various model sizes"
  - [section]: "We further investigate the impact of incorporating fine-tuning with LoRA... fine-tuning also improves progressively"
  - [corpus]: Limited evidence for generalization across model sizes in corpus neighbors
- Break condition: If model architecture fundamentally differs in how it handles demonstrations

## Foundational Learning

- **Concept**: Self-consistency in reasoning tasks
  - Why needed here: Used to select the most consistent final answer among multiple reasoning paths
  - Quick check question: How do you calculate confidence for reasoning tasks in this framework?

- **Concept**: Confidence-based filtering
  - Why needed here: Core mechanism for selecting demonstrations across iterations
  - Quick check question: What confidence metric is used for classification vs reasoning tasks?

- **Concept**: In-context learning sensitivity
  - Why needed here: Understanding how demonstration quality affects performance is key to the iterative improvement
  - Quick check question: Why might uniform initialization underperform random initialization?

## Architecture Onboarding

- **Component map**: LLM prompt generation → response generation → confidence calculation → top-k filtering → demonstration update → iteration
- **Critical path**: The filtering and selection of demonstrations based on confidence is the core mechanism that drives improvement
- **Design tradeoffs**: Higher k values provide more diverse demonstrations but may include lower quality samples; confidence thresholds balance precision vs recall
- **Failure signatures**: Performance plateaus or degrades after certain iterations; confidence scores stop correlating with accuracy
- **First 3 experiments**:
  1. Implement the iterative framework on a simple classification task (e.g., SST-2) with 2-3 iterations
  2. Compare random initialization vs uniform initialization on an extreme-label classification task
  3. Test the framework with a smaller model (e.g., 7B) vs a larger model (e.g., 70B) on the same reasoning task

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several emerge from the work:

- The theoretical upper limit of performance improvement achievable through zero-to-strong generalization before diminishing returns set in
- How the framework performs on tasks with continuous or multi-dimensional output spaces compared to discrete classification or reasoning tasks
- The impact of dataset size and label distribution on the effectiveness of zero-to-strong generalization
- How zero-to-strong generalization compares to other semi-supervised learning methods like self-training, co-training, or consistency regularization
- The relationship between task complexity and the number of iterations required for convergence

## Limitations
- Framework restricted to tasks with single definitive correct answers, limiting applicability to open-ended tasks
- Performance may plateau or fluctuate after certain iterations, requiring careful monitoring
- Limited evidence for bootstrap effectiveness from random demonstrations

## Confidence

**High Confidence Claims** (supported by multiple experiments):
- Iterative refinement improves performance over static demonstrations
- Stronger models benefit more from the iterative process
- Fine-tuning with LoRA shows progressive improvement

**Medium Confidence Claims** (supported but with some gaps):
- Random initialization matches or exceeds uniform initialization
- Framework works across different model sizes
- Performance gains are consistent across task types

**Low Confidence Claims** (limited or no direct evidence):
- Framework generalizes to tasks beyond classification and reasoning
- Confidence scores reliably indicate demonstration quality for all task types
- The method works with significantly smaller datasets than tested

## Next Checks
1. **Confidence Calibration Test**: Systematically evaluate how confidence scores correlate with actual accuracy across different task types and model sizes. Test multiple confidence metrics (probability, self-consistency scores, entropy) to determine which provides the most reliable filtering signal.

2. **Bootstrap Robustness Analysis**: Conduct ablation studies varying the quality of initial demonstrations from random to near-correct, measuring convergence speed and final performance. This would quantify how robust the framework is to poor starting conditions.

3. **Cross-Task Generalization**: Apply the framework to at least two additional task types (e.g., text generation with evaluation metrics, structured prediction) to validate the claimed broad applicability beyond the tested classification and reasoning domains.