---
ver: rpa2
title: Dataset Representativeness and Downstream Task Fairness
arxiv_id: '2407.00170'
source_url: https://arxiv.org/abs/2407.00170
tags:
- dataset
- sampling
- group
- each
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines the complex relationship between dataset representativeness
  and downstream algorithmic fairness. The authors propose a Bayesian multi-armed
  bandit framework for representative sampling from multiple data sources and theoretically
  and empirically analyze how representation affects classifier fairness.
---

# Dataset Representativeness and Downstream Task Fairness

## Quick Facts
- arXiv ID: 2407.00170
- Source URL: https://arxiv.org/abs/2407.00170
- Authors: Victor Borza; Andrew Estornell; Chien-Ju Ho; Bradley Malin; Yevgeniy Vorobeychik
- Reference count: 40
- Primary result: Improving dataset representativeness does not necessarily improve downstream classifier fairness, and increasing model complexity can reduce unfairness without sacrificing accuracy.

## Executive Summary
This paper investigates the complex relationship between dataset representativeness and algorithmic fairness in downstream classification tasks. The authors develop a Bayesian multi-armed bandit framework for representative sampling from multiple data sources and theoretically analyze how representation affects classifier fairness. Through extensive experiments on six real-world datasets, they demonstrate that improving dataset representativeness can actually increase unfairness when groups differ in their inherent difficulty to learn. The paper shows that increasing model complexity can be more effective at reducing unfairness than improving representativeness alone, and that multi-site sampling methodology fundamentally alters the representativeness-fairness relationship compared to direct sampling.

## Method Summary
The authors propose a Bayesian multi-armed bandit framework (PBRS and D-PBRS algorithms) for representative sampling from multiple data sources, treating each source as an "arm" with unknown reward distribution. They theoretically analyze how representation affects classifier fairness by examining the relationship between noise levels in different groups and fairness metrics (AUC parity, TPR parity, TNR parity). The method is validated through experiments on six datasets (Law School, Lending Club, Intensive Care, Texas Salary, Adult Income, Community Crime) using gradient boosted decision trees and logistic regression models, comparing representativeness-fairness relationships under different sampling methodologies.

## Key Results
- Improving dataset representativeness does not necessarily improve downstream classifier fairness; in fact, it can frequently result in higher rates of unfairness
- When groups differ in inherent learning difficulty, over-sampling the easier-to-learn group can improve overall accuracy while increasing unfairness to the harder group
- Increasing model complexity can reduce unfairness without sacrificing accuracy, addressing the representativeness-fairness tension more effectively than improving representation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Improving dataset representativeness does not necessarily improve downstream classifier fairness.
- Mechanism: When groups differ in inherent learning difficulty, over-sampling the easier-to-learn group can improve overall accuracy while increasing unfairness to the harder group.
- Core assumption: The joint distribution P(y|x) is more difficult to learn for one group than another, creating a natural tension between representation and fairness.
- Evidence anchors:
  - [abstract] "empirically we observe that training datasets with better representativeness can frequently result in classifiers with higher rates of unfairness"
  - [section] "Theorem 2" provides theoretical foundation showing unfairness depends on relative noise levels between groups
  - [corpus] Weak evidence - related works focus on bias mitigation rather than the representativeness-fairness tension
- Break condition: When group learning difficulties are similar (σ0 ≈ σ1), the representativeness-fairness tension diminishes.

### Mechanism 2
- Claim: Multi-site sampling methodology fundamentally alters the representativeness-fairness relationship compared to direct sampling.
- Mechanism: Site-specific feature, sensitive feature, and label distributions influence classifier fairness independently of overall dataset representativeness.
- Core assumption: The way data is collected (multi-site vs. direct sampling) creates different statistical relationships between features and labels across groups.
- Evidence anchors:
  - [section] "We find that downstream classifier performance differs significantly when cohorts are selected in a multi-site procedure to achieve a certain subgroup proportion compared to stratified random sampling"
  - [corpus] Weak evidence - related works don't address multi-site sampling effects on fairness
- Break condition: When site distributions are homogeneous or when sampling perfectly matches target population distribution.

### Mechanism 3
- Claim: Increasing model complexity can reduce unfairness without sacrificing accuracy, addressing the representativeness-fairness tension.
- Mechanism: More complex models can capture difficult-to-learn relationships between features and labels, reducing performance disparities between groups.
- Core assumption: Unfairness stems partly from model inability to capture complex data relationships, not just from representation issues.
- Evidence anchors:
  - [section] "We show how an alternate approach, increasing model complexity, can help close this performance gap"
  - [section] "Darker green lines indicate higher maximum tree depth for the GBC (higher complexity) and the x-axis shows number of estimation steps, with more indicating higher complexity"
  - [corpus] Weak evidence - related works focus on bias mitigation rather than model complexity
- Break condition: When the Bayes-optimal classifier is already algorithmically unfair, further complexity increases cannot improve fairness.

## Foundational Learning

- Concept: Multi-armed bandit problem with convex loss
  - Why needed here: The sampling framework treats each data source as an "arm" with unknown reward distribution, requiring optimization over T timesteps
  - Quick check question: If you have 5 sites and need to collect 1000 samples, how many timesteps would you need if you sample 40 per timestep?

- Concept: Group fairness metrics (AUC parity, TPR parity, TNR parity)
  - Why needed here: The paper evaluates fairness through multiple statistical measures to capture different aspects of algorithmic bias
  - Quick check question: If Group A has AUC 0.85 and Group B has AUC 0.75, what is the AUC parity gap?

- Concept: Bayesian updating with conjugate priors
  - Why needed here: The algorithm maintains estimates of group demographics at each site and updates them as samples are collected
  - Quick check question: If you observe 3 positive cases out of 10 samples from a site, what would be the posterior mean under a uniform prior?

## Architecture Onboarding

- Component map: Data collection module → Sampling algorithm (PBRS/D-PBRS) → Dataset construction → Classifier training → Fairness evaluation
- Critical path: Sampling algorithm selection → Site sampling → Dataset construction → Model training → Fairness assessment
- Design tradeoffs: PBRS (single site per timestep) vs D-PBRS (multiple sites per timestep) - single site allows more targeted sampling while multiple sites increases efficiency
- Failure signatures: 
  - Algorithm converges to non-representative dataset despite theoretical guarantees
  - Fairness improvements don't materialize despite increased model complexity
  - Multi-site sampling produces worse fairness than direct sampling for same target proportions
- First 3 experiments:
  1. Implement PBRS on a simple synthetic dataset with known group difficulty differences
  2. Compare PBRS vs random sampling on Law School dataset for a single sensitive feature
  3. Test effect of model complexity on fairness using Adult Income dataset with varying tree depths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the relationship between representativeness and fairness change when using different distance metrics M beyond ℓ2-norm?
- Basis in paper: [explicit] The authors use ℓ2-norm to measure representativeness but acknowledge other convex distance measures could be used
- Why unresolved: The paper only evaluates one distance metric for representativeness, leaving the question of whether results generalize to other metrics
- What evidence would resolve it: Experiments showing representativeness-fairness relationships under various distance metrics (ℓ1, ℓ∞, KL-divergence, etc.)

### Open Question 2
- Question: Under what conditions does increasing model complexity fail to improve fairness, and what alternative interventions might work in those cases?
- Basis in paper: [inferred] The authors mention that increasing model complexity sometimes fails to improve fairness and reference [14] showing that if a Bayesian optimal classifier is unfair, further fairness cannot be enforced without performance loss
- Why unresolved: The paper demonstrates when complexity helps but doesn't systematically identify when it fails or what alternatives exist
- What evidence would resolve it: A framework identifying conditions under which complexity interventions fail and testing alternative fairness interventions (feature engineering, post-processing, etc.)

### Open Question 3
- Question: How does the representativeness-fairness relationship differ when sampling from more complex site distributions with correlated sensitive features?
- Basis in paper: [inferred] The authors assume binary or uncorrelated sensitive features and don't explore the impact of feature correlations on representativeness-fairness trade-offs
- Why unresolved: The paper's theoretical results and experiments assume simple feature structures, but real-world datasets often have correlated sensitive attributes
- What evidence would resolve it: Experiments with datasets having correlated sensitive features showing how this affects the representativeness-fairness relationship

## Limitations
- The paper focuses primarily on binary classification tasks and sensitive attributes, with limited exploration of multi-class scenarios or intersectional fairness concerns
- The assumption that group learning difficulty differences drive the representativeness-fairness tension may not capture all real-world complexity
- The paper does not extensively validate findings across different classifier architectures beyond GBDTs and logistic regression

## Confidence
Our confidence in the main findings is **Medium**.

Key uncertainties include the exact conditions under which the representativeness-fairness tension manifests versus when it can be resolved through better sampling, and the degree to which multi-site sampling methodology contributes versus the inherent difficulty differences between groups.

## Next Checks
1. Test the representativeness-fairness relationship on multi-class classification problems to assess generalizability beyond binary tasks.

2. Evaluate intersectional fairness (multiple sensitive attributes simultaneously) to determine if the observed tension persists when considering combined demographic groups.

3. Compare the PBRS sampling methodology against active learning approaches to isolate whether the representativeness-fairness tension stems from sampling strategy or inherent learning difficulty differences.