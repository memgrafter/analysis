---
ver: rpa2
title: Hierarchical VAE with a Diffusion-based VampPrior
arxiv_id: '2412.01373'
source_url: https://arxiv.org/abs/2412.01373
tags:
- prior
- latent
- pseudoinputs
- variational
- vampprior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hierarchical VAE model with a diffusion-based
  VampPrior, called DVP-VAE, to address the challenge of training deep hierarchical
  VAEs. The authors introduce an amortized VampPrior that scales efficiently to models
  with many stochastic layers by conditioning the prior on a pseudoinput, which is
  a transformed version of the input data.
---

# Hierarchical VAE with a Diffusion-based VampPrior

## Quick Facts
- **arXiv ID**: 2412.01373
- **Source URL**: https://arxiv.org/abs/2412.01373
- **Reference count**: 25
- **Primary result**: Achieves 2.73 BPD on CIFAR10 with 20M parameters, outperforming models with more parameters

## Executive Summary
This paper proposes DVP-VAE, a hierarchical VAE model with a diffusion-based VampPrior that addresses training challenges in deep hierarchical VAEs. The key innovation is an amortized VampPrior that scales efficiently to many stochastic layers by conditioning the prior on a pseudoinput - a transformed version of the input data obtained through a non-trainable DCT-based transformation. A diffusion model is used as the prior over these pseudoinputs, enabling efficient sampling while maintaining scalability. The model achieves state-of-the-art performance on MNIST, OMNIGLOT, and CIFAR10 datasets with fewer parameters than competing approaches.

## Method Summary
DVP-VAE builds on the Ladder VAE architecture by introducing DCT-based pseudoinputs and a diffusion model prior. The pseudoinput is created by applying a non-trainable DCT transformation to the input data, then normalizing it. This pseudoinput is used to condition the prior distribution in the TopDown path of the Ladder VAE. A diffusion model serves as the prior over these pseudoinputs, providing a flexible and scalable approach to generating infinitely many pseudoinputs. The model also employs latent aggregation, where the conditional likelihood is explicitly conditioned on the average of all sampled latent variables, ensuring all layers contribute to reconstruction. The model is trained using the standard ELBO objective with these architectural modifications.

## Key Results
- Achieves 2.73 BPD on CIFAR10 with 20M parameters, outperforming models with more parameters
- Demonstrates improved training stability compared to standard hierarchical VAEs
- Shows better latent space utilization as measured by active units
- Achieves state-of-the-art performance on MNIST, OMNIGLOT, and CIFAR10 benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Amortizing the VampPrior distribution over pseudoinputs removes the need to encode pseudoinputs at each training step, enabling scaling to many stochastic layers.
- Mechanism: The pseudoinput distribution r(u|x) is defined as N(u|f(x), σ²I) with a non-trainable DCT transformation f. This allows direct conditioning of the prior on u without encoding overhead.
- Core assumption: A non-trainable, fixed linear transformation can provide a meaningful, lower-dimensional representation of the input that preserves sufficient information for the prior.
- Evidence anchors:
  - [abstract]: "We propose utilizing a non-trainable linear transformation like Discrete Cosine Transform (DCT) to obtain pseudoinputs."
  - [section 3.1]: "We propose to amortize the distribution of pseudoinputs in VampPrior and use them to directly condition the prior distribution."
  - [corpus]: No direct evidence in corpus about amortization scaling benefits.
- Break condition: If the DCT transformation fails to preserve enough information, the prior conditioning will be ineffective, leading to poor model performance.

### Mechanism 2
- Claim: Using a diffusion model as the prior over pseudoinputs enables flexible, unconditional sampling while maintaining scalability.
- Mechanism: The diffusion model pγ(u) provides a tractable approximation to the optimal prior over pseudoinputs, allowing generation of infinitely many pseudoinputs without fixed learnable parameters.
- Core assumption: A diffusion model can effectively approximate the aggregated posterior over pseudoinputs, providing both flexibility and scalability.
- Evidence anchors:
  - [abstract]: "A diffusion model is used as a prior over the pseudoinputs, allowing for efficient sampling."
  - [section 3.4]: "Following (Vahdat et al., 2021; Wehenkel & Louppe, 2021), we propose to use a diffusion-based generative model (Ho et al., 2020) as the prior and the approximation of r(u)."
  - [corpus]: No direct evidence in corpus about diffusion models for pseudoinputs.
- Break condition: If the diffusion model fails to approximate the aggregated posterior well, the quality of generated samples will degrade significantly.

### Mechanism 3
- Claim: Latent aggregation in the conditional likelihood forces all latent variables to contribute to reconstruction, improving latent space utilization.
- Mechanism: The conditional likelihood pθ(x|z₁:L) is explicitly conditioned on the average of all sampled latent variables, rather than just the bottom-up features from the TopDown path.
- Core assumption: Explicit dependency on all latent variables prevents some layers from being ignored during training, leading to more balanced utilization.
- Evidence anchors:
  - [section 4.3]: "We propose to enforce a strong connection between the conditional likelihood and all latent variables by explicitly conditioning on all of the sampled latent variables."
  - [section 6.3]: "Our idea bears some similarities with Skip-VAE (Dieng et al., 2019). Skip-VAE proposes to add a latent variable to each layer of the neural network parameterizing decoder of the VAE with a single stochastic layer."
  - [corpus]: No direct evidence in corpus about latent aggregation benefits.
- Break condition: If the aggregation operation dilutes important information from individual latent variables, it may hurt reconstruction quality.

## Foundational Learning

- **Concept**: Variational Autoencoders (VAEs) and Evidence Lower Bound (ELBO)
  - Why needed here: DVP-VAE is built on the VAE framework and optimizes the ELBO objective.
  - Quick check question: What is the relationship between the ELBO and the marginal log-likelihood in VAEs?

- **Concept**: Discrete Cosine Transform (DCT) and frequency domain representation
  - Why needed here: DCT is used as the non-trainable transformation to create pseudoinputs.
  - Quick check question: How does applying DCT to an image affect its spatial and frequency domain representations?

- **Concept**: Diffusion models and score matching
  - Why needed here: A diffusion model is used as the prior over pseudoinputs in DVP-VAE.
  - Quick check question: What is the key idea behind using a diffusion process for generative modeling?

## Architecture Onboarding

- **Component map**: Input → Bottom-up features → Pseudoinput transformation → TopDown posterior/prior computation → Latent aggregation → Conditional likelihood

- **Critical path**: Input → Bottom-up path (ResNet blocks) → Pseudoinput block (DCT transformation + normalization) → TopDown path (posterior/prior computation conditioned on pseudoinputs) → Latent aggregation (average of all latents) → Conditional likelihood

- **Design tradeoffs**:
  - Fixed DCT vs learnable pseudoinput transformation: Fixed DCT provides consistency and stability but may not be optimal for all datasets
  - Diffusion model complexity: More complex diffusion models may improve prior quality but increase computational cost
  - Latent aggregation vs deterministic features: Aggregation ensures all latents contribute but may reduce representational power of individual layers

- **Failure signatures**:
  - Poor sample quality: Indicates issues with pseudoinput prior or TopDown architecture
  - Training instability: May result from gradient issues in deep hierarchical structure
  - Low active units: Suggests some latent variables are being ignored in the reconstruction process

- **First 3 experiments**:
  1. Verify DCT transformation preserves essential information by reconstructing inputs from DCT pseudoinputs
  2. Test diffusion model prior quality by generating and evaluating pseudoinput samples
  3. Validate latent aggregation by comparing active units with and without aggregation in a small-scale model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed DCT-based pseudoinput transformation compare to other fixed linear transformations like PCA or learned linear projections in terms of model performance and computational efficiency?
- Basis in paper: [explicit] The paper proposes using DCT for pseudoinput transformation and briefly mentions downsampling as an alternative, but doesn't extensively compare with other fixed linear transformations.
- Why unresolved: The paper focuses on DCT and downsampling, leaving other potential fixed linear transformations unexplored. A comprehensive comparison would require training models with different transformations under identical conditions.
- What evidence would resolve it: Training multiple DVP-VAE models with different fixed linear transformations (DCT, PCA, learned linear, downsampling) on the same datasets and comparing their performance metrics (NLL, BPD) and computational requirements would provide a clear answer.

### Open Question 2
- Question: What is the optimal number of pseudoinputs (K) for different dataset complexities and model architectures, and how does it affect the trade-off between performance and computational cost?
- Basis in paper: [inferred] The paper mentions that K is a hyperparameter but doesn't provide a systematic study of its optimal value across different datasets or model configurations.
- Why unresolved: The paper uses a fixed number of pseudoinputs without exploring how this choice impacts performance across different scenarios. Finding the optimal K would require extensive hyperparameter tuning.
- What evidence would resolve it: Conducting experiments varying K across different datasets (MNIST, OMNIGLOT, CIFAR10) and model sizes, then analyzing the performance vs. computational cost trade-off, would provide insights into optimal K values.

### Open Question 3
- Question: How does the diffusion-based prior over pseudoinputs compare to other potential priors (e.g., normalizing flows, autoregressive models) in terms of sample quality and generation speed?
- Basis in paper: [explicit] The paper proposes using a diffusion model as the prior over pseudoinputs but acknowledges that diffusion models are slow at sampling.
- Why unresolved: While the paper demonstrates the effectiveness of diffusion priors, it doesn't explore alternative prior choices or their impact on sample quality and generation speed.
- What evidence would resolve it: Training DVP-VAE models with different priors over pseudoinputs (diffusion, normalizing flows, autoregressive) and comparing their sample quality (FID, Inception Score) and generation speed would provide a comprehensive answer.

### Open Question 4
- Question: How does the proposed latent aggregation technique affect the interpretability and disentanglement of the latent space in hierarchical VAEs?
- Basis in paper: [explicit] The paper introduces latent aggregation to improve latent space utilization but doesn't investigate its effects on latent space interpretability or disentanglement.
- Why unresolved: The paper focuses on quantitative metrics like active units but doesn't explore the qualitative aspects of the learned latent representations.
- What evidence would resolve it: Conducting experiments to measure latent space disentanglement (e.g., using metrics like SAP, MIG) and performing qualitative analysis of latent traversals with and without latent aggregation would provide insights into its impact on latent space interpretability.

## Limitations
- Limited empirical validation beyond three benchmark datasets (MNIST, OMNIGLOT, CIFAR10)
- Diffusion model sampling speed remains a computational bottleneck despite improved scalability
- No systematic ablation studies isolating the contributions of individual components

## Confidence

- **High confidence**: The architectural framework and implementation details are clearly described and reproducible
- **Medium confidence**: The claimed scalability benefits of DCT-based pseudoinputs are demonstrated but not thoroughly isolated through ablation studies
- **Low confidence**: Generalization of results beyond the three tested datasets, given the limited scope of empirical validation

## Next Checks

1. Conduct ablation studies to isolate the individual contributions of DCT-based pseudoinputs, diffusion prior, and latent aggregation to overall performance

2. Test the model on additional diverse datasets (e.g., CelebA, LSUN) to evaluate generalization beyond the three benchmark datasets

3. Perform quantitative analysis of the diffusion model prior quality by comparing generated pseudoinputs to empirical aggregated posteriors