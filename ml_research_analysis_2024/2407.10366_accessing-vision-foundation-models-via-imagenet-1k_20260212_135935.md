---
ver: rpa2
title: Accessing Vision Foundation Models via ImageNet-1K
arxiv_id: '2407.10366'
source_url: https://arxiv.org/abs/2407.10366
tags:
- proteus
- training
- dataset
- classification
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of compressing large vision foundation
  models into smaller equivalents using limited data (ImageNet-1K), as original training
  data is often inaccessible and computationally expensive. Proteus introduces a knowledge
  distillation framework that mitigates dataset bias by avoiding one-hot labels and
  projection heads, instead transferring knowledge at token, patch, and feature levels.
---

# Accessing Vision Foundation Models via ImageNet-1K

## Quick Facts
- arXiv ID: 2407.10366
- Source URL: https://arxiv.org/abs/2407.10366
- Authors: Yitian Zhang; Xu Ma; Yue Bai; Huan Wang; Yun Fu
- Reference count: 18
- Key outcome: Proteus distills vision foundation models to ImageNet-1K scale, matching DINOv2-L/14 performance across 19 benchmarks while using only 1.2M images

## Executive Summary
This paper addresses the challenge of compressing large vision foundation models into smaller equivalents using limited data (ImageNet-1K), as original training data is often inaccessible and computationally expensive. Proteus introduces a knowledge distillation framework that mitigates dataset bias by avoiding one-hot labels and projection heads, instead transferring knowledge at token, patch, and feature levels. When distilling from DINOv2-g/14, Proteus-L/14 matches DINOv2-L/14 performance across 19 benchmarks and outperforms CLIP, OpenCLIP, and SynCLR using only 1.2M images instead of 142M+. It also scales down to ViT-Tiny, surpassing DeiT significantly, and generalizes to other foundation models like SynCLR and CLIP.

## Method Summary
Proteus introduces a three-level knowledge distillation framework that transfers knowledge from large vision foundation models to smaller equivalents using only ImageNet-1K. The method removes conventional distillation components (one-hot labels and projection heads) that introduce dataset bias, instead combining token-level, patch-level, and feature-level learning objectives. The token-level objective aligns classification tokens, the patch-level objective implements masked image modeling, and the feature-level objective preserves intermediate representations. This approach enables training foundation model equivalents at ImageNet-level costs while maintaining strong generalization across downstream tasks.

## Key Results
- Proteus-L/14 matches DINOv2-L/14 performance across 19 benchmarks using only 1.2M images
- Outperforms CLIP, OpenCLIP, and SynCLR on ImageNet linear evaluation
- Scales down to ViT-Tiny, surpassing DeiT by 1.4% while matching performance of much larger models
- Generalizes to other foundation models including SynCLR and CLIP with consistent improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Removing one-hot labels and projection heads from conventional knowledge distillation mitigates dataset bias and enables better generalization from a small proxy dataset (ImageNet-1K) to unseen classes.
- **Mechanism**: One-hot labels force the student to memorize exact class patterns present in ImageNet-1K, which may not align with the distribution of the original large-scale datasets used to train the teacher foundation models. Projection heads map features to a fixed dimensionality (e.g., 1000 for ImageNet-1K) that may not match the dimensionality needed for downstream tasks. By discarding both and distilling directly on intermediate features, the student learns more general-purpose representations that are not tied to the specific class structure of ImageNet-1K.
- **Core assumption**: The teacher foundation model's intermediate features contain generalizable visual knowledge that is not dependent on the original training dataset's class distribution.
- **Evidence anchors**:
  - [abstract] "we remove the designs from conventional knowledge distillation settings that result in dataset bias"
  - [section 2.1] "we argue that this setup will hinder the knowledge transfer for two reasons: (1) The Cross-Entropy (CE) loss, which leverages the information of one-hot labels, can lead to dataset bias as the model tends to memorize the training images and classes. This memorization makes it challenging for the model to generalize to unseen classes during downstream evaluation. (2) The generation of class logits p implicitly introduces dataset bias because the intermediate features are projected onto a pre-defined dimensionality, such as 1000 for ImageNet-1K, which may be discarded during downstream evaluation."

### Mechanism 2
- **Claim**: Combining token-level, patch-level, and feature-level learning objectives maximizes knowledge transfer and ensures the student model performs well on both high-level understanding (classification) and dense prediction tasks (segmentation, depth estimation).
- **Mechanism**: Different downstream tasks require different levels of feature abstraction. Token-level distillation aligns the classification token, which is crucial for high-level understanding tasks. Patch-level distillation, inspired by masked image modeling, helps the student recover masked regions and learn spatial relationships. Feature-level distillation ensures the overall feature representation is preserved. By combining these three objectives, the student model learns a comprehensive representation that is useful across a wide range of tasks.
- **Core assumption**: The teacher foundation model's knowledge is distributed across different levels of abstraction (token, patch, and feature), and each level contributes uniquely to different types of downstream tasks.
- **Evidence anchors**:
  - [section 2.2] "we construct the proxy task by combining the token-level, patch-level, and feature-level learning objectives to learn the general-purpose visual representations, ensuring the performance of Proteus across various tasks."
  - [section 3.2] "Although hint distillation already achieves very good results on classification tasks, it is not enough to fully transfer the knowledge of foundation models as the performance gap on semantic segmentation is quite obvious (44.0% versus 51.0%). The explanation is that classification tasks only utilize the classification token which has been distilled from the teacher, whereas semantic segmentation requires the whole feature for dense prediction."

### Mechanism 3
- **Claim**: Using ImageNet-1K as a proxy dataset, despite being much smaller and potentially having a different distribution than the original training data, is sufficient to access the knowledge of vision foundation models when combined with the proposed distillation framework.
- **Mechanism**: ImageNet-1K, while smaller, still contains a diverse set of visual concepts that can serve as a proxy for the original large-scale datasets. The proposed distillation framework (removing dataset bias, multi-level objectives) compensates for the distribution shift and limited data by focusing on generalizable visual representations rather than memorizing specific class patterns. This allows the student model to learn effectively from the teacher even when trained on a smaller, potentially different dataset.
- **Core assumption**: The visual knowledge contained in ImageNet-1K is sufficiently representative of the broader visual world to enable effective knowledge transfer from the teacher foundation model, especially when combined with techniques to mitigate dataset bias.
- **Evidence anchors**:
  - [abstract] "Proteus is trained at ImageNet-level costs with surprising ability, facilitating the accessibility of training foundation models for the broader research community."
  - [section 3.5] "Dataset Diversity. Shown in Tab. 10, ImageNet-Merge brings almost 1 % average improvement on fine-grained classification with merely 0.2M additional data, which suggests the importance of a diverse dataset. We further consider the extreme case where all the source information comes from a single image and it surprisingly delivers very decent performance."

## Foundational Learning

- **Concept**: Knowledge Distillation
  - **Why needed here**: Knowledge distillation is the core technique used to transfer knowledge from the large vision foundation models (teachers) to smaller models (students) without requiring access to the original large-scale training data.
  - **Quick check question**: What is the difference between hard logits distillation and soft logits distillation, and why might soft logits be preferred in this context?

- **Concept**: Dataset Bias
  - **Why needed here**: Understanding dataset bias is crucial because it explains why directly using ImageNet-1K with conventional distillation techniques may not effectively transfer the knowledge of foundation models trained on much larger, potentially different datasets.
  - **Quick check question**: How does the use of one-hot labels in cross-entropy loss contribute to dataset bias?

- **Concept**: Multi-Task Learning
  - **Why needed here**: The combination of token-level, patch-level, and feature-level objectives in Proteus is an example of multi-task learning, where the model is trained to perform well on multiple related tasks simultaneously.
  - **Quick check question**: Why is it beneficial to combine multiple learning objectives (token, patch, feature) rather than focusing on just one?

## Architecture Onboarding

- **Component map**: Teacher -> Student -> Projection Heads -> Loss Functions -> Student Update
- **Critical path**:
  1. Load teacher and student models
  2. Initialize projection heads
  3. For each batch:
     - Forward pass through student
     - Forward pass through teacher (frozen)
     - Compute token-level, patch-level, and feature-level losses
     - Backpropagate combined loss to update student
  4. Linear evaluation on downstream tasks
- **Design tradeoffs**:
  - Using ImageNet-1K vs. original large-scale datasets: Reduced computational cost vs. potential distribution shift
  - Removing one-hot labels and projection heads: Reduced dataset bias vs. potential loss of task-specific information
  - Multi-level objectives: Improved generalization vs. increased computational complexity
- **Failure signatures**:
  - Poor performance on downstream tasks: Could indicate insufficient knowledge transfer or mismatch between teacher and student architectures
  - Overfitting to ImageNet-1K: Could indicate that dataset bias mitigation techniques are not effective enough
  - Training instability: Could indicate conflicting gradients from the multi-level objectives
- **First 3 experiments**:
  1. **Ablation on dataset bias mitigation**: Train a student model with and without removing one-hot labels and projection heads, evaluate on ImageNet-1K and fine-grained classification datasets to quantify the impact of dataset bias.
  2. **Ablation on learning objectives**: Train student models with only token-level, only patch-level, only feature-level, and all three objectives combined, evaluate on classification and dense prediction tasks to determine the contribution of each objective.
  3. **Scaling behavior**: Train student models of different sizes (Tiny, Small, Base, Large) using the same teacher, evaluate on ImageNet-1K and fine-grained classification datasets to observe how performance scales with model size.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does Proteus perform when trained on datasets larger than ImageNet-1K, and what is the relationship between dataset size and downstream task performance?
  - **Basis in paper**: [inferred] The paper mentions that computational constraints limited validation to ImageNet-1K and that efficacy at larger scales remains unverified.
  - **Why unresolved**: The paper explicitly states that scalability to larger datasets was not tested due to computational limitations.
  - **What evidence would resolve it**: Conducting experiments training Proteus on datasets larger than ImageNet-1K (e.g., ImageNet-21K) and measuring performance across downstream tasks.

- **Open Question 2**: What is the impact of using different patch sizes between teacher and student models in Proteus, and can the method be adapted to handle varying patch sizes?
  - **Basis in paper**: [explicit] The paper notes that Proteus must maintain the same patch size as the teacher due to patch and feature-level learning objectives.
  - **Why unresolved**: The paper identifies this as a limitation but does not explore alternative approaches to handle varying patch sizes.
  - **What evidence would resolve it**: Testing Proteus with teacher and student models having different patch sizes and evaluating performance degradation or adaptation strategies.

- **Open Question 3**: How does the choice of teacher model scale (e.g., DINOv2-g vs DINOv2-L) affect the efficiency and performance of Proteus, and is there an optimal teacher scale for different student model sizes?
  - **Basis in paper**: [explicit] The paper shows that average accuracy decreases with increased teacher scales and suggests smaller teachers could speed up training.
  - **Why unresolved**: While the paper demonstrates this trend, it does not systematically explore the optimal teacher scale for different student sizes or the trade-offs between efficiency and performance.
  - **What evidence would resolve it**: Conducting a systematic study varying teacher model scales for different student sizes and measuring training time and downstream performance.

## Limitations

- The framework's performance on models trained on completely inaccessible datasets (e.g., those with strict licensing) remains unverified
- The paper does not thoroughly investigate the impact of ImageNet-1K's class distribution on the distilled models' performance, particularly for domains that may be underrepresented in ImageNet-1K
- Computational requirements for training with 1024 batch size on 8 A100 GPUs for 300 epochs may still be prohibitive for many research groups

## Confidence

**High Confidence:**
- The effectiveness of removing one-hot labels and projection heads to mitigate dataset bias during knowledge distillation
- The superiority of Proteus over existing methods (DeiT, OpenCLIP) when distilling from DINOv2-g/14 to smaller models

**Medium Confidence:**
- The generalizability of the approach to other foundation models beyond DINOv2, specifically SynCLR and CLIP
- The claim that Proteus can match the performance of DINOv2-L/14 across 19 benchmarks

**Low Confidence:**
- The claim that Proteus can be trained at "ImageNet-level costs" while achieving the stated performance
- The assertion that Proteus "facilitates the accessibility of training foundation models for the broader research community"

## Next Checks

1. **Robustness to Distribution Shift**: Evaluate Proteus on a dataset that is significantly different from ImageNet-1K (e.g., medical images, satellite imagery, or a specialized fine-grained classification dataset not represented in ImageNet) to assess its ability to generalize beyond the proxy dataset.

2. **Ablation on Teacher Size and Quality**: Systematically vary the size and quality of the teacher model (e.g., using DINOv2-S/14 vs. DINOv2-L/14) to determine the minimum requirements for effective knowledge transfer and identify potential bottlenecks in the distillation process.

3. **Analysis of Learned Representations**: Conduct a detailed analysis of the learned representations by the student model, comparing them to those of the teacher model using techniques like centered kernel alignment (CKA) or probing classifiers. This would provide insights into what knowledge is being transferred and how it is being represented in the student model.