---
ver: rpa2
title: 'TIER: Text-Image Encoder-based Regression for AIGC Image Quality Assessment'
arxiv_id: '2401.03854'
source_url: https://arxiv.org/abs/2401.03854
tags:
- image
- text
- images
- encoder
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of assessing the quality of
  AI-generated images (AIGIs) from a human perception perspective. The authors propose
  a text-image encoder-based regression (TIER) framework that incorporates both the
  generated images and their corresponding text prompts as inputs.
---

# TIER: Text-Image Encoder-based Regression for AIGC Image Quality Assessment

## Quick Facts
- arXiv ID: 2401.03854
- Source URL: https://arxiv.org/abs/2401.03854
- Reference count: 40
- Proposes TIER framework that outperforms baseline methods in AIGI quality assessment using SRCC and PLCC metrics

## Executive Summary
This paper addresses the challenge of assessing the quality of AI-generated images (AIGIs) from a human perception perspective. The authors propose a text-image encoder-based regression (TIER) framework that incorporates both the generated images and their corresponding text prompts as inputs. Specifically, the framework utilizes a text encoder (BERT) to extract features from text prompts and an image encoder (e.g., ResNet or Inception) to extract features from generated images. These features are then concatenated and fed into a regression network to predict quality scores. Extensive experiments on three mainstream AIGCIQA databases (AGIQA-1K, AGIQA-3K, and AIGCIQA2023) demonstrate that the proposed TIER method generally outperforms the baseline method in terms of Spearman Rank Correlation Coefficient (SRCC) and Pearson Linear Correlation Coefficient (PLCC) metrics in most cases.

## Method Summary
The TIER framework combines text and image features through a two-stream architecture. Text prompts are encoded using BERT to capture semantic content, while generated images are processed through convolutional networks (ResNet or Inception) to extract visual features. These feature representations are concatenated and passed through a regression network that predicts quality scores. The model is trained on human-annotated quality judgments from AIGCIQA databases, learning to map the joint text-image representation to perceptual quality ratings.

## Key Results
- TIER outperforms baseline methods in SRCC and PLCC metrics on AGIQA-1K, AGIQA-3K, and AIGCIQA2023 databases
- The framework demonstrates effectiveness in capturing human perception of AIGI quality by leveraging both visual and textual information
- Performance improvements are most notable when quality assessment requires understanding the alignment between image content and text prompts

## Why This Works (Mechanism)
The TIER framework works by leveraging the complementary information from text prompts and generated images. Text prompts provide semantic context about what should be generated, while image features capture visual quality attributes. By combining these modalities, the model can better assess whether the generated image meets the expectations set by the prompt. The regression approach allows for continuous quality scoring rather than binary classification, enabling nuanced assessment of perceptual quality differences.

## Foundational Learning

### Text-Image Encoding
**Why needed:** AIGIs must be evaluated based on both their visual quality and their fidelity to the input text prompt
**Quick check:** Verify that BERT and image encoder outputs are properly aligned in feature space before concatenation

### Feature Concatenation Strategy
**Why needed:** Simple concatenation may not capture complex interactions between text and image features
**Quick check:** Experiment with attention-based fusion mechanisms as an alternative to concatenation

### Regression vs Classification
**Why needed:** Quality assessment is inherently a continuous problem rather than discrete categories
**Quick check:** Compare regression performance against ordinal classification approaches

## Architecture Onboarding

### Component Map
Text Prompt -> BERT Encoder -> Text Features
Generated Image -> ResNet/Inception Encoder -> Image Features
Text Features + Image Features -> Concatenation -> Regression Network -> Quality Score

### Critical Path
The critical path involves encoding both modalities, concatenating features, and passing through the regression network. The bottleneck is likely the feature fusion stage, where the model must learn to weight text versus image information appropriately.

### Design Tradeoffs
The framework trades off model complexity for interpretability - while a simpler approach might use only image features, incorporating text provides more context but requires additional computational overhead. The choice of BERT versus smaller text encoders represents a capacity versus efficiency tradeoff.

### Failure Signatures
The model may fail when text prompts are ambiguous or when generated images contain artifacts that don't clearly violate the prompt requirements. Performance degradation is expected when the text-image alignment is poor or when the quality assessment requires domain-specific knowledge not captured in the training data.

### First Experiments
1. Ablation study removing text features to quantify their contribution to performance
2. Cross-database evaluation to test generalization across different AIGI generators
3. Sensitivity analysis to determine the impact of different text and image encoder choices

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on three relatively new AIGCIQA databases that may not represent diverse real-world AIGI content
- Effectiveness unproven across diverse AIGI domains and newer generative models not included in evaluation
- Does not address potential biases in human perception data used for training and evaluation

## Confidence

### High confidence
- Framework architecture and methodology are clearly defined and reproducible
- Experimental results show consistent performance improvements over baseline methods

### Medium confidence
- Performance claims relative to baseline methods
- Generalizability across diverse AIGI domains and generative models

### Low confidence
- Claims about capturing all aspects of human perception of AIGI quality
- Bias mitigation and handling of diverse cultural perspectives in quality assessment

## Next Checks
1. Test TIER on additional, independently collected AIGI datasets that include images from newer generative models (e.g., SDXL, Midjourney v5+)
2. Conduct ablation studies to quantify the contribution of text features versus image features to overall performance
3. Perform bias analysis on the human perception data used for training to identify potential demographic or cultural skews in quality judgments