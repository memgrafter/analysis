---
ver: rpa2
title: Learning Fair Invariant Representations under Covariate and Correlation Shifts
  Simultaneously
arxiv_id: '2408.09312'
source_url: https://arxiv.org/abs/2408.09312
tags:
- uni00000013
- uni00000011
- fairness
- domain
- flair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FLAIR, a novel approach for fairness-aware
  domain generalization that simultaneously addresses both covariate and correlation
  shifts. The method disentangles data into content and style factors, then learns
  fairness-aware domain-invariant content representations by minimizing sensitive
  information while preserving non-sensitive information.
---

# Learning Fair Invariant Representations under Covariate and Correlation Shifts Simultaneously

## Quick Facts
- arXiv ID: 2408.09312
- Source URL: https://arxiv.org/abs/2408.09312
- Reference count: 40
- Key outcome: FLAIR achieves state-of-the-art performance on fairness-aware domain generalization, outperforming baselines on accuracy and fairness metrics across RCMNIST, NYPD, and FairFace datasets.

## Executive Summary
This paper introduces FLAIR, a novel approach for fairness-aware domain generalization that simultaneously addresses both covariate and correlation shifts. The method disentangles data into content and style factors, then learns fairness-aware domain-invariant content representations by minimizing sensitive information while preserving non-sensitive information. Experimental results on three benchmark datasets demonstrate that FLAIR significantly outperforms state-of-the-art baselines, achieving the best trade-off between model accuracy and both group and individual fairness metrics.

## Method Summary
FLAIR learns fair invariant representations through a three-stage process: first, it disentangles data into domain-invariant content and domain-specific style factors using a transformation model; second, it learns fairness-aware representations by clustering content factors with Gaussian mixture models and reconstructing them to minimize sensitive information; third, it trains a predictor using primal-dual optimization to enforce both domain invariance and fairness constraints simultaneously. The framework handles both covariate shifts (domain-specific feature distributions) and correlation shifts (variations in sensitive attribute-label dependencies) through this unified approach.

## Key Results
- FLAIR achieves the best trade-off between accuracy and fairness metrics compared to state-of-the-art baselines
- The method demonstrates robust performance across three diverse datasets (RCMNIST, NYPD, and FairFace)
- FLAIR successfully handles both covariate and correlation shifts simultaneously
- Setting the regularization parameter Œª2 to 0.5 provides optimal balance between accuracy and fairness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The transformation model allows disentangling data into domain-invariant content and domain-specific style factors, enabling generalization across unseen domains.
- Mechanism: FLAIR uses a transformation model T to convert instances between domains while preserving labels. The content encoder h_c extracts domain-invariant content features, while the style encoder h_s captures domain-specific variations. This disentanglement ensures that learned representations generalize across covariate shifts.
- Core assumption: The underlying transformation model T exists and can transform instances between domains while preserving class labels.
- Evidence anchors:
  - [abstract] "we assume there exists an underlying transformation model that can transform instances sampled from one domain to another while keeping the class labels unchanged"
  - [section 4.1] "we assume, ‚àÄùëí, ùëí‚Ä≤ ‚àà E, ùëí ‚â† ùëí‚Ä≤, there exists a function ùëá : X √ó X ‚Üí X that transforms instances from domain ùëí to ùëí‚Ä≤"

### Mechanism 2
- Claim: Gaussian Mixture Models with fairness constraints reconstruct content representations that minimize sensitive information while preserving non-sensitive information.
- Mechanism: The fair representation learner g uses GMMs to cluster content factors by sensitive attributes, then reconstructs fair content representations using weighted prototypes. The fairness constraint ensures equal posterior probabilities across sensitive subgroups for each prototype.
- Core assumption: Content factors can be meaningfully clustered using GMMs and that statistical parity can be enforced through prototype reconstruction.
- Evidence anchors:
  - [section 4.2] "we group the content factors along with the sensitive attributes... into ùêæ clusters based on their similarity"
  - [section 4.2] "To achieve fairness, the fundamental idea designingùëî is to make sure that the probability that a random content factor cùëé=‚àí1ùëñ from the sensitive subgroup ùëé = ‚àí1 mapping to the ùëò-th particular prototype ùùÅùëé=‚àí1ùëò is equal to the probability of a random content factor cùëé=1ùëñ mapping to the prototype ùùÅùëé=1ùëò"

### Mechanism 3
- Claim: The primal-dual optimization enforces both domain invariance and fairness simultaneously through Lagrangian multipliers.
- Mechanism: FLAIR uses primal-dual updates where the primal variables (model parameters) minimize the total loss while dual variables (ùúÜ1, ùúÜ2) enforce constraints on invariance and fairness. This approach balances the trade-off between accuracy and fairness.
- Core assumption: The Lagrangian relaxation provides an effective way to enforce hard constraints in the optimization problem.
- Evidence anchors:
  - [section 4.3] "We optimize ùúÜ1 and ùúÜ2 in the ùëÖùë°ùëúùë°ùëéùëô using the primal-dual algorithm, which is an effective tool for enforcing invariance"
  - [section 4.3] "where ùúÜ1, ùúÜ2 > 0 are Lagrangian multipliers"

## Foundational Learning

- Concept: Domain Generalization and Distribution Shifts
  - Why needed here: FLAIR addresses the challenge of generalizing from training domains to unseen test domains under covariate and correlation shifts.
  - Quick check question: What is the difference between covariate shift and correlation shift in the context of domain generalization?

- Concept: Fairness Metrics and Group vs Individual Fairness
  - Why needed here: FLAIR aims to achieve both group fairness (statistical parity) and individual fairness (treating similar individuals similarly).
  - Quick check question: How does FLAIR ensure both group fairness and individual fairness simultaneously?

- Concept: Disentangled Representation Learning
  - Why needed here: FLAIR disentangles data into content and style factors to achieve domain invariance while preserving relevant information.
  - Quick check question: Why is disentanglement into content and style factors crucial for FLAIR's approach?

## Architecture Onboarding

- Component map:
  - Transformation Model (T) -> Content Encoder (h_c) -> Fair Representation Learner (g) -> Classifier (œâ) -> prediction
  - Data flows through transformation, content extraction, fair reconstruction, and classification

- Critical path: T ‚Üí h_c ‚Üí g ‚Üí œâ ‚Üí prediction

- Design tradeoffs:
  - Number of GMM prototypes (K) vs model complexity and fairness performance
  - Lagrangian multipliers (ùúÜ1, ùúÜ2) vs balance between invariance and fairness
  - Trade-off between accuracy and fairness controlled by ùúÜ2

- Failure signatures:
  - Poor accuracy on unseen domains indicates transformation model issues
  - Large fairness metric gaps suggest problems with the fair representation learner
  - Unstable training may indicate incorrect Lagrangian parameter settings

- First 3 experiments:
  1. Verify transformation model works by testing domain conversion on RCMNIST with known correlation shifts
  2. Test GMM clustering and prototype reconstruction on a simple binary sensitive attribute dataset
  3. Validate primal-dual updates by checking constraint satisfaction on a small synthetic dataset with known bounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of prototypes K affect the trade-off between accuracy and fairness in FLAIR across different types of datasets?
- Basis in paper: [explicit] The paper mentions conducting a sensitivity analysis of K and finding that FLAIR had the highest average ranking across four metrics when K was set to 3, 3, and 4 for RCMNIST, NYPD, and FairFace datasets respectively.
- Why unresolved: While the paper identifies optimal K values for each dataset, it doesn't explain the underlying reasons for these differences or provide a systematic method to determine K for new datasets without extensive experimentation.
- What evidence would resolve it: Empirical studies showing the relationship between dataset characteristics (e.g., domain complexity, sample size, feature dimensionality) and optimal K values, or theoretical analysis explaining why different datasets require different numbers of prototypes.

### Open Question 2
- Question: Can FLAIR's framework be extended to handle continuous sensitive attributes rather than just binary ones?
- Basis in paper: [inferred] The current implementation uses binary sensitive attributes (A ‚àà {‚àí1, 1}) and groups data into two sensitive subgroups, but the paper doesn't explore scenarios with continuous or multi-category sensitive attributes.
- Why unresolved: The paper focuses on binary sensitive attributes and doesn't discuss how the framework would handle continuous attributes or attributes with multiple categories, leaving open questions about scalability and adaptability.
- What evidence would resolve it: Experimental results showing FLAIR's performance with continuous sensitive attributes, along with modifications to the fair representation learner to handle such cases effectively.

### Open Question 3
- Question: How does FLAIR perform when the correlation between sensitive attributes and labels changes dynamically across domains during training?
- Basis in paper: [explicit] The paper mentions that correlation shift is defined as the variation in the dependency between the sensitive attribute and label across domains, and that FLAIR handles correlation shifts, but doesn't specifically test scenarios where this correlation changes dynamically during training.
- Why unresolved: The experimental setup uses fixed correlation values for each domain, and there's no discussion of how FLAIR would handle dynamic correlation changes that might occur in real-world scenarios.
- What evidence would resolve it: Experiments where the correlation between sensitive attributes and labels is varied during training, showing how FLAIR adapts to these changes and whether it maintains its performance and fairness guarantees.

## Limitations

- Strong assumption about existence of transformation models between domains may not hold in real-world scenarios
- Effectiveness depends on Gaussian distribution assumptions for GMM clustering, which may not always be valid
- Requires careful hyperparameter tuning (particularly Œª1, Œª2, and K) to achieve desired balance between accuracy and fairness

## Confidence

- High confidence: The core mechanism of disentangling content and style factors for domain generalization is well-established and the experimental results show consistent improvements over baselines across multiple datasets.
- Medium confidence: The fairness guarantees through GMM-based prototype reconstruction are theoretically sound but may be sensitive to distributional assumptions and initialization.
- Medium confidence: The primal-dual optimization approach for enforcing constraints is valid, but the convergence properties and sensitivity to constraint bounds need further investigation.

## Next Checks

1. Test FLAIR on datasets where the transformation model assumption is violated to assess robustness to incorrect assumptions about domain transformations.
2. Evaluate the sensitivity of fairness metrics to different GMM initialization strategies and numbers of prototypes (K) to understand the stability of the approach.
3. Conduct ablation studies removing the style encoder to quantify the contribution of disentanglement to overall performance and validate that domain-specific variations are indeed being captured by the style factors.