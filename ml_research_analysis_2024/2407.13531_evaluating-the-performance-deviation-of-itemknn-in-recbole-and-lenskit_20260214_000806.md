---
ver: rpa2
title: Evaluating the performance-deviation of itemKNN in RecBole and LensKit
arxiv_id: '2407.13531'
source_url: https://arxiv.org/abs/2407.13531
tags:
- recbole
- lenskit
- ndcg
- data
- itemknn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the ItemKNN algorithms in RecBole and LensKit,
  two popular recommender system libraries. Using four datasets (Anime, Modcloth,
  ML-100K, and ML-1M), the research evaluates their efficiency, accuracy, and scalability,
  focusing on nDCG.
---

# Evaluating the performance-deviation of itemKNN in RecBole and LensKit

## Quick Facts
- arXiv ID: 2407.13531
- Source URL: https://arxiv.org/abs/2407.13531
- Authors: Michael Schmidt; Jannik Nitschke; Tim Prinz
- Reference count: 26
- This study compares ItemKNN implementations in RecBole and LensKit, finding that differences in similarity matrix calculation cause significant performance deviations.

## Executive Summary
This study systematically compares the ItemKNN algorithms in RecBole and LensKit, two popular recommender system libraries. Using four datasets (Anime, Modcloth, ML-100K, and ML-1M), the research evaluates their efficiency, accuracy, and scalability, focusing on nDCG as the primary metric. Initial results showed RecBole outperformed LensKit in two of three metrics, with 18% higher nDCG, 14% higher precision, and 35% lower recall. To ensure fair comparison, LensKit's nDCG calculation was adjusted to match RecBole's approach, reducing the performance gap. Further analysis revealed that differences in similarity matrix calculations were the primary cause of performance deviations. After modifying LensKit to retain only the top K similar items, both libraries achieved nearly identical nDCG values across all datasets. The findings highlight the importance of similarity matrix calculation in prediction accuracy and provide insights for developers implementing ItemKNN algorithms.

## Method Summary
The study uses four datasets (Anime, Modcloth, ML-100K, ML-1M) converted to implicit feedback format. Both libraries employ user-based 80/20 split with random seeds 21, 42, and 84. ItemKNN with k=20 is used for recommendations, generating 10 recommendations per user. The evaluation focuses on nDCG@10, precision@10, and recall@10. Initial comparisons revealed performance differences, prompting adjustments to LensKit's nDCG calculation and similarity matrix construction to match RecBole's implementation.

## Key Results
- RecBole initially outperformed LensKit with 18% higher nDCG, 14% higher precision, and 35% lower recall
- After standardizing nDCG calculation methodology, performance gaps reduced significantly
- Modifying LensKit's similarity matrix to retain only top-K items resulted in nearly identical nDCG values across all datasets
- The primary cause of performance deviation was identified as differences in similarity matrix calculation approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differences in similarity matrix calculation directly affect prediction accuracy and nDCG values
- Mechanism: RecBole limits each item's similarity column to top-K neighbors, while LensKit includes all similarity values. This causes LensKit to incorporate noise from less relevant items during score calculation, lowering nDCG
- Core assumption: Top-K filtering reduces noise in the similarity matrix without losing critical ranking information
- Evidence anchors:
  - [abstract] "Further analysis revealed that differences in similarity matrix calculations were the primary cause of performance deviations"
  - [section] "This difference becomes apparent in the prediction of items for a user...LensKit does it as follows...RecBole essentially does the same using matrix multiplication. But since there are many zero-entries in RecBole's similarity matrix, the array sims will contain many zeros"
  - [corpus] Weak evidence - no directly relevant citations found
- Break condition: When dataset has uniform similarity distributions or when K is set too low to capture meaningful relationships

### Mechanism 2
- Claim: Standardizing nDCG calculation methodology eliminates artificial performance gaps
- Mechanism: RecBole and LensKit originally used different nDCG calculation formulas, particularly in handling IDCG for fewer than k items. Aligning these formulas made performance comparisons fair
- Core assumption: nDCG differences are not due to algorithmic quality but calculation methodology
- Evidence anchors:
  - [abstract] "To ensure a fair head to head comparison, we adjusted LensKit's nDCG calculation to match RecBole's approach"
  - [section] "RecBole's nDCG calculation follows the standard formula...In contrast, the LensKit implementation differs slightly...To ensure a fair head to head comparison, we modified LensKit's nDCG calculation to match RecBole's approach"
  - [corpus] Weak evidence - no directly relevant citations found
- Break condition: When both libraries use identical calculation methods or when evaluation metric differences are negligible

### Mechanism 3
- Claim: User-based splitting provides better data distribution for collaborative filtering evaluation
- Mechanism: Using 80/20 holdout split with user-based splitting handles data sparsity and ensures sufficient training data while maintaining realistic evaluation conditions
- Core assumption: User-based splitting creates more representative test sets for recommender systems than random item-based splitting
- Evidence anchors:
  - [section] "In order to make it easier to use the splitted data of RecBole for LensKit, we used a 80/20 holdout split with a user-based splitting. User-based splitting is used to handle data sparsity and provides a good amount of data to create recommendations"
  - [corpus] Weak evidence - no directly relevant citations found
- Break condition: When datasets have uniform user-item interactions or when sparsity is not a concern

## Foundational Learning

- Concept: Cosine similarity calculation and its role in collaborative filtering
  - Why needed here: Both libraries use cosine distance to calculate item similarities, which forms the basis of their recommendation predictions
  - Quick check question: How does cosine similarity between two items relate to their dot product and magnitudes?

- Concept: nDCG (normalized Discounted Cumulative Gain) metric
  - Why needed here: The primary evaluation metric for comparing library performance, sensitive to ranking quality and position of relevant items
  - Quick check question: What's the difference between DCG and nDCG, and why is normalization important?

- Concept: Sparse matrix representations (CSR format)
  - Why needed here: RecBole uses CSR format to store similarity matrices with many zero entries, affecting memory usage and computation
  - Quick check question: Why is CSR format more memory-efficient for similarity matrices with mostly zero values?

## Architecture Onboarding

- Component map: Data preprocessing → Similarity matrix calculation → Prediction generation → nDCG evaluation
- Critical path: Similarity matrix calculation is the bottleneck; errors here propagate to final nDCG scores
- Design tradeoffs: RecBole prioritizes memory efficiency and noise reduction (top-K filtering) vs. LensKit's complete similarity inclusion
- Failure signatures: Inconsistent nDCG values across libraries suggest implementation differences in similarity calculation or evaluation metrics
- First 3 experiments:
  1. Run both libraries with identical data splits and hyperparameters, comparing raw nDCG values
  2. Implement and verify identical nDCG calculation formulas across both libraries
  3. Modify LensKit to use top-K similarity filtering and compare final nDCG values against RecBole

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does RecBole's approach of limiting similarity matrix entries to top-K items consistently outperform LensKit's full matrix approach across diverse datasets?
- Basis in paper: [explicit] The paper states that RecBole's implementation outperformed LensKit on three out of four datasets after adjusting the similarity matrix calculation
- Why unresolved: The study only used four specific datasets with particular characteristics (Anime, Modcloth, ML-100K, ML-1M), and results showed mixed performance
- What evidence would resolve it: Comprehensive testing across a broader range of datasets varying in sparsity, item diversity, and user interaction patterns to determine if the performance difference is consistent or dataset-dependent

### Open Question 2
- Question: What is the optimal value of K for similarity matrix calculations in ItemKNN algorithms, and does this optimal value vary by dataset characteristics?
- Basis in paper: [inferred] The study used a fixed K=20 for all experiments, but the paper mentions this was done for simplicity rather than optimization, and discusses the trade-off between accuracy and noise reduction
- Why unresolved: The paper only tested one K value across all datasets without exploring how different K values affect performance or whether optimal K varies by dataset
- What evidence would resolve it: Systematic experiments varying K values across multiple datasets with different characteristics to identify patterns in optimal K selection

### Open Question 3
- Question: How does the computational efficiency of the two similarity matrix approaches compare, particularly for large-scale datasets?
- Basis in paper: [explicit] The paper mentions that RecBole saves memory by using CSR format and reducing zero-entries, and notes that LensKit took 4 minutes vs RecBole's 1 minute for the Anime dataset
- Why unresolved: The study focused primarily on accuracy metrics (nDCG, precision, recall) rather than detailed computational efficiency analysis, and only measured one dataset with notably different processing times
- What evidence would resolve it: Comprehensive benchmarking of computational time and memory usage across multiple dataset sizes and types, including profiling of both training and prediction phases

### Open Question 4
- Question: Does the choice of similarity metric (currently both use cosine distance) significantly impact the relative performance of the two implementations?
- Basis in paper: [inferred] The paper focuses on cosine distance implementation differences but doesn't explore whether alternative similarity metrics would change the performance outcomes
- Why unresolved: The study was limited to cosine distance without testing whether the observed differences would persist with other similarity metrics like Pearson correlation or Jaccard index
- What evidence would resolve it: Replicating the experiments using multiple similarity metrics across the same datasets to determine if the implementation differences remain significant regardless of metric choice

## Limitations
- The study identifies implementation differences between RecBole and LensKit as the primary cause of performance deviation, but cannot definitively rule out other factors such as underlying optimization strategies, numerical precision differences, or framework-specific behaviors that may contribute to the observed gaps.
- Results are based on only four datasets, limiting generalizability to other data types and sizes.
- The study only tested one K value (k=20) across all datasets without exploring how different K values affect performance or whether optimal K varies by dataset.

## Confidence
- Medium confidence that similarity matrix calculation differences cause performance deviation
- High confidence that standardizing nDCG calculation methodology reduces artificial performance gaps

## Next Checks
1. Conduct independent replication of the entire experiment pipeline using fresh datasets and random seeds to verify the consistency of findings across different data samples.
2. Implement a hybrid approach where RecBole uses LensKit's similarity calculation and vice versa to isolate whether the performance difference is truly due to similarity matrix construction or other implementation details.
3. Perform ablation studies by systematically varying the K parameter in top-K filtering to identify the optimal threshold where performance converges and to test the robustness of the noise reduction hypothesis.