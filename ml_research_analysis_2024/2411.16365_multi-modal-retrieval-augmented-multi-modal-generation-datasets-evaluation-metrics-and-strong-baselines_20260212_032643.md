---
ver: rpa2
title: 'Multi-modal Retrieval Augmented Multi-modal Generation: Datasets, Evaluation
  Metrics and Strong Baselines'
arxiv_id: '2411.16365'
source_url: https://arxiv.org/abs/2411.16365
tags:
- image
- text
- score
- multi-modal
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic study of Multi-modal Retrieval
  Augmented Multi-modal Generation (M2RAG), a novel task requiring models to process
  multi-modal web content and generate multi-modal responses. The authors constructed
  a comprehensive benchmark dataset with 1,000 queries across 10 topics, using a rigorous
  data curation pipeline.
---

# Multi-modal Retrieval Augmented Multi-modal Generation: Datasets, Evaluation Metrics and Strong Baselines

## Quick Facts
- **arXiv ID**: 2411.16365
- **Source URL**: https://arxiv.org/abs/2411.16365
- **Reference count**: 40
- **Key outcome**: Multi-stage generation strategy and LLM-based approaches with detailed image descriptions outperform single-stage and MLLM approaches on M2RAG tasks, with fine-tuned 7B-8B models exceeding GPT-4o performance

## Executive Summary
This paper introduces Multi-modal Retrieval Augmented Multi-modal Generation (M2RAG), a novel task requiring models to process multi-modal web content and generate multi-modal responses. The authors construct a comprehensive benchmark dataset with 1,000 queries across 10 topics and introduce both text-modal and multi-modal evaluation metrics based on foundation models. They propose two generation strategies - single-stage and multi-stage - with the latter consistently outperforming the former. The study finds that LLMs generally outperform MLLMs in this task when provided with detailed image descriptions, and scaling up model size improves performance. A high-quality training dataset was constructed by filtering 3K samples using multi-modal metrics, which was used to fine-tune 7B-8B models that outperformed GPT-4o and approached OpenAI o3-mini in overall performance.

## Method Summary
The M2RAG task involves processing multi-modal web content to generate multi-modal responses, requiring both text and image generation. The authors constructed a benchmark dataset through a rigorous pipeline including query collection, web page and auxiliary image crawling, text/image processing, and element scoring. Two generation strategies were proposed: single-stage generation that processes all inputs simultaneously, and multi-stage generation that first generates text-only responses, then identifies segments needing images, and finally refines text with selected images. Evaluation metrics include four text-modal metrics (fluency, relevance, context precision, faithfulness) and five multi-modal metrics (image coherence, helpfulness, reference, recall, plus overall score). The training dataset was constructed by filtering 3K samples using multi-modal metrics, focusing on samples that score well on image coherence, helpfulness, reference, and recall, which was used to fine-tune 7B-8B models using LoRA with rank 128 and α 256.

## Key Results
- Multi-stage generation strategy consistently outperforms single-stage approach across all metrics
- LLMs with detailed image descriptions outperform MLLMs due to multi-image confusion issues
- Fine-tuned 7B-8B models on curated dataset exceed GPT-4o performance and approach OpenAI o3-mini
- Auxiliary images significantly improve model performance when combined with detailed descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage generation strategy improves M2RAG performance by explicitly handling image interleaving and text refinement.
- Mechanism: The multi-stage approach first generates text-only responses, then identifies segments needing images, and finally refines text with selected images. This breaks the complex task into manageable steps.
- Core assumption: LLMs/MLLMs can effectively identify text segments that would benefit from image insertion.
- Evidence anchors:
  - [abstract]: "multi-stage strategy consistently outperform the former. The study found that LLMs generally outperform MLLMs in this task, and scaling up model size improves performance."
  - [section]: "Multi-stage addresses the common limitation of foundation models, which often struggle to process a large number of images simultaneously."
  - [corpus]: Weak - no direct evidence in corpus about multi-stage approach performance

### Mechanism 2
- Claim: Using detailed image descriptions with contextual information enables LLMs to better understand and select relevant images.
- Mechanism: Instead of processing raw images, LLMs receive detailed textual descriptions of images including context, allowing them to reason about image relevance without visual processing limitations.
- Core assumption: Detailed textual descriptions can convey sufficient semantic information about images for relevance judgment.
- Evidence anchors:
  - [section]: "LLMs receive detailed image descriptions and do not directly process raw visual input, avoiding this issue."
  - [section]: "The exclusion of context in description generation leads to a significant decline in model performance"
  - [corpus]: Weak - corpus lacks specific evidence about image description quality impact

### Mechanism 3
- Claim: Fine-tuning on curated high-quality data significantly improves 7B-8B model performance on M2RAG tasks.
- Mechanism: The training dataset is constructed by filtering 3K samples using multi-modal metrics, focusing on samples that score well on image coherence, helpfulness, reference, and recall.
- Core assumption: The multi-modal metrics used for filtering correlate strongly with human judgment of quality.
- Evidence anchors:
  - [abstract]: "A training dataset constructed by filtering high-quality samples using our designed multi-modal evaluation metrics, which is used to improve the performance of 7B-8B LLMs and MLLMs."
  - [section]: "Fine-tuning 7B-8B LLMs and MLLMs on our curated dataset yields performance that exceeds GPT-4o"
  - [section]: "The designed evaluation metrics exhibit strong correlations with human judgments"

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: M2RAG extends RAG by incorporating multi-modal retrieval and generation, so understanding RAG fundamentals is essential
  - Quick check question: What are the key differences between traditional RAG and M2RAG?

- Concept: Multi-modal reasoning
  - Why needed here: Models must understand relationships between text and images to generate coherent multi-modal responses
  - Quick check question: How does multi-modal reasoning differ from single-modal reasoning in the context of question answering?

- Concept: Text-to-image generation evaluation
  - Why needed here: The evaluation metrics assess image quality and relevance, requiring understanding of how to measure visual content quality
  - Quick check question: What are the key dimensions for evaluating whether an image is helpful in answering a question?

## Architecture Onboarding

- Component map: Query collection → Web page and auxiliary image crawling → Text/image processing → Element scoring → Generation strategy → Evaluation metrics → Fine-tuning
- Critical path: Query collection → Web page and auxiliary image crawling → Text/image processing → Element scoring → Generation → Evaluation
- Design tradeoffs: Single-stage vs multi-stage generation (complexity vs performance), LLM vs MLLM backbone (cost vs capability)
- Failure signatures: Poor image selection (low image recall scores), incoherent text-image relationships (low image coherence scores), text quality degradation (low fluency scores)
- First 3 experiments:
  1. Compare single-stage vs multi-stage generation with the same model on a small subset of queries
  2. Test LLM vs MLLM performance on the same generation strategy
  3. Evaluate impact of auxiliary images by running with and without them on a subset of queries

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific architectural improvements in MLLMs would be needed to eliminate the "multi-image confusion" phenomenon and enable them to handle M2RAG tasks as effectively as LLMs with image descriptions?
  - Basis in paper: Explicit - "Due to the multi-image confusion phenomenon, current MLLMs struggle with reasoning over multiple images"
  - Why unresolved: The paper identifies this as a key limitation but does not propose specific architectural solutions or explore what changes would be necessary to overcome it.

- **Open Question 2**: How would the performance of M2RAG models change if evaluation metrics incorporated pairwise ranking instead of Likert scales, and would this approach better capture the relative quality differences between models?
  - Basis in paper: Inferred - "For example, pairwise ranking is more appropriate for assessing overall performance than Likert scale in M2RAG task, which lacks a standard optimal answer"
  - Why unresolved: The paper acknowledges this limitation but only uses Likert scales for evaluation, leaving open the question of how different evaluation methodologies might affect model comparisons.

- **Open Question 3**: What would be the performance impact of incorporating text-modal metrics (Context Precision and Faithfulness) into the training dataset filtering process, and could this improve overall model performance beyond the current 19.2% improvement?
  - Basis in paper: Explicit - "We do not utilize the text-modal metrics, since the time cost of Context Precision and Faithfulness metrics on 3K long generations are huge...and we plan to improve the efficiency of text-modal evaluation metrics"
  - Why unresolved: The authors acknowledge this as a limitation and future work, but the paper does not explore whether including these metrics would further enhance model performance.

## Limitations

- The evaluation methodology relies heavily on GPT-4o-based metrics, which introduces potential bias and raises questions about the independence of the evaluation from the proposed solutions
- The finding that LLMs outperform MLLMs is surprising and counterintuitive given that the task is fundamentally multi-modal, suggesting either a fundamental insight or a potential limitation in how MLLMs were configured or evaluated
- The auxiliary image retrieval component shows significant performance impact, but the paper does not fully explain the retrieval mechanism or why certain images were selected as "auxiliary"

## Confidence

- **High confidence**: The multi-stage generation strategy consistently outperforms single-stage approaches, supported by ablation studies and clear performance improvements across multiple metrics
- **Medium confidence**: The finding that LLMs generally outperform MLLMs, as this contradicts typical expectations for multi-modal tasks and may depend on specific implementation details not fully disclosed
- **Medium confidence**: The effectiveness of the curated training dataset, as the filtering methodology is sound but the small size (1.6K samples) and lack of human validation create uncertainty about generalization

## Next Checks

1. **Human evaluation validation**: Conduct blind human evaluations comparing the top-performing models (fine-tuned 7B-8B models, GPT-4o, o3-mini) on a random subset of 100 queries to verify that the automated metrics accurately predict human preferences for both text quality and image relevance

2. **MLLM configuration audit**: Systematically test different MLLM configurations (different models, different image processing approaches, different context window settings) to determine if the LLM superiority finding is robust or an artifact of specific implementation choices

3. **Auxiliary image necessity test**: Run a controlled experiment removing auxiliary images entirely to quantify their true contribution versus the cost/complexity they add, and analyze what types of queries benefit most from auxiliary image support versus those where they are redundant or distracting