---
ver: rpa2
title: 'FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational
  LLMs'
arxiv_id: '2410.19317'
source_url: https://arxiv.org/abs/2410.19317
tags:
- bias
- multi-turn
- fairness
- llms
- biased
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FairMT-Bench, the first comprehensive fairness
  benchmark for large language models (LLMs) in multi-turn dialogue scenarios. The
  authors identify that existing fairness evaluations focus primarily on single-turn
  dialogues, overlooking the complexities and bias accumulation challenges in realistic
  multi-turn conversations.
---

# FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs

## Quick Facts
- arXiv ID: 2410.19317
- Source URL: https://arxiv.org/abs/2410.19317
- Authors: Zhiting Fan; Ruizhe Chen; Tianxiang Hu; Zuozhu Liu
- Reference count: 40
- Primary result: Introduces FairMT-Bench, the first comprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios

## Executive Summary
This paper introduces FairMT-Bench, the first comprehensive fairness benchmark for large language models (LLMs) in multi-turn dialogue scenarios. The authors identify that existing fairness evaluations focus primarily on single-turn dialogues, overlooking the complexities and bias accumulation challenges in realistic multi-turn conversations. To address this gap, they develop a task taxonomy targeting LLM fairness capabilities across three stages: context understanding, user interaction, and instruction trade-offs, with six multi-turn dialogue tasks. The FairMT-10K dataset is constructed, covering two major bias types (stereotype, toxicity) and six bias attributes (gender, race, religion, etc.), using existing fairness datasets and template-based generation. Experiments reveal that current LLMs are significantly more likely to generate biased responses in multi-turn dialogues compared to single-turn settings, with performance varying notably across tasks, dialogue turns, and bias attributes.

## Method Summary
FairMT-Bench addresses fairness in multi-turn dialogue through a three-stage task taxonomy (context understanding, user interaction, instruction trade-offs) implemented across six tasks: Anaphora Ellipsis, Scattered Questions, Jailbreak Tips, Interference from Misinformation, Fixed Format, and Negative Feedback. The FairMT-10K dataset (10,000 samples) is constructed using existing fairness datasets (RedditBias, SBIC, HateXplain) and template-based generation, covering stereotype and toxicity bias types across six attributes. Evaluation uses GPT-4 as primary judge with specific bias detection instructions, supplemented by Llama-Guard-3 and human validation. Models are evaluated by feeding 5-turn prompts sequentially with temperature 0.7 and max 150 tokens, measuring bias rates across different tasks and dialogue turns.

## Key Results
- Current LLMs generate significantly more biased responses in multi-turn dialogues compared to single-turn settings
- Performance varies notably across different tasks, dialogue turns, and bias attributes
- Six representative LLMs (ChatGPT, Llama-2-7B/13B-chat, Llama-3.1-8B-instruct, Mistral-7B-instruct, Gemma-7b-it) show persistent fairness shortcomings across all models
- FairMT-1K dataset distilled from FairMT-10K demonstrates consistent bias patterns across 15 state-of-the-art LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn dialogues accumulate bias through contextual dependencies and pronoun ambiguity
- Mechanism: When LLMs process multi-turn dialogues, pronouns and ellipsis create ambiguity about social group references. The model's inability to correctly match these references to earlier context leads to biased generation, especially when stereotypes are distributed across turns
- Core assumption: LLM context understanding degrades with pronoun-heavy dialogue and scattered information
- Evidence anchors: Current LLMs are more likely to generate biased responses in multi-turn dialogues; LLMs fail to correctly match pronouns with corresponding people or events from earlier context

### Mechanism 2
- Claim: Multi-turn dialogue structure creates "bias amplification windows" through task-specific vulnerabilities
- Mechanism: Different multi-turn dialogue tasks create distinct failure modes - Scattered Questions task accumulates bias through indirect group linking, Jailbreak Tips task exploits role-playing vulnerabilities, and Interference Misinformation task uses historical context contamination
- Core assumption: Task structure and user interaction patterns create predictable bias amplification pathways
- Evidence anchors: Significant variation in performance across different tasks and models; Tasks like Scattered Questions, Anaphora Ellipsis, and Jailbreak Tips prove particularly challenging for these models

### Mechanism 3
- Claim: Model size and alignment strategy create task-specific fairness-performance trade-offs
- Mechanism: Larger models show improved comprehension for implicit bias detection but increased vulnerability to user instruction following that conflicts with fairness objectives. This creates a trade-off between understanding subtle bias and resisting biased instructions
- Core assumption: Model parameters and training objectives determine bias resistance vs comprehension balance
- Evidence anchors: Performance varying notably across tasks, dialogue turns, and bias attributes; tasks requiring strong comprehension and those demanding resistance to guided bias

## Foundational Learning

- Concept: Context window limitations and attention mechanisms
  - Why needed here: Understanding how LLMs process multi-turn dialogue context and maintain attention across turns
  - Quick check question: How does transformer attention mechanism handle pronoun references across multiple dialogue turns?

- Concept: Bias detection and classification frameworks
  - Why needed here: Evaluating bias requires understanding different bias types (stereotype vs toxicity) and attributes (gender, race, religion)
  - Quick check question: What distinguishes stereotype bias from toxicity in LLM outputs, and how are they detected?

- Concept: Multi-turn dialogue system architecture
  - Why needed here: Understanding how multi-turn conversations differ from single-turn interactions in LLM deployment
  - Quick check question: What architectural components enable LLMs to maintain conversational state across multiple dialogue turns?

## Architecture Onboarding

- Component map: Task taxonomy generation -> Dataset construction (FairMT-10K/FairMT-1K) -> Multi-turn prompt creation -> LLM response collection -> Bias evaluation -> Analysis reporting
- Critical path: Task taxonomy → Dataset generation → Multi-turn prompt creation → LLM response collection → Bias evaluation → Analysis reporting
- Design tradeoffs: Comprehensive bias coverage vs dataset size (10K vs 1K), GPT-4 evaluation accuracy vs computational cost, template-based generation vs natural conversation authenticity
- Failure signatures: Inconsistent bias detection across evaluation methods, performance variation across bias attributes, task-specific failure modes, accumulation patterns across dialogue turns
- First 3 experiments:
  1. Replicate single-turn vs multi-turn bias comparison on small subset to verify core mechanism
  2. Test pronoun resolution capability across different model sizes to validate mechanism 1
  3. Evaluate task-specific bias patterns to confirm mechanism 2's amplification windows

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the specific dialogue turn number (beyond just "multi-turn" vs "single-turn") impact the accumulation of bias in LLM responses, and at what turn count does bias stabilization occur?
- Basis in paper: Section 3.4 discusses bias accumulation across dialogue turns but doesn't identify a stabilization point or optimal maximum turn count for maintaining fairness
- Why unresolved: The paper shows bias generally increases with turn count but doesn't investigate whether bias eventually plateaus or if there's an optimal maximum number of turns for fairness preservation
- What evidence would resolve it: Systematic experiments measuring bias rates across a wider range of turn counts (e.g., 10-50 turns) to identify whether bias stabilizes, continues accumulating, or shows non-linear patterns at specific thresholds

### Open Question 2
- Question: What are the differential effects of implicit versus explicit bias cues in multi-turn dialogues on LLM fairness performance across different model architectures?
- Basis in paper: Section 3.2 notes that models like Llama-2 perform poorly on "Anaphora Ellipsis" tasks (implicit bias) while models like Mistral perform better on "Scattered Questions" (also implicit), suggesting architectural differences in handling implicit bias
- Why unresolved: The paper identifies performance differences but doesn't systematically compare how different architectures (transformer variants, parameter counts, training objectives) specifically handle implicit versus explicit bias cues
- What evidence would resolve it: Controlled experiments varying the explicitness of bias cues while holding other variables constant, comparing performance across model families with different architectural features

### Open Question 3
- Question: How do specific instruction-following capabilities (as opposed to general language understanding) mediate the trade-off between fairness preservation and user satisfaction in multi-turn dialogues?
- Basis in paper: Section 3.2 notes that models like Mistral, while stronger in contextual understanding, may prioritize utility and user satisfaction over safety when following user instructions
- Why unresolved: The paper observes this trade-off but doesn't investigate which specific instruction-following capabilities (format compliance, task completion, user preference alignment) most strongly conflict with fairness preservation
- What evidence would resolve it: Ablation studies varying instruction complexity and type while measuring both fairness metrics and user satisfaction proxies, identifying which instruction characteristics most strongly predict fairness violations

## Limitations

- Bias Evaluation Reliability: The benchmark relies on GPT-4 as primary bias judge, creating uncertainty about evaluation consistency across different cultural contexts
- Task Representation Completeness: The six tasks may not capture the full spectrum of multi-turn dialogue scenarios where fairness issues manifest
- Dataset Generation Methodology: Artificial dialogue construction may not fully represent the complexity of real conversational interactions where bias emerges

## Confidence

- Multi-turn bias accumulation exists: High confidence
- Task-specific vulnerability patterns: High confidence
- Model size and alignment trade-offs: Medium confidence
- GPT-4 evaluation reliability: Medium confidence

## Next Checks

1. Cross-cultural validation: Evaluate the same LLM models on FairMT-Bench using bias judges from different cultural backgrounds to test robustness of bias detection across diverse perspectives

2. Real conversation adaptation: Adapt FairMT-Bench tasks using transcripts from actual multi-turn conversations (customer service, tutoring, etc.) to validate whether task-specific vulnerability patterns hold in naturalistic settings

3. Fine-tuning intervention study: Take the most biased LLM models identified in the benchmark and fine-tune them specifically on FairMT-1K with fairness objectives, then re-evaluate to measure improvement and validate the benchmark's utility for model improvement