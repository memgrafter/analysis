---
ver: rpa2
title: End-to-End Supervised Hierarchical Graph Clustering for Speaker Diarization
arxiv_id: '2401.12850'
source_url: https://arxiv.org/abs/2401.12850
tags:
- speaker
- clustering
- diarization
- graph
- e-sharc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces E-SHARC, an end-to-end supervised hierarchical
  clustering framework for speaker diarization based on graph neural networks. The
  approach jointly optimizes an embedding extractor and a graph neural network clustering
  module, enabling representation learning, metric learning, and clustering with end-to-end
  optimization.
---

# End-to-End Supervised Hierarchical Graph Clustering for Speaker Diarization

## Quick Facts
- arXiv ID: 2401.12850
- Source URL: https://arxiv.org/abs/2401.12850
- Authors: Prachi Singh; Sriram Ganapathy
- Reference count: 40
- Key outcome: E-SHARC achieves 11-20% relative improvements over baseline systems on AMI, Voxconverse, and DISPLACE datasets, with an overlap-aware variant further improving results by up to 20% relative.

## Executive Summary
This paper introduces E-SHARC, an end-to-end supervised hierarchical clustering framework for speaker diarization based on graph neural networks. The approach jointly optimizes an embedding extractor and a graph neural network clustering module, enabling representation learning, metric learning, and clustering with end-to-end optimization. It extends prior work by providing detailed analysis and an overlap-aware variant (E-SHARC-Overlap) that can assign multiple speakers in overlapping speech regions using an external overlap detector.

## Method Summary
E-SHARC extends the SHARC hierarchical clustering algorithm by integrating a learnable graph neural network module that performs end-to-end optimization of speaker embeddings and clustering simultaneously. The framework uses x-vectors as node features in a k-NN graph, where the GNN module predicts edge probabilities and node densities for hierarchical clustering. The overlap-aware variant (E-SHARC-Overlap) employs a two-pass approach with an external overlap detector to handle overlapping speech regions by identifying second speakers in overlaps through contrastive training.

## Key Results
- E-SHARC achieves 11-20% relative improvements over baseline systems on AMI, Voxconverse, and DISPLACE datasets
- E-SHARC-Overlap + VBx shows up to 20% relative improvement over the best baseline
- Ablation studies demonstrate GNN architecture superiority over LSTM variants
- Strong speaker counting accuracy across all evaluated datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: End-to-end supervised hierarchical clustering using GNNs enables better speaker representation learning than modular clustering.
- Mechanism: The GNN model learns to predict both edge probabilities and node densities through pairwise interactions across the graph. This supervision from labeled speaker identities directly optimizes the representation space for clustering, unlike unsupervised similarity metrics (e.g., PLDA) that are fixed during clustering.
- Core assumption: The speaker labels in training data are accurate and cover the variability needed for generalization.
- Evidence anchors:
  - [abstract] "jointly optimizes the embedding extractor and the GNN clustering module, performing representation learning, metric learning, and clustering with end-to-end optimization."
  - [section III-D.1] "The GNN scoring function Φ is a learnable GNN module designed for supervised clustering. The module jointly predicts node densities and edge probabilities using the input embeddings at each level."
- Break condition: If the training data contains label noise or insufficient speaker variability, the learned metric may not generalize well to unseen speakers or conditions.

### Mechanism 2
- Claim: Overlap-aware clustering can accurately assign multiple speakers to overlapping speech regions.
- Mechanism: The E-SHARC-Overlap model creates a second-pass graph where nodes are connected only across different speaker clusters (excluding the parent cluster), enabling the model to identify the second speaker in overlaps. The external overlap detector provides ground truth overlap regions, and the GNN learns to distinguish between overlapping speakers using contrastive training with some preserved intra-cluster connections.
- Core assumption: Overlap regions contain exactly two speakers, and the external detector accurately identifies these regions.
- Evidence anchors:
  - [abstract] "with additional inputs from an external overlap detector, the E-SHARC approach is capable of predicting the speakers in the overlapping speech regions."
  - [section IV] "We extend the E-SHARC model also to perform GNN-based overlap prediction called E-SHARC-Overlap. Our approach assumes that a maximum of two speakers are present in an overlapping region."
- Break condition: If overlap regions contain more than two speakers, or if the overlap detector has high false positive/negative rates, the second-pass assignment will fail.

### Mechanism 3
- Claim: The hierarchical clustering approach with node feature aggregation improves speaker cluster quality over flat clustering.
- Mechanism: At each level, nodes with high density (better representing their cluster) are preserved as identity features while all node features are aggregated as average features. This creates progressively better representations for higher-level clusters, allowing the model to refine cluster assignments iteratively rather than making one-time decisions.
- Core assumption: Speaker clusters have internal structure that can be progressively refined through hierarchical merging.
- Evidence anchors:
  - [section III-D.3] "To obtain node representations for next level HHH(m+1)t, the clusters CCC(m)t and the features HHH(m)t are used to compute identity feature ˜h(m+1)i and average feature ¯¯hi(m+1) of each cluster i as..."
  - [section III-D.2] "Clustering is the process of grouping the nodes based on the presence of edge connections. After GNN scoring, clustering is performed hierarchically using the edge probabilities ˆp(m)ij and the estimated node densities ˆd(m)i."
- Break condition: If speaker turns are very short or speakers have highly similar characteristics, the hierarchical refinement may not provide meaningful improvements over flat clustering.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their variants (GraphSAGE vs GCN)
  - Why needed here: GNNs are used to learn edge probabilities and node densities for hierarchical clustering. Understanding the difference between transductive (GCN) and inductive (GraphSAGE) variants is critical for model design.
  - Quick check question: Why does the paper choose GraphSAGE over GCN for this task? (Hint: Consider the need to generalize to unseen nodes.)

- Concept: Speaker embedding extraction (x-vectors, ETDNN)
  - Why needed here: The system uses pre-trained x-vector models as initialization and fine-tunes them end-to-end. Understanding the architecture and training of these embeddings is essential for implementing the E-SHARC framework.
  - Quick check question: What is the role of the PLDA model in the initial graph construction, and why is it not updated during E-SHARC training?

- Concept: Hierarchical clustering algorithms and their stopping criteria
  - Why needed here: The SHARC algorithm performs iterative clustering until no connected components remain. Understanding when and how to stop clustering is crucial for both training and inference.
  - Quick check question: How does the threshold τ control the clustering process, and what happens if it's set too high or too low?

## Architecture Onboarding

- Component map:
  - Input: Mel-filterbank features → ETDNN model → x-vectors
  - Graph construction: x-vectors + PLDA similarity scores → k-NN graph
  - GNN module: Node features + edges → edge probabilities + node densities
  - Clustering: Edge probabilities + node densities → cluster assignments
  - Aggregation: Cluster nodes → next level node features
  - Overlap handling: External detector → second-pass E-SHARC-Overlap

- Critical path: Mel-spectrogram → ETDNN → x-vectors → Graph initialization → GNN scoring → Clustering → Output diarization labels

- Design tradeoffs:
  - GNN architecture choice (2048 units) vs computational cost
  - k value for k-NN graph (60 for train, 30 for test) vs graph density
  - τ threshold (0.0 for train, 0.8 for test) vs cluster granularity
  - Overlap assumption (max 2 speakers) vs real-world complexity

- Failure signatures:
  - Under-clustering: Low τ or small k produces too few speakers
  - Over-clustering: High τ or large k produces too many speakers
  - Poor overlap assignment: Inaccurate external detector or model confusion
  - Degraded performance on unseen conditions: Insufficient training data diversity

- First 3 experiments:
  1. Baseline comparison: Run AHC and SC with oracle VAD on AMI eval set, measure DER
  2. Ablation on GNN architecture: Replace GNN with BLSTM (same parameter count) and compare DER on AMI
  3. Hyperparameter sensitivity: Vary k (20-100) and τ (0.0, 0.4, 0.8) on Voxconverse dev set, plot DER curves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does joint optimization of the PLDA model with the E-SHARC training framework lead to further improvements in diarization performance compared to using a fixed pre-trained PLDA model?
- Basis in paper: [explicit] The paper mentions in the E-SHARC training section that "Following the E-SHARC training, we do not retrain the PLDA model as this was not found to improve the results further. The continuous update of the PLDA model parameters along with the SHARC training may potentially improve the modeling."
- Why unresolved: The authors explicitly note this as a potential area for improvement but did not explore it in their experiments.
- What evidence would resolve it: Experimental results comparing E-SHARC with and without joint PLDA optimization on the same benchmark datasets (AMI, Voxconverse, DISPLACE).

### Open Question 2
- Question: How does combining E-SHARC with end-to-end neural diarization (EEND) models like DiaPer affect diarization performance, particularly for handling overlapping speech?
- Basis in paper: [inferred] The discussion section mentions "It is worthwhile to explore the incorporation of EEND with our proposal of graph clustering. We hypothesize that such a framework could further improve the results and combine the advantages of graphs with EEND setting."
- Why unresolved: The authors acknowledge this as a potential direction but did not implement or test such a hybrid approach.
- What evidence would resolve it: Experimental results comparing hybrid EEND-Graph clustering models against standalone EEND and E-SHARC models on benchmark datasets with overlapping speech.

### Open Question 3
- Question: Can the E-SHARC framework be effectively extended to handle more than two speakers in overlapping regions, and what architectural modifications would be required?
- Basis in paper: [explicit] The paper states "Our approach assumes that a maximum of two speakers are present in an overlapping region" when describing the E-SHARC-Overlap model, suggesting this as a limitation.
- Why unresolved: The current overlap handling is limited to two speakers, and the paper does not explore extensions to handle more complex overlapping scenarios.
- What evidence would resolve it: Experimental results demonstrating the performance of an extended E-SHARC model on datasets with more than two simultaneous speakers, along with architectural details of the modifications.

## Limitations
- The overlap-aware variant assumes a maximum of two speakers in overlapping regions, which may not hold for multiparty conversations
- The external overlap detector introduces a dependency that could propagate errors into the second-pass clustering
- The GNN architecture and hyperparameters were tuned on specific datasets, raising questions about performance on different acoustic conditions or languages

## Confidence
- End-to-end supervised optimization effectiveness: **High** - Supported by strong ablation results showing GNN superiority over LSTM variants
- Overlap-aware clustering performance: **Medium** - Validated on test sets but relies heavily on external detector accuracy
- Generalization across datasets: **Medium** - Shows consistent improvements but with varying margins across different domains

## Next Checks
1. Test E-SHARC on a multiparty dataset (e.g., CALLHOME) where overlap regions may contain more than two speakers to evaluate robustness
2. Implement and evaluate a version without the external overlap detector to measure the contribution of the overlap-aware variant
3. Conduct cross-dataset validation by training on AMI and testing on Voxconverse (and vice versa) to assess generalization