---
ver: rpa2
title: Learning to Edit Visual Programs with Self-Supervision
arxiv_id: '2406.02383'
source_url: https://arxiv.org/abs/2406.02383
tags:
- edit
- program
- network
- visual
- programs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes learning to edit visual programs with self-supervision.
  The core idea is to train a network that predicts local edit operations to modify
  an input program towards a visual target.
---

# Learning to Edit Visual Programs with Self-Supervision

## Quick Facts
- arXiv ID: 2406.02383
- Source URL: https://arxiv.org/abs/2406.02383
- Reference count: 40
- Primary result: Combining an edit network with a one-shot model outperforms using only the one-shot model for visual program induction, with gains increasing with more search time or less training data

## Executive Summary
This paper addresses the challenge of visual program induction (VPI) by proposing a self-supervised learning approach that combines a one-shot program synthesis network with an edit network. The edit network predicts local program modifications that improve visual similarity to a target, reducing the burden on the one-shot model to generate complete programs from scratch. Through joint finetuning of both networks via a bootstrapped learning loop, the system can infer more accurate visual programs than either component alone, particularly when given additional search time or when training data is limited.

## Method Summary
The approach involves jointly training two networks: a one-shot model that predicts complete programs from visual inputs, and an edit network that predicts local modifications to improve program reconstruction. The edit network takes a complete input program and visual target, then predicts operations like modifying transform parameters, adding/removing transforms, or changing combinators. The system uses bootstrapped finetuning where the one-shot model generates initial programs that are edited and used to train both networks in a virtuous cycle. A domain-aware findEdits algorithm creates training data by identifying edit operations that transform start programs into end programs, enabling self-supervised learning without ground-truth program annotations.

## Key Results
- The joint edit network and one-shot model approach outperforms the one-shot model alone across Layout, 2D CSG, and 3D CSG domains
- Performance gains increase with more search time, demonstrating the edit network's effectiveness for goal-directed refinement
- The approach shows particular advantage with limited training data, reducing the need for extensive annotated program datasets

## Why This Works (Mechanism)

### Mechanism 1
The edit network learns to predict local program edits that improve visual similarity to a target, reducing the burden on the one-shot model to author programs from scratch. By focusing on small, goal-directed modifications rather than full program generation, the local nature of edits allows the network to concentrate on improving reconstruction through targeted changes.

### Mechanism 2
The joint finetuning creates a virtuous cycle where the one-shot model provides good initializations and the edit network refines them. During finetuning, the one-shot model generates initial program estimates, which are then edited by the edit network. These edited programs serve as better training data for the one-shot model in subsequent rounds, with each model benefiting from the other's improvements.

### Mechanism 3
The self-supervised learning paradigm allows the edit network to be trained without ground-truth program annotations by bootstrapping from the one-shot model. The one-shot model generates programs that are paired with target programs using a domain-aware findEdits algorithm to create training data, enabling the edit network to learn to predict edits without requiring annotated programs.

## Foundational Learning

- **Visual program induction (VPI)**: Finding programs that generate visual outputs matching a target. Needed because the paper addresses the core task of inferring programs for visual targets without annotations.
- **Bootstrapped finetuning**: Jointly training models through iterative improvement cycles. Needed because the paper uses this approach to train both the edit network and one-shot model without ground-truth annotations.
- **Domain-specific language (DSL)**: Formal language defining allowed program structures and operations. Needed because the paper defines DSLs for each visual programming domain to constrain the program space.

## Architecture Onboarding

- **Component map**: Visual encoders (CNNs) -> Visual token sequences -> One-shot model (Transformer decoder) -> Complete programs, Edit network (Transformer decoder) -> Local edit operations, findEdits algorithm -> Edit operation training data, Inference procedure -> Population of programs evolved through one-shot and edit network
- **Critical path**: 1) Pretrain one-shot model on synthetic programs, 2) Pretrain edit network on synthetic program pairs, 3) Jointly finetune both networks using bootstrapped approach, 4) Use inference procedure to find well-reconstructing programs for target visual data
- **Design tradeoffs**: Local vs. global edits (local simplifies learning but may limit expressiveness), Domain-specific vs. general edit operations (domain-specific improves performance but reduces generalizability), Complexity of findEdits algorithm (more complex algorithms may improve training data quality but increase computational cost)
- **Failure signatures**: Poor reconstruction performance (issues with one-shot model or edit network), High variance in edit predictions (edit network not learning effectively), Slow convergence during finetuning (issues with bootstrapped learning paradigm)
- **First 3 experiments**: 1) Train one-shot model and edit network separately on synthetic data and evaluate reconstruction, 2) Jointly finetune both networks using bootstrapped approach and evaluate reconstruction, 3) Compare joint approach against one-shot model alone on target visual data

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the generality of the approach to different DSL structures, the impact of various corruption processes on editing network performance, how the approach scales with program complexity, sensitivity to hyperparameter choices, and potential integration with other program synthesis techniques.

## Limitations

- Reliance on domain-specific languages and edit operations limits generalizability to other visual programming domains
- Performance depends heavily on the quality of the findEdits algorithm, which is not fully specified
- Evaluation focuses primarily on reconstruction accuracy without exploring interpretability or generalizability of learned programs

## Confidence

- High confidence in the core claim that the edit network improves reconstruction performance when combined with the one-shot model
- Medium confidence in the mechanism explanations, particularly the effectiveness of the bootstrapped finetuning loop
- Low confidence in the scalability and generalizability to more complex visual programming domains

## Next Checks

1. Conduct ablation studies to quantify the contribution of individual edit operations versus the edit network architecture
2. Implement the findEdits algorithm independently and evaluate its reliability across different domain pairs
3. Test the approach on a new visual programming domain with a different DSL structure to assess generalizability