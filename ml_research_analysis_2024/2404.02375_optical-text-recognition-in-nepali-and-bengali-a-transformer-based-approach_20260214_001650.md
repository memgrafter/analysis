---
ver: rpa2
title: 'Optical Text Recognition in Nepali and Bengali: A Transformer-based Approach'
arxiv_id: '2404.02375'
source_url: https://arxiv.org/abs/2404.02375
tags:
- bengali
- text
- nepali
- character
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study developed an OCR system for Bengali and Nepali, two low-resource
  languages, using a transformer-based encoder-decoder model. It fine-tuned Microsoft
  TrOCR with Bengali and Nepali text images, incorporating data preprocessing and
  augmentation.
---

# Optical Text Recognition in Nepali and Bengali: A Transformer-based Approach

## Quick Facts
- arXiv ID: 2404.02375
- Source URL: https://arxiv.org/abs/2404.02375
- Authors: S M Rakib Hasan; Aakar Dhakal; Md Humaion Kabir Mehedi; Annajiat Alim Rasel
- Reference count: 17
- One-line primary result: Transformer-based OCR system for Bengali and Nepali achieved low CER/WER, outperforming prior methods for these low-resource scripts.

## Executive Summary
This study develops an optical character recognition (OCR) system for Bengali and Nepali, two low-resource South Asian languages, using a transformer-based encoder-decoder architecture. The authors fine-tune Microsoft's TrOCR model, which combines a Vision Transformer (ViT) encoder with an XLM-RoBERTa decoder, on script-specific datasets. Data augmentation techniques including random flipping and rotation are applied to improve generalization. The model achieves competitive performance with training Character Error Rates (CER) of 0.04 for Bengali and 0.09 for Nepali, and Word Error Rates (WER) of 0.10 and 0.14 respectively, with slightly higher errors on test sets.

## Method Summary
The approach fine-tunes Microsoft TrOCR (ViT-base-patch16-384 encoder + XLM-RoBERTa decoder) on Bengali and Nepali handwritten text images. Input images are resized to 384x384 pixels and augmented with random flipping and rotation (-5 to 5 degrees). The BanglaWriting dataset is used for Bengali, while a manually prepared dataset serves for Nepali. The model is trained using AdamW optimizer with batch size 4 for 2000 epochs, and evaluated on 10 test images per language using CER and WER metrics.

## Key Results
- Training CER: 0.04 (Bengali), 0.09 (Nepali); Training WER: 0.10 (Bengali), 0.14 (Nepali)
- Test CER: 0.07 (Bengali), 0.11 (Nepali); Test WER: 0.12 (Bengali), 0.15 (Nepali)
- Outperformed prior methods for Bengali and Nepali OCR
- Robust text recognition across varying font styles, sizes, and orientations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer-based encoder-decoder models can effectively handle OCR for low-resource languages by treating images as patch sequences and leveraging multilingual decoder pre-training.
- Mechanism: The ViT encoder converts images into patch embeddings, which the transformer decoder maps to text using cross-attention. Multilingual pre-training (XLM-RoBERTa) supplies language priors for Bengali and Nepali, reducing data requirements.
- Core assumption: Visual patch sequences are sufficiently rich to represent character-level features, and multilingual decoder pretraining transfers linguistic knowledge effectively to unseen scripts.
- Evidence anchors:
  - [abstract] "using encoder-decoder transformers, a model was developed, and its efficacy was assessed using a collection of optical text images"
  - [section] "The model receives images as a series of linearly embedded, fixed-size patches with a 16x16 resolution"
  - [corpus] Weak evidence: Corpus contains only low-resource OCR and ASR papers, none reporting transformer patch-based image-to-text performance in Bengali/Nepali.
- Break condition: If patch embeddings fail to preserve local character shapes, the decoder cannot reconstruct accurate text, causing high CER/WER.

### Mechanism 2
- Claim: Data augmentation (random flipping, rotation, resizing) improves generalization for scripts with high variability like Bengali and Nepali.
- Mechanism: Augmentations expose the model to diverse glyph orientations and scales, reducing overfitting to specific image styles in the training set.
- Core assumption: The transformer can learn invariant representations under mild affine transformations; the language model can still map distorted features to correct characters.
- Evidence anchors:
  - [section] "The images were enhanced through random flipping and random rotation (-5 to 5 degrees)"
  - [abstract] "The model achieved a training Character Error Rate (CER) of 0.04 and Word Error Rate (WER) of 0.10; for Nepali, CER was 0.09 and WER was 0.14"
  - [corpus] No direct corpus evidence of augmentation impact on Bengali/Nepali OCR performance.
- Break condition: Excessive augmentation (e.g., >15° rotation) could distort script-specific features beyond recognizability, increasing error rates.

### Mechanism 3
- Claim: Fine-tuning a large multilingual model (XLM-RoBERTa) on script-specific datasets outperforms training from scratch for low-resource OCR.
- Mechanism: The multilingual decoder's pre-trained attention and contextualization are adapted to Bengali and Nepali via supervised fine-tuning on labeled image-text pairs.
- Core assumption: The multilingual model has captured sufficient linguistic priors for these scripts during pretraining, enabling rapid adaptation with limited data.
- Evidence anchors:
  - [section] "In this instance, the xlm-roberta-base decoder has been chosen. This is a multilingual text transformer model that has been trained on 2.5 terabytes of data from 100 languages"
  - [abstract] "The results signify that the suggested technique corresponds with current approaches and achieves high precision in recognizing text in Bengali and Nepali"
  - [corpus] No corpus evidence comparing multilingual fine-tuning vs. from-scratch training in these scripts.
- Break condition: If the pre-trained model lacks exposure to Bengali/Nepali script structure, fine-tuning yields negligible gains, and training from scratch may be preferable.

## Foundational Learning

- Concept: Vision Transformer (ViT) encoder fundamentals
  - Why needed here: The ViT encoder converts image patches into embeddings that the transformer decoder consumes for OCR; understanding patch tokenization and positional embeddings is essential.
  - Quick check question: How does ViT represent the spatial relationship between patches before feeding them into the transformer encoder?

- Concept: Cross-attention in encoder-decoder transformers
  - Why needed here: The decoder uses cross-attention to focus on relevant visual features from the encoder while generating text, directly impacting OCR accuracy.
  - Quick check question: What role does the decoder's cross-attention layer play in aligning visual features to predicted characters?

- Concept: Data augmentation impact on model generalization
  - Why needed here: Augmentation mitigates overfitting on limited Bengali/Nepali datasets, so understanding its limits and appropriate transformations is critical.
  - Quick check question: Why might rotations beyond ±5° harm recognition of scripts with connected characters like Bengali?

## Architecture Onboarding

- Component map:
  Input pipeline → 384x384 resize, grayscale, random flip/rotate → ViT encoder (vit-base-patch16-384) → patch embeddings + [CLS] token → XLM-RoBERTa decoder → cross-attention over encoder outputs → text prediction → Loss: CTC or token-level cross-entropy on character sequences
- Critical path:
  ViT patch tokenization → encoder transformer → decoder cross-attention → character sequence output
- Design tradeoffs:
  - Patch size (16x16) balances local detail vs. global context; smaller patches increase model size and computation.
  - Fixed 384x384 input may distort tall/short characters; adaptive resizing could help but adds complexity.
  - Multilingual decoder pre-training aids low-resource tasks but may dilute script-specific features.
- Failure signatures:
  - High CER/WER on test vs. train: overfitting or domain shift in image quality.
  - Decoder output repeating patterns: attention misalignment or insufficient training data.
  - Poor recognition of dependent letters: encoder not capturing fine stroke details.
- First 3 experiments:
  1. Train with no augmentation; compare CER/WER to augmented baseline to measure augmentation impact.
  2. Replace XLM-RoBERTa decoder with a script-specific Bengali/Nepali decoder; measure if multilingual transfer is beneficial.
  3. Vary ViT patch size (16x16 vs. 8x8); assess trade-off between detail preservation and computational cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the transformer-based OCR model compare to other state-of-the-art OCR models for Bengali and Nepali when applied to real-world, noisy, or low-quality images?
- Basis in paper: [explicit] The paper mentions that the model was tested on 10 images with varying font styles, sizes, and orientations, but does not provide a direct comparison with other OCR models.
- Why unresolved: The paper does not include a comparative analysis with other OCR models, making it difficult to assess the relative performance of the transformer-based approach.
- What evidence would resolve it: Conducting experiments that directly compare the transformer-based model with other state-of-the-art OCR models on the same dataset, including real-world, noisy, or low-quality images, would provide a clear comparison of their performance.

### Open Question 2
- Question: What is the impact of using different pre-trained vision transformer models as the encoder in the OCR system for Bengali and Nepali?
- Basis in paper: [explicit] The paper uses Google's ViT-base-patch16-384 as the encoder but does not explore the impact of using other pre-trained vision transformer models.
- Why unresolved: The choice of the encoder model can significantly affect the OCR system's performance, and the paper does not investigate this aspect.
- What evidence would resolve it: Experimenting with different pre-trained vision transformer models as the encoder and comparing their performance on the OCR task for Bengali and Nepali would provide insights into the impact of the encoder choice.

### Open Question 3
- Question: How does the OCR model perform on Bengali and Nepali text with complex layouts, such as multi-column documents or text with overlapping elements?
- Basis in paper: [explicit] The paper mentions that the model was tested on images with varying font styles, sizes, and orientations but does not address complex layouts.
- Why unresolved: The model's ability to handle complex document layouts is crucial for practical applications, and the paper does not explore this aspect.
- What evidence would resolve it: Testing the OCR model on documents with complex layouts, such as multi-column text or text with overlapping elements, and evaluating its performance would provide insights into its robustness in handling such scenarios.

### Open Question 4
- Question: What are the limitations of the current OCR model in handling cursive or highly stylized handwriting in Bengali and Nepali?
- Basis in paper: [explicit] The paper mentions that the model was trained on handwritten text but does not specifically address cursive or highly stylized handwriting.
- Why unresolved: Cursive and highly stylized handwriting can pose significant challenges for OCR systems, and the paper does not explore the model's limitations in this regard.
- What evidence would resolve it: Conducting experiments with cursive or highly stylized handwriting samples and evaluating the model's performance would help identify its limitations and potential areas for improvement.

## Limitations

- Extremely small test set size (10 images per language) raises concerns about statistical significance and generalizability of reported metrics
- Nepali dataset preparation described as manual but lacks details on image quality, writer diversity, or annotation methodology
- No ablation study provided to quantify the actual impact of data augmentation on performance improvements
- Comparison to "current approaches" lacks specific quantitative benchmarks or citations to prior work in Bengali/Nepali OCR

## Confidence

**High confidence**: The transformer-based architecture (ViT encoder + XLM-RoBERTa decoder) is technically sound and represents a valid approach for OCR tasks. The preprocessing pipeline (resizing, grayscale conversion, augmentation) follows standard practices.

**Medium confidence**: The reported CER and WER values are plausible given the architecture and dataset sizes, but the small test sets limit statistical significance. The claim of outperforming prior methods is weakly supported without specific comparisons.

**Low confidence**: The exact impact of data augmentation on performance is unclear without ablation experiments. The robustness of the model to variations in handwriting styles, document quality, and script complexities is not demonstrated.

## Next Checks

1. **Expand test set evaluation**: Increase the test set size to at least 100 images per language and report confidence intervals for CER/WER to establish statistical significance and robustness.

2. **Ablation study on data augmentation**: Train models with and without augmentation (and with different augmentation intensities) to quantify its actual contribution to performance improvements.

3. **Benchmark comparison**: Implement and evaluate at least two prior OCR methods (e.g., CRNN, traditional CNN-LSTM) on the same datasets to provide quantitative comparison with the transformer-based approach.