---
ver: rpa2
title: 'LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement
  Learning'
arxiv_id: '2406.05881'
source_url: https://arxiv.org/abs/2406.05881
tags:
- reward
- environment
- policy
- lgr2
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces LGR2, a hierarchical reinforcement learning
  framework that leverages large language models to generate stable, language-guided
  reward functions for the higher-level policy. By decoupling high-level reward generation
  from evolving lower-level policies, LGR2 mitigates non-stationarity and stabilizes
  hierarchical learning.
---

# LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.05881
- Source URL: https://arxiv.org/abs/2406.05881
- Reference count: 40
- Primary result: LLM-based hierarchical RL framework achieving >55% success rates on navigation and manipulation tasks with zero-shot real-robot transfer

## Executive Summary
LGR2 addresses non-stationarity in hierarchical reinforcement learning by decoupling high-level reward generation from evolving lower-level policies using large language models. The framework translates natural language instructions into stable, language-guided reward functions that remain invariant to policy changes, while integrating goal-conditioned hindsight experience replay to densify sparse reward signals. Experiments across simulated and real-world robotic tasks demonstrate significant performance improvements over strong baselines, with successful zero-shot transfer to real robots without fine-tuning.

## Method Summary
LGR2 implements a two-level hierarchical RL architecture where natural language instructions are processed through a two-stage LLM pipeline (motion descriptor → reward coder) to generate executable reward parameters. The higher-level policy selects subgoals every k environment steps, while the lower-level policy executes primitive actions to achieve these subgoals. Language-guided reward relabeling ensures the high-level reward function remains stationary despite lower-level policy evolution, and hindsight experience replay densifies the sparse reward signal. Both policies are trained using SAC, with separate replay buffers for each level.

## Key Results
- Achieves over 55% success rates on challenging navigation and manipulation tasks
- Demonstrates successful zero-shot transfer to real robots without fine-tuning
- Outperforms strong hierarchical and flat RL baselines across multiple simulated and real-world environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LGR2 stabilizes hierarchical learning by generating stationary, language-guided reward functions that are decoupled from evolving lower-level policies.
- **Mechanism:** LLM-based reward generation produces symbolic reward parameters ϕ that remain invariant to policy changes. By translating natural language instructions into structured task descriptions and then into executable reward parameters, LGR2 creates a reward function rφ(s, g⋆, g) that depends only on current state and goals, not on the lower-level policy's behavior.
- **Core assumption:** The LLM can accurately translate natural language instructions into structured, executable reward parameters that capture the semantic intent of the task.
- **Evidence anchors:** [abstract]: "By decoupling high-level reward generation from low-level policy changes, LGR2 fundamentally mitigates the non-stationarity problem in off-policy HRL"

### Mechanism 2
- **Claim:** Goal-conditioned hindsight experience replay (HER) addresses reward sparsity by densifying the high-level reward signal.
- **Mechanism:** For each transition in the higher-level replay buffer, HER samples alternative goals from states encountered within the same trajectory and relabels the transition with these goals and the corresponding language-guided reward. This increases the frequency of positive rewards in the replay buffer.
- **Core assumption:** The sampled alternative goals from the trajectory provide meaningful supervision for learning the higher-level policy.
- **Evidence anchors:** [abstract]: "we integrate goal-conditioned hindsight experience relabeling" and "significantly improving sample efficiency and generalization"

### Mechanism 3
- **Claim:** Hierarchical decomposition enables efficient learning of long-horizon tasks by reducing temporal credit assignment complexity.
- **Mechanism:** The two-level hierarchical structure decomposes complex tasks into manageable subgoals. The higher-level policy selects subgoals every k environment steps, while the lower-level policy executes primitive actions to achieve these subgoals. This temporal abstraction improves exploration and credit assignment.
- **Core assumption:** The task can be meaningfully decomposed into subgoals that the lower-level policy can achieve within k steps.
- **Evidence anchors:** [abstract]: "Hierarchical Reinforcement Learning (HRL) provides a natural framework to address this challenge in robotics" and "enabling efficient decomposition of complex, long-horizon tasks into manageable subgoals"

## Foundational Learning

- **Concept: Hierarchical Reinforcement Learning**
  - Why needed here: LGR2 builds on the hierarchical structure to decompose complex tasks into subgoals, addressing the credit assignment problem in long-horizon tasks
  - Quick check question: What is the key difference between a flat RL approach and the hierarchical approach used in LGR2?

- **Concept: Goal-Conditioned RL**
  - Why needed here: The higher-level policy in LGR2 is goal-conditioned, selecting subgoals that the lower-level policy then tries to achieve, which is essential for the hierarchical decomposition
  - Quick check question: How does goal-conditioning enable the higher-level policy to provide meaningful subgoals to the lower-level policy?

- **Concept: Non-stationarity in Off-Policy Learning**
  - Why needed here: Understanding non-stationarity is crucial because LGR2 specifically addresses this problem by making the high-level reward function stationary through language-guided reward relabeling
  - Quick check question: Why does non-stationarity occur in hierarchical RL and how does it affect the learning process?

## Architecture Onboarding

- **Component map:** Natural language instruction → LLM (Motion Descriptor) → Structured task description → LLM (Reward Coder) → Reward parameters → Higher-level Policy → Subgoals → Lower-level Policy → Primitive actions → Environment

- **Critical path:** 1. User provides natural language instruction 2. LLM generates structured task description 3. LLM generates reward parameters 4. Higher-level policy selects subgoals 5. Lower-level policy executes actions 6. Transitions stored in replay buffers 7. HER applied to higher-level buffer 8. Both policies trained using SAC

- **Design tradeoffs:** Using LLM for reward generation vs. learning reward models: LLM approach avoids need for large labeled datasets but requires careful prompt engineering; Fixed k-step interval vs. adaptive intervals: Fixed intervals simplify implementation but may not be optimal for all tasks; Language-guided rewards vs. environment rewards: Language rewards provide semantic meaning but may not capture all task nuances

- **Failure signatures:** Poor performance despite good prompts: Indicates the LLM is not generating appropriate reward parameters; High variance in training curves: Suggests non-stationarity is not fully addressed; Lower-level policy fails to achieve subgoals: May indicate subgoals are not well-formed or k is too small

- **First 3 experiments:** 1. Test LLM reward generation on simple navigation tasks with known optimal rewards to verify semantic alignment 2. Evaluate non-stationarity mitigation by measuring distance between predicted subgoals and achieved states over training 3. Compare performance with and without HER on sparse reward tasks to quantify densification benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LGR2 perform when using different LLM models for motion description and reward coding?
- Basis in paper: [inferred] The paper uses GPT-3.5 for reward generation but does not compare performance across different LLM models or explore the sensitivity to model choice.
- Why unresolved: The experiments only report results using a single LLM model (GPT-3.5), leaving open the question of whether other models (e.g., GPT-4, LLaMA) would yield better or worse performance.
- What evidence would resolve it: Systematic ablation studies comparing LGR2 performance using different LLM models for motion description and reward coding, including quantitative metrics and qualitative analysis of generated code quality.

### Open Question 2
- Question: Can LGR2 be extended to address non-stationarity in the lower-level policy transitions, not just reward signals?
- Basis in paper: [explicit] The discussion section explicitly mentions that LGR2 addresses reward-level non-stationarity but does not address shifts in underlying transition dynamics induced by the evolving lower-level policy.
- Why unresolved: The current framework only stabilizes high-level rewards, while the paper acknowledges that transition dynamics may still change, potentially limiting performance in more complex scenarios.
- What evidence would resolve it: Development and evaluation of an extended LGR2 framework that incorporates predictive or uncertainty-aware models to handle transition non-stationarity, with comparative experiments showing improved stability and performance.

### Open Question 3
- Question: How sensitive is LGR2 to the choice and design of prompts for the reward translator?
- Basis in paper: [explicit] The discussion section notes that designing effective prompts for the reward translator still requires manual effort and expertise, suggesting potential variability in performance based on prompt quality.
- Why unresolved: The paper does not report experiments varying prompt design or automating prompt generation, leaving open the question of how much performance depends on prompt engineering skill.
- What evidence would resolve it: Experiments systematically varying prompt templates, length, and specificity, along with automated prompt generation approaches, to quantify the impact on LGR2 performance and identify optimal prompt strategies.

## Limitations
- LLM generalization uncertainty: The approach relies on LLMs to generate reward parameters from natural language instructions, but generalization to diverse, complex tasks remains uncertain.
- Real-world transfer gaps: While zero-shot transfer to real robots is demonstrated, success rates on real-world tasks are not explicitly stated, raising concerns about practical applicability.
- Partial non-stationarity mitigation: The framework addresses reward-level non-stationarity but does not handle transition dynamics changes induced by evolving lower-level policies.

## Confidence

- **High Confidence**: The hierarchical decomposition of tasks into subgoals and the integration of HER for densifying reward signals are well-established concepts in HRL. The experimental results demonstrating improved success rates over baselines are consistent with these mechanisms.

- **Medium Confidence**: The LLM-based reward generation and its role in stabilizing hierarchical learning are novel aspects. While the theoretical framework is sound, the empirical validation across diverse tasks and the robustness of the LLM-generated rewards require further scrutiny.

- **Low Confidence**: The claim of zero-shot transfer to real robots is ambitious. Without detailed performance metrics on real-world tasks, the practical applicability of LGR2 in complex, unstructured environments remains uncertain.

## Next Checks

1. **LLM Reward Generalization**: Test the LLM-based reward generation on a diverse set of tasks, including those with complex reward structures not seen during training. Evaluate the semantic alignment of generated rewards with task objectives to ensure robustness.

2. **Non-Stationarity Quantification**: Conduct ablation studies to measure the impact of language-guided reward relabeling on non-stationarity. Compare the distance between predicted subgoals and achieved states over training with and without the LLM-based reward mechanism.

3. **Real-World Performance Evaluation**: Implement LGR2 on a set of real-world robotic tasks with varying complexity. Measure success rates, sample efficiency, and robustness to environmental variability to validate the zero-shot transfer claim and identify potential gaps between simulation and reality.