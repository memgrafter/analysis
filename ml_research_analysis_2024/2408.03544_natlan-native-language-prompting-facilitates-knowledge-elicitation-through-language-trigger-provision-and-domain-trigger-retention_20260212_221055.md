---
ver: rpa2
title: 'NatLan: Native Language Prompting Facilitates Knowledge Elicitation Through
  Language Trigger Provision and Domain Trigger Retention'
arxiv_id: '2408.03544'
source_url: https://arxiv.org/abs/2408.03544
tags:
- natlan
- language
- knowledge
- llms
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of multilingual large language
  models (MLLMs) performing worse on non-dominant languages compared to their dominant
  language. The authors interpret existing translate-then-answer methods through the
  lens of human cognitive features: Language Triggers (LTs) and Domain Triggers (DTs).'
---

# NatLan: Native Language Prompting Facilitates Knowledge Elicitation Through Language Trigger Provision and Domain Trigger Retention

## Quick Facts
- arXiv ID: 2408.03544
- Source URL: https://arxiv.org/abs/2408.03544
- Reference count: 40
- This paper proposes Native Language Prompting (NatLan) to improve multilingual large language model performance by preserving domain-specific terminology during translation.

## Executive Summary
This paper addresses the problem of multilingual large language models (MLLMs) performing worse on non-dominant languages compared to their dominant language. The authors interpret existing translate-then-answer methods through the lens of human cognitive features: Language Triggers (LTs) and Domain Triggers (DTs). They find that while these methods provide sufficient LTs, they often fail to retain DTs during translation. To address this, they propose Native Language Prompting (NatLan), a Multi-MLLM collaboration strategy that employs a domain-specific MLLM with strong multilingual understanding as a translator. Across five language QA benchmarks, NatLan achieves up to 31.28% improvement in accuracy and provides comparable or greater DT retention in up to 87% of cases compared to existing state-of-the-art methods.

## Method Summary
NatLan uses a Multi-MLLM collaboration strategy with role specialization. A Translator LLM converts non-native language questions to the native language while preserving domain-specific terminology, and a Speaker LLM answers questions using native language knowledge. The framework employs domain-specific role instructions and few-shot examples for both LLMs. The approach is tested across five language QA benchmarks covering 57 disciplines and 14,079 questions, using various MLLM combinations including Phi-3, Gemma, Mistral, Llama-2, and Qwen models.

## Key Results
- Achieves up to 31.28% improvement in accuracy compared to existing methods
- Provides comparable or greater Domain Trigger retention in up to 87% of cases
- Outperforms Self-Translation and Google-MT baselines across five target languages (Arabic, Chinese, French, German, Japanese)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language Triggers (LTs) reduce cognitive load by translating non-native questions into the native language, enabling better understanding.
- Mechanism: The model leverages richer knowledge in its dominant language by converting questions into that language, reducing the cognitive effort required to process non-native language questions.
- Core assumption: The model has significantly more knowledge encoded in its dominant language compared to non-dominant languages.
- Evidence anchors:
  - [abstract] "we analogize the dominant language of MLLMs to the native language of humans and use two human cognitive features: the Language Trigger (LT) and the Domain Trigger (DT), to interpret the mechanisms behind translate-then-answer methods"
  - [section] "The former reduces the cognitive load of understanding the non-native questions by translating them into the native language"

### Mechanism 2
- Claim: Domain Triggers (DTs) narrow the scope of knowledge elicitation by employing domain-specific terms.
- Mechanism: Using precise domain-specific terminology helps the model associate knowledge within that specific field, making it easier to retrieve relevant information.
- Core assumption: MLLMs can leverage domain-specific vocabulary to better access relevant knowledge clusters.
- Evidence anchors:
  - [abstract] "The latter narrows the scope of knowledge elicitation by employing domain-specific terms"
  - [section] "Different domains of knowledge have their own specialized terminology. Appropriate use of domain-specific terms enables humans to more easily associate knowledge within that field"

### Mechanism 3
- Claim: Multi-MLLM collaboration with role specialization improves translation quality and knowledge retention.
- Mechanism: By using separate Translator and Speaker LLMs with specialized capabilities, the system can better preserve both LTs and DTs during the translation-answering process.
- Core assumption: Different LLMs have complementary strengths that can be leveraged through role specialization.
- Evidence anchors:
  - [abstract] "we propose Native Language Prompting (NatLan), employing a Multi-MLLM collaboration strategy and introducing an additional role-enhanced domain-specific MLLM with stronger multilingual understanding capabilities as the translator"
  - [section] "In order to allow each MLLM to fully leverage its unique advantages, previous work has proposed using multiple MLLMs to fulfill distinct roles within a collaborative framework"

## Foundational Learning

- Concept: Multilingual language model capabilities
  - Why needed here: Understanding how MLLMs perform differently across languages is fundamental to why translation-based approaches work
  - Quick check question: Why do MLLMs typically perform better on questions in their dominant language compared to non-dominant languages?

- Concept: Domain-specific knowledge representation
  - Why needed here: The paper's core innovation relies on understanding how domain-specific terminology helps knowledge retrieval
  - Quick check question: How might using precise domain-specific terms versus generic terms affect a model's ability to retrieve relevant knowledge?

- Concept: Multi-agent collaboration systems
  - Why needed here: The proposed solution uses multiple specialized models working together, requiring understanding of collaborative AI systems
  - Quick check question: What are the potential benefits and drawbacks of using multiple specialized models versus a single general model for complex tasks?

## Architecture Onboarding

- Component map: User Interface -> Translator LLM -> Speaker LLM -> Answer
- Critical path:
  1. User submits question in non-dominant language
  2. Translator LLM converts question to native language while preserving domain-specific terminology
  3. Domain context is injected into both LLMs
  4. Speaker LLM answers question using native language knowledge
  5. Answer is returned to user
- Design tradeoffs:
  - Single LLM vs Multi-LLM: Multi-LLM approach requires more coordination but can leverage specialized capabilities
  - Generic vs Domain-specific: Domain-specific prompting improves accuracy but requires more setup
  - Cost vs Performance: Using more capable Translator LLMs improves results but increases computational cost
- Failure signatures:
  - Poor translation quality: Answers are incorrect or nonsensical
  - Domain context loss: Generic translations miss important specialized terminology
  - Role confusion: LLMs attempt tasks they're not specialized for
  - Communication errors: Information doesn't flow correctly between components
- First 3 experiments:
  1. Test baseline performance with direct answering in non-dominant language
  2. Test translation quality by comparing original vs translated questions for domain-specific terminology preservation
  3. Test role specialization by comparing performance of single LLM vs Multi-LLM approach on same questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of parameters for Translator LLMs to balance translation quality and computational efficiency in NatLan?
- Basis in paper: [explicit] The paper compares Qwen models with 4B, 7B, and 14B parameters, noting that 7B and 14B show comparable performance while 4B is weaker, suggesting a parameter threshold effect
- Why unresolved: The paper only tests three specific parameter sizes of one model family. It's unclear whether these findings generalize across different model architectures or whether even smaller/bigger models might yield better trade-offs
- What evidence would resolve it: Systematic experiments varying parameter counts across multiple model families (e.g., Phi, Gemma, Mistral) while measuring both translation quality (DT retention) and computational costs would identify optimal configurations

### Open Question 2
- Question: How does NatLan perform on languages that are typologically distant from English but not included in the five tested languages?
- Basis in paper: [inferred] The paper selects five languages representing different language families but acknowledges this may not capture all typological variations. The discussion of "Language-Knowledge Bound Disciplines" hints at language-specific knowledge patterns
- Why unresolved: The study only covers Arabic, Chinese, French, German, and Japanese. Many other language families (e.g., Turkic, Dravidian, Niger-Congo) remain untested, leaving uncertainty about generalizability
- What evidence would resolve it: Testing NatLan on additional languages from underrepresented families (e.g., Turkish, Hindi, Swahili) while comparing performance patterns to the original five would reveal broader applicability limits

### Open Question 3
- Question: Can NatLan be adapted to work effectively when neither the source nor target language is the model's native language?
- Basis in paper: [explicit] The paper explicitly states all Speaker LLMs use English as their native language and acknowledges this limitation, noting that most existing MLLMs are trained primarily on English corpora
- Why unresolved: The current framework assumes English as the translation target. The paper suggests developing MLLMs with different native languages could be impactful, but doesn't explore how to adapt the framework for non-English-native models
- What evidence would resolve it: Implementing NatLan with Speaker LLMs whose native language is non-English (e.g., Chinese-native Qwen models) and comparing performance to the English-native setup would demonstrate framework flexibility and identify new optimization strategies

## Limitations

- The core assumption that MLLMs have significantly more knowledge encoded in their dominant language lacks direct empirical validation
- The Multi-MLLM collaboration approach introduces complexity that could lead to error propagation between components
- The proposed theoretical framework of Language Triggers and Domain Triggers needs more rigorous testing to validate the mechanism-based explanations

## Confidence

- **High confidence**: The experimental results showing quantitative improvements over baselines (accuracy gains, DT retention rates)
- **Medium confidence**: The theoretical framework of LTs and DTs as explanatory mechanisms for translate-then-answer methods
- **Low confidence**: The generalizability of results across different model architectures and the long-term effectiveness of the proposed approach

## Next Checks

1. Conduct ablation studies removing either the Translator or Speaker LLM to quantify the marginal benefit of the Multi-MLLM approach versus single-model solutions.
2. Test the framework with non-English dominant languages as the native language to validate that the approach works beyond English-centric assumptions.
3. Perform error analysis on cases where NatLan underperforms to identify specific failure patterns and their root causes.