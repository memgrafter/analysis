---
ver: rpa2
title: Use of What-if Scenarios to Help Explain Artificial Intelligence Models for
  Neonatal Health
arxiv_id: '2410.09635'
source_url: https://arxiv.org/abs/2410.09635
tags:
- data
- aimen
- labor
- risk
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of early detection of intrapartum
  risk to prevent adverse labor outcomes like cerebral palsy. The authors propose
  "Artificial Intelligence for Modeling and Explaining Neonatal Health" (AIMEN), a
  deep learning framework that predicts adverse labor outcomes from maternal, fetal,
  obstetrical, and intrapartum risk factors while providing explanations for its predictions.
---

# Use of What-if Scenarios to Help Explain Artificial Intelligence Models for Neonatal Health

## Quick Facts
- arXiv ID: 2410.09635
- Source URL: https://arxiv.org/abs/2410.09635
- Reference count: 40
- Primary result: AIMEN achieves 0.784 average F1 score in predicting high-risk neonatal outcomes

## Executive Summary
This study addresses the challenge of early detection of intrapartum risk to prevent adverse labor outcomes like cerebral palsy. The authors propose "Artificial Intelligence for Modeling and Explaining Neonatal Health" (AIMEN), a deep learning framework that predicts adverse labor outcomes from maternal, fetal, obstetrical, and intrapartum risk factors while providing explanations for its predictions. AIMEN uses an ensemble of fully-connected neural networks as its classification backbone, with data augmentation supported by either ADASYN or CTGAN to address small and imbalanced datasets. The framework generates counterfactual explanations showing how changing 2-3 attributes could flip abnormal predictions to normal.

## Method Summary
AIMEN employs an ensemble of fully-connected neural networks trained on maternal, fetal, obstetrical, and intrapartum risk factors to predict adverse labor outcomes. The system uses data augmentation with CTGAN or ADASYN to address the small, imbalanced dataset (112 positive cases out of 1457 total). After training multiple neural networks on different folds, ensemble voting is applied where only qualified classifiers (F1 > 0.7) contribute to final predictions. Counterfactual explanations are generated using a nearest instance method that identifies real examples from the opposite class closest to the current prediction.

## Key Results
- AIMEN with CTGAN support achieved an average F1 score of 0.784 in predicting high-risk cases
- The system generates counterfactual explanations requiring changes to 2-3 attributes on average to flip abnormal predictions
- CTGAN-supported AIMEN outperformed the ADASYN-supported version in classification performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation with CTGAN improves model performance by expanding training data while preserving feature relationships
- Mechanism: CTGAN generates synthetic examples that fill gaps in minority class representation, allowing neural networks to learn more robust decision boundaries
- Core assumption: Generated synthetic data follows the same statistical distribution as real data
- Evidence anchors:
  - [abstract] "AIMEN, supported by CTGAN, outperforms AIMEN supported by ADASYN in classification"
  - [section] "Data generation with CTGAN allows specifying the categorical variables and the generated values for those variables will be integer values of 0 and 1"
  - [corpus] Weak - no direct evidence about CTGAN effectiveness found in corpus
- Break condition: Generated data deviates significantly from real distribution, causing distribution gap to increase

### Mechanism 2
- Claim: Ensemble voting with qualified classifiers improves robustness and generalization
- Mechanism: Multiple neural networks trained on different folds vote on test predictions, with only high-performing models (F1 > 0.7) contributing
- Core assumption: Different folds capture diverse aspects of the data distribution
- Evidence anchors:
  - [section] "AIMEN uses an ensemble of fully-connected neural networks as the backbone for its classification"
  - [section] "Eight neural networks of the same architecture...are trained and validated on eight different folds"
  - [corpus] Weak - no direct evidence about ensemble voting found in corpus
- Break condition: High variance among ensemble members indicates inconsistent learning

### Mechanism 3
- Claim: Counterfactual explanations provide actionable insights by identifying minimum feature changes needed to alter predictions
- Mechanism: Nearest instance counterfactual algorithm finds real examples from opposite class that are closest to current example, highlighting modifiable risk factors
- Core assumption: Changes suggested by counterfactuals are clinically actionable
- Evidence anchors:
  - [abstract] "AIMEN also provides counterfactual explanations that can be achieved by changing 2 to 3 attributes on average"
  - [section] "CEs were generated for each abnormal class example from the test set to identify the major contributors to the high risk of adverse labor outcomes"
  - [corpus] Weak - no direct evidence about counterfactual explanations found in corpus
- Break condition: Suggested changes require unrealistic or clinically impossible modifications

## Foundational Learning

- Concept: Imbalanced classification
  - Why needed here: Dataset has only 112 positive cases out of 1457 total examples
  - Quick check question: What performance metric is most appropriate when dealing with highly imbalanced datasets?

- Concept: Data augmentation with generative models
  - Why needed here: Small dataset size limits model learning capacity
  - Quick check question: How does CTGAN differ from traditional data augmentation techniques?

- Concept: Counterfactual explanations
  - Why needed here: Need to provide actionable insights for clinical decision-making
  - Quick check question: What makes a counterfactual explanation clinically useful versus just theoretically interesting?

## Architecture Onboarding

- Component map: Data generation (CTGAN) → Ensemble classification (MLP_v5) → Counterfactual explanation → Evaluation metrics
- Critical path: Training data → CTGAN generation → Ensemble training → Prediction → Counterfactual generation
- Design tradeoffs: Using synthetic data improves training but requires careful validation to ensure distribution match
- Failure signatures: High distribution gap indicates synthetic data doesn't match real data; low sensitivity indicates model misses actual high-risk cases
- First 3 experiments:
  1. Train baseline model without data augmentation to establish performance floor
  2. Add CTGAN data generation and compare distribution gap and classification metrics
  3. Implement ensemble voting and test impact on sensitivity and average F1 score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal synthetic data generation parameters for maximizing classification performance on neonatal health datasets?
- Basis in paper: [explicit] The paper discusses comparing CTGAN and ADASYN methods, testing different restrictions on CTGAN (R-AIMEN with silhouette score requirements), and varying data augmentation strategies
- Why unresolved: While the paper shows CTGAN without restrictions performs best, it doesn't exhaustively explore all possible parameter combinations or alternative generative models
- What evidence would resolve it: Systematic comparison of CTGAN variants with different hyperparameter settings, comparison with additional generative models (e.g., VAEs, normal distributions), and ablation studies showing impact of specific parameters

### Open Question 2
- Question: How does the AIMEN system's performance change when predicting different specific adverse neonatal outcomes beyond the composite endpoint used?
- Basis in paper: [inferred] The paper mentions that CP is only one of many possible adverse outcomes and notes that "more work will be needed to develop better classifiers associated with specific outcomes" and mentions future plans to "fine-tune the system for other adverse outcomes such as NICU admission"
- Why unresolved: The current study uses a composite outcome (CP = True or Apgar ≤ 3 or pH ≤ 7.05), which may mask performance differences for specific outcomes
- What evidence would resolve it: Training and evaluating AIMEN on datasets with clearly defined, specific adverse outcomes, comparing performance metrics for each outcome separately

### Open Question 3
- Question: What is the clinical utility of the counterfactual explanations provided by AIMEN in real-time decision-making scenarios?
- Basis in paper: [explicit] The paper discusses providing "what-if scenarios" showing how changing 2-3 attributes could flip predictions, and mentions that "physicians are often skeptical about AI/ML approaches in medicine"
- Why unresolved: The paper provides examples of counterfactual explanations but doesn't evaluate whether clinicians find them actionable or useful in practice
- What evidence would resolve it: Clinical validation studies where obstetricians use AIMEN's counterfactual explanations in simulated or real clinical scenarios, measuring decision quality, time to decision, and perceived usefulness

### Open Question 4
- Question: How would more complex neural network architectures (beyond fully connected networks) perform on this tabular neonatal health data?
- Basis in paper: [explicit] The paper states "The scope of the study for classifiers is limited to fully connected neural networks" and mentions that "a comparison with classical non-delective learning methods can help us understand if the performance of those models can be improved"
- Why unresolved: The study only tests five variations of fully connected networks, while the literature suggests other architectures like TabNet, DANETS, or even vision transformers applied to tabular data might perform better
- What evidence would resolve it: Head-to-head comparison of AIMEN's MLP backbone with state-of-the-art tabular data architectures, including XGBoost, TabNet, and transformer-based models, using identical data augmentation strategies

## Limitations
- The effectiveness of CTGAN-generated data in capturing complex relationships between risk factors remains uncertain
- The clinical feasibility of suggested counterfactual attribute changes hasn't been validated
- The small dataset size (1457 cases with only 112 positive cases) may limit generalizability

## Confidence
- High Confidence: Ensemble classification approach with qualified classifiers (F1 > 0.7) is methodologically sound
- Medium Confidence: CTGAN data augmentation's contribution to improved performance is plausible but lacks direct validation
- Low Confidence: Clinical actionability of counterfactual explanations remains unproven

## Next Checks
1. Calculate and compare the statistical distribution gap between real and CTGAN-generated data to quantify how well the synthetic data matches the original dataset's characteristics

2. Have domain experts review the counterfactual explanations to assess whether the suggested attribute changes are clinically realistic and actionable in labor and delivery settings

3. Perform k-fold cross-validation (k > 8) to test the stability of the ensemble approach and ensure the model performance isn't dependent on the specific train/test split used in the original study