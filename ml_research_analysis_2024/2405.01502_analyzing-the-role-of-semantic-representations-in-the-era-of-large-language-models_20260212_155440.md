---
ver: rpa2
title: Analyzing the Role of Semantic Representations in the Era of Large Language
  Models
arxiv_id: '2405.01502'
source_url: https://arxiv.org/abs/2405.01502
tags:
- text
- representation
- performance
- llms
- amrcot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether semantic representations like AMR
  help large language models (LLMs) when no training is involved. The authors propose
  a theoretical framework formalizing representation power and conduct experiments
  across five NLP tasks comparing AMR-driven chain-of-thought prompting (AMRCOT) to
  baseline LLM prompting.
---

# Analyzing the Role of Semantic Representations in the Era of Large Language Models

## Quick Facts
- arXiv ID: 2405.01502
- Source URL: https://arxiv.org/abs/2405.01502
- Reference count: 40
- Results show AMR causes slight performance fluctuations (-3 to +1 percentage points) with no clear overall benefit for LLMs

## Executive Summary
This paper investigates whether semantic representations like Abstract Meaning Representation (AMR) help large language models (LLMs) when no training is involved. Through a theoretical framework formalizing representation power and experiments across five NLP tasks, the authors find that while AMR changes predictions in significant subsets of cases (up to 37%), improvements are offset by harm to other examples. The study reveals that errors particularly occur with multi-word expressions, named entities, and the final inference step connecting AMR reasoning to predictions. The authors recommend focusing future work on improving LLMs' ability to map AMR representations to outputs rather than AMR parsing itself.

## Method Summary
The study compares baseline LLM prompting to AMR-driven chain-of-thought prompting (AMRCOT) across five NLP tasks using GPT-3.5-turbo-0613 and GPT-4. The method involves generating AMRs with a BART-based parser, creating prompts for each task that include both text and AMR representations, and evaluating performance differences. Experiments use zero-shot prompting without fine-tuning, comparing AMRCOT against baseline conditions across paraphrase detection, machine translation, logical fallacy detection, event extraction, and text-to-SQL generation tasks.

## Key Results
- AMR causes slight performance fluctuations (-3 to +1 percentage points) with no clear overall benefit
- AMR changes predictions in significant subsets of cases (up to 37%), but improvements are offset by harm to other examples
- Errors particularly occur with multi-word expressions, named entities, and the final inference step connecting AMR reasoning to predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AMR's intermediate representation doesn't inherently improve LLM performance because LLMs are pretrained on free-form text, making text the more natural representation for them.
- Mechanism: The theoretical framework shows that in the LLM era, representation power shifts from minimizing computation complexity to minimizing prediction error with a fixed LLM. Since LLMs are pretrained on raw text, text representations align better with their internal representations than structured AMR graphs.
- Core assumption: The LLM's pretraining data distribution and architecture make text representations more compatible with its learned transformations than structured semantic representations.
- Evidence anchors:
  - [abstract] "we find that it is difficult to predict which input examples AMR may help or hurt on, but errors tend to arise with multi-word expressions, named entities, and in the final inference step"
  - [section 2.4] "the ideal best representation r* is not necessarily equal to the representation r* LLM that works well with LLMs"
  - [corpus] Weak - neighboring papers focus on AMR applications but don't directly address LLM compatibility

### Mechanism 2
- Claim: AMR helps on a subset of examples but this benefit is offset by harm to other examples, resulting in no net performance gain.
- Mechanism: AMR's explicit semantic structure helps LLMs in cases requiring complex semantic reasoning by providing clearer entity and relation information. However, AMR's limitations with multi-word expressions and named entities cause errors that cancel out these gains.
- Core assumption: The subset of examples where AMR helps differs systematically from those where it harms, creating a balance that prevents overall improvement.
- Evidence anchors:
  - [abstract] "AMR causes slight performance fluctuations (-3 to +1 percentage points)" and "While AMR changes predictions in a significant subset of cases (up to 37%), improvements are offset by harm to other examples"
  - [section 5.1] "AMR has its unique advantages and limitations... One such limitation of AMR is its lack of ability to capture MWEs such as idiomatic expressions"
  - [corpus] Moderate - the neighboring paper "When Does Meaning Backfire? Investigating the Role of AMRs in NLI" directly addresses when AMR helps/harms

### Mechanism 3
- Claim: The key bottleneck is the LLM's ability to map AMR representations to task outputs, not the quality of AMR parsing itself.
- Mechanism: Even when using gold AMRs, performance gains over parser-generated AMRs are minimal, suggesting that improving the LLM's AMR-to-output mapping is more important than improving parsing accuracy.
- Core assumption: The LLM's reasoning over AMR is the limiting factor, and this limitation persists even with perfect AMR input.
- Evidence anchors:
  - [section 6.1] "Both lead to similar results, with a difference of less than two percentage points" when comparing gold vs parser-generated AMRs
  - [section 6.3] "Even though GPT-4 was able to correctly enumerate the relevant features of the AMRs, it still had trouble synthesizing this information into a correct paraphrasing judgment"
  - [corpus] Weak - no direct corpus evidence about AMR-to-output mapping bottlenecks

## Foundational Learning

- Concept: Kolmogorov complexity and its role in representation power
  - Why needed here: The paper's theoretical framework relies on understanding how representation complexity affects learning and generalization
  - Quick check question: If representation A requires a 100-line program to compute outputs from inputs, and representation B requires 50 lines, which has stronger representation power under the pre-LLM formulation?

- Concept: Chain-of-thought prompting and its variants
  - Why needed here: The AMRCOT method builds on CoT prompting by using AMR as the intermediate reasoning step instead of free-form text
  - Quick check question: What is the key difference between standard CoT prompting and AMRCOT in terms of the intermediate representation provided?

- Concept: Semantic representations and AMR specifically
  - Why needed here: Understanding AMR's structure (entities, events, relations as graphs) is crucial for interpreting why it may or may not help LLMs
  - Quick check question: What are the three main components of an AMR graph, and how does this differ from the sequential nature of text?

## Architecture Onboarding

- Component map: Text → AMR parser → LLM reasoning → output prediction
- Critical path: The bottleneck is at the LLM reasoning stage where it must map AMR to outputs
- Design tradeoffs: Using AMR provides explicit semantic structure but introduces parsing errors and representation mismatch; using text alone avoids these issues but may require more complex reasoning from the LLM
- Failure signatures: Performance fluctuations around zero (no net gain), specific failure modes with MWEs and named entities, and difficulty predicting when AMR will help vs hurt
- First 3 experiments:
  1. Replicate the zero-shot comparison between BASE and AMRCOT on one dataset to verify the marginal effect
  2. Test the gold vs parser-generated AMR comparison on the AMR-NER dataset to confirm the mapping bottleneck
  3. Conduct the text/AMR ablation study on WMT to verify that text has greater utility as a representation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we design intermediate representations that are better suited for LLM reasoning than AMR, potentially combining semantic structure with LLM-friendly text patterns?
- Basis in paper: [explicit] The authors show that while AMR helps on some samples, it overall causes slight performance fluctuations and is particularly challenging for LLMs to use effectively. They recommend improving LLMs' ability to map AMR representations to outputs rather than AMR parsing itself.
- Why unresolved: The paper demonstrates that AMR, despite being designed to efficiently represent semantic features, does not translate well to LLM performance improvements. This suggests a gap between traditional semantic representations and what LLMs can effectively process.
- What evidence would resolve it: Experiments comparing LLM performance across various intermediate representations (semantic, syntactic, hybrid) would reveal whether AMR's limitations are specific to its structure or indicative of broader challenges in using formal representations with LLMs.

### Open Question 2
- Question: Can we predict which input examples will benefit from AMR assistance, and if so, what linguistic features or patterns would enable such prediction?
- Basis in paper: [explicit] The authors find that AMR changes predictions for a significant subset of cases (up to 37%) but that the helpfulness is difficult to predict even for BERT models, suggesting either more data is needed or the changes correspond to unpredictable noise.
- Why unresolved: Despite analyzing linguistic features and training classifiers, the authors conclude that AMR helpfulness remains challenging to predict, indicating either a need for more sophisticated feature extraction or that the effect is inherently stochastic.
- What evidence would resolve it: Developing a highly accurate predictor of AMR helpfulness (e.g., >80% F1 score) would demonstrate that specific linguistic patterns or input characteristics determine when AMR is beneficial.

### Open Question 3
- Question: Would training an LLM specifically optimized for AMR as an intermediate representation significantly improve performance compared to zero-shot prompting?
- Basis in paper: [explicit] The authors note that their theoretical framework suggests the ideal representation for a task is not necessarily ideal for LLMs, and they call for future work to explore training LLMs specifically for AMR representations.
- Why unresolved: All experiments in the paper use zero-shot prompting with out-of-the-box LLMs, leaving open whether fine-tuning or training from scratch on AMR could unlock its potential.
- What evidence would resolve it: A comprehensive comparison between zero-shot AMRCOT and an LLM trained or fine-tuned specifically for AMR reasoning across multiple tasks would reveal whether training can overcome the representation mismatch.

## Limitations

- Experiments were conducted with zero-shot prompting only, without any fine-tuning of LLMs or AMR parsers
- Analysis focused on five specific NLP tasks, and results may not generalize to other domains or task types
- Study used a single AMR parser without exploring alternative parsing approaches or their impact on results

## Confidence

- High confidence: The observation that AMR provides no clear overall performance benefit across tasks
- Medium confidence: The finding that AMR changes predictions in significant subsets of cases but with improvements offset by harm to other examples
- Medium confidence: The identification of specific error patterns with multi-word expressions, named entities, and final inference steps

## Next Checks

1. Replicate the zero-shot comparison on additional NLP tasks beyond the five studied to determine if the lack of AMR benefit is task-specific or more general

2. Compare results using different AMR parsing approaches (rule-based vs neural, different training data) to determine if parser quality significantly affects the observed outcomes

3. Conduct experiments with minimal fine-tuning of either the LLM or AMR parser to assess whether adaptation could overcome the mapping bottleneck identified in the current zero-shot setup