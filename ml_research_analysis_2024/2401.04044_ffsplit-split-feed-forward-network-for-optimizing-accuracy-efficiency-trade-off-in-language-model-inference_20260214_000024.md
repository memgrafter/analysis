---
ver: rpa2
title: 'FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off
  in Language Model Inference'
arxiv_id: '2401.04044'
source_url: https://arxiv.org/abs/2401.04044
tags:
- heavy
- arxiv
- neurons
- language
- hitters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying large language models
  (LLMs) on resource-constrained hardware by optimizing the accuracy-efficiency trade-off
  in model compression. The authors observe that only a few neurons in the feed-forward
  network (FFN) of LLMs, termed "heavy hitters," have large output norms for most
  input tokens, while others are sparsely activated.
---

# FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference

## Quick Facts
- **arXiv ID:** 2401.04044
- **Source URL:** https://arxiv.org/abs/2401.04044
- **Reference count:** 35
- **Primary result:** 43.1% model size reduction with 1.25-1.56× wall-clock time speedup on BERT and OPT models

## Executive Summary
This paper addresses the challenge of deploying large language models on resource-constrained hardware by optimizing the accuracy-efficiency trade-off in model compression. The authors observe that only a few neurons in the feed-forward network (FFN) of LLMs, termed "heavy hitters," have large output norms for most input tokens while others are sparsely activated. Leveraging this observation, they propose FFSplit, which explicitly splits the FFN into two parts: one for heavy hitters and another for the remaining neurons. This allows for more resources to be allocated to the heavy hitter part during compression, improving the accuracy-efficiency trade-off. Experimental results show that FFSplit reduces model size by 43.1% and achieves 1.25-1.56× wall-clock time speedup on different hardware with negligible accuracy drop.

## Method Summary
FFSplit identifies "heavy hitter" neurons in each FFN layer by analyzing their output norms across the training set, selecting the top 25% of neurons with the largest norms. The FFN weight matrices U and V are then split into two parts: U1,V1 for heavy hitters and U2,V2 for the remaining neurons. Low-rank decomposition (SVD with rank 10% of full rank) is applied only to the non-heavy-hitter part (U2,V2), while the heavy-hitter part (U1,V1) is preserved. The compressed model is then fine-tuned for a few epochs. This selective compression approach allows for aggressive compression on less important neurons while preserving resources for the critical heavy hitters, resulting in better accuracy-efficiency trade-offs compared to vanilla compression methods.

## Key Results
- 43.1% reduction in model size compared to baseline compressed models
- 1.25-1.56× wall-clock time speedup across different hardware platforms
- Negligible accuracy drop (0.3-1.0%) on GLUE benchmark tasks
- Improved efficiency-accuracy trade-off compared to vanilla compression methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heavy hitter neurons exist in GeLU-based transformers and are crucial for model accuracy
- Mechanism: A small subset of neurons in each FFN layer consistently produce large output norms across most input tokens, making them critical for model performance. These neurons are identified through training set analysis based on their output norm magnitude.
- Core assumption: The output norm of neurons can be used as a proxy for their importance to model performance
- Evidence anchors:
  - [abstract] "we first observe that only a few neurons of FFN module have large output norm for any input tokens, a.k.a. heavy hitters"
  - [section 4.1] "we sort the neurons based on their output norm at each layer" and "a few neurons have very large ∥σ(XU :,j)∥2 F"
  - [corpus] Weak evidence - corpus focuses on different sparsity patterns
- Break condition: If neuron importance cannot be reliably measured through output norms, or if heavy hitters are not consistently important across different tasks

### Mechanism 2
- Claim: Splitting FFN into heavy hitter and non-heavy hitter parts improves compression efficiency
- Mechanism: By explicitly separating FFN weights into two parts based on heavy hitter identification, compression methods can apply more aggressive compression to the less important part while preserving resources for the critical heavy hitter neurons
- Core assumption: Resource allocation based on neuron importance improves overall accuracy-efficiency tradeoff
- Evidence anchors:
  - [abstract] "we explicitly split the FFN into two parts according to the heavy hitters. We improve the efficiency-accuracy trade-off"
  - [section 4.2] "we explicitly split FFN module into two parts according to the set of heavy hitters"
  - [section 4.1] "we uniformly remove the top-3% 'heavy hitter' and 'light hitter' neurons... heavy hitters matter for model performance"
- Break condition: If the split introduces computational overhead that negates efficiency gains, or if the two-part structure interferes with optimization

### Mechanism 3
- Claim: FFSplit achieves better wall-clock time speedup than vanilla compression
- Mechanism: The split structure allows hardware-friendly dense matrix operations on both parts while enabling more aggressive compression on the less important part, resulting in faster inference
- Core assumption: Dense matrix operations on split parts are more hardware-efficient than alternative sparse formats
- Evidence anchors:
  - [abstract] "our method can reduce model size by 43.1% and bring 1.25 ∼ 1.56× wall clock time speedup"
  - [section 4.2] "It still use dense matrix format to do the computation, and thus to be hardware-friendly for potentially obtaining wall-clock time speedup"
  - [corpus] No direct evidence - corpus neighbors focus on different optimization approaches
- Break condition: If hardware optimization techniques change to favor different matrix formats, or if split introduces memory overhead

## Foundational Learning

- Concept: Feed-forward network (FFN) structure in transformers
  - Why needed here: Understanding FFN as the computational bottleneck is crucial for grasping the paper's optimization target
  - Quick check question: What percentage of total parameters and inference latency does the FFN component typically account for in transformer models?

- Concept: Matrix decomposition and low-rank approximation
  - Why needed here: The paper uses low-rank decomposition as a compression technique applied selectively to FFN parts
  - Quick check question: How does low-rank decomposition reduce the number of parameters in a weight matrix?

- Concept: Neuron activation patterns and sparsity
  - Why needed here: The heavy hitter phenomenon relies on understanding how different neurons are activated across input tokens
  - Quick check question: What is the difference between activation sparsity and the "heavy hitter" phenomenon described in this paper?

## Architecture Onboarding

- Component map:
  - Input tensor X ∈ ℝ^(s×d) flows through FFN layer
  - Up-projection layer U ∈ ℝ^(d×df f) maps to hidden dimension
  - ReLU/GeLU activation function σ(·) applied element-wise
  - Down-projection layer V ∈ ℝ^(df f×d) maps back to output dimension
  - Split creates U1,V1 (heavy hitters) and U2,V2 (remaining neurons)

- Critical path:
  1. Identify heavy hitters by analyzing training data output norms
  2. Split FFN matrices into two parts based on heavy hitter identification
  3. Apply compression selectively to non-heavy hitter part
  4. Fine-tune compressed model

- Design tradeoffs:
  - Memory overhead of maintaining split structure vs. compression gains
  - Complexity of identifying heavy hitters vs. potential accuracy improvement
  - Hardware efficiency of dense operations on split parts vs. sparse alternatives

- Failure signatures:
  - Accuracy degradation exceeding acceptable thresholds
  - Wall-clock time increase instead of expected speedup
  - Memory usage increase preventing deployment on target hardware

- First 3 experiments:
  1. Verify heavy hitter phenomenon exists in target model architecture by computing neuron output norms across training set
  2. Test accuracy impact of removing top/bottom percentage of neurons by output norm
  3. Compare compression ratio and accuracy between FFSplit and vanilla compression methods on small model

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The heavy hitter identification mechanism relies on output norm as a proxy for neuron importance, but the stability of this proxy across different domains and model scales remains unclear
- The 25% threshold for heavy hitters appears empirically chosen without systematic sensitivity analysis
- The reported wall-clock time speedups are measured on unspecified hardware configurations, making generalization to different deployment scenarios uncertain
- The paper lacks comparison with state-of-the-art structured pruning methods that could achieve similar efficiency gains

## Confidence
- Heavy hitter phenomenon identification: Medium - Supported by empirical observations but limited cross-task validation
- Split FFN structure improving compression efficiency: High - Demonstrated through controlled experiments with clear baselines
- Hardware-friendly dense matrix operations: Medium - Theoretical justification provided but hardware-specific benchmarks missing
- 43.1% size reduction with 1.25-1.56× speedup: Low-Medium - Results reported but hardware details and statistical significance tests absent

## Next Checks
1. Perform cross-task transferability test: Apply heavy hitter identification from GLUE-trained BERT to evaluate on summarization or question answering tasks to verify if the same neurons remain important
2. Hardware benchmarking: Measure actual wall-clock time on representative edge devices (e.g., Raspberry Pi, mobile CPU) with varying memory bandwidths to validate claimed speedups
3. Sensitivity analysis: Systematically vary the heavy hitter percentage threshold (10%, 15%, 20%, 30%, 40%) and measure accuracy-efficiency trade-offs to identify optimal split ratios for different model scales