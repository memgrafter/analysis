---
ver: rpa2
title: 'Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and
  What to Retrieve for LLMs'
arxiv_id: '2402.12052'
source_url: https://arxiv.org/abs/2402.12052
tags:
- retrieval
- question
- query
- answer
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SlimPLM, a novel method that uses a smaller
  proxy model to determine when and what to retrieve for large language models (LLMs).
  The core idea is to generate a "heuristic answer" using the proxy model and then
  use it to judge the necessity of retrieval and formulate precise queries.
---

# Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs

## Quick Facts
- arXiv ID: 2402.12052
- Source URL: https://arxiv.org/abs/2402.12052
- Reference count: 37
- Key outcome: SlimPLM uses a smaller proxy model to determine when and what to retrieve for LLMs, achieving state-of-the-art performance with reduced computational costs

## Executive Summary
This paper introduces SlimPLM, a novel method that uses a smaller proxy model to determine when and what to retrieve for large language models (LLMs). The core idea is to generate a "heuristic answer" using the proxy model and then use it to judge the necessity of retrieval and formulate precise queries. This approach reduces computational costs while improving retrieval quality. Experiments on five QA datasets show that SlimPLM achieves state-of-the-art performance, outperforming existing methods in both accuracy and efficiency.

## Method Summary
SlimPLM uses a smaller proxy model (e.g., Llama2-7B-Chat) to generate heuristic answers for user questions. These heuristic answers are then used by a retrieval necessity judgment model to determine if retrieval is needed. If retrieval is necessary, a query rewriting model decomposes the heuristic answer into claims and generates precise queries. A claim-based filter selects which queries actually need retrieval by checking if the LLM would know those specific claims. Finally, retrieved documents are passed to the target LLM (e.g., Llama2-70B-Chat) to generate the final answer.

## Key Results
- SlimPLM achieves state-of-the-art performance on five QA datasets
- Outperforms existing methods in both accuracy and efficiency
- Reduces computational costs by intelligently deciding when retrieval is unnecessary

## Why This Works (Mechanism)

### Mechanism 1
Heuristic answers from smaller proxy models can accurately signal when LLMs need retrieval. The proxy model generates a preliminary answer that serves as a knowledge proxy. If this answer has high quality (measured by matching ratio to ground truth), it indicates the LLM likely possesses sufficient knowledge to answer without retrieval.

### Mechanism 2
Decomposing heuristic answers into claims enables more precise query formulation than using the original question. The proxy model's answer is decomposed into multiple factual claims, each generating a focused query. A claim-based filter then determines which queries actually need retrieval.

### Mechanism 3
The consensus between proxy model and LLM knowledge mastery can be quantified and leveraged. By comparing exact match scores between proxy model and LLM on the same questions, we can identify knowledge areas where they agree versus disagree. Retrieval is only triggered for questions where both models show low mastery.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG) architecture**
  - Why needed here: SlimPLM is a refinement of RAG that optimizes when and what to retrieve
  - Quick check question: What are the three main components of a typical RAG system and their roles?

- **Concept: Exact Match (EM) evaluation metric**
  - Why needed here: SlimPLM uses EM scores to measure knowledge consensus between proxy and LLM
  - Quick check question: How does EM differ from other QA metrics like ROUGE when evaluating short-form answers?

- **Concept: Claim decomposition in fact verification**
  - Why needed here: SlimPLM borrows claim decomposition from fact verification to break down heuristic answers
  - Quick check question: What is the purpose of decomposing statements into atomic claims in fact verification systems?

## Architecture Onboarding

- **Component map**: User Question → Proxy Model → Heuristic Answer → Retrieval Necessity Judgment → (If needed) Query Rewriting → Claim-based Filter → Document Retrieval → Target LLM → Final Answer

- **Critical path**: User Question → Proxy Model → Heuristic Answer → Retrieval Necessity Judgment → (If needed) Query Rewriting → Claim-based Filter → Document Retrieval → Target LLM → Final Answer

- **Design tradeoffs**:
  - Using a smaller proxy model reduces computational cost but may introduce hallucination
  - Decomposing answers into claims increases retrieval precision but adds complexity
  - The consensus assumption works well for general knowledge but may fail for specialized domains

- **Failure signatures**:
  - Low retrieval success rate despite high retrieval necessity judgments → Proxy model hallucinations
  - High retrieval cost with minimal performance improvement → Overly aggressive claim decomposition
  - Target LLM generates irrelevant answers → Poor query formulation or retrieval results

- **First 3 experiments**:
  1. Ablation study removing query rewriting to measure its impact on retrieval precision
  2. Vary proxy model size (2B, 7B, 13B) to find optimal tradeoff between cost and judgment accuracy
  3. Test on out-of-domain datasets to evaluate consensus assumption robustness

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of SlimPLM vary with different sizes of proxy models?
- **Open Question 2**: Can SlimPLM be extended to handle more complex query types requiring multi-hop reasoning?
- **Open Question 3**: How does SlimPLM perform in real-world scenarios with noisy or ambiguous user queries?

## Limitations

- The consensus-based retrieval necessity judgment mechanism relies on the assumption that smaller and larger LLMs share overlapping knowledge distributions
- The claim decomposition strategy depends heavily on the proxy model generating high-quality heuristic answers with concrete factual claims
- The approach may not generalize well to specialized domains where training corpora differ significantly

## Confidence

- **High Confidence**: The effectiveness of using smaller proxy models for computational efficiency
- **Medium Confidence**: The consensus assumption between proxy and target LLM knowledge distributions
- **Medium Confidence**: The claim decomposition approach for improving retrieval precision
- **Low Confidence**: The generalizability of the approach to specialized domains and different knowledge distributions

## Next Checks

1. **Domain Transfer Experiment**: Test SlimPLM on specialized domain datasets (medical, legal, or technical) to validate whether the consensus assumption holds across different knowledge distributions.

2. **Proxy Model Size Sensitivity**: Systematically vary the proxy model size (2B, 7B, 13B) and measure the tradeoff between computational efficiency and retrieval judgment accuracy.

3. **Failure Mode Analysis**: Conduct detailed error analysis on cases where SlimPLM fails to retrieve necessary knowledge or retrieves irrelevant information, categorizing failures by type.