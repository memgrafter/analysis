---
ver: rpa2
title: 'CLEAR: Character Unlearning in Textual and Visual Modalities'
arxiv_id: '2410.18057'
source_url: https://arxiv.org/abs/2410.18057
tags:
- unlearning
- forget
- retain
- methods
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLEAR, the first open-source benchmark for
  multimodal machine unlearning (MMU), addressing the lack of evaluation frameworks
  for removing cross-modal information from large multimodal models. CLEAR contains
  200 fictitious individuals with 3,700 AI-generated images and corresponding question-answer
  pairs, enabling rigorous assessment of unlearning effectiveness across text, vision,
  and multimodal domains.
---

# CLEAR: Character Unlearning in Textual and Visual Modalities

## Quick Facts
- **arXiv ID**: 2410.18057
- **Source URL**: https://arxiv.org/abs/2410.18057
- **Reference count**: 40
- **Primary result**: CLEAR is the first open-source benchmark for multimodal machine unlearning, demonstrating that joint unlearning across text and vision modalities outperforms single-modality approaches.

## Executive Summary
This paper introduces CLEAR, the first open-source benchmark for multimodal machine unlearning (MMU), addressing the lack of evaluation frameworks for removing cross-modal information from large multimodal models. CLEAR contains 200 fictitious individuals with 3,700 AI-generated images and corresponding question-answer pairs, enabling rigorous assessment of unlearning effectiveness across text, vision, and multimodal domains. The authors evaluate 11 state-of-the-art unlearning methods (e.g., SCRUB, LLMU, DPO) and demonstrate that jointly unlearning both modalities significantly outperforms single-modality approaches. Their comprehensive analysis reveals that while some methods achieve strong retention performance, few balance forgetting and retention effectively in multimodal settings, highlighting the unique challenges of MMU and the need for specialized evaluation frameworks.

## Method Summary
The CLEAR benchmark introduces a synthetic dataset with 200 fictitious individuals, each represented by biographical text and corresponding AI-generated images. The evaluation uses LLaVa models with ViT visual encoders and LLaMA2-7B language models, fine-tuned on image captioning tasks. Eleven unlearning methods are tested across four evaluation splits: forget set (targeted individuals), retain set (general knowledge), real faces, and real-world scenarios. The framework employs multiple metrics including ROUGE-L for word-level correspondence, probability scores for answer generation capability, truth ratio for prediction-ground truth alignment, and forget quality using Jensen-Shannon distance to comprehensively assess unlearning effectiveness.

## Key Results
- Joint unlearning across text and vision modalities significantly outperforms single-modality approaches
- Most unlearning methods struggle with the trade-off between effective forgetting and knowledge retention in multimodal settings
- SCRUB and SCRUBbio show strong performance but don't perfectly balance forgetting and retention
- Single-modality unlearning can be inconsistent, with some methods showing dramatically different performance across text-only versus visual-only settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal unlearning requires joint optimization across both textual and visual modalities to achieve effective forgetting while maintaining retention.
- Mechanism: When unlearning a specific individual's information, the model must simultaneously erase both their biography (text) and associated images (vision) to prevent cross-modal retrieval. This prevents situations where removing text leaves visual associations intact or vice versa.
- Core assumption: Information about an individual is encoded across modalities and cannot be effectively unlearned by targeting only one modality.
- Evidence anchors:
  - [abstract]: "demonstrating that jointly unlearning both modalities outperforms single-modality approaches"
  - [section]: "This leaves a critical gap: multimodal LLMs (MLLMs), which process both visual and textual data, introduce unique challenges for unlearning"
  - [corpus]: Weak - related papers focus on single-modality unlearning, not joint multimodal approaches
- Break condition: If cross-modal representations are truly independent or if modality-specific encoders are completely separated, single-modality unlearning might suffice.

### Mechanism 2
- Claim: Transferability from unimodal to multimodal unlearning is limited due to distinct failure modes and performance degradation patterns.
- Mechanism: Methods that perform well in text-only or vision-only settings often fail catastrophically when applied to multimodal models because they don't account for the interaction between modalities and the preservation of cross-modal capabilities.
- Core assumption: Single-domain evaluation metrics and performance rankings don't predict multimodal effectiveness due to the added complexity of cross-modal information processing.
- Evidence anchors:
  - [abstract]: "comprehensive analysis of 11 MU methods... demonstrating that jointly unlearning both modalities outperforms single-modality approaches"
  - [section]: "Methods that perform well in single domains can fail catastrophically in multimodal settings"
  - [corpus]: Weak - most related work focuses on unimodal unlearning without addressing multimodal transfer limitations
- Break condition: If multimodal models use completely separate processing streams for each modality with minimal interaction, transfer might be more predictable.

### Mechanism 3
- Claim: The trade-off between forgetting effectiveness and retention quality is fundamentally different in multimodal settings compared to unimodal settings.
- Mechanism: Most unlearning methods struggle to balance removing specific information while preserving general capabilities when both modalities are involved. The methods that achieve perfect forgetting often completely destroy retention, while those that maintain retention struggle with effective forgetting.
- Core assumption: Multimodal models have more complex internal representations where forgetting one aspect can disproportionately affect other capabilities across modalities.
- Evidence anchors:
  - [abstract]: "while some methods achieve strong retention performance, few balance forgetting and retention effectively in multimodal settings"
  - [section]: "Most unlearning methods struggle with the trade-off between effective forgetting and knowledge retention in multimodal settings"
  - [corpus]: Weak - related papers don't extensively discuss the forgetting-retention trade-off in multimodal contexts
- Break condition: If forgetting mechanisms can be precisely targeted to specific information without affecting general capabilities, better balance might be achievable.

## Foundational Learning

- Concept: Cross-modal information encoding and retrieval
  - Why needed here: Understanding how multimodal models store and retrieve information across text and vision is crucial for designing effective unlearning methods
  - Quick check question: How does a multimodal model associate a person's name with their face, and why does removing one potentially require removing the other?

- Concept: Gradient-based optimization for knowledge modification
  - Why needed here: Most unlearning methods rely on gradient manipulation to either maximize loss on forget data or minimize divergence from pre-unlearning states
  - Quick check question: What's the difference between maximizing negative log-likelihood and maximizing entropy for unlearning, and when would each be appropriate?

- Concept: Evaluation metrics for unlearning effectiveness
  - Why needed here: Proper assessment requires multiple metrics (forget quality, retain performance, real-world capability preservation) to capture the multidimensional nature of unlearning success
  - Quick check question: Why can't we rely on a single metric like accuracy on forget set to evaluate unlearning effectiveness?

## Architecture Onboarding

- Component map:
  Visual encoder (ViT) → Projection layer → Language model (LLaMA2-7B) → Output

- Critical path:
  1. Fine-tune base model on full dataset (both modalities)
  2. Apply unlearning method on forget subset
  3. Evaluate using forget/retain/real-world splits
  4. Calculate ROUGE-L, Probability Score, Truth Ratio, and Forget Quality metrics

- Design tradeoffs:
  - Single-modality vs joint modality unlearning: Joint approaches generally better but computationally more expensive
  - Perfect forgetting vs retention preservation: Most methods sacrifice one for the other
  - Synthetic vs real data: Synthetic ensures controlled evaluation but may not capture all real-world complexities

- Failure signatures:
  - Catastrophic forgetting: Complete loss of retention capability (retain metric ≈ 0)
  - Ineffective forgetting: High forget metric despite unlearning attempts
  - Cross-modal degradation: Loss of capabilities in unrelated tasks after unlearning specific information

- First 3 experiments:
  1. Run baseline evaluation: Apply each unlearning method to a small forget set (2 people) and measure all metrics
  2. Modality ablation: For each method, test text-only, visual-only, and joint unlearning to identify modality-specific effects
  3. Trade-off analysis: Systematically vary hyperparameters to find the optimal balance between forget and retain metrics for each method

## Open Questions the Paper Calls Out
The paper identifies several open questions: (1) How do unlearning methods perform on multimodal models with larger architectures beyond LLaVA and QwenVL2? (2) What is the impact of unlearning on fairness and bias in multimodal models? (3) How do unlearning methods perform on real-world multimodal data compared to synthetic data? (4) Can analytical or mechanical unlearning approaches outperform fine-tuning-based methods in multimodal settings?

## Limitations
- The synthetic nature of CLEAR's dataset may not fully capture the complexities of real-world unlearning scenarios where information is more entangled
- Evaluation focuses primarily on short-answer generation tasks, potentially missing other multimodal capabilities that could be affected by unlearning procedures
- The paper focuses on fine-tuning-based unlearning methods, leaving other broader unlearning techniques (analytical, mechanical) unexplored

## Confidence
**High confidence**: The claim that CLEAR provides the first comprehensive benchmark for multimodal machine unlearning is well-supported by the systematic evaluation of 11 methods across multiple metrics and the clear demonstration of the forgetting-retention trade-off in multimodal settings.

**Medium confidence**: The assertion that joint modality unlearning significantly outperforms single-modality approaches, while supported by experimental results, may be partially influenced by the specific synthetic dataset construction and evaluation methodology used.

**Low confidence**: The generalizability of findings to real-world scenarios involving actual individuals and more diverse multimodal tasks remains uncertain due to the synthetic nature of the CLEAR dataset.

## Next Checks
1. Test the evaluated unlearning methods on a real-world dataset with actual individuals to assess whether the synthetic dataset results translate to practical scenarios involving genuine privacy concerns.

2. Extend the evaluation beyond short-answer generation to include other multimodal tasks such as visual question answering, image captioning quality, and cross-modal retrieval to ensure comprehensive capability assessment.

3. Investigate the computational overhead of joint modality unlearning compared to single-modality approaches across different model scales to understand practical deployment considerations.