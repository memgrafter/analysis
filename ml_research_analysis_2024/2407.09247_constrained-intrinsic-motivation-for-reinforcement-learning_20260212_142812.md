---
ver: rpa2
title: Constrained Intrinsic Motivation for Reinforcement Learning
arxiv_id: '2407.09247'
source_url: https://arxiv.org/abs/2407.09247
tags:
- state
- intrinsic
- tasks
- skill
- rfpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Constrained Intrinsic Motivation (CIM) tackles two core issues
  in intrinsic motivation: 1) skill discovery in reward-free pretraining, and 2) bias
  reduction in exploration with intrinsic rewards. The method introduces a constrained
  objective that maximizes the conditional state entropy while aligning state encoder
  outputs across latent skills.'
---

# Constrained Intrinsic Motivation for Reinforcement Learning

## Quick Facts
- arXiv ID: 2407.09247
- Source URL: https://arxiv.org/abs/2407.09247
- Authors: Xiang Zheng; Xingjun Ma; Chao Shen; Cong Wang
- Reference count: 8
- Primary result: CIM outperforms 15 baselines in skill diversity, state coverage, and fine-tuning efficiency across MuJoCo locomotion and manipulation tasks

## Executive Summary
Constrained Intrinsic Motivation (CIM) addresses two fundamental challenges in intrinsic motivation for reinforcement learning: skill discovery in reward-free pretraining and bias reduction in exploration with intrinsic rewards. The method introduces a constrained optimization framework that maximizes conditional state entropy while maintaining alignment across latent skills, ensuring both dynamic behaviors and comprehensive state coverage. For exploration tasks, CIM employs Lagrangian-constrained policy optimization to adaptively adjust the intrinsic reward coefficient, preventing distraction from extrinsic task rewards. Extensive experiments demonstrate CIM's superiority over 15 baseline methods in skill diversity, state coverage, and fine-tuning performance across multiple MuJoCo environments.

## Method Summary
CIM operates through a constrained optimization framework that addresses two distinct RL tasks. For reward-free pretraining (RFPT), it maximizes the lower bound of conditional state entropy H(ϕ(s)|z) subject to an alignment constraint on the state encoder network, ensuring diverse skill discovery with good state coverage. For exploration with intrinsic motivation (EIM), CIM leverages constrained policy optimization to adaptively adjust the coefficient of the intrinsic objective via Lagrangian duality, mitigating bias from intrinsic rewards. The method employs a Frank-Wolfe-based intrinsic reward that estimates state density using a ξ-nearest neighbor scheme, avoiding the need for parameterized generative models while maintaining computational efficiency.

## Key Results
- Achieved 1,042 average coverage in Ant task, surpassing baseline methods by significant margins
- Outperformed CIC on 3 of 4 Walker tasks in skill diversity and coverage metrics
- Improved episodic rewards by up to 73% in sparse-reward settings through adaptive coefficient adjustment

## Why This Works (Mechanism)

### Mechanism 1
Maximizing conditional state entropy under an alignment constraint enables both dynamic and diverse skill discovery. The method uses H(ϕ(s)|z) as the intrinsic objective to encourage large state variations within each skill trajectory, while the alignment constraint (La(ϕ(s), z) ≤ C) via contrastive learning makes trajectories from different skills distinguishable. This works because state density estimation via ξ-nearest neighbor is sufficient to approximate conditional entropy in high-dimensional spaces. The approach breaks down if state density estimation fails in high dimensions or the alignment constraint becomes too loose, causing skills to collapse to static behaviors.

### Mechanism 2
Adaptive coefficient adjustment via constrained policy optimization mitigates bias from intrinsic rewards in EIM tasks. The method reformulates the EIM objective as a constrained problem (maximize intrinsic objective subject to extrinsic reward constraint) and uses Lagrangian duality to derive an adaptive coefficient τCIMk that reduces intrinsic reward weight as extrinsic performance improves. This relies on the approximation of Rk via max previous extrinsic rewards being sufficient to guide coefficient adaptation. The mechanism fails if the approximation Rk is poor or the coefficient adaptation is too slow, leaving the agent distracted by intrinsic rewards.

### Mechanism 3
Frank-Wolfe-based intrinsic reward provides a non-parametric, efficient estimate of state density without requiring generative model training. The method uses ξ-nearest neighbor estimator to approximate state density dπ(ϕ(s)), which is then used to compute intrinsic reward as log distance to nearest neighbor in projected space. This assumes nearest neighbor density estimation is scalable and accurate enough for high-dimensional state spaces. The approach breaks down in extremely high-dimensional spaces or with sparse data, where nearest neighbor estimates become unreliable.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: CIM operates within MDP framework where states, actions, rewards, and policies are defined
  - Quick check question: What are the components of an MDP tuple (S, A, P, R, γ, µ)?

- Concept: Intrinsic vs Extrinsic Motivation
  - Why needed here: CIM explicitly separates intrinsic objectives for exploration/skill discovery from extrinsic task rewards
  - Quick check question: How do knowledge-based, data-based, and competence-based intrinsic motivation methods differ?

- Concept: Mutual Information and Conditional Entropy
  - Why needed here: The core intrinsic objective H(ϕ(s)|z) is based on information-theoretic concepts
  - Quick check question: What is the relationship between I(s;z), H(s), and H(s|z)?

## Architecture Onboarding

- Component map: State observation → ϕ(s) → skill-conditioned policy → action → environment → state density update → intrinsic reward calculation → policy update (with adaptive coefficient in EIM)

- Critical path: State observation flows through the state encoder network to produce representations, which feed into the skill-conditioned policy. Actions from the policy interact with the environment, producing new states. The state density estimator updates based on these states, computing intrinsic rewards that influence policy updates. In EIM tasks, the adaptive coefficient mechanism modulates the influence of intrinsic rewards.

- Design tradeoffs:
  - Non-parametric density estimation vs. parametric generative models (simplicity vs. scalability)
  - Nearest neighbor distance vs. learned density models (accuracy vs. computational cost)
  - Fixed vs. adaptive intrinsic coefficient (simplicity vs. bias mitigation)

- Failure signatures:
  - Skills collapse to static behaviors → alignment constraint too loose or density estimation failing
  - Poor fine-tuning performance → insufficient state coverage or skill diversity
  - Slow convergence in EIM → adaptive coefficient not reducing intrinsic reward fast enough

- First 3 experiments:
  1. Test skill diversity and state coverage in a simple locomotion task (e.g., Ant) with different ξ values for nearest neighbor estimation
  2. Compare fixed vs. adaptive coefficient performance in a sparse-reward EIM task (e.g., SparseAnt)
  3. Ablation study: replace contrastive alignment loss with MSE or vMF to verify effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of nearest-neighbor estimator (e.g., k-NN vs. ε-ball) affect the estimation of conditional state entropy and the resulting intrinsic reward in CIM for RFPT? The paper uses a ξ-nearest neighbor (ξ-NN) estimator but does not explore the impact of different density estimation methods on performance. Empirical comparison of CIM for RFPT using different density estimation methods on the same set of tasks and evaluation metrics would resolve this.

### Open Question 2
Can CIM for RFPT be extended to handle high-dimensional state spaces, such as raw pixel observations, without relying on pre-trained feature extractors? The paper evaluates CIM for RFPT on state-based tasks and mentions that the image encoder f is used as an identity encoder for state-based tasks, implying pixel-based tasks might require a different approach. Extension to pixel-based tasks in MuJoCo or other environments with high-dimensional state spaces, along with comparison to state-of-the-art methods for unsupervised skill discovery in pixel-based settings, would resolve this.

### Open Question 3
How does the adaptive coefficient τCIM_k in CIM for EIM interact with different types of intrinsic rewards, such as those based on prediction error or state density estimation? The paper demonstrates effectiveness using state-of-the-art data-based intrinsic bonus rAPT_I and mentions that τCIM_k is orthogonal with any intrinsic bonuses rI. Empirical comparison of CIM for EIM using different types of intrinsic rewards and evaluating the impact of τCIM_k on performance in EIM tasks would resolve this.

## Limitations

- Scalability concerns in extremely high-dimensional state spaces where nearest neighbor density estimation becomes unreliable
- Reliance on approximations of extrinsic rewards that may not always be accurate in complex environments with sparse rewards
- Computational overhead of the ξ-nearest neighbor estimator and Frank-Wolfe algorithm may limit practical applicability

## Confidence

- High confidence: The core mechanism of maximizing conditional state entropy with alignment constraints for skill discovery is well-justified and empirically validated
- Medium confidence: The adaptive coefficient adjustment via Lagrangian-constrained policy optimization effectively mitigates bias from intrinsic rewards, though its robustness across diverse environments needs further validation
- Medium confidence: The Frank-Wolfe-based intrinsic reward provides a non-parametric, efficient estimate of state density, but its scalability to high-dimensional spaces is uncertain

## Next Checks

1. Test CIM's performance on a high-dimensional control task (e.g., Humanoid) to evaluate the scalability of the nearest neighbor density estimation and the adaptive coefficient mechanism
2. Compare CIM's skill diversity and state coverage against other state-of-the-art methods (e.g., DIAYN, APT) on a challenging locomotion task (e.g., Walker) to validate its superiority
3. Analyze the impact of different ξ values in the nearest neighbor estimator on CIM's performance to determine the optimal balance between accuracy and computational efficiency