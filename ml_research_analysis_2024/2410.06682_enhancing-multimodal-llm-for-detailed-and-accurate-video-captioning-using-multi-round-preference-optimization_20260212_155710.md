---
ver: rpa2
title: Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round
  Preference Optimization
arxiv_id: '2410.06682'
source_url: https://arxiv.org/abs/2410.06682
tags:
- video
- audio
- visual
- training
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces video-SALMONN 2, an audio-visual LLM that
  uses multi-round preference optimization (mrDPO) to generate detailed and accurate
  video captions. The method involves training with LoRA and periodically updating
  the DPO reference model, merging LoRA modules, and using ground-truth captions to
  stabilize training.
---

# Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization

## Quick Facts
- arXiv ID: 2410.06682
- Source URL: https://arxiv.org/abs/2410.06682
- Authors: Changli Tang; Yixuan Li; Yudong Yang; Jimin Zhuang; Guangzhi Sun; Wei Li; Zujun Ma; Chao Zhang
- Reference count: 31
- One-line primary result: Achieves 40% reduction in global error rate, 20% reduction in local error rate, and 35% decrease in repetition rate in video captioning

## Executive Summary
This paper introduces video-SALMONN 2, an audio-visual LLM that uses multi-round preference optimization (mrDPO) to generate detailed and accurate video captions. The method involves training with LoRA and periodically updating the DPO reference model, merging LoRA modules, and using ground-truth captions to stabilize training. A rebirth tuning stage is also introduced to restore non-captioning abilities. The model achieves state-of-the-art performance on video captioning benchmarks while maintaining competitive performance on video QA tasks.

## Method Summary
The method uses LoRA for efficient fine-tuning of a pre-trained visual LLM extended with an audio branch. Multi-round DPO is applied with periodic reference model updates, LoRA proxy regularization, and ground-truth caption guidance. After mrDPO, rebirth tuning restores non-captioning abilities using self-generated captions. The training pipeline involves audio-visual SFT on an internal dataset, followed by mrDPO optimization and final rebirth tuning.

## Key Results
- 40% reduction in global error rate (missing + hallucination) compared to baseline
- 20% reduction in local error rate for specific time intervals
- 35% decrease in repetition rate in generated captions
- Outperforms GPT-4o and Gemini-1.5-Pro on video captioning tasks
- Maintains competitive performance on video QA benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The use of atomic events as a bridge to evaluate caption quality enables targeted and precise optimization of video captions.
- **Mechanism:** Atomic events break down video captions into discrete, evaluable components. By comparing these components against generated captions, the model can quantify missing and hallucinated information, providing a clear optimization signal.
- **Core assumption:** GPT models can reliably decompose video captions into atomic events and accurately identify missing or hallucinated content.
- **Evidence anchors:**
  - [abstract] "We propose new metrics to evaluate the completeness and accuracy of video descriptions, which are optimized using DPO."
  - [section 3.3] "The pipeline for selecting preferred samples when applying RL to video-SALMONN 2 is illustrated in Fig. 3. First, distinct video captions are sampled from the model's output distribution, given the input video. These captions may be either global captions describing the entire video or local captions focusing on a specific time interval."
  - [corpus] Weak evidence. No related paper explicitly uses atomic events for video captioning evaluation.
- **Break condition:** If GPT models fail to consistently decompose captions into accurate atomic events or misidentify missing/hallucinated content, the optimization signal becomes unreliable.

### Mechanism 2
- **Claim:** Multi-round DPO with periodic reference model updates and LoRA proxy regularization improves video captioning performance while preventing catastrophic forgetting.
- **Mechanism:** Each round of DPO uses the updated model as the new reference, providing a more accurate target for optimization. The LoRA proxy introduces regularization by randomly initializing a new LoRA module each round, preventing overfitting to the reference model.
- **Core assumption:** The LoRA proxy effectively regularizes the training process and prevents overfitting.
- **Evidence anchors:**
  - [abstract] "To further improve training, we introduce a novel multi-round DPO (mrDPO) approach, which involves periodically updating the DPO reference model, merging and reinitializing the low-rank adaptation (LoRA) module as a proxy for parameter updates after each training round (1,000 steps), and incorporating guidance from ground-truth video captions to stabilize the process."
  - [section 3.3] "In the multi-round framework, at each tth round, the following steps are taken to perform DPO training for the current round."
  - [corpus] Weak evidence. While DPO is used in other papers, the specific use of LoRA proxy and multi-round updates is not explicitly mentioned.
- **Break condition:** If the LoRA proxy does not effectively regularize the training or if the reference model updates do not provide a meaningful optimization target, the performance gains may be limited.

### Mechanism 3
- **Claim:** Rebirth tuning restores the model's non-captioning abilities after mrDPO, preventing degradation in other tasks.
- **Mechanism:** After mrDPO, the model generates high-quality captions for a large dataset. These captions are then used as training data for rebirth tuning, which applies teacher-forcing to guide the model towards generating fluent text. This process helps the model escape local optima and maintain performance on non-captioning tasks.
- **Core assumption:** The model can generate high-quality captions after mrDPO, and these captions can effectively restore the model's non-captioning abilities.
- **Evidence anchors:**
  - [abstract] "To address potential catastrophic forgetting of non-captioning abilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO LLM by using the captions generated by the mrDPO-trained model as supervised labels."
  - [section 3.4] "Rebirth tuning is introduced to address the issue of declining non-captioning language abilities. This method applies teacher-forcing training on self-generated data, promoting a more stable learning process for video understanding."
  - [corpus] Weak evidence. No related paper explicitly uses rebirth tuning to restore model capabilities after preference optimization.
- **Break condition:** If the model cannot generate high-quality captions after mrDPO or if the rebirth tuning process does not effectively restore non-captioning abilities, the model's overall performance may degrade.

## Foundational Learning

- **Concept:** Direct Preference Optimization (DPO)
  - **Why needed here:** DPO is used to optimize the model's output distribution based on preference data, improving the quality of video captions.
  - **Quick check question:** How does DPO differ from traditional reinforcement learning methods like PPO in terms of the reward signal and optimization process?

- **Concept:** Low-Rank Adaptation (LoRA)
  - **Why needed here:** LoRA is used to efficiently adapt the LLM to video captioning tasks without modifying the entire model, reducing computational cost.
  - **Quick check question:** What are the advantages of using LoRA over full fine-tuning in terms of parameter efficiency and training speed?

- **Concept:** Teacher-forcing
  - **Why needed here:** Teacher-forcing is used in rebirth tuning to guide the model towards generating fluent text by providing the correct output as input during training.
  - **Quick check question:** How does teacher-forcing help prevent the model from converging on limited and repetitive patterns during training?

## Architecture Onboarding

- **Component map:** Video -> Audio Encoder & Visual Encoder -> Modality Aligner -> LLM Backbone -> Text Output
- **Critical path:** 1) Input video is processed by audio and visual encoders. 2) Encoded features are aligned and combined into audio-visual tokens. 3) Audio-visual tokens are fed into the LLM backbone. 4) LLM generates a text response based on the audio-visual input and user prompt. 5) Output is optimized using mrDPO and rebirth tuning.
- **Design tradeoffs:** Using LoRA for adaptation reduces computational cost but may limit the model's capacity to learn complex tasks. Multi-round DPO improves performance but increases training time and complexity. Rebirth tuning restores non-captioning abilities but requires generating a large dataset of high-quality captions.
- **Failure signatures:** Unnatural text patterns in generated captions. Degraded performance on non-captioning tasks after mrDPO. Inconsistent or inaccurate atomic event decomposition.
- **First 3 experiments:** 1) Evaluate the model's performance on a small video captioning benchmark before and after mrDPO to assess the impact of multi-round optimization. 2) Compare the model's performance with and without the LoRA proxy to determine the effectiveness of the regularization technique. 3) Test the model's non-captioning abilities before and after rebirth tuning to verify the restoration of capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of rounds for multi-round DPO (mrDPO) to maximize captioning quality while minimizing unnatural text patterns?
- Basis in paper: [explicit] The paper mentions that the model begins to generate unnatural patterns after multiple rounds of mrDPO, but it does not specify the optimal stopping point.
- Why unresolved: The paper only discusses stopping when significant degradation in language abilities is detected, without providing a clear threshold or optimal number of rounds.
- What evidence would resolve it: Experiments comparing the performance of models trained with different numbers of mrDPO rounds, measuring both captioning quality and the frequency of unnatural patterns, would help determine the optimal stopping point.

### Open Question 2
- Question: How does the proposed rebirth tuning method compare to other techniques for mitigating catastrophic forgetting in multimodal LLMs?
- Basis in paper: [inferred] The paper introduces rebirth tuning as a novel method to restore non-captioning abilities after mrDPO, but does not compare it to other existing techniques.
- Why unresolved: The paper focuses on the effectiveness of rebirth tuning without benchmarking it against other methods for addressing catastrophic forgetting.
- What evidence would resolve it: Comparative studies evaluating rebirth tuning against other techniques like elastic weight consolidation or gradient-based methods would clarify its relative effectiveness.

### Open Question 3
- Question: Can the mrDPO approach be generalized to other multimodal tasks beyond video captioning, such as video question answering or video summarization?
- Basis in paper: [explicit] The paper applies mrDPO specifically to video captioning tasks and mentions maintaining performance on video QA benchmarks, but does not explore its application to other multimodal tasks.
- Why unresolved: The paper does not investigate whether the principles of mrDPO can be adapted to other multimodal tasks, leaving its generalizability untested.
- What evidence would resolve it: Experiments applying mrDPO to other multimodal tasks like video summarization or action recognition would demonstrate its broader applicability.

## Limitations
- The model relies heavily on GPT-4o for atomic event decomposition and preference selection, introducing potential black-box dependencies that may not generalize across different video domains or captioning styles.
- The internal video dataset composition remains unspecified, making it difficult to assess the representativeness of the training data.
- The rebirth tuning process depends on the quality of self-generated captions, which could propagate errors if the mrDPO model produces systematic biases.

## Confidence

**High Confidence:** The performance improvements (40% reduction in global error rate, 20% reduction in local error rate, 35% decrease in repetition rate) are well-supported by the experimental results and comparison with established models like GPT-4o and Gemini-1.5-Pro.

**Medium Confidence:** The mechanism of atomic events for evaluation quality is sound in principle, but the reliability of GPT-4o's decomposition capability across diverse video content remains uncertain. The effectiveness of LoRA proxy regularization in preventing overfitting is supported by the training methodology but lacks extensive ablation studies.

**Low Confidence:** The rebirth tuning's ability to fully restore non-captioning abilities is promising but not thoroughly validated. The claim that self-generated captions are sufficient for comprehensive capability restoration lacks strong empirical backing.

## Next Checks
1. **Cross-domain robustness test:** Evaluate the model on videos from domains not present in the training data (e.g., medical, sports, educational content) to assess whether GPT-4o's atomic event decomposition remains reliable across diverse content types.

2. **Ablation study on LoRA proxy:** Conduct experiments comparing mrDPO with and without the LoRA proxy mechanism to quantify the regularization effect and determine if the performance gains are attributable to this specific design choice.

3. **Long-term capability retention:** After rebirth tuning, monitor the model's performance on non-captioning tasks (video QA, general language understanding) over multiple weeks to verify that the restored capabilities are stable and not subject to gradual degradation.