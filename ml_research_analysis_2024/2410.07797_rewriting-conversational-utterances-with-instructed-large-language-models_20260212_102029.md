---
ver: rpa2
title: Rewriting Conversational Utterances with Instructed Large Language Models
arxiv_id: '2410.07797'
source_url: https://arxiv.org/abs/2410.07797
tags:
- rewriting
- conversational
- utterances
- retrieval
- utterance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using instructed large language models
  (LLMs) like ChatGPT to improve conversational search by automatically rewriting
  user utterances. The authors propose five different prompting templates to instruct
  the LLM to rewrite utterances to be more self-explanatory and retrieval-friendly,
  considering the conversation context.
---

# Rewriting Conversational Utterances with Instructed Large Language Models

## Quick Facts
- arXiv ID: 2410.07797
- Source URL: https://arxiv.org/abs/2410.07797
- Reference count: 35
- Primary result: Instructed LLMs improve conversational search retrieval effectiveness by up to 31.7% in Precision@1 on TREC CAsT 2020 dataset

## Executive Summary
This paper addresses the challenge of conversational search by leveraging instructed large language models to automatically rewrite user utterances into more retrieval-friendly queries. The authors propose five different prompting templates to instruct LLMs to rewrite utterances while considering conversation context and generated answers. Their approach significantly outperforms state-of-the-art techniques on TREC CAsT 2019 and 2020 datasets, demonstrating that LLM-based query rewriting can substantially improve search effectiveness in conversational settings. The best results are achieved when LLMs use both previous conversation history and generated answers to rewrite the current utterance.

## Method Summary
The authors employ a two-stage retrieval pipeline where they first use BM25 to retrieve an initial set of documents, then re-rank them using BERT-based cross-encoder. The key innovation is the use of instructed LLMs (ChatGPT) to rewrite conversational queries before retrieval. They propose five different prompt templates: (1) reference to previous utterances only, (2) instruction to provide more context, (3) instruction to be more specific, (4) using both previous utterances and generated answers, and (5) an improved version of the fourth template. These prompts are designed to transform conversational utterances into self-explanatory queries that better capture the user's information need for document retrieval.

## Key Results
- Up to 25.2% improvement in MRR, 31.7% in Precision@1, 27% in NDCG@3, and 11.5% in Recall@500 on TREC CAsT 2020 dataset
- Significant performance gains over state-of-the-art techniques across all evaluation metrics
- Best performance achieved using the prompt template that incorporates both previous utterances and generated answers
- Consistent improvements observed across both TREC CAsT 2019 and 2020 datasets

## Why This Works (Mechanism)
The approach works by addressing the inherent ambiguity in conversational search queries, where users often use pronouns and references that are clear in conversation but problematic for document retrieval. Instructed LLMs can interpret these references using conversation context and generate more explicit, self-contained queries that better represent the user's information need. By leveraging both conversation history and generated answers, the LLMs can create queries that are more precise and retrieval-friendly, effectively bridging the gap between natural conversation and effective document retrieval.

## Foundational Learning
1. **Conversational Search Dynamics**: Understanding how users formulate queries in conversational settings differs from traditional keyword search - users often use implicit references and assume context. Why needed: Critical for designing effective query rewriting approaches that address the unique challenges of conversational search.
   - Quick check: Compare query ambiguity levels between conversational and traditional search logs.

2. **LLM Prompt Engineering**: Different prompt templates can significantly impact the quality and style of generated text from LLMs. Why needed: Essential for designing effective instructions that guide LLMs to produce retrieval-friendly queries.
   - Quick check: Evaluate prompt variations on a small sample to identify optimal instruction patterns.

3. **Two-Stage Retrieval Pipeline**: Combining initial retrieval with re-ranking using different models can improve overall effectiveness. Why needed: Provides a robust baseline for evaluating query rewriting improvements.
   - Quick check: Measure individual and combined performance of BM25 and BERT cross-encoder components.

## Architecture Onboarding

Component Map:
Conversational Query -> LLM Query Rewriting -> BM25 Initial Retrieval -> BERT Re-ranking -> Final Results

Critical Path:
The critical path flows from the original conversational query through the LLM rewriting step, as this transformation directly impacts all subsequent retrieval effectiveness. The rewritten query is then processed through the BM25 stage, and the results are passed to BERT for re-ranking. Each stage builds upon the previous one, with the LLM rewriting serving as the foundational transformation that enables improved retrieval performance.

Design Tradeoffs:
- LLM-based rewriting vs. traditional query expansion: LLMs offer semantic understanding but at higher computational cost
- Single vs. multi-stage retrieval: Two-stage approach provides better performance but increases latency
- Context incorporation methods: Using generated answers provides more complete information but requires additional LLM calls

Failure Signatures:
- Poor context interpretation leading to irrelevant query rewrites
- Overly verbose or complex rewritten queries that hurt retrieval precision
- LLM hallucinations introducing false information into queries
- Computational bottlenecks when scaling to large query volumes

First Experiments:
1. Test each of the five prompt templates on a small validation set to identify the most promising approach
2. Compare retrieval effectiveness with and without LLM rewriting using the same BM25+BERT pipeline
3. Evaluate the impact of different conversation context lengths on rewriting quality and retrieval performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on TREC CAsT datasets which may not represent real-world conversational search diversity
- Small dataset size (200 queries total) limits generalizability of findings
- Performance improvements may not scale linearly to larger, more diverse collections
- Computational and monetary costs of LLM-based query rewriting at scale are not fully explored

## Confidence
- LLM Effectiveness for Query Rewriting: High
- Importance of Context and Generated Answers: Medium  
- Generalizability of Approach: Low

## Next Checks
1. **Dataset Expansion Test**: Evaluate the approach on additional conversational search datasets beyond TREC CAsT, including those with different domains, conversation styles, and query distributions to assess robustness and generalizability.

2. **Cost-Benefit Analysis**: Conduct a comprehensive analysis of the computational and monetary costs associated with using LLMs for query rewriting at scale, including latency measurements and comparison with traditional approaches.

3. **Human Evaluation Study**: Implement a user study to assess whether the LLM-rewritten queries actually improve the end-user search experience, measuring factors such as satisfaction, task completion time, and perceived relevance of results.