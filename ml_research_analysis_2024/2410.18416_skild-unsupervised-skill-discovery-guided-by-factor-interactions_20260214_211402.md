---
ver: rpa2
title: 'SkiLD: Unsupervised Skill Discovery Guided by Factor Interactions'
arxiv_id: '2410.18416'
source_url: https://arxiv.org/abs/2410.18416
tags:
- learning
- skill
- skills
- agent
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SkiLD addresses the challenge of learning reusable skills in complex
  environments with many state factors by leveraging state factorization to guide
  skill discovery. The method introduces Skill Discovery from Local Dependencies,
  which uses interactions between state factors as an inductive bias for learning
  useful skills.
---

# SkiLD: Unsupervised Skill Discovery Guided by Factor Interactions

## Quick Facts
- arXiv ID: 2410.18416
- Source URL: https://arxiv.org/abs/2410.18416
- Reference count: 40
- One-line primary result: SkiLD achieves superior performance on 9 out of 10 downstream tasks compared to existing unsupervised RL methods in complex simulated household robot domains

## Executive Summary
SkiLD addresses the challenge of learning reusable skills in complex environments with many state factors by leveraging state factorization to guide skill discovery. The method introduces Skill Discovery from Local Dependencies, which uses interactions between state factors as an inductive bias for learning useful skills. SkiLD learns skills that induce diverse interactions between state factors, represented as state-specific dependency graphs, through a hierarchical reinforcement learning framework. The method shows superior performance on 9 out of 10 downstream tasks compared to existing unsupervised reinforcement learning methods, including challenging long-horizon sparse reward tasks in realistic simulated household robot domains.

## Method Summary
SkiLD is a hierarchical reinforcement learning framework that discovers skills by inducing interactions between state factors. The method uses a graph-selection policy to choose target dependency graphs representing desired interactions between objects, and a skill policy to execute primitive actions that induce these interactions while visiting diverse states. The framework represents each skill as a combination of a desired dependency graph and a diversity indicator, and uses a dynamics model to identify the actual local dependencies induced by the skill policy. A diversity discriminator encourages exploration of distinguishable states under different diversity indicators. The method assumes access to a factored MDP structure and uses ground truth local dependencies from the simulator in the evaluated environments.

## Key Results
- SkiLD achieves superior performance on 9 out of 10 downstream tasks compared to existing unsupervised RL methods
- The method successfully learns skills that induce interactions between objects rather than just manipulating individual objects
- SkiLD demonstrates effectiveness on challenging long-horizon sparse reward tasks in realistic simulated household robot domains (iGibson)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SkiLD learns skills that induce diverse interactions between state factors by modeling these interactions as local dependencies and using a graph-selection policy to guide exploration toward underlearned interactions.
- **Mechanism:** The method represents each skill as a combination of a desired dependency graph (specifying which state factors should interact) and a diversity indicator. The graph-selection policy prioritizes graphs that have been seen least often, encouraging the skill policy to explore novel interaction patterns. This hierarchical approach ensures that the agent doesn't just visit diverse states but actively induces meaningful interactions between objects.
- **Core assumption:** The factored MDP structure is known and accurate, and the local dependencies identified through the dynamics model reflect true causal interactions in the environment.
- **Evidence anchors:**
  - [abstract] "SkiLD develops a novel skill learning objective that explicitly encourages the mastering of skills that effectively induce different interactions within an environment."
  - [section] "To acquire skills that are useful for downstream tasks, the skill policy Ï€skill needs to learn to induce a wide range of local dependenciessample-efficiently."
  - [corpus] Weak evidence - the corpus neighbors focus on other factorization methods but don't directly support the local dependency mechanism.

### Mechanism 2
- **Claim:** The diversity reward component ensures that once a desired interaction is induced, the agent explores multiple ways to achieve that interaction, making skills more versatile for downstream tasks.
- **Mechanism:** When the skill policy successfully induces the desired dependency graph, it receives an additional reward proportional to the mutual information between visited states and the diversity indicator. This encourages the agent to visit distinguishable states while maintaining the specified interaction, creating a repertoire of behaviors for each interaction pattern.
- **Core assumption:** The diversity discriminator can effectively distinguish between different states, and the mutual information estimation is accurate enough to guide meaningful exploration.
- **Evidence anchors:**
  - [abstract] "SkiLD not only manipulate each object (left, middle) but also induce interactions between them (right), by specifying different local dependencies."
  - [section] "When the skill policy induces the desired graph, Rdiversity further encourages it to visit different distinguishable states under different diversity indicators b, e.g., hammering the nail to different locations."
  - [corpus] Weak evidence - no direct support for diversity reward effectiveness in corpus neighbors.

### Mechanism 3
- **Claim:** By focusing on interactions rather than just state coverage, SkiLD overcomes the exponential state space problem in complex environments with many state factors.
- **Mechanism:** Instead of trying to cover all possible states (which is impossible in high-dimensional factored spaces), SkiLD targets bottleneck states characterized by interactions between factors. The graph-selection policy intelligently samples from a history of seen graphs, prioritizing underlearned interactions, which allows the agent to reach important states more efficiently than random exploration.
- **Core assumption:** Bottleneck states that are critical for exploration can be characterized by interactions between state factors, and these interactions are sufficient to capture the important behavioral variations needed for downstream tasks.
- **Evidence anchors:**
  - [abstract] "In complex environments that contain many state factors...the exponential number of distinct states makes it impossible to learn skills that cover every state."
  - [section] "In environments that have a large state space due to many state factors, rather than inefficiently relying on randomly visiting different states to reach such bottlenecks, we propose to train the agent to actively induce these critical interactions."
  - [corpus] Weak evidence - the corpus neighbors discuss factorization but don't directly address the exponential state space problem.

## Foundational Learning

- **Concept: Factored Markov Decision Processes**
  - Why needed here: SkiLD assumes access to a factored state space where individual objects and their attributes are represented as separate state factors. This factorization is essential for identifying local dependencies between specific objects.
  - Quick check question: If an environment has 5 objects each with 3 attributes, how many state factors does the factored MDP have?

- **Concept: Local Dependencies and Conditional Independence**
  - Why needed here: The method uses conditional independence tests (via pointwise conditional mutual information) to determine whether one state factor locally depends on another during a transition. This is the core mechanism for identifying meaningful interactions.
  - Quick check question: What does it mean when pCMI((si)', sj|{s/sj, a}) > 0 for a transition?

- **Concept: Mutual Information and Variational Bounds**
  - Why needed here: SkiLD uses mutual information between states and diversity indicators to encourage exploration, and approximates this with a variational lower bound using a discriminator network. This is crucial for the diversity reward component.
  - Quick check question: How does the discriminator network q(b|s) help estimate the mutual information I(s; b)?

## Architecture Onboarding

- **Component map:**
  - Graph-selection policy -> Skill policy -> Dynamics model -> Reward computation -> Policy updates
  - Diversity discriminator -> Mutual information estimation -> Diversity reward computation
  - History buffer -> Graph sampling -> Graph-selection policy input

- **Critical path:**
  1. Graph-selection policy samples a target dependency graph and diversity indicator
  2. Skill policy receives these parameters and executes actions
  3. Dynamics model observes the transition and identifies induced local dependencies
  4. Rewards are computed based on graph matching and diversity
  5. Both policies are updated based on the rewards

- **Design tradeoffs:**
  - Factored skills vs. monolithic skills: Factored skills allow specialization for different state factors but increase complexity
  - Ground truth vs. learned local dependencies: Ground truth is accurate but requires simulator access; learned dependencies are more general but less reliable
  - Discrete vs. continuous diversity indicators: Discrete indicators are simpler but may limit expressiveness

- **Failure signatures:**
  - Skill policy learns only simple behaviors (moving agent): Likely the dynamics model fails to identify complex local dependencies
  - No improvement in downstream tasks: The learned skills may not capture the right interactions for the tasks
  - Graph-selection policy gets stuck: The history buffer may not be expanding with new graphs

- **First 3 experiments:**
  1. Verify local dependency identification: Run the dynamics model on known transitions and check if it correctly identifies simple dependencies (e.g., agent moving)
  2. Test graph-selection policy: Run with a small set of known graphs and verify it samples them according to their visitation frequency
  3. Validate skill diversity: Train on a simple environment and visualize the different behaviors learned for the same dependency graph under different diversity indicators

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SkiLD compare when using learned versus ground truth dependency graphs in complex environments?
- Basis in paper: [explicit] The paper states that SkiLD uses ground truth local dependencies from the simulator for Thawing and iGibson environments due to the challenge of identifying local dependencies using learned dynamics models.
- Why unresolved: The paper does not provide a direct comparison between SkiLD's performance with learned versus ground truth dependency graphs in these specific environments.
- What evidence would resolve it: Conducting experiments in Thawing and iGibson environments using learned dependency graphs and comparing the performance metrics (e.g., success rates) with those obtained using ground truth dependency graphs would provide a clear answer.

### Open Question 2
- Question: What is the impact of the number of state factors on the efficiency and effectiveness of SkiLD in learning useful skills?
- Basis in paper: [inferred] The paper mentions that SkiLD leverages state factorization as a natural inductive bias, and it is evaluated in environments with varying numbers of state factors (e.g., Mini-BH with 3 to 7 factors, iGibson with 4 factors).
- Why unresolved: The paper does not explicitly analyze how the number of state factors affects SkiLD's performance, such as its ability to learn diverse skills or solve downstream tasks efficiently.
- What evidence would resolve it: Performing a systematic study by varying the number of state factors in the environments and measuring SkiLD's performance (e.g., skill diversity, task success rates) would reveal the impact of state factor count on the method's efficiency and effectiveness.

### Open Question 3
- Question: How does the choice of the dynamics model architecture affect the accuracy of inferred local dependencies and the overall performance of SkiLD?
- Basis in paper: [explicit] The paper mentions using a learned dynamics model to approximate the transition probability p for inferring induced graphs, with specific details about the model architecture (e.g., number of attention layers, embedding size, number of heads).
- Why unresolved: The paper does not explore how different dynamics model architectures might influence the accuracy of local dependency inference and, consequently, SkiLD's performance.
- What evidence would resolve it: Experimenting with various dynamics model architectures (e.g., different numbers of layers, attention mechanisms) and evaluating their impact on the accuracy of inferred local dependencies and downstream task performance would provide insights into the importance of model choice.

## Limitations

- The method requires access to a factored MDP structure, which may not be available in many real-world scenarios
- The effectiveness depends on accurate identification of local dependencies through the dynamics model, which may be challenging in complex, noisy environments
- The method assumes that critical bottleneck states can be characterized by interactions between state factors, potentially missing important behaviors that don't involve factor interactions

## Confidence

- **High confidence**: The hierarchical framework structure and the general concept of using state factorization for skill discovery are well-established and clearly explained
- **Medium confidence**: The effectiveness of the diversity reward mechanism and its contribution to downstream task performance, as the experimental results are promising but the ablation studies are not detailed
- **Low confidence**: The robustness of the method when the state factorization is imperfect or when the dynamics model fails to accurately identify local dependencies, as this is not thoroughly explored in the paper

## Next Checks

1. Test SkiLD on environments where the state factorization is noisy or incomplete to assess robustness to factorization errors
2. Evaluate the impact of learned vs. ground-truth local dependencies on skill quality and downstream task performance
3. Conduct ablation studies to isolate the contribution of the diversity reward mechanism versus the graph-selection policy to overall performance