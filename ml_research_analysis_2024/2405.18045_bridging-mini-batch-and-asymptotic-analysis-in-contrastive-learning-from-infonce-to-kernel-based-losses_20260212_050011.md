---
ver: rpa2
title: 'Bridging Mini-Batch and Asymptotic Analysis in Contrastive Learning: From
  InfoNCE to Kernel-Based Losses'
arxiv_id: '2405.18045'
source_url: https://arxiv.org/abs/2405.18045
tags:
- learning
- loss
- contrastive
- batch
- dhel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper bridges theoretical and practical understanding of\
  \ contrastive learning (CL) by proving that InfoNCE variants share identical optimal\
  \ solutions in both mini-batch and asymptotic regimes under certain conditions.\
  \ To better achieve these optima\u2014perfect alignment and uniformity\u2014the\
  \ authors introduce Decoupled Hyperspherical Energy Loss (DHEL), which simplifies\
  \ the uniformity term by removing dependence on positive samples."
---

# Bridging Mini-Batch and Asymptotic Analysis in Contrastive Learning: From InfoNCE to Kernel-Based Losses

## Quick Facts
- arXiv ID: 2405.18045
- Source URL: https://arxiv.org/abs/2405.18045
- Reference count: 40
- Primary result: Proves InfoNCE variants share identical optimal solutions in mini-batch and asymptotic regimes, and introduces DHEL and KCL methods that achieve superior and more robust downstream performance, especially with small batch sizes.

## Executive Summary
This paper bridges the theoretical gap between mini-batch and asymptotic analysis in contrastive learning by proving that InfoNCE variants share identical optimal solutions under specific conditions. The authors introduce Decoupled Hyperspherical Energy Loss (DHEL) and analyze Kernel Contrastive Learning (KCL), demonstrating that these methods achieve the same theoretical optima while offering practical advantages. DHEL simplifies uniformity optimization by decoupling it from alignment, while KCL enjoys batch-size-independent expected loss, enabling non-asymptotic analysis. Extensive experiments across multiple vision datasets show that these methods consistently outperform traditional InfoNCE-based approaches, particularly in small-batch regimes, while mitigating dimensionality collapse and providing more robust hyperparameter sensitivity.

## Method Summary
The paper establishes theoretical connections between mini-batch and asymptotic regimes for InfoNCE variants, then introduces DHEL which simplifies the uniformity term by removing dependence on positive samples. KCL is analyzed as an alternative approach using kernel functions that provide batch-size-independent expected loss. The method involves pre-training encoders using SimCLR, DCL, DHEL, or KCL losses for 200 epochs with SGD and cosine annealing, followed by linear evaluation on frozen representations. The authors compare these methods across CIFAR10, CIFAR100, STL-10, and ImageNet-100 datasets, analyzing downstream accuracy and representation properties including alignment, uniformity, rank, and Wasserstein distance metrics.

## Key Results
- InfoNCE variants achieve identical optimal solutions in both mini-batch and asymptotic regimes when batch size ≤ d+1 and ϕ/ψ are convex/increasing
- DHEL and KCL consistently achieve superior downstream performance, particularly with small batch sizes (M=32-256)
- DHEL effectively utilizes more dimensions of the representation space, mitigating dimensionality collapse observed in InfoNCE
- KCL methods show remarkable stability across hyperparameter variations with reduced sensitivity to temperature and other parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: InfoNCE variants share identical optimal solutions in both mini-batch and asymptotic regimes under certain conditions.
- Mechanism: The convexity and monotonicity of the ϕ and ψ functions ensure that the loss landscape has a unique global minimum, where positive pairs are perfectly aligned and representations form a regular simplex inscribed in the sphere.
- Core assumption: The batch size M is no larger than the ambient dimension d + 1.
- Evidence anchors:
  - [abstract] "under certain conditions, they admit the same minimisers when optimizing either their batch-level objectives or their expectations asymptotically"
  - [section 4] "we show multiple InfoNCE variants share the same unique optimal solution attained when (i) positive pairs align perfectly and (ii) representations form a regular simplex inscribed in the sphere"
  - [corpus] "Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering" - supports the idea that InfoNCE has specific optimal solutions
- Break condition: If the batch size exceeds d + 1, or if ϕ or ψ are not convex/increasing, the unique optimal solution may not exist or may differ between variants.

### Mechanism 2
- Claim: DHEL simplifies the uniformity term by removing dependence on positive samples, allowing independent optimization of alignment and uniformity.
- Mechanism: By contrasting a datapoint against a single positive view of a negative, DHEL decouples the uniformity term from the alignment term, reducing coupling that may hinder optimization in InfoNCE variants.
- Core assumption: Perfect uniformity can be achieved by contrasting a datapoint against a single positive view of a negative, without needing multiple views.
- Evidence anchors:
  - [abstract] "DHEL simplifies the problem by decoupling the target hyperspherical energy from the alignment of positive examples while preserving the same theoretical guarantees"
  - [section 5] "we make a simple modification on InfoNCE and propose a new CL objective that allows for an expected uniformity term that is only dependent on p"
  - [corpus] "Temperature-Free Loss Function for Contrastive Learning" - suggests that modifying InfoNCE can improve optimization
- Break condition: If the uniformity term truly requires dependence on positive samples for optimal performance, or if the decoupling introduces instability in optimization.

### Mechanism 3
- Claim: KCL enjoys additional advantages over InfoNCE: its expected loss is independent of batch size, enabling non-asymptotic analysis, and it optimizes for uniformity and alignment more directly.
- Mechanism: By using kernel functions to measure similarity, KCL's expected loss is independent of batch size, and the kernel properties (monotonicity, convexity) ensure that the optimal solution is the uniform distribution on the sphere.
- Core assumption: The kernel functions used in KCL satisfy the necessary conditions (monotonicity, convexity, etc.).
- Evidence anchors:
  - [abstract] "kernel contrastive learning (KCL), with the additional advantage of the expected loss being independent of batch size, thus identifying the minimisers in the non-asymptotic regime"
  - [section 6] "we seek an alternative loss whose expectation will admit the same minimiser in the non-asymptotic regime"
  - [corpus] "A Principled Framework for Multi-View Contrastive Learning" - suggests that kernel methods can improve contrastive learning
- Break condition: If the kernel functions do not satisfy the necessary conditions, or if the independence of batch size introduces other issues (e.g., high variance in gradient estimates for small batches).

## Foundational Learning

- Concept: Convexity and monotonicity of loss functions
  - Why needed here: The proof that InfoNCE variants share optimal solutions relies on the convexity and monotonicity of the ϕ and ψ functions. Understanding these properties is crucial for grasping the theoretical guarantees.
  - Quick check question: Why does the convexity of ϕ and ψ ensure a unique global minimum for the loss function?

- Concept: Hyperspherical energy minimization (HEM)
  - Why needed here: The optimal solutions for InfoNCE variants and KCL are connected to HEM, which is a well-studied problem in mathematics. Understanding HEM helps explain why the optimal solutions involve perfect alignment and uniformity on the sphere.
  - Quick check question: How does the regular simplex inscribed in the sphere relate to the concept of minimal hyperspherical energy?

- Concept: Kernel functions and their properties
  - Why needed here: KCL relies on kernel functions to measure similarity. Understanding the properties of these kernels (monotonicity, convexity, etc.) is crucial for grasping the theoretical advantages of KCL over InfoNCE.
  - Quick check question: Why do the properties of the kernel functions (e.g., monotonicity, convexity) ensure that the optimal solution for KCL is the uniform distribution on the sphere?

## Architecture Onboarding

- Component map:
  - Encoder (fθ) -> Loss function (InfoNCE/DCL/DHEL/KCL) -> Optimization (SGD) -> Evaluation (downstream accuracy, alignment, uniformity metrics)

- Critical path:
  - Sample a batch of positive pairs from the data distribution
  - Compute the representations for the batch using the encoder
  - Calculate the contrastive loss using the chosen loss function
  - Compute the gradients of the loss with respect to the encoder parameters
  - Update the encoder parameters using gradient descent
  - Repeat until convergence

- Design tradeoffs:
  - Batch size: Larger batch sizes can improve gradient estimates but require more memory
  - Temperature (τ): Controls the sharpness of the similarity distribution. Lower temperatures encourage harder negatives
  - Kernel parameters (γ, w, s): Control the weighting of the uniformity term and the properties of the kernel function in KCL
  - Augmentation strength: Stronger augmentations can improve invariance but may also introduce noise

- Failure signatures:
  - Dimensionality collapse: The representations use only a fraction of the available dimensions, indicating poor utilization of the representation space
  - Poor downstream performance: The learned representations do not generalize well to downstream tasks
  - Sensitivity to hyperparameters: The performance is highly sensitive to the choice of batch size, temperature, or other hyperparameters

- First 3 experiments:
  1. Implement DHEL and KCL loss functions and integrate them into the existing contrastive learning pipeline
  2. Run experiments comparing DHEL and KCL to InfoNCE on a small dataset (e.g., CIFAR10) with varying batch sizes and hyperparameters
  3. Analyze the learned representations using the provided metrics (alignment, uniformity, rank, etc.) to understand the differences between the methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise conditions under which KCL methods maintain zero gradient bias across different batch sizes?
- Basis in paper: [explicit] Proposition 6.2 states that the expectation of mini-batch KCL loss is independent of batch size, but the paper notes that the actual estimation of the expected loss and its gradient is still affected by batch size.
- Why unresolved: While the theoretical gradient bias is zero, the paper acknowledges that in practice, the gradient estimation is affected by batch size due to variance. The exact threshold for batch sizes where this variance becomes negligible is not explored.
- What evidence would resolve it: Empirical studies showing the variance of gradients across a wide range of batch sizes for KCL methods, and identifying the batch size threshold where gradient variance becomes negligible.

### Open Question 2
- Question: How does the proposed Wasserstein distance metric compare to other uniformity metrics in capturing the quality of learned representations?
- Basis in paper: [explicit] The paper introduces a novel Wasserstein distance metric to measure the distance between the ideal inner product distribution and the learned distribution, arguing that it provides a more complete picture than the traditional uniformity metric.
- Why unresolved: While the paper shows that the Wasserstein distance captures uniformity differences not reflected by the traditional metric, it does not compare this metric to other potential uniformity measures or explore its correlation with downstream performance across various datasets.
- What evidence would resolve it: Comparative studies using different uniformity metrics (e.g., energy distance, maximum mean discrepancy) on multiple datasets, evaluating their correlation with downstream task performance.

### Open Question 3
- Question: What are the practical implications of the non-asymptotic optima for InfoNCE loss variants?
- Basis in paper: [inferred] The paper discusses that while InfoNCE variants share the same optima in mini-batch and asymptotic regimes, the non-asymptotic optima are not known, which is the scenario typically encountered in practice.
- Why unresolved: The paper highlights the difference between theoretical and practical behavior of InfoNCE losses, noting that the batch size affects both the value of the expected loss and its estimate. However, it does not explore the practical implications of this discrepancy or how it affects optimization in real-world scenarios.
- What evidence would resolve it: Empirical studies comparing the optimization dynamics and final performance of InfoNCE variants across different batch sizes, analyzing how the batch size affects the gradient estimates and the convergence to optimal solutions.

## Limitations

- The theoretical guarantees rely on specific conditions (batch size ≤ d+1, convex and monotonic ϕ and ψ functions) that may not hold in practical scenarios with larger batch sizes or different loss formulations.
- The empirical evaluation focuses on vision datasets, leaving open questions about generalization to other modalities.
- The computational overhead of KCL and DHEL implementations, particularly for large-scale datasets, is not thoroughly discussed.

## Confidence

- **High Confidence**: The theoretical analysis connecting mini-batch and asymptotic regimes through convex and monotonic loss functions is well-grounded mathematically.
- **Medium Confidence**: The practical advantages of DHEL and KCL are demonstrated empirically, but could benefit from more extensive ablation studies and analysis of failure modes.
- **Medium Confidence**: The claim about improved dimensionality utilization requires further validation across diverse architectures and datasets.

## Next Checks

1. Test the batch size independence claim of KCL on extremely small batches (M=2-4) to identify practical limits.
2. Evaluate performance degradation when ϕ or ψ functions deviate from strict convexity or monotonicity.
3. Implement ablation studies removing specific components of DHEL to isolate the contribution of the decoupled uniformity term.