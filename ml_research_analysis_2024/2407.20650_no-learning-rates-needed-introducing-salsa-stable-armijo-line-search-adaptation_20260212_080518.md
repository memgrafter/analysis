---
ver: rpa2
title: 'No learning rates needed: Introducing SALSA -- Stable Armijo Line Search Adaptation'
arxiv_id: '2407.20650'
source_url: https://arxiv.org/abs/2407.20650
tags:
- learning
- search
- line
- adam
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SALSA, an enhanced line search optimizer
  for deep learning. The authors address issues with existing line search methods,
  particularly in handling mini-batch noise and computational overhead.
---

# No learning rates needed: Introducing SALSA -- Stable Armijo Line Search Adaptation

## Quick Facts
- arXiv ID: 2407.20650
- Source URL: https://arxiv.org/abs/2407.20650
- Reference count: 30
- SALSA consistently outperforms previous line search methods and tuned optimizers, achieving 1.5% accuracy advantage and 50% lower log loss

## Executive Summary
This paper introduces SALSA (Stable Armijo Line Search Adaptation), an enhanced line search optimizer that addresses key limitations of existing methods in deep learning. SALSA incorporates a momentum term into the Armijo criterion and implements frequency-based line search to reduce computational overhead. The method was evaluated across diverse datasets and architectures, demonstrating superior performance to both previous line search methods and tuned optimizers while maintaining training stability.

## Method Summary
SALSA extends the Armijo line search criterion by adding exponential moving averages to smooth both the loss decrease and gradient norm terms, reducing sensitivity to mini-batch noise. The optimizer dynamically adjusts line search frequency based on step size stability, reducing computational overhead from ~30% to ~3%. SALSA also incorporates preconditioned gradient norms for compatibility with Adam-style optimizers. The method was tested on NLP and image datasets using Transformer, CNN, and MLP architectures, with results averaged over 5 training runs.

## Key Results
- SALSA outperforms tuned Adam and SGD optimizers on average by 1.5% in accuracy
- SALSA achieves 50% lower average log loss compared to tuned optimizers
- Computational overhead reduced from ~30% to ~3% through frequency-based line search

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SALSA reduces sensitivity to mini-batch noise by smoothing both sides of the Armijo criterion.
- Mechanism: Introduces exponential moving averages (hk, sk) for loss decrease and gradient norm terms in Armijo criterion.
- Core assumption: Mini-batch loss decreases and gradient norms follow a distribution where most batches are close to expected value.
- Evidence: [abstract] "outperforms both the previous Armijo implementation and a tuned learning rate schedule"
- Break condition: If loss landscape has extremely high variance or many local minima with sharp transitions.

### Mechanism 2
- Claim: SALSA improves training stability on large-scale datasets by preventing drastic step size fluctuations.
- Mechanism: Momentum term in criterion makes it less sensitive to individual mini-batch evaluations.
- Core assumption: Step size should change gradually while being robust to noisy mini-batch gradients.
- Evidence: [abstract] "demonstrates improved training stability, especially for large-scale datasets like ImageNet"
- Break condition: If dataset has very consistent gradients, momentum might unnecessarily slow adaptation.

### Mechanism 3
- Claim: SALSA reduces computational overhead by dynamically adjusting line search frequency.
- Mechanism: Tracks rate of change in step size and performs line searches less frequently when step size is stable.
- Core assumption: Most training steps don't require step size updates, so line searches can be periodic.
- Evidence: [section] "reduces extra compute needed from roughly 30% to approximately 3% for longer runs"
- Break condition: If loss landscape changes rapidly, reduced frequency might miss important adjustments.

## Foundational Learning

- Concept: Armijo line search criterion
  - Why needed here: Essential for understanding baseline criterion that SALSA improves
  - Quick check question: What is the mathematical form of the Armijo condition, and why does it guarantee sufficient decrease in loss?

- Concept: Exponential moving averages
  - Why needed here: SALSA uses exponential smoothing to reduce mini-batch noise effects
  - Quick check question: How does the smoothing factor β3 affect the responsiveness of averaged values to changes in underlying data?

- Concept: Preconditioned gradient norms
  - Why needed here: SALSA incorporates preconditioned gradient norms for Adam compatibility
  - Quick check question: How does using ||∇fk(wk)||²/√ˆvk+ϵ instead of ||∇fk(wk)||² affect step size selection?

## Architecture Onboarding

- Component map: SALSA class -> Armijo criterion with momentum -> Frequency controller -> State tracking (hk, sk, step sizes)
- Critical path: 1) Compute gradient and preconditioned gradient norm 2) Check if line search needed 3) Perform line search using momentum-enhanced Armijo criterion 4) Update weights with computed step size
- Design tradeoffs:
  - Momentum term (β3): Higher values reduce noise sensitivity but slow adaptation
  - Line search frequency: More frequent searches improve responsiveness but increase computation
  - c parameter: Controls strictness of Armijo condition
- Failure signatures:
  - Step size collapsing to near-zero: May indicate numerical precision issues or overly strict Armijo condition
  - High variance in step size across runs: May indicate insufficient smoothing or problematic dataset
  - No improvement in training speed: May indicate inappropriate hyperparameter choices
- First 3 experiments:
  1. Compare SALSA vs. standard Adam on CIFAR10 with ResNet34, monitoring training stability
  2. Test different β3 values (0.9, 0.99, 0.999) on MNIST to find optimal noise reduction
  3. Evaluate computational overhead by comparing training time with and without dynamic frequency control on ImageNet

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical convergence rate of SALSA with SGD under different smoothing factors β3?
- Basis: The authors mention extending convergence theorem for original Armijo method to include SaLSa criterion with SGD.
- Why unresolved: Paper provides convergence theorem but doesn't analyze convergence rate compared to other optimizers or different β3 values.
- Evidence needed: Mathematical proofs comparing convergence rates of SaLSa with different β3 values and other optimizers on convex and non-convex functions.

### Open Question 2
- Question: How does optimal c parameter in SaLSa vary across different architectures and dataset domains?
- Basis: Authors mention performance is robust to β3 choice but different c values have more substantial impact.
- Why unresolved: Paper doesn't provide systematic study of how optimal c parameter varies with architecture complexity or dataset domain.
- Evidence needed: Extensive experiments varying c across different architectures and dataset domains to identify patterns in optimal c values.

### Open Question 3
- Question: What is the impact of using different line search frequencies (Lk) on performance and computational efficiency of SaLSa?
- Basis: Authors introduce method to determine Lk based on rate of change of step size but don't extensively study impact.
- Why unresolved: Paper focuses on proposed method for determining Lk but doesn't explore how varying Lk affects performance-computational efficiency tradeoff.
- Evidence needed: Experiments comparing SaLSa performance and computational efficiency with different fixed and adaptive line search frequencies.

## Limitations
- Unknown hyperparameters: Exact values for smoothing factor β3 and Armijo parameter c not specified
- Limited architecture diversity: Evaluation focuses on standard Transformer and CNN architectures without exploring more exotic or specialized architectures
- Computational overhead claims: Methodology for measuring computational overhead not detailed

## Confidence
- High confidence: Mechanism for reducing mini-batch noise through exponential smoothing is well-founded theoretically
- Medium confidence: Claim of improved training stability on large-scale datasets is supported but lacks quantitative stability metrics
- Medium confidence: Computational overhead reduction is supported by described frequency control but implementation details not fully specified

## Next Checks
1. Systematically test SALSA with different β3 values (0.9, 0.99, 0.999) and c parameters on multiple datasets to determine optimal range and assess sensitivity
2. Implement SALSA and measure actual computational overhead across different batch sizes and dataset scales to verify claimed 30% to 3% reduction
3. Design experiments to quantitatively measure training stability by tracking step size variance and loss curve consistency across multiple training runs with identical hyperparameters