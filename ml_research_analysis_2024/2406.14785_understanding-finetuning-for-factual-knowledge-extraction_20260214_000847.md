---
ver: rpa2
title: Understanding Finetuning for Factual Knowledge Extraction
arxiv_id: '2406.14785'
source_url: https://arxiv.org/abs/2406.14785
tags:
- attention
- finetuning
- facts
- knowledge
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how fine-tuning data affects factuality
  in language models, finding that fine-tuning on well-known facts significantly improves
  factuality compared to less-known facts. The authors theoretically analyze a one-layer
  transformer, introducing "factual salience" to quantify how well a fact is encoded.
---

# Understanding Finetuning for Factual Knowledge Extraction

## Quick Facts
- arXiv ID: 2406.14785
- Source URL: https://arxiv.org/abs/2406.14785
- Authors: Gaurav Ghosal; Tatsunori Hashimoto; Aditi Raghunathan
- Reference count: 37
- One-line primary result: Fine-tuning on well-known facts significantly improves factuality compared to less-known facts, even when targeting less-known facts.

## Executive Summary
This paper investigates how fine-tuning data affects factuality in language models, finding that fine-tuning on well-known facts significantly improves factuality compared to less-known facts. The authors theoretically analyze a one-layer transformer, introducing "factual salience" to quantify how well a fact is encoded. They prove that fine-tuning on less salient facts can suppress pretrained knowledge via attention imbalance. Experiments on three QA datasets (PopQA, MMLU, Entity Questions) and two models (Llama-2-7B, Mistral-7B) confirm that fine-tuning on the top 50% most popular facts outperforms bottom 50% by 4-10% and matches or exceeds whole dataset fine-tuning. Attention analysis validates the theoretical predictions, showing reduced subject attention in models fine-tuned on less salient facts.

## Method Summary
The authors analyze fine-tuning on factual knowledge extraction using a combination of theoretical analysis and empirical experiments. They introduce a one-layer transformer model to theoretically analyze how fact popularity affects knowledge extraction, defining "factual salience" as a measure of how well a fact is encoded in the model. Empirically, they fine-tune two models (Llama-2-7B and Mistral-7B) on three QA datasets, partitioning the data by fact popularity to compare performance. They use LoRA and full fine-tuning with hyperparameter tuning on validation sets. The theoretical model predicts that fine-tuning on less salient facts can lead to attention imbalance that suppresses subject attention, while the empirical results validate this prediction across multiple datasets and models.

## Key Results
- Fine-tuning on the top 50% most popular facts outperforms bottom 50% by 4-10% on factual accuracy
- Fine-tuning on popular facts matches or exceeds whole dataset fine-tuning performance
- Attention analysis shows reduced subject attention in models fine-tuned on less salient facts
- A randomly selected 50% subset performs only slightly worse than FT-Top, suggesting data diversity matters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on lesser-known facts harms downstream factuality because the model learns to suppress subject attention and rely on generic shortcuts instead of retrieving stored knowledge.
- Mechanism: When a fact is less salient (poorly encoded in pretraining), the attention mechanism shifts away from the subject token toward more generic relation or prompt tokens. This attention imbalance suppresses the retrieval of the correct answer even when it is still stored in the value matrix.
- Core assumption: The value matrix (WV) stores all factual associations, and the attention mechanism controls how much of that stored knowledge is actually used in output.
- Evidence anchors:
  - [abstract] "We prove this phenomenon theoretically, showing that training on lesser-known facts can lead the model to ignore subject entity names and instead output a generic plausible response even when the relevant factual knowledge is encoded in the model."
  - [section] "We prove that fine-tuning gradients on less salient facts contribute to the formation of attention imbalance, while those on more salient facts counteract it."
  - [corpus] Weak: Only 8 corpus papers found; no direct discussion of attention imbalance or salience in fine-tuning data curation.
- Break condition: If the value matrix does not store the factual association (i.e., the fact was never seen enough times in pretraining), the mechanism fails because there is nothing to suppress.

### Mechanism 2
- Claim: Fact salience grows with pretraining frequency, so popular facts are more robustly stored and less sensitive to attention imbalance during fine-tuning.
- Mechanism: As a fact is seen more often during pretraining, its salience (S(s,r,a) = ϕ(a)⊤WVϕ(s)) increases, making it harder for fine-tuning to override it with incorrect attention patterns. Less frequent facts have low salience and are easily suppressed.
- Core assumption: Salience is monotonic in pretraining frequency and can be approximated by how often a fact appears in pretraining data.
- Evidence anchors:
  - [abstract] "fine-tuning on well-known facts significantly improves factuality compared to less-known facts."
  - [section] "We prove that the factual salience increases as a fact is seen during pretraining, justifying the use of pretraining frequency as a proxy."
  - [corpus] Weak: No corpus evidence directly linking pretraining frequency to downstream factuality robustness.
- Break condition: If pretraining is so extensive that all facts reach high salience, the gap between popular and unpopular facts disappears.

### Mechanism 3
- Claim: Fine-tuning on a mix of popular and unpopular facts can be worse than fine-tuning only on popular facts because gradients from unpopular facts globally suppress subject attention.
- Mechanism: Gradients from unpopular facts have srel - prel < 0, which decreases attention on all subject tokens when prompting with (s′, pr). This global suppression harms retrieval of even well-known facts at test time.
- Core assumption: Attention updates from one (s, pr) example affect the attention scores for all subject tokens in the same relation class.
- Evidence anchors:
  - [abstract] "fine-tuning on a subset of better-known examples matches or outperforms finetuning on the entire dataset."
  - [section] "We prove that the WKQ gradient up-weights attention on the most relevant token... the WKQ gradient up-weights attention on the most relevant token (as measured by srel and prel)."
  - [corpus] Weak: No corpus evidence showing that gradients from unpopular facts globally suppress attention for all subjects.
- Break condition: If the model architecture prevents global attention updates (e.g., per-relation independent attention heads), the mechanism does not apply.

## Foundational Learning

- Concept: Factual salience and its role in knowledge extraction.
  - Why needed here: Understanding why some facts are more extractable than others is central to interpreting the experimental results and the theoretical model.
  - Quick check question: If a fact appears only once in pretraining, will it have high or low salience? Why?

- Concept: Attention imbalance in transformers.
  - Why needed here: The paper's core argument hinges on how attention patterns can suppress stored knowledge during fine-tuning.
  - Quick check question: In a one-layer transformer, if the attention weight on the subject token goes to zero, can the model still retrieve the correct answer?

- Concept: Gradient dynamics during fine-tuning.
  - Why needed here: The mechanism by which unpopular facts harm factuality depends on how fine-tuning gradients update attention matrices.
  - Quick check question: What happens to the attention score on all subject tokens if you fine-tune on a fact where srel < prel?

## Architecture Onboarding

- Component map:
  - Pretraining corpus → Value matrix WV (stores factual associations)
  - Query-key matrix WKQ (controls attention patterns)
  - Fine-tuning dataset (controls gradient updates to WKQ)
  - Downstream test set (measures factuality under different fine-tuning regimes)

- Critical path:
  1. Pretrain model on long-tailed corpus → store facts in WV with varying salience.
  2. Fine-tune on QA examples → update WKQ based on srel - prel.
  3. At test time → attention imbalance can suppress retrieval of low-salience facts.

- Design tradeoffs:
  - Using only popular facts in fine-tuning maximizes downstream factuality but may leave the model less flexible on rare facts.
  - Including unpopular facts risks global attention suppression but could improve coverage if handled carefully.

- Failure signatures:
  - Model outputs plausible but incorrect answers even when the correct answer is stored in WV.
  - Attention maps show very low subject attention for certain relations.
  - Fine-tuning on unpopular facts causes performance drop not just on those facts but also on related popular facts.

- First 3 experiments:
  1. Train a one-layer transformer on synthetic data with Zipf-distributed fact frequencies; fine-tune on top vs. bottom popularity halves; measure downstream accuracy.
  2. Visualize attention patterns before and after fine-tuning on unpopular facts; check for subject attention suppression.
  3. Fine-tune a real LLM (e.g., Llama-2-7B) on a controlled dataset where only the popularity of facts varies; evaluate on held-out unpopular facts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed attention imbalance mechanism generalize beyond single-layer transformers to deeper architectures?
- Basis in paper: [explicit] The authors state: "we hypothesize that our conceptual model can guide the development of synthetic data to efficiently improve knowledge extractability" and "it can suffice to focus on a smaller number of well-known facts even when trying to improve factuality on less well-known facts."
- Why unresolved: The theory is developed for a one-layer transformer. The paper does not test or prove whether the attention imbalance effect holds in multi-layer models where residual connections and deeper attention patterns could modulate the effect.
- What evidence would resolve it: Empirical studies on multi-layer transformers (e.g., Llama-2-13B, GPT-3) measuring attention patterns and factuality outcomes when fine-tuning on popular vs. unpopular facts. Controlled ablation studies varying model depth and fine-tuning data composition.

### Open Question 2
- Question: How does the proposed method of focusing on popular facts during fine-tuning perform in knowledge-intensive domains with inherently long-tail distributions (e.g., biomedical, legal)?
- Basis in paper: [inferred] The authors demonstrate improved factuality by fine-tuning on popular facts, but note: "These findings suggest that differing impacts of the fine-tuning datasets is correlated with how significantly facts differ in their pretraining frequency." The paper tests on general knowledge datasets (PopQA, MMLU, Entity Questions) but not specialized long-tail domains.
- Why unresolved: The paper's evaluation focuses on general knowledge. Long-tail domains may have different pretraining distributions and require more obscure facts for task performance, potentially limiting the applicability of the "popular facts" strategy.
- What evidence would resolve it: Experiments on long-tail QA datasets (e.g., PubMedQA, HOTPOTQA) comparing fine-tuning on popular vs. unpopular facts and measuring downstream factuality. Analysis of pretraining corpus composition in specialized domains.

### Open Question 3
- Question: What is the precise mechanism by which popular facts mitigate attention imbalance when fine-tuning on unpopular facts?
- Basis in paper: [explicit] The authors state: "The gradients on more popular examples globally counteract attention imbalance, as shown in Theorem 4.5." They also note: "surprisingly, we find that even a randomly selected 50% subset (plotted sky-blue in Figure 5) significantly outperforms FT-Bottom, performing only slightly worse than FT-Top across all settings."
- Why unresolved: The theory shows that gradients from popular facts counteract attention imbalance, but does not explain the precise mechanism. The empirical finding that a random 50% subset outperforms FT-Bottom suggests the effect is not simply due to popular facts but potentially due to diversity or other factors in the fine-tuning data.
- What evidence would resolve it: Detailed analysis of gradient norms and directions during fine-tuning on different data subsets. Experiments isolating the effect of popular facts vs. data diversity (e.g., controlling for number of unique relations or answer types). Mechanistic interpretability studies examining how different fine-tuning datasets affect attention patterns across layers.

## Limitations
- Theoretical analysis is limited to one-layer transformers, which may not capture attention dynamics in deeper models.
- Empirical results rely on pretraining frequency as a proxy for fact salience, which may not uniformly apply across domains.
- Controlled experiment showing global attention suppression from unpopular facts is not validated on real LLMs.

## Confidence
- High Confidence: The empirical observation that fine-tuning on popular facts improves downstream factuality compared to unpopular facts.
- Medium Confidence: The theoretical claim that fine-tuning on unpopular facts globally suppresses subject attention.
- Medium Confidence: The mechanism that value matrix storage plus attention imbalance controls factuality.

## Next Checks
1. Validate the attention suppression mechanism in deeper models by running the same fine-tuning experiments on a 12-layer Llama-2-7B and comparing attention patterns before and after fine-tuning on unpopular facts.

2. Test the pretraining frequency proxy for salience by creating a synthetic pretraining corpus with controlled fact frequencies and measuring how well frequency predicts downstream factuality robustness.

3. Examine the tradeoff between coverage and factuality by fine-tuning models on mixed datasets containing both popular and unpopular facts, varying the ratio to find a sweet spot.