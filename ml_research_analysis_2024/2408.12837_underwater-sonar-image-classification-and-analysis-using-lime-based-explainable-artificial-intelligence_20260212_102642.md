---
ver: rpa2
title: Underwater SONAR Image Classification and Analysis using LIME-based Explainable
  Artificial Intelligence
arxiv_id: '2408.12837'
source_url: https://arxiv.org/abs/2408.12837
tags:
- images
- data
- sonar
- dataset
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of interpreting underwater SONAR
  image classification, which is critical for defence applications but hindered by
  the "black-box" nature of deep learning models. A custom SONAR dataset was created
  by combining four publicly available datasets.
---

# Underwater SONAR Image Classification and Analysis using LIME-based Explainable Artificial Intelligence

## Quick Facts
- arXiv ID: 2408.12837
- Source URL: https://arxiv.org/abs/2408.12837
- Reference count: 40
- Key outcome: DenseNet121 achieved 98.21% accuracy on SONAR image classification with SP-LIME providing efficient, interpretable explanations

## Executive Summary
This study addresses the critical challenge of interpreting underwater SONAR image classification, which is essential for defence applications but hindered by the "black-box" nature of deep learning models. The authors created a custom SONAR dataset by combining four publicly available datasets and applied transfer learning using benchmark CNN architectures. They addressed data imbalance through oversampling and undersampling techniques and incorporated LIME and SP-LIME XAI methods to provide interpretable explanations for model predictions. DenseNet121 achieved the highest accuracy of 98.21% using stratified sampling on balanced data.

## Method Summary
The authors combined four SONAR datasets (Seabed Objects KLSG, SCTD, Camera SONAR, and Mine SONAR) into a unified dataset of 2989 images across six classes. They applied data balancing techniques (oversampling minority classes, undersampling majority classes) and used stratified sampling for train/validation/test splits. Transfer learning was implemented with CNN architectures including VGG16, VGG19, ResNet50, InceptionV3, and DenseNet121. LIME and SP-LIME were incorporated for explainable AI analysis, with SP-LIME using the Quickshift super-pixel algorithm for computational efficiency.

## Key Results
- DenseNet121 achieved 98.21% classification accuracy using stratified sampling on balanced data
- SP-LIME outperformed LIME in computational efficiency and mask accuracy, particularly with Quickshift super-pixel algorithm
- Class-wise accuracy improved significantly after balancing: human (100%), plane (98%), ship (97%)
- Stratified sampling with balanced data produced superior validation accuracy (99.33%) compared to other approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transfer learning with DenseNet121 and stratified sampling on balanced data produces the highest classification accuracy (98.21%).
- **Mechanism:** Pre-trained weights provide a strong feature extractor, and stratified sampling ensures each class is represented proportionally in train/validation/test splits, preventing model bias toward majority classes.
- **Core assumption:** The SONAR image domain shares transferable visual features with ImageNet, and the dataset's class imbalance can be corrected by equal-sized class splits.
- **Evidence anchors:**
  - [abstract] "DenseNet121 achieved the highest accuracy of 98.21% using stratified sampling on balanced data."
  - [section] "Among the options considered, DenseNet121 from the stratified approach using balanced data stood out... with outstanding validation accuracy of 99.33% and a test accuracy of 98.21%."
- **Break condition:** If SONAR images differ structurally from ImageNet (e.g., grayscale vs RGB), pre-trained weights may not transfer effectively; stratified sampling fails if dataset cannot be perfectly balanced.

### Mechanism 2
- **Claim:** SP-LIME with Quickshift super-pixel segmentation is computationally faster than LIME while providing comparable interpretability.
- **Mechanism:** Instead of perturbing each pixel, SP-LIME perturbs super-pixels, reducing the number of perturbations and hence runtime; Quickshift identifies meaningful regions efficiently.
- **Core assumption:** Super-pixel boundaries align with semantically meaningful parts of SONAR images, so perturbing at that level retains explanatory fidelity.
- **Evidence anchors:**
  - [abstract] "SP-LIME outperformed LIME in computational efficiency and mask accuracy, particularly with the Quickshift super-pixel algorithm."
  - [section] "SP-LIME offers a faster runtime... The specified parameters for SP-LIME were crucial in achieving this efficiency... Quickshift algorithm in contrast to SLIC... Quickshift performs better."
- **Break condition:** If SONAR images have texture patterns that do not align with Quickshift super-pixel boundaries, the mask may lose relevance; too few super-pixels may oversimplify.

### Mechanism 3
- **Claim:** Data balancing via oversampling/undersampling improves minority class performance without severe accuracy loss on majority classes.
- **Mechanism:** Synthetic augmentation of minority classes and random removal from majority classes equalize class frequencies, allowing the model to learn minority patterns rather than overfit to majority ones.
- **Core assumption:** Generated synthetic samples preserve class characteristics, and discarding majority samples does not eliminate critical discriminative features.
- **Evidence anchors:**
  - [abstract] "Data imbalance was addressed using oversampling and undersampling techniques."
  - [section] "The classification report... indicates that the class-wise accuracy has improved... accuracy for 'human', 'plane' and 'ship' has reached 100%, 98% and 97% respectively."
- **Break condition:** If oversampling creates unrealistic synthetic examples, model may overfit to artifacts; undersampling may remove informative majority samples.

## Foundational Learning

- **Concept:** Transfer Learning
  - Why needed here: SONAR image datasets are small and domain-specific; pre-trained ImageNet models provide robust low-level feature detectors without training from scratch.
  - Quick check question: What happens to training time and accuracy if you replace DenseNet121 with a randomly initialized model on the same SONAR dataset?

- **Concept:** Stratified Sampling
  - Why needed here: SONAR dataset is highly imbalanced (fish 44.6% vs human 1.1%); stratified sampling ensures each class appears proportionally in all splits, avoiding biased test sets.
  - Quick check question: How would accuracy metrics change if you used simple random sampling instead of stratified sampling on this imbalanced dataset?

- **Concept:** XAI (LIME & SP-LIME)
  - Why needed here: Deep models are black boxes; in defence/SONAR, trust and interpretability are mandatory for deployment; LIME/SP-LIME provide local, human-comprehensible explanations.
  - Quick check question: What is the difference between pixel-level perturbation (LIME) and super-pixel perturbation (SP-LIME) in terms of computational cost and explanation quality?

## Architecture Onboarding

- **Component map:** Data pipeline → Balanced dataset (oversampling/undersampling) → Stratified split → Pre-trained DenseNet121 → Custom FC head → Classifier → SP-LIME explainer → Quickshift super-pixels → Explanations
- **Critical path:** Data balancing → Stratified split → Transfer learning training → SP-LIME explanation generation
- **Design tradeoffs:**
  - Using larger backbone (NASNetLarge) → higher accuracy possible but heavier compute
  - More super-pixels in SP-LIME → finer explanations but higher runtime
  - More synthetic samples in oversampling → better minority representation but risk overfitting
- **Failure signatures:**
  - Low minority class recall → likely insufficient synthetic samples or poor super-pixel masks
  - SP-LIME mask missing object parts → super-pixel algorithm parameters (kernel size, max distance) need tuning
  - High training accuracy but low test accuracy → overfitting due to insufficient regularization or imbalanced splits
- **First 3 experiments:**
  1. Train DenseNet121 with stratified sampling on balanced data; record accuracy, per-class recall
  2. Run SP-LIME on best model using Quickshift; compare mask quality vs. LIME with 10 features/500 samples
  3. Vary Quickshift kernel size (2→4) and max distance (100→200); observe changes in mask precision and runtime

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of features and samples for LIME and SP-LIME to balance interpretability and computational efficiency in underwater SONAR image classification?
- Basis in paper: [explicit] The paper discusses varying the number of features and samples for LIME and SP-LIME, finding that 10 features and 500 samples provided a good balance, but suggests this may vary depending on the dataset and computational constraints.
- Why unresolved: The optimal number likely depends on specific image characteristics, dataset size, and available computational resources, which vary across applications.
- What evidence would resolve it: Systematic experiments varying features and samples across different SONAR datasets and hardware configurations to determine optimal settings for each scenario.

### Open Question 2
- Question: How do image enhancement techniques like SRGAN impact the interpretability of XAI methods in underwater SONAR image classification?
- Basis in paper: [explicit] The authors mention that image enhancement techniques like SRGAN could be employed to enhance image quality and direct the explainer towards the area of interest in future work.
- Why unresolved: The paper only suggests this as future work without testing its impact on XAI interpretability.
- What evidence would resolve it: Comparative studies applying SRGAN or similar techniques to SONAR images and measuring the resulting changes in LIME/SP-LIME explanations and classification accuracy.

### Open Question 3
- Question: How do different super-pixel algorithms (Quickshift vs. SLIC) affect the quality and efficiency of SP-LIME explanations in underwater SONAR image classification?
- Basis in paper: [explicit] The paper compares Quickshift and SLIC algorithms for SP-LIME, noting that Quickshift performed better for their dataset, but acknowledges that performance may vary depending on the dataset.
- Why unresolved: The comparison was limited to one dataset, and the paper suggests that the optimal algorithm may depend on dataset characteristics.
- What evidence would resolve it: Extensive testing of Quickshift, SLIC, and other super-pixel algorithms across multiple SONAR datasets to determine which algorithm consistently provides the best balance of explanation quality and computational efficiency.

## Limitations

- The custom dataset combination process is not fully detailed, raising questions about potential domain shift between source datasets
- While SP-LIME showed computational advantages, the study did not compare against other super-pixel algorithms (SLIC, Felzenszwalb) or test robustness across varying SONAR image conditions
- The study did not conduct user studies or downstream task performance validation to verify the practical interpretability of XAI explanations

## Confidence

- DenseNet121 98.21% accuracy claim: High confidence
- SP-LIME computational efficiency claim: Medium confidence
- Data balancing effectiveness claim: Medium confidence
- XAI interpretability claim: Low-Medium confidence

## Next Checks

1. Test model robustness by evaluating on SONAR images from an independent dataset not used in training, measuring accuracy drop and explanation quality degradation
2. Compare SP-LIME with Quickshift against alternative super-pixel methods (SLIC, Felzenszwalb) using identical hyperparameter search to quantify algorithm sensitivity
3. Conduct ablation study removing stratified sampling and data balancing to isolate their contribution to the 98.21% accuracy, documenting per-class performance changes