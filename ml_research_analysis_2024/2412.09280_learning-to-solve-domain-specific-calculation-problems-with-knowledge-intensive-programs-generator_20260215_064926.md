---
ver: rpa2
title: Learning to Solve Domain-Specific Calculation Problems with Knowledge-Intensive
  Programs Generator
arxiv_id: '2412.09280'
source_url: https://arxiv.org/abs/2412.09280
tags:
- programs
- yuan
- knowledge
- domain
- domain-specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes KIPG, a pipeline to solve domain-specific calculation
  problems using knowledge-intensive programs. KIPG generates programs from domain
  documents that capture complex rules and conditions, extracts key variables from
  queries, executes programs to compute domain-dependent outcomes, and concludes answers.
---

# Learning to Solve Domain-Specific Calculation Problems with Knowledge-Intensive Programs Generator

## Quick Facts
- arXiv ID: 2412.09280
- Source URL: https://arxiv.org/abs/2412.09280
- Reference count: 19
- Primary result: KIPG significantly outperforms baselines in accuracy on legal and medical calculation problems, including when applied to unseen domains

## Executive Summary
KIPG is a pipeline that solves domain-specific calculation problems by generating knowledge-intensive programs from domain documents. The approach extracts key variables from queries, executes programs to compute domain-dependent outcomes, and concludes answers. Through iterative preference alignment training, the code generator learns to improve logical consistency with domain knowledge. Experiments demonstrate KIPG's effectiveness in handling complex, rule-based calculations that challenge traditional LLMs.

## Method Summary
The KIPG pipeline consists of four main components: a code generator that produces knowledge-intensive programs from domain documents, an extraction model that identifies key variables from queries, an executor that runs programs to calculate outcomes, and a conclusion model that generates final answers. The code generator is trained using iterative Direct Preference Optimization (DPO), where programs are sampled, executed, and ranked based on their correctness across all queries for a document. This approach captures complex domain rules better than query-oriented programs by representing hierarchical conditions and multiple variables as structured code.

## Key Results
- KIPG significantly outperforms baselines including vanilla LLMs, CoT, PoT, and RCI on legal and medical calculation datasets
- The approach generalizes to unseen domains without additional training
- Knowledge-intensive programs capture complex domain rules more effectively than query-oriented programs
- Iterative preference alignment improves program generation consistency with domain knowledge

## Why This Works (Mechanism)

### Mechanism 1
Knowledge-intensive programs capture complex domain rules better than query-oriented programs by representing hierarchical conditions and multiple variables as structured code, preserving the full logical space of domain knowledge rather than collapsing it into a single answer. This works when domain documents contain conditional logic that cannot be fully represented by a single formula.

### Mechanism 2
Iterative preference alignment improves program generation consistency through sampling multiple programs, executing them, and ranking based on correctness across all queries for a document. The model learns to generate programs that better match intended logic when the ranking process can reliably distinguish between correct and incorrect program logic.

### Mechanism 3
The extraction-execution-conclusion pipeline isolates domain-specific calculation from general reasoning by having the extraction model pull out variables needed by the program, the program execute the domain logic, and the conclusion model apply results to the specific query. This allows each component to specialize when domain-specific calculation and general reasoning are separable tasks.

## Foundational Learning

- **Domain-specific knowledge representation**: Needed to understand how to encode complex domain rules into executable programs. Quick check: Can you write a program that captures the logic of a tiered pricing structure?
- **Preference optimization**: Needed because the code generator is trained using iterative preference alignment. Quick check: What is the difference between DPO and standard supervised learning?
- **Retrieval-augmented generation**: Needed as the system retrieves relevant domain documents before processing each query. Quick check: How does semantic search differ from keyword search in document retrieval?

## Architecture Onboarding

- **Component map**: Retriever -> Code Generator (θG) -> Extraction Model (θE) -> Executor -> Conclusion Model (θC)
- **Critical path**: Query received → Retriever finds relevant document(s) → Code generator produces program(s) for document(s) → Extraction model pulls variables from query → Executor runs program with variables → Conclusion model generates final answer
- **Design tradeoffs**: Program complexity vs. execution efficiency, beam search size vs. training time in preference alignment, program generality vs. query-specific optimization
- **Failure signatures**: Programs fail to execute → Check syntax and variable definitions; Incorrect calculations → Check program logic and variable extraction; Hallucinations in answers → Check conclusion model's use of extracted variables; Poor performance on new domains → Check program generator's generalization
- **First 3 experiments**: 1) Run KIPG on a single legal document with known correct answers to verify the pipeline works end-to-end, 2) Test the code generator on a held-out legal document to measure generalization, 3) Compare program accuracy vs. line count to understand complexity-performance relationship

## Open Questions the Paper Calls Out

The paper acknowledges that optimizing retrieval accuracy is not its target and suggests this may be addressed in future work. The significant performance drop (10%+) when gold documents are not provided indicates that improving retrieval accuracy is an open question for extending the system's effectiveness in real-world applications.

## Limitations

- The paper doesn't provide detailed error analysis showing where and why KIPG fails in real-world applications
- Claims about superior performance on unseen domains rely heavily on the quality and representativeness of evaluation datasets
- The approach assumes domain-specific calculation and general reasoning are separable tasks, which may not hold for all problem types

## Confidence

- **High confidence**: The general pipeline architecture (extraction-execution-conclusion) is well-specified and theoretically sound
- **Medium confidence**: The claim that knowledge-intensive programs outperform query-oriented programs needs empirical validation
- **Low confidence**: The assertion that KIPG generalizes well to unseen domains is based on reported results but lacks transparency about evaluation methodology

## Next Checks

1. Request access to the legal and medical datasets to independently verify performance claims and assess dataset characteristics
2. Run ablation studies comparing knowledge-intensive programs against query-oriented programs on the same tasks
3. Generate detailed error analysis reports showing specific cases where KIPG fails and identifying root causes