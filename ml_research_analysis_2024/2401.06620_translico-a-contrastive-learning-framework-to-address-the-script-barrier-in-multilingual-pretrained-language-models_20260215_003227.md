---
ver: rpa2
title: 'TransliCo: A Contrastive Learning Framework to Address the Script Barrier
  in Multilingual Pretrained Language Models'
arxiv_id: '2401.06620'
source_url: https://arxiv.org/abs/2401.06620
tags:
- latn
- cyrl
- languages
- scripts
- furina
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of cross-lingual transfer for
  multilingual pretrained language models (mPLMs) across languages with different
  scripts. It proposes TransliCo, a contrastive learning framework that fine-tunes
  an mPLM by contrasting sentences in their original scripts with their transliterations
  in Latin script, using a Transliteration Contrastive Modeling (TCM) objective alongside
  Masked Language Modeling (MLM).
---

# TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models

## Quick Facts
- **arXiv ID**: 2401.06620
- **Source URL**: https://arxiv.org/abs/2401.06620
- **Reference count**: 34
- **Primary result**: TransliCo improves cross-lingual transfer for multilingual PLMs across languages with different scripts through transliteration contrastive modeling (TCM) alongside MLM, achieving consistent gains on sentence retrieval, text classification, and sequence labeling tasks.

## Executive Summary
TransliCo addresses the script barrier in multilingual pretrained language models (mPLMs) by fine-tuning an mPLM using a combination of Masked Language Modeling (MLM) and Transliteration Contrastive Modeling (TCM) objectives. The framework uses Latin-script transliterations of sentences as a bridge to align representations from different scripts in the same subspace. Experiments with Glot500-m demonstrate that TransliCo effectively improves cross-lingual transfer performance across related languages with different scripts, particularly in the Indic language group.

## Method Summary
TransliCo fine-tunes a pretrained mPLM (Glot500-m) by contrasting sentences in their original scripts with their Latin-script transliterations using a TCM objective, alongside standard MLM. The TCM objective uses the 8th layer's mean-pooled representations and InfoNCE loss to pull positive pairs (original sentence + transliteration) together while pushing negative pairs apart. The overall training objective combines MLM loss on both original and transliterated sentences with TCM loss, all with equal weights. The model is fine-tuned on 5% of the Glot500-c pretraining corpus, and Uroman is used for transliteration.

## Key Results
- FURINA (the fine-tuned model) consistently improves over the baseline Glot500-m on sentence retrieval, text classification, and sequence labeling tasks
- Case study on Indic languages shows TransliCo's effectiveness for groups of related languages with different scripts
- The combination of MLM and TCM objectives prevents catastrophic forgetting while improving cross-script alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TransliCo uses Latin-script transliterations as a bridge to align representations from different scripts in the same subspace.
- Mechanism: By contrasting original sentences with their Latin transliterations in the TCM objective, the model learns to assign similar representations to semantically equivalent sentences regardless of script, effectively "pulling" representations from different scripts into closer proximity in the embedding space.
- Core assumption: Transliteration preserves enough semantic information for meaningful contrastive learning, and the Latin script serves as a neutral common ground that can represent all scripts without excessive ambiguity.
- Evidence anchors:
  - [abstract] "we use TCM to learn better-aligned representations by contrasting the positive pairs (paired data) against negative pairs (the remaining in-batch samples)."
  - [section 3.2] "By doing this, we can improve the crosslinguality of the model across related languages that use different scripts, as transliteration has shown to be effective in capturing morphological inflection."
  - [corpus] Weak: Corpus neighbors show some prior work on transliteration but no direct evaluation of Latin-script bridge effect.
- Break condition: If transliteration introduces too much ambiguity (e.g., for logographic scripts like Chinese), the contrastive learning signal becomes noisy and alignment fails.

### Mechanism 2
- Claim: MLM on both original and transliterated text increases lexical overlap, which helps the model generalize cross-lingually.
- Mechanism: When the model sees the same word in different scripts (original and transliterated), the shared vocabulary space forces the model to learn shared representations for semantically similar tokens, improving cross-script transfer.
- Core assumption: Related languages share enough morphological and lexical features that transliteration creates meaningful overlap without losing critical distinctions.
- Evidence anchors:
  - [abstract] "The MLM is performed on both the original sentences and their transliterations in Latin script."
  - [section 3.1] "By doing this, we can improve the crosslinguality of the model across related languages that use different scripts, as transliteration has shown to be effective in capturing morphological inflection."
  - [corpus] Weak: No corpus evidence directly measuring lexical overlap improvements from MLM on transliterations.
- Break condition: If the original and transliterated vocabularies diverge too much (e.g., due to complex phonological rules), the MLM signal becomes ineffective.

### Mechanism 3
- Claim: The combination of MLM and TCM objectives prevents catastrophic forgetting while improving cross-script alignment.
- Mechanism: MLM preserves the model's language modeling ability learned during pretraining, while TCM specifically targets the script barrier. The balanced loss weighting ensures neither objective dominates.
- Core assumption: The source model has sufficient cross-lingual knowledge to build upon, and the small fine-tuning dataset is representative enough to generalize.
- Evidence anchors:
  - [abstract] "The overall training objective of TRANSLI CO is then the sum of the MLM loss (from the original data and the transliteration data) and the TCM loss."
  - [section 3.3] "By fine-tuning an mPLM with this overall training objective, the model is expected to (1) not forget the language modeling ability gained in its pretraining phase."
  - [corpus] Weak: Corpus neighbors mention post-training alignment but no evaluation of catastrophic forgetting prevention.
- Break condition: If the fine-tuning dataset is too small or unrepresentative, the model may overfit to specific language-script pairs without generalizing.

## Foundational Learning

- Concept: Contrastive learning objectives (InfoNCE loss)
  - Why needed here: TCM uses contrastive learning to align representations by pulling positive pairs together and pushing negative pairs apart in the embedding space.
  - Quick check question: What is the mathematical form of the InfoNCE loss used in TCM, and how does it balance alignment vs uniformity?

- Concept: Masked Language Modeling pretraining objective
  - Why needed here: MLM on both original and transliterated text ensures the model maintains language modeling ability while increasing cross-script lexical overlap.
  - Quick check question: How does the standard 15% masking rate affect the MLM objective when applied to both original and transliterated sentences?

- Concept: Representation space geometry and alignment metrics
  - Why needed here: Understanding how different scripts cluster in the embedding space and how TransliCo affects this clustering is crucial for evaluating success.
  - Quick check question: What geometric properties (e.g., clustering, uniformity) would you expect to see in the 8th layer representations before and after TransliCo fine-tuning?

## Architecture Onboarding

- Component map: Data preparation (original text + Uroman transliterations) -> Model initialization (Glot500-m) -> Fine-tuning with MLM + TCM objectives -> Checkpoint selection (early stopping) -> Evaluation on downstream tasks

- Critical path: Data preparation → Model initialization (Glot500-m) → Fine-tuning with MLM + TCM objectives → Checkpoint selection (early stopping) → Evaluation on downstream tasks. The most critical step is ensuring the TCM objective receives properly paired data and correct batch construction.

- Design tradeoffs: Using only 5% of pretraining data balances computational efficiency with effectiveness, but may limit generalization. The choice of 8th layer for TCM is based on prior findings but could be suboptimal for some tasks. Latin script as the bridge script is practical but may introduce ambiguity for logographic languages.

- Failure signatures: Poor performance on script groups with high transliteration ambiguity (e.g., Hani scripts), catastrophic forgetting of original script capabilities, or failure to improve over baseline on sequence-level tasks while showing improvements on token-level tasks.

- First 3 experiments:
  1. Verify TCM objective: Run with only TCM (no MLM) and check if representations from different scripts cluster together in the embedding space.
  2. Test MLM contribution: Run with only MLM on both original and transliterated text and measure lexical overlap improvements.
  3. Evaluate catastrophic forgetting: Compare performance on Latin-script languages with and without including Latin-script data in fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed TransliCo framework perform when integrated into the pretraining phase rather than as a post-pretraining fine-tuning step?
- Basis in paper: [inferred] The authors mention that due to limited computation budget, they were unable to pretrain a model from scratch or continue pretraining using the full Glot500-c corpus. They suggest that integrating TransliCo into pretraining might further benefit the model.
- Why unresolved: The authors only evaluated TransliCo as a post-pretraining fine-tuning approach due to computational constraints.
- What evidence would resolve it: Experiments comparing models pretrained with and without TransliCo, using the same data and architecture, would show the impact of integrating TransliCo during pretraining.

### Open Question 2
- Question: How does TransliCo perform on groups of related languages beyond the Indic group studied in the paper?
- Basis in paper: [explicit] The authors conducted a case study on the Indic group of languages and showed consistent improvement. They also mention that they did not validate the framework further by trying more groups of related languages.
- Why unresolved: The paper only provides evidence for the effectiveness of TransliCo on the Indic group of languages.
- What evidence would resolve it: Conducting case studies on other groups of related languages (e.g., Slavic, Germanic, Romance) and comparing the performance of models fine-tuned with and without TransliCo would provide evidence for the framework's effectiveness on other language groups.

### Open Question 3
- Question: How does the choice of the 8th layer for mean-pooling in the Transliteration Contrastive Modeling (TCM) objective affect the performance of TransliCo?
- Basis in paper: [explicit] The authors mention that they chose the 8th layer based on previous empirical findings, as the first layers are weak in terms of crosslinguality, and the last layers are too specialized on the pretraining task.
- Why unresolved: The authors did not experiment with different layers or provide evidence for the optimal choice of the layer for mean-pooling in the TCM objective.
- What evidence would resolve it: Experiments comparing the performance of TransliCo using different layers for mean-pooling in the TCM objective would show the impact of this choice on the framework's effectiveness.

## Limitations

- The evaluation framework focuses heavily on related languages within the same script group, providing limited analysis of completely unrelated script pairs
- The claim that TCM "pulls" representations into alignment relies on untested assumptions about transliteration quality across all script types
- The 5% data usage constraint may limit the model's ability to learn robust cross-script mappings, especially for low-resource language pairs

## Confidence

- **High confidence**: The core technical contribution of combining MLM with TCM for cross-script alignment is well-specified and reproducible. The experimental setup using standard downstream tasks (Taxi1500, WikiANN, UD) provides reliable baselines for comparison.
- **Medium confidence**: The mechanism claims about how TCM creates semantic alignment through transliteration are plausible but rely on untested assumptions about transliteration quality and script group characteristics. The performance improvements on Indic languages are promising but may not generalize to all language families.
- **Low confidence**: The claim that TransliCo prevents catastrophic forgetting is supported only by the inclusion of Latin-script data in training, without direct measurement of performance degradation on original script languages. The effectiveness of the 8th-layer TCM objective is based on prior work but not validated for this specific application.

## Next Checks

1. **Transliteration quality analysis**: Systematically evaluate the semantic preservation of Uroman transliterations across different script families (Indic, Cyrillic, Hani, etc.) using parallel corpus alignment metrics. This would validate the core assumption that transliteration creates meaningful semantic bridges.

2. **Geometric analysis of representation space**: Visualize and quantify the clustering of representations from different scripts in the embedding space before and after TransliCo fine-tuning. Use metrics like uniformity, alignment, and intra-class compactness to measure the actual impact of TCM on cross-script alignment.

3. **Cross-family transfer evaluation**: Test TransliCo on script pairs from completely different language families (e.g., Hindi-Arabic, Vietnamese-Chinese) to determine whether the learned cross-script alignment generalizes beyond related languages or is limited to specific script groups.