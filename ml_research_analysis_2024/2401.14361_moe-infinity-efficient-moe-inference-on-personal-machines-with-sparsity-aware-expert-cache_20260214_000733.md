---
ver: rpa2
title: 'MoE-Infinity: Efficient MoE Inference on Personal Machines with Sparsity-Aware
  Expert Cache'
arxiv_id: '2401.14361'
source_url: https://arxiv.org/abs/2401.14361
tags:
- expert
- cache
- experts
- activation
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoE-Infinity introduces a sparsity-aware expert cache for efficient
  inference of MoE-based LLMs on personal machines with limited GPU memory. By tracing
  expert activation patterns at the request level, it identifies frequently reused
  experts and guides cache replacement and prefetching, significantly reducing per-token
  latency.
---

# MoE-Infinity: Efficient MoE Inference on Personal Machines with Sparsity-Aware Expert Cache

## Quick Facts
- **arXiv ID**: 2401.14361
- **Source URL**: https://arxiv.org/abs/2401.14361
- **Reference count**: 14
- **Primary result**: Achieves 3.1–16.7× latency improvements for MoE-based LLM inference on personal machines

## Executive Summary
MoE-Infinity addresses the challenge of efficiently running Mixture-of-Experts (MoE) language models on personal machines with limited GPU memory. The system introduces a sparsity-aware expert cache that leverages request-level expert activation patterns to predict and prefetch frequently used experts. By tracing sparse expert activation at the request level and using this information for cache replacement and prefetching, MoE-Infinity significantly reduces per-token latency while enabling deployment of large MoE models on resource-constrained hardware.

## Method Summary
MoE-Infinity implements a request-level tracing mechanism that monitors expert activation patterns during LLM inference. The system maintains an Expert Activation Matrix Collection (EAMC) that stores historical activation patterns, enabling prediction of future expert usage. When processing new requests, the system matches their activation patterns with historical data to guide cache replacement and prefetching decisions. The approach is specifically designed for single-user environments where MoE models typically operate with batch size of one, allowing it to exploit the high activation sparsity inherent in this setting.

## Key Results
- Achieves 3.1–16.7× latency improvements over state-of-the-art systems like vLLM, Ollama, DeepSpeed, and BrainStorm
- Reduces per-token latency by effectively caching and prefetching frequently reused experts
- Demonstrates effectiveness across multiple MoE models including DeepSeek-V2-Lite, Mixtral-8x7B, Switch Transformers, Meta NLLB-MoE, and Snowflake Arctic
- Validated on diverse tasks including BIGBench, FLAN, and MMLU datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparsity-aware expert cache achieves 3.1–16.7× latency improvements by leveraging request-level expert activation patterns
- Mechanism: The system traces sparse expert activation at the request level, identifying frequently reused experts within individual requests. This information guides cache replacement and prefetching, reducing per-token latency
- Core assumption: MoE models on personal machines with batch size of one exhibit high activation sparsity, with a small subset of experts being frequently reused during decoding
- Evidence anchors:
  - [abstract] "The key idea for MoE-Infinity is that on personal machines, which are often single-user environments, MoE-based LLMs typically operate with a batch size of one. In this setting, MoE models exhibit a high degree of activation sparsity, meaning a small number of experts are frequently reused in generating tokens during the decode phase"
  - [section 4.1] "We conduct an extensive trace study across widely used MoE models for various LLM inference tasks, with key findings highlighted below. For MoE models with around 100 experts (e.g., DeepSeek, QWen-MoE, NLLB, and Switch-MoE), fewer than 5% of experts are repeatedly activated when decoding tokens for a single request"
  - [corpus] Weak evidence - no direct mention of sparsity-aware expert cache mechanism

### Mechanism 2
- Claim: The Expert Activation Matrix Collection (EAMC) enables effective prediction of expert activation by matching request-level activation patterns
- Mechanism: EAMC stores historical request-level Expert Activation Matrices (EAMs). When a new request arrives, the system matches its EAM with those in the EAMC to predict future expert activation, facilitating efficient prefetching and caching
- Core assumption: Expert activation patterns within individual requests are similar enough to be grouped, and these patterns can be used to predict future activations
- Evidence anchors:
  - [section 4.4] "We validate this in two folds: (i) Existence of expert activation groups, (ii) Transition between different groups is hard to predict. By prediction we consider few existing methods in the literature (Appendix A), none of the existing work fully meets the SGR model for MoE"
  - [section 4.5] "We thus must trace the sparse activation of experts at the request level and our tracing must reflect the entire group of experts. For this purpose, we have designed a novel data structure termed the Expert Activation Matrix Collection (EAMC), which acts as a trace for keeping historical request-level EAMs online"
  - [corpus] Weak evidence - no direct mention of EAMC mechanism

### Mechanism 3
- Claim: Integrating layer proximity into cache replacement strategy improves performance by prioritizing experts in later layers
- Mechanism: When deciding which expert to replace in the cache, the system considers not only the activation likelihood but also the layer proximity. Experts in later layers are given higher priority for caching as they are more likely to benefit from prefetching
- Core assumption: The execution of MoE layers is sequential, and experts in later layers can be effectively prefetched based on the activation patterns observed in earlier layers
- Evidence anchors:
  - [section 4.6] "By assigning higher caching priorities to experts in these initial layers, we not only counteract potential prefetching failures but also exploit the layer-by-layer execution property of MoE models: the subsequent layers are executed later and they are more likely to benefit from prefetching and thus less need caching"
  - [section 4.7] "As expert from all layers needs to be considered, the decay starts from the first layer"
  - [corpus] Weak evidence - no direct mention of layer proximity mechanism

## Foundational Learning

- Concept: Expert Activation Matrix (EAM)
  - Why needed here: EAM is the fundamental data structure used to represent and analyze expert activation patterns in MoE models
  - Quick check question: What are the dimensions of an EAM for a model with L layers and E experts per layer, and what does each element represent?

- Concept: Cosine Distance
  - Why needed here: Cosine distance is used to measure the similarity between Expert Activation Matrices when matching new requests with historical patterns in the EAMC
  - Quick check question: How is cosine distance calculated between two vectors, and why is it preferred over Euclidean distance for comparing EAMs?

- Concept: Cache Replacement Strategies
  - Why needed here: Understanding cache replacement strategies (like LRU, LFU) is crucial for grasping the novelty of the sparsity-aware approach and how it differs from conventional methods
  - Quick check question: What are the main differences between LRU and LFU cache replacement strategies, and in what scenarios might one be preferred over the other?

## Architecture Onboarding

- Component map:
  - Token Routing -> Expert Cache Lookup -> On-demand Fetching -> Expert Execution -> Result Aggregation

- Critical path:
  1. Token routing decision
  2. Expert cache lookup
  3. On-demand fetching (if cache miss)
  4. Expert execution on GPU
  5. Result aggregation

- Design tradeoffs:
  - EAMC size vs. prediction accuracy: Larger EAMC can capture more patterns but increases memory usage and search time
  - Cache size vs. hit rate: Larger cache can hold more experts but may increase eviction frequency
  - Prediction frequency vs. overhead: More frequent predictions can improve accuracy but increase computational overhead

- Failure signatures:
  - High cache miss rate: Indicates poor prediction of expert activation patterns
  - GPU idle time: Suggests ineffective prefetching or cache management
  - Increased PCIe traffic: May indicate frequent on-demand fetching due to cache misses

- First 3 experiments:
  1. Measure cache hit rate and latency with different EAMC capacities (1, 10, 50, 100, 200 entries)
  2. Compare performance against LRU and LFU baseline cache strategies
  3. Evaluate robustness by testing with workload shifts between different LLM tasks and datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal EAMC capacity for different MoE models and workloads?
- Basis in paper: [explicit] The paper states that increasing EAMC capacity from 1 to 120 allows all MoE models to achieve their lowest average latency, but notes that a suitably small capacity (3% of total requests) is adequate for capturing most MoE activation patterns
- Why unresolved: The paper only provides a general range for EAMC capacity and does not specify the optimal capacity for each MoE model or workload type. The effectiveness of EAMC capacity consistently manifests across different MoE models, but the specific optimal capacity remains unknown
- What evidence would resolve it: Conducting extensive experiments with various EAMC capacities for different MoE models and workloads, measuring latency performance and cache hit rates, would provide insights into the optimal EAMC capacity for each scenario

### Open Question 2
- Question: How does MOE-Infinity handle workload changes and adapt to new activation patterns?
- Basis in paper: [explicit] The paper mentions that MOE-Infinity's tracer adapts to task shifts within the same dataset in around 50 requests and between datasets in around 30 requests, but does not provide detailed information on the adaptation process or how it handles more complex workload changes
- Why unresolved: The paper only provides limited information on MOE-Infinity's ability to handle workload changes and adapt to new activation patterns. The specific mechanisms and performance under various workload changes remain unclear
- What evidence would resolve it: Conducting experiments with diverse workload changes, including task shifts, dataset changes, and sudden spikes in request volume, would provide insights into MOE-Infinity's adaptation capabilities and performance under different scenarios

### Open Question 3
- Question: How does the sparsity-aware expert cache perform in terms of cache hit rates and latency improvements compared to other caching strategies?
- Basis in paper: [explicit] The paper mentions that MOE-Infinity achieves 3.1-16.7x latency improvements over state-of-the-art systems, but does not provide detailed comparisons of cache hit rates or performance against other caching strategies
- Why unresolved: The paper only provides high-level performance metrics and does not offer a comprehensive comparison of MOE-Infinity's caching strategy against other approaches in terms of cache hit rates, latency improvements, or memory usage
- What evidence would resolve it: Conducting detailed experiments comparing MOE-Infinity's caching strategy against other approaches, measuring cache hit rates, latency improvements, and memory usage under various workloads and MoE models, would provide a comprehensive understanding of its performance and advantages

## Limitations

- Limited generalizability to multi-user or server environments where request patterns become more diverse and expert activation less sparse
- Unknown robustness to workload shifts and sudden changes in activation patterns
- Sensitivity to hyperparameters without thorough sensitivity analysis provided

## Confidence

**High Confidence**: The fundamental observation that MoE models exhibit high activation sparsity on personal machines with batch size one is well-supported by the trace study (fewer than 5% of experts are repeatedly activated). The performance improvements over baseline systems (3.1-16.7× latency reduction) are clearly demonstrated across multiple models and tasks.

**Medium Confidence**: The mechanism by which request-level tracing and expert activation prediction leads to these performance gains is plausible but relies on assumptions about the stability of activation patterns. The effectiveness of layer proximity in cache replacement is theoretically sound but may have limited practical impact depending on the specific MoE model architecture.

**Low Confidence**: The scalability of the EAMC mechanism to handle diverse and evolving workloads, and the system's behavior under memory pressure scenarios, are not thoroughly validated. The paper lacks comprehensive testing of failure modes and edge cases.

## Next Checks

1. **Evaluate performance degradation in multi-user scenarios**: Deploy MoE-Infinity in a simulated multi-user environment with concurrent requests from different users. Measure cache hit rates, latency, and system throughput compared to single-user performance to quantify the impact of request diversity on the sparsity-aware approach.

2. **Test workload shift robustness**: Implement a continuous evaluation pipeline that monitors expert activation patterns over time. Introduce sudden shifts in workload characteristics (e.g., switching between different types of LLM tasks or datasets) and measure how quickly the system adapts, tracking changes in cache hit rate and latency.

3. **Conduct hyperparameter sensitivity analysis**: Systematically vary critical hyperparameters (EAMC capacity, cache size, decay factors, matching thresholds) across a wide range of values. Generate performance profiles to identify optimal configurations and determine which parameters have the most significant impact on system effectiveness.