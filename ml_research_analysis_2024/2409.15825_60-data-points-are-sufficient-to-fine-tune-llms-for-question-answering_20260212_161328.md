---
ver: rpa2
title: 60 Data Points are Sufficient to Fine-Tune LLMs for Question-Answering
arxiv_id: '2409.15825'
source_url: https://arxiv.org/abs/2409.15825
tags:
- subject
- object
- uni00000013
- data
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how few data points are needed to fine-tune
  large language models (LLMs) for question-answering tasks. It proposes a multi-template
  complementation mechanism to assess how well pretrained LLMs memorize different
  types of knowledge.
---

# 60 Data Points are Sufficient to Fine-Tune LLMs for Question-Answering

## Quick Facts
- arXiv ID: 2409.15825
- Source URL: https://arxiv.org/abs/2409.15825
- Authors: Junjie Ye; Yuming Yang; Qi Zhang; Tao Gui; Xuanjing Huang; Peng Wang; Zhongchao Shi; Jianping Fan
- Reference count: 40
- Primary result: As few as 60 data points are sufficient to activate encoded knowledge and enable LLMs to perform QA tasks effectively, with no significant gains from larger datasets

## Executive Summary
This study investigates how few data points are needed to fine-tune large language models for question-answering tasks. Using a multi-template complementation mechanism to assess knowledge memorization levels, the research demonstrates that as few as 60 data points can activate pre-trained knowledge and enable effective QA performance. The study evaluates four LLMs across three families on location-related topics, finding that memory level matching between training and test data is critical for optimal performance. The results challenge traditional assumptions about supervised fine-tuning data requirements and show that optimal fine-tuning strategies vary across different LLMs based on their pre-training corpora.

## Method Summary
The study proposes a multi-template complementation mechanism to categorize knowledge into five memory levels based on model memorization. Researchers fine-tune four LLMs (LLaMA-2-7B, LLaMA-2-13B, LLaMA-3-8B, Qwen-2-7B) using varying amounts of training data (60 to full dataset) from the ENTITYQUESTIONS dataset covering 12 location-related topics. The training uses AdamW optimizer with cosine scheduling, learning rate 2e-5 for LLaMA-2 models and 1e-5 for others, batch size 16, and 1 epoch. Performance is evaluated across memory levels for both in-domain and out-of-domain test sets using accuracy metrics.

## Key Results
- Fine-tuning with as few as 60 data points is sufficient to activate pre-trained knowledge for effective QA performance
- Training with higher memory-level data improves performance, while low-memory-level training impairs it
- Data requirements for fine-tuning vary across LLMs due to differences in their pre-training corpora
- No significant performance gains are observed when using larger datasets beyond 60-120 data points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning activates pre-trained knowledge rather than learning new knowledge from scratch
- Mechanism: The model's parameters already encode extensive world knowledge during pre-training. Fine-tuning with minimal data (60 points) serves to activate and refine this existing knowledge by adjusting parameters just enough to optimize the activation process
- Core assumption: The knowledge needed for QA tasks is already encoded in the model's parameters during pre-training
- Evidence anchors: Abstract states "as few as 60 data points during the SFT stage can activate the knowledge encoded during pre-training, enabling LLMs to perform the QA task"; Section 3.1 speculates that SFT "activates and refines knowledge already encoded during pre-training, requiring only minimal parameter tuning"

### Mechanism 2
- Claim: Memory level matching between training data and test data is critical for optimal performance
- Mechanism: When fine-tuning data and test data have similar memory levels, the model performs best because it's activating and refining knowledge at the appropriate granularity level
- Core assumption: The model encodes knowledge at different levels of memorization, and these levels can be reliably measured and matched
- Evidence anchors: Abstract notes "SFT with data of varying memory levels has a significant impact on LLM performance, with the optimal dataset differing based on the specific model being fine-tuned"; Section 3.2 Finding 3 shows "for test data of a particular memory level, LLMs trained with data of the same memory level tend to perform the best"

### Mechanism 3
- Claim: Different LLMs require different fine-tuning data based on their pre-training corpus characteristics
- Mechanism: Each LLM's knowledge distribution varies based on its unique pre-training corpus. Therefore, the optimal fine-tuning data for one model may be suboptimal for another, even when both are fine-tuned on the same task
- Core assumption: Variations in pre-training corpora lead to corresponding differences in knowledge distribution and fine-tuning requirements
- Evidence anchors: Abstract states "data requirements for fine-tuning vary across LLMs due to differences in their pre-training corpora"; Section 4.2 Finding 6 shows "LLMs with similar distributions of knowledge memory levels (e.g., LLaMA-2-7B and Qwen-2-7B) perform more consistently after fine-tuning with the same data"

## Foundational Learning

- Concept: Knowledge memorization levels and their measurement
  - Why needed here: Understanding how knowledge is encoded at different levels is fundamental to the multi-template complementation mechanism and the entire study's approach
  - Quick check question: What does a memory level of 0.75-1.0 indicate about a model's knowledge of a particular fact?

- Concept: Supervised fine-tuning (SFT) vs. pre-training
  - Why needed here: The study hinges on understanding the distinction between what's learned during massive pre-training vs. what's refined during SFT
  - Quick check question: How does the 60 data point finding challenge traditional assumptions about SFT data requirements?

- Concept: Domain adaptation and generalization
  - Why needed here: The study examines both in-domain and out-of-domain performance, requiring understanding of how models generalize knowledge
  - Quick check question: Why is it significant that 60 samples enable strong out-of-domain generalization?

## Architecture Onboarding

- Component map: Multi-template mechanism -> Data segmentation -> SFT pipeline -> Evaluation framework
- Critical path: 1) Measure base model knowledge levels using multi-template mechanism 2) Segment data into memory levels 3) Fine-tune with varying data amounts and memory levels 4) Evaluate performance across memory levels 5) Compare results across different LLMs
- Design tradeoffs: Data efficiency vs. performance (60 points may be sufficient but is this optimal?); Memory level granularity (5 levels chosen - could more or fewer provide better insights?); Template diversity vs. stability (21 templates with temperature 0.7 balances these concerns)
- Failure signatures: If performance doesn't improve with higher memory-level training data, the memory level measurement may be unreliable; If different LLMs show similar patterns despite different pre-training corpora, the corpus-specific hypothesis may be wrong; If performance degrades significantly with minimal data, the activation hypothesis may be incorrect
- First 3 experiments: 1) Reproduce the 60 data point finding with a new LLM family to test generalizability 2) Test whether fine-tuning with mixed memory levels performs better than single-level training 3) Investigate whether the multi-template mechanism's measurements correlate with other knowledge assessment methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different sampling temperatures during the multi-template complementation mechanism affect the reliability of memory level assessments?
- Basis in paper: The paper mentions using temperature 0.7 with 10 repetitions per mapping template to calculate memory levels
- Why unresolved: The paper does not explore how varying temperature values might impact the robustness of memory level discrimination
- What evidence would resolve it: Systematic experiments comparing memory level distributions across different temperature settings would show whether 0.7 is optimal or if other values provide more stable assessments

### Open Question 2
- Question: Why does fine-tuning with low-memory-level data impair performance on high-memory-level knowledge?
- Basis in paper: The paper observes that using data that models barely memorized for SFT "severely impairs its activation of high-memory-level knowledge"
- Why unresolved: The paper speculates about knowledge being "encoded differently" but doesn't explain the underlying mechanism
- What evidence would resolve it: Detailed analysis of parameter changes during fine-tuning with different memory-level data could reveal whether specific knowledge pathways are being disrupted

### Open Question 3
- Question: What specific characteristics of pre-training corpora lead to differences in fine-tuning data requirements across LLMs?
- Basis in paper: The paper notes that "variations in the pre-training corpora of LLMs lead to corresponding differences in their data requirements during the SFT stage"
- Why unresolved: The paper observes this phenomenon but doesn't analyze the specific corpus characteristics causing it
- What evidence would resolve it: Comparative analysis of pre-training data distributions and their correlation with optimal SFT data characteristics would identify which corpus features matter most

### Open Question 4
- Question: Does the optimal 60-data-point threshold scale with model size or remain constant across different LLM architectures?
- Basis in paper: The paper finds 60 data points sufficient across four models from three families but doesn't explore scaling properties
- Why unresolved: The study only tests with 60 data points and doesn't examine whether this number changes for larger or smaller models
- What evidence would resolve it: Systematic experiments varying both data quantity and model size would reveal whether the threshold scales proportionally or remains fixed

## Limitations

- Domain specificity: Findings may not generalize to domains requiring specialized or technical knowledge outside location-related topics
- Template dependency: The multi-template complementation mechanism relies on specific mapping templates that may not capture all aspects of knowledge representation
- Model family bias: Study uses only four LLMs from three families in the 7-13B parameter range, limiting generalizability to larger models or different architectures

## Confidence

- High confidence in the core finding that minimal data points (60-120) can activate pre-trained knowledge for location-related QA tasks
- Medium confidence in the memory level matching hypothesis due to correlation evidence but unclear causal mechanism
- Medium confidence in corpus-specific fine-tuning requirements due to suggestive but not conclusive evidence
- Low confidence in extrapolating these findings to significantly different domains or model architectures

## Next Checks

1. Cross-domain replication: Test the 60 data point hypothesis on non-location domains (medical knowledge, technical subjects, or creative domains) to assess generalizability beyond the current scope

2. Alternative memory assessment: Validate the multi-template complementation mechanism's measurements against independent knowledge assessment methods, such as knowledge probing tasks or manual annotation of model capabilities

3. Scale-up validation: Evaluate whether the minimal data requirements hold for larger model families (70B+ parameter models) and different architectures to determine if the findings scale with model size