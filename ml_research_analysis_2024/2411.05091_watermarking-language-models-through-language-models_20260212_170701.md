---
ver: rpa2
title: Watermarking Language Models through Language Models
arxiv_id: '2411.05091'
source_url: https://arxiv.org/abs/2411.05091
tags:
- watermarking
- watermark
- language
- text
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a prompt-guided watermarking framework for
  large language models that embeds imperceptible yet detectable signals in generated
  text using modular components: a Prompting LM generates watermarking instructions,
  a Marking LM produces watermarked outputs conditioned on these instructions, and
  a Detecting LM classifies whether outputs carry a watermark. The framework operates
  entirely at the input level without requiring access to model internals.'
---

# Watermarking Language Models through Language Models

## Quick Facts
- arXiv ID: 2411.05091
- Source URL: https://arxiv.org/abs/2411.05091
- Authors: Agnibh Dasgupta; Abdullah Tanvir; Xin Zhong
- Reference count: 29
- Primary result: Introduces prompt-guided watermarking framework with 86-100% detection accuracy across 25 LM combinations, robust to fine-tuning, distillation, and adversarial attacks

## Executive Summary
This paper introduces a novel prompt-guided watermarking framework for large language models that embeds imperceptible yet detectable signals in generated text. The approach operates entirely at the input level without requiring access to model internals, making it practical for real-world deployment. The framework demonstrates strong generalization across diverse model architectures and remains robust under common transformations like fine-tuning, distillation, and adversarial attacks.

## Method Summary
The framework uses three cooperating components: a Prompting LM generates watermarking instructions from user prompts, a Marking LM produces watermarked outputs conditioned on these instructions, and a Detecting LM classifies whether outputs carry a watermark. Watermarks are embedded through instruction-induced modulation of the generation process, making them imperceptible to humans but statistically detectable. The system was evaluated across 25 different Prompting-Marking LM combinations using models like GPT-4o, Mistral, LLaMA3, and DeepSeek, with detection accuracies ranging from 86% to 100%.

## Key Results
- Detection accuracies ranging from 86% to 100% across 25 Prompting-Marking LM combinations
- Watermarks remain detectable after fine-tuning with LoRA/DeepSpeed and distillation into smaller models
- Framework withstands prompt-based adversarial attacks designed to suppress watermarking signals
- Generalization across architectures including GPT-4o, Mistral, LLaMA3, and DeepSeek

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Watermark signals can be embedded into LLM outputs through prompt-induced modulation of the generation process
- Mechanism: A Prompting LM generates system instructions based on user prompts. These instructions are concatenated with the user prompt and fed to a Marking LM, which generates watermarked outputs. The instruction serves as a soft controller that modulates the token generation process, shifting the output distribution compared to the baseline. The watermark is embedded at the distributional level—imperceptible to humans but consistently detectable under statistical analysis
- Core assumption: Modern instruction-tuned LMs, especially those refined through RLHF, have sufficient generation controllability to respond to natural language prompts and encode latent watermarking signals
- Evidence anchors: [abstract] "The framework comprises three cooperating components: a Prompting LM that synthesizes watermarking instructions from user prompts, a Marking LM that generates watermarked outputs conditioned on these instructions, and a Detecting LM trained to classify whether a response carries an embedded watermark." [section] "Instruction-tuned language models, particularly those refined through RLHF, exhibit a high degree of generation controllability in response to natural language prompts. This makes them highly amenable to watermark embedding, where instructions are used to induce subtle and consistent patterns into generated text without altering its semantics."
- Break condition: If the Marking LM lacks instruction-following capacity or if the instruction fails to meaningfully shift the output distribution, the watermark signal would be weak or undetectable

### Mechanism 2
- Claim: The watermark detection task can be effectively modeled as a discriminative inference problem using a binary classifier trained on watermarked and non-watermarked outputs
- Mechanism: A Detecting LM is trained to classify whether a given output contains a watermark. It implicitly learns a scoring function that reflects watermarking signal strength at each token by modeling fine-grained distributional cues. The classifier operates on the generated text alone, without requiring access to model internals
- Core assumption: The watermark signal embedded through prompt-based instruction is sufficiently strong and consistent to be learned as a discriminative feature by a binary classifier
- Evidence anchors: [abstract] "a Detecting LM trained to classify whether a response carries an embedded watermark." [section] "Given that watermark signals are embedded through instruction-induced modulation during generation, we approach detection as a discriminative learning task. Specifically, we train a binary classifier f (·) to distinguish watermarked outputs from non-watermarked text."
- Break condition: If the watermark signal is too subtle or inconsistent, the classifier may fail to learn meaningful discriminative features, resulting in poor detection accuracy

### Mechanism 3
- Claim: The watermarking framework remains robust under model transformations such as fine-tuning, distillation, and prompt-based adversarial attacks
- Mechanism: Watermark signals are embedded at the distributional level through prompt instructions, which are less sensitive to changes in model parameters or decoding strategies. Experiments show high detection accuracy even after fine-tuning with LoRA/DeepSpeed, distillation into smaller models, and exposure to adversarial prompts designed to suppress watermarking
- Core assumption: The prompt-based watermarking method is resilient because it operates through instruction-induced modulation rather than direct manipulation of model parameters or logits
- Evidence anchors: [abstract] "Experimental results show that watermark signals generalize across architectures and remain robust under fine-tuning, model distillation, and prompt-based adversarial attacks." [section] "A robust watermark should remain detectable even if the underlying M is later modified, either through additional fine-tuning or compression. This property is especially important given the widespread practice of customizing open-source models via instruction-tuning or distillation for downstream applications."
- Break condition: If the watermark signal is heavily dependent on specific model parameters or decoding behaviors, transformations like fine-tuning or distillation could significantly weaken or eliminate the signal

## Foundational Learning

- Concept: Instruction-tuned language models and their generation controllability
  - Why needed here: The framework relies on the Marking LM's ability to follow natural language instructions and modulate its generation accordingly. Understanding how instruction-tuning and RLHF enhance controllability is essential to grasp why prompt-based watermarking is feasible
  - Quick check question: How does RLHF improve a language model's ability to follow instructions compared to standard pretraining?

- Concept: Autoregressive text generation and token-level conditioning
  - Why needed here: The watermarking mechanism operates by influencing the token-by-token generation process. Understanding how the model conditions on context at each step is crucial for understanding how instructions can modulate output
  - Quick check question: In autoregressive generation, how does the model use previously generated tokens to predict the next token?

- Concept: Binary classification and discriminative learning
  - Why needed here: The Detecting LM is trained as a binary classifier to distinguish watermarked from non-watermarked text. Understanding the principles of discriminative learning and how classifiers learn to separate classes is essential for understanding the detection mechanism
  - Quick check question: What is the difference between generative and discriminative learning approaches in the context of classification tasks?

## Architecture Onboarding

- Component map: Prompting LM -> Marking LM -> Detecting LM
- Critical path:
  1. User submits prompt to Marking LM
  2. Prompt is forwarded to Prompting LM with fixed template
  3. Prompting LM generates system instruction
  4. Marking LM combines user prompt and instruction to generate watermarked output
  5. Detecting LM is trained on watermarked and non-watermarked outputs
- Design tradeoffs:
  - Flexibility vs. detectability: More varied instructions may improve adaptability but could weaken signal consistency
  - Model compatibility vs. performance: Framework works with diverse models but may perform best with instruction-tuned models
  - Black-box compatibility vs. control: Operating without model access limits direct manipulation but increases practicality
- Failure signatures:
  - Low detection accuracy: May indicate weak watermark signal or insufficient training data
  - Inconsistent instruction generation: Could suggest issues with the fixed prompt template or Prompting LM capability
  - Model-specific performance variations: Might reveal limitations in instruction-following across different architectures
- First 3 experiments:
  1. Single-model evaluation: Use one LM as both Prompting and Marking LM to establish baseline detectability
  2. Multi-model evaluation: Test framework across different Prompting–Marking LM combinations to assess generalizability
  3. Robustness testing: Apply fine-tuning, distillation, and adversarial attacks to evaluate watermark resilience under real-world conditions

## Open Questions the Paper Calls Out
- Question: How does the watermarking framework perform in multilingual settings, and what adaptations are necessary for non-English languages?
- Basis in paper: [explicit] The paper suggests future work could explore multilingual settings, indicating current limitations
- Why unresolved: The framework's effectiveness in languages other than English is not addressed
- What evidence would resolve it: Evaluating the watermarking framework on datasets containing multiple languages and assessing detection accuracy and robustness across them

## Limitations
- Exact prompt template content and formatting used to guide the Prompting LM is not specified
- Training hyperparameters for fine-tuning and distillation are not provided, making exact reproduction difficult
- Framework's applicability to base models or those without extensive instruction-following capabilities is unclear

## Confidence
- High confidence in the core mechanism: The prompt-based modulation approach is theoretically sound and well-supported by experimental results
- Medium confidence in generalization claims: Performance across 25 combinations is promising but limited to specific model families
- Low confidence in long-term robustness: Current resilience to known attacks may not extend to future attack strategies

## Next Checks
1. Template sensitivity analysis: Systematically vary the prompt template used to guide the Prompting LM and measure the impact on watermark detection accuracy and signal consistency across different Marking LM models
2. Cross-architecture generalization: Test the framework with a broader range of LLM architectures, including base models, different instruction-tuning approaches, and models from various research groups, to validate claims of broad applicability
3. Long-term robustness testing: Design and implement a battery of novel adversarial attack strategies that combine prompt manipulation, fine-tuning techniques, and output post-processing to stress-test the watermarking framework's resilience over time