---
ver: rpa2
title: Multi-model Ensemble Conformal Prediction in Dynamic Environments
arxiv_id: '2411.03678'
source_url: https://arxiv.org/abs/2411.03678
tags:
- prediction
- coverage
- each
- time
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a multi-model conformal prediction framework
  for dynamic environments with unknown distribution shifts. The core idea is to maintain
  multiple candidate models and adaptively select the most suitable one at each time
  step based on recent performance, rather than relying on a single fixed model.
---

# Multi-model Ensemble Conformal Prediction in Dynamic Environments

## Quick Facts
- arXiv ID: 2411.03678
- Source URL: https://arxiv.org/abs/2411.03678
- Authors: Erfan Hajihashemi; Yanning Shen
- Reference count: 40
- Primary result: SAMOCP achieves strongly adaptive regret while maintaining valid coverage probability in dynamic environments with unknown distribution shifts

## Executive Summary
This work introduces a multi-model conformal prediction framework for dynamic environments with unknown distribution shifts. The core idea is to maintain multiple candidate models and adaptively select the most suitable one at each time step based on recent performance, rather than relying on a single fixed model. The proposed Strongly Adaptive Multi-model Ensemble Online Conformal Prediction (SAMOCP) algorithm achieves strongly adaptive regret over all intervals while maintaining valid coverage probability. Theoretical analysis proves bounded coverage error and adaptive regret guarantees. Extensive experiments on CIFAR-10C, CIFAR-100C, and TinyImageNet-C datasets with both gradual and sudden distribution shifts demonstrate that SAMOCP consistently constructs more efficient prediction sets (smaller average width, higher single-width coverage) compared to state-of-the-art methods, while maintaining coverage close to the target value.

## Method Summary
SAMOCP maintains multiple experts with different step sizes and lifetimes, where each expert runs multi-model conformal prediction (MOCP) with its own miss coverage probabilities. At each time step, the framework selects the most suitable expert and model based on normalized weights that decay exponentially with performance loss. The miss coverage probabilities are updated individually for each model using pinball loss and SF-OGD, allowing the framework to adapt to changing data distributions. The expert lifetime formula λ(t) = g · max{n ∈ Z : 2^n : t ≡ 0 mod 2^n} creates a logarithmic number of experts at each time step, enabling coverage of all interval lengths simultaneously while achieving strongly adaptive regret guarantees.

## Key Results
- SAMOCP achieves smaller average prediction set widths compared to state-of-the-art methods (FACI, ScaleFreeOGD, SAOCP, SAOCP(MM))
- The framework maintains coverage probability close to the target value (90%) across all tested datasets and distribution shift scenarios
- SAMOCP demonstrates higher single-width coverage probability, indicating better performance for individual predictions
- The algorithm successfully adapts to both gradual and sudden distribution shifts in CIFAR-10C, CIFAR-100C, and TinyImageNet-C datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework achieves strongly adaptive regret by maintaining multiple experts with different step sizes and lifetimes, allowing each to specialize in different data distribution regimes.
- Mechanism: Each expert runs MOCP with its own step size and lifetime. The learner selects among active experts based on normalized weights that decay exponentially with expert loss. This ensures that experts better adapted to the current distribution receive higher selection probability.
- Core assumption: The active intervals defined by λ(t) = g · max{n ∈ Z : 2^n : t ≡ 0 mod 2^n} create a logarithmic number of experts at each time step, enabling coverage of all interval lengths simultaneously.
- Evidence anchors:
  - At the end of its lifetime, the expert becomes inactive, ensuring that it no longer affects the decision-making process
  - The active interval of the expert created at time t is defined as [t, t + λ(t) − 1]
  - Note that in equation (13), αm*n* is related to specific interval I, which can vary across different intervals with distinct distributions in a dynamic environment
- Break condition: If the maximum number of experts at each time exceeds system memory constraints, the logarithmic expert creation scheme could become infeasible.

### Mechanism 2
- Claim: The framework maintains valid coverage by updating miss coverage probabilities individually for each model within each expert, rather than using a single global probability.
- Mechanism: For each expert and each model, a separate miss coverage probability αm_t is maintained and updated using SF-OGD. The best possible coverage ᾱm_t is computed after observing the true label, and the loss L(ᾱm_t, αm_t) guides the update.
- Core assumption: The pinball loss function L(ᾱm_t, αm_t) = α(ᾱm_t - αm_t) - min{0, ᾱm_t - αm_t} provides a convex surrogate for coverage error that enables gradient-based updates.
- Evidence anchors:
  - To update miss coverage probability αm_t, we adopt the pinball loss [Koenker and Bassett, 1978]
  - ∇αm_t L(ᾱm_t, αm_t) = I[ᾱm_t < αm_t] - α = errm_t - α
  - When errm_t = 1, it signals that the coverage probability 1 - αm_t is too small, resulting in a prediction set that cannot encompass Yt true
- Break condition: If the learning rate η is set too high, the miss coverage probabilities could oscillate and fail to converge to values that maintain valid coverage.

### Mechanism 3
- Claim: The framework achieves better prediction set efficiency by dynamically selecting the most suitable model for each time step based on recent performance.
- Mechanism: Each expert maintains weights wm_t for each model. After observing the true label, weights are updated using wm_t+1 = wm_t exp(-ϵL(ᾱm_t, αm_t)). The normalized weights determine the probability of selecting each model's miss coverage probability.
- Core assumption: The exponential weight update rule with parameter ϵ allows the framework to adapt quickly to distribution shifts by reducing weights for poorly performing models.
- Evidence anchors:
  - The algorithm updates the weight associated with each model after revealing the true label Yt true. This update is performed with respect to the loss function of the corresponding miss coverage probability
  - Specifically, wm_t can be updated by wm_t+1 = wm_t exp(-ϵL(ᾱm_t, αm_t))
  - The learner selects the suitable miss coverage probability among all active experts according to the PMF ¯ht := (¯hn_t)n∈A(t)
- Break condition: If the step size ϵ is set too large, the framework could overreact to temporary performance fluctuations and switch models too frequently.

## Foundational Learning

- Concept: Online learning with expert advice and regret minimization
  - Why needed here: The framework treats each expert as a learning agent and minimizes regret relative to the best expert in hindsight, which is essential for the strongly adaptive guarantees
  - Quick check question: How does the framework ensure sublinear regret when there are multiple active experts at each time step?

- Concept: Conformal prediction and validity guarantees
  - Why needed here: The framework builds upon conformal prediction theory to construct prediction sets with guaranteed coverage probability, even in dynamic environments
  - Quick check question: What modification to standard conformal prediction allows the framework to handle non-exchangeable data?

- Concept: Adaptive miss coverage probabilities
  - Why needed here: Instead of using a fixed coverage probability, the framework updates individual probabilities for each model within each expert based on recent performance
  - Quick check question: How does the pinball loss function guide the update of miss coverage probabilities to maintain valid coverage?

## Architecture Onboarding

- Component map: Data -> Expert creation -> MOCP instance -> Weight update -> Model selection -> Prediction set construction -> True label observation -> Weight updates -> Next time step

- Critical path: Data → Expert selection → Model selection → Prediction set construction → True label observation → Weight updates → Next time step

- Design tradeoffs:
  - More experts provide better adaptivity but increase computational cost (O(T log T) complexity)
  - Larger step size ϵ enables faster adaptation but risks overreacting to noise
  - Multiple models improve efficiency but require maintaining separate miss coverage probabilities

- Failure signatures:
  - Coverage significantly below target: Step size ϵ too small or learning rate η too large
  - Prediction sets consistently too large: Models not being selected appropriately or weights not decaying properly
  - High regret: Experts not being selected efficiently or lifetimes not well-matched to distribution shift patterns

- First 3 experiments:
  1. Single expert with two models on synthetic data with gradual shift to verify basic functionality
  2. Multiple experts with fixed step sizes on data with sudden shifts to test expert selection
  3. Full framework with four models on CIFAR-10C with gradual shifts to validate efficiency improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SAMOCP compare to state-of-the-art methods when using different sets of learning models?
- Basis in paper: The paper states that SAMOCP outperforms existing methods in terms of prediction set size, regret, and single-width prediction sets while maintaining coverage close to the target value. However, it does not provide a direct comparison using different sets of learning models.
- Why unresolved: The paper only provides results using a specific set of learning models (ResNet-50, ResNet-18, GoogLeNet, and DenseNet-121). It would be valuable to see how SAMOCP performs with other combinations of models.
- What evidence would resolve it: Experimental results comparing SAMOCP to other methods using different sets of learning models would provide a clearer picture of its performance.

### Open Question 2
- Question: How does the choice of the hyperparameter g affect the performance of SAMOCP?
- Basis in paper: The paper mentions that the hyperparameter g is set to 8 for both SAMOCP and SAOCP(MM) through a grid search approach. However, it does not provide a detailed analysis of how different values of g impact the algorithm's performance.
- Why unresolved: The paper only presents results for a specific value of g, and it is unclear how the algorithm would perform with different choices of this hyperparameter.
- What evidence would resolve it: A comprehensive study of SAMOCP's performance with various values of g would help understand its sensitivity to this hyperparameter and identify the optimal setting.

### Open Question 3
- Question: How does SAMOCP handle scenarios with multiple distribution shifts occurring simultaneously?
- Basis in paper: The paper discusses SAMOCP's ability to adapt to both gradual and sudden distribution shifts. However, it does not explicitly address situations where multiple shifts occur concurrently.
- Why unresolved: In real-world applications, data distributions can change in complex ways, and it is unclear how SAMOCP would perform when faced with multiple simultaneous shifts.
- What evidence would resolve it: Experiments simulating scenarios with multiple concurrent distribution shifts would provide insights into SAMOCP's robustness and adaptability in such situations.

## Limitations

- Theoretical guarantees rely on bounded loss assumption for miss coverage probabilities, which may be violated with highly unstable data distributions
- Framework assumes access to multiple pre-trained models with different architectures, limiting applicability in resource-constrained settings
- Logarithmic expert creation scheme could become computationally prohibitive for very long sequences or environments with frequent distribution shifts

## Confidence

- Coverage validity guarantees: High - supported by conformal prediction theory and validated across multiple datasets
- Adaptive regret bounds: Medium - theoretical analysis assumes bounded losses and known parameters
- Efficiency improvements: Medium - demonstrated empirically but dependent on model diversity and distribution shift patterns

## Next Checks

1. Test SAMOCP with a single model to verify whether the framework maintains coverage when model diversity is limited
2. Implement a memory-constrained version that caps the maximum number of active experts and measure impact on regret and coverage
3. Evaluate performance on non-image datasets (e.g., tabular data with concept drift) to assess generalizability beyond computer vision tasks