---
ver: rpa2
title: Can MLLMs generate human-like feedback in grading multimodal short answers?
arxiv_id: '2412.19755'
source_url: https://arxiv.org/abs/2412.19755
tags:
- feedback
- such
- student
- https
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of grading multimodal short answers
  that include both text and diagrams, which is a common format in STEM education
  but lacks scalable automated solutions. To tackle this, the authors introduce the
  Multimodal Short Answer Grading with Feedback (MMSAF) problem and propose an automated
  data generation framework (MMSAF-DGF) that leverages LLM hallucinations to simulate
  student errors, creating a dataset of 2,197 instances across physics, chemistry,
  and biology.
---

# Can MLLMs generate human-like feedback in grading multimodal short answers?

## Quick Facts
- arXiv ID: 2412.19755
- Source URL: https://arxiv.org/abs/2412.19755
- Reference count: 29
- MLLMs achieved up to 62.5% accuracy in correctness prediction and 80.36% in image relevance assessment

## Executive Summary
This paper introduces MMSAF (Multimodal Short Answer Grading with Feedback), a novel task requiring MLLMs to grade responses containing both text and diagrams, predict correctness levels, assess image relevance, and generate explanatory feedback. The authors propose MMSAF-DGF, an automated data generation framework that leverages LLM hallucinations to create synthetic student responses by introducing errors into correct answers. Four MLLMs (ChatGPT, Gemini, Pixtral, Molmo) were evaluated on a dataset of 2,197 instances across physics, chemistry, and biology. Human experts assessed the feedback quality across five parameters, revealing ChatGPT as most aligned with human judgment for physics and chemistry, while Pixtral excelled in biology.

## Method Summary
The MMSAF-DGF framework generates synthetic multimodal student responses by introducing errors into correct answers using LLM hallucinations. Errors are applied separately to text and diagrams, then combined using a correctness matrix to create realistic responses. Four MLLMs were evaluated in a zero-shot manner on the generated dataset, assessing correctness prediction, image relevance, and feedback generation. Human evaluation by 9 subject matter experts across 5 parameters (fluency/grammar, emotional impact, feedback correctness, error mitigation, rubric-based error detection) provided qualitative assessment of feedback quality.

## Key Results
- Gemini achieved highest accuracy at 62.5% for correctness prediction and 80.36% for image relevance assessment
- ChatGPT generated feedback most aligned with human judgment in physics and chemistry
- Pixtral excelled in biology feedback quality
- Molmo showed strong bias toward labeling answers as "incorrect"
- All models struggled with domain-specific diagram understanding and complex reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MMSAF-DGF framework can generate realistic multimodal short answer datasets by leveraging LLM hallucinations to simulate common student errors.
- Mechanism: The framework introduces errors separately into the textual and visual components of correct answers using LLM hallucinations and manual diagram modifications, then combines them using a correctness matrix to generate realistic student responses.
- Core assumption: LLM hallucinations can effectively mimic the three main categories of student errors: errors made with confidence, misunderstanding, and conceptual change.
- Evidence anchors:
  - [abstract] The framework introduces errors separately in text and diagrams, then combines them using a correctness matrix to generate realistic student responses.
  - [section] "We argue that LLM hallucinations, particularly factual fabrication and inconsistency, can be used as a tool to simulate such responses."
  - [corpus] Weak - no direct corpus evidence for the effectiveness of LLM hallucinations in simulating student errors.
- Break condition: If LLM hallucinations fail to capture the nuanced patterns of student errors or if the manual diagram modification approach doesn't scale to complex diagrams.

### Mechanism 2
- Claim: MLLMs can effectively evaluate multimodal short answers by leveraging their diagram understanding, natural language comprehension, and comparative reasoning capabilities.
- Mechanism: MLLMs analyze both the textual and visual components of student responses, compare them against reference answers, and generate feedback based on their assessment of correctness and image relevance.
- Core assumption: MLLMs have sufficient prior knowledge and capabilities in diagram understanding, natural language comprehension, and comparative reasoning to perform this complex task.
- Evidence anchors:
  - [abstract] Evaluation of four MLLMs shows that they achieve accuracies of up to 62.5% in predicting answer correctness and up to 80.36% in assessing image relevance.
  - [section] "The MMSAF problems require the model to understand the semantics present in both the text and diagram. This involves the model to perform the following - Natural language understanding, Diagram understanding, Comparative reasoning."
  - [corpus] Weak - no direct corpus evidence for the specific capabilities of MLLMs in multimodal grading tasks.
- Break condition: If MLLMs fail to accurately interpret domain-specific diagrams or struggle with complex reasoning tasks involving both text and images.

### Mechanism 3
- Claim: Human evaluation using rubrics can effectively assess the quality of feedback generated by MLLMs in a semantically meaningful way.
- Mechanism: Subject matter experts score LLM-generated feedback across five parameters (fluency and grammatical correctness, emotional impact, level of feedback correctness, error mitigation, and rubrics for error detection) using a rubric-based approach.
- Core assumption: Rubric-based evaluation provides a more semantically accurate assessment of feedback quality than overlap-based metrics like BLEU or ROUGE.
- Evidence anchors:
  - [abstract] Human evaluation by 9 experts across 5 parameters confirms that ChatGPT provides feedback most aligned with human judgment.
  - [section] "The rubrics also serve as a way to evaluate the feedback quality semantically rather than using overlap-based approaches."
  - [corpus] Weak - no direct corpus evidence for the effectiveness of rubric-based evaluation in assessing LLM-generated feedback.
- Break condition: If expert evaluations show low inter-annotator agreement or if rubrics fail to capture important aspects of feedback quality.

## Foundational Learning

- Concept: Multimodal Learning and Integration
  - Why needed here: The MMSAF problem requires understanding and integrating information from both text and diagrams, which is a core challenge in multimodal learning.
  - Quick check question: What are the main challenges in integrating information from multiple modalities, and how do MLLMs address these challenges?

- Concept: Automated Data Generation and Augmentation
  - Why needed here: The MMSAF-DGF framework relies on generating synthetic data by introducing errors into correct answers, which requires understanding data generation techniques.
  - Quick check question: What are the advantages and limitations of using LLM hallucinations for data generation in educational contexts?

- Concept: Evaluation Metrics and Human Assessment
  - Why needed here: The paper employs both quantitative metrics (accuracy) and qualitative human evaluation to assess model performance, requiring understanding of different evaluation approaches.
  - Quick check question: How do rubric-based evaluation approaches differ from traditional overlap-based metrics, and what are their relative strengths and weaknesses?

## Architecture Onboarding

- Component map: MMSAF-DGF framework (data generation) -> Input: Standard questions and reference answers -> Error introduction module (textual and visual) -> Correctness matrix combiner -> Output: MMSAF dataset -> MLLM evaluation pipeline -> Input: MMSAF dataset -> MLLM models (ChatGPT, Gemini, Pixtral, Molmo) -> Evaluation metrics (accuracy, human assessment) -> Output: Model performance scores

- Critical path: MMSAF-DGF generates dataset → MLLMs evaluate dataset → Human experts assess feedback quality → Results inform model selection and improvement

- Design tradeoffs:
  - Automated data generation vs. real-world data collection (speed vs. authenticity)
  - Zero-shot evaluation vs. fine-tuning (generalizability vs. task-specific performance)
  - Rubric-based evaluation vs. automated metrics (semantic accuracy vs. scalability)

- Failure signatures:
  - Low accuracy in correctness prediction or image relevance assessment
  - Poor inter-annotator agreement in human evaluations
  - Feedback that is grammatically correct but semantically irrelevant
  - Models that consistently misclassify certain types of errors

- First 3 experiments:
  1. Test MMSAF-DGF with a small set of questions to verify error introduction and dataset generation
  2. Evaluate MLLMs on a subset of the dataset to identify performance patterns across subjects
  3. Conduct pilot human evaluations to refine rubrics and ensure consistent scoring

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the MMSAF dataset generation framework be extended to incorporate more complex diagram modifications beyond simple label removal and object replacement?
- Basis in paper: [explicit] The paper mentions that while LLMs can specify what needs to be modified in an image, actually modifying the image accordingly remains challenging, especially when using openly available models. The authors take a manual approach for simulating such errors as an initial step.
- Why unresolved: The current framework relies on manual modification of diagrams to simulate student errors, which is time-consuming and limits the complexity of modifications that can be made.
- What evidence would resolve it: Development and demonstration of automated methods for complex diagram modification using advanced image editing models or techniques, validated against human-generated modifications.

### Open Question 2
- Question: What specific fine-tuning strategies could improve the performance of MLLMs on the MMSAF task, particularly for models like Molmo that show strong biases in their predictions?
- Basis in paper: [explicit] The paper notes that Molmo exhibited a strong bias towards labelling answers as "Incorrect" and suggests that its performance indicates scope for improvement through fine-tuning.
- Why unresolved: The paper provides initial evaluation results but does not explore fine-tuning approaches or their potential impact on model performance.
- What evidence would resolve it: Experimental results showing improved accuracy and reduced bias after applying various fine-tuning strategies (e.g., supervised fine-tuning, reinforcement learning from human feedback) on the MMSAF dataset.

### Open Question 3
- Question: How does the performance of MLLMs on MMSAF tasks generalize to real-world student responses across different educational contexts and cultural backgrounds?
- Basis in paper: [inferred] The paper acknowledges that synthetic datasets provide a starting point but notes that collection and evaluation on real-life data ensures reliable deployment of such models in specific use cases.
- Why unresolved: The evaluation is based on a synthetic dataset generated from standardized questions, and there is no mention of testing on actual student responses from diverse educational settings.
- What evidence would resolve it: Comparative evaluation of MLLM performance on MMSAF tasks using both synthetic and real-world student response datasets from multiple educational institutions and cultural contexts.

## Limitations

- The effectiveness of LLM hallucinations in simulating authentic student errors remains untested against real student responses
- Manual diagram modification process lacks detailed specification, limiting reproducibility
- Zero-shot evaluation approach doesn't establish clear baselines for comparison with fine-tuned models

## Confidence

- High confidence: The MMSAF problem formulation is well-defined and addresses a real need in STEM education assessment. The framework's ability to generate a large dataset of multimodal responses is empirically validated.
- Medium confidence: MLLMs demonstrate measurable capabilities in multimodal grading, though their performance varies significantly across subjects and error types. The human evaluation methodology using rubrics is sound, but inter-annotator agreement wasn't reported.
- Low confidence: The claim that LLM hallucinations effectively simulate student errors requires further validation with real student data. The scalability of manual diagram modifications for complex scientific diagrams is uncertain.

## Next Checks

1. Validate synthetic data authenticity: Compare MMSAF-generated student responses against real student submissions to assess how well LLM hallucinations capture authentic error patterns and misconceptions.

2. Test diagram understanding limits: Systematically evaluate MLLM performance on progressively more complex diagrams, including domain-specific scientific notation and multi-step visual reasoning tasks.

3. Establish human evaluation reliability: Conduct inter-annotator agreement studies across all five evaluation parameters to quantify the consistency and reliability of expert feedback assessments.