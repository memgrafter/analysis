---
ver: rpa2
title: Preparing Lessons for Progressive Training on Language Models
arxiv_id: '2401.09192'
source_url: https://arxiv.org/abs/2401.09192
tags:
- training
- layer
- apollo
- layers
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient training of large
  Transformer-based language models, which are becoming increasingly resource-intensive.
  The authors propose Apollo, a novel method that accelerates training by learning
  high-layer functionality during training of low layers.
---

# Preparing Lessons for Progressive Training on Language Models

## Quick Facts
- arXiv ID: 2401.09192
- Source URL: https://arxiv.org/abs/2401.09192
- Authors: Yu Pan; Ye Yuan; Yichun Yin; Jiaxin Shi; Zenglin Xu; Ming Zhang; Lifeng Shang; Xin Jiang; Qun Liu
- Reference count: 9
- Key outcome: Apollo achieves 41.6% FLOPs saving on BERT-Base and 47.9% on GPT-Base, outperforming existing progressive training methods

## Executive Summary
This paper introduces Apollo, a method to accelerate training of large Transformer-based language models by learning high-layer functionality during low-layer training. Apollo employs low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. The method achieves state-of-the-art acceleration ratios while maintaining performance, even rivaling methods using pretrained models.

## Method Summary
Apollo accelerates Transformer training through progressive depth expansion with three key innovations: (1) low-value-prioritized sampling that preferentially trains shallower layers to save computation, (2) weight sharing across depths that allows early layers to implicitly learn functionality needed for later layers, and (3) interpolation-based depth expansion that is more stable than stacking. The method trains models from scratch with significant FLOPs savings while maintaining performance on downstream tasks.

## Key Results
- Achieves 41.6% FLOPs saving on BERT-Base and 47.9% on GPT-Base
- Outperforms existing progressive training methods including StackBERT and bert2BERT
- Matches performance of models using pretrained baselines despite training from scratch
- Demonstrates stable training dynamics during layer expansion through interpolation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Weight sharing across depth allows early layers to implicitly learn functionality needed for later layers, reducing redundancy and stabilizing training.
- **Mechanism**: During progressive training, the same parameter set is assigned to multiple depths via interpolation mapping. This shared representation is optimized once but benefits all corresponding layers, effectively transferring "lessons" learned in shallow layers to deeper ones without retraining.
- **Core assumption**: Deep Transformer layers share sufficient representational similarity such that parameters trained in low depths generalize to higher depths without catastrophic forgetting.
- **Evidence anchors**:
  - [abstract] "low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion"
  - [section] "we share the low-layer weights, enabling them to adapt to the selected layers by LVPS. These shared weights are well-prepared not only for learning high-layer functionality but also for recurrent transformation"
  - [corpus] Weak; no direct overlap found in neighbor titles/abstracts.
- **Break condition**: If layer functions are highly dissimilar, shared weights will fail to capture higher-layer semantics, causing training instability or performance collapse.

### Mechanism 2
- **Claim**: Low-value-prioritized sampling (LVPS) reduces FLOPs while retaining learning of high-layer properties by preferentially sampling shallower depths.
- **Mechanism**: At each training step, a probability function biased toward smaller depths is sampled. The model trains only on the sampled depth but uses shared weights that have already captured higher-layer patterns, so training efficiency is gained without losing representational power.
- **Core assumption**: Sampling lower depths most of the time yields sufficient gradient signal to train shared weights that can generalize to all depths.
- **Evidence anchors**:
  - [abstract] "Our approach involves low-value-prioritized sampling (LVPS) to train different depths"
  - [section] "LVPS tends to select shallower layers, it can greatly save computation costs"
  - [corpus] No relevant evidence in neighbor papers.
- **Break condition**: If sampling bias is too extreme, high-layer functionality may never be fully learned, leading to degraded model performance.

### Mechanism 3
- **Claim**: Interpolation-based depth expansion is more stable than stacking, producing smaller gradient spikes and smoother training dynamics.
- **Mechanism**: When increasing depth, interpolation maps old layer indices to new positions via rounding, ensuring neighboring relationships are preserved. This contrasts with stacking, which repeats layer sequences and can cause large shifts in output activation distributions.
- **Core assumption**: Preserving layer neighborhood during expansion reduces distribution shift in activations and gradients.
- **Evidence anchors**:
  - [section] "illustrated in Fig.5, directly stacking BERT-Base/2 exhibits a notable alteration in the distribution of output activations, whereas the interpolation method preserves this distribution"
  - [section] "Apollo showcases a reverse trend. Here, layer expansion within Apollo leads to a reduction in the loss and gradient"
  - [corpus] No relevant evidence in neighbor papers.
- **Break condition**: If interpolation mapping is too coarse, the model may lose fine-grained functional distinctions between layers, reducing performance.

## Foundational Learning

- **Concept**: Transformer architecture and layer composition
  - Why needed here: Apollo modifies how layers are trained and expanded; understanding layer roles (MHSA vs FFN) is essential to grasp why weight sharing and interpolation work.
  - Quick check question: What are the two main subcomponents of a standard Transformer block, and how do their parameters interact during forward pass?

- **Concept**: Progressive training and layer stacking
  - Why needed here: Apollo builds on progressive training but replaces stacking with interpolation and LVPS; knowing the baseline method clarifies the novelty and improvements.
  - Quick check question: In progressive training, how are new layers typically added, and what is the primary computational cost driver?

- **Concept**: Probability density functions and sampling strategies
  - Why needed here: LVPS relies on a custom inverse proportional PDF to bias sampling toward shallow layers; understanding this is key to implementing or tuning the method.
  - Quick check question: How does an inverse proportional PDF differ from uniform sampling in terms of expected sampled layer index?

## Architecture Onboarding

- **Component map**: Main training loop -> LVPS sampling -> weight sharing across depths -> interpolation expansion -> loss computation and backpropagation
- **Critical path**: Sampling (LVPS) -> weight assignment (SHARE) -> forward pass on sampled depth -> backward pass -> weight update
- **Design tradeoffs**: Weight sharing reduces parameters and FLOPs but risks over-constraining representations; LVPS reduces computation but may bias toward shallow patterns; interpolation trades off some functional specialization for stability
- **Failure signatures**: High gradient variance or exploding gradients (likely from stacking instead of interpolation); slow convergence or degraded performance (likely from over-aggressive LVPS); mismatch between training and inference depth (likely from incorrect SHARE mapping)
- **First 3 experiments**:
  1. Compare training loss and gradient norms when expanding from 6 to 12 layers using stacking vs interpolation on BERT-Base/2
  2. Evaluate FLOPs savings and final MLM loss when varying k in LVPS (e.g., k=0, k=5, k=10) on BERT-Base
  3. Test training from scratch vs. using a pretrained small model (bert2BERT) on GPT-Base for convergence speed and final loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the long-term performance implications of using low-value-prioritized sampling (LVPS) versus other sampling methods like uniform sampling (US) or full sampling (FS) in Apollo?
- Basis in paper: [explicit] The paper discusses the benefits of LVPS in terms of acceleration ratios and FLOP savings, but does not provide a long-term analysis of its impact on model performance.
- Why unresolved: The paper focuses on immediate training efficiency gains without exploring potential trade-offs in model generalization or performance over extended training periods.
- What evidence would resolve it: Conducting experiments comparing the long-term performance of models trained with LVPS, US, and FS across various tasks and datasets.

### Open Question 2
- Question: How does the interpolation method for model depth extension in Apollo compare to other methods like layer stacking in terms of training stability and convergence speed?
- Basis in paper: [explicit] The paper introduces the interpolation method and claims it enhances training stability, but does not provide a detailed comparison with other methods.
- Why unresolved: The paper does not provide a comprehensive analysis of the stability and convergence speed differences between interpolation and other methods.
- What evidence would resolve it: Conducting experiments that directly compare the stability and convergence speed of models trained with interpolation versus other methods.

### Open Question 3
- Question: What are the potential limitations of Apollo when applied to non-Transformer architectures or different types of neural networks?
- Basis in paper: [inferred] The paper focuses on Transformer-based models, implying that the method's effectiveness might vary with different architectures.
- Why unresolved: The paper does not explore the applicability of Apollo to non-Transformer architectures, leaving its generalizability unclear.
- What evidence would resolve it: Testing Apollo on a variety of neural network architectures to assess its effectiveness and identify any limitations.

## Limitations
- The paper lacks detailed empirical validation of the LVPS mechanism and its impact on long-term model performance
- The comparison to bert2BERT baseline is ambiguous, with unclear implementation details affecting reproducibility
- The method's effectiveness on non-Transformer architectures remains unexplored

## Confidence
- **High confidence**: Weight sharing mechanism and interpolation expansion (supported by explicit equations and experimental claims)
- **Medium confidence**: LVPS mechanism (described but lacking empirical validation details)
- **Medium-Low confidence**: Performance claims (not independently verified)

## Next Checks
1. Implement and test LVPS with varying k values (0, 5, 10) to verify the claimed computational efficiency vs. performance tradeoff
2. Compare gradient norms and training stability between Apollo's interpolation expansion and standard stacking during layer growth from 6â†’12 layers
3. Reproduce the FLOPs calculation methodology to verify the 41.6% and 47.9% savings claims against baseline progressive training approaches