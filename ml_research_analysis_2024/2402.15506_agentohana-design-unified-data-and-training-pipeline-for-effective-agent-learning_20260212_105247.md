---
ver: rpa2
title: 'AgentOhana: Design Unified Data and Training Pipeline for Effective Agent
  Learning'
arxiv_id: '2402.15506'
source_url: https://arxiv.org/abs/2402.15506
tags:
- data
- step
- arxiv
- agent
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of consolidating heterogeneous
  multi-turn agent trajectory data from diverse environments into a unified format
  for effective training of autonomous agents powered by large language models. The
  core method, AgentOhana, standardizes and unifies agent trajectories into a consistent
  JSON format, implements a filtering pipeline using an AgentRater to assess trajectory
  quality, and provides a generic data loader for distributed training.
---

# AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning

## Quick Facts
- arXiv ID: 2402.15506
- Source URL: https://arxiv.org/abs/2402.15506
- Authors: Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Ming Zhu, Juntao Tan, Thai Hoang, Zuxin Liu, Liangwei Yang, Yihao Feng, Shirley Kokane, Tulika Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan Wang, Caiming Xiong
- Reference count: 6
- One-line primary result: Successfully creates a comprehensive, high-quality agent trajectory dataset and develops xLAM-v0.1 model that outperforms or matches top-tier baselines across various agent-based tasks.

## Executive Summary
This paper addresses the challenge of consolidating heterogeneous multi-turn agent trajectory data from diverse environments into a unified format for effective training of autonomous agents powered by large language models. The core method, AgentOhana, standardizes and unifies agent trajectories into a consistent JSON format, implements a filtering pipeline using an AgentRater to assess trajectory quality, and provides a generic data loader for distributed training. This approach enables the training of xLAM-v0.1, a large action model that demonstrates strong performance across multiple benchmarks including Webshop, HotpotQA, ToolEval, and MINT-Bench, achieving high scores and rankings relative to other models.

## Method Summary
The paper proposes a three-stage pipeline: (1) Data Standardization - converting heterogeneous multi-turn agent trajectories from various environments into a unified JSON format capturing user queries, model responses, and structured steps; (2) Quality Filtering - using AgentRater to score entire trajectories on a 0-5 scale based on accuracy and efficiency, filtering out those below 4.0; and (3) Distributed Training - implementing a generic data loader that handles streaming, shuffling, and interleaving of datasets while preserving device-specific randomness to prevent bias. The unified dataset trains xLAM-v0.1 through supervised fine-tuning on pre-trained Mixtral-8x7B-Instruct.

## Key Results
- Successfully consolidated agent trajectories from 10 diverse environments into a unified JSON format
- xLAM-v0.1 achieves high scores across multiple benchmarks: Webshop, HotpotQA, ToolEval, and MINT-Bench
- The model outperforms or matches top-tier baselines in agent-based task performance
- Demonstrated effectiveness of trajectory-level quality filtering using AgentRater scoring system

## Why This Works (Mechanism)

### Mechanism 1: Data Heterogeneity Standardization
- Claim: Converting heterogeneous multi-turn agent trajectories into a unified JSON format enables consistent training across diverse environments.
- Mechanism: AgentOhana standardizes trajectory data into a homogeneous JSON structure capturing user query, model name, score, and structured steps (input, output, next observation). This unification allows a generic data loader to process all data uniformly.
- Core assumption: All environments can map their trajectory data to the unified JSON schema without losing critical information.
- Evidence anchors:
  - [abstract] "meticulously standardizes and unifies these trajectories into a consistent format"
  - [section 2.2] "we propose a unified agent data format, as depicted in row 2 of Figure 2"
  - [corpus] Weak evidence - corpus neighbors discuss multi-agent systems but don't directly address data standardization mechanisms
- Break condition: Environments with fundamentally incompatible data structures (e.g., real-time sensor streams vs. discrete API calls) that cannot be mapped to the JSON schema.

### Mechanism 2: Quality-Controlled Trajectory Filtering
- Claim: AgentRater evaluates entire agent trajectories using strong public models to filter out low-quality data before training.
- Mechanism: The AgentRater scores trajectories on a 0-5 scale based on overall accuracy and efficiency, removing trajectories scoring below 4.0. This ensures only high-quality trajectories enter the training pipeline.
- Core assumption: Strong public models (e.g., Mistral, ChatGPT) can reliably assess the quality of complex multi-turn agent trajectories.
- Evidence anchors:
  - [section 2.3] "we design a method, named AgentRater to rate the agent trajectory based on strong public models"
  - [section 2.3] "we rate the whole trajectory on agent data" rather than individual steps
  - [corpus] Weak evidence - corpus neighbors discuss evaluation but not trajectory-level quality assessment
- Break condition: Public models become unreliable at assessing domain-specific trajectory quality, or filtering removes too much data making the dataset too small.

### Mechanism 3: Distributed Training Randomness Preservation
- Claim: Preserving independent randomness across devices during dataset partitioning prevents bias introduction in distributed training.
- Mechanism: The training pipeline uses device-dependent seeding (combining default seed with process ID) during data shuffling and interleaving, ensuring each device sees a different data order while maintaining reproducibility.
- Core assumption: Device-specific randomness in data ordering prevents systematic biases that could arise from synchronized data access across devices.
- Evidence anchors:
  - [abstract] "preserves independent randomness across devices during dataset partitioning and model training"
  - [section 2.4.2] "we employ the init device seed function to diversify the controlled seeds based on the process ID"
  - [corpus] No direct evidence - corpus neighbors focus on training frameworks but not distributed randomness preservation
- Break condition: The seeding mechanism fails to maintain true independence across devices, or the computational overhead of device-specific shuffling becomes prohibitive.

## Foundational Learning

- Concept: JSON data serialization
  - Why needed here: AgentOhana relies on JSON dictionaries to store standardized trajectories, requiring understanding of nested data structures and serialization formats
  - Quick check question: How would you represent a multi-turn conversation with alternating user and assistant messages in JSON format?

- Concept: Distributed computing and data parallelism
  - Why needed here: The training pipeline uses multiple GPUs with device-specific data shuffling, requiring understanding of how data partitioning affects model training
  - Quick check question: What issues arise when multiple training devices access the same data in the same order?

- Concept: Model evaluation and quality assessment
  - Why needed here: AgentRater uses strong models to evaluate trajectory quality, requiring understanding of how to design evaluation prompts and interpret model outputs
  - Quick check question: How would you design a scoring rubric for evaluating multi-turn agent trajectories?

## Architecture Onboarding

- Component map: Raw environment data → Standardization → AgentRater filtering → DataLoader processing → Distributed training → Model evaluation

- Critical path: Raw environment data → Standardization → AgentRater filtering → DataLoader processing → Distributed training → Model evaluation

- Design tradeoffs:
  - Standardization vs. flexibility: The JSON schema provides consistency but may not capture all environment-specific nuances
  - Filtering strictness vs. dataset size: Higher AgentRater thresholds improve quality but reduce dataset size
  - Device randomness vs. reproducibility: Device-specific seeding prevents bias but complicates exact replication

- Failure signatures:
  - Training instability: Indicates poor data quality or insufficient filtering
  - Model bias toward certain environments: Suggests inadequate data balancing during standardization
  - Poor benchmark performance: May indicate over-filtering or insufficient training data

- First 3 experiments:
  1. Implement AgentRater scoring on a small sample of trajectories from 2-3 environments and analyze score distributions
  2. Test the generic data loader with heterogeneous datasets to verify proper shuffling and interleaving
  3. Run a small-scale training job with 2-4 GPUs to validate device-specific randomness preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal filtering thresholds for AgentRater scores to balance data quality and quantity in multi-turn agent trajectory datasets?
- Basis in paper: [inferred] The paper mentions using a score threshold of 4.0 for filtering trajectories but doesn't provide analysis on how different thresholds affect model performance or data diversity.
- Why unresolved: The paper doesn't explore how different filtering thresholds impact the downstream model performance or the trade-off between data quality and quantity.
- What evidence would resolve it: Experiments comparing model performance across different AgentRater score thresholds, along with analysis of data diversity metrics at each threshold.

### Open Question 2
- Question: How does the choice of AgentRater model (open-source vs. API-based) affect the quality and diversity of filtered trajectories?
- Basis in paper: [explicit] The paper mentions using both Mistral and ChatGPT for AgentRater but doesn't compare their effectiveness or impact on final model performance.
- Why unresolved: The paper uses AgentRater as a black box without exploring how different model choices affect the filtering outcomes.
- What evidence would resolve it: Comparative analysis of trajectories filtered by different AgentRater models and their corresponding impact on final model performance.

### Open Question 3
- Question: What is the optimal balance of data sources in the combined dataset to maximize performance across different agent tasks?
- Basis in paper: [inferred] While the paper mentions maintaining equilibrium across data sources, it doesn't explore how different mixing ratios affect performance on specific tasks or environments.
- Why unresolved: The paper uses fixed sampling probabilities for data sources without investigating whether these are optimal for different agent tasks.
- What evidence would resolve it: Ablation studies varying the sampling probabilities for different data sources and measuring their impact on task-specific performance metrics.

## Limitations

- The approach relies heavily on the quality and consistency of underlying public models used for trajectory evaluation, which could compromise the entire filtering pipeline if these models are biased or unreliable in certain domains.
- The JSON standardization assumes all environments can meaningfully map their data to the proposed schema, which may not hold for environments with fundamentally different interaction paradigms.
- The distributed training approach, while addressing bias through device-specific randomness, introduces complexity in reproducibility and may have performance implications for smaller datasets.

## Confidence

- **High Confidence**: The core claim that data standardization enables unified training is well-supported by the methodology description and implementation details. The JSON format specification and generic data loader design are clearly articulated.
- **Medium Confidence**: The effectiveness of AgentRater for trajectory quality assessment is plausible given the use of strong public models, but the paper lacks detailed validation of the evaluation criteria or comparison with alternative filtering methods.
- **Low Confidence**: The claim about preventing bias through device-specific randomness in distributed training is theoretically sound but not empirically validated. The paper asserts this prevents bias but doesn't demonstrate or measure bias in alternative training setups.

## Next Checks

1. **AgentRater Validation**: Implement a controlled test where AgentRater scores are compared against human expert ratings on a sample of trajectories from multiple environments to verify the filtering criteria's reliability.

2. **Data Schema Coverage Analysis**: Test the JSON standardization pipeline with trajectories from 2-3 additional environments not mentioned in the paper to identify edge cases where the schema breaks down or loses critical information.

3. **Distributed Training Bias Analysis**: Run identical training jobs with synchronized vs. device-specific shuffling on a small dataset and measure any differences in model performance or bias toward specific environment types.