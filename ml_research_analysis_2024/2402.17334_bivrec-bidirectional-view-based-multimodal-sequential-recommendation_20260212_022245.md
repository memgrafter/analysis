---
ver: rpa2
title: 'BiVRec: Bidirectional View-based Multimodal Sequential Recommendation'
arxiv_id: '2402.17334'
source_url: https://arxiv.org/abs/2402.17334
tags:
- recommendation
- information
- interest
- multimodal
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BivRec, a bidirectional view-based multimodal
  sequential recommendation framework that jointly trains recommendation tasks under
  both ID and multimodal views. BivRec addresses limitations of existing multimodal
  recommendation paradigms by leveraging the synergistic relationship between views
  to improve performance bidirectionally.
---

# BiVRec: Bidirectional View-based Multimodal Sequential Recommendation

## Quick Facts
- arXiv ID: 2402.17334
- Source URL: https://arxiv.org/abs/2402.17334
- Reference count: 40
- Primary result: Achieves up to 34.23% improvement in Recall@20 over baselines on multimodal sequential recommendation

## Executive Summary
BiVRec introduces a bidirectional view-based multimodal sequential recommendation framework that jointly trains on ID and multimodal views to leverage their synergistic relationship. The method addresses limitations of existing paradigms by using multi-scale interest embedding, intra-view interest decomposition with Gaussian and Cluster attention, and cross-view interest learning. Experiments on five public datasets demonstrate state-of-the-art performance with significant improvements over competitive baselines.

## Method Summary
BiVRec constructs structured interest representations for ID and multimodal views separately, then learns their synergistic relationship through both coarse-grained semantic similarity (using contrastive learning) and fine-grained interest allocation similarity. The framework uses multi-scale patching to capture user interests at different temporal granularities, applies Gaussian attention for feature interaction within each scale, and employs Cluster attention with Gumbel-softmax to create structured interest representations. The model is trained using a multi-task loss that combines recommendation objectives with contrastive and interest allocation losses.

## Key Results
- Achieves up to 34.23% improvement in Recall@20 compared to state-of-the-art baselines
- Demonstrates robust performance under noise and effectiveness for cold-start scenarios
- Shows superior cross-dataset recommendation capability while maintaining lower training costs than pure multimodal approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training of ID and multimodal views enables bidirectional performance improvement by learning synergistic relationships between the two representations.
- Mechanism: The model constructs structured interest representations for each view separately using multi-scale interest embedding and intra-view interest decomposition. Then it learns the synergistic relationship through both coarse-grained overall semantic similarity (using contrastive learning) and fine-grained interest allocation similarity.
- Core assumption: User interests can be represented in both ID and multimodal views, and these representations contain complementary information that can mutually enhance each other.
- Evidence anchors: [abstract], [section 2.1], [corpus]

### Mechanism 2
- Claim: Multi-scale interest embedding captures user interests at different temporal granularities by expanding interaction sequences with non-overlapping patching operations.
- Mechanism: The interaction sequence is segmented into different scales, and additive concatenation aggregates item embeddings for each scale. Positional embeddings annotate both scale information and interaction sequence position information.
- Core assumption: User purchasing behavior for the next item can be influenced by combinations of multiple items across different time scales, not just the most recent items.
- Evidence anchors: [section 2.3], [corpus]

### Mechanism 3
- Claim: Cluster attention with Gumbel-softmax creates structured interest representations by forcing each item to allocate interest information to a fixed number of interest tokens.
- Mechanism: The model uses trainable interest tokens and computes attention scores between items and interests. A filter operation with hyperparameter F ensures each item assigns interest information to only the F closest interest tokens. Gumbel-softmax provides differentiable approximation of the hard assignment.
- Core assumption: User interests have distinct, non-overlapping aspects that can be represented by separate interest tokens, and items belong to specific interest categories rather than having continuous interest distributions.
- Evidence anchors: [section 2.4.2], [corpus]

## Foundational Learning

- Concept: Multi-head self-attention mechanism
  - Why needed here: Understanding the baseline attention mechanism is crucial for grasping how BivRec modifies it with Gaussian attention and Cluster attention
  - Quick check question: How does multi-head attention differ from single-head attention, and why is it useful for modeling user interests?

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The coarse-grained learning component uses contrastive learning to align interest representations from the two views
  - Quick check question: What is the intuition behind using InfoNCE loss for learning semantic similarity between representations?

- Concept: Positional encoding in transformer models
  - Why needed here: BivRec uses positional embeddings to incorporate temporal information in both the multi-scale embedding and the attention mechanisms
  - Quick check question: How do positional encodings help transformers handle sequential data, and what are the differences between absolute and relative positional encodings?

## Architecture Onboarding

- Component map: Input -> Multi-scale Interest Embedding -> Intra-View Interest Decomposition -> Cross-View Interest Learning -> Recommendation Loss
- Critical path: Input ‚Üí Multi-scale Embedding ‚Üí Intra-View Decomposition ‚Üí Cross-View Learning ‚Üí Recommendation Loss
- Design tradeoffs:
  - Using frozen multimodal encoders reduces training cost but may limit fine-tuning capability
  - Structured interest representations improve interpretability but add complexity compared to standard attention
  - Joint training requires balancing two recommendation tasks, which may complicate optimization
- Failure signatures:
  - Poor performance on cold-start users may indicate insufficient interest clustering
  - Degradation when adding noise suggests weak robustness in interest representation learning
  - Imbalanced improvement between ID and multimodal views may indicate suboptimal synergistic learning
- First 3 experiments:
  1. Ablation study removing the Cluster Attention layer to verify its contribution to performance
  2. Modality ablation testing with only image, only text, and both modalities to understand modality importance
  3. Temperature sensitivity test for Gumbel-softmax in Cluster Attention to find optimal exploration-exploitation balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BivRec handle situations where one modality (e.g., text or image) is missing for certain items, and what is the impact on performance when using only a single modality versus both?
- Basis in paper: [explicit] The paper mentions that "For simplicity, we omit the subscript ùë¢ for simplicity, and (ii) as the recommendation tasks in both views utilize the same model structure, we only provide the formulas for the recommendation task in the ID view as an example." It also states "If both are used, their features will be concatenated to create a joint representation."
- Why unresolved: The paper does not provide experimental results or analysis on the performance of BivRec when using only one modality or when one modality is missing.
- What evidence would resolve it: Experiments comparing the performance of BivRec using only text, only image, and both modalities, as well as analysis of how missing modalities affect the model's performance.

### Open Question 2
- Question: How does the choice of the number of interests ùêæ and the number of interests each item can belong to ùêπ affect the model's performance, and what is the optimal configuration for different datasets?
- Basis in paper: [explicit] The paper states "As depicted in Figure 5 (a), the hyperparameters ùêæ, ùêπ play a critical role in determining the formation of interest representation by adjusting the number of interests per user and the number of interests each item can belong to."
- Why unresolved: While the paper mentions the importance of these hyperparameters, it does not provide a detailed analysis of their impact on performance or guidelines for choosing optimal values.
- What evidence would resolve it: Experiments systematically varying ùêæ and ùêπ across different datasets and providing insights into the relationship between these hyperparameters and model performance.

### Open Question 3
- Question: How does BivRec's performance compare to pure multimodal models when applied to cross-dataset recommendation tasks, and what are the limitations of BivRec in this scenario?
- Basis in paper: [explicit] The paper mentions that "BivRec-MM is effectively learning the information patterns that exist across datasets and is outperforming the approach of fusing multimodal and ID information."
- Why unresolved: The paper only provides results for BivRec-MM on cross-dataset recommendation tasks, but does not compare its performance to pure multimodal models or discuss potential limitations.
- What evidence would resolve it: Experiments comparing BivRec-MM's performance to pure multimodal models on cross-dataset recommendation tasks, as well as an analysis of the limitations and challenges faced by BivRec in this scenario.

## Limitations

- The effectiveness of bidirectional learning depends on the alignment quality between ID and multimodal interest representations, which may vary across datasets
- Multi-scale interest embedding requires careful hyperparameter tuning, and optimal scale parameters may be dataset-specific
- Cluster Attention performance is sensitive to the filter hyperparameter F, which controls the granularity of interest clustering

## Confidence

- **High Confidence**: The core bidirectional learning framework and its potential to improve recommendation performance by leveraging complementary information from ID and multimodal views
- **Medium Confidence**: The specific implementation details of multi-scale interest embedding and Cluster Attention, as these involve several design choices that could affect performance
- **Medium Confidence**: The robustness claims regarding noise tolerance and cold-start scenarios, as these were demonstrated on the tested datasets but may not generalize to all scenarios

## Next Checks

1. Conduct cross-dataset transfer learning experiments where models trained on one dataset are evaluated on another to assess the generalizability of the learned interest representations
2. Perform systematic ablation studies varying the number of interest tokens (K) and filter parameter (F) across different datasets to identify optimal configurations and understand their impact on performance
3. Implement controlled experiments adding different types and levels of noise to both ID and multimodal inputs to quantify the model's robustness and identify failure modes