---
ver: rpa2
title: 'Low-rank finetuning for LLMs: A fairness perspective'
arxiv_id: '2405.18572'
source_url: https://arxiv.org/abs/2405.18572
tags:
- fine-tuning
- lora
- fine-tuned
- toxic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how low-rank fine-tuning methods like LoRA
  affect model fairness and toxicity mitigation in large language models. Experiments
  across multiple models (OPT 1.3B, Llama-2 7B, GPT-2) and tasks (toxicity mitigation,
  sequential classification) show that LoRA models with lower ranks (commonly used
  in practice) retain more toxic behaviors and biases from baseline models compared
  to fully fine-tuned counterparts.
---

# Low-rank finetuning for LLMs: A fairness perspective

## Quick Facts
- arXiv ID: 2405.18572
- Source URL: https://arxiv.org/abs/2405.18572
- Reference count: 40
- Primary result: Lower LoRA ranks (commonly used in practice) retain more toxic behaviors and biases from baseline models compared to fully fine-tuned counterparts

## Executive Summary
This study investigates how low-rank fine-tuning methods like LoRA affect model fairness and toxicity mitigation in large language models. Experiments across multiple models (OPT 1.3B, Llama-2 7B, GPT-2) and tasks (toxicity mitigation, sequential classification) show that LoRA models with lower ranks retain more toxic behaviors and biases from baseline models compared to fully fine-tuned counterparts. The results demonstrate that while LoRA offers computational efficiency, it may inadequately capture fine-tuning data distribution shifts critical for fairness and toxicity mitigation.

## Method Summary
The study evaluates LoRA fine-tuning with various ranks (2, 4, 8, 16, 32, 64) compared to full fine-tuning across three models (Llama-2 7B, OPT 1.3B, GPT-2) on two tasks: toxicity mitigation using the HONEST dataset and sequential classification on IMDb and SST2 datasets. Models are fine-tuned for 1 epoch with batch size of 8 and learning rate of 5e-5 for toxicity mitigation, and batch size of 16 with learning rate of 2e-5 for classification. Performance is evaluated using toxicity classifiers, accuracy disparity metrics, and KL-divergence measurements between token distributions.

## Key Results
- LoRA models with rank 2 preserved toxic completions in 35-50% of cases versus <15% for fully fine-tuned models
- Lower-rank LoRA models exhibited accuracy disparities up to 14.5% between majority and minority groups, compared to 5.9% for fully fine-tuned models
- Lower-rank LoRA models have smaller divergence from baseline token distributions (measured via KL-divergence), indicating reduced adaptation to fine-tuning datasets

## Why This Works (Mechanism)

### Mechanism 1
Lower LoRA ranks preserve more toxic behaviors from baseline models because they fail to capture the distribution shift introduced by the fine-tuning data. LoRA approximates full fine-tuning by learning low-rank matrices that update the pre-trained weights. With lower ranks, these matrices have limited capacity to represent the complex changes needed to shift away from the original (toxic) distribution. The token posterior distribution remains closer to the baseline model's distribution, resulting in retained toxic behaviors.

### Mechanism 2
Lower LoRA ranks increase accuracy disparity between majority and minority groups in downstream classification tasks because they fail to capture nuanced decision boundaries. In classification tasks, lower-rank LoRA models create less distinct decision boundaries between classes compared to fully fine-tuned models. This results in reduced margins around the classification boundaries, making the model more sensitive to group differences and exacerbating accuracy disparities.

### Mechanism 3
Lower LoRA ranks create a "false sense of alignment" because they retain baseline model characteristics while appearing to perform well on standard metrics. LoRA models with lower ranks maintain computational efficiency and may achieve similar performance metrics as fully fine-tuned models on general test sets. However, they preserve undesirable behaviors from the baseline model because the low-rank adaptation doesn't sufficiently diverge from the original distribution.

## Foundational Learning

- **Low-rank matrix approximation**: Understanding how LoRA works at a mathematical level is crucial for grasping why lower ranks might be insufficient for capturing complex distribution shifts. *Quick check*: What is the computational complexity difference between updating full weight matrices versus low-rank matrices in LoRA?

- **KL-divergence and distributional similarity**: The paper uses KL-divergence to measure how much the fine-tuned model's token distribution differs from the baseline, which is central to understanding why lower ranks preserve toxic behaviors. *Quick check*: How does KL-divergence quantify the difference between two probability distributions over tokens?

- **Fairness metrics (accuracy parity, harmful bias gap)**: These metrics are used to evaluate how well different LoRA ranks perform on fairness criteria, which is a key focus of the paper. *Quick check*: What is the difference between accuracy parity and the harmful bias gap as fairness metrics?

## Architecture Onboarding

- **Component map**: Pre-trained LLMs (OPT 1.3B, Llama-2 7B, GPT-2) -> LoRA fine-tuning with varying ranks -> evaluation on toxicity/fairness metrics -> comparison with fully fine-tuned baselines

- **Critical path**: Model loading -> LoRA fine-tuning with specified rank -> generation of completions -> toxicity/fairness evaluation -> statistical analysis of distribution divergence

- **Design tradeoffs**: Lower ranks provide computational efficiency but may inadequately capture distribution shifts; higher ranks capture more information but lose efficiency benefits

- **Failure signatures**: Models retaining toxic completions despite fine-tuning, accuracy disparities between groups increasing with lower ranks, KL-divergence remaining high between LoRA and baseline distributions

- **First 3 experiments**:
  1. Implement LoRA fine-tuning on a small model with ranks 2, 8, 32, 64 and compare toxic completion retention rates
  2. Measure KL-divergence between fine-tuned models and baseline across different ranks
  3. Evaluate accuracy parity across groups for LoRA models with different ranks on a binary classification dataset

## Open Questions the Paper Calls Out

### Open Question 1
How do different LoRA rank configurations affect the model's ability to generalize across different downstream tasks beyond text completion and classification? The paper mentions evaluating LoRA models across multiple tasks but doesn't explore task-specific rank requirements.

### Open Question 2
What is the relationship between LoRA rank and the model's capacity to maintain original capabilities while acquiring new task-specific skills? The paper shows that lower ranks retain more toxic behaviors but doesn't systematically analyze the trade-off between preserving original capabilities and learning new behaviors.

### Open Question 3
How does the effectiveness of LoRA vary with different pre-training dataset characteristics and model architectures? The paper tests multiple models but doesn't systematically analyze how pre-training data diversity or model architecture affects LoRA performance.

### Open Question 4
What are the long-term effects of using low-rank fine-tuning on model behavior in production environments? The paper focuses on immediate fine-tuning effects but doesn't address how LoRA's limitations might manifest over extended use.

### Open Question 5
Can hybrid approaches combining LoRA with other parameter-efficient fine-tuning methods overcome the identified limitations? The paper identifies limitations of pure LoRA methods but doesn't explore potential solutions or combinations with other techniques.

## Limitations
- The paper relies on simulated/counterfactual data for toxicity mitigation evaluation, which may not fully represent real-world scenarios
- The study focuses on relatively small models (up to 7B parameters) and doesn't examine whether findings scale to larger models
- The fairness metrics used are task-specific and may not capture all dimensions of bias in language models

## Confidence
- Core claims about rank-capacity trade-offs affecting toxicity retention: Medium
- Experimental methodology is well-specified and results are statistically significant
- Simulation-based evaluation introduces uncertainty about real-world applicability
- KL-divergence measurements provide strong quantitative support for distributional shift claims

## Next Checks
1. Replicate the toxicity mitigation experiments using real-world toxic/non-toxic text pairs rather than generated counterfactuals to verify the rank-dependent behavior holds under more naturalistic conditions.

2. Conduct ablation studies isolating whether the observed effects stem from LoRA's rank limitations specifically versus other aspects of the fine-tuning procedure (learning rate, batch size, epochs).

3. Extend the evaluation to larger models (e.g., 13B-70B parameters) to determine whether the rank-fairness trade-off relationship observed here persists at scale.