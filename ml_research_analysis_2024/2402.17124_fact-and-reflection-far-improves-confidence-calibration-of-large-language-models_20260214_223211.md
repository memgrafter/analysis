---
ver: rpa2
title: Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language
  Models
arxiv_id: '2402.17124'
source_url: https://arxiv.org/abs/2402.17124
tags:
- confidence
- prompting
- calibration
- methods
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how different prompting strategies affect
  the confidence calibration of large language models (LLMs). The authors find that
  while various prompting methods improve expected confidence calibration, they also
  cause LLMs to be overconfident on certain instances.
---

# Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models

## Quick Facts
- **arXiv ID:** 2402.17124
- **Source URL:** https://arxiv.org/abs/2402.17124
- **Reference count:** 33
- **Primary result:** FaR improves confidence calibration (lower ECE) by having models reflect on generated facts before answering.

## Executive Summary
Large language models often produce overconfident predictions, especially when using popular prompting strategies like Chain-of-Thought or Self-Ask. This paper introduces Fact-and-Reflection (FaR), a two-step prompting method that first elicits relevant facts from the model, then asks it to reflect on those facts before generating a final answer. Experiments show FaR significantly improves expected calibration error (ECE) across multiple question-answering datasets, while also encouraging the model to express concerns in uncertain scenarios—potentially enabling better retrieval augmentation for difficult instances.

## Method Summary
The paper evaluates six prompting strategies on GPT-3.5 for question-answering tasks, measuring their confidence calibration using ECE and MacroCE metrics. Fact-and-Reflection (FaR) is proposed as a two-step method: first generating relevant facts and sources, then reflecting on them before producing the final answer. The approach is motivated by cognitive science research on mitigating human overconfidence through fact acquisition and reflection. Experiments compare FaR against baseline methods on StrategyQA and WebQuestions datasets.

## Key Results
- FaR reduces Expected Calibration Error (ECE) by 23.5% compared to standard prompting on multi-purpose question-answering tasks.
- The method improves calibration across both reasoning and knowledge-intensive datasets (StrategyQA and WebQuestions).
- FaR prompts models to express concerns in less confident scenarios, potentially triggering retrieval augmentation for harder instances.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fact-and-Reflection (FaR) prompting improves confidence calibration by separating fact elicitation from reflective reasoning.
- **Mechanism:** By first asking the model to generate relevant facts and their sources, and then prompting reflection over those facts before generating the final answer, FaR reduces anchoring bias. This prevents the model from prematurely anchoring its reasoning to the first fact encountered, allowing a more balanced consideration of multiple perspectives.
- **Core assumption:** Human cognitive biases, such as anchoring bias, are relevant to how LLMs generate answers and can be mitigated by prompt design.
- **Evidence anchors:**
  - [abstract] "Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known 'facts' that are relevant to the input prompt from the LLM. And then it asks the model to 'reflect' over them to generate the final answer."
  - [section] "Psychological and cognitive research (Block and Harper, 1991; George et al., 2000) indicates that human's over-confidence can be mitigated by disentangling the processes of fact acquisition and reasoning."
  - [corpus] "Weak corpus evidence; no direct citations of FaR in related work yet."
- **Break condition:** If the model fails to generate relevant facts or sources, the reflection step may not effectively mitigate overconfidence.

### Mechanism 2
- **Claim:** FaR prompting elicits the model's capability to express concerns in less confident scenarios, which aids in identifying hard instances.
- **Mechanism:** The reflection step encourages the model to verbalize concerns about its answers, such as insufficient evidence or conditional requirements. This expression of concern correlates with lower confidence scores and often lower accuracy, signaling instances where additional verification or external knowledge might be needed.
- **Core assumption:** LLMs can introspect their confidence and verbalize it in a way that correlates with their actual performance.
- **Evidence anchors:**
  - [abstract] "Notably, FaR prompting even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances."
  - [section] "Further analysis reveals that the improvement comes from that FaR prompting intrigues the model to generate cautious answers that express concerns, such as adding a comment like 'there is no sufficient evidence' after the answer."
  - [corpus] "Weak corpus evidence; no direct citations of FaR in related work yet."
- **Break condition:** If the model does not consistently express concerns when uncertain, the mechanism may not effectively identify hard instances.

### Mechanism 3
- **Claim:** Including sources for generated facts stabilizes the model's reflection and improves calibration.
- **Mechanism:** By prompting the model to identify sources for its generated facts, FaR encourages the model to consider the trustworthiness of the information. This can lead to more cautious and accurate reflections, as the model is aware of the reliability of its knowledge base.
- **Core assumption:** Trustworthiness of information sources can influence the model's confidence in its answers.
- **Evidence anchors:**
  - [abstract] "Upon acquiring the component thoughts, The final answer A is sampled from p(A|Q, Tf, Tr, θ), i.e., the model generates the answer with the final step prompt including thoughts at each step."
  - [section] "Motivated by (Weller et al., 2023), we believe the trustworthiness of the sources can help stabilize the model reflection Tr."
  - [corpus] "Weak corpus evidence; no direct citations of FaR in related work yet."
- **Break condition:** If the model generates irrelevant or incorrect sources, this may not improve reflection and could potentially introduce errors.

## Foundational Learning

- **Concept:** Confidence calibration
  - **Why needed here:** Understanding confidence calibration is crucial for evaluating how well the model's confidence scores match its actual performance. This is the primary metric used to assess the effectiveness of FaR prompting.
  - **Quick check question:** What are the two main metrics used to evaluate confidence calibration in this paper, and how do they differ?

- **Concept:** Prompt engineering and its impact on LLM behavior
  - **Why needed here:** FaR is a specific prompt engineering technique. Understanding how different prompting strategies influence LLM behavior is essential for grasping why FaR improves calibration.
  - **Quick check question:** How do step decomposition prompting methods generally affect the model's overall expected confidence calibration compared to standard prompting?

- **Concept:** Cognitive biases in decision-making
  - **Why needed here:** FaR is inspired by cognitive science research on mitigating human overconfidence through fact acquisition and reflection. Understanding these biases helps explain the rationale behind FaR's design.
  - **Quick check question:** What cognitive bias does FaR aim to mitigate, and how does its design address this bias?

## Architecture Onboarding

- **Component map:** Question -> Fact generation -> Source identification -> Reflection -> Final answer
- **Critical path:**
  1. Generate facts relevant to the question.
  2. Identify sources for the generated facts.
  3. Reflect over the facts and sources.
  4. Generate the final answer based on the reflection.
- **Design tradeoffs:**
  - Longer prompts due to multiple steps may increase computational cost but improve calibration.
  - Requiring sources for facts adds complexity but enhances reflection quality.
  - Potential loss in accuracy (calibration tax) in exchange for better confidence calibration.
- **Failure signatures:**
  - Model fails to generate relevant facts or sources.
  - Reflection step does not effectively mitigate overconfidence.
  - Model does not consistently express concerns when uncertain.
- **First 3 experiments:**
  1. Implement FaR prompting and compare ECE and MacroCE with standard prompting.
  2. Test FaR with human-annotated facts to isolate the effect of fact quality on calibration.
  3. Analyze the correlation between expressed concerns and confidence scores/accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FaR prompting compare to other state-of-the-art prompting methods like CoT and Self-Ask when applied to datasets beyond question-answering, such as summarization or machine translation?
- Basis in paper: [inferred] The paper mentions the potential for extending FaR to other tasks, but does not provide empirical evidence.
- Why unresolved: The paper primarily focuses on question-answering datasets and does not explore the generalizability of FaR to other natural language processing tasks.
- What evidence would resolve it: Conducting experiments on a variety of NLP tasks using FaR and comparing the results to other prompting methods would provide evidence of its generalizability.

### Open Question 2
- Question: How does the quality of the model-generated facts and sources in FaR prompting impact the overall performance and confidence calibration of the model?
- Basis in paper: [explicit] The paper discusses the use of model-generated facts and sources in FaR, but does not provide a detailed analysis of their quality or impact.
- Why unresolved: The paper does not provide a comprehensive evaluation of the quality of the model-generated facts and sources, nor does it explore how their quality affects the performance of FaR.
- What evidence would resolve it: Analyzing the quality of the model-generated facts and sources and correlating it with the performance of FaR would provide insights into their impact on the overall effectiveness of the method.

### Open Question 3
- Question: How does the inclusion of external knowledge sources, such as retrieved facts from a knowledge base, impact the performance and confidence calibration of FaR prompting?
- Basis in paper: [explicit] The paper mentions the potential for using external knowledge sources to augment the model's internal knowledge, but does not provide empirical evidence of its effectiveness.
- Why unresolved: The paper does not explore the use of external knowledge sources in conjunction with FaR, nor does it evaluate the impact of such sources on the model's performance and confidence calibration.
- What evidence would resolve it: Conducting experiments using FaR with external knowledge sources and comparing the results to FaR without external sources would provide evidence of their impact on the method's effectiveness.

## Limitations
- FaR relies on the model's ability to generate relevant facts and articulate concerns, which may not scale to domains where factual grounding is scarce or ambiguous.
- The study focuses on open-domain question answering, so performance on specialized or closed-domain tasks remains unverified.
- While ECE and MacroCE improve, the potential trade-off in raw accuracy versus calibration is not fully explored.

## Confidence
- **High confidence:** FaR improves expected calibration (ECE, MacroCE) across multiple datasets and prompting strategies. This is directly measured and statistically significant.
- **Medium confidence:** FaR's improvement is primarily due to the model expressing concerns in uncertain scenarios. While the correlation is observed, causation is inferred rather than proven.
- **Medium confidence:** The design of FaR (fact-then-reflection) is inspired by cognitive science on mitigating human overconfidence. The connection is theoretically sound but not directly validated in the model's internal reasoning.

## Next Checks
1. **Control for prompt length:** Since FaR uses longer prompts, run an ablation study comparing FaR to a similarly long standard prompt to ensure calibration gains are not due to prompt verbosity alone.

2. **Test robustness to fact quality:** Evaluate FaR when facts are provided externally (human-annotated) versus generated by the model to isolate the impact of fact generation versus reflection on calibration.

3. **Measure anchoring bias directly:** Design an experiment where the same facts are presented in different orders or with varying relevance to test if FaR consistently mitigates premature anchoring compared to other methods.