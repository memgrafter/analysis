---
ver: rpa2
title: Improving conversion rate prediction via self-supervised pre-training in online
  advertising
arxiv_id: '2401.16432'
source_url: https://arxiv.org/abs/2401.16432
tags:
- data
- training
- auto-encoder
- learning
- conversions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a self-supervised pre-training framework to
  improve conversion rate prediction in online advertising systems. The core challenge
  is data sparsity, where click events and click-attributed conversions are rare,
  making it difficult to train accurate models.
---

# Improving conversion rate prediction via self-supervised pre-training in online advertising

## Quick Facts
- arXiv ID: 2401.16432
- Source URL: https://arxiv.org/abs/2401.16432
- Authors: Alex Shtoff; Yohay Kaplan; Ariel Raviv
- Reference count: 37
- Key outcome: Self-supervised pre-training framework improves conversion rate prediction with 0.6% CPM lift and 0.67% revenue lift in Yahoo native advertising system

## Executive Summary
This paper addresses the critical challenge of data sparsity in online advertising conversion rate prediction, where click events and click-attributed conversions are rare. The authors propose a self-supervised pre-training framework that leverages an auxiliary auto-encoder trained on all conversion events (both click-attributed and not) as a feature extractor to enrich the main conversion rate prediction model. The framework is specifically adapted to the online advertising setup with a loss function designed for tabular data, continual learning capabilities, and integration into a large-scale real-time ad auction system. The approach demonstrates improvements in both offline training and online A/B tests while preserving model calibration, which is essential for achieving advertiser goals.

## Method Summary
The authors present a self-supervised pre-training framework to address data sparsity in conversion rate prediction for online advertising. The core innovation involves using an auxiliary auto-encoder trained on all conversion events as a feature extractor, which is then used to enrich the main conversion rate prediction model. This approach leverages additional data without impairing model calibration, critical for advertiser goals. The framework is adapted to the online advertising setup by using a loss function designed for tabular data, facilitating continual learning, and incorporating a neural network into a large-scale real-time ad auction system. The proposed method demonstrates improvements in offline training and online A/B tests, with a 0.6% CPM lift and 0.67% revenue lift, and is now fully deployed in the Yahoo native advertising system.

## Key Results
- 0.6% CPM lift in online A/B tests
- 0.67% revenue lift in online A/B tests
- Successful deployment in Yahoo native advertising system
- Improvements demonstrated in both offline training and online A/B tests

## Why This Works (Mechanism)
The self-supervised pre-training framework works by leveraging the auxiliary auto-encoder to extract features from all conversion events, including those not attributed to clicks. This creates a richer feature representation that captures patterns in the broader conversion data, addressing the data sparsity problem where click events and click-attributed conversions are rare. By using all available conversion data, the model gains exposure to a wider variety of conversion patterns and user behaviors, leading to improved prediction accuracy. The framework maintains calibration by carefully designing the pre-training objective and integration method, ensuring that the enriched model remains reliable for advertiser decision-making while improving overall performance metrics.

## Foundational Learning
- **Data Sparsity in CTR/Conversion Prediction**: Understanding why click events and attributed conversions are rare in online advertising is essential for appreciating the problem being solved. Quick check: Verify the ratio of click events to total impressions in typical ad systems.
- **Self-Supervised Learning Fundamentals**: Knowledge of how auxiliary tasks can be used to pre-train models without labeled data is crucial for understanding the approach. Quick check: Confirm that the auto-encoder is trained without conversion labels.
- **Model Calibration in Advertising Systems**: Understanding why calibration preservation is critical for advertiser goals helps contextualize the importance of this constraint. Quick check: Verify that predicted conversion rates match actual conversion rates across different segments.
- **Tabular Data Processing**: Knowledge of how to design loss functions and architectures for tabular data is necessary for adapting self-supervised methods to this domain. Quick check: Confirm the model handles mixed categorical and numerical features effectively.
- **Continual Learning in Production Systems**: Understanding how models can be updated continuously in production environments is important for real-world deployment. Quick check: Verify that the framework supports incremental updates without full retraining.

## Architecture Onboarding

Component Map:
Auto-encoder (trained on all conversions) -> Feature Extractor -> Main Conversion Rate Model -> Ad Auction System

Critical Path:
The critical path involves training the auto-encoder on all available conversion events, using its learned representations as features for the main conversion rate prediction model, and integrating this enriched model into the real-time ad auction system. The quality of the auto-encoder's learned representations directly impacts the main model's performance, making the pre-training phase crucial for overall success.

Design Tradeoffs:
The framework balances the benefit of leveraging additional conversion data against the risk of introducing noise from non-click-attributed conversions. The choice to use an auto-encoder rather than other self-supervised methods represents a tradeoff between model complexity and the ability to handle tabular data effectively. The continual learning adaptation trades off immediate performance gains against long-term model stability and adaptation to changing user behavior patterns.

Failure Signatures:
Potential failure modes include the auto-encoder learning irrelevant features from non-click-attributed conversions that harm the main prediction task, calibration degradation due to the pre-training process, and computational overhead that impacts real-time auction performance. The model might also fail to generalize across different advertiser types or user segments if the pre-training data is too homogeneous.

First 3 Experiments:
1. Ablation study comparing performance with and without auto-encoder pre-training across different advertiser segments
2. Calibration analysis measuring reliability diagrams and expected calibration error before and after pre-training
3. Cross-platform validation testing the framework on at least two additional advertising systems with different user demographics

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability and generalization beyond Yahoo native advertising system remains uncertain
- Calibration preservation claims lack quantitative validation metrics and specific error measurements
- Cross-platform applicability requires validation across diverse advertising contexts

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Offline training improvements | High |
| Basic self-supervised pre-training methodology | High |
| Online A/B test results | Medium |
| Deployment claims | Medium |
| Calibration preservation claims | Low |
| Cross-platform generalizability | Low |

## Next Checks

1. Conduct ablation studies to isolate the contribution of each component (auto-encoder feature extraction, loss function design, continual learning adaptation) to performance gains
2. Perform cross-platform validation by testing the framework on at least two additional advertising systems with different user demographics and ad formats
3. Implement comprehensive calibration analysis including reliability diagrams and expected calibration error metrics across multiple time periods and traffic conditions