---
ver: rpa2
title: Evaluating the Factuality of Large Language Models using Large-Scale Knowledge
  Graphs
arxiv_id: '2404.00942'
source_url: https://arxiv.org/abs/2404.00942
tags:
- llms
- judge
- knowledge
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GraphEval, a framework for evaluating the
  factuality of large language models (LLMs) using a large-scale knowledge graph (KG)
  with over 10 million facts. Instead of relying on expensive human-labeled datasets,
  GraphEval automatically generates prompts from the KG and uses a lightweight "judge
  model" to classify LLM outputs as True, False, or "I don't know" based on hidden
  states, bypassing full text generation.
---

# Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs

## Quick Facts
- arXiv ID: 2404.00942
- Source URL: https://arxiv.org/abs/2404.00942
- Reference count: 40
- Primary result: GraphEval framework achieves efficient factuality evaluation of LLMs using 10M+ KG facts with judge model accuracy closely tracking LLM correctness

## Executive Summary
This paper introduces GraphEval, a framework for evaluating the factuality of large language models (LLMs) using a large-scale knowledge graph (KG) with over 10 million facts. Instead of relying on expensive human-labeled datasets, GraphEval automatically generates prompts from the KG and uses a lightweight "judge model" to classify LLM outputs as True, False, or "I don't know" based on hidden states, bypassing full text generation. This approach enables efficient, scalable, and comprehensive factuality evaluation across diverse domains. Experiments on DBpedia show that the judge model's assessments closely align with LLM correctness, substantially reducing evaluation costs while offering detailed insights into model performance across relation types and entity categories. GraphEval thus provides a resource-efficient and reliable method for assessing LLM factuality on a large scale.

## Method Summary
GraphEval evaluates LLM factuality by leveraging a large knowledge graph (DBpedia with 10M+ facts) to automatically generate evaluation prompts. The framework converts KG triples into declarative statements using relation templates, which are then fed to target LLMs. Instead of generating full LLM responses, a judge model takes the LLM's hidden states as input and classifies them into True/False/IDK categories. A prompt encoder compresses instruction prefixes to reduce computational overhead. The system evaluates factuality across the entire KG rather than sampled subsets, providing comprehensive coverage. Judge model training uses LLM hidden states from sampled triples, and evaluation metrics include truthfulness, informativeness, and correctness across different relation types and entity categories.

## Key Results
- Judge model accuracy closely tracks LLM correctness when evaluating factuality on DBpedia KG
- Full KG evaluation provides more comprehensive coverage than subset-based approaches
- Prompt encoding reduces judge model computational overhead while maintaining classification performance
- GraphEval enables resource-efficient factuality evaluation across diverse domains and relation types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The judge model accurately estimates LLM factuality without full text generation
- Mechanism: Instead of generating full LLM responses, the judge model takes LLM hidden states as input and classifies them into True/False/IDK categories
- Core assumption: LLM hidden states contain sufficient information to determine factuality of the response
- Evidence anchors:
  - [abstract] "streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the LLM"
  - [section 3.2] "inspired by Azaria & Mitchell (2023), we train a classifier with LLMs' hidden states to make a selection within the above three options"
  - [corpus] Weak - no direct evaluation of hidden state quality for factuality prediction

### Mechanism 2
- Claim: Using KG triples directly instead of subsets provides comprehensive factuality evaluation
- Mechanism: GraphEval evaluates LLM factuality across the entire DBpedia KG with 10M+ facts rather than sampled subsets
- Core assumption: Full KG coverage captures diverse factuality scenarios across all entity types and relations
- Evidence anchors:
  - [abstract] "evaluate an LLM's performance using a substantially large test dataset...retrieved from a large knowledge graph with more than 10 million facts"
  - [section 1] "we propose GraphEval...evaluates the factuality of LLMs using the entire KGs, providing a more diversified and comprehensive evaluation"
  - [corpus] Weak - no direct comparison with subset-based evaluation methods

### Mechanism 3
- Claim: Prompt encoding compression reduces judge model computational overhead
- Mechanism: Fine-tuned prompt encoder reduces large input prompt prefix to minimal virtual tokens
- Core assumption: Compressed prompt representation preserves sufficient information for judge model classification
- Evidence anchors:
  - [section 3.2] "we include 2 extra components...we fine-tune a prompt encoder (Liu et al., 2022) to reduce the large input of the prompt prefix"
  - [section 4] "For the training of the prompt encoder, we use the same settings for all the evaluated LLMs"
  - [corpus] Weak - no quantitative comparison of performance with/without prompt encoding

## Foundational Learning

- Concept: Knowledge Graph Structure and Triples
  - Why needed here: Understanding how facts are represented as (head, relation, tail) triples is fundamental to generating evaluation prompts
  - Quick check question: Given a triple (Barack Obama, birthPlace, Hawaii), what would be the declarative statement template?

- Concept: LLM Hidden States and Token Embeddings
  - Why needed here: Judge model uses LLM hidden states instead of generated text, requiring understanding of transformer architecture internals
  - Quick check question: What layer of the LLM provides the hidden states used by the judge model for classification?

- Concept: Classification Metrics (Precision, Recall, F1)
  - Why needed here: Evaluating judge model performance requires understanding of classification metrics beyond simple accuracy
  - Quick check question: If judge model has high precision but low recall for "True" predictions, what does this indicate about its behavior?

## Architecture Onboarding

- Component map:
  KG Source (DBpedia) -> Prompt Generator -> LLM Evaluation Target -> Judge Model -> Prompt Encoder -> Evaluation Engine

- Critical path:
  1. KG triple retrieval → 2. Statement generation via templates → 3. LLM hidden state computation → 4. Judge model classification → 5. Metric aggregation

- Design tradeoffs:
  - Full KG evaluation vs. sampled subsets: Comprehensive coverage vs. computational cost
  - Hidden state classification vs. text generation: Efficiency vs. potential information loss
  - Prompt encoding vs. full prompts: Speed vs. possible semantic loss

- Failure signatures:
  - Low judge model precision: Hidden states not capturing factuality information
  - Systematic bias across relation types: Template generation issues or KG structural problems
  - GPU memory OOM: Too large prompt encoder or insufficient hardware for full KG evaluation

- First 3 experiments:
  1. Verify judge model performance on labeled validation set matches paper claims
  2. Test judge model consistency across different substitute LLM sizes (7B, 13B, 70B)
  3. Evaluate impact of prompt encoder on judge model accuracy and speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the judge model be extended to generate text outputs beyond the three categories (True, False, I don't know)?
- Basis in paper: [inferred] The paper mentions that the judge model is currently limited to classifying LLM outputs into three categories and does not generate text. It suggests that enhancing the judge model's functionality and application range could be an avenue for future research.
- Why unresolved: The paper does not explore the potential for expanding the judge model's capabilities beyond classification, leaving open the question of whether it could be adapted for text generation tasks.
- What evidence would resolve it: Developing and testing an extended version of the judge model that can generate text outputs, and comparing its performance and efficiency to the current classification model.

### Open Question 2
- Question: How would the judge model's performance change if it were trained on domain-specific knowledge graphs rather than general ones like DBpedia?
- Basis in paper: [inferred] The paper notes that the current research does not examine the method's efficacy on domain-specific knowledge graphs, suggesting that exploring this could provide insights into the judge model's adaptability and performance in specialized domains.
- Why unresolved: The experiments conducted in the paper focus on a general knowledge graph (DBpedia), without considering how the judge model would perform when trained on knowledge graphs specific to particular domains.
- What evidence would resolve it: Conducting experiments using the judge model on various domain-specific knowledge graphs and analyzing any differences in performance metrics such as accuracy, efficiency, and reliability.

### Open Question 3
- Question: What impact would incorporating temporal relations into the knowledge graph have on the judge model's ability to evaluate the factuality of LLMs?
- Basis in paper: [inferred] The paper mentions that temporal relations are not considered in the current research, indicating a potential area for future exploration to understand how they might affect the judge model's evaluations.
- Why unresolved: The absence of temporal relations in the current evaluation framework leaves uncertainty about how the judge model would handle time-sensitive information and whether it could accurately assess factuality in dynamic contexts.
- What evidence would resolve it: Integrating temporal relations into the knowledge graph and testing the judge model's performance in evaluating LLM outputs that involve time-sensitive facts, comparing results with and without temporal context.

## Limitations

- Judge model generalization across different LLM architectures is not validated
- Potential biases in DBpedia KG coverage may affect evaluation fairness
- Relation template quality and coverage are not systematically evaluated

## Confidence

**High Confidence** in the core framework design - The GraphEval architecture (KG → prompts → LLM → hidden states → judge model) is well-defined and technically sound. The use of hidden states for efficient classification is a reasonable approach supported by similar work in the field.

**Medium Confidence** in evaluation results - While the paper demonstrates judge model performance correlates with LLM correctness, the exact correlation strength and generalizability across different LLM families requires further validation. The reported metrics are promising but may not fully capture edge cases.

**Low Confidence** in scalability claims - The paper claims "resource-efficient" evaluation but does not provide comprehensive timing or resource usage comparisons against baseline methods. The actual computational savings from prompt encoding and hidden state classification need quantitative validation.

## Next Checks

1. Test judge model performance when trained on LLaMA-2 hidden states but applied to evaluate Gemma models, measuring performance degradation and potential calibration needs.

2. Analyze DBpedia's entity and relation distribution to identify potential biases in the evaluation corpus, then test whether results change when evaluating on different KG subsets.

3. Systematically evaluate the impact of different relation templates on LLM performance by comparing results across multiple template variants for the same KG triples.