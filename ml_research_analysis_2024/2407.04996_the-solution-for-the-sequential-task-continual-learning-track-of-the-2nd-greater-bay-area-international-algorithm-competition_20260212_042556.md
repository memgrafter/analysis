---
ver: rpa2
title: The Solution for the sequential task continual learning track of the 2nd Greater
  Bay Area International Algorithm Competition
arxiv_id: '2407.04996'
source_url: https://arxiv.org/abs/2407.04996
tags:
- task
- learning
- incremental
- tasks
- yang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a data-free, parameter-isolation-based continual
  learning algorithm for the sequential task continual learning track of the 2nd Greater
  Bay Area International Algorithm Competition. The method learns an independent parameter
  subspace for each task within the network's convolutional and linear layers and
  freezes the batch normalization layers after the first task.
---

# The Solution for the sequential task continual learning track of the 2nd Greater Bay Area International Algorithm Competition

## Quick Facts
- arXiv ID: 2407.04996
- Source URL: https://arxiv.org/abs/2407.04996
- Authors: Sishun Pan; Xixian Wu; Tingmin Li; Longfei Huang; Mingxu Feng; Zhonghua Wan; Yang Yang
- Reference count: 25
- Primary result: 78.13% average accuracy in competition preliminary stage, second-place finish

## Executive Summary
This paper presents a data-free, parameter-isolation-based continual learning algorithm for the sequential task continual learning track of the 2nd Greater Bay Area International Algorithm Competition. The method learns independent parameter subspaces for each task within convolutional and linear layers while freezing batch normalization layers after the first task. For domain incremental settings, it freezes the shared classification head after the first task to prevent catastrophic forgetting. The authors designed an inference task identity strategy to select appropriate mask matrices for each sample in domain incremental settings without task identity. The approach won a second-place prize in the competition.

## Method Summary
The method employs a parameter-isolation approach where each task is assigned a unique mask matrix that activates only a subset of parameters in convolutional and linear layers. After learning the first task, batch normalization layers are frozen to prevent forgetting caused by changing normalization statistics. For domain incremental settings, the shared classification head is also frozen after the first task. The method includes gradient supplementation to enhance the importance of unselected parameters, adaptive importance scoring to dynamically adjust parameter allocation, and mask matrix compression to save storage space and improve inference speed.

## Key Results
- Achieved 78.13% average accuracy in competition preliminary stage
- Won second-place prize in the competition
- Significantly outperformed baseline method in the sequential task continual learning track

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing batch normalization layers after the first task prevents catastrophic forgetting in parameter-isolation-based continual learning.
- Mechanism: Batch normalization layers update statistics based on incoming data. In continual learning, this causes the BN layers to adapt to new tasks, potentially overwriting the learned representations for previous tasks. By freezing BN layers after the first task, the method ensures consistent normalization statistics across all tasks.
- Core assumption: The BN statistics learned from the first task are representative enough to normalize features for all subsequent tasks.
- Evidence anchors:
  - [abstract] "The method learns an independent parameter subspace for each task within the network's convolutional and linear layers and freezes the batch normalization layers after the first task."
  - [section] "To address this, we made the following two improvements to WSN: (1) Under task increment conditions, after learning the first task, all BN layers are frozen, and subsequent tasks use the BN layers from the first task to prevent forgetting caused by changes in BN layers"
- Break condition: If the first task's data distribution is significantly different from subsequent tasks, the frozen BN layers may not properly normalize features, leading to degraded performance.

### Mechanism 2
- Claim: Learning independent parameter subspaces for each task through masking prevents interference between tasks.
- Mechanism: The method assigns a unique mask matrix to each task, activating only a subset of parameters for that task. This creates task-specific subnetworks within the shared architecture, ensuring that updates for one task don't affect parameters used by other tasks.
- Core assumption: The network has sufficient capacity to allocate disjoint parameter subspaces for all tasks without significant capacity constraints.
- Evidence anchors:
  - [abstract] "The method learns an independent parameter subspace for each task within the network's convolutional and linear layers"
  - [section] "There are two main reasons for choosing WSN: First, as a parameter isolation method, WSN fundamentally eliminates catastrophic forgetting by assigning a subnetwork to each task."
- Break condition: If the number of tasks exceeds the network's capacity to maintain independent subspaces, tasks will have to share parameters, leading to interference and forgetting.

### Mechanism 3
- Claim: Gradient supplementation strategy enhances learning for new tasks by providing additional gradient information to unused parameters.
- Mechanism: When parameters are not selected for the current task (mask value = 0), their importance scores are updated with a supplemented gradient term. This prevents these parameters from becoming completely irrelevant and maintains their potential utility for future tasks.
- Core assumption: Parameters not used for the current task may still be valuable for future tasks, so their importance scores should not decay completely.
- Evidence anchors:
  - [section] "We have identified certain deficiencies with the WSN method. The importance matrix for parameters of the current task is initialized based on the importance matrix from previous tasks, tending to use parameters from previous tasks while neglecting others. Therefore, we have adopted a gradient supplementation strategy where each weight Theta is associated with an importance score s, updated as follows: s ← s − (η·∂L/∂s · γ if Mt−1 = 1 or mt = 0 η·∂L/∂s otherwise"
- Break condition: If gradient supplementation is too aggressive, it may prevent unused parameters from being properly pruned, leading to inefficient use of network capacity.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding why neural networks forget previous tasks when learning new ones is fundamental to appreciating why continual learning methods are necessary
  - Quick check question: What happens to a neural network's performance on previous tasks when it is trained on new, unrelated data without any special mechanism?

- Concept: Parameter isolation vs. regularization
  - Why needed here: The paper uses a parameter isolation approach (creating task-specific subnetworks) rather than regularization-based methods (penalizing changes to important weights). Understanding this distinction is crucial for grasping the method's design choices.
  - Quick check question: How does parameter isolation fundamentally differ from regularization-based approaches in preventing catastrophic forgetting?

- Concept: Batch normalization dynamics
  - Why needed here: The method relies on freezing BN layers after the first task. Understanding how BN layers update their statistics during training and why this can cause forgetting is essential to appreciating this design choice.
  - Quick check question: Why might updating batch normalization statistics during continual learning cause a model to forget previously learned tasks?

## Architecture Onboarding

- Component map:
  ResNet50 backbone -> Task-specific mask matrices -> Frozen batch normalization layers -> Shared classification head -> Inference task identity module -> Gradient supplementation module -> Dynamic threshold adjustment module -> Mask matrix compression module

- Critical path:
  1. Task 1: Train full model with BN updates, create initial mask
  2. Task k (k > 1): Load previous masks, apply gradient supplementation, dynamically adjust thresholds, train with frozen BN
  3. Domain incremental inference: Determine task identity, select appropriate mask, perform inference

- Design tradeoffs:
  - Fixed BN vs. adaptive BN: Freezing BN provides stability but may not adapt to new data distributions
  - Mask compression: Reduces storage but adds computational overhead for encoding/decoding
  - Gradient supplementation: Helps unused parameters remain relevant but may slow down learning for the current task

- Failure signatures:
  - Performance degradation on early tasks: Indicates insufficient parameter isolation or inadequate mask allocation
  - Poor performance on new tasks: May indicate overly aggressive parameter pruning or ineffective gradient supplementation
  - High memory usage: Suggests mask compression is not effective or too many parameters are being retained

- First 3 experiments:
  1. Validate basic parameter isolation: Train on two tasks with mask matrices, verify no forgetting occurs
  2. Test BN freezing: Compare performance with and without BN freezing on a simple continual learning benchmark
  3. Evaluate gradient supplementation: Measure impact on learning efficiency for new tasks with different supplementation coefficients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inference task identity strategy perform when the number of tasks increases significantly beyond the competition's 20-task limit, particularly in terms of accuracy and computational overhead?
- Basis in paper: [explicit] The paper describes an inference task identity strategy that selects appropriate mask matrices based on mean and variance differences in the first convolutional layer, but only evaluates it within the competition's task limits.
- Why unresolved: The paper only demonstrates performance up to 20 tasks and doesn't explore scalability beyond this point or analyze how the strategy's effectiveness degrades with more tasks.
- What evidence would resolve it: Experimental results showing inference accuracy and computational costs across task sequences of varying lengths (e.g., 50, 100, 200 tasks) would clarify the strategy's scalability limits.

### Open Question 2
- Question: What is the theoretical relationship between the gradient supplementation coefficient γ and the learning rate η that would optimize new task learning while preserving old task knowledge?
- Basis in paper: [explicit] The paper introduces a gradient supplementation strategy but doesn't provide theoretical analysis of the optimal relationship between the supplementation coefficient and learning rate.
- Why unresolved: The authors only describe the strategy's implementation and report empirical results, without investigating the theoretical underpinnings of how these hyperparameters interact.
- What evidence would resolve it: Mathematical analysis or controlled experiments varying both parameters systematically to identify optimal ratios or relationships between them.

### Open Question 3
- Question: How does the mask matrix compression strategy's performance change when dealing with task sequences exceeding 32 tasks, and what alternative compression methods could maintain efficiency?
- Basis in paper: [explicit] The paper describes a compression method using 32-bit integers to store mask matrices, but only validates it for T ≤ 32 tasks.
- Why unresolved: The paper doesn't explore what happens when task counts exceed 32 or propose solutions for maintaining compression efficiency beyond this limit.
- What evidence would resolve it: Comparative analysis of compression ratios and computational efficiency using alternative methods (e.g., multi-level encoding, sparse representations) for task sequences of 64, 128, and 256 tasks.

## Limitations

- Competition-specific context limits generalizability to other datasets or tasks
- Missing ablation studies to quantify individual contributions of proposed strategies
- Hyperparameters for gradient supplementation coefficient γ and sparsity parameters not specified
- Vague details about mask matrix compression implementation and effectiveness

## Confidence

**High Confidence**: The basic mechanism of parameter isolation through task-specific masks is well-established in continual learning literature and the paper's description is clear and consistent. The approach of freezing BN layers after the first task to prevent forgetting is a reasonable and commonly used technique.

**Medium Confidence**: The gradient supplementation strategy's effectiveness depends heavily on the specific coefficient γ, which is not provided. The adaptive threshold adjustment mechanism is conceptually sound but its practical impact without specific α(l) values is uncertain. The claim of winning second place is verifiable but the paper doesn't provide sufficient detail to independently verify the exact performance metrics.

**Low Confidence**: The domain incremental task identity inference mechanism's accuracy is not quantified. The mask matrix compression strategy's impact on both storage savings and inference speed is claimed but not empirically validated with specific numbers. The paper's assertion that this is "data-free" is questionable since BN statistics are still learned from task 1 data.

## Next Checks

1. **Ablation Study Implementation**: Implement the method without gradient supplementation and with dynamic thresholds fixed at initial values to quantify the individual contribution of each proposed strategy to the overall performance.

2. **Domain Incremental Task Identity Accuracy**: Measure the accuracy of the task identity inference mechanism on a held-out validation set by comparing the predicted task identity against ground truth task labels, particularly focusing on cases where tasks have similar feature distributions.

3. **Generalization Benchmark**: Evaluate the method on a standard continual learning benchmark (e.g., Split CIFAR-100 or Split MiniImageNet) with multiple task sequences to assess whether the 78.13% performance translates to other datasets and whether the method maintains performance across diverse task orders.