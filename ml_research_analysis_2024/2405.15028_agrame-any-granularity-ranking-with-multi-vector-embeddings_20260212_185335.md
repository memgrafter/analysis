---
ver: rpa2
title: 'AGRaME: Any-Granularity Ranking with Multi-Vector Embeddings'
arxiv_id: '2405.15028'
source_url: https://arxiv.org/abs/2405.15028
tags:
- ranking
- query
- sentence-level
- sentence
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AGRaME, a method for ranking at different
  granularities while maintaining encoding at a single coarser level. The key idea
  is to leverage multi-vector embeddings and a multi-granular contrastive loss to
  enable ranking at finer granularities than the encoding level.
---

# AGRaME: Any-Granularity Ranking with Multi-Vector Embeddings

## Quick Facts
- arXiv ID: 2405.15028
- Source URL: https://arxiv.org/abs/2405.15028
- Authors: Revanth Gangi Reddy; Omar Attia; Yunyao Li; Heng Ji; Saloni Potdar
- Reference count: 20
- One-line primary result: Introduces AGRaME, a method enabling ranking at different granularities while maintaining encoding at a single coarser level.

## Executive Summary
AGR AME (Any-Granularity Ranking with Multi-vector Embeddings) addresses the challenge of ranking retrieval units at different granularities while maintaining a single encoding level. The method leverages multi-vector embeddings and a multi-granular contrastive loss to enable fine-grained ranking within coarser retrieval units. AGRaME demonstrates significant improvements in sentence-level and proposition-level ranking tasks, outperforming existing approaches that require separate encoding at each granularity. The method also shows superior performance in post-hoc citation addition for retrieval-augmented generation.

## Method Summary
AGR AME is a multi-vector ranking method that uses token-level embeddings to enable fine-grained scoring within coarser retrieval units. The approach employs a multi-granular contrastive loss that provides supervision at both passage and sentence levels, weighted by passage relevance scores. A key innovation is the use of different query markers to signal the model to adjust its scoring behavior based on the intended granularity. The method is trained on MS MARCO and evaluated on multiple open-domain QA datasets, demonstrating its effectiveness in ranking at any granularity while using the same encoding level.

## Key Results
- AGRaME significantly improves sentence-level ranking performance while maintaining passage-level encoding, outperforming sentence-level ranking at sentence-level encoding.
- The method demonstrates superior performance in post-hoc citation addition for retrieval-augmented generation, surpassing prompt-driven citation generation approaches.
- AGRaME achieves state-of-the-art results on sentence-level and proposition-level ranking tasks across multiple datasets including Natural Questions, TriviaQA, and Web Questions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-vector embeddings allow fine-grained scoring of sub-components within a retrieval unit while maintaining a single coarse encoding level.
- Mechanism: Token-level embeddings from multi-vector models enable computing MaxSim scores between each query token and each passage token. This allows discriminative scoring of sentences or propositions within a passage without requiring separate encoding at those granularities.
- Core assumption: The token-level embeddings preserve sufficient semantic information to distinguish relevance at sub-passage levels even when the overall passage context is encoded together.
- Evidence anchors:
  - [abstract] "AGR AME (Any-Granularity Ranking with Multi-vector Embeddings), a method that permits ranking at different levels of granularity while maintaining encoding at a single, coarser level."
  - [section] "We make the important observation that the use of token-level embeddings in multi-vector approaches can facilitate discriminative scoring of different sub-parts within a retrieval unit."
- Break condition: If token-level embeddings lose critical semantic distinctions when passage context is included, fine-grained scoring will degrade.

### Mechanism 2
- Claim: Multi-granular contrastive loss improves fine-grained ranking by providing supervision at multiple levels simultaneously.
- Mechanism: The training process incorporates both passage-level contrastive loss (identifying relevant passages) and sentence-level contrastive loss (identifying relevant sentences within passages). The sentence-level loss is weighted by passage relevance scores to focus learning on relevant passages.
- Core assumption: Joint supervision at multiple granularities teaches the model to dynamically adjust relevance criteria based on the ranking level being performed.
- Evidence anchors:
  - [section] "To enable the model to discriminatively select sub-units within the passage, we propose to incorporate a more finer-level of training supervision, by teaching to further identify the most relevant sentence within each passage."
  - [section] "The sentence-level loss Lsent.(q, [p]) is finally added to original passage-level loss Lpsg(q, [p]) to get the training loss L."
- Break condition: If the model cannot effectively learn to switch relevance criteria between different granularities, performance at fine-grained levels will suffer.

### Mechanism 3
- Claim: Using different query markers for different ranking granularities helps the model distinguish between coarse and fine-grained relevance scoring.
- Mechanism: A new query marker token (m'q) is prepended when scoring at sentence or proposition level, distinct from the marker (mq) used for passage-level scoring. This signals the model to adjust its scoring behavior based on the intended granularity.
- Core assumption: The encoder can learn to associate different query markers with different relevance scoring behaviors, allowing it to switch contexts appropriately.
- Evidence anchors:
  - [section] "To signal the model to score discriminatively within the passage for sentence-level ranking, we prepend a new query marker token m'q, different from mq used when ranking at passage-level."
  - [section] "We observe that our proposed approach significantly improves sentence-level ranking performance with passage-level encoding, even outperforming sentence-level ranking at sentence-level encoding."
- Break condition: If the model fails to learn the association between query markers and scoring behaviors, the granularity distinction will be ineffective.

## Foundational Learning

- Concept: MaxSim operation for multi-vector relevance scoring
  - Why needed here: The MaxSim operation computes relevance by taking the maximum dot product between each query token embedding and all passage token embeddings, then summing these maxima. This enables fine-grained matching while preserving computational efficiency.
  - Quick check question: What is the computational complexity of MaxSim compared to computing all pairwise token similarities, and why is this important for retrieval systems?

- Concept: Contrastive learning with soft labels
  - Why needed here: The training uses KL-divergence between soft relevance scores from a cross-encoder teacher and the student model's scores. This provides richer supervision than binary labels and helps the model learn graded relevance distinctions.
  - Quick check question: How does using soft labels from a cross-encoder differ from using binary labels, and what advantage does this provide for learning fine-grained relevance distinctions?

- Concept: Token-level embedding extraction and indexing
  - Why needed here: Multi-vector approaches require storing token-level embeddings for efficient retrieval. Understanding how these embeddings are extracted, normalized, and indexed is crucial for implementing the system.
  - Quick check question: What are the key considerations for designing an efficient index for token-level embeddings, and how do they differ from traditional single-vector indexing approaches?

## Architecture Onboarding

- Component map: Encoder (BERT-based) -> MaxSim scorer -> Index lookup -> Ranking output
- Critical path: Encoder produces token-level embeddings → MaxSim scorer computes relevance using these embeddings → Index lookup retrieves top candidates efficiently → Ranking output provides final results
- Design tradeoffs:
  - Index size vs. granularity: Token-level embeddings increase index size by ~1000x compared to single-vector approaches
  - Training complexity vs. performance: Multi-granular contrastive loss improves fine-grained ranking but increases training complexity
  - Marker system complexity vs. flexibility: Different markers enable granularity switching but add complexity to the encoding process
- Failure signatures:
  - Poor fine-grained ranking despite good coarse ranking: Likely issues with token-level embeddings or MaxSim scoring
  - Training instability: Multi-granular loss may be unbalanced or gradients may conflict
  - High computational cost: Inefficient MaxSim implementation or sub-optimal indexing strategy
- First 3 experiments:
  1. Validate MaxSim scoring: Compare ranking performance using MaxSim vs. simple average pooling on a small dataset
  2. Test marker effectiveness: Train with and without different query markers for sentence-level scoring and measure the impact
  3. Evaluate contrastive loss balance: Vary the weighting between passage and sentence-level losses to find optimal balance for fine-grained performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AGRaME scale with increasing retrieval unit granularity (e.g., paragraphs, documents) while maintaining encoding at a coarser level?
- Basis in paper: [explicit] The paper demonstrates AGRaME's effectiveness for sentence and proposition-level ranking while encoding at passage-level. However, it does not explore the impact of using increasingly larger retrieval units.
- Why unresolved: The paper's experiments focus on sentence and proposition-level ranking with passage-level encoding. Further investigation is needed to determine the limits of AGRaME's scalability and its performance with larger retrieval units.
- What evidence would resolve it: Experiments comparing AGRaME's performance with different retrieval unit granularities (e.g., paragraphs, documents) while maintaining encoding at a coarser level. Analysis of how the model's performance and efficiency change with increasing retrieval unit size.

### Open Question 2
- Question: How does the choice of the new query marker (m'q) in AGRaME affect the model's ability to rank at different granularities?
- Basis in paper: [explicit] The paper introduces a new query marker (m'q) to signal scoring at different granularities. It shows that using m'q improves sentence-level ranking performance compared to using the default query marker (mq).
- Why unresolved: The paper only explores the impact of using m'q for sentence-level ranking with passage-level encoding. Further investigation is needed to understand how the choice of m'q affects ranking at other granularities and with different encoding levels.
- What evidence would resolve it: Experiments comparing AGRaME's performance with different query markers for various ranking granularities and encoding levels. Analysis of how the choice of m'q influences the model's ability to learn and generalize across different granularities.

### Open Question 3
- Question: Can the multi-granular contrastive loss in AGRaME be extended to incorporate other types of supervision beyond sentence-level relevance scores?
- Basis in paper: [explicit] The paper proposes a multi-granular contrastive loss that incorporates sentence-level relevance scores from a cross-encoder model. However, it does not explore the potential of using other types of supervision.
- Why unresolved: The paper's experiments focus on using sentence-level relevance scores as additional supervision. Further investigation is needed to determine if other types of supervision, such as entity-level or relation-level relevance scores, can further improve AGRaME's performance.
- What evidence would resolve it: Experiments incorporating different types of supervision into AGRaME's multi-granular contrastive loss. Analysis of how the choice of supervision affects the model's performance and its ability to rank at different granularities.

## Limitations

- The method's effectiveness depends heavily on the quality of token-level embeddings from multi-vector models, which may vary across different architectures and domains.
- The computational overhead of storing and querying token-level embeddings (approximately 1000x larger than single-vector approaches) could limit practical deployment.
- The method assumes that query markers can effectively signal different granularity behaviors to the model, but the learning dynamics of this association remain underexplored.

## Confidence

**High Confidence**: The fundamental mechanism of using multi-vector embeddings with MaxSim scoring for fine-grained relevance assessment within passages is well-supported. The empirical results showing improved sentence-level ranking while maintaining passage-level encoding are robust across multiple datasets.

**Medium Confidence**: The multi-granular contrastive loss formulation and its contribution to fine-grained ranking performance shows consistent improvements but relies on specific hyperparameter choices that may not generalize. The effectiveness of different query markers for granularity control is demonstrated but the underlying learning mechanism is not fully characterized.

**Low Confidence**: The practical scalability of the approach for production systems given the significant increase in index size remains uncertain. The generalizability to other fine-grained ranking tasks beyond the evaluated domains (sentence and proposition-level) has limited validation.

## Next Checks

1. **Token Embedding Sensitivity Analysis**: Systematically evaluate how different multi-vector model architectures (ColBERTv2 vs. SPLADE, different BERT variants) affect fine-grained ranking performance to determine if the approach is architecture-dependent.

2. **Index Size Optimization**: Implement and benchmark compression techniques for token-level embeddings (product quantization, binary codes) to quantify the practical tradeoff between index size and ranking quality.

3. **Generalization to New Granularities**: Design and execute experiments testing the approach on paragraph-level ranking and phrase-level matching to evaluate whether the multi-granularity capability extends beyond the current scope.