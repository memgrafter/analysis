---
ver: rpa2
title: Coding schemes in neural networks learning classification tasks
arxiv_id: '2406.16689'
source_url: https://arxiv.org/abs/2406.16689
tags:
- layer
- neurons
- training
- readout
- coding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the emergent representations learned by
  neural networks in the non-lazy (feature learning) regime. Using a Bayesian framework
  and mean-field theory, the authors analyze fully-connected wide networks trained
  on classification tasks.
---

# Coding schemes in neural networks learning classification tasks

## Quick Facts
- arXiv ID: 2406.16689
- Source URL: https://arxiv.org/abs/2406.16689
- Authors: Alexander van Meegen; Haim Sompolinsky
- Reference count: 0
- One-line primary result: Emergent representations in non-lazy neural networks depend strongly on neuronal nonlinearity, exhibiting analog, redundant, or sparse coding schemes.

## Executive Summary
This paper analyzes emergent representations in neural networks operating in the non-lazy (feature learning) regime using Bayesian framework and mean-field theory. The authors demonstrate that the nature of learned representations - whether analog, redundant, or sparse coding - is determined by the neuronal nonlinearity. Linear networks produce analog coding, sigmoidal networks generate redundant coding through spontaneous symmetry breaking, and ReLU networks create sparse coding with outlier neurons. Despite strong task-dependent features, the mean predictor remains identical to the lazy case, with improved generalization arising solely from reduced predictor variance.

## Method Summary
The paper employs a Bayesian framework to analyze fully-connected wide neural networks trained on classification tasks. Using mean-field theory in the infinite width limit, the authors derive exact expressions for the weight posterior and kernels. For nonlinear networks, they employ Hamiltonian Monte Carlo sampling to explore the posterior distribution. The analysis focuses on non-lazy scaling (output scaled by 1/N) and examines how the choice of nonlinearity (linear, sigmoidal, ReLU) affects the emergent coding schemes through spontaneous symmetry breaking in the weight posterior.

## Key Results
- The nature of emergent representations depends critically on neuronal nonlinearity: linear networks show analog coding, sigmoidal networks exhibit redundant coding, and ReLU networks display sparse coding
- Spontaneous symmetry breaking in the weight posterior creates distinct coding schemes for sigmoidal and ReLU networks
- Despite strong task-dependent features, the mean predictor remains identical to the lazy case, with generalization improvement driven solely by reduced predictor variance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The coding scheme emerges from spontaneous symmetry breaking in the weight posterior.
- Mechanism: In the non-lazy regime, the weight posterior factorizes across neurons, but for certain nonlinearities (sigmoidal, ReLU) the readout weight posterior splits into disconnected branches. This leads to a frozen identity for each neuron's code, as neurons cannot change branches during sampling.
- Core assumption: The weight posterior remains invariant under neuron permutations, but the disconnected branches create barriers preventing transitions between symmetric solutions.
- Evidence anchors:
  - [abstract] "In nonlinear networks, spontaneous symmetry breaking leads to either redundant or sparse coding schemes."
  - [section] "Due to the neuron permutation symmetry of the posterior, each typical solution has permuted counterparts... Permutation symmetry breaking occurs if these permuted solutions are disconnected in solution space..."
  - [corpus] Weak evidence; no direct neighbor mentions symmetry breaking in this context.
- Break condition: If the temperature is too high, the barriers between branches disappear, and the symmetry is restored, destroying the coding scheme.

### Mechanism 2
- Claim: The nature of the coding scheme depends critically on the neuronal nonlinearity.
- Mechanism: Linear networks lead to analog coding because the readout weights are Gaussian and continuous, allowing graded responses to all classes. Sigmoidal networks produce redundant coding due to the posterior concentrating into discrete branches of readout weights, creating distinct populations of neurons active on specific class combinations. ReLU networks generate sparse coding because the bulk neurons have zero mean preactivations, leaving only a few outlier neurons to carry class-specific information.
- Core assumption: The choice of nonlinearity determines the structure of the single neuron posterior (Gaussian vs. multimodal vs. Dirac deltas).
- Evidence anchors:
  - [abstract] "the nature of the internal representations depends crucially on the neuronal nonlinearity... In linear networks, an analog coding scheme... In nonlinear networks, spontaneous symmetry breaking leads to either redundant or sparse coding schemes."
  - [section] "the nature of the learned representations varies crucially with the choice of the activation function... linear... analog coding... sigmoidal... redundant coding... ReLU... sparse coding."
  - [corpus] Weak evidence; no direct neighbor mentions coding schemes in neural networks.
- Break condition: If the scaling of the prior variance is not adjusted with dataset size P, the distinction between coding schemes may blur due to inappropriate regularization.

### Mechanism 3
- Claim: Feature learning in non-lazy networks reduces predictor variance without changing the mean predictor.
- Mechanism: The learned features align strongly with the readout weights, creating a low-rank structure in the kernel. This reduces the effective number of independent directions the predictor can vary in, lowering the variance. However, the mean predictor remains identical to the lazy case because the alignment does not change the expected output.
- Core assumption: The alignment between features and readout weights is strong enough to dominate the variance reduction, but the mean is determined by the overall alignment with the training targets.
- Evidence anchors:
  - [abstract] "Despite the strong representations, the mean predictor is identical to the lazy case. However, variance of the predictor is reduced, leading to improved generalization."
  - [section] "the mean predictor is identical to the lazy case... the variance of the predictor can be neglected... reduction of the generalization error is exclusively an effect of the reduced variance of the predictor."
  - [corpus] Weak evidence; no direct neighbor mentions variance reduction in non-lazy networks.
- Break condition: If the number of layers L increases, the effect on variance reduction diminishes, as the variance suppression scales with 1/N and is less effective in deeper networks.

## Foundational Learning

- Concept: Bayesian framework for neural network learning.
  - Why needed here: The paper uses the Bayesian posterior to analyze the solution space and emergent representations, contrasting with deterministic training methods.
  - Quick check question: What role does the prior distribution of weights play in shaping the posterior and thus the learned representations?

- Concept: Mean-field theory in the infinite width limit.
  - Why needed here: The analysis relies on taking the network width N to infinity to derive exact expressions for the weight posterior and kernels, leveraging the self-averaging property of large systems.
  - Quick check question: How does the scaling of the output with 1/N (non-lazy) vs. 1/√N (lazy) affect the strength of learned features?

- Concept: Neural coding schemes (analog, redundant, sparse).
  - Why needed here: The paper introduces these coding schemes as a way to characterize how learned features are represented by distinct populations of neurons, going beyond kernel analysis.
  - Quick check question: What is the difference between a coding scheme and a kernel, and why is the former more informative about representations?

## Architecture Onboarding

- Component map:
  - Input preprocessing: Normalize inputs to unit norm
  - Network architecture: Fully connected feedforward with L hidden layers, all of width N, m outputs
  - Nonlinearity: Choose from linear, sigmoidal (erf), or ReLU for hidden layers
  - Output scaling: Scale by 1/N for non-lazy regime
  - Prior: Gaussian with variance σ²_ℓ = 1 for hidden weights, σ²_a scaled with P^(-1/L) for readout weights
  - Training: Sample from Bayesian posterior using HMC (noisy gradient descent) or analyze via mean-field theory

- Critical path:
  1. Set up network architecture and nonlinearity
  2. Scale output by 1/N and adjust readout prior variance with P
  3. Sample from posterior or compute theoretical predictions
  4. Analyze learned representations via coding schemes and kernels
  5. Evaluate generalization performance

- Design tradeoffs:
  - Non-lazy scaling (1/N) leads to strong feature learning but requires careful prior scaling to prevent readout weights from dominating
  - Different nonlinearities lead to qualitatively different coding schemes, affecting interpretability and potential for transfer learning
  - Bayesian sampling provides full posterior information but is computationally expensive; mean-field theory offers analytical insights but relies on large N and P limits

- Failure signatures:
  - If the coding scheme is not observed, check if the prior variance scaling is correct (σ²_a should scale with P^(-1/L))
  - If the variance reduction is not apparent, verify that the non-lazy scaling (1/N) is correctly implemented
  - If the theory does not match empirical samples, ensure that the temperature scaling (T/N) is applied and that the system is in the large N, P limit

- First 3 experiments:
  1. Implement a single hidden layer network with sigmoidal nonlinearity on a toy classification task (orthogonal inputs, random labels) and visualize the coding scheme
  2. Compare the learned coding scheme and generalization performance for linear, sigmoidal, and ReLU nonlinearities on MNIST
  3. Analyze the effect of temperature on the coding scheme by sampling at different T values and observing the emergence/disappearance of distinct codes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What determines the number and identity of outlier neurons in ReLU networks?
- Basis in paper: [explicit] The paper shows that ReLU networks develop a sparse coding scheme with a small number of outlier neurons that carry task-relevant information, but the specific factors determining which neurons become outliers and how many outliers emerge are not specified.
- Why unresolved: The theoretical framework identifies the existence of outlier neurons and their role in sparse coding, but does not provide a mechanism for predicting their number or identity.
- What evidence would resolve it: Systematic analysis of how network initialization, architecture, or task complexity affects the number and identity of outlier neurons.

### Open Question 2
- Question: How does the strength of coding schemes change with network depth beyond two layers?
- Basis in paper: [explicit] The paper observes that coding schemes appear in all layers but become "sharpened" across layers, but only investigates networks up to two hidden layers.
- Why unresolved: The theoretical framework is limited to shallow networks (L ≤ 2), leaving open whether and how coding schemes evolve in deeper architectures.
- What evidence would resolve it: Extension of the mean-field theory to deeper networks and empirical validation on networks with 3+ hidden layers.

### Open Question 3
- Question: Why does the predictor variance reduction drive generalization improvement in non-lazy networks?
- Basis in paper: [explicit] The paper shows that non-lazy networks outperform Gaussian Process baselines primarily through variance reduction, not improved mean prediction, but does not explain the underlying mechanism.
- Why unresolved: The theoretical analysis quantifies the effect but does not identify why the non-lazy scaling specifically reduces predictor variance.
- What evidence would resolve it: Theoretical analysis linking the non-lazy scaling to specific properties of the weight posterior that suppress variance, or empirical studies comparing variance across different scaling regimes.

## Limitations

- The analysis is strictly valid in the infinite width limit, which may not hold for practical network sizes
- The Bayesian framework assumes exact posterior sampling, but the paper relies on approximate methods like HMC
- The coding scheme analysis depends on specific scaling assumptions for the prior variance that may not generalize to all dataset sizes
- The symmetry breaking mechanism, while theoretically appealing, lacks direct empirical validation beyond theoretical predictions

## Confidence

- **High confidence**: The mathematical framework for analyzing weight posteriors in the non-lazy regime is sound and well-established in the literature
- **Medium confidence**: The connection between neuronal nonlinearity and emergent coding schemes is theoretically derived but relies on specific assumptions about the posterior structure
- **Medium confidence**: The variance reduction mechanism improving generalization is supported by the theory but may be sensitive to implementation details and dataset characteristics

## Next Checks

1. **Numerical validation of symmetry breaking**: Implement numerical experiments to verify that the weight posterior indeed exhibits disconnected branches for sigmoidal and ReLU networks, and that sampling from different branches produces distinct coding schemes.

2. **Scaling parameter sensitivity**: Systematically vary the prior variance scaling parameters across different dataset sizes and network depths to determine the robustness of the coding scheme emergence and the variance reduction effect.

3. **Beyond mean-field validation**: Compare the theoretical predictions with empirical results from finite-width networks to quantify the deviation from the infinite width limit and identify the minimum width required for the theoretical framework to be applicable.