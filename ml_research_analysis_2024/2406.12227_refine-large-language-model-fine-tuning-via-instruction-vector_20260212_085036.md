---
ver: rpa2
title: Refine Large Language Model Fine-tuning via Instruction Vector
arxiv_id: '2406.12227'
source_url: https://arxiv.org/abs/2406.12227
tags:
- instruction
- forgetting
- arxiv
- output
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates catastrophic forgetting in large language
  models (LLMs) during fine-tuning, focusing on knowledge understanding versus instruction
  following capabilities. The authors propose the Instruction Vector (IV) framework
  to capture model representations related to instruction-following abilities, revealing
  that fine-tuning primarily adds specialized reasoning patterns rather than erasing
  previous skills.
---

# Refine Large Language Model Fine-tuning via Instruction Vector

## Quick Facts
- arXiv ID: 2406.12227
- Source URL: https://arxiv.org/abs/2406.12227
- Reference count: 18
- Reduces catastrophic forgetting in LLMs during fine-tuning, improving instruction-following capabilities while maintaining task plasticity

## Executive Summary
This paper addresses catastrophic forgetting in large language models (LLMs) during fine-tuning by distinguishing between knowledge understanding and instruction following capabilities. The authors propose the Instruction Vector (IV) framework to capture model representations related to instruction-following abilities, revealing that fine-tuning primarily adds specialized reasoning patterns rather than erasing previous skills. They develop IV-guided training, which preserves the original computation graph to mitigate forgetting. Experimental results across three benchmarks show significant improvements: IV-guided training reduces forgetting of general abilities by an average of 5.16 points and enhances in-context learning performance from 37.90 to 50.05, while maintaining task plasticity.

## Method Summary
The method involves sequential instruction fine-tuning on LLAMA2-7B-Chat using a LORA approach with rank dimension 8 and learning rate 1e-4. Instruction Vectors are extracted through causal mediation analysis on in-context learning samples, identifying top 10 attention heads with highest average causal effect. IV-guided training incorporates these vectors during fine-tuning with progressive intervention (scaling factor from 1 to 0) and KL-divergence loss (scale 0.05). The approach aims to preserve the original computation graph while learning new tasks, thereby mitigating catastrophic forgetting.

## Key Results
- IV-guided training reduces forgetting of general abilities by average 5.16 points (from 5.03 to -0.16)
- Improves in-context learning performance from 37.90 to 50.05
- Maintains task plasticity while enhancing instruction-following capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Catastrophic forgetting in LLMs primarily stems from loss of instruction-following capabilities rather than erasure of knowledge understanding
- Mechanism: Fine-tuning introduces specialized reasoning patterns that overshadow existing instruction-following computation graphs, making original capabilities appear lost
- Core assumption: Instruction-following and knowledge-understanding capabilities are distinct, separable functions within the model
- Evidence anchors: instruction accuracy sees average increase of 1.93, suggesting loss in instruction following ability is reason for task performance drop

### Mechanism 2
- Claim: The Instruction Vector (IV) captures model representations highly related to specific instruction-following capabilities
- Mechanism: IV represents computational graph (θc) that governs model's capability in following specific instructions, acting as intermediate variable in instruction-processing pathway
- Core assumption: High-dimensional latent variable θc exists that controls instruction-following ability and can be extracted through causal mediation analysis
- Evidence anchors: θc is obtained by aggregating task-conditioned activation from attention heads, demonstrating causal impact on performance

### Mechanism 3
- Claim: IV-guided training preserves original computation graph and mitigates catastrophic forgetting
- Mechanism: By progressively introducing and then phasing out IV during training, model maintains adherence to original instruction-following computation graph while learning new tasks
- Core assumption: Explicitly incorporating IV into model's computational graph can recover mastery of corresponding instructions
- Evidence anchors: Incorporates progressive IV-intervention training mechanism, initially introduced through intervention then gradually phased out

## Foundational Learning

- Concept: Causal mediation analysis
  - Why needed here: Used to identify attention heads with significant causal impacts on output and extract Instruction Vector
  - Quick check question: How does causal mediation analysis differ from correlation analysis in identifying causal relationships?

- Concept: Continual instruction tuning framework
  - Why needed here: Experimental setup where models are sequentially trained on streaming tasks while evaluated on held-out datasets
  - Quick check question: What metrics distinguish knowledge probability from instruction probability in this framework?

- Concept: In-context learning (ICL)
  - Why needed here: Used to gather task-conditioned activations and demonstrate effectiveness of Instruction Vector
  - Quick check question: How does presence of ICL samples activate Instruction Vector differently than zero-shot inputs?

## Architecture Onboarding

- Component map: Base LLM (Llama2-7B-Chat) → IV extraction module (causal mediation analysis on attention heads) → IV-guided training module (progressive intervention + KL-divergence loss) → Evaluation module (knowledge vs instruction probability metrics)
- Critical path: Data → IV extraction → Model intervention → Training with IV guidance → Performance evaluation
- Design tradeoffs: IV extraction is computationally intensive but enables targeted intervention; progressive IV phasing balances preservation with plasticity; KL-divergence loss adds complexity but improves alignment
- Failure signatures: Performance degradation despite IV intervention, IV extraction failing to capture meaningful patterns, computational overhead becoming prohibitive
- First 3 experiments:
  1. Validate IV extraction on simple tasks (Antonym, Last-Spanish) to confirm causal impact on performance
  2. Test progressive IV intervention on single-task fine-tuning to establish baseline effectiveness
  3. Compare IV-guided training against standard fine-tuning on multi-task continual learning benchmarks

Assumption: Framework assumes instruction-following and knowledge-understanding capabilities can be meaningfully separated and that IV represents real, causal intermediate variable rather than statistical artifact. These assumptions should be validated through ablation studies and alternative extraction methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is exact computational mechanism by which IV-associated computation graph is "overshadowed" by new reasoning patterns during fine-tuning?
- Basis in paper: [inferred] Paper states forgetting occurs when model's output becomes independent of IV-associated computation graph post-tuning, and fine-tuning "adds specialized reasoning patterns instead of erasing previous skills"
- Why unresolved: Paper does not provide detailed mechanistic explanation of how this overshadowing process occurs at neural level within transformer architecture
- What evidence would resolve it: Detailed causal mediation analysis showing specific attention heads and layer interactions where IV-associated patterns are suppressed by new fine-tuning-induced patterns

### Open Question 2
- Question: Why does IV-guided training method significantly reduce forgetting of newly learned knowledge in FUNC dataset but not in TRACE and LONG?
- Basis in paper: [explicit] Paper notes "interestingly, in FUNC dataset, our method significantly reduced forgetting of new knowledge on IncLora and OLora" and speculates this may be due to "simple and deterministic instructions" allowing integration with existing computation graph
- Why unresolved: Paper does not provide rigorous explanation for why this phenomenon is specific to FUNC and not observed in other benchmarks
- What evidence would resolve it: Comparative analysis of instruction complexity and task structures across FUNC, TRACE, and LONG, showing how instruction complexity affects model's ability to integrate new knowledge with existing computation graph

### Open Question 3
- Question: What are limitations of using attention head aggregation for IV extraction, and how would optimization-based methods improve upon this approach?
- Basis in paper: [explicit] Paper states "Although this method is fast and efficient, it is susceptible to input noise and may suffer from insufficient expressiveness. Therefore, we plan to use optimization-based methods in future to extract more generalized and accurate Instruction vector"
- Why unresolved: Paper does not elaborate on specific limitations of attention head aggregation method or demonstrate how optimization-based methods would address these limitations
- What evidence would resolve it: Comparative experiments showing performance of attention head aggregation versus optimization-based IV extraction methods on various benchmarks, with analysis of noise robustness and expressiveness

### Open Question 4
- Question: How does progressive IV intervention training mechanism (gradually reducing IV influence from 1 to 0) affect model's ability to generalize to unseen instructions?
- Basis in paper: [explicit] Paper describes this mechanism but does not evaluate its impact on generalization to instructions outside training distribution
- Why unresolved: Paper focuses on maintaining existing capabilities and reducing forgetting, but does not investigate how this approach affects model's ability to handle novel instructions
- What evidence would resolve it: Experiments testing model's performance on out-of-distribution instructions after IV-guided training, comparing with baseline methods to assess generalization capabilities

### Open Question 5
- Question: Does IV framework capture all task-relevant information, or are there additional latent variables beyond θc that contribute to task performance?
- Basis in paper: [inferred] Paper focuses on single latent variable θc for instruction following, but does not rule out existence of other latent variables that may influence task performance
- Why unresolved: Paper does not conduct comprehensive analysis to determine whether θc alone is sufficient to capture all task-relevant information or if other latent variables exist
- What evidence would resolve it: Experiments designed to identify and isolate additional latent variables through techniques such as sparse coding, variational inference, or other dimensionality reduction methods applied to model representations

## Limitations
- The fundamental assumption about separable instruction-following and knowledge-understanding capabilities requires further validation
- Causal mediation analysis implementation details are not fully specified, affecting reproducibility
- Results demonstrated primarily on Llama2-7B-Chat and three specific benchmarks, limiting generalizability

## Confidence

**High Confidence**:
- Experimental methodology and evaluation framework are clearly defined and reproducible
- Baseline results showing catastrophic forgetting across multiple benchmarks are robust
- Distinction between knowledge probability and instruction probability as evaluation metrics is valid

**Medium Confidence**:
- Causal mediation analysis approach for IV extraction, while methodologically sound, lacks implementation details
- Effectiveness of IV-guided training in reducing forgetting (average 5.16 point improvement) is demonstrated but may vary with implementation details
- Interpretation that instruction-following ability is primary contributor to forgetting is supported by evidence but could benefit from additional validation

**Low Confidence**:
- Fundamental assumption about separable instruction-following and knowledge-understanding capabilities
- Universality of IV representation across different model architectures
- Optimal parameters for IV-guided training (KL-divergence scale, progressive intervention schedule)

## Next Checks
1. Conduct ablation study on IV extraction by using random attention heads or alternative selection methods to verify that specific IV extraction method is necessary for observed improvements

2. Apply IV-guided training approach to different LLM architectures (e.g., GPT-2, OPT, or larger Llama models) to test generalizability beyond Llama2-7B-Chat

3. Design experiments that can independently measure impact on instruction-following versus knowledge-understanding capabilities to validate whether observed improvements can be attributed specifically to preserved instruction-following ability