---
ver: rpa2
title: 'Graph Neural Networks Are More Than Filters: Revisiting and Benchmarking from
  A Spectral Perspective'
arxiv_id: '2412.07188'
source_url: https://arxiv.org/abs/2412.07188
tags:
- gnns
- frequency
- graph
- energy
- spectral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a comprehensive benchmark to measure and
  evaluate the capability of Graph Neural Networks (GNNs) in capturing and leveraging
  information encoded in different frequency components of input graph data. The authors
  challenge the prevalent belief that neighborhood aggregation mechanisms dominate
  GNNs' behavioral characteristics in the spectral domain by demonstrating through
  exploratory studies that other components, such as non-linear layers, play a crucial
  role in shaping output frequency components.
---

# Graph Neural Networks Are More Than Filters: Revisiting and Benchmarking from A Spectral Perspective

## Quick Facts
- arXiv ID: 2412.07188
- Source URL: https://arxiv.org/abs/2412.07188
- Authors: Yushun Dong; Patrick Soga; Yinhan He; Song Wang; Jundong Li
- Reference count: 40
- Primary result: Comprehensive benchmark revealing GNNs exhibit V-shaped accuracy curves in spectral domain, challenging assumptions about neighborhood aggregation dominance

## Executive Summary
This paper introduces a novel benchmark to evaluate Graph Neural Networks' (GNNs) capability in capturing and leveraging information encoded in different frequency components of graph data. Through exploratory studies and extensive experiments across 14 GNN models on six real-world datasets, the authors demonstrate that GNNs typically exhibit V-shaped accuracy curves in the spectral domain, with stronger performance in low and high-frequency components compared to middle frequencies. The research challenges the prevalent belief that neighborhood aggregation mechanisms dominate GNNs' spectral behavior, showing that other components such as non-linear layers play crucial roles in shaping output frequency components.

## Method Summary
The benchmark employs a node classification task where ground truth labels are derived from specific frequency bins of graph eigenvectors, enabling consistent spectral evaluation for both spatial- and spectral-based GNNs. The method computes graph Laplacian eigenvectors, sorts them by eigenvalue, and bins them into 20 uniform-width ranges. Node labels are assigned based on frequency bin membership, and GNNs are trained using standard cross-entropy loss. The evaluation measures accuracy across different frequency bins and computes Normalized Area Under the Accuracy Curve (Normalized AUAC) as a quantitative metric. The benchmark is applied to 14 GNN models with 2 layers and 64 hidden dimensions, trained for 500 epochs using Adam optimizer.

## Key Results
- GNNs exhibit V-shaped accuracy curves in spectral domain, performing better on low and high-frequency components than middle frequencies
- SAGE, GCNII, and GATv2 demonstrate superior performance in capturing spectral information despite not being primarily designed from spectral perspective
- Non-linear layers significantly contribute to frequency component generation beyond neighborhood aggregation filtering
- The discretization process in node classification tasks maintains consistency with continuous regression targets in spectral energy distribution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GNNs can recover frequency components absent in input through non-linear layers.
- **Mechanism:** The non-linear activation functions in GNNs enable frequency component generation that wasn't present in the input signal, allowing the model to align outputs with target frequency distributions even when those frequencies are filtered out by neighborhood aggregation.
- **Core assumption:** Non-linear layers contribute meaningfully to frequency shaping beyond what the linear neighborhood aggregation filter alone provides.
- **Evidence anchors:**
  - [abstract] "recent studies suggest that other components such as non-linear layers may also significantly affecting how GNNs process the input graph data in the spectral domain"
  - [section] "several studies have observed that non-linear layers can affect the frequency components of the GNNs' output (Balcilar et al., 2021b; Yang et al., 2024)"
- **Confidence:** Medium

### Mechanism 2
- **Claim:** Node classification tasks with frequency-specific incentives reveal GNN spectral capabilities.
- **Mechanism:** By creating ground truth labels derived from specific frequency bins, the supervision signal provides clear incentives for GNNs to capture and leverage information from those frequency ranges, enabling evaluation of their spectral performance.
- **Core assumption:** The discretization process in node classification doesn't significantly distort the energy distribution compared to continuous regression targets.
- **Evidence anchors:**
  - [abstract] "The theoretical analysis proves that the additional discretization process in node classification tasks does not significantly deviate from the ground truth's energy distribution in the spectral domain"
  - [section] "the additional discretization process only brings an upper-bounded energy distribution deviation compared with the continuous ground truth values in the node-level regression task"
- **Confidence:** Medium

### Mechanism 3
- **Claim:** V-shaped accuracy curves indicate GNNs' strength in low and high frequencies over middle frequencies.
- **Mechanism:** The neighborhood aggregation mechanism naturally excels at capturing low-frequency information through homophily, while GNNs can learn to capture high-frequency information by avoiding uniform predictions for connected nodes. Middle frequencies are most challenging because neighborhoods have uniform label distributions.
- **Core assumption:** The difficulty in capturing middle frequency information stems from the inherent limitation of neighborhood aggregation when local neighborhoods lack discriminative label information.
- **Evidence anchors:**
  - [section] "GNNs typically exhibit V-shaped accuracy curves in the spectral domain, with stronger performance in low and high-frequency components compared to middle frequencies"
  - [section] "when the task-relevant information is encoded in the middle frequency components... the label distribution of each node's neighborhood is generally uniform"
- **Confidence:** High

## Foundational Learning

- **Concept:** Graph Laplacian spectrum and frequency components
  - Why needed here: Understanding how graph data can be decomposed into frequency components is essential for analyzing GNN behavior in the spectral domain
  - Quick check question: What property of the graph Laplacian matrix determines the frequency ordering of eigenvectors?

- **Concept:** Energy distribution in spectral domain
  - Why needed here: Energy distribution quantifies how much information is encoded in different frequency components, which is crucial for the benchmark evaluation
  - Quick check question: How does the energy distribution change when a signal passes through a low-pass filter?

- **Concept:** Node classification vs regression in spectral analysis
  - Why needed here: The paper transitions from regression (exploratory study) to classification (benchmark) while maintaining spectral consistency
  - Quick check question: What mathematical property ensures that discretization from regression to classification preserves spectral energy distribution?

## Architecture Onboarding

- **Component map:** Graph Laplacian eigendecomposition -> Binning into frequency ranges -> Node label assignment -> GNN training -> Frequency-specific accuracy evaluation -> Normalized AUAC calculation

- **Critical path:**
  1. Compute graph Laplacian eigenvectors and sort by eigenvalue
  2. Bin eigenvectors into frequency ranges
  3. Assign node labels based on frequency bin membership
  4. Train GNN with standard cross-entropy loss
  5. Evaluate accuracy for each frequency bin
  6. Compute Normalized AUAC metric

- **Design tradeoffs:**
  - Bin width selection: Wider bins reduce resolution but increase statistical stability; narrower bins provide more detailed analysis but may suffer from variance
  - Layer depth: Deeper networks may capture more complex frequency patterns but risk over-smoothing
  - Feature preprocessing: Standardization affects optimization dynamics and frequency recovery

- **Failure signatures:**
  - Flat accuracy curves across all frequencies: Model fails to capture any frequency-specific information
  - Random accuracy patterns: Potential issues with label assignment or frequency binning
  - Extremely high variance across runs: May indicate insufficient sample size per frequency bin

- **First 3 experiments:**
  1. Reproduce the exploratory study: Train GCN on eigenvector targets using different frequency combinations as features to verify frequency recovery capability
  2. Run the benchmark on a small dataset: Select one dataset and compute accuracy curves for 2-3 GNN models to validate the V-shaped pattern
  3. Test discretization effect: Compare energy distribution preservation between continuous regression targets and discretized classification targets on a small scale

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different non-linear activation functions (ReLU, LeakyReLU, GELU, etc.) affect GNNs' ability to recover frequency components that are filtered out by neighborhood aggregation?
- Basis in paper: [inferred] The paper mentions that non-linear layers can affect frequency components but doesn't systematically study different activation functions
- Why unresolved: The exploratory study uses a fixed GCN architecture without varying activation functions, leaving open the question of which activations are most effective for spectral recovery
- What evidence would resolve it: Controlled experiments comparing multiple activation functions on the same datasets, measuring their ability to recover specific frequency components

### Open Question 2
- Question: What is the theoretical relationship between the depth of GNNs and their ability to capture middle-frequency components?
- Basis in paper: [explicit] The paper notes that middle frequencies are most difficult to capture and conducts experiments with 2-4 layers, but doesn't provide theoretical analysis
- Why unresolved: While empirical results show consistent V-shaped curves across different depths, the paper doesn't explain why depth doesn't significantly change the spectral characteristics
- What evidence would resolve it: Mathematical analysis connecting layer depth to spectral response properties, explaining why additional layers don't improve middle-frequency capture

### Open Question 3
- Question: How does the proposed benchmark's evaluation protocol generalize to graph-level prediction tasks?
- Basis in paper: [inferred] The benchmark focuses on node-level tasks, but the problem statement mentions "graph-based learning tasks" more broadly
- Why unresolved: The frequency-specific incentives and evaluation metrics are designed for node classification, with no discussion of their applicability to graph-level tasks
- What evidence would resolve it: Extension of the benchmark to graph-level tasks with appropriate frequency-specific incentives and validation of the evaluation protocol's consistency

## Limitations
- Limited theoretical analysis of the relationship between GNN depth and spectral recovery capabilities
- Does not explore how different non-linear activation functions affect frequency component generation
- Benchmark focuses on node-level tasks, leaving open questions about graph-level generalization

## Confidence
- Mechanism 1 (non-linear layer frequency generation): Medium
- Mechanism 2 (discretization consistency): Medium
- Mechanism 3 (V-shaped accuracy pattern): High

## Next Checks
1. **Ablation study on non-linear layers**: Remove activation functions from selected GNN models and re-run the benchmark to directly measure the impact on frequency recovery capability, particularly for high-frequency components.

2. **Energy distribution verification**: Implement the theoretical proof showing that discretization introduces only bounded energy deviation by measuring actual energy distribution differences between continuous regression targets and discretized classification labels across multiple frequency ranges.

3. **Cross-dataset consistency analysis**: Apply the benchmark to synthetic graphs with known spectral properties (e.g., stochastic block models with controlled homophily) to verify that observed V-shaped patterns and frequency-specific performance align with theoretical expectations.