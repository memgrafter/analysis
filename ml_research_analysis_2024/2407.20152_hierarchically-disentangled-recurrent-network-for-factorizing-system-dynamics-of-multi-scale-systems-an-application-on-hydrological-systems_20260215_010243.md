---
ver: rpa2
title: 'Hierarchically Disentangled Recurrent Network for Factorizing System Dynamics
  of Multi-scale Systems: An application on Hydrological Systems'
arxiv_id: '2407.20152'
source_url: https://arxiv.org/abs/2407.20152
tags:
- data
- fhnn
- states
- streamflow
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a factorized hierarchical neural network
  (FHNN) for streamflow forecasting in hydrology. The model addresses the challenge
  of modeling multi-scale processes in dynamical systems by decomposing the internal
  states at different temporal scales (slow, medium, fast) and capturing their interactions.
---

# Hierarchically Disentangled Recurrent Network for Factorizing System Dynamics of Multi-scale Systems: An application on Hydrological Systems

## Quick Facts
- arXiv ID: 2407.20152
- Source URL: https://arxiv.org/abs/2407.20152
- Authors: Rahul Ghosh; Arvind Renganathan; Zac McEachran; Kelly Lindsay; Somya Sharma; Michael Steinbach; John Nieber; Christopher Duffy; Vipin Kumar
- Reference count: 22
- Primary result: Achieves up to 45% improvement in Nash-Sutcliffe Efficiency (NSE) over traditional methods for streamflow forecasting

## Executive Summary
This paper introduces a factorized hierarchical neural network (FHNN) for streamflow forecasting in hydrology that addresses the challenge of modeling multi-scale processes in dynamical systems. The model decomposes internal states at different temporal scales (slow, medium, fast) and captures their interactions through an inverse model that infers these states from historical data and a forward model that uses them for prediction. FHNN outperforms standard baselines including physics-based models and LSTM-based approaches, achieving significant improvements in forecasting accuracy while maintaining effectiveness even with limited training data through pre-training strategies and global modeling approaches.

## Method Summary
FHNN uses a factorized hierarchical architecture with an inverse model that infers temporal states at three different scales (slow, medium, fast) using parallel bidirectional LSTM layers, and a forward model that predicts future streamflow using these states. The model can be pre-trained on imperfect SAC-SMA simulation data to learn realistic hydrological patterns before fine-tuning on limited observed data, and can also be trained as a global model across multiple basins to capture shared hydrological knowledge. This approach allows the model to capture multi-scale temporal dependencies without expensive optimization approaches like Ensemble Kalman Filtering for data assimilation.

## Key Results
- FHNN achieves up to 45% improvement in Nash-Sutcliffe Efficiency (NSE) over traditional methods for 1-7 day streamflow forecasts
- The model is particularly effective for basins with low runoff ratios and colder climates
- Pre-training on SAC-SMA simulation data significantly improves performance, especially with limited observed data
- Global modeling approach maintains accuracy while reducing per-basin data requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FHNN captures multi-scale temporal dependencies by factorizing system dynamics into slow, medium, and fast latent states.
- Mechanism: The inverse model uses three parallel bidirectional LSTM layers operating at different temporal resolutions (fast: ∆t, medium: m∆t, slow: s∆t). These layers progressively build hierarchical representations by processing skip-connected sequences, allowing the model to capture both short-term fluctuations and long-term trends simultaneously.
- Core assumption: Hydrological systems exhibit distinct processes operating at different temporal scales that can be meaningfully separated and modeled independently.
- Evidence anchors:
  - [abstract] "The model addresses the challenge of modeling multi-scale processes in dynamical systems by decomposing the internal states at different temporal scales (slow, medium, fast) and capturing their interactions."
  - [section 3.1] "Our formulation captures the general intuition that we can separate the state of the entity into its dynamic components (described by z0, z1, ..., zl) of different time-scales."

### Mechanism 2
- Claim: FHNN maintains accuracy with limited training data through pre-training on imperfect simulation data and global modeling strategies.
- Mechanism: The model can be pre-trained on SAC-SMA simulation outputs (imperfect but physically consistent) to learn realistic hydrological patterns before fine-tuning on limited observed data. Additionally, a global model trained across multiple basins captures shared hydrological knowledge transferable to data-scarce basins.
- Core assumption: Simulation data, despite being imperfect, captures enough physical realism to provide useful pre-training signals, and hydrological systems share sufficient commonalities across basins for effective knowledge transfer.
- Evidence anchors:
  - [section 3.3.2] "pre-training a neural network using SAC-SMA simulations allows the network to emulate a synthetic realization of physical phenomena"
  - [section 5.2] "Table 3 shows that pre-training can significantly improve the performance. The improvement is relatively much larger given a small amount of observed data."

### Mechanism 3
- Claim: The hierarchical state encoder enables effective data assimilation without expensive optimization approaches like EnKF.
- Mechanism: Once trained, FHNN can incorporate new observations by updating its internal latent states (z) directly through the inverse model, avoiding the need for iterative optimization procedures typically required in physical sciences for data assimilation.
- Core assumption: The learned latent state representation captures sufficient information about the system's current state to allow direct updating with new observations.
- Evidence anchors:
  - [abstract] "A key advantage of our framework is that once trained, it can incorporate new observations into the model's context (internal state) without expensive optimization approaches (e.g., Ensemble Kalman Filtering) that are traditionally used in physical sciences for data assimilation."
  - [section 1] "These models have been used in operational settings for forecasting streamflow and water levels at NOAA's National Weather Service (NWS)."

## Foundational Learning

- Concept: Temporal hierarchy in recurrent networks
  - Why needed here: FHNN's effectiveness relies on understanding how different temporal scales can be modeled separately and combined, rather than treating all time steps equally as in standard RNNs.
  - Quick check question: How would you modify a standard LSTM to process the same input sequence at multiple temporal resolutions simultaneously?

- Concept: Data assimilation in dynamical systems
  - Why needed here: The paper positions FHNN as an alternative to traditional data assimilation methods like EnKF, so understanding the data assimilation problem and existing solutions is crucial.
  - Quick check question: What are the computational trade-offs between sequential Bayesian methods (like EnKF) and the direct latent state updating approach used in FHNN?

- Concept: Transfer learning in hydrological modeling
  - Why needed here: The global modeling approach and pre-training strategies rely on concepts from transfer learning applied to hydrological systems.
  - Quick check question: Under what conditions would training a single global model across multiple basins outperform training individual basin-specific models?

## Architecture Onboarding

- Component map: Historical data → Inverse Model (3 parallel BiLSTM layers at different temporal scales) → MLP (state encoding) → Latent states z → Forward Model (LSTM) → Streamflow predictions

- Critical path: Historical data → Inverse Model → Latent states z → Forward Model → Streamflow predictions

- Design tradeoffs:
  - More temporal scales in the encoder could capture more nuanced dynamics but increase complexity and data requirements
  - Using pre-training trades computational cost upfront for reduced data needs during fine-tuning
  - Global modeling reduces per-basin data requirements but may underperform for highly unique basins

- Failure signatures:
  - Poor performance on basins with very different characteristics from training data (global model limitation)
  - Degraded performance when simulation data quality is very low (pre-training limitation)
  - Overfitting on small datasets if model capacity is too high relative to available data

- First 3 experiments:
  1. Train FHNN on a single well-observed basin and compare performance to LSTM and LSTM-AR baselines
  2. Test pre-training effectiveness by training on simulation data only, then fine-tuning on increasing amounts of observed data
  3. Implement the global model and evaluate performance across basins with varying data availability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the inferred states from FHNN correlate with physical states in process-based models like SAC-SMA?
- Basis in paper: [explicit] The paper states "These FHNN states have the potential to capture underlying systems dynamics" and shows "strong correlation between the FHNN's internal states and the states of the SacSMA and Snow17 physics-based models."
- Why unresolved: While correlation is demonstrated, the paper doesn't explore the mechanistic reasons for this correlation or how the neural states map to specific physical processes.
- What evidence would resolve it: Detailed analysis showing which physical states (e.g., soil moisture, snowpack) each neural state (slow, medium, fast) corresponds to, and understanding the causal relationships between them.

### Open Question 2
- Question: How does the performance of FHNN change when applied to regions with significantly different climate regimes than the training data?
- Basis in paper: [inferred] The paper mentions "Environmental systems are highly heterogeneous" and discusses using global models, but doesn't test performance in dramatically different climate regimes.
- Why unresolved: The experiments focus on the North Central River Forecast Center region and CAMELS dataset, which may not represent extreme climate variations.
- What evidence would resolve it: Testing FHNN on catchments from tropical, arid, or polar regions to evaluate its generalization capabilities and identifying any performance degradation.

### Open Question 3
- Question: What is the optimal number and type of temporal scales to model for different hydrological systems?
- Basis in paper: [explicit] The paper uses three temporal scales (slow, medium, fast) but states "Our framework can be applied to model any number of such internal states."
- Why unresolved: The choice of three scales appears to be based on the specific application and may not be optimal for all systems.
- What evidence would resolve it: Systematic evaluation of FHNN performance with varying numbers of temporal scales across different types of catchments to identify optimal configurations.

## Limitations

- Model Generalization Across Climates: The paper does not extensively analyze performance degradation when applying the global model to basins with extreme characteristics (arid, tropical, or highly urbanized).
- Simulation Data Quality Dependency: The pre-training strategy's effectiveness is highly dependent on SAC-SMA simulation quality, but the paper lacks analysis of how simulation-data mismatches affect downstream performance.
- Temporal Scale Selection: The choice of three temporal scales appears empirically motivated rather than theoretically derived, with no exploration of whether different numbers of scales would yield better performance.

## Confidence

- High Confidence: The core mechanism of factorizing temporal states using parallel BiLSTM layers is well-supported by the architectural description and performance improvements shown in the results.
- Medium Confidence: The pre-training effectiveness claim is supported by quantitative results but lacks analysis of failure cases or the impact of simulation-data quality.
- Medium Confidence: The data assimilation advantage claim is theoretically sound but lacks empirical validation against traditional methods like EnKF.

## Next Checks

1. **Cross-Climate Robustness Test**: Apply the pre-trained global FHNN model to basins with significantly different climatic conditions (e.g., arid Southwest vs. humid Southeast) and quantify performance degradation compared to basin-specific training.

2. **Simulation Data Quality Analysis**: Systematically vary the quality of SAC-SMA simulation data used for pre-training (e.g., using uncalibrated vs. calibrated simulations) and measure the impact on FHNN performance across different data availability scenarios.

3. **Temporal Scale Sensitivity Analysis**: Conduct an ablation study testing FHNN with different numbers of temporal scales (2, 3, 4, 5) and measure the trade-off between model complexity and forecasting accuracy across basins with varying hydro-climatic characteristics.