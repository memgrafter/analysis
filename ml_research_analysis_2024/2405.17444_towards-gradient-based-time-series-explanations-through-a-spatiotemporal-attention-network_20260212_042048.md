---
ver: rpa2
title: Towards Gradient-based Time-Series Explanations through a SpatioTemporal Attention
  Network
arxiv_id: '2405.17444'
source_url: https://arxiv.org/abs/2405.17444
tags:
- local
- global
- stan
- frames
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explored using a transformer-based spatiotemporal attention
  network (STAN) for gradient-based time-series explanations. The STAN model was trained
  for video classification using global and local views of data and weakly supervised
  labels.
---

# Towards Gradient-based Time-Series Explanations through a SpatioTemporal Attention Network

## Quick Facts
- arXiv ID: 2405.17444
- Source URL: https://arxiv.org/abs/2405.17444
- Authors: Min Hun Lee
- Reference count: 37
- Primary result: STAN with global+local views and smoothgrad achieves 86.68 F1-score on short videos, outperforming CNN baselines

## Executive Summary
This paper explores using a transformer-based spatiotemporal attention network (STAN) for gradient-based time-series explanations. The STAN model was trained for video classification using global and local views of data with weakly supervised labels. Saliency maps were then used to identify salient frames in the time-series data. Experiments on four medically relevant activities showed that the STAN model with global and local views and smoothgrad method achieved an 86.68 F1-score for identifying important frames in short videos (20 frames), outperforming CNN models with global views. However, the STAN model performed worse than CNN models on longer video sequences (394 frames).

## Method Summary
The STAN model uses unified transformer (UniFormer) blocks with a four-stage hierarchical design. It processes video clips using both global views and local views (generated through ROI-based masking using pose estimation). The first two stages employ local UniFormer blocks with depthwise convolutions to reduce spatiotemporal redundancy, while the last two stages use global UniFormer blocks with self-attention to capture long-range dependencies. The model is trained on video-level labels and generates frame-level explanations through gradient-based saliency maps (vanilla gradient, smoothgrad, gradcam).

## Key Results
- STAN with global+local views and smoothgrad achieves 86.68 F1-score on short videos (20 frames)
- This is 4.68 F1-score higher than CNN model with only global views (82.04 F1-score)
- STAN performs worse than CNN models on longer video sequences (394 frames)
- Local views (ROI-masked) improve performance by focusing on human activity regions and reducing background noise

## Why This Works (Mechanism)

### Mechanism 1
- STAN's use of both global and local views improves video classification and time-series explanations compared to using either view alone.
- The local views, generated through ROI-based masking, help the model focus on relevant human activity regions, reducing noise from irrelevant background, while global views provide contextual information.
- Core assumption: Background pixels contain less relevant information for human activity recognition, and ROI-based local views can guide the model to learn more task-relevant features.
- Evidence: Results showed models with global + local views performed better or equivalent to other models using only global or local view.

### Mechanism 2
- STAN's Unified Transformer (UniFormer) blocks with mixed local and global attention capture both short-term spatiotemporal redundancy and long-range dependencies effectively.
- The first two stages use local UniFormer blocks with depthwise convolutions to reduce short-term redundancy and capture local context, while the last two stages use global UniFormer blocks with self-attention to capture long-range dependencies and context.
- Core assumption: Video activities have both local motion patterns (short-term) and global semantic context (long-range) that require different attention mechanisms.
- Evidence: The hierarchical design matches the multiscale nature of video data.

### Mechanism 3
- Gradient-based saliency maps from STAN can effectively identify important frames in short video sequences by highlighting regions contributing to activity classification.
- The model computes gradients of the classification score with respect to input data, which indicates how much each pixel contributes to the output. Normalizing these gradients across frames identifies which frames contain the most discriminative information for classification.
- Core assumption: Higher gradient values correspond to more important features for the model's decision, and these features align with human-annotated activity-relevant frames.
- Evidence: STAN with global and local views and smoothgrad method achieved 86.68 F1-score on short videos.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: STAN is built on transformer blocks, and understanding how self-attention works is crucial for grasping why global UniFormer blocks can capture long-range dependencies in video sequences.
  - Quick check question: How does self-attention in transformers differ from convolutional approaches in handling spatial relationships?

- Concept: Weakly supervised learning
  - Why needed here: The model is trained with only video-level labels (activity type) rather than frame-level annotations, which is important for understanding the training setup and the challenges in generating frame-level explanations.
  - Quick check question: What are the key challenges when training models with only video-level labels but needing frame-level predictions?

- Concept: Saliency map generation and gradient-based explanations
  - Why needed here: The paper's core contribution is using gradient-based methods (vanilla gradient, smoothgrad, gradcam) to generate explanations, so understanding how these methods work is essential.
  - Quick check question: How does smoothgrad improve upon vanilla gradient-based saliency maps?

## Architecture Onboarding

- Component map: Video input (3×T×H×W) → 3×4×4 conv (stride 2×4×4) → 1×2×2 conv (stride 1×2×2) between stages → Four-stage UniFormer blocks (Local → Global) → Classification scores → Gradient computation → Frame-level saliency scores

- Critical path: Video input → Unified transformer processing → Classification → Gradient computation → Frame importance scoring

- Design tradeoffs:
  - Global vs local views: Local views reduce background noise but require accurate ROI detection; global views provide context but may include irrelevant information
  - Short vs long sequences: STAN performs well on short sequences (20 frames) but worse on long sequences (394 frames) compared to CNN baselines
  - Gradient methods: Smoothgrad improves over vanilla gradient by reducing noise, but gradcam performs worse than vanilla gradient for this task

- Failure signatures:
  - Poor ROI detection leading to incorrect local views
  - Computational constraints preventing proper handling of long sequences
  - Gradients becoming too diffused, especially with gradcam
  - Model learning spurious correlations rather than true activity-related features

- First 3 experiments:
  1. Train STAN with only global views on NTU RGB+D dataset and evaluate classification performance
  2. Add local views (ROI-based) to the same model and measure improvement in both classification and explanation quality
  3. Compare vanilla gradient, smoothgrad, and gradcam methods on a held-out validation set for explanation quality using F1-score against frame-level annotations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of STAN for time-series explanations scale with increasing video length beyond 394 frames?
- Basis in paper: The paper found STAN performed worse than CNN models on longer video sequences (394 frames) compared to short sequences (20 frames).
- Why unresolved: The paper only tested up to 394 frames, leaving performance on longer sequences unknown.
- What evidence would resolve it: Experiments testing STAN on videos with 500+ frames to compare its time-series explanation performance against CNN models.

### Open Question 2
- Question: Can alternative transformer architectures or modifications to STAN improve its performance on longer video sequences?
- Basis in paper: The paper identified limitations of STAN on long sequences and suggested further research is needed.
- Why unresolved: The paper only tested one transformer-based STAN model and did not explore architectural variations.
- What evidence would resolve it: Comparative experiments testing STAN with architectural modifications or alternative transformer models on long video sequences.

### Open Question 3
- Question: How do STAN's time-series explanations compare to human expert annotations on medically relevant activities?
- Basis in paper: The paper used frame-level annotations to evaluate STAN but did not compare to human experts.
- Why unresolved: The paper only used frame-level annotations, not expert evaluations.
- What evidence would resolve it: Comparative studies having human experts evaluate STAN's identified important frames against their own assessments.

## Limitations
- STAN's performance degrades significantly on longer video sequences (394 frames) compared to CNN baselines
- The method relies on accurate ROI detection for local views, which could fail if pose estimation is imperfect
- The evaluation is limited to specific medical activities and may not generalize to other time-series domains

## Confidence

- **High confidence**: STAN's superior performance on short video sequences (20 frames) with global+local views achieving 86.68 F1-score
- **Medium confidence**: The claim that STAN captures hierarchical local-to-global dependencies better than CNNs, based on the architectural design but limited empirical validation
- **Low confidence**: Generalizability to other time-series domains beyond the tested medical activities, as the evaluation is constrained to specific datasets and activities

## Next Checks

1. Test STAN on longer sequences (500+ frames) with efficient attention mechanisms to validate scalability claims and identify computational bottlenecks
2. Conduct ablation studies on ROI detection quality to quantify the impact of local view accuracy on overall performance
3. Compare STAN against other transformer-based models (ViT, TimeSformer) using identical data preprocessing to isolate the contribution of the unified transformer architecture