---
ver: rpa2
title: 'SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded Entity
  Retrieval'
arxiv_id: '2401.02369'
source_url: https://arxiv.org/abs/2401.02369
tags:
- entities
- entity
- source
- notes
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper fine-tunes open-source LLMs (Mistral-7B-Instruct and
  Zephyr-7B-beta) on hospital-course summarization, finding that the models generate
  incomplete and unfaithful summaries. To improve entity coverage, the authors train
  a smaller encoder-only model to predict salient entities and use them as content
  plans to guide the LLM.
---

# SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded Entity Retrieval
## Quick Facts
- arXiv ID: 2401.02369
- Source URL: https://arxiv.org/abs/2401.02369
- Reference count: 19
- Fine-tunes Mistral-7B-Instruct and Zephyr-7B-beta on hospital-course summarization; proposes SPEER to improve entity coverage and faithfulness via sentence-level planning and embedded entity retrieval.

## Executive Summary
This paper addresses the challenge of generating complete and faithful long clinical summaries from hospital admissions. Open-source LLMs, while capable, tend to produce summaries that miss important entities and may hallucinate information. To mitigate these issues, the authors introduce SPEER (Sentence-level Planning via Embedded Entity Retrieval), a method that uses a trained salience model to identify important entity spans, marks them in the text, and guides the LLM to retrieve and use these spans before generating each sentence. Evaluated on a large dataset of hospital admissions and diverse test sets, SPEER improves both entity coverage and faithfulness compared to non-guided and other guided baselines.

## Method Summary
The authors fine-tune open-source LLMs (Mistral-7B-Instruct and Zephyr-7B-beta) for hospital-course summarization, but find that generated summaries are incomplete and sometimes unfaithful. To address this, they train a smaller encoder-only model to predict salient entities from the source text. SPEER then marks each salient entity with special boundary tags and instructs the LLM to retrieve these marked spans before generating each sentence. This process acts as a form of state tracking, recording which entities have been used. The method is evaluated on a large-scale dataset (~167k hospital admissions) and three diverse test sets, showing improvements in coverage and faithfulness metrics.

## Key Results
- SPEER improves entity coverage and faithfulness over non-guided and guided baselines in clinical summarization.
- Gains are demonstrated on a large-scale dataset of ~167k hospital admissions and three diverse test sets.
- SPEER acts as state tracking by recording used entities during sentence generation.

## Why This Works (Mechanism)
SPEER improves summarization by explicitly guiding the LLM to retrieve and use salient entities at the sentence level. By marking important entity spans and requiring their retrieval before each sentence, the model is less likely to omit critical information or hallucinate, as it has a concrete content plan to follow. This mechanism reduces the cognitive load on the LLM and provides a structured approach to long-form summarization.

## Foundational Learning
- **Entity Salience Prediction**: Identifying which entities in the source text are important for the summary. *Why needed*: Ensures the model focuses on relevant information. *Quick check*: Verify the salience model's precision/recall on a held-out set.
- **Special Boundary Tagging**: Marking salient entities with unique tags in the input. *Why needed*: Provides explicit signals to the LLM about which entities to use. *Quick check*: Confirm tags are correctly placed and distinguishable.
- **Sentence-Level Retrieval**: Requiring the LLM to retrieve marked entities before generating each sentence. *Why needed*: Acts as state tracking, ensuring entity usage is recorded. *Quick check*: Inspect generated sentences for correct entity usage.
- **State Tracking in Generation**: Recording which entities have been used as the summary is generated. *Why needed*: Prevents repetition and ensures completeness. *Quick check*: Verify entity usage tracking is accurate.

## Architecture Onboarding
- **Component Map**: Salience Model -> Entity Tagging -> SPEER Retrieval Loop -> LLM Summary Generation
- **Critical Path**: Salience model predicts salient entities → entities are tagged → SPEER retrieves marked spans → LLM generates sentence → entity usage is tracked
- **Design Tradeoffs**: Sentence-level retrieval improves entity coverage but adds computational overhead; entity tagging may increase input length; model complexity vs. faithfulness.
- **Failure Signatures**: Missing salient entities in summaries (low coverage), hallucination of non-existent entities, repetitive or incomplete sentences, retrieval failures due to incorrect tagging.
- **First Experiments**: 1) Validate salience model predictions on a validation set. 2) Test entity tagging and retrieval on a small sample of cases. 3) Compare generated summaries with and without SPEER on a held-out test set.

## Open Questions the Paper Calls Out
None

## Limitations
- It is unclear whether gains are due to entity tagging or retrieval; ablation study does not isolate these components.
- No computational overhead or latency impacts of the retrieval step are reported, which is important for clinical deployment.
- Automated metrics are used, but no human evaluation is reported to confirm clinical meaningfulness.
- Potential data leakage exists as the entity salience model and LLM are trained on the same dataset.

## Confidence
- **High**: Novelty of the sentence-level planning approach in clinical summarization.
- **Medium**: Core claim that SPEER improves entity coverage and faithfulness, given automated metric improvements but lack of human validation and potential data leakage.
- **Low**: Scalability and generalizability to other clinical domains due to single-hospital dataset and absence of cross-institutional validation.

## Next Checks
1. Conduct a human evaluation study with clinicians to assess whether SPEER-generated summaries are more complete and accurate than baselines in real-world clinical settings.
2. Perform an ablation study to separately evaluate the impact of entity tagging versus entity retrieval on summarization quality.
3. Train and evaluate the entity salience model and LLM on disjoint datasets to rule out data leakage and ensure robustness.