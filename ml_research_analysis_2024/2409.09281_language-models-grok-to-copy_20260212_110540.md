---
ver: rpa2
title: Language Models "Grok" to Copy
arxiv_id: '2409.09281'
source_url: https://arxiv.org/abs/2409.09281
tags:
- copying
- grokking
- context
- language
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how transformer-based language models develop
  copying abilities during pre-training. The authors propose that this development
  resembles "grokking," where models suddenly generalize after overfitting.
---

# Language Models "Grok" to Copy

## Quick Facts
- arXiv ID: 2409.09281
- Source URL: https://arxiv.org/abs/2409.09281
- Authors: Ang Lv; Ruobing Xie; Xingwu Sun; Zhanhui Kang; Rui Yan
- Reference count: 7
- Primary result: Transformer models exhibit delayed copying capability that emerges after training loss stabilizes, resembling grokking behavior

## Executive Summary
This paper investigates how transformer-based language models develop copying abilities during pre-training, revealing that this development resembles the "grokking" phenomenon where models suddenly generalize after overfitting. The authors demonstrate that context copying accuracy shows a sudden increase long after training loss stabilizes, with the speed of development depending on update steps rather than token count. They also show that induction heads responsible for copying form in a shallow-to-deep layer progression during training. The study suggests that regularization techniques known to enhance grokking can accelerate copying development, offering insights for more effective language model training.

## Method Summary
The authors train 12-layer Llama models (162M parameters) on 40 billion tokens from the RedPajama dataset using AdamW optimizer with learning rate 0.1, batch size 64 per GPU, and 2000 warmup steps. They evaluate context copying ability using a test set with 50 random token subsequences, each containing unique 12-gram prefixes and 6-gram suffixes. Copying accuracy is measured by whether models correctly complete prefixes by copying suffixes from context. Models are evaluated at regular checkpoints to track the development of copying abilities over training steps.

## Key Results
- Context copying accuracy shows a sudden increase long after training loss stabilizes, similar to grokking behavior
- Speed of developing copying ability depends on update steps rather than token count, consistent with grokking's data-amount-independent generalization
- Induction heads responsible for copying form from shallow to deep layers during training, mirroring circuit development in grokking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer models exhibit delayed copying capability that emerges after training loss stabilizes
- Mechanism: Models first learn general language modeling, then develop specific copying circuits (induction heads) that enable context copying
- Core assumption: Training loss reduction doesn't directly reflect copying ability development
- Evidence anchors:
  - [abstract] "context copying accuracy shows a sudden increase long after the training loss stabilizes"
  - [section] "The training loss stabilizes after 5B tokens, indicating that the fundamental language modeling has been established... However, the accuracy is low until 10B tokens have been trained. A surge in accuracy occurs at 15B tokens"
  - [corpus] Weak evidence - neighboring papers discuss copying but don't specifically address the delayed emergence phenomenon
- Break condition: If copying ability develops concurrently with training loss reduction, or if accuracy increases smoothly rather than abruptly

### Mechanism 2
- Claim: Copying ability development speed depends on update steps rather than token count
- Mechanism: The number of gradient updates determines when copying circuits form, not the total data volume processed
- Core assumption: Data distribution preservation matters more than data quantity for copying development
- Evidence anchors:
  - [abstract] "speed of developing copying ability is independent of the number of tokens trained, similarly to how grokking speed is unaffected by dataset size"
  - [section] "Results indicate that context copying is developed after certain updates, rather than after processing a specific quantity of tokens"
  - [corpus] Weak evidence - neighboring papers don't directly address the update-step vs token-count relationship
- Break condition: If copying ability scales linearly with token count regardless of batch size, or if different batch sizes show different copying speeds after equal updates

### Mechanism 3
- Claim: Induction heads responsible for copying form in shallow-to-deep layer progression
- Mechanism: Circuit development follows a specific architectural pattern where copying capability emerges from lower to higher layers
- Core assumption: Copying requires coordinated multi-layer attention mechanisms that develop sequentially
- Evidence anchors:
  - [abstract] "Induction heads, the attention heads responsible for copying, form from shallow to deep layers during training, mirroring the development of circuits in deeper layers during grokking"
  - [section] "induction heads... form from shallow to deep layers during training, consistent with research showing deeper circuits form in Transformers after grokking"
  - [corpus] Moderate evidence - neighboring papers discuss induction heads and circuit formation
- Break condition: If induction heads appear simultaneously across all layers, or if copying ability develops without proper induction head formation

## Foundational Learning

- Concept: Grokking phenomenon
  - Why needed here: Understanding that models can generalize long after overfitting is crucial for interpreting the delayed copying emergence
  - Quick check question: What distinguishes grokking from regular overfitting in terms of test set performance timing?

- Concept: Attention mechanisms and induction heads
  - Why needed here: Copying ability fundamentally relies on attention heads that can copy sequences from context
  - Quick check question: How do induction heads differ from regular attention heads in their copying behavior?

- Concept: Batch size vs update steps tradeoff
  - Why needed here: Understanding that copying development depends on optimization steps rather than data volume is counterintuitive
  - Quick check question: Why might a model trained with larger batch size need more tokens but the same number of updates to achieve copying ability?

## Architecture Onboarding

- Component map: 12-layer Transformer with 12 attention heads per layer, 768 hidden dimension, 3072 intermediate MLP dimension, 32,000 token vocabulary, 1,024 context length

- Critical path:
  1. Initial language modeling establishment (training loss stabilization)
  2. Copying circuit formation (induction head development)
  3. Copying ability manifestation (accuracy surge)
  4. Regularization application for enhancement

- Design tradeoffs:
  - Batch size vs copying speed: Larger batches reduce update frequency, slowing copying development
  - Learning rate impact: Higher rates accelerate copying but may affect stability
  - Layer depth: More layers provide more capacity for circuit formation but increase training complexity

- Failure signatures:
  - Copying accuracy never surges after training loss stabilizes
  - Copying ability scales linearly with token count regardless of batch size
  - Induction heads fail to form in proper shallow-to-deep sequence
  - Regularization techniques have no effect on copying development

- First 3 experiments:
  1. Train models with different batch sizes while monitoring copying accuracy after equal update steps
  2. Vary learning rates to observe impact on copying development timing
  3. Apply different regularization techniques (dropout, weight decay) to measure copying enhancement effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the grokking phenomenon observed in context copying extend to more complex in-context learning tasks beyond simple pattern completion?
- Basis in paper: [inferred] The paper acknowledges that ICL presents a higher level of complexity compared to simple copying tasks and that the authors were unable to train models to achieve robust ICL performance due to limited computational resources.
- Why unresolved: The study focused on copying tasks as a proxy for in-context learning, but didn't evaluate actual ICL performance due to computational constraints.
- What evidence would resolve it: Training and evaluating larger models on actual ICL benchmarks (like MMLU or BIG-bench) while tracking the development of in-context abilities over training steps.

### Open Question 2
- Question: What is the precise relationship between optimization intensity (learning rate Ã— update steps) and the emergence of context copying abilities?
- Basis in paper: [explicit] The authors note that increased learning rates facilitate earlier and stronger grokking, suggesting that grokked context copying emerges at a specific optimization intensity determined by both learning rate and update steps.
- Why unresolved: The paper observes this relationship but doesn't provide a quantitative model or threshold for when copying abilities emerge based on optimization intensity.
- What evidence would resolve it: Systematic experiments varying learning rates and batch sizes to create a heatmap showing when copying abilities emerge as a function of optimization intensity.

### Open Question 3
- Question: How do regularization techniques like dropout and weight decay differentially affect the development of context copying versus other language modeling abilities?
- Basis in paper: [explicit] The authors demonstrate that regularization techniques enhance grokked copying, but note that improvements in other abilities can still lead to decreased training loss after copying accuracy saturates.
- Why unresolved: The paper shows that regularization improves copying accuracy but doesn't analyze whether these techniques differentially affect copying versus other language modeling capabilities.
- What evidence would resolve it: Comparative analysis of how regularization affects copying accuracy versus other metrics like perplexity, ICL score, and downstream task performance on both training and test sets.

## Limitations
- Limited to a single model architecture (12-layer Llama) and dataset (RedPajama), potentially limiting generalizability
- Does not investigate whether observed phenomena occur in larger models or with different training objectives
- Does not fully characterize the exact mechanisms by which induction heads form and coordinate across layers

## Confidence
- Delayed copying capability emergence after training loss stabilization: Medium confidence
- Copying ability development speed depends on update steps rather than token count: Medium confidence
- Induction heads form in shallow-to-deep layer progression: High confidence
- Regularization techniques accelerate copying development: Medium confidence

## Next Checks
1. Test whether the copying development pattern holds across different model sizes (varying layer depth and hidden dimensions) to verify architectural dependencies
2. Conduct experiments with multiple datasets of varying complexity and distribution to assess whether copying ability development is dataset-independent
3. Implement ablation studies removing specific layers or attention heads to determine the minimum requirements for copying ability emergence