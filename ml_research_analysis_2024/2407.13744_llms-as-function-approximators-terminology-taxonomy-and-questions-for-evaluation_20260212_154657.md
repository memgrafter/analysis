---
ver: rpa2
title: 'LLMs as Function Approximators: Terminology, Taxonomy, and Questions for Evaluation'
arxiv_id: '2407.13744'
source_url: https://arxiv.org/abs/2407.13744
tags:
- function
- language
- what
- answer
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes to view large language models (LLMs) as function
  approximators to better evaluate their capabilities and limitations. It formalizes
  the idea of LLMs inducing functions from natural language task descriptions and
  categorizes task types based on semantic relationships between inputs and outputs.
---

# LLMs as Function Approximators: Terminology, Taxonomy, and Questions for Evaluation

## Quick Facts
- **arXiv ID**: 2407.13744
- **Source URL**: https://arxiv.org/abs/2407.13744
- **Authors**: David Schlangen
- **Reference count**: 20
- **Key outcome**: This paper proposes viewing LLMs as function approximators to evaluate their capabilities and limitations, formalizing task descriptions and categorizing functions based on semantic relationships between inputs and outputs.

## Executive Summary
This paper introduces a novel framework for evaluating large language models by conceptualizing them as universal function approximators that map natural language task descriptions to outputs. The approach formalizes how prompts induce functions and categorizes tasks into transformation, categorization, and additive types based on their semantic relationships. This "least commitment" perspective maintains the semantic relationship between inputs and outputs while enabling discussions about function quality, induction stability, and coverage. The framework connects various aspects of LLM evaluation, from reference-based testing to preference-based assessment and issues like prompt injection.

## Method Summary
The paper formalizes prompts into components (intensional task description, extensional task description with examples, and target instance) and abstracts them to define functions mapping. It categorizes prompt-induced functions based on the type of semantic relation between domain and co-domain: transformation (mapping between same-type domains), categorization (classification with fixed categories), and additive (producing output that extends beyond input domain). The framework evaluates LLMs through three key questions: quality of function approximation, discoverability of functions, and stability of function induction, connecting these to established ML evaluation concepts while adapting them for natural language specifications.

## Key Results
- LLMs can be viewed as function approximators where prompts serve as task descriptions inducing specific functions
- Task types can be categorized based on semantic relationships between inputs and outputs into transformation, categorization, and additive types
- This framing helps identify key evaluation questions around function quality, induction stability, and coverage of function space

## Why This Works (Mechanism)

### Mechanism 1
LLMs approximate functions through natural language specifications where prompts serve as task descriptions that induce specific functions from the latent space of possible mappings. The core assumption is that there exists a stable semantic relationship between input and output sequences that can be captured through natural language descriptions.

### Mechanism 2
Task descriptions can be formalized into intensional and extensional components that induce functions through a structured prompt format containing task description, examples, and target. The prompt structure reliably captures the necessary information to induce the intended function.

### Mechanism 3
Task types can be categorized based on semantic relationships between inputs and outputs into transformation, categorization, and additive types. These categories capture meaningful differences in how functions relate inputs to outputs, providing a framework for understanding function approximation capabilities.

## Foundational Learning

- **Function approximation theory**: Understanding how LLMs approximate functions requires knowledge of function spaces, continuity, and approximation quality. Quick check: Can you explain the difference between interpolation and extrapolation in function approximation?

- **Natural language semantics**: The framing relies on stable semantic relationships between task descriptions and outputs. Quick check: How would you distinguish between intensional and extensional task descriptions in practice?

- **Machine learning evaluation methodology**: The framework builds on established ML evaluation concepts but applies them to function approximation via natural language. Quick check: What are the key differences between reference-based and preference-based evaluation in this context?

## Architecture Onboarding

- **Component map**: LLM model -> Prompt construction system -> Function induction engine -> Evaluation framework -> Protection mechanisms
- **Critical path**: Prompt → LLM → Output → Parse → Function mapping → Quality assessment
- **Design tradeoffs**: Generality vs. specificity in task descriptions, expressiveness vs. simplicity in prompt structure, coverage vs. precision in function taxonomy, automation vs. manual tuning in function induction
- **Failure signatures**: High variance in outputs for semantically equivalent prompts, function induction that doesn't match intended behavior, safety violations in protected function domains, evaluation metrics that don't correlate with practical utility
- **First 3 experiments**: 1) Implement the formal prompt structure and test with simple transformation tasks (translation, summarization) 2) Create a prototype function taxonomy classifier and test on diverse task datasets 3) Build a minimal evaluation framework comparing reference-based vs. preference-based assessment for the same functions

## Open Questions the Paper Calls Out

### Open Question 1
How can we quantify the stability of function induction across semantically insignificant variations in task descriptions? The paper acknowledges that current models are not performing particularly well in this regard and mentions tools like DSPy that are being developed to address this issue, but there is no clear method presented for quantifying or measuring this stability.

### Open Question 2
What is the relationship between tasks that can be successfully prompt-induced (set F̂) and tasks that humans can do? The paper raises this question as part of understanding the "coverage of F" and its relation to human capabilities, but does not provide a methodology for mapping or comparing these sets of tasks.

### Open Question 3
How can we systematically evaluate the protection of functions against prompt injection attacks and undesirable inputs/outputs? While the paper identifies these as important questions for function approximation evaluation, it does not provide a systematic framework or methodology for testing these protections.

### Open Question 4
What is the relation between the performance of models on similar tasks and the underlying mechanism of their responses? The paper poses this question in the context of understanding model capabilities and the relation to human performance, but does not provide a method for systematically comparing model performance across similar tasks or for inferring the underlying mechanisms from performance patterns.

### Open Question 5
How can we determine the assertoric force of function outputs, particularly in additive tasks? The paper identifies this as an important question for understanding the implications of using LLMs as function approximators but does not provide a framework for determining or measuring the assertoric force of generated outputs.

## Limitations

- The three-category taxonomy (transformation, categorization, additive) may not comprehensively capture all possible semantic relationships between inputs and outputs
- Real-world testing shows significant sensitivity to prompt phrasing and context, undermining the stability of function induction
- The paper proposes multiple evaluation approaches but doesn't specify concrete thresholds or implementations for what constitutes "good" function approximation quality

## Confidence

- **High Confidence**: LLMs can approximate functions from natural language specifications; Task types can be categorized based on input-output relationships; The function approximation metaphor provides a useful evaluation framework
- **Medium Confidence**: The three-category taxonomy comprehensively captures task types; Function induction is sufficiently stable for practical evaluation; The proposed evaluation questions meaningfully advance LLM assessment
- **Low Confidence**: Specific performance thresholds for different task types; Exact implementations of proposed evaluation metrics; Scalability of the framework to complex, real-world tasks

## Next Checks

1. **Taxonomy completeness validation**: Empirically test the completeness of the three-category taxonomy by attempting to classify 100+ real-world tasks and analyze which categories remain empty or ambiguous

2. **Function induction stability study**: Conduct a systematic prompt variation study measuring function induction stability by testing 50+ semantically equivalent prompts for 10 different task types and quantifying performance variance

3. **Evaluation framework implementation**: Implement and compare the proposed evaluation metrics against established benchmarks like HELM, measuring correlation between function approximation quality scores and practical utility in real applications