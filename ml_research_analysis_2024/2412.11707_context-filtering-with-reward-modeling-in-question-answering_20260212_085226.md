---
ver: rpa2
title: Context Filtering with Reward Modeling in Question Answering
arxiv_id: '2412.11707'
source_url: https://arxiv.org/abs/2412.11707
tags:
- context
- question
- answer
- training
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a context filtering approach using Reward
  Modeling to improve Question Answering (QA) efficiency by removing irrelevant information
  from contexts. The method leverages Direct Preference Optimization (DPO) to train
  models that distinguish useful from non-essential context, using automatically generated
  "chosen" and "rejected" text pairs.
---

# Context Filtering with Reward Modeling in Question Answering

## Quick Facts
- **arXiv ID**: 2412.11707
- **Source URL**: https://arxiv.org/abs/2412.11707
- **Reference count**: 19
- **Primary result**: 6.8x improvement in Exact Match Per Token (EPT) metric while maintaining QA performance

## Executive Summary
This paper introduces a context filtering approach using Reward Modeling to improve Question Answering (QA) efficiency by removing irrelevant information from contexts. The method leverages Direct Preference Optimization (DPO) to train models that distinguish useful from non-essential context, using automatically generated "chosen" and "rejected" text pairs. Experiments on SQuAD, Natural Questions, and TriviaQA show that the proposed DPO-based summarizer significantly outperforms the baseline, achieving substantial gains in token efficiency for low-resource settings while maintaining strong QA performance.

## Method Summary
The approach uses DPO training with automatically generated preference pairs to learn context filtering. Three prompt strategies (Type 1, 2, 3) create diverse learning signals by varying the combinations of question, answer, and context information. The model learns to maximize reward differences between "chosen" contexts (containing all necessary information) and "rejected" contexts (missing either the answer or question). This creates a reward signal that guides the model to retain only the most relevant information for answering questions, implemented as an abstractive summarization task.

## Key Results
- Achieved 6.8-fold increase in Exact Match Per Token (EPT) metric
- Reduced token length to 20% of original while retaining 92% of initial performance
- Outperformed baseline SFT model across SQuAD, Natural Questions, and TriviaQA datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO training with automatically generated preference pairs can effectively learn to filter out irrelevant context while preserving answer-relevant information.
- Mechanism: The model learns to maximize the reward difference between "chosen" contexts (containing all three elements: question, answer, and context) and "rejected" contexts (missing either the answer or the question). This creates a reward signal that guides the model to retain only the most relevant information for answering the question.
- Core assumption: The automatically generated preference pairs (Type 1 vs Type 2 or Type 3) provide meaningful signal for what constitutes relevant vs. irrelevant context.
- Evidence anchors:
  - [abstract]: "We offer a framework for developing efficient QA models by discerning useful information from dataset pairs, bypassing the need for costly human evaluation."
  - [section 3.3]: "We aim to understand how the lack of information in each of Types 2 and 3 affects the reward model through DPO training compared to Type 1."

### Mechanism 2
- Claim: Context filtering improves token efficiency by removing information that doesn't contribute to answer accuracy.
- Mechanism: By summarizing context to only include essential information, the model reduces token count while maintaining high answer accuracy. The EPT metric quantifies this trade-off between token usage and performance.
- Core assumption: Not all tokens in the original context contribute equally to answer accuracy, and many can be removed without significant performance loss.
- Evidence anchors:
  - [abstract]: "achieving a 6.8-fold increase in the Exact Match Per Token (EPT) metric"
  - [section 4]: "reducing the token length to just 20% of the original retains 92% of the initial performance"

### Mechanism 3
- Claim: The three-prompt strategy (Type 1, 2, 3) creates diverse learning signals that help the model distinguish between different types of context relevance.
- Mechanism: Type 1 provides the gold standard (complete information), Type 2 forces the model to focus on question-relevant context without answer guidance, and Type 3 emphasizes answer presence regardless of context relevance. This multi-faceted training helps the model develop nuanced filtering capabilities.
- Core assumption: Different prompt types capture distinct aspects of what makes context relevant for QA, and combining these signals improves overall filtering performance.
- Evidence anchors:
  - [section 3.1]: "We can establish three strategies for summarization through the missing combinations in the necessary information denoted as the tuple I = ( Q, A, C)"

## Foundational Learning

- Concept: Preference optimization and reward modeling
  - Why needed here: The paper relies on Direct Preference Optimization (DPO) to train the context filtering model without human annotations. Understanding how preference pairs create reward signals is fundamental to the approach.
  - Quick check question: How does DPO differ from traditional reinforcement learning approaches in terms of stability and sample efficiency?

- Concept: Abstractive summarization techniques
  - Why needed here: The context filtering is implemented as a summarization task, requiring understanding of how seq2seq models generate concise representations while preserving key information.
  - Quick check question: What are the key challenges in ensuring that answer spans are preserved during abstractive summarization of QA contexts?

- Concept: Exact Match (EM) and token efficiency metrics
  - Why needed here: The paper introduces EPT as a novel metric and uses standard EM/F1 scores. Understanding these evaluation frameworks is crucial for interpreting results and designing experiments.
  - Quick check question: Why might a model with lower EM score still be considered more efficient according to the EPT metric?

## Architecture Onboarding

- Component map: Data generation pipeline -> SFT summarizer -> DPO summarizer -> Reader model -> Evaluation framework
- Critical path: Data generation → SFT training → DPO training → Reader evaluation
  The reader evaluation is the final arbiter of whether context filtering actually improves QA performance.
- Design tradeoffs:
  - Automatic vs. human preference generation: The paper uses automatic generation to avoid costs, but this may introduce noise compared to human annotations.
  - Abstractive vs. extractive summarization: Abstractive methods can be more compact but risk losing specific answer spans that extractive methods preserve.
  - Prompt design complexity: The three-prompt strategy adds complexity but provides richer learning signals.
- Failure signatures:
  - IRA scores drop significantly while EM remains stable: The model is filtering context but losing answer information.
  - EPT improvement without EM improvement: The model is reducing tokens but not maintaining accuracy proportionally.
  - One DPO variant consistently outperforms others: May indicate that one prompt strategy is more effective than anticipated.
- First 3 experiments:
  1. Baseline comparison: Run SFT and DPO models on a small subset of NQ dataset, measuring EM, F1, and IRA to verify the core filtering behavior.
  2. Token efficiency analysis: Compare token reduction vs. accuracy retention across different context lengths to validate the EPT metric behavior.
  3. Prompt strategy ablation: Train models using only single prompt types (Type 1 vs Type 2 only, Type 1 vs Type 3 only) to understand individual contribution of each strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of the answer in the summarization prompt (Type 3) affect the quality of the generated summary compared to Type 2, and can this be quantified beyond the IRA metric?
- Basis in paper: Explicit - The paper states that Type 3 outputs are more reflective of lexical elements surrounding the answer, and DPO training aims to produce shorter outputs centered around elements present in Type 1 but absent in Type 3 outputs.
- Why unresolved: The paper only uses IRA to evaluate if the answer is included in the summary, not the quality or relevance of the summary itself. The analysis of how the answer inclusion affects the summary's content and its impact on downstream QA performance is limited.
- What evidence would resolve it: A detailed analysis of the generated summaries comparing Type 2 and Type 3 outputs in terms of information relevance, conciseness, and how these factors contribute to QA performance.

### Open Question 2
- Question: How do different strategies for determining the "chosen" and "rejected" text pairs in DPO training impact the final performance of the context filtering model?
- Basis in paper: Inferred - The paper mentions that human labelers or an LLM typically determine yw and yl, but they assume that outputs from the base model with two types of prompts (Type 2, Type 3) can be candidates of the rejected output.
- Why unresolved: The paper does not explore alternative strategies for generating the "chosen" and "rejected" pairs, such as using different models, incorporating additional information, or employing more sophisticated selection criteria.
- What evidence would resolve it: Experiments comparing the performance of the context filtering model when trained with different strategies for generating the preference pairs.

### Open Question 3
- Question: How does the proposed context filtering approach generalize to datasets with different characteristics, such as multi-hop QA or long context QA, and what are the potential limitations in these scenarios?
- Basis in paper: Inferred - The paper mentions that they used datasets modified from existing sources and that adding more diverse datasets, including those covering multi-hop QA and long context QA, would allow for deeper interpretations.
- Why unresolved: The experiments are conducted on extractive QA datasets, and the paper acknowledges the need to explore the approach's performance on other types of datasets.
- What evidence would resolve it: Experiments evaluating the context filtering approach on multi-hop QA and long context QA datasets, analyzing its performance in terms of accuracy, efficiency, and potential limitations.

## Limitations
- The core mechanism relies on automatically generated preference pairs without human validation, which introduces significant uncertainty about the quality and reliability of the learning signal.
- The three-prompt strategy adds complexity that may not be fully justified by the results, and the ablation studies for individual prompt types are limited.
- The approach has only been tested on extractive QA datasets, with limited exploration of generalization to other QA types like multi-hop or long-context scenarios.

## Confidence

**High Confidence**: The token efficiency improvements measured by EPT are clearly demonstrated across multiple datasets. The methodology for calculating EPT and showing relative improvements is straightforward and reproducible.

**Medium Confidence**: The core claim that DPO-based context filtering maintains strong QA performance while reducing tokens is supported by EM/F1 metrics, but the reliance on automatic preference generation without human validation introduces uncertainty about the robustness of the learning signal.

**Low Confidence**: The specific contribution of the three-prompt strategy (Type 1 vs Type 2 vs Type 3) to overall performance is not fully established. The paper suggests multi-faceted benefits but provides limited ablation evidence for individual prompt type contributions.

## Next Checks

1. **Human Preference Validation**: Generate a small set of human-annotated preference pairs (chosen vs rejected contexts) for the same datasets and retrain the DPO model to compare performance against the automatic generation approach. This would directly test whether the automatic preference generation creates reliable learning signals.

2. **Prompt Strategy Ablation**: Conduct a comprehensive ablation study where models are trained using only individual prompt types (Type 1 vs Type 2 only, Type 1 vs Type 3 only) and combinations, measuring both EPT and QA performance. This would quantify the specific contribution of each prompt strategy to the overall improvements.

3. **Robustness Across Question Types**: Analyze model performance across different question types (fact-based, reasoning, multi-hop) to identify whether context filtering degrades more severely for certain question categories. This would reveal whether the automatic filtering reliably preserves answer-relevant information across diverse QA scenarios.