---
ver: rpa2
title: 'Interpreting token compositionality in LLMs: A robustness analysis'
arxiv_id: '2410.12924'
source_url: https://arxiv.org/abs/2410.12924
tags:
- layers
- information
- token
- layer
- compositional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how large language models (LLMs) process
  compositional linguistic structures through a novel perturbation method called Constituent-Aware
  Pooling (CAP). CAP systematically intervenes in model activations by pooling tokens
  into linguistic units at various layers.
---

# Interpreting token compositionality in LLMs: A robustness analysis

## Quick Facts
- arXiv ID: 2410.12924
- Source URL: https://arxiv.org/abs/2410.12924
- Reference count: 40
- Key outcome: LLMs do not build compositional representations incrementally; larger models are more fragile to compositional perturbations due to greater information dispersion across layers

## Executive Summary
This study investigates how large language models process compositional linguistic structures through a novel perturbation method called Constituent-Aware Pooling (CAP). The methodology systematically intervenes in model activations by pooling tokens into linguistic units at various layers. Experiments across three tasks reveal that transformers maximize information gain about next tokens by minimizing mutual information between tokens in the same layer, leading to longer aggregation paths across multiple layers rather than localized compositional representations. Larger models exhibit greater fragility to compositional perturbations than smaller ones, with performance drops most severe in early and middle layers.

## Method Summary
The study introduces Constituent-Aware Pooling (CAP), a perturbation method that intervenes in transformer model activations by pooling tokens into linguistic units (words or phrases) using three aggregation protocols (max, mean, sum). CAP is applied progressively from first to last layers across three tasks: inverse definition modeling, synonym prediction, and hypernym prediction. The methodology tests both original and fine-tuned models across multiple decoder-only transformer architectures (GPT-2 small/medium/large, Gemma-2B, Llama3-8B) using WordNet-derived datasets. Performance is measured by accuracy differences between original and perturbed models.

## Key Results
- LLMs show significant performance drops when compositional structures are abstracted away, with the most severe degradation occurring in early and middle layers
- Larger models (Gemma-2B, Llama3-8B) are more fragile to compositional perturbations than smaller ones, exhibiting greater information dispersion across layers
- Fine-tuning improves recovery from perturbations but does not fully mitigate performance reduction
- The pattern of fragility is consistent across different aggregation functions (max, mean, sum) and is independent of model size, supervision type, or inference task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Larger models are more fragile to compositional perturbations than smaller ones due to greater information dispersion across layers.
- **Mechanism**: As model size increases, information becomes more fragmented across layers, making early-layer interventions more disruptive since these models rely on layer-wise information gain rather than localized compositional representations.
- **Core assumption**: Larger models distribute information in a more uncorrelated manner within their parameters, leading to increased sensitivity to perturbations.
- **Evidence anchors**:
  - [abstract] "Larger models are more fragile to compositional perturbations than smaller ones, exhibiting greater information dispersion."
  - [section] "Larger models are more sensitive to early-layer interventions than smaller models. For instance, while applying word-level CAP, GPT2-large maintains an accuracy close to 90% in both the original and fine-tuned cases for the IDM task, while Llama3-8B shows an early-layer accuracy of around 75% in both cases."
- **Break condition**: If models were trained with objectives that encourage information integration across layers rather than maximizing layer-wise information gain, this fragility might not manifest.

### Mechanism 2
- **Claim**: Transformers maximize information gain about next tokens by minimizing mutual information between tokens in the same layer, leading to longer aggregation paths across multiple layers.
- **Mechanism**: The autoregressive training objective creates an incentive to delay token aggregation to later layers, as each layer is optimized to maximize information gain about the predicted token independently, reducing redundancy between tokens within the same layer.
- **Core assumption**: The training objective of maximizing information gain for each layer independently creates a distributed representation structure that avoids local compositional aggregation.
- **Evidence anchors**:
  - [section] "We argue that this behaviour results from the model's training objective, which seeks to maximise information gain from each layer towards the predicted tokens, at the cost of reducing mutual information between tokens in a single layer."
  - [section] "This can be explained in that optimising Equation 4 can be achieved by retaining at each Rlp(X) only the necessary information to maximise P i,j IG Rlq (Xi),M HA(Rlp (Xj )), where M HA(Rlp(Xj)) is the multi-head attention weighted representation. Such an objective implies minimising the mutual information I(Rlp(Xi), Rlp(Xj)), i.e., reducing redundancy across tokens from the same layer."
- **Break condition**: If the training objective were modified to explicitly encourage token integration within layers, or if architectural constraints forced local compositionality, this mechanism would break down.

### Mechanism 3
- **Claim**: Tokenization strategies that minimize token redundancy (like BPE) compound the effects of information dispersion by creating longer aggregation paths.
- **Mechanism**: Subword tokenization methods aim to minimize vocabulary size by breaking words into frequently occurring character sequences, which reduces mutual information between tokens and forces the model to build compositional representations across multiple layers rather than within single layers.
- **Core assumption**: Tokenization methods like BPE and WordPiece are designed to minimize I(Xi, Xj) to reduce vocabulary size, which aligns with the model's tendency to minimize token redundancy.
- **Evidence anchors**:
  - [section] "The effects of I(Rl(Xi), Rl(Xj)) on LLMs are further compounded by the tokenisation objective (e.g., BPE, WordPiece), which minimises I(Xi, Xj), i.e., token redundancy, as a means of reducing the vocabulary size, leading to longer aggregation paths."
  - [section] "This is independent of model size, supervision type or inference task, but linked to the number of hidden layers."
- **Break condition**: If tokenization methods were designed to preserve word-level or phrase-level information rather than minimize redundancy, or if character-level tokenization were used, this mechanism would be less prominent.

## Foundational Learning

- **Concept**: Information Gain and Mutual Information
  - Why needed here: Understanding how transformers optimize for information gain about next tokens while minimizing redundancy between tokens in the same layer is crucial to interpreting the CAP results and explaining why compositional aggregation is distributed across layers.
  - Quick check question: What is the relationship between information gain about the next token and mutual information between token representations in the same layer?

- **Concept**: Transformer Architecture and Residual Streams
  - Why needed here: The residual stream serves as the shared data pathway where information is propagated across layers, and understanding how CAP interventions affect this pathway is essential for interpreting the performance drops observed in the experiments.
  - Quick check question: How do residual connections in transformers allow for the integration of new outputs with existing representations while preserving original input information?

- **Concept**: Subword Tokenization and BPE
  - Why needed here: Tokenization strategies like BPE affect how information is distributed across tokens and influence the model's ability to form compositional representations, which is central to understanding the observed fragility to compositional perturbations.
  - Quick check question: How does byte-pair encoding (BPE) minimize vocabulary size, and what are the implications for token mutual information?

## Architecture Onboarding

- **Component map**: Attention heads (A) -> Feed-forward layers (P) -> Residual streams (R)
- **Critical path**: The residual stream (R) serves as the primary information highway across layers. Interventions here directly affect how information flows from one layer to the next, making it the most sensitive component to compositional perturbations.
- **Design tradeoffs**: CAP prioritizes understanding compositional robustness over maintaining performance, accepting significant accuracy drops as a feature rather than a bug. The choice between max, mean, and sum aggregation protocols represents a tradeoff between preserving dominant features versus capturing cumulative information loss.
- **Failure signatures**: The most pronounced failure signatures are the sharp accuracy drops in early and middle layers (0-50% layer positions) across all tasks, with larger models showing more severe degradation. Complete breakdown of performance, particularly for phrasal-level CAP in larger models, indicates the model's inability to process abstracted compositional information.
- **First 3 experiments**:
  1. Apply CAP to the residual stream component at 25% layer position using max aggregation on word-level constituents, measure accuracy drop on IDM task across all models.
  2. Apply CAP to attention heads at 75% layer position using mean aggregation on phrase-level constituents, measure accuracy drop on SP task for GPT2-large and Llama3-8B.
  3. Apply CAP to feed-forward layers at 100% layer position using sum aggregation on word-level constituents, measure accuracy drop on HP task for Gemma-2B and GPT2-small.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do transformer models have any localized compositional representations that could be activated through alternative training objectives or architectural modifications?
- Basis in paper: [explicit] The authors explicitly state "we postulate that traditional compositional semantic representations do not exist in any particular stage of a standard Transformer model" and suggest this "points toward the use of specialised architectures and/or training objectives in order to elicit such representations."
- Why unresolved: The paper only demonstrates that standard transformer architectures don't have localized compositional representations, but doesn't test alternative architectures or training objectives that might induce them.
- What evidence would resolve it: Empirical comparison of standard transformers versus transformers with modified architectures (e.g., specialized composition modules) or modified training objectives (e.g., explicitly compositional loss functions) on CAP performance across layers.

### Open Question 2
- Question: What specific architectural or training features cause larger models to be more fragile to compositional perturbations than smaller ones?
- Basis in paper: [explicit] The paper observes "larger models are more fragile to compositional perturbations than smaller ones" and notes they "exhibit greater information dispersion," but doesn't identify the underlying mechanism.
- Why unresolved: While the paper observes this phenomenon, it doesn't systematically investigate what architectural or training characteristics drive this increased fragility in larger models.
- What evidence would resolve it: Controlled experiments varying model size while holding other factors constant, or ablation studies isolating specific architectural components (e.g., attention head count, layer width) to identify which features correlate with perturbation sensitivity.

### Open Question 3
- Question: How does the information-theoretic framework explain the relationship between mutual information reduction across layers and the model's ability to handle compositional perturbations?
- Basis in paper: [explicit] The paper presents an information-theoretic model suggesting "optimising Equation 4 can be achieved by retaining at each Rlp(X) only the necessary information to maximise P i,j IG Rlq (Xi),M HA(Rlp (Xj ))" and that "minimising the mutual information I(Rlp(Xi), Rlp(Xj))... implies that token dependencies will tend to be modelled by aggregation paths spanning multiple layers."
- Why unresolved: The paper proposes this framework but doesn't empirically validate the relationship between measured mutual information across layers and perturbation sensitivity.
- What evidence would resolve it: Empirical measurement of mutual information between token representations at different layers, correlated with CAP performance, to validate whether lower mutual information at early layers predicts higher perturbation sensitivity.

## Limitations

- Generalization scope is limited to decoder-only transformer architectures and three specific tasks (IDM, SP, HP), raising questions about applicability to encoder-decoder models, encoder-only models, or different NLP tasks
- The causal link between CAP interventions and claimed training dynamics remains correlative, with alternative explanations such as specific architectural constraints or tokenization artifacts potentially contributing to results
- The role of subword tokenization in compounding information dispersion is asserted but not experimentally isolated through comparison with different tokenization strategies

## Confidence

**High Confidence**: The empirical observation that CAP interventions cause significant accuracy drops, particularly in early and middle layers, is well-supported by the experimental data. The comparative analysis showing larger models are more fragile to compositional perturbations than smaller ones is robust across multiple model families and tasks.

**Medium Confidence**: The proposed mechanism linking training objectives to information dispersion and compositional fragility is logically coherent but relies on indirect evidence. The connection between tokenization strategies and extended aggregation paths is plausible but not experimentally validated within the study.

**Low Confidence**: The generalizability of findings to non-WordNet semantic relationships, different model architectures (encoders, encoder-decoders), and tasks outside the studied scope remains uncertain without additional validation.

## Next Checks

1. **Tokenization Isolation Test**: Reproduce the CAP experiments using character-level tokenization (e.g., Charformer) versus BPE tokenization to isolate the specific contribution of subword tokenization to information dispersion and aggregation path length.

2. **Architectural Transfer Test**: Apply the CAP methodology to encoder-only models (BERT variants) and encoder-decoder models (T5 variants) to determine whether the observed compositional fragility is specific to decoder-only architectures or represents a more general transformer phenomenon.

3. **Task Diversity Test**: Evaluate CAP performance on tasks with different compositional structures, such as syntactic parsing, semantic role labeling, or multi-hop reasoning, to assess whether the layer-wise fragility pattern holds across diverse compositional challenges beyond WordNet semantic relationships.