---
ver: rpa2
title: 'Multilevel Interpretability Of Artificial Neural Networks: Leveraging Framework
  And Methods From Neuroscience'
arxiv_id: '2408.12664'
source_url: https://arxiv.org/abs/2408.12664
tags:
- neural
- neuroscience
- interpretability
- brain
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper argues that understanding complex neural systems\u2014\
  both biological and artificial\u2014requires analyzing them at multiple levels of\
  \ abstraction, inspired by Marr's three levels: computation, algorithm/representation,\
  \ and implementation. It advocates using tools from neuroscience to guide interpretability\
  \ research in AI, such as neuroethology, psychophysics, Bayesian modeling, and causal\
  \ manipulation."
---

# Multilevel Interpretability Of Artificial Neural Networks: Leveraging Framework And Methods From Neuroscience

## Quick Facts
- **arXiv ID:** 2408.12664
- **Source URL:** https://arxiv.org/abs/2408.12664
- **Reference count:** 40
- **Primary result:** Proposes a multilevel interpretability framework for neural networks based on Marr's three levels (computation, algorithm, implementation) that leverages methods from neuroscience to better understand and control AI systems.

## Executive Summary
This paper presents a framework for interpreting complex neural systems by adapting Marr's three-level analysis (computation, algorithm/representation, and implementation) from neuroscience to artificial neural networks. The authors argue that understanding AI systems requires examining them at all three levels rather than focusing solely on implementation-level circuit analysis. By organizing interpretability tools according to this framework and drawing on decades of neuroscience research, the paper provides a principled approach to linking structure, computation, and behavior in neural networks, with important implications for safety, fairness, and trust in AI systems.

## Method Summary
The paper surveys analytical tools from neuroscience and cognitive science, organizing them according to Marr's three levels of analysis. At the computational level, it highlights methods like neuroethology and psychophysics for characterizing behavior and identifying computational goals. For the algorithmic/representational level, it discusses reverse engineering, decoding/encoding models, and neural population geometry. At the implementation level, it covers function localization, neural selectivity measurement, and causal manipulation techniques. The framework emphasizes that methods from each level provide non-redundant insights and that consistency across levels is essential for holistic understanding.

## Key Results
- Marr's three-level framework provides a principled way to organize interpretability research and tackle neural system complexity
- Neuroscience methods can be adapted to yield insights at each level of analysis in artificial neural networks
- Behavioral characterization through ethology and psychophysics can identify computational goals beyond training objectives
- The framework helps clarify assumptions and research priorities while linking structure, computation, and behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Marr's three-level framework improves interpretability by forcing researchers to explicitly address computational, algorithmic, and implementation questions rather than defaulting to implementation-level analysis.
- Mechanism: By requiring explicit statement of which level(s) a study addresses, researchers avoid conflating behavioral observations with circuit explanations and ensure that structural findings are grounded in functional understanding.
- Core assumption: Different levels provide non-redundant understanding of neural systems and that missing any level leads to incomplete or misleading interpretations.

### Mechanism 2
- Claim: Methods from neuroscience can be adapted to provide specific tools for each level of analysis in artificial neural networks.
- Mechanism: Neuroscience has developed specialized methods for studying neural selectivity, population geometry, and causal manipulation that can be directly applied to ANN interpretability research.
- Core assumption: The fundamental computational problems solved by biological and artificial neural networks share enough similarity that methods developed for one can inform the other.

### Mechanism 3
- Claim: Behavioral characterization through ethology and psychophysics can identify the computational goals of artificial neural networks beyond their training objectives.
- Mechanism: By studying how ANNs behave under naturalistic conditions and in extreme test cases, researchers can identify emergent capabilities and computational strategies not explicit in the training objective.
- Core assumption: ANNs, like biological systems, exhibit behaviors that reveal their computational goals through systematic observation and controlled experimentation.

## Foundational Learning

- Concept: Marr's three levels of analysis (computation, algorithm/representation, implementation)
  - Why needed here: This framework provides the organizing principle for the entire paper and helps researchers understand what type of questions to ask at each level.
  - Quick check question: What are the three levels of analysis and what type of question does each level address?

- Concept: Polysemanticity vs. mixed selectivity
  - Why needed here: Understanding this parallel concept helps bridge neuroscience and interpretability research on neuron specialization.
  - Quick check question: How do polysemanticity in ANNs and mixed selectivity in biological neurons relate to each other?

- Concept: Causal manipulation techniques
  - Why needed here: These methods are essential for establishing necessity and sufficiency relationships between network components and behaviors.
  - Quick check question: What are the main techniques for performing causal manipulation in ANNs and how do they differ from those used in neuroscience?

## Architecture Onboarding

- Component map: The paper presents a framework for understanding neural systems that maps neuroscience methods to Marr's three levels. The core components are: (1) computational level methods like neuroethology and psychophysics, (2) algorithmic/representational level methods like reverse engineering and population geometry, and (3) implementation level methods like functional localization and causal manipulation.

- Critical path: To apply this framework effectively, researchers should: (1) explicitly identify which level(s) their study addresses, (2) select appropriate methods from neuroscience for that level, (3) ensure consistency across levels through mutual constraints, and (4) iterate between levels as needed to refine understanding.

- Design tradeoffs: The framework trades specificity for breadth - it provides a comprehensive organizing structure but doesn't prescribe specific techniques for particular problems. Researchers must still make judgment calls about which methods to apply and how to interpret results across levels.

- Failure signatures: Common failures include: (1) focusing exclusively on implementation-level circuit analysis without grounding in computational goals, (2) assuming that decoding accuracy implies necessity or sufficiency without causal manipulation, (3) conflating behavioral observations with algorithmic explanations.

- First 3 experiments:
  1. Select a specific behavior in an ANN (e.g., in-context learning) and apply neuroethology-like analysis to characterize its conditions and variations
  2. Use psychophysics-style experiments to test the behavior under extreme or unusual conditions to reveal underlying computational strategies
  3. Apply representation engineering techniques to track latent neural trajectories during the behavior and identify algorithmic-level patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can interpretability researchers effectively bridge the computational level understanding of AI behaviors with implementation-level circuit analysis, particularly for complex emergent capabilities like in-context learning or planning?
- Basis in paper: The paper emphasizes that "a proper understanding of AI systems has practical implications" and that "understanding in this field is perhaps best measured as an enhanced ability to fix existing systems or train better ones," yet notes current research often focuses heavily on implementation without sufficient computational-level foundation.
- Why unresolved: Current interpretability research heavily focuses on implementation-level analysis (neurons, attention heads, circuits) while the computational level (defining what the system is actually doing and why) remains relatively underexplored. The paper argues this gap limits our ability to predict and control AI behaviors effectively.
- What evidence would resolve it: Empirical demonstrations showing that computational-level analysis (e.g., behavioral characterization, Bayesian modeling) of AI capabilities leads to more effective and generalizable implementation-level circuit discoveries, or case studies where computational-level insights directly enabled better model editing or training approaches.

### Open Question 2
- Question: What is the appropriate level of abstraction for studying neural representations in AI systems - should researchers focus on individual neurons/attention heads, populations with sparse autoencoders, or higher-level representational geometry?
- Basis in paper: The paper discusses the tension between "grandmother neurons" versus "mixed selectivity/polysemanticity" and notes that "from the view of the interpretability community, having such polysemantic neurons makes interpretability and localization more challenging," while also highlighting neuroscience's shift from single neurons to population-level analysis.
- Why unresolved: Current interpretability research shows mixed approaches - some researchers catalog individual neuron selectivity while others use sparse autoencoders to find interpretable features, with debates about which approach better captures AI system representations and enables meaningful interventions.
- What evidence would resolve it: Comparative studies showing which level of abstraction (individual units vs. populations vs. geometric analysis) most effectively predicts model behavior, enables successful interventions, and generalizes across different architectures and tasks.

### Open Question 3
- Question: How can the concept of modularity in neural systems be effectively applied to understand and improve artificial neural networks, given the fundamental differences between biological and artificial systems?
- Basis in paper: The paper discusses modularity research in both neuroscience and interpretability, noting that "there are both theoretical and empirical motivations for researchers to investigate modularity" and describing different approaches to instantiating modularity in ANNs, while acknowledging "biological and artificial systems are fundamentally different."
- Why unresolved: While modularity is well-studied in biological systems and has theoretical benefits, its application to artificial neural networks remains unclear due to differences in learning rules, architecture, and the ability to directly manipulate artificial systems versus biological constraints.
- What evidence would resolve it: Systematic studies comparing modular versus non-modular ANN architectures on various tasks, demonstrating whether modularity improves interpretability, generalization, or editability, and identifying which approaches to modularity transfer effectively between biological and artificial systems.

## Limitations

- The framework's applicability depends on the assumption that biological and artificial neural systems share sufficient computational commonalities to justify direct method transfer
- The paper provides theoretical arguments but limited empirical validation across diverse AI systems
- The effectiveness of behavioral characterization in revealing computational goals of ANNs remains untested for complex, emergent behaviors

## Confidence

- **High confidence** in the conceptual validity of using Marr's three-level framework to organize interpretability research
- **Medium confidence** in the specific neuroscience-to-ML method transfers proposed
- **Low confidence** in the framework's ability to reveal meaningful computational goals for highly trained, black-box models without extensive behavioral validation

## Next Checks

1. Apply the framework to a specific ANN behavior (e.g., in-context learning) and document whether each level reveals non-redundant insights
2. Conduct a systematic comparison of causal manipulation results between biological neurons and ANN units to test the transfer validity
3. Design psychophysical-style experiments to test whether behavioral characterization can distinguish between different computational strategies in functionally equivalent models