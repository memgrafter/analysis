---
ver: rpa2
title: Understanding the performance gap between online and offline alignment algorithms
arxiv_id: '2405.08448'
source_url: https://arxiv.org/abs/2405.08448
tags:
- offline
- online
- policy
- performance
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance gap between online and
  offline reinforcement learning from human feedback (RLHF) algorithms for language
  model alignment. The authors conduct a series of carefully designed experiments
  to understand why online methods consistently outperform offline methods, despite
  both being trained on the same preference data.
---

# Understanding the performance gap between online and offline alignment algorithms

## Quick Facts
- arXiv ID: 2405.08448
- Source URL: https://arxiv.org/abs/2405.08448
- Reference count: 34
- Primary result: Online RLHF algorithms consistently outperform offline algorithms despite equal data coverage, with the gap attributed to on-policy sampling benefits

## Executive Summary
This paper investigates why online reinforcement learning from human feedback (RLHF) algorithms consistently outperform offline methods for language model alignment, even when both are trained on identical preference data. Through carefully designed experiments across four datasets, the authors demonstrate that the performance gap stems from fundamental differences in how online and offline algorithms interact with data distributions. The study reveals that while offline algorithms excel at pairwise classification tasks, they struggle with generative quality - a capability where online algorithms shine. These findings challenge the assumption that data coverage alone determines alignment performance and suggest that on-policy sampling plays a pivotal role in effective AI alignment.

## Method Summary
The authors conduct controlled experiments comparing online and offline RLHF algorithms using the same preference data across four open-source datasets. They implement IPO loss for both variants and systematically vary dataset composition, model sizes, and loss functions to isolate the sources of performance differences. The study uses KL divergence from a reference SFT policy as a unified measure of policy drift, allowing fair comparison between methods. Experiments range from basic online-offline comparisons to sophisticated analyses involving shuffled datasets and scaled-up model sizes, with performance measured through win rates against golden policies.

## Key Results
1. Online algorithms achieve superior KL divergence vs. performance trade-offs compared to offline algorithms across all tested datasets
2. Data coverage alone cannot explain the performance gap - even identical datasets yield persistent differences
3. Offline algorithms optimize for pairwise classification accuracy but produce inferior generations compared to online methods
4. The performance discrepancy persists across different loss functions and model scales, pointing to sampling process as the fundamental issue

## Why This Works (Mechanism)

### Mechanism 1
- Claim: On-policy sampling maintains distributional proximity between the sampling distribution and the evolving policy, enabling direct optimization of generative quality.
- Mechanism: Online algorithms constantly update the sampling distribution to match the current policy, so the policy receives immediate feedback on its own generations. This creates a tight coupling between sampling and policy improvement, allowing the policy to shift probability mass toward high-quality responses directly.
- Core assumption: The quality signal (preference model) is sufficiently accurate to guide policy updates when applied to the policy's own samples.
- Evidence anchors:
  - [abstract] "we observe that the performance discrepancy persists for both contrastive and non-contrastive loss functions, and appears not to be addressed by simply scaling up policy networks."
  - [section] "the online algorithm creates a stream of data(洧논洧노, 洧녽 (洧녻)洧노 , 洧녽 (洧녳)洧노 )洧노 over time, with each(洧논洧노, 洧녽 (洧녻)洧노 , 洧녽 (洧녳)洧노 ) being a batch of洧냣 = 32 prompts and responses labeled by the proxy preference model."
- Break condition: If the preference model becomes too inaccurate or overfits to the initial dataset, on-policy sampling may amplify errors rather than correct them.

### Mechanism 2
- Claim: Offline algorithms train policies to optimize classification accuracy on a static dataset rather than generative quality, leading to a discriminative-generative capability gap.
- Mechanism: Because offline algorithms fix the dataset, the policy learns to classify responses from that dataset well. However, this does not translate to generating better responses, as the policy never practices generation under its own distribution. The optimization focuses on discriminating between fixed examples rather than improving the generative process itself.
- Core assumption: The loss function's contrastive nature creates a trade-off where improving classification accuracy does not necessarily improve generation.
- Evidence anchors:
  - [abstract] "while offline algorithms train policy to become good at pairwise classification, it is worse at generations; in the meantime the policies trained by online algorithms are good at generations while worse at pairwise classification."
  - [section] "focusing just on the offline experiments, we find little statistical correlation between the classification accuracy and model performance."
- Break condition: If the dataset is continuously updated to match the policy's own distribution, the gap between classification and generation may shrink.

### Mechanism 3
- Claim: Scaling up policy networks does not resolve the online-offline performance gap because the fundamental issue is the sampling process, not model capacity.
- Mechanism: Larger models may improve absolute performance but do not inherently solve the distributional mismatch problem. The gap persists because offline algorithms still lack on-policy sampling, regardless of how expressive the policy becomes.
- Core assumption: The bottleneck is the sampling process rather than the model's representational capacity.
- Evidence anchors:
  - [abstract] "we observe that the performance discrepancy persists for both contrastive and non-contrastive loss functions, and appears not to be addressed by simply scaling up policy networks."
  - [section] "The persistence of the performance discrepancy implies that the sampling issue is likely not to be addressed by simply scaling up models."
- Break condition: If the policy becomes so large that it can perfectly memorize and classify the offline dataset, the gap might narrow, but this is unlikely in practice.

## Foundational Learning

- Concept: KL divergence as optimization budget
  - Why needed here: The paper uses KL divergence between the RLHF policy and the reference SFT policy as a unified measure of how much the policy has drifted, allowing fair comparison between online and offline methods.
  - Quick check question: If two policies have the same KL divergence from SFT, does that mean they have the same performance? (Answer: No, as shown by the performance gap despite equal KL.)

- Concept: Contrastive vs non-contrastive loss functions
  - Why needed here: Understanding the difference is crucial because the paper tests whether the performance gap is due to the loss function being contrastive or due to the sampling being offline.
  - Quick check question: What is the key difference between IPO (contrastive) and Bo2 (non-contrastive) losses? (Answer: IPO increases the log ratio of winning to losing responses, while Bo2 only maximizes the log probability of winning responses.)

- Concept: On-policy vs off-policy sampling
  - Why needed here: The core distinction between online and offline algorithms, where on-policy sampling means the policy generates its own data, while off-policy sampling uses a fixed dataset.
  - Quick check question: Why does on-policy sampling potentially provide better alignment performance? (Answer: Because the policy receives feedback on its own outputs, allowing direct optimization of generative quality.)

## Architecture Onboarding

- Component map:
  SFT policy (reference) -> Preference model -> Policy network (online/offline) -> Dataset (fixed/updated)

- Critical path:
  1. Start with SFT policy
  2. Train preference model on initial dataset
  3. For online: generate responses, score with preference model, update policy
  4. For offline: directly optimize policy on fixed dataset using preference labels
  5. Measure KL divergence and performance (win rate against golden policy)

- Design tradeoffs:
  - Online: Higher computational cost (generating data, training reward model) but better performance
  - Offline: Cheaper and simpler but suffers from distributional mismatch and inferior generative quality
  - Scaling: Increases capacity but does not resolve the fundamental sampling issue

- Failure signatures:
  - Online: Over-optimization where policy performance degrades despite low KL
  - Offline: High classification accuracy but low generative performance; inability to improve beyond the dataset policy
  - Both: Performance plateaus or degrades when KL divergence becomes too large

- First 3 experiments:
  1. Run online and offline IPO on the same dataset, measure KL vs performance trade-off
  2. Create Donline-shuffled dataset and run offline algorithm on it, compare to original offline
  3. Test different offline datasets (Dsft vs. 800, D800 vs. 4k) to see which properties improve performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance gap between online and offline algorithms persist when using a ground truth reward signal instead of a learned preference model?
- Basis in paper: [inferred] The authors attribute the performance gap partly to over-optimization under Goodhart's law, where both algorithms optimize a proxy objective that deviates from the true objective. This suggests that if the true reward were available, the gap might be smaller.
- Why unresolved: The paper focuses on RLHF where a learned preference model is the proxy reward. They do not experiment with a ground truth reward signal to isolate the effect of the proxy reward optimization from the online/offline sampling difference.
- What evidence would resolve it: Experiments comparing online and offline algorithms when both have access to the ground truth reward signal would show whether the performance gap is primarily due to the proxy reward optimization or the sampling process.

### Open Question 2
- Question: How does the performance gap between online and offline algorithms scale with model size, and is there a point where the gap becomes negligible?
- Basis in paper: [explicit] The authors scale up policy networks from Large (770M) to XL (3B) to XXL (11B) parameters and observe that the performance gap persists, though it decreases at a slower rate than the improvement from scaling alone.
- Why unresolved: The authors only scale up to 11B parameters and do not explore whether the gap eventually closes or plateaus at some point. The rate of gap reduction relative to scaling is also unclear.
- What evidence would resolve it: Experiments scaling up to much larger model sizes (e.g., 100B+ parameters) would show whether the gap eventually becomes negligible or continues to persist. Measuring the gap reduction rate per order of magnitude increase in model size would also be informative.

### Open Question 3
- Question: Can offline algorithms achieve competitive performance by using an on-policy data generation process, and what is the minimal frequency of on-policy data updates required?
- Basis in paper: [explicit] The authors find that making the offline dataset more on-policy by including responses generated by high-performing policies improves offline performance. This suggests that offline algorithms can benefit from on-policy data.
- Why unresolved: The paper does not explore the frequency or extent of on-policy data updates required to close the performance gap. It also does not compare this approach to fully online algorithms.
- What evidence would resolve it: Experiments varying the frequency of on-policy data updates for offline algorithms (e.g., updating the dataset every 100, 1000, 10000 steps) and comparing performance to fully online algorithms would show the trade-off between data update frequency and performance. This would also reveal the minimal update frequency needed to achieve competitive performance.

## Limitations

- The study focuses specifically on pairwise preference learning, which may not generalize to other alignment objectives or reward structures
- Experiments are conducted within a specific framework using TLT models, limiting applicability to other architectures
- The fundamental question of whether hybrid approaches combining offline and online elements could achieve optimal performance remains unexplored

## Confidence

- **High Confidence**: The fundamental finding that online methods outperform offline methods despite equal data coverage, supported by consistent experimental results across four distinct datasets
- **Medium Confidence**: The mechanism explanations (on-policy sampling benefits and discriminative-generative capability gap) are well-reasoned but not definitively proven, as alternative explanations could exist
- **Medium Confidence**: The claim that scaling model size does not resolve the performance gap is supported by evidence but could potentially be overcome with much larger models than tested

## Next Checks

1. Test whether the performance gap persists when using different reward structures beyond pairwise preferences (e.g., scalar rewards, multi-objective alignment)
2. Investigate whether hybrid approaches that combine offline pre-training with online fine-tuning can achieve the benefits of both methods while mitigating their respective weaknesses
3. Examine whether the gap can be reduced by carefully curating offline datasets that better approximate the policy's own distribution through dataset augmentation strategies