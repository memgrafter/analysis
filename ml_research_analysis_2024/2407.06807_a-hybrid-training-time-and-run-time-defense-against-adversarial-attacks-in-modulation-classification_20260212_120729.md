---
ver: rpa2
title: A Hybrid Training-time and Run-time Defense Against Adversarial Attacks in
  Modulation Classification
arxiv_id: '2407.06807'
source_url: https://arxiv.org/abs/2407.06807
tags:
- adversarial
- training
- defense
- examples
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the vulnerability of deep learning-based modulation
  classification systems to adversarial attacks, which can cause misclassification
  by introducing imperceptible perturbations to input signals. To counter this threat,
  the authors propose a hybrid defense combining customized adversarial training (CAT)
  at training time and a support vector machine-based neural rejection (NR) system
  at runtime.
---

# A Hybrid Training-time and Run-time Defense Against Adversarial Attacks in Modulation Classification

## Quick Facts
- arXiv ID: 2407.06807
- Source URL: https://arxiv.org/abs/2407.06807
- Authors: Lu Zhang; Sangarapillai Lambotharan; Gan Zheng; Guisheng Liao; Ambra Demontis; Fabio Roli
- Reference count: 15
- Primary result: HTRD achieves up to 30% higher accuracy against white-box adversarial attacks at high perturbation levels without sacrificing normal classification accuracy.

## Executive Summary
This paper addresses the vulnerability of deep learning-based modulation classification systems to adversarial attacks by proposing a hybrid defense combining customized adversarial training (CAT) at training time and a support vector machine-based neural rejection (NR) system at runtime. The CAT approach adapts perturbation levels and applies label smoothing to improve model robustness, while the NR system detects low-confidence inputs likely to be adversarial. Experiments on the RML2016.10a dataset show that the proposed hybrid training-time and run-time defense (HTRD) outperforms existing methods, achieving up to 30% higher accuracy against white-box adversarial attacks at high perturbation levels, without sacrificing normal classification accuracy.

## Method Summary
The HTRD scheme combines CAT-based DNN with an SVM-based NR system. The CAT training uses PGD attacks with l2-norm constraints, adaptive perturbation allocation per sample, and label smoothing. The trained DNN extracts features from the last layer, which are used to train an RBF-SVM (γ=0.01) for the NR system. During inference, samples with low SVM decision scores are rejected as adversarial. The system is evaluated on the RML2016.10a dataset with 11 modulation types across 20 SNR levels.

## Key Results
- HTRD achieves up to 30% higher accuracy against white-box adversarial attacks at high perturbation levels compared to existing methods
- The defense maintains normal classification accuracy while significantly improving robustness
- Forces attackers to use more transmission power, hindering stealthy attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Customized Adversarial Training (CAT) increases the margin in the input space by allocating smaller perturbations to samples near decision boundaries.
- Mechanism: CAT computes a sample-specific perturbation bound $\epsilon_i$ such that the adversarial example is misclassified, then applies adaptive label smoothing that increases uncertainty for samples with larger perturbations. This prevents the decision boundary from being distorted by overly aggressive perturbations.
- Core assumption: Samples near the decision boundary are more sensitive to perturbations and require smaller $\epsilon_i$ to avoid boundary distortion.
- Evidence anchors:
  - [abstract] "The training-time defense consists of adversarial training and label smoothing, while the run-time defense employs a support vector machine-based neural rejection (NR)."
  - [section] "First, identical and large $\epsilon$ for all data samples is avoided. Instead, for samples that are originally closer to the decision boundary, a smaller $\epsilon$ is used."
- Break condition: If the margin is already large enough or the perturbation allocation does not reflect true boundary proximity, the benefit of CAT diminishes.

### Mechanism 2
- Claim: The Neural Rejection (NR) system detects adversarial examples by identifying low-confidence samples in the last feature layer.
- Mechanism: The last feature layer of the DNN is used to train an SVM classifier. Samples with low SVM decision scores are classified as adversarial and rejected. This exploits the amplification of adversarial perturbations in deeper layers.
- Core assumption: Adversarial examples produce low-confidence scores in the last feature layer, creating a detectable rejection region.
- Evidence anchors:
  - [abstract] "The run-time defense employs a support vector machine-based neural rejection (NR)."
  - [section] "The rationale behind the NR system is that there is an amplification effect for the difference between adversarial attacks and benign samples during the propagation of adversarial perturbations through layers of DNN."
- Break condition: If adversarial perturbations are small enough to not significantly affect confidence scores, the NR system's detection capability weakens.

### Mechanism 3
- Claim: Combining CAT with NR forces attackers to use more transmission power, hindering stealth operations.
- Mechanism: CAT increases the input space margin, which enlarges the rejection region for the NR system. This forces adversaries to apply larger perturbations to evade detection, increasing transmission power and reducing stealth.
- Core assumption: A larger rejection region requires adversaries to use more perturbation power to evade detection, making attacks less stealthy.
- Evidence anchors:
  - [abstract] "This forces attackers to use more transmission power, hindering stealthy attacks."
  - [section] "A larger rejection region will force the adversary to use more transmission power to attack the modulation classification scheme, which will hinder stealth operation of the adversarial transmitter."
- Break condition: If the cost of increased transmission power is acceptable to the adversary, the deterrence effect is reduced.

## Foundational Learning

- Concept: Adversarial examples and their generation using Projected Gradient Descent (PGD).
  - Why needed here: Understanding how adversarial examples are crafted is crucial for designing effective defenses.
  - Quick check question: What is the primary goal of an adversarial attack in the context of modulation classification?

- Concept: Label smoothing and its role in mitigating the effects of adversarial training.
  - Why needed here: Label smoothing helps prevent the decision boundary from being distorted by overly aggressive perturbations during adversarial training.
  - Quick check question: How does label smoothing affect the confidence of the classifier's predictions?

- Concept: Support Vector Machines (SVMs) and their use in detecting low-confidence samples.
  - Why needed here: SVMs are used in the NR system to classify samples with low decision scores as adversarial.
  - Quick check question: What is the primary advantage of using SVMs for detecting low-confidence samples in the last feature layer?

## Architecture Onboarding

- Component map: Input signal → DNN (CAT-trained) → Last feature layer → SVM (NR) → Classification or rejection
- Critical path: Input signal → DNN (CAT-trained) → Last feature layer → SVM (NR) → Classification or rejection
- Design tradeoffs: The tradeoff between robustness and normal accuracy is managed by the CAT's adaptive perturbation allocation and label smoothing.
- Failure signatures: A failure occurs when the SVM incorrectly classifies a benign sample as adversarial (false positive) or fails to detect an adversarial sample (false negative).
- First 3 experiments:
  1. Evaluate the normal accuracy of the CAT-trained DNN without the NR system.
  2. Measure the rejection rate of the NR system on benign samples to ensure it meets the 10% threshold.
  3. Test the combined HTRD system against white-box adversarial attacks at various perturbation levels to assess robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed HTRD scheme perform against adaptive adversarial attacks where the attacker can dynamically adjust the perturbation level based on the defense's rejection threshold?
- Basis in paper: [explicit] The paper mentions using adaptive (white-box) attacks for evaluation, but does not explore scenarios where the attacker can dynamically adjust perturbation levels in response to the defense's rejection threshold.
- Why unresolved: The paper focuses on fixed perturbation levels and does not investigate the interaction between adaptive attacks and the HTRD's rejection mechanism.
- What evidence would resolve it: Experiments showing the performance of HTRD against adaptive attacks with varying rejection thresholds and perturbation levels.

### Open Question 2
- Question: What is the impact of different kernel functions and parameters on the performance of the SVM-based neural rejection system in the HTRD scheme?
- Basis in paper: [inferred] The paper uses a radial basis function (RBF) kernel with a fixed parameter (γ = 0.01) for the SVM classifier, but does not explore the impact of other kernel functions or parameter tuning on the system's performance.
- Why unresolved: The choice of kernel function and its parameters can significantly affect the SVM's ability to distinguish between adversarial and benign samples, which in turn impacts the overall performance of the HTRD scheme.
- What evidence would resolve it: Comparative experiments using different kernel functions and parameter settings to evaluate their impact on the HTRD's accuracy and rejection rate.

### Open Question 3
- Question: How does the proposed HTRD scheme scale to larger datasets with more modulation types and higher-dimensional input features?
- Basis in paper: [explicit] The paper evaluates the HTRD scheme using the RML2016.10a dataset with 11 modulation types, but does not explore its performance on larger datasets or with higher-dimensional input features.
- Why unresolved: The scalability of the HTRD scheme to larger and more complex datasets is crucial for its practical applicability in real-world scenarios.
- What evidence would resolve it: Experiments demonstrating the HTRD's performance on larger datasets with more modulation types and higher-dimensional input features, including analysis of computational complexity and training time.

## Limitations

- Sparse technical details: Core algorithmic parameters (exact PGD iteration counts, margin estimation procedure, SVM threshold calibration) are not disclosed.
- Lack of ablation studies: No experiments isolating the individual contributions of CAT vs NR components to overall robustness.
- Unverified claims: The transmission power deterrence argument is stated but not experimentally verified.

## Confidence

- CAT margin maximization claim: Low confidence without explicit mathematical justification or empirical margin visualization.
- NR detection mechanism: Medium confidence based on the stated rationale about perturbation amplification but no quantitative evidence linking confidence scores to attack strength.
- Overall superiority claim: Medium confidence given clear experimental comparisons but lacks ablation studies isolating CAT vs NR contributions.

## Next Checks

1. Replicate the CAT training procedure with ablation: train identical models with uniform epsilon, sample-based epsilon, and no label smoothing to isolate their individual contributions to robustness.
2. Measure the rejection region size across SNR levels and correlate it with required adversarial perturbation magnitude to quantify the claimed transmission power tradeoff.
3. Test NR system sensitivity by varying the SVM threshold S0 and measuring false rejection rate vs. adversarial detection rate to determine optimal operating point.