---
ver: rpa2
title: "A Simple and Adaptive Learning Rate for FTRL in Online Learning with Minimax\
  \ Regret of $\u0398(T^{2/3})$ and its Application to Best-of-Both-Worlds"
arxiv_id: '2405.20028'
source_url: https://arxiv.org/abs/2405.20028
tags:
- learning
- regret
- inequality
- bandits
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a novel adaptive learning rate framework,\
  \ Stability-Penalty-Bias (SPB) matching, for Follow-the-Regularized-Leader (FTRL)\
  \ in online learning problems with a minimax regret of \u0398(T^2/3). The framework\
  \ is designed by matching the stability, penalty, and bias terms that appear in\
  \ regret bounds, addressing a gap in existing research focused primarily on problems\
  \ with \u0398(\u221AT) minimax regret."
---

# A Simple and Adaptive Learning Rate for FTRL in Online Learning with Minimax Regret of $Θ(T^{2/3})$ and its Application to Best-of-Both-Worlds

## Quick Facts
- arXiv ID: 2405.20028
- Source URL: https://arxiv.org/abs/2405.20028
- Reference count: 40
- Achieves optimal Θ(T^(2/3)) regret for hard online learning problems using SPB-matching framework

## Executive Summary
This paper introduces the Stability-Penalty-Bias (SPB) matching framework for Follow-the-Regularized-Leader (FTRL) in online learning problems with minimax regret of Θ(T^(2/3)). The key innovation is a learning rate design that simultaneously matches stability, penalty, and bias terms in regret bounds, addressing a gap in existing research focused on problems with Θ(√T) minimax regret. The framework is applied to partial monitoring with global observability and graph bandits with weak observability, achieving Best-of-Both-Worlds (BOBW) guarantees with surprisingly simple learning rates compared to existing approaches.

## Method Summary
The method employs FTRL with a Tsallis entropy regularizer and a novel adaptive learning rate designed through SPB-matching. The learning rate β_t is updated to balance three competing terms: stability (2√z_t/β_t), penalty ((β_t - β_t-1)h_t), and bias (u_t/β_t). Forced exploration with rate γ_t = √z_t/β_t + u_t/β_t ensures sufficient information gathering while controlling regret. The approach achieves simultaneous optimality in stochastic (O(log T)) and adversarial (O(T^(2/3))) regimes for hard online learning problems.

## Key Results
- New BOBW regret bounds for partial monitoring: O(√(k log k · T)) in adversarial regime, O(log T) in stochastic regime
- New BOBW regret bounds for graph bandits: O(δ^(1/3) · T^(2/3)) in adversarial regime, O(log T) in stochastic regime
- Improved dependencies on problem parameters k (number of actions) and δ (weak domination number)
- Surprisingly simple learning rate compared to existing BOBW algorithms for hard problems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SPB-matching learning rate achieves optimal Θ(T^(2/3)) regret for hard online learning problems by simultaneously matching stability, penalty, and bias terms in regret bounds.
- **Mechanism**: The learning rate β_t is updated to match 2√z_t/β_t + u_t/β_t = (β_t - β_t-1)h_t, where z_t is the stability component, h_t is the penalty component, and u_t represents forced exploration bias. This balancing ensures neither term dominates, achieving optimal worst-case regret.
- **Core assumption**: The stability, penalty, and bias terms can be accurately estimated and matched through the learning rate update rule.
- **Evidence anchors**: [abstract] "Our learning rate is designed by matching the stability, penalty, and bias terms that naturally appear in regret upper bounds for problems with a minimax regret of Θ(T^(2/3))." [section 3.2] "We consider determining (β_t)t by matching the stability–bias terms and the penalty term as 2√z_t/β_t + u_t/β_t = (β_t - β_t-1)h_t."
- **Break condition**: If any of the three components (stability, penalty, bias) cannot be accurately estimated or grow disproportionately, the matching fails and regret bounds degrade.

### Mechanism 2
- **Claim**: Using Tsallis entropy regularization with exponent α = 1 - 1/(log k) achieves simultaneous optimality in stochastic and adversarial regimes for hard problems.
- **Mechanism**: The Tsallis entropy regularizer with exponent α = 1 - 1/(log k) provides the right balance between regularization strength and adaptability. This specific exponent choice minimizes the dependence on the number of actions k while maintaining optimal regret bounds.
- **Core assumption**: The optimal exponent for Tsallis entropy depends on the number of actions k and can be determined as α = 1 - 1/(log k).
- **Evidence anchors**: [abstract] "The resulting learning rate is surprisingly simple compared to the existing learning rates for BOBW algorithms for problems with a minimax regret of Θ(T^(2/3))." [section 4] "For q_t and some p_0 ∈ P_k, we use the action selection probability p_t ∈ P_k defined by p_t = (1 - γ_t)q_t + γ_t p_0 for γ_t = γ'_t + u_t/β_t = √z_t/β_t + u_t/β_t"
- **Break condition**: If the problem structure changes significantly (e.g., different feedback graph properties), the optimal exponent may need adjustment.

### Mechanism 3
- **Claim**: Forced exploration rate γ_t = √z_t/β_t + u_t/β_t balances the need for information gathering with regret minimization.
- **Mechanism**: The forced exploration rate combines a stability-matching term (√z_t/β_t) with a lower bound term (u_t/β_t) to ensure sufficient exploration while controlling regret. This prevents the algorithm from exploiting prematurely while maintaining optimal regret bounds.
- **Core assumption**: The forced exploration rate must be sufficiently large to ensure accurate loss estimation but not so large that it dominates the regret.
- **Evidence anchors**: [section 3.1] "Therefore, we consider exploration rate of γ_t = γ'_t + u_t/β_t for γ'_t = √z_t/β_t and some u_t > 0, where γ'_t is chosen so that the stability and bias terms are matched." [section 3] "In hard problems, it is common to combine FTRL with forced exploration [17, 4, 34, 51]."
- **Break condition**: If the forced exploration rate is too small, loss estimation becomes inaccurate; if too large, regret becomes dominated by the exploration bias term.

## Foundational Learning

- **Concept**: Follow-the-Regularized-Leader (FTRL) framework
  - Why needed here: FTRL provides the optimization structure that allows adaptive learning rates and regularization to be incorporated naturally into online learning algorithms.
  - Quick check question: Can you explain how FTRL differs from Online Gradient Descent and Hedge algorithm?

- **Concept**: Bregman divergence and its role in online learning
  - Why needed here: Bregman divergence is used to measure the distance between probability distributions in the FTRL update and is crucial for analyzing the stability term in regret bounds.
  - Quick check question: What is the relationship between Bregman divergence and the entropy regularizers used in this paper?

- **Concept**: Best-of-Both-Worlds (BOBW) algorithms
  - Why needed here: The paper aims to achieve optimal regret bounds simultaneously in stochastic (O(log T)) and adversarial (O(T^(2/3))) regimes, which requires understanding the BOBW framework.
  - Quick check question: How does the BOBW framework handle the transition between stochastic and adversarial environments?

## Architecture Onboarding

- **Component map**: Loss estimator -> FTRL optimization with Tsallis entropy -> SPB-matching learning rate update -> Forced exploration rate calculation -> Action selection probability -> Regret calculation

- **Critical path**: 
  1. Compute loss estimator based on feedback
  2. Update learning rate using SPB-matching rule
  3. Solve FTRL optimization problem
  4. Compute action selection probability
  5. Choose action and observe feedback
  6. Accumulate regret and verify bounds

- **Design tradeoffs**:
  - Tsallis entropy vs. Shannon entropy: Tsallis provides better dependence on k but requires careful exponent selection
  - Forced exploration rate: Tradeoff between sufficient exploration for accurate estimation and minimizing regret from exploration
  - Learning rate adaptivity: More frequent updates provide better adaptation but increase computational cost

- **Failure signatures**:
  - Regret growing faster than expected (indicates learning rate or exploration rate issues)
  - Numerical instability in FTRL optimization (indicates regularization parameter problems)
  - Poor performance in stochastic regime (indicates suboptimal Tsallis entropy exponent)
  - Suboptimal performance in adversarial regime (indicates insufficient forced exploration)

- **First 3 experiments**:
  1. Implement SPB-matching learning rate with synthetic stability, penalty, and bias components to verify the matching mechanism works as expected
  2. Test Tsallis entropy regularization with different exponents on a simple multi-armed bandit problem to find the optimal α ≈ 1 - 1/(log k)
  3. Apply the complete algorithm to a small graph bandit problem with known weak domination number to verify the O(δ^(1/3) · T^(2/3)) bound holds in practice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the SPB-matching framework be extended to achieve Best-of-Both-Worlds guarantees for partial monitoring games with weak observability?
- Basis in paper: [explicit] The paper discusses partial monitoring with global observability and graph bandits with weak observability, but does not explore the combination of these two settings.
- Why unresolved: The paper only applies SPB-matching to problems with minimax regret of Θ(T^(2/3)), and partial monitoring with weak observability has a different regret bound.
- What evidence would resolve it: A theoretical analysis showing that SPB-matching can be adapted to handle the specific challenges of partial monitoring with weak observability, including the different regret bound and the nature of the feedback.

### Open Question 2
- Question: Can the SPB-matching framework be applied to other hard online learning problems, such as bandits with switching costs or bandits with paid observations?
- Basis in paper: [inferred] The paper mentions these problems as examples of hard online learning problems with minimax regret of Θ(T^(2/3)), but does not provide specific results for them.
- Why unresolved: The paper focuses on demonstrating the effectiveness of SPB-matching for partial monitoring and graph bandits, and does not explore its potential for other problem settings.
- What evidence would resolve it: A theoretical analysis showing that SPB-matching can be adapted to handle the specific challenges of these problems, including the nature of the feedback and the regret bound.

### Open Question 3
- Question: Can the SPB-matching framework be used to derive tighter regret bounds for graph bandits, potentially depending on the fractional weak domination number instead of the fractional domination number?
- Basis in paper: [inferred] The paper mentions that using the fractional weak domination number could lead to tighter bounds, but does not provide a specific analysis for this case.
- Why unresolved: The paper uses the fractional domination number in its analysis, and deriving bounds based on the fractional weak domination number would require a different approach.
- What evidence would resolve it: A theoretical analysis showing that SPB-matching can be adapted to handle the specific challenges of graph bandits with weak observability, and that the resulting bounds depend on the fractional weak domination number.

### Open Question 4
- Question: Can the SPB-matching framework be used to derive optimal regret bounds for partial monitoring games in the stochastic regime, potentially improving upon the existing bounds that depend on log T or k?
- Basis in paper: [inferred] The paper mentions that the existing bounds for partial monitoring with global observability are suboptimal in the stochastic regime, but does not provide a specific analysis for achieving optimal bounds.
- Why unresolved: The paper focuses on demonstrating the effectiveness of SPB-matching for achieving BOBW guarantees, and does not explore the potential for deriving optimal regret bounds in the stochastic regime.
- What evidence would resolve it: A theoretical analysis showing that SPB-matching can be adapted to handle the specific challenges of partial monitoring with global observability in the stochastic regime, and that the resulting bounds are optimal with respect to variables other than T.

## Limitations

- The SPB-matching mechanism's effectiveness relies heavily on accurate estimation of stability, penalty, and bias terms, but the paper doesn't fully address how estimation errors propagate through the learning rate update
- While the Tsallis entropy exponent α = 1 - 1/(log k) is theoretically motivated, empirical validation across diverse problem instances is limited
- The forced exploration rate design balances two competing objectives, but the optimal tradeoff point may depend on problem-specific characteristics not captured in the general framework

## Confidence

- **High confidence**: The existence of Θ(T^(2/3)) minimax regret for the problems studied is well-established in the literature
- **Medium confidence**: The SPB-matching learning rate framework is theoretically sound but requires careful implementation to avoid numerical instability
- **Low confidence**: The practical performance advantages over existing BOBW algorithms need more extensive empirical validation across diverse problem instances

## Next Checks

1. **Robustness testing**: Implement the algorithm on synthetic problems with varying levels of observability and domination numbers to verify the claimed regret bounds hold across the full parameter space
2. **Estimation error analysis**: Introduce controlled noise in the stability, penalty, and bias term estimations to study how estimation errors affect the learning rate update and regret performance
3. **Comparative evaluation**: Benchmark against state-of-the-art BOBW algorithms on standard partial monitoring and graph bandit problem instances to quantify practical performance improvements