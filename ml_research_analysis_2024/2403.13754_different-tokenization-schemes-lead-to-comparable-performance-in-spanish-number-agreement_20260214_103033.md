---
ver: rpa2
title: Different Tokenization Schemes Lead to Comparable Performance in Spanish Number
  Agreement
arxiv_id: '2403.13754'
source_url: https://arxiv.org/abs/2403.13754
tags:
- tokenization
- plural
- nouns
- language
- agreement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of different tokenization schemes
  on Spanish number agreement in language models. The authors examine single-token,
  morphemically-tokenized, and non-morphemically-tokenized plural nouns using a masked
  article prediction task.
---

# Different Tokenization Schemes Lead to Comparable Performance in Spanish Number Agreement

## Quick Facts
- **arXiv ID**: 2403.13754
- **Source URL**: https://arxiv.org/abs/2403.13754
- **Reference count**: 7
- **Primary result**: Different tokenization schemes (single-token, morphemically-tokenized, non-morphemically-tokenized) lead to comparable performance in Spanish number agreement tasks.

## Executive Summary
This study investigates how different tokenization schemes affect Spanish number agreement in language models. Using a masked article prediction task with the BETO Spanish BERT model, the authors examine single-token, morphemically-tokenized, and non-morphemically-tokenized plural nouns. They find that all tokenization schemes perform similarly with near-ceiling accuracy for article agreement. The study also tests whether artificially inducing morphemic tokenization for words not originally tokenized this way still allows successful task performance, finding that it does, though slightly less accurately than original tokenization. Exploratory analysis shows that different plural tokenizations have similar distributions along the embedding axis that maximally distinguishes singular and plural nouns.

## Method Summary
The study uses the BETO Spanish BERT model to assess number agreement through masked article prediction. Plural nouns were extracted from the AnCora Treebanks and categorized into three tokenization schemes: one-token, multi-token morphemic, and multi-token non-morphemic. Artificial morphemic tokenizations were created for single-token and non-morphemically-tokenized plurals by concatenating appropriate affix tokens. Agreement was assessed by computing log-odds of plural vs. singular article predictions. Linear mixed-effects models analyzed the effects of tokenization scheme, word number, and article type, while Linear Discriminant Analysis identified embedding axes that separate singular and plural forms.

## Key Results
- All tokenization schemes achieved near-ceiling accuracy in Spanish number agreement tasks
- Artificially-induced morphemic tokenization maintained successful task performance, though slightly less accurate than original tokenization
- Different plural tokenizations showed similar distributions along the embedding axis maximally distinguishing singular and plural nouns
- Word frequency correlated with tokenization scheme selection, with more frequent words more likely to receive single-token representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Morphologically-aligned tokenization is not strictly required for Spanish number agreement performance, as models can generalize morphological patterns to unseen token sequences.
- Mechanism: The model learns abstract morphological representations that are transferable across different tokenization schemes. When presented with artificially induced morphemic tokenization, the model applies learned patterns to maintain high agreement accuracy.
- Core assumption: The model's internal representations capture morphological regularities that transcend specific tokenization boundaries.
- Evidence anchors:
  - [abstract]: "Artificially inducing morphemic tokenization for words not originally tokenized this way still allows successful task performance"
  - [section]: "Artificial tokenization schemes, where we coerce an initially single-token or non-morphemically-tokenized plural into a morphemic representation, leads to successful task performance"
  - [corpus]: Weak - no direct corpus evidence of morphological generalization mechanisms
- Break condition: If the model cannot generalize morphological patterns beyond its training tokenization, artificially induced morphemic tokenization would fail to maintain high accuracy.

### Mechanism 2
- Claim: Frequency of wordforms influences tokenization scheme selection, which in turn affects agreement performance.
- Mechanism: More frequent words are more likely to be assigned single-token representations during tokenizer training, while less frequent words are decomposed into subword units. This frequency-based tokenization affects the model's ability to predict correct articles.
- Core assumption: Tokenizer training prioritizes frequent words for single-token assignment, creating a frequency-tokenization correlation that impacts downstream performance.
- Evidence anchors:
  - [section]: "Using oral frequency measures... we examined the relationship between a wordform's frequency and how it was tokenized. A linear model predicting Log Frequency from Tokenization Scheme explained significant variance"
  - [section]: "the frequency of a wordform was likely a major factor in how it was tokenized"
  - [corpus]: Weak - frequency analysis is based on a specific corpus but no broader corpus evidence provided
- Break condition: If frequency effects are not present in the training data or if the tokenizer uses different criteria for token assignment, the correlation between frequency and tokenization scheme would break down.

### Mechanism 3
- Claim: Language model embeddings for different plural tokenizations have similar distributions along the embedding space axis that maximally distinguishes singular and plural nouns.
- Mechanism: The model learns a common embedding space where singular and plural forms are linearly separable, regardless of tokenization scheme. This shared axis of separation allows the model to apply the same agreement mechanism across different tokenizations.
- Core assumption: The embedding space contains axes that capture morphological number distinctions independent of tokenization boundaries.
- Evidence anchors:
  - [section]: "We find axes with high overlap between all plural forms (regardless of tokenization scheme) and high discriminability between plural and singular forms"
  - [section]: "all types of plurals... patterned together and were not linearly discriminable along this axis"
  - [corpus]: Weak - no corpus evidence of embedding space structure
- Break condition: If the embedding space does not contain shared axes for morphological distinctions, different tokenization schemes would pattern differently and agreement performance would vary significantly.

## Foundational Learning

- **Linear Discriminant Analysis (LDA)**
  - Why needed here: LDA is used to identify axes in the embedding space that maximally separate singular and plural forms, revealing how the model represents morphological distinctions across different tokenizations.
  - Quick check question: What does LDA compute when given n sets of representations? (Answer: n-1 directions that maximize separation between the sets)

- **Masked Language Model Prediction**
  - Why needed here: The study uses masked article prediction to assess number agreement, where the model predicts masked articles based on noun representations learned through different tokenization schemes.
  - Quick check question: How does the log-odds calculation work for article prediction in this study? (Answer: Log-odds = log(P(plural article)/P(singular article)), where positive values indicate higher probability of plural article)

- **Subword Tokenization and Morphological Boundaries**
  - Why needed here: Understanding how different tokenization schemes (single-token, morphemic, non-morphemic) affect the model's ability to learn and apply morphological patterns is central to the study's investigation.
  - Quick check question: What distinguishes morphemically-tokenized plurals from non-morphemically-tokenized plurals in this study? (Answer: Morphemically-tokenized follow morpheme boundaries like 'naranja'+'##s', while non-morphemically-tokenized do not like 'neuro'+'##nas')

## Architecture Onboarding

- **Component map**:
  Tokenizer -> Language Model (BETO BERT) -> Embedding Space (768-dim) -> Agreement Task (Masked Article Prediction) -> Analysis Tools (LDA, Mixed-Effects Models)

- **Critical path**:
  1. Extract plural nouns and categorize by tokenization scheme
  2. Create artificial morphemic tokenizations for non-morphemic and single-token words
  3. Run masked article prediction task for all tokenization variants
  4. Analyze agreement performance using linear mixed-effects models
  5. Compare embeddings using LDA to identify shared representation axes

- **Design tradeoffs**:
  - Single-token vs. subword tokenization: Single-token offers semantic precision but requires more computational resources and may not generalize to unseen words
  - Original vs. artificial tokenization: Original tokenization performs slightly better but artificial tokenization still works, suggesting model generalization
  - Layer selection for embeddings: Using last 4 layers balances task-relevant information with computational efficiency

- **Failure signatures**:
  - Agreement accuracy drops significantly for artificially-tokenized words
  - LDA shows no shared axes between different tokenization schemes
  - Mixed-effects models show strong interactions between tokenization scheme and word number
  - Frequency effects disappear when controlling for tokenization scheme

- **First 3 experiments**:
  1. Replicate the masked article prediction task with a different Spanish BERT model to test generalizability across architectures
  2. Test agreement performance on a different morphological phenomenon (e.g., gender agreement) to see if findings extend beyond number
  3. Apply the artificial tokenization procedure to a morphologically-rich language like Turkish to test cross-linguistic applicability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do different tokenization schemes rely on distinct neural mechanisms for number agreement, or do they utilize the same underlying processing pathways?
- Basis in paper: [inferred] The paper notes that similar agreement performance across tokenization schemes "could indicate multiple different agreement mechanisms in the model" and suggests future work applying causal interventions on different embedding axes.
- Why unresolved: The study only analyzed correlations between tokenization schemes and agreement performance; it did not conduct causal interventions to determine whether the same neural sub-networks are involved in number agreement across different tokenization types.
- What evidence would resolve it: Causal intervention experiments (e.g., as in Mueller et al., 2022) that manipulate specific embedding axes identified through LDA could reveal whether the same neural sub-networks are responsible for number agreement across different tokenization schemes.

### Open Question 2
- Question: How generalizable are the findings on Spanish number agreement to other morphosyntactic phenomena and languages with different morphological structures?
- Basis in paper: [explicit] The authors explicitly state "A key limitation of the current work is scope" and suggest considering "additional morphosyntactic phenomena, additional languages, and a larger range of language models or tokenization schemes."
- Why unresolved: The study only examined one specific morphosyntactic phenomenon (number agreement in Spanish plurals) using one language model (BETO). The effects of tokenization on other morphosyntactic rules and in languages with different morphological structures remain unknown.
- What evidence would resolve it: Replicating the study design with other morphosyntactic phenomena (e.g., case agreement, gender agreement) and in languages with different morphological typologies (e.g., agglutinative languages like Turkish or fusional languages like Russian) would reveal the generalizability of the findings.

### Open Question 3
- Question: Does morphological tokenization consistently improve performance across all frequency ranges of words, or is the effect frequency-dependent?
- Basis in paper: [inferred] The authors found that word frequency was correlated with tokenization scheme and that "the language model made better predictions for more frequent nouns than less frequent nouns." However, they did not directly test whether morphological tokenization improves performance across all frequency ranges.
- Why unresolved: The study only examined the overall effect of tokenization scheme on agreement performance, without considering whether this effect varies across different frequency ranges of words.
- What evidence would resolve it: Analyzing the effect of tokenization scheme on agreement performance separately for high-frequency, medium-frequency, and low-frequency words would reveal whether morphological tokenization provides consistent benefits across all frequency ranges or if the effect is frequency-dependent.

## Limitations

- The study focuses on a single morphological phenomenon (Spanish number agreement) in one language, limiting generalizability to other morphological systems or agreement types.
- The analysis relies on a single Spanish BERT model (BETO) without testing whether findings extend to other architectures or training regimes.
- The artificial tokenization procedure may not fully capture the complexity of natural morpheme boundaries, and the study does not investigate how the model handles truly novel morphological formations.

## Confidence

- **High Confidence**: The finding that different tokenization schemes lead to comparable performance in Spanish number agreement is well-supported by the experimental data, showing near-ceiling accuracy across all tokenization types and successful performance with artificially-induced morphemic tokenization.
- **Medium Confidence**: The claim that morphologically-aligned tokenization is viable but not strictly required is supported by the data but depends on the specific architecture and training procedure used. The mechanism of morphological generalization across tokenization schemes is plausible but not directly tested.
- **Low Confidence**: The assertion that frequency of wordforms is a major factor in tokenization scheme selection, while showing statistical significance, lacks direct causal evidence and may be an artifact of the specific tokenizer training procedure rather than a general principle.

## Next Checks

1. **Cross-linguistic validation**: Apply the artificial tokenization procedure and masked article prediction task to a morphologically-rich language with different morphological typology (e.g., Turkish or Finnish) to test whether the pattern of comparable performance across tokenization schemes generalizes beyond Spanish.

2. **Architecture comparison**: Replicate the study using different language model architectures (e.g., RoBERTa, DeBERTa) and training regimes to determine whether the observed tokenization independence is specific to BERT-style models or represents a more general property of language models.

3. **Novel morphological formation test**: Design stimuli that test the model's ability to handle truly novel morphological combinations (e.g., compound words or productive affixation) beyond the artificially-created examples to directly assess the limits of morphological generalization across tokenization schemes.