---
ver: rpa2
title: An Experimental Design Framework for Label-Efficient Supervised Finetuning
  of Large Language Models
arxiv_id: '2401.06692'
source_url: https://arxiv.org/abs/2401.06692
tags:
- learning
- active
- design
- arxiv
- experimental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the high annotation costs of supervised finetuning
  (SFT) for large language models (LLMs) by leveraging experimental design to select
  the most informative samples for annotation, rather than relying on expensive active
  learning or random sampling. The method employs uncertainty-based, k-center, and
  submodular selection strategies to choose diverse and representative prompts from
  a large pool, requiring only a single step of model inference.
---

# An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models

## Quick Facts
- arXiv ID: 2401.06692
- Source URL: https://arxiv.org/abs/2401.06692
- Authors: Gantavya Bhatt; Yifang Chen; Arnav M. Das; Jifan Zhang; Sang T. Truong; Stephen Mussmann; Yinglun Zhu; Jeffrey Bilmes; Sang T. Truong; Stephen Mussmann; Yinglun Zhu; Jeffrey Bilmes; Simon S. Du; Kevin Jamieson; Jordan T. Ash; Robert D. Nowak
- Reference count: 17
- Primary result: Achieves same generalization performance as random sampling while reducing annotation costs by up to 50%

## Executive Summary
This paper addresses the high annotation costs of supervised finetuning (SFT) for large language models by introducing an experimental design framework that selects the most informative samples for annotation. The method employs uncertainty-based, k-center, and submodular selection strategies to choose diverse and representative prompts from a large pool, requiring only a single step of model inference. Experiments on FLAN V2 with the LLaMA-2 7B model demonstrate that this approach achieves the same generalization performance as random sampling while reducing annotation costs by up to 50%, achieving 47.63 accuracy on MMLU and 41.30 on BBH with just 45K annotations instead of 90K.

## Method Summary
The method uses experimental design techniques to select the most informative samples to label in a single step based on the initial model's uncertainty estimates. It explores three selection strategies: uncertainty-based methods that measure token-level confidence, k-center methods that ensure diversity in feature space, and submodular facility location functions that optimize both diversity and representativeness. The selected prompts are then annotated and used to finetune the LLM using LoRA. This approach avoids the computational overhead of active learning while maintaining high label efficiency through strategic sample selection.

## Key Results
- Achieved 47.63 accuracy on MMLU and 41.30 on BBH with only 45K annotations instead of 90K
- Experimental design methods matched random sampling performance while reducing annotation costs by up to 50%
- Minimum margin uncertainty scoring showed promise for identifying informative samples, though redundancy remained an issue

## Why This Works (Mechanism)

### Mechanism 1
Experimental design avoids the computational overhead of active learning while still achieving high label efficiency by selecting the most informative samples to label in a single step using initial model uncertainty and diversity measures, eliminating the need for iterative model retraining and inference.

### Mechanism 2
Facility location functions optimize diversity while maintaining informativeness for annotation selection by using submodular optimization to select samples that are both diverse (avoiding redundancy) and representative of the instruction distribution, maximizing the utility of each annotation.

### Mechanism 3
Minimum margin uncertainty scoring captures harder-to-generate samples that are more informative for finetuning by identifying sequences where the model has the least confidence between top predictions, indicating samples that require more nuanced understanding.

## Foundational Learning

- Concept: Submodular optimization and greedy approximation guarantees
  - Why needed here: Facility location functions are submodular, allowing efficient near-optimal solutions for diverse sample selection
  - Quick check question: What is the approximation guarantee achieved by greedy maximization of a submodular function?

- Concept: Uncertainty quantification in sequence generation models
  - Why needed here: Different uncertainty measures (entropy, confidence, margin) help identify which samples are most informative for annotation
  - Quick check question: How does minimum margin differ from mean margin in measuring sequence generation uncertainty?

- Concept: Feature extraction from LLM hidden states
  - Why needed here: K-center and facility location methods require meaningful feature representations of prompts for diversity optimization
  - Quick check question: What hidden state representation is used for decoder-only architectures in this framework?

## Architecture Onboarding

- Component map: Pretrained LLM (initial model for uncertainty estimation) -> Feature extractor (hidden state representations) -> Selection strategy module (uncertainty, k-center, or submodular methods) -> Annotation pipeline (human or automated response generation) -> Finetuning module (LoRA or similar parameter-efficient methods)

- Critical path: Prompt pool → Uncertainty/feature computation → Sample selection → Annotation → Finetuning → Evaluation

- Design tradeoffs: Single-step selection (computational efficiency) vs. iterative active learning (potentially better sample quality)

- Failure signatures: Poor performance on evaluation benchmarks despite high annotation savings, high variance in results across random seeds

- First 3 experiments:
  1. Compare random sampling vs. minimum margin selection on MMLU benchmark with 45K annotations
  2. Ablation study on facility location kernel width (γ) parameter at 45K budget
  3. Evaluate sensitivity of different uncertainty measures (entropy, confidence, mean margin, min margin) across annotation budgets

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of experimental design methods compare to active learning methods when computational resources are not a limiting factor? The paper mentions that active learning can provide gains over experimental design, but under certain assumptions, the expected loss using active learning is no more than twice that of experimental design.

### Open Question 2
Can the experimental design framework be extended to other domains beyond natural language processing, such as computer vision or reinforcement learning? The paper focuses on the application of experimental design for supervised finetuning of large language models, but the concept of selecting informative samples for annotation could be applicable to other domains.

### Open Question 3
How does the performance of experimental design methods vary with different types of language models, such as encoder-only or encoder-decoder architectures? The paper focuses on the application of experimental design for supervised finetuning of large language models, specifically using the LLaMA-2 7B model.

## Limitations
- Reliance on initial model's uncertainty estimates without iterative refinement may lead to suboptimal sample selection
- Focus on single architecture (LLaMA-2 7B) and dataset (FLAN V2 subset) limits generalizability
- Facility location method's kernel width parameter (γ) requires careful tuning without extensive sensitivity analysis

## Confidence

**High Confidence**: Experimental design can achieve comparable performance to random sampling with reduced annotation costs

**Medium Confidence**: Submodular facility location optimization provides diversity while maintaining informativeness

**Medium Confidence**: Minimum margin uncertainty scoring effectively identifies informative samples

## Next Checks

1. **Cross-architecture validation**: Test the experimental design framework on different model sizes (e.g., LLaMA-2 13B, 33B) and architectures to assess generalizability beyond the 7B model used in the study.

2. **Dataset diversity evaluation**: Apply the framework to instruction datasets with different characteristics (e.g., different task distributions, complexity levels) to evaluate robustness across various instruction-following scenarios.

3. **Long-term stability analysis**: Conduct multi-round evaluation to assess whether the initial uncertainty estimates remain valid as the model is finetuned, potentially leading to selection bias in subsequent iterations if the framework were extended to active learning scenarios.