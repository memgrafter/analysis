---
ver: rpa2
title: Learning to Predict Usage Options of Product Reviews with LLM-Generated Labels
arxiv_id: '2410.12470'
source_url: https://arxiv.org/abs/2410.12470
tags:
- usage
- options
- data
- product
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes using large language models (LLMs) to generate\
  \ labels for a complex natural language task\u2014extracting product usage options\
  \ from customer reviews. The task requires implicit multi-step reasoning to identify\
  \ usage options that may not be explicitly stated."
---

# Learning to Predict Usage Options of Product Reviews with LLM-Generated Labels

## Quick Facts
- arXiv ID: 2410.12470
- Source URL: https://arxiv.org/abs/2410.12470
- Authors: Leo Kohlenberg; Leonard Horns; Frederic Sadrieh; Nils Kiele; Matthis Clausen; Konstantin Ketterer; Avetis Navasardyan; Tamara Czinczoll; Gerard de Melo; Ralf Herbrich
- Reference count: 23
- One-line primary result: LLM-generated labels, especially from GPT-4, perform comparably to expert labels for extracting product usage options from customer reviews.

## Executive Summary
This paper tackles the challenge of extracting product usage options from customer reviews—a complex NLP task requiring implicit multi-step reasoning. The authors propose using large language models (LLMs) to generate high-quality training labels for this task, comparing them against expert annotations and crowd-sourced vendor labels. They introduce HAMS4, a novel evaluation metric for comparing sets of strings with multiple reference sets, and demonstrate that models fine-tuned on LLM-generated labels achieve performance comparable to expert-labeled models while offering significant energy efficiency advantages at scale.

## Method Summary
The authors preprocess the Amazon Customer Review Dataset (reducing from 151M to ~63M reviews) and create a labeled subset of 4,252 reviews. They use LLMs (GPT-3.5 Turbo, GPT-4, Llama 2 70B) with different prompting strategies to generate labels for 2,000 training reviews, comparing these against expert annotations on 2,000 evaluation reviews. The labels are then used to fine-tune a Flan-T5 Base model using Adafactor optimizer with inverse-square-root learning rate schedule. Model performance is evaluated using the novel HAMS4 metric alongside WMS (Word Mover's Similarity) and classification F1 scores.

## Key Results
- GPT-4-generated labels achieve HAMS4 scores of 0.749 and WMS scores of 0.750, reaching the level of domain expert annotations
- LLM-generated labels significantly outperform crowd-sourced vendor labels (Vendor A: 0.335 HAMS4, Vendor B: 0.189 HAMS4)
- Smaller T5 models trained on LLM-generated labels offer energy efficiency advantages, reaching FLOPs parity with LLMs around 10,000 requests
- High inter-annotator variance even among experts highlights the task's complexity and validates the need for sophisticated annotation approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate high-quality training labels for complex tasks like usage option extraction from customer reviews
- Mechanism: LLMs act as few-shot learners, leveraging their large-scale pretraining to understand implicit multi-step reasoning required for identifying and summarizing usage options from unstructured text
- Core assumption: LLMs can handle semantic ambiguity and paraphrase invariance better than traditional annotation methods
- Evidence anchors:
  - [abstract] "LLM-generated labels even reach the level of domain experts"
  - [section] "We show that due to the task's complexity, the inter-annotator variance is high, even for experts"
  - [corpus] Weak - no direct evidence in corpus about LLM label quality for this specific task
- Break condition: When reviews contain highly ambiguous opinions or grammatical errors that confuse the LLM's reasoning process

### Mechanism 2
- Claim: Using a smaller specialized model trained on LLM-generated labels offers better long-term efficiency than direct LLM usage
- Mechanism: The smaller model (T5) provides energy-efficient inference after initial training costs are amortized across many requests
- Core assumption: Energy costs for inference dominate over training costs in deployment scenarios
- Evidence anchors:
  - [section] "Training our own T5 model incurs upfront energy costs... However, once trained, the energy consumption for inference is a lot smaller"
  - [section] "For the most conservative estimate we reach FLOPs parity around 10,000 requests"
  - [corpus] Weak - no direct corpus evidence about energy efficiency comparisons
- Break condition: When the deployment scenario involves fewer than 10,000 inference requests, making training costs prohibitive

### Mechanism 3
- Claim: The proposed HAMS4 metric effectively evaluates set-to-set similarity for usage option prediction
- Mechanism: HAMS4 uses harmonic mean of precision and recall with semantic weighting based on embedding similarity, handling multiple reference sets and order invariance
- Core assumption: Semantic similarity between usage options can be captured through sentence transformer embeddings
- Evidence anchors:
  - [section] "HAMS4... can be used to compare a set of strings with multiple reference sets"
  - [section] "S4 greedily matches each string in ũ with a string in u to calculate precision PS4, and vice versa"
  - [corpus] Weak - no corpus evidence about HAMS4 metric validation against established metrics
- Break condition: When usage options are semantically very similar or when the sentence transformer model fails to capture meaningful distinctions

## Foundational Learning

- Concept: Few-shot learning with LLMs
  - Why needed here: The task requires understanding complex natural language without extensive labeled training data
  - Quick check question: What distinguishes few-shot learning from zero-shot learning in LLM applications?

- Concept: Knowledge distillation
  - Why needed here: Transferring knowledge from large LLMs to smaller, more efficient models while maintaining performance
  - Quick check question: How does knowledge distillation help mitigate hallucinations in LLM outputs?

- Concept: Set-to-set evaluation metrics
  - Why needed here: Traditional metrics fail to handle the unordered, semantic similarity requirements of usage option evaluation
  - Quick check question: Why can't BLEU or ROUGE be used effectively for this usage option prediction task?

## Architecture Onboarding

- Component map: Amazon Reviews -> LLM Annotation Layer (GPT-3.5 Turbo, GPT-4, Llama 2 70B) -> T5 Model Training -> HAMS4 Evaluation -> Deployment
- Critical path: Review → LLM label generation → T5 model training → HAMS4 evaluation → Deployment
- Design tradeoffs: Accuracy vs. efficiency (using GPT-4 gives better results but higher costs vs. GPT-3.5 Turbo)
- Failure signatures: High variance in inter-annotator agreement, hallucinations in LLM outputs, poor semantic matching in HAMS4
- First 3 experiments:
  1. Test different prompt formats (chain-of-thought vs. few-shot) on prompt selection set to optimize label quality
  2. Compare HAMS4 scores between LLM-generated labels and expert annotations on validation set
  3. Measure energy efficiency break-even point between LLM inference and T5 model inference across different request volumes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-generated labels compare to expert-labeled data for product usage extraction tasks?
- Basis in paper: [explicit] The paper compares GPT-4-generated labels to expert-labeled data and finds that GPT-4 labels "reach the level of domain experts"
- Why unresolved: While the paper shows GPT-4 matches expert performance on their specific task, the comparison is limited to one domain (product reviews) and one specific task (usage option extraction). It's unclear how generalizable these results are to other NLP tasks or domains.
- What evidence would resolve it: Testing GPT-4 and other LLMs on a broader range of NLP tasks and domains, comparing their label quality to expert-labeled data across multiple domains.

### Open Question 2
- Question: What is the prevalence and impact of hallucinations in LLM-generated labels for complex NLP tasks?
- Basis in paper: [explicit] The paper acknowledges that hallucinations are a problem with LLMs, showing examples where GPT-4 hallucinates usage options that are refuted by the review text itself.
- Why unresolved: The paper provides qualitative examples of hallucinations but doesn't quantify their prevalence or measure their impact on downstream model performance. It's unclear how common hallucinations are and how much they affect model quality.
- What evidence would resolve it: A systematic study measuring the frequency of hallucinations in LLM-generated labels across different tasks and domains, and evaluating how hallucinations impact the performance of models trained on such data.

### Open Question 3
- Question: How do different LLM architectures and prompting strategies affect label quality for complex NLP tasks?
- Basis in paper: [explicit] The paper compares GPT-3.5 Turbo, GPT-4, and Llama2 70B with different prompting strategies (few-shot vs. chain-of-thought) and finds significant performance differences.
- Why unresolved: The study is limited to a few LLM variants and prompt types. It's unclear how other LLM architectures or more sophisticated prompting techniques would perform on this task or other complex NLP tasks.
- What evidence would resolve it: Extensive experimentation with a wider range of LLM architectures (including open-source models), prompt engineering techniques, and few-shot learning approaches to identify optimal configurations for complex NLP tasks.

## Limitations

- The evaluation relies heavily on the novel HAMS4 metric, which hasn't been validated against established evaluation frameworks
- The energy efficiency analysis depends on specific hardware configurations and usage patterns that may not generalize across different deployment scenarios
- The comparison between vendor services and LLM labels uses different annotation paradigms (binary classification vs. extraction), making direct performance comparisons potentially misleading

## Confidence

**High Confidence**: The core finding that LLM-generated labels (particularly GPT-4) achieve comparable quality to expert annotations is well-supported by the experimental results, including the strong HAMS4 scores (0.749) and WMS scores (0.750) for GPT-4-generated labels.

**Medium Confidence**: The efficiency claims about smaller models trained on LLM labels require careful interpretation. While the mathematical framework for calculating FLOPs parity is sound, real-world energy costs include additional factors like model maintenance, data transfer, and hardware-specific considerations not fully addressed.

**Low Confidence**: The generalization analysis showing that T5 models trained on different labeling sources capture similar knowledge is based on a single embedding similarity metric (WMS) without exploring alternative evaluation methods or conducting ablation studies.

## Next Checks

1. **HAMS4 Metric Validation**: Conduct a comprehensive validation of the HAMS4 metric by comparing its rankings against human judgments on the same data, and test its behavior with synthetic datasets where ground truth similarity is known.

2. **Energy Efficiency Field Test**: Deploy both the LLM-based approach and the T5-based approach in a realistic production environment for 30 days, measuring actual energy consumption, latency, and accuracy degradation under varying load conditions.

3. **Cross-Domain Generalization**: Test the T5 models trained on Amazon review data on reviews from different domains (e.g., restaurant reviews, software reviews) to assess whether the usage option extraction capability transfers beyond the original training distribution.