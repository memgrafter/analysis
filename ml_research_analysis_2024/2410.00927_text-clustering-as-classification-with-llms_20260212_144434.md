---
ver: rpa2
title: Text Clustering as Classification with LLMs
arxiv_id: '2410.00927'
source_url: https://arxiv.org/abs/2410.00927
tags:
- clustering
- labels
- text
- llms
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel approach to text clustering that
  leverages Large Language Models (LLMs) exclusively, eliminating the need for additional
  embedding models or traditional clustering algorithms. The method reframes clustering
  as a two-stage classification task: first, the LLM generates potential cluster labels
  by processing the dataset in mini-batches and merging similar labels; second, it
  classifies each text sample into the most appropriate label.'
---

# Text Clustering as Classification with LLMs

## Quick Facts
- arXiv ID: 2410.00927
- Source URL: https://arxiv.org/abs/2410.00927
- Authors: Chen Huang; Guoxiu He
- Reference count: 40
- One-line primary result: Novel LLM-only framework achieves state-of-the-art clustering performance without embedding models or traditional algorithms

## Executive Summary
This paper introduces a novel approach to text clustering that reframes the problem as a two-stage classification task using Large Language Models exclusively. The framework eliminates the need for fine-tuning embedding models or traditional clustering algorithms by leveraging LLMs' in-context learning capabilities. Extensive experiments on five diverse datasets demonstrate that this approach significantly outperforms traditional embedding-based methods and other LLM-based clustering approaches across multiple evaluation metrics.

## Method Summary
The proposed framework transforms text clustering into a classification problem through a two-stage process. First, the LLM generates candidate cluster labels by processing the dataset in mini-batches and merging similar labels across batches. Second, the LLM classifies each text sample into the most appropriate label from the generated set. The method uses GPT-3.5-turbo and processes data in batches of 15, with few-shot examples provided to guide the LLM's label generation and classification tasks.

## Key Results
- Achieved state-of-the-art performance across all five tested datasets (ArxivS2S, GoEmo, Massive-I, Massive-D, MTOP-I)
- Outperformed traditional embedding-based methods and other LLM-based clustering approaches in accuracy, NMI, and ARI metrics
- Demonstrated effectiveness across datasets with 18-102 clusters and sizes ranging from 10k to 500k instances

## Why This Works (Mechanism)

### Mechanism 1
Reframing clustering as a classification task leverages LLM's in-context learning and generalization abilities to eliminate the need for fine-tuning embedding models. The LLM is prompted to generate candidate labels by processing mini-batches of data and merging similar labels, then classify each sample into the most appropriate label, effectively transforming clustering into a classification problem.

Core assumption: LLMs can generate semantically meaningful cluster labels and accurately classify samples without explicit training on clustering-specific data.

### Mechanism 2
Sequential mini-batch processing overcomes input length limitations of LLMs while maintaining clustering quality. The dataset is split into mini-batches, each processed independently by the LLM to generate potential labels, which are then aggregated and merged to form final clusters.

Core assumption: Processing data in smaller chunks preserves semantic relationships while allowing the LLM to handle large datasets.

### Mechanism 3
Merging semantically similar labels improves clustering granularity and coherence by reducing label redundancy. After generating potential labels from all mini-batches, the LLM is prompted to merge labels with similar expressions into unified cluster labels.

Core assumption: LLMs can accurately identify semantically similar labels and merge them appropriately to improve cluster quality.

## Foundational Learning

- Concept: In-context learning in LLMs
  - Why needed here: The framework relies on providing few-shot examples to guide the LLM in generating meaningful labels and performing classification.
  - Quick check question: How many example labels are provided to the LLM during label generation?

- Concept: Semantic similarity and clustering metrics
  - Why needed here: Understanding metrics like NMI and ARI is crucial for evaluating clustering performance and comparing against baselines.
  - Quick check question: What does NMI measure in the context of clustering evaluation?

- Concept: Batch processing and context windows
  - Why needed here: The framework processes data in mini-batches to overcome LLM context length limitations while maintaining semantic relationships.
  - Quick check question: What is the batch size used in the experiments?

## Architecture Onboarding

- Component map: Input processor -> Label generator -> Label merger -> Classifier -> Evaluator
- Critical path: 1. Mini-batch label generation, 2. Label aggregation and merging, 3. Sample classification, 4. Metric computation
- Design tradeoffs:
  - Batch size vs. context length: Larger batches provide more context but may exceed LLM limits
  - Label merging aggressiveness vs. granularity: More merging creates fewer, broader clusters
  - Prompt complexity vs. consistency: Detailed prompts improve quality but may reduce reliability
- Failure signatures:
  - Inconsistent label generation across batches
  - Poor merging leading to redundant or fragmented clusters
  - Classification errors due to ambiguous or poorly generated labels
  - High computational cost relative to traditional methods
- First 3 experiments:
  1. Vary batch size (10, 15, 20) and measure impact on clustering quality
  2. Test different percentages of few-shot labels (10%, 15%, 20%, 25%) and observe performance changes
  3. Compare clustering results with and without the label merging step to quantify its impact on granularity

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed LLM-based clustering framework scale with dataset size beyond the tested range (up to 500k instances)? The paper only extrapolates cost and time estimates for larger datasets but does not empirically validate the clustering performance at these scales.

### Open Question 2
How sensitive is the clustering performance to the choice of LLM model (e.g., GPT-3.5 vs GPT-4 vs open-source alternatives)? The paper uses GPT-3.5-turbo for all experiments but does not compare its performance against other LLM models.

### Open Question 3
What is the impact of prompt engineering quality on clustering performance, and can automated prompt optimization further improve results? While the framework shows robustness to prompt variations, there may be undiscovered prompt formulations that could significantly enhance performance.

## Limitations

- The framework's performance depends heavily on proprietary LLM API calls without published weights or reproducible implementations
- Sequential mini-batch processing may struggle with datasets containing long-range semantic dependencies that span across batches
- Computational cost and latency implications of multiple LLM calls for label generation, merging, and classification are not adequately addressed

## Confidence

**High Confidence**: The core innovation of reframing clustering as a classification task using LLMs is theoretically sound and supported by the experimental results showing superior performance on all five tested datasets.

**Medium Confidence**: The effectiveness of the sequential mini-batch processing approach is demonstrated, but the potential loss of semantic relationships across batch boundaries represents an unverified assumption.

**Medium Confidence**: The label merging mechanism shows promise in reducing redundancy, but the criteria for determining "semantic similarity" between labels remain partially opaque.

## Next Checks

1. Apply the framework to a sixth, independently sourced dataset (e.g., biomedical literature or legal documents) to verify generalizability beyond the five tested datasets.

2. Systematically vary batch sizes (5, 10, 15, 20, 25) and measure the trade-off between computational efficiency and clustering quality.

3. Run controlled experiments comparing clustering results with and without the label merging step across all datasets, measuring not only performance metrics but also cluster granularity and interpretability.