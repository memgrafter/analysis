---
ver: rpa2
title: 'RedStone: Curating General, Code, Math, and QA Data for Large Language Models'
arxiv_id: '2412.03398'
source_url: https://arxiv.org/abs/2412.03398
tags:
- data
- code
- redstone
- arxiv
- filtering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REDSTONE, a scalable pipeline for extracting
  and filtering large-scale datasets from Common Crawl for pre-training Large Language
  Models (LLMs). The pipeline addresses the need for high-quality, domain-specific
  data by leveraging the breadth of Common Crawl to create datasets across general
  language understanding, code, mathematics, and question-answering tasks.
---

# RedStone: Curating General, Code, Math, and QA Data for Large Language Models

## Quick Facts
- arXiv ID: 2412.03398
- Source URL: https://arxiv.org/abs/2412.03398
- Reference count: 36
- Primary result: Introduces REDSTONE, a scalable pipeline for extracting and filtering large-scale datasets from Common Crawl, yielding 3.48 trillion tokens across general, code, math, and QA domains.

## Executive Summary
This paper introduces REDSTONE, a scalable pipeline for extracting and filtering large-scale datasets from Common Crawl for pre-training Large Language Models (LLMs). The pipeline addresses the need for high-quality, domain-specific data by leveraging the breadth of Common Crawl to create datasets across general language understanding, code, mathematics, and question-answering tasks. REDSTONE employs extraction and filtering modules to process raw web data, resulting in the construction of datasets totaling 3.48 trillion tokens, including REDSTONE-Web (3.17 trillion tokens), REDSTONE-Code (250.2 billion tokens), REDSTONE-Math (15.9 billion tokens), and REDSTONE-QA (51.4 billion tokens). The effectiveness of REDSTONE is demonstrated through extensive evaluations, showing significant improvements in common sense reasoning, code generation, mathematics, and question-answering tasks compared to existing open-source datasets.

## Method Summary
REDSTONE is a pipeline designed to extract and filter large-scale datasets from Common Crawl for pre-training LLMs. It uses two main modules: Extraction and Filtering. The Extraction module processes raw web data from both WET and WARC formats, capturing text content while preserving domain-specific structures like code snippets, mathematical expressions, and QA patterns. The Filtering module applies rule-based cleaning (e.g., length, duplication) followed by model-based quality assessment to retain only relevant and high-quality content. Deduplication is performed using MinHash-LSH to remove near-duplicate content across snapshots. The pipeline is demonstrated across four domains—general language, code, mathematics, and question-answering—yielding a total of 3.48 trillion tokens.

## Key Results
- REDSTONE-Web: 3.17 trillion tokens from general web content.
- REDSTONE-Code: 250.2 billion tokens from code-related pages.
- REDSTONE-Math: 15.9 billion tokens from mathematical content.
- REDSTONE-QA: 51.4 billion tokens from question-answering pages.
- Significant improvements in common sense reasoning, code generation, mathematics, and QA tasks compared to existing open-source datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pipeline's two-stage filtering—rule-based then model-based—effectively removes low-quality data while preserving relevant content.
- Mechanism: Rule-based filtering quickly eliminates pages using fixed patterns (e.g., length, duplication, structural markers). Model-based filtering then applies a trained classifier to assess deeper quality dimensions like coherence and informativeness.
- Core assumption: Rules can capture obvious noise, and the classifier can reliably distinguish high-quality from low-quality text at scale.
- Evidence anchors:
  - [abstract] "Filtering module refines this data, retaining only the most relevant content through keyword searches, regular expressions, and machine learning models."
  - [section] "Rule-based Filtering. Given the vast number of original pages, we initially use rule-based filtering to quickly sift through all pages, effectively reducing the subsequent filtering workload."
  - [corpus] Weak—no direct citations or quantitative results provided.
- Break condition: If the classifier mislabels large portions of data, or if rule thresholds are too aggressive, data quality and quantity will degrade.

### Mechanism 2
- Claim: Extraction from both WET and WARC formats increases token yield without sacrificing content quality.
- Mechanism: WET files offer pre-processed plain text, while WARC files contain raw HTML. By extracting from both, the pipeline captures more data and mitigates format-specific losses.
- Core assumption: The two formats are complementary and not redundant in a way that would inflate token counts artificially.
- Evidence anchors:
  - [section] "We extract general domain data from Common Crawl in both WET and W ARC formats... Although these formats originate from the same data source and represent different forms of the same dataset, the differences in text extraction... lead to variations in the resulting plain text."
  - [abstract] "Extraction and filtering modules to process raw web data, resulting in the construction of datasets totaling 3.48 trillion tokens."
  - [corpus] Weak—no evidence comparing overlap or uniqueness between formats.
- Break condition: If both formats yield highly overlapping content, the incremental token gain may not justify the processing cost.

### Mechanism 3
- Claim: Domain-specific pipelines can be derived by adapting the core extraction/filtering logic to specialized HTML patterns.
- Mechanism: For code, math, and QA data, the pipeline modifies tag detection and content extraction to target domain-relevant structures (e.g., `<code>`, `<math>`, or question-answer patterns).
- Core assumption: Domain-specific content in Common Crawl follows predictable HTML or textual patterns that can be reliably extracted.
- Evidence anchors:
  - [abstract] "We exemplify its capability by constructing pre-training datasets across multiple fields, including general language understanding, code, mathematics, and question-answering tasks."
  - [section] "Code snippets in HTML documents can be represented in various ways... To filter HTML documents for code snippets and identify the root element of each filtered code snippet, we follow these steps..."
  - [corpus] Weak—no cross-domain performance or accuracy comparisons provided.
- Break condition: If domain-specific patterns are inconsistent or noisy, extraction recall and precision will drop.

## Foundational Learning

- Concept: Tokenization and deduplication in large-scale text processing.
  - Why needed here: The pipeline must process billions of tokens while avoiding duplicate content that could bias training.
  - Quick check question: What is the purpose of using MinHash-LSH for deduplication instead of pairwise comparison?
- Concept: HTML content extraction and noise filtering.
  - Why needed here: Raw HTML contains menus, ads, and scripts that are irrelevant for training LLMs.
  - Quick check question: How does Trafilatura identify main content blocks in an HTML page?
- Concept: Language identification and filtering.
  - Why needed here: Common Crawl contains multilingual content; filtering to English ensures consistency in training data.
  - Quick check question: Why is a confidence threshold (e.g., 0.5) used when filtering non-English pages?

## Architecture Onboarding

- Component map: Raw HTML → Extract main content → Rule filtering → Model filtering → Deduplication → Dataset output
- Critical path: Extraction Module (format-specific text extraction, domain pattern recognition) → Filtering Module (rule-based cleaning, model-based quality assessment) → Deduplication Layer (MinHash-LSH) → Domain Adapters (code, math, QA-specific extraction logic)
- Design tradeoffs:
  - Using both WET and WARC increases coverage but doubles processing cost.
  - Rule-based filtering is fast but coarse; model-based filtering is accurate but slower.
  - MinHash-LSH trades exactness for scalability.
- Failure signatures:
  - Sudden drop in token count → Over-aggressive rule filtering.
  - Spike in duplicates → Deduplication parameters misconfigured.
  - Low domain-specific recall → Extraction patterns too narrow.
- First 3 experiments:
  1. Run extraction on a small WARC sample and compare token yield vs WET.
  2. Validate rule thresholds by sampling filtered vs unfiltered content.
  3. Test MinHash-LSH on a synthetic duplicate set to tune similarity threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does REDSTONE handle multilingual data extraction and filtering, and what are the performance differences compared to English-only processing?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on English language filtering using fastText but does not discuss multilingual capabilities or performance trade-offs.
- What evidence would resolve it: Experimental results comparing REDSTONE performance on multilingual vs English-only datasets, including quality metrics and processing efficiency.

### Open Question 2
- Question: What are the long-term effects of using REDSTONE-extracted datasets on LLM generalization across different domains?
- Basis in paper: [explicit] The paper demonstrates short-term effectiveness but does not address long-term generalization impacts.
- Why unresolved: The study focuses on immediate performance improvements but lacks longitudinal analysis of model behavior over time.
- What evidence would resolve it: Extended training and evaluation studies showing model performance stability and adaptation capabilities over multiple iterations.

### Open Question 3
- Question: How does REDSTONE's deduplication strategy affect dataset diversity and model performance compared to more aggressive deduplication methods?
- Basis in paper: [explicit] The paper describes their MinHash-LSH approach but does not compare it to alternative deduplication strategies.
- Why unresolved: The choice of deduplication parameters is presented without comparative analysis of their impact on final dataset quality.
- What evidence would resolve it: Comparative studies showing model performance differences when trained on datasets with varying deduplication intensities.

## Limitations
- The paper provides limited quantitative evidence for the effectiveness of its filtering and extraction mechanisms. While it claims significant improvements across multiple domains, specific performance metrics comparing REDSTONE against other datasets are not detailed.
- The domain-specific extraction logic relies heavily on HTML patterns, but the robustness of these patterns across diverse websites is not empirically validated. There is no mention of cross-domain accuracy or recall comparisons.
- The scalability and efficiency of the pipeline, particularly the model-based filtering stage, are not discussed in terms of computational cost or processing time.

## Confidence
- **High Confidence**: The overall pipeline architecture and the use of Common Crawl as a data source are well-established and align with existing literature.
- **Medium Confidence**: The claim that REDSTONE improves model performance is supported by the dataset's scale and diversity, but lacks direct comparative results.
- **Low Confidence**: The effectiveness of domain-specific extraction and the robustness of filtering mechanisms are not sufficiently validated with empirical data.

## Next Checks
1. **Quantitative Performance Comparison**: Conduct experiments to compare REDSTONE datasets against other open-source datasets on specific benchmarks (e.g., HumanEval, GSM8k, MMLU) to quantify improvements.
2. **Domain-Specific Extraction Validation**: Test the extraction patterns on a diverse set of websites to assess the recall and precision of domain-specific content retrieval.
3. **Filtering Mechanism Evaluation**: Analyze the impact of rule-based and model-based filtering on data quality by sampling filtered and unfiltered content, and measuring the reduction in noise versus the retention of relevant data.