---
ver: rpa2
title: 'The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism'
arxiv_id: '2407.10457'
source_url: https://arxiv.org/abs/2407.10457
tags:
- greedy
- sampling
- llms
- generation
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how non-determinism in large language model
  (LLM) generation affects performance. It systematically compares greedy decoding
  and sampling-based generation across seven popular benchmarks, showing that greedy
  decoding generally outperforms sampling methods for most tasks.
---

# The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism

## Quick Facts
- **arXiv ID**: 2407.10457
- **Source URL**: https://arxiv.org/abs/2407.10457
- **Reference count**: 11
- **Primary result**: Non-determinism significantly affects LLM performance; greedy decoding often outperforms sampling, and best-of-N sampling enables smaller models to match larger ones.

## Executive Summary
This paper systematically investigates how non-determinism in large language model (LLM) generation affects performance across seven popular benchmarks. Through comprehensive comparisons of greedy decoding versus sampling-based generation, the study reveals that greedy decoding generally produces more consistent and higher-scoring outputs for most tasks. Notably, the research demonstrates that alignment techniques like DPO can reduce sampling variance, and that smaller LLMs can achieve performance comparable to or exceeding GPT-4-Turbo when using best-of-N sampling with reward model selection. The findings highlight a critical gap in current LLM evaluation practices that often rely on single outputs per example, potentially underestimating model capabilities.

## Method Summary
The study evaluates multiple open-weight LLMs (Llama-3, Yi-1.5, Qwen2, Mistral) and GPT-4-Turbo across seven benchmarks (AlpacaEval 2, Arena-Hard, MixEval, WildBench v2, MMLU-Redux, GSM8K, HumanEval). Each example generates 16-128 completions using both greedy decoding and sampling (temperature=1.0, top-p=1.0). The evaluation framework includes zero-shot CoT for MMLU-Redux and standard metrics like win rate, accuracy, and length-controlled win rate. Additional analyses examine temperature effects, repetition penalties, and alignment method impacts (DPO, KTO, IPO, ORPO, RDPO, SimPO) on sampling variance and performance.

## Key Results
- Greedy decoding consistently outperforms average sampling performance across most benchmarks, with notable performance gaps observed
- Alignment techniques like DPO reduce sampling variance, leading to more consistent outputs across different LLM sizes
- Best-of-N sampling enables smaller LLMs (e.g., Llama-3-8B-Instruct) to match or surpass GPT-4-Turbo on MMLU, GSM8K, and HumanEval when using oracle selection
- Performance varies significantly across task types, with creative tasks showing different patterns than reasoning or knowledge-based tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Greedy decoding produces more consistent and higher-scoring outputs compared to sampling for most tasks, except in open-ended creative tasks.
- Mechanism: Greedy decoding deterministically selects the highest probability next token at each step, reducing variance in the generated output. Sampling introduces randomness, leading to diverse outputs but also higher variance in quality.
- Core assumption: The highest probability next token is often the most correct or appropriate continuation for the given context.
- Evidence anchors:
  - [abstract] "we observe that greedy decoding generally outperforms sampling methods for most evaluated tasks"
  - [section] "For most benchmarks we evaluated, a notable performance gap is observed between greedy generation and the average score of multiple sampling"
- Break condition: If the highest probability token is not actually the best choice (e.g., in tasks requiring creativity or avoiding repetition), greedy decoding may produce suboptimal outputs.

### Mechanism 2
- Claim: Alignment techniques like DPO reduce the variance in sampling outputs, making them more consistent.
- Mechanism: Alignment techniques train the model to prefer certain types of outputs based on human feedback. This reduces the range of acceptable outputs, leading to lower variance in sampling.
- Core assumption: Human feedback during alignment captures preferences that lead to more consistent and higher-quality outputs.
- Evidence anchors:
  - [abstract] "we also observe consistent performance across different LLM sizes and alignment methods, noting that alignment can reduce sampling variance"
  - [section] "We evaluate the effects of alignment methods such as DPO... a decrease in standard deviation is observed, suggesting that alignment may reduce the diversity of sampling outputs"
- Break condition: If the alignment process overfits to specific preferences or fails to capture the full range of acceptable outputs, it may reduce variance but also limit the model's ability to handle diverse inputs.

### Mechanism 3
- Claim: Best-of-N sampling can significantly improve the performance of smaller LLMs, allowing them to match or surpass larger models.
- Mechanism: By generating multiple samples and selecting the best one, best-of-N sampling exploits the non-deterministic nature of sampling to find high-quality outputs that may not be generated by greedy decoding.
- Core assumption: Among multiple sampled outputs, at least one will be of high quality, and this can be reliably identified.
- Evidence anchors:
  - [abstract] "our best-of-N sampling approach demonstrates that smaller LLMs can match or surpass larger models such as GPT-4-Turbo"
  - [section] "We observe a significant performance enhancement when applying simple best-of-N strategy for multiple sampled responses. Notably, with the oracle selection, even smaller LLMs like Llama-3-8B-Instruct can outperform GPT-4-Turbo on MMLU, GSM8K, and HumanEval"
- Break condition: If the selection process for the best output is unreliable or if the quality of sampled outputs is consistently low, best-of-N sampling may not provide significant improvements.

## Foundational Learning

- Concept: Non-determinism in LLM generation
  - Why needed here: The paper's core argument is that non-determinism significantly impacts LLM performance, and current evaluations often overlook this.
  - Quick check question: What is the difference between greedy decoding and sampling-based generation in terms of determinism?
- Concept: Alignment techniques (e.g., DPO)
  - Why needed here: The paper investigates how alignment techniques affect the variance in sampling outputs.
  - Quick check question: How do alignment techniques like DPO influence the distribution of generated outputs?
- Concept: Best-of-N sampling
  - Why needed here: The paper proposes best-of-N sampling as a way to improve the performance of smaller LLMs.
  - Quick check question: How does best-of-N sampling work, and what are its potential benefits and drawbacks?

## Architecture Onboarding

- Component map:
  - LLMs: Llama-3, Yi-1.5, Qwen2, Mistral, GPT-4-Turbo
  - Benchmarks: AlpacaEval 2, Arena-Hard, MixEval, WildBench v2, MMLU-Redux, GSM8K, HumanEval
  - Generation configurations: Greedy decoding, sampling (with temperature=1.0, top-p=1.0)
  - Alignment methods: DPO, KTO, IPO, ORPO, RDPO, SimPO
  - Evaluation metrics: Win rate, accuracy, length-controlled win rate, WB-Score

- Critical path: Generate outputs using different configurations → Evaluate outputs using benchmarks → Analyze results to answer research questions

- Design tradeoffs:
  - Greedy decoding vs. sampling: Consistency vs. diversity
  - Number of samples in best-of-N: Computational cost vs. performance improvement
  - Alignment strength: Reduced variance vs. potential overfitting

- Failure signatures:
  - Unexpected performance gaps between greedy and sampling
  - High variance in sampling outputs
  - Poor performance of best-of-N sampling

- First 3 experiments:
  1. Generate outputs for a benchmark using greedy decoding and sampling, compare performance.
  2. Apply an alignment technique (e.g., DPO) to an LLM, compare the variance in sampling outputs before and after alignment.
  3. Implement best-of-N sampling for a smaller LLM, compare its performance against a larger LLM using greedy decoding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms behind certain models exhibiting inverse behavior on specific tasks compared to others?
- Basis in paper: [explicit] The paper notes that some models like Mistral-7B-Instruct-v0.2 and Llama-3-8B-Instruct display inverse behavior on tasks like AlpacaEval and Arena-Hard compared to other models.
- Why unresolved: The paper identifies this as an intriguing phenomenon but does not delve into the underlying mechanisms or reasons for these unique characteristics.
- What evidence would resolve it: Conducting detailed analyses of model architectures, training data, and alignment techniques to identify correlations with the observed inverse behaviors.

### Open Question 2
- Question: How can the unique characteristics of certain models be leveraged to develop more robust language models?
- Basis in paper: [inferred] The paper mentions that these observations raise intriguing questions for future research, suggesting potential applications in model development.
- Why unresolved: While the paper identifies unique model behaviors, it does not explore practical applications or methods to utilize these characteristics for model improvement.
- What evidence would resolve it: Developing new model architectures or training techniques that specifically target and enhance the unique characteristics observed in certain models.

### Open Question 3
- Question: What is the optimal balance between response quality and diversity in non-deterministic generation for different task categories?
- Basis in paper: [explicit] The paper discusses the performance gap between greedy decoding and sampling, and how temperature and repetition penalty affect generation, but does not determine an optimal balance for different tasks.
- Why unresolved: The paper provides insights into how different parameters affect generation but does not establish guidelines for balancing quality and diversity across various task types.
- What evidence would resolve it: Conducting extensive experiments across a wide range of tasks with varying parameter settings to identify optimal configurations for different task categories.

## Limitations
- Benchmark-based evaluations may not fully capture real-world performance variations and practical deployment scenarios.
- The study focuses primarily on open-weight models, potentially limiting generalizability to proprietary models with different architectures.
- Specific hyperparameters (temperature=1.0, top-p=1.0) for sampling may not represent optimal settings for all tasks or models.

## Confidence
- **High Confidence**: The general observation that greedy decoding outperforms average sampling performance across most benchmarks is well-supported by the systematic comparison approach and clear performance gaps reported.
- **Medium Confidence**: Claims about alignment techniques reducing sampling variance are supported but could benefit from more detailed analysis of which specific alignment methods are most effective.
- **Medium Confidence**: The assertion that best-of-N sampling enables smaller models to match larger ones requires further validation, as the effectiveness likely depends heavily on the quality of the selection mechanism and the specific tasks involved.

## Next Checks
1. **Reproduce benchmark results** using the exact hyperparameters and evaluation procedures specified, particularly focusing on the zero-shot CoT implementation for MMLU-Redux and the alignment method configurations to verify reported performance gaps.
2. **Test temperature sensitivity** by running the same experiments with multiple temperature values (e.g., 0.5, 0.7, 1.5) to determine if the observed patterns hold across a broader range of sampling configurations.
3. **Validate best-of-N selection quality** by comparing oracle selection results against heuristic-based selection methods to assess whether the performance gains for smaller models are reproducible without perfect selection criteria.