---
ver: rpa2
title: Beyond the Boundaries of Proximal Policy Optimization
arxiv_id: '2411.00666'
source_url: https://arxiv.org/abs/2411.00666
tags:
- outer
- learning
- rate
- momentum
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces outer-PPO, a framework that reinterprets
  PPO as estimating outer gradients and applying them with arbitrary optimizers. By
  decoupling update estimation and application, the authors challenge three implicit
  PPO design choices: unity learning rates, lack of momentum, and unbiased initialization.'
---

# Beyond the Boundaries of Proximal Policy Optimization

## Quick Facts
- arXiv ID: 2411.00666
- Source URL: https://arxiv.org/abs/2411.00666
- Authors: Charlie B. Tan; Edan Toledo; Benjamin Ellis; Jakob N. Foerster; Ferenc Huszár
- Reference count: 40
- Key outcome: Outer-PPO framework with non-unity learning rates and momentum improves PPO performance by 5-10% on Brax and Jumanji tasks

## Executive Summary
This paper introduces outer-PPO, a framework that reinterprets PPO's update mechanism as estimating outer gradients that can be applied with arbitrary optimizers. By decoupling update estimation from application, the authors challenge three implicit PPO design choices: unity learning rates, lack of momentum, and unbiased initialization. Through extensive hyperparameter sweeps on Brax, Jumanji, and MinAtar environments, they demonstrate that non-unity outer learning rates and momentum both yield statistically significant improvements on Brax and Jumanji tasks, with gains of 5-10% over a strongly-tuned PPO baseline.

## Method Summary
The authors implement outer-PPO by first creating an aggressively-tuned PPO baseline through 600 hyperparameter trials per task. They then evaluate three outer-PPO variants - non-unity outer learning rates, outer Nesterov momentum, and biased initialization - using grid searches with 40-100 trials each while keeping the base PPO hyperparameters fixed. All experiments use a 1×10^7 transition budget and evaluate performance through normalized mean episode returns across tasks.

## Key Results
- Non-unity outer learning rates (σ ≠ 1) achieve statistically significant improvements on Brax and Jumanji environments
- Outer-Nesterov momentum improves performance on Brax and Jumanji, with gains exceeding 10% on some tasks
- Biased initialization shows improvements on Jumanji but limited effects on other environments
- All outer-PPO variants maintain stability and do not degrade performance compared to the tuned baseline

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Update Estimation and Application
- Claim: Decoupling update estimation from application enables independent tuning of gradient magnitude and direction
- Core assumption: The outer gradient estimated by the inner loop is a reliable direction for improvement
- Break condition: If outer gradients are highly noisy or biased, scaling them could amplify errors

### Mechanism 2: Momentum Smoothing
- Claim: Momentum in the outer loop smooths noisy gradient estimates and increases effective step size
- Core assumption: Outer gradients contain consistent underlying direction that momentum can capture
- Break condition: If outer gradients are highly correlated with noise, momentum could amplify this noise

### Mechanism 3: Leveraging Prior Trajectory Information
- Claim: Biased initialization leverages prior trajectory information to improve inner-loop optimization efficiency
- Core assumption: The outer trajectory contains useful information about the optimal update direction
- Break condition: If the outer trajectory is unreliable, momentum-based initialization could degrade performance

## Foundational Learning

- Concept: Proximal Policy Optimization and trust regions
  - Why needed here: Understanding how PPO works and what trust regions are is essential to grasp why outer-PPO is effective
  - Quick check question: What is the purpose of the clipping parameter ε in PPO's surrogate objective?

- Concept: Gradient-based optimization and learning rates
  - Why needed here: Outer-PPO applies arbitrary gradient-based optimizers to the outer loop
  - Quick check question: How does increasing the learning rate in gradient ascent affect convergence and stability?

- Concept: Momentum in optimization
  - Why needed here: Outer-Nesterov applies momentum to the outer loop
  - Quick check question: What problem does momentum solve in optimization, and how does it do so?

## Architecture Onboarding

- Component map: Inner loop -> Estimate outer gradient -> Outer loop (optimizer) -> Update behavior policy
- Critical path:
  1. Collect dataset using current behavior policy
  2. Estimate advantages using GAE
  3. Solve surrogate objective to estimate outer gradient
  4. Apply outer gradient using chosen optimizer
  5. Update behavior policy

- Design tradeoffs:
  - Unity vs non-unity outer learning rate: Unity maintains trust region but may be too conservative
  - No momentum vs momentum: No momentum treats updates independently; momentum smooths noise but can amplify it
  - Standard vs biased initialization: Standard starts fresh; biased leverages prior information but risks inaccuracy

- Failure signatures:
  - Divergence or instability: Could indicate outer learning rate too high or momentum amplifying noise
  - Poor performance despite convergence: Could indicate unreliable outer gradients or poor initialization

- First 3 experiments:
  1. Compare standard PPO with outer-PPO using non-unity learning rate on Pendulum-v1
  2. Compare outer-PPO with and without momentum on a noisy environment
  3. Compare biased initialization with standard initialization on a task with long horizons

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do outer-PPO methods interact with and depend on base PPO hyperparameters?
- Basis in paper: The authors acknowledge interaction between base and outer-PPO hyperparameters but only fixed base parameters during experiments
- Why unresolved: Authors didn't explore simultaneous optimization of both parameter sets
- What evidence would resolve it: Experiments simultaneously optimizing both base PPO and outer-PPO hyperparameters

### Open Question 2
- Question: What is the asymptotic performance of outer-PPO methods with larger transition budgets?
- Basis in paper: Authors only consider 1×10^7 transition budget and note improvement may diminish with more data
- Why unresolved: Experiments constrained to fixed budget without investigating scaling behavior
- What evidence would resolve it: Experiments with progressively larger budgets (2×10^7, 5×10^7, 1×10^8 steps)

### Open Question 3
- Question: How does learning rate annealing in inner optimization affect outer gradients and performance?
- Basis in paper: Authors note presence of learning rate annealing but didn't investigate its effects
- Why unresolved: Authors acknowledge this factor but didn't explore its impact
- What evidence would resolve it: Experiments comparing outer-PPO performance with and without inner loop learning rate annealing

## Limitations

- Improvements are most pronounced on Brax continuous control tasks, with mixed results on Jumanji and MinAtar
- The paper focuses specifically on PPO, leaving unclear generalizability to other RL algorithms
- Only considers a fixed transition budget of 1×10^7 steps, not asymptotic performance

## Confidence

- **High confidence**: Outer-PPO framework correctly reinterprets PPO's update mechanism; non-unity outer learning rates improve Brax performance
- **Medium confidence**: Momentum and biased initialization improvements are statistically significant but less consistent across domains
- **Low confidence**: Generalizability to other RL algorithms beyond PPO; robustness to different reward scales and task difficulties

## Next Checks

1. Test outer-PPO variants on additional continuous control benchmarks (DM Control Suite) to verify Brax-specific improvements don't reflect dataset-specific effects
2. Implement ablation studies comparing outer-PPO with other policy optimization methods (TRPO, SAC) to assess generalizability
3. Conduct sensitivity analysis on reward scaling and task difficulty to determine which characteristics make tasks amenable to outer-PPO improvements