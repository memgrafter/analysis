---
ver: rpa2
title: Deep Neural Network Benchmarks for Selective Classification
arxiv_id: '2401.12708'
source_url: https://arxiv.org/abs/2401.12708
tags:
- 'true'
- selnet
- 'false'
- adam
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper benchmarks 18 selective classification methods across
  44 datasets, including both image and tabular data, to evaluate their performance
  in terms of selective error rate, coverage, rejection bias, and robustness to out-of-distribution
  data. Results show no single method dominates across all criteria, with ensemble-based
  approaches like ENS+SR generally performing well, but with statistical significance
  varying by dataset and target coverage.
---

# Deep Neural Network Benchmarks for Selective Classification

## Quick Facts
- arXiv ID: 2401.12708
- Source URL: https://arxiv.org/abs/2401.12708
- Reference count: 40
- Primary result: No single selective classification method dominates across all criteria; ENS+SR generally performs well but statistical significance varies by dataset and target coverage

## Executive Summary
This paper presents a comprehensive benchmark of 18 selective classification methods across 44 datasets (20 image, 24 tabular), evaluating performance in terms of selective error rate, coverage, rejection bias, and out-of-distribution robustness. The study reveals that while ensemble-based approaches like ENS+SR generally achieve lower error rates, no method consistently outperforms others across all metrics and dataset types. Most methods exhibit bias against minority classes, except for PLUGINAUC and AUCROSS, and all methods struggle to reject out-of-distribution instances effectively.

## Method Summary
The study benchmarks 18 selective classification methods on 44 datasets using a standardized experimental pipeline. Each dataset is split into 60% training, 20% calibration, 10% validation, and 10% test sets. Neural networks are trained for 300 epochs with Optuna-based hyperparameter optimization. Confidence functions are calibrated on the calibration set, and performance is evaluated on bootstrapped test sets using metrics including relative error rate, coverage violations, class distribution bias (MinCoeff), and out-of-distribution robustness. The study uses a single neural network architecture per dataset to reduce computational cost.

## Key Results
- ENS+SR consistently achieves the lowest relative error rate across target coverages and dataset types
- Coverage violations are generally small across all methods
- Most methods reject more minority class instances, except PLUGINAUC and AUCROSS
- All methods fail to consistently reject out-of-distribution instances
- Switching from bounded-abstention to bounded-improvement models via SGR improves coverage but struggles under strict error constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ENS+SR consistently achieves the lowest relative error rate across all target coverages and dataset types.
- Mechanism: Ensemble methods combine predictions from multiple independently trained neural networks. Averaging their softmax responses reduces variance in confidence estimates, leading to better selective classification performance.
- Core assumption: Individual network predictions are sufficiently diverse and uncorrelated so that averaging reduces overall uncertainty.
- Evidence anchors: Abstract states "Results show no single method dominates across all criteria, with ensemble-based approaches like ENS+SR generally performing well"; section 3.3 describes ENS+SR using average softmax response from ensemble networks as confidence measure; corpus shows ENS+SR listed among top performing methods in confidence-aware contrastive learning paper.

### Mechanism 2
- Claim: Learn-to-Abstain methods (DG, SAT) often underperform because adding an abstention class can lead to overfitting on easier examples.
- Mechanism: These methods modify the training loss to include an extra abstention class, encouraging the model to assign higher scores to uncertain instances. However, this can cause the model to over-reject easy examples and under-reject hard ones.
- Core assumption: The abstention class provides useful signal about uncertainty that generalizes beyond training distribution.
- Evidence anchors: Section 3.1 explains learn-to-abstain methods add abstention class and use it as confidence function; abstract notes "coverage violations are generally small" suggesting methods do learn to reject appropriately; corpus includes learn-to-abstain approaches in comparative evaluations.

### Mechanism 3
- Claim: PLUGINAUC and AUCROSS maintain balanced rejection rates across classes by design, unlike other methods that bias against minority classes.
- Mechanism: These methods explicitly optimize for AUC-based selective classification, which considers both minority and majority class performance when determining rejection thresholds.
- Core assumption: AUC-based optimization naturally balances the trade-off between rejecting minority and majority class instances.
- Evidence anchors: Section 3.3 describes PLUGINAUC and AUCROSS as specifically designed for binary classification with imbalanced classes; section 5.2 Q3 results show these two methods have MinCoeff close to 1 (no bias), while others show clear minority class bias; corpus mentions these methods in context of fairness-aware selective classification.

## Foundational Learning

- Concept: Confidence function calibration
  - Why needed here: All selective classification methods rely on confidence functions to decide when to reject predictions. Poor calibration leads to either too many or too few rejections.
  - Quick check question: Given a classifier's softmax outputs on test data, how would you determine the optimal threshold for a target coverage of 90%?

- Concept: Statistical significance testing in model comparison
  - Why needed here: The paper uses Friedman and Nemenyi tests to determine if performance differences between methods are statistically significant across multiple datasets.
  - Quick check question: If Method A has average rank 3.2 and Method B has average rank 4.8 across 10 datasets, what additional information would you need to determine if this difference is statistically significant?

- Concept: Out-of-distribution detection vs selective classification
  - Why needed here: The paper evaluates how selective classification methods handle data distribution shifts, which is related but distinct from pure OOD detection.
  - Quick check question: What's the key difference between rejecting a prediction because you're uncertain about its correctness versus rejecting because the input is unlike anything seen during training?

## Architecture Onboarding

- Component map: Data preprocessing -> Dataset splitting (60/20/10/10%) -> Neural network training (300 epochs) -> Confidence function calibration -> Evaluation with bootstrapping
- Critical path: For each dataset-baseline combination: (1) hyperparameter tuning on training+calibration+validation sets, (2) final training on training set, (3) confidence calibration on calibration set, (4) evaluation on bootstrapped test sets
- Design tradeoffs: The paper uses a single architecture per dataset to reduce computational cost, but this may limit generalizability. The choice of 60/10/10/20% split balances training data needs with calibration requirements.
- Failure signatures: If coverage violations are large (>10%), the calibration procedure likely failed. If minority class MinCoeff is << 1, the method introduces class bias. If all methods reject almost nothing under distribution shift, the confidence functions are not OOD-aware.
- First 3 experiments:
  1. Run a single baseline (e.g., SR) on one dataset with c=0.9 and verify it achieves expected coverage and error rate.
  2. Compare two methods (e.g., SR vs ENS+SR) on the same dataset to confirm ENS+SR achieves lower error rate.
  3. Test coverage calibration by running SR with different target coverages (0.7, 0.9, 0.99) and plotting achieved vs target coverage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do selective classification methods perform when evaluated on larger datasets (e.g., >300k instances) or different data modalities (e.g., text, audio, time series)?
- Basis in paper: The authors acknowledge that their study is limited to datasets not exceeding ~300k instances and only considers image and tabular data, noting that their results may not generalize to other data types or sizes.
- Why unresolved: The paper prioritizes variety of data over size and does not evaluate methods on larger or different data modalities due to computational constraints.
- What evidence would resolve it: Empirical results on larger datasets and diverse data modalities would show whether the observed trends (e.g., no clear winner, coverage violations) hold or differ in these settings.

### Open Question 2
- Question: Can selective classification methods be effectively combined with novelty rejection techniques to improve out-of-distribution rejection?
- Basis in paper: The authors observe that current SC methods fail to consistently reject out-of-distribution instances and suggest that mixing ambiguity rejection with novelty rejection methods could mitigate these issues.
- Why unresolved: The paper focuses on benchmarking existing SC methods and does not explore hybrid approaches that integrate novelty rejection.
- What evidence would resolve it: Experimental results comparing hybrid SC+novelty rejection methods against standard SC baselines on out-of-distribution tasks would demonstrate their effectiveness.

### Open Question 3
- Question: What are the optimal strategies for the bounded-improvement model (maximizing coverage under error constraints) beyond using SGR?
- Basis in paper: The authors evaluate SGR for switching to the bounded-improvement model but note that it often fails under strict error constraints, suggesting a need for methods specifically designed for this setting.
- Why unresolved: SGR is the only method explicitly designed for the bounded-improvement model, and the paper finds it inadequate for very low error rates, indicating a gap in the literature.
- What evidence would resolve it: Development and benchmarking of new methods tailored to the bounded-improvement model, showing improved coverage under strict error constraints, would address this gap.

### Open Question 4
- Question: How do selective classification methods perform when using tree-based models instead of neural networks for tabular data?
- Basis in paper: The authors note that tree-based models are state-of-the-art for tabular data and that model-agnostic SC methods could benefit from using other base classifiers, but their study focuses on neural networks for fairness.
- Why unresolved: The paper uses neural networks consistently to ensure fair comparison among methods that assume deep learning architectures, leaving the performance of tree-based models unexplored.
- What evidence would resolve it: Benchmarking SC methods with tree-based classifiers on tabular datasets would reveal if performance differences exist compared to neural network-based approaches.

### Open Question 5
- Question: How do selective classification methods behave under milder distribution shifts compared to extreme random pixel shifts?
- Basis in paper: The authors test SC methods on extreme out-of-distribution data (random pixel images) and milder shifts using OpenOOD benchmark datasets, observing that no method consistently rejects all shifted instances.
- Why unresolved: While the paper provides initial results on milder shifts, it does not deeply analyze the behavior of SC methods across a spectrum of shift intensities or develop shift-aware methods.
- What evidence would resolve it: Comprehensive evaluation of SC methods across varying degrees of distribution shifts, along with the development of shift-aware selection functions, would clarify their robustness.

## Limitations

- The study's evaluation reveals significant limitations in selective classification methods' ability to handle out-of-distribution data, with all methods failing to consistently reject OOD instances
- Performance variability across datasets and target coverages suggests that no single method is universally superior, making method selection highly dependent on specific application requirements
- The computational cost of ensemble-based methods like ENS+SR may limit their practical deployment despite their strong performance

## Confidence

- **High Confidence**: ENS+SR's consistent strong performance across multiple datasets and coverages; the general trend of coverage violations being small; the finding that most methods reject more minority class instances
- **Medium Confidence**: The specific ranking of methods across all datasets; the statistical significance of performance differences; the exact magnitude of coverage violations for each method
- **Low Confidence**: The generalizability of results to datasets not included in the study; the performance of methods under extreme class imbalance; the behavior of methods with different train/calibration splits

## Next Checks

1. **OOD Detection Validation**: Test selective classification methods specifically on established OOD benchmarks (e.g., CIFAR-10 vs CIFAR-100, SVHN vs CIFAR-10) to quantify their OOD rejection capabilities independently of in-distribution performance.

2. **Class Balance Stress Test**: Evaluate all methods on datasets with extreme class imbalance (e.g., 1:99 minority:majority ratio) to verify the robustness of PLUGINAUC and AUCROSS's balanced rejection claims.

3. **Calibration Data Sensitivity**: Repeat key experiments with varying calibration set sizes (5%, 10%, 20% of training data) to assess how sensitive coverage calibration is to calibration data availability.