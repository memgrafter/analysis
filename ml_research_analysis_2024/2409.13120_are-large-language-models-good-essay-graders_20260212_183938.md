---
ver: rpa2
title: Are Large Language Models Good Essay Graders?
arxiv_id: '2409.13120'
source_url: https://arxiv.org/abs/2409.13120
tags:
- effect
- size
- large
- mention
- rater
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates Large Language Models (LLMs) for automated
  essay scoring (AES), comparing their grades with human raters. ChatGPT and Llama
  are tested on the ASAP dataset using zero-shot and few-shot learning with various
  prompts.
---

# Are Large Language Models Good Essay Graders?
## Quick Facts
- arXiv ID: 2409.13120
- Source URL: https://arxiv.org/abs/2409.13120
- Reference count: 40
- Primary result: LLMs generally assign lower scores than human raters with weak correlations between LLM and human scores

## Executive Summary
This study evaluates Large Language Models (LLMs) for automated essay scoring (AES), comparing their grades with human raters. ChatGPT and Llama are tested on the ASAP dataset using zero-shot and few-shot learning with various prompts. Results show both LLMs generally assign lower scores than human raters, with weak correlations between LLM and human scores. ChatGPT is harsher and more misaligned than Llama. LLM scores negatively correlate with spelling and grammar mistakes, unlike human scores. Llama-3, a newer model, shows better alignment with human scoring. While LLMs can detect language errors and provide self-consistent explanations, they do not fully replicate human grading.

## Method Summary
The study uses the ASAP dataset containing 13,000 essays from grades 7-10 with human rater scores and rubric guidelines. ChatGPT and Llama models are prompted to generate scores and explanations for essays using zero-shot and few-shot learning approaches. Correlation analysis measures alignment between LLM-generated scores and human scores. Features are extracted from essays and LLM explanations, including spelling/grammar mistakes detected by external tools and sentiment analysis of explanations.

## Key Results
- Both LLMs assign consistently lower scores than human raters
- Weak correlations (below 0.7) between LLM and human scores indicate limited alignment
- LLM scores negatively correlate with spelling and grammar mistakes, unlike human scores
- Llama-3 shows better alignment with human scoring than earlier models

## Why This Works (Mechanism)
### Mechanism 1
- Claim: LLMs can perform zero-shot essay scoring without fine-tuning if given a rubric.
- Mechanism: In-context learning allows the model to map rubric descriptions to essay features through prompt conditioning.
- Core assumption: The rubric semantics are fully captured in the prompt and the model has pre-trained knowledge to interpret them.
- Evidence anchors:
  - [abstract] "We consider both zero-shot and few-shot learning and different prompting approaches."
  - [section] "In this study, we examine the performance of two popular LLMs: OpenAI's ChatGPT and Meta's Llama as automated essay scoring tools."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.391. [weak evidence]
- Break condition: If rubric semantics are ambiguous or overly complex, the model cannot reliably align its scoring.

### Mechanism 2
- Claim: LLMs reliably detect spelling and grammar errors but may not reflect them in scores the same way humans do.
- Mechanism: Language models trained on large corpora encode grammar and spelling patterns, enabling error detection; however, scoring logic may differ from human grading priorities.
- Core assumption: Error detection is a learned capability separate from holistic essay quality assessment.
- Evidence anchors:
  - [abstract] "LLM scores negatively correlate with spelling and grammar mistakes, unlike human scores."
  - [section] "Both LLMs not only can reliably detect spelling and grammar mistakes but also seem to take those mistakes into account when computing their score."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.391. [weak evidence]
- Break condition: If the model is tuned or prompted to ignore error patterns, the correlation will weaken.

### Mechanism 3
- Claim: LLM explanations can align with their numeric scores but tone may differ from human feedback.
- Mechanism: The model generates textual explanations conditioned on its internal scoring decision, allowing for self-consistency.
- Core assumption: The explanation generation is a direct consequence of the scoring process and not an independent judgment.
- Evidence anchors:
  - [abstract] "While LLMs do not seem an adequate replacement for human grading, our results are somewhat encouraging for their use as a tool to assist humans in the grading of written essays in the future."
  - [section] "ChatGPT often delivered explanations in a harsh tone, whereas Llama used generally less negative language."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.391. [weak evidence]
- Break condition: If the explanation generation is decoupled from the scoring process, consistency will break.

## Foundational Learning
- Concept: In-context learning
  - Why needed here: The study relies on prompting LLMs with rubrics and essays without fine-tuning.
  - Quick check question: Can the model generalize to a scoring task it hasn't been explicitly trained on?
- Concept: Correlation analysis
  - Why needed here: Comparing LLM scores to human scores requires measuring statistical alignment.
  - Quick check question: Does a weak correlation imply the model is wrong, or just differently calibrated?
- Concept: Sentiment analysis of generated text
  - Why needed here: To assess whether the tone of LLM explanations matches the assigned score.
  - Quick check question: Can VADER sentiment reliably capture the nuance of educational feedback?

## Architecture Onboarding
- Component map: Rubric + essay prompt -> LLM inference -> Score extractor + explanation parser -> Feature extractor -> Statistical analyzer
- Critical path: Rubric + essay prompt → LLM → numeric score + explanation
- Design tradeoffs:
  - Zero-shot (no fine-tuning) vs. few-shot (add examples): zero-shot is faster but less aligned; few-shot improves alignment but increases prompt length.
  - Using external spell checkers (Aspell, LanguageTool) vs. LLM internal error detection: external tools are deterministic; LLM detection may vary.
- Failure signatures:
  - Out-of-range scores (e.g., Llama giving score 26 in a 0-15 range)
  - Low correlation with human scores (< 0.3 Pearson r)
  - Mismatch between explanation tone and score magnitude
- First 3 experiments:
  1. Vary prompt length and complexity to see impact on score consistency.
  2. Compare external spell checkers vs. LLM-detected errors for correlation.
  3. Test with a rubric that has very specific scoring criteria to assess rubric adherence.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can LLMs be fine-tuned or prompted to better align with human grading rubrics and reduce the disparity between human and LLM scores?
- Basis in paper: [explicit] The paper discusses the significant difference between LLM and human scores, suggesting that LLMs may have a different understanding of the rubric. It also mentions the potential for fine-tuning models or prompting LLMs with more annotated scores and explanations data by human raters to better mimic human grading practices.
- Why unresolved: While the paper explores the performance of LLMs in AES tasks, it does not investigate the impact of fine-tuning or more sophisticated prompting strategies on improving alignment with human grading.
- What evidence would resolve it: Conducting experiments to compare the performance of fine-tuned LLMs or LLMs with enhanced prompts against human grading on the same dataset, measuring the correlation between their scores and human scores.

### Open Question 2
- Question: How do LLM-generated explanations impact student learning and writing improvement compared to human feedback?
- Basis in paper: [inferred] The paper mentions that offering feedback is crucial for students to learn from their mistakes and develop their writing skills. It also discusses the explanations provided by LLMs and their potential as feedback tools, but does not investigate their impact on student learning.
- Why unresolved: The paper focuses on the alignment between LLM scores and human scores, but does not explore the educational implications of LLM-generated feedback on student performance.
- What evidence would resolve it: Conducting a study where students receive feedback from both LLMs and human graders, and measuring the impact of each type of feedback on student writing improvement and learning outcomes.

### Open Question 3
- Question: How do different LLM architectures and training data influence their performance in AES tasks, and can newer models like GPT-4 or Gemini outperform Llama-3 in terms of alignment with human grading?
- Basis in paper: [explicit] The paper compares the performance of ChatGPT and Llama in AES tasks and mentions the potential for future work to include comparisons with other models like Google Gemini. It also discusses the performance of Llama-3, a newer model, which shows improvements over ChatGPT and Llama-2.
- Why unresolved: The paper provides a limited comparison of LLM architectures and does not explore the full range of available models or their potential for improved performance in AES tasks.
- What evidence would resolve it: Conducting a comprehensive comparison of various LLM architectures and training data on the same AES dataset, measuring their correlation with human scores and analyzing their strengths and weaknesses in different aspects of essay grading.

## Limitations
- Both LLMs consistently assign lower scores than human raters, with correlation coefficients well below the acceptable threshold of 0.7 for practical AES applications
- Llama occasionally generates scores outside the valid range (e.g., assigning 26 in a 0-15 scale)
- The study uses only one dataset (ASAP) and focuses on English essays from middle school students, limiting generalizability

## Confidence
- **High confidence**: LLMs can detect spelling and grammar errors reliably; both models show self-consistent scoring explanations.
- **Medium confidence**: Zero-shot and few-shot learning approaches work but with limited correlation to human scores; newer models like Llama-3 show better alignment.
- **Low confidence**: LLMs can serve as adequate replacements for human graders in practical AES applications; current models are ready for deployment in educational settings.

## Next Checks
1. **Cross-dataset validation**: Test the same LLM models and prompts on additional AES datasets (e.g., from different educational systems, languages, or grade levels) to assess generalizability.

2. **Human-in-the-loop evaluation**: Conduct controlled studies where human graders use LLM-generated scores and explanations as input, measuring whether this assistance improves grading efficiency or consistency compared to solo human grading.

3. **Correlation threshold analysis**: Systematically vary the correlation threshold for "acceptable" AES performance and determine what minimum correlation would make LLM assistance practically useful in educational settings, then test whether any prompt engineering or model combination approaches can achieve this threshold.