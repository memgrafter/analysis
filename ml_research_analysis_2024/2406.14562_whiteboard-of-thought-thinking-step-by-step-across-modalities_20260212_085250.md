---
ver: rpa2
title: 'Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities'
arxiv_id: '2406.14562'
source_url: https://arxiv.org/abs/2406.14562
tags:
- visual
- reasoning
- arxiv
- text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of enabling multimodal large
  language models (MLLMs) to perform visual and spatial reasoning tasks, which they
  often struggle with even when given text-based reasoning instructions. The core
  method, "whiteboard-of-thought" (WoT), allows MLLMs to generate code that creates
  visualizations of intermediate reasoning steps using libraries like Matplotlib or
  Turtle, and then uses their visual input capabilities to process these images and
  produce final answers.
---

# Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities

## Quick Facts
- arXiv ID: 2406.14562
- Source URL: https://arxiv.org/abs/2406.14562
- Authors: Sachit Menon; Richard Zemel; Carl Vondrick
- Reference count: 24
- Primary result: WoT significantly outperforms text-based CoT prompting on visual reasoning tasks, achieving up to 92% accuracy vs 0% for CoT on ASCII art recognition

## Executive Summary
The paper introduces "whiteboard-of-thought" (WoT), a method that enables multimodal large language models to perform visual and spatial reasoning tasks by generating code to create intermediate visualizations. Instead of relying on text-based chain-of-thought reasoning, WoT leverages the model's ability to write code with libraries like Matplotlib or Turtle, executes this code to create images, and then uses the model's visual input capabilities to process these images and produce final answers. The approach demonstrates state-of-the-art results on four challenging tasks that involve visual and spatial reasoning, including ASCII art recognition, spatial navigation across different geometries, and two BIG-Bench tasks.

## Method Summary
The method uses MLLMs to generate code that creates visualizations of intermediate reasoning steps, then processes the resulting images to produce final answers. The approach is self-contained, leveraging existing code-writing capabilities without requiring demonstrations or specialized modules. WoT addresses the limitation that text-based chain-of-thought reasoning is insufficient for tasks requiring spatial or visual understanding, as MLLMs can generate actual visual representations that unlock their latent visual reasoning abilities.

## Key Results
- Achieved 92% accuracy on ASCII art recognition compared to 0% with text-based CoT
- Scored 61% accuracy on spatial navigation tasks versus 8% with CoT on hexagonal geometries
- Successfully solved previously unsolvable problems like calligram understanding and video game art recognition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual reasoning tasks require visual processing, and MLLMs can generate code to create intermediate visualizations that unlock this capability.
- Mechanism: The model generates Python code using libraries like Matplotlib or Turtle to visualize intermediate reasoning steps, then processes the resulting images to produce final answers.
- Core assumption: MLLMs possess latent visual reasoning abilities that can be activated through self-generated visualizations, even without specialized training.
- Evidence anchors: Strong evidence from related work discussing similar approaches like "Thinking with Generated Images" and "VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search."

### Mechanism 2
- Claim: Text-based chain-of-thought reasoning is insufficient for tasks requiring spatial or visual understanding.
- Mechanism: CoT fails on visual reasoning tasks because it cannot create or manipulate mental images, while WoT bypasses this limitation by generating actual visual representations.
- Core assumption: Visual reasoning requires different cognitive processes than text-based reasoning, and these processes cannot be adequately expressed through text alone.
- Evidence anchors: GPT-4o achieving "0% accuracy" on some visual reasoning tasks, with text-based reasoning notions quickly falling apart upon deeper inspection.

### Mechanism 3
- Claim: The self-contained nature of WoT, using only the model's existing capabilities, makes it more practical than approaches requiring external tools or specialized modules.
- Mechanism: By leveraging the model's ability to write code and process images, WoT avoids the complexity of integrating external APIs or training specialized modules.
- Core assumption: The existing capabilities of MLLMs are sufficient for the core functions needed by WoT, and adding external dependencies would not significantly improve performance.
- Evidence anchors: WoT shows state-of-the-art results while obviating the need for external tools or handcrafted visual modules.

## Foundational Learning

- **Multimodal large language models (MLLMs)**: Understanding how MLLMs process different input modalities (text, images) is crucial for grasping why WoT works. *Quick check*: What are the key differences between how MLLMs process text tokens versus image tokens?

- **Chain-of-thought (CoT) reasoning**: CoT is the primary baseline against which WoT is compared, and understanding its limitations helps explain WoT's advantages. *Quick check*: Why does CoT fail on visual reasoning tasks that humans solve easily?

- **Python visualization libraries (Matplotlib, Turtle)**: These libraries are the primary tools used by WoT to generate intermediate visualizations. *Quick check*: How do Matplotlib and Turtle differ in their capabilities for creating visualizations suitable for reasoning tasks?

## Architecture Onboarding

- **Component map**: Input query -> MLLM code generation -> Python interpreter -> Visualization creation -> Image processing by MLLM -> Final answer generation

- **Critical path**: 1) Query received by MLLM 2) MLLM generates visualization code 3) Code executed to create image 4) Image sent back to MLLM 5) MLLM processes image and generates final answer

- **Design tradeoffs**: Simplicity vs. specialized modules (WoT uses existing capabilities rather than custom modules); Self-containment vs. external tools (no need for external APIs but limited by MLLM capabilities); Code generation vs. direct visualization (relies on code generation which may introduce errors)

- **Failure signatures**: No code generation or invalid code; Code execution errors; Poor visualization quality; MLLM fails to interpret generated images correctly

- **First 3 experiments**: 1) Replicate ASCII art recognition results from Table 1 to verify basic functionality 2) Test spatial navigation tasks from Table 3 to verify geometric reasoning capabilities 3) Attempt calligram understanding task to verify more complex visual reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can WoT effectively handle geometric diagrams and mathematical visualizations, given the current limitations of MLLMs in understanding detailed geometric figures?
- Basis in paper: The paper explicitly mentions that MLLMs struggle with understanding detailed geometric figures, citing related work on this topic.
- Why unresolved: The current WoT method relies on the MLLM's ability to process the generated visualizations. If the MLLM cannot accurately understand complex geometric diagrams, WoT's performance will be limited in these domains.
- What evidence would resolve it: Testing WoT on a benchmark of geometric problems and mathematical diagrams, comparing its performance to existing methods, and analyzing the sources of error in cases where WoT fails.

### Open Question 2
- Question: Can WoT be extended to use text-to-image models for creating more precise and diverse visualizations, beyond the current limitations of Matplotlib and Turtle?
- Basis in paper: The paper discusses the potential of using text-to-image models in the discussion section, noting that they could be integrated into WoT as they improve in creating precise figures and drawings suitable for visual reasoning.
- Why unresolved: Current text-to-image models, while capable of creating diverse and high-quality images, still struggle to produce the precise and abstract visualizations needed for WoT's intermediate reasoning steps.
- What evidence would resolve it: Developing a method to effectively control text-to-image models to generate the specific types of visualizations needed for WoT, and evaluating its performance on the tasks presented in the paper compared to the current approach.

### Open Question 3
- Question: How does the performance of WoT scale with the complexity of the visual reasoning tasks, and what are the limitations of the current method in handling increasingly complex problems?
- Basis in paper: The paper demonstrates WoT's effectiveness on a range of tasks, but does not explore its limitations in handling extremely complex visual reasoning problems that may require more sophisticated visualizations or multiple iterations of image generation and processing.
- Why unresolved: The paper focuses on tasks that can be solved with relatively simple visualizations, and does not provide insights into how WoT would perform on more complex problems that may require advanced visual reasoning capabilities.
- What evidence would resolve it: Testing WoT on a diverse set of increasingly complex visual reasoning tasks, analyzing the performance trends and identifying the specific challenges that arise as the complexity increases.

## Limitations
- Evaluation limited to constrained task domains (ASCII art, simple geometric navigation, selected BIG-Bench tasks)
- Performance relies heavily on MLLM's ability to generate accurate visualization code
- Absolute accuracy levels remain modest for complex spatial navigation tasks

## Confidence

**High Confidence**: The core mechanism of using code-generated visualizations for intermediate reasoning steps is well-supported by empirical results, particularly the dramatic improvement from 0% to 92% accuracy on ASCII art recognition.

**Medium Confidence**: The claim that WoT can solve previously unsolvable problems is supported by results but lacks extensive validation across diverse examples.

**Low Confidence**: The assertion that improvements are solely due to visual processing capabilities rather than potential side effects requires further investigation.

## Next Checks

1. **Code Generation Robustness Test**: Systematically evaluate the rate of code generation failures and visualization errors across a diverse set of reasoning tasks to quantify the reliability of this critical component.

2. **Cross-Domain Generalization**: Apply WoT to a benchmark of real-world visual reasoning problems (e.g., diagram interpretation, spatial puzzle solving) to assess whether performance gains extend beyond the evaluated task domains.

3. **Component Ablation Study**: Compare WoT against variants that use text descriptions instead of images for intermediate steps, or that use fixed visualization templates instead of generated code, to isolate the contribution of visual processing versus structured problem decomposition.