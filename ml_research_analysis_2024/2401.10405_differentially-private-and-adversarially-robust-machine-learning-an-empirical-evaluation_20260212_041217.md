---
ver: rpa2
title: 'Differentially Private and Adversarially Robust Machine Learning: An Empirical
  Evaluation'
arxiv_id: '2401.10405'
source_url: https://arxiv.org/abs/2401.10405
tags:
- training
- accuracy
- privacy
- membership
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically evaluates a differentially private adversarial
  training approach (DP-Adv) to assess whether it preserves privacy comparable to
  standard differentially private training. The method replaces each training sample
  with an adversarial example before applying DP-SGD.
---

# Differentially Private and Adversarially Robust Machine Learning: An Empirical Evaluation

## Quick Facts
- arXiv ID: 2401.10405
- Source URL: https://arxiv.org/abs/2401.10405
- Reference count: 24
- Primary result: DP-Adv training preserves individual-level privacy comparable to standard DP-SGD while maintaining adversarial robustness

## Executive Summary
This paper presents an empirical evaluation of DP-Adv (Differentially Private Adversarial training), which combines adversarial training with differential privacy to achieve both robustness against adversarial examples and privacy guarantees. The approach replaces training samples with adversarial examples before applying DP-SGD, aiming to maintain privacy while improving model robustness. The authors benchmark the method using membership inference attacks across multiple datasets (CIFAR10, Fashion-MNIST, MNIST) and find that DP-Adv achieves privacy levels comparable to non-robust private models, with individual-level membership inference accuracy around 50-52%.

## Method Summary
The DP-Adv approach modifies the standard DP-SGD training pipeline by first generating adversarial examples for each training sample using projected gradient descent, then applying the differentially private stochastic gradient descent algorithm to these perturbed examples. This creates a dynamic dataset where each epoch uses different adversarial perturbations, potentially affecting the privacy accounting. The method is evaluated across three benchmark datasets using a standard CNN architecture, with privacy measured through membership inference attacks that attempt to determine whether specific data points were used in training.

## Key Results
- Membership inference attack accuracy remains near random guessing (~50-52%) across all tested datasets, indicating strong individual-level privacy preservation
- Group-level privacy analysis shows no significant leakage across different classes, suggesting the approach maintains privacy at the class level
- DP-Adv achieves comparable privacy guarantees to standard DP-SGD while simultaneously providing adversarial robustness

## Why This Works (Mechanism)
The DP-Adv mechanism works by introducing adversarial perturbations before the differential privacy mechanism, which adds an additional layer of uncertainty to the training data. The dynamic nature of the adversarial examples (changing each epoch) may help obscure the original data patterns while still allowing the model to learn robust features. The combination appears to maintain the privacy guarantees of DP-SGD while the adversarial training provides robustness against input perturbations.

## Foundational Learning
1. **Differential Privacy (DP)**: A framework ensuring that individual data points cannot be distinguished in the training set, providing formal privacy guarantees - needed to understand the baseline privacy requirements; quick check: ε-DP definition and Laplace mechanism
2. **Adversarial Training**: Training models on adversarially perturbed examples to improve robustness against input attacks - needed to understand how robustness is achieved; quick check: PGD attack generation and training loop
3. **Membership Inference Attacks**: Techniques to determine whether specific data points were used in training - needed to evaluate privacy leakage; quick check: shadow model training and attack classification
4. **DP-SGD**: Differentially private version of stochastic gradient descent that adds noise to gradients - needed to understand the baseline training method; quick check: gradient clipping and noise addition steps
5. **Rényi Differential Privacy (RDP)**: A relaxation of differential privacy that provides tighter privacy accounting - needed for formal privacy analysis; quick check: RDP composition theorem

## Architecture Onboarding

Component Map:
Data Pipeline -> Adversarial Example Generator -> DP-SGD Trainer -> Privacy Evaluator -> Robustness Tester

Critical Path:
Training samples flow through adversarial perturbation generation, then undergo DP-SGD with gradient clipping and noise addition, followed by evaluation of both privacy (via membership inference) and robustness (via adversarial attack testing).

Design Tradeoffs:
- Higher adversarial perturbation strength improves robustness but may degrade privacy guarantees
- Increased noise scale improves privacy but reduces model utility
- More training epochs improve accuracy but require more careful privacy accounting

Failure Signatures:
- Membership inference accuracy significantly above random guessing (>60%) indicates privacy leakage
- Sharp drop in accuracy suggests insufficient training or excessive noise
- Robustness testing failure indicates adversarial examples bypass the protection

First Experiments:
1. Baseline DP-SGD training without adversarial perturbations to establish privacy baseline
2. Standard adversarial training without privacy constraints to establish robustness baseline
3. DP-Adv training with varying perturbation strengths to find optimal privacy-robustness tradeoff

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the lack of formal privacy analysis and the need for broader attack evaluations represent implicit areas for future work.

## Limitations
- The empirical privacy evaluation relies solely on membership inference attacks, which may not capture all privacy vulnerabilities
- No formal privacy analysis is provided to mathematically verify the privacy guarantees of DP-Adv
- The study is limited to three datasets and a single model architecture, limiting generalizability

## Confidence
- High confidence in the experimental methodology and implementation
- Medium confidence in the privacy preservation claims due to the empirical nature of the evidence
- Low confidence in the generalizability to other datasets, model architectures, or attack types

## Next Checks
1. Conduct formal privacy analysis using Rényi DP or zCDP accounting to mathematically verify privacy guarantees under DP-Adv training
2. Test against a broader range of membership inference attacks, including adaptive and model-specific variants, to ensure comprehensive privacy assessment
3. Evaluate the approach on additional datasets and model architectures to assess robustness across different scenarios