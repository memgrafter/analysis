---
ver: rpa2
title: Language Models can Self-Lengthen to Generate Long Texts
arxiv_id: '2410.23933'
source_url: https://arxiv.org/abs/2410.23933
tags:
- length
- response
- arxiv
- long
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-Lengthen is a novel method that uses iterative self-alignment
  to improve long text generation in LLMs. It employs a Generator and an Extender
  that work together to progressively lengthen responses through micro-iterations,
  then trains on the extended outputs.
---

# Language Models can Self-Lengthen to Generate Long Texts

## Quick Facts
- arXiv ID: 2410.23933
- Source URL: https://arxiv.org/abs/2410.23933
- Reference count: 17
- Self-Lengthen method enables LLMs to generate texts up to 8x longer while maintaining quality

## Executive Summary
Self-Lengthen introduces a novel approach for long text generation in language models through iterative self-alignment. The method employs two specialized components - a Generator and an Extender - that work together to progressively lengthen responses through micro-iterations. By training on the extended outputs, the model learns to generate substantially longer texts while preserving quality. The approach demonstrates significant improvements over existing methods for length-following tasks without requiring auxiliary data or proprietary models.

## Method Summary
Self-Lengthen uses an iterative self-alignment framework where a Generator creates initial responses and an Extender progressively lengthens them through micro-iterations. The system alternates between generation and extension phases, with the Extender using the Generator's output as input to produce longer responses. This process repeats multiple times to achieve substantial length increases (up to 8x). The model is then trained on these extended outputs, enabling it to generate longer texts directly. The method outperforms instruction backtranslation and plan-and-write baselines on length-following tasks while maintaining quality.

## Key Results
- Achieves up to 8x length extension compared to baseline models
- Outperforms instruction backtranslation and plan-and-write methods on length-following tasks
- Maintains quality during length extension without requiring auxiliary data or proprietary models

## Why This Works (Mechanism)
Self-Lengthen works by addressing the fundamental limitation of LLMs in generating long coherent texts through a novel iterative self-alignment approach. The method recognizes that traditional models struggle with maintaining coherence and following length instructions over extended outputs. By decomposing the task into generation and extension phases, the system can focus on quality in the initial phase and length in subsequent phases. The iterative nature allows the model to learn from progressively longer examples, creating a curriculum that naturally scales up the generation capability.

## Foundational Learning
- **Self-alignment**: Training models on their own generated data to improve capabilities without external supervision. Needed because it enables the model to learn from its own extended outputs without requiring additional labeled data.
- **Iterative refinement**: Progressive improvement through repeated cycles of generation and extension. Required to gradually scale up the model's ability to handle longer texts while maintaining quality.
- **Length-following**: The ability to generate text that matches specified length requirements. Critical for practical applications where precise output length matters.
- **Micro-iterations**: Small, incremental extensions rather than attempting large jumps in length. Allows the model to maintain coherence and quality during the extension process.
- **Instruction backtranslation**: Converting instructions into examples and back to improve instruction-following. Serves as a baseline comparison for evaluating Self-Lengthen's effectiveness.

## Architecture Onboarding

**Component Map**: Generator -> Extender -> Training Loop -> Generator

**Critical Path**: Initial prompt → Generator produces response → Extender iteratively extends response → Extended output → Model training → Improved Generator

**Design Tradeoffs**: The method trades computational overhead (multiple generation/extension cycles) for improved length-following capability without requiring additional data sources. This avoids dependency on proprietary models but requires more training iterations.

**Failure Signatures**: Quality degradation in extended outputs, inability to maintain coherence beyond certain length thresholds, or failure to follow length instructions accurately would indicate limitations in the iterative extension process.

**First Experiments**: 1) Test length extension capabilities on simple instruction-following tasks with varying length requirements. 2) Compare quality metrics between Self-Lengthen and baseline methods across different text domains. 3) Evaluate computational overhead and training time compared to traditional fine-tuning approaches.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on instruction-following benchmarks rather than diverse real-world long-form generation tasks
- Method's effectiveness beyond 8x length extension remains unclear with potential diminishing returns
- Reliance on GPT-4 for initial bootstrapping raises questions about transferability to scenarios without access to proprietary models
- Multiple-pass generation-training cycle may be computationally expensive at scale

## Confidence
- **High confidence**: The core methodology of using iterative self-alignment with Generator/Extender pairs is technically sound and the experimental results showing improved length-following are robust and well-documented.
- **Medium confidence**: Claims about maintaining quality during length extension are supported by the metrics presented, but the evaluation scope is limited to specific benchmarks rather than diverse real-world scenarios.
- **Low confidence**: The generalizability of Self-Lengthen to other domains beyond the tested instruction-following tasks and the method's performance without GPT-4 for initial bootstrapping remain uncertain.

## Next Checks
1. Evaluate Self-Lengthen on long-form creative writing and technical documentation tasks to assess real-world applicability beyond instruction-following benchmarks.
2. Test the method's effectiveness when starting from different base model sizes (smaller than 1.8B) and with varying quality levels of initial bootstrapped data.
3. Measure computational overhead and training time compared to alternative methods, particularly for the multiple-pass generation-training cycle, to determine practical deployment costs.