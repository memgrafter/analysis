---
ver: rpa2
title: 'Bias of Stochastic Gradient Descent or the Architecture: Disentangling the
  Effects of Overparameterization of Neural Networks'
arxiv_id: '2407.03848'
source_url: https://arxiv.org/abs/2407.03848
tags:
- loss
- networks
- accuracy
- test
- normalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the interplay between stochastic gradient
  descent (SGD) and neural network architecture in influencing generalization performance
  in the overparameterized regime. The authors conduct extensive experiments comparing
  networks trained with SGD to randomly sampled networks that achieve zero training
  error (G&C) on binary classification tasks from MNIST and CIFAR10.
---

# Bias of Stochastic Gradient Descent or the Architecture: Disentangling the Effects of Overparameterization of Neural Networks

## Quick Facts
- arXiv ID: 2407.03848
- Source URL: https://arxiv.org/abs/2407.03848
- Authors: Amit Peleg; Matthias Hein
- Reference count: 40
- Primary result: Width improves generalization via SGD's implicit bias; depth harms generalization due to architectural bias

## Executive Summary
This paper investigates whether improvements in generalization for overparameterized neural networks stem from stochastic gradient descent's (SGD) implicit bias or the network architecture itself. The authors conduct controlled experiments comparing SGD-trained networks to randomly sampled networks achieving zero training error (G&C) on MNIST and CIFAR10. They examine two forms of overparameterization: increasing width and increasing depth. The key finding is that while increasing width improves generalization for SGD but not for G&C, indicating an optimizer-induced bias, increasing depth harms generalization for both approaches, suggesting an inherent architectural limitation.

## Method Summary
The authors employ a systematic experimental design comparing SGD-trained networks with randomly sampled networks that achieve zero training error (G&C). They conduct binary classification tasks on MNIST and CIFAR10 using fully connected networks. The study focuses on two dimensions of overparameterization: width (number of neurons per layer) and depth (number of layers). For the G&C approach, they use a sampling method that randomly selects networks from the solution space that perfectly fit the training data. This allows them to disentangle whether generalization improvements are due to the optimization process or the architecture itself.

## Key Results
- Increasing network width improves generalization for SGD but not for G&C, indicating SGD's implicit bias drives width-related improvements
- Increasing network depth is detrimental to generalization for both SGD and G&C, suggesting a negative architectural bias
- The results show that overparameterization effects differ based on whether width or depth is increased, with width benefiting from SGD's optimization while depth suffers from architectural complexity

## Why This Works (Mechanism)
The paper demonstrates that SGD's implicit bias favors solutions with better generalization properties when networks are made wider, but this benefit doesn't extend to randomly sampled networks achieving zero training error. Conversely, deeper networks show worse generalization regardless of training method, indicating that depth introduces architectural complexity that harms generalization. This mechanism suggests that current trends toward larger networks may be partially driven by the optimizer's implicit bias rather than purely architectural advantages.

## Foundational Learning
- **Implicit bias of SGD**: Why needed - To understand how optimization affects generalization beyond explicit regularization. Quick check - Verify that SGD converges to different solutions than random sampling for wide networks.
- **Random sampling of zero-error networks (G&C)**: Why needed - To create a baseline that isolates architectural effects from optimization effects. Quick check - Confirm that sampled networks achieve zero training error while being truly random in parameter space.
- **Overparameterization**: Why needed - To study the double descent phenomenon and understand when more parameters help or hurt. Quick check - Ensure networks have sufficient parameters to interpolate training data.

## Architecture Onboarding
- **Component map**: Input -> Fully connected layers (variable width/depth) -> Output layer
- **Critical path**: Data flow through network layers to output, with width and depth as tunable parameters
- **Design tradeoffs**: Width provides implicit regularization via SGD but increases computational cost; depth increases representational capacity but may harm generalization
- **Failure signatures**: Poor generalization with increased depth; no generalization improvement with increased width in random sampling
- **First experiments**: 1) Compare SGD vs G&C for varying widths, 2) Compare SGD vs G&C for varying depths, 3) Test with different activation functions

## Open Questions the Paper Calls Out
None

## Limitations
- The comparison assumes SGD and G&C span the full solution space, which may not be true
- The G&C sampling method may not perfectly represent the actual solution space that SGD explores
- Attribution of depth-related detriments to architecture alone may overlook optimization challenges

## Confidence
- High confidence: Experimental methodology and implementation details are clearly described and reproducible
- Medium confidence: Attribution of width improvements to SGD's implicit bias is reasonable but could benefit from additional theoretical analysis
- Medium confidence: Claim that depth detriments are purely architectural requires further investigation as optimization difficulties may confound results

## Next Checks
1. Conduct experiments with different optimization algorithms (Adam, RMSprop) to verify if width-related improvements are specific to SGD's implicit bias
2. Perform ablation studies varying initialization schemes and learning rate schedules to isolate optimization effects from architectural properties
3. Test additional network architectures (residual connections, attention mechanisms) to determine if depth-related generalization bounds hold across different paradigms