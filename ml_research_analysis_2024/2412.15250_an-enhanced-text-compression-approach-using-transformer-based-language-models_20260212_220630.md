---
ver: rpa2
title: An Enhanced Text Compression Approach Using Transformer-based Language Models
arxiv_id: '2412.15250'
source_url: https://arxiv.org/abs/2412.15250
tags:
- text
- compression
- data
- performance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a text compression method combining a vowel-removal\
  \ preprocessing step with the Lempel-Ziv-Welch (LZW) algorithm and a transformer-based\
  \ model for restoration. By removing vowels before compression, the approach significantly\
  \ improves compression ratios, achieving 12.57\xD7, 13.38\xD7, and 11.42\xD7 on\
  \ the BookCorpus, EN-DE, and EN-FR corpora, respectively."
---

# An Enhanced Text Compression Approach Using Transformer-based Language Models

## Quick Facts
- arXiv ID: 2412.15250
- Source URL: https://arxiv.org/abs/2412.15250
- Authors: Chowdhury Mofizur Rahman; Mahbub E Sobhani; Anika Tasnim Rodela; Swakkhar Shatabda
- Reference count: 34
- Primary result: Achieves compression ratios of 12.57×, 13.38×, and 11.42× on BookCorpus, EN-DE, and EN-FR corpora using vowel-removal preprocessing with LZW compression

## Executive Summary
This paper presents a text compression method that combines vowel-removal preprocessing with LZW compression and transformer-based restoration. By removing vowels before compression, the approach significantly improves compression ratios while maintaining high-quality text restoration. The proposed RejuvenateFormer model is smaller and faster than comparable transformers while achieving competitive restoration quality metrics.

## Method Summary
The method involves three main stages: preprocessing where vowels are removed from text, compression using the Lempel-Ziv-Welch algorithm, and restoration using a transformer-based model. The preprocessing step reduces character set entropy, making LZW more efficient at encoding repeated patterns. For restoration, a six-layer encoder-decoder transformer called RejuvenateFormer is trained to reconstruct the original text from vowel-removed input. The model uses subword tokenization and is trained on 100K sentence pairs from WMT14 and BookCorpus datasets with standard transformer hyperparameters.

## Key Results
- Compression ratios achieved: 12.57× on BookCorpus, 13.38× on EN-DE, and 11.42× on EN-FR corpora
- RejuvenateFormer achieves BLEU scores of 27.31 (EN-DE), 25.78 (EN-FR), and 50.45 (BookCorpus)
- RejuvenateFormer is 3.6× smaller than T5-Small while maintaining competitive restoration quality
- Corpus size directly correlates with restoration performance, with 100K instances outperforming smaller datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing vowels reduces entropy before compression, making LZW more efficient
- Mechanism: Vowel removal reduces character set size, creating longer repeated substrings that LZW can encode with fewer dictionary entries
- Core assumption: Vowel letters are statistically predictable enough to reconstruct reliably after decompression
- Evidence anchors: Abstract reports compression ratios of 12.57×, 13.38×, and 11.42× on three corpora; section describes vowel removal process using character set V = {a, e, i, o, u}

### Mechanism 2
- Claim: RejuvenateFormer architecture is optimized for vowel restoration with reduced parameters
- Mechanism: Six-layer encoder-decoder design with 512 hidden dimensions balances computational efficiency against restoration quality
- Core assumption: Smaller model trained on sufficient data can match larger models on specific tasks like vowel restoration
- Evidence anchors: Abstract reports BLEU scores of 27.31, 25.78, and 50.45; section describes encoder-decoder architecture with positional encoding

### Mechanism 3
- Claim: Larger training corpora improve restoration performance by providing more contextual patterns
- Mechanism: Corpus size directly correlates with model's ability to learn vowel placement rules across different sentence structures
- Core assumption: Statistical patterns for vowel placement are learnable from data rather than requiring explicit linguistic rules
- Evidence anchors: Abstract highlights importance of larger training datasets; section shows corpus with 100K instances outperformed those with 30K and 50K instances

## Foundational Learning

- Concept: Lempel-Ziv-Welch compression algorithm
  - Why needed here: LZW is the core lossless compression component that achieves state-of-the-art ratios when combined with vowel removal
  - Quick check question: How does LZW build and update its dictionary during compression?

- Concept: Transformer encoder-decoder architecture
  - Why needed here: RejuvenateFormer uses transformer layers to restore vowels based on contextual information
  - Quick check question: What is the difference between self-attention and masked self-attention in the decoder?

- Concept: BLEU and BERTScore evaluation metrics
  - Why needed here: These metrics assess restoration quality against reference text
  - Quick check question: How does BERTScore differ from traditional n-gram based metrics like BLEU?

## Architecture Onboarding

- Component map: Vowel removal → LZW compression → LZW decompression → RejuvenateFormer restoration → evaluation
- Critical path: Pre-processing (vowel removal + LZW) → model training → inference → metric calculation
- Design tradeoffs: Smaller model size vs. restoration accuracy, computational efficiency vs. training data requirements
- Failure signatures: Poor compression ratios indicate preprocessing issues; low restoration scores suggest model or data problems
- First 3 experiments:
  1. Test vowel removal impact on compression ratio using sample text with LZW
  2. Train RejuvenateFormer on small corpus subset and measure restoration quality
  3. Compare RejuvenateFormer performance against T5-Small on identical test sets

## Open Questions the Paper Calls Out
- How does RejuvenateFormer performance scale with increasingly larger training datasets beyond the 100K sentence pairs tested?
- What is the theoretical limit of compression ratio improvement when combining vowel removal preprocessing with advanced lossless compression algorithms?
- How does the RejuvenateFormer model's vowel restoration capability generalize to languages with different vowel systems or more complex orthographic rules?

## Limitations
- Language scope limited to English, with no evaluation of cross-linguistic applicability
- Lack of comparison against modern compression algorithms beyond basic LZW variants
- No evaluation of restoration quality on out-of-domain text or non-standard vocabulary

## Confidence
- High confidence: LZW compression ratios after vowel removal are empirically validated across multiple datasets
- Medium confidence: RejuvenateFormer architecture details are sufficiently specified for reproduction
- Medium confidence: Corpus size effects are demonstrated but limited to three training set sizes

## Next Checks
1. Test the vowel-removal and LZW pipeline on Romance languages (Spanish, French) to assess generalization beyond English vowel patterns
2. Benchmark against modern approaches like PAQ, Zstandard, or neural compressors on the same datasets to establish relative performance
3. Evaluate restoration quality on text with non-standard vocabulary (technical terms, proper nouns, or domain-specific jargon) to identify preprocessing failure modes