---
ver: rpa2
title: 'Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for Legal
  Question Answering'
arxiv_id: '2412.19482'
source_url: https://arxiv.org/abs/2412.19482
tags:
- question
- legal
- questions
- pre-training
- answering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a three-stage framework (PFR-LQA) for legal
  question answering that combines domain-specific pre-training, task-specific fine-tuning,
  and contextual re-ranking. The method uses a dual-encoder architecture with Legal-SCP
  pre-training on legal documents, followed by fine-tuning on question-answer pairs
  using circle loss, and finally applies a contextual re-ranking objective to improve
  question representations.
---

# Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for Legal Question Answering

## Quick Facts
- arXiv ID: 2412.19482
- Source URL: https://arxiv.org/abs/2412.19482
- Authors: Shiwen Ni; Hao Cheng; Min Yang
- Reference count: 24
- One-line primary result: PFR-LQA achieves 79.9% P@1 and 87.3% MRR@16 on Chinese legal QA, outperforming state-of-the-art by 5.7% P@1 and 4.2% MRR@16

## Executive Summary
This paper introduces PFR-LQA, a three-stage framework for legal question answering that combines domain-specific pre-training, task-specific fine-tuning, and contextual re-ranking. The method uses a dual-encoder architecture with Legal-SCP pre-training on legal documents, followed by fine-tuning on question-answer pairs using circle loss, and finally applies a contextual re-ranking objective to improve question representations. The approach is evaluated on a manually annotated Chinese legal QA dataset (LawQA) containing 549,668 positive QA pairs across nine categories, achieving significant improvements over state-of-the-art baselines.

## Method Summary
PFR-LQA employs a three-stage framework: First, domain-specific pre-training (Legal-SCP) uses masked auto-encoding on legal questions and answers to adapt the model to legal terminology. Second, task-specific fine-tuning optimizes a dual-encoder using circle loss to maximize similarity between correct QA pairs while minimizing similarity for incorrect pairs. Third, contextual re-ranking refines question representations by leveraging semantic similarity among top-ranked candidates using Transformer-based affinity aggregation. The framework processes queries through BM25 candidate retrieval, fine-tuned dual-encoder scoring, and contextual re-ranking to produce final answer recommendations.

## Key Results
- Achieves P@1 of 79.9% and MRR@16 of 87.3% on LawQA dataset
- Outperforms best competitor by 5.7% on P@1 and 4.2% on MRR@16
- Ablation study confirms all three components contribute to performance
- Task-specific fine-tuning has the largest impact on overall performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pre-training adapts the model to legal terminology and structure, improving retrieval performance over general-domain PLMs.
- Mechanism: Legal-SCP pre-training applies masked auto-encoding with legal questions and answers, allowing the model to learn legal-specific semantics before fine-tuning.
- Core assumption: Legal documents contain domain-specific language patterns that are not captured by general-domain pre-training.
- Evidence anchors:
  - [abstract]: "However, these methods could suffer from lacking domain knowledge and sufficient labeled training data."
  - [section II-A]: "We first conduct domain-specific pre-training on legal questions and answers through a self-supervised training objective, allowing the pre-trained model to be adapted to the legal domain."
- Break condition: If pre-training corpus lacks sufficient legal diversity or contains non-legal noise, adaptation may not generalize to real legal queries.

### Mechanism 2
- Claim: Task-specific fine-tuning with circle loss maximizes similarity between correct QA pairs while minimizing similarity for incorrect pairs.
- Mechanism: Circle loss function optimizes dual-encoder by increasing within-class similarity and decreasing between-class similarity using positive and negative QA pairs.
- Core assumption: Legal QA pairs have clear semantic relationships that can be learned through supervised fine-tuning.
- Evidence anchors:
  - [abstract]: "Then, we perform task-specific fine-tuning of the dual-encoder on legal question-answer pairs by using the supervised learning objective, leading to a high-quality dual-encoder for the specific downstream QA task."
  - [section II-B]: "We adopt the circle loss [16] to optimize the dual-encoder model, which can maximize within-class similarity while minimizing between-class similarity, leading to improved deep feature representations."
- Break condition: If negative sampling is poor or unrepresentative, model may learn incorrect similarity patterns.

### Mechanism 3
- Claim: Contextual re-ranking refines question representations by leveraging semantic similarity among top-ranked candidates.
- Mechanism: Re-ranking stage uses Transformer-based affinity aggregation to refine question representations based on contextual similarity with other candidate questions.
- Core assumption: Semantically similar questions share similar distances from anchor documents, allowing contextual refinement.
- Evidence anchors:
  - [abstract]: "Finally, we employ a contextual re-ranking objective to further refine the output representations of questions produced by the document encoder, which uses contextual similarity to increase the discrepancy between the anchor and hard negative samples for better question re-ranking."
  - [section II-C]: "To learn a better question representation, we select L questions from the candidate set R as anchors for each query and calculate an affinity feature representation for each question to explore rich relational information contained in top-ranked questions."
- Break condition: If initial candidate pool lacks semantically similar questions, re-ranking may not provide meaningful refinement.

## Foundational Learning

- Concept: Dense retrieval with dual-encoder architecture
  - Why needed here: Legal QA requires efficient retrieval from large-scale QA databases, and dual-encoders provide scalable similarity computation
  - Quick check question: What is the computational advantage of dual-encoder over cross-encoder architectures for large-scale retrieval?

- Concept: Masked language modeling and contextual pre-training
  - Why needed here: Pre-training on legal domain data helps model learn legal terminology and document structure before task-specific fine-tuning
  - Quick check question: How does masked auto-encoding differ from standard language modeling in pre-training objectives?

- Concept: Contrastive learning and loss functions
  - Why needed here: Circle loss and contextual re-ranking objectives require understanding of contrastive learning principles to optimize similarity metrics
  - Quick check question: What is the key difference between circle loss and standard contrastive loss in handling positive and negative pairs?

## Architecture Onboarding

- Component map: Query → BM25 retrieval → fine-tuned Legal-SCP dual-encoder → contextual re-ranking → final answer selection

- Critical path: Query → BM25 retrieval → fine-tuned dual-encoder → contextual re-ranking → answer recommendation

- Design tradeoffs:
  - Pre-training vs. fine-tuning: More pre-training improves domain adaptation but increases computational cost
  - Circle loss vs. standard contrastive loss: Circle loss handles hard negatives better but requires careful hyperparameter tuning
  - Re-ranking complexity: Adding contextual re-ranking improves accuracy but increases inference latency

- Failure signatures:
  - Poor pre-training: Model fails to capture legal terminology, performs similarly to general-domain PLMs
  - Weak fine-tuning: Circle loss doesn't improve over standard supervised learning, indicating poor negative sampling
  - Ineffective re-ranking: Contextual similarity doesn't improve over initial dual-encoder rankings, suggesting poor anchor selection

- First 3 experiments:
  1. Ablation test: Remove domain-specific pre-training and compare against full PFR-LQA to measure domain adaptation impact
  2. Negative sampling test: Compare circle loss with different negative sampling strategies to optimize hard negative identification
  3. Re-ranking sensitivity: Vary the number of anchor questions (L parameter) to find optimal trade-off between accuracy and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PFR-LQA perform on other legal domains or jurisdictions with different legal systems and terminologies?
- Basis in paper: [inferred] The paper evaluates PFR-LQA on a Chinese legal QA dataset (LawQA) and mentions that it outperforms state-of-the-art baselines. However, it does not explore performance across different legal domains or jurisdictions.
- Why unresolved: The paper focuses on a single dataset (LawQA) from Chinese law forums, limiting the generalizability of the results to other legal systems.
- What evidence would resolve it: Testing PFR-LQA on legal QA datasets from different jurisdictions (e.g., US, EU) or domains (e.g., tax law, environmental law) and comparing its performance to baselines.

### Open Question 2
- Question: What is the impact of varying the number of anchor questions (L) in the contextual re-ranking stage on PFR-LQA's performance?
- Basis in paper: [explicit] The paper mentions that the number of anchor questions is set to 8 (L=8) but does not explore how different values of L affect performance.
- Why unresolved: The choice of L is presented as a hyperparameter without analysis of its sensitivity or optimal range.
- What evidence would resolve it: Conducting experiments with varying L (e.g., L=4, 8, 12, 16) and analyzing the trade-off between performance and computational cost.

### Open Question 3
- Question: How does PFR-LQA handle out-of-distribution queries or rare legal cases that are not well-represented in the training data?
- Basis in paper: [inferred] The paper evaluates PFR-LQA on a large-scale dataset but does not address its robustness to rare or out-of-distribution queries.
- Why unresolved: Legal systems often encounter unique or rare cases, and the model's ability to generalize to such scenarios is not tested.
- What evidence would resolve it: Testing PFR-LQA on a subset of rare or out-of-distribution queries and analyzing its performance compared to baselines.

## Limitations

- The manually annotated Chinese legal QA dataset (LawQA) is not publicly accessible, making independent verification difficult
- Test set size of 900 pairs may not capture full complexity of real-world legal query distributions
- Study focuses exclusively on Chinese legal domains, limiting generalizability to other languages or legal systems

## Confidence

**High Confidence**: The three-stage framework architecture and overall experimental methodology are well-documented and reproducible. The reported improvements over baseline models (5.7% P@1 and 4.2% MRR@16) are specific and measurable, though dependent on the proprietary dataset.

**Medium Confidence**: The effectiveness of individual components (Legal-SCP pre-training, circle loss fine-tuning, contextual re-ranking) is supported by ablation studies, but the relative contribution of each stage may vary depending on the legal domain and query complexity. The assumption that legal questions have clear semantic relationships suitable for circle loss optimization requires further validation across diverse legal topics.

**Low Confidence**: The scalability claims for the dual-encoder architecture in large-scale legal QA systems are not empirically validated beyond the reported experiments. The paper does not address potential biases in the legal training data or how the model handles ambiguous legal queries.

## Next Checks

1. **Dataset Generalization Test**: Evaluate PFR-LQA performance on publicly available legal QA datasets (e.g., COLIEE competition data) to assess cross-domain generalization and identify potential overfitting to the LawQA dataset.

2. **Component Sensitivity Analysis**: Systematically vary the number of anchor questions (L parameter) in the re-ranking stage and test different negative sampling strategies in circle loss to optimize the trade-off between accuracy and computational efficiency.

3. **Bias and Fairness Audit**: Analyze the model's performance across different legal categories and demographic groups to identify potential biases in legal recommendations, particularly for sensitive areas like criminal offenses and labor disputes.