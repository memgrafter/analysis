---
ver: rpa2
title: Debiasing Classifiers by Amplifying Bias with Latent Diffusion and Large Language
  Models
arxiv_id: '2411.16079'
source_url: https://arxiv.org/abs/2411.16079
tags:
- bias-conflict
- dataset
- samples
- bias
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiffuBias, a novel debiasing framework that
  leverages pretrained diffusion models to generate bias-conflict samples without
  additional training. The method extracts bias-conflict samples using a biased classifier
  trained with Generalized Cross Entropy loss, generates text captions with an image
  captioning model, and synthesizes new images using latent diffusion models conditioned
  on the captions.
---

# Debiasing Classifiers by Amplifying Bias with Latent Diffusion and Large Language Models

## Quick Facts
- arXiv ID: 2411.16079
- Source URL: https://arxiv.org/abs/2411.16079
- Authors: Donggeun Ko; Dongjun Lee; Namjun Park; Wonkyeong Shim; Jaekwang Kim
- Reference count: 37
- Primary result: State-of-the-art debiasing performance on four benchmark datasets using pretrained diffusion models without additional training

## Executive Summary
This paper introduces DiffuBias, a novel debiasing framework that leverages pretrained diffusion models to generate bias-conflict samples without additional training. The method extracts bias-conflict samples using a biased classifier trained with Generalized Cross Entropy loss, generates text captions with an image captioning model, and synthesizes new images using latent diffusion models conditioned on the captions. Experiments on four benchmark datasets show that DiffuBias achieves state-of-the-art performance, particularly on real-world datasets, while being computationally efficient compared to GAN-based methods.

## Method Summary
DiffuBias debiases classifiers by first training a biased classifier with Generalized Cross Entropy loss to identify bias-conflict samples through their high loss values. These samples are then processed through an image captioning model to generate text descriptions, which are filtered to retain class-relevant terms. Finally, a pretrained latent diffusion model generates synthetic images conditioned on these filtered captions. The original training data is augmented with these synthetic bias-conflict samples, and a new debiased classifier is trained on this augmented dataset.

## Key Results
- Achieves state-of-the-art performance on four benchmark datasets (CCIFAR-10, BFFHQ, BAR, and Dogs & Cats)
- Particularly effective on real-world datasets compared to synthetic ones
- Lower energy consumption and CO2 emissions compared to GAN-based debiasing methods
- Ablation studies confirm the importance of text filtering in improving performance
- Grad-CAM visualizations demonstrate the debiased classifier focuses on task-relevant features rather than biases

## Why This Works (Mechanism)

### Mechanism 1
Using top-K losses from a biased classifier identifies bias-conflict samples effectively without requiring human labels. The biased classifier trained with Generalized Cross Entropy (GCE) loss disproportionately weights bias-aligned samples during training. This amplifies errors on bias-conflict samples, making them identifiable by their high GCE loss values. The core assumption is that bias-conflict samples are inherently harder for the biased classifier to predict correctly due to the spurious correlation learned during training.

### Mechanism 2
Text filtering improves the quality of generated images by ensuring prompts contain class-relevant terms. The text filter selects captions containing the most frequent class label words (top-F words), removing noisy or irrelevant captions that might lead to off-topic image generation. The core assumption is that captions containing class label words are more likely to generate images that preserve the intended class while challenging the bias.

### Mechanism 3
Pretrained diffusion models can generate high-quality bias-conflict samples without additional training, making the approach computationally efficient. Latent Diffusion Models (LDMs) are used in their pretrained form to generate images conditioned on text prompts derived from bias-conflict captions. This avoids the computational cost of training a generative model from scratch. The core assumption is that pretrained diffusion models have learned generalizable image features that can be adapted to new text prompts without fine-tuning.

## Foundational Learning

- Concept: Generalized Cross Entropy (GCE) loss
  - Why needed here: GCE loss is used to intentionally train a biased classifier by amplifying the weights of bias-aligned samples, making bias-conflict samples more identifiable through high loss values.
  - Quick check question: How does GCE loss with q < 1 differ from standard cross-entropy loss in terms of gradient weighting?

- Concept: Latent Diffusion Models (LDMs)
  - Why needed here: LDMs operate in the latent space of an autoencoder, allowing efficient image generation conditioned on text prompts without the need for training the generative model.
  - Quick check question: What is the role of the U-Net in the LDM architecture during the denoising process?

- Concept: Text-to-image generation conditioning
  - Why needed here: The generated images must be conditioned on text prompts that describe bias-conflict attributes to ensure the generated samples challenge the classifier's bias.
  - Quick check question: How does the conditioning mechanism in LDMs use text embeddings to guide image generation?

## Architecture Onboarding

- Component map: Biased Classifier (ResNet-18) → GCE Loss → Top-K Loss Selection → Image Captioning (LLaVA) → Text Filtering → Latent Diffusion Model → Generated Images → Debiased Classifier Training
- Critical path: Biased classifier training → bias-conflict sample extraction → image captioning → text filtering → image generation → debiased classifier training
- Design tradeoffs:
  - Using pretrained models reduces training time but may limit adaptability to niche domains
  - Text filtering improves prompt quality but may exclude valid bias-conflict captions if class labels are ambiguous
  - Top-K loss selection is simple but may miss subtle bias-conflict samples if K is too small
- Failure signatures:
  - Poor performance on synthetic datasets (e.g., CCIFAR-10) due to resolution mismatch and caption generation errors
  - Generated images that don't match labels, indicating issues in the text-to-image generation or caption extraction
  - Grad-CAM showing focus on bias attributes rather than task-relevant features, suggesting insufficient bias-conflict sample generation
- First 3 experiments:
  1. Train the biased classifier with GCE loss and verify that bias-conflict samples have higher losses than bias-align samples
  2. Test the text filter by generating captions with and without filtering and comparing the quality of generated images
  3. Generate a small set of bias-conflict images using the LDM and manually inspect them for label alignment and bias-challenge

## Open Questions the Paper Calls Out

### Open Question 1
How would fine-tuning the latent diffusion model for specific domains improve bias-conflict sample generation quality and debiasing performance? The paper mentions extending work to fine-tune pretrained diffusion models to become adaptive to several domains to mitigate bias in the dataset and enhance performance. This is unresolved because while the paper demonstrates successful bias-conflict generation using a pretrained diffusion model, it acknowledges limitations in generating high-quality samples for certain domains (e.g., medical images) and tasks (e.g., anomaly detection), explicitly suggesting fine-tuning as a potential solution but not implementing or evaluating this approach.

### Open Question 2
What is the optimal text filter size (F) and methodology for balancing bias-conflict sample generation quality across different datasets and bias types? The authors use a simple frequency-based text filter with F=20 for CCIFAR-10 (2× number of classes) and demonstrate through ablation studies that text filtering impacts performance, but do not explore optimal filter sizes or alternative filtering strategies. This remains unresolved because the current approach uses a heuristic based on class count, and while ablation studies show filtering helps, the paper doesn't systematically investigate how different filter sizes or methods (e.g., semantic filtering, attribute-based filtering) affect generation quality and debiasing effectiveness across diverse datasets.

### Open Question 3
How does the choice of image captioning model and its fine-tuning impact the semantic quality and bias-conflict attribute capture in generated text prompts? The authors use llava-llama-3-8b as a frozen image captioning model and observe that it struggles with synthetic datasets like CCIFAR-10 where it cannot reliably capture noise types, while performing better on real-world datasets with clearer bias attributes. This is unresolved because the paper demonstrates that the choice of captioning model affects prompt quality and downstream generation, particularly noting difficulties with synthetic datasets, but does not explore whether fine-tuning the captioning model on dataset-specific data or using alternative captioning approaches would improve performance.

## Limitations
- Effectiveness depends on sufficient bias-conflict samples being identifiable through GCE loss, which may not hold for complex or subtle biases
- Text filtering relies on class label words being present in captions, potentially failing when class labels are ambiguous or overlapping
- Dependence on pretrained diffusion models may limit effectiveness on domains significantly different from natural images

## Confidence
- **High Confidence**: Computational efficiency advantage of using pretrained diffusion models without additional training
- **Medium Confidence**: Effectiveness of top-K GCE loss selection for identifying bias-conflict samples
- **Medium Confidence**: Improvement in classification performance through text filtering

## Next Checks
1. **GCE Loss Distribution Analysis**: Plot the distribution of GCE losses for bias-align vs. bias-conflict samples on each dataset to verify that top-K selection effectively separates these groups, and test how performance changes with different K values.

2. **Caption Quality Assessment**: Compare generated image quality and label alignment with and without text filtering using human evaluation or automated metrics like CLIP score to quantify the impact of the filtering mechanism.

3. **Domain Generalization Test**: Evaluate DiffuBias on a medical or technical image dataset (far from natural images) to assess whether pretrained diffusion models can still generate meaningful bias-conflict samples in specialized domains.