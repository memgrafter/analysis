---
ver: rpa2
title: Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration
arxiv_id: '2403.04629'
source_url: https://arxiv.org/abs/2403.04629
tags:
- optimization
- shapley
- human
- values
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ShapleyBO, a framework for explaining Bayesian
  optimization (BO) proposals using Shapley values from cooperative game theory. The
  core idea is to quantify each parameter's contribution to BO's acquisition function,
  thus revealing why certain configurations are selected.
---

# Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration

## Quick Facts
- arXiv ID: 2403.04629
- Source URL: https://arxiv.org/abs/2403.04629
- Authors: Julian Rodemann; Federico Croppi; Philipp Arens; Yusuf Sale; Julia Herbinger; Bernd Bischl; Eyke Hüllermeier; Thomas Augustin; Conor J. Walsh; Giuseppe Casalicchio
- Reference count: 13
- One-line primary result: ShapleyBO-assisted human-BO teams achieve lower regret than teams without Shapley values in real-world wearable robotic device personalization.

## Executive Summary
This paper introduces ShapleyBO, a framework that explains Bayesian optimization (BO) proposals using Shapley values from cooperative game theory. By quantifying each parameter's contribution to the acquisition function, ShapleyBO reveals why certain configurations are selected, enabling users to understand and potentially intervene in the optimization process. The method exploits the linearity of Shapley values to disentangle contributions to exploration and exploitation in additive acquisition functions, and further extends this to separate aleatoric and epistemic uncertainty contributions.

## Method Summary
ShapleyBO applies Shapley values from cooperative game theory to quantify parameter contributions in Bayesian optimization. The framework treats the acquisition function as a cooperative game where each parameter is a player, and Shapley values distribute the "informativeness" payoff based on marginal contributions. For additive acquisition functions like confidence bounds, ShapleyBO decomposes contributions into exploration (uncertainty reduction) and exploitation (mean optimization) components. The method further extends to disentangle aleatoric and epistemic uncertainty contributions. This interpretability is integrated into a human-in-the-loop BO system through a Human-Machine Interface that allows user intervention when proposals don't align with human reasoning.

## Key Results
- ShapleyBO successfully quantifies parameter contributions to BO's acquisition function
- The framework disentangles exploration and exploitation contributions in additive acquisition functions
- Real-world experiments show ShapleyBO-assisted human-BO teams achieve lower regret than teams without Shapley values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shapley values can quantify each parameter's contribution to the acquisition function in Bayesian optimization.
- Mechanism: By treating the acquisition function as a cooperative game, Shapley values distribute the "informativeness" payoff among parameters based on their marginal contributions across all possible coalitions.
- Core assumption: The acquisition function can be expressed as a value function for a cooperative game, and the Shapley axioms apply.
- Evidence anchors:
  - [abstract] "They quantify each parameter's contribution to BO's acquisition function."
  - [section] "The key idea is to consider each feature as a player in a game where the prediction is the payoff, and to distribute the payoff fairly among the players according to their marginal contributions."
- Break condition: If the acquisition function violates the linearity or dummy player axioms, Shapley values may not accurately represent parameter contributions.

### Mechanism 2
- Claim: The linearity of Shapley values allows disentangling contributions to exploration and exploitation in additive acquisition functions.
- Mechanism: For additive acquisition functions like the confidence bound (m - λse), Shapley values can be decomposed into mean (exploitation) and uncertainty (exploration) components, revealing how much each parameter drives each aspect.
- Core assumption: The acquisition function is additive and the Shapley value linearity axiom holds.
- Evidence anchors:
  - [abstract] "Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO's exploration and exploitation for additive acquisition functions like the confidence bound."
  - [section] "According to the linearity axiom the cb contribution of any parameter j of explicand ˜θ can then be decomposed into mean contribution ϕj(m) and uncertainty contribution ϕj(se)."
- Break condition: If the acquisition function is not additive, the decomposition into exploration and exploitation contributions is invalid.

### Mechanism 3
- Claim: ShapleyBO can disentangle exploratory uncertainty reduction into aleatoric and epistemic components.
- Mechanism: By extending the acquisition function to explicitly model aleatoric and epistemic uncertainty, Shapley values can attribute parameter contributions to each type of uncertainty reduction.
- Core assumption: The extended acquisition function accurately models both types of uncertainty, and the Shapley framework can handle the additional components.
- Evidence anchors:
  - [abstract] "We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty."
  - [section] "In the theoretical part of this paper, we delve further into this distinction. In particular, we propose a method to further dissect the exploratory component into different kinds of uncertainties that a proposal seeks to reduce."
- Break condition: If the uncertainty decomposition is inaccurate or the Shapley framework cannot handle the extended acquisition function, the attribution of parameter contributions to specific uncertainty types is unreliable.

## Foundational Learning

- Concept: Cooperative game theory and Shapley values
  - Why needed here: The paper relies on Shapley values from cooperative game theory to quantify parameter contributions in Bayesian optimization.
  - Quick check question: Can you explain the four Shapley axioms (dummy player, efficiency, linearity, symmetry) and why they are desirable for interpreting optimization problems?

- Concept: Bayesian optimization with Gaussian processes
  - Why needed here: The paper applies Shapley values to explain proposals in Bayesian optimization using Gaussian processes as surrogate models.
  - Quick check question: What is the role of the acquisition function in Bayesian optimization, and how does the confidence bound balance exploration and exploitation?

- Concept: Aleatoric and epistemic uncertainty
  - Why needed here: The paper extends ShapleyBO to disentangle exploratory uncertainty reduction into aleatoric (irreducible) and epistemic (reducible) components.
  - Quick check question: Can you distinguish between aleatoric and epistemic uncertainty, and provide examples of each in the context of Bayesian optimization?

## Architecture Onboarding

- Component map: Bayesian optimization loop -> ShapleyBO computation -> Human-in-the-loop interface -> User intervention -> Bayesian optimization loop update
- Critical path: Bayesian optimization loop → ShapleyBO computation → Human-in-the-loop interface → User intervention (if any) → Bayesian optimization loop update
- Design tradeoffs:
  - Computational cost of Shapley value computation vs. interpretability benefits
  - Choice of acquisition function (additive vs. non-additive) for Shapley decomposition
  - Level of uncertainty decomposition (aleatoric vs. epistemic) based on the specific use case
- Failure signatures:
  - Shapley values not summing to the acquisition function value (violation of efficiency axiom)
  - Inconsistent Shapley values across similar proposals (violation of symmetry axiom)
  - Inability to decompose acquisition function contributions (non-additive acquisition function)
- First 3 experiments:
  1. Implement ShapleyBO on a simple synthetic function (e.g., sphere function) and verify that Shapley values correctly attribute contributions to each parameter.
  2. Compare ShapleyBO-assisted human-in-the-loop optimization with standard Bayesian optimization on a simple benchmark problem (e.g., Hartmann function) to demonstrate efficiency gains.
  3. Extend ShapleyBO to a more complex use case (e.g., hyperparameter optimization for a machine learning model) and analyze the interpretability benefits for users.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the specific choice of intervention criterion (hard-coded vs learned through regression trees) impact the performance of Shapley-assisted human-BO teams in real-world applications?
- Basis in paper: [explicit] The authors compare hard-coded intervention criteria (Equation 11 and 12) with tree-based learned intervention criteria (Figures 7 and 8) and conclude that results are relatively robust to the specification.
- Why unresolved: The authors only tested this on simulated data and a limited set of intervention criteria. Real-world applications may have different noise characteristics and human preferences that could affect the performance of different intervention criteria.
- What evidence would resolve it: A user study with actual humans interacting with a real-world system (e.g., personalizing exosuits) comparing the performance of different intervention criteria (hard-coded vs learned) in terms of optimization efficiency and user satisfaction.

### Open Question 2
- Question: What are the theoretical bounds on the regret of Bayesian optimization with confidence bound under Shapley-assisted human interventions?
- Basis in paper: [inferred] The authors mention in the discussion that a thorough mathematical study of sublinear regret bounds under Shapley-assisted interventions "might foster theoretical understanding of why Shapley-assisted teams outperform competitors."
- Why unresolved: The authors have not conducted a formal theoretical analysis of the regret bounds. Existing regret bounds for Bayesian optimization with confidence bound assume no human intervention.
- What evidence would resolve it: A rigorous mathematical proof establishing sublinear regret bounds for Bayesian optimization with confidence bound when human interventions are guided by Shapley values. This would require analyzing the interplay between the exploration-exploitation trade-off and the human's ability to rectify proposals based on Shapley value insights.

### Open Question 3
- Question: How can ShapleyBO be extended to multi-criteria Bayesian optimization while maintaining the interpretability of Shapley values?
- Basis in paper: [explicit] The authors state in the discussion that "extensions of ShapleyBO to multi-criteria BO appear straightforward, as long as additive acquisition functions are used."
- Why unresolved: While additive acquisition functions seem like a natural extension, it is unclear how to decompose the Shapley values into meaningful contributions for each objective in a multi-criteria setting. Additionally, the interpretation of Shapley values becomes more complex when dealing with multiple objectives.
- What evidence would resolve it: A concrete algorithm for extending ShapleyBO to multi-criteria BO, along with a thorough evaluation of its interpretability and performance on benchmark multi-objective optimization problems. This would involve developing methods to visualize and interpret the Shapley values for each objective and understanding how they influence the human's decision-making process.

## Limitations

- Specific implementation details for the Human-Machine Interface (HMI) used in real-world experiments are not provided
- Intervention criteria for human users in the human-in-the-loop experiments are not explicitly defined
- Empirical evaluation is limited to a single real-world application (exosuit personalization)

## Confidence

- **High Confidence**: The theoretical framework connecting Shapley values to Bayesian optimization contributions is well-established
- **Medium Confidence**: The empirical demonstration of improved human-AI collaboration through lower regret metrics is convincing but limited to a single real-world application
- **Medium Confidence**: The extension to disentangle aleatoric and epistemic uncertainty contributions is conceptually sound, but the practical impact on user decision-making is not thoroughly evaluated

## Next Checks

1. Implement ShapleyBO on multiple benchmark optimization problems (beyond the provided synthetic functions) to assess generalizability across different problem landscapes and parameter dimensionalities.
2. Conduct a user study with controlled conditions to quantify the actual impact of Shapley value explanations on human decision quality and confidence, isolating the effect from the Bayesian optimization performance itself.
3. Evaluate the computational overhead of ShapleyBO in resource-constrained settings and analyze the tradeoff between interpretability benefits and runtime costs for different problem scales.