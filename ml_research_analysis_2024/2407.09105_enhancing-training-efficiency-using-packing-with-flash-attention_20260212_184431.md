---
ver: rpa2
title: Enhancing Training Efficiency Using Packing with Flash Attention
arxiv_id: '2407.09105'
source_url: https://arxiv.org/abs/2407.09105
tags:
- packing
- posid
- padding
- fixedlengthpacking
- position
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Packing with Position IDs was implemented in Hugging Face Transformers
  4.44 to address inefficiencies from padding in LLM training. The method concatenates
  multiple training examples into a single tensor and uses position IDs to mask example
  boundaries, preventing cross-attention between packed examples.
---

# Enhancing Training Efficiency Using Packing with Flash Attention

## Quick Facts
- **arXiv ID**: 2407.09105
- **Source URL**: https://arxiv.org/abs/2407.09105
- **Reference count**: 17
- **Primary result**: Up to 2x faster training and lower memory usage while maintaining validation loss comparable to padding

## Executive Summary
This paper addresses the inefficiency of padding in LLM training by implementing a packing method using position IDs in Hugging Face Transformers 4.44. The approach concatenates multiple training examples into a single tensor and uses position IDs to mask example boundaries, preventing cross-attention between packed examples. Three collating strategies were tested: online minibatch, offline batch, and bin-packing-based sampling. The results demonstrate significant throughput improvements across 10 supported models while maintaining comparable validation loss to traditional padding approaches.

## Method Summary
The method implements packing with position IDs to reduce padding overhead in LLM training. It concatenates multiple training examples into a single tensor and uses position IDs to demarcate boundaries between examples, preventing cross-attention. The implementation supports three collating strategies: online minibatch packing (best tradeoff between speed and loss), offline batch packing (maximum throughput but worse loss), and bin-packing-based sampling (minor additional gains). The solution requires models to support Flash Attention and expose position IDs, with modifications to the attention forward pass to handle position-based masking.

## Key Results
- Up to 2x faster training throughput compared to padding-based approaches
- Lower memory usage through efficient packing of sequences
- Validation loss comparable to padding while achieving significant speed improvements
- Online minibatch packing provides optimal tradeoff between throughput and loss behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Position IDs allow packing multiple sequences into a single tensor without cross-contamination by masking example boundaries.
- Mechanism: Position IDs track the start of each sequence in a packed tensor, enabling Flash Attention to compute attention only within each sequence. When attention_mask is None and the number of examples exceeds the batch size, cu_seq_len is derived from position_ids to apply correct masking.
- Core assumption: The model supports Flash Attention and exposes position IDs for each token.
- Evidence anchors:
  - [abstract]: "This capability has now been added to Hugging Face Transformers 4.44. We analyse this new feature and show the benefits across different variations of packing."
  - [section]: "To avoid cross-contamination while packing examples, we propose to utilize position IDs to demarcate boundaries of individual examples in a packed sequence."

### Mechanism 2
- Claim: Online minibatch packing with Position IDs provides the best tradeoff between throughput and loss behavior.
- Mechanism: Sequences are packed into a tensor of shape (1, sum of sequence lengths) for each minibatch, and position IDs are generated to mark sequence boundaries. This allows maximal GPU utilization while preserving the number of optimization steps, maintaining validation loss comparable to padding.
- Core assumption: Packing fewer sequences per batch preserves the loss reduction pattern while still improving throughput.
- Evidence anchors:
  - [abstract]: "Online minibatch packing provided the best trade-off between speed and loss behavior."
  - [section]: "Due to the packing of fewer examples in each pack, minibatch packing does not achieve the maximal throughput, however it achieves the same optimal loss pattern and hence validation loss, as the inefficient padding-based approach."

### Mechanism 3
- Claim: Bin-packing-based sampling coupled with Position IDs yields minor additional gains in throughput and loss behavior.
- Mechanism: The Multi-pack sampler selects sequences to pack together based on a bin-packing heuristic, optimizing GPU utilization further. Position IDs ensure proper masking of packed sequences.
- Core assumption: The sample selection method can be optimized to further improve packing efficiency without degrading model performance.
- Evidence anchors:
  - [abstract]: "Coupling with bin-packing-based sampling yielded minor additional gains."
  - [section]: "One may wish to leverage bin-packing-type sample selection algorithms such as [6] in conjunction with the solution provided by Packing with PositionIDs."

## Foundational Learning

- **Flash Attention**
  - Why needed here: Flash Attention is a faster attention mechanism that reduces memory usage and increases throughput, essential for efficient packing.
  - Quick check question: What is the primary advantage of Flash Attention over standard attention?

- **Position IDs**
  - Why needed here: Position IDs track the position of each token within a sequence, enabling proper masking when sequences are packed together.
  - Quick check question: How do Position IDs help prevent cross-contamination when packing sequences?

- **Masking in Transformers**
  - Why needed here: Masking ignores padding tokens and prevents tokens from attending to tokens in other sequences when packed together.
  - Quick check question: What is the purpose of masking in transformer models?

## Architecture Onboarding

- **Component map**: DataCollatorWithFlattening -> PaddingFreeCollator -> _flash_attention_forward()

- **Critical path**:
  1. Tokenize input sequences
  2. Pack sequences using DataCollatorWithFlattening or PaddingFreeCollator
  3. Generate position IDs for packed sequences
  4. Pass packed sequences and position IDs to model
  5. Model applies masking based on position IDs and computes attention

- **Design tradeoffs**:
  - Throughput vs. Loss: Offline packing maximizes throughput but may degrade loss behavior; online minibatch packing provides better tradeoff
  - Complexity vs. Benefit: Bin-packing-based sampling offers minor gains but adds complexity

- **Failure signatures**:
  - If position IDs are not properly generated, tokens may attend to tokens in other sequences
  - If the model does not support Flash Attention or expose position IDs, packing with Position IDs fails

- **First 3 experiments**:
  1. Test packing with Position IDs on a small dataset with a model that supports both Flash Attention and position IDs
  2. Compare throughput and loss behavior of online minibatch packing vs. offline packing
  3. Evaluate the impact of bin-packing-based sampling on throughput and loss behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Packing with Position IDs vary across different model architectures beyond the 10 tested models?
- Basis in paper: [explicit] The paper tested 10 of 14 supported models and notes that Gemma-7B and Qwen1.5-MoE-A2.7B showed different behavior from others
- Why unresolved: The paper only tested 10 models, leaving 4 supported models unexamined, and the behavioral differences in Gemma-7B and Qwen1.5-MOE-A2.7B suggest architecture-specific variations
- What evidence would resolve it: Comprehensive testing of all 14 supported models across multiple datasets with detailed performance metrics and loss analysis

### Open Question 2
- Question: What is the optimal balance between packing density and validation loss across different sequence length distributions?
- Basis in paper: [inferred] The paper notes that offline packing provides highest throughput but worse validation loss compared to minibatch packing, while showing different optimal approaches for FLAN (mostly short sequences) vs OrcaMath (longer sequences)
- Why unresolved: The paper doesn't systematically explore the trade-off space between packing density and validation loss across varying sequence length distributions
- What evidence would resolve it: Controlled experiments varying packing density and measuring validation loss across multiple datasets with different sequence length characteristics

### Open Question 3
- Question: How does Packing with Position IDs perform in distributed training settings with multiple GPUs compared to single GPU?
- Basis in paper: [explicit] The paper mentions that bin-packing-based sample selection methods are used in distributed computing settings but doesn't evaluate this approach
- Why unresolved: All experiments were conducted on a single A100-80GB node with 8 GPUs using FSDP, but the interaction with distributed bin-packing methods wasn't evaluated
- What evidence would resolve it: Distributed training experiments comparing packing strategies across multiple nodes with varying GPU counts and batch sizes

## Limitations

- Limited testing scope: Only 10 out of 14 supported models were evaluated
- Single-node testing: All experiments conducted on a single A100-80GB node, limiting distributed training insights
- Quantitative gaps: Minor additional gains from bin-packing-based sampling not precisely quantified

## Confidence

**High Confidence**: The general principle that reducing padding improves GPU utilization is well-established. The claim that position IDs can be used for masking within packed sequences is supported by Hugging Face implementation details.

**Medium Confidence**: Experimental results showing throughput improvements and loss preservation are credible given controlled experimental setup, but limited model and dataset scope reduces broader applicability confidence.

**Low Confidence**: Specific claims about bin-packing-based sampling providing "minor additional gains" lack quantitative support in the abstract, making practical value difficult to assess.

## Next Checks

1. **Position ID Masking Verification**: Create a minimal test case with two sequences of different lengths packed together. Instrument the model to verify that attention scores between tokens from different sequences are properly masked to zero using position IDs.

2. **Cross-Model Generalization Test**: Implement the packing mechanism with position IDs on a model not in the original 10-model set (e.g., a custom model or different architecture family). Measure whether the same throughput improvements and loss preservation hold.

3. **Extreme Sequence Length Variance**: Test the packing strategy on datasets with highly variable sequence lengths (e.g., some sequences 10x longer than others). Measure degradation in packing efficiency and any corresponding impact on validation loss.