---
ver: rpa2
title: 'SafeLLM: Domain-Specific Safety Monitoring for Large Language Models: A Case
  Study of Offshore Wind Maintenance'
arxiv_id: '2410.10852'
source_url: https://arxiv.org/abs/2410.10852
tags:
- safety
- maintenance
- llms
- wang
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of using Large Language Models\
  \ (LLMs) in safety-critical contexts, specifically for Offshore Wind (OSW) turbine\
  \ maintenance, by developing a domain-specific safety monitoring system called SafeLLM.\
  \ The core method introduces a specialized conversational agent that employs statistical\
  \ techniques, particularly Empirical Cumulative Distribution Function (ECDF) statistical\
  \ distance measures like Wasserstein (Earth Mover\u2019s) Distance (EMD), alongside\
  \ traditional Cosine similarity, to calculate distances between sentence embeddings\
  \ for detecting hallucinations and filtering unsafe LLM outputs."
---

# SafeLLM: Domain-Specific Safety Monitoring for Large Language Models: A Case Study of Offshore Wind Maintenance

## Quick Facts
- arXiv ID: 2410.10852
- Source URL: https://arxiv.org/abs/2410.10852
- Reference count: 8
- Primary result: SafeLLM uses ECDF statistical distance measures (EMD) alongside cosine similarity to detect hallucinations and filter unsafe LLM outputs for offshore wind maintenance

## Executive Summary
SafeLLM addresses the critical challenge of deploying Large Language Models in safety-critical industrial contexts by developing a domain-specific safety monitoring system for offshore wind turbine maintenance. The system employs statistical techniques, particularly Empirical Cumulative Distribution Function (ECDF) measures like Wasserstein distance (EMD), alongside traditional cosine similarity, to detect hallucinations and filter unsafe outputs. Tested on ChatGPT-4 generated sentences simulating offshore wind maintenance scenarios, the approach achieved 87.5-92.5% accuracy with cosine similarity and 50.0-85.0% with EMD across ten categories, demonstrating the viability of statistical distance measures for safety-critical LLM applications.

## Method Summary
SafeLLM implements a conversational agent that monitors LLM outputs for safety and hallucinations in offshore wind maintenance contexts. The system generates N responses for each input, computes sentence similarity using both cosine similarity and EMD (Wasserstein distance) on Universal Sentence Encoder embeddings, and compares results against a dictionary of unsafe concepts. Variance thresholding across N samples identifies potential hallucinations, with sentences showing unusually high deviation flagged for review. The methodology was evaluated on ChatGPT-4 generated sentences across ten safety-related categories, using ROC curve analysis with AUC values ranging from 0.65 to 0.98 for EMD.

## Key Results
- Cosine similarity achieved higher overall accuracy (87.5-92.5%) than EMD (50.0-85.0%) in 7 out of 10 categories
- EMD demonstrated competitive performance with AUC values ranging from 0.65 to 0.98 across categories
- The system successfully identified unsafe and hallucinated responses in offshore wind maintenance scenarios
- Variance-based hallucination detection showed promise but requires further fine-tuning with real OSW data

## Why This Works (Mechanism)

### Mechanism 1
- ECDF statistical distance measures (Wasserstein/EMD) detect hallucinations by comparing sentence embedding distributions against a safe reference set
- EMD calculates minimal "work" needed to transform safe reference distribution into candidate's distribution; low values indicate similarity (safe), high values indicate divergence (potential hallucination)
- Core assumption: Embedding space distance correlates with semantic safety; low EMD implies the sentence is within the safe manifold
- Break condition: If the safe reference set is incomplete or biased, EMD may falsely flag safe but novel sentences as hallucinations

### Mechanism 2
- Cosine similarity serves as baseline and complementary filter to EMD, providing higher accuracy in most categories
- Computes cosine of angle between sentence embeddings; values near 1 indicate high similarity and safety, values near 0 indicate divergence
- Core assumption: Pre-trained embedding models produce vectors where angular proximity maps to semantic similarity
- Break condition: If embeddings are poorly calibrated or domain-shifted, cosine similarity may misclassify safe but out-of-distribution sentences

### Mechanism 3
- Variance thresholding across N generated responses isolates hallucinations by flagging sentences with unusually high deviation from the group
- Generate N candidate sentences, compute EMD/Cosine for each against safe dictionary, measure variance; high variance triggers hallucination detection
- Core assumption: Consistent generation across samples indicates grounding in training data; large variance signals divergence
- Break condition: If LLM consistently generates diverse but safe outputs, variance may falsely trigger

## Foundational Learning

- Concept: Sentence embeddings and distributional similarity
  - Why needed here: Both EMD and cosine similarity operate on embeddings; understanding vector space geometry is essential to tune thresholds and interpret scores
  - Quick check question: What is the range of cosine similarity and what does a value of 0.7 indicate relative to safety?

- Concept: Optimal Transport Theory and EMD
  - Why needed here: EMD is grounded in OTT; knowing its formulation helps in choosing cost metrics and interpreting low/high distances
  - Quick check question: How does EMD differ from Euclidean distance when comparing probability distributions?

- Concept: Variance and statistical thresholding
  - Why needed here: Variance-based hallucination detection requires understanding how to set and tune thresholds for anomaly detection
  - Quick check question: If you generate 10 sentences with similarity scores [0.9, 0.91, 0.89, 0.92, 0.88, 0.9, 0.85, 0.87, 0.9, 0.4], which one is flagged as a hallucination?

## Architecture Onboarding

- Component map:
  - Input Alarm Sequence → LLM Generator → Sentence Embedding Layer → Safety Filter (EMD/Cosine vs. Unsafe Concepts Dictionary) → Hallucination Detector (Variance Threshold on N samples) → Output Decision
  - Knowledge Graph (proposed) → Ongoing Training Data Store

- Critical path:
  - Alarm → LLM → Embedding → Safety/Filter → Decision; Hallucination path is parallel but consumes extra generation budget

- Design tradeoffs:
  - EMD offers distributional nuance but is computationally heavier; cosine is faster but may miss subtle semantic drift
  - Larger N improves hallucination detection reliability but increases latency and cost

- Failure signatures:
  - High false positives: thresholds too low or reference dictionary too restrictive
  - High false negatives: thresholds too high or reference dictionary too sparse
  - System slowdown: N too large or EMD used for every sentence without caching

- First 3 experiments:
  1. Baseline: Run SafeLLM with only cosine similarity on a small test set of ChatGPT-generated sentences; record accuracy per category
  2. EMD-only: Replace cosine with EMD, keep same thresholds, compare per-category accuracy and ROC curves
  3. Hybrid: Use both metrics in sequence (cosine first, EMD on failures), tune thresholds to maximize F1 score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold for detecting hallucinations using Wasserstein distance (EMD) across different categories of Offshore Wind maintenance tasks?
- Basis in paper: The paper mentions that the threshold is a hyper-parameter that will be fine-tuned depending on the application, and that limited fine-tuning was performed to show potential with the distance measure
- Why unresolved: The paper only shows preliminary results with limited threshold tuning, and different categories show varying performance, suggesting that an optimal universal threshold may not exist
- What evidence would resolve it: Systematic experimentation with larger datasets and fine-tuning across all categories to determine category-specific optimal thresholds and evaluate their effectiveness in real-world OSW maintenance scenarios

### Open Question 2
- Question: How does the performance of SafeLLM compare when trained on actual OSW maintenance data versus ChatGPT-4 generated synthetic data?
- Basis in paper: The paper acknowledges that confidentiality of data within the OSW sector provides limited access to maintenance records, and that current testing uses ChatGPT-4 generated sentences instead of real OSW data
- Why unresolved: The paper only presents preliminary results using ChatGPT-4 generated data, which may not accurately represent the complexity and nuances of real OSW maintenance scenarios
- What evidence would resolve it: Implementation of SafeLLM with actual OSW maintenance data from industry partners and comparative analysis of performance metrics against the current synthetic data approach

### Open Question 3
- Question: What is the impact of using different sentence embedding models (beyond Universal Sentence Encoder) on the accuracy of hallucination detection and safety filtering?
- Basis in paper: The paper states that Universal Sentence Encoder was used for embedding sentences, but mentions that comparative analysis of other encoder models will be essential in finding the best suited to OSW application
- Why unresolved: The paper only tests one embedding model and does not explore how different embedding approaches might affect the performance of SafeLLM
- What evidence would resolve it: Systematic testing of SafeLLM using multiple sentence embedding models (e.g., BERT, RoBERTa, Sentence-BERT) with the same dataset to compare detection accuracy and identify optimal embedding approaches for OSW maintenance contexts

## Limitations
- Evaluation relies on synthetic ChatGPT-4 generated sentences rather than real OSW maintenance data, raising concerns about ecological validity
- Exact thresholds used for each category are not specified, making precise reproduction challenging without extensive parameter tuning
- EMD implementation details (embedding model, cost metric, distance calculation) are not fully specified, potentially affecting reproducibility

## Confidence
- **High confidence**: The core methodology of combining ECDF statistical distances (EMD) with cosine similarity for sentence embedding comparison is well-founded and clearly described
- **Medium confidence**: The reported accuracy figures (87.5-92.5% for cosine, 50.0-85.0% for EMD) are credible but may not generalize to real-world OSW data
- **Low confidence**: The hallucination detection via variance thresholding across N samples is theoretically sound but lacks empirical validation and specific implementation details

## Next Checks
1. **Domain transfer test**: Apply SafeLLM to real OSW maintenance logs and technician communications to assess performance degradation compared to synthetic data
2. **Threshold sensitivity analysis**: Systematically vary EMD and cosine thresholds across all categories to identify optimal operating points and measure robustness
3. **Reference dictionary expansion**: Test SafeLLM's performance with progressively larger unsafe concepts dictionaries to evaluate how coverage affects false positive/negative rates