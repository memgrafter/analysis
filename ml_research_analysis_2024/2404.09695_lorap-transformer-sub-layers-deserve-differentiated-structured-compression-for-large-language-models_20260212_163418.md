---
ver: rpa2
title: 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression
  for Large Language Models'
arxiv_id: '2404.09695'
source_url: https://arxiv.org/abs/2404.09695
tags:
- ratio
- compression
- uni00000013
- pruning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work observes that different transformer sub-layers have varying
  low-rank properties, motivating a differentiated compression strategy. For the multi-head
  self-attention (MHA) sub-layer, which exhibits a strong low-rank structure, it proposes
  activation-weighted SVD (AWSVD) with parameter allocation based on low-rank degrees.
---

# LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models

## Quick Facts
- arXiv ID: 2404.09695
- Source URL: https://arxiv.org/abs/2404.09695
- Authors: Guangyan Li; Yongqiang Tang; Wensheng Zhang
- Reference count: 24
- Key outcome: Differentiated compression for MHA (low-rank) and FFN (channel pruning) sub-layers achieves superior perplexity and task classification with or without fine-tuning

## Executive Summary
This paper observes that different transformer sub-layers have varying low-rank properties, motivating a differentiated compression strategy. For the multi-head self-attention (MHA) sub-layer, which exhibits a strong low-rank structure, it proposes activation-weighted SVD (AWSVD) with parameter allocation based on low-rank degrees. For the feed-forward network (FFN) sub-layer, which lacks low-rank structure, it applies gradient-free structured channel pruning while retaining the least important 1% of parameters. Experiments on LLaMA models show that this approach outperforms previous structured compression methods at multiple compression ratios, achieving superior zero-shot perplexity and task classification performance with or without fine-tuning.

## Method Summary
The method applies differentiated structured compression to transformer sub-layers. For MHA sub-layers, it uses activation-weighted SVD (AWSVD) with parameter allocation based on low-rank degrees, weighting each weight entry by the ℓ2 norm of its corresponding input activation. For FFN sub-layers, it applies gradient-free structured channel pruning while retaining the least important 1% of parameters. After compression, LoRA fine-tuning is used to recover performance on calibration data.

## Key Results
- Differentiated compression for MHA (low-rank) and FFN (channel pruning) sub-layers achieves superior perplexity and task classification with or without fine-tuning
- AWSVD improves compression by weighting SVD reconstruction according to input activation importance
- Retaining the least important 1% of weights in FFN pruning preserves performance by protecting critical but low-magnitude parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MHA sub-layers exhibit a stronger low-rank property than FFN sub-layers, enabling more effective compression via low-rank approximation.
- **Mechanism**: SVD decomposes the MHA weight matrices into low-rank matrices, exploiting their concentrated singular value spectrum.
- **Core assumption**: The energy distribution across singular values in MHA matrices is more skewed (i.e., fewer singular values carry most of the information) compared to FFN matrices.
- **Evidence anchors**:
  - [abstract] "the multi-head self-attention (MHA) sub-layer of Transformer exhibits noticeable low-rank structure, while the feed-forward network (FFN) sub-layer does not."
  - [section] "we discover that the weight matrices in MHA sub-layer have different low-rank degrees. Thus, a novel parameter allocation scheme according to the discrepancy of low-rank degrees is devised."
  - [corpus] No direct evidence; corpus neighbors focus on low-rank compression but do not validate the MHA vs FFN distinction.
- **Break condition**: If the singular value distribution in MHA becomes less skewed (e.g., due to task-specific fine-tuning), the low-rank approximation may yield diminishing returns.

### Mechanism 2
- **Claim**: AWSVD improves compression by weighting SVD reconstruction according to input activation importance.
- **Mechanism**: Each weight entry's importance is scored using the ℓ2 norm of its corresponding input activation; SVD is performed on the weighted matrix, yielding a better low-rank approximation for the same rank.
- **Core assumption**: Columns with larger activation norms correspond to more critical weights; scaling by these norms during SVD aligns the approximation with task-relevant features.
- **Evidence anchors**:
  - [section] "we propose an Activation Weighted SVD (AWSVD) method, which evaluates the weight importance in terms of the ℓ2 norm of the corresponding input activations."
  - [section] "the weighted matrix exhibits a stronger low-rank property when compared with the original matrix."
  - [corpus] No direct evidence; AWSVD appears unique to this paper.
- **Break condition**: If input activations become uniform across columns (e.g., after batch normalization), the weighting would have little effect.

### Mechanism 3
- **Claim**: Retaining the least important 1% of weights in FFN pruning preserves performance by protecting critical but low-magnitude parameters.
- **Mechanism**: During channel pruning, parameters with the lowest importance scores are not pruned, maintaining a minimal set of weights deemed vital for model performance.
- **Core assumption**: Some weights with low magnitude or activation are essential for difficult downstream tasks (Junk DNA Hypothesis).
- **Evidence anchors**:
  - [abstract] "we discover that the least important 1% of parameter actually play a vital role in model performance."
  - [section] "Previous studies commonly prune the least important parts. However, we observe that the least important 1% of parameters play a vital role in model performance."
  - [corpus] No direct evidence; this hypothesis is introduced here.
- **Break condition**: If the 1% threshold is too high, unnecessary parameters may be retained; if too low, performance may degrade.

## Foundational Learning

- **Concept**: Low-rank matrix approximation (SVD).
  - Why needed here: LoRAP uses SVD to compress MHA weight matrices, so understanding rank-k approximation and singular value decay is essential.
  - Quick check question: If a matrix has 100 singular values and 80% of its Frobenius norm is captured by the top 10, what is the approximate rank needed to preserve most energy?

- **Concept**: Importance scoring via input activations.
  - Why needed here: AWSVD and structured pruning rely on scoring weights based on input activation norms; this informs how to prioritize retention.
  - Quick check question: Given a weight matrix W and input activations X, how is the importance score I(W_ij) computed?

- **Concept**: Group-level channel pruning.
  - Why needed here: FFN compression prunes entire groups of interconnected channels to preserve functional dependencies.
  - Quick check question: If a down-projection weight at index i is pruned, which other weights in the same group must also be removed?

## Architecture Onboarding

- **Component map**: Calibration data -> Input activations -> AWSVD on MHA matrices -> Group-level pruning on FFN matrices -> LoRA fine-tuning -> Compressed model
- **Critical path**:
  1. Forward pass on calibration data to compute input activations.
  2. AWSVD on MHA matrices (weighted by activation norms).
  3. Group-level pruning on FFN matrices (retain 1% lowest importance).
  4. LoRA fine-tuning with small learning rate.
- **Design tradeoffs**:
  - AWSVD vs plain SVD: AWSVD may yield better compression but requires computing activation norms.
  - Channel pruning vs unstructured pruning: Channel pruning is hardware-friendly but may remove more useful information.
  - Retaining 1% lowest importance: Protects critical weights but may retain some irrelevant ones.
- **Failure signatures**:
  - If perplexity spikes after compression, likely the parameter allocation between MHA matrices is suboptimal.
  - If accuracy drops sharply without fine-tuning, the 1% retention threshold may be insufficient.
  - If training diverges after LoRA, the LoRA learning rate or rank may be too high.
- **First 3 experiments**:
  1. Apply AWSVD to MHA matrices and measure singular value decay vs plain SVD.
  2. Vary the parameter allocation ratio in MHA (e.g., 1:2 vs 1:3) and observe perplexity changes.
  3. Test different retention thresholds (0.5%, 1%, 2%) in FFN pruning and measure impact on downstream task accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different weight matrices within the MHA sub-layer vary in their low-rank properties, and what implications does this have for parameter allocation?
- Basis in paper: [explicit] The paper explicitly states that weight matrices in the MHA sub-layer have different low-rank degrees and proposes allocating more parameters to matrices with poorer low-rank properties.
- Why unresolved: While the paper proposes a parameter allocation scheme based on low-rank degrees, it does not provide a detailed analysis of how these properties vary across different weight matrices or the impact of different allocation strategies.
- What evidence would resolve it: Detailed empirical studies comparing the performance of various parameter allocation strategies based on the low-rank properties of different weight matrices within the MHA sub-layer.

### Open Question 2
- Question: What is the impact of retaining the least important 1% of parameters during structured pruning on the overall model performance?
- Basis in paper: [explicit] The paper observes that the least important 1% of parameters play a vital role in model performance and suggests retaining these parameters under a fixed parameter budget.
- Why unresolved: The paper suggests retaining the least important 1% of parameters but does not explore the impact of retaining different proportions of these parameters or the underlying reasons for their importance.
- What evidence would resolve it: Experiments varying the proportion of least important parameters retained and analysis of their contribution to model performance across different tasks and model sizes.

### Open Question 3
- Question: How does the proposed activation-weighted SVD method compare to other low-rank approximation techniques in terms of compression efficiency and performance?
- Basis in paper: [explicit] The paper introduces an activation-weighted SVD method for compressing the MHA sub-layer but does not provide a comprehensive comparison with other low-rank approximation techniques.
- Why unresolved: While the paper demonstrates the effectiveness of the proposed method, it does not compare it to other low-rank approximation techniques in terms of compression efficiency and performance across different tasks and model sizes.
- What evidence would resolve it: Comparative studies evaluating the performance of the activation-weighted SVD method against other low-rank approximation techniques, including standard SVD, low-rank approximation based on activation magnitudes, and low-rank approximation based on weight magnitudes.

## Limitations
- The claim that MHA sub-layers exhibit stronger low-rank properties than FFN sub-layers lacks quantitative validation through singular value decay analysis.
- The AWSVD method is not benchmarked against standard weighted SVD variants or demonstrated to consistently outperform uniform weighting.
- The 1% retention threshold for FFN pruning is not empirically validated through ablation studies with alternative thresholds.

## Confidence
- **High confidence**: The overall framework of applying differentiated compression to MHA and FFN sub-layers is technically sound and supported by the experimental results on LLaMA models.
- **Medium confidence**: The AWSVD method and the 1% retention threshold for FFN pruning are likely beneficial but require more extensive validation across different model sizes and tasks.
- **Low confidence**: The specific parameter allocation scheme between MHA matrices and the exact impact of AWSVD weighting are not fully validated and may be sensitive to model-specific factors.

## Next Checks
1. Compute and compare the singular value decay curves for MHA and FFN weight matrices across multiple layers and model sizes to quantify the low-rank property differences.
2. Implement a baseline weighted SVD using uniform activation norms and compare its compression performance against AWSVD across different MHA configurations and compression ratios.
3. Conduct an ablation study varying the retention threshold (0.5%, 1%, 2%, 5%) in FFN pruning and measure the impact on downstream task accuracy and perplexity to determine the optimal threshold.