---
ver: rpa2
title: Compact Language Models via Pruning and Knowledge Distillation
arxiv_id: '2407.14679'
source_url: https://arxiv.org/abs/2407.14679
tags:
- pruning
- retraining
- loss
- training
- minitron
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a method to compress large language models\
  \ (LLMs) by pruning and knowledge distillation, eliminating the need to train each\
  \ model variant from scratch. The authors explore structured pruning across multiple\
  \ axes\u2014neurons, attention heads, embedding channels, and depth\u2014and combine\
  \ them with efficient retraining using only a small fraction of the original training\
  \ data."
---

# Compact Language Models via Pruning and Knowledge Distillation

## Quick Facts
- arXiv ID: 2407.14679
- Source URL: https://arxiv.org/abs/2407.14679
- Reference count: 40
- One-line primary result: Achieves 40× fewer training tokens while maintaining performance through pruning and knowledge distillation

## Executive Summary
This paper proposes a method to compress large language models (LLMs) by pruning and knowledge distillation, eliminating the need to train each model variant from scratch. The authors explore structured pruning across multiple axes—neurons, attention heads, embedding channels, and depth—and combine them with efficient retraining using only a small fraction of the original training data. They develop best practices for pruning and distillation, then apply them to compress the Nemotron-4 15B model into smaller variants, named Minitron. The resulting Minitron models (8B and 4B) achieve comparable or better performance than similarly sized models while requiring up to 40× fewer training tokens.

## Method Summary
The approach involves structured pruning across neurons, attention heads, embedding channels, and depth, followed by retraining using knowledge distillation with minimal data (<3% of original). The method uses activation-based importance estimation for width axes and perplexity/block importance for depth pruning. A lightweight neural architecture search identifies optimal compressed architectures from feasible candidates, which are then fully retrained. The technique is applied to compress Nemotron-4 15B into Minitron 8B and 4B models using 1.8 billion tokens for retraining.

## Key Results
- Minitron 8B outperforms Nemotron-3 8B and Llama-2 7B and matches Mistral-7B, Gemma 7B, and Llama-3 8B on downstream tasks
- Achieves up to 16% improvement in MMLU scores compared to training from scratch
- Reduces training costs by 1.8× for the full model family
- Requires up to 40× fewer training tokens compared to traditional training approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning and retraining with knowledge distillation can compress LLMs to smaller sizes without sacrificing accuracy.
- Mechanism: The process leverages structured pruning across multiple axes (neurons, attention heads, embedding channels, depth) followed by retraining with a small fraction of the original training data using knowledge distillation from the unpruned model.
- Core assumption: Knowledge distillation can effectively transfer knowledge from a larger model to a smaller, pruned version, enabling accuracy recovery with minimal retraining data.
- Evidence anchors:
  - [abstract] "This paper proposes a method to compress large language models (LLMs) by pruning and knowledge distillation, eliminating the need to train each model variant from scratch."
  - [section] "In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.472, average citations=0.0. Top related titles: Model Compression and Efficient Inference for Large Language Models: A Survey, TF3-RO-50M: Training Compact Romanian Language Models from Scratch on Synthetic Moral Microfiction, A Free Lunch in LLM Compression: Revisiting Retraining after Pruning.
- Break condition: If knowledge distillation fails to transfer sufficient knowledge, or if the pruned model loses critical information that cannot be recovered through retraining, accuracy will degrade.

### Mechanism 2
- Claim: Iterative pruning across multiple axes (depth, width, attention, embedding) is more effective than single-axis pruning.
- Mechanism: The approach combines pruning of neurons, attention heads, embedding channels, and model depth in a coordinated manner, using activation-based importance estimation to guide pruning decisions.
- Core assumption: Different architectural components contribute differently to model performance, and their combined pruning can achieve higher compression rates without significant accuracy loss.
- Evidence anchors:
  - [section] "Through our experiments, we gain valuable non-trivial insights on the metrics and hyper-parameters to use for each axis and how to effectively combine axes for higher compression rates."
  - [section] "We also investigate in detail how a pruned model can be efficiently retrained for optimal performance using minimal additional data."
  - [corpus] Weak or missing corpus evidence for combined multi-axis pruning effectiveness.
- Break condition: If the combination of pruning axes removes too much information simultaneously, the model may become too degraded to recover through retraining.

### Mechanism 3
- Claim: Lightweight neural architecture search combined with minimal retraining can identify optimal compressed architectures.
- Mechanism: The approach enumerates feasible architectures meeting parameter budget constraints, then performs lightweight retraining on candidates to stabilize rankings before selecting the best performer.
- Core assumption: Relative performance rankings of pruned architectures become stable after a small amount of retraining, allowing efficient identification of optimal compressed models.
- Evidence anchors:
  - [section] "At this stage, while it's possible to further reduce the search space size using strategies such as genetic search and/or Bayesian optimization, we found that sticking to commonly used neuron, head and embedding dimensions, along with a reasonably narrow target parameter range (less than 1 billion) was sufficient to obtain tractable solution sets (less than 20 candidates)."
  - [section] "We perform a search on multiple axes: number of layers, attention head count, MLP and embedding dimensions to arrive at a set of feasible architectures meeting the parameter budget."
  - [corpus] Weak or missing corpus evidence for effectiveness of lightweight search with minimal retraining.
- Break condition: If relative rankings do not stabilize after lightweight retraining, the search process may select suboptimal architectures.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The paper uses knowledge distillation as the primary method for retraining pruned models, transferring knowledge from the original uncompressed model to the compressed version.
  - Quick check question: How does knowledge distillation differ from conventional training, and why is it more effective for pruned models?

- Concept: Structured Pruning
  - Why needed here: The paper employs structured pruning across multiple axes (neurons, attention heads, embedding channels, depth) to reduce model size while maintaining performance.
  - Quick check question: What are the key differences between structured and unstructured pruning, and why is structured pruning preferred for LLMs?

- Concept: Neural Architecture Search
  - Why needed here: The paper uses a lightweight neural architecture search to identify optimal compressed architectures from a set of feasible candidates.
  - Quick check question: How does the paper's approach to neural architecture search differ from traditional methods, and why is lightweight search preferred?

## Architecture Onboarding

- Component map: Pruning module -> Importance estimation -> Architecture search -> Lightweight retraining -> Full retraining
- Critical path: Compute importance scores -> Prune to target architecture -> Perform lightweight retraining -> Select best architecture -> Full retraining
- Design tradeoffs: Trades off between compression rate and accuracy, using knowledge distillation to minimize accuracy loss while achieving high compression. Balances search space size against computational efficiency.
- Failure signatures: Pruning removes too much information, leading to accuracy collapse; knowledge distillation fails to transfer sufficient knowledge; search process selects suboptimal architectures due to unstable rankings.
- First 3 experiments:
  1. Test single-axis pruning (e.g., only depth pruning) vs multi-axis pruning to verify combined approach effectiveness.
  2. Compare knowledge distillation vs conventional training for pruned models to validate retraining strategy.
  3. Test lightweight vs full neural architecture search to confirm efficiency of the search approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed activation-based importance estimation strategy for structured pruning generalize effectively to larger models beyond the 15B parameter scale, such as those with hundreds of billions of parameters?
- Basis in paper: [explicit] The paper focuses on pruning a 15B parameter model and discusses the computational challenges of gradient-based methods for larger models, suggesting activation-based methods as a scalable alternative.
- Why unresolved: The paper does not empirically test the activation-based importance estimation on models significantly larger than 15B parameters.
- What evidence would resolve it: Empirical results demonstrating the effectiveness and scalability of the activation-based importance estimation on models with hundreds of billions of parameters, including comparisons with other pruning methods in terms of accuracy and efficiency.

### Open Question 2
- Question: What is the impact of the proposed pruning and distillation techniques on the model's ability to perform well on long-context tasks and reasoning capabilities, especially after aggressive pruning?
- Basis in paper: [inferred] The paper evaluates models on downstream tasks like MMLU, HellaSwag, and HumanEval, but does not specifically address long-context or complex reasoning tasks, which are crucial for real-world applications.
- Why unresolved: The paper does not include evaluations on long-context or complex reasoning tasks, leaving the impact of pruning on these capabilities unclear.
- What evidence would resolve it: Comprehensive evaluations of pruned models on benchmarks specifically designed for long-context understanding and complex reasoning, such as BIG-bench or tasks requiring multi-step reasoning.

### Open Question 3
- Question: How does the proposed method perform when applied to other model architectures, such as decoder-only models or those with different attention mechanisms like Mamba or RWKV?
- Basis in paper: [inferred] The paper applies the pruning and distillation techniques to a specific model family (Nemotron-4) and discusses the importance of choosing the right architecture for pruning, but does not explore the method's applicability to other architectures.
- Why unresolved: The paper does not test the method on model architectures other than the Nemotron-4 family, leaving the generalizability to other architectures unknown.
- What evidence would resolve it: Experimental results showing the effectiveness of the pruning and distillation techniques on a variety of model architectures, including decoder-only models and those with alternative attention mechanisms, with comparisons to state-of-the-art methods for each architecture.

## Limitations
- Effectiveness of pruning across multiple axes is demonstrated but specific combinations that work best are not fully explained
- Lightweight neural architecture search approach lacks comparison to more comprehensive search methods
- Evaluation focuses primarily on academic benchmarks with limited discussion of real-world deployment considerations

## Confidence

**High Confidence:** The core methodology of combining structured pruning with knowledge distillation is well-established in the literature. The claimed improvements in training efficiency (1.8× reduction) are supported by the described approach.

**Medium Confidence:** The specific performance claims for Minitron models versus competitors need independent verification, particularly the 16% MMLU improvement. The effectiveness of multi-axis pruning combinations may vary depending on the base model architecture.

**Low Confidence:** The scalability of this approach to models significantly larger than 15B parameters is not demonstrated. The long-term stability of pruned models under domain shift is not addressed.

## Next Checks

1. Replicate single-axis vs. multi-axis pruning comparison using the same base model (Nemotron-4 15B) to verify that combined pruning across neurons, attention heads, embedding channels, and depth achieves superior compression rates without accuracy loss.

2. Test knowledge distillation effectiveness by comparing Minitron performance when trained with knowledge distillation versus conventional training from scratch using the same reduced dataset (1.8 billion tokens).

3. Validate neural architecture search efficiency by running the lightweight search with varying amounts of retraining data (e.g., 0.5B, 1.8B, 3B tokens) to confirm that rankings stabilize after the claimed minimal retraining.