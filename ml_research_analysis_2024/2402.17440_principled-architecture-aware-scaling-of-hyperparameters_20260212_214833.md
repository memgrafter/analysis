---
ver: rpa2
title: Principled Architecture-aware Scaling of Hyperparameters
arxiv_id: '2402.17440'
source_url: https://arxiv.org/abs/2402.17440
tags:
- learning
- network
- networks
- neural
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a principled method for scaling learning rates
  and initialization based on neural network architecture. The key idea is to preserve
  the flow of information through the network by scaling layer weights based on their
  in-degree, and to choose learning rates such that pre-activation changes remain
  O(1) during training.
---

# Principled Architecture-aware Scaling of Hyperparameters

## Quick Facts
- arXiv ID: 2402.17440
- Source URL: https://arxiv.org/abs/2402.17440
- Reference count: 40
- Primary result: Architecture-aware scaling of learning rates and initialization based on network topology and kernel size

## Executive Summary
This paper proposes a principled method for scaling learning rates and initialization based on neural network architecture. The key idea is to preserve the flow of information through the network by scaling layer weights based on their in-degree, and to choose learning rates such that pre-activation changes remain O(1) during training. For general DAG networks, the learning rate scales as the inverse square root of the sum of cubed path depths. For CNNs, there is an additional inverse dependence on kernel size. The authors verify these principles on MLPs and CNNs with various topologies, achieving strong correlations between predicted and measured optimal learning rates.

## Method Summary
The method introduces a modified fan-in initialization scheme that scales weights based on the in-degree of each layer, and derives learning rate scaling formulas based on network topology. For DAG networks, the learning rate scales as η* ∝ (Σ_p L_p³)^(-1/2) where L_p is the depth of each path. For CNNs, an additional factor of q^(-1) accounts for kernel size. The initialization variance is set to 2/(n·d_in^(ℓ)) where d_in^(ℓ) is the in-degree. These principles are validated through experiments on CIFAR-10, CIFAR-100, ImageNet16-120, and NAS-Bench-201.

## Key Results
- Strong correlation (r ≈ 0.8-0.9) between predicted and measured optimal learning rates for MLPs and CNNs
- The method can significantly improve accuracy on NAS-Bench-201 and change network rankings
- Current benchmarks may be unreliable due to improper hyperparameter scaling
- Provides a way to scale hyperparameters for novel architectures without expensive tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The initialization scheme preserves the flow of information through the network by scaling weight variances based on the in-degree of each layer.
- Mechanism: When multiple inputs are summed at a layer, the variance of the sum grows with the number of inputs. By dividing the weight variance by the in-degree (2/d_in), the method normalizes this effect so that the expected squared norm of pre-activations remains constant across layers.
- Core assumption: The input distribution is symmetric around zero so that ReLU activations have zero mean and variance half the squared input norm (E[σ²(z)] = ½E[z²]).
- Evidence anchors: [abstract] "By pursuing every parameter to be maximally updated with the same mean squared change in pre-activations..."; [section] "we propose an architecture-aware initialization strategy..."
- Break condition: If inputs are not zero-mean or if the architecture contains non-linearities with non-zero mean outputs, the preservation property fails.

### Mechanism 2
- Claim: The learning rate scales inversely with the square root of the sum of cubed path depths to keep pre-activation changes O(1) in the first training step.
- Mechanism: The analysis tracks how the squared change in a pre-activation depends on all upstream weights. In a ReLU network, each path of depth L_p contributes a factor proportional to η²L_p³ to the change. Summing over all paths and requiring the total to be O(1) yields η* ∝ (Σ_p L_p³)^(-1/2).
- Core assumption: The change in pre-activations is dominated by the first SGD step and that all weights in a layer share the same learning rate at initialization.
- Evidence anchors: [abstract] "For general DAG networks, the learning rate scales as the inverse square root of the sum of cubed path depths."; [section] "we deduce how the learning rate depends on the network topology..."
- Break condition: If the network includes normalization layers (batch norm, layer norm) or non-ReLU activations, the depth³ scaling may not hold.

### Mechanism 3
- Claim: For CNNs, the learning rate has an additional inverse dependence on kernel size due to local connectivity patterns.
- Mechanism: Convolutional layers aggregate information over q neighboring spatial locations. This local aggregation increases the effective depth contribution by a factor of q, so the learning rate is further scaled by q⁻¹.
- Core assumption: The convolutional operation can be modeled as a local linear aggregation followed by ReLU, and the analysis of path contributions extends to this setting.
- Evidence anchors: [abstract] "For CNNs, there is an additional inverse dependence on kernel size."; [section] "We further expand our scaling principles to convolutional neural networks..."
- Break condition: If the CNN uses dilated convolutions, asymmetric padding, or strided convolutions, the simple q⁻¹ factor may be incorrect.

## Foundational Learning

- Concept: Directed Acyclic Graph (DAG) representation of neural architectures
  - Why needed here: The method relies on counting paths and depths in the computational graph; without a DAG formulation, the topology-aware scaling cannot be defined.
  - Quick check question: Given a network with skip connections, can you draw its DAG and count the number of unique input-to-output paths?

- Concept: Variance propagation through random neural networks
  - Why needed here: The initialization scaling is derived by requiring that the variance of pre-activations be preserved across layers; this requires understanding how random weight matrices transform variance.
  - Quick check question: If a layer receives two independent inputs each with variance σ², what is the variance of their sum before scaling?

- Concept: Gradient descent dynamics and pre-activation updates
  - Why needed here: The learning rate scaling is based on the size of the first-step update to pre-activations; understanding how gradients flow backward through the network is essential.
  - Quick check question: For a simple two-layer ReLU network, write the expression for the change in the first hidden layer's pre-activations after one SGD step.

## Architecture Onboarding

- Component map:
  - Input: Data tensor (shape depends on task)
  - DAG topology: Directed acyclic graph specifying connectivity (list of edges (ℓ′, ℓ))
  - Layer ops: Each edge labeled with operation type (Linear, Conv, Skip, Zero)
  - Parameters: Weight matrices W^(ℓ′, ℓ) initialized with variance 2/d_in^(ℓ)
  - Learning rates: η^(ℓ′, ℓ) set per layer using the scaling formula
  - Output: Scalar or vector depending on task

- Critical path:
  1. Parse architecture → build DAG
  2. For each vertex ℓ, compute in-degree d_in^(ℓ)
  3. Initialize weights W^(ℓ′, ℓ) ~ N(0, 2/(n·d_in^(ℓ)))
  4. Count all unique input→output paths and their depths L_p
  5. Compute η* = c / sqrt(Σ_p L_p³) for MLPs or η* = c / (sqrt(Σ_p L_p³)·q) for CNNs
  6. Train with scaled η*

- Design tradeoffs:
  - Pros: No hyperparameter search needed for new architectures; principled scaling based on topology
  - Cons: Assumes ReLU activations; does not account for normalization layers; requires DAG enumeration which can be expensive for very large graphs
  - Alternatives: Use adaptive optimizers (Adam) which may reduce sensitivity to initialization; apply layer-wise tuning only for problematic layers

- Failure signatures:
  - Divergence in training: Likely due to incorrect in-degree counting or missing normalization
  - Vanishing updates: Learning rate too small; check if path depth sum is overestimated
  - Poor final accuracy: Initialization may be too conservative; consider scaling up variance slightly

- First 3 experiments:
  1. Implement a simple MLP with two hidden layers and skip connection; apply the method and verify that pre-activation norms are roughly constant across layers.
  2. Take a CNN with kernel size 3 and a residual block; scale the learning rate according to the formula and compare training stability to default He init + fixed LR.
  3. Use NAS-Bench-201 to sample 5 random architectures; apply the method and measure accuracy improvement over default training protocol.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do depth-dependent learning rate scaling principles extend to networks containing normalization layers?
- Basis in paper: [inferred] from the paper's stated limitation "Characterization of depth-dependence of learning rates beyond the first step / early training of networks" and the absence of any discussion of normalization layers
- Why unresolved: The paper's analysis focuses on networks without normalization layers, and the interaction between normalization and learning rate scaling remains unexplored
- What evidence would resolve it: Experiments comparing learning rate scaling with and without normalization layers, showing how batch normalization, layer normalization, or other normalization techniques affect the optimal learning rate dependencies

### Open Question 2
- Question: Can the architecture-aware initialization and learning rate scaling principles be extended to recurrent neural networks and transformers?
- Basis in paper: [inferred] from the paper's focus on MLPs and CNNs, with no mention of sequential or attention-based architectures
- Why unresolved: The paper's mathematical framework is developed specifically for DAG architectures with feed-forward connections, and does not address the unique properties of recurrent or attention-based networks
- What evidence would resolve it: Empirical validation of the scaling principles on RNNs, LSTMs, GRUs, and Transformer architectures across different depths, widths, and topologies

### Open Question 3
- Question: What is the impact of using the maximal update (µP) learning rates throughout extended training periods, rather than just in early training?
- Basis in paper: [explicit] from the paper's stated limitation "Characterization of depth-dependence of learning rates beyond the first step / early training of networks"
- Why unresolved: The paper focuses on deriving learning rates based on the first step of optimization, but does not investigate whether these rates remain optimal throughout full training runs
- What evidence would resolve it: Long-term training experiments comparing networks trained with µP-derived learning rates throughout training versus those with learning rates tuned for later stages, measuring final performance and training stability

## Limitations
- The analysis assumes ReLU activations and does not account for normalization layers, batch statistics, or other architectural modifications common in modern networks
- The depth³ scaling relies on first-step analysis and may not capture long-term training dynamics
- For complex DAGs, path enumeration can become computationally expensive, though the authors suggest approximations may be possible

## Confidence
- **High**: The initialization scheme's dependence on in-degree is well-founded in variance propagation theory. Empirical results show strong correlations (r ≈ 0.8-0.9) between predicted and measured optimal learning rates.
- **Medium**: The depth³ scaling for learning rates follows logically from the first-step analysis, but the assumption that this captures the true optimal learning rate is not fully validated across training epochs.
- **Medium**: The CNN kernel-size scaling is theoretically motivated but relies on the same first-step assumptions and may not hold for non-standard convolution operations.

## Next Checks
1. **Path depth calculation verification**: Implement a test suite that enumerates all paths in small DAGs (e.g., residual blocks, inception modules) and verifies the sum of cubed depths matches manual calculations.
2. **Long-term stability test**: Train networks using the scaled learning rates for multiple epochs and monitor whether the first-step analysis continues to predict optimal rates, or if adaptation occurs.
3. **Normalization layer integration**: Extend the framework to include batch normalization or layer normalization by deriving how these layers modify the variance propagation and path depth calculations.