---
ver: rpa2
title: Learning to Generate and Evaluate Fact-checking Explanations with Transformers
arxiv_id: '2410.15669'
source_url: https://arxiv.org/abs/2410.15669
tags:
- explanations
- arxiv
- quality
- explanation
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces transformer-based models for generating and
  evaluating fact-checking explanations. It addresses the challenge of providing human-accessible
  explanations for automated fact-checking verdicts by developing both generative
  and evaluation models.
---

# Learning to Generate and Evaluate Fact-checking Explanations with Transformers
## Quick Facts
- arXiv ID: 2410.15669
- Source URL: https://arxiv.org/abs/2410.15669
- Reference count: 20
- The study introduces transformer-based models for generating and evaluating fact-checking explanations, achieving a ROUGE-1 score of 47.77 for generation and moderately strong correlation with human judgements for evaluation.

## Executive Summary
This paper presents transformer-based models for generating and evaluating fact-checking explanations to address the challenge of providing human-accessible explanations for automated fact-checking verdicts. The authors develop both generative models (T5 and Longformer-based) for creating explanations from claim-evidence pairs and evaluation models (DeBERTa-based) for assessing explanation quality across multiple dimensions. Through extensive experiments, they demonstrate that their generative model achieves a ROUGE-1 score of 47.77 when provided with high-quality evidence, while their metric learning approach shows moderate correlation with human judgements on objective quality dimensions.

## Method Summary
The approach involves two main components: explanation generation and quality evaluation. For generation, T5 and Longformer models are fine-tuned on claim-evidence-explanation triples from fact-checking sources like BBC, FullFact, and FactCheck. The input format concatenates claim and evidence with a special token, and the model auto-regressively generates explanations. For evaluation, DeBERTa classifiers are trained on human-annotated explanations to predict quality scores across dimensions like self-contradiction, hallucination, convincingness, and overall quality. The authors use crowdsourced annotations from Amazon Mechanical Turk, applying quality controls including agreement thresholds and tie-breaking with ChatGPT.

## Key Results
- The generative model achieves a ROUGE-1 score of 47.77 when generating explanations from high-quality evidence
- Metric learning models show moderately strong correlation with human judgements on objective dimensions, achieving MCC of around 0.7
- Expanding articles with retrieved snippets did not substantially improve performance compared to using snippets alone
- Subjective dimensions like convincingness and overall quality are harder to predict, with only marginal improvements over statistical baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transformer-based sequence-to-sequence models can generate human-accessible fact-checking explanations.
- **Mechanism:** The model takes a concatenated input of claim and evidence, then auto-regressively generates an explanation that contextualizes the verdict. Larger models (t5-large vs t5-base) capture more complex patterns, leading to better explanations.
- **Core assumption:** The quality of generated explanations depends on both the model architecture and the quality/quantity of training data.
- **Evidence anchors:**
  - [abstract] "Based on experimental results, our best performing generative model ROUGE-1 score of 47.77, demonstrating superior performance in generating fact-checking explanations, particularly when provided with high-quality evidence."
  - [section] "Following this, the conducted experiments... indicate that expanding articles with retrieved snippets did not substantially improve performance, compared to using snippets as input alone."
- **Break condition:** If the input evidence is noisy or insufficient, the model generates self-contradictory or hallucinated content, as seen when comparing full articles to Google snippets.

### Mechanism 2
- **Claim:** Metric learning models can predict human judgements on explanation quality across multiple dimensions.
- **Mechanism:** DeBERTa-based classifiers are fine-tuned to predict binary labels for dimensions like contradiction and hallucination, and regression for overall quality. Higher model scale (xxlarge vs base) improves correlation with human ratings.
- **Core assumption:** Human judgements on explanation quality can be approximated by learned metrics, especially for objective dimensions.
- **Evidence anchors:**
  - [abstract] "the best performing metric learning model showed a moderately strong correlation with human judgements on objective dimensions such as (self)-contradiction and hallucination, achieving a Matthews Correlation Coefficient (MCC) of around 0.7."
  - [section] "Overall, our results indicate that the prediction of human judgements remains a hard task that warrants further academic investigation, even as the best of our optimised models reach around 0.7 MCC on more objective questions."
- **Break condition:** Subjective dimensions like convincingness and overall quality are harder to predict, with only marginal improvements over statistical baselines.

### Mechanism 3
- **Claim:** Human-centered evaluation methods improve alignment between AI-generated explanations and human standards.
- **Mechanism:** Crowdsourced annotations on multiple quality dimensions (self-contradiction, hallucination, convincingness, overall quality) are used to train metric learning models. Filtering low-agreement annotations and using ChatGPT for tie-breaking improves data quality.
- **Core assumption:** Human judgements on explanation quality are noisy but can be aggregated to provide a reliable signal for training.
- **Evidence anchors:**
  - [abstract] "By introducing human-centred evaluation methods and developing specialised datasets, we emphasise the need for aligning Artificial Intelligence (AI)-generated explanations with human judgements."
  - [section] "Investigating the average agreement per annotator, we find that some annotators performed only marginally better than the random selection strategy... To reduce this noise, we regarded only annotations of those annotators whose average agreement was higher than 0.75."
- **Break condition:** If annotator agreement is too low or inconsistent, the learned metrics may not generalize well to new explanations.

## Foundational Learning

- **Concept:** Sequence-to-sequence modeling with transformers
  - **Why needed here:** The task of generating explanations from claim-evidence pairs is naturally framed as a conditional text generation problem.
  - **Quick check question:** Can you explain how the input format "summarize: C \n E." helps the model distinguish between claim and evidence?

- **Concept:** Metric learning for text quality evaluation
  - **Why needed here:** Automated evaluation of explanation quality is crucial for reducing manual assessment burden and improving model training.
  - **Quick check question:** Why is MCC preferred over accuracy for evaluating the binary classifiers in this imbalanced dataset?

- **Concept:** Crowdsourcing and annotation quality control
  - **Why needed here:** Human judgements are essential for training models that align with human standards, but require careful filtering to ensure reliability.
  - **Quick check question:** What are the trade-offs between using majority voting vs. individual annotator agreement thresholds for data selection?

## Architecture Onboarding

- **Component map:** Data collection (Google FactCheck API) → Claim/Evidence/Explanation triples → T5/LED fine-tuning → Generated explanations → Human annotation → DeBERTa fine-tuning → Quality prediction
- **Critical path:** Claim + Evidence → Explanation Generation → Explanation Quality Evaluation
  - Each stage depends on the previous one's output quality.
- **Design tradeoffs:**
  - Model size vs. computational cost (t5-large vs t5-base)
  - Full articles vs. snippets as evidence (quality vs. availability)
  - Binary vs. regression tasks for metric learning (simplicity vs. nuance)
- **Failure signatures:**
  - Low ROUGE scores indicate poor generation quality
  - MCC near 0 suggests metric learning model fails to distinguish quality
  - Low annotator agreement points to ambiguous or subjective quality dimensions
- **First 3 experiments:**
  1. Fine-tune t5-base on full explanation dataset and evaluate ROUGE scores on held-out test set.
  2. Compare t5-base fine-tuned on fullfact vs fullfact-snippets to quantify impact of evidence quality.
  3. Train DeBERTa-base classifier on article contradiction dimension and evaluate MCC against majority baseline.

## Open Questions the Paper Calls Out
- **Open Question 1:** How would the quality of generated fact-checking explanations improve if trained on multi-modal evidence (text, images, videos) rather than text-only?
  - **Basis in paper:** [explicit] The paper notes a key limitation is reliance on text-only evidence, stating "a critical consideration... involves addressing ethical concerns related to fairness, bias and responsibility. Given the societal implications... it is important to recognise that models can perpetuate biases present in their training data. Our approach... focusses on contextualising claims rather than making definitive judgements... However, the generation of explanations themselves might be biased..."
  - **Why unresolved:** The study only evaluated text-based evidence. No experiments were conducted with multi-modal inputs.
  - **What evidence would resolve it:** Experiments comparing explanation quality metrics (ROUGE scores, human evaluations) when models are trained on multi-modal vs text-only evidence for the same claims.

- **Open Question 2:** What is the optimal annotation agreement threshold for training metric learning models to maximize correlation with human judgements while minimizing dataset size?
  - **Basis in paper:** [explicit] The paper experimented with different agreement thresholds (0.69, 0.75) and found "perfect agreement scores were low, which hints at the subjectiveness of the task" and used ChatGPT as a tie-breaker for objective questions.
  - **Why unresolved:** The paper settled on 0.75 as a threshold but didn't systematically explore how different thresholds affect model performance across dimensions.
  - **What evidence would resolve it:** A systematic study varying agreement thresholds and measuring resulting model performance (MCC, Spearman correlation) for each quality dimension.

- **Open Question 3:** How does incorporating human quality judgements directly into the training of generative models affect explanation quality compared to using learned evaluation metrics?
  - **Basis in paper:** [inferred] The paper mentions "an intriguing direction for future research would be to scale up these annotation efforts and directly integrate them into the training process" after noting that learned metrics show only moderate correlation with human judgements.
  - **Why unresolved:** The study only used human judgements for evaluation, not for training the generative models themselves.
  - **What evidence would resolve it:** Experiments comparing explanation quality (human evaluations, automated metrics) from models trained with standard objectives versus models trained with direct human preference optimization incorporating the annotated quality dimensions.

## Limitations
- The reliance on journalist-written explanations as training targets may not fully capture diverse explanation styles
- The evaluation dataset's small size (310 annotated explanations) limits the robustness of the metric learning results
- The finding that full articles don't improve performance over snippets suggests limitations in either the model architecture or evidence distribution

## Confidence
- **High confidence**: The generative model's ROUGE scores and the basic finding that higher-quality evidence produces better explanations
- **Medium confidence**: The correlation between metric learning models and human judgements on objective dimensions (MCC ~0.7)
- **Low confidence**: Performance on subjective dimensions like convincingness and overall quality, where models barely exceed statistical baselines

## Next Checks
1. **Evidence Quality Experiment**: Systematically vary the relevance and completeness of evidence snippets to quantify their impact on generated explanation quality, beyond the binary comparison of snippets vs. full articles.

2. **Cross-Domain Generalization**: Test the trained metric learning models on explanations from different fact-checking organizations or domains to assess whether the learned quality metrics transfer beyond the training distribution.

3. **Human-in-the-Loop Evaluation**: Conduct a controlled study comparing human assessors' agreement with model predictions versus majority vote baselines, specifically for the subjective dimensions where current models underperform.