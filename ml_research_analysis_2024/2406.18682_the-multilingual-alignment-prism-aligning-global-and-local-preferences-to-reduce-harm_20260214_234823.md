---
ver: rpa2
title: 'The Multilingual Alignment Prism: Aligning Global and Local Preferences to
  Reduce Harm'
arxiv_id: '2406.18682'
source_url: https://arxiv.org/abs/2406.18682
tags:
- global
- local
- language
- safety
- harm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Multilingual Aya Red-teaming dataset,
  the first human-annotated collection of harmful prompts across 8 languages, distinguishing
  between global and local harms. It evaluates multilingual safety alignment using
  SFT and DPO, showing that DPO(SFT) reduces harmful generations by 54.7% while achieving
  71% win-rates on general tasks.
---

# The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm

## Quick Facts
- **arXiv ID**: 2406.18682
- **Source URL**: https://arxiv.org/abs/2406.18682
- **Reference count**: 40
- **Primary result**: DPO(SFT) reduces harmful generations by 54.7% while maintaining 71% win-rates on general tasks across 6 languages

## Executive Summary
This paper introduces the Multilingual Aya Red-teaming dataset, the first human-annotated collection of harmful prompts across 8 languages, distinguishing between global and local harms. The study evaluates multilingual safety alignment using supervised fine-tuning (SFT) and direct preference optimization (DPO), demonstrating that DPO(SFT) significantly reduces harmful generations while maintaining general task performance. The approach shows consistent safety improvements across English, Hindi, Arabic, French, Spanish, and Russian, with larger gains observed for underrepresented languages. Notably, training on local harms transfers more effectively to global harm mitigation than the reverse direction.

## Method Summary
The study constructs a multilingual red-teaming dataset with 10,000 prompts across 8 languages, annotated for global and local harms. The researchers evaluate safety alignment through SFT and DPO techniques, with DPO(SFT) emerging as the most effective approach. They conduct comprehensive evaluations across 6 languages (English, Hindi, Arabic, French, Spanish, Russian) using both human judgments and LLM-as-judge evaluations. The methodology includes careful distinction between global harms (widely unacceptable) and local harms (culturally specific), enabling analysis of how different training approaches transfer between harm types.

## Key Results
- DPO(SFT) reduces harmful generations by 54.7% while maintaining 71% win-rates on general tasks
- Safety improvements are consistent across all 6 tested languages
- Training on local harms transfers more effectively to global harm mitigation than the reverse

## Why This Works (Mechanism)
The success stems from the dataset's unique structure that captures both global and local harm distinctions, allowing models to learn nuanced harm boundaries across cultures. DPO(SFT) combines the breadth of SFT with the preference optimization of DPO, creating a more robust alignment that generalizes across languages. The transfer learning effect from local to global harms suggests that culturally specific harm awareness enhances general harm detection capabilities, likely because local harms provide richer contextual signals for harm identification.

## Foundational Learning
- **Multilingual harm annotation**: Understanding harm perception across cultures is essential for creating effective safety datasets. Quick check: Validate annotation consistency across cultural contexts using inter-annotator agreement metrics.
- **SFT vs DPO optimization**: Different alignment approaches have distinct strengths in harm reduction versus general performance. Quick check: Compare performance curves during training to identify optimization dynamics.
- **Harm transfer learning**: Local harm training can enhance global harm detection through contextual generalization. Quick check: Measure transfer efficiency using pre/post training harm classification accuracy.

## Architecture Onboarding
- **Component map**: Dataset (Multilingual Aya Red-teaming) -> Alignment (SFT/DPO) -> Evaluation (Human/LLM-as-judge) -> Performance metrics
- **Critical path**: Data collection → Annotation → Model training → Evaluation → Analysis
- **Design tradeoffs**: Local vs global harm focus, human vs automated evaluation, SFT vs DPO approaches
- **Failure signatures**: Inconsistent harm annotations across cultures, overfitting to specific harm types, performance degradation on general tasks
- **First experiments**: 1) Evaluate baseline model's cross-cultural harm detection accuracy, 2) Test transfer learning from local to global harms, 3) Compare human vs LLM-as-judge evaluation consistency

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Focus on only 6 languages despite dataset spanning 8 languages may limit generalizability
- 10,000 prompt dataset may not capture full diversity of harmful content across languages
- Reliance on LLM-as-judge evaluations may introduce systematic biases in harm assessment

## Confidence
- **High confidence**: DPO(SFT) achieves significant harm reduction while maintaining general task performance; local-to-global harm transfer is more effective than reverse
- **Medium confidence**: Relative performance differences between model initialization strategies; larger gains for underrepresented languages
- **Low confidence**: Generalizability to languages beyond 6 tested; long-term stability of harm reduction improvements

## Next Checks
1. Evaluate safety alignment approach across 10-15 languages to assess generalizability beyond initial 6 tested
2. Conduct longitudinal studies tracking model performance over 6-12 months to assess stability of safety improvements
3. Implement comparative analysis between human-annotated and LLM-as-judge evaluations across different cultural contexts to quantify systematic biases