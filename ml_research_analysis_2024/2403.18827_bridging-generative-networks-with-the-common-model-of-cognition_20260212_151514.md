---
ver: rpa2
title: Bridging Generative Networks with the Common Model of Cognition
arxiv_id: '2403.18827'
source_url: https://arxiv.org/abs/2403.18827
tags:
- production
- system
- memory
- cognitive
- shadow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a theoretical framework for integrating large
  generative network models with cognitive architectures, specifically the Common
  Model of Cognition (CMC). The core idea is to restructure CMC modules into "shadow
  production systems" that interface with predictive neural networks through a middle
  memory layer.
---

# Bridging Generative Networks with the Common Model of Cognition

## Quick Facts
- arXiv ID: 2403.18827
- Source URL: https://arxiv.org/abs/2403.18827
- Authors: Robert L. West; Spencer Eckler; Brendan Conway-Smith; Nico Turcas; Eilene Tomkins-Flanagan; Mary Alexandria Kelly
- Reference count: 7
- One-line primary result: Proposes a theoretical framework for integrating generative network models with cognitive architectures through shadow production systems

## Executive Summary
This paper presents a theoretical framework for integrating large generative network models with the Common Model of Cognition (CMC). The core innovation involves restructuring CMC modules into "shadow production systems" that interface with predictive neural networks through a middle memory layer. This approach enables seamless connection between cognitive reasoning and generative network outputs, addressing the challenge of combining symbolic reasoning with non-symbolic prediction in a unified system. The framework potentially supports complex reasoning, metacognition, and causal inference by allowing both bottom-up network processing and top-down cognitive control.

## Method Summary
The proposed method involves restructuring CMC modules into shadow production systems that operate in parallel with a central production system. These shadow productions monitor outputs from generative networks via a Middle Memory (MM) interface, evaluate them using network tags and working memory context, and place refined information into working memory buffers. The framework uses TD learning to coordinate utility values across central and shadow productions, creating unified learning across symbolic reasoning and network predictions. While no empirical implementation is provided, the method outlines how to theoretically bridge generative networks with cognitive architectures.

## Key Results
- Proposes shadow production systems as parallel modules that interface with generative networks without disrupting central processing
- Introduces Middle Memory as an adaptive interface bridging continuous network predictions with discrete symbolic reasoning
- Suggests TD learning across production types creates unified learning between network predictions and cognitive reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shadow production systems allow bottom-up generative network outputs to influence cognitive control without disrupting the serial bottleneck of the central production system.
- Mechanism: Shadow productions monitor outputs from generative networks via the Middle Memory (MM), evaluate them using both network tags and working memory context, and place refined information into working memory buffers. This allows continuous network processing while the central system operates serially.
- Core assumption: Network outputs can be meaningfully tagged and contextualized without direct interference with central processing cycles.
- Evidence anchors: [abstract] "restructuring modules within the Common Model into shadow production systems that are peripheral to a central production system"; [section] "shadow productions operate in parallel with central productions, yet they do not disrupt the serial bottleneck"; [corpus] Weak - no direct corpus evidence; concept appears unique to this paper
- Break condition: If network outputs cannot be effectively tagged or if shadow productions cause buffer contention with central productions.

### Mechanism 2
- Claim: The Middle Memory (MM) acts as an adaptive interface that bridges continuous network predictions with discrete symbolic reasoning.
- Mechanism: MM receives vector outputs from all generative networks, assigns activation values based on recency, frequency, and spreading activation from working memory, and stores both predictions and graph structures. This allows procedural memory to access both predictive and symbolic information in a unified format.
- Core assumption: Vector representations from different networks can be normalized and integrated within a common activation framework.
- Evidence anchors: [abstract] "a memory system that interfaces between networks and production systems"; [section] "Middle Memory (MM), between the CMC and the various underlying predictive networks"; [corpus] Weak - no direct corpus evidence; appears to be a novel architectural element
- Break condition: If activation-based integration fails to preserve network-specific information or if memory capacity becomes a bottleneck.

### Mechanism 3
- Claim: TD learning applied to both central and shadow productions creates unified learning across symbolic reasoning and network predictions.
- Mechanism: When central productions achieve reward, shadow productions that contributed information to working memory also receive utility boosts. This aligns learning across both levels and enables coordinated skill acquisition.
- Core assumption: Reward signals can be appropriately distributed to multiple production types without causing learning instability.
- Evidence anchors: [section] "when the central production system achieves a reward, all of the shadow productions that contributed...would also get a boost to their utility"; [section] "TD learning algorithms to adjust the utility values of productions"; [corpus] Weak - no direct corpus evidence; appears to be a proposed extension
- Break condition: If reward distribution causes utility inflation or if shadow productions learn irrelevant patterns.

## Foundational Learning

- Concept: Production Systems
  - Why needed here: The entire architecture is built on production system principles for both central and shadow productions, understanding their operation is fundamental to grasping the framework.
  - Quick check question: How does a production system decide which rule to fire when multiple conditions are met?

- Concept: Working Memory and Buffers
  - Why needed here: Working memory buffers are the communication medium between all components, and understanding their role is critical for implementing the interface between networks and productions.
  - Quick check question: What distinguishes working memory buffers from long-term memory in this architecture?

- Concept: Activation and Spreading Activation
  - Why needed here: Activation mechanisms in Middle Memory determine which network predictions and stored chunks are most relevant for current processing, directly affecting system performance.
  - Quick check question: How does spreading activation from working memory affect activation values in Middle Memory?

## Architecture Onboarding

- Component map:
  - Central Production System (Procedural Memory) - handles symbolic reasoning and decision-making
  - Shadow Production Systems - one per peripheral module (perception, motor, emotion, etc.)
  - Middle Memory (MM) - interfaces between generative networks and production systems
  - Generative Networks - underlying predictive models for each modality
  - Working Memory Buffers - communication medium between productions and modules

- Critical path: Network output → MM tagging and activation → shadow production matching → WM buffer update → central production processing → action selection

- Design tradeoffs:
  - Memory vs. Speed: MM must balance comprehensive storage with real-time access requirements
  - Granularity vs. Complexity: Shadow productions can match on fine-grained or coarse features, affecting both accuracy and computational load
  - Integration vs. Isolation: Tight coupling between MM and networks enables better coordination but increases system complexity

- Failure signatures:
  - Network outputs never appear in WM → MM tagging or shadow production matching is broken
  - System becomes unresponsive → buffer contention between shadow and central productions
  - Learning plateaus → TD reward distribution is not functioning correctly

- First 3 experiments:
  1. Implement a single shadow production system for one network type (e.g., visual network) and verify it can place information into WM without blocking central processing.
  2. Test MM activation mechanisms by varying recency and frequency parameters and measuring retrieval accuracy.
  3. Implement basic TD learning and verify utility updates propagate correctly from central to shadow productions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How exactly would the shadow production systems be created and learned within this proposed framework?
- Basis in paper: [explicit] The paper mentions this is a "more difficult issue" and proposes a Clarion-like approach where highly activated MM contents form productions, but acknowledges this is a general idea with many possible implementations.
- Why unresolved: The paper doesn't provide a detailed mechanism for how shadow productions would be created, only suggesting it would be similar to Clarion's approach. The actual implementation details and algorithms for this process are not specified.
- What evidence would resolve it: A concrete algorithmic specification of how shadow productions are created, updated, and pruned over time, along with experimental results showing this mechanism's effectiveness in a cognitive model.

### Open Question 2
- Question: How would the middle memory interface handle conflicts between bottom-up predictions from generative networks and top-down cognitive control from the central production system?
- Basis in paper: [inferred] The paper discusses that shadow productions "fire by matching to information in WM and MM" and can "detect network predictions that are not in line with what is expected," but doesn't specify how conflicts would be resolved.
- Why unresolved: While the paper acknowledges that shadow productions can detect conflicts between predictions and expectations, it doesn't provide a clear mechanism for how these conflicts would be resolved or which system's output would take precedence.
- What evidence would resolve it: A formal conflict resolution mechanism with decision rules for when to prioritize bottom-up vs. top-down information, validated through cognitive modeling experiments.

### Open Question 3
- Question: What would be the computational and memory overhead of implementing this framework compared to traditional CMC models?
- Basis in paper: [explicit] The paper notes that without proper buffering, the "Pipeline Architecture" would put "a very heavy load on the serial bottleneck," implying significant computational considerations, but doesn't quantify the overhead.
- Why unresolved: The paper proposes a significantly more complex architecture than traditional CMC models, but doesn't provide any analysis of the computational costs or memory requirements of implementing this system.
- What evidence would resolve it: Empirical benchmarks comparing the computational complexity, memory usage, and processing time of this proposed architecture versus traditional CMC models on standard cognitive tasks.

## Limitations
- Framework remains entirely theoretical with no empirical validation
- Lacks specific implementation details for critical components like network tokenization/detokenization
- Unclear whether shadow productions can effectively operate without interfering with central processing bottlenecks

## Confidence
- **High Confidence:** The core architectural concept of restructuring CMC modules into shadow production systems is logically sound and builds on established ACT-R principles.
- **Medium Confidence:** The MM interface concept provides a plausible solution to the symbolic/non-symbolic integration challenge, though specific implementation details remain unclear.
- **Low Confidence:** The TD learning extension for coordinating central and shadow productions lacks sufficient detail to assess feasibility or potential learning dynamics issues.

## Next Checks
1. Prototype Implementation: Build a minimal working version with one shadow production system and one generative network to empirically test whether parallel shadow productions can operate without blocking central processing cycles.

2. MM Activation Validation: Systematically vary recency, frequency, and spreading activation parameters in a controlled setting to determine if the proposed activation mechanisms can effectively prioritize relevant network outputs.

3. Learning Stability Analysis: Implement the TD learning coordination mechanism and test whether utility distribution from central to shadow productions converges appropriately without causing learning instability or pattern inflation.