---
ver: rpa2
title: Geodesic Optimization for Predictive Shift Adaptation on EEG data
arxiv_id: '2407.03878'
source_url: https://arxiv.org/abs/2407.03878
tags:
- gopsa
- data
- domain
- riemannian
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses distribution shifts in EEG data across multiple
  recording sites, where both input features (covariance matrices) and target variables
  (e.g., age) may vary. Existing domain adaptation methods struggle when shifts occur
  simultaneously in both X and y.
---

# Geodesic Optimization for Predictive Shift Adaptation on EEG data

## Quick Facts
- arXiv ID: 2407.03878
- Source URL: https://arxiv.org/abs/2407.03878
- Reference count: 40
- The paper proposes GOPSA, a test-time adaptation method for EEG data that handles joint shifts in both features and targets across multiple recording sites, achieving significant performance improvements on age prediction tasks.

## Executive Summary
This paper addresses the challenge of distribution shifts in EEG data across multiple recording sites, where both input features (covariance matrices) and target variables (e.g., age) may vary. Existing domain adaptation methods struggle when shifts occur simultaneously in both X and y. The authors propose GOPSA (Geodesic Optimization for Predictive Shift Adaptation), a method that learns domain-specific re-centering operators via parallel transport along the Riemannian manifold of SPD matrices, combined with a shared regression model. GOPSA is a test-time adaptation method that jointly learns geodesic intercepts per domain and a global regression model. Evaluated on the HarMNqEEG dataset (14 sites, >1500 participants) for age prediction, GOPSA significantly outperformed baselines (R2, MAE, Spearman's ρ) on multiple source-target site combinations, demonstrating effectiveness in multi-source domain adaptation with predictive shifts in EEG analysis. The method has potential applications in multicenter clinical trials and biomedical EEG research.

## Method Summary
GOPSA addresses joint shifts in both input features (covariance matrices) and target variables by learning domain-specific re-centering operators via parallel transport along the Riemannian manifold of SPD matrices. The method jointly learns parallel transport parameters α for each domain along the geodesic between the domain-specific Riemannian mean and the identity matrix. This allows shifting each domain to a different point on the manifold, accommodating both feature and target shifts. The approach uses a shared regression model trained on source domains, which is then adapted to new target domains at test time by optimizing the α parameter to align the target domain's mean prediction with the known target mean yT. This enables GOPSA to generalize to new target domains without requiring access to source data or retraining.

## Key Results
- GOPSA significantly outperformed baselines (R2, MAE, Spearman's ρ) on age prediction tasks across multiple source-target site combinations in the HarMNqEEG dataset
- The method effectively handles joint shifts in both input features and target variables simultaneously, which existing domain adaptation methods struggle with
- GOPSA demonstrates test-time adaptation capability, allowing it to generalize to new target domains without requiring access to source data or retraining

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GOPSA addresses joint shifts in both the input features (covariance matrices) and target variables by learning domain-specific re-centering operators via parallel transport along the Riemannian manifold.
- **Mechanism**: The method jointly learns parallel transport parameters α for each domain along the geodesic between the domain-specific Riemannian mean and the identity matrix. This allows shifting each domain to a different point on the manifold, accommodating both feature and target shifts.
- **Core assumption**: The mean of the target domain's outcome variable yT is known or can be estimated.
- **Evidence anchors**:
  - [abstract] "GOPSA exploits the geodesic structure of the Riemannian manifold to jointly learn a domain-specific re-centering operator representing site-specific intercepts and the regression model."
  - [section 3] "GOPSA aims to learn simultaneously features from (4) and a regression model... with αS = [α1, . . . , αK]⊤."
  - [corpus] Weak evidence; related papers focus on SPD matrices but not joint X and y shifts.
- **Break condition**: If the target mean yT is unknown or poorly estimated, the optimization in Equation (10) becomes unreliable.

### Mechanism 2
- **Claim**: By learning α parameters per domain, GOPSA can model the relationship between shifts in X and shifts in y, effectively capturing log-linear brain dynamics.
- **Mechanism**: The optimization process in Equation (6) minimizes the prediction error while learning how much to shift each domain's covariance matrices. This allows the model to find the optimal alignment that preserves the underlying relationship between X and y across domains.
- **Core assumption**: The relationship between X and y follows a log-linear model that can be approximated by linear regression in the tangent space after appropriate re-centering.
- **Evidence anchors**:
  - [section 4] "we shifted the label distribution by modifying the variance of the underlying signal... Thus, the distribution of y is shifted per domain because of the log-linear relationship of (13)."
  - [section 3.2] "GOPSA adapts to this new target domain by minimizing the error between yT and its estimation computed with the fitted linear model."
  - [corpus] Weak evidence; related papers don't address the specific log-linear relationship.
- **Break condition**: If the underlying relationship between X and y is not log-linear or if the shift pattern is too complex for the learned α parameters to capture.

### Mechanism 3
- **Claim**: GOPSA's test-time adaptation capability allows it to generalize to new target domains without requiring access to source data or retraining.
- **Mechanism**: After training on source domains, GOPSA learns a shared regression model βS. At test time, it only needs to optimize the α parameter for the new target domain to align its mean prediction with the known yT, without accessing source data.
- **Core assumption**: The shared regression model learned from source domains is applicable to target domains after appropriate re-centering.
- **Evidence anchors**:
  - [abstract] "GOPSA is a test-time method that does not require to retrain a model when a new domain is presented."
  - [section 3.2] "The goal is to adapt a new target domain... by minimizing the error between yT and its estimation computed with the fitted linear model."
  - [corpus] No direct evidence; this is a novel contribution.
- **Break condition**: If the target domain is too different from source domains, the shared regression model may not generalize even after optimal re-centering.

## Foundational Learning

- **Riemannian geometry of SPD matrices**:
  - Why needed here: EEG data is represented as covariance matrices which lie on the Riemannian manifold of SPD matrices. Understanding the geometry is crucial for proper data alignment.
  - Quick check question: What is the difference between the affine invariant Riemannian metric and the log-Euclidean metric for SPD matrices?

- **Parallel transport on manifolds**:
  - Why needed here: GOPSA uses parallel transport to move covariance matrices along geodesics while preserving inner products, which is essential for domain adaptation.
  - Quick check question: How does parallel transport along a geodesic differ from simply translating data in Euclidean space?

- **Domain adaptation concepts**:
  - Why needed here: The paper addresses the problem of distribution shifts between domains, specifically when both features and targets shift simultaneously.
  - Quick check question: What is the difference between covariate shift, target shift, and conditional shift in domain adaptation?

## Architecture Onboarding

- **Component map**: Riemannian mean computation -> Parallel transport layer (with α) -> Tangent space projection -> Ridge regression -> Prediction
- **Critical path**: Riemannian mean → Parallel transport (with α) → Tangent space projection → Ridge regression → Prediction
- **Design tradeoffs**: GOPSA trades computational complexity (optimizing α parameters) for better handling of joint X and y shifts compared to simpler re-centering methods
- **Failure signatures**: Poor performance on domains where the shared regression model doesn't generalize, or when yT estimation is inaccurate
- **First 3 experiments**:
  1. Test GOPSA on a synthetic dataset with known joint X and y shifts to verify it learns appropriate α parameters
  2. Compare GOPSA with simpler re-centering baselines on a real EEG dataset with clear site-specific differences
  3. Evaluate GOPSA's test-time adaptation by holding out one domain and measuring performance on it after training on others

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GOPSA perform on datasets with continuous shifts in both X and y, rather than discrete site-based shifts?
- Basis in paper: [inferred] The authors note that the current evaluation uses discrete site-based shifts, but a continuous shift scenario could be more realistic for some applications.
- Why unresolved: The paper focuses on discrete shifts across recording sites, so the performance of GOPSA on continuous shifts is unknown.
- What evidence would resolve it: Testing GOPSA on datasets with continuous, gradual shifts in both X and y would show its robustness in such scenarios.

### Open Question 2
- Question: Can GOPSA be extended to handle multi-dimensional y variables, such as predicting multiple biomarkers simultaneously?
- Basis in paper: [inferred] The paper focuses on single-dimensional y variables, but biomedical applications often require predicting multiple outcomes.
- Why unresolved: The current formulation of GOPSA is designed for scalar regression, so its extension to multi-output scenarios is not addressed.
- What evidence would resolve it: Modifying GOPSA to handle multi-dimensional y and evaluating its performance on multi-biomarker prediction tasks would demonstrate its applicability to more complex problems.

### Open Question 3
- Question: How does GOPSA compare to deep learning-based domain adaptation methods in terms of computational efficiency and scalability?
- Basis in paper: [explicit] The authors mention that GOPSA is implemented using PyTorch and can be included in more complex Riemannian deep learning models, but no direct comparison to deep learning methods is provided.
- Why unresolved: The paper focuses on shallow regression models, so the computational trade-offs between GOPSA and deep learning methods are unclear.
- What evidence would resolve it: Benchmarking GOPSA against deep learning-based domain adaptation methods on large-scale datasets would reveal its computational efficiency and scalability.

## Limitations

- The method assumes the target mean yT is known or can be accurately estimated, which may not hold in many real-world scenarios
- The log-linear relationship between X and y is assumed but not rigorously validated across different EEG datasets
- Computational complexity of the method is not thoroughly analyzed, particularly for the test-time optimization of α parameters

## Confidence

- **High confidence**: The geometric framework for SPD matrices and the mathematical formulation of parallel transport are sound and well-established
- **Medium confidence**: The empirical results showing improved performance over baselines are convincing, but the analysis could be deeper regarding failure cases
- **Low confidence**: The scalability of the method to larger datasets and different types of EEG features is not addressed

## Next Checks

1. Test GOPSA on a held-out domain from the HarMNqEEG dataset to verify true test-time adaptation capability without access to source data
2. Evaluate performance when the target mean yT is estimated rather than known, using cross-validation on source domains
3. Compare GOPSA's computational efficiency with baselines during both training and test-time adaptation phases on increasingly large datasets