---
ver: rpa2
title: 'The Approximate Fisher Influence Function: Faster Estimation of Data Influence
  in Statistical Models'
arxiv_id: '2407.08169'
source_url: https://arxiv.org/abs/2407.08169
tags:
- influence
- loss
- data
- where
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Approximate Fisher Influence Function
  (AFIF), a method for efficiently estimating the influence of individual training
  data points on a model's behavior. AFIF leverages the Fisher Information Matrix
  (FIM) instead of the Hessian to approximate the influence, significantly reducing
  computational cost while maintaining accuracy.
---

# The Approximate Fisher Influence Function: Faster Estimation of Data Influence in Statistical Models

## Quick Facts
- arXiv ID: 2407.08169
- Source URL: https://arxiv.org/abs/2407.08169
- Authors: Omri Lev; Ashia C. Wilson
- Reference count: 40
- One-line primary result: Introduces AFIF, a method using Fisher Information Matrix for faster influence estimation that maintains accuracy while reducing computational cost.

## Executive Summary
This paper introduces the Approximate Fisher Influence Function (AFIF), a method for efficiently estimating the influence of individual training data points on a model's behavior. AFIF leverages the Fisher Information Matrix (FIM) instead of the Hessian to approximate the influence, significantly reducing computational cost while maintaining accuracy. The paper establishes theoretical guarantees for AFIF's performance across various tasks, including cross-validation, machine unlearning, data attribution, and fairness evaluation. Empirical results demonstrate that AFIF achieves similar utility to Hessian-based methods but with substantial computational improvements, making it a practical and theoretically sound approach for influence estimation in machine learning models.

## Method Summary
The paper proposes AFIF, which uses the Fisher Information Matrix (FIM) instead of the Hessian to approximate influence functions. This approach reduces computational cost by replacing second-order derivatives with first-order gradients. The method employs the LiSSA algorithm for efficient stochastic estimation of inverse matrix-vector products and includes a proximal operator to handle non-differentiable regularization. The framework provides theoretical error bounds for various influence estimation tasks and demonstrates effectiveness across different model architectures and datasets.

## Key Results
- AFIF achieves similar influence estimation accuracy to Hessian-based methods while requiring fewer FLOPs
- Theoretical guarantees establish error bounds for AFIF-based estimates across cross-validation, unlearning, data attribution, and fairness evaluation tasks
- AFIF remains informative even in non-convex settings, providing a practical alternative to Hessian-based methods
- Empirical validation on Adult, Crime, and Insurance datasets demonstrates computational efficiency and maintained model performance

## Why This Works (Mechanism)

### Mechanism 1
Replacing the Hessian with the Fisher Information Matrix (FIM) reduces computational cost while maintaining accuracy in influence estimation. The FIM depends only on first-order gradients, whereas the Hessian requires second-order derivatives. This difference allows more efficient stochastic estimation methods like LiSSA, which use fewer matrix-vector products and less memory. The core assumption is that the Hessian and FIM are close enough in the relevant parameter region so that replacing one with the other doesn't materially degrade estimation quality.

### Mechanism 2
The Approximate Fisher Influence Function (AFIF) provides provable error bounds for influence estimation tasks. By using the FIM and proximal operator, AFIF bounds the error between the approximate and true influence estimates. The theoretical framework shows the error decreases as O(1/n²) with the dataset size, matching classical results for Hessian-based methods. The core assumption is that the loss functions belong to an exponential family and satisfy regularity conditions (strong convexity, Lipschitz Hessians, etc.) ensuring the approximation error is controlled.

### Mechanism 3
AFIF remains informative even in non-convex settings, unlike some Hessian-based methods. The framework connects AFIF to the minimizer of an approximation to the Proximal Bregman Response Function (PBRF), which provides a principled justification for its use in non-convex optimization landscapes. The core assumption is that locally, the loss can be approximated by a convex function, and the PBRF-based connection holds approximately even when global convexity fails.

## Foundational Learning

- Concept: Exponential family distributions and their properties
  - Why needed here: The paper's theoretical analysis and FIM approximation rely on the loss functions being log-likelihoods of exponential family distributions, which allows for tractable calculations of gradients and Hessians.
  - Quick check question: What is the canonical form of an exponential family distribution, and how does it relate to the natural parameters and sufficient statistics?

- Concept: Influence functions and their role in machine learning
  - Why needed here: The paper builds on the concept of influence functions, which measure the sensitivity of model predictions to changes in the training data, and extends them using the FIM for computational efficiency.
  - Quick check question: How do influence functions use the Hessian of the loss to approximate the effect of removing a training point, and why is this computationally expensive?

- Concept: Fisher Information Matrix and natural gradients
  - Why needed here: The FIM is used as a computationally efficient approximation to the Hessian, and natural gradients leverage the FIM to precondition the gradient in a way that accounts for the underlying geometry of the parameter space.
  - Quick check question: How does the Fisher Information Matrix relate to the curvature of the likelihood function, and why is it a natural choice for preconditioning gradients in probabilistic models?

## Architecture Onboarding

- Component map:
  - Loss function -> Model -> Regularization -> FIM computation -> LiSSA algorithm -> Proximal operator -> Influence tasks
  - Loss function: Must be from an exponential family (e.g., cross-entropy, MSE)
  - Model: Any differentiable model (e.g., neural network) with features f(x; θ)
  - Regularization: Can be non-differentiable (e.g., L1, group sparsity) handled via proximal operator
  - FIM computation: Uses first-order gradients of the loss w.r.t. model outputs
  - LiSSA algorithm: For efficient stochastic estimation of inverse matrix-vector products
  - Influence tasks: Cross-validation, unlearning, data attribution, fairness evaluation

- Critical path:
  1. Train the model to obtain ˆθ(1)
  2. Compute the approximate FIM F(D, ˆθ(1))
  3. For each influence task, compute b(ˆθ(1), wn) (weight vector dependent)
  4. Use LiSSA to compute F(D, ˆθ(1))⁻¹ b(ˆθ(1), wn)
  5. Apply proximal operator if non-differentiable regularization is used
  6. Evaluate the influence task-specific objective T(·)

- Design tradeoffs:
  - FIM vs Hessian: FIM is faster but may be less accurate in highly non-convex settings
  - LiSSA depth and repetitions: Trade-off between accuracy and computational cost
  - Proximal operator: Necessary for non-differentiable regularization but adds overhead

- Failure signatures:
  - Large discrepancy between FIM-based and Hessian-based influence estimates
  - Instability in LiSSA iterations (divergence or slow convergence)
  - Poor performance on influence tasks despite low computational cost

- First 3 experiments:
  1. Implement cross-validation using AFIF on a simple logistic regression model with L2 regularization, comparing against exact leave-one-out cross-validation.
  2. Apply AFIF to a neural network with L1 regularization for data attribution, comparing the influential samples identified by AFIF and Hessian-based methods.
  3. Use AFIF for fairness evaluation on a dataset with a binary sensitive attribute, measuring the reduction in demographic disparity after unlearning influential samples.

## Open Questions the Paper Calls Out

### Open Question 1
Under what precise conditions does the approximate Fisher Information Matrix (FIM) become a sufficiently accurate approximation of the Hessian to ensure reliable influence estimation in non-convex models? The paper demonstrates empirically that AFIF works well in non-convex settings and provides theoretical justification for the proximity of FIM to Hessian in certain cases, but does not establish exact conditions for when this approximation is reliable.

### Open Question 2
How does the choice of the scaling parameter σ in the LiSSA algorithm affect the stability and accuracy of both Hessian-based and Fisher-based influence function computations across different model architectures and datasets? While the paper shows some sensitivity to σ in experiments, it does not provide systematic guidelines for selecting σ or understanding its theoretical impact on approximation quality.

### Open Question 3
Can the AFIF framework be extended to provide theoretical guarantees for influence estimation in deep learning models with complex architectures (e.g., transformers, graph neural networks) that involve non-differentiable components or non-standard regularizers? The current theoretical framework is built for simpler models, and extending it to modern deep learning architectures with their unique challenges requires new theoretical tools.

## Limitations

- The paper assumes exponential family distributions, which may not hold for complex models like deep neural networks with non-standard loss functions
- Empirical validation relies on relatively small datasets and simple neural network architectures, raising questions about scalability
- The paper does not thoroughly investigate cases where the FIM and Hessian diverge significantly
- Theoretical guarantees assume strong convexity and Lipschitz Hessians, which may not hold in practical, non-convex settings

## Confidence

- Theoretical framework: Medium
- Computational efficiency claims: High
- Accuracy maintenance in complex settings: Low
- Scalability to large models/datasets: Low

## Next Checks

1. Evaluate AFIF on larger, more complex models (e.g., transformers) and datasets to assess scalability and robustness to non-convexity.
2. Systematically investigate cases where FIM and Hessian diverge (e.g., highly non-convex loss landscapes) and quantify the impact on influence estimation accuracy.
3. Develop and validate hyperparameter tuning procedures for the LiSSA algorithm to ensure stable and accurate estimation across different problem settings.