---
ver: rpa2
title: Benchmarking Large Language Models on CFLUE -- A Chinese Financial Language
  Understanding Evaluation Dataset
arxiv_id: '2405.10542'
source_url: https://arxiv.org/abs/2405.10542
tags:
- financial
- llms
- text
- assessment
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CFLUE, a comprehensive Chinese financial
  language understanding evaluation benchmark designed to assess the capabilities
  of large language models (LLMs) across multiple dimensions. CFLUE consists of two
  main sections: knowledge assessment, which includes over 38,000 multiple-choice
  questions with solution explanations, and application assessment, featuring 16,000+
  test instances across five groups of NLP tasks such as text classification, machine
  translation, relation extraction, reading comprehension, and text generation.'
---

# Benchmarking Large Language Models on CFLUE -- A Chinese Financial Language Understanding Evaluation Dataset

## Quick Facts
- arXiv ID: 2405.10542
- Source URL: https://arxiv.org/abs/2405.10542
- Authors: Jie Zhu; Junhui Li; Yalong Wen; Lifan Guo
- Reference count: 35
- Only GPT-4 and GPT-4-turbo achieve accuracy exceeding 60% in answer prediction for knowledge assessment

## Executive Summary
This paper introduces CFLUE, a comprehensive Chinese financial language understanding evaluation benchmark designed to assess large language models across multiple dimensions. CFLUE consists of two main sections: knowledge assessment with over 38,000 multiple-choice questions covering financial topics, and application assessment featuring 16,000+ test instances across five groups of NLP tasks. The evaluation reveals that current LLMs have substantial room for improvement in financial domain knowledge, with only GPT-4 variants achieving reasonable performance, while fine-tuning lightweight models with LoRA can significantly boost their capabilities.

## Method Summary
The CFLUE benchmark was constructed through systematic data collection from authoritative financial sources, including qualification exam questions and financial documents. The knowledge assessment section was created by extracting questions from Chinese financial qualification exams across eight subjects, with manual verification and rewriting to ensure quality. The application assessment section includes tasks such as text classification, machine translation, relation extraction, reading comprehension, and text generation, sourced from financial documents and websites. Models were evaluated using zero-shot prompting, with performance measured using standard metrics like accuracy for classification tasks and BLEU/ROUGE for generation tasks.

## Key Results
- Only GPT-4 and GPT-4-turbo achieve accuracy exceeding 60% in answer prediction for knowledge assessment
- Lightweight LLMs fine-tuned with LoRA on CFLUE show significant performance improvements, with Qwen-7B accuracy increasing from 45.70% to 49.84%
- While GPT-4 variants remain top performers in application assessment on average, their advantage over lightweight LLMs is noticeably diminished

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multiple-choice format with solution explanations enables dual assessment of answer prediction and reasoning capabilities in LLMs.
- Mechanism: By structuring questions to require both a selected answer and a generated explanation, the benchmark can evaluate not just factual recall but also the model's ability to articulate reasoning processes.
- Core assumption: LLMs can effectively generate coherent explanations that align with their predicted answers.
- Evidence anchors:
  - [abstract] "These questions serve dual purposes: answer prediction and question reasoning."
  - [section] "In the knowledge assessment, it consists of 38K+ multiple-choice questions with associated solution explanations."
- Break condition: If the generated explanations frequently do not align with the predicted answers, indicating the model is not truly reasoning but rather generating plausible-sounding text.

### Mechanism 2
- Claim: Task diversity across knowledge and application domains reveals differential model strengths and weaknesses.
- Mechanism: By including tasks from text classification to machine translation to text generation, the benchmark exposes models to varied linguistic and reasoning challenges, preventing overfitting to a single task type.
- Core assumption: Model performance varies significantly across different types of NLP tasks.
- Evidence anchors:
  - [abstract] "CFLUE provides datasets tailored for both knowledge assessment and application assessment."
  - [section] "In application assessment, CFLUE features 16K+ test instances across distinct groups of NLP tasks such as text classification, machine translation, relation extraction, reading comprehension, and text generation."
- Break condition: If models perform uniformly across all task types, suggesting the benchmark is not effectively differentiating capabilities.

### Mechanism 3
- Claim: Fine-tuning with LoRA on the CFLUE dataset significantly improves performance of general domain LLMs.
- Mechanism: Applying low-rank adaptation to lightweight LLMs on the benchmark data allows them to acquire domain-specific knowledge without full fine-tuning, bridging the gap with larger models.
- Core assumption: Domain-specific fine-tuning transfers effectively to benchmark performance.
- Evidence anchors:
  - [section] "For instance, ChatGLM3-6B (Zeng et al., 2022), Qwen-7B (Bai et al., 2023), and Baichuan2-7B (Baichuan, 2023) achieve comparable or superior performance to ChatGPT (OpenAI, 2022) in both answer prediction and reasoning tasks, despite having only 4% of ChatGPT's parameters."
  - [section] "Fine-tuning general domain LLMs with LoRA on the CFLUE dataset significantly boosts their performance. For example, the accuracy of Qwen-7B increases to 49.84% from 45.70%."
- Break condition: If fine-tuned models do not show significant improvement over their base versions on the benchmark.

## Foundational Learning

- Concept: Zero-shot evaluation methodology
  - Why needed here: The benchmark aims to assess inherent model capabilities without task-specific training, providing a fair comparison across models.
  - Quick check question: What distinguishes zero-shot from few-shot evaluation in the context of this benchmark?

- Concept: Financial domain knowledge requirements
  - Why needed here: Understanding the specific knowledge domains covered (e.g., accounting, banking, securities) is crucial for interpreting model performance and identifying knowledge gaps.
  - Quick check question: Which financial qualification exams are represented in the knowledge assessment section?

- Concept: Chinese language processing nuances
  - Why needed here: Since this is a Chinese financial language benchmark, understanding language-specific challenges (e.g., tokenization, character-level vs word-level processing) is important for proper evaluation.
  - Quick check question: How might Chinese language characteristics affect the performance of English-oriented LLMs on this benchmark?

## Architecture Onboarding

- Component map: Data collection pipeline (web scraping, OCR, manual verification) -> Preprocessing module (deduplication, filtering, rewriting) -> Evaluation framework (zero-shot prompts, metrics calculation) -> Result aggregation and analysis system

- Critical path: Data collection → Preprocessing → Prompt engineering → Model evaluation → Result analysis

- Design tradeoffs: The benchmark balances comprehensiveness (covering many task types) against practical evaluation costs (API calls, compute resources). The zero-shot setting prioritizes fairness over potential performance gains from few-shot learning.

- Failure signatures:
  - Data contamination issues (if models perform suspiciously well on certain question types)
  - Metric misalignment (if scores don't reflect true capabilities)
  - Prompt sensitivity (if small prompt variations cause large performance swings)

- First 3 experiments:
  1. Evaluate a small subset of questions with multiple models to verify prompt effectiveness and establish baseline performance distribution
  2. Test the impact of input length on model performance to identify potential truncation issues
  3. Compare zero-shot vs few-shot performance on a representative sample to quantify the trade-off between evaluation fairness and performance potential

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the key differences in performance between LLMs fine-tuned on CFLUE and those that are not, specifically for the knowledge assessment tasks?
- Basis in paper: [explicit] The paper mentions that fine-tuning general domain LLMs with LoRA on the CFLUE dataset significantly boosts their performance, with examples like Qwen-7B increasing accuracy from 45.70% to 49.84%.
- Why unresolved: The paper provides a general overview of performance improvements but lacks detailed comparative analysis between fine-tuned and non-fine-tuned models across all tasks and subjects.
- What evidence would resolve it: Detailed performance metrics comparing fine-tuned and non-fine-tuned models across all subjects and question types in the knowledge assessment section.

### Open Question 2
- Question: How do the reasoning capabilities of LLMs in the knowledge assessment compare to their performance in the application assessment tasks?
- Basis in paper: [explicit] The paper discusses the dual purposes of questions in the knowledge assessment (answer prediction and question reasoning) and mentions that the performance trend differs between these tasks, with Qwen-72B excelling in BLEU and ROUGE metrics.
- Why unresolved: The paper provides separate performance metrics for knowledge assessment and application assessment but does not directly compare the reasoning capabilities across these two sections.
- What evidence would resolve it: A comparative analysis of reasoning performance between the knowledge assessment and application assessment tasks, including metrics like BLEU and ROUGE scores.

### Open Question 3
- Question: What are the limitations of using BLEU, COMET, ROUGE, and BERTScore metrics for evaluating LLM outputs in the CFLUE benchmark?
- Basis in paper: [explicit] The paper acknowledges that these metrics are used to evaluate performance but notes that they may not provide a comprehensive assessment of LLM outputs.
- Why unresolved: The paper does not delve into the specific limitations of these metrics or explore alternative evaluation methods.
- What evidence would resolve it: An analysis of the strengths and weaknesses of BLEU, COMET, ROUGE, and BERTScore metrics, along with suggestions for additional or alternative evaluation methods.

## Limitations
- Lack of detailed analysis on potential data contamination between benchmark questions and model training data
- Evaluation focuses primarily on zero-shot performance, which may not reflect real-world usage scenarios
- Chinese language focus limits generalizability to other languages and financial contexts

## Confidence
- **High confidence**: The benchmark construction methodology (data collection from authoritative sources, question type diversity, task coverage) is well-documented and methodologically sound
- **Medium confidence**: Performance comparisons between models are valid, though the absence of contamination analysis creates uncertainty about whether scores reflect true capabilities
- **Medium confidence**: The claim about LoRA fine-tuning effectiveness is supported by specific accuracy improvements, but the analysis doesn't explore optimal fine-tuning strategies or potential overfitting

## Next Checks
1. **Data contamination analysis**: Conduct a systematic comparison between CFLUE questions and model training corpora (where available) to quantify potential contamination rates and assess impact on performance scores.

2. **Prompt sensitivity study**: Test multiple prompt variations across different models on a representative subset of questions to measure performance variance and establish robustness thresholds for fair comparison.

3. **Fine-tuning optimization experiment**: Compare LoRA fine-tuning against full fine-tuning and few-shot prompting on a subset of tasks to determine the optimal approach for domain adaptation while controlling for training data overlap.