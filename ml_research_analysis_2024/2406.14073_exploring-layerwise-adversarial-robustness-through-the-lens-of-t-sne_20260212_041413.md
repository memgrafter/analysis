---
ver: rpa2
title: Exploring Layerwise Adversarial Robustness Through the Lens of t-SNE
arxiv_id: '2406.14073'
source_url: https://arxiv.org/abs/2406.14073
tags:
- layer
- robustness
- clean
- adversarial
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to assess adversarial robustness of
  image-classifying neural networks by using t-SNE to visualize and compare clean
  vs. perturbed representations across layers.
---

# Exploring Layerwise Adversarial Robustness Through the Lens of t-SNE

## Quick Facts
- arXiv ID: 2406.14073
- Source URL: https://arxiv.org/abs/2406.14073
- Authors: Inês Valentim; Nuno Antunes; Nuno Lourenço
- Reference count: 20
- Key outcome: Proposed method uses t-SNE to visualize and measure overlap between clean and adversarial representations across layers, showing robustness degradation begins in early feature extraction layers

## Executive Summary
This paper introduces a method to assess adversarial robustness of neural networks by analyzing layerwise representations of clean and perturbed images using t-SNE visualization. A metric measures the overlap between clean and adversarial embeddings in the t-SNE space, with experiments on CIFAR-10 models showing robustness degradation starts in early feature extraction layers. The approach helps identify vulnerable layers and can guide defenses by revealing where representational shifts precede classification-level adversarial failure.

## Method Summary
The method extracts intermediate layer representations from clean and adversarial images, applies t-SNE dimensionality reduction to create 2D visualizations, and computes a robustness metric based on Euclidean distances between corresponding clean-perturbed pairs. Adversarial examples are generated using APGD attacks (CE, DLR, targeted DLR) under L2 and L∞ threat models. The robustness metric quantifies overlap by comparing distances between clean-perturbed pairs to the minimum distance between different-class clean images in the t-SNE space.

## Key Results
- Robustness degradation begins in early feature extraction layers rather than classification layers
- For Wide ResNet WRN-28-10, the metric drops to near-zero after the last group of residual blocks
- For NeuroEvolution DENSER, robustness starts declining after mid-level layers with more pronounced drops later

## Why This Works (Mechanism)

### Mechanism 1
- Claim: t-SNE visualization reveals representational shifts that precede classification-level adversarial failure
- Mechanism: t-SNE maps high-dimensional hidden activations to 2D such that clean and adversarial samples move apart when feature extraction is compromised
- Core assumption: Clean-perturbed pairs should remain close in t-SNE space if the layer's representation is robust
- Evidence anchors: [abstract] differences emerge early in feature extraction layers; [section] metric shows deterioration begins in feature extraction layers
- Break condition: If t-SNE stochasticity or perplexity causes random overlaps, the metric becomes unreliable

### Mechanism 2
- Claim: A distance-based heuristic can quantify t-SNE overlap without requiring supervised labels
- Mechanism: For each clean image, find minimum distance to clean images from different classes; mark as overlapping if clean-perturbed distance is less
- Core assumption: Overlap in t-SNE space correlates with the model's ability to preserve semantic identity under perturbation
- Evidence anchors: [section] If distance between clean and perturbed is less than minimum, consider they overlap; [section] Metric values near zero mean little overlap
- Break condition: If t-SNE embedding quality is poor, minimum-distance heuristics may misclassify non-overlapping points as overlapping

### Mechanism 3
- Claim: Layerwise analysis can identify specific architectural blocks responsible for robustness loss
- Mechanism: Metric computed at multiple layers reveals where overlap drops occur
- Core assumption: Adversarial degradation propagates from feature extraction to classification layers
- Evidence anchors: [section] differences emerge early in feature extraction layers, affecting subsequent classification; [section] For WRN-28-10, drastic drops occur in last group of residual blocks
- Break condition: If adversarial perturbations are too small or model is highly robust, overlap may remain high across all layers

## Foundational Learning

- Concept: t-SNE dimensionality reduction
  - Why needed here: t-SNE enables visual inspection of high-dimensional hidden activations in 2D space, making overlap patterns observable
  - Quick check question: What does perplexity control in t-SNE, and why is it important to tune for visualization quality?

- Concept: Adversarial attack generation (APGD variants)
  - Why needed here: Reliable adversarial examples are required to test representation robustness
  - Quick check question: How does the difference of logits ratio (DLR) loss differ from cross-entropy in targeted attacks?

- Concept: Residual block structure in CNNs
  - Why needed here: Understanding layer naming and grouping is necessary to map metric drops to architectural locations
  - Quick check question: What is the role of the skip connection in a residual block, and how does it affect gradient flow?

## Architecture Onboarding

- Component map: Data pipeline → Model inference (clean) → Attack generation (clean → adversarial) → Layer-wise representation extraction → t-SNE embedding → Overlap metric computation → Visualization
- Critical path: Clean image → adversarial perturbation → layer activation → t-SNE → metric
- Design tradeoffs: t-SNE is non-parametric and stochastic, so results may vary across runs; using PCA initialization reduces variance but adds computation; metric ignores misclassified clean images to avoid bias
- Failure signatures: If metric stays near 1 for all layers, either attack failed or model is highly robust; if it drops immediately in early layers, feature extractor is vulnerable; if overlap observed late but not early, model may rely on last-layer decision boundaries
- First 3 experiments:
  1. Run APGD-CE on simple CNN (CIFAR-10 ConvNet) and compute metric at first conv layer and final FC layer to confirm basic functionality
  2. Compare metric curves for L2 vs. L∞ attacks on same model to verify sensitivity to perturbation type
  3. Visualize t-SNE maps for layer where metric drops significantly to confirm spatial separation reflects semantic drift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does layerwise adversarial robustness of models designed with adversarial defenses compare to undefended models across different datasets and architectures?
- Basis in paper: [explicit] The paper uses undefended models and suggests evaluating approach on models designed to be adversarially robust in future work
- Why unresolved: Study only examined undefended models on CIFAR-10, leaving open how robustness patterns differ with adversarial training or architectural defenses
- What evidence would resolve it: Experiments applying same t-SNE and robustness metric to adversarially trained models on CIFAR-10 and other datasets

### Open Question 2
- Question: What is the relationship between the t-SNE-based robustness metric and task-specific representation similarity metrics like centered kernel alignment (CKA) in identifying vulnerable layers?
- Basis in paper: [inferred] Paper mentions Cianfarani et al.'s work on representation similarity but does not compare their metric to such alternatives
- Why unresolved: Proposed metric is novel but its correlation with established similarity measures and added value for layer selection remains untested
- What evidence would resolve it: Comparative analysis of t-SNE overlap ratios versus CKA scores across layers for same models and attacks

### Open Question 3
- Question: Does the proposed robustness metric generalize across different attack types and perturbation norms beyond L2 and L∞?
- Basis in paper: [explicit] Experiments were limited to white-box L2 and L∞ AutoAttack variants
- Why unresolved: Metric's sensitivity to attack strategy or norm choice is unknown, affecting utility in real-world scenarios
- What evidence would resolve it: Application of metric to black-box attacks, L1/L0 norms, and universal adversarial perturbations

### Open Question 4
- Question: How does choice of t-SNE hyperparameters affect the robustness metric and its interpretability?
- Basis in paper: [inferred] Default t-SNE settings were used; sensitivity to these choices is not explored
- Why unresolved: t-SNE is stochastic and parameter-sensitive; without ablation studies, unclear if metric drops reflect true representation changes or visualization artifacts
- What evidence would resolve it: Systematic variation of perplexity, iteration counts, and initialization methods with comparison of resulting robustness metrics

## Limitations
- t-SNE stochasticity may cause metric reproducibility issues across runs
- The overlap detection method relies on Euclidean distance comparisons in 2D space
- DENSER model architecture details are not fully specified, potentially hindering exact reproduction

## Confidence

- **High confidence**: Core observation that adversarial degradation begins in feature extraction layers is well-supported by both metric trends and visual evidence
- **Medium confidence**: Proposed overlap metric provides reasonable heuristic for quantifying robustness, though sensitivity to t-SNE parameters requires further validation
- **Medium confidence**: Layer-specific identification of vulnerability points is methodologically sound but depends on consistent t-SNE embeddings

## Next Checks
1. Run ablation study: Compare metric values when using different t-SNE perplexities (30, 50, 100) on same model/layer to assess sensitivity to dimensionality reduction parameters
2. Validate metric monotonicity: Generate adversarial examples with increasing perturbation strength and verify that robustness metric decreases monotonically across layers
3. Test transferability: Apply same methodology to a third model architecture (e.g., VGG-16) to confirm whether early-layer degradation is a general phenomenon or specific to residual networks