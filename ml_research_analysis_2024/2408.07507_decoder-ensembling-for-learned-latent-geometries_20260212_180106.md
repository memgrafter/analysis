---
ver: rpa2
title: Decoder ensembling for learned latent geometries
arxiv_id: '2408.07507'
source_url: https://arxiv.org/abs/2408.07507
tags:
- latent
- data
- space
- uncertainty
- hauberg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of topological mismatch in latent
  spaces of deep generative models, where Euclidean topology does not match the true
  data manifold. The authors propose using ensembles of decoders in VAEs to capture
  model uncertainty and compute geodesics on the expected manifold.
---

# Decoder ensembling for learned latent geometries

## Quick Facts
- arXiv ID: 2408.07507
- Source URL: https://arxiv.org/abs/2408.07507
- Authors: Stas Syrota; Pablo Moreno-Muñoz; Søren Hauberg
- Reference count: 11
- One-line primary result: Ensemble-based uncertainty estimation in VAEs provides more stable and reliable geodesics in latent spaces compared to RBF-based methods, especially in high dimensions.

## Executive Summary
This paper addresses the problem of topological mismatch in latent spaces of deep generative models, where Euclidean topology does not match the true data manifold. The authors propose using ensembles of decoders in VAEs to capture model uncertainty and compute geodesics on the expected manifold. The method improves upon existing heuristics for uncertainty estimation, particularly in high-dimensional latent spaces. The approach is evaluated on MNIST and FMNIST datasets, showing lower coefficient of variation in geodesic distances compared to RBF-based uncertainty methods.

## Method Summary
The method builds on VAEs by training an ensemble of independently initialized decoders rather than a single decoder. During training, a random decoder is selected for each mini-batch and optimized using the ELBO loss. At inference, the ensemble provides uncertainty estimates through the variance across decoders. The expected metric is computed using the ensemble, dropping cross-covariance terms to amplify uncertainty effects. Geodesics are then computed on this expected manifold using discretized energy minimization. The approach requires minimal code changes to existing VAE implementations.

## Key Results
- Ensemble-based uncertainty estimation provides more stable geodesic computations with lower coefficient of variation across random seeds
- The method demonstrates superior performance particularly in high-dimensional latent spaces (up to 50 dimensions) where RBF-based methods break down
- Requires minimal code changes to existing VAE implementations while providing significant improvements in topological fidelity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensembles of decoders reduce model uncertainty in latent spaces where Euclidean topology does not match the true manifold.
- Mechanism: By training multiple decoders independently on the same data, the ensemble captures diverse posterior samples of the decoder weights. The variance across ensemble members approximates model uncertainty, which is then used to inflate the Riemannian metric and guide geodesics toward regions of high data support.
- Core assumption: Neural network ensembles provide a reasonable approximation to the posterior over decoder weights, and their diversity is sufficient to capture topological mismatch.
- Evidence anchors:
  - [abstract] "We propose using ensembles of decoders to capture model uncertainty and show how to easily compute geodesics on the associated expected manifold."
  - [section] "We have proposed to use neural network ensembles to capture model uncertainty... Empirically, we have found that geodesics that minimize this discretized energy do not follow the data as closely as one could hope for... we drop them..."
  - [corpus] No direct evidence from neighbors; ensemble method is not covered in corpus.
- Break condition: If ensemble members are not sufficiently diverse (e.g., if all trained on identical mini-batches without augmentation), uncertainty estimates collapse and topological guidance fails.

### Mechanism 2
- Claim: Dropping cross-covariance terms in the expected metric improves geodesic alignment with data manifold.
- Mechanism: The ensemble-based expected metric includes terms like `tr[Σ11 + Σ22]` but drops `tr[Σ12]`. This amplification of uncertainty without correlation reduction ensures geodesics avoid regions of high uncertainty (low data support).
- Core assumption: Cross-covariance terms in the ensemble metric are not beneficial for guiding geodesics and can introduce spurious correlations that mislead the path.
- Evidence anchors:
  - [section] "To counter this, we will modify the energy to amplify the impact of model uncertainty by disregarding correlations... we drop them, i.e. `E[∥∆∥2] ≈ ∥E[∆]∥2 + tr[Σ11 + Σ22]`"
  - [corpus] No corpus evidence.
- Break condition: If the latent space dimension is very low or data manifold is convex, cross-covariance terms might carry useful information; dropping them could hurt performance.

### Mechanism 3
- Claim: Ensemble-based uncertainty is more robust across different random seeds than RBF-based heuristics.
- Mechanism: By training ensembles with different initializations, the variance across runs captures stable uncertainty patterns. This leads to lower coefficient of variation in geodesic distances compared to RBF heuristics, which are sensitive to initialization.
- Core assumption: RBF-based uncertainty models rely on fixed hyperparameters (e.g., number of Gaussians) that are sensitive to initialization and data distribution, while ensembles adapt naturally.
- Evidence anchors:
  - [section] "Table 1 shows results for the one-sided paired Student's t-test with the null hypothesis of ensemble geodesics having a lower coefficient of variation than by using the RBF-based model. The results show that the ensemble of decoders is consistently more reliable..."
  - [corpus] No corpus evidence.
- Break condition: If the data manifold is very simple (e.g., low-dimensional and convex), both methods may perform similarly, reducing the advantage of ensembles.

## Foundational Learning

- Concept: Riemannian geometry and pull-back metrics
  - Why needed here: The paper redefines Euclidean latent distances using the pull-back metric `G_z = J_z^T J_z` to measure distances along the decoded manifold, which is essential for capturing true manifold topology.
  - Quick check question: Given a decoder `f(z)` and a small displacement `Δz`, write the expression for the squared distance in latent space using the pull-back metric.

- Concept: Variational autoencoders (VAEs) and ELBO training
  - Why needed here: The ensemble method is built on top of a VAE framework, where decoders are trained to maximize the ELBO. Understanding the VAE loss is critical for modifying it to train ensembles.
  - Quick check question: What are the two terms in the ELBO loss for a Gaussian VAE, and what does each represent?

- Concept: Uncertainty quantification in neural networks
  - Why needed here: The core innovation relies on using ensemble variance as a proxy for model uncertainty. Without understanding how to interpret ensemble diversity as uncertainty, the method cannot be applied.
  - Quick check question: How does the variance across ensemble members approximate the posterior variance of the decoder weights?

## Architecture Onboarding

- Component map:
  - Data → VAE encoder → latent space `z`
  - Multiple independently trained decoders `{f_θ_s}` (ensemble)
  - Uncertainty estimation via ensemble variance
  - Expected metric `G = E[J_f^T J_f]` (cross-covariances dropped)
  - Geodesic solver (energy minimization with Monte Carlo sampling)

- Critical path:
  1. Train base VAE (encoder + single decoder) to convergence.
  2. Instantiate S decoders with random initialization.
  3. For each mini-batch, randomly select one decoder and update only its weights via ELBO.
  4. After training, compute ensemble uncertainty for each latent point.
  5. Build expected metric (dropping cross-covariances).
  6. Compute geodesics using discretized energy minimization.

- Design tradeoffs:
  - Ensemble size S vs. computational cost: Larger S gives better uncertainty estimates but increases memory and training time.
  - Dropping cross-covariances vs. metric fidelity: Simplifies computation but may lose some geometric information.
  - Fixed RBF hyperparameters vs. ensemble adaptability: RBFs require tuning; ensembles adapt automatically but need more data.

- Failure signatures:
  - Geodesics collapse to straight lines in latent space → uncertainty not being captured.
  - High variance in geodesic distances across seeds → ensemble not stabilizing uncertainty.
  - Slow convergence of geodesic solver → metric too noisy or ill-conditioned.

- First 3 experiments:
  1. Train a single VAE on MNIST with 2D latent space; visualize uncertainty and geodesics using RBF baseline.
  2. Train an ensemble of 5 decoders on same data; compare uncertainty maps and geodesic stability across 10 random seeds.
  3. Increase latent dimension to 10; measure coefficient of variation in geodesic distances for both methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ensemble-based approach scale to very high-dimensional latent spaces (e.g., >50 dimensions) compared to RBF-based methods?
- Basis in paper: [inferred] The paper mentions that RBF-based methods break down when the latent dimension exceeds a handful, and evaluates up to 50 dimensions, but does not provide extensive analysis for higher dimensions.
- Why unresolved: The paper only tests up to 50-dimensional latent spaces, leaving uncertainty about performance in even higher dimensions where both methods might struggle.
- What evidence would resolve it: Experiments comparing ensemble and RBF methods on latent spaces with dimensions significantly higher than 50, along with computational cost analysis.

### Open Question 2
- Question: Can the ensemble method be extended to non-Euclidean latent space structures (e.g., spherical or hyperbolic spaces) while maintaining its advantages?
- Basis in paper: [inferred] The paper mentions that other latent structures have been investigated but focuses on Euclidean latent spaces. It does not explore how the ensemble approach would work with alternative geometries.
- Why unresolved: The paper does not investigate the compatibility of the ensemble method with non-Euclidean latent space structures, which could be valuable for certain types of data.
- What evidence would resolve it: Implementation and evaluation of the ensemble method with spherical or hyperbolic latent spaces, comparing results to both the original method and RBF-based approaches in these geometries.

### Open Question 3
- Question: How does the ensemble method perform on more complex, real-world datasets beyond MNIST and FMNIST?
- Basis in paper: [explicit] The paper evaluates the method on MNIST and FMNIST datasets, but acknowledges the need for testing on more complex data.
- Why unresolved: The experiments are limited to relatively simple image datasets, leaving uncertainty about the method's effectiveness on more challenging, high-variety data.
- What evidence would resolve it: Experiments on diverse, complex datasets such as CIFAR-10, ImageNet, or domain-specific datasets (e.g., medical imaging, natural language) comparing ensemble and RBF methods in terms of geodesic reliability and computational efficiency.

## Limitations
- Limited to simple datasets (MNIST and FMNIST) without testing on more complex, real-world data
- Only evaluates up to 50-dimensional latent spaces, leaving uncertainty about scalability to higher dimensions
- Does not compare against other uncertainty quantification methods like MC dropout or deep ensembles

## Confidence
- Claim: Ensemble-based uncertainty estimation is superior to RBF heuristics for geodesic computations
  - Confidence: Medium
  - Reason: Limited experimental scope (only two datasets, moderate dimensions)
- Claim: Dropping cross-covariance terms improves geodesic alignment
  - Confidence: Low
  - Reason: Lacks empirical validation beyond reported results
- Claim: Method requires minimal code changes to existing VAEs
  - Confidence: High
  - Reason: Implementation details support this claim and are straightforward

## Next Checks
1. Test the ensemble method on more complex datasets (e.g., CIFAR-10) and higher-dimensional latent spaces (20-50 dimensions) to evaluate scalability and robustness.

2. Compare ensemble-based uncertainty with MC dropout and deep ensembles using the same VAE architecture to isolate the contribution of the ensemble mechanism versus the underlying model architecture.

3. Perform ablation studies on the cross-covariance term dropping - test whether including these terms improves or degrades geodesic alignment on simple manifolds where the true topology is known.