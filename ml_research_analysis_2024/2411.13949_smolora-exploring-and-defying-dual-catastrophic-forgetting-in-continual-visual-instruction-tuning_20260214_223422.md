---
ver: rpa2
title: 'SMoLoRA: Exploring and Defying Dual Catastrophic Forgetting in Continual Visual
  Instruction Tuning'
arxiv_id: '2411.13949'
source_url: https://arxiv.org/abs/2411.13949
tags:
- instruction
- tasks
- visual
- question
- cvit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies dual catastrophic forgetting in continual
  visual instruction tuning (CVIT), where models forget both visual understanding
  and instruction-following capabilities. To address this, the authors propose SMoLoRA,
  a separable mixture-of-LoRA framework that uses two distinct modules - visual understanding
  and instruction following - with dynamic routing to prevent interference between
  tasks.
---

# SMoLoRA: Exploring and Defying Dual Catastrophic Forgetting in Continual Visual Instruction Tuning

## Quick Facts
- arXiv ID: 2411.13949
- Source URL: https://arxiv.org/abs/2411.13949
- Reference count: 40
- Models trained with SMoLoRA achieve 83.44% average performance and 84.85% mean average performance, outperforming baselines while minimizing backward transfer to -3.23%

## Executive Summary
This paper addresses the problem of dual catastrophic forgetting in continual visual instruction tuning (CVIT), where multimodal models forget both visual understanding and instruction-following capabilities when learning new tasks sequentially. The authors propose SMoLoRA, a separable mixture-of-LoRA framework that uses two distinct modules with dynamic routing to prevent interference between tasks. Experiments on a new CVIT benchmark demonstrate that SMoLoRA achieves superior performance with minimal backward transfer while maintaining strong instruction following capabilities.

## Method Summary
SMoLoRA introduces a separable mixture-of-LoRA framework that addresses dual catastrophic forgetting through two distinct modules: one for visual understanding and one for instruction following. The method employs dynamic routing to select appropriate LoRA blocks for each task, with an adaptive fusion mechanism to combine outputs from both modules. The approach uses a base LLaVA-v1.5-7B model and applies LoRA adaptation with rank 16 and 4 blocks per module. Training proceeds sequentially on upstream tasks converted to unified text-to-text instruction format, followed by evaluation on downstream tasks using various metrics including accuracy, average performance, mean average performance, backward transfer, and mean instruction following.

## Key Results
- SMoLoRA achieves 83.44% average performance and 84.85% mean average performance on downstream tasks
- The method demonstrates minimal backward transfer (-3.23%) while maintaining strong instruction following (97.79%)
- Outperforms baseline methods including SeqLoRA, MoLoRA, EWC, Replay, and DoRA across zero-shot and few-shot generalization tasks

## Why This Works (Mechanism)
The paper identifies that traditional continual learning approaches fail in CVIT because they don't address the dual nature of forgetting - models lose both visual understanding and instruction-following capabilities simultaneously. SMoLoRA works by separating these two capabilities into distinct modules that can be dynamically routed based on task requirements. The separable routing prevents interference between visual and instruction components, while the adaptive fusion mechanism ensures smooth integration of both capabilities when needed. This architectural separation allows the model to maintain performance on previously learned tasks while adapting to new ones.

## Foundational Learning

**Catastrophic Forgetting**: The phenomenon where neural networks rapidly lose performance on previously learned tasks when trained on new ones. Critical for understanding why traditional fine-tuning fails in continual learning scenarios.

Quick check: Verify that your model shows performance degradation on earlier tasks when sequentially fine-tuning on new datasets without any continual learning strategy.

**Low-Rank Adaptation (LoRA)**: A parameter-efficient fine-tuning method that approximates weight updates using low-rank matrices. Essential for enabling efficient adaptation while maintaining the base model's knowledge.

Quick check: Confirm that your LoRA implementation correctly applies rank-k decomposition and that the combined weights (base + LoRA) produce expected outputs.

**Dynamic Routing**: A mechanism that selects which neural network components to activate based on input characteristics. Needed to ensure the right module (visual or instruction) is used for each task.

Quick check: Monitor router decisions during training to ensure they're consistent with task requirements and not randomly switching between modules.

## Architecture Onboarding

**Component Map**: LLaVA-v1.5-7B (base) -> Visual Understanding LoRA Module -> Instruction Following LoRA Module -> Adaptive Fusion -> Output

**Critical Path**: Input text/image → Router matrix → Select visual/instruction modules → LoRA adaptation → Adaptive fusion → Final output

**Design Tradeoffs**: The paper trades computational efficiency (using only 8 LoRA blocks total) for performance, choosing separable routing over more complex multi-task learning approaches. The adaptive fusion adds minimal overhead while providing flexibility.

**Failure Signatures**: 
- Poor routing decisions leading to module interference
- Overfitting on few-shot tasks due to limited data
- Inconsistent instruction format causing routing errors

**3 First Experiments**:
1. Verify routing decisions are task-appropriate by logging router outputs across all training tasks
2. Test performance with different numbers of LoRA blocks (2, 4, 8) to find optimal balance
3. Evaluate zero-shot performance on downstream tasks before any fine-tuning to establish baseline

## Open Questions the Paper Calls Out

**Open Question 1**: What is the optimal number of LoRA blocks for SMoLoRA in terms of balancing performance gains and computational efficiency?
- The paper shows performance improves with more blocks but plateaus at higher values
- An ablation study varying LoRA blocks while measuring performance and computational costs would clarify the optimal balance

**Open Question 2**: How does SMoLoRA perform on other MLLMs beyond LLaVA-v1.5-7B and MiniGPT-4?
- The paper only evaluates on two MLLMs
- Testing on additional architectures like GPT-4V or Qwen-VL would demonstrate generalizability

**Open Question 3**: How does SMoLoRA handle tasks with highly diverse or conflicting visual and instruction requirements?
- The paper doesn't extensively test edge cases with conflicting task requirements
- Testing on tasks requiring simultaneous image classification and complex reasoning would clarify robustness

## Limitations
- The paper doesn't specify exact instruction templates used for converting diverse datasets to unified text-to-text format
- Implementation details of the adaptive fusion mechanism are not fully specified
- Hyperparameter tuning strategy for router matrices and fusion weights is unclear

## Confidence

High confidence in: The identification of dual catastrophic forgetting as a distinct problem in CVIT, and the general architecture of using separate LoRA modules for visual understanding and instruction following.

Medium confidence in: The effectiveness of the separable routing approach in preventing interference between tasks, based on the reported metrics (AP 83.44%, MAP 84.85%).

Low confidence in: The specific implementation details of the adaptive fusion mechanism and the exact routing decision process, which are critical for the method's success but not fully specified.

## Next Checks

1. **Routing Analysis**: Implement logging to track router matrix decisions across tasks to verify that visual understanding and instruction following modules are being properly separated and not interfering with each other during sequential training.

2. **Ablation Study**: Conduct controlled experiments removing the adaptive fusion mechanism to quantify its contribution to overall performance, and test alternative routing strategies (top-2 vs top-1) to understand sensitivity to this design choice.

3. **Generalization Test**: Evaluate SMoLoRA on additional unseen tasks beyond the reported downstream datasets to verify the claimed superior zero-shot and few-shot generalization capabilities hold across a broader task distribution.