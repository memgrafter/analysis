---
ver: rpa2
title: Extreme Value Monte Carlo Tree Search for Classical Planning
arxiv_id: '2405.18248'
source_url: https://arxiv.org/abs/2405.18248
tags:
- self
- node
- return
- class
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Extreme Value Monte Carlo Tree Search for Classical Planning addresses
  the mismatch between bandit algorithms and classical planning heuristics by leveraging
  Peaks-Over-Threshold Extreme Value Theory. The paper identifies that standard UCB1
  bandits incorrectly assume bounded reward ranges incompatible with unbounded heuristic
  values, while Gaussian bandits over-specify the support.
---

# Extreme Value Monte Carlo Tree Search for Classical Planning

## Quick Facts
- arXiv ID: 2405.18248
- Source URL: https://arxiv.org/abs/2405.18248
- Reference count: 40
- Primary result: GUCT-Uniform solves 67.8, 23.4, and 33.2 more instances than GBFS, GUCT-Normal2, and Softmin-Type(h) respectively under 10,000 node evaluations

## Executive Summary
This paper addresses the theoretical mismatch between standard Multi-Armed Bandit algorithms and classical planning heuristics by introducing Extreme Value Monte Carlo Tree Search (EVT-MCTS). The authors identify that UCB1 bandits incorrectly assume bounded reward ranges incompatible with unbounded heuristic values in classical planning. By leveraging Peaks-Over-Threshold Extreme Value Theory, they propose UCB1-Uniform which models rewards with uniform distributions, providing both theoretical justification for Full Bellman backup and significantly improved empirical performance across multiple benchmark domains.

## Method Summary
The paper introduces UCB1-Uniform, a bandit algorithm that uses uniform distribution maximum likelihood estimates derived from Peaks-Over-Threshold Extreme Value Theory. The algorithm models heuristic values as uniformly distributed with unknown support, allowing it to handle the unbounded nature of classical planning heuristics. EVT-MCTS is implemented using Pyperplan and Fast Downward, with experiments conducted on 772 IPC benchmark instances across 24 domains using various heuristics (hFF, hadd, hmax, hGC). The method is compared against GBFS, GUCT-Normal2, Softmin-Type(h), and Max-k bandits under different node evaluation budgets and time constraints.

## Key Results
- GUCT-Uniform significantly outperforms GBFS, GUCT-Normal2, and Softmin-Type(h) across multiple heuristics and domains
- The algorithm solves 67.8, 23.4, and 33.2 more instances than GBFS, GUCT-Normal2, and Softmin-Type(h) respectively under 10,000 node evaluations
- EVT-MCTS provides theoretical justification for Full Bellman backup in optimization tasks
- Performance gains are consistent across different domain sizes and complexity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The mismatch between standard UCB1 bandits and classical planning heuristics arises because UCB1 assumes bounded reward ranges, but heuristic values are unbounded.
- Mechanism: UCB1-Uniform leverages Peaks-Over-Threshold Extreme Value Theory to model heuristic values with uniform distributions, correctly handling their unbounded nature.
- Core assumption: Heuristic values in classical planning follow a uniform distribution with unknown support.
- Evidence anchors:
  - [abstract] "standard UCB1 bandits incorrectly assume bounded reward ranges incompatible with unbounded heuristic values"
  - [section 3] "The first is UCB1-Normal2's assumption that cost-to-go estimates fall anywhere in (−∞,∞), which is an under-specification that can be narrowed down to [0,∞) or even further."
- Break condition: If heuristic values do not follow a uniform distribution or have complex dependencies that violate the EVT assumptions.

### Mechanism 2
- Claim: Full Bellman backup lacks theoretical justification when used with average-based bandit algorithms like UCB1.
- Mechanism: EVT provides theoretical foundation for using extrema (minimum/maximum) in backup functions when the underlying distribution is uniform, resolving the theoretical conflict.
- Core assumption: The reward distribution is uniform, making extrema-based backup theoretically sound.
- Evidence anchors:
  - [abstract] "second is the insufficient statistical characterization of extrema (maximum/minimum) in so-called Full Bellman backup"
  - [section 3] "One candidate for addressing the extrema of reward distributions was proposed by Tesauro, Rajan, and Segal (2010)."
- Break condition: If the reward distribution is not uniform or if the assumptions about the relationship between extrema and averages do not hold.

### Mechanism 3
- Claim: The spread of heuristic values in a subtree affects exploration effectiveness, with plateaus (flat regions) hindering search progress.
- Mechanism: UCB1-Uniform penalizes small spreads in heuristic values, encouraging exploration of regions with more varied heuristic values and enabling quick escape from plateaus.
- Core assumption: The range of heuristic values (spread) provides meaningful information about the informativeness of a subtree.
- Evidence anchors:
  - [section 5] "Penalizing a small spread gives UCB1-Uniform/Normal2 an ability to avoid plateaus."
  - [section 5] "However, UCB1-Uniform can not only avoid, but also escape plateaus quickly."
- Break condition: If the spread of heuristic values does not correlate with the informativeness of the subtree or if the exploration term becomes too dominant.

## Foundational Learning

- Concept: Multi-Armed Bandit (MAB) theory and cumulative regret minimization
  - Why needed here: Understanding MAB theory is essential for grasping how bandit algorithms like UCB1-Uniform guide action selection in MCTS.
  - Quick check question: What is the difference between cumulative regret and extreme regret in the context of bandit algorithms?

- Concept: Extreme Value Theory (EVT) and Peaks-Over-Threshold (POT) approach
  - Why needed here: EVT provides the statistical foundation for modeling the extremes of heuristic value distributions, which is crucial for the theoretical justification of UCB1-Uniform.
  - Quick check question: How does the Peaks-Over-Threshold EVT differ from the Block Maxima EVT in modeling extreme values?

- Concept: Conjugate prior distributions and Bayesian updating
  - Why needed here: Understanding conjugate priors is important for grasping how the algorithm maintains and updates beliefs about the heuristic value distributions.
  - Quick check question: What is the relationship between the UniformConjugate class and the Pareto distribution mentioned in the appendix?

## Architecture Onboarding

- Component map:
  - MCTS core -> Tree-based search with selection, expansion, evaluation, and backpropagation phases
  - Bandit module -> UCB1-Uniform implementation for action selection using uniform distribution estimates
  - Conjugate distribution classes -> GaussianConjugate, UniformConjugate, etc. for maintaining beliefs about reward distributions
  - Transposition table -> Duplicate detection and subtree pruning based on g-values
  - Evaluation module -> Heuristic function evaluation with optional random sampling

- Critical path: Select leaf → Expand → Evaluate → Backpropagate → Repeat
- Design tradeoffs:
  - Using uniform distributions vs. Gaussian distributions: Uniform provides better theoretical justification for extrema-based backup but may be less flexible
  - Frequentist vs. Bayesian backup: Frequentist uses maximum likelihood estimates, while Bayesian incorporates prior beliefs
  - Node locking mechanism: Prevents duplicate search effort but adds complexity

- Failure signatures:
  - Poor performance on domains where heuristic values don't follow uniform distributions
  - Excessive exploration due to overestimation of spread in heuristic values
  - Memory issues due to large transposition tables in dense state spaces

- First 3 experiments:
  1. Compare GUCT-Uniform vs. GBFS on a small domain with known uniform heuristic distribution
  2. Test the effect of the exploration coefficient on performance across different domains
  3. Evaluate the impact of the threshold parameter in the PeaksOverThresholdEvaluator on solution quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical explanation for UCB1-Uniform significantly outperforming the asymptotically optimal CHK-Uniform, and UCB1-Normal2 outperforming UCB1-Normal?
- Basis in paper: [explicit] The paper notes that "UCB1-Uniform significantly outperformed CHK-Uniform" and "UCB1-Normal2 vs. UCB1-Normal" shows similar patterns where non-asymptotically-optimal bandits outperform asymptotically optimal ones.
- Why unresolved: The authors acknowledge this as "an important avenue of future work" but do not provide a theoretical explanation for why the sub-optimal bandits perform better in practice.
- What evidence would resolve it: Empirical studies comparing various parameter settings, theoretical analysis of regret bounds under different assumptions, and investigation of the trade-offs between exploration and exploitation in heuristic search contexts.

### Open Question 2
- Question: How does the choice of threshold θ in Peaks-Over-Threshold EVT affect the performance of UCB1-Uniform, and what is the optimal way to set this threshold?
- Basis in paper: [inferred] The paper mentions that "one last bit of the detail is how to set θ, but we actually do not use any explicit threshold" and relies on implicit filtering through not expanding states with heuristic values above the initial state's heuristic value.
- Why unresolved: The paper does not provide a systematic method for choosing θ, and the effectiveness of the implicit approach is not theoretically justified.
- What evidence would resolve it: Comparative studies with different threshold-setting strategies, theoretical analysis of the impact of threshold choice on regret bounds, and empirical validation across diverse planning domains.

### Open Question 3
- Question: Can the EVT-based approach be extended to handle unbounded or heavy-tailed heuristic distributions more effectively than current methods?
- Basis in paper: [explicit] The paper focuses on short-tailed distributions and uniform distributions, but acknowledges that "choosing an appropriate distribution" is important for different heuristics.
- Why unresolved: The current implementation uses uniform distributions as a special case of GP distributions to avoid numerical difficulties, but this may not be optimal for all heuristics.
- What evidence would resolve it: Development of EVT-based methods for heavy-tailed distributions, comparative studies across various heuristic types, and theoretical analysis of the trade-offs between different EVT approaches.

## Limitations

- The theoretical claims about EVT providing justification for extrema-based backups rely heavily on the assumption that heuristic values follow uniform distributions, which may not hold across all planning domains
- The experimental validation is limited to a specific node evaluation budget (10,000 nodes) and may not generalize to different computational constraints
- The paper does not provide extensive ablation studies on the threshold parameter in PeaksOverThresholdEvaluator or the exploration coefficient, leaving their optimal values domain-dependent

## Confidence

- High confidence in the experimental results showing GUCT-Uniform outperforms baselines, as the methodology and metrics are clearly specified
- Medium confidence in the theoretical justification, as EVT provides a solid foundation but the uniform distribution assumption needs more empirical validation
- Medium confidence in the mechanism explanations, particularly regarding plateau avoidance, which is intuitively appealing but could benefit from more quantitative analysis

## Next Checks

1. Test GUCT-Uniform with different exploration coefficients (0.1 to 2.0) across multiple domains to determine the optimal range and identify domains where the current value may be suboptimal
2. Implement and compare against alternative EVT-based approaches like Block Maxima to verify that Peaks-Over-Threshold is indeed the most appropriate method for this application
3. Conduct experiments with varying node evaluation budgets (1,000 to 100,000) to assess how performance scales with computational resources and identify potential diminishing returns