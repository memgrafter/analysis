---
ver: rpa2
title: Towards Cost Sensitive Decision Making
arxiv_id: '2410.03892'
source_url: https://arxiv.org/abs/2410.03892
tags:
- acquisition
- features
- task
- action
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Active-Acquisition POMDP (AA-POMDP) to model
  sequential decision making with cost-sensitive feature acquisition. It identifies
  two types: Sequential (features acquired one-by-one before task action) and Batch
  (all features acquired simultaneously).'
---

# Towards Cost Sensitive Decision Making

## Quick Facts
- arXiv ID: 2410.03892
- Source URL: https://arxiv.org/abs/2410.03892
- Authors: Yang Li; Junier Oliva
- Reference count: 40
- The paper introduces Active-Acquisition POMDP (AA-POMDP) to model sequential decision making with cost-sensitive feature acquisition, achieving state-of-the-art performance on CartPole and Sepsis environments.

## Executive Summary
This paper addresses cost-sensitive decision making in partially observable environments where an agent must actively acquire features to make task decisions. The authors introduce Active-Acquisition POMDP (AA-POMDP) as a framework that models both sequential and batch feature acquisition modes. They propose a model-based approach using a deep generative model (POSS) to impute missing features and form belief states, combined with hierarchical reinforcement learning that separates feature acquisition from task execution. The method demonstrates superior performance compared to baselines while providing theoretical grounding through Bellman equations and exploration-exploitation analysis.

## Method Summary
The method employs a hierarchical reinforcement learning architecture with two policies: a low-level acquisition policy for selecting features to observe and a high-level task policy for making decisions based on belief states. The belief states are generated using POSS (Partially Observed Set models for Sequences), a generative model that imputes missing features using variational autoencoders with normalizing flows. The acquisition policy receives intrinsic rewards based on acquisition cost, entropy of the task policy, value estimates, and imputation accuracy. Training uses PPO with these intrinsic rewards, balancing feature acquisition costs against task performance improvements.

## Key Results
- Achieves state-of-the-art performance on CartPole and Sepsis environments compared to baselines
- Belief estimation consistently improves results over using observation history directly
- Demonstrates faster convergence and more stable training than non-hierarchical approaches
- Shows effectiveness in both sequential (one-by-one feature acquisition) and batch (simultaneous acquisition) settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Belief estimation via generative imputation enables better decisions than raw partial observations
- Mechanism: The model imputes unobserved features using a sequential generative model (POSS) conditioned on observed features, actions, and timestamps
- Core assumption: The generative model can learn the joint distribution of features across time steps well enough to produce useful imputations
- Evidence anchors: "The imputations essentially represent the beliefs of the agent"; "b(h) = {(x(h)v, ˆx(h)u)n}N n=1"
- Break condition: If the feature dependencies are too complex or non-stationary for the generative model to capture, imputations will be inaccurate and beliefs will mislead the policy

### Mechanism 2
- Claim: Intrinsic rewards from belief accuracy guide the acquisition policy toward informative features
- Mechanism: The acquisition policy receives rewards based on acquisition cost, entropy of task policy, value estimates of task policy, and imputation accuracy of the generative model
- Core assumption: Features that improve belief accuracy also improve task policy performance
- Evidence anchors: "We hence use the imputation accuracy as one of the acquisition rewards, i.e., Acc(x(h)u , ˆx(h)u )"
- Break condition: If the relationship between belief accuracy and task performance is weak or noisy, the intrinsic reward signal becomes unreliable

### Mechanism 3
- Claim: Hierarchical decomposition of acquisition and task policies enables efficient exploration and stable learning
- Mechanism: The acquisition policy (low-level) selects features to observe, while the task policy (high-level) makes decisions based on the resulting beliefs
- Core assumption: The two policies can be effectively coordinated through the belief interface
- Evidence anchors: "We decompose the agent into two policies, the feature acquisition policy πf and the task policy πc"
- Break condition: If the coordination between policies becomes too complex or the belief representation becomes a bottleneck, the hierarchical structure may degrade performance

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The environment is only partially observable because features are acquired actively, requiring reasoning over belief states
  - Quick check question: What is the difference between a POMDP and an MDP in terms of state observability?

- Concept: Variational Autoencoders (VAEs) and normalizing flows
  - Why needed here: The generative model uses VAEs with normalizing flows to capture complex feature dependencies and enable flexible belief imputation
  - Quick check question: How do normalizing flows improve upon standard Gaussian posteriors in VAEs?

- Concept: Hierarchical Reinforcement Learning
  - Why needed here: The acquisition and task policies are decomposed into a hierarchy to handle the different timescales and objectives of feature acquisition versus task execution
  - Quick check question: What are the benefits and challenges of hierarchical RL compared to flat RL approaches?

## Architecture Onboarding

- Component map:
  Environment -> Partial Observation -> Generative Model (POSS) -> Belief Representation -> Acquisition Policy (πf) -> Environment -> Task Policy (πc) -> Action

- Critical path:
  1. Environment provides partial observation
  2. Generative model imputes missing features to create belief
  3. Acquisition policy selects features to acquire
  4. Environment returns new partial observation
  5. Task policy selects action based on updated belief
  6. Reward is computed and policies are updated

- Design tradeoffs:
  - Belief estimation vs. direct use of partial observations: Belief estimation adds computational overhead but can improve decision quality
  - Intrinsic rewards vs. task rewards only: Intrinsic rewards guide acquisition but may introduce noise
  - Hierarchical vs. flat policies: Hierarchy can improve exploration but adds coordination complexity

- Failure signatures:
  - Poor belief accuracy: Acquisition policy may not improve task performance despite selecting informative features
  - Unstable training: Intrinsic rewards or hierarchical coordination may cause training instability
  - High computational cost: Belief imputation and hierarchical updates may be too slow for real-time applications

- First 3 experiments:
  1. Compare belief-based policy vs. observation-history policy on a simple POMDP environment
  2. Evaluate the impact of intrinsic rewards on acquisition efficiency and task performance
  3. Test the hierarchical policy vs. flat policy on a more complex environment with diverse feature costs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the belief estimation quality affect the exploration-exploitation trade-off in AA-POMDP?
- Basis in paper: The paper mentions belief estimation helps alleviate the exploration-exploitation dilemma, but doesn't quantify this effect
- Why unresolved: The paper shows belief estimation improves performance but doesn't analyze its specific impact on exploration-exploitation balance
- What evidence would resolve it: Empirical studies comparing exploration metrics (e.g., entropy of acquisition policies, state visitation counts) with and without belief estimation across different environments

### Open Question 2
- Question: What is the theoretical relationship between the imputation accuracy reward weight (ωa) and optimal policy performance?
- Basis in paper: The paper uses a fixed weight of 100.0 for imputation accuracy reward but doesn't explore sensitivity to this parameter
- Why unresolved: The paper sets hyperparameters based on experience but doesn't analyze how varying ωa affects learning dynamics or final performance
- What evidence would resolve it: Systematic hyperparameter sweeps showing performance as a function of ωa, potentially revealing optimal ranges or monotonic relationships

### Open Question 3
- Question: How does the choice between sequential and batch acquisition affect sample efficiency in different environments?
- Basis in paper: The paper compares both settings but doesn't analyze their relative sample efficiency during training
- Why unresolved: While final performance is compared, the paper doesn't quantify how many environment interactions each approach needs to converge
- What evidence would resolve it: Learning curves normalized by number of interactions showing convergence speed for both acquisition modes across environments with varying complexity

## Limitations
- The effectiveness relies heavily on the generative model's ability to accurately impute missing features, which may struggle with highly complex or non-stationary feature dependencies
- The hierarchical decomposition assumes effective coordination between acquisition and task policies, but the belief representation could become a bottleneck in more complex environments
- The intrinsic reward signal linking belief accuracy to task performance is based on reasonable assumptions but lacks direct empirical validation

## Confidence
- High confidence in the technical framework and hierarchical decomposition approach, as these follow established POMDP and RL methodologies with clear theoretical grounding
- Medium confidence in the belief estimation mechanism's general effectiveness, as the paper demonstrates improved performance over observation-history baselines but doesn't fully characterize failure modes or limitations
- Medium confidence in the intrinsic reward design, as the connection between belief accuracy and task performance is logical but not rigorously proven, and the weighting of multiple reward components could be sensitive to hyperparameter tuning

## Next Checks
1. **Ablation study on belief vs. observation history**: Systematically compare belief-based policies against observation-history baselines across environments with varying feature complexity to quantify the marginal benefit of belief estimation and identify when it becomes essential versus optional.

2. **Robustness analysis of generative imputation**: Test the model's performance when feature distributions shift or when dependencies become more complex, measuring imputation accuracy degradation and its impact on downstream policy performance to establish failure boundaries.

3. **Intrinsic reward component sensitivity**: Conduct a hyperparameter sweep on the intrinsic reward weights (entropy, value, imputation accuracy) to identify optimal configurations and characterize the sensitivity of training stability and convergence to these design choices.