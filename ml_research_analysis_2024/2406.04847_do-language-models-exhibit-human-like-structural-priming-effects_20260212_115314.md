---
ver: rpa2
title: Do Language Models Exhibit Human-like Structural Priming Effects?
arxiv_id: '2406.04847'
source_url: https://arxiv.org/abs/2406.04847
tags:
- priming
- effects
- language
- structural
- overlap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Language models exhibit structural priming effects similar to humans,
  showing stronger priming for less frequent structures and increased priming with
  lexical overlap between prime and target. Priming effects are asymmetric, with models
  showing higher priming for double object constructions compared to prepositional
  object constructions, contrary to human patterns.
---

# Do Language Models Exhibit Human-like Structural Primating Effects?

## Quick Facts
- **arXiv ID:** 2406.04847
- **Source URL:** https://arxiv.org/abs/2406.04847
- **Reference count:** 40
- **One-line primary result:** Language models exhibit structural priming effects similar to humans, showing stronger priming for less frequent structures and increased priming with lexical overlap between prime and target

## Executive Summary
This paper investigates whether large language models exhibit structural priming effects similar to humans. The authors test eight different language models on dative constructions from the Prime-LM corpus, measuring how exposure to prime sentences influences the likelihood of target sentences with congruent versus incongruent grammatical structures. They find that models do show priming effects, with stronger effects for less frequent structures and increased priming with lexical overlap, particularly function words. However, the asymmetry of priming effects differs from human patterns, with models showing higher priming for double object constructions compared to prepositional object constructions.

## Method Summary
The authors use the Prime-LM corpus containing dative constructions (double object and prepositional object) with three conditions: Core (no lexical overlap), Semantic Similarity (varying noun/verb similarity), and Lexical Overlap (shared words). They compute sentence-level and token-level priming effects using the diagnose library across eight language models. Linear mixed-effects models are fitted with predictors including lexical overlap, surprisal (prime/target), and structural verb preference. The analysis examines which factors predict priming effects and how they compare to human priming literature.

## Key Results
- Language models exhibit structural priming effects similar to humans, showing stronger priming for less frequent structures
- Priming effects are asymmetric, with models showing higher priming for double object constructions compared to prepositional object constructions, contrary to human patterns
- Token-level analysis reveals that priming effects are strongest at the start of the second noun phrase and are significantly influenced by function word overlap, particularly prepositions and determiners

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models exhibit structural priming effects similar to humans
- Mechanism: The models adjust their likelihood estimates for target sentences based on prior exposure to structurally similar sentences, with effects mediated by surprisal and structural preference
- Core assumption: The models have learned abstract grammatical representations that can be primed through exposure to specific structures
- Evidence anchors:
  - [abstract] "Language models exhibit structural priming effects similar to humans, showing stronger priming for less frequent structures and increased priming with lexical overlap between prime and target"
  - [section] "We show that these effects can be explained via the inverse frequency effect, known in human priming, where rarer elements within a prime increase priming effects, as well as lexical dependence between prime and target"
  - [corpus] "Average neighbor FMR=0.459" (moderate corpus similarity, but not strong direct evidence)
- Break condition: If the model's training data did not contain sufficient examples of the target structures, or if the model architecture cannot represent abstract grammatical structures, priming effects would not manifest

### Mechanism 2
- Claim: Priming effects are asymmetric and predictable through inverse frequency effects
- Mechanism: Models show stronger priming for less preferred structures because the surprisal of encountering a dispreferred structure increases the subsequent likelihood of producing that structure
- Core assumption: The model's learned probability distribution reflects human-like preferences for certain syntactic structures
- Evidence anchors:
  - [abstract] "Priming effects are asymmetric, with models showing higher priming for double object constructions compared to prepositional object constructions, contrary to human patterns"
  - [section] "We demonstrate that models, like humans, exhibit inverse frequency effects in terms of surprisal and verb preference, and that these are predictive of priming"
  - [corpus] Weak corpus evidence - the related papers don't directly address asymmetry
- Break condition: If the model's training data had different frequency distributions than human language, or if the model weights were initialized differently, the asymmetry might not manifest

### Mechanism 3
- Claim: Lexical overlap, particularly function words, strongly influences priming effects
- Mechanism: Shared lexical items between prime and target activate relevant syntactic frames, with function words playing a surprisingly strong role in models compared to humans
- Core assumption: The model's representations encode both content and function word information in ways that can be leveraged for structural prediction
- Evidence anchors:
  - [abstract] "Token-level analysis reveals that priming effects are strongest at the start of the second noun phrase and are significantly influenced by function word overlap, particularly prepositions and determiners"
  - [section] "We observe that lexical overlap is the strongest predictor of priming behaviour, in particular for primes sharing the same verbs, prepositions and determiners as the targets"
  - [corpus] "Average neighbor FMR=0.459" - moderate evidence of related work on priming mechanisms
- Break condition: If the model's attention mechanisms don't effectively utilize function word information, or if the tokenization breaks apart function words, this mechanism would fail

## Foundational Learning

- Concept: Structural priming in human language processing
  - Why needed here: Understanding the human priming literature is essential to interpret model behavior and identify which factors to investigate
  - Quick check question: What is the inverse frequency effect and how does it relate to structural priming in humans?

- Concept: Linear mixed effects models for statistical analysis
  - Why needed here: The regression analysis used to identify predictive factors for priming effects requires understanding of mixed models and their interpretation
  - Quick check question: What is the purpose of including random intercepts for models in the mixed effects model?

- Concept: Token-level probability computation in autoregressive models
  - Why needed here: The token-level priming effect metric requires understanding how conditional probabilities are computed across token positions
  - Quick check question: How is the token-level priming effect computed and how does it relate to the sentence-level measure?

## Architecture Onboarding

- Component map: Prime sentence → Model processing → Target sentence probability computation → Priming effect calculation → Statistical analysis → Interpretation of factors
- Critical path: Prime sentence → Model processing → Target sentence probability computation → Priming effect calculation → Statistical analysis → Interpretation of factors
- Design tradeoffs: Sentence-level vs token-level analysis (coarser vs finer grained), core condition vs lexical overlap conditions (controlled vs ecological validity), linear mixed models vs other statistical approaches (flexibility vs interpretability)
- Failure signatures: Negative priming effects (inverse priming), asymmetric priming favoring one structure, lack of lexical overlap effects, failure of inverse frequency predictions
- First 3 experiments:
  1. Replicate sentence-level priming effect computation for a single model and construction pair to verify basic functionality
  2. Compute token-level priming effects for the same pair to locate where priming manifests
  3. Test lexical overlap manipulation by creating prime-target pairs with shared verbs and measuring the change in priming effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do priming effects in language models generalize to other syntactic constructions beyond dative alternations?
- Basis in paper: [inferred] The paper focuses specifically on dative constructions and notes that "More work remains to be done to investigate whether these results generalize across other constructions within English or further extend to other languages."
- Why unresolved: The study only examined dative constructions, leaving open whether similar priming patterns and asymmetries exist for other syntactic structures.
- What evidence would resolve it: Experiments testing structural priming effects across diverse syntactic constructions (e.g., relative clauses, passive vs. active voice, verb-particle constructions) in the same language models, with comparison to human priming data for these structures.

### Open Question 2
- Question: What specific mechanisms in language model training lead to the observed inverse frequency effects in priming?
- Basis in paper: [explicit] The paper notes that "An interesting endeavour for future work would be to test this finding in a setting with control over data distribution... to ensure that inverse frequency effects do not stem from some other indirect effect of language learning."
- Why unresolved: While the paper observes inverse frequency effects matching human patterns, it cannot definitively attribute these to error-based implicit learning rather than other aspects of training data or model architecture.
- What evidence would resolve it: Controlled training experiments varying data distributions, training objectives, or architectural components to isolate whether inverse frequency effects specifically emerge from error-based learning mechanisms.

### Open Question 3
- Question: Why do language models show such high sensitivity to function word overlap (particularly prepositions and determiners) in priming effects, contrary to human findings?
- Basis in paper: [explicit] The paper states "contrary to human findings in both production (e.g. Bock, 1989) and comprehension (e.g. Traxler, 2008), prepositional overlap is one of the strongest priming predictors" and observes "determiner overlap is consistently of high importance."
- Why unresolved: The paper identifies this discrepancy but does not explain the underlying reason for why models treat function word overlap differently than humans.
- What evidence would resolve it: Comparative analysis of how function words are processed in model attention mechanisms versus human neural processing, potentially through interpretability techniques or controlled experiments manipulating function word properties.

### Open Question 4
- Question: How does model size and architectural complexity affect the strength and asymmetry of structural priming effects?
- Basis in paper: [explicit] The paper notes "although the sample is too small to make broad generalisations, in the models we test, we observe larger models exhibit more skewed priming behaviour in the core, and higher susceptibility to lexical boosting than the smaller GPT2."
- Why unresolved: The paper tested a limited range of model sizes and cannot draw firm conclusions about the relationship between model scale and priming behavior.
- What evidence would resolve it: Systematic testing across a wider range of model sizes and architectures (including varying transformer depths, widths, and attention mechanisms) to determine whether priming effects scale predictably with model capacity.

### Open Question 5
- Question: How do priming effects observed in forced prediction settings translate to actual generation in language models?
- Basis in paper: [explicit] The paper discusses that "It is important to remember, however, that through the way LLMs are trained, their predictions will be driven by human production patterns in the training data" and suggests investigating "the extent to which these priming effects influence structural repetition patterns in generation."
- Why unresolved: The study uses a fixed-target priming metric that constrains generation, leaving unclear whether the same effects manifest in free-form generation.
- What evidence would resolve it: Generation experiments where models produce sentences conditioned on primes, measuring whether structural preferences persist without target constraints, and comparing these patterns to the constrained priming effects observed in the study.

## Limitations

- The corpus similarity score of 0.459 indicates moderate relatedness to prior work but suggests the findings are not directly replicating established human priming studies
- The asymmetry in priming effects (higher for double object constructions despite human data showing the opposite pattern) raises questions about whether models truly capture human-like structural preferences
- The strong influence of function words on priming effects may reflect artifacts of model architecture or tokenization rather than genuine linguistic processing similarities

## Confidence

**High Confidence**: The existence of measurable structural priming effects in language models, the positive relationship between lexical overlap and priming magnitude, and the inverse frequency effect showing stronger priming for less frequent structures.

**Medium Confidence**: The asymmetric nature of priming effects between construction types, the specific role of function words in driving priming effects, and the predictive power of surprisal and verb structural preferences.

**Low Confidence**: The claim that models exhibit "human-like" structural priming given the observed asymmetries, and the interpretation that these effects represent genuine abstract grammatical representations rather than statistical artifacts.

## Next Checks

1. **Cross-linguistic validation**: Test the same priming paradigms with dative constructions in a non-English language to determine if the observed asymmetries are language-specific or reflect general model properties.

2. **Architectural ablation study**: Compare priming effects across models with different attention mechanisms (e.g., transformer variants, recurrent architectures) to isolate whether the function word effects stem from specific architectural features.

3. **Human comparison replication**: Conduct a controlled human priming experiment using identical materials to directly compare the magnitude, direction, and lexical sensitivity of priming effects between humans and models.