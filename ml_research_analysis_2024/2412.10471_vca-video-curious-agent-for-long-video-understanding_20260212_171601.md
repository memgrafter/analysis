---
ver: rpa2
title: 'VCA: Video Curious Agent for Long Video Understanding'
arxiv_id: '2412.10471'
source_url: https://arxiv.org/abs/2412.10471
tags:
- video
- reward
- frames
- agent
- segment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VCA, a curiosity-driven video agent that autonomously
  explores long video content for understanding tasks. Instead of uniformly sampling
  frames, VCA employs tree-search exploration guided by a self-generated intrinsic
  reward from the VLM, efficiently identifying relevant segments while maintaining
  a fixed-size memory buffer to manage computational cost.
---

# VCA: Video Curious Agent for Long Video Understanding

## Quick Facts
- arXiv ID: 2412.10471
- Source URL: https://arxiv.org/abs/2412.10471
- Reference count: 40
- Primary result: Achieves 41.3% accuracy on LVBench while observing fewer frames than baselines

## Executive Summary
This paper introduces VCA, a curiosity-driven video agent that autonomously explores long video content for understanding tasks. Rather than uniformly sampling frames, VCA employs tree-search exploration guided by self-generated intrinsic rewards from a VLM, efficiently identifying relevant segments while maintaining a fixed-size memory buffer to manage computational cost. Experiments show VCA achieves superior performance on long video benchmarks while observing fewer frames and incurring comparable or lower computational cost than existing baselines.

## Method Summary
VCA treats video exploration as an interactive task, using a tree-search algorithm to navigate video segments guided by a self-generated intrinsic reward model. The agent maintains a fixed-size memory buffer to store relevant frames, discarding less relevant ones when capacity is reached. At each step, the reward model scores segment relevance using chain-of-thought reasoning, the agent selects segments based on these scores and memory content, and the process iterates until sufficient information is gathered to answer the query. The approach requires no training, using off-the-shelf LLMs/VLMs for exploration and reasoning.

## Key Results
- Achieves 41.3% accuracy on LVBench, outperforming VideoAgent (40.1%) and VideoTree (39.9%)
- Reaches 73.6% accuracy on EgoSchema while observing only 22.3 frames per query
- Demonstrates 2.4x fewer frames observed compared to VideoAgent on average across benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tree-search exploration with intrinsic reward enables efficient identification of relevant video segments
- Mechanism: The agent treats the video as an environment to explore, using a tree structure to navigate segments. A reward model assigns relevance scores to segments based on their alignment with the query, guiding the agent's exploration decisions. The agent can backtrack and explore different paths if the current trajectory seems suboptimal.
- Core assumption: VLMs can generate meaningful intrinsic rewards that effectively guide exploration toward relevant content
- Evidence anchors:
  - [abstract]: "Rather than relying on external feedback or reward, VCA leverages VLM's self-generated intrinsic reward to guide its exploration"
  - [section 3.3]: "we incorporate a reward model R to guide the agent's selection process, i.e., the agent will gain a higher reward if it plans to take the action to explore more relevant segment"
  - [corpus]: Weak evidence - no direct citations for this specific mechanism

### Mechanism 2
- Claim: Memory management with fixed-size buffer reduces computational costs while preserving relevant information
- Mechanism: A fixed-size memory buffer stores the most relevant frames throughout exploration. When the buffer exceeds capacity, frames with the lowest relevance scores are discarded. This allows reasoning within a fixed resource limit regardless of exploration trajectory length.
- Core assumption: Relevance scores can reliably identify which frames contain the most crucial information
- Evidence anchors:
  - [section 3.2]: "we introduce a fixed-size memory buffer M that stores only the most relevant frames throughout exploration"
  - [abstract]: "VCA employs a fixed-size memory buffer to manage computational cost"
  - [corpus]: Weak evidence - no direct citations for this specific memory management approach

### Mechanism 3
- Claim: Segment-level exploration is more effective than frame-level similarity matching for identifying key content
- Mechanism: Instead of using image similarity to select key frames, VCA explores video segments as whole units. The agent makes decisions based on segment semantics rather than individual frame appearance, allowing it to capture video-specific features that frame similarity might miss.
- Core assumption: Segment-level semantic understanding captures more relevant information than frame-level visual similarity
- Evidence anchors:
  - [section 3.3]: "Inspired by humans adaptively focusing on different video segments during exploration rather than targeting specific frames directly"
  - [section 5.2]: "we observe that relying solely on image relevance instead of inferring segment semantics results in a performance drop"
  - [corpus]: Weak evidence - no direct citations for this specific approach

## Foundational Learning

- Concept: Tree search algorithms
  - Why needed here: The exploration strategy uses a tree structure to navigate video segments, requiring understanding of how tree search balances exploration and exploitation
  - Quick check question: How does the agent decide which segment to explore next when multiple candidates have different reward scores?

- Concept: Intrinsic motivation in reinforcement learning
  - Why needed here: The reward model generates intrinsic rewards rather than using external feedback, requiring understanding of how agents can be guided by self-generated signals
  - Quick check question: What makes a reward model's intrinsic rewards more reliable than random exploration?

- Concept: Memory management and buffer overflow handling
  - Why needed here: The fixed-size buffer must efficiently manage limited resources while preserving the most relevant information
  - Quick check question: How does the system decide which frames to discard when the buffer reaches capacity?

## Architecture Onboarding

- Component map: Query → Reward Model Scoring → Agent Selection → Memory Update → Answer or Continue
- Critical path: Query → Reward Model Scoring → Agent Selection → Memory Update → Answer or Continue
- Design tradeoffs:
  - Fixed buffer size vs. adaptive memory: Fixed size simplifies implementation but may discard relevant information
  - Segment-based vs. frame-based exploration: Segment-based captures video semantics but may miss frame-level details
  - Greedy vs. guided exploration: Pure greedy selection is faster but may miss optimal paths
- Failure signatures:
  - Low accuracy despite high frame count: Reward model is assigning poor relevance scores
  - High frame count with low accuracy: Tree search is exploring irrelevant paths
  - Memory overflow errors: Buffer size too small for the exploration trajectory
- First 3 experiments:
  1. Ablation test: Remove reward model to measure impact on exploration quality
  2. Parameter sweep: Vary memory buffer size to find optimal trade-off between accuracy and efficiency
  3. Comparison test: Replace segment-level exploration with frame-level similarity matching to validate the approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between exploration and exploitation in VCA's tree-search framework, and how does this balance affect performance across different video domains?
- Basis in paper: [inferred] The paper discusses how VCA balances exploration and exploitation by allowing the agent to backtrack to previous segments when current exploration paths seem suboptimal, rather than purely following reward scores. The analysis in Sec. 5.2 shows that this behavior leads to more robust performance.
- Why unresolved: The paper demonstrates that this balance works in practice but doesn't systematically investigate how different exploration-exploitation trade-offs affect performance across various video understanding tasks or domains.
- What evidence would resolve it: Controlled experiments varying the exploration-exploitation parameters across different video domains (e.g., sports, documentaries, tutorials) to identify optimal settings for each domain type.

### Open Question 2
- Question: How would training a specialized reward model specifically for VCA impact performance compared to using the same VLM for both exploration and reward functions?
- Basis in paper: [explicit] The paper acknowledges that using the same VLM for both roles is "neat and efficient" but also shows in Sec. 5.4 that using ground truth reward scores yields a consistent 3% improvement on LVBench, suggesting room for improvement.
- Why unresolved: The paper uses the same VLM for both roles for simplicity but doesn't explore whether a dedicated, trained reward model could provide more accurate guidance and further improve performance.
- What evidence would resolve it: Comparative experiments training a specialized reward model (potentially using synthetic data) versus the current approach, measuring improvements in accuracy and efficiency across benchmarks.

### Open Question 3
- Question: What is the theoretical upper bound of VCA's performance with perfect reward guidance, and how does this compare to human-level performance on long video understanding tasks?
- Basis in paper: [explicit] Sec. 5.4 demonstrates that using ground truth reward scores yields a 3% improvement, establishing a clear performance gap between current reward guidance and perfect guidance.
- Why unresolved: The paper shows the gap between current and ground truth reward scores but doesn't investigate how close this brings VCA to human-level performance or what the theoretical maximum performance might be.
- What evidence would resolve it: Comparative studies measuring VCA's performance with ground truth rewards against human performance on the same tasks, along with analysis of remaining gaps to identify whether they stem from the exploration strategy, reasoning capabilities, or other factors.

## Limitations

- The approach relies on black-box LLM/VLM models without fine-tuning, making performance dependent on the quality of these underlying systems
- The fixed-size memory buffer represents a hard constraint that could lead to information loss in complex videos
- The segment-based exploration approach assumes that segment boundaries align with meaningful content boundaries, which may not always hold

## Confidence

- High Confidence: The core architectural design (tree-search exploration with intrinsic rewards and memory buffer) is clearly specified and experimentally validated on multiple benchmarks
- Medium Confidence: The effectiveness of segment-level exploration over frame-level similarity matching is supported by ablation studies
- Low Confidence: The reward model's ability to consistently generate meaningful intrinsic rewards across diverse video domains is not extensively validated

## Next Checks

1. **Reward Model Robustness Test:** Systematically evaluate the reward model's performance on videos with varying information density, content complexity, and temporal structure to identify failure modes and reliability limits

2. **Segment Boundary Sensitivity Analysis:** Test how the agent performs when segment boundaries are varied (overlapping vs. non-overlapping, different sizes) to understand the sensitivity of the approach to segmentation choices

3. **Cross-Domain Generalization Study:** Apply VCA to video domains outside the benchmark datasets (e.g., surveillance footage, sports analysis, educational content) to assess whether the intrinsic reward mechanism generalizes beyond the tested scenarios