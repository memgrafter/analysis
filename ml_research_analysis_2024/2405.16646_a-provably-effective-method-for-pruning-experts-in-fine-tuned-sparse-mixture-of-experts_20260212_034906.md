---
ver: rpa2
title: A Provably Effective Method for Pruning Experts in Fine-tuned Sparse Mixture-of-Experts
arxiv_id: '2405.16646'
source_url: https://arxiv.org/abs/2405.16646
tags:
- pruning
- experts
- expert
- learning
- router
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first theoretical analysis and empirical
  method for pruning experts in fine-tuned mixture-of-experts (MoE) models. The key
  insight is that experts with smaller changes in router L2 norm during fine-tuning
  can be safely pruned without harming accuracy, while significantly reducing model
  size and computational requirements.
---

# A Provably Effective Method for Pruning Experts in Fine-tuned Sparse Mixture-of-Experts

## Quick Facts
- arXiv ID: 2405.16646
- Source URL: https://arxiv.org/abs/2405.16646
- Reference count: 40
- Primary result: First theoretical analysis showing experts with smaller router L2 norm changes during fine-tuning can be safely pruned

## Executive Summary
This paper presents the first theoretical analysis and empirical method for pruning experts in fine-tuned sparse Mixture-of-Experts (MoE) models. The authors prove that experts learning task-relevant features exhibit larger changes in router L2 norm during fine-tuning than irrelevant experts, enabling effective pruning. Their method can prune up to 75% of experts while reducing model size by 60% and inference FLOPs by 40%, all while maintaining accuracy within 1% of the original model. Experiments on vision MoE models (V-MoE and E3-MoE) fine-tuned on CIFAR-10, CIFAR-100, and ImageNet demonstrate superior performance compared to baseline pruning methods.

## Method Summary
The proposed method prunes experts based on the L2 norm changes of their router weights during fine-tuning. The core insight is that experts learning task-relevant features undergo larger norm changes than those learning irrelevant features. After pruning, post-pruning fine-tuning promotes remaining experts to learn task-specific features more effectively. The method involves: (1) fine-tuning pre-trained MoE models on downstream tasks, (2) calculating router L2 norm changes for each expert, (3) pruning experts with smallest norm changes based on desired ratio, and (4) optionally performing post-pruning fine-tuning to recover accuracy.

## Key Results
- Up to 75% of experts can be pruned while maintaining accuracy within 1% of unpruned model
- Model size reduced by 60% and inference FLOPs reduced by 40% through pruning
- Outperforms baseline methods using importance scores or magnitude-based pruning metrics
- Validated on V-MoE and E3-MoE architectures across CIFAR-10, CIFAR-100, and ImageNet datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Experts learning task-relevant features exhibit larger router L2 norm changes during fine-tuning than experts learning irrelevant features.
- Mechanism: During fine-tuning, experts specializing in task-specific patterns receive higher gating values, leading to stronger gradient updates on router weights and larger norm changes.
- Core assumption: Router's ability to select task-specific patterns correlates with magnitude of weight norm change during fine-tuning.
- Evidence anchors: [abstract] experts with smaller changes in router L2 norm can be safely pruned; [section 4.1] experts learning task-relevant features lead to large router L2 norm changes.
- Break condition: If routing becomes saturated or irrelevant experts receive high gating values for task-relevant patterns.

### Mechanism 2
- Claim: Post-pruning fine-tuning promotes unpruned experts to learn task-specific features more effectively.
- Mechanism: After pruning irrelevant experts, remaining experts receive more focused training on task-relevant patterns since tokens are routed exclusively to them.
- Core assumption: Token distribution among remaining experts becomes more concentrated on task-relevant patterns after pruning.
- Evidence anchors: [section 4.1] post-pruning fine-tuning promotes learning task-specific features; [section 4.3] ensures hidden neurons learn task-specific features.
- Break condition: If remaining experts cannot represent full task distribution or pruning ratio is too high.

### Mechanism 3
- Claim: The proposed pruning method guarantees generalization for a wide range of pruning ratios.
- Mechanism: Pruning experts with smallest router norm changes (proven unimportant) removes redundant components without affecting core task-specific learning.
- Core assumption: Theoretical framework correctly identifies unimportant experts based on router norm change, robust across different pruning ratios.
- Evidence anchors: [abstract] pruning smaller norm changes guarantees pruning of irrelevant experts; [section 4.3] shows non-task-specific experts can be pruned without hurting accuracy.
- Break condition: If theoretical assumptions about data distribution or expert behavior don't hold in practice.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) framework and feature learning dynamics
  - Why needed here: Theoretical analysis relies on understanding how neural networks learn features during training, specifically how experts develop specialization in task-relevant patterns
  - Quick check question: Can you explain the difference between NTK-based approaches and feature learning frameworks in analyzing neural network training dynamics?

- Concept: Mixture-of-Experts (MoE) architecture and routing mechanisms
  - Why needed here: Paper builds on understanding how MoE layers work, particularly interaction between experts and routers, and how tokens are routed
  - Quick check question: What is the difference between token-choice routing and expert-choice routing in MoE architectures?

- Concept: Network pruning theory and practice
  - Why needed here: Paper applies pruning concepts specifically to MoE architectures, requiring understanding of both unstructured and structured pruning methods
  - Quick check question: How does structured pruning differ from unstructured pruning in terms of hardware efficiency and implementation complexity?

## Architecture Onboarding

- Component map:
  - Experts: Multiple position-wise FFN modules with weights W1(s) and W2(s)
  - Routers: Trainable parameters ws that determine gating values for routing tokens to experts
  - Gating network: Collective term for all routers, implementing token-choice or expert-choice routing
  - MoE layer: Combines experts and routers to process input tokens and produce output tokens

- Critical path:
  1. Input tokens enter MoE layer
  2. Routers calculate routing values for each token-expert pair
  3. Routing function (token-choice or expert-choice) selects top experts/tokens
  4. Gating values are computed using softmax over selected experts/tokens
  5. Selected experts process tokens and produce output
  6. Outputs are aggregated to form final output tokens

- Design tradeoffs:
  - Token-choice vs expert-choice routing: Token-choice provides more flexible routing but requires more computation; expert-choice is more efficient but less flexible
  - Number of experts vs router complexity: More experts increase model capacity but also increase routing computation and memory requirements
  - Expert capacity vs load balancing: Larger experts can learn more complex features but may create imbalance in token distribution

- Failure signatures:
  - If experts show similar router norm changes regardless of importance, pruning method loses effectiveness
  - If post-pruning fine-tuning fails to converge or significantly changes model behavior
  - If remaining experts cannot adequately represent full task distribution after pruning

- First 3 experiments:
  1. Implement proposed pruning method on simple MoE model with synthetic data following data model in section 4.2
  2. Compare proposed method against random pruning and magnitude-based pruning on CIFAR-10 using V-MoE model
  3. Test post-pruning fine-tuning effectiveness by measuring accuracy recovery on ImageNet with different pruning ratios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed expert pruning method perform when applied to MoE architectures with different routing strategies, such as random routing or learned routing beyond token-choice and expert-choice?
- Basis in paper: [inferred] Paper primarily focuses on token-choice and expert-choice routing methods, with theoretical analysis centered on expert-choice routing but validated on token-choice routing MoEs.
- Why unresolved: Paper does not explore effectiveness on other routing strategies, leaving generalizability to different MoE architectures open.
- What evidence would resolve it: Empirical results demonstrating performance on MoE models with various routing strategies like random routing or other learned routing methods.

### Open Question 2
- Question: Can the expert pruning method be extended to handle multi-class classification tasks, and what are the theoretical guarantees for such an extension?
- Basis in paper: [explicit] Paper mentions theoretical analysis is centered on binary classification tasks but can be extended to multi-class cases at cost of higher complexity.
- Why unresolved: Paper does not provide detailed analysis or experimental results for multi-class classification tasks.
- What evidence would resolve it: Theoretical extension of analysis to multi-class classification tasks along with experimental results demonstrating method's performance on such tasks.

### Open Question 3
- Question: How does the expert pruning method impact the robustness of MoE models to adversarial attacks, and can it be integrated with adversarial training techniques?
- Basis in paper: [inferred] Paper focuses on reducing model size and computational requirements while maintaining accuracy but does not address impact on robustness to adversarial attacks.
- Why unresolved: Potential trade-off between model compression and adversarial robustness is not explored.
- What evidence would resolve it: Empirical studies evaluating robustness of pruned MoE models to adversarial attacks and experiments integrating pruning method with adversarial training techniques.

### Open Question 4
- Question: What are the limitations of the expert pruning method when applied to MoE models with a large number of experts, and how does the method scale with increasing model complexity?
- Basis in paper: [inferred] Paper demonstrates effectiveness on MoE models with up to 8 experts per encoder but does not explore scalability to models with significantly larger number of experts.
- Why unresolved: Scalability to larger and more complex MoE architectures is not addressed.
- What evidence would resolve it: Experimental results showing performance on MoE models with large number of experts along with analysis of computational complexity and scalability.

## Limitations
- Theoretical analysis relies on specific data model assumptions (i.i.d. Gaussian data) and simplified expert architectures that may not hold in practice
- Experiments limited to two specific MoE architectures (V-MoE and E3-MoE) and vision tasks, leaving generalizability to other domains uncertain
- Correlation between router norm changes and expert importance lacks extensive empirical validation across diverse tasks and architectures

## Confidence

**High Confidence:** The empirical effectiveness of the proposed pruning method in reducing model size and computational requirements while maintaining accuracy within 1% of unpruned models.

**Medium Confidence:** The theoretical proof that experts learning task-relevant features exhibit larger router norm changes than irrelevant experts under stated assumptions.

**Low Confidence:** The claim that the method guarantees generalization across a wide range of pruning ratios due to dependence on specific data model assumptions.

## Next Checks

1. **Cross-domain generalization test:** Apply the method to fine-tuned MoE language models (e.g., Mixtral) and evaluate whether router norm changes still effectively identify unimportant experts across different task types.

2. **Extreme pruning ratio validation:** Systematically test pruning ratios from 50% to 95% on CIFAR-100 and ImageNet to identify practical limits of the method and validate theoretical bounds.

3. **Alternative routing mechanism evaluation:** Test the method with expert-choice routing instead of token-choice routing to determine if correlation between router norm changes and expert importance holds across different MoE architectures.