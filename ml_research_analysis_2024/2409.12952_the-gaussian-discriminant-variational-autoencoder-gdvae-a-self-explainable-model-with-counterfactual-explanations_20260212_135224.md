---
ver: rpa2
title: 'The Gaussian Discriminant Variational Autoencoder (GdVAE): A Self-Explainable
  Model with Counterfactual Explanations'
arxiv_id: '2409.12952'
source_url: https://arxiv.org/abs/2409.12952
tags:
- classifier
- loss
- consistency
- training
- gdvae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Gaussian Discriminant Variational Autoencoder
  (GdVAE), a self-explainable model designed for visual counterfactual explanations.
  The GdVAE bridges the gap between transparent self-explainable models (SEMs) and
  high-quality counterfactual explanation methods.
---

# The Gaussian Discriminant Variational Autoencoder (GdVAE): A Self-Explainable Model with Counterfactual Explanations

## Quick Facts
- arXiv ID: 2409.12952
- Source URL: https://arxiv.org/abs/2409.12952
- Reference count: 40
- Introduces a self-explainable model that combines CVAE with GDA classifier for transparent counterfactual explanations

## Executive Summary
The GdVAE introduces a novel self-explainable model that bridges transparent classification with high-quality counterfactual generation. By combining a conditional variational autoencoder with a Gaussian discriminant analysis classifier, the model achieves both interpretability through class-specific prototypes and realistic counterfactual generation through closed-form solutions in the latent space. The method is evaluated on MNIST, CelebA, and CIFAR-10, demonstrating competitive performance in consistency, realism, and proximity compared to state-of-the-art approaches while also uncovering model biases through transparent prototypes.

## Method Summary
The GdVAE integrates a conditional variational autoencoder (CVAE) with a Gaussian discriminant analysis (GDA) classifier. The CVAE component learns class-conditional latent representations, while the GDA classifier uses class-specific prototypes for transparent classification. Counterfactual explanations are generated through linear traversal in the latent space, with two variants: local counterfactuals that minimize distance to the current instance, and global counterfactuals that move toward opposing class prototypes. The model is trained end-to-end using a joint objective that balances reconstruction, classification, and consistency objectives. A consistency loss ensures that generated counterfactuals remain coherent with the classifier's predictions when used as input.

## Key Results
- Achieves competitive performance on consistency (Pearson correlation), realism (FID), and proximity (MSE) metrics
- Demonstrates transparent classification through class-specific prototypes that reveal model biases
- Shows superior or comparable performance to state-of-the-art counterfactual methods on MNIST, CelebA, and CIFAR-10
- Effectively uncovers model biases through prototype visualization and analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The linear explainer function in latent space provides optimal counterfactual generation under the GdVAE's regularization.
- Mechanism: The classifier's linear discriminant f(z) = wᵀz + b induces a linear path for counterfactual generation. The optimal CF zδ minimizes the distance to the current latent point z while crossing the decision boundary, resulting in the closed-form solution zδ = z + κw with κ = (δ - wᵀz - b)/(wᵀw).
- Core assumption: The recognition model qϕ(z|x,y) closely approximates the true posterior pθ(z|y), making the latent space sufficiently regularized for linear traversal to remain within the data manifold.
- Evidence anchors:
  - [abstract] "full transparency is achieved with a generative classifier using class-specific prototypes and a closed-form solution for CFs in the latent space"
  - [section] "the optimal CF zδ minimizes the distance to the current instance z while ensuring the decision function matches the requested value δ"
  - [corpus] Weak - no direct corpus evidence about linear optimality in regularized latent spaces
- Break condition: If the assumption of qϕ(z|x,y) ≈ pθ(z|y) fails, the linear path may leave the data manifold, requiring non-linear methods.

### Mechanism 2
- Claim: The consistency loss ensures that counterfactuals remain coherent with the classifier's predictions when used as input.
- Mechanism: The consistency loss Lcon = E_p(δ)[KL(qϕ(z|xδ)||qϕ(zδ|x))] penalizes divergence between the latent distribution of the counterfactual input and the distribution obtained by encoding the counterfactual. This enforces that when a CF is used as input, the classifier produces the desired confidence value.
- Core assumption: The encoder and decoder act as approximate inverses, so encoding a decoded counterfactual should yield a similar latent representation.
- Evidence anchors:
  - [section] "we introduce a tailored consistency loss Lcon = E_p(δ)[KL(qϕ(z|xδ)||qϕ(zδ|x))] where the term addresses classification consistency for generated CF inputs"
  - [section] "To enforce this property, similar to [30,43,44], we introduce a tailored consistency loss"
  - [corpus] Weak - no direct corpus evidence about consistency loss effectiveness
- Break condition: If the encoder-decoder inversion assumption breaks down, the consistency loss cannot maintain coherence between CF generation and classifier predictions.

### Mechanism 3
- Claim: The joint training of M1 and M2 regularizes the latent space to encode class-specific attributes, enabling realistic image generation.
- Mechanism: M1 trains the autoencoder with class-conditional priors pθ(z|y), while M2 provides discriminative training for the classifier using pθ(y|z). The joint loss balances generative and discriminative objectives, creating a latent space where class-specific attributes are well-separated and smooth transitions between classes are possible.
- Core assumption: The balance between M1 and M2 losses (controlled by α and β) is properly tuned to achieve both good reconstruction and discriminative power.
- Evidence anchors:
  - [abstract] "Joint training of the classifier and generative model regularizes the latent space for class-specific attributes, enabling realistic image and CF generation"
  - [section] "α and β control the balance between M1 and M2" and "we obtain L = Lgd + γLcon"
  - [corpus] Weak - no direct corpus evidence about joint training effects on latent space regularization
- Break condition: If the balance between M1 and M2 is poorly tuned, the latent space may become either too focused on reconstruction (losing discriminative power) or too discriminative (losing reconstruction quality).

## Foundational Learning

- Concept: Variational Autoencoders and ELBO
  - Why needed here: The GdVAE is built on CVAE architecture, requiring understanding of evidence lower bound derivation and variational inference
  - Quick check question: What is the relationship between the ELBO and the true log-likelihood in variational inference?

- Concept: Gaussian Discriminant Analysis
  - Why needed here: The classifier uses GDA with class-specific prototypes, requiring knowledge of Mahalanobis distance and linear discriminant functions
  - Quick check question: How does assuming equal covariance matrices simplify the discriminant function in GDA?

- Concept: Counterfactual Explanations and Consistency Metrics
  - Why needed here: The paper evaluates CF quality using realism (FID), consistency (Pearson correlation), and proximity (MSE), requiring understanding of these metrics
  - Quick check question: What is the trade-off between consistency and realism in counterfactual generation?

## Architecture Onboarding

- Component map: Feature Detection & Reconstruction (encoder/decoder) -> Prior Encoder & Classifier (class-conditional latent distributions) -> Explanation (counterfactual generation in latent space)
- Critical path: During inference, input x → encoder qϕ(z|x,y) → classifier qϕ(y|x) → sample y* → compute z* → apply explainer function If(z*,δ) → decoder g(zδ) → counterfactual image xδ. The consistency loss ensures zδ produces desired classifier output when re-encoded.
- Design tradeoffs: The linear explainer function trades flexibility for transparency and closed-form solutions. The joint training of M1 and M2 balances reconstruction quality against discriminative power. The choice between local and global counterfactuals trades proximity against revealing model biases.
- Failure signatures: Poor counterfactual realism (high FID) suggests insufficient latent space regularization. Low consistency correlation indicates the explainer function doesn't align with classifier predictions. High reconstruction error suggests imbalance between M1 and M2 objectives.
- First 3 experiments:
  1. Train GdVAE on MNIST binary classification and verify that prototypes in latent space correspond to class means
  2. Generate local counterfactuals for a test image and verify that classifier confidence matches requested confidence
  3. Compare FID scores between local and global counterfactual methods to assess realism tradeoffs

## Open Questions the Paper Calls Out

- **Question**: How does the GdVAE's performance compare to state-of-the-art methods on multi-class classification tasks beyond binary classification?
  - Basis in paper: [explicit] The paper states "Our quantitative evaluation of CF methods is currently limited to binary classification problems, and future evaluations of multi-class problems are needed to advance CF literature."
  - Why unresolved: The experiments in the paper were limited to binary classification tasks, and the paper acknowledges the need for further evaluation on multi-class problems.
  - What evidence would resolve it: Conducting experiments on multi-class datasets like CIFAR-10 or ImageNet and comparing the GdVAE's performance to other state-of-the-art methods on metrics such as accuracy, FID, and correlation.

- **Question**: What is the impact of using a class-conditional encoder model q_φ(z|x, y) versus an unconditional model q_φ(z|x) on the GdVAE's performance and counterfactual quality?
  - Basis in paper: [explicit] The paper mentions "Our approach utilizes a class-conditional encoder model q_φ(z|x, y). Significantly reduced computational costs can be achieved by employing unconditional models, such as q_φ(z|x). In [12], a clustering solution is presented, directly leveraging unconditional encoder and decoder models. However, further analyses are needed to conclude the performance implications."
  - Why unresolved: The paper acknowledges the potential for reduced computational costs with unconditional models but does not provide a direct comparison of their performance.
  - What evidence would resolve it: Implementing and training the GdVAE with both conditional and unconditional encoder models and comparing their performance on metrics such as accuracy, FID, and correlation.

- **Question**: How does the GdVAE's ability to uncover model biases through its transparent prototypes compare to other methods that require quantitative analysis of counterfactuals on simulated datasets?
  - Basis in paper: [explicit] The paper states "Illustrated in Fig. 3c, the classifier's decision on smiling is shaped by female prototypes, revealing a potential bias or data imbalance not observed in our local CFs and other CF methods. The gender bias is exposed by evaluating the smile classifier across hidden attributes (Tab. 4), indicating reduced performance with increased uncertainty in males."
  - Why unresolved: While the paper demonstrates the GdVAE's ability to uncover biases through its prototypes, it does not provide a direct comparison to other methods that require quantitative analysis of counterfactuals.
  - What evidence would resolve it: Conducting experiments on datasets with known biases and comparing the GdVAE's ability to uncover these biases through its prototypes to other methods that rely on quantitative analysis of counterfactuals.

## Limitations
- The method relies on linear counterfactual generation in the latent space, which may not capture complex decision boundaries
- The joint training objective requires careful hyperparameter tuning to balance reconstruction and discriminative performance
- The approach is currently limited to binary classification tasks, with multi-class evaluation needed

## Confidence
- **High Confidence**: The theoretical framework of combining CVAE with GDA classifier is well-established and mathematically sound
- **Medium Confidence**: The empirical results show competitive performance, but the small sample sizes and limited dataset diversity reduce generalizability
- **Low Confidence**: The effectiveness of the consistency loss and the assumption that linear traversal remains within the data manifold are not thoroughly validated

## Next Checks
1. Test the GdVAE on a multi-class classification problem (e.g., CIFAR-100) to evaluate scalability beyond binary tasks
2. Compare counterfactual generation quality when using non-linear explainer functions versus the proposed linear approach
3. Perform ablation studies on the consistency loss to quantify its contribution to counterfactual quality