---
ver: rpa2
title: Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs
  for Open-ended Question Answering
arxiv_id: '2402.09911'
source_url: https://arxiv.org/abs/2402.09911
tags:
- knowledge
- questions
- graph
- lake
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method combining Pseudo-Graph Generation
  and Atomic Knowledge Verification (PG&AKV) to address hallucination in Large Language
  Models (LLMs), particularly for open-ended question answering. PG&AKV generates
  pseudo-graphs using LLM capabilities to identify needed knowledge and then verifies
  these graphs using atomic-level semantic querying across diverse Knowledge Graphs
  (KGs).
---

# Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering

## Quick Facts
- **arXiv ID**: 2402.09911
- **Source URL**: https://arxiv.org/abs/2402.09911
- **Reference count**: 30
- **Primary result**: Achieves 11.5 point increase in ROUGE-L score for open-ended questions and 7.5% improvement in accuracy for precise-answered questions

## Executive Summary
This paper introduces PG&AKV (Pseudo-Graph Generation and Atomic Knowledge Verification), a method to enhance Large Language Models for open-ended question answering by addressing hallucination through external knowledge incorporation. The approach leverages LLMs to generate structured knowledge frameworks (pseudo-graphs) when complete facts are unavailable, then verifies these graphs using atomic-level semantic querying across diverse Knowledge Graphs. The method demonstrates significant performance improvements and strong generalization across different KG sources, showing at least 3.5% improvement even with KG sources different from the question sources.

## Method Summary
PG&AKV operates in two phases: Pseudo-Graph Generation (PG) and Atomic Knowledge Verification (AKV). In PG, LLMs generate Cypher queries representing the knowledge structure needed to answer questions, which are executed on Neo4j to produce pseudo-triples. In AKV, these pseudo-triples are used to perform cosine similarity-based semantic querying on vectorized KGs, selecting the most similar triples at an atomic level independent of specific KG schemas. A two-step pruning method first selects top-k entities based on triple frequency, then applies semantic ranking using cosine similarity with a 0.7 confidence threshold. The verified knowledge is then used to generate final answers, effectively mitigating factual hallucinations while maintaining generalization across different knowledge sources.

## Key Results
- Minimum 11.5 point increase in ROUGE-L score for open-ended questions
- 7.5% improvement in accuracy for precise-answered questions
- At least 3.5% performance improvement when using KG sources different from question sources
- Strong generalization across different KG schemas (Wikidata and Freebase)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-Graph Generation leverages LLMs' language generation capabilities to create structured knowledge frameworks even when complete facts are unavailable
- Mechanism: LLMs are prompted to generate Cypher queries that represent the knowledge structure needed to answer a question, which are then executed on a Neo4j graph database to produce pseudo-triples
- Core assumption: LLMs can reliably generate syntactically correct Cypher queries that capture the essential knowledge structure for answering open-ended questions
- Evidence anchors: [abstract] "Enhancement of open-ended question-answering begins with leveraging the Pseudo-Graph Generation to provide the related knowledge framework"; [section] "Since LLMs are trained on large-scale natural corpora, they naturally tend to generate continuous language rather than the discrete, rule-bound format triples"

### Mechanism 2
- Claim: Atomic Knowledge Verification enables generalization across different KG sources through schema-independent semantic querying
- Mechanism: Generated pseudo-triples are used to perform cosine similarity-based semantic querying on vectorized KGs, selecting the most similar triples at an atomic level independent of any specific KG schema
- Core assumption: Semantic similarity in vector space can effectively bridge schema differences between different knowledge graphs
- Evidence anchors: [abstract] "Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources"; [section] "Because both the querying and verification operate at an atomic level independent of any specific KG schema, this approach generalizes well across different KGs"

### Mechanism 3
- Claim: Two-step pruning method overcomes limitations of LLM-based scoring while efficiently leveraging generated pseudo-graphs
- Mechanism: First selects top-k entities based on triple frequency, then applies semantic ranking using cosine similarity with a 0.7 confidence threshold to filter entities
- Core assumption: Entity frequency ranking combined with semantic similarity provides better entity selection than pure LLM-based scoring
- Evidence anchors: [section] "We propose a two-step pruning method that overcomes the limitations of relying solely on LLM judgments, as in the ToG method"; [section] "For each subject s ∈ St, we calculate an entity confidence score by averaging the cosine similarity scores of all triples in Gt that use s as the subject"

## Foundational Learning

- **Knowledge Graph (KG) structure and triple representation**
  - Why needed here: Understanding how knowledge is represented as subject-predicate-object triples is fundamental to both generating pseudo-graphs and performing atomic knowledge verification
  - Quick check question: What are the three components of a knowledge graph triple and how do they relate to each other?

- **Semantic similarity and vector embeddings**
  - Why needed here: The atomic knowledge verification mechanism relies on cosine similarity between vector representations of triples to find relevant knowledge across different KG sources
  - Quick check question: How does cosine similarity measure semantic relatedness between two pieces of text?

- **Prompt engineering and in-context learning**
  - Why needed here: The method relies heavily on carefully crafted prompts to guide LLMs through multiple steps (pseudo-graph generation, verification, answer generation)
  - Quick check question: What are the key differences between zero-shot, few-shot, and chain-of-thought prompting strategies?

## Architecture Onboarding

- **Component map**: LLM (GPT-3.5/GPT-4) → Pseudo-Graph Generation → Semantic Query Engine (Sentence-BERT) → Atomic Knowledge Verification → LLM Verification → Answer Generation
- **Critical path**: Question → Pseudo-Graph Generation → Semantic Query → Verification → Answer
- **Design tradeoffs**: Uses LLM generation capabilities instead of direct KG querying (more flexible but potentially less accurate); schema-independent approach (more generalizable but may miss schema-specific optimizations)
- **Failure signatures**: Low pseudo-graph generation accuracy; poor semantic matching results; verification step fails to improve upon pseudo-graph
- **First 3 experiments**:
  1. Test pseudo-graph generation accuracy on simple factual questions with known answers
  2. Validate semantic querying performance by comparing results from different KG sources on the same questions
  3. Measure improvement from verification step by comparing pseudo-graph vs. fixed graph performance on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PG&AKV compare when using different embedding models for semantic querying instead of Sentence-BERT?
- Basis in paper: [inferred] The paper mentions that "For future research, we plan to improve semantic querying by incorporating more advanced embedding models" and that Sentence-BERT was chosen for encoding semantic triples into Gbase for retrieval
- Why unresolved: The current implementation uses Sentence-BERT, but the paper explicitly suggests exploring more advanced embedding models as future work, indicating this comparison hasn't been conducted
- What evidence would resolve it: Comparative experiments showing performance metrics (accuracy, precision) when using alternative embedding models like GPT embeddings, RoBERTa, or domain-specific embeddings for the semantic querying component

### Open Question 2
- Question: What is the impact of different pruning strategies on the quality of acquired knowledge in the Atomic Knowledge Verification phase?
- Basis in paper: [explicit] The paper states "we aim to explore alternative pruning strategies to enhance the quality of acquired knowledge" and describes their current two-step pruning method (candidate selection and semantic ranking)
- Why unresolved: The current pruning method is described but not compared against other possible strategies, and the authors explicitly mention this as future work
- What evidence would resolve it: Systematic evaluation comparing the current pruning approach with alternatives (e.g., different similarity thresholds, machine learning-based pruning, or graph centrality-based methods) showing their effects on knowledge graph quality and downstream QA performance

### Open Question 3
- Question: How does the performance of PG&AKV vary across different knowledge graph schemas beyond Wikidata and Freebase?
- Basis in paper: [explicit] The paper demonstrates generalization across Wikidata and Freebase but notes that "some relations that are single-hop in Freebase require multi-hop reasoning in Wikidata," suggesting schema differences affect performance
- Why unresolved: While the paper shows some generalization capability, it doesn't comprehensively test across diverse KG schemas with varying structural properties, relation types, and entity representations
- What evidence would resolve it: Extensive testing across multiple KG sources with different schema designs (e.g., DBpedia, YAGO, domain-specific KGs) with detailed analysis of how schema characteristics impact PG&AKV performance

### Open Question 4
- Question: What is the optimal balance between using pseudo-graph generation versus direct knowledge graph querying for different types of questions?
- Basis in paper: [inferred] The ablation study shows that pseudo-graph generation stimulates model knowledge and atomic verification increases precision, but doesn't systematically explore when each approach is most beneficial
- Why unresolved: The current framework uses both components together, but the paper doesn't analyze under which conditions one approach might be preferable or how to dynamically select between them
- What evidence would resolve it: Controlled experiments comparing different combinations of pseudo-graph generation and direct KG querying across various question types (single-hop, multi-hop, open-ended) with analysis of computational efficiency and accuracy trade-offs

## Limitations

- Reliance on LLM-generated Cypher queries introduces uncertainty about query quality and coverage, particularly for complex questions requiring multi-hop reasoning
- Semantic similarity-based verification may struggle with polysemous entities or when KG schemas differ substantially in granularity
- Performance improvements are primarily demonstrated on factual questions, with less evidence for handling truly open-ended, subjective queries
- Two-step pruning method's thresholds (top-k selection and 0.7 cosine similarity cutoff) appear to be heuristic choices without systematic optimization

## Confidence

- **High confidence**: The core mechanism of using LLM-generated pseudo-graphs followed by semantic verification demonstrates clear improvements over baseline LLM performance (11.5 point ROUGE-L increase)
- **Medium confidence**: Generalization across different KG sources is demonstrated but relies heavily on the assumption that semantic similarity can bridge schema differences effectively
- **Low confidence**: The specific pruning thresholds and entity selection methodology lack empirical justification and may not generalize well to KG sources with different characteristics

## Next Checks

1. **Schema Robustness Test**: Evaluate performance when KG schemas differ not just in source but in fundamental structure (e.g., hierarchical vs. flat relationships) to validate the claimed schema-independent verification
2. **Query Generation Quality Analysis**: Measure the accuracy and syntactic validity of LLM-generated Cypher queries across different question types to quantify the reliability of the pseudo-graph generation mechanism
3. **Threshold Sensitivity Analysis**: Systematically vary the top-k entity selection and cosine similarity thresholds to identify optimal values and understand the sensitivity of performance to these hyperparameters