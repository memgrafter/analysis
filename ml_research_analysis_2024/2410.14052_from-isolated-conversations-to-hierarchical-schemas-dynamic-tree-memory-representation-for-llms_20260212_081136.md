---
ver: rpa2
title: 'From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation
  for LLMs'
arxiv_id: '2410.14052'
source_url: https://arxiv.org/abs/2410.14052
tags:
- memtree
- level
- memory
- information
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MemTree is a dynamic tree-structured memory algorithm for large
  language models that organizes information hierarchically with aggregated textual
  content and semantic embeddings at each node. It adaptively updates its structure
  by comparing new information's embedding to existing nodes, creating or expanding
  nodes based on depth-adaptive similarity thresholds.
---

# From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs

## Quick Facts
- arXiv ID: 2410.14052
- Source URL: https://arxiv.org/abs/2410.14052
- Authors: Alireza Rezazadeh; Zichao Li; Wei Wei; Yujia Bao
- Reference count: 40
- MemTree achieves 80.5% accuracy on MultiHop RAG benchmark, outperforming MemoryStream (74.7%) and approaching offline methods like RAPTOR (81.0%)

## Executive Summary
MemTree introduces a dynamic tree-structured memory algorithm for large language models that organizes information hierarchically with aggregated textual content and semantic embeddings at each node. The system adaptively updates its structure by comparing new information's embedding to existing nodes, creating or expanding nodes based on depth-adaptive similarity thresholds. MemTree demonstrates superior performance in long-term conversation management, achieving 80.5% accuracy on the MultiHop RAG benchmark and maintaining 82.5% accuracy in extended conversations with 200+ rounds, significantly outperforming baseline methods while supporting real-time updates.

## Method Summary
MemTree implements a hierarchical tree memory structure where each node contains textual content, semantic embeddings, parent/child relationships, and depth information. The algorithm updates memory by traversing from root, computing cosine similarity between new information and existing nodes, and inserting content based on depth-adaptive thresholds (θ(d) = θ0e^λd). Content aggregation at parent nodes uses LLM-based summarization in parallelized fashion. Retrieval employs a collapsed tree approach with top-k selection based on cosine similarity to query embeddings. The system was evaluated on MultiHop RAG (609 documents, 2,556 questions), Multi-Session Chat Extended (70 sessions, 200+ rounds), and QuALITY datasets using GPT-4o for aggregation and text-embedding-3-large for embeddings.

## Key Results
- Achieves 80.5% accuracy on MultiHop RAG benchmark, outperforming MemoryStream (74.7%) and approaching offline method RAPTOR (81.0%)
- Maintains 82.5% accuracy in extended conversations with 200+ rounds, compared to MemoryStream's 80.7%
- Demonstrates effective long-term memory management while supporting real-time updates through hierarchical tree structure

## Why This Works (Mechanism)
MemTree works by organizing information in a hierarchical tree structure where each node represents a cluster of semantically similar content. The depth-adaptive similarity thresholds allow the tree to dynamically adjust its granularity - closer to root nodes use broader similarity thresholds while leaf nodes use finer thresholds. This creates a multi-resolution representation where coarse-grained information is available near the top for quick retrieval while detailed information is preserved at lower levels. The LLM-based content aggregation at parent nodes enables the system to maintain coherent summaries of subtree contents, facilitating efficient retrieval without losing contextual information.

## Foundational Learning
- **Tree data structures**: Hierarchical organization of nodes with parent-child relationships - needed for implementing the memory structure; quick check: verify node depth and parent/child links
- **Cosine similarity computation**: Measuring semantic similarity between embeddings - needed for comparing new information with existing nodes; quick check: validate similarity scores between known similar/dissimilar pairs
- **Depth-adaptive thresholds**: Dynamic similarity thresholds based on node depth - needed for controlling tree granularity; quick check: test threshold behavior at different depths
- **LLM-based content aggregation**: Using language models to summarize and aggregate content - needed for maintaining coherent node representations; quick check: verify aggregated content preserves key information
- **Embedding models**: Converting text to semantic vectors - needed for similarity computation; quick check: ensure embeddings capture semantic relationships
- **Top-k retrieval**: Selecting most relevant items from a set - needed for collapsed tree retrieval; quick check: test retrieval accuracy with varying k values

## Architecture Onboarding

**Component Map**: Text input -> Embedding model -> Tree traversal -> Similarity computation -> Node insertion/update -> LLM aggregation -> Collapsed retrieval -> Final output

**Critical Path**: During memory updates, the critical path is: embedding computation → tree traversal → similarity comparison → node insertion decision → LLM aggregation (parallel at parent nodes)

**Design Tradeoffs**: 
- Memory vs. accuracy: Deeper trees provide better granularity but increase computational overhead
- Update speed vs. aggregation quality: Parallel LLM aggregation speeds updates but may reduce summary quality
- Threshold parameters affect tree structure: Higher thresholds create shallower trees (faster but less detailed), lower thresholds create deeper trees (slower but more granular)

**Failure Signatures**:
- Tree becomes too shallow: Few nodes at deeper levels, many leaf nodes at shallow depth
- Tree becomes too deep: Excessive node creation, poor aggregation quality
- Retrieval misses relevant information: Low accuracy on inference queries despite high traversal accuracy
- Slow updates: High latency during memory update operations

**First Experiments**:
1. Test tree structure quality by visualizing node depth distribution and similarity thresholds
2. Validate retrieval performance by comparing collapsed retrieval vs traversal retrieval on a small dataset
3. Measure update efficiency by timing memory updates with varying numbers of new information items

## Open Questions the Paper Calls Out

**Open Question 1**: What is the theoretical upper bound on MemTree's performance relative to offline methods like RAPTOR when scaling to datasets with 10,000+ documents? The paper only tests on datasets up to 609 documents, leaving scalability limits unknown. Empirical testing on larger datasets would resolve this.

**Open Question 2**: How does MemTree's performance degrade when using smaller embedding models (e.g., 384-dim vs 1536-dim) compared to text-embedding-3-large? The ablation only tests one smaller model; performance across varying dimensions remains unexplored. Systematic testing with different embedding dimensions would resolve this.

**Open Question 3**: What is the impact of MemTree's depth-adaptive threshold parameters (θ0 and λ) on retrieval accuracy for queries requiring cross-subtree reasoning? The ablation tests parameter effects but doesn't specifically analyze cross-subtree query performance. Targeted experiments on cross-subtree queries would resolve this.

**Open Question 4**: How does MemTree's memory update efficiency scale when new information arrives at high velocity (e.g., 100+ updates per second) compared to MemoryStream? The paper evaluates efficiency on static datasets without testing high-velocity update scenarios. Benchmarking under simulated high-velocity conditions would resolve this.

## Limitations
- Performance gains come with increased computational overhead during memory updates compared to simpler methods
- Method relies heavily on quality of embedding model and LLM for content aggregation, making it sensitive to model variations
- Still lags behind some offline methods, suggesting potential limitations in handling extremely complex queries
- Specific parameter choices (θ0, λ) and prompt templates for LLM operations are not fully specified, affecting reproducibility

## Confidence

**High**: Claims about MemTree's performance advantages on MultiHop RAG and extended conversations, the core algorithmic approach of hierarchical tree updates with depth-adaptive thresholds, and the general methodology of using LLM-based content aggregation.

**Medium**: Claims about reduced performance gap with offline methods and specific accuracy numbers in different scenarios, as these may depend on implementation details not fully specified.

**Low**: Claims about generalizability to domains beyond the tested datasets, as the evaluation is limited to specific benchmark datasets.

## Next Checks

1. Verify the depth-adaptive threshold parameters (θ0, λ) through ablation studies to determine their impact on tree structure quality and retrieval performance
2. Test MemTree's performance with different embedding models and LLMs to assess robustness to model variations
3. Implement a controlled comparison measuring computational overhead during memory updates to quantify the trade-off between performance gains and resource requirements