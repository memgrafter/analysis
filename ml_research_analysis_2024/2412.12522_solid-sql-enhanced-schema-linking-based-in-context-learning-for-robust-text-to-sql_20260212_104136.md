---
ver: rpa2
title: 'Solid-SQL: Enhanced Schema-linking based In-context Learning for Robust Text-to-SQL'
arxiv_id: '2412.12522'
source_url: https://arxiv.org/abs/2412.12522
tags:
- text-to-sql
- schema
- solid-sql
- llms
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Solid-SQL, a text-to-SQL system that addresses
  robustness issues in existing LLM-based approaches when facing adversarial perturbations.
  The authors identify that pre-processing is crucial for robustness, and propose
  a pipeline that includes a robust schema-linking model, a two-round example retrieval
  strategy, and an explicit attention mechanism.
---

# Solid-SQL: Enhanced Schema-linking based In-context Learning for Robust Text-to-SQL

## Quick Facts
- arXiv ID: 2412.12522
- Source URL: https://arxiv.org/abs/2412.12522
- Reference count: 5
- Solid-SQL achieves state-of-the-art SQL execution accuracy of 82.1% on Spider and 58.9% on Bird benchmarks

## Executive Summary
Solid-SQL is a text-to-SQL system designed to enhance the robustness of LLM-based approaches when facing adversarial perturbations. The authors identify that pre-processing is crucial for robustness and propose a pipeline that includes a robust schema-linking model, a two-round example retrieval strategy, and an explicit attention mechanism. Solid-SQL achieves state-of-the-art SQL execution accuracy of 82.1% on Spider and 58.9% on Bird benchmarks, demonstrating an average improvement of 11.6% over baselines on perturbed benchmarks. The approach is shown to be effective across various LLMs, highlighting its versatility and potential for enhancing the reliability of text-to-SQL systems in real-world applications.

## Method Summary
Solid-SQL addresses the robustness limitations of LLM-based text-to-SQL systems, particularly their vulnerability to adversarial perturbations. The method employs a pre-processing pipeline consisting of robust schema linking via LLM-based data augmentation, a two-round example retrieval strategy based on structural similarity, and an explicit attention mechanism to stabilize prompt generation. The system uses text-to-SQL datasets like Spider, Bird, Spider-Syn, Spider-Realistic, and Dr. Spider, and leverages augmented datasets with synonym substitutions and structural changes to improve schema linking model robustness. The primary metric is execution accuracy (EX), which measures whether generated SQL queries produce the same results as ground truth, with exact match (EM) accuracy also used for additional evaluation.

## Key Results
- Solid-SQL achieves state-of-the-art SQL execution accuracy of 82.1% on Spider and 58.9% on Bird benchmarks
- Demonstrates an average improvement of 11.6% over baselines on perturbed benchmarks
- Shows effectiveness across various LLMs, highlighting its versatility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Robust schema-linking through LLM-based data augmentation improves resilience to synonym and structure perturbations.
- Mechanism: Perturb training data by generating synonym replacements and structural variants using an LLM, then fine-tune a schema linking model on this augmented dataset. This exposes the model to adversarial variations it must handle in deployment.
- Core assumption: The LLM can generate semantically equivalent but structurally varied questions that cover realistic adversarial scenarios.
- Evidence anchors:
  - [abstract] "We focus on the pre-processing stage, training a robust schema-linking model enhanced by LLM-based data augmentation."
  - [section 3.3.1] "In order to improve the robustness of our schema linking model, we first expanded the original text-to-SQL training dataset. The data in the training set is in the form of a triplet Q, SC, S... We use an LLM to rewrite Q, including changing the sentence structure and replacing synonyms... resulting in new questions Q1 and Q2."
  - [corpus] Weak evidence: Related papers focus on schema linking but do not explicitly mention LLM-based data augmentation for robustness.
- Break condition: If the LLM fails to generate realistic perturbations, or if augmented data distribution diverges too far from real adversarial inputs, robustness gains will not materialize.

### Mechanism 2
- Claim: Two-round in-context learning with structural similarity-based example retrieval stabilizes SQL generation under perturbations.
- Mechanism: First round: Retrieve examples by matching question skeleton similarity; generate preliminary SQL. Second round: Retrieve examples by matching SQL skeleton similarity to the preliminary SQL; refine final output. This multi-stage retrieval adapts to both question and SQL-level perturbations.
- Core assumption: Structural similarity in skeletons correlates with functional similarity in SQL generation, and round two refinement can correct round one errors.
- Evidence anchors:
  - [abstract] "Additionally, we design a two-round, structural similarity-based example retrieval strategy for in-context learning."
  - [section 3.4.1] "To guide the LLM towards generating the desired SQL, it is reasonable to select analogous questions from the candidate set as examples, based on the target question Q."
  - [section 3.4.2] "Direct matching can also be achieved through the similarity of SQL statements... employing the edit distance derived from the parse tree of an SQL skeleton."
- Break condition: If skeleton extraction fails to capture critical domain-specific information, or if edit-distance similarity does not align with functional SQL similarity, retrieval may pull irrelevant examples.

### Mechanism 3
- Claim: Explicit attention mechanism in prompt construction mitigates schema omission failures during SQL generation.
- Mechanism: Instead of silently filtering irrelevant schema, the prompt explicitly emphasizes ("focus on") schema elements likely to be needed. This gives the LLM a full view while directing focus, reducing failure when key columns are omitted by the schema linker.
- Core assumption: LLMs can leverage full schema context even when attention is explicitly guided, and explicit focus cues do not disrupt general reasoning.
- Evidence anchors:
  - [section 3.5] "We use 'focus on' to emphasize schema elements that are more likely to form the final SQL statement to the LLM. Thus the model still has a comprehensive view of the entire schema information while understanding the priority."
  - [section 4.3.2] Ablation shows explicit attention improves exact match accuracy, especially with fewer examples.
- Break condition: If the LLM over-focuses on highlighted schema and ignores contextual reasoning, or if attention cues are misinterpreted, SQL generation may degrade.

## Foundational Learning

- Concept: Schema linking in text-to-SQL
  - Why needed here: Correct schema linking is essential for selecting relevant tables/columns; failures directly cause wrong SQL.
  - Quick check question: Given a question "Find customers from New York", which schema elements (tables, columns) should be linked?
- Concept: In-context learning with few-shot examples
  - Why needed here: LLMs rely on retrieved examples in prompt to guide SQL generation; quality and relevance of examples determine performance.
  - Quick check question: How does adding a relevant (question, SQL) pair to the prompt influence the LLM's generated SQL?
- Concept: Adversarial robustness and perturbation handling
  - Why needed here: Text-to-SQL systems must maintain accuracy when questions or schemas are slightly altered (synonyms, structure changes).
  - Quick check question: If "singer" is replaced with "musician", will the schema linking model still identify the correct columns?

## Architecture Onboarding

- Component map:
  - Schema linking fine-tuning → retrieves relevant schema subset
  - Question skeleton extraction → filters domain/value info
  - Example retrieval (Q-similarity) → first round examples
  - LLM SQL generation (round 1) → preliminary SQL
  - SQL skeleton extraction → parse preliminary SQL
  - Example retrieval (SQL-similarity) → second round examples
  - LLM SQL generation (round 2) → final SQL
  - Explicit attention in prompt → guides schema focus
- Critical path:
  1. Input question + full schema → schema linking model
  2. Schema + question → question skeleton extraction
  3. Skeleton → retrieve first-round examples
  4. Prompt (question, schema, examples, explicit focus) → LLM round 1 → SQL1
  5. SQL1 → SQL skeleton extraction
  6. SQL skeleton → retrieve second-round examples
  7. Prompt (SQL1, schema, examples, explicit focus) → LLM round 2 → final SQL
- Design tradeoffs:
  - Schema linking vs. including full schema: precision vs. risk of missing key elements
  - Number of examples in prompt: richer context vs. token limits
  - Skeleton extraction granularity: better similarity matching vs. potential loss of critical info
- Failure signatures:
  - Low column-selection accuracy in schema linking → SQL references missing tables/columns
  - Irrelevant examples retrieved → LLM generates SQL mismatched to question
  - Explicit attention poorly calibrated → LLM ignores needed schema elements or over-focuses
- First 3 experiments:
  1. Evaluate schema linking accuracy on synonym-perturbed questions with and without data augmentation.
  2. Test in-context learning retrieval quality by varying similarity thresholds and measuring example relevance.
  3. Measure impact of explicit attention prompts on exact match accuracy with reduced example counts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Solid-SQL's robustness generalize to completely unseen domains beyond those represented in the Spider and Bird benchmarks?
- Basis in paper: [explicit] The authors state that Solid-SQL "demonstrates its versatility and potential for enhancing the reliability of text-to-SQL systems in real-world applications," but the evaluation is limited to Spider and Bird datasets.
- Why unresolved: The paper does not provide evidence of performance on entirely new domains outside the training and evaluation sets.
- What evidence would resolve it: Testing Solid-SQL on diverse, real-world databases from various industries (e.g., healthcare, finance, retail) and comparing its performance to existing methods would demonstrate its true generalization capabilities.

### Open Question 2
- Question: What is the impact of Solid-SQL's two-round example retrieval strategy on computational efficiency and latency in production environments?
- Basis in paper: [explicit] The authors describe a "two-round, structural similarity-based example retrieval strategy for in-context learning," but do not discuss the computational cost or time implications of this approach.
- Why unresolved: While the strategy may improve accuracy, its effect on processing speed and resource utilization is not addressed, which is crucial for real-world deployment.
- What evidence would resolve it: Benchmarking Solid-SQL's inference time and computational requirements compared to single-round approaches across various dataset sizes would quantify the trade-off between accuracy and efficiency.

### Open Question 3
- Question: What is the impact of Solid-SQL's two-round example retrieval strategy on computational efficiency and latency in production environments?
- Basis in paper: [explicit] The authors describe a "two-round, structural similarity-based example retrieval strategy for in-context learning," but do not discuss the computational cost or time implications of this approach.
- Why unresolved: While the strategy may improve accuracy, its effect on processing speed and resource utilization is not addressed, which is crucial for real-world deployment.
- What evidence would resolve it: Benchmarking Solid-SQL's inference time and computational requirements compared to single-round approaches across various dataset sizes would quantify the trade-off between accuracy and efficiency.

### Open Question 4
- Question: What is the optimal balance between the number of examples retrieved and the prompt size for different types of queries (simple vs. complex)?
- Basis in paper: [explicit] The authors conduct a study on the number of examples (N=7 optimal), but do not analyze how this optimal number varies based on query complexity.
- Why unresolved: The study provides a general optimal number, but does not account for the varying nature of text-to-SQL queries, which can range from simple single-table selections to complex multi-table joins.
- What evidence would resolve it: Conducting a detailed analysis of Solid-SQL's performance across queries of varying complexity (e.g., categorizing queries by number of tables involved) with different numbers of examples would reveal if a dynamic approach to example selection is needed.

## Limitations
- The explicit attention mechanism's prompt integration is underspecified, making faithful reproduction difficult
- The SQL skeleton extraction method and its edit-distance computation are referenced but not detailed
- Robustness claims rely heavily on synthetic perturbations, which may not fully represent real-world adversarial scenarios

## Confidence

- **High Confidence:** The two-round example retrieval mechanism is well-documented and grounded in established similarity-matching principles
- **Medium Confidence:** The schema-linking robustness claims are supported by ablation studies, but the exact impact of LLM-based data augmentation depends on the quality and coverage of generated perturbations
- **Low Confidence:** The explicit attention mechanism's effectiveness is demonstrated via ablation, but the lack of implementation details makes it unclear how much of the gain comes from the mechanism itself

## Next Checks

1. **Schema Linking Robustness Test:** Evaluate the fine-tuned schema linking model on a held-out subset of Spider-Syn and Spider-Realistic to measure accuracy degradation under synonym and structural perturbations. This isolates the impact of data augmentation.
2. **Retrieval Relevance Analysis:** Perform a qualitative review of top-5 retrieved examples in both retrieval rounds across 50 random questions from Spider. Label whether examples are functionally relevant to the query to validate skeleton-based matching.
3. **Attention Mechanism Ablation:** Systematically remove the "focus on" attention cues from prompts while keeping all other components constant, then measure exact match accuracy drop across varying example counts to quantify the explicit attention contribution.