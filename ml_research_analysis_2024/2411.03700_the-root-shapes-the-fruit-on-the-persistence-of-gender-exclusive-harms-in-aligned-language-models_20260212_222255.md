---
ver: rpa2
title: 'The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in
  Aligned Language Models'
arxiv_id: '2411.03700'
source_url: https://arxiv.org/abs/2411.03700
tags:
- bias
- gender
- language
- tgnb
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how alignment techniques like Direct Preference
  Optimization (DPO) can amplify gender-diverse biases in large language models. The
  authors evaluate 16 models across pretraining, supervised fine-tuning (SFT), and
  DPO stages using TGNB-specific benchmarks and a framework for analyzing implicit
  reward signals.
---

# The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models

## Quick Facts
- arXiv ID: 2411.03700
- Source URL: https://arxiv.org/abs/2411.03700
- Reference count: 40
- Models evaluated: 16 models across pretraining, SFT, and DPO stages

## Executive Summary
This paper investigates how alignment techniques like Direct Preference Optimization (DPO) can amplify gender-diverse biases in large language models. The authors evaluate 16 models across pretraining, supervised fine-tuning (SFT), and DPO stages using TGNB-specific benchmarks and a framework for analyzing implicit reward signals. They find that DPO can exacerbate stigmatization and gender non-affirmative language, especially when initialized from biased SFT models. In output evaluations, models showed increased negative regard toward TGNB identities after DPO, with shifts toward narratives of social rejection, hardship, and sexualization. Reward signal analysis revealed models systematically selecting more stigmatizing texts for TGNB groups, with bias often correlating to reference model behavior.

## Method Summary
The study evaluates 16 publicly available models from Llama and Pythia families across three alignment stages: pretraining, supervised fine-tuning, and DPO. TGNB bias is measured using the tango dataset with 90,000 prompts covering 100 names, 50 gender identities, and 18 disclosure forms. Generations are classified for regard (positive, negative, neutral) and undergo thematic analysis. Reward signals are analyzed using WinoQueer templates to simulate preference datasets and compute log ratios between final policy and reference model selections. DPO is implemented with β=0.1 using English preference data from hh-rlhf, oasst1, and shp datasets.

## Key Results
- DPO alignment amplified TGNB stigmatization when initialized from biased SFT models
- Negative regard toward TGNB identities increased after DPO compared to base models
- Reward signal analysis showed systematic selection of more stigmatizing texts for TGNB groups
- Bias amplification patterns varied with model size and architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO alignment can amplify existing gender-diverse biases when initialized from biased SFT models
- Mechanism: SFT models learn to generate outputs reflecting harmful stereotypes from preference data, and DPO preserves these biases while amplifying them due to the KL penalty encouraging divergence from reference model behavior
- Core assumption: Preference data contains harmful content that SFT models learn to reproduce without comparison to rejected examples
- Evidence anchors:
  - [abstract] "DPO can exacerbate stigmatization and gender non-affirmative language, especially when initialized from biased SFT models"
  - [section 4.2.1] "DPO's ability to reduce these disparities seemed to critically depend on reference model choice: when using respective base models as reference, disparities decreased significantly"
  - [corpus] Weak evidence - related work shows alignment can amplify bias but doesn't specifically address SFT→DPO bias amplification

### Mechanism 2
- Claim: Implicit reward signals in DPO encode and perpetuate gender-diverse biases from reference models
- Mechanism: Reward signals derived from BT preference modeling capture the model's relative preference between paired texts, preserving biases when the reference model shows preference for stigmatizing content
- Core assumption: Reward signals can be extracted and analyzed using paired evaluation datasets designed for masked language models
- Evidence anchors:
  - [abstract] "We propose a flexible framework to uncover bias patterns in these signals"
  - [section 5.3.2] "Llama models showed a non-trivial preservation of reference model TGNB biases under DPO alone (Llama 7B: 0.19, Llama 13B: 0.21)"
  - [corpus] Moderate evidence - related work on reward model bias detection supports this mechanism but lacks specific TGNB focus

### Mechanism 3
- Claim: Size and architecture differences affect bias amplification patterns in aligned models
- Mechanism: Larger models and different architectural choices influence how biases are encoded and amplified during alignment, with some architectures showing increased bias with scale while others show decreased bias
- Core assumption: Model architecture and scale interact with alignment procedures to produce different bias outcomes
- Evidence anchors:
  - [section 5.3.1] "Interestingly, the level and direction of amplification varied with size and model architecture"
  - [section 5.3.1] "Biases in Pythia amplified with size under DPO and SFT+DPO, respectively. Llama models, however, show more mixed outcomes"
  - [corpus] Weak evidence - corpus contains related work on model size effects but lacks specific TGNB bias analysis

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Understanding DPO mechanics is crucial for analyzing how biases propagate through alignment stages
  - Quick check question: How does DPO differ from RLHF in terms of reward signal handling and what implications does this have for bias analysis?

- Concept: Bradley-Terry preference modeling
  - Why needed here: This preference framework underlies both the alignment process and the reward signal analysis methodology
  - Quick check question: How can paired evaluation datasets be repurposed to simulate preference data for bias analysis in aligned models?

- Concept: Thematic analysis methodology
  - Why needed here: Understanding inductive thematic analysis is essential for interpreting narrative shifts in model outputs and identifying specific forms of harmful content
  - Quick check question: What are the key differences between deductive and inductive thematic analysis and why was inductive approach chosen for this study?

## Architecture Onboarding

- Component map: Base pretraining -> SFT fine-tuning -> DPO alignment -> Output evaluation + Reward signal analysis
- Critical path: Model selection -> Bias evaluation across alignment stages -> Reward signal extraction -> Thematic analysis of outputs
- Design tradeoffs: Community-informed bias evaluations vs. dominant social norm benchmarks; transparency in alignment practices vs. proprietary methods
- Failure signatures: Increased negative regard disparity between TGNB and binary gender groups; systematic selection of stigmatizing texts for TGNB groups; correlation between reference model biases and aligned model preferences
- First 3 experiments:
  1. Replicate regard disparity analysis across additional model families to test generalizability
  2. Conduct controlled experiments varying reference model selection to isolate SFT→DPO bias amplification effects
  3. Implement and test explicit fairness constraints at different alignment stages to measure mitigation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the amplification of TGNB biases differ between DPO and RLHF alignment methods?
- Basis in paper: [explicit] The paper mentions RLHF requires a separate reward model, while DPO does not, and suggests future work should explore this difference.
- Why unresolved: The study only evaluates DPO-aligned models, leaving a gap in understanding how other alignment methods might behave differently with regard to bias amplification.
- What evidence would resolve it: Comparative experiments applying both DPO and RLHF to the same base models and measuring bias amplification across TGNB-specific benchmarks and reward signals.

### Open Question 2
- Question: What specific aspects of the preference data contribute most to TGNB bias amplification during SFT and DPO?
- Basis in paper: [explicit] The paper identifies TGNB-related terms in preference datasets but notes the need for future work to disentangle effects of preference data from existing biases.
- Why unresolved: The study only qualitatively analyzes preference datasets without systematically isolating which data characteristics drive bias amplification.
- What evidence would resolve it: Controlled experiments varying TGNB content, context, and framing in preference datasets while measuring downstream bias amplification effects.

### Open Question 3
- Question: How do model size and architecture interact with bias amplification across different alignment stages?
- Basis in paper: [explicit] The paper observes mixed results across model sizes (2.8B, 6.9B, 7B, 13B) but cannot fully explain the patterns.
- Why unresolved: The study identifies size/architecture effects but lacks mechanistic understanding of why certain configurations amplify or mitigate bias.
- What evidence would resolve it: Systematic scaling experiments tracking bias amplification patterns across model sizes and architectures, coupled with analysis of how architectural differences affect reward signal processing.

## Limitations

- Analysis restricted to Llama and Pythia model families, limiting generalizability to other architectures
- Thematic analysis of generated narratives introduces interpretive subjectivity in identifying harmful content patterns
- Focus on TGNB identities may not capture how bias amplification manifests for other marginalized groups

## Confidence

- High confidence: The empirical finding that DPO can amplify existing biases when initialized from biased SFT models is strongly supported by the correlation data between reference model preferences and final policy selections
- Medium confidence: The narrative shift analysis showing increased negative regard toward TGNB identities after DPO is well-documented but relies on the thematic coding framework
- Medium confidence: The claim that size and architecture differences affect bias amplification patterns is supported by the mixed results across Llama and Pythia families, though the underlying mechanisms remain incompletely explained

## Next Checks

1. Cross-architecture validation: Test the bias amplification hypothesis across additional model families (e.g., Mistral, Gemma) and alignment approaches (RLHF, constitutional AI) to determine if the SFT→DPO bias amplification pattern is architecture-specific or more general
2. Reference model intervention: Conduct controlled experiments systematically varying reference model selection (base vs SFT vs debiased models) to quantify the exact contribution of SFT bias to final DPO outcomes and test the proposed break condition
3. Reward signal debiasing: Implement explicit fairness constraints or bias mitigation techniques at the reward signal level and measure their effectiveness in reducing TGNB stigmatization while maintaining overall alignment quality