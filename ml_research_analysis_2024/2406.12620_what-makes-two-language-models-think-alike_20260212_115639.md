---
ver: rpa2
title: What Makes Two Language Models Think Alike?
arxiv_id: '2406.12620'
source_url: https://arxiv.org/abs/2406.12620
tags:
- neural
- similarity
- features
- layers
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach using Metric-Learning Encoding
  Models (MLEMs) to compare how different language models represent linguistic information.
  The authors apply MLEMs to analyze feature importance profiles across layers of
  BERT, GPT-2, and Mamba, identifying which linguistic features (e.g., part-of-speech,
  grammatical number) most influence neural representations.
---

# What Makes Two Language Models Think Alike?

## Quick Facts
- **arXiv ID:** 2406.12620
- **Source URL:** https://arxiv.org/abs/2406.12620
- **Reference count:** 10
- **One-line primary result:** MLEMs identify which linguistic features drive representational similarity across different language model architectures.

## Executive Summary
This paper introduces Metric-Learning Encoding Models (MLEMs) as a novel approach for comparing how different language models represent linguistic information. The authors apply MLEMs to analyze feature importance profiles across layers of BERT, GPT-2, and Mamba, identifying which linguistic features most influence neural representations. They then compute feature-based similarity matrices to quantify how similarly pairs of model layers process language, contrasting this with traditional feature-agnostic approaches like Representational Similarity Analysis (RSA).

## Method Summary
The method uses MLEMs to learn distance functions in feature space that align with observed neural distances among word representations. For each layer of a language model, MLEMs identify which linguistic features (part-of-speech, grammatical number, etc.) predict neural distances. Feature importance is computed by measuring the decrease in Spearman correlation when permuting each feature. Similarity between layers is quantified by weighted Kendall correlation on feature importance profiles, and compared against traditional RSA using Spearman correlation on dissimilarity matrices.

## Key Results
- MLEM feature importance profiles successfully predict neural distances (Spearman correlation typically >0.7) across BERT, GPT-2, and Mamba layers
- Feature-based similarity matrices reveal different block structures compared to feature-agnostic RSA, with Mamba showing increased word position sensitivity in later layers
- The feature-based approach provides transparent, theory-driven insights about specific linguistic feature contributions to representational similarity

## Why This Works (Mechanism)

### Mechanism 1
MLEMs identify which linguistic features most strongly predict neural distances among word representations by optimizing a distance function in feature space to align with observed neural distances. Feature importance is revealed through how much the correlation score drops when a feature is permuted. This works because neural distances among representations reflect underlying linguistic similarity that can be modeled by explicit symbolic features.

### Mechanism 2
Feature-based similarity matrices capture shared representational strategies across layers better than feature-agnostic RSA by correlating feature-importance profiles using weighted Kendall correlation. This quantifies to what extent the same linguistic features drive neural distances in both layers, indicating similar computational strategies when dominant features align.

### Mechanism 3
The sudden increase in word-position feature importance in Mamba's later layers explains the block structure in its similarity matrix because MLEMs detect that positional encoding becomes the dominant factor for neural distances in higher layers of Mamba, leading to high within-block similarity and low cross-block similarity.

## Foundational Learning

- **Concept: Metric learning and bi-linear forms for distance modeling**
  - Why needed here: MLEMs use bi-linear forms to compute feature distances that align with neural distances
  - Quick check question: What is the role of the symmetric positive definite matrix W in MLEMs?

- **Concept: Feature importance estimation via permutation**
  - Why needed here: Feature importance is quantified by how much Spearman correlation drops when permuting each feature
  - Quick check question: How does permuting a feature help estimate its importance in predicting neural distances?

- **Concept: Kendall correlation with weighted features**
  - Why needed here: Feature-based similarity uses weighted Kendall correlation to emphasize dominant features
  - Quick check question: Why use a weighted version of Kendall correlation instead of raw correlation for comparing feature importance profiles?

## Architecture Onboarding

- **Component map:** Controlled grammar-based sentence generator -> feature extraction -> model forward pass for each layer -> compute neural distances -> train MLEMs -> extract feature importance -> compute similarity matrix -> interpret via MDS
- **Critical path:** Generate probing data -> compute neural distances per layer -> train MLEMs -> extract feature importance -> compute similarity matrix -> interpret via MDS
- **Design tradeoffs:**
  - Diagonal W assumption simplifies training but ignores feature interactions; using full W increases expressiveness but risks overfitting
  - Spearman correlation focuses on rank preservation but ignores absolute scale differences; Pearson would capture scale but be more sensitive to outliers
  - Controlled grammar ensures clean feature effects but limits linguistic diversity; natural data would increase realism but introduce confounding factors
- **Failure signatures:**
  - Low Spearman correlation across all models indicates poor fit of MLEMs to neural distances
  - Feature importance concentrated on one feature suggests dataset bias or model over-reliance on that feature
  - Similarity matrix shows no block structure when expected (e.g., Mamba layers) suggests positional encoding differences are not captured
- **First 3 experiments:**
  1. Train MLEMs on a single layer of BERT and validate Spearman correlation on held-out data
  2. Compare feature importance profiles across early vs late layers within BERT to check for expected trends (e.g., PoS dominance)
  3. Compute feature-based vs feature-agnostic similarity matrices for BERT-BERT pairs to confirm high agreement (baseline sanity check)

## Open Questions the Paper Calls Out

### Open Question 1
How do linguistic feature interactions affect model similarity beyond the diagonal assumption? The paper notes that their current approach assumes no interactions among linguistic features when computing feature importance, but acknowledges this is a limitation. Introducing interactions would be straightforward within the MLEM framework but was not implemented in this study for simplicity.

### Open Question 2
Does the feature-based similarity approach generalize to other model architectures beyond Transformers and Mamba? The authors state the approach "can straightforwardly be extended to other domains, such as speech and vision" and "to other neural systems, including human brains." However, the paper only applies the method to three specific architectures (BERT, GPT-2, Mamba), leaving open whether it works for other model types.

### Open Question 3
How does the choice of linguistic features affect the feature-based similarity results? The authors note they used "only included features that we consider essential to the list of words in the dataset" and suggest future work could explore "more exhaustive lists of features." The paper uses a controlled dataset with a specific set of linguistic features but doesn't test sensitivity to feature selection or compare results with alternative feature sets.

## Limitations
- The controlled grammar dataset may not adequately capture the full complexity of linguistic phenomena that drive neural representations in real-world usage
- The method assumes that linguistic features alone can explain neural distances, potentially overlooking non-linguistic factors like frequency or surface form effects
- The diagonal W assumption in MLEMs may oversimplify feature interactions, limiting the model's ability to capture complex feature dependencies

## Confidence
- **High confidence** in the general framework of using MLEMs for feature-based comparison of neural models
- **Medium confidence** in the specific feature importance rankings, as these depend heavily on dataset design and feature selection
- **Medium confidence** in the interpretation of Mamba's positional encoding findings, given limited corpus evidence for this specific architectural difference

## Next Checks
1. **Cross-dataset validation**: Apply the same MLEM analysis to a natural language corpus (e.g., GLUE or SuperGLUE) to verify that feature importance profiles remain stable outside the controlled grammar environment.

2. **Feature interaction analysis**: Train MLEMs with full (non-diagonal) W matrices to quantify the importance of feature interactions versus individual features, comparing results with the diagonal case to assess the validity of the independence assumption.

3. **Architectural ablation study**: Conduct a controlled experiment where Transformer positional encodings are modified (e.g., absolute to relative, or removed) to determine if observed differences in feature importance between Mamba and Transformers are primarily due to positional encoding differences.