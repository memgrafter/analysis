---
ver: rpa2
title: On the Worst Prompt Performance of Large Language Models
arxiv_id: '2406.10248'
source_url: https://arxiv.org/abs/2406.10248
tags:
- performance
- prompt
- prompts
- worst
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the sensitivity of large language models (LLMs)
  to prompt phrasing by introducing a new benchmark, ROBUST ALPACA EVAL, which consists
  of semantically equivalent case-level queries. The study evaluates the robustness
  of ChatGPT and six open-source LLMs (Llama, Mistral, and Gemma families) by measuring
  the worst-case performance, revealing substantial variability; for instance, Llama-2-70B-chat
  exhibits a 45.48% difference between worst and best performance, with the worst
  as low as 9.38%.
---

# On the Worst Prompt Performance of Large Language Models

## Quick Facts
- arXiv ID: 2406.10248
- Source URL: https://arxiv.org/abs/2406.10248
- Reference count: 12
- This paper introduces ROBUST ALPACA EVAL to measure worst-case performance of LLMs on semantically equivalent prompts.

## Executive Summary
This paper addresses the sensitivity of large language models (LLMs) to prompt phrasing by introducing a new benchmark, ROBUST ALPACA EVAL, which consists of semantically equivalent case-level queries. The study evaluates the robustness of ChatGPT and six open-source LLMs (Llama, Mistral, and Gemma families) by measuring the worst-case performance, revealing substantial variability; for instance, Llama-2-70B-chat exhibits a 45.48% difference between worst and best performance, with the worst as low as 9.38%. The paper also explores the difficulty of identifying worst prompts, showing that such prompts are often model-specific and not predictable through model-agnostic features like perplexity or hidden states. Existing methods to improve worst-case performance, such as self-refinement and distillation, are found to be limited, while a voting-based approach shows promise in enhancing stability. These findings highlight the need for more resilient LLMs.

## Method Summary
The authors created ROBUST ALPACA EVAL by combining TinyAlpacaEval with 10 paraphrased versions of each query, generated using GPT-4 and manually reviewed. They evaluated eight LLMs (ChatGPT, Llama-2-7B/13B/70B-chat, Mistral-7B-instruct, and Gemma-2B/7B) by computing weighted win-rates against GPT-4 Turbo. Performance metrics included worst-case, best-case, average, and standard deviation. They also analyzed model-agnostic features (perplexity, Min-K% probability) and model-dependent features (hidden states via PCA) to predict worst prompts. Finally, they tested self-refinement, voting-based generation, and swarm distillation to improve worst-case performance.

## Key Results
- LLMs show large performance gaps between worst and best semantically equivalent prompts (e.g., 45.48% difference for Llama-2-70B-chat).
- Worst prompts are highly model-specific, with low overlap across different models.
- Model-agnostic features like perplexity and hidden states do not reliably predict worst-case performance.
- Voting-based methods improve worst-case performance more than self-refinement or distillation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model performance on semantically equivalent prompts varies drastically, revealing sensitivity to phrasing.
- Mechanism: The same task described with different but equivalent wording triggers different internal representations, leading to performance drops as large as 45.48% between worst and best.
- Core assumption: The model's internal state is highly sensitive to surface form, not just semantics.
- Evidence anchors:
  - [abstract] "substantial variability in model performance; for instance, a difference of 45.48% between the worst and best performance for the Llama-2-70B-chat model"
  - [section 3.2] "For instance, the worst and best performance of Llama-2-70B-chat are 0.094 and 0.549, respectively, indicating a difference of 0.455"
  - [corpus] Weak: no direct corpus evidence; relies on internal model behavior not documented in literature
- Break condition: If a future model's representations are more robust to surface-level changes, the gap would shrink.

### Mechanism 2
- Claim: Worst prompts are model-specific and cannot be predicted using model-agnostic features.
- Mechanism: Each model has a unique internal mapping from prompt surface features to performance; the same prompt can be bad for one model but fine for another.
- Core assumption: There is no universal "bad" phrasing that applies across models.
- Evidence anchors:
  - [section 4.1] "Overlap rate of the worst-k prompts for Llama/Gemma family models is noticeably higher than that for all models" and "when k = 1 the metric for Llama family models is only 2% (13% for Gemma)"
  - [section 4.1] "The rankings within the Llama and Gemma family models exhibit only moderate consistency"
  - [corpus] Weak: no corpus study comparing cross-model worst prompts
- Break condition: If future models converge in architecture or training data, worst prompts might become more shared.

### Mechanism 3
- Claim: Model-dependent features like perplexity or hidden states do not reliably predict prompt quality.
- Mechanism: Internal representations and probability scores are not linearly correlated with downstream task performance.
- Core assumption: The model's perception of prompt "difficulty" or "familiarity" does not align with its actual output quality.
- Evidence anchors:
  - [section 4.2] "Pearson correlation coefficients between perplexity and performance across all cases" shows ~45% weak or no correlation
  - [section 4.2] "Hidden states provide limited insight into prompt quality" and probing accuracy near chance (0.5)
  - [section 4.2] "The accuracy of the Llama family models and ChatGPT hovers around 50%" when models try to pick their own better prompt
  - [corpus] Weak: no corpus evidence on hidden state predictability
- Break condition: If future probing methods become more sophisticated, hidden states might yield better predictions.

## Foundational Learning

- Concept: Prompt sensitivity and surface form effects
  - Why needed here: The entire study hinges on understanding that LLMs do not treat semantically equivalent prompts identically.
  - Quick check question: If two prompts have the same meaning but different wording, can you predict which will perform better without running the model?

- Concept: Model-specific performance variance
  - Why needed here: To understand why robustness improvements cannot rely on generic prompt heuristics.
  - Quick check question: If a phrasing is the worst for Model A, is it guaranteed to be bad for Model B?

- Concept: Hidden state analysis and probing
  - Why needed here: To evaluate whether internal representations can serve as a shortcut to predict performance.
  - Quick check question: If hidden states are visualized with PCA and show no clear separation between good and bad prompts, what does that imply about using them for prediction?

## Architecture Onboarding

- Component map:
  - Data layer: ROBUST ALPACA EVAL benchmark (TinyAlpacaEval + 10 paraphrases per query)
  - Evaluation layer: Weighted win-rate against GPT-4 Turbo as evaluator
  - Analysis layer: Correlation studies (perplexity, hidden states, Min-K% Prob), overlap metrics, Kendall's W for ranking consistency
  - Improvement layer: Self-refinement, voting-based generation, swarm distillation for consistency
- Critical path:
  1. Generate paraphrases → 2. Run all prompts on target models → 3. Compute worst/best/average/std metrics → 4. Analyze model-agnostic vs model-dependent predictors → 5. Test improvement methods
- Design tradeoffs:
  - Using GPT-4 Turbo as evaluator introduces a reference bias but provides a consistent baseline.
  - Model-agnostic features (perplexity, Min-K% Prob) are easy to compute but unreliable; model-dependent features (hidden states) are more expensive and still ineffective.
  - Voting-based improvement is robust but computationally expensive (requires generating outputs for all paraphrases).
- Failure signatures:
  - If overlap of worst prompts across models increases significantly, the model-specific assumption breaks.
  - If hidden states start cleanly separating good/bad prompts, the probing assumption breaks.
  - If prompt engineering improves worst performance without hurting average, the self-refinement assumption breaks.
- First 3 experiments:
  1. Run a small subset of prompts on two different models and measure overlap of worst prompts.
  2. Compute perplexity for all paraphrases and correlate with actual performance.
  3. Visualize hidden states via PCA for one model and one case to check for performance clustering.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can worst prompts be reliably identified using a combination of model-agnostic and model-dependent features?
- Basis in paper: [explicit] The paper states that worst prompts are model-specific and not predictable through model-agnostic features like perplexity or hidden states, but also notes the difficulty in using model-dependent features.
- Why unresolved: The study finds that neither model-agnostic nor model-dependent features alone are sufficient to predict worst prompts, but it does not explore the potential of combining these features or using alternative approaches.
- What evidence would resolve it: Developing and testing a hybrid model that integrates both model-agnostic and model-dependent features to predict worst prompts, and evaluating its performance against existing methods.

### Open Question 2
- Question: How do different paraphrasing techniques affect the identification of worst prompts?
- Basis in paper: [inferred] The paper mentions that paraphrases are created using GPT4 and manually reviewed, but does not investigate how different paraphrasing techniques might influence the robustness of models to prompt variations.
- Why unresolved: The study focuses on a specific set of paraphrases but does not explore the impact of varying paraphrasing techniques on the identification of worst prompts.
- What evidence would resolve it: Conducting experiments with multiple paraphrasing techniques and analyzing their effects on model performance and the identification of worst prompts.

### Open Question 3
- Question: Are there specific linguistic features or prompt structures that consistently lead to worst performance across models?
- Basis in paper: [explicit] The paper states that model-agnostic traits are absent and model-dependent features are inadequate, but it does not explore specific linguistic features or prompt structures.
- Why unresolved: The study concludes that worst prompts are model-specific and difficult to predict, but it does not investigate whether certain linguistic features or structures are more likely to cause poor performance.
- What evidence would resolve it: Analyzing the linguistic features and structures of worst-performing prompts across models to identify any consistent patterns or characteristics.

## Limitations

- The benchmark relies on synthetic paraphrases, which may not fully capture real-world prompt variability.
- GPT-4 Turbo is used as an evaluator, introducing potential bias and limiting generalizability.
- The study does not explore the impact of different paraphrasing techniques on worst-prompt identification.
- Hidden state analysis is limited to specific probing methods that may not capture all relevant information.

## Confidence

**High Confidence:**
- The existence of substantial performance gaps between worst and best prompts for individual models
- The observation that self-refinement and distillation provide limited improvement to worst-case performance
- The finding that voting-based methods improve stability

**Medium Confidence:**
- The claim that worst prompts are model-specific (based on overlap analysis, but sample size and prompt diversity may limit generalizability)
- The assertion that model-agnostic features cannot predict worst prompts (though correlation evidence is present, the probing method may not be exhaustive)

**Low Confidence:**
- The generalizability of the benchmark results to all real-world prompt distributions
- The completeness of hidden state analysis for predicting prompt quality (only specific probing methods were tested)

## Next Checks

1. **Cross-Model Worst Prompt Consistency Test**: Generate a larger set of paraphrases for each case and measure worst-prompt overlap across all models, including newer architectures. Verify if the low overlap rate persists with increased prompt diversity and model scale.

2. **Hidden State Probing Extension**: Apply more sophisticated probing techniques (e.g., linear classifiers, attention-based probes) to hidden states to determine if better predictive power emerges. Compare results against the simple linear probing used in the paper.

3. **Real-World Prompt Validation**: Collect a set of real-world user prompts that failed across multiple models and test them against the ROBUST ALPACA EVAL framework. Measure whether the synthetic benchmark correlates with actual deployment failures.