---
ver: rpa2
title: Scaling Granite Code Models to 128K Context
arxiv_id: '2407.13739'
source_url: https://arxiv.org/abs/2407.13739
tags:
- code
- context
- long-context
- long
- granite
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces long-context Granite code models (3B and 8B)
  that support effective context windows of up to 128K tokens. The authors address
  the challenge of extending context length for open-source code language models by
  employing a light-weight continual pretraining approach, gradually increasing RoPE
  base frequency with repository-level file packing and length-upsampled long-context
  data.
---

# Scaling Granite Code Models to 128K Context

## Quick Facts
- arXiv ID: 2407.13739
- Source URL: https://arxiv.org/abs/2407.13739
- Reference count: 5
- One-line primary result: 3B/8B Granite code models achieve effective 128K context without significant performance degradation on short-context benchmarks

## Executive Summary
This paper presents a lightweight approach to extend Granite code models from 2K/4K to 128K context length using progressive RoPE base frequency scaling and repository-level file packing. The authors introduce instruction-tuned models derived from further fine-tuning long-context base models on mixed short and long-context instruction data. The approach achieves significant improvements on long-context tasks while maintaining strong performance on standard code completion benchmarks, outperforming original short-context models on tasks like Long Code Completion and RepoQA.

## Method Summary
The approach involves two main phases: light-weight continual pretraining and instruction tuning. During pretraining, the authors progressively scale the RoPE base frequency from 8K to 128K tokens while using repository-level file packing with semantic dependency analysis. This involves packing files from the same repository, analyzing import dependencies to create a directed acyclic graph, and topologically sorting files for training. For instruction tuning, they mix permissively licensed short-context data with synthetically generated long-context instruction-response pairs created by bootstrapping the pretraining data using the original Granite-8B-Code-Instruct model.

## Key Results
- Models achieve effective 128K context without significant degradation on short-context benchmarks
- Exact Match scores on Long Code Completion, RepoBench-P, and RepoQA exceed original short-context models
- Pass@1 performance on HumanEvalPack remains competitive while showing improved long-context capabilities
- Key retrieval accuracy on synthetic benchmarks demonstrates effective long-context understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graduating RoPE base frequency allows the model to handle longer sequences without full retraining.
- Mechanism: RoPE encodes positional information using sine and cosine functions with a base frequency. By gradually increasing the base frequency (e.g., from 100K to 10M), the model learns to represent positions up to 128K tokens. This avoids the need for sparse or linear attention mechanisms.
- Core assumption: The model's ability to process positional information is retained and adaptable through small, incremental changes in RoPE base frequency.
- Evidence anchors:
  - [abstract]: "Our solution for scaling context length of Granite code models from 2K/4K to 128K consists of a light-weight continual pretraining by gradually increasing its RoPE base frequency"
  - [section 2.1]: "We adjust the RoPE base frequency, introduced in Xiong et al. (2023), to support long context windows up to 128K where the base model itself is trained on 2K/4K context length. For training, we adopt a progressive approach where we doubled the context window until it reached the desired length of 128K."
  - [corpus]: Weak. Neighboring papers discuss long-context training but do not directly reference RoPE base frequency scaling in code models.

### Mechanism 2
- Claim: Repository-level file packing with semantic dependency analysis improves long-context understanding.
- Mechanism: Files from the same repository are packed together, prioritizing semantic dependencies (identified via import analysis). This creates a directed acyclic graph, topologically sorted, and organized to reflect real-world code structure. The model learns to attend to related code across files.
- Core assumption: Real-world code dependencies are better captured by organizing files by repository and semantic relationship rather than random or sequential packing.
- Evidence anchors:
  - [section 2.1]: "To create long-context data, we develop a new approach that packs files from the same repository together, arranging them to prioritize semantic dependencies... We identify these dependencies by analyzing file imports and create a directed acyclic graph..."
  - [abstract]: "Our solution for scaling context length of Granite 3B/8B code models from 2K/4K to 128K consists of a light-weight continual pretraining by gradually increasing its RoPE base frequency with repository-level file packing and length-upsampled long-context data."
  - [corpus]: Weak. While long-context data engineering is mentioned in neighboring papers, the specific repository-level file packing with dependency analysis is not discussed.

### Mechanism 3
- Claim: Synthetic long-context instruction data generation bootstraps model capability without relying on existing long-context models.
- Mechanism: The original Granite-8B-Code-Instruct model generates multi-turn instruction-response pairs from repository-level file-packed documents. These synthetic instructions target long-context tasks like generation, retrieval, and translation, exposing the model to realistic long-context scenarios.
- Core assumption: A short-context model can generate useful long-context instruction data when given structured, semantically organized input.
- Evidence anchors:
  - [section 2.2]: "The long context instruction data was synthetically generated by bootstrapping the pretraining data... The responses were either parsed semantically from the original document or generated using Granite-8b-Code-Instruct-4K."
  - [abstract]: "Additionally, we also release instruction-tuned models with long-context support which are derived by further finetuning the long context base models on a mix of permissively licensed short and long-context instruction-response pairs."
  - [corpus]: Weak. Synthetic data generation for long-context tasks is not explicitly covered in neighboring papers.

## Foundational Learning

- Concept: Rotary Position Embedding (RoPE)
  - Why needed here: RoPE is the core mechanism for encoding positional information in transformers, and its base frequency must be scaled to support 128K tokens.
  - Quick check question: What happens to the positional encoding resolution when the RoPE base frequency is increased?

- Concept: Topological sorting and dependency graphs
  - Why needed here: Organizing code files by repository and semantic dependencies ensures the model learns to attend to related code across files, mimicking real-world development.
  - Quick check question: How does a directed acyclic graph (DAG) help in organizing code files for training?

- Concept: Instruction tuning with multi-turn data
  - Why needed here: Fine-tuning on a mix of short and long-context instruction-response pairs teaches the model to handle both scenarios without degrading short-context performance.
  - Quick check question: Why is a multi-turn loss mask used during instruction tuning with long-context data?

## Architecture Onboarding

- Component map:
  Data pipeline -> Model architecture -> Training loop -> Instruction tuning
  Repository-level file packing -> Granite 3B/8B base models with RoPE -> Progressive RoPE scaling -> Mixed data instruction tuning

- Critical path:
  1. Curate repository-level code corpus with per-language context length upsampling.
  2. Pack files by repository, analyze dependencies, create DAG, topologically sort.
  3. Continue pretrain Granite base models with progressive RoPE base frequency scaling.
  4. Generate synthetic long-context instruction data using Granite-8B-Code-Instruct-4K.
  5. Instruction tune long-context base models on mix of short and long-context data.

- Design tradeoffs:
  - RoPE base frequency scaling vs. sparse/linear attention: Simpler implementation, but may have limits on context length.
  - Synthetic data generation vs. real long-context instruction data: Avoids dependency on other long-context models, but quality depends on bootstrapping model.
  - Progressive RoPE scaling vs. single-step scaling: More stable training, but longer overall training time.

- Failure signatures:
  - Long-context performance degrades while short-context performance improves: Possible overfitting to short-context data during instruction tuning.
  - Model fails to converge during RoPE scaling: Base frequency increase too aggressive or data not representative of long-context scenarios.
  - Retrieval accuracy drops at high similarity thresholds: Packing or dependency analysis not capturing true semantic relationships.

- First 3 experiments:
  1. Test RoPE base frequency scaling: Continue pretrain a small model (e.g., 125M) with progressive RoPE scaling on synthetic long-context data; measure loss and position encoding stability.
  2. Validate repository packing: Create a small repository dataset, apply packing and dependency analysis; manually verify the organization matches semantic dependencies.
  3. Assess synthetic data quality: Generate a small set of synthetic long-context instructions using Granite-8B-Code-Instruct-4K; evaluate the relevance and diversity of the generated instructions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum context length that could be achieved with further RoPE base frequency adjustments and what would be the computational cost?
- Basis in paper: [explicit] The paper mentions gradually increasing RoPE base frequency up to 128K tokens, with specific values found for 8K, 16K, 32K, 64K, and 128K contexts
- Why unresolved: The paper stops at 128K tokens without exploring the theoretical limits of RoPE scaling or the computational complexity implications of further increases
- What evidence would resolve it: Experiments showing performance and computational costs at context lengths beyond 128K (e.g., 256K, 512K) with different RoPE theta values

### Open Question 2
- Question: How does the long-context performance vary across different programming languages beyond the five tested (Python, C++, Java, TypeScript, Rust)?
- Basis in paper: [explicit] The paper evaluates performance on five specific programming languages but mentions pretraining on seven languages including C, Go, and JavaScript
- Why unresolved: The paper only provides detailed results for five languages, leaving uncertainty about performance on other languages in the pretraining corpus
- What evidence would resolve it: Comprehensive evaluation results across all seven pretraining languages with the same metrics used for the primary five languages

### Open Question 3
- Question: What is the optimal balance between short and long-context data during instruction tuning to maximize both short and long-context performance?
- Basis in paper: [explicit] The paper mentions using a mix of short and long-context instruction data but doesn't explore different mixing ratios
- Why unresolved: The paper uses a fixed combination of short and long-context data without testing alternative ratios or methods for determining the optimal balance
- What evidence would resolve it: Comparative results showing performance across different short-to-long context data ratios during instruction tuning

## Limitations

- The quality and accuracy of semantic dependency analysis for repository-level file packing is not validated
- Synthetic long-context instruction data generation depends entirely on the quality of the bootstrapping model
- Progressive RoPE scaling may have inherent limits on how far context length can be extended without architectural changes

## Confidence

- High confidence: Models achieve effective 128K context without significant degradation on short-context benchmarks
- Medium confidence: Repository-level file packing with dependency analysis improves long-context understanding
- Medium confidence: Synthetic instruction data generation bootstraps long-context capability without external models

## Next Checks

1. **Dependency graph validation**: Manually examine 10-15 randomly selected repositories to verify that the topological sorting and file packing correctly reflects semantic dependencies, checking whether import-based dependency analysis captures actual code relationships.

2. **Synthetic data quality assessment**: Generate 50 synthetic long-context instruction-response pairs and have human annotators rate their relevance, coherence, and difficulty. Compare these ratings against real long-context instruction data if available.

3. **Position encoding stability test**: During RoPE base frequency scaling, monitor the variance of positional embeddings at different frequency values (100K, 1M, 10M) to ensure the encoding remains stable and doesn't produce overlapping or degenerate representations at 128K tokens.