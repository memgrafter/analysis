---
ver: rpa2
title: 'vFusedSeg3D: 3rd Place Solution for 2024 Waymo Open Dataset Challenge in Semantic
  Segmentation'
arxiv_id: '2408.15254'
source_url: https://arxiv.org/abs/2408.15254
tags:
- lidar
- feature
- fusion
- features
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces VFusedSeg3D, a multi-modal fusion architecture
  combining camera images and LiDAR point clouds for 3D semantic segmentation. The
  system leverages DLA34 for image feature extraction and Point Transform v3 (PTv3)
  for LiDAR processing, with a modified fusion module that integrates geometric and
  semantic features across modalities.
---

# vFusedSeg3D: 3rd Place Solution for 2024 Waymo Open Dataset Challenge in Semantic Segmentation

## Quick Facts
- arXiv ID: 2408.15254
- Source URL: https://arxiv.org/abs/2408.15254
- Authors: Osama Amjad; Ammad Nadeem
- Reference count: 3
- Primary result: 3rd place in 2024 Waymo Open Dataset Challenge with 72.46% mIoU

## Executive Summary
vFusedSeg3D is a multi-modal fusion architecture for 3D semantic segmentation that combines camera images and LiDAR point clouds. The system achieves 72.46% mIoU on the Waymo validation set, securing 3rd place in the 2024 Waymo Open Dataset Challenge. Due to computational constraints, the model was trained sequentially: LiDAR features first (45 epochs, 70.51% mIoU), then image features, and finally the fusion model (25 epochs). The architecture leverages DLA34 for image feature extraction and Point Transform v3 (PTv3) for LiDAR processing, with a modified fusion module that integrates geometric and semantic features across modalities.

## Method Summary
The vFusedSeg3D architecture combines camera images and LiDAR point clouds through a sequential training approach. DLA34 with Deep Layer Aggregation upsampling serves as the image backbone, while PTv3 processes LiDAR point clouds with 0.1 grid sampling resolution. A modified MSeg3D-based fusion module integrates features using Geometry-based Feature Fusion Module (GFFM), Semantic Feature Aggregation Module (SFAM) for both modalities, and Semantic Feature Fusion Module (SFFM). The system was trained on the Waymo Open Dataset with 5 RGB camera images (960x640) and 64-beam LiDAR point clouds, using AdamW optimizer with cosine scheduler and cross-entropy plus Lovasz loss functions.

## Key Results
- Achieved 72.46% mIoU on Waymo validation set
- LiDAR-only baseline: 70.51% mIoU after 45 epochs
- Sequential training approach successfully addressed memory constraints
- Test-time augmentations on LiDAR point clouds provided additional performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential training of individual modalities before fusion reduces GPU memory pressure and improves convergence
- Mechanism: Training LiDAR and image encoders separately allows each to reach reasonable mIoU (70.51% for LiDAR) without OOM errors, then freezing them before fusion training. This staged approach prevents catastrophic interference during joint optimization
- Core assumption: Separate training of unimodal encoders is sufficient for effective multimodal fusion later
- Evidence anchors:
  - "trained sequentially: LiDAR features were trained separately for 45 epochs achieving 70.51% mIoU, followed by image features, and finally the fusion model for 25 epochs"
  - "Due to computational resource constraints, we trained the model sequentially and in segments... This staged training approach not only addressed our resource limitations but also ensured that the integration layer... received the dedicated attention needed for effective learning."
  - Weak evidence - no directly comparable sequential training strategies in neighbors

### Mechanism 2
- Claim: The modified fusion module (MSeg3D-based with GFFM and SFAM) effectively integrates geometric and semantic features across modalities
- Mechanism: Point-wise LiDAR features and point-wise camera features are concatenated and processed through Geometry-based Feature Fusion Module (GFFM), then aggregated using Semantic Feature Aggregation Module (SFAM) for both modalities, finally combined in Semantic Feature Fusion Module (SFFM) with additional layers for optimal semantic cue integration
- Core assumption: Point-wise fusion is superior to voxel-based fusion for this dataset and architecture
- Evidence anchors:
  - "our novel feature fusion technique combines geometric features from LiDAR point clouds with semantic features from camera images"
  - "Instead of voxel Features as used by MSeg3d, we utilized point-wise lidar features to be aggregated using LIDAR SFAM... Semantic Feature Fusion Module (SFFM), which has been significantly improved, with new layers added to optimise the integration of semantic cues across modalities."
  - Weak evidence - no detailed description of fusion modules in neighbors

### Mechanism 3
- Claim: Test-time augmentations on LiDAR point clouds provide performance gains without additional training overhead
- Mechanism: Random scaling, flipping, rotation, and translation augmentations are applied to LiDAR point clouds during inference to improve robustness and mIoU without requiring retraining
- Core assumption: TTA on LiDAR provides more benefit than TTA on images for this task
- Evidence anchors:
  - "Test-time augmentations on LiDAR point clouds further improved performance"
  - "We only utilized Test Time Augmentations on lidar point clouds to boost our accuracies as mentioned on Table 3"
  - Weak evidence - no discussion of TTA strategies in neighbors

## Foundational Learning

- Concept: Multi-modal feature fusion principles
  - Why needed here: The architecture combines camera semantic features with LiDAR geometric features through custom fusion modules
  - Quick check question: What are the key differences between early, mid, and late fusion strategies in multi-modal learning?

- Concept: 3D point cloud processing and voxelization
  - Why needed here: The LiDAR side uses PTv3 for point cloud feature extraction with specific grid sampling and serialization methods
  - Quick check question: How does grid sampling resolution affect memory usage and accuracy in point cloud processing?

- Concept: Backbone architectures (DLA34, PTv3)
  - Why needed here: The system uses DLA34 for images and PTv3 for LiDAR, each with specific design choices and limitations
  - Quick check question: What are the trade-offs between hierarchical feature aggregation (DLA) and transformer-based point cloud processing (PTv3)?

## Architecture Onboarding

- Component map:
  LiDAR → PTv3 → Feature maps → GFFM → SFAM → SFFM → Output
  Image → DLA34 → Feature maps → GFFM → SFAM → SFFM → Output

- Critical path: Image → DLA34 → Feature maps → Fusion → Output
  LiDAR → PTv3 → Feature maps → Fusion → Output

- Design tradeoffs:
  - Sequential training vs. joint training (memory vs. potential performance)
  - Point-wise vs. voxel fusion (flexibility vs. computational efficiency)
  - Single GPU vs. multi-GPU training (accessibility vs. higher accuracy)

- Failure signatures:
  - Degradation in mIoU when switching from sequential to joint training
  - Memory OOM errors with higher grid sampling resolution
  - Fusion module outputs that don't improve upon individual modality performance

- First 3 experiments:
  1. Train LiDAR encoder alone for 45 epochs with 0.1 grid sampling, verify 70.51% mIoU target
  2. Train image encoder alone for 10 epochs, verify reasonable feature extraction
  3. Freeze both encoders and train fusion model for 25 epochs, verify improvement to 72.46% mIoU

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would increasing the grid sampling resolution beyond 0.1 affect the model's mIoU performance, and what is the theoretical upper limit before computational constraints become prohibitive?
- Basis in paper: The paper notes that increasing the grid resolution beyond 0.1 resulted in Out of Memory (OOM) errors, constraining their ability to achieve higher accuracy reported in the original PTv3 paper
- Why unresolved: The authors were limited by computational resources and could not experimentally determine the relationship between grid resolution and performance gains
- What evidence would resolve it: Systematic experimentation with different grid sampling resolutions (0.05, 0.075, 0.1, 0.125, 0.15) while monitoring both mIoU performance and memory usage would establish the optimal resolution and its performance impact

### Open Question 2
- Question: Would implementing mini-batching for gradient accumulation during the fusion model training phase improve the final mIoU score, and by what magnitude?
- Basis in paper: The authors mention that "We did not use mini-batching for gradient accumulation, although it is a good idea, but due to time limitation for more experimentation, we trained our models on 2 batch size."
- Why unresolved: The authors explicitly acknowledged this as a potential improvement but did not have time to test it due to resource constraints
- What evidence would resolve it: Training the fusion model with gradient accumulation strategies (accumulating gradients over multiple forward passes before updating weights) while keeping the effective batch size constant, then comparing the resulting mIoU to the current 72.46%

### Open Question 3
- Question: How would extending the fusion model training beyond 25 epochs impact the final mIoU performance, and is there a point of diminishing returns?
- Basis in paper: The sequential training approach allocated only 25 epochs to the fusion model after separately training the LiDAR and image components, suggesting potential for further optimization
- Why unresolved: The authors prioritized sequential training due to resource constraints but did not explore whether additional training time for the fusion component would yield significant improvements
- What evidence would resolve it: Extending fusion model training to 50, 75, and 100 epochs while monitoring validation mIoU to identify the optimal training duration and detect any overfitting patterns

## Limitations

- Lack of detailed architectural specifications for modified fusion modules (GFFM, SFAM, SFFM) makes exact reproduction challenging
- Sequential training approach may not represent the optimal solution despite addressing memory constraints
- Claim of being "the best solution to date" not fully supported by comprehensive literature comparison, only competition ranking

## Confidence

- High confidence: The sequential training methodology and its rationale (memory constraints, staged optimization) - directly supported by training logs and competition constraints
- Medium confidence: The fusion module architecture improvements - partially described but lacks implementation details for exact reproduction
- Medium confidence: The claim of being "the best solution to date" - supported by competition results but not comprehensive literature comparison

## Next Checks

1. Implement the sequential training pipeline and verify that individual modality encoders can achieve the reported baseline performances (70.51% mIoU for LiDAR, reasonable image features) before fusion

2. Conduct ablation studies comparing point-wise fusion against voxel-based fusion to validate the architectural choice and fusion module effectiveness

3. Test alternative training strategies (joint training with gradient accumulation, mixed precision training) to determine if sequential training is truly necessary or optimal for this architecture