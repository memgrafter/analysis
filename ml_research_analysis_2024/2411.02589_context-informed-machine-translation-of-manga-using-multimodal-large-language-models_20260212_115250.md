---
ver: rpa2
title: Context-Informed Machine Translation of Manga using Multimodal Large Language
  Models
arxiv_id: '2411.02589'
source_url: https://arxiv.org/abs/2411.02589
tags:
- translation
- manga
- page
- text
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of multimodal large language models
  (LLMs) for automatic manga translation, addressing the challenge of incorporating
  visual context into translation to resolve ambiguities. The authors propose several
  translation approaches leveraging visual context, translation unit size, and context
  length, including a novel token-efficient method.
---

# Context-Informed Machine Translation of Manga using Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2411.02589
- Source URL: https://arxiv.org/abs/2411.02589
- Authors: Philip Lippmann; Konrad Skublicki; Joshua Tanner; Shonosuke Ishiwatari; Jie Yang
- Reference count: 17
- This paper introduces multimodal approaches for manga translation that achieve state-of-the-art performance for Japanese-English and set new standards for Japanese-Polish translation

## Executive Summary
This paper addresses the challenge of translating manga by incorporating visual context into the translation process to resolve ambiguities that are only visually apparent. The authors propose several translation approaches that leverage visual context, translation unit size, and context length, including a novel token-efficient method that removes text from images before processing. They introduce a new Japanese-Polish manga translation dataset and evaluate their methods using both automated metrics and human evaluation, achieving significant improvements over text-only baselines.

## Method Summary
The authors evaluate multiple LLM-based translation approaches using GPT-4 Turbo, including line-by-line (LBL), page-by-page (PBP), and volume-based translation with and without visual context. They introduce a novel token-efficient approach (PBP-VIS-NUM) that removes text from images before sending them to the model, addressing OCR limitations. The evaluation uses both automated metrics (ChrF, BERTScore, BLEURT, xCOMET) and human evaluation (MQM), testing on Japanese-English and Japanese-Polish translation tasks using their newly created dataset and existing OpenMantra dataset.

## Key Results
- Visual context significantly improves translation quality, with PBP-VIS achieving the highest scores across all metrics
- Page-level context outperforms both line-by-line and volume-level approaches
- The token-efficient PBP-VIS-NUM approach maintains high quality while reducing token usage
- Methods achieve state-of-the-art results for Japanese-English translation and set new standards for Japanese-Polish

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal context significantly improves translation quality by resolving ambiguities that are only visually apparent
- Mechanism: The model leverages visual features (like TV symbol, presenter appearance) to disambiguate textual references that would otherwise be misinterpreted
- Core assumption: Visual context contains disambiguating information that text alone cannot provide
- Evidence anchors:
  - [abstract] "the need to effectively incorporate visual elements into the translation process to resolve ambiguities"
  - [section 5.1] "When the key region, i.e., the border of the TV and its 'off' symbol, is obscured, the translation accuracy for that particular sentence using PBP-VIS decreases significantly compared to when it is visible"

### Mechanism 2
- Claim: Smaller translation units (page-level) outperform larger ones due to context window limitations
- Mechanism: The model maintains translation coherence better with page-level context than with volume-level context
- Core assumption: LLMs have a sweet spot for context length where additional information is beneficial but not overwhelming
- Evidence anchors:
  - [section 5.1] "Interestingly, providing context beyond the page level does not enhance translation quality" and "VBV-VIS, which translates the entire volume in one query, shows the lowest performance"
  - [section 5.1] "These findings suggest an inverse relationship between translation quality and input length beyond a single page"

### Mechanism 3
- Claim: Visual context provides cross-lingual benefits independent of language pair
- Mechanism: The model's ability to leverage visual context transfers across different language pairs, improving performance regardless of linguistic distance
- Core assumption: Visual understanding capabilities are language-agnostic and can benefit translation in any direction
- Evidence anchors:
  - [section 5.1] "The effectiveness of our methods across translations suggests broad applicability to different language pairs"
  - [section 5.1] "The cross-lingual success of our methods indicates that the benefits of incorporating visual context in manga translation are language-independent"

## Foundational Learning

- Concept: Multimodal large language models
  - Why needed here: The entire approach relies on models that can process both text and images together
  - Quick check question: What is the key architectural difference between multimodal LLMs and standard text-only LLMs?

- Concept: Context window management in LLMs
  - Why needed here: The research shows that context length dramatically affects performance, so understanding how LLMs handle different input sizes is crucial
  - Quick check question: Why might longer context windows sometimes degrade translation quality?

- Concept: Optical Character Recognition (OCR) limitations
  - Why needed here: The PBP-VIS-NUM method specifically addresses OCR limitations by removing text from images
  - Quick check question: What are the main challenges that make OCR difficult for manga text?

## Architecture Onboarding

- Component map:
  - Image → Text detection (OCR) → Panel detection → Reading order estimation → Context assembly (text + visual) → LLM prompt → Translation output → Evaluation

- Critical path: Image → OCR/Text extraction → Context assembly (text + visual) → LLM prompt → Translation output → Evaluation

- Design tradeoffs:
  - Single page vs volume context: Better quality vs computational efficiency
  - Text-in-image vs text-removed: Better visual context vs simpler model reasoning
  - One-shot vs few-shot prompting: Cost vs potential quality improvement
  - Commercial LLM vs open-source: Better performance vs data privacy and reproducibility

- Failure signatures:
  - Performance degrades significantly with longer context (indicates context window issues)
  - Visual context provides minimal improvement (indicates model doesn't effectively use visual features)
  - Inconsistent performance across similar language pairs (indicates language-specific issues)
  - High critical error rate in human evaluation (indicates fundamental translation problems)

- First 3 experiments:
  1. Compare PBP-VIS vs PBP performance on a small validation set to confirm visual context helps
  2. Test PBP-VIS vs PBP-VIS-NUM to validate the text-removal approach
  3. Evaluate single page vs 3-page context to determine optimal context length

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal context length for multimodal manga translation using LLMs?
- Basis in paper: [explicit] The paper explicitly evaluates different context lengths and finds that longer context beyond a single page does not improve translation quality, with VBP-VIS-COD (using a summary) performing better than VBP-VIS-ALL (using entire volume)
- Why unresolved: While the paper shows that longer context degrades performance, it doesn't identify an optimal sweet spot
- What evidence would resolve it: Systematic testing of intermediate context lengths (3-10 pages) with varying summarization strategies, comparing against the single-page baseline

### Open Question 2
- Question: How do different multimodal LLMs compare for manga translation tasks?
- Basis in paper: [inferred] The paper uses only GPT-4 Turbo for all experiments due to cost constraints, but acknowledges that "the availability and quality of open-source multimodal multilingual LLMs is very limited"
- Why unresolved: The paper's findings are specific to GPT-4 Turbo, leaving open whether other multimodal models would perform similarly or better
- What evidence would resolve it: Direct comparison of multiple multimodal LLMs (including open-source alternatives) on the same manga translation benchmarks, evaluating both performance and cost-effectiveness

### Open Question 3
- Question: What is the impact of different manga genres and art styles on translation quality?
- Basis in paper: [explicit] The paper acknowledges this limitation: "there is still a severe lack of evaluation data, making it difficult to determine how consistent our findings would be across different authors and genres"
- Why unresolved: The evaluation uses only two manga titles (different genres but limited sample)
- What evidence would resolve it: Testing the translation methods across a diverse corpus of manga spanning multiple genres, art styles, publication eras, and complexity levels

## Limitations
- Commercial LLM dependency creates reproducibility challenges and cost barriers
- Context window limitations affect performance with longer context lengths
- Dataset scope constraints limit evaluation to only two language pairs (JA-EN and JA-PL)

## Confidence
**High confidence**: Visual context significantly improves translation quality by resolving ambiguities; Page-level context outperforms larger context windows for translation quality; The proposed methods achieve state-of-the-art performance for Japanese-English translation

**Medium confidence**: Cross-lingual benefits of visual context are independent of language pair; Token-efficient approaches maintain translation quality while reducing costs; Context length has an inverse relationship with translation quality beyond single pages

**Low confidence**: Broad applicability to different language pairs beyond JA-EN and JA-PL; Long-term sustainability of commercial LLM-based approaches; Performance consistency across different manga genres and artistic styles

## Next Checks
1. **Context window optimization**: Systematically test context lengths from 1 to 10 pages on a diverse set of manga pages to identify the precise optimal context window and determine whether this varies by manga genre or complexity

2. **Open-source LLM comparison**: Implement the same translation approaches using an open-source multimodal model (such as LLaVA or Qwen-VL) to assess whether the performance benefits persist without commercial API dependencies and to improve reproducibility

3. **Cross-linguistic generalization**: Evaluate the proposed methods on at least two additional language pairs with different linguistic distances (e.g., JA-ES and JA-ZH) to empirically validate the claimed cross-lingual benefits and identify any language-specific limitations