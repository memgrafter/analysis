---
ver: rpa2
title: Benchmarking Hallucination in Large Language Models based on Unanswerable Math
  Word Problem
arxiv_id: '2403.03558'
source_url: https://arxiv.org/abs/2403.03558
tags:
- llms
- hallucination
- questions
- unanswerable
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to evaluate hallucination
  in large language models (LLMs) using unanswerable math word problems (MWP). The
  authors construct a dataset called UMWP containing 5200 questions, half answerable
  and half unanswerable, across five categories of unanswerability.
---

# Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem

## Quick Facts
- arXiv ID: 2403.03558
- Source URL: https://arxiv.org/abs/2403.03558
- Authors: Yuhong Sun; Zhangyue Yin; Qipeng Guo; Jiawen Wu; Xipeng Qiu; Hui Zhao
- Reference count: 0
- Primary result: Novel approach to evaluate LLM hallucination using unanswerable math word problems

## Executive Summary
This paper introduces a novel methodology for benchmarking hallucination in large language models (LLMs) using unanswerable math word problems (MWP). The authors construct a comprehensive dataset called UMWP containing 5200 questions, equally divided between answerable and unanswerable problems across five categories of unanswerability. The evaluation framework combines text similarity metrics with mathematical expression detection to determine whether LLMs can correctly identify unanswerable questions. Through extensive experiments on 31 different LLMs, the study demonstrates that in-context learning and reinforcement learning with human feedback (RLHF) significantly enhance models' ability to avoid hallucination when faced with unanswerable problems.

## Method Summary
The paper proposes a two-pronged approach to evaluating LLM hallucination. First, it constructs the UMWP dataset containing 5200 math word problems, with half being answerable and half unanswerable, categorized across five types of unanswerability (missing information, contradictory data, impossible conditions, etc.). Second, it develops an evaluation method that assesses whether LLMs can recognize unanswerable questions by analyzing both textual similarity and mathematical expression validity. The approach tests whether models can refrain from providing false answers to questions they should identify as unanswerable.

## Key Results
- In-context learning and RLHF significantly improve LLM performance in avoiding hallucination on unanswerable math problems
- The combined text similarity and mathematical expression detection evaluation method effectively identifies hallucination behaviors
- 31 different LLMs were tested, showing varying capabilities in handling unanswerable questions
- The UMWP dataset successfully differentiates between models that can recognize unanswerability versus those that hallucinate answers

## Why This Works (Mechanism)
The approach works because math word problems provide a structured, quantifiable domain where correctness can be objectively measured. Unlike open-ended tasks, MWPs have clear mathematical constraints that make hallucination detection more straightforward. By focusing on unanswerable problems, the method isolates the model's ability to recognize its limitations rather than just its ability to generate plausible-sounding but incorrect answers.

## Foundational Learning
- **Math Word Problem Structure**: Understanding how mathematical relationships are encoded in natural language text
  - Why needed: MWPs combine linguistic understanding with mathematical reasoning, making them ideal for testing hallucination
  - Quick check: Can the model correctly parse and represent mathematical relationships in text?

- **Hallucination Detection Metrics**: Combining text similarity with mathematical expression validation
  - Why needed: Simple text similarity alone is insufficient for mathematical reasoning tasks
  - Quick check: Does the evaluation method correctly identify both semantically and mathematically incorrect responses?

- **In-context Learning Effects**: How demonstration examples influence model behavior on unanswerable questions
  - Why needed: Understanding whether few-shot examples can teach models to recognize their limitations
  - Quick check: Does providing examples of unanswerable questions improve recognition rates?

## Architecture Onboarding
**Component Map**: UMWP Dataset -> LLM Inference -> Text Similarity Analysis -> Math Expression Validation -> Hallucination Score

**Critical Path**: Dataset construction → Model evaluation → Metric computation → Result analysis

**Design Tradeoffs**: The approach trades domain specificity (math problems only) for measurement precision and objectivity. While this limits generalizability, it provides clear, quantifiable results that are difficult to achieve in more ambiguous domains.

**Failure Signatures**: Models may fail by either incorrectly answering answerable questions or failing to recognize truly unanswerable problems. False positives (marking answerable questions as unanswerable) and false negatives (attempting to answer unanswerable questions) both indicate hallucination issues.

**First Experiments**:
1. Evaluate a small language model on UMWP to establish baseline performance
2. Test whether providing examples of unanswerable questions improves recognition rates
3. Compare text similarity-only evaluation versus combined metric approach

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset construction lacks detailed inter-annotator agreement rates for question categorization
- Evaluation metric may not capture all forms of hallucination, particularly subtle numerical errors
- Sample size of 31 LLMs may be insufficient for definitive conclusions about broader model families
- Findings may not generalize beyond mathematical domains to other types of unanswerable questions

## Confidence
- Dataset construction methodology: Medium confidence (solid approach but limited validation details)
- Evaluation method effectiveness: Medium confidence (innovative but potentially incomplete)
- Impact of in-context learning and RLHF: Medium confidence (statistically supported but limited sample size)
- Generalizability of findings: Low confidence (domain-specific validation only)

## Next Checks
1. Conduct inter-annotator agreement analysis on the UMWP dataset to establish reliability of question categorization
2. Expand evaluation to include non-mathematical unanswerable questions to test domain generalizability
3. Implement additional hallucination detection metrics beyond text similarity and expression matching to validate completeness of evaluation approach