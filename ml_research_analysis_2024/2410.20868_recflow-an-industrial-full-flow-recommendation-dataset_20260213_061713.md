---
ver: rpa2
title: 'RecFlow: An Industrial Full Flow Recommendation Dataset'
arxiv_id: '2410.20868'
source_url: https://arxiv.org/abs/2410.20868
tags:
- stage
- ranking
- samples
- user
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces RecFlow, the first industrial-scale full-flow\
  \ recommendation dataset capturing samples across all stages of a multi-stage recommendation\
  \ pipeline (retrieval \u2192 pre-ranking \u2192 ranking \u2192 re-ranking). Unlike\
  \ existing datasets focused solely on exposed items, RecFlow includes 38M exposure\
  \ samples and 1.9B stage samples from 42K users across 9M items over 37 days, enabling\
  \ study of the training-serving distribution shift problem."
---

# RecFlow: An Industrial Full Flow Recommendation Dataset

## Quick Facts
- arXiv ID: 2410.20868
- Source URL: https://arxiv.org/abs/2410.20868
- Reference count: 27
- Primary result: First industrial-scale full-flow recommendation dataset capturing 38M exposure samples and 1.9B stage samples across all stages of a multi-stage recommendation pipeline

## Executive Summary
This paper introduces RecFlow, the first industrial-scale full-flow recommendation dataset capturing samples across all stages of a multi-stage recommendation pipeline (retrieval → pre-ranking → ranking → re-ranking). Unlike existing datasets focused solely on exposed items, RecFlow includes 38M exposure samples and 1.9B stage samples from 42K users across 9M items over 37 days, enabling study of the training-serving distribution shift problem. The dataset supports research on selection bias, multi-stage consistency, multi-task recommendation, and user behavior modeling.

## Method Summary
RecFlow is constructed from a large-scale industrial recommendation system, collecting data from 42K randomly sampled users over 37 days. The dataset captures both exposed items and items filtered at each stage of the multi-stage pipeline, creating a comprehensive view of the recommendation process. The data collection involves random seed user sampling, online request logging, and stage-wise sampling strategy. The dataset is structured at the request level with user, item, stage, and feedback features. Experiments use baseline models (SASRec, DSSM, DIN) with stage sample augmentation to evaluate improvements in retrieval, coarse ranking, and ranking stages.

## Key Results
- Using filtered stage samples as hard negatives improves Recall@100 by 24.7% and NDCG@100 by 28.3%
- FS-LTR (modeling subsequent stage preferences) achieves 73.7% Recall@100 (+60%) improvement
- Stage samples as extra negatives improve coarse ranking Recall@200 from 0.535 to 0.668

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using filtered stage samples as hard negatives significantly improves retrieval performance by providing challenging negatives closer to positive samples in feature space.
- Mechanism: The multi-stage pipeline filters out items at each stage, and these filtered items are semantically similar to items that would be displayed but were not selected. Using these as hard negatives provides more informative training signals than random negatives, especially for small K values in ranking metrics.
- Core assumption: Items filtered at later stages (e.g., rerank_pos) are more semantically similar to positive items than items filtered at earlier stages, making them harder negatives.
- Evidence anchors:
  - [abstract]: "Using filtered stage samples as hard negatives improves Recall@100 by 24.7% and NDCG@100 by 28.3%"
  - [section 3.1.1]: "We think the unexposed stage samples indeed satisfy the definition of hard negative samples" and "videos from rerank pos are negative samples of appropriate difficulty"
  - [corpus]: No direct corpus evidence found for this specific mechanism
- Break condition: If the filtered items are not actually similar to positive items in the learned feature space, or if the distribution shift between stages is too large for the model to learn effectively from these negatives.

### Mechanism 2
- Claim: Modeling the preference of subsequent stages (FS-LTR) improves overall system performance by learning a unified preference model across stages.
- Mechanism: Each stage in the multi-stage pipeline has its own selection criteria. By incorporating knowledge of subsequent stages' preferences into earlier stage models through auxiliary ranking losses, the model learns to select items that will survive the entire pipeline, not just be preferred by the user.
- Core assumption: The optimal solution for each stage is to select items that satisfy both user preference and subsequent stage preferences simultaneously (Generalized Probability Ranking Principle).
- Evidence anchors:
  - [abstract]: "FS-LTR (modeling subsequent stage preferences) achieves 73.7% Recall@100 (+60%)"
  - [section 3.1.2]: "The optimal solution for the model of each stage is to select videos that satisfy the preference of the user and subsequent stages simultaneously" and "FS-LTR has proposed the Generalized Probability Ranking Principle (GPRP) to prove the solution proposed above is optimal theoretically"
  - [corpus]: No direct corpus evidence found for this specific mechanism
- Break condition: If the subsequent stage preferences are too complex to model effectively, or if the auxiliary ranking loss interferes with learning user preference.

### Mechanism 3
- Claim: Supplementing stage samples as extra negatives addresses data distribution shift by making training distribution more consistent with serving distribution.
- Mechanism: Models trained only on exposed items face a distribution shift when serving, as they must score thousands of items but only see a small subset in training. Adding unexposed stage samples as negatives increases the consistency between training and serving distributions.
- Core assumption: The distribution of unexposed items in the serving environment is similar to the distribution of items filtered at each stage.
- Evidence anchors:
  - [abstract]: "Data distribution shift is a longstanding problem in RS" and "Supplementing stage videos as extra negative samples can largely enhance the Recall and NDCG metric"
  - [section 3.2.1]: "The coarse ranking model is trained based on the exposed samples which contains 6 videos at most but has to score 3, 000 videos in each request. The data distribution between training and testing exists huge inconsistency" and "The more consistent data distribution between training and testing, the more improvement"
  - [corpus]: No direct corpus evidence found for this specific mechanism
- Break condition: If the stage samples introduce too much noise or false negatives, degrading model performance despite the distribution shift.

## Foundational Learning

- Concept: Multi-stage recommendation pipeline
  - Why needed here: Understanding the retrieval → pre-ranking → ranking → re-ranking stages is crucial for understanding how RecFlow captures data and why stage samples are valuable
  - Quick check question: What is the approximate output size at each stage in the industrial pipeline described in the paper?
  - Answer: 8000 → 3000 → 500 → 120 → 10 → 6

- Concept: Hard negative mining
  - Why needed here: The paper's experiments rely on using filtered stage samples as hard negatives, which requires understanding what makes a good negative sample
  - Quick check question: Why are items filtered at later stages considered "harder" negatives than items filtered at earlier stages?
  - Answer: Items filtered at later stages are more semantically similar to positive items and therefore more challenging for the model to distinguish

- Concept: Data distribution shift
  - Why needed here: The paper addresses the training-serving distribution mismatch problem, which is fundamental to understanding RecFlow's contribution
  - Quick check question: What is the ratio of stage samples to exposed samples in the 2nd period of RecFlow?
  - Answer: 236 times (the difference between stage samples and exposure samples has increased to 236 times in the 2nd period)

## Architecture Onboarding

- Component map: Random seed user sampling -> online request logging -> stage-wise sampling strategy
- Critical path: User request -> multi-stage filtering -> stage sample collection -> model training -> evaluation with stage-aware metrics
- Design tradeoffs:
  - Sampling strategy: 10 vs 40 negatives per stage balances storage pressure vs information completeness
  - Feature selection: Identity features anonymized vs contextual features retained
  - Evaluation metrics: Classical AUC vs stage-aware Recall/NDCG for different research questions
- Failure signatures:
  - Poor performance on small K values despite stage sample augmentation may indicate hard negatives are too difficult
  - AUC degradation when adding extra negatives suggests false negative contamination
  - Inconsistent improvements across stages may indicate stage-specific modeling challenges
- First 3 experiments:
  1. Implement hard negative mining using pre-rank neg samples and evaluate Recall@100 improvement
  2. Add auxiliary ranking loss using stage samples to a baseline retrieval model and compare with hard negative baseline
  3. Supplement coarse ranking training with rank neg samples and evaluate both AUC and Recall@200 to observe distribution shift effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the hardness level of stage samples vary across different recommendation stages (retrieval → pre-ranking → ranking → re-ranking)?
- Basis in paper: [explicit] The paper demonstrates that videos from different stages have varying effectiveness as hard negatives in retrieval, with rerank pos outperforming other stages, and shows that adding more negatives from harder stages can degrade performance.
- Why unresolved: The paper only explores this phenomenon in the retrieval stage. The hardness characteristics of stage samples in pre-ranking, coarse ranking, and ranking stages remain unexplored.
- What evidence would resolve it: Systematic experiments measuring retrieval performance when using stage samples as hard negatives at each stage, quantifying the hardness level of samples from different stages.

### Open Question 2
- Question: What is the optimal ratio between easy negatives (random samples) and hard negatives (stage samples) for different recommendation stages?
- Basis in paper: [inferred] The paper shows that increasing hard negative samples improves performance with diminishing returns in retrieval, and mentions (He et al., 2014; Zhang et al., 2023a) that the ratio between easy and hard negatives critically influences performance.
- Why unresolved: While the paper explores varying numbers of hard negatives in retrieval, it doesn't systematically study the optimal easy-to-hard negative ratio or determine if this ratio should vary by stage.
- What evidence would resolve it: Controlled experiments varying both the number of easy and hard negatives across all stages, identifying optimal ratios that maximize performance metrics.

### Open Question 3
- Question: How can the distribution shift problem be most effectively addressed across different recommendation stages?
- Basis in paper: [explicit] The paper demonstrates that supplementing stage samples as extra negatives improves Recall and NDCG metrics in both coarse ranking and ranking stages by reducing training-serving distribution inconsistency.
- Why unresolved: The paper only explores one method (adding stage samples as negatives) and doesn't compare alternative approaches like importance weighting, domain adaptation, or data synthesis techniques.
- What evidence would resolve it: Comparative studies of multiple distribution shift mitigation techniques across all stages, measuring their effectiveness on both classical metrics (AUC, Logloss) and new metrics (Recall, NDCG).

## Limitations
- The specific effectiveness of stage samples may be sensitive to the sampling strategy and distribution characteristics of the industrial dataset used
- The mechanisms rely on assumptions about semantic similarity of filtered items that require further validation
- The generalizability of the specific improvements (e.g., 24.7% Recall@100) to other industrial recommendation systems is uncertain

## Confidence
- **High confidence**: The dataset construction methodology and the general effectiveness of using stage samples as extra negatives to address distribution shift
- **Medium confidence**: The specific mechanisms of hard negative mining using stage samples and FS-LTR for modeling subsequent stage preferences
- **Low confidence**: The generalizability of the specific improvements (e.g., 24.7% Recall@100 improvement) to other industrial recommendation systems

## Next Checks
1. **Semantic similarity validation**: Conduct embedding analysis to verify that items filtered at later stages (rerank_pos) are indeed more semantically similar to positive items than items filtered at earlier stages, confirming the hard negative assumption.

2. **Distribution shift quantification**: Measure the KL divergence or other statistical distance metrics between training distributions (exposed samples only) and serving distributions (all stage samples) to quantify the actual distribution shift being addressed.

3. **Cross-domain replication**: Apply the same hard negative mining and FS-LTR techniques to a different industrial recommendation dataset to test the generalizability of the improvements beyond the RecFlow dataset.