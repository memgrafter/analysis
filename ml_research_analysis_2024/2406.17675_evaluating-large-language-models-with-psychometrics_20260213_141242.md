---
ver: rpa2
title: Evaluating Large Language Models with Psychometrics
arxiv_id: '2406.17675'
source_url: https://arxiv.org/abs/2406.17675
tags:
- llms
- personality
- language
- table
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a comprehensive psychometric benchmark for\
  \ assessing psychological constructs in large language models (LLMs), covering five\
  \ dimensions\u2014personality, values, emotion, theory of mind, and self-efficacy\u2014\
  across 13 datasets. The work identifies inconsistencies between self-reported traits\
  \ and open-ended responses, demonstrates variability across evaluation scenarios,\
  \ and validates test reliability through measures like internal consistency, parallel\
  \ forms reliability, and inter-rater agreement."
---

# Evaluating Large Language Models with Psychometrics

## Quick Facts
- arXiv ID: 2406.17675
- Source URL: https://arxiv.org/abs/2406.17675
- Reference count: 40
- This paper presents a comprehensive psychometric benchmark for assessing psychological constructs in large language models (LLMs), covering five dimensions—personality, values, emotion, theory of mind, and self-efficacy—across 13 datasets.

## Executive Summary
This paper introduces a comprehensive psychometric benchmark to assess psychological constructs in large language models (LLMs), evaluating five dimensions: personality, values, emotion, theory of mind, and self-efficacy. The authors identify significant inconsistencies between self-reported traits and open-ended responses in LLMs, demonstrating that these models lack stable psychological patterns and are highly sensitive to prompt variations. The benchmark provides insights for AI development, social science research, and user experience enhancement by revealing how LLMs process and express psychological attributes differently from humans.

## Method Summary
The study evaluates LLMs across five psychological dimensions using 13 established psychometric datasets, including personality assessments (Big Five Inventory), value scales (Schwartz Value Survey), emotional recognition tasks, theory of mind scenarios, and self-efficacy measures. The authors validate test reliability through multiple methods including internal consistency (Cronbach's alpha), parallel forms reliability, and inter-rater agreement. They compare self-reported traits with open-ended responses and assess model behavior across different prompt formulations to identify stability patterns. The evaluation spans multiple LLM architectures to examine generalizability of findings.

## Key Results
- LLMs show inconsistent responses to psychological assessments across different prompt formulations and scenarios
- Models demonstrate instability in self-reported traits that don't align with their open-ended behavioral responses
- LLMs often fail to recognize their own limitations when responding to real-world queries
- The benchmark reveals that LLMs lack stable psychological patterns comparable to human patterns

## Why This Works (Mechanism)
The paper's approach works because psychometric instruments, when properly validated, provide standardized methods for quantifying psychological constructs that can be systematically applied to both humans and artificial systems. By adapting these instruments to LLM evaluation, the authors can leverage decades of psychometric research to identify whether models exhibit human-like psychological patterns. The multiple validation methods employed (Cronbach's alpha, parallel forms reliability, inter-rater agreement) provide robust statistical frameworks for assessing whether observed patterns represent genuine psychological constructs rather than random noise or prompt-specific artifacts.

## Foundational Learning
- Psychometric validation methods - Understanding Cronbach's alpha, test-retest reliability, and inter-rater agreement is essential for evaluating the reliability of psychological assessments in LLMs.
- Theory of mind assessment - Methods for evaluating whether models can understand and predict others' mental states, crucial for human-like interaction.
- Prompt sensitivity analysis - Techniques for measuring how model responses vary with different prompt formulations, revealing model stability characteristics.

## Architecture Onboarding
- Component map: Psychometric datasets -> LLM evaluation pipeline -> Reliability validation -> Cross-model comparison
- Critical path: Dataset selection and validation -> Model prompt engineering -> Response collection -> Statistical analysis -> Reliability assessment
- Design tradeoffs: Using human-validated psychometric instruments may not capture equivalent constructs in LLMs; balancing comprehensive coverage versus focused assessment.
- Failure signatures: Inconsistent responses across prompt variations, misalignment between self-reported traits and behavioral responses, failure to recognize model limitations.
- First experiments: 1) Test same model with identical prompts across multiple inference runs to measure temporal stability. 2) Compare model responses to human responses on identical psychometric instruments. 3) Evaluate response consistency across different prompt formulations for the same psychological construct.

## Open Questions the Paper Calls Out
The paper raises several important open questions that warrant further investigation:
- How do different pretraining strategies and data distributions affect the psychological profile stability of LLMs?
- To what extent do observed inconsistencies reflect fundamental differences between artificial and biological intelligence versus implementation artifacts?
- Can we develop more appropriate psychometric instruments specifically designed for artificial systems rather than adapted human measures?
- How do these psychological assessment patterns correlate with downstream task performance and user experience?

## Limitations
- The study relies on existing psychometric instruments designed for humans, raising questions about construct validity in artificial systems
- The sample size of evaluated LLMs remains relatively small compared to the rapidly expanding landscape of models with different pretraining strategies
- The observed instability in responses may be partially attributable to specific prompting strategies rather than inherent model limitations
- The paper doesn't fully explore how different temperature settings or sampling strategies affect psychological assessment consistency

## Confidence
- **High Confidence**: LLMs show inconsistent responses to psychological assessments across different prompt formulations and scenarios
- **Medium Confidence**: LLMs lack stable psychological patterns comparable to human patterns
- **Medium Confidence**: LLMs often fail to recognize their own limitations in real-world queries

## Next Checks
1. Cross-architecture validation: Test the same psychometric benchmark across a broader range of LLM architectures to determine whether observed patterns are universal or architecture-dependent
2. Human-LLM construct equivalence: Conduct parallel validation studies comparing human responses to the same psychometric instruments, examining whether the instruments measure equivalent constructs across biological and artificial agents
3. Temporal stability assessment: Evaluate the same models at multiple time points with identical prompts to quantify the degree of temporal instability and determine whether observed variability represents fundamental model properties or transient artifacts