---
ver: rpa2
title: 'The MovieLens Beliefs Dataset: Collecting Pre-Choice Data for Online Recommender
  Systems'
arxiv_id: '2405.11053'
source_url: https://arxiv.org/abs/2405.11053
tags:
- data
- user
- beliefs
- movies
- recommendations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a procedure for collecting user beliefs about
  movies they have not yet experienced, addressing a gap in recommender system design
  where traditional ratings data is incomplete. The method was implemented on the
  MovieLens platform over a year, generating a dataset combining user ratings, beliefs,
  and observed recommendations.
---

# The MovieLens Beliefs Dataset: Collecting Pre-Choice Data for Online Recommender Systems

## Quick Facts
- **arXiv ID:** 2405.11053
- **Source URL:** https://arxiv.org/abs/2405.11053
- **Reference count:** 22
- **Primary result:** A procedure for collecting user beliefs about un-experienced movies, implemented on MovieLens, generating 28,457 belief responses from 3,199 users about 7,518 movies over one year.

## Executive Summary
This paper introduces a novel procedure for collecting user beliefs about movies they haven't yet experienced, addressing a critical gap in recommender system design. The method was implemented on the MovieLens platform over a year, generating a dataset that combines user ratings, beliefs, and observed recommendations. The procedure involves sampling movies across genres based on popularity, ratings, and recent releases, then eliciting beliefs about a subset for each user. The resulting dataset exhibits internal consistency and aligns with prior findings that users are more certain about popular movies and more likely to watch those they expect to like.

## Method Summary
The procedure collects user beliefs about un-experienced movies through a non-invasive interface integrated into the MovieLens homepage. Movies are sampled monthly across genres based on popularity, ratings, recent releases, and serendipitous categories. Users are presented with a row of 8 movies and asked to provide beliefs about each. The dataset contains 28,457 belief responses from 3,199 users about 7,518 movies, with no significant selection bias in response rates across movie types.

## Key Results
- Collected 28,457 belief responses from 3,199 users about 7,518 movies over one year
- Belief data exhibits internal consistency and aligns with prior findings about user certainty and consumption preferences
- No significant selection bias in response rates across movie types or popularity levels
- Users are more certain about popular movies and more likely to watch those they expect to like

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Belief data collection works because user uncertainty about un-experienced goods predicts both future consumption and choice shifts when recommendations are introduced.
- Mechanism: Users assign expected utilities to un-experienced goods based on prior beliefs. When recommendations are shown, these beliefs shift, altering the choice set. By collecting beliefs before and after recommendations, the system can model the causal pathway from recommendation to choice.
- Core assumption: User beliefs about un-experienced goods are meaningful and predictive of actual consumption behavior.
- Evidence anchors: [abstract] "beliefs data exhibits internal consistency and aligns with prior findings that users are more certain about popular movies and more likely to watch those they expect to like"; [section] "beliefs predict what they will consume"
- Break condition: If beliefs about un-experienced goods do not correlate with actual consumption, the predictive value of the dataset collapses.

### Mechanism 2
- Claim: The sampling procedure generates representative variation in the belief dataset, enabling extrapolation across the full product space.
- Mechanism: The procedure samples movies across genres based on popularity, ratings, recent releases, and serendipitous categories. This ensures coverage of both popular and niche items, allowing collaborative filtering and content-based methods to estimate beliefs for the entire catalog.
- Core assumption: User preferences and beliefs exhibit correlation both across goods and across users, enabling statistical imputation.
- Evidence anchors: [section] "standard collaborative filtering and content-based methods may work to estimate the full set of beliefs"; [abstract] "beliefs data exhibits internal consistency"
- Break condition: If the sampled movies fail to capture the diversity of the full catalog, imputation methods will produce biased estimates.

### Mechanism 3
- Claim: Non-invasive data collection integrated into the platform interface achieves sustainable user engagement without selection bias.
- Mechanism: By embedding belief elicitation into the homepage as a second row rather than a separate survey, the procedure maintains natural user flow while collecting consistent data over time.
- Core assumption: Users will engage with the belief elicitation task when it's presented as part of normal platform usage rather than as an external survey.
- Evidence anchors: [section] "we do this so that the data collection is non-invasive and blended into the platform in a non-intrusive manner"; [abstract] "response rates across movie types" show no significant selection bias
- Break condition: If user engagement drops significantly over time or if certain user segments systematically avoid the belief elicitation task, the dataset becomes biased.

## Foundational Learning

- Concept: Economic model of choice under uncertainty
  - Why needed here: The procedure is explicitly designed around an economic framework where users maximize expected utility based on beliefs about un-experienced goods
  - Quick check question: What is the mathematical relationship between user beliefs, recommendations, and choice according to the model in Section 2.1?

- Concept: Collaborative filtering and content-based recommendation foundations
  - Why needed here: The paper assumes these standard methods can be applied to belief data to estimate the full belief distribution across all movies
  - Quick check question: How do the assumptions about user preference correlation support the use of collaborative filtering on belief data?

- Concept: Experimental design and sampling theory
  - Why needed here: The procedure's effectiveness depends on proper sampling across genres and movie types to ensure representative variation
  - Quick check question: Why does the procedure use stratified sampling by genre rather than simple random sampling of all movies?

## Architecture Onboarding

- Component map: MovieLens platform backend integration -> Belief elicitation UI component -> Sampling algorithm for movie selection -> Data logging system -> Analysis pipeline

- Critical path:
  1. User visits MovieLens homepage
  2. System generates belief elicitation row with 8 movies
  3. User interacts (provides beliefs or marks as seen)
  4. Response logged with timestamp, user ID, movie ID
  5. Data aggregated for monthly M_t refresh
  6. Analysis pipeline processes new data for patterns

- Design tradeoffs:
  - Larger M_t provides better coverage but fewer beliefs per movie
  - Smaller M_t provides denser user-movie matrix but less catalog coverage
  - Integration vs. survey approach balances user burden against data quality
  - Frequency of M_t refresh vs. stability of sampling

- Failure signatures:
  - Declining response rates over time
  - Systematic bias in which movies receive beliefs
  - High proportion of "already seen" responses
  - Poor correlation between elicited beliefs and actual consumption

- First 3 experiments:
  1. Validate belief prediction: Use collaborative filtering to predict beliefs for movies not sampled, then test against actual responses
  2. Measure recommendation effect: Compare belief distributions for movies before and after they appear in user recommendations
  3. Assess selection bias: Analyze response rates across popularity, genre, and other movie characteristics to confirm no systematic patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are collaborative filtering and content-based methods in predicting user beliefs about un-experienced items compared to their effectiveness in predicting ratings?
- Basis in paper: [explicit] The paper mentions that standard collaborative filtering and content-based methods may work to estimate the full set of beliefs, but does not provide empirical validation of this claim
- Why unresolved: The paper demonstrates internal consistency and behavioral patterns in the belief data but does not test predictive models for beliefs
- What evidence would resolve it: Empirical comparison of prediction accuracy for beliefs versus ratings using the same methodology on the collected dataset

### Open Question 2
- Question: To what extent do recommendations causally influence user beliefs versus merely reflecting existing preferences?
- Basis in paper: [explicit] The paper references prior work showing recommendations affect beliefs but does not measure this effect using the newly collected dataset
- Why unresolved: The procedure elicits beliefs before and after recommendations, but the paper does not analyze the causal effect of recommendations on belief formation
- What evidence would resolve it: Statistical analysis comparing belief changes for items that appeared in recommendations versus those that did not, controlling for user characteristics and item features

### Open Question 3
- Question: What is the optimal balance between coverage (number of movies in M_t) and response density (number of beliefs per movie) for effective belief prediction?
- Basis in paper: [explicit] The paper acknowledges a tradeoff between larger M_t providing coverage but fewer beliefs per movie versus smaller M_t providing denser data but less coverage
- Why unresolved: The procedure uses a fixed parameter (y=11) without empirical justification or sensitivity analysis of different configurations
- What evidence would resolve it: Systematic evaluation of belief prediction accuracy under different M_t sizes and sampling strategies using the collected data

## Limitations
- Potential for selection bias despite stated absence of patterns, particularly if certain user segments systematically avoid the belief elicitation task
- Sparsity of the belief data matrix requiring advanced imputation techniques or collaborative filtering approaches
- Uncertainty around whether beliefs about un-experienced goods truly predict consumption behavior across all user segments

## Confidence

- **High confidence**: Beliefs predict consumption and exhibit internal consistency (directly validated in dataset)
- **Medium confidence**: Sampling procedure generates representative variation (depends on assumed validity of collaborative filtering)
- **Low confidence**: Non-invasive data collection mechanism (primarily a methodological claim without external validation)

## Next Checks
1. Validate belief prediction accuracy by holding out a subset of beliefs and testing collaborative filtering imputation against actual responses
2. Measure temporal stability by comparing belief distributions across different time periods to ensure consistency
3. Test selection bias formally using statistical tests across movie popularity, genre, and other characteristics to confirm the absence of systematic patterns in response rates