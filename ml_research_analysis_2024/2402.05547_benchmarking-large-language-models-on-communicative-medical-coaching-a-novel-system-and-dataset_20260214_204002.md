---
ver: rpa2
title: 'Benchmarking Large Language Models on Communicative Medical Coaching: a Novel
  System and Dataset'
arxiv_id: '2402.05547'
source_url: https://arxiv.org/abs/2402.05547
tags:
- medical
- agent
- patient
- coaching
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChatCoach, a human-AI cooperative framework
  for communicative medical coaching. The system employs a patient agent for realistic
  doctor-patient dialogue and a coach agent that provides real-time feedback on medical
  terminology usage.
---

# Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset

## Quick Facts
- **arXiv ID**: 2402.05547
- **Source URL**: https://arxiv.org/abs/2402.05547
- **Reference count**: 4
- **Primary result**: Introduces ChatCoach system and dataset for medical terminology coaching, with Llama2 outperforming ChatGPT on detection and correction tasks

## Executive Summary
This paper presents ChatCoach, a human-AI cooperative framework designed to improve communicative medical coaching by providing real-time feedback on medical terminology usage in doctor-patient dialogues. The system employs a patient agent for realistic interactions and a coach agent that offers immediate corrections and suggestions. To address the scarcity of domain-specific datasets, the authors develop a multi-agent data generation framework that creates synthetic training data from external medical knowledge and dialogue databases, complemented by a human-annotated testing set.

The experimental results demonstrate that instruction-tuned Llama2 significantly outperforms ChatGPT prompting strategies in detecting and correcting medical terminology errors. The performance metrics show BLEU-2 scores of 39.8 for detection and 4.0 for correction, with BERTScores reaching up to 77.8. This work establishes the first benchmark dataset and evaluation metrics for communicative medical coaching in healthcare settings.

## Method Summary
The ChatCoach system implements a multi-agent framework where a patient agent simulates realistic doctor-patient dialogues while a coach agent provides real-time feedback on medical terminology usage. The core innovation is a synthetic data generation pipeline that leverages external medical knowledge bases and dialogue databases to create training data, addressing the lack of existing domain-specific datasets. A human-annotated test set is compiled for evaluation. The system is evaluated using instruction-tuned Llama2 and compared against ChatGPT prompting strategies, with performance measured through BLEU-2 scores for detection and correction tasks, and BERTScores for overall quality assessment.

## Key Results
- Instruction-tuned Llama2 achieves BLEU-2 scores of 39.8 for medical terminology detection and 4.0 for correction tasks
- BERTScores reach up to 77.8, demonstrating strong linguistic quality in model outputs
- Llama2 significantly outperforms ChatGPT prompting strategies across all evaluation metrics
- The multi-agent data generation framework successfully creates usable synthetic training data from external sources

## Why This Works (Mechanism)
The ChatCoach system succeeds by combining realistic simulation through the patient agent with immediate corrective feedback via the coach agent, creating an iterative learning environment. The multi-agent data generation framework addresses the fundamental challenge of data scarcity in specialized medical communication by synthesizing diverse scenarios from existing knowledge bases and dialogue corpora. Instruction tuning of Llama2 enables the model to better understand and execute the specific tasks of terminology detection and correction compared to general prompting approaches.

## Foundational Learning
- **Multi-agent dialogue systems**: Needed to simulate realistic doctor-patient interactions for training and evaluation; quick check: verify agent behaviors maintain medical context consistency
- **Synthetic data generation**: Required to overcome dataset limitations in specialized medical communication; quick check: assess synthetic data diversity and medical accuracy
- **Instruction tuning**: Essential for task-specific performance in detection and correction; quick check: compare instruction-tuned vs base model performance on targeted tasks
- **BLEU and BERTScore metrics**: Used to evaluate linguistic quality and similarity; quick check: ensure metrics align with clinical communication objectives
- **Real-time feedback mechanisms**: Critical for effective coaching systems; quick check: measure feedback timing and relevance in simulated interactions
- **Medical terminology standards**: Foundation for accurate detection and correction; quick check: validate terminology coverage against medical knowledge bases

## Architecture Onboarding

**Component Map**: External Medical Knowledge -> Multi-Agent Generator -> Synthetic Training Data -> Llama2 Model -> ChatCoach System -> Patient Agent & Coach Agent -> Feedback Loop

**Critical Path**: Medical knowledge → Synthetic data generation → Model training → Real-time coaching → Performance evaluation

**Design Tradeoffs**: The system trades computational complexity for data quality by using synthetic generation rather than collecting real medical dialogues, which balances ethical concerns with training needs. The choice of instruction-tuned Llama2 over larger models optimizes for task-specific performance while maintaining efficiency.

**Failure Signatures**: Poor synthetic data quality leads to model hallucinations and incorrect terminology suggestions. Insufficient medical knowledge coverage results in missed errors or inappropriate corrections. Real-time feedback delays compromise the coaching effectiveness. Evaluation metric limitations may miss clinically relevant communication issues.

**First 3 Experiments**:
1. Test synthetic data generation quality by comparing generated dialogues against real medical conversations
2. Evaluate model performance across different medical specialties to assess generalizability
3. Measure the impact of real-time feedback timing on coaching effectiveness

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Synthetic training data may not fully capture the complexity and variability of real doctor-patient interactions, limiting model generalization
- The human-annotated test set may be too small or narrowly scoped to represent diverse clinical scenarios
- Evaluation metrics focus on linguistic similarity rather than clinical accuracy or patient safety implications

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Technical implementation of ChatCoach system | High |
| Multi-agent data generation framework effectiveness | High |
| Performance comparisons between Llama2 and ChatGPT | Medium |
| Establishing definitive benchmark for medical coaching | Low |

## Next Checks
1. Evaluate model performance across multiple medical specialties and clinical contexts to assess generalizability
2. Conduct human clinical expert review of model outputs to verify medical accuracy beyond linguistic metrics
3. Test the system with real doctor-patient dialogue recordings to validate synthetic data generation quality and model robustness in realistic scenarios