---
ver: rpa2
title: 'Interpretable Cross-Examination Technique (ICE-T): Using highly informative
  features to boost LLM performance'
arxiv_id: '2405.06703'
source_url: https://arxiv.org/abs/2405.06703
tags:
- ice-t
- patient
- arxiv
- questions
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ICE-T, a novel approach for binary classification
  using LLMs that combines multi-prompt techniques with traditional classifiers to
  improve performance over zero-shot and few-shot methods. ICE-T addresses the need
  for interpretability in domains like medicine and law, where black-box models are
  often not acceptable.
---

# Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance

## Quick Facts
- arXiv ID: 2405.06703
- Source URL: https://arxiv.org/abs/2405.06703
- Authors: Goran Muric; Ben Delay; Steven Minton
- Reference count: 40
- Key outcome: ICE-T consistently improves LLM binary classification performance across 17 diverse tasks, with smaller models matching or exceeding larger models' zero-shot results

## Executive Summary
ICE-T introduces a novel approach for binary classification using LLMs that combines multi-prompt techniques with traditional classifiers to improve performance over zero-shot and few-shot methods. The method addresses the need for interpretability in domains like medicine and law, where black-box models are often not acceptable. ICE-T generates multiple prompts, converts LLM responses into numerical feature vectors, and uses these vectors as input to a traditional classifier. Experiments across 17 tasks from diverse domains consistently show ICE-T outperforms zero-shot baselines in F1 scores, with notable improvements from GPT-3.5's average F1 improving from 0.683 to 0.845, and GPT-4's from 0.700 to 0.892. Remarkably, smaller models using ICE-T achieved comparable or better results than larger models using zero-shot prompting.

## Method Summary
ICE-T is a binary classification method that generates multiple secondary questions for each task, prompts an LLM with each question, converts the responses to numerical feature vectors (Yes=1, No=0, Unknown=0.5), and uses these vectors as input to a traditional classifier. The approach uses 5-fold cross-validation with grid search to select the best classifier for each task. The method was tested across 17 tasks from diverse domains including medicine, law, climate science, and politics, comparing ICE-T-enhanced models against zero-shot baselines using the same LLMs.

## Key Results
- GPT-3.5's average F1 score improved from 0.683 to 0.845 using ICE-T
- GPT-4's average F1 score improved from 0.700 to 0.892 using ICE-T
- Smaller models using ICE-T achieved comparable or better results than larger models using zero-shot prompting
- ICE-T consistently outperformed zero-shot baselines across all 17 tested tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICE-T improves classification by transforming qualitative LLM outputs into a structured, numerical feature vector that captures multiple perspectives on the classification task.
- Mechanism: Instead of relying on a single LLM prompt, ICE-T generates multiple secondary questions that probe different aspects of the decision problem. The LLM answers each question, and these answers are converted to numerical values (Yes=1, No=0, Unknown=0.5). This creates a low-dimensional feature vector that represents the LLM's reasoning process in quantifiable form.
- Core assumption: Multiple well-designed secondary questions can extract more informative features about the classification task than a single prompt.
- Evidence anchors:
  - [abstract]: "responses from the LLM are then converted into numerical feature vectors and processed by a traditional classifier"
  - [section]: "We take the responses from the LLM, convert them into numerical values to create a feature vector, and then input this vector into a traditional classifier"
- Break condition: If secondary questions fail to capture meaningful distinctions about the classification task, the feature vector becomes uninformative and performance degrades to baseline levels.

### Mechanism 2
- Claim: ICE-T enables smaller models to match or exceed larger models by leveraging the structured reasoning process rather than raw model capacity.
- Mechanism: The technique uses a smaller LLM (GPT-3.5) with the ICE-T framework to achieve comparable or better performance than a larger model (GPT-4) using zero-shot prompting. This works because the structured multi-prompt approach extracts more reliable information from the smaller model than a single prompt would.
- Core assumption: The quality of information extraction matters more than model size when using structured approaches.
- Evidence anchors:
  - [abstract]: "allows for smaller, less capable models to achieve or exceed the performance of larger, more advanced models under zero-shot conditions"
  - [section]: "In nearly all cases, except for two, the ICE-T-enhanced GPT-3.5 either outperformed or equaled the larger GPT-4 model on identical tasks"
- Break condition: If the smaller model cannot provide reliable answers to the secondary questions, or if the questions are too complex for the model's capabilities, performance will drop below that of larger models.

### Mechanism 3
- Claim: ICE-T maintains high interpretability by creating a transparent decision-making process where each feature in the vector corresponds to a specific question with a clear answer.
- Mechanism: The feature vector is directly traceable to specific questions and answers, allowing experts to understand exactly which aspects of the input influenced the final classification. This is particularly valuable in domains like medicine and law where decision rationale is critical.
- Core assumption: Domain experts can understand and validate the reasoning process when it's broken down into discrete, answerable questions.
- Evidence anchors:
  - [abstract]: "This method not only maintains high interpretability but also allows for smaller, less capable models to achieve or exceed the performance of larger, more advanced models under zero-shot conditions"
  - [section]: "experts can trace back the decision-making process, understanding which factors contributed most significantly to the model's classification"
- Break condition: If the secondary questions become too numerous or complex, the interpretability advantage diminishes as the relationship between features and final decisions becomes harder to follow.

## Foundational Learning

- Concept: Feature engineering and vectorization
  - Why needed here: ICE-T converts qualitative LLM responses into numerical feature vectors that traditional classifiers can process
  - Quick check question: Can you explain how categorical LLM responses (Yes/No/Unknown) are converted to numerical values for classification?

- Concept: Cross-validation and hyperparameter tuning
  - Why needed here: The paper uses 5-fold cross-validation with grid search to select the best classifier for each task
  - Quick check question: What is the purpose of using cross-validation when training classifiers on ICE-T feature vectors?

- Concept: Binary classification metrics (F1, precision, recall)
  - Why needed here: Performance is evaluated using micro F1 scores, which are particularly useful for imbalanced datasets
  - Quick check question: How does micro F1 differ from macro F1, and why is it more appropriate for imbalanced classification tasks?

## Architecture Onboarding

- Component map: Question generation module -> LLM prompt execution system -> Response verbalization component -> Traditional classifier training pipeline -> Inference engine
- Critical path: Question generation → LLM prompting → Response verbalization → Feature vector creation → Classifier prediction
- Design tradeoffs: Structured multi-prompt approach trades increased prompt engineering effort for improved performance and interpretability; uses smaller models with feature engineering instead of larger models with zero-shot prompting
- Failure signatures: Poor performance on tasks where secondary questions fail to capture task-relevant information; decreased interpretability when question set becomes too large or complex
- First 3 experiments:
  1. Implement basic ICE-T with 3 secondary questions on a simple binary classification task to verify feature vector creation works
  2. Compare zero-shot vs ICE-T performance on a single dataset using the same LLM to establish baseline improvement
  3. Test ICE-T with GPT-3.5 vs zero-shot GPT-4 on the same task to verify the "smaller model outperforms larger model" claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ICE-T scale with the number of secondary questions beyond 9, and what is the optimal number for balancing performance and computational efficiency?
- Basis in paper: Explicit - The paper mentions a sensitivity analysis up to 9 secondary questions but doesn't explore beyond this point.
- Why unresolved: The paper only tests up to 9 secondary questions in the sensitivity analysis, leaving open the question of how performance changes with even more questions.
- What evidence would resolve it: Additional experiments testing ICE-T performance with 10, 15, 20+ secondary questions across multiple datasets, measuring the point of diminishing returns in F1 score improvement.

### Open Question 2
- Question: How robust is ICE-T to variations in the quality and phrasing of secondary questions, and can automated question generation methods match expert-crafted questions?
- Basis in paper: Explicit - The paper notes that secondary questions crafted by experts generally lead to improved performance compared to those generated by LLMs, but uses LLM-generated questions in experiments.
- Why unresolved: The experiments use only LLM-generated questions without comparison to expert-crafted ones or testing robustness to question quality variations.
- What evidence would resolve it: Head-to-head comparison of ICE-T performance using expert-crafted vs. LLM-generated questions, plus experiments varying question quality/ambiguity while keeping other factors constant.

### Open Question 3
- Question: Can ICE-T be effectively extended to multi-class classification problems while maintaining interpretability and performance gains?
- Basis in paper: Explicit - The paper states ICE-T is specifically evaluated for binary classification but notes its applicability "potentially extends across a broad spectrum of scenarios."
- Why unresolved: All experiments and analysis focus exclusively on binary classification, leaving open whether the technique generalizes to multi-class problems.
- What evidence would resolve it: Experiments applying ICE-T to multi-class datasets with various numbers of classes, measuring performance and interpretability compared to baseline multi-class approaches.

## Limitations
- The exact nature of secondary questions for each task is not fully specified, making replication dependent on subjective prompt engineering quality
- Comparison between smaller and larger models assumes equivalent prompt quality, which may not hold across all domains
- The paper doesn't address potential biases introduced by the structured questioning approach

## Confidence
- **High confidence** in the core mechanism: Converting LLM responses to numerical features for classifier input is well-specified and directly verifiable
- **Medium confidence** in performance claims: While F1 improvements are consistently reported, the exact secondary questions and their quality are not fully disclosed
- **Medium confidence** in interpretability claims: The feature traceability is clear, but the actual interpretability benefit for domain experts is not empirically validated

## Next Checks
1. Replicate ICE-T on a single binary classification task using the provided clinical trials dataset, comparing against zero-shot prompting with the same model to verify the F1 improvement claim
2. Conduct ablation studies by varying the number of secondary questions (1-5) on a representative task to identify the optimal question set size and determine if performance plateaus
3. Implement a user study where domain experts evaluate the interpretability of ICE-T decisions versus black-box LLM outputs on the same classification tasks