---
ver: rpa2
title: How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study
arxiv_id: '2402.16061'
source_url: https://arxiv.org/abs/2402.16061
tags:
- knowledge
- uni00000013
- evidence
- llms
- probing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a layer-wise probing study to investigate how
  large language models (LLMs) encode context knowledge. The authors leverage ChatGPT
  to construct probing datasets with diverse and coherent evidence for various facts.
---

# How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study

## Quick Facts
- arXiv ID: 2402.16061
- Source URL: https://arxiv.org/abs/2402.16061
- Reference count: 0
- This paper conducts a layer-wise probing study to investigate how large language models (LLMs) encode context knowledge.

## Executive Summary
This paper investigates how large language models encode context knowledge through a layer-wise probing study. The authors construct probing datasets with diverse and coherent evidence using ChatGPT and employ V-usable information as the validation metric to measure LLM's capability to encode context knowledge across different layers. Through experiments on conflicting and newly acquired knowledge, the study reveals that LLMs prefer to encode more context knowledge in upper layers, primarily encode knowledge within entity tokens at lower layers while expanding to other tokens at upper layers, and gradually forget earlier context knowledge when provided with irrelevant evidence.

## Method Summary
The study constructs probing datasets using ChatGPT-generated evidence for various facts, including factual, counterfactual, and newly acquired knowledge. LLaMA 2 models (7B, 13B, 70B) are used to extract hidden layer representations. Linear probing classifiers are trained on these representations to predict knowledge categories, and V-usable information is computed to evaluate the encoding capability. The experiments analyze layer-wise patterns in how LLMs process and retain context knowledge, comparing knowledge-related entity tokens with other tokens and examining the effects of conflicting or irrelevant evidence.

## Key Results
- LLMs encode more context knowledge in upper layers than in lower layers
- Knowledge-related entity tokens encode context knowledge more easily at lower layers, while other tokens encode more knowledge at upper layers
- LLMs gradually forget earlier context knowledge when provided with irrelevant evidence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs encode more context knowledge in upper layers than in lower layers.
- **Mechanism:** Context knowledge is progressively refined and accumulated across layers through self-attention mechanisms. Early layers handle basic token representations, while upper layers integrate and differentiate between conflicting or newly acquired knowledge.
- **Core assumption:** Self-attention enables token representations to absorb and integrate contextual information from earlier layers.
- **Evidence anchors:**
  - [abstract]: "Our experiments on conflicting and newly acquired knowledge show that LLMs: (1) prefer to encode more context knowledge in the upper layers;"
  - [section]: "It is evident that each component of LLaMA 13B exhibits significantly higher V-information at upper layers, implying that context knowledge is encoded within their representations."
  - [corpus]: Weak or missing; neighboring papers focus on different probing angles, not specifically on layer-wise encoding trends.
- **Break condition:** If self-attention fails to effectively propagate context information, the upper layers would not show higher V-information.

### Mechanism 2
- **Claim:** Knowledge-related entity tokens encode context knowledge more easily at lower layers, while other tokens encode more knowledge at upper layers.
- **Mechanism:** Knowledge-related tokens are directly linked to the content of the knowledge and are processed early by the model. Other tokens gain knowledge indirectly through the attention mechanism as layers deepen.
- **Core assumption:** Self-attention allows non-knowledge tokens to accumulate contextual information from knowledge-related tokens.
- **Evidence anchors:**
  - [abstract]: "(2) primarily encode context knowledge within knowledge-related entity tokens at lower layers while progressively expanding more knowledge within other tokens at upper layers;"
  - [section]: "For knowledge-related entity tokens such as Mike and occupation, LLMs achieve high V-information at lower layers... To our surprise, the advantage gradually diminishes and is even surpassed by other tokens at upper layers."
  - [corpus]: Weak or missing; neighboring papers do not specifically address token-wise encoding differences.
- **Break condition:** If attention does not effectively transfer knowledge to non-entity tokens, the advantage of other tokens at upper layers would not be observed.

### Mechanism 3
- **Claim:** LLMs gradually forget earlier context knowledge when provided with irrelevant evidence.
- **Mechanism:** Irrelevant evidence interferes with the encoding of relevant knowledge, causing a degradation of V-information in intermediate layers.
- **Core assumption:** Irrelevant information is not encoded orthogonally and interferes with relevant knowledge encoding.
- **Evidence anchors:**
  - [abstract]: "(3) gradually forget the earlier context knowledge retained within the intermediate layers when provided with irrelevant evidence."
  - [section]: "The heatmap illustrates similar phenomena to conflicting knowledge scenarios... This discovery indicates that LLMs encode irrelevant evidence non-orthogonally, thus causing interference with the knowledge that has already been encoded."
  - [corpus]: Weak or missing; neighboring papers do not specifically address long-term memory degradation due to irrelevant evidence.
- **Break condition:** If LLMs could encode irrelevant and relevant information orthogonally, V-information would not degrade.

## Foundational Learning

- **Concept:** V-usable information
  - **Why needed here:** It is used as the validation metric to measure the capability of LLMs to encode context knowledge across different layers.
  - **Quick check question:** What is the advantage of using V-usable information over test set accuracy in probing tasks?

- **Concept:** Probing tasks
  - **Why needed here:** They are used to investigate the layer-wise capability of LLMs in encoding context knowledge by training classifiers on hidden layer representations.
  - **Quick check question:** How do probing tasks help in understanding the inner mechanisms of LLMs?

- **Concept:** Self-attention mechanism
  - **Why needed here:** It is crucial for understanding how LLMs transfer and integrate contextual information across layers.
  - **Quick check question:** How does the self-attention mechanism contribute to the encoding of context knowledge in LLMs?

## Architecture Onboarding

- **Component map:** ChatGPT evidence generation -> LLM (LLaMA 2) -> Hidden layer representation extraction -> Linear probing classifier -> V-usable information computation
- **Critical path:**
  1. Generate diverse evidence for facts/counterfactuals/newly acquired knowledge using ChatGPT.
  2. Feed evidence into the LLM and extract hidden layer representations.
  3. Train probing classifiers on these representations.
  4. Compute V-usable information to assess encoding capability.
- **Design tradeoffs:**
  - Using a linear classifier reduces extraneous interference but may limit the probing capability.
  - Focusing on the last token of questions simplifies the analysis but may miss nuances in other tokens.
  - Probing only chat models may not generalize to base models.
- **Failure signatures:**
  - Low V-information across all layers suggests the LLM is not encoding context knowledge effectively.
  - High V-information only in lower layers may indicate the LLM is relying on parametric memorization.
  - Inconsistent V-information patterns across tokens may suggest attention issues.
- **First 3 experiments:**
  1. Probe a simple fact (e.g., "London is the capital of England") and analyze layer-wise V-information.
  2. Compare V-information for knowledge-related entity tokens vs. other tokens in a question.
  3. Introduce irrelevant evidence and observe the degradation of V-information in intermediate layers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mathematical mechanism by which self-attention enables the transfer of context knowledge from knowledge-related entity tokens to other tokens at upper layers?
- Basis in paper: [explicit] The paper discusses how self-attention facilitates the transfer of information between token representations, leading to more context knowledge being captured by other tokens at upper layers.
- Why unresolved: The paper speculates on the role of self-attention but does not provide a detailed mathematical proof of the mechanism.
- What evidence would resolve it: A formal mathematical model demonstrating how self-attention weights evolve across layers and contribute to the observed phenomenon of knowledge transfer from entity tokens to other tokens.

### Open Question 2
- Question: How do different layer-wise components (Transformer layers, MLP layers, Attention layers) individually contribute to the encoding of context knowledge in LLMs?
- Basis in paper: [explicit] The paper mentions that each component of LLaMA 13B exhibits higher V-information at upper layers, but the role of Attention layers appears relatively chaotic.
- Why unresolved: The paper does not provide a detailed analysis of the individual contributions of each component to the overall encoding capability.
- What evidence would resolve it: Experiments isolating the contributions of each component by systematically disabling or modifying them and observing the impact on context knowledge encoding.

### Open Question 3
- Question: How does the encoding of context knowledge in LLMs change when exposed to long-term irrelevant evidence?
- Basis in paper: [explicit] The paper discusses how LLMs gradually forget earlier context knowledge when provided with irrelevant evidence, indicating non-orthogonal encoding of irrelevant information.
- Why unresolved: The paper does not explore the long-term effects of irrelevant evidence on the encoding of context knowledge beyond the initial stages.
- What evidence would resolve it: Longitudinal studies tracking the encoding of context knowledge in LLMs over extended periods with continuous exposure to irrelevant evidence, measuring the decay rate of knowledge retention.

## Limitations

- The exact implementation details for computing V-usable information are not fully specified, introducing uncertainty in reproducing the results
- The use of ChatGPT-generated datasets may introduce biases or inconsistencies in the probing data that are not fully characterized
- The study focuses primarily on LLaMA models and does not investigate how these findings generalize to other LLM architectures or training paradigms

## Confidence

**High Confidence:** The finding that upper layers encode more context knowledge than lower layers is well-supported by the experimental evidence and aligns with established understanding of transformer architectures.

**Medium Confidence:** The claim about knowledge-related entity tokens encoding context knowledge more easily at lower layers is supported by the data, though the exact mechanism and why other tokens eventually surpass entity tokens at upper layers requires further investigation.

**Medium Confidence:** The observation that LLMs gradually forget earlier context knowledge when provided with irrelevant evidence is demonstrated, but the practical implications and the exact nature of this interference effect need more exploration.

## Next Checks

1. **Probe alternative architectures:** Test the same layer-wise probing methodology on different LLM architectures (e.g., GPT models, BERT variants) to verify if the observed patterns are architecture-agnostic.

2. **Investigate metric sensitivity:** Systematically vary the parameters and implementation details of V-usable information computation to assess its sensitivity and ensure the observed patterns are robust to metric choices.

3. **Control for dataset bias:** Conduct ablation studies where the ChatGPT-generated evidence is controlled for specific linguistic patterns or content biases to isolate the effect of evidence diversity on the observed encoding patterns.