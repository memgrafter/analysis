---
ver: rpa2
title: 'Just Say What You Want: Only-prompting Self-rewarding Online Preference Optimization'
arxiv_id: '2409.17534'
source_url: https://arxiv.org/abs/2409.17534
tags:
- preference
- arxiv
- responses
- preprint
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of online Reinforcement Learning
  from Human Feedback (RLHF) for smaller language models, where existing self-rewarding
  approaches rely on discriminator judgment capabilities that are less effective for
  smaller models. The authors propose a novel online only-prompting self-rewarding
  alignment framework that leverages the generation capability of language models
  to create preference datasets without needing a discriminator.
---

# Just Say What You Want: Only-prompting Self-rewarding Online Preference Optimization

## Quick Facts
- arXiv ID: 2409.17534
- Source URL: https://arxiv.org/abs/2409.17534
- Reference count: 7
- The paper proposes a novel online only-prompting self-rewarding alignment framework that achieves 34.5% in the Length-controlled Win Rates of AlpacaEval 2.0 for Mistral-7B models.

## Executive Summary
This paper addresses the challenge of online Reinforcement Learning from Human Feedback (RLHF) for smaller language models, where existing self-rewarding approaches relying on discriminator judgment capabilities are less effective. The authors propose a novel only-prompting self-rewarding alignment framework that leverages the generation capability of language models to create preference datasets without needing a discriminator. By using different prefixes to guide the model to generate both positive (high-quality) and negative (low-quality) responses, with fine-grained arithmetic control over the optimality gap between them, the method generates more challenging negative examples in later training stages, helping the model better capture subtle human preferences.

## Method Summary
The proposed method is an iterative online training framework that operates in three main stages: (1) Initialization with supervised fine-tuning and offline DPO training, (2) Iterative generation of preference datasets using paired prompts with different scores (chosen prefix with score 10, rejected prefix with progressively increasing scores of 3, 5, 7 across iterations), and (3) Application of DPO training with fine-grained arithmetic control over the optimality gap between positive and negative examples. The approach uses length-normalized rewards and targets a margin between response scores, progressively reducing the gap between chosen and rejected responses to capture subtle human preferences more effectively.

## Key Results
- Achieves 34.5% in the Length-controlled Win Rates of AlpacaEval 2.0, surpassing multiple baseline methods
- Outperforms existing methods on both Mistral-7B and Mistral-Instruct-7B models
- Demonstrates effectiveness of fine-grained arithmetic control over optimality gap between positive and negative examples
- Shows that only-prompting approach generates more challenging negative examples in later training stages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The generation capability of smaller language models can be leveraged to create preference datasets without needing a discriminator.
- Mechanism: By defining two distinct prefixes - one representing high-quality outcomes and the other low-quality outcomes - the model is guided to produce corresponding responses. The high-scoring responses are labeled as "chosen" and the low-scoring responses as "rejected."
- Core assumption: Smaller models' generation capability is sufficient to produce meaningfully different quality responses when guided by different prefixes, even if their judgment capability is limited.
- Evidence anchors:
  - [abstract]: "we propose a novel, only-prompting self-rewarding online algorithm that generates preference datasets without relying on judgment capabilities"
  - [section 3.3]: "we explicitly define a response score for each response, with higher scores indicating better quality. We design two different prompts to leverage the generative ability for eliciting both positive and negative examples."
  - [corpus]: Weak evidence - no direct corpus support found for this specific mechanism
- Break condition: If the model cannot generate meaningfully different quality responses based on prefix instructions, the approach fails. This could happen if the model's generation capability is too weak or if the prefixes are not effective at guiding quality differentiation.

### Mechanism 2
- Claim: Fine-grained arithmetic control over the optimality gap between positive and negative examples improves alignment with complex human preferences.
- Mechanism: The quality gap between chosen and rejected responses is progressively reduced during training. Early in training, large gaps are beneficial. Later, smaller gaps force the model to detect subtle differences, improving alignment with nuanced preferences.
- Core assumption: Gradually reducing the gap between positive and negative examples forces the model to learn more subtle distinctions rather than just obvious differences.
- Evidence anchors:
  - [abstract]: "we employ fine-grained arithmetic control over the optimality gap between positive and negative examples, generating more hard negatives in the later stages of training to help the model better capture subtle human preferences"
  - [section 3.4]: "we reduce the gap between pair-wise data in the later stages of training. Specifically, we progressively increase the rejected score of the rejected prefix pr"
  - [section 4.3]: "Human preferences are complex and difficult to model with simple methods. Gradually reducing the gap during training helps the model focus on hard negatives, allowing it to better capture the nuances of human preferences."
- Break condition: If reducing the gap too much or too quickly causes the model to lose the ability to distinguish between positive and negative examples, the approach fails. There's likely an optimal schedule for gap reduction.

### Mechanism 3
- Claim: The theoretical framework proves that responses generated with different reward scores exhibit quality differences that can be used for preference optimization.
- Mechanism: Mathematical proof shows that a policy optimized for high reward scores (πgood) produces higher quality responses than one optimized for lower reward scores (πbad), creating a quality gap exploitable for preference learning.
- Core assumption: The reward function and policy optimization framework create a measurable quality difference between responses generated under different scoring conditions.
- Evidence anchors:
  - [section 3.3.1]: "Proposition 1 (Quality gap between responses). Given the instruction x and different reward scores, the policy can generate responses of varying quality."
  - [section 3.3.1]: "Since β > 0, πgood ≠ πbad, and J(π) is a strongly convex function with respect to π, we obtain J(πgood) > J (πbad). From Equation 15, we observe a quality gap in the objective function between πgood and πbad."
  - [corpus]: No direct corpus evidence found for this theoretical claim
- Break condition: If the theoretical assumptions about reward functions and policy optimization don't hold in practice, or if the quality gap is too small to be useful for training, the approach fails.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper builds on RLHF methodology but addresses its limitations for smaller models
  - Quick check question: What are the main differences between online and offline RLHF approaches?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: The method uses DPO as the optimization algorithm after generating preference datasets
  - Quick check question: How does DPO differ from traditional RLHF methods that use reward models?

- Concept: Policy gradient methods and KL regularization
  - Why needed here: The theoretical framework relies on understanding policy optimization with KL regularization
  - Quick check question: What role does the KL divergence term play in the RLHF objective function?

## Architecture Onboarding

- Component map:
  Base language model -> Prefix generation system -> Preference dataset generator -> DPO training loop -> Iterative training framework

- Critical path:
  1. Initialize model with SFT and offline DPO
  2. Generate preference dataset using prefixes
  3. Apply DPO training with the generated dataset
  4. Update reference model and repeat with adjusted prefix scores

- Design tradeoffs:
  - Prefix design vs. generation quality: Simpler prefixes are easier to implement but may produce less distinct quality differences
  - Gap reduction schedule: Faster reduction may lead to quicker learning but risks losing useful signal
  - Model size vs. generation capability: Smaller models are more efficient but may struggle to generate high-quality distinctions

- Failure signatures:
  - Narrow score distribution in generated responses (indicating inability to create meaningful quality differences)
  - Plateau in AlpacaEval 2.0 scores despite multiple iterations
  - Large gap between chosen and rejected response scores that doesn't decrease over training

- First 3 experiments:
  1. Test prefix effectiveness: Generate responses with different prefix scores and measure the quality difference using an open reward model
  2. Validate gap control: Run training with fixed vs. adaptive gap schedules and compare performance
  3. Ablation study: Compare results with and without arithmetic gap control to confirm its importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the only-prompting self-rewarding approach scale with model size beyond Mistral-7B?
- Basis in paper: [explicit] The paper evaluates the method on Mistral-7B and Mistral-Instruct-7B, showing significant improvements, but does not explore larger or smaller models
- Why unresolved: The study is limited to two specific model sizes, and the authors note that smaller models have weaker judgment capabilities, suggesting potential scaling issues
- What evidence would resolve it: Systematic evaluation across a range of model sizes (e.g., 1B, 3B, 13B, 33B) comparing performance and efficiency metrics would clarify scalability

### Open Question 2
- Question: What is the optimal schedule for adjusting the optimality gap between positive and negative examples during training?
- Basis in paper: [explicit] The authors describe a heuristic approach (scores of 3, 5, 7 across iterations) but note this requires further study
- Why unresolved: The current approach is based on empirical observation rather than theoretical optimization, and the authors acknowledge the need for better understanding of gap scheduling
- What evidence would resolve it: Comparative studies testing different gap schedules (linear, exponential, adaptive) and their impact on convergence speed and final performance would identify optimal strategies

### Open Question 3
- Question: How does the quality of preference datasets generated by the only-prompting approach compare to those generated using discriminator-based methods?
- Basis in paper: [inferred] The authors claim their method generates high-quality preference datasets without discriminators, but do not provide direct quantitative comparisons
- Why unresolved: While the authors demonstrate good downstream performance, they do not measure the intrinsic quality of the generated preference data or compare it to alternative generation methods
- What evidence would resolve it: Direct comparison of preference dataset quality metrics (e.g., inter-annotator agreement, preference consistency) between the only-prompting method and discriminator-based approaches would provide objective assessment

## Limitations
- Theoretical foundation relies heavily on assumptions about reward function differences translating to measurable quality gaps
- Approach assumes base model's generation capability is sufficient to produce meaningfully different quality responses when guided by different prefixes
- Arithmetic control mechanism for optimality gap reduction requires careful tuning that may be dataset-specific
- Performance relative to traditional discriminator-based approaches for larger models is not directly compared

## Confidence
- **High Confidence**: The experimental results showing significant improvements on AlpacaEval 2.0 and MT-Bench benchmarks for Mistral-7B models
- **Medium Confidence**: The theoretical framework proving quality gaps exist between responses generated under different reward scores
- **Medium Confidence**: The mechanism that fine-grained arithmetic control over optimality gaps improves alignment with subtle human preferences
- **Low Confidence**: The generalizability of prefix effectiveness across different domains and tasks beyond instruction following

## Next Checks
1. **Cross-model validation**: Test the approach on significantly smaller language models (e.g., 1-3B parameters) to determine the minimum model size required for effective only-prompting self-rewarding alignment.
2. **Ablation study on gap control**: Systematically vary the optimality gap reduction schedule across multiple training runs to identify optimal parameters and test sensitivity to different schedules.
3. **Discriminator comparison**: Implement a traditional discriminator-based self-rewarding approach on the same Mistral-7B models to directly compare performance, efficiency, and quality of generated preference datasets.