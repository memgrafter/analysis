---
ver: rpa2
title: 'A Large-Scale Study of Relevance Assessments with Large Language Models: An
  Initial Look'
arxiv_id: '2411.08275'
source_url: https://arxiv.org/abs/2411.08275
tags:
- manual
- umbrela
- relevance
- fully
- topics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the use of large language models (LLMs) for
  relevance assessment in information retrieval evaluation, specifically comparing
  three LLM-assisted approaches against traditional manual assessment in the TREC
  2024 RAG Track. Using the UMBRELA tool to automatically generate relevance judgments,
  the researchers found that system rankings derived from LLM-generated assessments
  correlate highly with those from manual assessments (Kendall's tau of 0.89 for nDCG@20),
  suggesting that automatic assessments can effectively replace manual ones.
---

# A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look

## Quick Facts
- arXiv ID: 2411.08275
- Source URL: https://arxiv.org/abs/2411.08275
- Reference count: 35
- Primary result: LLM-generated relevance assessments correlate highly with manual assessments (Kendall's tau 0.89) for system ranking in TREC 2024 RAG Track

## Executive Summary
This study investigates the use of large language models (LLMs) for relevance assessment in information retrieval evaluation, comparing three LLM-assisted approaches against traditional manual assessment. Using the UMBRELA tool with GPT-4o, the researchers found that automatically generated relevance judgments correlate highly with manual assessments at the system level, suggesting LLMs can effectively replace manual assessment for academic IR evaluations. Surprisingly, they discovered that human involvement in post-editing or filtering LLM judgments does not increase correlation with manual assessments, indicating no clear benefit from hybrid approaches despite higher costs. The study also found that human assessors tend to apply stricter relevance criteria than UMBRELA, with the LLM sometimes making unwarranted inferences about document relevance.

## Method Summary
The study compares four assessment conditions on the TREC 2024 RAG Track: fully manual assessment, manual assessment with LLM filtering, manual assessment with LLM post-editing, and fully automatic UMBRELA assessment. UMBRELA uses recursive DNA prompting to decompose relevance assessment into intent matching, trustworthiness, and final score. The evaluation uses 301 topics from the TREC 2024 RAG Track, 77 retrieval runs from 19 teams, and the MS MARCO V2.1 deduped segment collection. System rankings are compared using Kendall's tau correlation across nDCG@20, nDCG@100, and Recall@100 metrics.

## Key Results
- UMBRELA-generated relevance assessments correlate highly with manual assessments (Kendall's tau 0.89 for nDCG@20)
- Human post-editing or filtering of LLM judgments does not increase correlation with manual assessments
- Human assessors apply stricter relevance criteria than UMBRELA, which sometimes makes unwarranted inferences
- Fully automatic LLM assessment is more cost-effective than hybrid approaches without sacrificing correlation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UMBRELA judgments correlate highly with manual assessments at run level
- Mechanism: UMBRELA uses recursive DNA prompting to decompose relevance assessment into intent matching, trustworthiness, and final score, enabling consistent scoring across topics
- Core assumption: The LLM can reliably identify relevance through structured decomposition of the assessment task
- Evidence anchors:
  - [abstract] "system rankings induced by automatically generated relevance assessments from UMBRELA correlate highly with those induced by fully manual assessments across a diverse set of 77 runs from 19 teams"
  - [section] "We find that system rankings induced by automatically generated relevance assessments using UMBRELA correlate highly with those induced by manual assessments in terms of nDCG@20, nDCG@100, and Recall@100"
- Break condition: If LLM's interpretation of relevance criteria systematically diverges from human assessors, correlation will degrade especially for nuanced queries

### Mechanism 2
- Claim: LLM assistance does not increase correlation with fully manual assessments
- Mechanism: Human post-editing and filtering don't improve correlation because assessors may introduce variance rather than reduce it
- Core assumption: Human assessors have systematic biases that persist regardless of LLM guidance
- Evidence anchors:
  - [abstract] "surprisingly, we find that LLM assistance does not appear to increase correlation with fully manual assessments"
  - [section] "human involvement in post-editing or filtering LLM judgments did not increase correlation with manual assessments, indicating no clear benefit from hybrid approaches"
- Break condition: If human assessors have domain expertise that corrects systematic LLM errors, hybrid approaches might outperform pure LLM methods

### Mechanism 3
- Claim: Human assessors apply stricter relevance criteria than UMBRELA
- Mechanism: UMBRELA makes broader inferences about document relevance that human assessors reject as unwarranted
- Core assumption: LLMs are more permissive in drawing connections between query intent and document content
- Evidence anchors:
  - [abstract] "Overall, human assessors appear to be stricter than UMBRELA in applying relevance criteria"
  - [section] "analyses suggest that assessors apply stricter relevance criteria than UMBRELA. We find cases where the LLM draws inferences that humans would consider unwarranted"
- Break condition: If UMBRELA's inference mechanisms are tuned to match human strictness, this systematic difference may diminish

## Foundational Learning

- Concept: Rank correlation metrics (Kendall's tau)
  - Why needed here: The study uses Kendall's tau to compare system rankings derived from different assessment methods
  - Quick check question: What does a Kendall's tau of 0.89 between two ranking lists indicate about their similarity?

- Concept: Pooling methodology in IR evaluation
  - Why needed here: The study uses depth-20 pooling from 77 runs to create assessment pools
  - Quick check question: Why is pooling depth important for relevance assessment coverage and cost efficiency?

- Concept: nDCG and Recall metrics
  - Why needed here: The study evaluates correlation across nDCG@20, nDCG@100, and Recall@100 metrics
  - Quick check question: How does evaluating at different cutoffs (20 vs 100) affect the stability of relevance assessments?

## Architecture Onboarding

- Component map: UMBRELA pipeline includes prompt engineering, LLM inference, pool construction, and correlation analysis components
- Critical path: LLM inference → judgment aggregation → correlation computation with manual assessments
- Design tradeoffs: Fully automatic vs hybrid approaches trade cost against potential quality improvements that the study found not to materialize
- Failure signatures: Low correlation indicates systematic divergence in relevance criteria; high variance across topics suggests instability
- First 3 experiments:
  1. Run correlation analysis on subset of topics (10-20) to validate scalability claims
  2. Test different prompting strategies to see if correlation improves
  3. Compare inter-annotator agreement among human assessors to LLM-human agreement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reliability of LLM-based relevance assessments compare to human inter-annotator agreement rates in the same evaluation context?
- Basis in paper: [explicit] The paper notes that it lacks a comparison between two human assessors, stating "we are missing an important point of reference, as the divergence between human and LLM-assisted processes needs to be compared to human-human inter-annotator agreement."
- Why unresolved: The study only compared LLM assessments against a single human assessment per topic, not measuring how much humans disagree with each other.
- What evidence would resolve it: Conducting the same evaluation with multiple independent human assessors for the same topics to establish baseline inter-annotator agreement rates.

### Open Question 2
- Question: What is the optimal number of topics needed for LLM-based relevance assessments to achieve reliable system rankings?
- Basis in paper: [explicit] The paper asks "How many topics do we need?" and conducts experiments showing rank correlations decrease with fewer topics, but doesn't establish a definitive minimum.
- Why unresolved: While the paper provides evidence that reasonable correlations can be achieved with ~10 topics, it doesn't determine the precise threshold for reliable evaluation.
- What evidence would resolve it: Systematic experiments varying topic counts and measuring rank correlation stability to identify the minimum number of topics required.

### Open Question 3
- Question: Can prompting strategies be optimized to reduce the systematic differences between human and LLM relevance judgments?
- Basis in paper: [inferred] The paper notes that "UMBRELA often finds a passage to be more relevant than the NIST assessor" and suggests this may be due to the LLM making unwarranted inferences, while acknowledging that "we can perhaps alter LLM behavior with better prompting."
- Why unresolved: The study used a fixed prompting approach (UMBRELA) without exploring whether different prompts could produce judgments more aligned with human assessors.
- What evidence would resolve it: Comparative experiments testing multiple prompting strategies to identify approaches that minimize systematic differences between human and LLM judgments.

## Limitations
- The study relies on a single dataset (TREC 2024 RAG Track) and one LLM (GPT-4o), limiting generalizability
- The finding that hybrid approaches don't improve correlation could be specific to the UMBRELA implementation or particular human assessors
- The study doesn't examine long-term stability of LLM assessments or how well they transfer across different information needs

## Confidence
- High confidence: The strong correlation (Kendall's tau 0.89) between UMBRELA and manual assessments is well-supported by the experimental results
- Medium confidence: The conclusion that LLM assistance doesn't improve correlation with manual assessments is supported but could be context-dependent
- Medium confidence: The finding that humans are stricter than UMBRELA is based on qualitative analysis but would benefit from more systematic quantification

## Next Checks
1. Test UMBRELA's correlation performance on different IR tasks (web search, academic search, etc.) to assess generalizability beyond the RAG Track domain
2. Compare UMBRELA performance using different LLM models (Claude, Llama, etc.) to determine if the correlation advantage is model-specific
3. Conduct a controlled experiment where hybrid approaches are explicitly designed to correct known LLM biases to test whether systematic human intervention can improve correlation