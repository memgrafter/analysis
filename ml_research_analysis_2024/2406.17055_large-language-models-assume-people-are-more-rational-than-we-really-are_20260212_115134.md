---
ver: rpa2
title: Large Language Models Assume People are More Rational than We Really are
arxiv_id: '2406.17055'
source_url: https://arxiv.org/abs/2406.17055
tags:
- human
- llms
- people
- decisions
- choice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large Language Models (LLMs) show a systematic bias toward assuming\
  \ human decision-making is more rational than it actually is. Across three forward\
  \ modeling tasks\u2014predicting individual choices, aggregate behavior, and simulating\
  \ human decisions\u2014LLMs (GPT-4o, GPT-4-Turbo, Llama-3-8B/70B, Claude 3 Opus)\
  \ align more closely with rational choice models (expected value theory) than with\
  \ real human choices."
---

# Large Language Models Assume People are More Rational than We Really are
## Quick Facts
- arXiv ID: 2406.17055
- Source URL: https://arxiv.org/abs/2406.17055
- Reference count: 23
- LLMs systematically overestimate human rationality in decision-making tasks

## Executive Summary
Large Language Models (LLMs) exhibit a systematic bias toward assuming human decision-making is more rational than it actually is. Across three distinct forward modeling tasks and inverse inference tasks, LLMs consistently align more closely with rational choice models than with real human choices. The study demonstrates that while LLMs can effectively simulate rational decision-making, they fail to capture the systematic irrationalities that characterize actual human behavior, suggesting their internal models of human decision-making are fundamentally shaped by assumptions of rationality.

## Method Summary
The study evaluated multiple state-of-the-art LLMs (GPT-4o, GPT-4-Turbo, Llama-3-8B/70B, Claude 3 Opus) across three forward modeling tasks: predicting individual choices, predicting aggregate behavior, and simulating human decision-making. These tasks used established behavioral economics datasets to compare LLM predictions against both rational choice theory and actual human decisions. The researchers measured correlation with expected value theory (representing rational choice) versus correlation with actual human choices. Additionally, they conducted inverse modeling tasks where LLMs inferred human preferences from observed choices, comparing these inferences against both rational models and human inferences from the same data.

## Key Results
- GPT-4o achieves Spearman correlation of 0.94 with expected value but only 0.48 with actual human decisions
- Across all tested LLMs, correlations with rational choice models consistently exceed correlations with real human behavior
- In inverse modeling tasks, LLM inferences correlate highly with human inferences (up to 0.97) while both deviate from true human behavior

## Why This Works (Mechanism)
LLMs' systematic alignment with rational choice models appears to stem from their training data composition, which likely contains more examples of rational decision-making patterns than descriptions of systematic human biases. The models may be optimizing for coherence and logical consistency during training, inadvertently reinforcing rational decision patterns. This creates a feedback loop where the models' internal representations of human decision-making become increasingly divorced from the actual irrationalities that characterize human behavior.

## Foundational Learning
**Expected Value Theory**: Mathematical framework for rational decision-making based on probability-weighted outcomes. Needed to establish the rational baseline against which human irrationality can be measured. Quick check: Can the model correctly compute expected values for simple probabilistic choices.

**Behavioral Economics Datasets**: Collections of real human decision-making data showing systematic deviations from rationality. Needed to provide ground truth for realistic human behavior. Quick check: Do the datasets contain clear examples of classic biases like loss aversion and framing effects.

**Forward Modeling**: Predicting human choices based on a model of decision-making. Needed to test whether LLMs can accurately simulate human behavior. Quick check: Can the model predict individual choices with accuracy exceeding random guessing.

**Inverse Modeling**: Inferring underlying preferences from observed choices. Needed to understand how LLMs reason about human decision-making processes. Quick check: Does the model's inferred preference structure match known human biases when given realistic choice data.

## Architecture Onboarding
**Component Map**: Input prompt -> LLM inference -> Correlation calculation -> Comparison with rational/behavioral baselines
**Critical Path**: Decision scenario presentation -> LLM response generation -> Statistical alignment measurement -> Benchmark comparison
**Design Tradeoffs**: Computational efficiency of correlation calculations versus depth of qualitative analysis of decision patterns
**Failure Signatures**: High correlation with rational models but low correlation with human choices indicates systematic rationality bias
**First Experiments**: 1) Test correlation patterns on out-of-distribution decision contexts 2) Evaluate fine-tuned versions on human bias datasets 3) Compare qualitative decision patterns between LLM and human choices

## Open Questions the Paper Calls Out
None

## Limitations
- Human irrationality baseline derived from limited experimental datasets that may not represent real-world complexity
- Evaluation focuses on aggregate statistical alignment rather than qualitative assessment of specific decision patterns
- Does not examine whether LLMs can be fine-tuned to better capture realistic human biases

## Confidence
- High: Core finding that LLMs systematically overestimate human rationality is robust across multiple models and tasks
- Medium: Claim about LLMs' "internal models" is supported by correlation patterns but cannot be directly verified

## Next Checks
1. Test whether fine-tuning LLMs on datasets specifically containing human biases improves alignment with realistic human decision patterns
2. Conduct qualitative error analysis comparing specific cases where LLMs choose rationally versus where humans choose irrationally
3. Evaluate LLM predictions on out-of-distribution decision scenarios to test generalizability of the rationality bias