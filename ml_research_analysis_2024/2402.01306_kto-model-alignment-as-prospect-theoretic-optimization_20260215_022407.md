---
ver: rpa2
title: 'KTO: Model Alignment as Prospect Theoretic Optimization'
arxiv_id: '2402.01306'
source_url: https://arxiv.org/abs/2402.01306
tags:
- data
- loss
- value
- human
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KTO, a human-aware loss function for aligning
  language models with human preferences using Kahneman-Tversky's prospect theory.
  KTO directly maximizes the utility of generations by incorporating a Kahneman-Tversky
  value function, rather than maximizing the log-likelihood of preferences like existing
  methods (e.g., DPO).
---

# KTO: Model Alignment as Prospect Theoretic Optimization

## Quick Facts
- arXiv ID: 2402.01306
- Source URL: https://arxiv.org/abs/2402.01306
- Reference count: 29
- One-line primary result: KTO matches or exceeds DPO performance at scales from 1B to 30B parameters using only binary desirable/undesirable signals

## Executive Summary
This paper introduces KTO (Kahneman-Tversky Optimization), a human-aware loss function for aligning language models with human preferences using prospect theory. Unlike existing methods that maximize the log-likelihood of preferences, KTO directly maximizes the utility of generations by incorporating a Kahneman-Tversky value function. The key innovation is that KTO only requires a binary signal of whether an output is desirable or undesirable, making it more practical in real-world settings where preference data is scarce and expensive to collect. Experimental results show that KTO matches or exceeds the performance of preference-based methods like DPO across different model scales, despite learning from a weaker signal.

## Method Summary
KTO is a human-aware loss function that aligns language models with human preferences by directly optimizing the utility of generations using Kahneman-Tversky's prospect theory. The method requires only binary signals indicating whether outputs are desirable or undesirable, rather than pairwise preference data. KTO uses a value function that models human biases like loss aversion and diminishing sensitivity to calculate the utility of each generation. The loss function incorporates a KL penalty to prevent the model from drifting too far from its supervised fine-tuning baseline. KTO can work with or without an initial supervised fine-tuning stage and demonstrates robustness to data imbalances.

## Key Results
- KTO matches or exceeds DPO performance at scales from 1B to 30B parameters
- KTO only requires binary signals (desirable/undesirable) versus preference pairs
- KTO shows greater robustness to data imbalances and can work without supervised finetuning

## Why This Works (Mechanism)

### Mechanism 1
KTO matches or exceeds DPO performance by directly maximizing human utility instead of maximizing the log-likelihood of preferences. The Kahneman-Tversky value function models human biases (loss aversion, diminishing sensitivity) which are implicitly present in human feedback. KTO uses this value function to directly optimize the utility of generations rather than maximizing preference likelihood. Core assumption: The Kahneman-Tversky value function accurately captures human perception of language model outputs.

### Mechanism 2
KTO handles noisy and intransitive preference data better than existing methods. KTO does not learn from undesirable examples with sufficiently high rewards or desirable examples with sufficiently low rewards, allowing it to ignore unlearnable preferences and reduce the impact of noise. Core assumption: Human preferences are often noisy and intransitive.

### Mechanism 3
KTO can match DPO performance despite learning from weaker binary signals. The strong inductive bias in KTO's loss function compensates for the weaker signal. The KL penalty forces the model to learn exactly what makes an output desirable rather than making generic improvements. Core assumption: A strong inductive bias can compensate for weaker signals.

## Foundational Learning

- Concept: Prospect Theory and Human Biases
  - Why needed here: KTO is based on Kahneman-Tversky's prospect theory which models how humans perceive random variables in a biased but well-defined manner
  - Quick check question: What are the key properties of the Kahneman-Tversky value function and how do they relate to human decision-making?

- Concept: Human-Aware Loss Functions (HALOs)
  - Why needed here: KTO is a type of HALO that incorporates human biases into the loss function. Understanding HALOs helps explain why KTO works
  - Quick check question: What are the key characteristics that define a human-aware loss function?

- Concept: Preference Optimization vs Direct Utility Optimization
  - Why needed here: KTO differs from methods like DPO in that it directly optimizes human utility rather than maximizing preference likelihood
  - Quick check question: What is the theoretical difference between optimizing for preference likelihood and optimizing for human utility?

## Architecture Onboarding

- Component map: Input prompts -> Pretrained language model (1B-30B parameters) -> KTO optimization -> Aligned language model

- Critical path:
  1. Load pretrained model and SFT model
  2. Load alignment data (binary desirable/undesirable labels)
  3. Initialize KTO optimizer with appropriate hyperparameters (β, λD, λU)
  4. Train using KTO loss
  5. Evaluate aligned model

- Design tradeoffs:
  - KTO vs DPO: KTO uses binary signals vs preference pairs, requires careful tuning of λD/λU
  - Binary vs preference data: Binary data is more abundant but potentially noisier
  - KL penalty: Controls how far model can drift from SFT model

- Failure signatures:
  - Training instability: May indicate incorrect λD/λU settings
  - Poor performance: May indicate insufficient model capacity or incorrect β setting
  - Mode collapse: May indicate overly aggressive KL penalty

- First 3 experiments:
  1. Compare KTO with baseline (DPO) on a small dataset to verify implementation
  2. Test KTO's robustness to data imbalance by varying the ratio of desirable/undesirable examples
  3. Evaluate KTO's performance when skipping SFT stage to verify it can work independently

## Open Questions the Paper Calls Out

Open Question 1: How does KTO's performance compare to other alignment methods when the data is not in preference format?
Basis in paper: Inferred - The paper mentions that KTO can handle data that is not in preference format, but does not provide a direct comparison with other methods.
Why unresolved: The paper focuses on comparing KTO to other methods using preference data, and does not explore its performance with other data formats.
What evidence would resolve it: Experiments comparing KTO's performance with other alignment methods using data that is not in preference format.

Open Question 2: How does KTO's performance scale with model size beyond 30B parameters?
Basis in paper: Inferred - The paper only reports results up to 30B parameters, and does not discuss scalability beyond this point.
Why unresolved: The paper does not explore the performance of KTO on larger models.
What evidence would resolve it: Experiments evaluating KTO's performance on models larger than 30B parameters.

Open Question 3: How does KTO's performance compare to other alignment methods when the data is highly imbalanced?
Basis in paper: Inferred - The paper mentions that KTO can handle imbalanced data, but does not provide a direct comparison with other methods.
Why unresolved: The paper focuses on comparing KTO to other methods using balanced data, and does not explore its performance with imbalanced data.
What evidence would resolve it: Experiments comparing KTO's performance with other alignment methods using imbalanced data.

Open Question 4: How does KTO's performance compare to other alignment methods when the data is sourced from different domains?
Basis in paper: Inferred - The paper only reports results using data from a specific domain (HH, SHP, OASST), and does not explore its performance with data from other domains.
Why unresolved: The paper does not explore the generalizability of KTO to different domains.
What evidence would resolve it: Experiments evaluating KTO's performance using data from different domains.

## Limitations

- The paper's claims about KTO's superiority rely on several assumptions that require further validation
- Empirical evidence is limited to specific model scales (1B-30B) and may not generalize to larger models
- The binary signal approach may lose important information compared to pairwise preferences

## Confidence

**High Confidence**: The theoretical foundation of KTO based on Kahneman-Tversky utility functions is well-established in behavioral economics. The mathematical formulation of the loss function is clearly specified and internally consistent.

**Medium Confidence**: The claim that KTO can match or exceed DPO performance across different model scales is supported by experimental results, but the sample size and diversity of evaluation tasks could be larger. The robustness to data imbalance is demonstrated but may depend on specific hyperparameter settings.

**Low Confidence**: The assertion that KTO handles noisy and intransitive preference data better than existing methods lacks direct empirical validation. The claim about being able to skip supervised fine-tuning in some cases is based on limited evidence.

## Next Checks

1. **Cross-Domain Robustness Test**: Evaluate KTO on specialized domains (medical, legal, technical) where human preferences may deviate from standard prospect-theoretic patterns to verify the general applicability of the Kahneman-Tversky value function.

2. **Large-Scale Comparison**: Test KTO on models larger than 30B parameters and compare performance across multiple orders of magnitude to establish scalability limits and identify any emerging failure modes.

3. **Noise Tolerance Experiment**: Systematically inject varying levels of noise into preference data (including controlled amounts of intransitive preferences) and measure how KTO's performance degrades compared to DPO and other baselines.