---
ver: rpa2
title: Contrastive Language Prompting to Ease False Positives in Medical Anomaly Detection
arxiv_id: '2411.07546'
source_url: https://arxiv.org/abs/2411.07546
tags:
- attention
- 'false'
- clap
- positive
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses false positives in medical anomaly detection
  using contrastive language-image models like CLIP. The proposed Contrastive LAnguage
  Prompting (CLAP) method uses both positive and negative text prompts to improve
  attention on lesion regions while suppressing attention on normal tissue.
---

# Contrastive Language Prompting to Ease False Positives in Medical Anomaly Detection

## Quick Facts
- arXiv ID: 2411.07546
- Source URL: https://arxiv.org/abs/2411.07546
- Authors: YeongHyeon Park; Myung Jin Kim; Hyeong Seok Kim
- Reference count: 0
- Primary result: CLAP improves AUC-ROC scores to 78.89 compared to 77.23 for positive prompts alone

## Executive Summary
This paper addresses the persistent challenge of false positives in medical anomaly detection using contrastive language-image models like CLIP. The proposed Contrastive LAnguage Prompting (CLAP) method introduces a dual-prompt approach that uses both positive and negative text prompts to guide attention toward lesion regions while suppressing attention on normal tissue. By computing the difference between positive and negative attention maps, the method effectively reduces false positive detections while maintaining sensitivity to actual anomalies. The approach was evaluated on the BMAD dataset across six medical imaging benchmarks, demonstrating improved performance over single-prompt methods.

## Method Summary
The CLAP method uses BioMedCLIP to generate attention maps from paired positive and negative text prompts. The attention difference (ACLAP = Apositive - Anegative) highlights regions of interest while suppressing normal tissue attention. Regions exceeding a Q3 threshold (μ + 0.674σ) are obfuscated using a mosaic process and passed to a U-Net trained only on normal samples. Anomaly detection is performed by measuring reconstruction error of these obfuscated regions, with higher errors indicating potential anomalies. The method requires no labeled anomaly data and operates in a zero-shot manner.

## Key Results
- CLAP achieves average AUC-ROC of 78.89 across six BMAD benchmarks
- Outperforms positive language prompting alone (77.23 AUC-ROC)
- Demonstrates particular effectiveness on datasets with small, irregular patterns
- Successfully reduces false positives by suppressing attention on normal tissue regions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using both positive and negative text prompts in CLIP attention reduces false positives in medical anomaly detection by explicitly suppressing attention on normal tissue regions.
- Mechanism: The positive prompt guides attention toward lesion regions while the negative prompt highlights normal regions. By computing the difference between these two attention maps (ACLAP = Apositive - Anegative), regions that attract attention from both prompts are suppressed, reducing false positives.
- Core assumption: The negative prompt reliably generates high attention values for normal tissue regions, and these values are sufficiently different from lesion regions to enable effective suppression through subtraction.
- Evidence anchors:
  - [abstract]: "To reduce false positives, we attenuate attention on normal regions using negative prompts"
  - [section]: "To mitigate the false positive issue on Apositive, we additionally check the results of negative prompts as shown in Anegative. Those results show not very strong attention on non-negative regions (false negatives). Moreover, the true negative regions are mostly highlighted that can be used to suppress the false positives of Apositive."
  - [corpus]: Weak evidence - No direct corpus support found for this specific negative-prompt suppression mechanism.
- Break condition: If the negative prompt fails to generate distinct attention patterns for normal versus abnormal regions, or if normal tissue patterns overlap significantly with pathological features.

### Mechanism 2
- Claim: The reconstruction-by-inpainting approach using U-Net trained only on normal samples can detect anomalies by measuring reconstruction error in obfuscated regions.
- Mechanism: Strong attention regions identified by CLAP are obfuscated (masked) before being input to a U-Net trained exclusively on normal samples. The U-Net struggles to reconstruct these masked regions when they contain abnormal patterns, resulting in higher reconstruction error that indicates anomalies.
- Core assumption: The U-Net model learns to reconstruct normal patterns well but cannot adequately reconstruct abnormal patterns when they are obfuscated and replaced with normal-looking regions.
- Evidence anchors:
  - [section]: "The U-Net is trained to reconstruct the partially obfuscated image to its original form, a process known as reconstruction-by-inpainting. Only normal samples are used for training, ensuring the model generalizes well to normal patterns while struggling with abnormal pattern reconstruction."
  - [abstract]: "We employ an unsupervised anomaly detection (UAD) method that features a reconstruction-by-inpainting strategy"
  - [corpus]: Weak evidence - No direct corpus support found for this specific reconstruction-by-inpainting approach.
- Break condition: If the U-Net learns to reconstruct abnormal patterns despite being trained only on normal data, or if the obfuscation process fails to adequately mask the abnormal regions.

### Mechanism 3
- Claim: BioMedCLIP's biomedical knowledge enables more accurate lesion localization compared to general-purpose visual models like DINO.
- Mechanism: BioMedCLIP, fine-tuned on biomedical image-text pairs, understands medical terminology and concepts, allowing it to generate attention maps that focus on relevant pathological features rather than generic visual saliency.
- Core assumption: Fine-tuning CLIP on biomedical data creates meaningful semantic understanding of medical concepts that translates to better attention localization for lesions.
- Evidence anchors:
  - [section]: "When applying our method CLAP on the visual-language model BioMedCLIP [1], false attentions are successfully removed" and "CLAP, built on BioMedCLIP [1], leverages language prompts to focus on specific lesion regions within an image"
  - [abstract]: "Despite CLIP's strong multi-modal data capabilities, it remains limited in specialized environments, such as medical applications. For this purpose, many CLIP variants-i.e., BioMedCLIP, and MedCLIP-SAMv2-have emerged"
  - [corpus]: Weak evidence - No direct corpus support found for BioMedCLIP's superior performance over DINO in this specific context.
- Break condition: If the biomedical knowledge doesn't generalize well to specific anatomies or if the language prompts don't effectively leverage this knowledge.

## Foundational Learning

- Concept: Contrastive Language-Image Pre-training (CLIP) and attention mechanisms
  - Why needed here: Understanding how CLIP uses text prompts to guide visual attention is fundamental to grasping how CLAP works
  - Quick check question: How does CLIP compute similarity between image and text embeddings, and what role does this play in generating attention maps?

- Concept: Zero-shot learning and unsupervised anomaly detection
  - Why needed here: The method doesn't require labeled anomaly data and uses reconstruction error as the detection criterion
  - Quick check question: What distinguishes zero-shot learning from traditional supervised approaches, and how does reconstruction error indicate anomalies?

- Concept: Mutual information maximization in multimodal models
  - Why needed here: The attention map generation uses mutual information between image and text features
  - Quick check question: How does maximizing mutual information between image and text features help in identifying relevant regions for a given prompt?

## Architecture Onboarding

- Component map: Image → BioMedCLIP with positive and negative prompts → Attention map → Threshold → Obfuscate → U-Net → Reconstruction error → Anomaly decision

- Critical path: Medical image → BioMedCLIP attention generation → Dual-prompt attention subtraction → Q3 thresholding → Mosaic obfuscation → U-Net reconstruction → MSGMS error computation → Anomaly scoring

- Design tradeoffs:
  - Single-prompt vs. dual-prompt approach: Simpler implementation but higher false positives vs. better accuracy with more complex prompt engineering
  - Attention map vs. direct classification: Provides interpretability but adds computational overhead
  - U-Net architecture choice: Deeper networks may capture more complex patterns but risk overfitting with limited normal training data

- Failure signatures:
  - High false positive rate: Negative prompts not effectively suppressing normal tissue attention
  - High false negative rate: Positive prompts missing subtle lesions or attention maps too sparse
  - Inconsistent performance across anatomies: Prompts not well-tuned for specific medical domains
  - Poor reconstruction error discrimination: U-Net learns to reconstruct anomalies or normal samples produce high error

- First 3 experiments:
  1. Test attention map generation with only positive prompts on normal and abnormal samples to establish baseline false positive rate
  2. Implement dual-prompt approach and verify that ACLAP = Apositive - Anegative reduces false positives while maintaining true positive detection
  3. Train U-Net on normal samples only, then test reconstruction error on both normal and abnormal samples with obfuscated regions to validate anomaly detection capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is CLAP at suppressing false positives when applied to other CLIP variants beyond BioMedCLIP, such as MedCLIP-SAMv2 or general-purpose CLIP models?
- Basis in paper: [explicit] The paper mentions that various CLIP variants like BioMedCLIP and MedCLIP-SAMv2 have emerged but false positives persist, and suggests future work on automated fine prompting for more practical usage.
- Why unresolved: The study only evaluates CLAP with BioMedCLIP and does not test its performance on other CLIP variants or explore cross-model generalizability.
- What evidence would resolve it: Comparative experiments applying CLAP to multiple CLIP variants (BioMedCLIP, MedCLIP-SAMv2, general CLIP) on the same BMAD dataset would demonstrate generalizability and relative effectiveness.

### Open Question 2
- Question: What is the optimal balance between positive and negative prompt strength in CLAP for different medical imaging modalities?
- Basis in paper: [inferred] The current CLAP formulation uses a simple subtraction (Apositive - Anegative) without discussing prompt weighting or balance, and mentions future work on automated prompting methods.
- Why unresolved: The paper does not explore parameter tuning for the relationship between positive and negative prompts, which could significantly impact performance across different imaging types.
- What evidence would resolve it: Systematic experiments varying the weighting between positive and negative attention maps for each anatomy, with ablation studies showing optimal configurations.

### Open Question 3
- Question: How does CLAP perform on medical images with multiple simultaneous pathologies or complex lesion patterns?
- Basis in paper: [inferred] The evaluation focuses on single-anatomy benchmarks, and the paper notes that false positives remain a challenge even with BioMedCLIP, suggesting limitations in complex cases.
- Why unresolved: The BMAD dataset evaluation does not specifically address multi-pathology scenarios, and the qualitative examples show single-lesion cases.
- What evidence would resolve it: Testing CLAP on datasets containing multi-lesion cases or images with overlapping pathologies, comparing attention map accuracy and detection performance against single-lesion scenarios.

## Limitations

- Dataset scope limited to six specific medical imaging modalities in the BMAD dataset
- Heavy dependency on prompt engineering requiring domain expertise
- Model-specific constraints tied to BioMedCLIP's performance and availability

## Confidence

- **High Confidence**: The general framework of using contrastive language prompting with attention map subtraction to reduce false positives is theoretically sound and demonstrates consistent performance improvements across multiple benchmarks in the BMAD dataset.
- **Medium Confidence**: The reconstruction-by-inpainting approach using U-Net for anomaly detection shows promise but requires careful implementation. The effectiveness may vary depending on the U-Net architecture choice and training procedure specifics.
- **Medium Confidence**: The superiority of BioMedCLIP over general-purpose models like DINO is demonstrated on the BMAD dataset, but the generalizability to other biomedical imaging tasks requires further validation.

## Next Checks

1. Cross-Dataset Validation: Test CLAP performance on independent medical imaging datasets outside the BMAD collection to verify generalizability across different clinical settings and imaging protocols.

2. Prompt Robustness Analysis: Systematically evaluate how variations in prompt wording and structure affect detection performance to establish guidelines for prompt engineering and identify optimal prompt formulations.

3. Computational Efficiency Benchmarking: Measure inference time and resource requirements compared to baseline methods to assess practical deployment feasibility in clinical settings with computational constraints.