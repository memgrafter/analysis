---
ver: rpa2
title: 'NICE: To Optimize In-Context Examples or Not?'
arxiv_id: '2402.06733'
source_url: https://arxiv.org/abs/2402.06733
tags:
- task
- examples
- tasks
- instruction
- nice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the widely held belief that optimizing in-context
  examples (ICE) is essential for improving large language model (LLM) performance.
  Through extensive experiments on various tasks and instruction sets, the authors
  demonstrate that for many tasks, detailed instructions with randomly chosen ICE
  can match or even surpass the accuracy of prompts with optimized ICE.
---

# NICE: To Optimize In-Context Examples or Not?

## Quick Facts
- **arXiv ID**: 2402.06733
- **Source URL**: https://arxiv.org/abs/2402.06733
- **Reference count**: 25
- **Primary result**: For many tasks, detailed instructions with random ICE match or surpass optimized ICE performance

## Executive Summary
This paper challenges the conventional wisdom that optimizing in-context examples (ICE) is essential for improving LLM performance. Through extensive experiments across multiple datasets and tasks, the authors demonstrate that detailed instructions can often substitute for carefully selected ICE, particularly for tasks that are highly learnable from instructions alone. They introduce the NICE (Normalized Invariability to Choice of Examples) metric to quantify task learnability from instructions, providing practitioners with a tool to decide when to focus on instruction quality versus ICE optimization. The findings suggest that many NLP tasks may be over-optimized for ICE selection when instruction quality could yield better returns on investment.

## Method Summary
The paper introduces a systematic approach to evaluate when in-context example optimization is necessary. The core method involves creating instruction templates with varying levels of detail and using the NICE metric to quantify how much task performance depends on example selection. NICE is computed by partitioning candidate examples into bins based on a query-dependent scoring function (e.g., cosine similarity), then measuring the performance variance across these bins. The authors compare four ICE selection methods (top-k-BERT, BM-25, DPP, and random) and evaluate performance across eight diverse datasets using GPT-3.5-Turbo and GPT-4-Turbo. The experimental design systematically varies instruction detail while controlling for ICE quality to isolate the effects of instructions versus examples.

## Key Results
- For high-NICE tasks (e.g., sentiment classification, textual entailment), detailed instructions with random ICE match or exceed the performance of optimized ICE without instructions
- The NICE metric effectively predicts which tasks benefit from ICE optimization, with low-NICE tasks (e.g., semantic parsing, question decomposition) requiring ICE optimization
- With detailed instructions, model performance becomes less sensitive to label choice in examples, suggesting reduced reliance on examples for input-label mapping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Detailed instructions can reduce or eliminate the need for optimizing ICE for certain tasks
- Mechanism: When instructions become sufficiently detailed, they encode the task's label space, format, and mapping rules, allowing the model to learn the task without relying heavily on example demonstrations
- Core assumption: LLMs can generalize task understanding from natural language instructions without explicit input-label mapping examples
- Evidence anchors:
  - [abstract] "for high-NICE tasks...detailed instructions with random ICE yield comparable or better performance than optimized ICE without instructions"
  - [section] "we find that a detailed instruction with randomly chosen ICE surpasses or matches the accuracy of a prompt with ICE selected using criteria from past work"
  - [corpus] No direct corpus evidence available - weak signal
- Break condition: Tasks requiring complex input-output syntax or schema understanding (e.g., semantic parsing) where instructions alone cannot capture the structural mapping

### Mechanism 2
- Claim: The NICE metric effectively predicts whether ICE optimization will be beneficial for a given task
- Mechanism: NICE measures the invariability of model performance across different example bins, with high scores indicating task learnability from instructions alone and low scores indicating need for ICE optimization
- Core assumption: Performance variance across example bins correlates with the task's dependence on ICE for learning
- Evidence anchors:
  - [abstract] "The NICE metric effectively captures the dependence of tasks on ICE optimization, allowing practitioners to decide whether to focus on improving instructions or optimizing ICE for a given task"
  - [section] "We introduce a task-specific metric called Normalized Invariability to Choice of Examples (NICE) that quantifies the learnability of tasks from a given instruction"
  - [corpus] Weak evidence - only mentions related works without direct NICE validation
- Break condition: When example bins don't represent meaningful variation in example quality or when the grouping function poorly captures example relevance

### Mechanism 3
- Claim: Instructions can reduce the model's reliance on ground-truth labels in in-context examples
- Mechanism: As instructions become more detailed, they provide sufficient task information that random or incorrect labels in examples don't significantly degrade performance
- Core assumption: Models can use instructions to understand task structure even when example labels are misleading
- Evidence anchors:
  - [abstract] "With detailed instructions, the model's performance becomes less sensitive to the choice of labels in the in-context examples, indicating reduced reliance on examples for input-label mapping"
  - [section] "even using ICE with incorrect labels provides the same accuracy as using the correct ICE on most tasks"
  - [corpus] No direct corpus evidence available - weak signal
- Break condition: Tasks where input-label mapping is highly complex or when instructions cannot adequately capture the mapping rules

## Foundational Learning

- Concept: In-context learning (ICL) and few-shot prompting
  - Why needed here: The paper's central focus is on how ICE selection affects ICL performance and when instructions can replace this optimization
  - Quick check question: What distinguishes zero-shot, one-shot, and few-shot prompting in terms of model behavior?

- Concept: Instruction-tuning and natural language task descriptions
  - Why needed here: The paper leverages instruction-tuned models and systematically varies instruction detail to study its interaction with ICE optimization
  - Quick check question: How does instruction-tuning differ from standard pre-training in terms of model capabilities?

- Concept: Task learnability and generalization from demonstrations
  - Why needed here: The NICE metric and experimental design both rely on understanding when models can learn tasks from instructions versus examples
  - Quick check question: What factors determine whether a model learns a task from examples or instruction alone?

## Architecture Onboarding

- Component map: Instruction template creation -> NICE metric computation (cosine similarity binning) -> ICE selection methods (top-k-BERT, BM-25, DPP, random) -> LLM evaluation pipeline -> Performance analysis
- Critical path: 1) Define task and create instruction template 2) Compute NICE score by binning examples and measuring performance variance 3) Use NICE to decide optimization strategy 4) Implement chosen optimization (instruction improvement or ICE optimization)
- Design tradeoffs: NICE provides a simple metric but may not capture all nuances of task-LLM interactions; more sophisticated binning strategies could improve accuracy but increase computational cost
- Failure signatures: High NICE scores that don't predict instruction effectiveness; low NICE scores that don't predict ICE optimization benefits; inconsistent results across different LLMs or datasets
- First 3 experiments:
  1. Compute NICE for a simple classification task (e.g., SST-2) with varying instruction detail levels to observe the flattening effect
  2. Compare instruction-based and ICE-based performance for a low-NICE task (e.g., MTOP) to validate NICE's prediction
  3. Test label perturbation on a high-NICE task to confirm reduced sensitivity to example labels with detailed instructions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the NICE metric perform when applied to tasks with continuous output spaces (e.g., regression) rather than discrete classification or structured generation?
- Basis in paper: [inferred] The paper focuses on discrete classification and structured generation tasks. It does not explore tasks with continuous output spaces.
- Why unresolved: The paper does not provide evidence or analysis for tasks with continuous outputs. The current formulation of NICE relies on binning examples and computing performance scores, which may not be directly applicable to regression tasks.
- What evidence would resolve it: Empirical results showing the effectiveness of NICE on regression tasks, including how binning strategies and performance metrics would be adapted for continuous outputs.

### Open Question 2
- Question: How does the NICE metric change when using more advanced clustering strategies beyond cosine similarity-based bins, such as influence functions or other semantic similarity measures?
- Basis in paper: [explicit] The paper mentions that the current baseline uses cosine similarity-based bins and suggests that more advanced clustering strategies could be explored.
- Why unresolved: The paper only uses cosine similarity as the grouping function for binning examples. It does not provide results or analysis for other clustering methods.
- What evidence would resolve it: Comparative results showing the NICE scores and performance trends when using different clustering strategies (e.g., influence functions, semantic similarity measures) across various tasks.

### Open Question 3
- Question: How does the NICE metric and the observed trends in ICE optimization vary across different model sizes and architectures, particularly for smaller models or non-transformer architectures?
- Basis in paper: [explicit] The paper uses state-of-the-art LLMs (GPT-4, GPT-3.5-Turbo) and mentions results for open-source models (Mixtral, Llama-2-70b-chat-hf) but does not extensively explore smaller models or non-transformer architectures.
- Why unresolved: The paper does not provide a comprehensive analysis of how NICE and ICE optimization trends vary with model size and architecture. It only shows results for large language models.
- What evidence would resolve it: Empirical results comparing NICE scores and ICE optimization trends across a range of model sizes (e.g., from small to large) and different architectures (e.g., convolutional, recurrent, transformer-based models).

## Limitations

- The experiments primarily focus on instruction-tuned LLMs, limiting generalizability to base models
- The NICE metric relies on binning strategies that may not capture all nuances of example quality
- The boundary between high and low-NICE tasks could be more precisely defined
- The study doesn't explore the interaction between instruction detail level and ICE optimization depth

## Confidence

- **High Confidence**: The empirical finding that detailed instructions reduce sensitivity to ICE choice for high-NICE tasks
- **Medium Confidence**: The NICE metric's predictive power for task learnability from instructions
- **Medium Confidence**: The claim that instructions can reduce reliance on ground-truth labels in examples

## Next Checks

1. **Cross-model validation**: Test whether the NICE metric and instruction effectiveness findings hold for base LLMs (not instruction-tuned) to assess generalizability beyond instruction-tuned models

2. **Instruction-ICE interaction analysis**: Systematically vary both instruction detail level and ICE optimization quality to map the complete instruction-ICE performance landscape, particularly for tasks near the high/low-NICE boundary

3. **Robustness to label corruption**: Conduct more extensive experiments with varying degrees of label corruption in ICE to precisely quantify when and how instructions can compensate for misleading examples across different task types