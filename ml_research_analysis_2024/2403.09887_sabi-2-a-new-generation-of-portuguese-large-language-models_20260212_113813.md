---
ver: rpa2
title: "Sabi\xE1-2: A New Generation of Portuguese Large Language Models"
arxiv_id: '2403.09887'
source_url: https://arxiv.org/abs/2403.09887
tags:
- sabi
- exams
- medium
- questions
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Sabi\xE1-2, a family of large language models\
  \ trained on Portuguese texts and evaluated on diverse Brazilian exams and cultural\
  \ conversations. Sabi\xE1-2 Medium matches or surpasses GPT-4 on 23 of 64 exams\
  \ and outperforms GPT-3.5 on 58 of 64 exams."
---

# Sabiá-2: A New Generation of Portuguese Large Language Models

## Quick Facts
- **arXiv ID**: 2403.09887
- **Source URL**: https://arxiv.org/abs/2403.09887
- **Reference count**: 29
- **Key outcome**: Sabiá-2 Medium matches or surpasses GPT-4 on 23 of 64 exams and outperforms GPT-3.5 on 58 of 64 exams, offering 10x cost savings.

## Executive Summary
Sabiá-2 is a family of large language models specialized for Portuguese language tasks, trained on Portuguese texts and evaluated on diverse Brazilian exams and cultural conversations. The models demonstrate strong performance on academic and professional exams, matching or surpassing GPT-4 on 23 of 64 exams while offering significant cost advantages. A novel Portuguese conversation benchmark (BRACEval) assesses cultural knowledge, reasoning, and chat capabilities. The work highlights the benefits of specialization in Portuguese-language training data for improved performance without increasing model size.

## Method Summary
Sabiá-2 models were trained on Portuguese text corpora and evaluated using few-shot learning on multiple-choice subsets of Brazilian academic and professional exams. The BRACEval benchmark was created to assess cultural knowledge and conversational abilities using LLM-as-a-judge evaluation with GPT-4 as the reference. Models were compared against GPT-3.5 and GPT-4 across 64 different exams and conversation tasks, with performance measured through automated scoring and pairwise comparisons.

## Key Results
- Sabiá-2 Medium matches or surpasses GPT-4 on 23 of 64 exams and outperforms GPT-3.5 on 58 of 64 exams
- The model offers approximately 10x cost savings versus GPT-4 while maintaining competitive performance
- Sabiá-2 Medium excels in specialized domains but shows weaknesses in math and coding tasks
- BRACEval benchmark demonstrates strong cultural knowledge and reasoning capabilities in Portuguese

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specialization in Portuguese-language training data yields performance gains across Brazilian exams without increasing model size.
- Mechanism: Focused pretraining on domain-relevant corpora captures linguistic patterns and cultural context, allowing the model to perform better on culturally specific tasks.
- Core assumption: Portuguese text corpora contain sufficient domain-specific knowledge for high exam performance; the models generalize from pretraining to downstream tasks without fine-tuning.
- Evidence anchors:
  - [abstract]: "trained on Portuguese texts...specialization has a significant impact on a model’s performance without the need to increase its size"
  - [section]: "specialization has shown to be a highly effective strategy for improving the performance of models without the need to increase their parameter count"
  - [corpus]: Weak evidence; related papers focus on Portuguese-specific LLMs and benchmarks, but do not directly address performance gains from specialization.
- Break condition: Pretraining data is insufficient in domain-specific knowledge, or the exams test knowledge outside the pretraining distribution.

### Mechanism 2
- Claim: Using only multiple-choice questions from exams avoids subjective evaluation and reduces cost compared to open-ended questions.
- Mechanism: Automated evaluation of multiple-choice answers is more efficient and objective than human or LLM grading of open-ended responses.
- Core assumption: Multiple-choice formats sufficiently represent the model’s ability to answer exam questions; automated evaluation is unbiased.
- Evidence anchors:
  - [section]: "To facilitate the evaluation of LLMs and mitigate subjectivity and bias...we have chosen to use only the multiple-choice subset of these exams."
  - [section]: "The benchmarks were evaluated in a few-shot setting, with three examples from previous exam editions composing the prompt."
  - [corpus]: No direct evidence; related work discusses Portuguese benchmarks but not evaluation methodology.
- Break condition: Multiple-choice formats fail to capture real-world reasoning ability, or automated evaluation introduces unintended biases.

### Mechanism 3
- Claim: BRACEval benchmark, designed with Brazilian cultural context, reveals LLM strengths and weaknesses in culturally relevant tasks.
- Mechanism: Prompts designed with culturally specific content and multi-turn interactions test model knowledge of Brazilian culture, reasoning, and conversational skills.
- Core assumption: The BRACEval questions are well-designed and representative of Brazilian cultural knowledge; LLM judges are reliable evaluators.
- Evidence anchors:
  - [section]: "BRACEval encompasses a diverse set of contextualized categories reflecting content pertinent to Brazil...the model’s knowledge related to Brazil."
  - [section]: "We adopt the approach of ‘LLM-as-a-judge’...using strong LLMs to judge the quality and utility of assistant-generated responses."
  - [corpus]: No direct evidence; related papers mention Portuguese LLMs and benchmarks but not BRACEval specifically.
- Break condition: BRACEval questions are not representative of Brazilian culture, or LLM judges are unreliable.

## Foundational Learning

- Concept: Pretraining on large corpora of text
  - Why needed here: The model’s ability to perform well on exams depends on having learned linguistic patterns and domain knowledge from large amounts of Portuguese text.
  - Quick check question: What is the relationship between pretraining data size and downstream performance on specialized tasks?

- Concept: Few-shot learning
  - Why needed here: The model is evaluated on new exams using only a few examples from previous exams, requiring the ability to generalize from limited context.
  - Quick check question: How does the number of few-shot examples affect the model’s performance on new tasks?

- Concept: LLM-as-a-judge evaluation
  - Why needed here: The BRACEval benchmark uses LLM judges to evaluate open-ended responses, requiring understanding of the strengths and limitations of this approach.
  - Quick check question: What are the potential biases and limitations of using LLM judges for evaluation?

## Architecture Onboarding

- Component map: Pretraining on Portuguese text → Few-shot evaluation on exams → LLM-as-a-judge evaluation on BRACEval
- Critical path: Pretraining data quality and size → Few-shot evaluation methodology → LLM judge reliability
- Design tradeoffs: Multiple-choice vs. open-ended evaluation → Pretraining vs. fine-tuning → LLM judge vs. human judge
- Failure signatures: Poor exam performance → Inconsistent or biased LLM judge evaluations → Inability to generalize from few-shot examples
- First 3 experiments:
  1. Evaluate model performance on a diverse set of Brazilian exams with varying formats (multiple-choice, open-ended) to assess generalization ability.
  2. Compare LLM judge evaluations to human judge evaluations on a subset of BRACEval questions to assess reliability and bias.
  3. Analyze the relationship between pretraining data size and downstream performance on specialized tasks to determine optimal data requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Sabiá-2's specialization in Portuguese language data significantly improve its performance on other Latin American Spanish-language exams, or is the improvement limited to Brazilian Portuguese contexts?
- Basis in paper: [inferred] The paper demonstrates Sabiá-2's strong performance on Brazilian exams and cultural conversations, suggesting domain specialization benefits. However, it does not test the model on Spanish-language exams from other Latin American countries.
- Why unresolved: The paper focuses exclusively on Brazilian benchmarks and Portuguese language contexts. Testing the model on Spanish-language exams from other Latin American countries would determine whether the specialization is transferable across similar languages or limited to the Brazilian Portuguese domain.
- What evidence would resolve it: Testing Sabiá-2 on a diverse set of Spanish-language exams from countries like Argentina, Mexico, or Chile, and comparing performance with general-purpose models and Spanish-specialized models.

### Open Question 2
- Question: How does Sabiá-2's performance on mathematics and coding tasks compare when using different prompting strategies, such as chain-of-thought prompting or program-of-thought prompting?
- Basis in paper: [explicit] The paper identifies math and coding as key abilities that need improvement for Sabiá-2, noting weaknesses in these areas across multiple benchmarks.
- Why unresolved: The paper uses standard prompting approaches for evaluation but does not explore advanced prompting techniques that have shown promise in improving reasoning capabilities in other models.
- What evidence would resolve it: Conducting experiments with various prompting strategies (chain-of-thought, program-of-thought, scratchpad methods) on the same math and coding benchmarks, comparing accuracy improvements against baseline performance.

### Open Question 3
- Question: What is the impact of including visual information (images, diagrams) in prompts on Sabiá-2's performance across different academic domains?
- Basis in paper: [explicit] The paper explicitly states that Sabiá-2 does not yet support images and excludes questions requiring image understanding from evaluations, representing a significant limitation.
- Why unresolved: The model's inability to process visual information is noted as a limitation, but the paper does not explore how this affects performance in domains where visual reasoning is important (science diagrams, mathematical figures, technical drawings).
- What evidence would resolve it: Comparing Sabiá-2's performance on image-based questions versus text-only versions of the same questions across different academic domains, or evaluating an upgraded version with vision capabilities.

## Limitations

- Model architecture and training details are withheld due to competitive considerations, limiting reproducibility
- BRACEval benchmark relies entirely on automated LLM-as-a-judge evaluation without human validation
- Clear performance limitations exist in mathematical and coding domains, with Sabiá-2 Medium underperforming compared to competitors

## Confidence

**High Confidence**: Claims about cost advantages (10x savings vs GPT-4) and exam performance relative to GPT-3.5 are supported by specific quantitative results and standardized benchmark comparisons.

**Medium Confidence**: Claims about specialization benefits and cultural knowledge gains are supported by comparative results but lack detailed ablation studies showing what specific pretraining choices drove improvements.

**Low Confidence**: Claims about generalization beyond the specific exam domains tested, and about the model's ability to handle real-world Portuguese language tasks beyond cultural conversation, are not directly evaluated.

## Next Checks

1. Conduct human evaluation of a subset of BRACEval responses to measure agreement with LLM judge scores and assess potential systematic biases in automated evaluation.

2. Perform controlled experiments varying pretraining corpus composition and size to isolate the specific factors contributing to specialization benefits, and to identify optimal training strategies.

3. Evaluate model performance on Portuguese language tasks outside the exam and cultural conversation domains, including creative writing, technical documentation, and real-world customer service interactions, to assess generalization capabilities.