---
ver: rpa2
title: 'VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval Augmented
  Generation'
arxiv_id: '2412.10151'
source_url: https://arxiv.org/abs/2412.10151
tags:
- passages
- vlr-if
- passage
- data
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLR-Bench introduces a novel benchmark for evaluating vision-language
  models' retrieval-augmented generation capabilities. Unlike existing datasets, it
  presents five passages per query (two gold, two silver, one bronze) to test models'
  ability to identify relevant external knowledge.
---

# VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2412.10151
- Source URL: https://arxiv.org/abs/2412.10151
- Authors: Hyeonseok Lim; Dongjae Shin; Seohyun Song; Inho Won; Minjun Kim; Junghun Yuk; Haneol Jang; KyungTae Lim
- Reference count: 14
- Primary result: VLR-Bench achieves 37.72% performance drop when passages are removed, validating external knowledge requirement

## Executive Summary
VLR-Bench introduces a novel benchmark for evaluating vision-language models' retrieval-augmented generation capabilities. Unlike existing datasets, it presents five passages per query (two gold, two silver, one bronze) to test models' ability to identify relevant external knowledge. The benchmark includes 300 multilingual examples (English, Chinese, Korean) across cultural and general knowledge domains. A corresponding VLR-IF training dataset with 32,000 examples was developed to improve RAG capabilities.

## Method Summary
VLR-Bench consists of 300 multilingual examples (150 general knowledge, 150 cultural knowledge) across English, Chinese, and Korean. Each query is paired with five passages: two gold (containing answer-relevant information), two silver (topic-related but not answer-relevant), and one bronze (unrelated). The VLR-IF training dataset contains 32,000 examples constructed using GPT-based instruction following to generate valid and invalid passage pairs. Models are evaluated using KMS, ROUGE-2/ROUGE-L, BLEU, and BERTScore metrics, with comparisons between performance with and without passages to verify external knowledge dependency.

## Key Results
- VLR-Bench demonstrates 37.72% performance drop when passages are removed, confirming external knowledge requirement
- Models trained on VLR-IF show 22.67% performance improvement over baseline models
- Gold passages show highest correlation to ground truth answers, validating benchmark design
- Multilingual design captures cultural knowledge dependencies across different languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Five-passage structure enables fine-grained evaluation of retrieval selection capability
- Mechanism: By including two gold passages (containing answer-relevant information), two silver passages (topic-related but not answer-relevant), and one bronze passage (unrelated), the benchmark forces models to distinguish between passages that contain useful information versus those that do not. This directly tests whether models can identify which retrieved documents are actually helpful for answering a given query.
- Core assumption: Models need to learn to differentiate between passages that contain relevant information and those that do not, rather than simply relying on the presence of any retrieved information.
- Evidence anchors:
  - [abstract]: "Unlike existing evaluation datasets for external knowledge-based VQA, the proposed VLR-BENCH includes five input passages. This allows testing of the ability to determine which passage is useful for answering a given query, a capability lacking in previous research."
  - [section]: "VLR-BENCH was constructed to evaluate whether VLMs can use the correct external knowledge to generate accurate responses to query. We constructed a parallel corpus of 300 datasets: 150 based on general knowledge and 150 based on cultural data from English, Chinese, and Korean."

### Mechanism 2
- Claim: Training data construction through instruction following improves RAG capability
- Mechanism: The VLR-IF dataset is constructed using a GPT-based method that generates valid and invalid passages for each image. By training models on this contrastive data where they learn to distinguish between relevant and irrelevant passages, they develop better ability to select appropriate external knowledge during inference.
- Core assumption: Contrastive learning between valid and invalid passages enables models to develop better judgment about which passages to use.
- Evidence anchors:
  - [abstract]: "This dataset is specifically designed to enhance the RAG capabilities of VLMs by enabling them to learn how to generate appropriate answers based on input passages."
  - [section]: "We conducted experiments to assess the utility of the VLR-IF data using the baseline LLaVA-Llama-3 and its version enhanced by VLR-IF training. According to the results in Table 1, the model trained with the VLR-IF data showed a 22.67% performance improvement over the baseline model when external knowledge was provided."

### Mechanism 3
- Claim: Multilingual design captures cultural knowledge dependencies
- Mechanism: By including parallel datasets in English, Chinese, and Korean with cultural-specific queries, the benchmark tests whether models can understand and retrieve culturally-specific knowledge across different languages, revealing limitations in cross-cultural knowledge transfer.
- Core assumption: Cultural knowledge is not uniformly distributed across languages and requires specific understanding of each culture's context.
- Evidence anchors:
  - [abstract]: "The benchmark includes 300 multilingual examples (English, Chinese, Korean) across cultural and general knowledge domains."
  - [section]: "VLR-BENCH comprises 150 datasets for each language, incorporating language-specific cultural aspects. The benchmark is designed to include queries that require an understanding of the respective culture to accurately select the correct information from the provided passages."

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) systems
  - Why needed here: The entire benchmark and training approach is built around evaluating and improving RAG capabilities, where models retrieve external knowledge and use it to generate answers.
  - Quick check question: What are the two main components of a RAG system and how do they interact during inference?

- Concept: Contrastive learning for passage selection
  - Why needed here: The VLR-IF training data uses contrastive pairs of valid and invalid passages to teach models which passages to select for answering queries.
  - Quick check question: How does contrastive learning help models distinguish between relevant and irrelevant information in retrieval tasks?

- Concept: Multimodal alignment in vision-language models
  - Why needed here: The benchmark requires models to integrate visual information from images with textual information from passages to generate accurate answers.
  - Quick check question: What challenges arise when trying to align visual and textual representations in vision-language models?

## Architecture Onboarding

- Component map: Image → Vision encoder → Text encoder → Passage retrieval → Answer generation → Evaluation
- Critical path: Image → Vision encoder → Text encoder → Passage retrieval → Answer generation → Evaluation
  - Key decision points: Which passages to use, how to integrate visual and textual information, how to generate coherent answers
- Design tradeoffs:
  - 5-passage structure vs. single passage: More realistic evaluation but increased complexity
  - Manual review vs. automatic generation: Higher quality but more resource intensive
  - Multilingual vs. single language: Broader applicability but increased development effort
- Failure signatures:
  - High KMS but low ROUGE/BLEU: Model is outputting passages verbatim without true understanding
  - Similar performance with and without passages: Benchmark questions may not actually require external knowledge
  - Poor performance on cultural queries: Model lacks cultural knowledge or cross-lingual transfer capability
- First 3 experiments:
  1. Test model performance on VLR-Bench with passages removed to verify external knowledge dependency
  2. Train model on VLR-IF data and evaluate improvement on VLR-Bench with passages
  3. Compare model performance across different passage types (gold vs. silver vs. bronze) to understand selection capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance on VLR-Bench vary when the five passages are presented in different orders?
- Basis in paper: [inferred] The paper describes that VLR-Bench includes five passages (two gold, two silver, one bronze) per query, but does not specify whether passage order affects model performance.
- Why unresolved: The paper evaluates model performance on VLR-Bench but does not conduct experiments to test whether the order of passages impacts retrieval accuracy or generation quality.
- What evidence would resolve it: Experiments showing performance metrics (KMS, ROUGE, BLEU, BERT-Score) for models when passages are presented in different permutations (e.g., gold first vs. gold last) would reveal whether passage ordering influences model behavior.

### Open Question 2
- Question: What is the impact of including bronze passages on model performance compared to datasets that only provide gold passages?
- Basis in paper: [explicit] The paper explicitly states that VLR-Bench includes bronze passages (unrelated to the query) alongside gold and silver passages, which is a distinguishing feature from other datasets.
- Why unresolved: While the paper demonstrates that VLR-Bench requires external knowledge and that performance drops without passages, it does not isolate the specific contribution of bronze passages to the benchmark's difficulty or the model's learning process.
- What evidence would resolve it: Comparative experiments showing model performance on VLR-Bench with and without bronze passages, or comparing performance on VLR-Bench versus datasets with only gold passages, would quantify the bronze passages' impact on retrieval-augmented generation.

### Open Question 3
- Question: How does the VLR-IF training data generalize to other multimodal tasks beyond VQA, such as image captioning or visual reasoning?
- Basis in paper: [inferred] The paper demonstrates that VLR-IF improves performance on VLR-Bench and InfoSeek, but does not test its effectiveness on other multimodal tasks.
- Why unresolved: The paper focuses on VQA and retrieval-augmented generation, but does not explore whether the training data enhances general vision-language understanding capabilities.
- What evidence would resolve it: Experiments evaluating models trained on VLR-IF on other multimodal benchmarks (e.g., image captioning datasets, visual reasoning tasks) would show whether the training data has broader applicability beyond VQA.

## Limitations
- Benchmark relies on manual review for gold and silver passages, introducing potential subjectivity in passage labeling
- Dataset size of 300 examples may limit statistical significance for certain analyses
- VLR-IF training data effectiveness depends on quality of GPT-4o-generated instructions, which are not fully specified

## Confidence
- High confidence: The 37.72% performance drop when passages are removed (p. 5) directly demonstrates that VLR-Bench questions require external knowledge, validating the benchmark's design purpose.
- Medium confidence: The 22.67% performance improvement from VLR-IF training (p. 5) is promising but limited to a single baseline model comparison, requiring broader validation across different model architectures.
- Medium confidence: The five-passage structure effectively tests passage selection capability, though this assumes models cannot simply use all passages indiscriminately, which would need to be verified experimentally.

## Next Checks
1. **Cross-model validation**: Test the VLR-Bench and VLR-IF training approach across at least three additional vision-language model architectures (e.g., GPT-4V, Gemini Pro Vision, and a smaller open-source model) to verify that performance improvements are not specific to the LLaVA-Llama-3 architecture used in the initial experiments.

2. **Human evaluation study**: Conduct a blind human evaluation where annotators assess whether model-generated answers with and without passages show statistically significant differences in accuracy and relevance, particularly for cultural knowledge queries where automated metrics may be less reliable.

3. **Generalization test**: Evaluate whether models trained on VLR-IF can generalize to completely unseen cultural domains by testing them on cultural queries from different regions or time periods not represented in the training data, to assess true cross-cultural knowledge transfer capability.