---
ver: rpa2
title: Extracting Sentence Embeddings from Pretrained Transformer Models
arxiv_id: '2408.08073'
source_url: https://arxiv.org/abs/2408.08073
tags:
- online
- available
- https
- authors
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores methods to extract sentence embeddings from
  pretrained transformer models without fine-tuning. It evaluates various token aggregation
  strategies (such as weighted averaging and frequency-based filtering) and post-processing
  techniques (including normalization, quantile transformation, and dominant component
  removal) across multiple static and contextual models.
---

# Extracting Sentence Embeddings from Pretrained Transformer Models

## Quick Facts
- **arXiv ID:** 2408.08073
- **Source URL:** https://arxiv.org/abs/2408.08073
- **Reference count:** 40
- **Primary result:** Aggregating and post-processing transformer token embeddings significantly improves sentence embedding quality on semantic textual similarity and clustering tasks.

## Executive Summary
This paper investigates how to extract effective sentence embeddings from pretrained transformer models without any fine-tuning. The authors systematically evaluate various token aggregation methods (including weighted averaging and frequency-based filtering) and post-processing techniques (such as normalization, quantile transformation, and dominant component removal) across multiple static and contextual models. Their results demonstrate substantial improvements in semantic textual similarity and clustering performance for all models tested, including a simple random embedding baseline. Notably, they introduce a new BERT + Avg model that combines contextual and context-averaged representations, further boosting performance. These findings reveal that careful shaping of token representations can significantly enhance sentence embeddings, even from models not specifically designed for this purpose.

## Method Summary
The authors propose and evaluate multiple strategies for generating sentence embeddings from pretrained transformers. First, they explore various token aggregation methods, including weighted averaging (where weights can be based on attention scores or token importance) and frequency-based filtering to remove common tokens. Second, they apply post-processing techniques to the aggregated embeddings, including normalization, quantile transformation to spread values, and dominant component removal to eliminate common directions in the embedding space. These methods are applied both individually and in combination across a diverse set of models, including static embeddings (Word2Vec, GloVe), contextual models (BERT, RoBERTa, ALBERT, ELECTRA), and sentence-specific models (Sentence-BERT, Sentence-T5). A novel contribution is the BERT + Avg model, which concatenates the standard BERT embedding with an average of all token embeddings, leveraging both contextual and context-averaged information.

## Key Results
- Average Spearman correlation on STS tasks improves from 62.3% to 71.6% with aggregation and post-processing
- Clustering accuracy increases from 59.2% to 64.8% across all tested models
- The BERT + Avg model achieves the highest clustering accuracy at 66.7% and classification accuracy at 87.5%
- Improvements are consistent across both static and contextual models, as well as sentence-specific models

## Why This Works (Mechanism)
Assumption: The proposed methods work because they address fundamental limitations in how transformer models generate sentence representations. Token aggregation methods like weighted averaging and frequency filtering help select the most informative tokens while reducing noise from less relevant ones. Post-processing techniques like normalization and quantile transformation help standardize the embedding distributions and amplify meaningful differences. Dominant component removal eliminates shared directional biases that can interfere with similarity comparisons. Together, these techniques effectively shape the raw token representations into more discriminative sentence embeddings that better capture semantic relationships.

## Foundational Learning
- **Token aggregation methods**: Why needed - Combining multiple token representations into a single sentence vector; Quick check - Does weighted averaging outperform simple averaging?
- **Post-processing techniques**: Why needed - Improving embedding quality and removing noise/bias; Quick check - Does normalization or quantile transformation provide larger gains?
- **Contextual vs static embeddings**: Why needed - Understanding differences in information captured by model types; Quick check - Which model type benefits most from proposed methods?
- **Semantic textual similarity evaluation**: Why needed - Standard benchmark for measuring embedding quality; Quick check - How do improvements on STS translate to clustering performance?
- **Dominant component removal**: Why needed - Eliminating common directions that may introduce bias; Quick check - Does removing the first principal component improve downstream performance?

## Architecture Onboarding

**Component map:** Input sentence -> Tokenization -> Token embeddings -> Aggregation (weighted avg, frequency filtering) -> Post-processing (normalization, quantile, component removal) -> Sentence embedding

**Critical path:** Tokenization → Token embeddings → Aggregation → Post-processing → Sentence embedding

**Design tradeoffs:** The main tradeoff is between preserving semantic information and removing noise/bias. Weighted averaging preserves more information but may include noise, while frequency filtering removes noise but may lose important information. Post-processing techniques like dominant component removal can improve similarity tasks but may harm classification tasks that rely on absolute distances.

**Failure signatures:** If token aggregation is too aggressive (e.g., frequency filtering removes too many tokens), embeddings may lose semantic content. If post-processing is too strong (e.g., quantile transformation is too extreme), embeddings may become distorted and lose meaningful relationships.

**First experiments:**
1. Compare simple averaging vs weighted averaging on STS tasks
2. Evaluate impact of each post-processing technique individually
3. Test BERT + Avg model against standard BERT on clustering tasks

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions, but potential areas for further investigation include: how these methods generalize to languages other than English, whether the improvements transfer to more diverse downstream tasks beyond STS and clustering, and if the proposed techniques can be combined with supervised fine-tuning for even better performance.

## Limitations
- Performance gains measured primarily on STS and clustering benchmarks; generalization to other downstream tasks unclear
- Limited comparison with supervised sentence embedding models; does not demonstrate ability to match fine-tuned representations
- Evaluation focused on a fixed set of aggregation and post-processing strategies; alternative approaches not explored

## Confidence
- High: Relative gains achieved by proposed methods within evaluated benchmarks
- Medium: Absolute performance and generalizability to diverse real-world applications

## Next Checks
1. Evaluate the proposed sentence embedding methods on a diverse set of downstream tasks (e.g., classification, information retrieval, QA) to assess practical utility beyond STS and clustering
2. Compare the proposed methods against state-of-the-art supervised and unsupervised sentence embedding models on multiple benchmarks to establish relative effectiveness
3. Investigate alternative aggregation and post-processing strategies (e.g., attention-based pooling, dynamic weighting, contrastive objectives) to determine if further improvements are possible