---
ver: rpa2
title: Reinforcement Learning for Sequence Design Leveraging Protein Language Models
arxiv_id: '2407.03154'
source_url: https://arxiv.org/abs/2407.03154
tags:
- sequences
- protein
- sequence
- should
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates using reinforcement learning (RL) to generate
  protein sequences by leveraging protein language models (PLMs) as reward functions.
  A core challenge is that querying PLMs directly is computationally expensive.
---

# Reinforcement Learning for Sequence Design Leveraging Protein Language Models

## Quick Facts
- arXiv ID: 2407.03154
- Source URL: https://arxiv.org/abs/2407.03154
- Authors: Jithendaraa Subramanian; Shivakanth Sujit; Niloy Irtisam; Umong Sain; Riashat Islam; Derek Nowrouzezahrai; Samira Ebrahimi Kahou
- Reference count: 40
- Primary result: RL methods achieve high pTM scores and strong diversity metrics, with PPO and SAC performing best, and all RL methods matching oracle performance in the proxy-finetuning setting while reducing runtime by ~10×.

## Executive Summary
This paper investigates using reinforcement learning (RL) to generate protein sequences by leveraging protein language models (PLMs) as reward functions. The authors propose a proxy-finetuning approach where a smaller transformer model approximates PLM scores and is periodically updated during RL training to address computational costs. Experiments on sequences of length 50 and 100 compare multiple RL algorithms against evolutionary baselines, showing RL methods achieve high pTM scores and strong diversity metrics while significantly reducing runtime.

## Method Summary
The approach frames protein sequence design as an RL problem where sequences are states, mutations are actions, and PLM scores are rewards. A proxy transformer model is trained to approximate the expensive PLM oracle's scores and is periodically finetuned with true oracle scores during training. Multiple RL algorithms (PPO, SAC, Rainbow, DQN, PPO-RND, GFlowNets) are compared against evolutionary baselines on sequences of length 50 and 100, with evaluation focusing on pTM scores for biological plausibility and diversity metrics (MP-TM, MP-RMSD, MP-HD).

## Key Results
- RL methods achieve high pTM scores and strong diversity metrics, with PPO and SAC performing best in oracle setting
- All RL methods match oracle performance in proxy-finetuning setting while reducing runtime by ~10×
- RL methods significantly outperform evolutionary baselines (MH-MCMC) in both score and diversity metrics
- Proxy-finetuning approach maintains high correlation with oracle while enabling efficient training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RL-based sequence design using PLMs as reward functions can outperform evolutionary algorithms in both score and diversity.
- Mechanism: The policy learns to explore the combinatorial sequence space efficiently by receiving dense, differentiable feedback from the PLM (via proxy or oracle) and updating mutations accordingly.
- Core assumption: The PLM's score correlates with biological plausibility and provides a useful gradient signal for RL optimization.
- Evidence anchors: Experimental results show RL methods achieve high pTM scores and strong diversity metrics compared to baselines.

### Mechanism 2
- Claim: Periodic finetuning of a smaller proxy model with true PLM scores reduces computational cost while maintaining performance.
- Mechanism: The proxy is trained on pairs of sequences and their oracle scores, then used for fast reward evaluation; periodic updates with oracle scores prevent drift.
- Core assumption: The proxy model can approximate the PLM's scores with high correlation and is cheap to query and update.
- Evidence anchors: Results show ESM-PF (proxy-finetuning) achieves comparable performance to direct oracle optimization while reducing runtime by ~10×.

### Mechanism 3
- Claim: Multi-objective evaluation (pTM, pLDDT, structural and sequence diversity metrics) ensures both biological plausibility and novelty.
- Mechanism: Diversity metrics prevent collapse to a narrow region of sequence space while high pTM/pLDDT scores ensure functional viability.
- Core assumption: High pTM and pLDDT scores correlate with good structure and function; diversity metrics meaningfully capture novelty.
- Evidence anchors: The paper reports multiple diversity metrics (MP-TM, MP-RMSD, MP-HD) alongside biological plausibility scores to provide comprehensive evaluation.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation of sequence design
  - Why needed here: The paper frames sequence mutation as an RL problem with states as sequences, actions as mutations, and rewards from PLMs.
  - Quick check question: In the MDP, what is the state representation for a protein sequence of length L?
    - Answer: A one-hot encoding vector of size L x 20 (alphabet size).

- Concept: Proxy-based knowledge distillation
  - Why needed here: To reduce computational cost of querying the large PLM, a smaller transformer is trained to approximate its scores.
  - Quick check question: How is the proxy model trained in this work?
    - Answer: On pairs of sequences and their oracle PLM scores using mean squared regression.

- Concept: Multi-objective evaluation in protein design
  - Why needed here: Ensures generated sequences are both high-scoring (plausible) and diverse (novel).
  - Quick check question: Which two structural diversity metrics are used in the paper?
    - Answer: Mean Pairwise TM score (MP-TM) and Mean Pairwise RMSD (MP-RMSD).

## Architecture Onboarding

- Component map: Random sequence initialization -> RL agent (PPO/SAC/etc.) -> Proxy model or Oracle (ESMFold) -> Reward score -> Policy update -> Diversity evaluation

- Critical path:
  1. Initialize random sequence(s)
  2. RL agent proposes mutations
  3. Query proxy for scores (fast) or oracle (slow)
  4. If using proxy, periodically finetune proxy on oracle scores
  5. Update RL policy
  6. Periodically evaluate diversity and plausibility

- Design tradeoffs:
  - Oracle vs. proxy: Accuracy vs. speed
  - Episode length: Shorter for diversity, longer for high scores
  - Proxy size: Smaller is faster but may underfit

- Failure signatures:
  - Proxy scores diverge from oracle → RL optimizes wrong objective
  - RL policy collapses to few sequences → diversity metrics drop
  - RL algorithm fails to improve → check reward signal quality

- First 3 experiments:
  1. Compare PPO on proxy vs. oracle (check runtime vs. score trade-off)
  2. Run MCMC baseline for comparison (diversity vs. plausibility)
  3. Vary proxy finetuning interval (check impact on correlation and performance)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RL-based protein sequence design methods scale with increasing sequence length beyond 100 amino acids?
- Basis in paper: [explicit] The paper notes that results are constrained by sequence length due to prohibitive computational requirements for longer sequences, and suggests future work should explore larger proxy reward models for longer sequences.
- Why unresolved: The experiments only tested sequences up to length 100, and the authors explicitly state that computational costs limit their ability to test longer sequences.
- What evidence would resolve it: Experimental results showing RL performance on sequences of 200+ amino acids, comparing proxy-based and oracle-based approaches.

### Open Question 2
- Question: What is the impact of using different protein language models (PLMs) as reward functions on the quality and diversity of generated sequences?
- Basis in paper: [explicit] The authors suggest extending results to optimization on other PLMs such as AlphaFold2 or the larger 15B parameter ESMFold model, and mention their implementation supports replacing the reward model.
- Why unresolved: The experiments only used the 3B ESMFold model, and the authors acknowledge this as a limitation.
- What evidence would resolve it: Comparative experiments using multiple PLMs (e.g., AlphaFold2, 15B ESMFold, RoseTTAFold) as reward functions, measuring both performance and computational efficiency.

### Open Question 3
- Question: How robust are the proxy-finetuning approach and RL algorithms to variations in the quality and reliability of the oracle reward model?
- Basis in paper: [inferred] The paper discusses the possibility that reward models could be imperfect or misspecified, and mentions comparing outputs of ESMFold with AlphaFold to quantify uncertainty.
- Why unresolved: The experiments assume the oracle model (ESMFold) is perfect, and do not test how well the methods perform when the oracle is noisy or biased.
- What evidence would resolve it: Experiments where the oracle reward model is intentionally corrupted with noise or systematic biases, measuring the degradation in proxy and RL performance.

## Limitations

- The study focuses on relatively short sequences (50-100 amino acids), limiting generalizability to full-length proteins
- While diversity metrics are reported, the biological relevance of generated sequences beyond PLM scores is not experimentally validated
- The proxy model's performance heavily depends on the quality of oracle-PLM score correlations, which may degrade for longer or more complex sequences

## Confidence

- High confidence: RL algorithms achieving high pTM scores and matching oracle performance in proxy settings
- Medium confidence: Diversity metrics meaningfully capturing sequence novelty
- Low confidence: Generalization to longer sequences and real-world biological function

## Next Checks

1. Test proxy-finetuning performance on sequences >100 amino acids to assess scalability
2. Validate generated sequences using wet-lab experiments or additional functional prediction tools beyond PLM scores
3. Compare proxy-RL performance against additional baselines including generative models and structure-based methods