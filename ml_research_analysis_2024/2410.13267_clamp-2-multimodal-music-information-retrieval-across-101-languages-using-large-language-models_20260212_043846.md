---
ver: rpa2
title: 'CLaMP 2: Multimodal Music Information Retrieval Across 101 Languages Using
  Large Language Models'
arxiv_id: '2410.13267'
source_url: https://arxiv.org/abs/2410.13267
tags:
- music
- clamp
- note
- midi
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLaMP 2 addresses challenges in managing linguistic diversity and
  integrating musical modalities in music information retrieval. It leverages large
  language models to refine multilingual descriptions, reducing noise and balancing
  language distribution.
---

# CLaMP 2: Multimodal Music Information Retrieval Across 101 Languages Using Large Language Models

## Quick Facts
- arXiv ID: 2410.13267
- Source URL: https://arxiv.org/abs/2410.13267
- Reference count: 17
- Supports 101 languages for music information retrieval across ABC notation and MIDI formats

## Executive Summary
CLaMP 2 addresses the challenge of linguistic diversity in music information retrieval by leveraging large language models to generate refined multilingual descriptions, reducing noise and balancing language distribution. The system unifies ABC notation and MIDI through novel encoding techniques and employs contrastive learning to align multilingual text with multimodal music data. Pre-trained on 1.5 million ABC-MIDI-text triplets, CLaMP 2 achieves state-of-the-art results in multilingual semantic search and music classification across modalities.

## Method Summary
CLaMP 2 employs a multilingual text encoder (XLM-R) and a multimodal music encoder (M3-based) aligned through contrastive learning. The system uses GPT-4 to refine and enrich pre-training data with multilingual descriptions across 101 languages. The music encoder supports both ABC notation and MIDI formats through novel interleaved encoding techniques. Training involves contrastive alignment of text-music pairs from a dataset of 1.5 million triplets, with evaluation on music classification and semantic search tasks.

## Key Results
- Achieves state-of-the-art performance in multilingual semantic search across 101 languages
- Demonstrates superior music classification accuracy for both ABC notation and MIDI formats
- Shows significant improvements over single-modality approaches through joint training

## Why This Works (Mechanism)

### Mechanism 1
GPT-4-generated multilingual text significantly improves multilingual semantic search by reducing noise and balancing language distribution. The LLM filters out non-musical metadata and generates concise, coherent summaries in multiple languages, enriching the dataset with high-quality, linguistically diverse descriptions.

### Mechanism 2
Joint training on ABC notation and MIDI enhances feature extraction quality for both modalities. The multimodal music encoder processes both symbolic music formats, allowing the model to learn shared musical representations across modalities.

### Mechanism 3
Contrastive learning aligns multilingual text and multimodal music data in a shared representation space. The model minimizes distances between aligned text-music pairs and maximizes distances for unpaired data, creating semantically meaningful clusters.

## Foundational Learning

- **Cross-modal contrastive learning**: Why needed - To align multilingual text and multimodal music data in a shared semantic space. Quick check - How does contrastive learning ensure that semantically related text and music representations are close in the embedding space?
- **Multilingual text encoding**: Why needed - To process text queries in 101 languages for global music retrieval. Quick check - Why is XLM-R chosen as the text encoder, and how does it support low-resource languages?
- **Symbolic music representation**: Why needed - To handle both ABC notation and MIDI as input formats for music encoding. Quick check - What are the key differences between ABC notation and MIDI, and why is joint training beneficial?

## Architecture Onboarding

- **Component map**: Multilingual text encoder (XLM-R-base) → Multimodal music encoder (M3-based) → Contrastive learning framework → Shared representation → Downstream tasks
- **Critical path**: Text/Music → Encoder → Contrastive Alignment → Shared Representation → Downstream Tasks
- **Design tradeoffs**: Joint training on ABC and MIDI improves generalization but increases computational cost; LLM-generated text enhances multilingual retrieval but may introduce translation artifacts; contrastive learning aligns modalities but may lose fine-grained temporal details
- **Failure signatures**: Poor multilingual retrieval → Check GPT-4 filtering and XLM-R tokenization; Degraded music classification → Verify M3 encoding and modality alignment; Unstable contrastive training → Inspect batch sampling and logit scaling
- **First 3 experiments**:
  1. Train M3 on ABC-only, MIDI-only, and both modalities to verify joint training benefits
  2. Evaluate semantic search with and without GPT-4-generated text to measure multilingual gains
  3. Test cross-modal retrieval (e.g., ABC query → MIDI result) to validate shared representation quality

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of LLM-generated text in low-resource languages impact CLaMP 2's multilingual semantic search performance compared to high-resource languages? The paper notes that LLM-generated responses in low-resource languages like Amharic did not conform to the expected format as well as high-resource languages, resulting in fewer entries for these languages.

### Open Question 2
What is the optimal balance between LLM-generated text and original metadata for maximizing CLaMP 2's performance across different tasks? The ablation studies show that removing LLM-generated text affects performance differently across tasks, but don't provide guidance on optimal ratios.

### Open Question 3
How would CLaMP 2's performance change if trained on truly multilingual music-text datasets rather than translated English benchmarks? The authors acknowledge the lack of multilingual music-text benchmarks and resort to machine translation, noting that translation quality directly affects retrieval effectiveness.

## Limitations
- Effectiveness depends on quality of GPT-4-generated multilingual descriptions across 101 languages
- Conversion between ABC notation and MIDI formats may introduce information loss
- Contrastive learning may struggle with fine-grained temporal details in musical data
- Evaluation focuses on specific tasks and may not generalize to all MIR applications

## Confidence

- **High Confidence**: Architectural design choices (XLM-R, M3-based encoder, contrastive learning) are well-established and align with current best practices
- **Medium Confidence**: Reported improvements are promising but may be sensitive to GPT-4 quality and implementation details
- **Low Confidence**: "State-of-the-art results" claim requires independent validation due to lack of direct comparisons

## Next Checks

1. Cross-lingual transfer evaluation on low-resource languages to assess robustness of GPT-4-generated descriptions
2. Modality-specific ablation study to quantify benefits of joint training and identify weaknesses
3. Human evaluation of LLM-generated multilingual descriptions for quality, coherence, and musical relevance