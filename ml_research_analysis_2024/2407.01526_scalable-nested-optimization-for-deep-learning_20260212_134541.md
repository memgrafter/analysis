---
ver: rpa2
title: Scalable Nested Optimization for Deep Learning
arxiv_id: '2407.01526'
source_url: https://arxiv.org/abs/2407.01526
tags:
- optimization
- page
- cited
- momentum
- hyperparameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis develops scalable methods for nested optimization in
  deep learning, focusing on hyperparameter optimization and generative adversarial
  networks (GANs). Chapter 2 introduces a hypernetwork-based approach that learns
  to map hyperparameters to optimal neural network weights, enabling efficient gradient-based
  hyperparameter optimization.
---

# Scalable Nested Optimization for Deep Learning

## Quick Facts
- arXiv ID: 2407.01526
- Source URL: https://arxiv.org/abs/2407.01526
- Authors: Jonathan Lorraine
- Reference count: 0
- This thesis develops scalable methods for nested optimization in deep learning, focusing on hyperparameter optimization and generative adversarial networks (GANs).

## Executive Summary
This thesis addresses the challenge of scalable nested optimization in deep learning, proposing novel methods for hyperparameter optimization and training in adversarial settings. The work introduces hypernetwork-based approaches that learn to map hyperparameters to optimal neural network weights, enabling efficient gradient-based hyperparameter optimization. Additionally, it presents implicit differentiation methods that scale to millions of hyperparameters by approximating the inverse Hessian using relationships with unrolled differentiation. The thesis also generalizes gradient descent with momentum for games by introducing complex-valued momentum, improving convergence in adversarial settings. These methods are evaluated on various tasks including dataset distillation, learned data augmentation, RNN hyperparameter tuning, and training BigGAN, demonstrating improved performance and scalability compared to existing approaches.

## Method Summary
The thesis develops several novel approaches to nested optimization in deep learning. Chapter 2 introduces a hypernetwork-based approach that learns to map hyperparameters to optimal neural network weights, enabling efficient gradient-based hyperparameter optimization. Chapter 3 proposes an implicit differentiation method that scales to millions of hyperparameters by approximating the inverse Hessian using a relationship with unrolled differentiation. Chapter 4 generalizes gradient descent with momentum for games by introducing complex-valued momentum, improving convergence in adversarial settings. Chapter 5 extends Ridge Rider, an algorithm for finding diverse solutions in single-objective optimization, to multi-agent games using Lyapunov exponents to locate bifurcations. These methods are evaluated on various tasks including dataset distillation, learned data augmentation, RNN hyperparameter tuning, and training BigGAN, demonstrating improved performance and scalability compared to existing approaches.

## Key Results
- Hypernetwork-based approach enables efficient gradient-based hyperparameter optimization
- Implicit differentiation method scales to millions of hyperparameters
- Complex-valued momentum improves convergence in adversarial settings
- Lyapunov exponents help locate bifurcations in multi-agent games

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hypernetworks approximate the best-response function $w^*_ϕ(λ)$ of neural network weights to hyperparameters.
- **Mechanism:** A hypernetwork parameterized by $ϕ$ takes hyperparameters $λ$ as input and outputs weights $w_ϕ(λ)$ that approximate the optimal weights $w^*(λ)$ from the inner optimization loop. This collapses the nested optimization into a joint stochastic optimization of both the hypernetwork and hyperparameters.
- **Core assumption:** The hypernetwork can learn to approximate the best-response function locally around the current hyperparameter setting.
- **Evidence anchors:**
  - [abstract]: "We train a hypernetwork that takes in hyperparameters and outputs neural network weights."
  - [section 2.2.1]: "We propose to learn this function. We train a neural network that takes hyperparameters as input and outputs an approximately optimal set of weights."
  - [corpus]: Weak - no corpus evidence found for hypernetwork approximation quality.
- **Break condition:** The hypernetwork cannot learn the best-response function accurately enough, especially in regions far from the current hyperparameter setting.

### Mechanism 2
- **Claim:** The implicit function theorem provides a way to compute hypergradients without differentiating through optimization.
- **Mechanism:** By leveraging the IFT, the best-response Jacobian $\frac{∂w^*}{∂λ}$ can be expressed as a matrix equation involving the inverse of the training Hessian. This allows computation of the hypergradient $\frac{∂L^*_V}{∂λ}$ without unrolling optimization.
- **Core assumption:** The training Hessian is invertible at the best-response weights.
- **Evidence anchors:**
  - [abstract]: "We propose an inexpensive gradient-based hyperparameter optimization algorithm that combines the implicit function theorem (IFT) with efficient inverse Hessian approximations."
  - [section 3.2]: "We leverage the IFT to evaluate the best-response Jacobian locally."
  - [corpus]: Weak - no corpus evidence found for IFT application to large-scale neural networks.
- **Break condition:** The training Hessian is singular or too ill-conditioned to invert accurately.

### Mechanism 3
- **Claim:** Complex momentum can mitigate rotational dynamics in adversarial games by storing historical gradient information with oscillating addition/subtraction.
- **Mechanism:** Complex-valued momentum coefficients allow the momentum buffer to oscillate between adding and subtracting past gradients at a frequency determined by the momentum's phase. This reduces rotational dynamics during training by canceling out opposing updates.
- **Core assumption:** The spectrum of the Jacobian of the joint-gradient field has both real and imaginary eigenvalues.
- **Evidence anchors:**
  - [abstract]: "We generalize gradient descent with momentum for optimization in differentiable games to have complex-valued momentum."
  - [section 4.3.1]: "The final line is simply by Euler's formula. Equation 4.8 shows how β controls the momentum buffer µ by having |β| dictate the decay rates of the prior gradient, while arg(β) controls the oscillation frequency."
  - [corpus]: Weak - no corpus evidence found for complex momentum effectiveness.
- **Break condition:** The game dynamics do not exhibit significant rotational behavior, or the spectrum is dominated by purely real eigenvalues.

## Foundational Learning

- **Concept:** Nested optimization (bilevel optimization)
  - Why needed here: Understanding the relationship between hyperparameters and model weights is crucial for developing efficient optimization methods.
  - Quick check question: What is the difference between a direct and indirect hypergradient?

- **Concept:** Implicit Function Theorem
  - Why needed here: The IFT provides a theoretical foundation for computing hypergradients without differentiating through optimization.
  - Quick check question: Under what conditions can we apply the IFT to compute the best-response Jacobian?

- **Concept:** Lyapunov exponents and bifurcations
  - Why needed here: These concepts help identify regions in parameter space where small perturbations lead to qualitatively different optimization trajectories, enabling diverse solution finding.
  - Quick check question: How do Lyapunov exponents relate to trajectory separation in optimization?

## Architecture Onboarding

- **Component map:** Chapter 1 (Introduction) -> Chapter 2 (Hypernetworks for Hyperparameter Optimization) -> Chapter 3 (Implicit Differentiation for Large-Scale Hyperparameter Tuning) -> Chapter 4 (Complex Momentum for Games) -> Chapter 5 (Lyapunov-based Bifurcation Analysis in Games) -> Chapter 6 (Conclusions and Future Work)

- **Critical path:** The key insight connecting all chapters is the challenge of efficiently finding solutions in nested optimization problems, where different subsets of parameters update on different objectives. Each chapter provides a different approach to addressing this challenge.

- **Design tradeoffs:** The various methods presented balance computational efficiency, memory usage, and convergence guarantees. For example, hypernetworks are computationally efficient but may not scale well, while implicit differentiation is more scalable but requires matrix inversion approximations.

- **Failure signatures:** Common failure modes include: hypernetwork approximation errors leading to suboptimal hyperparameter tuning, ill-conditioned Hessians causing instability in implicit differentiation, and inappropriate momentum choices leading to convergence issues in adversarial games.

- **First 3 experiments:**
  1. Implement and compare hypernetwork-based hyperparameter optimization with standard methods on a small-scale problem.
  2. Evaluate the effectiveness of implicit differentiation for hyperparameter optimization on a medium-scale problem.
  3. Test complex momentum on a simple adversarial game to verify its ability to mitigate rotational dynamics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal way to introduce and regularize hyperparameters to prevent overfitting when tuning millions of hyperparameters?
- Basis in paper: [inferred] The paper demonstrates that introducing millions of hyperparameters can lead to overfitting the validation dataset, and discusses the need for better methods to introduce many hyperparameters.
- Why unresolved: The paper does not provide a concrete solution or methodology for this problem, only noting that it is an area for future investigation.
- What evidence would resolve it: Experiments comparing different regularization techniques and hyperparameter introduction strategies on various tasks and datasets, demonstrating improved generalization and reduced overfitting.

### Open Question 2
- Question: How can we effectively find and leverage the structure of the Jacobian of the joint-gradient vector field in games to guide the choice of optimizer parameters for each player?
- Basis in paper: [explicit] The paper shows that the structure of the spectrum of the Jacobian of the joint-gradient vector field differs between players in GANs and may motivate separate optimizer choices.
- Why unresolved: The paper does not provide a concrete method for finding or leveraging this structure, only suggesting it as a potential direction for future work.
- What evidence would resolve it: Development and demonstration of an algorithm that can effectively analyze and utilize the structure of the Jacobian of the joint-gradient vector field to select optimal optimizer parameters for each player in a game, leading to improved convergence and performance.

### Open Question 3
- Question: How can we design optimizers that are robust to a wide range of mixtures of cooperative and adversarial eigenspaces in games, without requiring explicit knowledge of the spectrum?
- Basis in paper: [explicit] The paper introduces complex momentum as a method that is robust to different mixtures of cooperative and adversarial eigenspaces, but notes that finding the optimal parameters for this method may be challenging without knowledge of the spectrum.
- Why unresolved: The paper does not provide a concrete method for selecting the optimal parameters for complex momentum without explicit knowledge of the spectrum, only suggesting that almost-positive momentum may be a good initial guess.
- What evidence would resolve it: Development and demonstration of an adaptive method for selecting the parameters of complex momentum (or a similar robust optimizer) that can effectively handle a wide range of eigenspace mixtures without requiring explicit knowledge of the spectrum, leading to improved convergence and performance across various games.

## Limitations

- The hypernetwork approach relies on the assumption that the best-response function can be locally approximated, yet the approximation quality for deep networks with millions of parameters remains untested.
- The implicit differentiation method assumes invertible Hessians, but real-world neural networks often produce ill-conditioned matrices.
- The complex momentum approach introduces additional hyperparameters (magnitude and phase) without clear guidelines for selection.
- The Lyapunov-based bifurcation analysis in games is theoretically sound but computationally expensive, potentially limiting its practical applicability.

## Confidence

- **High confidence**: The theoretical framework connecting nested optimization to bilevel optimization is well-established. The relationship between implicit differentiation and unrolled optimization is mathematically rigorous.
- **Medium confidence**: The practical implementations of the proposed methods show promise on benchmark tasks, but comprehensive ablation studies are limited. The scalability claims need validation on state-of-the-art architectures.
- **Low confidence**: The effectiveness of complex momentum in diverse game scenarios and the practical utility of the Lyapunov-based bifurcation detection algorithm have not been thoroughly demonstrated across multiple domains.

## Next Checks

1. **Hypernetwork Generalization Test**: Evaluate the hypernetwork-based hyperparameter optimization method on a significantly larger network (e.g., ResNet-50 or Transformer) with hundreds of hyperparameters to assess true scalability and approximation quality.

2. **Hessian Conditioning Analysis**: Systematically analyze the conditioning of training Hessians across different architectures and datasets to quantify the impact of ill-conditioning on the implicit differentiation approach's stability and accuracy.

3. **Complex Momentum Ablation**: Conduct a comprehensive ablation study varying the magnitude and phase of complex momentum coefficients across multiple game types (e.g., GAN variants, actor-critic methods) to establish guidelines for hyperparameter selection and demonstrate consistent performance improvements.