---
ver: rpa2
title: Neural Network Plasticity and Loss Sharpness
arxiv_id: '2409.17300'
source_url: https://arxiv.org/abs/2409.17300
tags:
- learning
- loss
- plasticity
- sharpness
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of sharpness regularization techniques,
  specifically Sharpness-Aware Minimization (SAM) and Gradient Norm Penalty (GNP),
  to address plasticity loss in continual learning settings. The authors conduct experiments
  on both domain-incremental and class-incremental learning tasks using the MNIST
  dataset.
---

# Neural Network Plasticity and Loss Sharpness

## Quick Facts
- arXiv ID: 2409.17300
- Source URL: https://arxiv.org/abs/2409.17300
- Authors: Max Koster; Jude Kukla
- Reference count: 2
- One-line primary result: Sharpness regularization techniques (SAM and GNP) do not significantly reduce plasticity loss in continual learning settings

## Executive Summary
This paper investigates whether sharpness-aware regularization techniques can mitigate plasticity loss in continual learning scenarios. The authors test Sharpness-Aware Minimization (SAM) and Gradient Norm Penalty (GNP) on both domain-incremental and class-incremental learning tasks using the MNIST dataset. Contrary to expectations, the results show that these techniques do not significantly improve plasticity compared to standard SGD optimization. The findings suggest that while sharpness regularization may help with generalization in stationary settings, it does not effectively address the unique challenges of continual learning where maintaining performance across multiple sequential tasks is crucial.

## Method Summary
The study tests three optimization methods (SGD, SAM, and GNP) with two learning rates each (0.01 and 0.001) on MNIST-based continual learning tasks. For domain-incremental learning, the authors create 100 tasks by randomly permuting pixel positions. For class-incremental learning, they construct 45 tasks by pairing MNIST classes (0-1, 0-2, ..., 8-9). The neural network architecture consists of a 28×28 input layer, three fully connected linear layers with ReLU activation, and a 10-hot or 2-hot output layer depending on the task type. Performance is evaluated by tracking task-specific test accuracy across multiple runs (10 per setting).

## Key Results
- Sharpness regularization techniques showed no significant improvement in reducing plasticity loss across tasks
- In domain-incremental learning, all methods performed similarly with SGD (α = 0.001) showing the best performance
- In class-incremental learning, SGD maintained stable performance while SAM showed significant plasticity loss; GNP was most effective at maintaining plasticity in this setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sharpness regularization techniques are expected to reduce plasticity loss by finding flatter minima that generalize better across tasks
- Mechanism: These methods penalize loss sharpness by either optimizing within an epsilon neighborhood (SAM) or penalizing gradient norms (GNP), theoretically leading to minima that maintain performance across multiple tasks
- Core assumption: Flatter minima discovered through sharpness regularization will transfer better between related tasks, preserving plasticity
- Evidence anchors:
  - [abstract] "Our findings indicate that such techniques have no significant effect on reducing plasticity loss"
  - [section] "Our findings show that applying sharpness regularization alone to promote neural network plasticity will not effectively work"
  - [corpus] No strong direct evidence in corpus; related papers focus on different plasticity mechanisms
- Break condition: If the relationship between loss landscape sharpness and plasticity is not causal, or if the tasks used do not adequately represent continual learning scenarios

### Mechanism 2
- Claim: Gradient norm penalty (GNP) may help maintain plasticity in class-incremental learning by reducing loss landscape sharpness
- Mechanism: By penalizing the gradient norm, GNP effectively reduces the Lipschitz constant in a neighborhood of the minimum, creating a flatter loss surface that should theoretically support better adaptation to new classes
- Core assumption: The gradient norm is directly related to loss landscape sharpness, and reducing it will preserve plasticity
- Evidence anchors:
  - [section] "GNP was most effective at maintaining plasticity in this setting" (referring to class-incremental learning)
  - [section] "penalization of the gradient norm is a penalization of the sharpness of a minima"
  - [corpus] Weak evidence; corpus focuses on different plasticity mechanisms like reinitialization and adaptive linearity
- Break condition: If the relationship between gradient norm and plasticity is task-dependent or if the MNIST dataset is not representative of real-world continual learning scenarios

### Mechanism 3
- Claim: Sharpness-aware minimization (SAM) should improve generalization by finding flat minima, which should help in maintaining plasticity across tasks
- Mechanism: SAM optimizes the worst-case loss within a neighborhood, encouraging the model to find parameters where the loss is stable across small perturbations, theoretically preserving performance on multiple tasks
- Core assumption: Flat minima found by SAM will have better generalization properties that transfer across tasks
- Evidence anchors:
  - [section] "SAM showed significant declines in per-task accuracy, indicating substantial plasticity loss" in class-incremental learning
  - [section] "SAM and GNP with higher learning rates showed noticeable declines in per-task accuracy over tasks" in domain-incremental learning
  - [corpus] No direct evidence; related works focus on different aspects of plasticity
- Break condition: If SAM's effectiveness is limited to stationary tasks or if the epsilon neighborhood size is not optimal for continual learning scenarios

## Foundational Learning

- Concept: Loss landscape sharpness and its relation to generalization
  - Why needed here: Understanding why sharpness regularization is expected to help with plasticity requires knowledge of how loss landscape geometry affects model performance across tasks
  - Quick check question: What is the relationship between loss landscape sharpness and a model's ability to generalize to new tasks?

- Concept: Continual learning problem settings (class-incremental vs domain-incremental)
  - Why needed here: The experiments test both types of continual learning, and understanding their differences is crucial for interpreting the results
  - Quick check question: How do class-incremental and domain-incremental learning differ in terms of the challenges they present to maintaining plasticity?

- Concept: Sharpness-aware optimization techniques (SAM and GNP)
  - Why needed here: The paper tests these specific techniques, and understanding their mechanisms is essential for evaluating their effectiveness in reducing plasticity loss
  - Quick check question: How do SAM and GNP differ in their approach to reducing loss landscape sharpness?

## Architecture Onboarding

- Component map:
  - Data preprocessing (MNIST permutation for domain-incremental, pairwise class extraction for class-incremental) -> Feed-forward neural network (28×28 input -> three FC layers with ReLU -> 10-hot/2-hot output) -> Training loop (SGD/SAM/GNP with α = 0.01/0.001) -> Task-specific accuracy evaluation after each task -> Results aggregation and analysis

- Critical path:
  1. Data preparation (MNIST permutation or class pairing)
  2. Model initialization with specified architecture
  3. Training loop with chosen optimization method
  4. Task-specific accuracy evaluation after each task
  5. Aggregation and analysis of results across runs

- Design tradeoffs:
  - Using a simple feed-forward network vs. more complex architectures (easier to implement but may not capture all relevant dynamics)
  - Testing only two learning rates for each method (comprehensive but may miss optimal configurations)
  - Using MNIST dataset (well-understood but may not generalize to more complex scenarios)

- Failure signatures:
  - No improvement in plasticity across tasks with any method
  - Significant performance degradation with SAM in class-incremental learning
  - Similar performance across all methods in domain-incremental learning

- First 3 experiments:
  1. Reproduce SGD baseline results on domain-incremental learning to establish performance baseline
  2. Test GNP with optimal learning rate (0.01) on class-incremental learning to verify its effectiveness
  3. Test SAM with different epsilon neighborhood sizes to explore parameter sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the lack of effectiveness of sharpness regularization techniques in reducing plasticity loss generalize to more complex datasets beyond Permuted MNIST?
- Basis in paper: [explicit] The paper acknowledges that there is "some question on the relevance of the Permuted MNIST dataset as an adequate example of continual learning" and suggests testing on "more complex benchmark datasets"
- Why unresolved: The experiments were only conducted on MNIST dataset variations (Permuted MNIST for domain-incremental and pairwise class MNIST for class-incremental learning). These are relatively simple datasets that may not capture the full complexity of real-world continual learning scenarios.
- What evidence would resolve it: Conducting similar experiments on more complex datasets like CIFAR-10/100, TinyImageNet, or RL benchmarks with non-stationary reward functions would provide evidence for whether the ineffectiveness of sharpness regularization generalizes to more challenging tasks.

### Open Question 2
- Question: Is there a causal relationship between loss landscape sharpness and plasticity loss, or is there an unknown confounding factor?
- Basis in paper: [explicit] The paper states "there is also the possibility that despite the empirical observations of the relation between sharpness and plasticity, this is not causal and there exists some (currently) unknown confounding factor"
- Why unresolved: The paper builds its hypothesis on previous work showing a correlation between sharpness and plasticity loss, but correlation does not imply causation. The experiments only tested sharpness regularization techniques without investigating other potential factors.
- What evidence would resolve it: Systematic ablation studies that control for various network properties (weight norms, feature rank, etc.) while varying sharpness could help determine if sharpness is truly causal. Alternatively, discovering and testing the hypothesized confounding factor would provide clarity.

### Open Question 3
- Question: Could adaptive or task-specific modifications to sharpness regularization techniques improve their effectiveness in continual learning settings?
- Basis in paper: [explicit] The conclusion mentions "Adaptations to the sharpness regularization terms may also further improve continual learning performance—there is still much of the parameter space to be explored"
- Why unresolved: The experiments used standard implementations of SAM and GNP with fixed hyperparameters (λ = 0.1). The paper only explored a limited parameter space and didn't investigate adaptive or task-specific variants of these techniques.
- What evidence would resolve it: Testing adaptive sharpness regularization methods that adjust their strength based on task difficulty or learning stage, or developing task-specific variants that account for the continual learning setting, would provide evidence for whether modified approaches could be effective.

## Limitations
- Results based solely on MNIST dataset, which may not generalize to more complex scenarios
- Simplified feed-forward network architecture may not capture dynamics of deeper models
- Limited hyperparameter exploration (only two learning rates tested per method)

## Confidence
- High confidence in the finding that sharpness regularization does not significantly improve plasticity in domain-incremental learning
- Medium confidence in the observation that GNP maintains plasticity better than SAM in class-incremental learning
- Medium confidence in the overall conclusion that sharpness regularization alone is insufficient for improving neural network plasticity

## Next Checks
1. Cross-dataset validation: Replicate experiments on CIFAR-10 or miniImageNet to test whether results generalize beyond MNIST
2. Architecture scaling: Test the same sharpness regularization techniques on a convolutional neural network to determine if architecture depth affects plasticity outcomes
3. Hyperparameter optimization: Conduct a systematic grid search over SAM epsilon values and GNP lambda coefficients to identify optimal configurations for continual learning scenarios