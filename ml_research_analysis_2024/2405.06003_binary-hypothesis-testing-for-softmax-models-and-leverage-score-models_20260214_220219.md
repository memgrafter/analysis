---
ver: rpa2
title: Binary Hypothesis Testing for Softmax Models and Leverage Score Models
arxiv_id: '2405.06003'
source_url: https://arxiv.org/abs/2405.06003
tags:
- arxiv
- softmax
- testing
- leverage
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the problem of binary hypothesis testing for
  softmax models and leverage score models, which are widely used in machine learning
  and linear algebra. Given two models, the goal is to determine which one is the
  true model by making queries to the unknown model.
---

# Binary Hypothesis Testing for Softmax Models and Leverage Score Models

## Quick Facts
- **arXiv ID**: 2405.06003
- **Source URL**: https://arxiv.org/abs/2405.06003
- **Reference count**: 21
- **Primary result**: Binary hypothesis testing for softmax and leverage score models has sample complexity asymptotically O(ε⁻²)

## Executive Summary
This paper studies the problem of distinguishing between two softmax or leverage score models through query access, with applications to understanding large language model behavior. The authors establish that the sample complexity for this binary hypothesis testing problem is asymptotically O(ε⁻²) where ε measures the distance between model parameters. They draw an analogy between softmax and leverage score models, showing similar theoretical results for both, and provide local analysis for small perturbations. The work provides fundamental insights into how many queries are needed to distinguish between different model parameterizations.

## Method Summary
The paper employs information-theoretic analysis using Hellinger distance bounds to establish lower bounds on sample complexity, and variance-based algorithms to achieve matching upper bounds. For softmax models, the algorithm queries along directions that maximize output variance under one hypothesis, while for leverage score models it uses a similar approach with projection matrices. The analysis assumes energy-constrained queries for softmax models and bounded probability constraints for leverage score models. Local analysis for small perturbations (B = A + εM) shows O(ε⁻²ν) complexity where ν depends on the specific matrices and query direction.

## Key Results
- Binary hypothesis testing for softmax models has sample complexity Θ(δ⁻²) where δ is the maximum Hellinger distance over allowed inputs
- Leverage score models have sample complexity O(ε⁻¹) under appropriate constraints
- Local analysis shows O(ε⁻²ν) queries suffice when B = A + εM for small ε
- Both models share structural similarities in their sample complexity behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Softmax binary hypothesis testing has sample complexity O(ε⁻²)
- Mechanism: Algorithm distinguishes models by querying inputs that maximize variance of model output, providing maximal information gain per query
- Core assumption: Energy-constrained queries prevent pathological inputs from trivially distinguishing similar models
- Evidence anchors: Abstract states O(ε⁻²) complexity; Theorem 3.1 proves Θ(δ⁻²) bound using Hellinger distance

### Mechanism 2
- Claim: Leverage score models have sample complexity O(ε⁻¹)
- Mechanism: Queries maximize variance of statistics involving diagonal projection matrices
- Core assumption: Bounded probability constraints prevent degenerate cases
- Evidence anchors: Abstract mentions similar results for leverage scores; Theorem 4.2 provides lower bound using operator norm differences

### Mechanism 3
- Claim: Local analysis gives O(ε⁻²ν) complexity for small perturbations
- Mechanism: Taylor expansion around A with optimal query direction maximizing variance
- Core assumption: Perturbation εM is sufficiently small for local approximation
- Evidence anchors: Abstract mentions O(ε⁻²ν) bound; Theorems 3.5 and 4.3 provide local upper bounds

## Foundational Learning

- Concept: Hellinger distance and TV distance relationship
  - Why needed here: Provides information-theoretic lower bound for hypothesis testing
  - Quick check question: If two distributions have Hellinger distance 0.1, what is the maximum possible total variation distance between them?

- Concept: Softmax and leverage score distributions as parameterized models
  - Why needed here: Understanding parameterization is crucial for analyzing distinguishability
  - Quick check question: For softmax model with matrix A, what is probability of outputting i-th element given input x?

- Concept: Operator norms and matrix difference measurement
  - Why needed here: Different operator norms (ℓ∞ vs operator norm) quantify when models are "close"
  - Quick check question: What is the ℓ∞→∞ operator norm of matrix A, and how does it relate to maximum row norm?

## Architecture Onboarding

- Component map: Input matrices A and B → Distance computation (Hellinger/Operator norm) → Sample complexity bound application → Optional local analysis
- Critical path: 1) Parse input matrices A and B, 2) Check for constant vector/invertible matrix differences, 3) Compute distance measure, 4) Apply corresponding theorem for sample complexity, 5) For local analysis, find optimal query vector maximizing variance
- Design tradeoffs: Energy constraints prevent pathological cases but may limit algorithm power; operator norm choice reflects different mathematical structures
- Failure signatures: Sample complexity growing with n suggests A and B differ only in rows with negligible probability weight; failure to find optimal query may indicate perturbation too large
- First 3 experiments:
  1. Test softmax algorithm on matrices differing by constant vector to verify indistinguishability identification
  2. Test leverage score algorithm on matrices differing only in one row to verify O(ε⁻¹) bound
  3. Test local analysis on B = A + εM with small ε to verify O(ε⁻²ν) bound and compute ν

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical bounds are asymptotic and don't provide explicit constants
- Energy-constrained and bounded probability assumptions may limit practical applicability
- Local analysis requires small perturbations but doesn't specify how small ε must be
- The connection between softmax and leverage score models remains somewhat superficial

## Confidence
- **High confidence**: O(ε⁻²) sample complexity bound for softmax models - rigorous information-theoretic lower bounds with matching upper bounds
- **Medium confidence**: O(ε⁻¹) sample complexity bound for leverage score models - proof structure parallels softmax but with additional technical complexity
- **Medium confidence**: O(ε⁻²ν) complexity for local analysis - Taylor expansion approach standard but ν characterization depends on optimal query computation

## Next Checks
1. Implement hypothesis testing algorithms for both models and verify predicted sample complexity on tight examples
2. Demonstrate necessity of energy constraint by showing O(ε⁻²) bound fails without it
3. Explicitly compute ν parameter for specific A and M matrices to verify local analysis predictions