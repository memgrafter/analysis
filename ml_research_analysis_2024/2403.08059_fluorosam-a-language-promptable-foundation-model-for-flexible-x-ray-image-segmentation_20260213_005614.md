---
ver: rpa2
title: 'FluoroSAM: A Language-promptable Foundation Model for Flexible X-ray Image
  Segmentation'
arxiv_id: '2403.08059'
source_url: https://arxiv.org/abs/2403.08059
tags:
- x-ray
- fluorosam
- image
- images
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FluoroSAM, a language-promptable foundation
  model for X-ray image segmentation. Unlike prior models limited to specific tasks
  or imaging modalities, FluoroSAM leverages a novel vector quantization approach
  to enable flexible, natural language prompting for diverse X-ray imaging contexts,
  including diagnostic chest X-rays and interventional fluoroscopy.
---

# FluoroSAM: A Language-promptable Foundation Model for Flexible X-ray Image Segmentation

## Quick Facts
- arXiv ID: 2403.08059
- Source URL: https://arxiv.org/abs/2403.08059
- Reference count: 0
- Primary result: Achieves IoU scores of 0.47 on interventional X-rays and 0.50 on chest X-rays with text-only prompting

## Executive Summary
FluoroSAM introduces a language-promptable foundation model for X-ray image segmentation that enables flexible human-machine interaction in clinical workflows. Unlike prior models limited to specific tasks, FluoroSAM leverages vector quantization of text embeddings and a large-scale synthetic dataset to support diverse X-ray imaging contexts including diagnostic chest X-rays and interventional fluoroscopy. The model demonstrates competitive segmentation performance while enabling natural language prompting for organ and surgical tool segmentation.

## Method Summary
The method involves training a language-promptable variant of SAM from scratch using a novel text encoder with vector quantization (VQ) of CLIP embeddings. The model is trained on FluoroSeg, a synthetic dataset of 3 million X-ray images generated from 1,621 CT scans, with pseudo-ground truth masks for 128 organ types and 464 surgical tools. Training uses domain randomization and strong data augmentation techniques, running for 10 epochs with base learning rate of 8 Ã— 10^-4 and batch size of 16 across 2 GPUs.

## Key Results
- Achieves IoU scores of 0.47 and 0.50 on real interventional X-ray images and chest X-rays respectively with text-only prompting
- Performance improves with point prompts, demonstrating the model's interactive capabilities
- Successfully segments 128 organ classes and 464 surgical tools across diverse X-ray imaging contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VQ-based text embedding reduces ambiguity in natural language prompts for X-ray segmentation.
- Mechanism: Vector quantization maps diverse text descriptions for the same anatomical structure into a discrete token space, limiting variability while preserving semantic alignment.
- Core assumption: Different textual descriptions for the same object map to similar VQ tokens, and unrelated descriptions map to distinct tokens.
- Evidence anchors: [abstract] "thanks to the novel incorporation of vector quantization (VQ) of text embeddings in the training process"; [section] "we use a vector quantization (VQ) module on top of a frozen CLIP encoder to provide a more consistent signal to the mask decoder"
- Break condition: If VQ tokens become too coarse-grained, segmentation may fail for fine-grained anatomical distinctions.

### Mechanism 2
- Claim: Synthetic dataset generation from CT scans enables large-scale training without manual annotation.
- Mechanism: CT scans are automatically segmented into organ masks using TotalSegmentator, then projected into synthetic X-ray views using realistic simulation of imaging geometries.
- Core assumption: CT-based segmentation masks accurately reflect the appearance of organs in X-ray projections, and simulated X-rays preserve relevant visual cues for segmentation.
- Evidence anchors: [section] "Each scan is segmented into 128 organ classes using TotalSegmentator... The simulation environment selects a range of standard views from which to render synthetic X-ray images"
- Break condition: If anatomical details lost in X-ray projection differ significantly from CT-derived masks, segmentation accuracy degrades.

### Mechanism 3
- Claim: Strong domain randomization improves sim-to-real transfer for X-ray segmentation.
- Mechanism: Training images are augmented with intensity transformations and geometric augmentations to mimic the variability in real X-ray imaging.
- Core assumption: Augmentations applied during training span the distribution of real X-ray image variations, enabling the model to generalize beyond the synthetic domain.
- Evidence anchors: [section] "we apply strong domain randomization of the image during training to facilitate sim-to-real transfer, including coarse dropout, inversion, blurring, Gaussian contrast adjustment, random windowing, and CLAHE histogram equalization"
- Break condition: If augmentation range is too narrow, model fails on unseen X-ray styles.

## Foundational Learning

- Concept: Vector quantization in multimodal embeddings
  - Why needed here: Enables mapping diverse text prompts to consistent segmentation tokens while retaining language flexibility
  - Quick check question: How does VQ handle two semantically identical but lexically different prompts (e.g., "left lung" vs. "left pulmonary organ")?

- Concept: Sim-to-real transfer via domain randomization
  - Why needed here: Synthetic X-ray images differ systematically from real X-rays; randomization helps bridge this domain gap
  - Quick check question: What augmentation would best simulate the effect of patient positioning variability in fluoroscopy?

- Concept: SAM architecture for promptable segmentation
  - Why needed here: Supports both text and point prompts, enabling interactive and automated workflows
  - Quick check question: Why does SAM predict three masks per prompt, and how is the best mask selected during training?

## Architecture Onboarding

- Component map: Swin-L backbone -> text encoder (CLIP + VQ) -> mask decoder (SAM) -> prompt handling -> output segmentation
- Critical path: Image -> Swin backbone -> feature fusion with VQ-encoded text -> mask decoder -> segmentation mask
- Design tradeoffs: VQ reduces text variability but limits generalizability to unseen organ classes; Swin backbone handles larger synthetic images better than ViT; frozen CLIP keeps text alignment stable
- Failure signatures: Low IoU on real X-rays indicates domain gap; poor segmentation on unusual prompts suggests VQ too coarse; slow inference may indicate Swin-L overhead
- First 3 experiments:
  1. Ablate VQ: train without VQ and compare IoU on text-only prompts
  2. Vary augmentation strength: test segmentation robustness across synthetic/real splits
  3. Test multi-prompt selection: evaluate if IOU head reliably picks best mask during inference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the incorporation of vector quantization (VQ) in the text encoder affect the model's ability to generalize to new segmentation classes not present in the training data?
- Basis in paper: [explicit] The paper mentions that VQ theoretically limits the generalizability of the framework to new segmentation classes, but expands generalizability to new language prompts by reducing variability from disparate descriptions for the same object.
- Why unresolved: The paper does not provide empirical evidence or quantitative results on how VQ affects the model's performance when encountering new segmentation classes not seen during training.
- What evidence would resolve it: Conducting experiments with a held-out test set containing new segmentation classes and comparing the performance of FluoroSAM with and without VQ would provide insights into the impact of VQ on generalization to new classes.

### Open Question 2
- Question: How does the performance of FluoroSAM on real X-ray images compare to models trained on real data?
- Basis in paper: [explicit] The paper demonstrates FluoroSAM's performance on real interventional X-ray images and chest X-rays, but it does not compare its performance to models trained on real data.
- Why unresolved: The paper does not provide a direct comparison between FluoroSAM and models trained on real X-ray images, making it difficult to assess the impact of using synthetic data for training.
- What evidence would resolve it: Conducting a comparative study between FluoroSAM and models trained on real X-ray images using the same evaluation metrics and datasets would provide insights into the effectiveness of synthetic data for training.

### Open Question 3
- Question: How does the performance of FluoroSAM vary with different levels of domain randomization applied to the synthetic X-ray images during training?
- Basis in paper: [explicit] The paper mentions that strong domain randomization of the image is applied during training to facilitate sim-to-real transfer, but it does not investigate the impact of different levels of domain randomization on the model's performance.
- Why unresolved: The paper does not provide an ablation study or quantitative results on how different levels of domain randomization affect the model's performance on real X-ray images.
- What evidence would resolve it: Conducting experiments with varying levels of domain randomization applied to the synthetic X-ray images during training and evaluating the model's performance on real X-ray images would provide insights into the optimal level of domain randomization for sim-to-real transfer.

## Limitations

- Limited evaluation on real X-ray datasets (28 interventional images and 119 chest X-rays) raises questions about generalizability
- Claims about clinical workflow integration extend beyond what experimental results demonstrate
- Synthetic data generation may not fully capture the complexity of real X-ray imaging variations

## Confidence

- **High confidence**: The synthetic data generation pipeline using CT-to-X-ray simulation is technically sound and well-documented. The model architecture incorporating VQ into the text encoding process is clearly specified and implemented according to established principles.
- **Medium confidence**: The segmentation performance metrics are presented with appropriate statistical context, but the limited evaluation datasets raise questions about generalizability across diverse clinical scenarios.
- **Low confidence**: Claims about the model's effectiveness for "rich human-machine interaction" in clinical workflows extend beyond what the presented experiments demonstrate.

## Next Checks

1. **Ablation study on VQ mechanism**: Conduct controlled experiments comparing FluoroSAM with and without the vector quantization layer to quantify its specific contribution to segmentation accuracy and prompt generalization. Test the model's ability to handle semantically equivalent but lexically different prompts.

2. **Expanded real-world evaluation**: Test the model on a larger, more diverse dataset of real X-ray images covering multiple clinical scenarios, anatomical variations, and imaging conditions. Include datasets with known challenging cases such as pathological findings and unusual patient positioning.

3. **Interactive workflow validation**: Develop a prototype clinical workflow that demonstrates the model's promptable capabilities in a realistic setting. Evaluate the model's performance in handling sequential prompts, point corrections, and multi-organ segmentation tasks that would be encountered in actual clinical practice.