---
ver: rpa2
title: 'Position: Why We Must Rethink Empirical Research in Machine Learning'
arxiv_id: '2405.02200'
source_url: https://arxiv.org/abs/2405.02200
tags:
- research
- learning
- empirical
- machine
- science
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that most empirical ML research is exploratory
  in nature but is treated as confirmatory, leading to non-replicable results. The
  authors call for more recognition of exploratory research (both insight-oriented
  and method-developing), more rigorous confirmatory research (neutral comparisons,
  replications), and improved infrastructure and education.
---

# Position: Why We Must Rethink Empirical Research in Machine Learning

## Quick Facts
- arXiv ID: 2405.02200
- Source URL: https://arxiv.org/abs/2405.02200
- Reference count: 40
- Most current empirical ML research is exploratory but treated as confirmatory, leading to non-replicable results

## Executive Summary
This paper argues that the majority of empirical machine learning research is exploratory in nature but is mislabeled as confirmatory, resulting in misleading statistical inference and non-replicable results. The authors call for greater recognition of exploratory research (both insight-oriented and method-developing), more rigorous confirmatory research through neutral comparisons and replications, and improved infrastructure and education. They warn against overreliance on statistical significance testing when underlying assumptions are violated, particularly in exploratory contexts.

## Method Summary
The paper provides a conceptual analysis of empirical research practices in machine learning, distinguishing between exploratory and confirmatory research types. It synthesizes existing literature on research methodology and reproducibility to argue for structural changes in how ML research is conducted, evaluated, and published. The authors propose practical recommendations for researchers, reviewers, and conference organizers to foster more reliable and insightful empirical research.

## Key Results
- Most ML empirical research is exploratory but presented as confirmatory, invalidating statistical inference
- Two types of exploratory research need recognition: insight-oriented and method-developing
- Infrastructure improvements (open datasets, benchmark suites, replication platforms) are essential for rigorous research
- Statistical significance testing is often misapplied when assumptions about fixed hypotheses are violated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Most current empirical ML research is exploratory but labeled as confirmatory, leading to misleading statistical inference
- Mechanism: Researchers perform method development and optimization (exploratory) but then apply confirmatory statistical tests, violating assumptions about fixed hypotheses and valid inference
- Core assumption: Exploratory phase invalidates the statistical assumptions underlying post-hoc significance testing
- Evidence anchors:
  - [abstract] "most current empirical machine learning research is fashioned as confirmatory research while it should rather be considered exploratory"
  - [section] "we think, currently, most of the empirical research in ML is conducted as part of a paper introducing a new method and is fashioned as confirmatory research even though it is exploratory in nature"
  - [corpus] Weak - corpus neighbors focus on modality-specific concerns or evaluation methodology but don't directly address this statistical validity issue
- Break condition: If researchers clearly separate exploratory development from confirmatory evaluation, or use appropriate exploratory analysis methods instead of significance testing

### Mechanism 2
- Claim: Recognition and proper categorization of exploratory research types (insight-oriented vs method-developing) will improve empirical rigor and understanding
- Mechanism: Distinguishing exploratory research types allows appropriate evaluation methods and publication venues, reducing pressure to falsely claim confirmatory results and encouraging genuine scientific inquiry
- Core assumption: Current ML publication incentives and reviewer expectations force exploratory work into confirmatory frameworks
- Evidence anchors:
  - [abstract] "we call for more awareness of the plurality of ways of gaining knowledge experimentally but also of some epistemic limitations"
  - [section] "we distinguish two types of exploratory empirical methodological research... insight-oriented exploratory research in contrast to method-developing exploratory research"
  - [corpus] Weak - corpus focuses on specific ML domains rather than broader epistemological framework changes
- Break condition: If publication venues and reviewer guidelines explicitly recognize and value both exploratory and confirmatory research types appropriately

### Mechanism 3
- Claim: More infrastructure (open datasets, benchmark suites, replication platforms) will enable rigorous confirmatory research and reduce bias
- Mechanism: Better infrastructure provides neutral grounds for method comparison and replication, reducing authors' ability to bias experiments toward their methods and increasing reproducibility
- Core assumption: Current infrastructure favors positive results and method promotion over neutral comparison and replication
- Evidence anchors:
  - [section] "We require more dedicated infrastructure to make the proposed forms of research more (easily) realizable... well-maintained open-source software for systematic benchmark experiments"
  - [abstract] "more rigorous confirmatory research (neutral comparisons, replications), and improved infrastructure and education"
  - [corpus] Weak - corpus neighbors don't address infrastructure needs for empirical research rigor
- Break condition: If infrastructure development outpaces researcher adoption or if new infrastructure introduces new biases

## Foundational Learning

- Concept: Difference between exploratory and confirmatory research
  - Why needed here: The paper's central argument hinges on misclassifying exploratory work as confirmatory
  - Quick check question: If you're developing a new algorithm and testing many variants, is this exploratory or confirmatory research?

- Concept: Statistical testing assumptions and limitations
  - Why needed here: The paper warns against inappropriate use of significance testing in exploratory contexts
  - Quick check question: What key assumption must be met for a statistical test to be valid in a method comparison study?

- Concept: Research reproducibility vs replicability
  - Why needed here: The paper distinguishes computational reproducibility from scientific replicability
  - Quick check question: Can code that runs and produces identical outputs be considered scientifically replicable?

## Architecture Onboarding

- Component map:
  - Research pipeline → Data collection → Experiment design → Analysis → Publication
  - Infrastructure layer → Datasets → Benchmark platforms → Replication tools → Publication venues
  - Education layer → Training programs → Reviewer guidelines → Community standards

- Critical path: Research question → Experimental design → Implementation → Analysis → Publication → Replication

- Design tradeoffs:
  - Speed vs rigor: Quick publication vs thorough exploratory/confirmatory distinction
  - Innovation vs validation: Method development vs neutral comparison
  - Openness vs protection: Sharing code/data vs competitive advantage

- Failure signatures:
  - Overuse of statistical significance testing in exploratory work
  - Lack of replication studies despite widespread method claims
  - Publication bias toward positive results
  - Insufficient infrastructure for neutral comparison

- First 3 experiments:
  1. Conduct a neutral method comparison using established benchmark infrastructure (OpenML, HPOBench)
  2. Design an exploratory study focusing on understanding algorithm behavior rather than improving performance
  3. Create a replication study attempting to verify published results under different conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between exploratory and confirmatory research in ML, and how can this continuum be formalized?
- Basis in paper: [explicit] The paper discusses the exploratory-confirmatory research continuum and suggests that ML is a maturing science
- Why unresolved: The paper argues for a continuum but does not provide a concrete framework for mapping abstract concepts into guidelines for scientific practice
- What evidence would resolve it: A formal framework or model that clearly delineates the exploratory-confirmatory spectrum in ML research, with examples and criteria for classification

### Open Question 2
- Question: How can we effectively operationalize abstract concepts in ML to ensure experimental validity and generalizability?
- Basis in paper: [explicit] The paper highlights the problem of lack of conceptual clarity and operationalization, especially in supervised and unsupervised learning
- Why unresolved: The paper points out the issue but does not provide a clear solution for operationalizing abstract concepts in ML experiments
- What evidence would resolve it: Case studies or empirical research demonstrating successful operationalization of abstract concepts in ML, with clear methodologies and outcomes

### Open Question 3
- Question: What are the specific guidelines for the correct use of statistical tests in ML research, and how can we avoid common misconceptions?
- Basis in paper: [explicit] The paper warns against the misguided use of statistical significance testing and suggests a more diverse set of analysis tools
- Why unresolved: While the paper cautions against overreliance on statistical tests, it does not provide specific guidelines for their correct use
- What evidence would resolve it: A comprehensive guide or set of best practices for statistical testing in ML, with examples of correct and incorrect usage, and clear explanations of common pitfalls

## Limitations

- The paper lacks systematic empirical evidence quantifying how prevalent confirmatory mislabeling is in the ML community
- Implementation challenges for proposed infrastructure solutions are not adequately addressed
- The paper does not provide specific guidelines for operationalizing abstract concepts in ML experiments

## Confidence

- Confidence: Medium. The distinction between exploratory and confirmatory research is conceptually clear but practical implementation remains ambiguous
- Confidence: Low. Proposed infrastructure solutions require significant community adoption and funding that the paper doesn't address
- Confidence: Medium. The concern about statistical significance testing misuse is well-founded but lacks quantitative evidence

## Next Checks

1. Conduct a systematic review of recent ML conference papers to quantify how many claim confirmatory results while showing exploratory methodology
2. Implement a pilot benchmark platform using existing infrastructure (OpenML) to test neutral method comparison under controlled conditions
3. Survey ML researchers about their awareness and use of exploratory vs confirmatory research distinctions in practice