---
ver: rpa2
title: 'MedExpQA: Multilingual Benchmarking of Large Language Models for Medical Question
  Answering'
arxiv_id: '2404.05590'
source_url: https://arxiv.org/abs/2404.05590
tags:
- medical
- gold
- knowledge
- llms
- options
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedExpQA, the first multilingual benchmark
  for medical question answering (QA) that includes gold reference explanations written
  by medical doctors. The benchmark leverages the Antidote CasiMedicos dataset, which
  contains medical exams with clinical cases, multiple-choice questions, and explanations
  for both correct and incorrect answers in four languages (Spanish, English, French,
  and Italian).
---

# MedExpQA: Multilingual Benchmarking of Large Language Models for Medical Question Answering

## Quick Facts
- arXiv ID: 2404.05590
- Source URL: https://arxiv.org/abs/2404.05590
- Authors: Iñigo Alonso; Maite Oronoz; Rodrigo Agerri
- Reference count: 17
- Introduces first multilingual medical QA benchmark with gold explanations

## Executive Summary
MedExpQA introduces the first multilingual benchmark for medical question answering that includes gold reference explanations written by medical doctors. The benchmark leverages the Antidote CasiMedicos dataset containing medical exams with clinical cases, multiple-choice questions, and explanations in Spanish, English, French, and Italian. The study evaluates four state-of-the-art LLMs using various knowledge grounding approaches and demonstrates that fine-tuning significantly improves performance, especially when using gold explanations. The benchmark and fine-tuned models will be made publicly available to facilitate reproducibility in medical domain benchmarking.

## Method Summary
The authors created MedExpQA by leveraging the Antidote CasiMedicos dataset, which contains medical exams with clinical cases, multiple-choice questions, and explanations for both correct and incorrect answers across four languages. They evaluated four LLMs (PMC-LLaMA, LLaMA-2, Mistral, and BioMistral) using various knowledge grounding approaches including gold explanations and RAG methods. The evaluation involved fine-tuning the LLMs on the CasiMedicos dataset and comparing performance across different grounding strategies. The study focused on measuring accuracy improvements when using different types of explanations and retrieval methods.

## Key Results
- Fine-tuning LLMs on CasiMedicos dataset achieved up to 88% accuracy when using gold explanations
- Performance drops significantly for non-English languages, highlighting multilingual challenges
- RAG-based approaches showed moderate improvements but still lagged behind gold explanations

## Why This Works (Mechanism)
The benchmark works by providing structured medical exam data with explanations that enable LLMs to learn both the correct answers and the reasoning behind them. The multilingual aspect allows for testing cross-lingual transfer and bias in medical knowledge representation. The gold explanations serve as high-quality training signals that guide the models toward clinically accurate reasoning patterns.

## Foundational Learning

**Medical Knowledge Representation**
- Why needed: Medical QA requires understanding complex clinical concepts and relationships
- Quick check: Can models correctly identify symptoms, diagnoses, and treatments in clinical cases

**Multilingual Processing**
- Why needed: Medical knowledge exists across multiple languages but quality varies
- Quick check: Compare performance across languages for same medical concepts

**Knowledge Grounding**
- Why needed: LLMs need external medical knowledge for accurate clinical reasoning
- Quick check: Measure improvement when using gold vs. retrieved explanations

## Architecture Onboarding

**Component Map**
CasiMedicos dataset -> Fine-tuning pipeline -> Evaluation framework -> LLM models

**Critical Path**
Dataset preprocessing → Model fine-tuning → Knowledge grounding → Performance evaluation

**Design Tradeoffs**
- Using gold explanations provides high accuracy but limits scalability
- RAG methods offer flexibility but lower performance
- Multilingual approach increases coverage but introduces quality variations

**Failure Signatures**
- Low performance on non-English languages suggests translation or data quality issues
- RAG method underperformance indicates retrieval system limitations
- Large gap between gold and retrieved explanations suggests knowledge gap issues

**First Experiments**
1. Test baseline LLM performance without fine-tuning on medical QA
2. Evaluate cross-lingual transfer by testing models trained in one language on others
3. Compare RAG performance using different retrieval strategies and sources

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to four languages and single dataset source
- Performance drops significantly for non-English languages without clear cause
- Fine-tuning improvements may not translate to real-world clinical scenarios

## Confidence
- High confidence in benchmark construction and dataset quality
- Medium confidence in generalizability of fine-tuning results across medical domains
- Medium confidence in comparative performance analysis between different models and approaches
- Low confidence in real-world clinical applicability of results

## Next Checks
1. Test the fine-tuned models on external medical QA datasets not used in training to assess generalizability
2. Conduct human evaluation studies with medical professionals to validate model explanations and clinical relevance
3. Evaluate the models on time-sensitive medical scenarios to assess performance under clinical time pressure