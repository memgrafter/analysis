---
ver: rpa2
title: 'Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial
  Perspective'
arxiv_id: '2402.18607'
source_url: https://arxiv.org/abs/2402.18607
tags:
- data
- property
- diffusion
- attack
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores privacy and fairness risks in sharing diffusion
  models by proposing two adversarial attacks: a fairness poisoning attack and a property
  inference attack. The fairness poisoning attack allows the sharer to manipulate
  the training data distribution to degrade the fairness of the receiver''s downstream
  classifier while maintaining accuracy.'
---

# Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective

## Quick Facts
- arXiv ID: 2402.18607
- Source URL: https://arxiv.org/abs/2402.18607
- Reference count: 40
- Primary result: Proposes fairness poisoning and property inference attacks on shared diffusion models, demonstrating privacy and fairness risks in data-sharing applications.

## Executive Summary
This paper investigates privacy and fairness vulnerabilities in sharing diffusion models for data dissemination. The authors introduce two adversarial attacks: a fairness poisoning attack that allows the sharer to degrade the receiver's downstream classifier fairness while maintaining accuracy, and a property inference attack that enables receivers to estimate sensitive property distributions in the sharer's private dataset. Through experiments across multiple datasets and diffusion model architectures, the paper demonstrates the effectiveness of these attacks and highlights the need for robust privacy protection protocols in collaborative data-sharing environments.

## Method Summary
The paper proposes two adversarial attacks on shared diffusion models. The fairness poisoning attack manipulates the training data distribution to degrade downstream classifier fairness while preserving accuracy, leveraging diffusion models' distribution coverage property. The property inference attack estimates sensitive property ratios in the sharer's private dataset using synthetic samples and property discriminators. The attacks are evaluated across multiple datasets (MNIST, CelebA, AFAD, Adult) using various diffusion models (NCSN, DDPM, SDEM, TabDDPM) and compared against GANs and VAEs.

## Key Results
- Fairness poisoning attack successfully degrades downstream classifier fairness while keeping accuracy loss below 5% across all tested datasets and diffusion models.
- Property inference attack accurately estimates sensitive property ratios in the sharer's dataset with bounded error, using both CLIP-based and auxiliary data-trained discriminators.
- The attacks demonstrate interconnections where each can serve as a countermeasure against the other in untrusted collaborative environments.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sharer can manipulate downstream classifier fairness by modifying the distribution of training data fed into the diffusion model.
- Mechanism: Diffusion models exhibit strong distribution coverage, meaning synthetic samples generated from a poisoned training dataset inherit and propagate the biases introduced by the sharer. By carefully selecting a subset of the original data to maximize mutual information between sensitive features and labels while preserving overall data utility, the sharer can shift the synthetic dataset distribution in a way that degrades the receiver's downstream classifier fairness without significantly harming accuracy.
- Core assumption: The diffusion model's sampling process maintains sufficient fidelity to the training data distribution such that biased patterns in the training set are reflected in the synthetic samples used for downstream tasks.
- Evidence anchors:
  - [abstract]: "the sharer can execute fairness poisoning attacks to undermine the receiver's downstream models by manipulating the training data distribution of the diffusion model"
  - [section]: "An important characteristic of diffusion models is their excellent distribution coverage... the sharer can modify the distribution of ˆD by altering the training dataset D of the diffusion model"
- Break condition: If the diffusion model's distribution coverage is poor or if the downstream learning algorithm is highly robust to distributional shifts, the poisoned bias may not transfer effectively.

### Mechanism 2
- Claim: The receiver can accurately infer the proportion of a sensitive property in the sharer's private dataset by analyzing synthetic samples generated from the shared diffusion model.
- Mechanism: Using a property discriminator (either trained on auxiliary data or instantiated via a pre-trained CLIP model), the receiver estimates the property distribution in the synthetic dataset. Hoeffding's inequality bounds the estimation error, ensuring that with sufficient samples, the inferred proportion closely approximates the true proportion in the original private dataset.
- Core assumption: The diffusion model's generative process preserves the relative proportions of sensitive properties from the original dataset in the synthetic samples, and the property discriminator is sufficiently accurate.
- Evidence anchors:
  - [abstract]: "the receiver can perform property inference attacks to reveal the distribution of sensitive features in the sharer's dataset"
  - [section]: "we propose a sampling-based method to estimate the property distribution of D, leveraging Hoeffding's inequality to bound the estimation error"
- Break condition: If the diffusion model's generation process significantly distorts property proportions, or if the discriminator's accuracy is too low, the inference will fail to be accurate.

### Mechanism 3
- Claim: The interplay between fairness poisoning and property inference attacks creates a feedback loop where each attack can serve as a countermeasure against the other.
- Mechanism: The property inference attack can be used by the receiver to audit the synthetic dataset for fairness bias introduced by the sharer's poisoning. Conversely, the fairness poisoning attack can be adapted to obscure sensitive property distributions, thereby defending against property inference.
- Core assumption: Both attacks operate on the same distribution coverage property of diffusion models, making them inherently linked in their effectiveness and detectability.
- Evidence anchors:
  - [abstract]: "we discuss the interconnections between the attacks and conduct a comparative analysis"
  - [section]: "In an untrusted collaborative environment, these two attacks can be employed as countermeasures against each other"
- Break condition: If the diffusion model's distribution coverage is not robust enough to preserve subtle changes in both fairness bias and property proportions, the countermeasure effects may not manifest.

## Foundational Learning

- Concept: Mutual Information as a Fairness Metric
  - Why needed here: It quantifies the dependence between sensitive features and model predictions, which is central to both the poisoning attack objective and the data utility constraint.
  - Quick check question: How does maximizing mutual information between sensitive features and labels in the poisoned dataset help degrade downstream fairness?

- Concept: Hoeffding's Inequality for Error Bounds
  - Why needed here: It provides a probabilistic guarantee on the accuracy of the receiver's property proportion estimates based on finite synthetic samples.
  - Quick check question: What sample size is required to ensure the property inference error is within 0.1 with probability at least 0.9?

- Concept: Distribution Coverage in Generative Models
  - Why needed here: It is the key property that enables both attacks to function by ensuring that synthetic samples reflect the characteristics of the training data.
  - Quick check question: How can you empirically verify that a diffusion model has good distribution coverage before using it for data sharing?

## Architecture Onboarding

- Component map:
  - Sharer side: Private dataset → Poisoning algorithm (Algorithm 1) → Diffusion model training → Black-box model sharing
  - Receiver side: Black-box model access → Synthetic data sampling → Downstream classifier training → Property inference (Algorithm 2)
  - Shared components: Diffusion model implementations (NCSN, DDPM, SDEM, TabDDPM), property discriminators, mutual information estimators

- Critical path:
  1. Sharer prepares poisoned training data using Algorithm 1.
  2. Sharer trains diffusion model on poisoned data.
  3. Receiver samples synthetic data and trains classifier.
  4. Receiver applies property inference to estimate sensitive property proportions.

- Design tradeoffs:
  - Poisoning ratio vs. data utility: Higher poisoning ratios increase fairness degradation but risk classifier accuracy loss.
  - Sample size vs. inference accuracy: Larger sample sizes improve property inference but increase computational cost.
  - Discriminator choice vs. attack success: Using auxiliary data yields more accurate discriminators but may not always be available.

- Failure signatures:
  - Fairness poisoning fails: Downstream classifier fairness gap remains small; accuracy loss is minimal but not accompanied by fairness loss.
  - Property inference fails: Estimated property proportions deviate significantly from ground truth; high variance across trials.

- First 3 experiments:
  1. Vary poisoning ratio α from 0.1 to 0.5 and measure ℓfair and ℓacc on CelebA-A dataset.
  2. Sample 30, 50, 100, 200, 300 synthetic images and measure PIA estimation error on MNIST.
  3. Train diffusion models with different epochs (600, 800, 1000, 1200, 1400) and evaluate attack performance to assess impact of underfitting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are the proposed attacks under realistic privacy-preserving protocols that combine differential privacy with data auditing?
- Basis in paper: [explicit] The paper discusses differential privacy as a potential countermeasure but notes its limitations for protecting group-level privacy and the trade-off with model utility.
- Why unresolved: The paper only provides theoretical arguments and does not conduct empirical experiments to evaluate the effectiveness of combined DP and auditing protocols against the proposed attacks.
- What evidence would resolve it: Experiments comparing attack success rates under various DP mechanisms (e.g., different privacy budgets, gradient clipping) combined with data auditing techniques would demonstrate the practical effectiveness of these defenses.

### Open Question 2
- Question: Can the proposed fairness poisoning attack be extended to achieve more complex fairness notions beyond demographic parity, such as equalized odds or equal opportunity, in a black-box setting?
- Basis in paper: [explicit] The paper acknowledges the limitation of the proposed FPA to demographic parity due to the black-box setting and lack of access to the receiver's classifier.
- Why unresolved: The paper does not explore alternative attack strategies or modifications to the optimization objective that could potentially achieve other fairness notions without requiring white-box access.
- What evidence would resolve it: Developing and testing new attack algorithms that target equalized odds or equal opportunity fairness notions, while still operating in a black-box setting, would demonstrate the feasibility of extending the proposed attack.

### Open Question 3
- Question: How do the proposed attacks perform against more advanced generative models that may offer better distribution coverage and quality than the models evaluated in the paper?
- Basis in paper: [explicit] The paper compares the attack performance against GANs and VAEs, but acknowledges that diffusion models are currently the most favored choice for data-sharing applications due to their superior performance.
- Why unresolved: The paper does not evaluate the attacks against newer or more advanced generative models that may emerge in the future, such as large-scale text-to-image models or models specifically designed for tabular data.
- What evidence would resolve it: Conducting experiments with a wider range of generative models, including state-of-the-art models, would provide insights into the generalizability and robustness of the proposed attacks.

## Limitations

- Architectural details: The paper provides limited specifics on diffusion model architectures and hyperparameters, making exact reproduction challenging.
- Generalization concerns: Experiments focus on specific datasets and relatively simple downstream classifiers, leaving effectiveness on complex models unclear.
- Mutual information estimation: The paper mentions using MINE but doesn't provide implementation details or validate its accuracy, which is critical for the fairness poisoning attack's data utility constraint.

## Confidence

- High confidence: The theoretical framework connecting diffusion model distribution coverage to both attack mechanisms is sound.
- Medium confidence: The experimental results demonstrating attack effectiveness appear consistent with the proposed mechanisms, but limited architectural details reduce reproducibility confidence.
- Low confidence: Claims about the interplay between attacks serving as countermeasures lack empirical validation beyond theoretical discussion.

## Next Checks

1. **Architecture verification**: Train diffusion models with explicit hyperparameter specifications (epochs, learning rates, batch sizes) and verify sample quality and distribution coverage through quantitative metrics like Inception Score or FID.

2. **Robustness testing**: Evaluate attack effectiveness across a broader range of downstream classifiers (CNNs, transformers) and datasets to assess generalization beyond the paper's experimental scope.

3. **Countermeasure validation**: Implement and test the proposed countermeasure relationship by empirically measuring whether property inference can detect fairness poisoning and whether fairness poisoning can obscure sensitive property distributions in controlled experiments.