---
ver: rpa2
title: Tailored Design of Audio-Visual Speech Recognition Models using Branchformers
arxiv_id: '2407.06606'
source_url: https://arxiv.org/abs/2407.06606
tags:
- speech
- layer
- recognition
- encoder
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for designing tailored,
  unified audio-visual encoders for Audio-Visual Speech Recognition (AVSR) systems.
  The proposed method leverages the interpretability and flexibility of Branchformer
  architectures to identify context dependencies relevant for processing each modality,
  enabling the design of parameter-efficient AVSR systems.
---

# Tailored Design of Audio-Visual Speech Recognition Models using Branchformers

## Quick Facts
- **arXiv ID**: 2407.06606
- **Source URL**: https://arxiv.org/abs/2407.06606
- **Reference count**: 40
- **Primary result**: Parameter-efficient AVSR system with ~50% fewer parameters while achieving competitive WER (2.5% English, 9.1% Spanish)

## Executive Summary
This paper introduces a novel framework for designing tailored, unified audio-visual encoders for Audio-Visual Speech Recognition (AVSR) systems. The proposed method leverages the interpretability and flexibility of Branchformer architectures to identify context dependencies relevant for processing each modality, enabling the design of parameter-efficient AVSR systems. By training modality-specific models and analyzing branchformer encoder layers, the framework designs a tailored AVSR model that selects appropriate global- or local-context processing modules for each modality. The approach achieves competitive word error rates while significantly reducing model complexity compared to conventional approaches.

## Method Summary
The framework involves training modality-specific Branchformer models (audio-only and video-only), analyzing the branch weights at each encoder layer to determine which context (global or local) is more relevant for each modality, and then designing a tailored unified encoder. The tailored encoder selectively assigns self-attention or cgMLP modules per layer based on the dominant branch from the modality-specific models. This is combined with an adaptive audio-visual fusion module that uses learnable modality weights to balance contributions. The system is trained using a hybrid CTC/Attention framework with multi-task loss and evaluated on English and Spanish AVSR benchmarks.

## Key Results
- Achieved WER of approximately 2.5% for English and 9.1% for Spanish on benchmark datasets
- Reduced model parameters by nearly half compared to conventional two-encoder AVSR approaches
- Demonstrated the effectiveness of Branchformer's interpretability for designing parameter-efficient AVSR systems

## Why This Works (Mechanism)

### Mechanism 1
The Branchformer encoder's dual-branch structure allows explicit identification of which context (global or local) is more relevant for each modality at each encoder layer. By analyzing the learnable weighted merge layer's attention scores, the framework identifies which branch dominates per layer. Layers where one branch's weight approaches saturation can be pruned to the single active branch, reducing parameters without significant accuracy loss. This assumes modality-specific contextual needs are stable enough across training to allow layer-wise pruning decisions.

### Mechanism 2
Unified cross-modal encoders can be optimized by designing modality-specific sub-networks rather than forcing identical structures for both audio and visual inputs. The tailored encoder assigns self-attention or cgMLP modules per layer based on the dominant branch from the modality-specific models. This avoids unnecessary parameter sharing and respects the intrinsic differences in temporal resolution and information density between audio and video.

### Mechanism 3
Adaptive audio-visual fusion with learnable modality weights can dynamically balance contributions based on input quality and modality reliability. The fusion module computes weights via attention pooling over the encoded audio and video sequences. These weights adapt during training to reflect the relative informativeness of each modality, with acoustic cues typically dominating (~73% relevance).

## Foundational Learning

- **Concept**: Branchformer encoder architecture with dual branches (self-attention for global context, cgMLP for local context).
  - Why needed here: The paper's entire parameter-efficiency strategy depends on interpreting and pruning these branches based on their relevance per layer and modality.
  - Quick check question: Can you explain how the weighted merge layer decides between global and local context processing?

- **Concept**: Attention-based fusion with learnable modality weights.
  - Why needed here: The adaptive fusion module is central to balancing audio and visual contributions without hard-coded rules.
  - Quick check question: What is the formula for computing modality weights in the fusion layer?

- **Concept**: Pruning and parameter sharing in neural networks.
  - Why needed here: The tailored design reduces parameters by ~50% through selective branch removal and cross-modal parameter sharing.
  - Quick check question: How does the tailored encoder share parameters between modalities, and why?

## Architecture Onboarding

- **Component map**: Visual frontend (3D Conv + 2D ResNet-18) → modality embedding → tailored Branchformer encoder → shared FFNs → adaptive fusion → CTC/Attention decoder
- **Critical path**: Frontend → modality embedding → tailored encoder → fusion → decoder → output
- **Design tradeoffs**: Reduced parameters vs. potential loss of modality-specific fine-tuning capacity; shared FFNs improve cross-modal generalization but may limit modality-specific adaptation; fixed layer-wise pruning decisions may not adapt to dataset shifts
- **Failure signatures**: Large WER increase in video-only mode → visual frontend or tailored encoder layers are insufficient; no noise robustness improvement → fusion weights are too rigid or encoder lacks denoising capacity; convergence issues in Spanish VSR → reliance on English pre-training not fully transferable
- **First 3 experiments**: 1) Train audio-only and video-only models, extract branch scores, verify pruning decisions; 2) Build and train tailored encoder, compare WER and parameter count vs. conventional two-encoder AVSR; 3) Test noise robustness (babble noise at multiple SNR levels), analyze modality weight adaptation

## Open Questions the Paper Calls Out
- How does the proposed framework perform when scaled to larger datasets with more diverse speakers and languages beyond English and Spanish?
- To what extent can the model's interpretability be extended to identify modality-specific features beyond global/local context dependencies?
- How does the tailored architecture's performance compare to models that incorporate explicit cross-modal attention mechanisms?

## Limitations
- The framework's pruning decisions are static and may not adapt well to varying noise conditions or domain shifts
- The visual frontend's limited capacity (ResNet-18) may constrain overall system performance
- The trade-off between efficiency and maximum achievable accuracy is not fully explored

## Confidence
- **High confidence**: The general approach of using Branchformer's interpretability for layer-wise pruning decisions
- **Medium confidence**: The claim about modality weights favoring audio (~73% relevance)
- **Medium confidence**: The effectiveness of the tailored design approach on tested benchmarks

## Next Checks
1. Test the stability of branch pruning decisions across different noise conditions and SNR levels
2. Evaluate the system's performance when audio is completely absent
3. Compare the parameter efficiency gains against alternative pruning strategies