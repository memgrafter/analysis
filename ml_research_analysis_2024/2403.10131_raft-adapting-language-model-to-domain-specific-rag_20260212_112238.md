---
ver: rpa2
title: 'RAFT: Adapting Language Model to Domain Specific RAG'
arxiv_id: '2403.10131'
source_url: https://arxiv.org/abs/2403.10131
tags:
- documents
- raft
- training
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAFT introduces a novel fine-tuning approach for domain-specific
  Retrieval-Augmented Generation (RAG) by training models to identify relevant documents
  while ignoring distractors. The method combines instruction tuning with simulated
  imperfect retrieval during training, where models learn to cite verbatim from relevant
  documents and provide chain-of-thought reasoning.
---

# RAFT: Adapting Language Model to Domain Specific RAG

## Quick Facts
- arXiv ID: 2403.10131
- Source URL: https://arxiv.org/abs/2403.10131
- Reference count: 9
- RAFT achieves accuracy gains of up to 35.25% on Hotpot QA and 76.35% on Torch Hub compared to base models

## Executive Summary
RAFT introduces a novel fine-tuning approach for domain-specific Retrieval-Augmented Generation (RAG) by training models to identify relevant documents while ignoring distractors. The method combines instruction tuning with simulated imperfect retrieval during training, where models learn to cite verbatim from relevant documents and provide chain-of-thought reasoning. Experiments across PubMed, HotpotQA, and Gorilla datasets show consistent performance improvements over standard domain-specific fine-tuning and general-purpose models with RAG.

## Method Summary
RAFT fine-tunes pre-trained LLMs on domain-specific RAG tasks by training with question-document-answer triples that include both relevant and distractor documents. The model learns to generate answers with chain-of-thought reasoning while citing verbatim from relevant documents. Training data includes varying proportions (P%) of golden documents versus distractor-only examples, forcing the model to handle imperfect retrieval scenarios and improve robustness to document noise.

## Key Results
- RAFT improves performance over standard domain-specific fine-tuning by 6.1% on PubMed, 35.25% on Hotpot QA, and 76.35% on Torch Hub datasets
- Training with distractor documents significantly improves model robustness to varying numbers of retrieved documents at test time
- Incorporating a portion of training data without golden documents (P < 100%) enhances downstream RAG performance compared to training with only golden documents

## Why This Works (Mechanism)

### Mechanism 1
RAFT improves model performance by training with both relevant documents and distractor documents, forcing the model to learn document selection skills. The model learns to distinguish relevant from irrelevant documents by being trained on question-document-answer triples where some documents are distractors, simulating real-world retrieval imperfections.

### Mechanism 2
Chain-of-thought reasoning with source citations improves model reasoning and answer quality. By requiring models to generate reasoning chains that cite specific passages from documents, RAFT forces the model to ground its reasoning in evidence and explain its logic step-by-step.

### Mechanism 3
Training with varying proportions of golden documents (P%) improves generalization to different retrieval scenarios. By training on data where only a portion includes golden documents, the model learns to handle both cases where relevant documents are present and where it must rely on its own knowledge.

## Foundational Learning

- **Document relevance ranking**: Needed to understand how models distinguish relevant from irrelevant documents. Quick check: Can you explain how a retrieval system determines which documents are most relevant to a query?
- **Chain-of-thought prompting**: Needed to understand how structured reasoning improves model performance. Quick check: What are the key components of a well-structured chain-of-thought response?
- **Supervised fine-tuning with structured outputs**: Needed to understand how RAFT differs from other fine-tuning approaches. Quick check: How does supervised fine-tuning differ from prompt engineering in terms of model adaptation?

## Architecture Onboarding

- **Component map**: Retriever -> Top-k documents -> RAFT fine-tuned model -> Answer generation with reasoning and citations
- **Critical path**: Question input → Document retrieval (top-k) → RAFT model processing → Answer generation with reasoning and citations → Output delivery
- **Design tradeoffs**: Number of distractor documents (more improves robustness but slows training), P% parameter (higher improves performance when golden docs are available but may reduce generalization), reasoning detail level (more detailed improves explainability but increases token usage)
- **Failure signatures**: Model generates irrelevant answers despite having relevant documents, model fails to cite source material even when correct answer is in documents, performance degrades significantly when number of retrieved documents changes
- **First 3 experiments**: Ablation study: Train with only relevant documents vs. RAFT approach with distractors, parameter sensitivity: Vary P% from 0% to 100% to find optimal proportion of golden documents, robustness test: Evaluate performance with varying numbers of retrieved documents (top-1 to top-10)

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal proportion (P%) of training data that should include golden documents versus distractor-only documents? The paper explicitly investigates this hyperparameter and shows different optimal values (40%, 60%, 100%) across different datasets, but lacks a theoretical framework for predicting optimal P% values for new datasets.

### Open Question 2
How does the number of distractor documents in training data affect model robustness to varying numbers of test-time documents in RAG? While the paper demonstrates improved robustness, it does not establish clear guidelines for choosing the optimal number of distractor documents during training or explain why different datasets require different numbers.

### Open Question 3
Does RAFT's effectiveness generalize to non-domain-specific open-book settings where the test documents may differ from training documents? The paper focuses on domain-specific open-book settings where test documents match training documents, while noting that related work studies cross-domain RAG settings, but does not evaluate RAFT's performance when test documents differ from training documents.

## Limitations

- Optimal P% parameter appears dataset-specific and requires careful tuning, limiting generalizability
- Limited ablation studies prevent isolation of individual RAFT component contributions
- Reproducibility concerns due to lack of specific implementation details and hyperparameters

## Confidence

**High confidence**: The core claim that RAFT improves domain-specific RAG performance is well-supported by experimental results across multiple datasets.

**Medium confidence**: The mechanism explanation for why RAFT works is logical but not fully validated, with evidence primarily observational rather than causal.

**Low confidence**: The claim about the specific benefits of varying P% values is based on limited experimental evidence and lacks a clear theoretical foundation.

## Next Checks

1. Conduct controlled experiments isolating each RAFT component (distractor training, chain-of-thought reasoning, citation requirements) to determine their individual contributions to performance improvements.

2. Systematically vary P% values across a wider range and multiple datasets to identify patterns in optimal parameter selection and develop guidelines for choosing appropriate values.

3. Evaluate RAFT performance with varying numbers of retrieved documents (top-1 to top-10) and different levels of retrieval noise to assess real-world applicability.