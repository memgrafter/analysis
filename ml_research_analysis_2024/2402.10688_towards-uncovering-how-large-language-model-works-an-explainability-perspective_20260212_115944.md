---
ver: rpa2
title: 'Towards Uncovering How Large Language Model Works: An Explainability Perspective'
arxiv_id: '2402.10688'
source_url: https://arxiv.org/abs/2402.10688
tags:
- arxiv
- llms
- these
- preprint
- mechanistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews explainability techniques that
  uncover the internal mechanisms of large language models (LLMs). The paper examines
  how knowledge is composed in model architectures through mechanistic interpretability
  at the neuron, circuit, and attention head levels, how knowledge is encoded in intermediate
  representations using probing and representation engineering, and how generalization
  abilities emerge during training.
---

# Towards Uncovering How Large Language Model Works: An Explainability Perspective

## Quick Facts
- **arXiv ID**: 2402.10688
- **Source URL**: https://arxiv.org/abs/2402.10688
- **Reference count**: 20
- **Primary result**: This survey systematically reviews explainability techniques that uncover the internal mechanisms of large language models (LLMs), examining knowledge composition through mechanistic interpretability, knowledge encoding through probing and representation engineering, and generalization emergence during training.

## Executive Summary
This survey provides a comprehensive overview of explainability techniques for understanding how large language models work internally. The authors systematically examine three main approaches: mechanistic interpretability at the neuron, circuit, and attention head levels; representation engineering through probing intermediate representations; and training dynamics analysis to explain phenomena like grokking and memorization. The work distinguishes itself from prior surveys by focusing specifically on mechanistic insights into LLM functionality rather than just explaining model outputs. The paper also discusses practical applications of these insights for model editing, pruning, and alignment, while acknowledging key limitations including the curse of dimensionality and evaluation challenges.

## Method Summary
The survey systematically reviews existing literature on LLM explainability through three complementary lenses. First, it examines mechanistic interpretability techniques that analyze individual neurons, circuits, and attention heads to understand how knowledge is composed in model architectures. Second, it explores representation engineering approaches that use probing techniques to analyze how knowledge is encoded in intermediate representations and identify manipulable directions in the embedding space. Third, it investigates training dynamics to explain phenomena like grokking and memorization through monitoring validation accuracy, weight norms, and loss patterns. The paper synthesizes findings from these approaches to provide a holistic view of LLM functionality while acknowledging limitations and open questions.

## Key Results
- Mechanistic interpretability reveals how knowledge is composed through neuron-level and circuit-level analysis of model components
- Representation engineering identifies and manipulates directions in the embedding space corresponding to specific behaviors
- Training dynamics analysis explains how generalization abilities emerge through phenomena like grokking and memorization patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Mechanistic interpretability provides neuron-level and circuit-level explanations that uncover how knowledge is composed in model architectures
- **Mechanism**: By analyzing individual neurons, attention heads, and their connections, researchers can identify functional units that implement specific algorithms and patterns
- **Core assumption**: Models have interpretable internal structures that can be reverse-engineered like biological systems
- **Evidence anchors**:
  - [abstract] "First, we review how knowledge is architecturally composed within LLMs and encoded in their internal parameters via mechanistic interpretability techniques"
  - [section 2] "It focuses on the functionality of each model component and interprets how models operate at the level of neurons, circuits, and attention heads"
  - [corpus] Found related work on mechanistic interpretability for time series classification (75674) with h-index 18
- **Break condition**: If superposition and polysemanticity cannot be disentangled, the neuron-level approach breaks down

### Mechanism 2
- **Claim**: Representation engineering operates at the embedding space level to identify and manipulate directions corresponding to specific behaviors
- **Mechanism**: By probing intermediate representations, researchers can find linear or non-linear directions in the representation space that correspond to concepts like truthfulness, toxicity, or spatial reasoning
- **Core assumption**: Model behaviors manifest as identifiable directions in the high-dimensional representation space
- **Evidence anchors**:
  - [abstract] "Then, we summarize how knowledge is embedded in LLM representations by leveraging probing techniques and representation engineering"
  - [section 3] "A direction in the representation space is identified as contributing to a specific behavior. This direction will then be used to adjust the representations so that models' behaviors can be controlled"
  - [corpus] Weak evidence - only found general explainability surveys but no specific representation engineering work
- **Break condition**: If representations are too entangled or if the number of dimensions exceeds the ability to identify meaningful directions

### Mechanism 3
- **Claim**: Training dynamics analysis reveals how generalization abilities emerge through phenomena like grokking and memorization patterns
- **Mechanism**: By monitoring validation accuracy, weight norms, and loss patterns during training, researchers can identify phases where models transition from memorization to generalization
- **Core assumption**: Generalization emerges as a distinct phase during training that can be observed through specific patterns in metrics
- **Evidence anchors**:
  - [abstract] "Additionally, we investigate the training dynamics through a mechanistic perspective to explain phenomena such as grokking and memorization"
  - [section 4.1] "Grokking is a phenomenon in which models suddenly improve their validation accuracy after severely overfitting on over-parameterized neural networks"
  - [corpus] Found related work on mechanistic interpretability for time series (75674) and relevance understanding (25138)
- **Break condition**: If grokking patterns don't generalize across different architectures or if the phenomenon disappears with larger datasets

## Foundational Learning

- **Concept**: Mechanistic interpretability vs representation engineering
  - Why needed here: The paper explicitly distinguishes between these two paradigms as complementary approaches to understanding LLM internals
  - Quick check question: Which approach operates at the neuron/circuit level and which operates at the representation space level?

- **Concept**: Polysemanticity and superposition
  - Why needed here: Section 2.1 discusses how individual neurons can represent multiple unrelated features, which is a fundamental challenge in mechanistic interpretability
  - Quick check question: What is the difference between a neuron being polysemantic versus monosemantic?

- **Concept**: Grokking phenomenon
  - Why needed here: Section 4.1 explains grokking as a key mechanism for understanding how generalization emerges during training
  - Quick check question: What distinguishes grokking from simple overfitting, and what are the key metrics that indicate grokking is occurring?

## Architecture Onboarding

- **Component map**: Input embedding layer → Residual stream → Multiple attention blocks (each with query/key/value circuits) → MLP layers → Output embedding
- **Critical path**: The path from input to output that carries the most information about task-specific reasoning - typically through attention heads in middle layers that enable in-context learning via induction heads
- **Design tradeoffs**: 
  - Neuron-level analysis provides granular detail but struggles with polysemanticity
  - Circuit-level analysis offers interpretable units but may miss emergent behaviors
  - Representation-level analysis captures high-level concepts but loses mechanistic detail
- **Failure signatures**: 
  - If neuron-level analysis consistently fails to find interpretable patterns across different models
  - If representation directions don't generalize across different prompting strategies
  - If grokking patterns don't emerge in larger models with more data
- **First 3 experiments**:
  1. Analyze neuron activations in a small transformer on a simple task (like Othello board state) to identify monosemantic vs polysemantic neurons
  2. Probe the residual stream of a pre-trained LLM to find directions corresponding to a specific concept (like truthfulness)
  3. Train a small transformer on algorithmic data and monitor validation accuracy over time to observe grokking patterns and weight norm evolution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the superposition and polysemanticity phenomena scale in larger LLMs with billions of parameters?
- Basis in paper: [explicit] The paper discusses superposition and polysemanticity as key concepts in mechanistic interpretability, noting that these phenomena become more complex in larger models.
- Why unresolved: Current studies on superposition and polysemanticity are mostly limited to smaller toy models, and it's unclear how these concepts scale to the massive parameter spaces of modern LLMs.
- What evidence would resolve it: Empirical studies examining superposition and polysemanticity in LLMs with varying parameter scales, particularly focusing on how feature capacity and neuron utilization change with model size.

### Open Question 2
- Question: What are the fundamental mechanisms that enable induction heads to facilitate in-context learning, and how can we systematically identify and modify these mechanisms?
- Basis in paper: [explicit] The paper discusses induction heads as crucial for in-context learning, but notes that the exact mechanisms and how to systematically modify them remain unclear.
- Why unresolved: While induction heads have been identified as important, the precise mathematical and functional relationships that enable their role in in-context learning are not fully understood, making systematic modification difficult.
- What evidence would resolve it: A comprehensive theoretical framework explaining how induction heads enable in-context learning, coupled with practical methods for identifying and modifying these heads in various LLM architectures.

### Open Question 3
- Question: How can we develop evaluation frameworks that effectively validate mechanistic interpretability explanations across different LLM architectures and tasks?
- Basis in paper: [inferred] The paper discusses challenges in evaluating concepts and circuits, noting that current ad-hoc methods are neither universal nor scalable.
- Why unresolved: Current evaluation methods like ablation studies and causal scrubbing are limited in scope and don't provide comprehensive validation across different architectures and tasks, making it difficult to assess the quality of mechanistic explanations.
- What evidence would resolve it: Development of standardized evaluation metrics and frameworks that can validate mechanistic explanations across diverse LLM architectures, tasks, and scales, with clear criteria for what constitutes a valid explanation.

## Limitations
- The curse of dimensionality makes it difficult to interpret results from mechanistic interpretability in models with billions of parameters
- Current evaluation methods for validating conceptual explanations and functional circuits are neither universal nor scalable
- Most analyses are based on toy models and smaller architectures, raising concerns about generalizability to frontier LLMs

## Confidence
- **High confidence**: The existence of multiple interpretability paradigms (mechanistic vs representation engineering) and their basic distinctions
- **Medium confidence**: The described phenomena like grokking and polysemanticity, supported by empirical observations though with limited theoretical grounding
- **Low confidence**: The scalability of these techniques to frontier models and the practical utility of identified circuits for real-world applications

## Next Checks
1. **Scalability test**: Apply the same mechanistic interpretability pipeline to a model 10x larger and measure whether the same patterns (neurons, circuits, attention heads) remain identifiable
2. **Generalization test**: Transfer identified representation directions across different prompting strategies and datasets to verify robustness of the engineering approach
3. **Evaluation benchmark**: Develop and apply standardized metrics to assess the validity of proposed conceptual explanations against ground truth mechanisms in controlled synthetic tasks