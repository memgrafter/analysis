---
ver: rpa2
title: 'VELOCITI: Benchmarking Video-Language Compositional Reasoning with Strict
  Entailment'
arxiv_id: '2406.10889'
source_url: https://arxiv.org/abs/2406.10889
tags:
- caption
- video
- test
- dataset
- tests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces VELOCITI, a benchmark designed to evaluate
  the compositional reasoning capabilities of video-language models. The key innovation
  lies in creating test suites that require models to bind semantic concepts across
  time, distinguishing between agents, actions, and their associations in complex
  movie clips.
---

# VELOCITI: Benchmarking Video-Language Compositional Reasoning with Strict Entailment

## Quick Facts
- **arXiv ID:** 2406.10889
- **Source URL:** https://arxiv.org/abs/2406.10889
- **Reference count:** 40
- **Primary result:** Introduces VELOCITI benchmark for evaluating compositional reasoning in video-language models, revealing significant performance gaps between current models and human accuracy

## Executive Summary
VELOCITI is a novel benchmark designed to evaluate the compositional reasoning capabilities of video-language models. The benchmark addresses the challenge of understanding complex relationships between agents, actions, and objects across time in movie clips. By leveraging rich semantic role label annotations from the VidSitu dataset, VELOCITI generates test cases that require models to bind semantic concepts across time and distinguish between subtle perceptual differences. The benchmark reveals that even state-of-the-art video-language models, including Video-LLMs and contrastive vision-language encoders, perform significantly below human accuracy on these compositional reasoning tasks.

## Method Summary
The VELOCITI benchmark is built on top of the VidSitu dataset, which provides detailed semantic role label annotations for movie clips. The method involves generating test cases that require models to reason about the relationships between agents, actions, and objects across multiple time steps. Two main types of tests are included: perception tests that assess the ability to distinguish subtle visual differences, and visual binding tests that evaluate the capacity to associate agents with their corresponding actions and objects across time. The benchmark uses a strict entailment evaluation framework, where models must correctly identify both positive and negative examples to demonstrate true understanding of the compositional relationships.

## Key Results
- Current video-language models, including LLaVA-OneVision and Gemini-1.5-Pro, perform significantly below human accuracy (93.0%) on VELOCITI tasks
- The benchmark reveals substantial performance gaps in compositional reasoning, particularly in action understanding and agent co-reference across time
- VELOCITI's evaluation methodology, using semantic role labels for test generation, effectively identifies limitations in state-of-the-art models' ability to bind semantic concepts across time

## Why This Works (Mechanism)
VELOCITI's effectiveness stems from its use of semantic role labels to generate test cases that require compositional reasoning across time. By focusing on the binding of agents, actions, and objects, the benchmark creates scenarios that demand temporal understanding and the ability to track entities across video frames. The strict entailment evaluation framework ensures that models must demonstrate true understanding of the relationships between concepts, rather than relying on superficial pattern matching or single-frame analysis.

## Foundational Learning

### Semantic Role Labeling
- **Why needed:** Provides the semantic structure for understanding agent-action-object relationships in video
- **Quick check:** Can the model identify who did what to whom in a given video clip?

### Temporal Reasoning
- **Why needed:** Essential for tracking entities and their relationships across multiple video frames
- **Quick check:** Can the model maintain consistency in entity identification across different time steps?

### Compositional Reasoning
- **Why needed:** Required to understand complex relationships between multiple concepts in video
- **Quick check:** Can the model infer higher-level meaning from the combination of lower-level concepts?

## Architecture Onboarding

### Component Map
VidSitu annotations -> Test case generation -> Video-language model -> Entailment evaluation

### Critical Path
1. Semantic role label extraction from VidSitu
2. Test case generation (perception and binding tests)
3. Video-language model processing
4. Entailment-based evaluation

### Design Tradeoffs
- Rich semantic annotations vs. generalizability to other datasets
- Strict entailment evaluation vs. more lenient scoring methods
- Focus on compositional reasoning vs. broader video understanding capabilities

### Failure Signatures
- Inability to maintain entity consistency across time steps
- Confusion between similar actions or objects
- Failure to bind agents with their corresponding actions and objects

### First Experiments
1. Evaluate model performance on perception tests vs. visual binding tests
2. Compare results across different video-language model architectures
3. Analyze failure cases to identify specific reasoning deficiencies

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark's reliance on VidSitu annotations may limit its generalizability to other video datasets
- The strict entailment evaluation may be overly challenging for current models, potentially underestimating their true capabilities
- The focus on compositional reasoning may not fully capture other important aspects of video-language understanding

## Confidence
- **Models significantly underperform humans:** High
- **Gap specifically demonstrates compositional reasoning deficiencies:** Medium
- **VELOCITI's approach is superior for compositional reasoning assessment:** Medium

## Next Checks
1. Conduct ablation studies removing temporal components to determine if performance drops are due specifically to compositional reasoning demands versus general video understanding
2. Test whether simpler baselines that ignore compositionality can achieve competitive performance on VELOCITI, which would challenge the benchmark's specificity
3. Evaluate whether models trained specifically on compositional reasoning tasks show improved performance on VELOCITI compared to standard video-language pretraining