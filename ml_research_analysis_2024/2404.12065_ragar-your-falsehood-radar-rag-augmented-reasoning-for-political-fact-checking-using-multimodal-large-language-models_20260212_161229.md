---
ver: rpa2
title: 'RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political Fact-Checking
  using Multimodal Large Language Models'
arxiv_id: '2404.12065'
source_url: https://arxiv.org/abs/2404.12065
tags:
- claim
- image
- multimodal
- information
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces RAGAR, a novel multimodal fact-checking\
  \ system that combines retrieval-augmented generation (RAG) with two new reasoning\
  \ techniques\u2014Chain of RAG (CoRAG) and Tree of RAG (ToRAG). The system addresses\
  \ the challenge of verifying multimodal claims (text + image) in political discourse\
  \ by extracting claim details, retrieving external evidence, and reasoning through\
  \ iterative question-answering."
---

# RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2404.12065
- Source URL: https://arxiv.org/abs/2404.12065
- Authors: M. Abdul Khaliq; P. Chang; M. Ma; B. Pflugfelder; F. Miletić
- Reference count: 15
- One-line primary result: RAGAR achieves 0.85 weighted F1-score on multimodal political fact-checking, outperforming baseline by 0.14 points

## Executive Summary
This paper introduces RAGAR, a novel multimodal fact-checking system that combines retrieval-augmented generation (RAG) with two new reasoning techniques—Chain of RAG (CoRAG) and Tree of RAG (ToRAG). The system addresses the challenge of verifying multimodal claims (text + image) in political discourse by extracting claim details, retrieving external evidence, and reasoning through iterative question-answering. CoRAG builds on prior answers to ask follow-up questions, while ToRAG branches into multiple question paths and selects the best evidence via elimination. Evaluated on a curated political fact-checking dataset, RAGAR achieves a weighted F1-score of 0.85, outperforming a baseline reasoning method by 0.14 points. Human evaluation confirms that the vast majority of generated fact-check explanations include all key information from gold standard data. The work demonstrates that sophisticated RAG-augmented reasoning can significantly improve both accuracy and explanation quality in multimodal political fact-checking.

## Method Summary
RAGAR uses GPT-4V to generate multimodal claims from text and images, then applies evidence retrieval via DuckDuckGo Search and reverse image search (SerpAPI). Two reasoning approaches are introduced: CoRAG iteratively refines evidence through follow-up questions based on prior answers, while ToRAG branches into three parallel question paths and selects the best evidence via elimination. The system evaluates veracity using three prediction variants (Standard, Chain of Thought, and Chain of Verification) and generates explanations. Tested on 300 political claims from MOCHEG dataset, with human evaluation against Politifact's gold standard ruling outlines.

## Key Results
- RAGAR achieves 0.85 weighted F1-score on multimodal political fact-checking, outperforming baseline by 0.14 points
- ToRAG with CoVe (Chain of Verification) achieves the highest accuracy among RAGAR variants
- Human evaluation shows majority of generated explanations include all key information from gold standard outlines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoRAG improves fact-checking accuracy by iteratively refining evidence through follow-up questions based on prior answers.
- Mechanism: The model generates a first question, retrieves evidence, and then uses a follow-up check to decide if more information is needed. If yes, it generates a follow-up question informed by the previous Q-A pair, building a chain of reasoning.
- Core assumption: Each new question can meaningfully narrow the search space and improve the relevance of retrieved evidence.
- Evidence anchors:
  - [abstract] "CoRAG builds on prior answers to ask follow-up questions"
  - [section] "The 'Chain' in 'Chain of RAG' is thus to be interpreted as a chain of question-answer pairs that are iteratively generated"
  - [corpus] Weak – the cited neighbor papers focus on fact-checking or RAG but not specifically on iterative question refinement
- Break condition: If the follow-up check determines that enough information has been gathered, or the maximum number of steps is reached.

### Mechanism 2
- Claim: ToRAG improves accuracy by branching into multiple question paths and selecting the best via elimination.
- Mechanism: At each step, three distinct questions are generated simultaneously. The model retrieves answers for all three, then uses an elimination prompt to choose the best Q-A pair based on relevance, detail, additional information, and answer confidence.
- Core assumption: Parallel questioning captures more diverse aspects of the claim, and elimination can reliably identify the most useful evidence path.
- Evidence anchors:
  - [abstract] "Tree of RAG (ToRAG) branches into multiple question paths and selects the best evidence via elimination"
  - [section] "the LLM generates the answers. New candidate evidence is chosen by the elimination prompt and is added to the existing list"
  - [corpus] Weak – no direct neighbor evidence for multi-path branching with elimination in RAG
- Break condition: When the follow-up check determines sufficient evidence is gathered or the maximum number of steps is reached.

### Mechanism 3
- Claim: Multimodal evidence retrieval using image captions provides context that improves fact-checking accuracy.
- Mechanism: After reverse image search, captions from matching images are retrieved and passed to GPT-4V along with the image and question. This meta-information helps contextualize the image content relative to the claim.
- Core assumption: Image captions contain relevant metadata (time, place, event) that the image alone does not convey but is useful for verification.
- Evidence anchors:
  - [section] "we use SerpAPI to conduct a reverse image search over the images associated with the claims. We extract the captions for the images from the first 10 results and use them as additional information"
  - [section] "Using the image captions retrieved from the internet and prompting the evidence retrieval along with the image caption, GPT-4V is able to identify the Afghan dignitary as Mullah Abdul Ghani Baradar"
  - [corpus] Weak – multimodal retrieval with captions is not discussed in neighbor papers
- Break condition: If the retrieved captions do not add useful information, the evidence retriever falls back to standard text-based retrieval.

## Foundational Learning

- Concept: RAG (Retrieval-Augmented Generation)
  - Why needed here: The system needs to supplement LLM knowledge with up-to-date external evidence to verify political claims.
  - Quick check question: What are the two main steps in a RAG pipeline, and why is temporal filtering applied in this work?

- Concept: Multimodal reasoning
  - Why needed here: Claims include both text and images, requiring joint analysis to capture full context.
  - Quick check question: How does multimodal claim generation differ from directly encoding the image, and why was this design chosen?

- Concept: Iterative question refinement
  - Why needed here: Complex claims may require multiple rounds of questioning to gather sufficient evidence for accurate verification.
  - Quick check question: What is the difference between CoRAG's follow-up check and traditional Chain of Thought reasoning?

## Architecture Onboarding

- Component map: Multimodal Claim Generation (GPT-4V) → Multimodal Evidence Retrieval (DuckDuckGo + reverse image search) → RAGAR Reasoning (CoRAG/ToRAG) → Veracity Prediction (Standard/CoT/CoVe) → Explanation Generation
- Critical path: Claim → Multimodal Claim Generation → Question Generation → Evidence Retrieval → Answer Generation → Follow-up Check → Repeat or Terminate → Veracity Prediction → Explanation
- Design tradeoffs:
  - Using GPT-4V for claim generation ensures rich multimodal context but adds cost; multimodal-agnostic reasoning allows swapping LLMs.
  - Reverse image search adds contextual metadata but introduces variability in search results.
  - Maximum of 6 question steps balances thoroughness with computational cost.
- Failure signatures:
  - High rate of "failed" predictions suggests retrieval quality issues or ambiguous claims.
  - Inconsistent results across runs indicate variability in search tools.
  - Lower F1 on supported class vs refuted suggests class imbalance in evidence availability.
- First 3 experiments:
  1. Run CoRAG with Standard VP on a small subset; verify that follow-up questions are generated and that evidence retrieval improves over baseline.
  2. Test ToRAG elimination prompt by feeding it three Q-A pairs and checking if it selects the most relevant one.
  3. Compare multimodal evidence retrieval with and without image captions on a few samples to quantify the impact of caption-based context.

## Open Questions the Paper Calls Out
None

## Limitations
- Exact prompt templates for reasoning steps are not fully specified, requiring recreation which may affect reproducibility
- Multimodal evidence retrieval using image captions introduces variability due to reliance on external search tools
- Dataset limited to 300 curated samples, which may not capture full complexity of real-world political fact-checking scenarios

## Confidence
- High confidence: Overall system architecture and evaluation methodology are well-documented and reproducible
- Medium confidence: Effectiveness of CoRAG and ToRAG reasoning approaches is demonstrated, but exact prompt details would strengthen validation
- Low confidence: Scalability and robustness of multimodal evidence retrieval approach across diverse political contexts remains untested

## Next Checks
1. Test the sensitivity of CoRAG performance to different prompt templates for follow-up question generation
2. Evaluate the impact of caption quality on multimodal evidence retrieval by comparing results across different search engines
3. Validate the elimination mechanism in ToRAG by conducting ablation studies with and without the elimination step