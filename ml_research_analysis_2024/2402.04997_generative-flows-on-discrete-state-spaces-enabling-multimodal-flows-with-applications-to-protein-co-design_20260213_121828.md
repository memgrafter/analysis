---
ver: rpa2
title: 'Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with
  Applications to Protein Co-Design'
arxiv_id: '2402.04997'
source_url: https://arxiv.org/abs/2402.04997
tags:
- discrete
- flow
- have
- rate
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Discrete Flow Models (DFMs), a flow-based
  generative model for discrete data that enables flow-based generative models to
  handle multimodal continuous and discrete data problems. The key insight is that
  the discrete equivalent of continuous space flow matching can be realized using
  Continuous Time Markov Chains (CTMCs).
---

# Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design

## Quick Facts
- arXiv ID: 2402.04997
- Source URL: https://arxiv.org/abs/2402.04997
- Reference count: 40
- Key outcome: Discrete Flow Models (DFMs) enable flow-based generative models to handle discrete data by constructing probability flows via CTMCs, achieving state-of-the-art protein co-design performance

## Executive Summary
This paper introduces Discrete Flow Models (DFMs), a flow-based generative model for discrete data that enables flow-based generative models to handle multimodal continuous and discrete data problems. The key insight is that the discrete equivalent of continuous space flow matching can be realized using Continuous Time Markov Chains (CTMCs). DFMs benefit from a simple derivation that includes discrete diffusion models as a specific instance while allowing improved performance over existing diffusion-based approaches. The authors apply DFMs to build a multimodal flow-based modeling framework and use it to develop Multiflow, a state-of-the-art generative protein co-design model.

## Method Summary
DFMs construct a probability flow from noise to data via CTMCs by defining a conditional flow pt|1 that linearly interpolates between distributions. The rate matrix Rt is derived from pt|1 using the Kolmogorov equation, and sampling is performed by simulating a sequence trajectory xt following pt across time using Euler steps. DFMs are trained with cross-entropy loss on a denoising neural network pθ. The framework is extended to multimodal generation by combining DFMs with continuous flow models like FrameFlow, where the conditional flow factorizes over both discrete and continuous modalities.

## Key Results
- DFMs outperform discrete diffusion models (D3PM) on text generation tasks with expanded sample time flexibility
- Multiflow achieves state-of-the-art protein co-design performance while allowing flexible generation of sequence or structure
- The CTMC stochasticity level η can be adjusted at inference time to optimize sample quality and control distributional properties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DFM enables flow-based generative models to handle discrete data by constructing a probability flow from noise to data via CTMCs
- Mechanism: The key insight is that the discrete equivalent of continuous space flow matching can be realized using Continuous Time Markov Chains (CTMCs). A probability flow pt is defined that linearly interpolates from noise to data. Sampling is done by simulating a sequence trajectory xt that follows pt across time using Euler steps.
- Core assumption: The conditional flow pt|1(·|x1) can be explicitly written down and is factorizable over dimensions

### Mechanism 2
- Claim: DFM outperforms discrete diffusion models through expanded sample time flexibility
- Mechanism: DFMs do not rely on a specific forward corruption process defined by a matrix exponential like discrete diffusion models. Instead, pt|1 can be directly written down and Rt can be chosen at inference time. This allows adjusting the CTMC stochasticity level η to optimize sample quality and control distributional properties
- Core assumption: There exists an optimal stochasticity level η that can be chosen at inference time to improve performance over a fixed process

### Mechanism 3
- Claim: DFMs enable multimodal generative modeling by combining with continuous flow-based methods
- Mechanism: A DFM is defined for the discrete modality (e.g. protein sequence) and a continuous flow model (e.g. FrameFlow) is defined for the continuous modality (e.g. protein structure). The multimodal conditional flow pt|1(Tt|T1) factorizes over both dimensions and modality. Velocities and rate matrices are chosen such that they individually generate their respective conditional flows
- Core assumption: The conditional flows for the discrete and continuous modalities can be factorized and individually generated by their respective velocities and rate matrices

## Foundational Learning

- Concept: Continuous Time Markov Chains (CTMCs)
  - Why needed here: CTMCs provide the mathematical framework for simulating the sequence trajectory xt that follows the probability flow pt. The rate matrix Rt and initial distribution p0 define the CTMC
  - Quick check question: What is the relationship between the rate matrix Rt and the marginal distribution pt in a CTMC?

- Concept: Kolmogorov equation
  - Why needed here: The Kolmogorov equation relates the rate matrix Rt to the change in the marginal distribution pt. It is used to verify that a given rate matrix generates the desired conditional flow pt|1
  - Quick check question: How does the Kolmogorov equation for a CTMC relate the incoming and outgoing probability mass to the time derivative of the marginal?

- Concept: Probability flow
  - Why needed here: The probability flow pt interpolates from noise to data and is the target distribution that the sequence trajectory xt should follow. It is constructed from the conditional flows pt|1
  - Quick check question: How is the probability flow pt defined in terms of the conditional flows pt|1?

## Architecture Onboarding

- Component map:
  - Denoising neural network pθ
1|t(x1|xt) trained with cross-entropy loss
  - Rate matrix Rt(xt, j|x1) derived from pt|1
  - Unconditional rate matrix Rθ
t (xt, j) = Epθ
1|t(x1|xt) [Rt(xt, j|x1)]
  - Euler step sampler using Rθ
t

- Critical path:
  1. Define conditional flow pt|1(·|x1)
  2. Train denoising model pθ
1|t(x1|xt) with cross-entropy
  3. Derive conditional rate matrix Rt(xt, j|x1) from pt|1
  4. Construct unconditional rate matrix Rθ
t (xt, j)
  5. Sample using Euler steps with Rθ
t

- Design tradeoffs:
  - Choice of conditional flow pt|1(·|x1) affects expressiveness and tractability
  - Choice of rate matrix Rt(xt, j|x1) affects sampling flexibility and performance
  - CTMC stochasticity level η trades off diversity vs designability

- Failure signatures:
  - Poor sample quality: check if Rt is correctly derived from pt|1, check if pθ
1|t is well-trained
  - Mode dropping: check if stochasticity level η is too low
  - Degenerate samples: check if stochasticity level η is too high

- First 3 experiments:
  1. Implement and train DFM on small text dataset (e.g. text8) and compare to D3PM
  2. Implement and train DFM on protein sequence generation task and compare to autoregressive baselines
  3. Implement and train multimodal DFM on protein co-design task and evaluate co-design performance

## Open Questions the Paper Calls Out
None explicitly called out in the provided text.

## Limitations
- The assumption that conditional flows pt|1 can be explicitly written down and factorized may not hold for complex data distributions
- The performance benefits from CTMC stochasticity level η need thorough empirical validation across diverse tasks
- The multimodal framework's factorized approach may struggle with strong correlations between discrete and continuous modalities

## Confidence
- Discrete Flow Models (DFM) Framework: High
- Performance Claims: Medium
- Multimodal Framework: Medium

## Next Checks
1. Test the multimodal factorization assumption by measuring cross-modal information loss when training discrete and continuous components separately vs jointly on synthetic correlated data
2. Systematically evaluate how CTMC stochasticity level η affects sample quality across different data types (text, proteins) to determine if the claimed flexibility translates to consistent performance gains
3. Evaluate DFM performance and training stability on larger sequence lengths and more complex discrete structures to verify practical scalability beyond the proof-of-concept experiments