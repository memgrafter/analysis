---
ver: rpa2
title: 'Looking Beyond The Top-1: Transformers Determine Top Tokens In Order'
arxiv_id: '2410.20210'
source_url: https://arxiv.org/abs/2410.20210
tags:
- saturation
- layer
- layers
- token
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies what Transformers compute after determining their
  top-1 prediction, revealing that saturation events also occur in order for top-k
  tokens (k1) across text, vision, and speech models, even in untrained ones. It proposes
  a task-transition mechanism where each task corresponds to determining the next
  top-ranking token, with transitions occurring at saturation layers.
---

# Looking Beyond The Top-1: Transformers Determine Top Tokens In Order

## Quick Facts
- arXiv ID: 2410.20210
- Source URL: https://arxiv.org/abs/2410.20210
- Reference count: 29
- Key outcome: Shows that Transformers perform discrete task transitions where each task corresponds to determining the next highest-ranking token, with saturation events occurring in order across text, vision, and speech models, even in untrained ones.

## Executive Summary
This work investigates what Transformers compute after determining their top-1 prediction, revealing that saturation events also occur in order for top-k tokens (k>1) across text, vision, and speech models, including untrained ones. The authors propose a task-transition mechanism where each task corresponds to determining the next top-ranking token, with transitions occurring at discrete saturation layers. This is supported by showing that task indices can be predicted from hidden layer embeddings and that interventions can induce task switches. The findings enable a novel token-level early-exit strategy that outperforms existing methods in balancing accuracy and efficiency.

## Method Summary
The authors extend the definition of top-1 saturation to k-th saturation layers, where each token in the top-k ranking becomes fixed at a specific layer. They use logit lens to project hidden states from intermediate layers onto the vocabulary space to analyze changes over layers. To validate the task-transition mechanism, they train logistic regression classifiers on hidden state embeddings to predict task number (which token is being determined), comparing against random embeddings as control. They also perform intervention experiments by injecting activations from saturation layers to cause task switches. The work introduces a new early-exit strategy based on the task-transition classifier that predicts which tokens have been determined at each layer.

## Key Results
- Saturation events for top-k tokens occur in strict order of token ranking (1, 2, ..., k) across GPT2-XL, ViT-L/16, and Whisper-large models, including untrained versions
- Task indices can be predicted from hidden layer embeddings with high accuracy using simple logistic regression classifiers, while random embeddings perform at chance level
- Early-exit strategy based on task-transition classifier outperforms softmax response and hidden-state saturation methods on GPT2-XL, achieving 97.9% accuracy with 50% computational cost
- Tokens with true saturation (determined before the final layer) yield more accurate predictions than those determined only at the final layer

## Why This Works (Mechanism)

### Mechanism 1: Task Transition Sequence
- Claim: Transformers perform discrete task transitions where each task corresponds to determining the next highest-ranking token in the final output.
- Mechanism: After the top-1 token is determined and remains fixed (saturation event), the model transitions to a new task of determining the second-highest token, and this process continues sequentially. Each transition occurs at a discrete saturation layer where the model switches from one task to the next while keeping previously determined tokens fixed.
- Core assumption: The saturation events represent discrete transitions between distinct computational tasks rather than continuous refinement of a single task.
- Evidence anchors:
  - [abstract] "We propose an underlying mechanism of task transition for this sequential saturation, where task k corresponds to predicting the k-th most probable token, and the saturation events are in fact discrete transitions between the tasks."
  - [section 2.3] "We argue that the mechanism underlying the saturation of the top-k tokens in order is one of task transition, such that determining the identity of each token in the final ranking is a separate task, and the model performs them sequentially."

### Mechanism 2: Task Index Encoding
- Claim: The task index (which token the model is currently determining) is encoded in the hidden layer embeddings.
- Mechanism: At each layer, the model's internal representation contains information about which ranking position it's currently working on. This information is sufficient to predict the task number using a simple classifier, independent of the specific tokens or context.
- Core assumption: Layer embeddings contain task-specific information that is separable from content-specific information.
- Evidence anchors:
  - [abstract] "we show that it is possible to predict the current task from hidden layer embedding."
  - [section 3.1] "Table 1 shows that the logistic regression classifier trained on embeddings extracted from pretrained GPT2-XL model achieves very high accuracy, while the classifier trained on the random embeddings in the control setting performs approximately at chance level."

### Mechanism 3: Switch Signal at Saturation
- Claim: The model contains a "switch" mechanism that flips on at saturation layers, signaling that a token has been determined and the model should transition to the next task.
- Mechanism: When a token reaches saturation, a discrete signal is activated that tells subsequent layers to keep that token fixed while beginning work on the next ranking position. This is demonstrated through intervention experiments where injecting activations from saturation layers causes task switches.
- Core assumption: Saturation layers encode a specific "task completion" signal that propagates to subsequent layers.
- Evidence anchors:
  - [abstract] "we can cause the model to switch from one task to the next by 'injecting' embeddings from either the top-1 saturation layer or of one of the subsequent layers."
  - [section 4.2] "There is a stark difference in the effect the injected activations have on the 1st saturation layer post-intervention when the activations are taken from the 1st saturation layer in the original run or one of the following layers, compared to the layers before it."

## Foundational Learning

- **Logit lens projection**: Why needed here - The method projects hidden states from intermediate layers onto the vocabulary space to extract rankings over tokens, enabling analysis of saturation events at layers before the final output. Quick check question: How does projecting hidden states using the unembedding matrix allow us to extract token rankings from intermediate layers?

- **Kendall's tau coefficient for ranking comparison**: Why needed here - Used to statistically validate that saturation layers occur in order of token ranking by measuring agreement between the sequence of saturation layers and the ideal sequence (1, 2, ..., k). Quick check question: What does a high Kendall's tau coefficient indicate about the relationship between token ranking and saturation layer ordering?

- **Multi-class logistic regression probing**: Why needed here - The task transition mechanism is validated by training classifiers to predict task index from layer embeddings, demonstrating that task information is encoded in representations. Quick check question: Why is it significant that a simple logistic regression classifier can predict task index from layer embeddings with high accuracy?

## Architecture Onboarding

- **Component map**: Input tokenization -> Embedding layer -> N transformer layers -> Final projection to vocabulary space -> Softmax for token probabilities. For saturation analysis, the critical path extends to extracting hidden states from each intermediate layer and projecting them to the vocabulary space.

- **Critical path**: The core computational path involves: input tokenization → embedding layer → N transformer layers → final projection to vocabulary space → softmax for token probabilities. For saturation analysis, the critical path extends to extracting hidden states from each intermediate layer and projecting them to the vocabulary space.

- **Design tradeoffs**: The choice between projecting all tokens versus only special tokens (like [CLS] in ViT) affects the analysis scope. Using only [CLS] assumes it best represents the model's prediction but may miss token-level saturation events in other positions. The decision to analyze only top-k tokens balances computational feasibility with capturing the full saturation pattern.

- **Failure signatures**: If saturation layers don't occur in order of ranking, the task transition hypothesis fails. If task index cannot be predicted from embeddings, the encoding hypothesis fails. If intervention experiments don't show the step-function pattern, the switch mechanism hypothesis fails. Inconsistent results across modalities suggest architectural dependencies.

- **First 3 experiments**:
  1. Implement logit lens projection to extract rankings from intermediate layers and verify that top-1 tokens show saturation events as described in Geva et al. (2022).
  2. Extend saturation analysis to top-k tokens (k=2,3,4,5) and compute Kendall's tau to verify ordered saturation.
  3. Implement task transition probing by training logistic regression classifiers on layer embeddings to predict task index, comparing against random embeddings control.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural components in Transformers cause the ordered saturation of top-k tokens?
- Basis in paper: [explicit] The paper discusses the task-transition mechanism but does not pinpoint the exact architectural elements responsible for the phenomenon.
- Why unresolved: While the paper demonstrates the existence of ordered saturation across various models, it does not provide a detailed analysis of the internal mechanisms that lead to this behavior.
- What evidence would resolve it: Detailed ablation studies isolating and testing the contributions of different Transformer components (e.g., attention mechanisms, feed-forward layers) to the ordered saturation phenomenon would clarify the underlying causes.

### Open Question 2
- Question: How does the number of layers after the first saturation layer affect the accuracy of higher-ranked tokens?
- Basis in paper: [explicit] The paper mentions a hyperparameter related to the number of layers for improved language modeling but does not explore its impact in depth.
- Why unresolved: The paper suggests that tokens with true saturation yield more accurate predictions but does not systematically investigate how the number of available layers influences this accuracy.
- What evidence would resolve it: Conducting experiments varying the number of layers after the first saturation layer and measuring the accuracy of higher-ranked tokens would provide insights into the relationship between layer availability and prediction accuracy.

### Open Question 3
- Question: Do other types of deep neural networks, such as Recurrent Neural Networks, exhibit similar ordered saturation behavior?
- Basis in paper: [explicit] The paper notes that future work should explore whether other DNN architectures also determine top-k tokens in order, suggesting that this is an open question.
- Why unresolved: The paper focuses on Transformer architectures and does not extend the analysis to other types of neural networks, leaving the question of whether the observed behavior is unique to Transformers.
- What evidence would resolve it: Applying the same saturation analysis to different neural network architectures, such as RNNs or CNNs, and comparing the results would determine if the ordered saturation is a general phenomenon or specific to Transformers.

## Limitations

- The underlying reasons why transformers exhibit ordered saturation behavior remain unclear, as the work demonstrates the phenomenon exists but doesn't explain why it emerges during training or whether it's an optimal computational strategy.
- The intervention experiments, while showing strong evidence for a switch mechanism, could have alternative explanations as the effects might result from general state shifts rather than specifically activating a task-completion signal.
- The early-exit strategy was only validated on text generation with GPT2-XL and would benefit from testing across different model architectures and tasks to establish broader applicability.

## Confidence

- **High confidence** in the observation that saturation events occur in order for top-k tokens across multiple modalities and model types. This is supported by direct empirical measurements using Kendall's tau coefficients and consistent results across GPT2-XL, ViT-L/16, and Whisper-large models, including untrained versions.
- **Medium confidence** in the task-transition mechanism itself. While the logistic regression probing shows that task indices can be predicted from embeddings with high accuracy, and intervention experiments demonstrate task switching behavior, the interpretation that this represents a discrete task-completion signal rather than continuous refinement remains somewhat speculative.
- **Medium confidence** in the practical applications, particularly the early-exit strategy. The method shows quantitative improvements over existing approaches, but the evaluation is limited to one specific use case (text generation with GPT2-XL) and would benefit from broader validation.

## Next Checks

1. **Cross-architecture validation**: Test the task-transition mechanism and early-exit strategy on additional transformer architectures beyond GPT2-XL, including encoder-only models like BERT and encoder-decoder models like T5, to determine if the observed patterns are universal or architecture-dependent.

2. **Temporal dynamics analysis**: Conduct ablation studies that systematically remove or modify layers around saturation points to determine whether the task-completion signal is truly discrete or represents a threshold in a continuous process. This could involve interpolating between non-saturation and saturation layer activations.

3. **Alternative probing methods**: Validate the task-encoding hypothesis using different probing techniques beyond logistic regression, such as information-theoretic measures (mutual information between embeddings and task indices) or more sophisticated classifiers, to ensure that the observed task information is not an artifact of the specific probing method used.