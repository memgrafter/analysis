---
ver: rpa2
title: 'MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation'
arxiv_id: '2409.19483'
source_url: https://arxiv.org/abs/2409.19483
tags:
- segmentation
- medical
- image
- zero-shot
- supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedCLIP-SAMv2 introduces a text-driven framework for medical image
  segmentation using fine-tuned BiomedCLIP and SAM, achieving strong zero-shot and
  weakly supervised performance across four modalities. The method employs DHN-NCE
  loss for efficient CLIP fine-tuning, M2IB for saliency map generation, and nnUNet
  checkpoint ensembling for uncertainty-aware refinement.
---

# MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation

## Quick Facts
- arXiv ID: 2409.19483
- Source URL: https://arxiv.org/abs/2409.19483
- Authors: Taha Koleilat; Hojat Asgariandehkordi; Hassan Rivaz; Yiming Xiao
- Reference count: 40
- Achieves 78.21% DSC and 82.57% NSD in zero-shot segmentation across four medical imaging modalities

## Executive Summary
MedCLIP-SAMv2 introduces a text-driven framework for universal medical image segmentation that combines fine-tuned BiomedCLIP with the Segment Anything Model (SAM). The method employs a novel Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss for efficient CLIP fine-tuning, Multi-modal Information Bottleneck (M2IB) for saliency map generation, and nnUNet checkpoint ensembling for uncertainty-aware weakly supervised refinement. Across four modalities (breast tumor ultrasound, brain tumor MRI, lung X-ray, lung CT), the framework achieves strong zero-shot and weakly supervised performance without requiring extensive labeled data.

## Method Summary
The framework fine-tunes BiomedCLIP on the MedPix dataset using DHN-NCE loss, which combines InfoNCE with hard negative sampling and decoupled contrastive learning to improve efficiency. M2IB generates saliency maps by optimizing mutual information between image and text embeddings while filtering irrelevant information. These saliency maps undergo k-means clustering and area-based filtering to create visual prompts for SAM. For weakly supervised refinement, zero-shot segmentation masks serve as pseudo-labels to train nnUNet with checkpoint ensembling, providing uncertainty estimation through entropy calculation across checkpoints.

## Key Results
- Zero-shot segmentation achieves 78.21% DSC and 82.57% NSD on average across four modalities
- Weakly supervised performance reaches 82.11% DSC and 87.33% NSD with checkpoint ensembling
- DHN-NCE fine-tuning improves BiomedCLIP cross-modal retrieval accuracy from 75.0% to 84.3% (top-1)
- M2IB outperforms CAM-based methods in saliency map generation when using fine-tuned BiomedCLIP
- Prompt engineering (P5 configuration) with GPT-4-generated descriptive prompts significantly improves segmentation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The DHN-NCE loss improves BiomedCLIP fine-tuning by reducing NPC effect and emphasizing hard negative samples.
- Mechanism: Combines InfoNCE with hard negative sampling and decoupled contrastive learning by removing the positive term in the denominator.
- Core assumption: Hard negative samples and decoupled learning improve efficiency and performance when training with smaller batch sizes.
- Evidence anchors:
  - [abstract]: "We propose the Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss, which 1) combines the InfoNCE loss [38] with hard negative sampling [40], emphasizing "close samples", and 2) incorporates decoupled contrastive learning [39] by removing the positive term in the denominator, allowing for smaller batch sizes."
  - [section III.A]: Provides the mathematical formulation of DHN-NCE loss and hardness weighting functions.
  - [corpus]: Weak; no direct evidence in corpus neighbors about DHN-NCE specifically.
- Break condition: If the hardness parameters β1 and β2 are set too high or too low, the weighting of negative samples may not effectively emphasize "close samples," leading to poor contrastive learning.

### Mechanism 2
- Claim: M2IB generates more effective saliency maps for zero-shot segmentation by aligning image and text modalities while filtering irrelevant information.
- Mechanism: Optimizes mutual information between image and text embeddings while minimizing mutual information between image embedding and the input image itself.
- Core assumption: Filtering out irrelevant information between image embedding and input image improves saliency map quality for segmentation.
- Evidence anchors:
  - [abstract]: "leveraging the Multi-modal Information Bottleneck (M2IB) to create visual prompts for generating segmentation masks from SAM in the zero-shot setting."
  - [section III.B]: Describes M2IB objective function: S = M I(Zimg, Ztext; θ) − β × M I(Zimg, I; θ)
  - [section IV.C.3]: Shows M2IB achieved highest performance across all tasks when using fine-tuned BiomedCLIP.
- Break condition: If β hyperparameter is not properly tuned, M2IB may either over-filter relevant information or fail to remove irrelevant information, degrading saliency map quality.

### Mechanism 3
- Claim: Checkpoint ensembling with nnUNet trained on pseudo-labels improves weakly supervised segmentation while providing uncertainty estimation.
- Mechanism: Trains nnUNet for multiple cycles, saving checkpoints, and averages predictions from all checkpoints; uncertainty is estimated via entropy across checkpoint predictions.
- Core assumption: Multiple diverse checkpoints capture different aspects of the solution space, and averaging them reduces variance while ensembling provides better uncertainty estimates than single models.
- Evidence anchors:
  - [abstract]: "We also investigate using zero-shot segmentation labels within a weakly supervised paradigm to enhance segmentation quality further... providing uncertainty estimation via checkpoint ensembling [25]."
  - [section III.B]: Describes checkpoint ensembling process and uncertainty estimation formula using entropy.
  - [section IV.C.2]: Shows ensembling improved DSC from 78.21 to 82.11 and NSD from 82.57 to 87.33.
- Break condition: If checkpoints are too similar (e.g., saved too frequently or with insufficient variation), ensembling provides minimal benefit and uncertainty estimates may be unreliable.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: Understanding how CLIP models learn to align images and text through contrastive objectives is essential for grasping why DHN-NCE improves upon standard InfoNCE.
  - Quick check question: What is the key difference between standard InfoNCE and the proposed DHN-NCE loss in terms of how they handle negative samples?

- Concept: Mutual information and information bottleneck theory
  - Why needed here: M2IB builds on information bottleneck theory to create better saliency maps, so understanding mutual information optimization is crucial.
  - Quick check question: In the M2IB objective function, what is the purpose of the second term (β × M I(Zimg, I; θ))?

- Concept: Uncertainty estimation in deep learning
  - Why needed here: The framework uses entropy-based uncertainty estimation from checkpoint ensembling, which requires understanding probabilistic predictions and entropy.
  - Quick check question: How is pixel-wise uncertainty calculated from the ensemble of checkpoint predictions in this framework?

## Architecture Onboarding

- Component map:
  BiomedCLIP (fine-tuned with DHN-NCE) -> M2IB -> K-means clustering -> SAM (with visual prompts) -> nnUNet (checkpoint ensembling) -> Uncertainty estimation

- Critical path:
  1. Fine-tune BiomedCLIP with DHN-NCE on MedPix dataset
  2. Generate saliency maps using M2IB with fine-tuned BiomedCLIP
  3. Post-process saliency maps (k-means + filtering)
  4. Create visual prompts and generate zero-shot masks with SAM
  5. Train nnUNet on pseudo-labels with checkpoint ensembling
  6. Generate final segmentation and uncertainty maps

- Design tradeoffs:
  - DHN-NCE vs standard InfoNCE: DHN-NCE allows smaller batch sizes but requires tuning hardness parameters
  - M2IB vs CAM-based methods: M2IB provides better alignment but is more computationally intensive
  - Checkpoint ensembling vs single model: Ensembling provides uncertainty but increases computational cost

- Failure signatures:
  - Poor fine-tuning performance: Check if hardness parameters β1, β2 are appropriately set; verify batch size is sufficient
  - Low-quality saliency maps from M2IB: Verify M2IB β parameter; check if BiomedCLIP fine-tuning was successful
  - SAM fails to refine segmentation: Check visual prompt quality; verify SAM is receiving properly post-processed input
  - Weakly supervised training underperforms: Verify pseudo-label quality; check if checkpoint diversity is sufficient

- First 3 experiments:
  1. Fine-tune BiomedCLIP with DHN-NCE on MedPix dataset and evaluate cross-modal retrieval accuracy on ROCO dataset
  2. Generate saliency maps using M2IB with fine-tuned BiomedCLIP on a single test image and visualize results
  3. Run complete zero-shot pipeline (M2IB → k-means → SAM) on a single test image and compare with ground truth

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of MedCLIP-SAMv2 scale with the size and diversity of the medical image dataset used for fine-tuning?
  - Basis in paper: [inferred] The paper fine-tunes BiomedCLIP using the MedPix dataset and tests on four diverse datasets, but does not explore scaling effects.
  - Why unresolved: The study focuses on demonstrating effectiveness across modalities but does not systematically investigate the impact of dataset size or diversity on performance.
  - What evidence would resolve it: Conducting experiments with varying sizes and diversities of medical image datasets to measure changes in segmentation accuracy and generalization.

- **Open Question 2**: What are the computational and memory requirements for deploying MedCLIP-SAMv2 in real-time clinical settings?
  - Basis in paper: [inferred] The paper does not discuss deployment considerations or resource constraints.
  - Why unresolved: While the framework is shown to be effective, practical deployment details such as computational cost and memory usage are not addressed.
  - What evidence would resolve it: Profiling the framework's resource usage during inference and identifying bottlenecks or optimization opportunities for clinical deployment.

- **Open Question 3**: How does the choice of SAM backbone architecture (e.g., ViT-H vs. ViT-B) affect segmentation performance in different medical imaging modalities?
  - Basis in paper: [explicit] The paper compares SAM, MedSAM, and SAM-Med2D with different backbones in Table VI.
  - Why unresolved: While differences are noted, a comprehensive analysis of how backbone choice impacts performance across modalities is not provided.
  - What evidence would resolve it: Systematic testing of various SAM backbone architectures across all evaluated modalities to quantify performance differences and identify optimal configurations.

- **Open Question 4**: Can the framework be extended to handle 3D volumetric medical images, and what modifications would be necessary?
  - Basis in paper: [inferred] The paper mentions future work on extending to 3D data but does not explore it.
  - Why unresolved: The current framework is designed for 2D images, and adapting it to 3D data involves significant architectural changes not discussed in the paper.
  - What evidence would resolve it: Developing and testing a 3D version of the framework, detailing necessary modifications to the model components and evaluating performance on volumetric datasets.

## Limitations

- The performance improvements from DHN-NCE loss depend critically on hardness parameters that are not fully specified, making reproducibility challenging.
- M2IB's effectiveness relies heavily on proper hyperparameter tuning, with potential degradation if the β parameter is improperly set.
- The framework's computational requirements for checkpoint ensembling and multiple model components may limit real-time clinical deployment.
- The relationship between zero-shot pseudo-label quality and final weakly supervised performance is not explicitly validated.

## Confidence

- Zero-shot segmentation performance (78.21% DSC, 82.57% NSD): High confidence
- Weakly supervised improvement (82.11% DSC, 87.33% NSD): Medium confidence
- DHN-NCE loss efficiency gains: Low confidence
- M2IB saliency map quality: Medium confidence

## Next Checks

1. **Cross-modal retrieval validation**: Evaluate fine-tuned BiomedCLIP on ROCO dataset using top-1 and top-2 accuracy metrics before proceeding with segmentation pipeline. This provides an intermediate validation step to ensure contrastive learning is functioning properly.

2. **Hyperparameter sensitivity analysis**: Systematically vary the β parameter in M2IB (e.g., 0.1, 0.5, 1.0) and the hardness parameters in DHN-NCE to determine robustness ranges and identify potential break conditions.

3. **Single-dataset ablation study**: Run the complete pipeline on a single dataset (e.g., brain tumor MRI) with and without each major component (DHN-NCE, M2IB, checkpoint ensembling) to isolate individual contributions and validate claimed improvements.