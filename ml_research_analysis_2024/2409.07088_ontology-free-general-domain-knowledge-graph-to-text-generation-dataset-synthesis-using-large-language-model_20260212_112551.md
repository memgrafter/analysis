---
ver: rpa2
title: Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset Synthesis
  using Large Language Model
arxiv_id: '2409.07088'
source_url: https://arxiv.org/abs/2409.07088
tags:
- text
- generation
- data
- datasets
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scarcity of high-quality, general-domain
  knowledge graph-to-text (G2T) datasets, which limits progress in the field. The
  authors propose a novel method to generate such datasets using a large language
  model (LLM) and Data-QuestEval, without relying on external ontologies.
---

# Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset Synthesis using Large Language Model

## Quick Facts
- arXiv ID: 2409.07088
- Source URL: https://arxiv.org/abs/2409.07088
- Authors: Daehee Kim; Deokhyung Kang; Sangwon Ryu; Gary Geunbae Lee
- Reference count: 40
- Key outcome: WikiOFGraph dataset (5.85M graph-text pairs) demonstrates high graph-text consistency and enables superior G2T model performance

## Executive Summary
This paper addresses the scarcity of high-quality, general-domain knowledge graph-to-text (G2T) datasets by proposing a novel method to generate such datasets using a large language model (LLM) and Data-QuestEval, without relying on external ontologies. The authors create WikiOFGraph, a large-scale dataset containing 5.85 million graph-text pairs extracted from Wikipedia. The dataset demonstrates high graph-text consistency, comparable to fully human-crafted datasets, and significantly outperforms other ontology-based datasets in terms of scale and domain diversity. Experiments show that a pretrained language model fine-tuned on WikiOFGraph achieves superior performance across various evaluation metrics on both human-crafted and LLM-synthesized test sets.

## Method Summary
The method involves extracting graph representations from Wikipedia sentences using an LLM with in-context learning from WebNLG examples, followed by filtering the resulting graph-text pairs using Data-QuestEval to ensure high consistency. The filtered pairs form the WikiOFGraph dataset, which is then used to fine-tune a pretrained T5-large model for G2T generation. The training procedure employs cross-entropy loss, bf16 precision, cosine learning rate decay with warmup, and ZeRO stage 3 optimization through DeepSpeed.

## Key Results
- WikiOFGraph contains 5.85 million high-quality graph-text pairs extracted from Wikipedia
- The dataset achieves graph-text consistency comparable to fully human-crafted datasets
- Models fine-tuned on WikiOFGraph significantly outperform baselines on both human-crafted and LLM-synthesized test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using Data-QuestEval to filter graph-text pairs ensures high consistency by eliminating samples where the text contains information not present in the graph or omits graph information.
- Mechanism: Data-QuestEval evaluates graph-text consistency by generating questions from the graph and checking if the text answers them. Pairs scoring below a threshold are filtered out.
- Core assumption: Data-QuestEval's question-answering approach accurately measures the completeness and exclusivity of graph information in the text.
- Evidence anchors:
  - [abstract]: "we utilize Data-QuestEval for data curation"
  - [section 3.4]: "We adapt referenceless evaluation by using the X as the source data and the Y as the predicted sentence"
- Break condition: If Data-QuestEval's question generation fails to capture all graph information or the question-answering model is biased, filtering may remove high-quality pairs or retain low-quality ones.

### Mechanism 2
- Claim: Leveraging LLM for graph extraction from text captures "all" and "only" information, leading to high graph-text consistency.
- Mechanism: LLM extracts graph representations directly from source text, avoiding the misalignment issues of ontology-based approaches.
- Core assumption: The LLM can accurately parse and structure information from diverse natural language texts into consistent graph representations.
- Evidence anchors:
  - [abstract]: "Our new dataset, which contains 5.85M general-domain graph-text pairs, offers high graph-text consistency without relying on external ontologies"
- Break condition: If the LLM struggles with complex sentence structures, ambiguous language, or domain-specific terminology, the extracted graphs may be incomplete or inaccurate.

### Mechanism 3
- Claim: In-context learning with examples from WebNLG guides the LLM to extract appropriate graph representations for G2T tasks.
- Mechanism: Providing manually selected examples helps the LLM understand the desired output format and content for graph extraction.
- Core assumption: The selected examples are representative of the variety and complexity of sentences in the target corpus.
- Evidence anchors:
  - [section 3.3]: "We manually select three examples from the WebNLG (Castro Ferreira et al., 2020) dataset for in-context examples"
- Break condition: If the examples are not diverse enough or do not cover the full range of sentence structures in the target corpus, the LLM may struggle with unseen cases.

## Foundational Learning

- Concept: Knowledge Graph-to-Text (G2T) generation
  - Why needed here: Understanding the G2T task is fundamental to grasping the paper's contribution of creating a high-quality dataset for this task.
  - Quick check question: What is the difference between G2T and T2G tasks?

- Concept: Large Language Models (LLMs) and in-context learning
  - Why needed here: The paper leverages LLM's capabilities for graph extraction and uses in-context learning to guide this process.
  - Quick check question: How does in-context learning differ from traditional fine-tuning of LLMs?

- Concept: Data-QuestEval and referenceless evaluation
  - Why needed here: Data-QuestEval is the key filtering mechanism used to ensure high graph-text consistency in the dataset.
  - Quick check question: What are the advantages of referenceless evaluation in the context of G2T dataset creation?

## Architecture Onboarding

- Component map: Wikipedia sentences -> LLM graph extraction -> Data-QuestEval filtering -> WikiOFGraph dataset
- Critical path: Source sentences → LLM graph extraction → Data-QuestEval filtering → WikiOFGraph dataset
- Design tradeoffs:
  - Using Wikipedia provides domain diversity but may introduce noise in sentence quality.
  - LLM-based extraction is flexible but may struggle with complex or ambiguous sentences.
  - Data-QuestEval filtering ensures quality but may be computationally expensive.
- Failure signatures:
  - Low graph-text consistency: Check LLM extraction quality and Data-QuestEval threshold.
  - Limited domain diversity: Review source sentence collection rules.
  - Dataset size too small: Investigate LLM extraction rate and Data-QuestEval filtering ratio.
- First 3 experiments:
  1. Test LLM graph extraction on a small, manually labeled subset of Wikipedia sentences to assess accuracy.
  2. Run Data-QuestEval filtering on LLM-extracted pairs to determine the filtering ratio and identify common error types.
  3. Compare the quality of filtered pairs against a human-crafted dataset using a subset of evaluators.

## Open Questions the Paper Calls Out
- How does the quality of WikiOFGraph compare to human-crafted datasets in terms of graph-text consistency?
- What is the impact of using different LLMs on the quality of the generated WikiOFGraph dataset?
- How does the performance of models fine-tuned on WikiOFGraph generalize to unseen domains?
- What is the optimal number of in-context examples to use for graph extraction using LLMs?

## Limitations
- LLM extraction reliability is not empirically validated, and the effectiveness of in-context learning with limited examples is questionable.
- Data-QuestEval filtering threshold determination and computational cost are not addressed.
- Downstream model performance attribution lacks ablation studies to isolate dataset size versus quality contributions.

## Confidence
- High Confidence: WikiOFGraph achieves superior performance on evaluation metrics compared to baselines (Tables 3 and 4).
- Medium Confidence: Dataset offers "high graph-text consistency" supported by Data-QuestEval filtering but lacks direct validation against human judgments.
- Low Confidence: LLM captures "all" and "only" information from source text is asserted but not empirically verified.

## Next Checks
1. Manually annotate a random sample of 100-200 WikiOFGraph pairs to create ground truth graphs, then measure the precision and recall of the LLM's graph extraction to quantify hallucination and omission rates.
2. Vary the Data-QuestEval filtering threshold and measure its impact on dataset size, graph-text consistency scores, and downstream model performance to identify the optimal balance between quality and quantity.
3. Create smaller subsets of WikiOFGraph with varying quality levels (e.g., applying different filtering thresholds) and measure their impact on downstream model performance to isolate the contribution of dataset size versus quality.