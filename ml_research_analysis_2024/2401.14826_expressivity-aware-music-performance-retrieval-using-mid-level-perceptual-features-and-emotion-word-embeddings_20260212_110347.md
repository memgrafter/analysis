---
ver: rpa2
title: Expressivity-aware Music Performance Retrieval using Mid-level Perceptual Features
  and Emotion Word Embeddings
arxiv_id: '2401.14826'
source_url: https://arxiv.org/abs/2401.14826
tags:
- music
- retrieval
- performance
- text
- mid-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of retrieving specific expressive
  performances of musical pieces based on descriptive text queries. The authors propose
  a cross-modal retrieval system that combines emotion-enriched word embeddings (EWE)
  for text representation with mid-level perceptual features extracted from audio
  recordings.
---

# Expressivity-aware Music Performance Retrieval using Mid-level Perceptual Features and Emotion Word Embeddings

## Quick Facts
- arXiv ID: 2401.14826
- Source URL: https://arxiv.org/abs/2401.14826
- Authors: Shreyan Chowdhury; Gerhard Widmer
- Reference count: 33
- One-line primary result: Emotion-enriched word embeddings (EWE) and mid-level perceptual features improve music performance retrieval accuracy over baseline methods.

## Executive Summary
This paper introduces a cross-modal retrieval system for expressive music performances, using emotion-enriched word embeddings for text representation and mid-level perceptual features extracted from audio. The system is trained using contrastive learning to align text and audio embeddings in a shared space. Experimental results on the Con Espressione dataset show significant improvements over baseline methods, with interpretable mid-level features providing explainability for retrieval decisions.

## Method Summary
The method replaces generic audio and text embeddings in the Music-Text Retrieval (MTR) model with emotion-enriched word embeddings (EWE) and pre-trained mid-level perceptual features, respectively. A linear projection maps EWE embeddings to the mid-level feature space, and the system is trained using contrastive learning with cosine similarity. The model is evaluated using 9-fold cross-validation on the Con Espressione dataset, measuring top-1 accuracy, top-2 accuracy, and mean reciprocal rank (MRR).

## Key Results
- The EWE + mid-level feature model achieves 38% top-1 retrieval accuracy and 0.61 MRR.
- This represents nearly double the performance of the baseline model.
- Mid-level perceptual features provide interpretable insights into the musical qualities influencing retrieval decisions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing the audio encoder with mid-level perceptual features improves retrieval by capturing musically meaningful, expressive dimensions directly.
- Mechanism: The pre-trained mid-level model extracts 8 interpretable features (e.g., melodiousness, articulation, rhythm stability) that are perceptually grounded and trained to predict human annotations of expressive qualities. These features are mapped into the shared embedding space, preserving musically relevant variance.
- Core assumption: The mid-level feature space is semantically coherent and better aligns with how humans describe expressive performance than generic spectro-temporal embeddings.
- Evidence anchors:
  - [abstract]: "We use emotion-enriched word embeddings (EWE) and on the audio side, we extract mid-level perceptual features instead of generic audio embeddings."
  - [section]: "We focus on so-called Mid-level Perceptual Features – relatively high-level musical qualities that are considered to be perceptually important [5]; in our work, these are learnt from human annotations."
  - [corpus]: Weak evidence; no related paper in the corpus explicitly discusses mid-level perceptual features for retrieval.
- Break condition: If the mid-level feature space fails to align with human semantic descriptions, retrieval performance drops to baseline levels.

### Mechanism 2
- Claim: Emotion-enriched word embeddings (EWE) improve text encoding by resolving emotional polysemy.
- Mechanism: EWE incorporates emotion labels during training, so words like "sad" and "happy" are mapped to different regions despite co-occurrence in similar contexts. This yields more discriminative embeddings for emotional descriptors.
- Core assumption: Emotional polarity in text descriptors is a key driver for distinguishing expressive performances.
- Evidence anchors:
  - [abstract]: "we use emotion-enriched word embeddings (EWE) and on the audio side, we extract mid-level perceptual features instead of generic audio embeddings."
  - [section]: "Such language representation models carry the inductive bias that words used in the same context tend to possess similar meanings, thus resulting in emotionally dissimilar words like happy and sad having close proximity in the representation space... To derive correct emotional meanings from our text queries, we experiment with emotion-enriched word embeddings from Agrawal et al. [3] and find that they improve retrieval results significantly..."
  - [corpus]: No corpus paper directly validates EWE for music retrieval; this is domain-specific.
- Break condition: If queries are mostly non-emotional descriptors, EWE gains may vanish.

### Mechanism 3
- Claim: The linear mapping ℎ projects text embeddings into mid-level feature space, enabling interpretability.
- Mechanism: By mapping text embeddings to the same space as mid-level features, the system can directly compare and explain retrieval decisions via the 8 interpretable dimensions.
- Core assumption: A linear projection is sufficient to align EWE embeddings with mid-level feature space geometry.
- Evidence anchors:
  - [abstract]: "Additionally, our interpretable mid-level features provide a route for introducing explainability in the retrieval and downstream recommendation processes."
  - [section]: "We choose to project the text embeddings onto the mid-level feature space using a linear model ℎ... Preserving the mid-level feature predictions has the additional advantage of providing explainable insight into the retrieval process..."
  - [corpus]: No explicit corpus support; this is a design choice.
- Break condition: If the embedding manifolds are highly nonlinear, a linear ℎ will be insufficient and MRR will plateau.

## Foundational Learning

- Concept: Cross-modal contrastive learning
  - Why needed here: The system must learn to align semantically related audio-text pairs in a shared embedding space without explicit paired labels beyond retrieval pairs.
  - Quick check question: In a triplet loss, what is the role of the anchor, positive, and negative examples in shaping the embedding space?

- Concept: Perceptual feature extraction
  - Why needed here: Mid-level features encode high-level, human-interpretable musical attributes (e.g., articulation, dissonance) that align with expressive descriptors.
  - Quick check question: Why might low-level spectral features be less effective than mid-level perceptual features for capturing expressive nuance?

- Concept: Emotion-enriched embeddings
  - Why needed here: Standard embeddings collapse emotional antonyms into nearby regions; EWE preserves polarity, critical for distinguishing "joyful" vs "melancholic" performances.
  - Quick check question: How does incorporating emotion labels during word embedding training change the geometry of the embedding space compared to unsupervised training?

## Architecture Onboarding

- Component map:
  - Audio path: Spectrogram → pre-trained mid-level model → 8D feature vector
  - Text path: Tokenization → EWE model → 300D word vectors → summed → PCA (optional) → linear ℎ → 8D feature vector
  - Retrieval: Cosine similarity in 8D mid-level space

- Critical path: Text embedding → ℎ → retrieval score; any bottleneck in ℎ directly impacts top-1 accuracy.

- Design tradeoffs:
  - Interpretability vs expressivity: Mid-level features are interpretable but may miss fine-grained acoustic details captured by deep spectro-temporal embeddings.
  - Fixed feature space vs adaptability: Using a pre-trained mid-level model locks the feature space; domain adaptation could improve results but adds complexity.

- Failure signatures:
  - Top-1 accuracy near random guessing → ℎ mapping misaligned or feature spaces poorly correlated.
  - High MRR but low top-1 → retrieval ranks correct item but not at top; may indicate ℎ compression issues.

- First 3 experiments:
  1. Verify mid-level feature predictions match human ratings on held-out samples.
  2. Ablate PCA: compare retrieval with and without PCA on text embeddings.
  3. Compare retrieval with generic GloVe vs EWE on a subset of queries with known emotional polarity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance and interpretability of mid-level perceptual features change when applied to genres beyond classical piano music?
- Basis in paper: [explicit] The paper uses a dataset of piano performances and notes that the mid-level model is "domain-adapted for piano music," but does not test other genres.
- Why unresolved: The study is limited to classical piano, and the generalizability of mid-level features to other musical styles (e.g., jazz, pop, or non-Western music) remains unexplored.
- What evidence would resolve it: Testing the retrieval system on datasets from diverse genres and comparing the performance of mid-level features with genre-specific adaptations.

### Open Question 2
- Question: How does the choice of text encoder affect the system's ability to handle more complex or nuanced descriptions of musical expressiveness?
- Basis in paper: [explicit] The paper contrasts emotion-enriched word embeddings (EWE) with traditional BERT embeddings but does not explore the impact of more advanced or context-aware language models.
- Why unresolved: While EWE improves retrieval accuracy, the paper does not investigate whether transformer-based models or other advanced NLP techniques could further enhance the system's sensitivity to nuanced descriptions.
- What evidence would resolve it: Comparative experiments using context-aware language models (e.g., RoBERTa, T5) and analyzing their performance on complex queries.

### Open Question 3
- Question: Can the mid-level perceptual features be optimized further to improve retrieval accuracy for expressivity-aware tasks?
- Basis in paper: [inferred] The paper uses a fixed set of seven mid-level features plus onset density, but does not explore whether additional or modified features could better capture expressive qualities.
- Why unresolved: The feature set is based on prior research but may not be exhaustive or optimally tuned for the specific task of expressivity-aware retrieval.
- What evidence would resolve it: Systematic experimentation with expanded or dynamically learned feature sets and evaluating their impact on retrieval performance.

## Limitations
- The dataset size (45 performances across 9 pieces) limits generalizability.
- Mid-level perceptual features are pre-trained on a separate dataset and may not fully capture the specific expressive qualities of piano performances in the Con Espressione dataset.
- The interpretability claims rely on the assumption that the 8 mid-level features are truly representative of human-perceived expressiveness, but this mapping is not explicitly validated against listener feedback.

## Confidence
- **High confidence**: The retrieval architecture and evaluation methodology are clearly specified. The improvement over baseline models (38% top-1 accuracy vs. lower baseline performance) is well-documented with appropriate metrics (MRR, Top-1, Top-2).
- **Medium confidence**: The mechanism by which mid-level features improve retrieval is plausible but relies on the assumption that pre-trained perceptual features align well with human expressive descriptors. The EWE improvement claim is supported by experimental results but lacks comparison to other emotion-aware embedding approaches.
- **Low confidence**: The interpretability claims are primarily asserted rather than empirically validated. The paper doesn't provide listener studies confirming that the 8 mid-level features actually explain retrieval decisions in ways users find meaningful.

## Next Checks
1. **Cross-dataset validation**: Test the retrieval model on a different expressive performance dataset (e.g., MAESTRO or other piano performance collections) to assess generalizability beyond the Con Espressione dataset.
2. **Feature importance analysis**: Conduct ablation studies removing individual mid-level features to identify which dimensions contribute most to retrieval accuracy, and verify these align with human judgments of expressiveness.
3. **Human evaluation study**: Recruit listeners to rate the perceptual relevance of the 8 mid-level features for explaining retrieval decisions, comparing system explanations against ground truth listener descriptions.