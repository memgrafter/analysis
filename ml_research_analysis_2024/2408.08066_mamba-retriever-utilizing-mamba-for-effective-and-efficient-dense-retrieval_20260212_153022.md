---
ver: rpa2
title: 'Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval'
arxiv_id: '2408.08066'
source_url: https://arxiv.org/abs/2408.08066
tags:
- mamba
- retrieval
- retriever
- long-text
- effectiveness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mamba Retriever, a dense retrieval model
  that leverages the Mamba architecture to achieve both high effectiveness and efficiency
  in information retrieval tasks. The study addresses the challenge of balancing retrieval
  performance with computational efficiency, particularly for long-text retrieval
  where traditional Transformer-based models suffer from quadratic time complexity.
---

# Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval

## Quick Facts
- arXiv ID: 2408.08066
- Source URL: https://arxiv.org/abs/2408.08066
- Authors: Hanqi Zhang; Chong Chen; Lang Mei; Qi Liu; Jiaxin Mao
- Reference count: 31
- Key outcome: Mamba Retriever achieves comparable or better effectiveness than Transformer-based models on MS MARCO and BEIR benchmarks while offering superior inference speed with linear time scaling for long-text retrieval

## Executive Summary
This paper introduces Mamba Retriever, a dense retrieval model that leverages the Mamba architecture to achieve both high effectiveness and efficiency in information retrieval tasks. The study addresses the challenge of balancing retrieval performance with computational efficiency, particularly for long-text retrieval where traditional Transformer-based models suffer from quadratic time complexity. The Mamba Retriever uses a bi-encoder architecture with Mamba as the base model, fine-tuned on both short-text (MS MARCO) and long-text (LoCoV0) retrieval datasets.

## Method Summary
Mamba Retriever employs a bi-encoder architecture where separate Mamba models encode queries and passages into dense representations. The model uses selective state space modeling (SSM) with a discretized recurrent structure that enables linear time complexity O(N) instead of quadratic O(L¬≤) for self-attention. Training uses InfoNCE loss with negative sampling, and the model is fine-tuned on retrieval datasets after pre-training. The architecture extracts embeddings from the <EOS> token, applies L2 normalization, and computes cosine similarity between query and passage representations.

## Key Results
- Achieves comparable or better effectiveness than Transformer-based models on MS MARCO passage ranking and BEIR benchmarks
- Demonstrates effectiveness growth with increasing model size (130M ‚Üí 370M ‚Üí 790M parameters)
- Shows superior inference speed with linear time scaling for long-text retrieval on LoCoV0 dataset
- Successfully extends to handle longer sequences than pre-training length after fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mamba Retriever achieves linear time complexity for long-text retrieval by replacing Transformer self-attention with selective state space modeling
- Mechanism: The Mamba architecture uses a discretized state space model where recurrent update operates in O(N) time per token rather than O(L¬≤) for self-attention. The selective mechanism adaptively controls which information to retain or forget across sequence length
- Core assumption: Selective state space model can capture long-range dependencies as effectively as self-attention for retrieval tasks despite being unidirectional
- Evidence anchors:
  - [abstract] "Some recently proposed non-Transformer PLMs, especially the Mamba architecture PLMs, have demonstrated not only comparable effectiveness to Transformer-based PLMs on generative language tasks but also better efficiency due to linear time scaling in sequence length"
  - [section] "Based on the SSM, Mamba introduces a selection mechanism and corresponding hardware-aware parallel algorithm. The selection mechanism is making Œî, ùêµ, ùê∂ dependent on the current token"
  - [corpus] Weak evidence - no direct citations found for Mamba's effectiveness in retrieval tasks specifically

### Mechanism 2
- Claim: Mamba Retriever maintains or improves retrieval effectiveness as model size increases
- Mechanism: Larger Mamba models have more parameters in selective state space blocks, enabling richer representation of query-passage relationships. Linear scaling allows larger models to still be efficient at inference
- Core assumption: Increasing model capacity in Mamba architecture translates to better semantic understanding for retrieval similar to Transformer models
- Evidence anchors:
  - [abstract] "on the MS MARCO passage ranking dataset and BEIR, the Mamba Retriever achieves comparable or better effectiveness compared to Transformer-based retrieval models, and the effectiveness grows with the size of the Mamba model"
  - [section] "This suggests that Mamba has stronger text comprehension and summarization ability, possibly due to its advantages discussed in Section 3.2"
  - [corpus] Weak evidence - limited citations for Mamba's effectiveness growth with size in retrieval

### Mechanism 3
- Claim: Mamba Retriever can extend beyond its pre-training length for long-text retrieval after fine-tuning
- Mechanism: During fine-tuning on retrieval tasks, model learns to compress longer sequences into fixed-size latent state more effectively, extending context window beyond pre-training
- Core assumption: Retrieval fine-tuning adapts selective mechanism to handle longer sequences than pre-training without requiring architectural modifications
- Evidence anchors:
  - [abstract] "on the long-text LoCoV0 dataset, the Mamba Retriever can extend to longer text length than its pre-trained length after fine-tuning on retrieval task"
  - [section] "Although Mamba's memory capacity for long text is limited by latent state size due to the lack of self-attention mechanism, it still has comparable or better capability to Transformer decoder-only models"
  - [corpus] No direct evidence found for length extension capability in retrieval

## Foundational Learning

- Concept: State Space Models (SSM)
  - Why needed here: Understanding SSM is crucial to grasp how Mamba achieves linear time complexity instead of quadratic self-attention
  - Quick check question: How does the state space model equation ‚Ñé‚Ä≤ (ùë°) = ùê¥‚Ñé(ùë°) + ùêµùë• (ùë°) differ fundamentally from self-attention in terms of computational complexity?

- Concept: Selective State Space Mechanism
  - Why needed here: The selection mechanism enables Mamba to adaptively control information flow, which is key to its effectiveness
  - Quick check question: What is the role of making Œî, ùêµ, ùê∂ dependent on the current token in the Mamba architecture?

- Concept: Bi-encoder Architecture
  - Why needed here: Mamba Retriever uses bi-encoder approach, so understanding how separate query and passage encoders work is fundamental
  - Quick check question: How does cosine similarity calculation in bi-encoder differ from cross-attention approaches in terms of information sharing?

## Architecture Onboarding

- Component map: Token sequence ‚Üí Mamba blocks ‚Üí latent state update ‚Üí <EOS> output ‚Üí embedding normalization ‚Üí cosine similarity
- Critical path: Token sequence ‚Üí Mamba blocks ‚Üí latent state update ‚Üí <EOS> output ‚Üí embedding normalization ‚Üí cosine similarity. The bottleneck is sequential processing through Mamba blocks, though this is O(N) rather than O(L¬≤)
- Design tradeoffs: Unidirectional vs bidirectional (Mamba is unidirectional for efficiency, sacrificing bidirectional context); Fixed latent state size (limits maximum information capacity but enables linear scaling); <EOS> token usage (simple but may not capture all relevant information)
- Failure signatures: Performance degradation on very long sequences (>8K tokens); Sensitivity to learning rate settings during fine-tuning; Suboptimal performance when pre-training length is much shorter than target retrieval length
- First 3 experiments:
  1. Compare inference time of Mamba vs Transformer on sequences of increasing length (512, 2K, 8K tokens)
  2. Evaluate retrieval effectiveness on MS MARCO with different Mamba model sizes (130M, 370M, 790M)
  3. Test Mamba's ability to handle sequences longer than pre-training length by fine-tuning on 2K sequences then evaluating on 8K sequences

## Open Questions the Paper Calls Out

- How does Mamba Retriever's performance scale when pre-trained on longer sequences (e.g., 8k or 32k tokens) compared to its current 2k pre-training length?
- How does Mamba Retriever's effectiveness compare to other non-Transformer architectures like RWKV or Monarch Mixer in dense retrieval tasks?
- What is the impact of using bidirectional Mamba (if implemented) on retrieval effectiveness compared to the unidirectional version used in this study?

## Limitations

- Limited evaluation scope: Only tested on MS MARCO, BEIR, and LoCoV0 datasets without exploring diverse retrieval tasks
- Pre-training data uncertainty: Doesn't specify pre-training corpus or methodology for Mamba models used
- Length extrapolation without theoretical guarantees: Claims of handling sequences longer than pre-training lack theoretical justification

## Confidence

- High confidence: Efficiency claims regarding linear time complexity are well-supported by Mamba architecture's theoretical foundations
- Medium confidence: Effectiveness comparisons with Transformer-based models are credible but show modest improvements in many cases
- Low confidence: Mechanism by which fine-tuning enables longer sequence handling is not well-explained and lacks theoretical foundation

## Next Checks

1. Evaluate Mamba Retriever on additional retrieval benchmarks (e.g., Natural Questions, TriviaQA) to verify effectiveness gains extend beyond MS MARCO and BEIR
2. Compare Mamba Retriever against variant where selection mechanism is disabled to quantify performance contribution of adaptive selection
3. Systematically test Mamba Retriever on sequences of 16K, 32K, and 64K tokens to empirically determine practical limits of length extension