---
ver: rpa2
title: Towards Goal-Oriented Agents for Evolving Problems Observed via Conversation
arxiv_id: '2401.05822'
source_url: https://arxiv.org/abs/2401.05822
tags:
- agent
- learning
- user
- square
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a DQN-based chatbot agent trained to solve
  evolving problems through conversation with a simulated user. The agent interacts
  with a gridworld navigation task by asking questions and giving movement instructions,
  without directly observing the environment.
---

# Towards Goal-Oriented Agents for Evolving Problems Observed via Conversation

## Quick Facts
- arXiv ID: 2401.05822
- Source URL: https://arxiv.org/abs/2401.05822
- Authors: Michael Free; Andrew Langworthy; Mary Dimitropoulaki; Simon Thompson
- Reference count: 37
- Primary result: DQN-based chatbot agent achieves 84% success rate on evolving gridworld navigation tasks through conversation

## Executive Summary
This paper presents a novel approach to training goal-oriented agents that solve navigation problems through conversational interaction rather than direct environmental observation. The system uses a Deep Q-Network (DQN) architecture where the agent asks questions and gives movement instructions to a simulated user who has direct access to the gridworld environment. Through curriculum learning, the agent learns to effectively gather information and navigate to target locations, achieving strong performance while requiring 40% fewer training episodes compared to standard approaches.

## Method Summary
The system consists of a DQN-based agent that interacts with a simulated user (based on MAC network) to solve gridworld navigation problems. The agent uses an LSTM architecture to process conversational history encoded via BERT, making decisions between asking questions about the environment or executing movement commands. Training employs curriculum learning by progressively introducing more complex scenarios based on minimum path length requirements. The agent is trained on 130,000 randomly generated 6x6 gridworlds with various obstacles and traps, using double DQN with Îµ-greedy exploration.

## Key Results
- Best-performing LSTM-based architecture achieved 84% success rate and 44.6 average reward
- Curriculum learning reduced required training episodes by 40% compared to training on full dataset from start
- Human baseline tests showed 95% success but lower average reward, indicating agent optimizes for reward over guaranteed success
- Modified reward functions showed minimal impact on agent behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The DQN-based architecture learns to map conversational histories to optimal actions in the gridworld environment.
- Mechanism: The agent uses a double DQN approach, learning a Q-function that approximates the value of state-action pairs. The state is encoded as a combination of the most recent utterance embedding, conversational history, and turn number. This information is processed through an LSTM layer and then through fully connected layers to produce a score over possible actions.
- Core assumption: The LSTM can effectively capture long-term dependencies in the conversational history to inform action selection.
- Evidence anchors:
  - [abstract] "a Deep Q-Network (DQN)-based chatbot architecture"
  - [section] "The architecture of the agent is shown in Figure 2... The conversation utterances are encoded as sentence-level embeddings using a pre-trained BERT model... Once the input has been created, it is fed into an LSTM with 64 hidden units. The output of that is then fed into a 3-layer network that returns a score over the 14 next possible utterances for the agent."
  - [corpus] Weak - the corpus neighbors focus on related dialogue systems but don't provide direct evidence for this specific mechanism.
- Break condition: If the conversational history becomes too long, the LSTM may struggle to capture relevant information, leading to suboptimal action selection.

### Mechanism 2
- Claim: Curriculum learning significantly improves training efficiency by gradually increasing problem complexity.
- Mechanism: The training data is partitioned based on the minimum number of turns required to complete each scenario. The model starts by learning from simpler problems and progressively incorporates more complex ones as its performance improves.
- Core assumption: Learning simpler problems first provides a foundation for tackling more complex ones.
- Evidence anchors:
  - [section] "Before training, we partitioned the training data into three sets based on the minimum number of turns required to complete the scenario... During training, the model learned in a guided way, where at first only the problems with short path lengths are seen."
  - [section] "The agent trained using a curriculum learning regime reached a success rate of 0.5 on the full dataset by 324,000 runs, whereas it took the agent trained on the whole dataset from the beginning 453,000 runs, a 39.8% increase."
  - [corpus] Weak - the corpus neighbors don't directly address curriculum learning in this context.
- Break condition: If the curriculum stages are not well-designed, the agent may overfit to simpler problems and struggle to generalize to more complex ones.

### Mechanism 3
- Claim: The simulated user provides a reliable intermediary for the agent to interact with the environment.
- Mechanism: The simulated user, based on a MAC network, can answer natural language questions about the gridworld and execute movement commands. It provides the agent with information about the environment and confirms the outcomes of actions.
- Core assumption: The MAC network can accurately interpret the agent's questions and generate appropriate responses based on the current state of the gridworld.
- Evidence anchors:
  - [section] "The simulated user observes the gridsworld, can answer natural language questions about the environment, and can take actions upon request. This simulated user is based on the MAC network implementation from [9]."
  - [section] "Given a gridsworld scene we automatically generated questions, actions, and their associated answers in a rules-based fashion. The MAC network was then trained on these in an identical regime to that of [9]."
  - [corpus] Weak - the corpus neighbors focus on related dialogue systems but don't provide direct evidence for the reliability of the simulated user.
- Break condition: If the MAC network fails to accurately interpret questions or generate correct responses, the agent's ability to solve the gridworld problems will be compromised.

## Foundational Learning

- Concept: Reinforcement Learning (RL)
  - Why needed here: The agent learns to solve the gridworld problems through trial and error, receiving rewards for successful actions and penalties for unsuccessful ones.
  - Quick check question: What is the difference between Q-learning and SARSA in reinforcement learning?

- Concept: Deep Q-Networks (DQN)
  - Why needed here: DQN is used to approximate the Q-function, allowing the agent to handle high-dimensional state spaces like conversational histories.
  - Quick check question: How does double DQN address the overestimation problem in Q-learning?

- Concept: Curriculum Learning
  - Why needed here: Curriculum learning is used to improve training efficiency by gradually increasing problem complexity.
  - Quick check question: What are the potential drawbacks of using curriculum learning in reinforcement learning?

## Architecture Onboarding

- Component map: Gridworld environment -> Simulated user (MAC network) -> DQN agent -> Training pipeline
- Critical path: 1) Generate gridworld instances and associated questions/actions/answers 2) Train the simulated user (MAC network) on the generated data 3) Train the RL agent (DQN) on the gridworld problems using the simulated user as an intermediary 4) Evaluate the agent's performance on a hold-out dataset
- Design tradeoffs: Using a simulated user adds complexity but allows the agent to learn without direct access to the environment; Curriculum learning improves training efficiency but requires careful design of the curriculum stages; The choice of LSTM for encoding conversational history balances the need to capture long-term dependencies with computational efficiency
- Failure signatures: Low success rate; High variance in performance; Slow convergence
- First 3 experiments: 1) Train the RL agent on a small subset of simple gridworld problems without curriculum learning to establish a baseline 2) Implement curriculum learning and compare the agent's performance and training efficiency to the baseline 3) Experiment with different architectures for the RL agent (e.g., fully connected layers, CNN) and compare their performance to the LSTM-based architecture

## Open Questions the Paper Calls Out

- Question: How would the system perform with a non-template-based conversational user that generates natural language without fixed patterns?
  - Basis in paper: [explicit] The paper mentions that further work could allow the simulated user to generate natural language without a template, referencing [14], and suggests an encoding method that could deal with this was chosen.
  - Why unresolved: The current system is designed with a template-based simulated user, and the paper does not explore the impact of transitioning to a more natural, non-template-based conversational model.
  - What evidence would resolve it: Testing the system with a generative model for the simulated user that produces varied natural language responses and comparing performance metrics to the current template-based approach.

- Question: What is the optimal reward function for real-world customer interaction scenarios?
  - Basis in paper: [explicit] The paper notes that the agent exhibits behavior optimal in the context of the given reward function but may not be suited to real interaction, such as abandoning questions and utilizing only movement in a searching pattern as the conversational turn limit is approached. It suggests further exploration of the reward function optimal for customer experience.
  - Why unresolved: The current reward function is simplistic and designed for the specific gridworld task. Real-world scenarios may require more nuanced reward structures that balance task completion with customer satisfaction.
  - What evidence would resolve it: Conducting experiments with various reward functions that incorporate customer satisfaction metrics and comparing their impact on agent performance and user experience in simulated customer service scenarios.

- Question: How does the system scale with increased complexity in the gridworld environment, such as larger grid sizes or additional elements like walls and mountains?
  - Basis in paper: [inferred] The paper discusses the possibility of increasing complexity in the problem by changing the grid size or shape, the number of question iterations, the reward function, or introducing new elements to the world. It also mentions that very complex scenarios would be out of scope due to the complexity of simulation required.
  - Why unresolved: The current experiments are limited to a 6x6 grid with basic elements (obstacles, traps, circle, and square). The paper does not explore the system's performance with more complex environments or additional elements that could affect the agent's decision-making process.
  - What evidence would resolve it: Training and testing the agent on progressively more complex gridworld environments, including larger grids and additional elements like walls and mountains, and analyzing the impact on performance metrics such as success rate and average reward.

## Limitations
- The simulated user's reliability is a critical bottleneck that could compromise agent performance
- The curriculum learning design is opaque with undisclosed threshold values for progression
- The relatively small gridworld size (6x6) and limited complexity may not generalize to real-world navigation tasks

## Confidence
- High confidence: The DQN architecture with LSTM-based conversational encoding works effectively for the specified gridworld task (84% success rate achieved)
- Medium confidence: Curriculum learning provides significant training efficiency improvements (40% reduction in episodes) though exact threshold mechanisms are unclear
- Medium confidence: The simulated user approach is viable for this controlled environment, but scalability to more complex scenarios is uncertain

## Next Checks
1. Test agent performance when the simulated user is replaced with a human intermediary to verify the approach generalizes beyond the MAC network
2. Vary curriculum learning thresholds systematically to determine sensitivity and identify optimal progression criteria
3. Scale gridworld complexity (larger grids, more obstacles, dynamic environments) to assess the approach's limitations and robustness