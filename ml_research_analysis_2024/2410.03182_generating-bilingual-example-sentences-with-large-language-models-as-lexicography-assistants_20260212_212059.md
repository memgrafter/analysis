---
ver: rpa2
title: Generating bilingual example sentences with large language models as lexicography
  assistants
arxiv_id: '2410.03182'
source_url: https://arxiv.org/abs/2410.03182
tags:
- example
- language
- rating
- sentences
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates large language models (LLMs) for generating
  and rating bilingual example sentences across three languages: French (high-resource),
  Indonesian (mid-resource), and Tetun (low-resource), with English as the target
  language. The study assesses LLM-generated examples against GDEX criteria (typicality,
  informativeness, and intelligibility) and translation accuracy.'
---

# Generating bilingual example sentences with large language models as lexicography assistants

## Quick Facts
- arXiv ID: 2410.03182
- Source URL: https://arxiv.org/abs/2410.03182
- Reference count: 24
- Large language models can generate reasonably good dictionary examples, but performance degrades significantly for lower-resourced languages

## Executive Summary
This paper evaluates large language models (LLMs) for generating and rating bilingual example sentences across three languages: French (high-resource), Indonesian (mid-resource), and Tetun (low-resource), with English as the target language. The study assesses LLM-generated examples against GDEX criteria (typicality, informativeness, and intelligibility) and translation accuracy. Results show that LLMs can generate reasonably good dictionary examples, but performance degrades significantly for lower-resourced languages. For instance, 95% of French examples are rated "typical" compared to only 69% for Tetun. The study also finds high variability in human preferences, reflected in low inter-annotator agreement rates. However, in-context learning can successfully align LLMs with individual annotator preferences. Additionally, sentence perplexity serves as a good proxy for typicality and intelligibility in higher-resourced languages.

## Method Summary
The study uses 50 word pairs per language (French, Indonesian, Tetun to English) from top 10,000 most frequent words, generates example sentence pairs using GPT-4o and Llama 3.1, and has native speakers rate 600 sentence pairs across five criteria. The research employs in-context learning for LLM alignment with individual annotator preferences and uses pre-trained language model metrics (perplexity, word probability, entropy) for automated rating. The methodology focuses on evaluating performance across different resource levels and testing automated rating methods against human judgments.

## Key Results
- LLM performance degrades significantly with language resource level: 95% of French examples rated "typical" vs 69% for Tetun
- High inter-annotator disagreement requires aligning LLM ratings with individual preferences rather than averaging
- In-context learning successfully aligns LLM ratings with individual annotator preferences (correlation coefficients 0.29-0.54)
- Sentence perplexity correlates with typicality and intelligibility for higher-resource languages (-0.57 to -0.41) but not for Tetun

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM performance on dictionary example generation degrades predictably with language resource level
- Mechanism: Lower resource languages have less training data, leading to poorer quality outputs across all GDEX criteria
- Core assumption: Training data quantity and quality directly impacts LLM output quality for specific tasks
- Evidence anchors:
  - [abstract] "their performance degrades significantly for lower-resourced languages" with specific numbers: French 95% typical vs Tetun 69% typical
  - [section 3.2] "This pattern is consistent with previously observed LLM performance degradation on lower-resourced languages" and reference to MADALAD-400 corpus showing 5,000x more French documents than Tetun
  - [corpus] Strong evidence: 6x more French documents (220M) than Indonesian (38M), 5,000x more French than Tetun (40k)
- Break condition: When pretraining corpora size difference becomes negligible, or when targeted adaptation techniques compensate for data scarcity

### Mechanism 2
- Claim: In-context learning can align LLM ratings with individual annotator preferences despite high inter-annotator disagreement
- Mechanism: LLMs can learn and replicate individual judgment patterns from small sets of examples, even when different annotators have conflicting standards
- Core assumption: Individual rating patterns are learnable and reproducible through few-shot examples
- Evidence anchors:
  - [abstract] "in-context learning can successfully align LLMs with individual annotator preferences"
  - [section 4.1] Results show significant correlations (0.29 to 0.54) between LLM predictions and individual annotator ratings across all languages
  - [section 3.3] High inter-annotator disagreement (low Krippendorff's alpha) makes aligning with single annotators necessary
- Break condition: When individual preferences are too subtle to capture in few examples, or when annotator preferences change over time

### Mechanism 3
- Claim: Sentence perplexity serves as a reliable proxy for typicality and intelligibility in higher-resource languages
- Mechanism: Perplexity measures predictability of text, which correlates with how typical and clear example sentences are
- Core assumption: Lower perplexity indicates more natural, fluent text that better meets GDEX criteria
- Evidence anchors:
  - [abstract] "sentence perplexity serves as a good proxy for typicality and intelligibility in higher-resourced languages"
  - [section 4.2] Correlation coefficients of -0.57 to -0.41 between perplexity and intelligibility/typicality for French and Indonesian
  - [section 4.2] No significant correlations found for Tetun, indicating threshold effect based on resource level
- Break condition: When language models cannot adequately capture sentence quality due to insufficient training data

## Foundational Learning

- Concept: GDEX criteria (Typicality, Informativeness, Intelligibility)
  - Why needed here: Provides standardized framework for evaluating dictionary examples
  - Quick check question: What are the three main criteria used to evaluate good dictionary examples in this study?

- Concept: Inter-annotator agreement measurement (Krippendorff's alpha)
  - Why needed here: Quantifies consistency between human raters, revealing subjectivity in quality judgments
  - Quick check question: What metric is used to measure agreement between different annotators rating the same examples?

- Concept: In-context learning
  - Why needed here: Enables LLMs to adapt to specific rating preferences without fine-tuning
  - Quick check question: How does the study align LLM ratings with individual annotator preferences despite high disagreement?

## Architecture Onboarding

- Component map:
  Word selection (50 words per language from top 10k most frequent) -> Example generation (LLMs generating source-target sentence pairs) -> Human annotation (rating against GDEX criteria and overall quality) -> Automated rating (both LLM-based in-context learning and LM-based metrics) -> Dataset compilation (600 sentence pairs with 3,000 individual annotations)

- Critical path:
  1. Select words from frequency lists
  2. Generate example pairs using LLMs
  3. Have annotators rate examples
  4. Evaluate automated rating methods
  5. Analyze results across languages and annotators

- Design tradeoffs:
  - Single vs multiple annotators per language: Multiple captures variability but introduces disagreement
  - LLM vs traditional LM for automated rating: LLMs can learn preferences but are resource-intensive
  - Resource level coverage: Broader coverage reveals degradation patterns but complicates analysis

- Failure signatures:
  - Low inter-annotator agreement indicates subjective quality judgments
  - Poor perplexity-LM correlation for low-resource languages indicates inadequate language models
  - LLM self-preference bias if same model used for generation and evaluation

- First 3 experiments:
  1. Generate examples for a small word set and have one annotator rate them to validate the process
  2. Compare GPT-4o vs Llama 3.1 on a held-out word set to establish baseline LLM performance differences
  3. Test in-context learning alignment on French examples first (highest quality, easiest to work with) before extending to other languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM performance in generating bilingual example sentences vary when using languages with non-Latin scripts as source or target?
- Basis in paper: [inferred] The paper exclusively uses Latin-script languages and English as the target language, raising questions about performance with other scripts.
- Why unresolved: The study's scope was limited to Latin-script languages, leaving a gap in understanding cross-script performance.
- What evidence would resolve it: Experiments comparing LLM-generated examples across languages with different scripts (e.g., Arabic, Chinese, Cyrillic) would provide insights into cross-script performance differences.

### Open Question 2
- Question: Can incorporating part-of-speech information and handling polysemous words improve the quality of LLM-generated example sentences?
- Basis in paper: [explicit] The authors note they did not include part-of-speech information and did not study words with multiple definitions, which may have affected example quality.
- Why unresolved: The study's methodology intentionally excluded these factors, leaving their impact on example quality unexplored.
- What evidence would resolve it: Comparative studies generating examples with and without part-of-speech tagging and polysemy handling would reveal their impact on quality.

### Open Question 3
- Question: How effective is automated LLM-based evaluation of example sentences compared to human evaluation, particularly for low-resource languages?
- Basis in paper: [explicit] The paper explores using pre-trained language models for automated rating but finds limited success for low-resource languages like Tetun.
- Why unresolved: While the study identifies the potential of automated metrics, it does not establish their reliability or effectiveness compared to human evaluation, especially for low-resource languages.
- What evidence would resolve it: Comprehensive evaluations comparing automated LLM ratings with human judgments across multiple languages and resource levels would determine the effectiveness of automated evaluation.

## Limitations
- High inter-annotator disagreement (particularly for French) introduces uncertainty about the reliability of human ratings as ground truth
- Study limited to Latin-script languages, leaving cross-script performance unexplored
- Automated rating methods show limited success for low-resource languages like Tetun

## Confidence
- **High confidence**: LLM performance degrades predictably with language resource level (supported by corpus statistics showing 5,000x more French than Tetun documents, and consistent 95% vs 69% typicality rates)
- **Medium confidence**: In-context learning successfully aligns LLMs with individual annotator preferences (supported by correlation coefficients of 0.29-0.54, but limited by small sample size and potential overfitting to individual annotators)
- **Medium confidence**: Sentence perplexity serves as a reliable proxy for typicality and intelligibility in higher-resource languages (supported by correlation coefficients of -0.57 to -0.41 for French/Indonesian, but not validated for Tetun)

## Next Checks
1. Replicate the study with a larger sample of annotators per language to determine if inter-annotator disagreement is consistent across different evaluator groups, or if it's specific to the initial annotator set.
2. Test additional LLM architectures beyond GPT-4o and Llama 3.1 to verify that performance degradation patterns hold across different model families and sizes.
3. Conduct ablation studies on the in-context learning approach by varying the number of examples shown to the LLM and measuring how this affects alignment accuracy with individual annotator preferences.