---
ver: rpa2
title: 'STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing'
arxiv_id: '2411.00387'
source_url: https://arxiv.org/abs/2411.00387
tags:
- mathematical
- arxiv
- symbols
- dataset
- symbol
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STEM-PoM, a comprehensive benchmark dataset
  designed to evaluate language models' reasoning abilities on mathematical symbols
  within scientific text. The dataset contains over 2,000 math symbols extracted from
  arXiv documents, classified into four main categories (variables, constants, operators,
  unit descriptors) with sub-attributes including scalar/vector/matrix for variables
  and local/global/discipline-specific labels for constants and operators.
---

# STEM-PoM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing

## Quick Facts
- arXiv ID: 2411.00387
- Source URL: https://arxiv.org/abs/2411.00387
- Authors: Jiaru Zou; Qing Wang; Pratyush Thakur; Nickvash Kani
- Reference count: 40
- Primary result: State-of-the-art language models achieve only 20-60% accuracy in classifying mathematical symbols from scientific text, with GPT-4o reaching 68.5% accuracy on first-level classification

## Executive Summary
This paper introduces STEM-PoM, a comprehensive benchmark dataset designed to evaluate language models' reasoning abilities on mathematical symbols within scientific text. The dataset contains over 2,000 math symbols extracted from arXiv documents, classified into four main categories with sub-attributes. Experiments on seven language models show significant gaps in mathematical symbol classification, with state-of-the-art models achieving only 20-60% accuracy in in-context learning and 50-60% with fine-tuning. The results highlight the challenge of extracting and categorizing mathematical symbols from large text corpora and provide insights into how different mathematical symbol attributes affect language models' understanding of math-rich documents.

## Method Summary
The STEM-PoM dataset contains 2,109 math symbols extracted from arXiv documents, classified into four main categories (variables, constants, operators, unit descriptors) with sub-attributes. The evaluation uses in-context learning on seven models (LSTM, Mixtral-8x7B, Llama2-13B, Llama3-80B, Claude-3.5-sonnet, GPT-3.5, GPT-4o) with varying context lengths (one sentence, ten sentences, full manuscript). Additionally, GPT-3.5 is fine-tuned using LoRA on the dataset. Performance is measured through first-level classification accuracy (main categories) and second-level classification accuracy (sub-attributes), with precision accuracy calculated as correct predictions divided by total samples.

## Key Results
- GPT-4o achieves highest accuracy at 68.5% on first-level classification, while other models range from 20-60%
- Smaller models like Llama2-13B show stronger improvements with longer contexts compared to larger models
- Fine-tuned GPT-3.5 achieves 67.4% accuracy with one-sentence context but performance decreases with longer contexts
- Second-level classification accuracy is significantly lower than first-level, with Matrix and DS categories showing particular difficulty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextual information is essential for distinguishing mathematical symbols with multiple meanings.
- Mechanism: Mathematical symbols exhibit contextual polymorphism - the same symbol can represent different attributes depending on surrounding text. The STEM-PoM dataset captures this by providing surrounding text context for each symbol.
- Core assumption: Mathematical symbols cannot be accurately classified without their contextual environment.
- Evidence anchors:
  - [abstract]: "Without the corresponding contextual information of a mathematical symbol, LLMs are unable to distinguish between different attributes of the symbol"
  - [section]: "In addition, integrating mathematical language into NLP models remains a substantial challenge [3, 29], especially in the realm of document parsing [8, 24, 44]."
  - [corpus]: Weak - corpus shows related work on document parsing but lacks direct evidence of context-necessity claims.

### Mechanism 2
- Claim: Larger models with more pre-trained knowledge demonstrate superior scalability with longer contexts.
- Mechanism: Larger models like GPT-4o and Claude3.5-Sonnet show consistent performance improvements with increased context length, while smaller models show diminishing returns.
- Core assumption: Pre-training knowledge enables models to better leverage extended context for distinguishing mathematical symbols.
- Evidence anchors:
  - [section]: "GPT-4o outperforms Llama3-80B by 16.0%, 14.4%, and 16.8% for context lengths of one sentence, ten sentences, and the full manuscript, respectively."
  - [section]: "Larger models like GPT-4o and Claude3.5-Sonnet, which come with extensive pre-trained knowledge, show relatively smaller performance gains as context length increases."
  - [corpus]: Weak - corpus contains related document parsing work but no direct evidence of scaling behavior with context length.

### Mechanism 3
- Claim: Fine-tuning amplifies sensitivity to noisy or irrelevant information in longer contexts.
- Mechanism: Fine-tuned GPT-3.5 shows superior performance on short contexts but experiences performance degradation as context length increases, unlike vanilla inference which improves steadily.
- Core assumption: Fine-tuning makes models more sensitive to context quality and less able to filter irrelevant information in longer sequences.
- Evidence anchors:
  - [section]: "The diminishing return for fine-tuned models with longer contexts indicates that fine-tuning amplifies sensitivity to the introduction of noisy or less relevant information"
  - [section]: "The difference in overall accuracy between fine-tuned and vanilla models is 10.6% for one-sentence contexts but only 1.6% for full manuscripts."
  - [corpus]: Weak - corpus contains document parsing benchmarks but lacks direct evidence about fine-tuning's effect on context sensitivity.

## Foundational Learning

- Concept: Mathematical symbol categorization and contextual polymorphism
  - Why needed here: Understanding that mathematical symbols have different meanings in different contexts is fundamental to grasping why STEM-PoM requires contextual information
  - Quick check question: Why does the symbol 'y' need different classification in 'y = mx + p' versus 'L(x, y) = -Σ xi log(yi)'?

- Concept: Part-of-Math tagging as an NLP task
  - Why needed here: STEM-PoM is specifically designed for Part-of-Math tagging evaluation, which is analogous to part-of-speech tagging but for mathematical expressions
  - Quick check question: How does Part-of-Math tagging differ from traditional semantic parsing methods like LateXML?

- Concept: Language model scaling laws and context window utilization
  - Why needed here: The experiments demonstrate how model size affects performance with varying context lengths, which is central to interpreting the results
  - Quick check question: Why do larger models like GPT-4o show smaller performance gains from increasing context length compared to smaller models like Llama2-13B?

## Architecture Onboarding

- Component map: Data extraction pipeline -> Context capture -> Manual annotation -> Model evaluation -> Performance analysis
- Critical path: Symbol extraction → Context capture → Manual annotation → Model evaluation → Performance analysis
- Design tradeoffs: Comprehensive context vs. dataset size (2,109 instances is relatively small but highly contextualized)
- Failure signatures: Low accuracy on specific symbol types (Matrix, DS classification), performance degradation with fine-tuning on longer contexts
- First 3 experiments:
  1. Evaluate model performance on first-level classification with one-sentence context only
  2. Compare fine-tuned vs. vanilla inference performance across different context lengths
  3. Test second-level classification accuracy assuming first-level classification is correct

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of language models on mathematical symbol classification scale with increasingly longer context lengths beyond the full manuscript (e.g., 20 sentences, 50 sentences, or full-document level)?
- Basis in paper: [explicit] The paper notes that "the overall performance gain from increasing context length is more pronounced in smaller models" and that larger models like GPT-4o show "relatively smaller performance gains as context length increases," suggesting potential saturation effects.
- Why unresolved: The paper only tests up to full manuscript length (ten sentences for LSTM, unspecified for others), leaving open whether performance plateaus or continues improving with even longer contexts.
- What evidence would resolve it: Experimental results showing classification accuracy on mathematical symbols using progressively longer contexts (e.g., 20, 50 sentences, or full-document) across multiple model sizes would reveal whether there's a context length threshold beyond which additional context no longer improves or potentially degrades performance.

### Open Question 2
- Question: What specific characteristics of mathematical symbols (e.g., frequency of appearance, syntactic role, domain specificity) most strongly predict successful classification by language models?
- Basis in paper: [inferred] The paper observes that "symbols with more distinct contextual or syntactical patterns are easier for models to classify" and that "certain structures or content types within manuscripts remain difficult to categorize," but doesn't identify specific predictive characteristics.
- Why unresolved: While the paper provides general observations about classification difficulty, it doesn't conduct a detailed feature importance analysis to determine which symbol characteristics drive classification success or failure.
- What evidence would resolve it: A feature importance analysis or ablation study that systematically varies symbol characteristics (frequency, syntactic role, domain specificity, etc.) and measures their impact on classification accuracy would identify the key predictors of model performance.

### Open Question 3
- Question: How do different fine-tuning strategies (e.g., LoRA, full fine-tuning, prefix-tuning) compare in their effectiveness for mathematical symbol classification across varying context lengths?
- Basis in paper: [explicit] The paper only applies LoRA fine-tuning to GPT-3.5 and observes that "the fine-tuned GPT-3.5 model achieves an accuracy of 67.4% in the one-sentence context" but performance decreases with longer contexts, suggesting context-length sensitivity in the fine-tuning approach.
- Why unresolved: The paper doesn't compare LoRA with other fine-tuning methods or investigate whether different strategies handle context length differently.
- What evidence would resolve it: Comparative experiments applying multiple fine-tuning strategies (LoRA, full fine-tuning, prefix-tuning, etc.) to the same models and measuring their performance across different context lengths would reveal which approaches are most robust to context length variations and most effective overall for mathematical symbol classification.

## Limitations

- The dataset size of 2,109 instances is relatively small for comprehensive machine learning evaluation
- Proprietary models (GPT-4o, Claude-3.5-sonnet) cannot be fully examined to understand performance differences
- Second-level classification accuracy is significantly lower than first-level, with limited error analysis for specific symbol types

## Confidence

- High confidence in the core finding that context is essential for mathematical symbol classification
- Medium confidence in explanations for why larger models show smaller performance gains with longer contexts
- Medium confidence in the claim that fine-tuning amplifies sensitivity to noisy contexts

## Next Checks

1. **Dataset size scaling experiment**: Validate whether the observed performance trends hold with a significantly larger dataset (e.g., 10x or 100x the current size) to assess if the small dataset size influenced the results, particularly the observed performance gaps between models.

2. **Ablation study on context extraction**: Systematically vary the quality and relevance of surrounding text for each symbol to determine the minimum effective context window and test whether the diminishing returns for larger models are due to context saturation or other factors.

3. **Fine-tuning methodology comparison**: Replicate the experiments using different fine-tuning approaches (full fine-tuning, different PEFT methods, curriculum learning) to isolate whether the sensitivity to noisy contexts is specific to LoRA or a general property of fine-tuning mathematical symbol classifiers.