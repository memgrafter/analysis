---
ver: rpa2
title: Discrete Cosine Transform Based Decorrelated Attention for Vision Transformers
arxiv_id: '2405.13901'
source_url: https://arxiv.org/abs/2405.13901
tags:
- attention
- linear
- initialization
- vision
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of inefficient initialization
  and high computational overhead in Vision Transformers (ViTs) by introducing two
  DCT-based methods: a novel DCT-based initialization approach and a DCT-based compression
  technique for the attention mechanism. The DCT-based initialization improves ViT
  accuracy by initializing query, key, or value weight matrices with DCT matrices,
  which cover the entire frequency spectrum.'
---

# Discrete Cosine Transform Based Decorrelated Attention for Vision Transformers

## Quick Facts
- arXiv ID: 2405.13901
- Source URL: https://arxiv.org/abs/2405.13901
- Reference count: 40
- Key outcome: DCT-based methods improve ViT accuracy by 0.7-2.1% and reduce parameters by 13.1% while maintaining accuracy on ImageNet-1K

## Executive Summary
This paper introduces two novel Discrete Cosine Transform (DCT) based methods to address initialization and computational efficiency challenges in Vision Transformers. The first method uses DCT matrices to initialize query, key, or value weight matrices, providing a structured starting point that covers the full frequency spectrum. The second method applies DCT to input patches, truncates high-frequency components, and uses the compressed representation to reduce attention matrix dimensions. Both approaches leverage DCT's property of decorrelating correlated image data in the frequency domain.

## Method Summary
The paper proposes two DCT-based techniques for Vision Transformers. First, DCT-based initialization replaces one of the query/key/value weight matrices with a DCT matrix, providing a structured starting point that covers the entire frequency spectrum rather than random initialization. Second, DCT-based compression transforms input patches using DCT, truncates high-frequency components (which typically contain noise), applies zero-padding and inverse DCT, then computes attention with reduced dimensionality. The compression ratio τ controls the trade-off between efficiency and accuracy. Experiments use Swin Transformer architectures on CIFAR-10 and ImageNet-1K datasets.

## Key Results
- DCT-based initialization improves accuracy by 0.7-2.1% across various Vision Transformer architectures
- Swin-T with 75% DCT-compressed attention achieves 81.474% accuracy on ImageNet-1K with 13.1% fewer parameters and 6.9% fewer FLOPs
- On CIFAR-10, Swin-T with DCT initialization and compression (τ=0.5) achieves 97.884% accuracy with 28.2% fewer parameters
- The methods are particularly effective for small-scale datasets where proper initialization is critical

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DCT-based initialization improves Vision Transformer performance by providing a structured, spectrum-aware starting point for weight matrices.
- Mechanism: Initializing query, key, or value weight matrices with DCT matrices covers the full frequency spectrum from the start, avoiding the random white-noise initialization that limits initial bandwidth.
- Core assumption: DCT basis vectors approximate the Karhunen-Loeve Transform (KLT) for correlated input data, providing a decorrelated, efficient feature extraction basis.
- Evidence anchors:
  - [abstract] "Our proposed DCT-based attention initialization marks a significant gain compared to traditional initialization strategies"
  - [section] "In traditional random initialization, all the weight matrices are initialized as random numbers that are independently generated. As a result, the initial matrices can be considered as a realization of white noise covering the entire spectrum"
  - [corpus] Weak evidence - related papers focus on DCT in different contexts (activation functions, compression) but not initialization for ViTs
- Break condition: If input patches are not sufficiently correlated for DCT basis vectors to approximate KLT, or if symmetry-breaking issues arise from initializing multiple matrices with DCT

### Mechanism 2
- Claim: DCT-based compression reduces computational overhead while maintaining accuracy by exploiting frequency-domain decorrelation and noise characteristics.
- Mechanism: Truncating high-frequency DCT components of input patches reduces weight matrix dimensions since high frequencies correspond to noise and carry less perceptual information.
- Core assumption: Attention weight matrices are inter-correlated due to similar gradient vectors during training, making them amenable to DCT-based decorrelation and compression.
- Evidence anchors:
  - [abstract] "We also recognize that since DCT effectively decorrelates image information in the frequency domain, this decorrelation is useful for compression"
  - [section] "When the input data is highly correlated, the covariance matrix can be approximated by a Toeplitz matrix... Fig. 2 shows that DCT basis vectors approximate the KLT of a first-order Markov process"
  - [corpus] Weak evidence - related papers focus on DCT for compression but not specifically for attention mechanism compression in ViTs
- Break condition: If high-frequency components contain critical information for the task, or if the correlation assumption doesn't hold for the specific dataset or architecture

### Mechanism 3
- Claim: DCT-based methods improve training dynamics on small datasets by providing better starting points and reducing parameter redundancy.
- Mechanism: The structured DCT initialization provides a more stable starting point for training, particularly beneficial when data is limited, while DCT compression reduces the effective parameter count without sacrificing representational capacity.
- Core assumption: Small-scale datasets exacerbate the challenges of random initialization, making structured approaches more beneficial.
- Evidence anchors:
  - [section] "Training vision transformers from scratch on the CIFAR-10 dataset is notoriously difficult. We demonstrate the benefits of using our DCT initialization and DCT compression for the Swin Transformer Tiny (Swin-T) on from-scratch CIFAR-10 training"
  - [section] "Proper initialization of these matrices ensures that the model starts from a stable state, facilitating effective learning"
  - [corpus] No direct evidence - corpus papers don't address small dataset training with DCT-based methods
- Break condition: If dataset is sufficiently large that initialization effects are negligible, or if compression removes too much information for the task complexity

## Foundational Learning

- Concept: Discrete Cosine Transform (DCT) and its properties
  - Why needed here: DCT is the core mathematical tool used for both initialization and compression in the proposed methods
  - Quick check question: What property of DCT makes it particularly suitable for image compression compared to Fourier transform?

- Concept: Vision Transformer architecture and self-attention mechanism
  - Why needed here: Understanding how queries, keys, and values are computed and used in self-attention is crucial for implementing the DCT-based modifications
  - Quick check question: In the self-attention mechanism, what mathematical operation combines queries and keys to determine attention weights?

- Concept: Frequency domain analysis and decorrelation
  - Why needed here: The methods rely on the principle that DCT decorrelates correlated data, which is fundamental to both the initialization and compression approaches
  - Quick check question: How does the DCT transform relate to the Karhunen-Loève Transform (KLT) for correlated data?

## Architecture Onboarding

- Component map:
  Input: Image patches (N × M² × C) -> DCT transformation and truncation (for compression) -> Query/Key/Value computation (with DCT initialization if applicable) -> Scaled dot-product attention with relative position bias -> Concatenation and linear projection -> IDCT (for compression) -> Output: Attention output (N × M² × C)

- Critical path:
  1. Image patch extraction and flattening
  2. DCT transformation and truncation (for compression)
  3. Query/Key/Value computation (with DCT initialization if applicable)
  4. Scaled dot-product attention with relative position bias
  5. Concatenation and linear projection
  6. IDCT (for compression) and final linear layer

- Design tradeoffs:
  - Single vs. multiple DCT initialization: Initializing multiple matrices causes symmetry-breaking issues; single initialization provides benefits without these issues
  - Compression ratio (τ): Higher compression reduces parameters and FLOPs but may impact accuracy; 75% retention provides good balance
  - Non-trainable vs. trainable DCT weights: Non-trainable weights reduce parameters but may limit adaptability

- Failure signatures:
  - Accuracy degradation with high compression ratios (τ < 0.5)
  - Training instability or convergence issues with DCT initialization
  - Performance worse than baseline on large datasets where initialization effects are negligible

- First 3 experiments:
  1. Implement DCT-based initialization with WK initialized as DCT matrix on CIFAR-10, compare accuracy to baseline
  2. Implement DCT-based compression with τ = 0.75 on ImageNet-1K, measure parameter reduction and accuracy retention
  3. Compare training curves of DCT-initialized vs. randomly-initialized models on small dataset to demonstrate training stability benefits

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions.

## Limitations
- The paper lacks ablation studies showing which component (queries, keys, or values) benefits most from DCT initialization
- Compression ratio tradeoffs are not fully explored - the 75% threshold appears empirically chosen without systematic analysis
- The correlation assumption for input patches may not hold for all vision tasks or datasets

## Confidence
- **High Confidence**: DCT-based compression mechanism and its mathematical justification
- **Medium Confidence**: DCT initialization benefits, as results are promising but limited ablation exists
- **Low Confidence**: Generalization claims across different vision architectures beyond Swin Transformers

## Next Checks
1. Perform systematic ablation studies testing DCT initialization on each component (queries, keys, values) separately to identify which provides maximum benefit
2. Analyze the frequency spectrum of attention matrices across different layers to validate the correlation assumption and determine optimal compression ratios
3. Test the DCT-based methods on additional transformer architectures (DeiT, ConvNeXt) and diverse tasks (segmentation, detection) to assess generalizability