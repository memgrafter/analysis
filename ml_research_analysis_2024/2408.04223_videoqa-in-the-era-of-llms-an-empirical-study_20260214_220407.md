---
ver: rpa2
title: 'VideoQA in the Era of LLMs: An Empirical Study'
arxiv_id: '2408.04223'
source_url: https://arxiv.org/abs/2408.04223
tags:
- video
- question
- questions
- arxiv
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates the robustness and generalization
  capabilities of video-language models (Video-LLMs) on VideoQA tasks. Through a comprehensive
  suite of adversarial probes targeting temporal understanding, visual grounding,
  multimodal reasoning, robustness, and generalization, the research reveals that
  while Video-LLMs excel in standard VideoQA performance, they exhibit significant
  weaknesses in handling temporal reasoning, visual grounding, and language variations.
---

# VideoQA in the Era of LLMs: An Empirical Study

## Quick Facts
- arXiv ID: 2408.04223
- Source URL: https://arxiv.org/abs/2408.04223
- Authors: Junbin Xiao, Nanxin Huang, Hangyu Qin, Dongyang Li, Yicong Li, Fengbin Zhu, Zhulin Tao, Jianxing Yu, Liang Lin, Tat-Seng Chua, Angela Yao
- Reference count: 22
- Primary result: Video-LLMs excel in standard VideoQA but fail at temporal reasoning, visual grounding, and handling language variations

## Executive Summary
This study systematically evaluates the robustness and generalization capabilities of video-language models (Video-LLMs) on VideoQA tasks. Through a comprehensive suite of adversarial probes targeting temporal understanding, visual grounding, multimodal reasoning, robustness, and generalization, the research reveals that while Video-LLMs excel in standard VideoQA performance, they exhibit significant weaknesses in handling temporal reasoning, visual grounding, and language variations. The models show surprising insensitivity to video perturbations like frame shuffling but high sensitivity to question rephrasing and candidate answer modifications. Additionally, fine-tuned Video-LLMs demonstrate limited improvement in generalization to long-tailed distributions and out-of-distribution data compared to non-LLM methods. These findings highlight the urgent need for developing more robust and interpretable Video-LLMs with better temporal reasoning capabilities and improved resistance to language variations.

## Method Summary
The study evaluates 7 Video-LLM models (FrozenBiLM, FrozenGQA, SeViLA, LLaMA-VQA, VideoChat2, LLoVi, GPT-4o) and 3 non-LLM baselines on NExT-QA and related datasets. Models are tested using adversarial probes including Temporal Exchange/Description edits, video-answer and question-answer short-cuts, frame shuffling, and question variations. Accuracy and flip rates are measured before and after modifications, with GPT-4 used for data curation and human verification. Frame sampling is set at 3fps with 32 frames per video for GPT-4o evaluations.

## Key Results
- Video-LLMs show high sensitivity to question rephrasing and candidate answer modifications but remain unresponsive to frame shuffling and other video perturbations
- Models demonstrate poor visual grounding capabilities, relying on language priors rather than video content for predictions
- Fine-tuned Video-LLMs show limited improvement in generalization to long-tailed distributions and out-of-distribution data compared to non-LLM methods

## Why This Works (Mechanism)

### Mechanism 1
- Video-LLMs leverage frozen LLMs with frozen visual encoders, avoiding full fine-tuning and enabling zero-shot or few-shot capabilities
- Core assumption: The frozen LLM already encodes sufficient linguistic and world knowledge to generalize across tasks without fine-tuning
- Break condition: If the LLM lacks coverage of domain-specific concepts or visual grounding knowledge, zero-shot performance degrades sharply

### Mechanism 2
- Video-LLMs outperform non-LLM methods on standard VideoQA due to stronger language priors and spurious vision-text correlations rather than visual grounding
- Core assumption: Candidate answer sets contain exploitable shortcuts (e.g., only one answer contains video objects or question keywords)
- Break condition: If candidate answers are balanced (no shortcuts), Video-LLM advantage diminishes

### Mechanism 3
- Video-LLMs generalize better across question types and datasets than non-LLM methods due to implicit world knowledge encoded in LLMs
- Core assumption: Pretrained LLM knowledge transfers to video domain reasoning tasks
- Break condition: If transferred domain is too far from pretraining distribution, generalization fails

## Foundational Learning

- Concept: Visual grounding in VideoQA
  - Why needed here: The study shows Video-LLMs rely on language priors over actual visual content, making grounding a core failure mode
  - Quick check question: Can you explain why PosVQA vs NegVQA accuracy differs so little for Video-LLMs?

- Concept: Temporal reasoning in video
  - Why needed here: The probes reveal models struggle with content order despite high accuracy on temporal questions
  - Quick check question: Why do TE/TD edits cause high flip rates even when video content is unchanged?

- Concept: Multimodal bias exploitation
  - Why needed here: The study shows models exploit QA/VA shortcuts rather than multimodal reasoning
  - Quick check question: How would you construct a balanced answer set to prevent shortcut learning?

## Architecture Onboarding

- Component map: Video frames → CLIP-ViT encoder → visual tokens → Frozen LLM backbone (DeBERTa-V2/Flan T5/LLaMA/Vicuna) → Adaptation layer → Output head
- Critical path: Video frame encoding → LLM prompt construction → answer generation/decoding
- Design tradeoffs:
  - Frozen vs fine-tuned: Frozen saves compute but limits task adaptation; fine-tuned improves performance but risks overfitting
  - Frame sampling strategy: Uniform vs adaptive sampling affects temporal coverage and computational cost
  - LLM size: Larger LLMs improve accuracy but increase latency and resource needs
- Failure signatures:
  - High flip rates on adversarial edits → sensitivity to language variations
  - Marginal accuracy drop on shuffled frames → lack of temporal reasoning
  - Low grounding accuracy → reliance on language priors
- First 3 experiments:
  1. Run NormalVQA vs BlindQA to quantify language bias exploitation
  2. Test PosVQA vs NegVQA to measure grounding capability
  3. Apply TE/TD edits to assess temporal reasoning robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Video-LLMs be made more robust to language variations in questions while maintaining high accuracy?
- Basis in paper: Explicit - The paper identifies that Video-LLMs are highly sensitive to question rephrasing and spoken phrases, yet unresponsive to video perturbations like frame shuffling
- Why unresolved: Current Video-LLMs rely heavily on language priors and spurious vision-text correlations, making them brittle to linguistic variations
- What evidence would resolve it: Developing and testing Video-LLMs that maintain consistent performance across diverse question formulations and adversarial language perturbations

### Open Question 2
- Question: What architectural improvements are needed for Video-LLMs to better understand video temporality and content ordering?
- Basis in paper: Explicit - The study reveals that Video-LLMs struggle with reasoning about temporal content ordering and grounding QA-relevant temporal moments, despite high accuracy on standard temporal questions
- Why unresolved: End-to-end learning approaches, even with LLMs, may not effectively capture temporal relations, and current position embeddings do not fully reflect ordering information
- What evidence would resolve it: Creating Video-LLMs that accurately answer temporal questions involving content order and demonstrate improved performance on adversarial temporal probes

### Open Question 3
- Question: How can Video-LLMs be developed to provide more visually grounded answers and reduce reliance on language priors?
- Basis in paper: Explicit - The research shows that Video-LLMs achieve high accuracy by exploiting language biases and spurious vision-text correlations, rather than grounding predictions on relevant video content
- Why unresolved: Current Video-LLMs prioritize language priors over visual evidence, leading to poor visual grounding despite good overall performance
- What evidence would resolve it: Developing Video-LLMs that maintain or improve accuracy while significantly increasing the proportion of visually grounded answers, as measured by joint answer localization tasks

## Limitations

- Findings about Video-LLM weaknesses may be specific to controlled adversarial probes rather than reflecting real-world usage patterns
- Comparison between frozen and fine-tuned Video-LLMs lacks analysis of why generalization improvements remain limited despite fine-tuning
- High sensitivity to question rephrasing and answer modifications could be dataset-specific rather than fundamental model limitations

## Confidence

- High confidence: Temporal reasoning weaknesses (TE/TD edits show consistent high flip rates across models)
- Medium confidence: Language variation sensitivity (results depend heavily on specific probe implementations)
- Low confidence: Generalization comparisons (limited sample sizes for long-tailed and OOD evaluations)

## Next Checks

1. Test Video-LLM performance on a held-out dataset with balanced answer options to verify if shortcut exploitation is dataset-specific rather than model-specific

2. Implement cross-dataset validation by training on NExT-QA and testing on MSVD-QA/ActivityNet-QA to measure true generalization capabilities beyond the reported probe results

3. Conduct ablation studies on frame sampling rates and temporal modeling components to quantify their impact on temporal reasoning performance and determine if current architectures are fundamentally limited or require architectural modifications