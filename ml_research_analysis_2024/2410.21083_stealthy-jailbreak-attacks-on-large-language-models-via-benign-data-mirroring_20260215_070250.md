---
ver: rpa2
title: Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring
arxiv_id: '2410.21083'
source_url: https://arxiv.org/abs/2410.21083
tags:
- attack
- target
- jailbreak
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ShadowBreak, a stealthy jailbreak attack
  method that uses benign data mirroring to locally train a mirror model, enabling
  high success rates with minimal detectable queries. By aligning the mirror model
  with benign data from the target, the approach improves transferability of adversarial
  prompts while avoiding malicious queries during the search phase.
---

# Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring

## Quick Facts
- arXiv ID: 2410.21083
- Source URL: https://arxiv.org/abs/2410.21083
- Reference count: 21
- This paper introduces ShadowBreak, a stealthy jailbreak attack method that uses benign data mirroring to locally train a mirror model, enabling high success rates with minimal detectable queries.

## Executive Summary
This paper presents ShadowBreak, a novel stealthy jailbreak attack method that leverages benign data mirroring to improve both attack success rates and stealth. By fine-tuning a local mirror model using benign responses from the target LLM, ShadowBreak conducts adversarial prompt searches locally rather than directly querying the target with malicious instructions. This approach achieves up to 92% attack success rate while averaging only 1.5 detectable jailbreak queries per sample, significantly outperforming baseline methods in both effectiveness and stealth.

## Method Summary
ShadowBreak operates through a two-phase approach: first, it collects benign instructions and queries the target model to gather responses for alignment data. This data is used to fine-tune a local mirror model, creating an approximation of the target's behavior. The second phase involves running adversarial prompt generation algorithms (GCG or AutoDAN) on the mirror model to identify effective jailbreak prompts. Only the most promising prompts are then submitted to the target model, minimizing detectable malicious queries. The method is evaluated against baselines including Direct Query, GCG transfer, AutoDAN transfer, PAIR, and PAL across datasets like Alpaca Small, Safety-tuned Llama, AdvBench, and StrongReject using models like GPT-3.5 Turbo and GPT-4o mini.

## Key Results
- Achieves maximum attack success rate of 92% or balanced value of 80% with average of 1.5 detectable jailbreak queries per sample
- Improves transfer attack performance by 48%-92% compared to naïve transfer attacks
- Demonstrates enhanced stealth by not involving identifiable malicious instructions during the search phase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ShadowBreak reduces detection risk by conducting adversarial prompt searches on a locally fine-tuned mirror model rather than directly querying the target model with malicious instructions.
- Mechanism: The method first queries the target model with benign instructions, uses its responses to align a local mirror model, then performs adversarial search on this aligned model before transferring the results to the target.
- Core assumption: The local mirror model sufficiently approximates the target model's behavior in both general and safety-relevant domains, enabling effective transfer of adversarial prompts.
- Evidence anchors:
  - [abstract]: "This method offers enhanced stealth, as it does not involve submitting identifiable malicious instructions to the target model during the search phase."
  - [section]: "Unlike mainstream black-box attack methods that repeatedly probe the target model with malicious instructions, ShadowBreak reduces detection risk by conducting searches on a local mirror model."
  - [corpus]: Weak evidence - the corpus contains related work on stealthy jailbreak methods but none specifically using benign data mirroring for alignment.

### Mechanism 2
- Claim: Using benign data for mirror model alignment improves transfer attack success rates compared to direct transfer or using malicious data for alignment.
- Mechanism: By fine-tuning the local model on benign responses from the target model, the mirror model learns to mimic the target's behavior across diverse tasks while maintaining its safety alignment, making adversarial prompts more transferable.
- Core assumption: Alignment with benign data captures the target model's general behavior patterns sufficiently to enable effective adversarial prompt transfer without requiring malicious queries during search.
- Evidence anchors:
  - [abstract]: "By using purely benign data, we improve transfer attack performance by 48%-92% compared to naïve transfer attacks."
  - [section]: "Using benign data proved crucial, covering the most harmful categories."
  - [corpus]: Weak evidence - the corpus shows related work on transfer attacks but doesn't specifically address the impact of using benign versus malicious alignment data.

### Mechanism 3
- Claim: The method achieves high attack success rates while minimizing detectable queries by only submitting the final optimized adversarial prompts to the target model.
- Mechanism: The local optimization phase using GCG or AutoDAN generates multiple candidate prompts, which are tested against the mirror model. Only the most effective prompts are then submitted to the target model, reducing the number of detectable malicious queries.
- Core assumption: Local optimization can effectively identify successful adversarial prompts without requiring iterative feedback from the target model, thus avoiding detection during the search phase.
- Evidence anchors:
  - [abstract]: "achieved a maximum attack success rate of 92%, or a balanced value of 80% with an average of 1.5 detectable jailbreak queries per sample"
  - [section]: "By deploying only the most promising prompts, we minimize detectable queries, enhancing overall attack stealth."
  - [corpus]: Weak evidence - the corpus contains related work on black-box attacks but lacks specific data on query efficiency or stealth metrics.

## Foundational Learning

- Concept: Transfer learning and model alignment
  - Why needed here: The method relies on transferring adversarial prompts from a locally aligned model to the target model, requiring understanding of how model behaviors can be approximated and transferred.
  - Quick check question: How does fine-tuning a model on benign data from a target model help approximate the target's behavior for adversarial prompt transfer?

- Concept: Adversarial example generation in discrete spaces
  - Why needed here: The method uses GCG and AutoDAN to generate adversarial prompts, which requires understanding of how to optimize discrete sequences like text.
  - Quick check question: What makes generating adversarial prompts in text space different from continuous optimization problems?

- Concept: Safety alignment and content filtering mechanisms
  - Why needed here: Understanding how LLMs are safety-aligned and how content filters work is crucial for developing effective jailbreak attacks and defenses.
  - Quick check question: How do typical LLM safety mechanisms detect and block potentially harmful content?

## Architecture Onboarding

- Component map: Benign query -> Target response -> Mirror model training -> Adversarial search -> Prompt transfer -> Success evaluation

- Critical path: Benign query → Target response → Mirror model training → Adversarial search → Prompt transfer → Success evaluation

- Design tradeoffs:
  - Alignment data quantity vs. quality: More data improves alignment but increases computational cost
  - Local model capacity vs. efficiency: Larger models may better approximate target behavior but require more resources
  - Adversarial search depth vs. stealth: Deeper searches may find better prompts but risk detection

- Failure signatures:
  - Low transfer success rates indicate poor mirror model alignment
  - High detectable query counts suggest ineffective local optimization
  - Inconsistent results across different instruction categories may indicate safety alignment gaps

- First 3 experiments:
  1. Validate mirror model alignment by comparing outputs on shared benign instructions
  2. Test adversarial prompt transferability from mirror to target model
  3. Measure detectable query reduction compared to baseline black-box attacks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of benign data size affect the balance between attack success rate and stealth in ShadowBreak?
- Basis in paper: [explicit] The paper mentions using 1,000 and 20,000 sample subsets of the Alpaca dataset for alignment, with different results in attack success rates.
- Why unresolved: The paper does not provide a detailed analysis of how varying the size of benign data impacts the trade-off between attack success rate and stealth.
- What evidence would resolve it: A systematic study varying the size of benign data and measuring corresponding attack success rates and stealth metrics would provide insights into this trade-off.

### Open Question 2
- Question: What are the theoretical underpinnings that explain why aligning with benign data improves the transferability of adversarial prompts?
- Basis in paper: [inferred] The paper suggests that aligning the mirror model with benign data enhances the transferability of adversarial prompts, but does not provide a theoretical explanation for this phenomenon.
- Why unresolved: The paper relies on empirical evidence to demonstrate the effectiveness of benign data alignment without delving into the theoretical reasons behind it.
- What evidence would resolve it: A theoretical analysis or mathematical model explaining the relationship between benign data alignment and adversarial prompt transferability would address this question.

### Open Question 3
- Question: How does ShadowBreak perform against more recent and advanced language models compared to GPT-3.5 Turbo and GPT-4o mini?
- Basis in paper: [explicit] The paper evaluates ShadowBreak on GPT-3.5 Turbo and GPT-4o mini, but does not explore its effectiveness against newer models.
- Why unresolved: The paper focuses on a specific set of target models and does not investigate the generalizability of ShadowBreak to other or more recent models.
- What evidence would resolve it: Testing ShadowBreak on a range of newer and more advanced language models would provide insights into its broader applicability and effectiveness.

## Limitations
- The method's effectiveness appears highly dependent on the quality of benign data mirroring, which may not hold for models with more sophisticated or dynamic safety mechanisms.
- Computational overhead of maintaining and fine-tuning local mirror models could limit practical deployment in resource-constrained scenarios.
- The long-term robustness of this approach against evolving safety mechanisms remains unclear.

## Confidence
- **High Confidence**: The core mechanism of using benign data to align a local mirror model is technically sound and well-demonstrated through controlled experiments.
- **Medium Confidence**: The transferability claims are supported by experimental data but may not generalize to all LLM architectures or safety configurations.
- **Low Confidence**: The long-term robustness of this approach against evolving safety mechanisms remains unclear.

## Next Checks
1. Evaluate ShadowBreak's performance against a diverse set of LLM architectures beyond GPT-3.5 Turbo, including open-source models with varying safety alignments.
2. Test the approach against adaptive safety mechanisms that can detect benign data mirroring patterns, measuring how quickly the method's effectiveness degrades under active defense.
3. Quantify the computational overhead of maintaining mirror models across multiple target models and measure the break-even point where local optimization costs outweigh stealth benefits.