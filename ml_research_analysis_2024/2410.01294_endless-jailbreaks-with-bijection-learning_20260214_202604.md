---
ver: rpa2
title: Endless Jailbreaks with Bijection Learning
arxiv_id: '2410.01294'
source_url: https://arxiv.org/abs/2410.01294
tags:
- bijection
- attack
- learning
- arxiv
- jailbreaks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces bijection learning, an automated black-box
  jailbreak attack that teaches language models arbitrary bijective encodings to bypass
  safety mechanisms. The method uses in-context learning to teach custom encodings,
  passes encoded harmful queries to models, and decodes the responses.
---

# Endless Jailbreaks with Bijection Learning

## Quick Facts
- arXiv ID: 2410.01294
- Source URL: https://arxiv.org/abs/2410.01294
- Reference count: 40
- Primary result: Automated bijective encoding jailbreak achieves 86.3% success rate on frontier models

## Executive Summary
This paper presents bijection learning, a novel automated black-box jailbreak attack that exploits language models' ability to learn arbitrary bijective encodings through in-context learning. The attack teaches models custom encodings, uses these to bypass safety mechanisms by encoding harmful queries, and decodes the responses. Tested on frontier models including Claude 3.5 Sonnet and GPT-4o, the method achieves state-of-the-art success rates and demonstrates that larger models with advanced reasoning capabilities are more vulnerable to complex encodings.

## Method Summary
Bijection learning is an automated black-box jailbreak attack that teaches language models arbitrary bijective encodings to bypass safety mechanisms. The method uses in-context learning to teach custom encodings to models, passes encoded harmful queries to the models, and decodes the responses to extract harmful outputs. The attack is universal and scale-agnostic, showing that model capability directly correlates with vulnerability to complex encodings.

## Key Results
- Achieves state-of-the-art success rates up to 86.3% on HarmBench
- Larger models produce more severe harmful outputs compared to smaller models
- Model vulnerability scales with encoding complexity - smaller models fail to learn complex encodings

## Why This Works (Mechanism)
The attack exploits language models' strong in-context learning capabilities to teach them arbitrary bijective encodings. By presenting the model with encoding rules and examples in the prompt, the model learns to interpret encoded inputs correctly. Since the safety mechanisms are applied before the model processes the query, encoding the harmful content bypasses these filters. The model's advanced reasoning capabilities make it particularly susceptible to complex encodings that smaller models cannot learn effectively.

## Foundational Learning
- **Bijective encodings**: One-to-one mappings between original and encoded text - needed for reversible transformations that preserve meaning while evading detection
- **In-context learning**: Teaching models new behaviors through prompt examples - needed to dynamically teach encoding rules without fine-tuning
- **Safety alignment mechanisms**: Filters and constraints preventing harmful outputs - needed as the target to bypass
- **Scale-dependent capabilities**: Larger models handling more complex reasoning tasks - needed to understand why bigger models are more vulnerable
- **Token-level processing**: How models tokenize and process text - needed to understand encoding effectiveness
- **Encoding complexity metrics**: Measures of transformation difficulty - needed to quantify vulnerability relationships

## Architecture Onboarding
**Component map**: Encoding generator -> In-context teaching prompt -> Model input -> Safety filter -> Model output -> Decoding mechanism

**Critical path**: Bijective encoding creation → In-context teaching → Encoded query submission → Response decoding → Harm extraction

**Design tradeoffs**: 
- Complex encodings increase bypass success but require more tokens and model capacity
- Automated encoding generation enables scalability but may produce suboptimal encodings
- Black-box approach enables universal application but limits optimization opportunities

**Failure signatures**: 
- Models failing to learn encodings produce garbled or nonsensical outputs
- Safety filters catching encoded attempts return refusal messages
- Inconsistent decoding indicates encoding complexity exceeds model capabilities

**First experiments**:
1. Test simple substitution ciphers on various model sizes to establish baseline vulnerability
2. Measure token overhead and cost implications of encoding/decoding at scale
3. Evaluate cross-model transferability of learned encodings

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important considerations about the relationship between model capability, encoding complexity, and safety vulnerability that warrant further investigation.

## Limitations
- Limited testing across different safety fine-tuning methods and model architectures
- Encoding complexity scale lacks theoretical grounding and comprehensive validation
- Practical deployment costs and token efficiency at scale not fully explored

## Confidence
High confidence in core finding that larger models are more susceptible to complex bijective encodings, supported by systematic testing across model sizes. Medium confidence in the universality claim, as testing was primarily conducted on a limited set of frontier models from major providers. Medium confidence in the safety implications, given that the evaluation framework focuses on success rate metrics rather than comprehensive assessment of harm severity or real-world impact.

## Next Checks
1. Test the attack across models with different safety fine-tuning approaches (RLHF, constitutional AI, etc.) to assess vulnerability variation
2. Evaluate token efficiency and practical costs of the attack at scale by measuring encoding/decoding overhead
3. Conduct adversarial robustness testing where defenders attempt to detect or block encoded inputs