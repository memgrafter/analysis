---
ver: rpa2
title: 'tinyBenchmarks: evaluating LLMs with fewer examples'
arxiv_id: '2402.14992'
source_url: https://arxiv.org/abs/2402.14992
tags:
- examples
- llms
- evaluation
- performance
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates reducing the number of examples needed to
  accurately estimate LLM performance on key benchmarks like MMLU, HELM, and Open
  LLM Leaderboard. The authors propose using item response theory (IRT) to represent
  examples by their difficulty and required abilities, enabling more efficient evaluation.
---

# tinyBenchmarks: evaluating LLMs with fewer examples

## Quick Facts
- arXiv ID: 2402.14992
- Source URL: https://arxiv.org/abs/2402.14992
- Reference count: 40
- Primary result: Reduce LLM evaluation examples from thousands to ~100 per scenario while maintaining ~2% error accuracy using IRT-based methods

## Executive Summary
This paper addresses the computational and financial burden of evaluating large language models (LLMs) on extensive benchmarks. The authors propose using item response theory (IRT) to represent examples by their difficulty and required abilities, enabling efficient evaluation with fewer examples. They develop methods to select representative anchor examples and use IRT to improve performance estimates from limited data. Experiments show that 100 curated examples per scenario suffice to estimate LLM performance within ~2% error on average, significantly reducing evaluation costs.

## Method Summary
The method involves three key strategies: (1) stratified random sampling to ensure representation across sub-scenarios, (2) clustering based on model correctness data to identify anchor points, and (3) using IRT to cluster examples based on their latent ability and difficulty parameters. The authors develop IRT-based tools including the p-IRT estimator (using IRT predictions) and gp-IRT estimator (a convex combination of raw estimates and IRT predictions). They release tiny benchmark versions with 100 curated examples per scenario and provide tools for efficient evaluation of future LLMs.

## Key Results
- 100 curated examples per scenario suffice to estimate LLM performance within ~2% error on average
- Reduces evaluation costs significantly (e.g., from 14K to 100 examples for MMLU)
- IRT-based methods remain robust for specialized models while correctness-based methods deteriorate
- gp-IRT estimator provides balanced performance between raw estimates and IRT predictions

## Why This Works (Mechanism)

### Mechanism 1
Using IRT-based anchor points reduces the number of evaluation examples needed by exploiting correlations between example difficulty and model performance. IRT models each example with parameters (difficulty, ability requirements) that cluster examples into groups where correct/incorrect predictions are highly correlated. Anchor points from these clusters serve as proxies for the full dataset.

### Mechanism 2
The gp-IRT estimator improves accuracy by combining raw data estimates with IRT model predictions, weighted by their relative variance and bias. gp-IRT is a convex combination of the simple average over selected examples and the IRT-predicted performance, with the weight on raw data decreasing as more examples are used.

### Mechanism 3
Stratified random sampling ensures representation across sub-scenarios, reducing variance in performance estimates. Examples are selected uniformly from each sub-scenario to ensure no group is underrepresented, then performance is averaged across sub-scenarios.

## Foundational Learning

- **Concept**: Item Response Theory (IRT) models the probability of correct responses based on latent abilities and item difficulty.
  - **Why needed here**: IRT provides a principled way to cluster examples and estimate model performance with few examples.
  - **Quick check question**: What does the IRT parameter αi represent in the context of LLM evaluation?

- **Concept**: Stratified sampling ensures representation across subgroups in the data.
  - **Why needed here**: It prevents certain sub-scenarios from being underrepresented, which could bias performance estimates.
  - **Quick check question**: Why is stratified sampling particularly important for benchmarks with many sub-scenarios?

- **Concept**: Anchor points are representative examples that capture the performance patterns of a group.
  - **Why needed here**: Using anchor points allows us to estimate performance on the full dataset by evaluating only a small subset.
  - **Quick check question**: How does the clustering method identify anchor points from model correctness data?

## Architecture Onboarding

- **Component map**: Data ingestion -> IRT model fitting -> Anchor point selection -> Performance estimation -> Tiny benchmark construction

- **Critical path**: IRT model fitting → Anchor point selection → Performance estimation → Evaluation results

- **Design tradeoffs**: Using IRT vs. correctness clustering (IRT more robust but requires model fitting), number of examples (fewer reduces cost but increases variance), estimator choice (raw simple but high variance; p-IRT low variance but biased; gp-IRT balances both)

- **Failure signatures**: High estimation error on specialized models (anchor points not representative), large variance in estimates (too few examples or poor clustering), systematic bias (IRT model mis-specification)

- **First 3 experiments**: 1) Fit IRT model on training data and visualize example embeddings to check clustering quality. 2) Compare estimation error of raw, p-IRT, and gp-IRT estimators on a held-out test set. 3) Evaluate performance on specialized models to test robustness.

## Open Questions the Paper Calls Out

### Open Question 1
How stable are the IRT-based example representations and anchor points over time, and what is the optimal frequency for updating them? The paper acknowledges that rapid advancements in LLM capabilities may cause extrapolation errors and recommends periodically updating the curated examples and IRT parameter estimates using data from more modern LLMs, but does not specify the frequency or methodology.

### Open Question 2
Can the proposed methods be effectively extended to non-binary and continuous correctness scores beyond simple binarization? The authors propose a simple binarization approach but acknowledge this may not be optimal for all scenarios and do not explore alternative methods for handling continuous or multi-level correctness scores.

### Open Question 3
How do the proposed methods perform on specialized benchmarks and domains beyond the four considered in the paper? While the paper conducts an experiment on MMLU with specialized models and observes that correctness-based anchor strategies deteriorate while IRT-based methods remain robust, it is unclear how the methods would generalize to other specialized domains or benchmarks.

## Limitations
- Performance degrades for specialized models (GPT-4, Claude) not well-represented in training data
- Assumes sub-scenario performance averages meaningfully represent overall performance, which may not hold for imbalanced benchmarks
- Relies on publicly available correctness data that may not be comprehensive or representative of all important model capabilities

## Confidence

- **High confidence**: Core claim that IRT-based methods can reduce evaluation examples while maintaining reasonable accuracy (within ~2% error on average)
- **Medium confidence**: Claim that 100 examples per scenario is universally sufficient for accurate evaluation
- **Medium confidence**: Effectiveness of gp-IRT estimator as a balanced approach between raw estimates and IRT predictions

## Next Checks

1. Test on specialized capabilities: Evaluate the method's performance on models with specialized training (e.g., code models, mathematical reasoning models) to assess robustness beyond general-purpose LLMs.

2. Benchmark generalizability: Apply the tiny benchmark construction method to a different benchmark family (e.g., BIG-bench or HumanEval) to verify that the ~100 example rule holds across different task types.

3. Sub-scenario importance weighting: Investigate whether uniform averaging across sub-scenarios is optimal, or if performance-weighted sampling would yield better estimates, particularly for benchmarks with imbalanced sub-scenario distributions.