---
ver: rpa2
title: 'Revisiting Neural Networks for Continual Learning: An Architectural Perspective'
arxiv_id: '2404.14829'
source_url: https://arxiv.org/abs/2404.14829
tags:
- network
- task
- performance
- learning
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how network architecture design impacts continual
  learning (CL) performance, focusing on both network scaling (width and depth) and
  components (skip connections, pooling, down-sampling). Through systematic experiments,
  it reveals that wider and shallower architectures better address the stability-plasticity
  dilemma in CL.
---

# Revisiting Neural Networks for Continual Learning: An Architectural Perspective

## Quick Facts
- arXiv ID: 2404.14829
- Source URL: https://arxiv.org/abs/2404.14829
- Reference count: 9
- This paper proposes ArchCraft, a neural architecture search method that discovers CL-friendly architectures achieving state-of-the-art performance while being significantly more parameter-efficient than baselines

## Executive Summary
This paper explores how network architecture design impacts continual learning performance, focusing on both network scaling (width and depth) and components (skip connections, pooling, down-sampling). Through systematic experiments, it reveals that wider and shallower architectures better address the stability-plasticity dilemma in CL. Based on these insights, the authors propose ArchCraft, a neural architecture search method with a CL-friendly search space that recrafts AlexNet and ResNet into CL-optimized architectures (AlexAC and ResAC). ArchCraft achieves state-of-the-art CL performance while being 86%, 61%, and 97% more parameter-efficient than baseline architectures in Task IL and Class IL settings.

## Method Summary
The paper proposes ArchCraft, a neural architecture search method that systematically explores network scaling (width and depth) and component choices (skip connections, pooling, down-sampling) to discover CL-friendly architectures. The method employs a genetic algorithm with a constrained search space based on ResNet-like architectures, encoding architectures as 12-dimensional vectors representing depth, width, channel increase locations, and down-sampling positions. The search optimizes for CL performance using the Average Incremental Accuracy (AIA) metric, producing architectures AlexAC and ResAC that significantly outperform baseline ResNet models while being much more parameter-efficient.

## Key Results
- ArchCraft achieves state-of-the-art CL performance while being 86%, 61%, and 97% more parameter-efficient than baseline architectures
- Wider and shallower architectures consistently outperform deeper, narrower ones in CL settings
- Skip connections significantly improve CL performance while global average pooling has opposite effects in Task IL vs Class IL scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wider and shallower architectures better balance stability and plasticity in continual learning
- Mechanism: Increased width provides more representational capacity to store multiple tasks' information simultaneously, while reduced depth prevents deep feature hierarchies from becoming overly task-specific and thus forgetting earlier tasks
- Core assumption: Network width contributes more to CL performance than depth, and shallower networks maintain better generalization across tasks
- Evidence anchors:
  - [abstract] "wider and shallower architectures better address the stability-plasticity dilemma in CL"
  - [section] "Figure 2 compares the CL performance of networks with different W and D. We observe that in both Task IL and Class IL, the network with a larger width tends to exhibit better performance of CL in most cases"
  - [corpus] Weak evidence - no directly comparable papers found
- Break condition: If width increase leads to overfitting or if depth reduction compromises feature extraction quality for later tasks

### Mechanism 2
- Claim: Network components like skip connections and pooling configurations significantly impact CL performance
- Mechanism: Skip connections enable gradient flow and feature reuse across tasks, while pooling configurations affect feature resolution and task discriminability preservation
- Core assumption: Architectural components can be optimized independently of network scaling to improve CL
- Evidence anchors:
  - [abstract] "components (skip connections, pooling, down-sampling)"
  - [section] "Table 1, different configurations of network components influence the performance of both Task IL and Class IL"
  - [corpus] Weak evidence - limited corpus papers directly addressing component optimization for CL
- Break condition: If component optimization leads to architectural instability or if component interactions become too complex to optimize effectively

### Mechanism 3
- Claim: Neural architecture search can effectively discover CL-friendly architectures beyond manual design
- Mechanism: NAS explores the architectural space systematically to find configurations that manual design might miss, leveraging insights from network scaling and component experiments
- Core assumption: The search space can be constrained to architectures similar to ResNet while allowing variation in width, depth, and component placement
- Evidence anchors:
  - [abstract] "propose ArchCraft, a neural architecture search method with a CL-friendly search space"
  - [section] "We employ NAS as a promising solution for CL and probe catastrophic forgetting from an architectural perspective"
  - [corpus] Moderate evidence - related papers on NAS for CL exist but are limited
- Break condition: If search space becomes too large for practical exploration or if NAS converges to suboptimal architectures

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding the problem being solved is crucial for appreciating why architectural changes help
  - Quick check question: What happens to a neural network's performance on previous tasks when trained on new tasks without any mitigation strategy?

- Concept: Stability-plasticity dilemma
  - Why needed here: The core challenge in continual learning that architectural design aims to address
  - Quick check question: Why is it difficult for neural networks to simultaneously maintain performance on old tasks while learning new ones?

- Concept: Neural architecture search fundamentals
  - Why needed here: Understanding how ArchCraft works requires knowledge of search spaces, optimization strategies, and evaluation metrics
  - Quick check question: What are the key components of a neural architecture search framework?

## Architecture Onboarding

- Component map: ArchCraft builds on ResNet-like architectures with controllable width, depth, skip connections, pooling layers, and down-sampling positions
- Critical path: Design search space → Initialize population → Evaluate fitness (AIA metric) → Evolve architectures → Select best performer
- Design tradeoffs: Parameter efficiency vs performance, search space size vs exploration time, generalizability vs task-specific optimization
- Failure signatures: Poor performance on earlier tasks (forgetting), inability to learn new tasks effectively (lack of plasticity), excessive parameter growth
- First 3 experiments:
  1. Baseline comparison: Test ResNet-32 performance on C100-inc5 to establish forgetting baseline
  2. Width scaling: Vary initial channel width while keeping depth constant to observe impact on stability vs plasticity
  3. Component ablation: Test networks with/without skip connections and different pooling configurations to identify optimal component choices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different network components (skip connections, global pooling, down-sampling methods) specifically affect catastrophic forgetting mechanisms at the architectural level?
- Basis in paper: [explicit] The paper systematically investigates how strided convolution, skip connections, and global average pooling impact CL performance, finding that skip connections significantly improve performance while GAP has opposite effects in Task IL vs Class IL.
- Why unresolved: While the paper identifies correlations between components and performance, it doesn't establish causal mechanisms for how these architectural elements influence forgetting at the parameter or representation level.
- What evidence would resolve it: Detailed analysis of representation similarity (like CKA) across incremental tasks for architectures with different component configurations, combined with forgetting measurements at individual layer levels.

### Open Question 2
- Question: What is the optimal relationship between classifier width and overall network width for maximizing CL performance across different incremental learning scenarios?
- Basis in paper: [explicit] The paper finds that in Task IL, classifier width matters more than overall network width, while in Class IL both are important, but doesn't determine the optimal ratio between them.
- Why unresolved: The experiments show different patterns for Task IL vs Class IL but don't establish how to optimally balance these widths or whether the relationship changes with more classes or different task distributions.
- What evidence would resolve it: Systematic experiments varying classifier width independently across a wide range of overall network widths and incremental learning scenarios to establish optimal scaling relationships.

### Open Question 3
- Question: How generalizable are the architectural improvements from ArchCraft to other backbone architectures beyond ResNet and AlexNet?
- Basis in paper: [explicit] The paper demonstrates ArchCraft works well for ResNet and AlexNet series but doesn't test it on other popular architectures like EfficientNet, Vision Transformers, or MobileNet.
- Why unresolved: The search space and insights are derived from ResNet experiments, so their applicability to fundamentally different architectural families remains unknown.
- What evidence would resolve it: Applying ArchCraft's methodology to diverse backbone architectures and comparing performance improvements against naive implementations.

## Limitations
- The search space design may limit generalizability to architectures outside the ResNet-like family
- The computational cost of the NAS approach is not fully quantified
- The focus on image classification datasets may not generalize to other task domains

## Confidence
- **High confidence**: Network scaling effects (wider/shallower networks improving CL performance) - supported by systematic ablation studies and consistent results across datasets
- **Medium confidence**: Component-level optimizations (skip connections, pooling configurations) - supported by controlled experiments but may have interaction effects not fully explored
- **Medium confidence**: ArchCraft NAS methodology - shows state-of-the-art results but relies on specific search space constraints that may limit broader applicability

## Next Checks
1. **Component isolation experiment**: Systematically test each architectural component (skip connections, pooling type, down-sampling location) in isolation to quantify their individual contributions versus combined effects
2. **Cross-domain generalization**: Apply ArchCraft-optimized architectures to non-image classification tasks (e.g., NLP sequence tagging or reinforcement learning) to validate architectural principles beyond the current scope
3. **Search space expansion**: Extend the NAS search space to include architectures beyond ResNet-like structures (e.g., Vision Transformers or attention-based components) to test whether the discovered principles generalize to different architectural paradigms