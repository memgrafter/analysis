---
ver: rpa2
title: Neural Architecture Search of Hybrid Models for NPU-CIM Heterogeneous AR/VR
  Devices
arxiv_id: '2410.08326'
source_url: https://arxiv.org/abs/2410.08326
tags:
- hybrid
- search
- system
- edge
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes H4H-NAS, a neural architecture search framework
  to design efficient hybrid CNN/ViT models for heterogeneous edge systems with both
  NPU and CIM. The framework leverages architectural heterogeneity from NPU and CIM
  to perform diverse execution schemas for efficient execution of hybrid models.
---

# Neural Architecture Search of Hybrid Models for NPU-CIM Heterogeneous AR/VR Devices

## Quick Facts
- arXiv ID: 2410.08326
- Source URL: https://arxiv.org/abs/2410.08326
- Reference count: 40
- Primary result: H4H-NAS achieves up to 1.34% top-1 accuracy improvement on ImageNet with 56.08% latency and 41.72% energy improvements

## Executive Summary
This paper introduces H4H-NAS, a neural architecture search framework designed to create efficient hybrid CNN/ViT models for heterogeneous edge systems combining NPUs and CIMs. The framework leverages architectural heterogeneity between compute-intensive NPUs and memory-efficient CIMs to execute diverse workloads optimally. By decoupling architecture search from supernet training and using a two-stage NAS approach with evolutionary search, the method achieves significant improvements in both accuracy and efficiency over baseline solutions.

## Method Summary
H4H-NAS employs a two-stage neural architecture search approach where a supernet is first trained using weight-sharing across all possible subnets, followed by evolutionary search to find optimal subnets under hardware constraints. The framework includes a performance estimator that models both NPU performance using real silicon measurements and CIM performance using industry IPs. The search space allows flexible combination of CNNs and ViTs, and the resulting models are partitioned between NPU and CIM based on their computational characteristics, with NPU handling compute-intensive operations and CIM managing memory-bounded workloads.

## Key Results
- Achieves up to 1.34% top-1 accuracy improvement on ImageNet compared to baselines
- Delivers up to 56.08% latency reduction through heterogeneous execution
- Provides up to 41.72% energy efficiency improvements over baseline solutions

## Why This Works (Mechanism)

### Mechanism 1: Two-stage NAS with weight-sharing
- Claim: H4H-NAS enables efficient search by decoupling architecture search from supernet training
- Mechanism: Trains a supernet with weight-sharing across all possible subnets, then performs evolutionary search for optimal subnets
- Core assumption: Weight-sharing allows effective training of large supernets, making subsequent search efficient
- Evidence: Performance estimator built with real NPU silicon measurements and CIM IP data
- Break condition: If weight-sharing leads to co-adaptation issues causing suboptimal subnet performance

### Mechanism 2: Heterogeneous NPU+CIM execution
- Claim: Complementary NPU and CIM architectures enable efficient hybrid model execution
- Mechanism: NPU optimized for compute-intensive workloads (regular convolution), CIM for memory-bounded workloads (depthwise convolution, fully-connected layers)
- Core assumption: NPU and CIM have complementary strengths that can be exploited through heterogeneous execution
- Evidence: Performance modeling based on NPU silicon and CIM IP data
- Break condition: If layer partitioning overhead exceeds performance gains

### Mechanism 3: Multi-CU and multi-macro CIM design
- Claim: Multiple CIM macros and compute units improve performance through parallelism
- Mechanism: Multiple CIM macros enable parallel execution of different layer channels; multiple compute units reduce data movement
- Core assumption: Adding compute units and macros provides linear speedup without prohibitive overhead
- Evidence: Design analysis showing latency reduction and energy efficiency benefits
- Break condition: If area overhead outweighs performance benefits

## Foundational Learning

- Concept: Neural Architecture Search (NAS)
  - Why needed: H4H-NAS is the core methodology for finding optimal hybrid models
  - Quick check: What are the two main stages in the two-stage NAS approach?

- Concept: Compute-in-Memory (CIM) architecture
  - Why needed: CIM is a key hardware component being co-designed with algorithms
  - Quick check: What type of neural network layers does CIM excel at executing?

- Concept: Systolic array architecture in NPUs
  - Why needed: NPU is the other key hardware component determining efficient operations
  - Quick check: Which type of neural network layers are NPUs typically optimized for?

## Architecture Onboarding

- Component map: Supernet training -> Evolutionary search (with performance estimation) -> Model selection -> Layer partitioning onto NPU/CIM
- Critical path: Supernet training → Evolutionary search (with performance estimation) → Model selection → Layer partitioning onto NPU/CIM
- Design tradeoffs:
  - Flexibility vs. search space size: More flexible search space increases search cost
  - Hardware heterogeneity vs. partitioning complexity: More diverse hardware requires more complex partitioning decisions
  - Accuracy vs. efficiency: Trade-off between model accuracy and hardware resource usage
- Failure signatures:
  - Supernet training fails to converge: Weight-sharing assumption breaks down
  - Performance estimator inaccuracies: Model partitioning leads to suboptimal resource usage
  - Layer partitioning overhead exceeds benefits: Heterogeneous execution complexity outweighs gains
- First 3 experiments:
  1. Validate supernet training with a small subset of the search space
  2. Test performance estimator accuracy against real silicon measurements
  3. Verify layer partitioning logic on a simple hybrid model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy-performance tradeoff change when scaling H4H-NAS to larger datasets like JFT-300M or larger-scale models?
- Basis: Paper mentions ImageNet results but doesn't explore larger-scale datasets
- Why unresolved: Larger datasets and models might reveal different tradeoffs
- Evidence needed: Running H4H-NAS on JFT-300M with larger hybrid models and comparing metrics

### Open Question 2
- Question: How does H4H-NAS perform on tasks beyond image classification like object detection or semantic segmentation?
- Basis: Paper focuses on ImageNet classification but mentions AR/VR applications
- Why unresolved: Different tasks have different computational requirements
- Evidence needed: Applying H4H-NAS to COCO, Cityscapes, or video analysis datasets

### Open Question 3
- Question: How robust is the performance modeling to variations in CIM technology and NPU architectures?
- Basis: Paper uses MRAM-based digital CIM and ARM Ethos-U55 NPU
- Why unresolved: Different technologies have varying characteristics
- Evidence needed: Testing with analog CIM, different non-volatile memories, and various NPU architectures

## Limitations
- Performance estimator relies on specific silicon measurements that may not generalize
- Two-stage NAS assumes weight-sharing effectiveness which may not hold for all search spaces
- Layer partitioning algorithm optimality is not formally proven

## Confidence
- High confidence: Two-stage NAS methodology is well-established
- Medium confidence: Performance estimator accuracy for real-world deployment
- Medium confidence: Claimed accuracy improvements and efficiency gains

## Next Checks
1. Validate the performance estimator against a broader set of real hardware measurements to assess generalizability
2. Conduct ablation studies to quantify the impact of each mechanism on overall performance
3. Test the framework on a diverse set of computer vision tasks beyond ImageNet classification to evaluate robustness