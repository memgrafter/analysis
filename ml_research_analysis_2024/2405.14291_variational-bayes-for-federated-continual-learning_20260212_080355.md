---
ver: rpa2
title: Variational Bayes for Federated Continual Learning
arxiv_id: '2405.14291'
source_url: https://arxiv.org/abs/2405.14291
tags:
- learning
- data
- task
- distribution
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses federated continual learning (FCL), where
  multiple clients collaboratively train a global model while facing evolving data
  distributions and privacy constraints. The proposed FedBNN framework employs variational
  Bayesian neural networks to integrate knowledge from local and historical data distributions
  into a single model, mitigating catastrophic forgetting without requiring explicit
  task boundaries.
---

# Variational Bayes for Federated Continual Learning

## Quick Facts
- arXiv ID: 2405.14291
- Source URL: https://arxiv.org/abs/2405.14291
- Reference count: 40
- Primary result: FedBNN achieves state-of-the-art performance in federated continual learning with variational Bayesian neural networks, mitigating catastrophic forgetting without requiring explicit task boundaries.

## Executive Summary
This paper introduces FedBNN, a novel framework for Federated Continual Learning (FCL) that addresses the challenge of learning from evolving data distributions while preserving privacy constraints. The method employs variational Bayesian neural networks to integrate knowledge from local and historical data distributions into a single model, effectively mitigating catastrophic forgetting. FedBNN demonstrates superior performance compared to existing federated learning and continual learning baselines across various FCL settings, including class-incremental and task-incremental scenarios on multiple datasets.

## Method Summary
FedBNN is a framework for federated continual learning that combines variational Bayesian neural networks with a prototype library approach. The method integrates knowledge from local and historical data distributions through a Bayesian prior-posterior updating mechanism, where the posterior from each communication round becomes the prior for the next. A prototype library maintains class-specific parameters to handle dynamic label spaces, and an SNN-based initialization strategy is used to improve BNN training. The framework operates in a federated setting where clients collaboratively train a global model while preserving privacy, with the server aggregating local likelihoods and maintaining a global prototype library.

## Key Results
- FedBNN achieves state-of-the-art performance in various FCL settings on CIFAR-100, Tiny-ImageNet, and image classification tasks.
- The method demonstrates superior accuracy and adaptability compared to existing federated learning and continual learning baselines.
- FedBNN maintains performance across participation ratios ranging from 0.1 to 0.6, showing robustness to varying levels of client engagement.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FedBNN mitigates catastrophic forgetting by integrating historical knowledge into the Bayesian prior distribution.
- Mechanism: During each communication round, the posterior from the previous round becomes the prior for the next round, allowing the model to retain performance on historical data distributions while adapting to new ones.
- Core assumption: The prior distribution effectively regularizes the model to stay close to previously learned knowledge.
- Evidence anchors:
  - [abstract]: "Our method continually integrates knowledge from local and historical data distributions into a single model, adeptly learning from new data distributions while retaining performance on historical distributions."
  - [section]: "VCL utilizes the relation to address the problem of continual learning. When a new task is encountered, a new posterior is obtained by setting the latest posterior as the prior and then performing variational inference on the new task."
  - [corpus]: Weak evidence - related papers mention continual learning but don't specifically discuss the prior-based knowledge integration mechanism.
- Break condition: If the prior becomes too strong relative to the likelihood of new data, the model may fail to adapt to significant distribution changes.

### Mechanism 2
- Claim: FedBNN handles dynamic label spaces through a prototype library that maintains class-specific parameters.
- Mechanism: The prototype library stores learned prototypes for each encountered class. When training, only relevant prototypes are assembled into the classifier, allowing the model to handle new classes without architectural changes.
- Core assumption: Each neuron in the classifier layer can be treated as a learned prototype for its corresponding class.
- Evidence anchors:
  - [abstract]: "The proposed FedBNN framework employs variational Bayesian neural networks to integrate knowledge from local and historical data distributions into a single model, mitigating catastrophic forgetting without requiring explicit task boundaries."
  - [section]: "We inherit the concept of feature extractor and classifier, but propose a distinct approach to handle the dynamic label space as shown in Fig. 3."
  - [corpus]: Weak evidence - related papers discuss federated continual learning but don't specifically address dynamic label space handling.
- Break condition: If the prototype library grows too large or becomes too heterogeneous across clients, aggregation may become unstable.

### Mechanism 3
- Claim: FedBNN uses SNN-based initialization to overcome prior-induced forgetting in small datasets.
- Mechanism: Before BNN training, an SNN is trained using FedAvg. The SNN parameters initialize the BNN prior, providing better starting points than zero-centered priors.
- Core assumption: SNN parameters represent a good MAP estimate that can serve as initialization for BNN training.
- Evidence anchors:
  - [section]: "To overcome this, we suggest the prior initialized from an unconstrained SNN learned from the data distribution. In the probabilistic view, SNN represents the max-a-posteriori (MAP) estimation of the parameter distribution."
  - [abstract]: "Our method continually integrates knowledge from local and historical data distributions into a single model, adeptly learning from new data distributions while retaining performance on historical distributions."
  - [corpus]: Weak evidence - related papers mention continual learning but don't specifically discuss SNN-based initialization for BNNs.
- Break condition: If the SNN is poorly trained or the dataset is too small to provide meaningful initialization, the benefit may be minimal or negative.

## Foundational Learning

- Concept: Variational Inference
  - Why needed here: FedBNN uses variational inference to approximate the posterior distribution of BNN parameters, making training tractable.
  - Quick check question: What is the key difference between variational inference and MCMC methods for approximating posterior distributions?

- Concept: Catastrophic Forgetting
  - Why needed here: The primary problem FedBNN addresses is catastrophic forgetting in federated continual learning settings.
  - Quick check question: What are the two main approaches to mitigate catastrophic forgetting in neural networks?

- Concept: Bayesian Neural Networks
  - Why needed here: FedBNN is built on BNNs, which treat model parameters as random variables rather than point estimates.
  - Quick check question: How does the uncertainty quantification in BNNs differ from standard neural networks?

## Architecture Onboarding

- Component map:
  - Server: Aggregates local likelihoods, maintains global prototype library
  - Clients: Perform local inference, maintain local prototype libraries
  - Communication: Exchange of local likelihoods and prototype libraries
  - Model: Shared feature extractor + dynamic classifier (prototype library)

- Critical path:
  1. Server broadcasts global model and prototype library
  2. Clients assemble local classifiers and perform local training
  3. Clients extract local likelihoods and update prototype libraries
  4. Clients send likelihoods and libraries to server
  5. Server aggregates information to create new global model

- Design tradeoffs:
  - BNN vs SNN: BNNs provide uncertainty quantification but are more complex; SNNs are simpler but lack this capability
  - Prototype library vs static classifier: Prototype library enables dynamic class handling but increases communication overhead
  - Full vs partial client participation: Full participation ensures better aggregation but may be impractical in real-world scenarios

- Failure signatures:
  - Poor adaptation to new tasks: Model may be overly regularized by historical knowledge
  - Degradation in performance: Could indicate issues with likelihood extraction or aggregation
  - Slow convergence: May suggest problems with the variational inference implementation

- First 3 experiments:
  1. Test FedBNN on a simple class-incremental task with synthetic data to verify basic functionality
  2. Compare FedBNN with FedAvg on a standard federated learning benchmark to isolate the benefits of the BNN approach
  3. Evaluate the impact of different participation ratios on FedBNN performance to understand scalability limits

## Open Questions the Paper Calls Out
- [explicit] The paper assumes that the distribution drift among clients follows a global trend, and states that "if the distribution drift among clients diverges significantly, collaborative learning among the clients becomes ineffective."

## Limitations
- The effectiveness of the prior-based knowledge integration mechanism may degrade with significant domain shifts between tasks.
- The prototype library approach could face scalability challenges as the number of classes grows, potentially leading to unstable aggregations.
- The SNN-based initialization relies on the quality of the FedAvg-trained SNN, which may be compromised with very small datasets or high non-IIDness.

## Confidence
- Confidence in the core claims is Medium, as the theoretical framework is sound but the empirical validation, while comprehensive, does not fully explore edge cases or extreme non-IID scenarios.
- The comparison with baselines is robust, but the specific implementation details of the prototype library and variational inference steps leave some uncertainty about reproducibility.

## Next Checks
- Test FedBNN with extreme non-IID partitions to assess robustness to highly heterogeneous data distributions.
- Evaluate the scalability of the prototype library approach with datasets containing hundreds of classes.
- Compare the performance of FedBNN with and without SNN-based initialization to quantify its contribution under different data regimes.