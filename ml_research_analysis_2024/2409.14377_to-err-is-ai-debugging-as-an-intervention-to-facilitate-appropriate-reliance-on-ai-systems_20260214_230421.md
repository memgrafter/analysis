---
ver: rpa2
title: To Err Is AI! Debugging as an Intervention to Facilitate Appropriate Reliance
  on AI Systems
arxiv_id: '2409.14377'
source_url: https://arxiv.org/abs/2409.14377
tags:
- debugging
- reliance
- performance
- participants
- intervention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored whether debugging AI systems could help users
  develop appropriate reliance on AI advice. The researchers designed an explanation-based
  debugging intervention where participants evaluated AI predictions and adjusted
  explanations in hotel review classification tasks.
---

# To Err Is AI! Debugging as an Intervention to Facilitate Appropriate Reliance on AI Systems

## Quick Facts
- arXiv ID: 2409.14377
- Source URL: https://arxiv.org/abs/2409.14377
- Authors: Gaole He; Abri Bharos; Ujwal Gadiraju
- Reference count: 40
- Primary result: Debugging intervention failed to improve appropriate reliance on AI systems and instead reduced user reliance on AI advice.

## Executive Summary
This study investigated whether a debugging intervention could help users develop appropriate reliance on AI systems in human-AI decision-making tasks. Using hotel review classification as a testbed, the researchers designed an explanation-based debugging intervention where participants evaluated AI predictions and adjusted explanations. Surprisingly, the intervention failed to improve appropriate reliance or calibration of AI performance estimation. Instead, participants showed decreased reliance on the AI system after the intervention, particularly when exposed to AI weaknesses early. The study found that users who performed poorly tended to underestimate AI performance, leading to under-reliance. These findings suggest that debugging interventions must be carefully designed to avoid anchoring effects that harm appropriate reliance.

## Method Summary
The study used a deceptive review detection task with 234 participants recruited from Prolific. Participants completed two-stage decision making: first making decisions without AI input, then with AI advice and BERT-LIME explanations. A debugging intervention group evaluated AI advice correctness and adjusted explanations with real-time feedback across 8 selected tasks. The study tested four experimental conditions: Control, Debugging-R (random order), Debugging-D (decreasing impression), and Debugging-I (increasing impression). Reliance was measured through agreement fractions, switch fractions, RAIR, and RSR metrics, while AI performance estimation accuracy was assessed through multiple estimation-based metrics.

## Key Results
- Debugging intervention significantly decreased appropriate reliance (RAIR and RSR) compared to control condition
- Participants exposed to AI weaknesses early showed stronger tendency to under-rely on the system
- Lower-performing participants consistently underestimated AI performance, leading to under-reliance
- Confidence calibration showed participants were more confident when AI advice agreed with their initial decisions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Debugging intervention exposes AI weaknesses, which should help users calibrate trust and develop appropriate reliance.
- **Mechanism**: By providing real-time feedback on AI advice correctness and allowing users to adjust explanations, participants learn when AI advice is reliable versus unreliable. This critical evaluation process should improve their ability to estimate AI performance at both instance and global levels.
- **Core assumption**: Users can effectively learn from debugging feedback and apply this knowledge to calibrate their trust in AI advice.
- **Evidence anchors**:
  - [abstract] "Inspired by existing literature on critical thinking and a critical mindset, we propose the use of debugging an AI system as an intervention to foster appropriate reliance."
  - [section 3.3] "Through the debugging phase, all participants are supposed to learn two important facts about the AI system: (1) the AI advice is not always correct, and (2) explanations are not always informative and helpful in identifying the trustworthiness of AI advice."
  - [corpus] Weak evidence - only 5 related papers found with average FMR 0.441, suggesting limited direct research on debugging interventions for appropriate reliance.
- **Break condition**: If debugging tasks present AI weaknesses too early, users may develop anchoring bias that causes underestimation of AI performance and under-reliance.

### Mechanism 2
- **Claim**: The ordering of debugging tasks affects user impressions and subsequent reliance patterns.
- **Mechanism**: Task presentation order creates first impressions that influence trust calibration. Positive impressions (correct advice, informative explanations) early on may foster appropriate reliance, while negative impressions (incorrect advice, uninformative explanations) may cause under-reliance.
- **Core assumption**: First impressions have lasting effects on user trust and reliance judgments in human-AI collaboration.
- **Evidence anchors**:
  - [section 3.3] "When presenting the debugging phase to participants, the order of tasks may have an impact on their estimation of AI performance and reliance on the AI system."
  - [section 5.3.3] "Participants who were exposed to the weakness of the AI system at the beginning of the debugging intervention, showed a more obvious tendency to disuse the AI system."
  - [corpus] Moderate evidence - related papers discuss ordering effects and cognitive biases in human-AI interaction.
- **Break condition**: If task ordering is randomized or balanced, the first impression effect may be diluted, reducing the impact of ordering on reliance patterns.

### Mechanism 3
- **Claim**: Debugging intervention should improve user confidence calibration by exposing limitations of both AI advice and explanations.
- **Mechanism**: Users learn that explanations are not always reliable indicators of advice trustworthiness, which should lead to more nuanced confidence judgments aligned with actual AI performance.
- **Core assumption**: Users can develop metacognitive awareness about the limitations of XAI explanations through debugging experience.
- **Evidence anchors**:
  - [section 3.3] "We posit that such a debugging intervention has the potential to help users understand the limitations of AI systems — that neither explanations of the AI advice nor the advice itself are always reliable."
  - [section 5.3.4] "In general, participants indicated increased confidence when AI advice agreed with their initial decision, and showed decreased confidence when AI advice disagreed with their initial decision."
  - [corpus] Weak evidence - limited corpus research on debugging's effect on confidence calibration.
- **Break condition**: If users become overly skeptical of all AI advice after debugging, they may maintain low confidence even when AI advice is correct, leading to under-reliance.

## Foundational Learning

- **Concept: Appropriate Reliance**
  - Why needed here: This study's core goal is to understand how to foster appropriate reliance on AI systems. Understanding the concept is fundamental to interpreting results.
  - Quick check question: What distinguishes appropriate reliance from over-reliance and under-reliance in human-AI decision making?

- **Concept: Dunning-Kruger Effect**
  - Why needed here: The study found that less-competent individuals underestimated AI performance, potentially related to metacognitive bias. Understanding this effect is crucial for interpreting user behavior.
  - Quick check question: How might the Dunning-Kruger effect manifest in a human-AI collaboration context where users underestimate AI capabilities?

- **Concept: XAI Explanation Plausibility**
  - Why needed here: The debugging intervention may have reduced the plausibility of explanations, affecting trust calibration. Understanding this concept helps explain unexpected results.
  - Quick check question: Why might exposing users to explanation limitations during debugging reduce their perceived trustworthiness of the AI system?

## Architecture Onboarding

- **Component map**: Hotel review task → Initial human decision → AI advice + BERT-LIME explanation → Debugging intervention (evaluate correctness, adjust highlights) → Final human decision → Real-time feedback → Reliance and performance measurement
- **Critical path**: Task → Initial human decision → AI advice + explanation → Debugging intervention (if applicable) → Final human decision → Feedback → Reliance and performance measurement
- **Design tradeoffs**: The system prioritizes controlled experimental conditions over ecological validity by using a specific task (deceptive review detection) and limiting AI advice correctness to 80% to create diagnostic opportunities.
- **Failure signatures**: Unexpected reduction in reliance after debugging intervention, underestimation of AI performance particularly by lower-performing users, and persistence of confidence in initial decisions even when incorrect.
- **First 3 experiments**:
  1. Test whether providing only correct AI advice (no debugging) affects reliance patterns differently than the current design.
  2. Test whether reversing the debugging task order (strength first, then weakness) changes the anchoring effect.
  3. Test whether adding explicit instruction about common cognitive biases improves appropriate reliance outcomes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Dunning-Kruger effect directly cause under-reliance on AI systems, or is it mediated by other factors like confidence or task difficulty?
- Basis in paper: [explicit] The paper suggests that participants who performed worse tended to underestimate AI performance, leading to under-reliance. This observation aligns with the Dunning-Kruger effect, where less-competent individuals overestimate their own competence.
- Why unresolved: The paper only provides correlational evidence between performance and underestimation of AI trustworthiness. It doesn't establish a causal link or explore mediating factors.
- What evidence would resolve it: An experimental design that manipulates participants' self-assessment accuracy and measures the resulting reliance on AI systems, controlling for task difficulty and other potential confounders.

### Open Question 2
- Question: How can the plausibility of explanations be improved to avoid negative impacts on user trust and reliance on AI systems?
- Basis in paper: [explicit] The paper suggests that the debugging intervention may have made the XAI (text highlights) less plausible to users, leading to more tendency to underestimate AI performance. This aligns with the idea that plausibility substantially affects user perceived trustworthiness of the AI system.
- Why unresolved: The paper doesn't explore specific strategies for improving explanation plausibility or investigate how different explanation methods might affect user trust and reliance.
- What evidence would resolve it: Comparative studies of different explanation methods and their impact on user trust and reliance, potentially involving user studies with iterative feedback and refinement of explanation designs.

### Open Question 3
- Question: What is the optimal ordering of tasks within a debugging intervention to maximize its effectiveness in promoting appropriate reliance on AI systems?
- Basis in paper: [explicit] The paper explores different task orderings within the debugging intervention but finds no significant difference. However, it observes that participants exposed to the AI system's weaknesses early showed a more obvious tendency to under-rely on the system.
- Why unresolved: The paper's analysis of task orderings is limited to a few predefined sequences. It doesn't explore the full space of possible orderings or investigate the underlying mechanisms driving the observed effects.
- What evidence would resolve it: A more comprehensive study of task orderings, potentially using optimization techniques to identify sequences that maximize appropriate reliance while minimizing negative impacts like under-reliance.

## Limitations
- Controlled experimental setting using specific deceptive review detection task may not generalize to other domains
- Fixed 80% AI accuracy rate creates artificial conditions that don't reflect real-world AI performance variability
- Debugging intervention interface and specific task selection criteria not fully detailed, limiting reproducibility

## Confidence
- **High Confidence**: Debugging intervention led to decreased reliance on AI systems (supported by RAIR, RSR, and agreement fractions with statistical significance)
- **Medium Confidence**: Ordering effect on reliance patterns (observed but may be influenced by task selection and participant variability)
- **Medium Confidence**: Relationship between user performance and AI performance estimation accuracy (statistically significant but mechanisms require investigation)

## Next Checks
1. **Domain Transfer Test**: Replicate the study with a different AI-assisted decision task (e.g., medical diagnosis or financial risk assessment) to assess generalizability of the debugging intervention effects across domains.

2. **Adaptive Ordering Validation**: Implement a counterbalanced design where half participants receive positive-first ordering and half receive negative-first ordering, with equal task difficulty, to isolate the pure ordering effect from task content effects.

3. **Metacognitive Training Experiment**: Add a brief training module about cognitive biases and appropriate reliance before the debugging intervention to test whether explicit bias awareness mitigates the anchoring effects observed in the study.