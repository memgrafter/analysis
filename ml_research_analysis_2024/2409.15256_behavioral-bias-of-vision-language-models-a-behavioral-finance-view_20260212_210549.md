---
ver: rpa2
title: 'Behavioral Bias of Vision-Language Models: A Behavioral Finance View'
arxiv_id: '2409.15256'
source_url: https://arxiv.org/abs/2409.15256
tags:
- stock
- bias
- report
- price
- latest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work evaluates behavioral biases in Large Vision-Language
  Models (LVLMs) from a behavioral finance perspective. The authors propose an end-to-end
  framework involving data collection and evaluation metrics to assess LVLMs'' reasoning
  capabilities regarding two human financial behavioral biases: recency bias and authority
  bias.'
---

# Behavioral Bias of Vision-Language Models: A Behavioral Finance View

## Quick Facts
- arXiv ID: 2409.15256
- Source URL: https://arxiv.org/abs/2409.15256
- Authors: Yuhang Xiao; Yudi Lin; Ming-Chang Chiu
- Reference count: 22
- Key outcome: This work evaluates behavioral biases in Large Vision-Language Models (LVLMs) from a behavioral finance perspective, showing that open-source models suffer significantly from recency and authority biases while GPT-4o is negligibly impacted.

## Executive Summary
This paper evaluates behavioral biases in Large Vision-Language Models (LVLMs) through a behavioral finance lens, focusing on recency bias and authority bias. The authors propose an end-to-end framework involving data collection and evaluation metrics to assess LVLMs' reasoning capabilities in financial contexts. They create a dynamic multimodal dataset called DynoStock containing S&P 500 stock histories and quarterly EPS reports, then design structured prompts for evaluating six recent LVLMs. Results show that open-source models suffer significantly from both biases, while the proprietary GPT-4o is negligibly impacted. The study demonstrates that recency bias can be mitigated by using longer historical data, but authority bias mitigation appears more challenging.

## Method Summary
The authors developed an end-to-end framework for evaluating behavioral biases in LVLMs. They collected stock price data from yfinance API and quarterly EPS reports from Alpha Vantage API for S&P 500 companies from 2000-01-01 to 2024-04-11. The DynoStock dataset was created with time windows of varying sizes (4, 8, 12, 16, 20 quarters) and corresponding stock charts generated using mplfinance with EPS data markers. Structured prompts were designed using a scratchpad style with Chain-of-Thought reasoning to ensure models consider all relevant information and follow the desired output format. Six LVLMs were evaluated (GPT-4o, LLaVA-NeXT, MobileVLM-V2, Mini-Gemini, MiniCPM-Llama3-V 2.5, Phi-3-vision-128k) on 100 sampled data points per window size using accuracy and Behavioral Bias Index (BBI) metrics.

## Key Results
- Open-source LVLMs (LLaVA-NeXT, MobileVLM-V2, Mini-Gemini, MiniCPM-Llama3-V 2.5, and Phi-3-vision-128k) suffer significantly from both recency and authority biases
- GPT-4o demonstrates the highest accuracy while maintaining the lowest BBI (below 2% for both biases)
- Recency bias can be mitigated by using longer historical data, with increased window size correlating with reduced bias index
- Authority bias mitigation appears more challenging and persists across different window sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4o's superior performance stems from its larger model size, strong high-resolution image handling, and better-curated training data, which enable better contextual understanding and mitigation of behavioral biases.
- Mechanism: The larger model size allows GPT-4o to capture more complex patterns and relationships in the multimodal data. Its ability to handle high-resolution images means it can process more detailed visual information from stock charts. Better-curated training data likely includes more diverse and representative examples that help the model avoid overfitting to specific biases.
- Core assumption: Model size, image resolution handling, and training data quality are the primary factors determining a model's ability to resist behavioral biases in financial decision-making tasks.
- Evidence anchors:
  - [abstract]: "GPT-4o demonstrates the highest accuracy while maintaining the lowest BBI (below 2% for both biases), indicating that most wrong predictions of it may not be induced by bias."
  - [section]: "We suspect that GPT4-o's larger model size, strong ability to handle high-resolution images and better-curated training data contribute to its superior contextual understanding and mitigation of potential bias, resulting in its strong performance."
- Break condition: If the model's performance on bias mitigation is primarily driven by factors other than size, resolution handling, and training data quality (e.g., specific architectural innovations or fine-tuning techniques).

### Mechanism 2
- Claim: Recency bias in LVLMs can be mitigated by increasing the window size of historical data, allowing the model to consider longer-term trends and reduce the impact of recent events.
- Mechanism: By expanding the time window of historical data, the model is exposed to a broader range of market conditions and EPS report outcomes. This helps dilute the influence of recent events and encourages the model to consider longer-term patterns and trends, reducing the impact of recency bias.
- Core assumption: LVLMs are sensitive to the temporal scope of their input data, and expanding this scope can help mitigate short-term biases.
- Evidence anchors:
  - [section]: "Our results suggest that this bias can be mitigated by using a larger window size. In general, for open-source models, an increase in window size correlates with a reduction in the bias index."
  - [abstract]: "Recency bias can be mitigated by using longer historical data, but authority bias mitigation appears more challenging."
- Break condition: If the relationship between window size and bias mitigation is not consistent across different market conditions or if other factors (e.g., data quality or model architecture) play a more significant role.

### Mechanism 3
- Claim: Authority bias in LVLMs is primarily caused by pretraining data instilling varying beliefs in authority figures, leading models to prioritize authoritative statements over their own reasoning.
- Mechanism: During pretraining, LVLMs are exposed to a large corpus of text that includes statements from various authority figures. If the model's training data disproportionately emphasizes the importance of certain authorities, it may develop a tendency to prioritize their statements over its own reasoning, even when contradictory evidence exists.
- Core assumption: Pretraining data significantly influences a model's perception of authority and its tendency to prioritize authoritative statements.
- Evidence anchors:
  - [section]: "We suspect that the authority bias is primarily caused by the different pretraining data instilling varying beliefs in authority figures into the models."
  - [abstract]: "authority bias mitigation appears more challenging."
- Break condition: If the authority bias is primarily driven by factors other than pretraining data (e.g., fine-tuning objectives or architectural choices) or if the model can effectively balance authoritative statements with its own reasoning.

## Foundational Learning

- Concept: Behavioral Finance
  - Why needed here: Understanding behavioral finance is crucial for recognizing and evaluating the psychological biases that can affect decision-making in financial contexts, including those of LVLMs.
  - Quick check question: What are the two main behavioral biases studied in this work, and how do they relate to financial decision-making?

- Concept: Multimodal Learning
  - Why needed here: LVLMs integrate visual and textual information, and understanding how they process and reason with multimodal data is essential for evaluating their performance on financial tasks.
  - Quick check question: How does the DynoStock dataset combine stock price data and EPS reports to create a multimodal learning environment for LVLMs?

- Concept: Prompt Engineering
  - Why needed here: The effectiveness of LVLMs in this study heavily relies on carefully designed prompts that elicit the desired reasoning and output format, highlighting the importance of prompt engineering.
  - Quick check question: What key elements are included in the structured prompt template to ensure LVLMs consider all relevant information and follow the desired output format?

## Architecture Onboarding

- Component map: Data Collection (APIs) -> Bias Data Retrieval -> Prompt Generation -> Multimodal Input Processing -> Model Prediction -> Evaluation (Accuracy & BBI)
- Critical path: Data collection → Bias data retrieval → Prompt generation → Model input → Prediction → Evaluation (accuracy and BBI calculation)
- Design tradeoffs: Using larger window sizes mitigates recency bias but increases computational complexity; carefully curating training data reduces authority bias but requires more resources.
- Failure signatures: High BBI values indicate strong influence of behavioral biases; low accuracy suggests issues with model reasoning or prompt design; inconsistent performance across window sizes may indicate data distribution shifts.
- First 3 experiments:
  1. Evaluate GPT-4o and open-source LVLMs on a small subset of DynoStock data with varying window sizes to confirm the relationship between window size and bias mitigation.
  2. Compare the performance of LVLMs using naive prompts versus structured prompts to quantify the impact of prompt engineering on model reasoning.
  3. Analyze the outputs of authority bias evaluations to identify specific patterns in how LVLMs prioritize authoritative statements over their own reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does pretraining data influence authority bias in LVLMs?
- Basis in paper: [explicit] The paper states "we suspect that the authority bias is primarily caused by the different pretraining data instilling varying beliefs in authority figures into the models"
- Why unresolved: The paper only hypothesizes about the relationship between pretraining data and authority bias without conducting a controlled study to confirm this connection.
- What evidence would resolve it: A controlled experiment comparing LVLMs trained on different authority-focused pretraining data, measuring changes in authority bias behavior.

### Open Question 2
- Question: What is the optimal window size for mitigating recency bias in LVLMs?
- Basis in paper: [explicit] The paper shows that "an increase in window size correlates with a reduction in the bias index" for recency bias, but notes accuracy doesn't necessarily increase.
- Why unresolved: The paper demonstrates the relationship between window size and bias reduction but doesn't identify the point of diminishing returns or optimal trade-off with accuracy.
- What evidence would resolve it: A systematic study varying window sizes across multiple LVLMs to identify the point where additional historical data no longer meaningfully reduces recency bias.

### Open Question 3
- Question: Can authority bias be mitigated through fine-tuning or prompt engineering approaches?
- Basis in paper: [inferred] The paper shows authority bias persists across window sizes and different models, unlike recency bias which can be mitigated by longer historical data.
- Why unresolved: The paper identifies authority bias as more challenging to mitigate than recency bias but doesn't explore potential intervention methods.
- What evidence would resolve it: Experiments testing various fine-tuning strategies, prompt modifications, or architectural changes designed to reduce authority bias in LVLMs.

## Limitations

- The study relies on a single financial dataset (S&P 500 stocks from 2000-2024), which may not capture the full diversity of market conditions and behavioral patterns.
- While the paper claims authority bias stems from pretraining data, it provides limited empirical evidence directly linking specific training data characteristics to observed bias patterns.
- The study evaluates only six LVLMs, with most being relatively recent open-source models, which may not represent the full spectrum of model architectures and capabilities available in the field.

## Confidence

**High Confidence**: The finding that GPT-4o demonstrates significantly lower behavioral biases compared to open-source models is well-supported by the presented experimental results. The quantitative metrics (accuracy percentages and BBI scores) provide clear evidence of this performance gap, and the results are consistent across multiple window sizes.

**Medium Confidence**: The claim that recency bias can be mitigated by increasing historical window size is supported by the experimental data showing a correlation between window size and reduced bias. However, the causal mechanism and whether this relationship holds under different market conditions requires further validation.

**Low Confidence**: The assertion that authority bias stems primarily from pretraining data composition lacks direct empirical support. While the authors provide a plausible theoretical explanation, they do not demonstrate through controlled experiments how different training data characteristics specifically influence authority bias manifestation.

## Next Checks

1. **Cross-market validation**: Evaluate the same LVLMs on non-US stock markets (e.g., European or Asian exchanges) to determine if the observed behavioral bias patterns generalize across different financial markets and regulatory environments.

2. **Pretraining data analysis**: Conduct ablation studies where models are fine-tuned on carefully curated datasets with controlled authority signal distributions to empirically test whether pretraining data composition directly influences authority bias manifestation.

3. **Temporal robustness test**: Replicate the experiments using rolling time windows that exclude the most recent market data to verify whether the observed window size effects on recency bias hold when the most recent information is not available for training or evaluation.