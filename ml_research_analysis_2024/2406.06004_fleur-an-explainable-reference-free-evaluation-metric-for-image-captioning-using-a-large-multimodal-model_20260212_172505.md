---
ver: rpa2
title: 'FLEUR: An Explainable Reference-Free Evaluation Metric for Image Captioning
  Using a Large Multimodal Model'
arxiv_id: '2406.06004'
source_url: https://arxiv.org/abs/2406.06004
tags:
- caption
- image
- fleur
- score
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FLEUR, a reference-free and explainable image
  captioning evaluation metric that leverages a large multimodal model (LMM) to evaluate
  captions without reference captions and provide explanations for the scores. FLEUR
  introduces score smoothing to align scores more closely with human judgment and
  improve robustness to user-defined grading criteria.
---

# FLEUR: An Explainable Reference-Free Evaluation Metric for Image Captioning Using a Large Multimodal Model

## Quick Facts
- arXiv ID: 2406.06004
- Source URL: https://arxiv.org/abs/2406.06004
- Authors: Yebin Lee; Imseong Park; Myungjoo Kang
- Reference count: 12
- Reference-free evaluation metric achieving state-of-the-art correlation with human judgment on multiple benchmark datasets

## Executive Summary
This paper introduces FLEUR, a reference-free evaluation metric for image captioning that uses a large multimodal model (LLaVA) to assess caption quality without requiring reference captions. The method introduces score smoothing to transform discrete probabilities into calibrated continuous scores, improving granularity and alignment with human judgment. FLEUR provides both a quality score and an explanation for the score by directly comparing the caption against the image content, enabling better hallucination detection than previous metrics.

## Method Summary
FLEUR leverages LLaVA to evaluate image captions by constructing prompts that include both the image and caption along with detailed grading criteria (0.0 to 1.0 score definitions). The LMM outputs a score string and explanation, from which FLEUR extracts token probabilities and applies score smoothing to create a continuous calibrated score. The smoothing technique captures the probability distribution over digit tokens at each decimal place and computes a weighted sum to reduce ties and improve granularity. This approach enables reference-free evaluation while providing explainable scores based on direct image-caption comparison.

## Key Results
- Achieves state-of-the-art correlation with human judgment on Flickr8k-CF, COMPOSITE, and Pascal-50S datasets among reference-free metrics
- Introduces score smoothing that aligns scores more closely with human judgment and reduces ties
- Outperforms previous reference-based metrics on object hallucination detection tasks
- Provides better explanations than previous explainable metrics by directly considering image content rather than just reference captions

## Why This Works (Mechanism)

### Mechanism 1: Score Smoothing for Calibration
The method transforms raw discrete probabilities from LLaVA's token output into calibrated continuous scores by computing weighted sums of digit probabilities. This leverages the fine-grained information in token probability distributions to improve score granularity and reduce ties. The core assumption is that probability distributions contain meaningful confidence information that can be calibrated into more accurate scores. Evidence shows improved correlation with human judgment when smoothing is applied.

### Mechanism 2: Grading Criteria Alignment
Explicit grading criteria in prompts (0.0 to 1.0 definitions) guide LLaVA's scoring behavior to align with human judgment. The criteria provide detailed guidelines for evaluating captions at each score level, making the model's output more predictable and consistent. The core assumption is that LMMs are sensitive to explicit instructions and will adjust scoring based on provided criteria. Experiments show that including more detailed criteria improves correlation with human judgment.

### Mechanism 3: Direct Image-Caption Comparison
By feeding both image and caption into the multimodal LMM, FLEUR generates explanations based on actual visual content rather than textual similarity to references. This enables better hallucination detection by identifying objects mentioned in captions but absent from images. The core assumption is that LLaVA has sufficient visual understanding to accurately assess caption-image correspondence. Evidence shows superior performance on object hallucination detection compared to reference-based metrics.

## Foundational Learning

- **Multimodal Large Language Models (LMMs)**: LLaVA processes both image and text simultaneously to evaluate caption quality and generate explanations. *Why needed*: FLEUR relies on LMMs for reference-free evaluation and explainability. *Quick check*: What components make up LLaVA and how do they interact to handle vision-language tasks?

- **Evaluation metrics and correlation with human judgment**: FLEUR's performance is measured by correlation with human judgments on benchmark datasets. *Why needed*: The paper uses Kendall's tau correlation to compare evaluation metrics. *Quick check*: What is Kendall's tau correlation and why is it used to compare evaluation metrics?

- **Prompt engineering and instruction following in LLMs**: FLEUR uses carefully crafted prompts to guide LLaVA's behavior for scoring and explanation generation. *Why needed*: The method depends on LMMs following grading criteria to produce aligned scores. *Quick check*: How do grading criteria in the prompt influence the model's output distribution?

## Architecture Onboarding

- **Component map**: Image + Caption → Prompt → LLaVA → Raw Score + Probabilities → Score Smoothing → Final Score; Image + Caption → Prompt → LLaVA → Explanation
- **Critical path**: Image + Caption → Prompt → LLaVA → Raw Score + Probabilities → Score Smoothing → Final Score; Image + Caption → Prompt → LLaVA → Explanation
- **Design tradeoffs**: LMM enables reference-free evaluation and explainability but increases inference time (0.70s per sample) compared to non-LMM metrics; score smoothing improves granularity but adds computation; explanations increase interpretability but may inherit hallucination issues
- **Failure signatures**: High correlation but poor explainability suggests overfitting to scoring patterns; low correlation indicates misaligned grading criteria or prompts; hallucinated explanations suggest LMM misinterpretation
- **First 3 experiments**: 1) Compare FLEUR's correlation with human judgment using different grading criteria; 2) Measure impact of score smoothing by comparing raw vs. smoothed scores; 3) Test FLEUR's sensitivity to object hallucination on FOIL dataset

## Open Questions the Paper Calls Out

### Open Question 1
How does FLEUR's performance change when using different LMMs with varying model sizes and architectures? The paper mentions FLEUR can be extended to other LMMs but only tests a limited number. A comprehensive study comparing FLEUR across various LMMs would resolve this.

### Open Question 2
How does FLEUR's performance on object hallucination detection compare to other reference-free metrics on larger and more diverse datasets? The paper only tests on FOIL dataset. Evaluating FLEUR on multiple hallucination detection datasets would provide better understanding.

### Open Question 3
How does the choice of grading criteria impact FLEUR's performance and explanation quality? The paper conducts limited ablation studies on grading criteria. A systematic study varying criteria would clarify their impact.

## Limitations
- High computational cost (0.70s per sample) compared to existing metrics limits practical deployment
- Heavy reliance on LLaVA's capabilities means FLEUR inherits any limitations or hallucinations from the base model
- Evaluation focuses primarily on correlation with human judgment rather than other aspects like robustness or cross-dataset generalization

## Confidence

**High Confidence**: FLEUR achieves state-of-the-art correlation with human judgment among reference-free metrics, well-supported by statistically significant results across multiple benchmark datasets.

**Medium Confidence**: Superior explainability is supported qualitatively but lacks comprehensive quantitative evaluation of explanation quality or usefulness.

**Low Confidence**: Claims of outperforming reference-based metrics on hallucination detection are based on limited comparisons and require validation across more comprehensive benchmarks.

## Next Checks

1. Conduct ablation study on score smoothing across all benchmark datasets to quantify its exact contribution to correlation improvements
2. Test cross-dataset generalization by training FLEUR on one dataset and evaluating on held-out datasets
3. Conduct human study evaluating explanation quality, accuracy, and helpfulness compared to ground truth or reference-based metric explanations