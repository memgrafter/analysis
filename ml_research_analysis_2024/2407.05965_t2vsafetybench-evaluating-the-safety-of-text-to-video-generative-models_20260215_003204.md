---
ver: rpa2
title: 'T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models'
arxiv_id: '2407.05965'
source_url: https://arxiv.org/abs/2407.05965
tags:
- video
- prompt
- content
- safety
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces T2VSafetyBench, a new benchmark for evaluating
  the safety of text-to-video (T2V) generative models. The authors identify 12 critical
  safety aspects, including Pornography, Violence, Public Figures, and Temporal Risk,
  and construct a malicious prompt dataset using LLMs and jailbreaking prompt attacks.
---

# T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models

## Quick Facts
- arXiv ID: 2407.05965
- Source URL: https://arxiv.org/abs/2407.05965
- Reference count: 40
- One-line primary result: No single text-to-video model excels in all safety aspects; different models show various strengths and weaknesses.

## Executive Summary
This paper introduces T2VSafetyBench, a comprehensive benchmark for evaluating the safety of text-to-video generative models. The authors identify 12 critical safety aspects and construct a malicious prompt dataset using LLMs and jailbreaking prompt attacks. They evaluate popular T2V models including Pika, Gen2, Stable Video Diffusion, and Open-Sora using both GPT-4 and human assessments. Key findings include the trade-off between usability and safety, with no model excelling across all aspects, and the general but imperfect correlation between GPT-4 and human safety evaluations.

## Method Summary
The method involves three main steps: (1) generating malicious prompts for 12 safety aspects using GPT-4 and jailbreaking attacks (RAB, JPA, BSPA), (2) evaluating popular T2V models by generating videos from these prompts and assessing safety through both GPT-4 (using multi-frame inputs) and human validation, and (3) calculating NSFW rates and correlation coefficients between automated and human evaluations. The system captures one frame per second from each generated video and uses structured prompts to guide GPT-4's safety assessment.

## Key Results
- No single T2V model excels in all safety aspects, with different models showing various strengths
- GPT-4 assessments generally correlate well with human evaluations (correlation coefficients exceeding 0.8) but show significant divergence on "Disturbing Content"
- A trade-off exists between model capability and safety, where weaker models may be safer due to limited generative ability

## Why This Works (Mechanism)

### Mechanism 1
LLM-generated prompts combined with jailbreak attacks increase safety test case diversity and severity. The system first uses GPT-4 to produce baseline malicious prompts, then applies attack methods like Ring-A-Bell, Jailbreaking Prompt Attack, and Black-box Stealthy Prompt Attack to introduce harmful concepts while maintaining semantic coherence. This assumes original LLM prompts are somewhat sanitized and attack methods are needed to expose deeper vulnerabilities.

### Mechanism 2
GPT-4 can reliably evaluate T2V safety using multi-frame image inputs and structured output prompts. For each video, the system extracts one frame per second and feeds these frames plus the original prompt to GPT-4, asking it to classify safety and score confidence. This leverages GPT-4's cross-modal understanding of both text and images, assuming its understanding is sufficient for safety judgment.

### Mechanism 3
Safety risk in T2V models shows a trade-off with model capability. Models with limited generative ability fail to produce complex or abstract unsafe content, thus scoring safer in certain aspects. More capable models can generate nuanced unsafe content, increasing risk. This assumes the inability to generate complex content translates to lower safety risk.

## Foundational Learning

- **Diffusion models and their vulnerabilities to adversarial prompts**: Why needed here - The system evaluates T2V models built on diffusion models, which are known to be vulnerable to prompt-based attacks. Quick check question: How do diffusion models generate images/videos from text, and what makes them susceptible to jailbreak attacks?

- **Multimodal evaluation using large language models**: Why needed here - GPT-4 is used to evaluate video safety by analyzing both text prompts and video frames. Quick check question: What are the challenges in using LLMs to evaluate safety across text and image modalities?

- **Safety aspect categorization and definition**: Why needed here - The system defines 12 safety aspects, some unique to video (e.g., Temporal Risk), requiring understanding of what makes video content unsafe. Quick check question: Why is Temporal Risk a unique safety concern for videos that doesn't apply to static images?

## Architecture Onboarding

- **Component map**: Data generation (LLM + attacks) → Video generation → Frame extraction → Multimodal safety evaluation (GPT-4 + human) → Correlation analysis
- **Critical path**: Prompt generation (LLM + attacks) → Video generation → Frame extraction → GPT-4 safety evaluation → Human correlation check
- **Design tradeoffs**: Using GPT-4 for evaluation provides scalability but may not fully align with human judgment; human validation is more accurate but less scalable
- **Failure signatures**: Low correlation between GPT-4 and human evaluations indicates potential issues with the evaluation methodology; models showing high NSFW rates across all aspects suggest fundamental safety vulnerabilities
- **First 3 experiments**:
  1. Generate prompts for one safety aspect using GPT-4, then enhance with RAB attack method, compare generated video safety
  2. Test GPT-4's ability to evaluate safety by providing frames and prompts for a known safe and unsafe video, compare with human judgment
  3. Compare safety evaluation results between a high-capability model (Pika) and a lower-capability model (Open-Sora) on the same prompts to observe the capability-safety tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of GPT-4 evaluations compare to human assessments across different safety aspects in T2VSafetyBench, and what are the limitations of automated metrics? The paper states that the correlation between GPT-4 assessments and manual reviews is generally high, with correlation coefficients exceeding 0.8 in most dimensions, but there is significant divergence in the dimension of Disturbing Content, where the correlation coefficient is only 0.589. Further research is needed to understand the specific limitations of automated metrics and to develop better automatic evaluation methods that excel across multiple safety aspects.

### Open Question 2
How does the trade-off between the accessibility and safety of text-to-video generative models evolve as model capabilities advance, and what are the implications for video safety? The paper discusses a trade-off between accessibility and safety, noting that models with weaker comprehension and generation capabilities may inadvertently enhance safety by failing to capture abstract and complex aspects of safety risks. However, as video generation technology advances and model capabilities strengthen, safety risks are likely to surge. Further research is needed to understand how this trade-off will evolve and to develop strategies for prioritizing video safety.

### Open Question 3
What are the most effective methods for mitigating the unique temporal risks associated with video generation, and how can these methods be integrated into existing text-to-video models? The paper introduces Temporal Risk as a unique safety dimension associated with videos, where individual frames may appear harmless but the entire sequence can convey inappropriate content or themes through interaction, continuity, or dynamic changes between frames. However, the paper does not provide specific methods for mitigating these risks. Further research is needed to develop effective methods for mitigating these risks and to integrate these methods into existing text-to-video models.

## Limitations
- The evaluation relies heavily on GPT-4's ability to accurately assess video safety, which may not fully capture human judgment nuances, particularly for abstract concepts like "Disturbing Content"
- Some T2V models struggled to generate content for more abstract safety aspects, potentially understating actual safety risks
- The system only evaluates single-prompt, single-aspect cases, missing potential compounding effects when multiple safety concerns appear simultaneously

## Confidence
- **High**: Overall methodology and framework design
- **Medium**: Reliability of GPT-4 as a safety evaluator
- **Medium**: Comprehensiveness of the safety aspects identified
- **Low**: Generalizability of results to future, more capable T2V models

## Next Checks
1. Conduct a targeted evaluation of GPT-4's safety judgment reliability by having multiple human annotators independently assess a subset of generated videos across all 12 safety aspects, measuring inter-annotator agreement and comparing with GPT-4 scores to identify specific failure patterns.

2. Test the system's ability to detect compound safety violations by generating multi-aspect prompts (e.g., combining violence with political sensitivity) and evaluating whether the current framework can effectively identify multiple simultaneous safety concerns in single videos.

3. Validate the temporal risk assessment methodology by conducting a longitudinal study tracking how safety violations evolve as T2V models advance over time, specifically testing whether the identified trade-off between capability and safety holds as models improve.