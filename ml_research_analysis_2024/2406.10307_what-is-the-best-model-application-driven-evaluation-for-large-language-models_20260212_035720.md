---
ver: rpa2
title: What is the best model? Application-driven Evaluation for Large Language Models
arxiv_id: '2406.10307'
source_url: https://arxiv.org/abs/2406.10307
tags:
- evaluation
- best
- tasks
- accuracy
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces A-Eval, an application-driven benchmark for
  evaluating large language models (LLMs) based on practical task requirements. The
  authors categorize tasks into 5 main categories and 27 sub-categories, then construct
  a dataset of 678 question-and-answer pairs across three difficulty levels.
---

# What is the best model? Application-driven Evaluation for Large Language Models

## Quick Facts
- arXiv ID: 2406.10307
- Source URL: https://arxiv.org/abs/2406.10307
- Reference count: 20
- This paper introduces A-Eval, an application-driven benchmark for evaluating large language models based on practical task requirements.

## Executive Summary
This paper addresses the practical challenge of selecting the optimal large language model (LLM) for specific tasks. Rather than assuming bigger is always better, the authors develop A-Eval, a benchmark with 678 question-answer pairs across 5 categories and 27 sub-categories, organized by difficulty (easy/medium/hard). Through zero-shot evaluation of models ranging from 0.5B to 110B parameters, they demonstrate that the best model depends on both task difficulty and desired accuracy - smaller models suffice for easy tasks while harder tasks require larger models. The study provides a practical framework for model selection that balances performance requirements with computational efficiency.

## Method Summary
The paper constructs the A-Eval dataset with 678 QA pairs across 5 main categories and 27 sub-categories, annotated with difficulty levels (easy/medium/hard). Models ranging from 0.5B to 110B parameters are evaluated using zero-shot approaches with both expert human evaluation (15 experts) and automatic evaluation using a scoring model (Qwen1.5-72B-Chat). Accuracy is calculated as the proportion of correct responses for each model-difficulty-task combination. The authors then propose a model selection method that recommends the smallest model meeting specified accuracy thresholds for given task difficulty levels.

## Key Results
- Model size requirements scale with task difficulty: easy tasks work well with 0.5B-7B models, medium tasks need 7B-14B, and hard tasks require 14B-110B models
- The greatest accuracy improvement for easy tasks occurs when parameters increase from 0.5B to 7B, extending to 14B for medium and 110B for hard tasks
- Expert evaluation results closely align with automatic evaluation (threshold 90), validating the benchmark's reliability
- The optimal model isn't always the largest but rather the smallest that meets accuracy requirements for specific tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task difficulty modulates the optimal model size through the shape of the accuracy improvement curve.
- Mechanism: The paper shows that for easy tasks, accuracy improves substantially up to 7B parameters, then plateaus; for medium tasks, improvement continues to 14B; for hard tasks, improvement continues to 110B. This means each difficulty level has a "sweet spot" where additional parameters yield diminishing returns.
- Core assumption: The relationship between model size and accuracy is monotonically increasing but with decreasing marginal returns, and this relationship varies by difficulty level.
- Evidence anchors:
  - "For easy data, the greatest increase in accuracy occurs when the number of parameters increases from 0.5B to 7B. This range extends to 4B to 14B for medium data and 14B to 110B for hard data."
  - "For easy data, the rate of accuracy improvement diminishes significantly when the model size exceeds 7B."

### Mechanism 2
- Claim: Expert and automatic evaluation methods validate each other, ensuring benchmark reliability.
- Mechanism: The paper uses both expert human evaluation (15 experts) and automatic evaluation using a scoring model (Qwen1.5-72B-Chat). The results show close alignment when the scoring threshold is 90, indicating both methods are measuring the same underlying model capability.
- Core assumption: Both expert and automatic evaluation methods are measuring the same construct (model accuracy) with sufficient reliability.
- Evidence anchors:
  - "Accuracy results from expert evaluation closely align with those from automatic evaluation with a scoring threshold of 90, validating the reliability of our dataset and automatic evaluation."
  - "Our evaluation team consists of L experts, represented as E = {E1, E2, ..., Ej, ..., EL}. These evaluation experts can refer to all relevant materials during the evaluation process."

### Mechanism 3
- Claim: Zero-shot evaluation provides realistic guidance for practical LLM deployment.
- Mechanism: The paper uses only zero-shot evaluation because real-world users typically don't have access to task-specific examples for few-shot learning. This makes the benchmark results directly applicable to actual usage scenarios.
- Core assumption: Zero-shot performance is the primary metric that matters for users selecting models for new, unseen tasks.
- Evidence anchors:
  - "We evaluate models using only the zero-shot approach for two primary reasons. Firstly, the chat models being evaluated have already been fine-tuned using SFT, allowing them to follow human instructions."
  - "in real-world usage scenarios, users usually lack QA pairs for few-shot learning."

## Foundational Learning

- Concept: Scaling laws in language models
  - Why needed here: The paper's core contribution relies on understanding how model performance scales with parameter count across different task difficulties.
  - Quick check question: What does the scaling law predict about model performance as parameter count increases?

- Concept: Zero-shot vs few-shot evaluation
  - Why needed here: The paper explicitly chooses zero-shot evaluation as more realistic for practical applications.
  - Quick check question: What is the key difference between zero-shot and few-shot evaluation methods?

- Concept: Accuracy as evaluation metric
  - Why needed here: The paper uses accuracy to compare models across different scales and task difficulties.
  - Quick check question: How is accuracy calculated in this evaluation framework?

## Architecture Onboarding

- Component map:
  Data collection module (678 QA pairs across 5 categories, 27 sub-categories) -> Difficulty annotation system (easy/medium/hard) -> Expert review pipeline (15 experts scoring each sample) -> Automatic evaluation module (Qwen1.5-72B-Chat scoring model) -> Model evaluation engine (8 models from 0.5B to 110B parameters) -> Model selection algorithm (based on desired accuracy thresholds)

- Critical path:
  1. Collect and annotate QA pairs
  2. Expert review and validation
  3. Run zero-shot evaluation on all models
  4. Compute accuracy for each model-difficulty-task combination
  5. Apply model selection algorithm based on user requirements

- Design tradeoffs:
  - Zero-shot vs few-shot: Chosen zero-shot for practical realism, but this may underestimate some models' capabilities
  - Expert vs automatic evaluation: Both used to validate results, but automatic evaluation is more scalable
  - Task granularity: 27 sub-categories provide detailed analysis but increase complexity

- Failure signatures:
  - Large accuracy gaps between expert and automatic evaluation (>10%)
  - No clear relationship between model size and accuracy for certain task-difficulty combinations
  - Model selection algorithm producing counterintuitive results

- First 3 experiments:
  1. Verify accuracy calculation by manually checking 10 random samples
  2. Test sensitivity of model selection to scoring threshold changes
  3. Validate that smaller models indeed plateau earlier than larger models for each difficulty level

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on A-Eval when evaluated using other automatic evaluation metrics beyond accuracy, such as F1-score, precision, or recall?
- Basis in paper: [inferred] The paper only reports accuracy as the evaluation metric, but other metrics could provide additional insights into model performance.
- Why unresolved: The paper does not explore or report results using alternative evaluation metrics, leaving open the question of how model rankings or insights might change with different metrics.
- What evidence would resolve it: Evaluating the same models on A-Eval using metrics like F1-score, precision, recall, or others, and comparing the results to the accuracy-based findings.

### Open Question 2
- Question: How well does the proposed model selection method generalize to LLMs from other architectures or training processes beyond the Qwen1.5-Chat series?
- Basis in paper: [explicit] The paper states that controlling for factors like model architecture, training process, and training data is important for fair evaluation, but only tests the selection method on Qwen1.5-Chat models.
- Why unresolved: The method is only validated on one model family, so its applicability to other architectures or training approaches remains unknown.
- What evidence would resolve it: Applying the model selection method to evaluate and select among LLMs from diverse architectures (e.g., transformers, recurrent networks) and training processes (e.g., different fine-tuning strategies), and assessing the method's effectiveness and generalizability.

### Open Question 3
- Question: How does the optimal model size for a given task and accuracy requirement change as the size and diversity of the training data increase?
- Basis in paper: [inferred] The paper holds training data constant while varying model size, but scaling laws suggest model performance improves with more data, which could shift the optimal model size.
- Why unresolved: The experiments only explore the relationship between model size and task performance for a fixed training dataset, not how this relationship changes with different amounts or types of training data.
- What evidence would resolve it: Conducting experiments that vary both model size and the size/diversity of the training data, and analyzing how the optimal model size for achieving a target accuracy changes across different training data regimes.

## Limitations

- The study's reliance on zero-shot evaluation may underestimate model capabilities compared to few-shot approaches
- The automatic evaluation using Qwen1.5-72B-Chat as a scoring model introduces potential bias due to architectural similarities
- The expert evaluation involved only 15 experts whose selection criteria and potential biases are not fully disclosed

## Confidence

- **High confidence**: The finding that model size requirements scale with task difficulty (easy tasks work well with smaller models, hard tasks need larger models). This is supported by clear patterns across multiple evaluation methods and task categories.
- **Medium confidence**: The practical model selection algorithm, which assumes users can accurately estimate task difficulty and desired accuracy thresholds. Real-world application may be more complex.
- **Medium confidence**: The reliability of automatic evaluation using Qwen1.5-72B-Chat, which shows good correlation with expert evaluation but may have systematic biases not captured in the study.

## Next Checks

1. **Cross-model evaluation consistency**: Test whether the automatic evaluation method produces consistent results when using different scoring models (e.g., GPT-4, Claude) to ensure the validation isn't model-specific.

2. **Few-shot capability comparison**: Re-run the evaluation using few-shot learning for a subset of tasks to quantify the performance gap and determine if the zero-shot findings hold when additional examples are available.

3. **Real-world deployment testing**: Apply the model selection algorithm to three real-world use cases (e.g., customer service, document summarization, code generation) and measure whether the recommended model sizes actually meet accuracy requirements in practice.