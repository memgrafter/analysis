---
ver: rpa2
title: 'PRCL: Probabilistic Representation Contrastive Learning for Semi-Supervised
  Semantic Segmentation'
arxiv_id: '2402.18117'
source_url: https://arxiv.org/abs/2402.18117
tags:
- representations
- learning
- representation
- segmentation
- prototype
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of inaccurate pseudo-labels
  in semi-supervised semantic segmentation by proposing a robust contrastive learning
  framework. The core idea is to model pixel-wise representations as probabilistic
  representations using multivariate Gaussian distributions, allowing the model to
  tolerate the risk of inaccurate guidance.
---

# PRCL: Probabilistic Representation Contrastive Learning for Semi-Supervised Semantic Segmentation

## Quick Facts
- **arXiv ID**: 2402.18117
- **Source URL**: https://arxiv.org/abs/2402.18117
- **Reference count**: 40
- **Primary result**: Achieves superior performance on PASCAL VOC 2012 and Cityscapes datasets with mIoU improvements of 1.1-2.4% over state-of-the-art methods

## Executive Summary
This paper addresses the challenge of inaccurate pseudo-labels in semi-supervised semantic segmentation by proposing a robust contrastive learning framework. The core idea is to model pixel-wise representations as probabilistic representations using multivariate Gaussian distributions, allowing the model to tolerate the risk of inaccurate guidance. Additionally, the paper introduces Global Distribution Prototypes (GDP) to maintain prototype consistency and Virtual Negatives (VNs) to compensate for the fragmentary negative distribution. Extensive experiments on PASCAL VOC 2012 and Cityscapes datasets demonstrate the effectiveness of the proposed method, achieving superior performance compared to state-of-the-art approaches.

## Method Summary
The PRCL framework employs a teacher-student paradigm with DeepLabv3+ architecture, incorporating probabilistic representations through multivariate Gaussian distributions for each pixel. The method uses a probability head to estimate variance for each representation, enabling the Mutual Likelihood Score (MLS) to down-weight uncertain pixels during contrastive learning. Global Distribution Prototypes (GDP) aggregate representations across training iterations using Bayesian estimation to maintain stable reference points. Virtual Negatives (VNs) are generated from GDP distributions using a reparameterization trick, providing diverse negative samples without expensive memory banks. The framework trains with standard cross-entropy loss for labeled images and weighted cross-entropy for unlabeled images, combined with InfoNCE contrastive loss using MLS similarity metric.

## Key Results
- Achieves 1.1-2.4% mIoU improvement over state-of-the-art semi-supervised semantic segmentation methods
- Demonstrates effectiveness on both PASCAL VOC 2012 and Cityscapes datasets
- Shows robustness to varying levels of label noise through probabilistic representation modeling
- Reduces memory usage compared to memory bank approaches (42KB vs 2.63GB)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic representations improve robustness by down-weighting ambiguous pixels during contrastive learning.
- Mechanism: Modeling each pixel's representation as a multivariate Gaussian (µ, σ²) allows the model to assign lower confidence to ambiguous regions. The Mutual Likelihood Score (MLS) in the contrastive loss incorporates σ² to reduce the contribution of uncertain pixels, preventing them from disrupting the learning process.
- Core assumption: Pixel-wise representations can be effectively modeled as Gaussian distributions, and the variance captures meaningful uncertainty.
- Evidence anchors:
  - [abstract]: "We model the pixel-wise representation as Probabilistic Representations (PR) via multivariate Gaussian distribution and tune the contribution of the ambiguous representations to tolerate the risk of inaccurate guidance in contrastive learning."
  - [section]: "The MLS incorporates the probabilities of zi and zj to account for inaccurate pseudo-labels from two perspectives: (i): In the first term, the weight of ℓ2 distance is reduced when the σ2 is large. This indicates that the similarity between zi and zj decreases due to the low probabilities, even if their ℓ2 distance suggests they are similar."
- Break condition: If the variance estimation fails to capture true uncertainty, or if the Gaussian assumption is invalid for the data distribution.

### Mechanism 2
- Claim: Global Distribution Prototypes (GDP) provide consistent reference points across training iterations, preventing prototype shift.
- Mechanism: Instead of computing prototypes from the current mini-batch only, GDP aggregates representations across all training iterations using Bayesian estimation. This creates stable prototype distributions that are robust to instant noise and intra-class variance.
- Core assumption: Historical representations contain valuable semantic information that should be preserved and aggregated over time.
- Evidence anchors:
  - [abstract]: "Furthermore, we introduce Global Distribution Prototypes (GDP) by gathering all PRs throughout the whole training process. Since the GDP contains the information of all representations with the same class, it is robust from the instant noise in representations and bears the intra-class variance of representations."
- Break condition: If the aggregation becomes too memory-intensive or if historical representations become stale/distorted over time.

### Mechanism 3
- Claim: Virtual Negatives (VNs) compensate for fragmentary negative distribution without expensive memory banks.
- Mechanism: VNs are generated from GDP distributions using a reparameterization trick, creating compact yet diverse negative samples that cover all classes globally rather than just the current mini-batch.
- Core assumption: Generated samples from global prototype distributions can effectively substitute for real negative representations.
- Evidence anchors:
  - [abstract]: "In addition, we generate Virtual Negatives (VNs) based on GDP to involve the contrastive learning process."
- Break condition: If generated VNs fail to provide sufficient diversity or if the generation process introduces artifacts that mislead the model.

## Foundational Learning

- Concept: Multivariate Gaussian distributions for uncertainty modeling
  - Why needed here: To represent both the mean representation and its associated uncertainty, allowing the model to down-weight ambiguous pixels during contrastive learning
  - Quick check question: How does modeling representation as N(µ, σ²) differ from traditional deterministic representations, and why is this beneficial for handling noisy pseudo-labels?

- Concept: Contrastive learning with probabilistic similarity metrics
  - Why needed here: Standard ℓ2 distance or cosine similarity don't account for uncertainty; the Mutual Likelihood Score incorporates variance to measure similarity between probabilistic representations
  - Quick check question: What components make up the Mutual Likelihood Score, and how does it differ from standard distance metrics in handling uncertain representations?

- Concept: Bayesian estimation for prototype aggregation
- Why needed here: To create stable global prototypes that aggregate information across training iterations rather than being affected by instant noise in any single batch
- Quick check question: How does the GDP update equation combine current local prototypes with historical information, and what advantage does this provide over simple moving averages?

## Architecture Onboarding

- Component map: Input → Student encoder → Representation heads → Probabilistic representations → Contrastive loss with GDP and VNs → Segmentation output → Teacher model for pseudo-labels

- Critical path: Input → Student encoder → Representation heads → Probabilistic representations → Contrastive loss with GDP and VNs → Segmentation output → Teacher model for pseudo-labels

- Design tradeoffs:
  - Probabilistic representation adds computational overhead but improves robustness to noise
  - GDP requires additional memory for storing prototype distributions but provides stability
  - VNs reduce memory usage compared to memory banks but require careful tuning of virtual radius parameter

- Failure signatures:
  - Poor performance with very noisy pseudo-labels: check if probability head is learning meaningful variance
  - Degraded performance over long training: check if GDP becomes stale or memory-limited
  - Inconsistent results across runs: check virtual radius β and VN generation parameters

- First 3 experiments:
  1. Replace probabilistic representation with deterministic representation and measure performance drop
  2. Disable GDP update strategy and use only current-batch prototypes to observe prototype shift effects
  3. Replace VNs with memory bank approach and compare memory usage and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Probabilistic Representation (PR) approach compare to other uncertainty estimation methods, such as Bayesian neural networks or Monte Carlo dropout, in terms of handling inaccurate pseudo-labels in semi-supervised semantic segmentation?
- Basis in paper: [inferred] The paper introduces PR as a way to model pixel-wise representations as probabilistic representations using multivariate Gaussian distributions, which allows the model to tolerate the risk of inaccurate guidance. However, it does not directly compare PR to other uncertainty estimation methods.
- Why unresolved: The paper focuses on the effectiveness of PR in the context of contrastive learning for semi-supervised semantic segmentation, but does not explore its performance compared to other uncertainty estimation techniques.
- What evidence would resolve it: A comparative study evaluating PR against other uncertainty estimation methods, such as Bayesian neural networks or Monte Carlo dropout, on semi-supervised semantic segmentation tasks with varying levels of label noise.

### Open Question 2
- Question: Can the Global Distribution Prototypes (GDP) and Virtual Negatives (VN) strategies be effectively applied to other self-supervised learning tasks beyond semi-supervised semantic segmentation?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of GDP and VN in maintaining prototype consistency and compensating for fragmentary negative distribution in the context of semi-supervised semantic segmentation. However, it does not explore their applicability to other self-supervised learning tasks.
- Why unresolved: The paper focuses on the specific application of GDP and VN to semi-supervised semantic segmentation, but does not investigate their potential in other self-supervised learning domains.
- What evidence would resolve it: Experiments applying GDP and VN to other self-supervised learning tasks, such as image classification or object detection, and comparing their performance against existing methods in those domains.

### Open Question 3
- Question: How does the performance of the proposed PRCL framework scale with the size of the unlabeled dataset and the diversity of the classes present in the dataset?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of PRCL on two public benchmarks, PASCAL VOC 2012 and Cityscapes, with varying amounts of labeled data. However, it does not explore the scalability of PRCL with respect to the size of the unlabeled dataset and the diversity of classes.
- Why unresolved: The paper focuses on the performance of PRCL with limited labeled data, but does not investigate how the framework scales with larger unlabeled datasets and more diverse class distributions.
- What evidence would resolve it: Experiments evaluating PRCL on datasets with varying sizes of unlabeled data and different levels of class diversity, and analyzing the impact of these factors on the framework's performance.

## Limitations

- Computational overhead from probabilistic representations and GDP maintenance may limit scalability to larger datasets
- Virtual radius parameter β requires careful tuning and may not generalize across different datasets
- The Gaussian assumption for pixel-wise representations may not hold for all types of data distributions

## Confidence

- **High confidence**: The experimental results demonstrating superior performance on PASCAL VOC 2012 and Cityscapes datasets (mIoU improvements of 1.1-2.4% over state-of-the-art methods)
- **Medium confidence**: The theoretical framework of probabilistic representations and Mutual Likelihood Score for handling uncertainty in contrastive learning
- **Medium confidence**: The GDP aggregation strategy and VN generation methodology, as these appear novel with limited ablation study details

## Next Checks

1. **Sensitivity analysis**: Systematically vary the virtual radius parameter β and measure its impact on both performance and memory usage to establish optimal ranges
2. **Ablation study**: Implement a version without probabilistic representations (using deterministic representations only) to quantify the specific contribution of uncertainty modeling to overall performance gains
3. **Scalability test**: Evaluate the method on a larger dataset (e.g., ADE20K) to assess computational overhead and memory requirements of GDP maintenance at scale