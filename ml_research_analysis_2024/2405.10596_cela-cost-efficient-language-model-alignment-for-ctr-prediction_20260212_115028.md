---
ver: rpa2
title: 'CELA: Cost-Efficient Language Model Alignment for CTR Prediction'
arxiv_id: '2405.10596'
source_url: https://arxiv.org/abs/2405.10596
tags:
- cela
- item
- features
- text
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CELA is a cost-efficient framework for click-through rate prediction
  that aligns pre-trained language models with ID-based models to leverage textual
  features while maintaining collaborative filtering capabilities. The method addresses
  cold-start limitations of ID-based models by integrating item textual features through
  domain-adaptive pre-training and item-level alignment, reducing inference latency
  by caching aligned text representations.
---

# CELA: Cost-Efficient Language Model Alignment for CTR Prediction

## Quick Facts
- arXiv ID: 2405.10596
- Source URL: https://arxiv.org/abs/2405.10596
- Reference count: 40
- Primary result: CELA achieves 0.8996 AUC on Amazon dataset, outperforming best baseline DIN (0.8949) while maintaining comparable training and inference costs

## Executive Summary
CELA is a cost-efficient framework for click-through rate (CTR) prediction that aligns pre-trained language models (PLMs) with ID-based models to leverage textual features while maintaining collaborative filtering capabilities. The framework addresses cold-start limitations by integrating item textual features through domain-adaptive pre-training and item-level alignment, reducing inference latency by caching aligned text representations. Experiments on public and industrial datasets show significant performance improvements, with online A/B testing demonstrating a 1.48% increase in eCPM and 0.93% increase in DTR on an industrial advertising platform.

## Method Summary
CELA operates through three stages: Domain-Adaptive Pre-training (DAP) further pre-trains a PLM on dataset-specific item texts using MLM and SimCSE losses; Recommendation-Oriented Modal Alignment (ROMA) aligns PLM text representations with ID-based embeddings at the item level using contrastive learning; and Multi-Modal Feature Fusion (MF2) integrates aligned text representations with non-textual features for final CTR prediction. The framework maintains efficiency by pre-computing and caching text embeddings in a lookup table, eliminating real-time PLM processing during inference while preserving collaborative filtering signals from ID-based models.

## Key Results
- CELA achieves 0.8996 AUC on Amazon dataset, outperforming best baseline DIN (0.8949)
- Significant improvements in cold-start scenarios by leveraging item textual features
- Online A/B testing shows 1.48% increase in eCPM and 0.93% increase in DTR on industrial platform

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CELA addresses cold-start limitations by leveraging item textual features through domain-adaptive pre-training and item-level alignment
- Mechanism: The framework uses PLMs to extract valuable information from item descriptions, even when historical interactions are sparse, and aligns these text representations with ID-based embeddings
- Core assumption: Item textual features contain sufficient semantic information to compensate for limited interaction data in cold-start scenarios
- Evidence anchors:
  - [abstract]: "CELA is a cost-efficient framework for click-through rate prediction that aligns pre-trained language models with ID-based models to leverage textual features while maintaining collaborative filtering capabilities. The method addresses cold-start limitations of ID-based models by integrating item textual features through domain-adaptive pre-training and item-level alignment"
  - [section]: "To mitigate cold-start issues, CELA expands its feature scope by leveraging item text features and PLMs to generate more accurate item and user profiles"
  - [corpus]: Weak - The corpus contains related papers but doesn't directly validate the cold-start mechanism
- Break condition: If item textual features lack sufficient semantic information to differentiate items in cold-start scenarios, the alignment would fail to provide meaningful improvements

### Mechanism 2
- Claim: CELA maintains training and inference efficiency by caching aligned text representations and avoiding real-time PLM processing
- Mechanism: The framework pre-computes item text embeddings during the ROMA stage and stores them in a text embedding table, eliminating the need for real-time PLM processing during inference
- Core assumption: Item textual features are relatively static, making it feasible to pre-compute and cache their embeddings
- Evidence anchors:
  - [abstract]: "reducing inference latency by caching aligned text representations"
  - [section]: "To reduce training time and online latency, we create an item text embedding table. Text for each item is processed through the aligned PLM to obtain the embedding, and stored for quick access by item identifiers, eliminating the need for real-time PLM processing"
  - [corpus]: Weak - The corpus doesn't provide direct evidence about inference efficiency mechanisms
- Break condition: If item textual features change frequently, the cached embeddings would become stale, requiring frequent updates that negate the efficiency benefits

### Mechanism 3
- Claim: CELA achieves superior performance by combining collaborative filtering signals with semantic information from PLMs through item-level alignment
- Mechanism: The framework aligns PLM text representations with ID-based embeddings at the item level, allowing both collaborative signals and semantic information to be leveraged simultaneously
- Core assumption: Item-level alignment can effectively bridge the gap between PLM outputs and collaborative filtering requirements
- Evidence anchors:
  - [abstract]: "CELA incorporates item textual features and language models while preserving the collaborative filtering capabilities of ID-based models"
  - [section]: "Contrastive learning is then utilized to align the representations of textual and non-textual features on the item side"
  - [corpus]: Weak - The corpus contains related work on multimodal fusion but doesn't directly validate the item-level alignment mechanism
- Break condition: If the alignment between PLM representations and ID embeddings is poor, the combined model would underperform both pure ID-based and pure text-based approaches

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Used in the ROMA stage to align PLM text representations with ID-based embeddings at the item level
  - Quick check question: What is the key difference between instance-level and item-level contrastive learning in the context of CTR prediction?

- Concept: Domain-adaptive pre-training
  - Why needed here: The DAP stage further pre-trains PLMs on dataset-specific item texts to improve understanding of domain-specific language
  - Quick check question: Why is domain-adaptive pre-training necessary when using pre-trained language models for CTR prediction?

- Concept: Embedding tables and lookup operations
  - Why needed here: Fundamental to both ID-based models (for categorical features) and CELA's text embedding table (for cached PLM outputs)
  - Quick check question: How does the size of the text embedding table scale with the number of items in the catalog?

## Architecture Onboarding

- Component map:
  DAP stage -> ROMA stage -> MF2 stage -> Text embedding table -> CTR prediction

- Critical path: Text embedding table → MF2 stage → CTR prediction
  The text embedding table must be pre-computed and available for efficient inference

- Design tradeoffs:
  - Item-level vs. interaction-level alignment: Item-level is more efficient but may miss interaction-specific nuances
  - Pre-computed vs. real-time PLM processing: Pre-computed is faster but may become stale if item texts change frequently
  - Backbone model selection: Different ID-based backbones may require different alignment strategies

- Failure signatures:
  - Poor performance on cold-start items: Indicates alignment between PLM and ID embeddings is ineffective
  - High inference latency: Suggests text embedding table is not being utilized properly
  - Degraded performance on popular items: May indicate overfitting to textual features at the expense of collaborative signals

- First 3 experiments:
  1. Measure performance impact of removing the DAP stage on cold-start vs. popular items
  2. Compare item-level alignment vs. interaction-level alignment on a small dataset
  3. Test inference latency with and without text embedding table caching

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CELA's performance scale with extremely large language models (e.g., 10B+ parameters) in CTR prediction tasks?
- Basis in paper: [explicit] The paper evaluates CELA with various PLM sizes including BERTTiny (14.5M), BERTSmall (29.1M), BERTMedium (41.7M), RoBERTaBase (110M), BERTLarge (336M), and OPT (1.3B), but does not explore models larger than 1.3B parameters
- Why unresolved: The authors observe diminishing returns with larger models and note that fine-tuning larger models with limited data is challenging, but do not test the upper bounds of scalability
- What evidence would resolve it: Experimental results comparing CELA's performance and efficiency with state-of-the-art extremely large language models (e.g., GPT-3, PaLM) on CTR prediction tasks, including training time, inference latency, and performance metrics

### Open Question 2
- Question: What is the optimal frequency for updating the text embedding table in CELA for different types of applications with varying content update rates?
- Basis in paper: [explicit] The paper mentions that the text embedding table is updated weekly for the DAP stage and daily for the ROMA and MF2 stages in the industrial app advertising platform, but does not explore different update frequencies or their impact on performance
- Why unresolved: The paper establishes a specific update schedule for the industrial application but does not investigate how different update frequencies affect performance, storage costs, or computational efficiency across various use cases
- What evidence would resolve it: Comparative analysis showing CTR prediction performance, computational costs, and storage requirements when varying the update frequencies (hourly, daily, weekly, monthly) across different applications with different content update rates

### Open Question 3
- Question: How does CELA perform in multi-modal scenarios where item features include images, audio, or video alongside text?
- Basis in paper: [inferred] The paper focuses exclusively on textual features and their integration with ID-based models, but mentions that ID-based models overlook other modalities like text which may harbor untapped knowledge
- Why unresolved: While the paper demonstrates CELA's effectiveness with textual features, it does not explore how the framework could be extended to handle multiple modalities simultaneously, which is common in many real-world recommendation scenarios
- What evidence would resolve it: Experimental results comparing CELA's performance when extended to handle multiple modalities (text, image, audio, video) versus single-modality approaches, including analysis of computational costs and performance trade-offs

## Limitations
- Framework depends on relatively static item textual features, limiting applicability to dynamic content domains
- Lack of detailed ablation studies on individual components to quantify their individual contributions
- No direct comparison with real-time PLM processing approaches to validate claimed efficiency gains

## Confidence
- High confidence: Core claim that CELA improves CTR prediction performance by integrating textual features with ID-based models
- Medium confidence: Efficiency claims regarding inference latency reduction
- Low confidence: Generalizability to domains with rapidly changing item descriptions

## Next Checks
1. **Cold-start validation**: Conduct controlled experiments comparing CELA's performance on items with zero interactions versus items with rich interaction histories to quantify the cold-start improvement margin.

2. **Embedding staleness analysis**: Simulate scenarios where item textual features change over time and measure the performance degradation when using cached embeddings versus real-time PLM processing.

3. **Efficiency benchmarking**: Implement a variant of CELA that uses real-time PLM processing and compare inference latency and throughput against the cached approach across varying catalog sizes to validate the claimed efficiency benefits.