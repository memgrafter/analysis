---
ver: rpa2
title: 'Mind the truncation gap: challenges of learning on dynamic graphs with recurrent
  architectures'
arxiv_id: '2412.21046'
source_url: https://arxiv.org/abs/2412.21046
tags:
- graph
- learning
- each
- dynamic
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical limitation in Graph Recurrent
  Neural Networks (GRNNs) for dynamic graphs: truncated backpropagation-through-time
  (T-BPTT) prevents learning dependencies beyond a single hop. The authors demonstrate
  this "truncation gap" through two approaches: (1) a synthetic edge regression task
  requiring longer memory, where T-BPTT fails to learn as memory length increases
  while full BPTT succeeds, and (2) experiments on three real-world dynamic graph
  benchmarks (Reddit, Wikipedia, MOOC) showing T-BPTT consistently underperforms full
  BPTT by 3-22% across metrics.'
---

# Mind the truncation gap: challenges of learning on dynamic graphs with recurrent architectures

## Quick Facts
- arXiv ID: 2412.21046
- Source URL: https://arxiv.org/abs/2412.21046
- Authors: João Bravo; Jacopo Bono; Pedro Saleiro; Hugo Ferreira; Pedro Bizarro
- Reference count: 13
- Key outcome: Truncated backpropagation-through-time (T-BPTT) in Graph Recurrent Neural Networks (GRNNs) limits learning to single-hop temporal dependencies, creating a measurable performance gap versus full backpropagation-through-time (F-BPTT).

## Executive Summary
This paper identifies a critical limitation in Graph Recurrent Neural Networks (GRNNs) for dynamic graphs: truncated backpropagation-through-time (T-BPTT) prevents learning dependencies beyond a single hop. The authors demonstrate this "truncation gap" through two approaches: (1) a synthetic edge regression task requiring longer memory, where T-BPTT fails to learn as memory length increases while full BPTT succeeds, and (2) experiments on three real-world dynamic graph benchmarks (Reddit, Wikipedia, MOOC) showing T-BPTT consistently underperforms full BPTT by 3-22% across metrics. The issue arises because GRNNs process batches of temporally-ordered events where each entity typically appears only once, limiting gradient propagation to a single hop. This performance gap persists even when comparing T-BPTT with accumulated gradients against full BPTT, suggesting that standard GRNN training approaches fail to fully exploit model capacity for learning long-term temporal dependencies.

## Method Summary
The paper compares truncated backpropagation-through-time (T-BPTT) with full backpropagation-through-time (F-BPTT) for training GRNN models on dynamic graphs. The methodology involves implementing a synthetic edge regression task with varying memory lengths (M=1,2,3,4,5) and testing on three real-world dynamic graph datasets (Reddit, Wikipedia, MOOC). Both methods use the same GRNN architecture based on GRU cells, with T-BPTT accumulating gradients over batches while F-BPTT backpropagates through entire epochs. The training procedure uses AdamW optimizer with learning rate 1e-3 and weight decay 1e-4, with hyperparameter tuning via random search. The key innovation is demonstrating that the truncation gap persists even with gradient accumulation, revealing a fundamental limitation in how GRNNs process temporally-ordered event batches.

## Key Results
- On synthetic edge regression tasks, T-BPTT fails to learn as memory length increases beyond 1 hop, while F-BPTT maintains performance
- On real-world datasets (Reddit, Wikipedia, MOOC), T-BPTT underperforms F-BPTT by 3-22% across MRR and Recall@10 metrics
- The truncation gap persists even when T-BPTT uses accumulated gradients over entire epochs, matching the effective gradient flow of F-BPTT
- Performance degradation correlates with increasing memory requirements, demonstrating the structural limitation of batch-based processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Truncated BPTT limits learning to single-hop temporal dependencies because entities rarely appear in the same batch more than once
- Mechanism: In dynamic graphs, each batch typically contains one update per entity due to the sequential nature of events. This means backpropagation can only flow through one time step per entity, preventing the model from learning multi-hop temporal relationships
- Core assumption: The batch processing strategy used for GRNNs inherently limits gradient propagation to single-hop dependencies
- Evidence anchors:
  - [section] "Because such a sequential batch of events can involve completely different entities, if the number of entities is large enough one can find themselves in a situation where batches include a single update per entity"
  - [abstract] "This issue arises because GRNNs process batches of temporally-ordered events where each entity typically appears only once, limiting gradient propagation to a single hop"
- Break condition: When batch sizes become large enough to include multiple updates for the same entity, or when entities appear more frequently in the temporal sequence

### Mechanism 2
- Claim: The truncation gap persists even with accumulated gradients because the structural limitation of batch processing remains
- Mechanism: Even when gradients are accumulated over an entire epoch (as in the synthetic task experiments), the truncation effect remains because each entity's gradient contributions are isolated within single time steps, preventing proper credit assignment for long-term dependencies
- Core assumption: Accumulated gradients do not overcome the fundamental batching limitation
- Evidence anchors:
  - [section] "We also show that a gap exists between the performance achievable with truncated and full backpropagation, on real world dynamic graph datasets by comparing both training methods on popular benchmarks"
  - [abstract] "This performance gap persists even when comparing T-BPTT with accumulated gradients against full BPTT"
- Break condition: When full backpropagation is used instead of truncated versions, allowing gradients to flow through the complete temporal sequence

### Mechanism 3
- Claim: The truncation gap manifests as a measurable performance difference between full and truncated BPTT methods
- Mechanism: Models trained with full BPTT can learn longer-term temporal dependencies that truncated methods cannot capture, resulting in measurable performance improvements on tasks requiring multi-hop reasoning
- Core assumption: The performance gap is directly attributable to the truncation of backpropagation
- Evidence anchors:
  - [abstract] "We reveal a performance gap between full backpropagation-through-time (F-BPTT) and the truncated backpropagation-through-time (T-BPTT) commonly used to train GRNN models"
  - [section] "We propose a synthetic edge regression task on a dynamic graph that truncated backpropagation fails to learn when dependencies over more hops are required"
- Break condition: When the task does not require long-term temporal dependencies, making the truncation gap negligible

## Foundational Learning

- Concept: Backpropagation Through Time (BPTT)
  - Why needed here: Understanding how gradients flow through time in recurrent architectures is essential to grasp why truncation limits learning
  - Quick check question: What happens to gradient flow when backpropagation is truncated after N time steps?

- Concept: Dynamic graphs and temporal dependencies
  - Why needed here: The paper's core argument depends on understanding how temporal relationships in evolving networks differ from static graphs
  - Quick check question: How do temporal dependencies in dynamic graphs differ from spatial dependencies in static graphs?

- Concept: Recurrent neural network architectures (GRU/LSTM)
  - Why needed here: GRNNs are built on these foundations, and their limitations directly impact the model's ability to learn temporal patterns
  - Quick check question: What are the key architectural differences between GRU and LSTM cells that might affect their ability to learn long-term dependencies?

## Architecture Onboarding

- Component map: Event → Node state update → Batch accumulation → Gradient computation → Parameter update
- Critical path: Event → Node state update → Batch accumulation → Gradient computation → Parameter update
- Design tradeoffs:
  - Memory vs. temporal context: Full BPTT requires storing all intermediate states, while truncated BPTT saves memory but loses context
  - Batch size vs. gradient truncation: Larger batches may include multiple updates per entity but increase computational cost
  - Parallelization vs. sequential correctness: Parallel processing within batches improves efficiency but may introduce inconsistencies
- Failure signatures:
  - Performance degradation as memory length increases (synthetic task)
  - Consistent underperformance on tasks requiring multi-hop reasoning
  - Inability to learn patterns that span multiple time steps
- First 3 experiments:
  1. Implement the synthetic edge regression task with varying memory lengths (M=1, 2, 4, 8) and compare F-BPTT vs T-BPTT performance
  2. Run the Reddit dataset benchmark with both training methods, measuring MRR and Recall@10
  3. Test different batch sizes (50, 200, 1000) to observe the relationship between batch composition and performance gap size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to GRNNs could enable learning beyond the single-hop truncation barrier?
- Basis in paper: [explicit] The authors identify that backpropagation truncation limits learning beyond a single hop and propose future research directions including approximations to real-time recurrent learning (RTRL) and simpler recurrent architectures that make RTRL feasible.
- Why unresolved: While the paper identifies the truncation gap problem and suggests potential directions, it does not provide concrete architectural solutions or demonstrate their effectiveness on the identified benchmark tasks.
- What evidence would resolve it: Empirical results showing a GRNN variant trained with a proposed solution (e.g., RTRL approximation or simplified architecture) successfully solving the synthetic edge regression task for M > 1 and achieving performance gains on real-world benchmarks comparable to full BPTT.

### Open Question 2
- Question: How does the truncation gap vary across different types of dynamic graph datasets and what factors contribute to this variation?
- Basis in paper: [inferred] The authors observe different truncation gap magnitudes across Reddit (18.1% improvement), MOOC (22.2% improvement), and Wikipedia datasets, but do not analyze the underlying reasons for these differences.
- Why unresolved: The paper presents the existence of the truncation gap but does not investigate what dataset characteristics (e.g., node degree distribution, temporal sparsity, event frequency) influence its severity.
- What evidence would resolve it: A systematic study correlating truncation gap size with dataset properties, potentially identifying which graph characteristics make certain datasets more or less susceptible to the truncation problem.

### Open Question 3
- Question: Can hybrid approaches combining GRNNs with transformer or random walk methods overcome the truncation gap while maintaining computational efficiency?
- Basis in paper: [explicit] The authors mention that transformer and random walk approaches don't suffer from the same truncation issue as GRNNs, suggesting they could be potential solutions to explore.
- Why unresolved: The paper does not explore whether combining the strengths of different approaches (GRNN's state persistence with transformer/random walk's context modeling) could provide a practical solution that addresses both the truncation gap and computational constraints.
- What evidence would resolve it: Experimental results demonstrating a hybrid model that achieves comparable performance to full BPTT on the synthetic task and real-world benchmarks while maintaining reasonable computational efficiency compared to full BPTT.

## Limitations
- The synthetic task uses a simplified edge regression scenario that may not fully capture real-world dynamic graph learning complexity
- The paper assumes accumulated gradients provide fair comparison between T-BPTT and F-BPTT without exploring alternative batching strategies
- The performance gap, while consistent, is relatively modest (3-22%), suggesting practical implications may vary by use case

## Confidence
- High confidence in the existence of a truncation gap and its mechanism (Mechanisms 1-3)
- Medium confidence in the synthetic task's ability to isolate this phenomenon
- Medium confidence in the real-world benchmark results due to unknown preprocessing details

## Next Checks
1. Implement the synthetic edge regression task with varying memory lengths (M=1, 2, 4, 8) and compare F-BPTT vs T-BPTT performance to verify the degradation pattern
2. Run the Reddit dataset benchmark with both training methods, measuring MRR and Recall@10, ensuring identical batching strategies and hyperparameter tuning
3. Test different batch sizes (50, 200, 1000) to observe how batch composition affects the size of the truncation gap, potentially revealing batching strategies that could mitigate the issue