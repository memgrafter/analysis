---
ver: rpa2
title: 'Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts'
arxiv_id: '2409.16040'
source_url: https://arxiv.org/abs/2409.16040
tags:
- time
- series
- forecasting
- conference
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Time-MoE, a scalable mixture-of-experts architecture
  for large-scale time series foundation models. By activating only a subset of networks
  for each prediction, Time-MoE enhances computational efficiency while maintaining
  high model capacity, allowing it to scale effectively without increasing inference
  costs.
---

# Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts

## Quick Facts
- **arXiv ID:** 2409.16040
- **Source URL:** https://arxiv.org/abs/2409.16040
- **Reference count:** 40
- **Primary result:** Time-MoE achieves 20% and 24% error reductions in zero-shot and in-distribution forecasting respectively while scaling to 2.4B parameters using MoE architecture

## Executive Summary
Time-MoE introduces a mixture-of-experts architecture for time series foundation models, enabling efficient scaling to 2.4 billion parameters without proportional increases in inference costs. By activating only relevant expert networks for each prediction, the model maintains high capacity while improving computational efficiency. Trained on a large-scale dataset spanning 9 domains with over 300 billion time points, Time-MoE demonstrates significant performance improvements over dense models with similar computational budgets across six real-world benchmarks.

## Method Summary
Time-MoE employs a mixture-of-experts (MoE) architecture where each time series prediction activates only a subset of the total network parameters, dramatically reducing computation while maintaining model capacity. The model was trained on Time-300B, a newly introduced large-scale dataset containing over 300 billion time points across 9 diverse domains. The MoE approach allows the model to scale to 2.4 billion parameters - the largest reported for time series modeling - while keeping inference costs manageable. The architecture validates scaling laws for time series forecasting, demonstrating that larger models with appropriate expert routing can achieve superior performance across both zero-shot and in-distribution scenarios.

## Key Results
- Achieves 20% average error reduction in zero-shot forecasting scenarios across six benchmarks
- Demonstrates 24% average error reduction in in-distribution forecasting tasks
- Scales to 2.4 billion parameters, establishing new state-of-the-art performance for universal time series forecasting

## Why This Works (Mechanism)
The mixture-of-experts architecture enables selective activation of specialized subnetworks based on input characteristics, allowing the model to maintain high capacity while reducing computational overhead. By routing different types of time series patterns to specialized experts, Time-MoE can develop domain-specific knowledge within its architecture while sharing common representations across experts. This selective activation means that at inference time, only a fraction of the total parameters are computed, enabling the practical deployment of billion-parameter models for time series forecasting tasks.

## Foundational Learning

**Time Series Forecasting** - Predicting future values in sequential data streams. Needed because foundation models must generalize across diverse temporal patterns. Quick check: Model should handle both periodic and irregular patterns.

**Mixture-of-Experts (MoE)** - Architecture where multiple specialized networks (experts) are activated based on gating mechanisms. Needed to balance model capacity with computational efficiency. Quick check: Only a subset of experts should activate per input.

**Scaling Laws** - Relationship between model size, dataset size, and performance. Needed to guide optimal resource allocation. Quick check: Performance should improve predictably with parameter count.

**Zero-shot Learning** - Model's ability to perform well on unseen tasks without fine-tuning. Needed for foundation model versatility. Quick check: Performance on held-out domains should remain competitive.

**In-distribution Testing** - Evaluation on data similar to training distribution. Needed to establish baseline capability. Quick check: Performance should significantly exceed random baselines.

## Architecture Onboarding

**Component Map:** Input Data -> Gating Network -> Expert Selection -> Expert Networks -> Output Aggregation -> Forecast

**Critical Path:** The gating network routing to appropriate experts represents the most critical component, as poor routing decisions directly impact model performance and efficiency.

**Design Tradeoffs:** The paper balances model capacity against computational efficiency through selective expert activation, trading some routing complexity for significant gains in parameter efficiency and inference speed.

**Failure Signatures:** Poor expert routing could lead to underutilization of capacity or incorrect pattern matching, manifesting as degraded performance on specific time series types or domains.

**First Experiments:** 1) Test expert utilization rates across different time series types, 2) Evaluate gating network accuracy on validation data, 3) Measure inference latency versus dense baseline models

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Limited details on Time-300B dataset composition, potential biases, and representativeness across domains
- Computational efficiency claims lack specific metrics on inference costs, memory usage, and energy consumption
- Does not explore diminishing returns point for model scaling or provide optimal sizing guidance for different use cases

## Confidence
- Performance improvements (20% and 24% error reductions): **High**
- Architectural innovation of MoE for time series: **High**
- Scaling to 2.4B parameters: **High**
- Practical computational efficiency benefits: **Medium** (lacks specific metrics)
- Universal applicability claims: **Medium** (limited out-of-distribution testing)

## Next Checks
1. Conduct ablation studies removing MoE components to quantify exact contribution of mixture-of-experts architecture versus other design choices
2. Test model on truly out-of-distribution time series data from domains not represented in training set to validate universal applicability claims
3. Measure actual inference costs (latency, memory, energy) across different hardware configurations to verify computational efficiency benefits