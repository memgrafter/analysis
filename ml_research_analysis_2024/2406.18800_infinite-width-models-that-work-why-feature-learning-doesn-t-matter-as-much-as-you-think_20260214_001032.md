---
ver: rpa2
title: 'Infinite Width Models That Work: Why Feature Learning Doesn''t Matter as Much
  as You Think'
arxiv_id: '2406.18800'
source_url: https://arxiv.org/abs/2406.18800
tags:
- infinite
- learning
- feature
- finite
- width
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates why infinite-width models like Neural Tangent
  Kernels (NTKs) underperform finite-width models, traditionally attributed to lack
  of feature learning. The authors demonstrate that this explanation is insufficient
  by showing that infinite-width NTKs can select relevant subfeatures from their infinite
  frozen feature vector, effectively replacing feature learning.
---

# Infinite Width Models That Work: Why Feature Learning Doesn't Matter as Much as You Think

## Quick Facts
- arXiv ID: 2406.18800
- Source URL: https://arxiv.org/abs/2406.18800
- Authors: Luke Sernau
- Reference count: 15
- Primary result: ADAM* NTK closes the performance gap between infinite-width and finite-width models by modeling sophisticated optimizers rather than SGD

## Executive Summary
This paper challenges the conventional wisdom that infinite-width neural networks underperform finite models due to lack of feature learning. Through theoretical analysis and empirical experiments, the authors demonstrate that feature selection from an infinite frozen feature vector can achieve the same expressiveness as feature learning. The key insight is that the performance gap stems primarily from optimizer choice - traditional NTK constructions model SGD while successful finite models use sophisticated optimizers like ADAM. The proposed ADAM* NTK construction preserves kernel representation while incorporating momentum and adaptive learning rates, effectively closing the performance gap on transformer models trained on C4.

## Method Summary
The authors develop a new infinite-width limit based on ADAM-like learning dynamics (ADAM*) that models sophisticated optimizer behavior while maintaining the kernel representation. They compare four model variants: original unfrozen transformers, frozen-feature MLPs, traditional NTK (SGD-based), and ADAM* NTK on a 6-layer decoder-only transformer trained on C4. The experiments measure final loss after 40k training steps across varying MLP widths. The theoretical framework shows that infinite-width NTKs can select relevant subfeatures from their infinite frozen feature vector, making feature learning unnecessary.

## Key Results
- Traditional NTKs underperform even when compared to models with feature learning disabled
- ADAM* NTK closely tracks the performance of frozen-feature models across various scales
- The performance gap between NTKs and finite models is primarily due to optimizer choice rather than feature learning
- Infinite-width models can achieve full expressiveness through feature selection from their frozen feature vector

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Infinite-width NTKs can achieve full expressiveness by selecting relevant subfeatures from their infinite frozen feature vector, eliminating the need for feature learning during training.
- Mechanism: At infinite width, the feature vector contains every possible feature representation with probability one. The final layer can simply upweight and downweight subvectors to access any desired behavior, effectively performing feature selection without updating the features themselves.
- Core assumption: The feature distribution Ω has a differentiable density and is totally supported, ensuring all feature representations exist in the infinite limit.
- Evidence anchors:
  - [abstract]: "infinite width NTKs obviate the need for feature learning. They can learn identical behavior by selecting relevant subfeatures from their (infinite) frozen feature vector."
  - [section]: "At infinite width, feature vectors contain an infinite variety of structure, all of which is available to the final layer. We'll prove that merely by upweighting and downweighting subvectors, the final layer of an infinite width model has access to every possible behavior the previous layers could have learned."

### Mechanism 2
- Claim: The performance gap between NTKs and finite models stems primarily from optimizer choice rather than absence of feature learning.
- Mechanism: Traditional NTK constructions model SGD dynamics, while successful finite models use optimizers like ADAM with momentum and adaptive learning rates. The ADAM* NTK construction preserves kernel representation while incorporating these beneficial optimization dynamics.
- Core assumption: The effectiveness of ADAM-like optimizers transfers to infinite-width settings when properly modeled.
- Evidence anchors:
  - [abstract]: "weak performance is at least partly due to the fact that existing constructions depend on weak optimizers like SGD. We provide a new infinite width limit based on ADAM-like learning dynamics and demonstrate empirically that the resulting models erase this performance gap."
  - [section]: "Modern machine learning is almost exclusively done using optimizers that support, at a minimum, momentum. These methods are more effective in practice than naive SGD. We give an NTK construction that models ADAM-style gradient updates."

### Mechanism 3
- Claim: The expected value of finite models equals the infinite model, allowing results about infinite models to apply to finite models in expectation.
- Mechanism: By initializing finite model features from a continuous distribution Ω, the expected model output equals the infinite model output, creating a bridge between finite and infinite analyses.
- Core assumption: Features are initialized by sampling from the same distribution Ω used in the infinite model.
- Evidence anchors:
  - [section]: "Another intriguing property of this notation is that it emphasizes a fundamental relationship between infinite and finite models. Let ωh be initialized by sampling from some continuous distribution Ω. Then from (1), the expected value of the model (taken over the random initialization) will be exactly the expression in (2)."

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its relationship to infinite-width neural networks
  - Why needed here: The entire paper builds on understanding how NTKs work and why they underperform finite models
  - Quick check question: What is the key difference between standard SGD-based NTKs and the proposed ADAM* NTK construction?

- Concept: Feature learning vs. feature selection in deep networks
  - Why needed here: The paper challenges the conventional wisdom that NTK underperformance is due to lack of feature learning
  - Quick check question: How does the paper argue that infinite-width models can achieve the same expressiveness as feature-learning models without updating features?

- Concept: Lottery ticket hypothesis and its extension to infinite-width limits
  - Why needed here: The paper uses lottery ticket intuition to explain how infinite-width models can select useful features from their initialization
  - Quick check question: According to the lottery ticket hypothesis, what happens to model parameters as model size increases?

## Architecture Onboarding

- Component map:
  - Input layer → Feature extraction (frozen) → Kernel computation (Θ) → Final layer (M) → Output
  - For ADAM*: Additional momentum (m) and variance (v) tracking components

- Critical path:
  1. Initialize feature distribution Ω
  2. Compute NTK kernel Θ(x, xi) = Eω∼Ω[gω(x)gω(xi)]
  3. Apply optimizer updates to final layer M
  4. For ADAM*: Maintain and update momentum and variance statistics

- Design tradeoffs:
  - Memory efficiency: ADAM* NTK requires O(t) memory vs O(HE) for standard NTK
  - Expressiveness: Frozen features vs. trainable features
  - Computational complexity: Kernel computation vs. full backpropagation

- Failure signatures:
  - Poor performance despite correct implementation → Check if Ω is properly supported
  - Memory overflow → Verify kernel computation doesn't accidentally materialize full feature matrix
  - Convergence issues → Check optimizer hyperparameters and initialization

- First 3 experiments:
  1. Compare standard NTK vs. ADAM* NTK on a simple dataset with varying widths
  2. Test feature selection mechanism by examining final layer weights on synthetic data
  3. Validate the expected value relationship by comparing finite model averages to infinite predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the ADAM* NTK converge to the same performance as the unfrozen model in the infinite-width limit?
- Basis in paper: [explicit] The authors state "In the infinite-width limit, our theoretical analysis suggests that the performance of both ADAM variants should ultimately converge with the unfrozen model, but we leave experimental exploration of this phenomenon to future work."
- Why unresolved: The paper only provides experimental validation for finite-width models and does not explore the infinite-width limit.
- What evidence would resolve it: Experiments comparing the performance of ADAM* NTK with unfrozen models as the width approaches infinity.

### Open Question 2
- Question: How does the performance of ADAM* NTK compare to other sophisticated optimizers beyond ADAM in the infinite-width limit?
- Basis in paper: [inferred] The paper focuses on ADAM as a sophisticated optimizer but does not explore other optimizers like RMSprop or Adagrad.
- Why unresolved: The paper only compares ADAM* NTK to SGD-based NTK and does not investigate other optimizer dynamics.
- What evidence would resolve it: Comparative experiments of ADAM* NTK against other optimizer-based NTK constructions.

### Open Question 3
- Question: Can the concept of feature selection from infinite frozen feature vectors be extended to other neural network architectures beyond transformers?
- Basis in paper: [explicit] The authors demonstrate feature selection in the context of transformers but do not explore other architectures.
- Why unresolved: The paper focuses on transformers and does not investigate the applicability of the feature selection concept to other architectures.
- What evidence would resolve it: Experimental validation of feature selection in other architectures like CNNs or RNNs using the proposed NTK construction.

## Limitations
- The theoretical claims about feature selection eliminating the need for feature learning may not hold for all practical scenarios, particularly when feature distributions lack sufficient diversity
- The ADAM* NTK construction has only been tested on transformer models trained on C4, limiting generalizability to other architectures and datasets
- The relationship between optimizer choice and NTK performance, while compelling, may oversimplify the complex interactions between optimization dynamics and model expressiveness

## Confidence

- **High confidence**: The theoretical framework connecting infinite and finite models through initialization distributions is mathematically sound and well-supported.
- **Medium confidence**: The experimental results showing ADAM* NTK closing the performance gap are convincing for the specific transformer architecture tested, but generalizability remains uncertain.
- **Medium confidence**: The argument that feature learning can be replaced by feature selection in infinite-width models is theoretically valid but may not hold for all practical scenarios.

## Next Checks

1. **Cross-architecture validation**: Test ADAM* NTK on diverse architectures (CNNs, MLPs) and datasets to verify the generalizability of the optimizer-based performance improvement.

2. **Feature distribution analysis**: Systematically investigate what happens when the feature distribution Ω is not totally supported or lacks diversity, to identify the precise conditions under which feature learning becomes necessary.

3. **Optimizer ablation study**: Conduct controlled experiments isolating different optimizer components (momentum, adaptive learning rates, weight decay) to determine which properties are most critical for NTK performance.