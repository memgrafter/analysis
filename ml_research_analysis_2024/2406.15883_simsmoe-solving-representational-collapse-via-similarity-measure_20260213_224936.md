---
ver: rpa2
title: 'SimSMoE: Solving Representational Collapse via Similarity Measure'
arxiv_id: '2406.15883'
source_url: https://arxiv.org/abs/2406.15883
tags:
- uni00a0
- experts
- similarity
- collapse
- smoe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the representation collapse problem in sparse
  mixture-of-experts (SMoE) models, where experts learn similar representations, reducing
  model effectiveness. The proposed SimSMoE method directly tackles this issue by
  introducing a Similarity Learning module that quantifies expert similarity using
  Centered Kernel Alignment (CKA) and applies a similarity loss to encourage diverse
  expert representations.
---

# SimSMoE: Solving Representational Collapse via Similarity Measure

## Quick Facts
- **arXiv ID**: 2406.15883
- **Source URL**: https://arxiv.org/abs/2406.15883
- **Reference count**: 40
- **Key outcome**: Addresses representation collapse in SMoE models by introducing SimSMoE method using CKA similarity learning, achieving up to 4.8% improvement on downstream tasks

## Executive Summary
This paper tackles the critical representation collapse problem in Sparse Mixture-of-Experts (SMoE) models, where expert representations become overly similar, reducing model effectiveness. The proposed SimSMoE method introduces a Similarity Learning module that quantifies expert similarity using Centered Kernel Alignment (CKA) and applies a similarity loss to encourage diverse expert representations. Unlike existing approaches that focus on routing mechanisms, SimSMoE works with any routing algorithm and directly optimizes expert representations. Extensive experiments across three SMoE architectures (Brainformer, GLaM, Mistral) for both pre-training and fine-tuning tasks demonstrate consistent performance improvements, achieving up to 4.8% gains on downstream tasks while maintaining faster convergence rates.

## Method Summary
SimSMoE introduces a Similarity Learning module that uses CKA to measure expert representation similarity and applies a similarity loss to prevent representation collapse. The method works with any routing algorithm by focusing on expert representation diversity rather than routing policy. During training, expert outputs are passed through an MLP projection head, and CKA similarity is computed between all expert pairs. A similarity loss is then applied when pairs exceed a threshold, encouraging diverse representations. The total loss combines task loss, balancing loss, and similarity loss, with hyperparameters controlling the frequency of similarity checks and loss weighting.

## Key Results
- Achieves up to 4.8% improvement on downstream tasks compared to state-of-the-art routing methods
- Adds only 0.08-0.16M parameters compared to baseline SMoE models
- Demonstrates faster convergence rates across all tested architectures and tasks
- Works effectively with any routing algorithm, not just top-k routing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The SimSMoE method directly addresses the representation collapse problem by minimizing the similarity among expert representations.
- Mechanism: SimSMoE introduces a Similarity Learning module that quantifies expert similarity using Centered Kernel Alignment (CKA) and applies a similarity loss to encourage diverse expert representations.
- Core assumption: The similarity between expert representations is a primary cause of representation collapse, and reducing this similarity will improve model performance.
- Evidence anchors:
  - [abstract]: "The proposed SimSMoE method directly tackles this issue by introducing a Similarity Learning module that quantifies expert similarity using Centered Kernel Alignment (CKA) and applies a similarity loss to encourage diverse expert representations."
  - [section]: "To deal with the challenge, Kornblith et al. (2019) [33] proves that the similarity index based on centered kernel alignment (CKA) reliably identifies correspondences between representations in neural networks."
- Break condition: If the similarity metric fails to capture the true representational similarity or if the loss function is not properly tuned, the method may not effectively prevent representation collapse.

### Mechanism 2
- Claim: The SimSMoE method can be applied to any routing algorithm, enhancing model performance by addressing the representation collapse problem.
- Mechanism: By focusing on expert representation rather than routing policy, SimSMoE leverages the strengths of existing routing algorithms while directly improving expert diversity.
- Core assumption: Improving expert representation diversity will lead to better model performance regardless of the underlying routing mechanism.
- Evidence anchors:
  - [abstract]: "Unlike existing approaches that focus on improving routing mechanisms, SimSMoE works with any routing algorithm and guarantees better performance by directly optimizing expert representations."
  - [section]: "Unlike the router policy approach [13, 10, 15], our framework can be applied to any routing algorithms, as it directly improves expert representations."
- Break condition: If the routing algorithm is so ineffective that it cannot properly utilize diverse expert representations, the benefits of SimSMoE may be limited.

### Mechanism 3
- Claim: The SimSMoE method ensures superior SMoE training strategies by quantifying the similarity between expert representations and minimizing similarity among experts.
- Mechanism: SimSMoE employs a quantitative method using CKA to illustrate the collapse issue and solves it by applying a similarity loss function.
- Core assumption: A quantitative measure of similarity between expert representations is necessary to effectively address representation collapse.
- Evidence anchors:
  - [abstract]: "Moreover, our method guarantees superior SMoE training strategies compared to the existing methods by quantifying the similarity between expert representations and minimizing similarity among experts by the CKA [33] loss function."
  - [section]: "To deal with the challenge, Kornblith et al. (2019) [33] proves that the similarity index based on centered kernel alignment (CKA) reliably identifies correspondences between representations in neural networks."
- Break condition: If the CKA metric does not accurately capture the similarity between expert representations or if the loss function is not properly balanced with the task loss, the method may not effectively prevent representation collapse.

## Foundational Learning

- Concept: Sparse Mixture of Experts (SMoE)
  - Why needed here: Understanding the basic structure and functioning of SMoE is crucial to comprehend the representation collapse problem and how SimSMoE addresses it.
  - Quick check question: What is the main advantage of using SMoE over dense models in terms of computational cost?

- Concept: Representation Collapse
  - Why needed here: This is the core problem that SimSMoE aims to solve. Understanding its causes and effects is essential to appreciate the method's significance.
  - Quick check question: What are the two main manifestations of representation collapse in SMoE models?

- Concept: Centered Kernel Alignment (CKA)
  - Why needed here: CKA is the key metric used by SimSMoE to quantify similarity between expert representations. Understanding its properties and applications is crucial.
  - Quick check question: How does CKA differ from other similarity metrics in its ability to handle different neural network architectures?

## Architecture Onboarding

- Component map: Input tokens -> Router network (selects top-k experts) -> Expert MLPs -> Similarity Learning module (CKA + similarity loss) -> Combined loss (task + balancing + similarity) -> Model update

- Critical path: Input tokens are routed to top-k experts based on router scores → Expert outputs are passed through the Similarity Learning module → Similarity loss is calculated and added to the total loss → Model is trained to minimize the combined loss

- Design tradeoffs:
  1. Adding the Similarity Learning module increases model complexity slightly (0.08-0.16M parameters)
  2. The method works best with top-k routing (k >= 2) to fully utilize pairwise expert outputs
  3. Hyperparameters f* and T* control computational resources and collapse identification quality

- Failure signatures:
  1. If experts still collapse despite using SimSMoE, check if the similarity threshold T* is too high
  2. If model performance degrades, ensure the similarity loss coefficient β is properly balanced with the task loss
  3. If computational cost is too high, adjust the frequency parameter f*

- First 3 experiments:
  1. Implement SimSMoE on a small-scale SMoE model and compare performance with baseline routing methods on a simple language modeling task
  2. Vary the similarity threshold T* and observe its effect on model performance and expert diversity
  3. Compare the convergence rate of SimSMoE with other state-of-the-art routing methods on a medium-scale SMoE model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal frequency (f*) for checking representation collapse that balances computational efficiency with effectiveness?
- Basis in paper: [explicit] The paper discusses that checking all expert pairs is computationally expensive and introduces f* as a hyperparameter to control this frequency, but does not determine the optimal value.
- Why unresolved: The paper only shows that SimSMoE is effective across a range of f* values, but does not provide guidance on finding the optimal value for different model sizes or datasets.
- What evidence would resolve it: Systematic experiments across multiple model scales, datasets, and routing methods to identify the optimal f* value that maximizes performance while minimizing computational overhead.

### Open Question 2
- Question: How does SimSMoE perform when applied to non-Transformer architectures or different types of neural networks beyond LLMs?
- Basis in paper: [inferred] The paper focuses exclusively on Transformer-based SMoE architectures for language modeling, but mentions that CKA similarity learning could be a general approach for addressing representation collapse.
- Why unresolved: The evaluation is limited to three specific SMoE architectures (Brainformer, GLaM, Mistral) and does not explore applicability to other neural network architectures like CNNs, RNNs, or graph neural networks.
- What evidence would resolve it: Experiments applying SimSMoE to diverse architectures and tasks (e.g., computer vision, speech recognition, graph learning) to validate generalizability of the similarity-based approach.

### Open Question 3
- Question: What is the relationship between the number of experts and the optimal similarity threshold (T*) for identifying representation collapse?
- Basis in paper: [explicit] The paper introduces T* as a hyperparameter to identify collapse but only tests it in a limited range (0.3-0.7) without analyzing how it should scale with model size.
- Why unresolved: The paper shows that T* values between 0.3-0.7 work well for the tested models, but does not explore how this threshold should be adjusted as the number of experts increases or decreases.
- What evidence would resolve it: Systematic experiments varying the number of experts (e.g., 2, 4, 8, 16, 32) while measuring optimal T* values to establish a scaling relationship between model capacity and similarity threshold.

## Limitations
- **Limited hyperparameter guidance**: The paper does not provide clear guidelines for selecting optimal values of similarity loss coefficient β and CKA bandwidth σ
- **Computational overhead**: While parameter count increase is minimal (0.08-0.16M), the additional similarity calculations may impact training time, which is not thoroughly analyzed
- **Narrow routing algorithm validation**: Experiments primarily focus on top-k routing, limiting empirical validation of claims about universal compatibility with all routing algorithms

## Confidence
- **Medium**: The paper presents compelling theoretical arguments and experimental results, but several key implementation details remain underspecified, particularly around hyperparameter selection
- **Medium**: While the paper claims compatibility with any routing algorithm, experiments are primarily limited to top-k routing, leaving broader claims largely theoretical
- **High**: The CKA-based similarity measurement is well-established in the literature, providing solid theoretical grounding, though potential limitations for deep architectures are not addressed

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Conduct systematic experiments varying the similarity loss coefficient β and CKA bandwidth σ across a wider range to establish robustness boundaries and provide clearer guidelines for practical deployment.

2. **Cross-Routing Algorithm Validation**: Test SimSMoE with alternative routing mechanisms (e.g., hash-based, random, or adaptive routing) to verify the claimed universal compatibility and identify any routing-specific considerations or limitations.

3. **Long-Training Stability Evaluation**: Monitor expert diversity and performance metrics over extended training periods (beyond the reported 50k steps) to assess whether the benefits of SimSMoE persist and whether any long-term stability issues emerge.