---
ver: rpa2
title: 'Prompting Large Language Models to Tackle the Full Software Development Lifecycle:
  A Case Study'
arxiv_id: '2403.08604'
source_url: https://arxiv.org/abs/2403.08604
tags:
- code
- software
- design
- testing
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents DevEval, a comprehensive benchmark that evaluates
  large language models (LLMs) across the entire software development lifecycle, including
  software design, environment setup, implementation, and testing. The study addresses
  the gap in existing benchmarks that focus on isolated aspects of coding by providing
  a holistic evaluation framework with multi-file codebases, four programming languages,
  and multiple domains.
---

# Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study

## Quick Facts
- arXiv ID: 2403.08604
- Source URL: https://arxiv.org/abs/2403.08604
- Authors: Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, Zhiyin Yu, He Du, Ping Yang, Dahua Lin, Chao Peng, Kai Chen
- Reference count: 15
- Primary result: Current LLMs struggle significantly with DevEval tasks, achieving less than 10% pass rates on repository-level implementation and facing challenges in multi-file codebases.

## Executive Summary
This paper introduces DevEval, a comprehensive benchmark designed to evaluate large language models across the entire software development lifecycle, including software design, environment setup, implementation, and testing. The study addresses the gap in existing benchmarks that focus on isolated aspects of coding by providing a holistic evaluation framework with multi-file codebases, four programming languages, and multiple domains. The results reveal that current LLMs, including GPT-4, struggle significantly with complex multi-file repositories, function redefinitions, file references, and build configuration generation.

## Method Summary
The study presents DevEval, a benchmark consisting of 22 curated repositories spanning Python, C/C++, Java, and JavaScript, each with product requirements, UML diagrams, architecture designs, implementation code, and comprehensive test suites. The evaluation framework processes these through five tasks: software design, environment setup, implementation, acceptance testing, and unit testing. Models are evaluated using automated testing frameworks and an LLM-as-a-judge approach for design quality assessment. The study investigates different prompting methods, particularly comparing single-turn approaches with execution feedback against multi-turn review-based approaches.

## Key Results
- Current LLMs achieve less than 10% pass rates on repository-level implementation tasks in DevEval
- Models struggle significantly with multi-file repositories, facing challenges with function redefinitions and file reference errors
- Execution feedback during prompting yields notable improvements, with GPT-4-Turbo achieving 8.9% pass rate on implementation versus 3.0% without feedback
- While testing code demonstrates promising coverage (up to 79.4% when executable), models consistently fail to generate functional tests

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-file repository evaluation reveals LLMs' inability to handle file references and dependencies correctly.
- Mechanism: When LLMs generate code across multiple files, they fail to maintain consistent function definitions and proper import/include statements, leading to compilation errors and runtime failures.
- Core assumption: The training data for LLMs contains insufficient examples of complex multi-file projects with proper dependency management.
- Evidence anchors:
  - [section]: "Function Redefinition in Multi-file Repositories" and "File Reference and Linkage Errors" describe how models redundantly implement functions and make reference errors.
  - [corpus]: No direct evidence in corpus, but related work on code generation benchmarks typically use single-file or simplified multi-file scenarios.
- Break condition: Models could handle multi-file repositories if trained on more diverse, complex project structures with proper dependency patterns.

### Mechanism 2
- Claim: Execution feedback during prompting significantly improves LLM performance on implementation tasks.
- Mechanism: When models receive runtime error messages and performance metrics during the review process, they can iteratively correct their code, leading to better pass rates on acceptance and unit tests.
- Core assumption: LLMs can effectively utilize execution feedback to identify and fix code errors when given appropriate prompts.
- Evidence anchors:
  - [abstract]: "our investigation into different prompting methods shows that prompts with external information and execution feedback could yield notable and consistent improvements."
  - [section]: Table 3 shows GPT-4-Turbo achieves 8.9% pass rate on implementation with Execution-Feedback vs 3.0% without.
- Break condition: If execution feedback is not actionable or models cannot parse error messages effectively, improvements would not materialize.

### Mechanism 3
- Claim: Software design evaluation using LLM-as-a-judge is viable and aligns with human judgments.
- Mechanism: By providing detailed evaluation guidelines covering general principles (cohesion, decoupling, practicability) and faithfulness to requirements, LLMs can effectively compare design documents and reach decisions consistent with human majority annotations.
- Core assumption: LLMs can understand and apply abstract software design principles consistently across different design tasks.
- Evidence anchors:
  - [section]: "Low agreements are observed with tie considered... Without tie, GPT-4-Turbo reaches 79.2% and 83.2% agreements on the general principles and faithfulness metrics, respectively."
  - [corpus]: No direct evidence, but pairwise comparison approaches are established in other LLM evaluation contexts.
- Break condition: If evaluation criteria are too subjective or LLM judges show position bias, alignment with human judgments would degrade.

## Foundational Learning

- Concept: Software Development Lifecycle (SDLC) stages
  - Why needed here: DevEval evaluates models across all SDLC stages (design, environment setup, implementation, testing), requiring understanding of how these phases interconnect.
  - Quick check question: What is the primary purpose of acceptance testing versus unit testing in the SDLC?

- Concept: Multi-file repository structure and dependency management
  - Why needed here: The benchmark uses multi-file codebases where models must handle proper file references, function declarations, and build configurations across different programming languages.
  - Quick check question: Why do models struggle with function redefinition when generating code across multiple files in a repository?

- Concept: Prompt engineering with execution feedback
  - Why needed here: The study shows that prompts incorporating runtime feedback significantly improve model performance, indicating the importance of iterative refinement strategies.
  - Quick check question: How does execution feedback help models correct errors that they cannot identify through static code analysis alone?

## Architecture Onboarding

- Component map: 22 curated repositories (Python, C/C++, Java, JavaScript) -> PRD/UML/Architecture documents -> Implementation code -> Test suites -> Five evaluation tasks (Design, Environment Setup, Implementation, Acceptance Testing, Unit Testing) -> Automated testing framework -> LLM-as-a-judge evaluation
- Critical path: Repository preparation -> Code cleanup -> Document preparation -> Task execution (with appropriate prompting methods) -> Automated testing framework evaluation -> Results aggregation and analysis
- Design tradeoffs: Single-turn vs multi-turn prompting (favoring execution feedback despite computational cost), automated vs human evaluation (using LLM-as-a-judge with human validation), comprehensive vs focused evaluation (covering full SDLC despite complexity)
- Failure signatures: Low pass rates on implementation tasks (<10%), inability to generate executable Makefiles/Gradle files, function redefinition errors in multi-file contexts, and fabrication of non-existent variables
- First 3 experiments:
  1. Run implementation task with No-Review prompting on a Python repository to establish baseline performance
  2. Execute the same task with Execution-Feedback prompting to measure improvement from runtime information
  3. Compare software design quality using LLM-as-a-judge against GPT-3.5-Turbo baseline across UML class diagrams and sequence diagrams

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more enriched training data and advanced prompting methods bridge the gap between current LLM capabilities and real-world software development requirements?
- Basis in paper: [explicit] The paper highlights the need for more enriched training data and advanced prompting methods to bridge this competency gap.
- Why unresolved: While the paper identifies the need, it does not explore specific methods or datasets that could effectively improve LLM performance in complex coding scenarios.
- What evidence would resolve it: Comparative studies showing improvements in LLM performance on DevEval tasks after implementing enriched training data and advanced prompting methods.

### Open Question 2
- Question: How can LLMs be improved to better handle multi-file repositories, particularly in managing function redefinitions and file references?
- Basis in paper: [inferred] The paper discusses challenges faced by LLMs in multi-file repositories, including function redefinitions and file reference errors.
- Why unresolved: The paper identifies these issues but does not propose solutions or investigate potential strategies for improving LLM handling of multi-file structures.
- What evidence would resolve it: Development and testing of LLM architectures or training techniques specifically designed to improve multi-file repository handling.

### Open Question 3
- Question: What are the specific limitations of current LLMs in understanding and generating accurate Makefiles and Gradle files, and how can these be addressed?
- Basis in paper: [explicit] The paper notes that LLMs often face challenges in generating accurate Makefile for C/C++ and Gradle files for Java, attributing this to insufficient training data related to these compilation tools.
- Why unresolved: While the paper identifies the problem, it does not explore the underlying causes or potential solutions in detail.
- What evidence would resolve it: Analysis of LLM training data and architecture to identify specific gaps in compilation tool knowledge, followed by targeted improvements and performance evaluations.

## Limitations
- Small sample size of 22 curated repositories may not fully represent real-world software development complexity
- LLM-as-a-judge approach may introduce systematic biases despite showing good alignment with human judgments
- Limited number of human annotators (three) may not capture the full spectrum of design quality assessments

## Confidence
- Confidence in major claims: Medium
- The experimental results demonstrating LLMs' struggles with multi-file repositories and the benefits of execution feedback prompting are well-supported by quantitative evidence
- However, the generalizability of these findings to larger, more complex projects remains uncertain
- The software design evaluation results show good agreement with human judgments, but the limited number of human annotators may not capture full variability

## Next Checks
1. **Scale-up validation**: Test the evaluation framework on a larger corpus of 100+ real-world repositories to assess whether current findings hold at scale and across more diverse project types
2. **Human validation expansion**: Increase the number of human annotators for software design evaluation from 3 to 10+ to better characterize inter-annotator agreement and identify potential systematic biases in the LLM-as-a-judge approach
3. **Cross-model comparison**: Conduct controlled experiments comparing different prompting strategies (Execution-Feedback vs Chain-of-Thought) across multiple model families (GPT-4, Claude, Llama) to isolate the impact of model architecture versus prompting methodology