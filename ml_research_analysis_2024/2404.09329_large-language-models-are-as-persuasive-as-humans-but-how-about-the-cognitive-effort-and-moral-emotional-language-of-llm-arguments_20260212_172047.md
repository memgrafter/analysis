---
ver: rpa2
title: Large Language Models are as persuasive as humans, but how? About the cognitive
  effort and moral-emotional language of LLM arguments
arxiv_id: '2404.09329'
source_url: https://arxiv.org/abs/2404.09329
tags:
- llms
- arguments
- humans
- moral
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the persuasive strategies employed by Large
  Language Models (LLMs) compared to human-generated arguments. Using a dataset of
  1,251 participants, the research analyzes differences in cognitive effort (lexical
  and grammatical complexity) and moral-emotional language (sentiment and morality)
  between LLM-generated and human-generated arguments.
---

# Large Language Models are as persuasive as humans, but how? About the cognitive effort and moral-emotional language of LLM arguments

## Quick Facts
- arXiv ID: 2404.09329
- Source URL: https://arxiv.org/abs/2404.09329
- Authors: Carlos Carrasco-Farre
- Reference count: 0
- Primary result: LLM-generated arguments exhibit higher cognitive effort and greater use of moral language compared to human-generated arguments, yet achieve similar persuasion levels.

## Executive Summary
This study investigates how Large Language Models (LLMs) achieve persuasive effectiveness comparable to humans by analyzing cognitive effort (lexical and grammatical complexity) and moral-emotional language in their arguments. Using a dataset of 1,251 participants, the research reveals that LLM-generated arguments are more complex in structure than human-generated ones, requiring higher cognitive processing. Additionally, LLMs demonstrate a greater reliance on moral language, particularly negative moral foundations like harm and cheating, while showing no significant difference in emotional content compared to humans. These findings contribute to understanding LLM persuasion strategies and their potential implications for digital communication and misinformation.

## Method Summary
The study analyzes a dataset of 1,251 participants' arguments on 56 claims, including LLM-generated (Claude 3 Opus) and human-generated arguments, plus a control group. It measures cognitive effort using readability (Flesch-Kincaid score) and lexical complexity (perplexity), and analyzes moral-emotional language using sentiment (AFINN dictionary) and moral foundation theory (eMFD dictionary). Independent sample t-tests compare LLM and human arguments on each metric, with FDR correction for multiple comparisons.

## Key Results
- LLM-generated arguments exhibit higher cognitive effort, demonstrated by more complex grammatical and lexical structures.
- LLMs show a greater reliance on moral language, utilizing both positive and negative moral foundations more frequently than humans.
- No significant difference was found in the emotional content between LLM-generated and human-generated arguments.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs achieve human-level persuasion through increased cognitive effort in arguments, contrary to prior findings that lower cognitive effort boosts persuasion.
- Mechanism: LLMs generate arguments with higher grammatical and lexical complexity, requiring more cognitive processing, which engages readers more deeply and leads to persuasion.
- Core assumption: Higher cognitive effort signals argument substance or importance, prompting deeper engagement and persuasion.
- Evidence anchors:
  - [abstract] "LLM-generated arguments exhibit higher cognitive effort, demonstrated by more complex grammatical and lexical structures."
  - [section] "our results indicate that LLM arguments, which require higher cognitive effort due to increased grammatical and lexical complexity, are as persuasive as human-authored arguments."
  - [corpus] Weak; corpus focuses on LLM persuasion capabilities but not the cognitive effort mechanism specifically.
- Break condition: If readers perceive high complexity as overwhelming rather than engaging, or if complexity does not correlate with perceived argument quality.

### Mechanism 2
- Claim: LLMs leverage moral-emotional language more than humans to achieve persuasion, particularly negative moral foundations like harm and cheating.
- Mechanism: LLMs use more moral language, both positive and negative, to tap into inherent human responsiveness to moral cues, enhancing persuasive impact.
- Core assumption: Moral language resonates more deeply with readers than emotional language alone, driving persuasion.
- Evidence anchors:
  - [abstract] "LLMs demonstrate a significant propensity to engage more deeply with moral language, utilizing both positive and negative moral foundations more frequently than humans."
  - [section] "LLMs tend to incorporate more moral language into their arguments than humans do" and "LLMs utilize negative moral foundations more frequently, particularly harm and cheating-related language."
  - [corpus] Weak; corpus neighbors discuss LLM persuasion but do not specifically address moral-emotional language usage.
- Break condition: If moral language is perceived as manipulative or if overuse of negative moral appeals backfires and reduces credibility.

### Mechanism 3
- Claim: LLMs achieve persuasion through tailored communication strategies based on prompts, adjusting cognitive effort and moral-emotional language use.
- Mechanism: LLMs can modify their persuasive approach (e.g., "Compelling Case," "Logical Reasoning," "Deceptive") to optimize for different persuasive styles and contexts.
- Core assumption: Different persuasive contexts benefit from different balances of complexity and moral-emotional appeal, and LLMs can strategically adjust these.
- Evidence anchors:
  - [abstract] "we repeat the analysis comparing different LLM prompts that elicit different persuasion strategies."
  - [section] "LLMs are able to adjust their arguments based on the prompt, applying persuasive strategies that require different levels of cognitive effort and different usage of moral-emotional language."
  - [corpus] Weak; corpus neighbors do not discuss prompt-based strategy variation in LLM persuasion.
- Break condition: If prompt-based adjustments do not translate to increased persuasion, or if certain prompts consistently fail across contexts.

## Foundational Learning

- Concept: Cognitive Effort in Text Processing
  - Why needed here: Understanding how complexity (grammatical and lexical) affects persuasion is central to the study's findings.
  - Quick check question: How do readability scores and perplexity relate to cognitive effort in text processing?

- Concept: Moral Foundations Theory (MFT)
  - Why needed here: The study relies on MFT to analyze moral language usage in LLM vs. human arguments.
  - Quick check question: What are the five moral foundations in MFT, and how do they map to positive and negative moral appeals?

- Concept: Sentiment Analysis
  - Why needed here: The study uses sentiment analysis to compare emotional content between LLM and human arguments.
  - Quick check question: How does sentiment analysis measure emotional polarity in text, and what does a neutral score indicate?

## Architecture Onboarding

- Component map: Data pipeline includes experiment data extraction (LLM, human, control arguments), readability and perplexity computation for cognitive effort, sentiment analysis, and moral foundation analysis using eMFD dictionary. Statistical testing (t-tests, proportions tests) compares LLM vs. human strategies.
- Critical path: Extract arguments → Compute cognitive effort metrics → Analyze moral-emotional language → Perform statistical comparisons → Interpret differences in persuasion strategies.
- Design tradeoffs: Balancing comprehensive linguistic analysis with computational efficiency; choosing appropriate thresholds for statistical significance while accounting for multiple comparisons (FDR correction).
- Failure signatures: Statistical tests show no significant differences where expected; metrics fail to capture nuanced persuasive elements; overfitting to prompt-based variations without generalization.
- First 3 experiments:
  1. Replicate readability and perplexity analysis on a new dataset to validate cognitive effort findings.
  2. Conduct a user study to measure perceived complexity vs. persuasiveness of LLM vs. human arguments.
  3. Test the effect of varying moral language intensity on persuasion outcomes to isolate the moral-emotional mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does awareness of interacting with an LLM affect the persuasion route (central vs peripheral) and the relative effectiveness of rational versus moral-emotional arguments?
- Basis in paper: [explicit] The paper discusses how prior research shows rational arguments are more effective when participants know they are interacting with an LLM, while their results show different outcomes when participants are unaware of the LLM source.
- Why unresolved: The paper identifies this as a key difference but does not experimentally test how awareness affects persuasion mechanisms.
- What evidence would resolve it: Experimental studies comparing persuasion effectiveness when participants are aware vs unaware they are interacting with an LLM, measuring both argument type effectiveness and processing routes.

### Open Question 2
- Question: What are the long-term effects of exposure to LLM-generated persuasive content on individuals' beliefs and behaviors?
- Basis in paper: [explicit] The authors note that their study did not examine long-term impacts of LLM persuasion on beliefs and behaviors, suggesting this as an important avenue for future research.
- Why unresolved: The study was cross-sectional, measuring only immediate persuasion effects, not tracking changes over time.
- What evidence would resolve it: Longitudinal studies tracking participants' attitudes and behaviors over extended periods following exposure to LLM-generated content.

### Open Question 3
- Question: How does cognitive load or distraction influence the persuasiveness of LLM-generated arguments, particularly those using moral-emotional language?
- Basis in paper: [explicit] The paper mentions that future research should determine if LLM-generated arguments are more persuasive under higher cognitive load, potentially shifting processing to peripheral routes.
- Why unresolved: The study did not manipulate cognitive load or measure its interaction with LLM persuasiveness.
- What evidence would resolve it: Experiments varying cognitive load levels while measuring persuasion effectiveness of LLM arguments with different characteristics (e.g., moral-emotional content).

## Limitations
- The analysis is constrained by the specific dataset and experimental conditions, which may not generalize to all persuasive contexts or argument types.
- The study relies on computational metrics (readability, perplexity, sentiment, moral language) that may not fully capture the nuanced cognitive and emotional processes involved in persuasion.
- The exact dataset and experiment details from Durmus et al. (2024) are not publicly available, making it difficult to replicate the specific analysis.

## Confidence
- High: The finding that LLM-generated arguments exhibit higher cognitive effort (more complex grammatical and lexical structures) compared to human arguments is well-supported by the data and analysis.
- Medium: The conclusion that LLMs rely more on moral language, particularly negative moral foundations, is supported but may be context-dependent and require further validation across diverse datasets.
- Medium: The observation that there is no significant difference in emotional content between LLM and human arguments is statistically sound but may not fully capture the role of emotion in persuasion.

## Next Checks
1. Replicate Cognitive Effort Analysis: Conduct a new study with a larger and more diverse dataset to validate the finding that LLM-generated arguments have higher cognitive effort (complexity) than human arguments.

2. User Perception Study: Design an experiment to measure how readers perceive the complexity and persuasiveness of LLM vs. human arguments, to test whether higher cognitive effort translates to deeper engagement and persuasion.

3. Moral Language Intensity Test: Perform an experiment varying the intensity of moral language in LLM-generated arguments to isolate the effect of moral-emotional language on persuasion outcomes.