---
ver: rpa2
title: Evaluation of RAG Metrics for Question Answering in the Telecom Domain
arxiv_id: '2407.12873'
source_url: https://arxiv.org/abs/2407.12873
tags:
- metrics
- answer
- evaluation
- context
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates RAGAS, a widely-used LLM-based framework for
  assessing RAG system performance, specifically in the telecom domain. The authors
  enhanced the RAGAS codebase to expose intermediate outputs of its evaluation prompts,
  enabling detailed analysis.
---

# Evaluation of RAG Metrics for Question Answering in the Telecom Domain

## Quick Facts
- **arXiv ID:** 2407.12873
- **Source URL:** https://arxiv.org/abs/2407.12873
- **Reference count:** 13
- **Primary result:** Factual Correctness and Faithfulness metrics are most aligned with human expert judgment for telecom QA tasks.

## Executive Summary
This paper evaluates RAGAS, a widely-used LLM-based framework for assessing RAG system performance, specifically in the telecom domain. The authors enhanced the RAGAS codebase to expose intermediate outputs of its evaluation prompts, enabling detailed analysis. They found that two metrics—Factual Correctness and Faithfulness—are most aligned with human expert judgment for telecom QA tasks. Domain adaptation of both retriever and generator components improved metric reliability, and using both metrics jointly further enhanced correctness detection. However, other metrics like Context Relevance and Answer Relevance were found to be less interpretable or unreliable due to issues such as sentence weighting and reliance on cosine similarity. The study provides actionable insights for evaluating RAG systems in specialized domains and highlights the importance of domain adaptation and metric selection.

## Method Summary
The authors modified the RAGAS codebase to expose intermediate outputs of LLM evaluation prompts, enabling analysis of the internal reasoning behind metric scores. They evaluated six RAGAS metrics (Faithfulness, Answer Relevance, Context Relevance, Answer Similarity, Factual Correctness, Answer Correctness) on a telecom QA dataset using both base and domain-adapted retriever and generator models. Expert evaluations were used to compare metric alignment with human judgment, and the impact of domain adaptation and instruction tuning on metric reliability was assessed.

## Key Results
- Factual Correctness and Faithfulness metrics align best with human expert judgment for telecom QA tasks.
- Domain adaptation of retriever and generator components improves RAGAS metric reliability.
- Using Factual Correctness and Faithfulness jointly enhances correctness detection in RAG systems.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RAGAS metrics can be made interpretable by exposing intermediate outputs.
- **Mechanism:** By modifying the RAGAS library to store the intermediate outputs of LLM prompts (e.g., generated statements, verdicts), researchers can analyze the internal reasoning process and identify where metric scores diverge from human judgment.
- **Core assumption:** The intermediate LLM outputs directly reflect the reasoning behind the final metric score.
- **Evidence anchors:**
  - [abstract] "One of the outcomes of this work is a modified version of this package for few metrics (faithfulness, context relevance, answer relevance, answer correctness, answer similarity and factual correctness) through which we provide the intermediate outputs of the prompts by using any LLMs."
  - [section] "To address this, we store the intermediate outputs and verdicts."
  - [corpus] Weak - no direct corpus evidence on intermediate output analysis.
- **Break condition:** If LLM reasoning is opaque or inconsistent, intermediate outputs may not reliably explain metric scores.

### Mechanism 2
- **Claim:** Factual Correctness and Faithfulness metrics align better with human expert judgment in telecom QA than other RAGAS metrics.
- **Mechanism:** These two metrics focus on exact statement matching and context-grounded generation, which are critical in technical domains where factual precision is paramount.
- **Core assumption:** Human experts prioritize factual accuracy and context fidelity over semantic similarity or relevance scoring.
- **Evidence anchors:**
  - [abstract] "We found that two metrics—Factual Correctness and Faithfulness—are most aligned with human expert judgment for telecom QA tasks."
  - [section] "Our results indicate that of these metrics, F aiF ul and AnsCor are perhaps best aligned with human expert judgment."
  - [corpus] Weak - no corpus evidence directly comparing metric alignment with human judgment.
- **Break condition:** If the telecom domain shifts toward more subjective or open-ended QA, semantic relevance metrics may become more important.

### Mechanism 3
- **Claim:** Domain adaptation of retriever and generator components improves RAGAS metric reliability.
- **Mechanism:** Fine-tuning embeddings and instruction-tuning LLMs on telecom data enhances their ability to retrieve and generate relevant, accurate content, which is reflected in improved metric scores.
- **Core assumption:** Domain-specific training aligns LLM embeddings and generation with the terminology and structure of telecom documents.
- **Evidence anchors:**
  - [abstract] "Domain adaptation of both retriever and generator components improved metric reliability."
  - [section] "We observe better concordance of RAGAS metrics with that of SME evaluation, if both the metrics are considered together. We also observe that domain adaptation of the LLM via IFT and PT-IFT improves the scores significantly."
  - [corpus] Weak - no corpus evidence on domain adaptation effects on metrics.
- **Break condition:** If domain adaptation introduces overfitting to specific document styles, generalization to new telecom queries may suffer.

## Foundational Learning

- **Concept:** Cosine similarity limitations in embedding spaces
  - **Why needed here:** RAGAS uses cosine similarity for answer relevance and similarity metrics, but these can be unreliable due to embedding isotropy and lack of semantic thresholds.
  - **Quick check question:** What are two known weaknesses of cosine similarity in sentence embeddings?

- **Concept:** RAG pipeline components (retriever + generator)
  - **Why needed here:** Understanding how retrievers and generators interact is essential for interpreting metric behavior and the impact of domain adaptation.
  - **Quick check question:** How does the choice of k retrieved contexts affect generator performance and metric scores?

- **Concept:** Prompt engineering for LLM-based evaluation
  - **Why needed here:** RAGAS relies on carefully crafted prompts to extract statements, generate questions, and classify factual correctness; modifying these prompts can improve metric interpretability.
  - **Quick check question:** What is the role of the verdict generation prompt in the Faithfulness metric?

## Architecture Onboarding

- **Component map:** Retriever (embedding model + cosine similarity) -> Generator (LLM) -> RAGAS Evaluator (modified to expose intermediate outputs)
- **Critical path:** Question -> Retriever (top-k contexts) -> Generator (RAG response) -> RAGAS metrics (intermediate outputs stored)
- **Design tradeoffs:** Exposing intermediate outputs increases transparency but may add latency; using multiple metrics jointly improves accuracy but complicates decision logic.
- **Failure signatures:** Low Faithfulness with high Factual Correctness may indicate out-of-context generation; high Context Relevance with low Answer Relevance may signal poor answer grounding.
- **First 3 experiments:**
  1. Run RAG pipeline with base embeddings, store intermediate outputs, compare metric scores to SME judgments.
  2. Fine-tune retriever embeddings on telecom corpus, rerun, and analyze metric changes.
  3. Instruction-tune generator LLM, rerun, and evaluate impact on Factual Correctness and Faithfulness scores.

## Open Questions the Paper Calls Out
None

## Limitations
- The telecom domain QA dataset used is small and focused on a narrow technical area, limiting generalizability.
- The modified RAGAS codebase is not publicly available, preventing independent verification.
- The paper lacks direct corpus evidence comparing metric alignment with human judgment.
- The study does not explore the impact of varying the number of retrieved contexts (k) on metric reliability.

## Confidence
- **High Confidence:** Factual Correctness and Faithfulness metrics are most aligned with human expert judgment (supported by experimental results).
- **Medium Confidence:** Domain adaptation improves metric reliability (supported by score improvements but lacks corpus-level evidence).
- **Low Confidence:** Interpretability of intermediate LLM outputs as a mechanism for understanding metric behavior (proposed but not empirically validated).

## Next Checks
1. Replicate Intermediate Output Analysis: Obtain or reconstruct the modified RAGAS codebase and rerun the evaluation pipeline to verify that intermediate outputs can be reliably extracted and analyzed for all six metrics.
2. Expand Dataset and Domain Coverage: Test the metric evaluation framework on a larger, more diverse telecom QA dataset and assess whether Factual Correctness and Faithfulness remain the most reliable metrics across different telecom subdomains.
3. Investigate k Retrieval Impact: Systematically vary the number of retrieved contexts (k) in the RAG pipeline and analyze how this affects the performance of each RAGAS metric, particularly Factual Correctness and Faithfulness, to identify optimal k values for telecom QA tasks.