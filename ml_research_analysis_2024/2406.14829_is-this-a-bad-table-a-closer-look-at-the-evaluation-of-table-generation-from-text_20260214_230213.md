---
ver: rpa2
title: Is This a Bad Table? A Closer Look at the Evaluation of Table Generation from
  Text
arxiv_id: '2406.14829'
source_url: https://arxiv.org/abs/2406.14829
tags:
- table
- tables
- statements
- quality
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating the quality of tables
  generated from text descriptions. Existing metrics fail to capture overall table
  semantics and can unfairly penalize good tables or reward bad ones.
---

# Is This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text

## Quick Facts
- arXiv ID: 2406.14829
- Source URL: https://arxiv.org/abs/2406.14829
- Reference count: 40
- Primary result: TabEval shows significantly higher correlation with human judgments of table quality compared to existing metrics

## Executive Summary
This paper addresses a critical challenge in text-to-table generation: how to reliably evaluate the quality of generated tables. Existing metrics fail to capture overall table semantics, often penalizing good tables or rewarding bad ones by evaluating cells independently without considering contextual information. The authors propose TabEval, a novel two-stage evaluation strategy that first unrolls tables into natural language atomic statements using an LLM, then compares these statements with ground truth using entailment-based measures. Experiments on four datasets show TabEval achieves significantly higher correlation with human judgments compared to existing metrics, particularly in unsupervised settings.

## Method Summary
TabEval is a two-stage evaluation pipeline for text-to-table generation. First, the TabUnroll component uses an LLM (GPT-4 or Claude-3-Opus) with Chain-of-Thought prompting to decompose both reference and generated tables into atomic natural language statements. Second, entailment-based scoring using RoBERTa-large-mnli computes precision, recall, and F1 scores between the unrolled statements. The method was validated using a newly curated dataset of 1,250 diverse Wikipedia tables with text descriptions, and tested across four datasets (DescToTTo, RotoWire, WikiBio, WikiTableText) using various text-to-table generation methods.

## Key Results
- TabEval achieves significantly higher correlation with human judgments of table quality compared to existing metrics across four datasets
- The method shows particularly strong performance in unsupervised settings where no reference tables are available
- Existing metrics that evaluate table cells independently without contextual information fail to capture overall table semantics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Existing table evaluation metrics fail because they treat table cells independently without considering contextual information from neighboring cells.
- Mechanism: The proposed method TabEval captures overall table semantics by first breaking down a table into a list of natural language atomic statements and then compares them with ground truth using entailment-based measures.
- Core assumption: Tables can be accurately decomposed into atomic statements that preserve semantic meaning, and these statements can be meaningfully compared using entailment measures.
- Evidence anchors:
  - [abstract] "We propose TabEval, a novel table evaluation strategy that captures table semantics by first breaking down a table into a list of natural language atomic statements and then compares them with ground truth statements using entailment-based measures."
  - [section] "A major limitation with such measures is that they evaluate the table cells (or tuples) independently without considering contextual information from the neighboring cells. This can lead to incorrect penalization of good tables, or incorrect rewarding of bad tables."
- Break condition: If the LLM fails to generate accurate atomic statements or if entailment measures cannot capture semantic equivalence, the method will break down.

### Mechanism 2
- Claim: TabEval shows significantly higher correlation with human judgments of table quality compared to existing metrics.
- Mechanism: By evaluating tables holistically through natural language statements and entailment, TabEval better captures what humans consider important in table quality.
- Core assumption: Human judgments of table quality align with semantic completeness and correctness rather than exact cell matching.
- Evidence anchors:
  - [abstract] "Experiments on four datasets using various text-to-table generation methods show that TabEval has significantly higher correlation with human judgments of table quality compared to existing metrics, particularly in unsupervised settings."
  - [section] "TABEVAL shows significantly higher correlations with human ratings compared to the existing metrics across most scenarios."
- Break condition: If human judgment criteria differ significantly from semantic completeness (e.g., if humans value exact formatting or specific terminology), correlation would drop.

### Mechanism 3
- Claim: The two-stage pipeline of table unrolling followed by entailment scoring provides a more robust evaluation than direct table comparison.
- Mechanism: Breaking tables into natural language statements first allows the evaluation to focus on semantic content rather than structural representation, while entailment measures can handle paraphrasing and variation in expression.
- Core assumption: Natural language statements extracted from tables preserve semantic meaning and can be compared using entailment measures that handle linguistic variation.
- Evidence anchors:
  - [section] "We then compute the entailment scores between the unrolled NL statements of predicted and ground truth tables and provide an aggregate as the measure of table quality."
  - [section] "Entailment-based Scoring. After obtaining the unrolled statements from the ground truth and predicted tables... we employ Natural Language Inference (Liu et al., 2019) to determine whether the information conveyed by the predicted table is also present in the ground truth table, and vice versa."
- Break condition: If the table unrolling process introduces significant errors or if entailment models cannot accurately capture semantic equivalence, the two-stage approach may perform worse than direct comparison.

## Foundational Learning

- Concept: Natural Language Inference (NLI) and entailment measures
  - Why needed here: TabEval uses entailment-based scoring to compare unrolled statements from generated and reference tables
  - Quick check question: What does it mean for one statement to entail another, and how is this different from exact matching?

- Concept: Chain-of-Thought prompting
  - Why needed here: TabUnroll uses Chain-of-Thought prompting with an LLM to decompose tables into atomic statements
  - Quick check question: How does Chain-of-Thought prompting help LLMs perform complex reasoning tasks like table unrolling?

- Concept: Table semantics and atomic statements
  - Why needed here: The method relies on breaking tables into meaningful atomic statements that preserve semantic content
  - Quick check question: What makes a statement "atomic" in the context of table unrolling, and why is this important for evaluation?

## Architecture Onboarding

- Component map: Text description → Table generation → TabEval pipeline (Unrolling → Entailment scoring) → Metric scores → Correlation with human ratings
- Critical path: Text description → Table generation → TabEval pipeline (Unrolling → Entailment scoring) → Metric scores → Correlation with human ratings
- Design tradeoffs:
  - LLM-based unrolling provides semantic understanding but introduces computational cost and potential hallucination
  - Entailment measures handle semantic variation but may miss structural nuances
  - Two-stage pipeline adds complexity but improves semantic evaluation
- Failure signatures:
  - Low correlation with human ratings despite high technical performance
  - Inconsistent results across different datasets or generation methods
  - Excessive computational cost making the metric impractical for large-scale use
- First 3 experiments:
  1. Implement TabUnroll with a simple LLM (e.g., GPT-3.5) on a small table dataset to verify basic functionality
  2. Compare entailment-based scoring with existing metrics (BERTScore, exact match) on a small set of generated tables
  3. Run correlation analysis between TabEval scores and human ratings on a subset of the DESC TOTTO dataset

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several remain:

### Open Question 1
- Question: How does the performance of TabEval change when evaluating tables with more complex structures, such as nested tables or tables with hierarchical relationships?
- Basis in paper: The paper mentions that TabEval focuses on general and domain-specific tables with relatively simpler structures, and future work includes evaluating more complex tables.
- Why unresolved: The paper does not provide experimental results or analysis on the performance of TabEval with complex table structures.
- What evidence would resolve it: Experimental results showing the correlation of TabEval with human judgments on tables with nested structures, hierarchical relationships, or multiple tables derived from a single text.

### Open Question 2
- Question: How does the computational cost of TabEval compare to existing metrics, especially when using larger language models like GPT-4 for table unrolling?
- Basis in paper: The paper mentions that there is a tradeoff while using large models, as they can be computationally expensive, and notes that using GPT-4 and LLaMa variants can be costly.
- Why unresolved: The paper does not provide a detailed comparison of the computational costs of TabEval versus existing metrics.
- What evidence would resolve it: A comparative analysis of the time and resource requirements for TabEval and existing metrics, including the cost implications of using different language models for table unrolling.

### Open Question 3
- Question: How robust is TabEval to variations in table intent descriptions, and does it maintain high correlation with human judgments across different types of table intents?
- Basis in paper: The paper introduces TabEval, which uses table intent (table name/caption/description) as part of the input for table unrolling, but does not explore the impact of variations in table intent descriptions.
- Why unresolved: The paper does not provide experiments or analysis on how variations in table intent descriptions affect the performance of TabEval.
- What evidence would resolve it: Experimental results showing the correlation of TabEval with human judgments when using different types of table intent descriptions, such as varying levels of detail or different phrasing styles.

## Limitations
- Reliance on LLM-based table unrolling introduces computational overhead and potential hallucination risks
- Quality of unrolled statements is critical, and poor unrolling quality could propagate errors through the entailment scoring stage
- Requires access to capable LLMs (GPT-4 or Claude-3-Opus), which may limit accessibility for some research groups

## Confidence

- High confidence: The overall effectiveness of TabEval compared to existing metrics (based on human correlation studies)
- Medium confidence: The robustness of the two-stage pipeline approach across different table types and generation methods
- Low confidence: The generalizability of the TabUnroll prompting strategy to other LLM models or different table domains

## Next Checks

1. **Ablation study on LLM choice**: Compare TabEval performance using different LLM models (GPT-4, Claude-3-Opus, GPT-3.5) to quantify the impact of model choice on unrolling quality and final correlation scores.

2. **Cross-domain evaluation**: Apply TabEval to tables from domains not represented in the curated dataset (e.g., scientific tables, financial data) to test generalizability beyond Wikipedia-style tables.

3. **Human evaluation of unrolled statements**: Conduct a separate human study specifically focused on evaluating the quality and accuracy of the unrolled natural language statements, independent of the final table quality assessment.