---
ver: rpa2
title: Comparison of Machine Learning Classification Algorithms and Application to
  the Framingham Heart Study
arxiv_id: '2402.15005'
source_url: https://arxiv.org/abs/2402.15005
tags:
- training
- testing
- equal
- positive
- proportional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates machine learning classification algorithms
  for healthcare using the Framingham coronary heart disease dataset, focusing on
  generalizability and fairness. The author introduces the double discriminant scoring
  method, combining linear and quadratic discriminant functions, and compares its
  performance against seven other algorithms (including XGBoost and SVM) across four
  training/testing scenarios.
---

# Comparison of Machine Learning Classification Algorithms and Application to the Framingham Heart Study

## Quick Facts
- arXiv ID: 2402.15005
- Source URL: https://arxiv.org/abs/2402.15005
- Authors: Nabil Kahouadji
- Reference count: 40
- Primary result: Double Discriminant Scoring Type I consistently outperforms other algorithms on Framingham CHD data across all training/testing scenarios with true positive rates above 75%

## Executive Summary
This study investigates machine learning classification algorithms for healthcare using the Framingham coronary heart disease dataset, focusing on generalizability and fairness. The author introduces the double discriminant scoring method, combining linear and quadratic discriminant functions, and compares its performance against seven other algorithms (including XGBoost and SVM) across four training/testing scenarios. The study shows that both XGBoost and SVM perform poorly when trained on imbalanced datasets, while the double discriminant scoring of type I consistently outperforms others with true positive rates above 75% across all scenarios. A novel methodology for extracting optimal variable hierarchies is introduced, reducing computational complexity from 2^p-1 to at most p(p+1)/2 tests. For the Framingham data, excluding systolic blood pressure yielded optimal prediction performance, with age, diastolic blood pressure, and smoking being the most important variables.

## Method Summary
The study compares 8 classification algorithms (XGBoost, SVM, Random Forest, Logistic, Linear/Quadratic Discriminant, Double Discriminant Scoring Types I/II) on the Framingham CHD dataset using 4 training/testing scenarios with varying training ratios (0.1 to 0.9). Performance is evaluated across 100 simulations per algorithm, measuring true positive rate, true negative rate, accuracy, precision, and observed/expected prevalence. The double discriminant scoring method combines linear and quadratic discriminant functions using a liberal rule (accept if either discriminant says positive). A Bellman-based variable hierarchy extraction methodology is introduced to reduce computational complexity from 2^p-1 to p(p+1)/2 tests.

## Key Results
- Double Discriminant Scoring Type I consistently outperforms all other algorithms with true positive rates above 75% across all training/testing scenarios
- XGBoost and SVM perform poorly when trained on imbalanced datasets (15% prevalence), with true positive rates below 40%
- Excluding systolic blood pressure from the model yielded optimal prediction performance for the Framingham data
- Age, diastolic blood pressure, and smoking were identified as the most important variables
- Double Discriminant Scoring Type I achieved positive precision of 93% when testing prevalence was 15% and 71% when 50%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Balanced classifier cutoff equal to training prevalence improves logistic and random forest classifiers.
- Mechanism: Using a cutoff that matches the proportion of positive cases in the training data minimizes bias from class imbalance, leading to more stable true positive and true negative rates.
- Core assumption: The optimal cutoff is the prevalence of the training dataset, not the testing dataset.
- Evidence anchors:
  - [abstract] "Using the Framingham coronary heart disease data as a case study, we first show how a probability cutoff can be determined to effectively convert logistic [41] and random forest [44] regression models into classifiers."
  - [section] "A good classifier cutoff should minimize misclassification. As the classifier cutoff increases, the number of true negative (TN, green curve) increases, and the number of true positive (TP, blue curve) decreases. Thus an equilibrium classifier cutoff must strike a balance between the number of true positive and true negative cases."
  - [corpus] Weak: No direct evidence for prevalence-based cutoff in corpus neighbors.

### Mechanism 2
- Claim: Double Discriminant Scoring of Type I outperforms XGBoost and SVM when trained on imbalanced datasets.
- Mechanism: Combining linear and quadratic discriminant functions with a liberal rule (accept if either discriminant says positive) increases true positive rates and reduces bias toward the majority class.
- Core assumption: The combination of linear and quadratic discriminant functions captures both equal and unequal covariance structures, improving robustness.
- Evidence anchors:
  - [abstract] "We introduced and show that the double discriminant scoring of type I is the most generalizable as it consistently outperforms the other classification algorithms regardless of the training/testing scenario."
  - [section] "The Double Discriminant Scoring of Type 1 consistently outperformed all other algorithms for all training/testing scenarios and for all training ratio Ï„ = 0.1, 0.2, ..., 0.9 when comparing true positive rates using all seven explanatory variables."
  - [corpus] Weak: No direct evidence for double discriminant scoring in corpus neighbors.

### Mechanism 3
- Claim: Bellman principle enables efficient variable hierarchy extraction, reducing tests from 2^p - 1 to at most p(p+1)/2.
- Mechanism: Once the optimal single variable is found, only variable combinations that include it need to be tested, because optimal subsets satisfy the Bellman principle of optimality.
- Core assumption: The optimal subset for k+1 variables must contain the optimal subset for k variables.
- Evidence anchors:
  - [abstract] "Finally, we introduce a methodology to extract an optimal variable hierarchy for a classification algorithm, and illustrate it on the overall, male and female Framingham coronary heart disease data."
  - [section] "Using the mean true positive rate (out of 1000 simulations), the data frame of 127 mean true positive rates have been sorted from the highest to the lowest true positive rate for each of the four training/testing scenarios, and the top five variable selections are reported in Table 6."
  - [corpus] Weak: No direct evidence for Bellman-based variable selection in corpus neighbors.

## Foundational Learning

- Concept: Prevalence and its role in classification
  - Why needed here: The paper shows that choosing a classifier cutoff equal to training prevalence improves performance, so understanding prevalence is critical.
  - Quick check question: If a dataset has 20% positive cases, what cutoff should be used for a logistic classifier to balance false positives and false negatives?

- Concept: Confusion matrix metrics (TPR, TNR, precision, accuracy)
  - Why needed here: The paper uses these metrics to compare algorithms and detect bias, so they must be understood.
  - Quick check question: If a model predicts all cases as negative, what are its TPR, TNR, and accuracy if the prevalence is 15%?

- Concept: Bellman principle of optimality in dynamic programming
  - Why needed here: The paper uses this principle to reduce the number of variable subsets tested, so understanding it is essential.
  - Quick check question: If the optimal 2-variable subset is (X1, X4), which 3-variable subsets must be tested to find the optimal 3-variable subset?

## Architecture Onboarding

- Component map: Data ingestion -> Train/test split (4 scenarios) -> Model training (8 algorithms) -> Cutoff calibration -> Prediction -> Performance metrics -> Variable hierarchy extraction
- Critical path: Data split -> Model training -> Cutoff calibration -> Performance evaluation -> Variable selection
- Design tradeoffs: More algorithms -> higher computational cost but better comparison; balanced vs proportional splits -> different bias profiles; using all variables vs significant ones -> similar performance but less interpretability
- Failure signatures: XGBoost and SVM failing on proportional splits; low TPR despite high accuracy; variable hierarchy not satisfying Bellman principle
- First 3 experiments:
  1. Run logistic regression with 15% cutoff on proportional split, record TPR and accuracy.
  2. Run double discriminant scoring type I on equal split, compare to logistic.
  3. Extract optimal variable hierarchy for true positive rate and verify Bellman property.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Double Discriminant Scoring of Type 1 perform on healthcare datasets with different prevalence distributions than the Framingham CHD data?
- Basis in paper: [explicit] The paper shows that DDS1 consistently outperforms other algorithms across four training/testing scenarios with different prevalence distributions, but only uses the Framingham CHD dataset.
- Why unresolved: The study only tests DDS1 on one dataset with specific prevalence distributions (15% and 50%). Its generalizability to other healthcare datasets with different prevalence distributions remains unknown.
- What evidence would resolve it: Testing DDS1 on multiple healthcare datasets with varying prevalence distributions (e.g., cancer screening data with 1% prevalence, diabetes data with 10% prevalence) and comparing its performance to other algorithms across these different scenarios.

### Open Question 2
- Question: What is the theoretical explanation for why DDS1 is less sensitive to distributional shifts than other algorithms like XGBoost and SVM?
- Basis in paper: [explicit] The paper shows DDS1 is less sensitive to distributional shifts but does not provide a theoretical explanation for this phenomenon.
- Why unresolved: While empirical results demonstrate DDS1's robustness, the mathematical or statistical properties that make it less sensitive to distributional shifts compared to other algorithms are not explained.
- What evidence would resolve it: A theoretical analysis comparing the mathematical properties of DDS1 (combining linear and quadratic discriminant functions) with XGBoost and SVM, particularly focusing on how they handle changes in data distribution between training and testing sets.

### Open Question 3
- Question: Does the optimal variable hierarchy methodology work equally well for other performance metrics beyond true positive rate and true negative rate?
- Basis in paper: [explicit] The paper demonstrates the methodology for true positive rate and true negative rate, showing it reduces tests from 2^p-1 to p(p+1)/2, but doesn't test other metrics.
- Why unresolved: The Bellman principle of optimality is demonstrated for TPR and TNR, but the methodology's effectiveness for other metrics like precision, F1-score, or AUC has not been tested.
- What evidence would resolve it: Applying the variable hierarchy methodology to other common performance metrics and comparing the resulting hierarchies and computational efficiency against the brute-force approach of testing all 2^p-1 variable subsets.

## Limitations
- Double discriminant scoring method lacks external validation on datasets beyond Framingham CHD
- Bellman-based variable hierarchy extraction assumes performance metric follows Bellman's principle of optimality, which may not hold for all classification objectives
- Study does not address potential temporal biases in the Framingham data spanning multiple decades of medical practice

## Confidence
- High confidence: Performance comparison methodology, true positive rate measurements, and identification of XGBoost/SVM failure modes on imbalanced datasets
- Medium confidence: Double discriminant scoring superiority claims (limited to one dataset)
- Low confidence: Bellman principle applicability to all classification scenarios and external generalizability of variable selection methodology

## Next Checks
1. Validate double discriminant scoring performance on at least two external healthcare datasets with different prevalence distributions
2. Test Bellman-based variable hierarchy extraction on datasets where performance metrics are known to violate optimality principles
3. Implement stratified sampling during training to confirm whether XGBoost and SVM performance improves on imbalanced datasets