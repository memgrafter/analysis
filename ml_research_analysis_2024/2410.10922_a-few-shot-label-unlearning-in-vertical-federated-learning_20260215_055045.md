---
ver: rpa2
title: A few-shot Label Unlearning in Vertical Federated Learning
arxiv_id: '2410.10922'
source_url: https://arxiv.org/abs/2410.10922
tags:
- unlearning
- uni00000013
- passive
- party
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the critical challenge of label unlearning
  in Vertical Federated Learning (VFL), where an active party aims to remove sensitive
  labels while preserving privacy. The proposed method tackles the risk of label leakage
  by leveraging a limited amount of private data, using manifold mixup to augment
  forward embeddings and gradient ascent to erase label information from both active
  and passive models.
---

# A few-shot Label Unlearning in Vertical Federated Learning

## Quick Facts
- arXiv ID: 2410.10922
- Source URL: https://arxiv.org/abs/2410.10922
- Authors: Hanlin Gu; Hong Xi Tae; Chee Seng Chan; Lixin Fan
- Reference count: 40
- One-line primary result: The method achieves effective label unlearning in vertical federated learning within seconds using limited private data while maintaining model utility.

## Executive Summary
This paper addresses the critical challenge of label unlearning in Vertical Federated Learning (VFL), where an active party aims to remove sensitive labels while preserving privacy. The proposed method tackles the risk of label leakage by leveraging a limited amount of private data, using manifold mixup to augment forward embeddings and gradient ascent to erase label information from both active and passive models. The approach is efficient, completing unlearning within seconds, and effective across diverse datasets including MNIST, CIFAR10, CIFAR100, and ModelNet. Extensive experiments demonstrate strong unlearning effectiveness while maintaining utility, with the method outperforming baselines in both unlearning effectiveness and runtime efficiency.

## Method Summary
The method introduces a few-shot label unlearning approach for VFL systems. The active party (holding labels) and K passive parties (holding features) collaborate to unlearn specific classes. The process involves three main steps: (1) Forward pass where passive parties compute embeddings and transfer them to the active party, (2) Unlearning where the active party performs manifold mixup on embeddings to create synthetic data, then applies gradient ascent to unlearn both active and passive models, transferring inverse gradients to passive parties, and (3) Model updates where passive parties independently update their models using the received gradients. The method only requires a small labeled dataset (np << n) to achieve effective unlearning while preserving privacy.

## Key Results
- Achieves effective unlearning across diverse datasets (MNIST, CIFAR10, CIFAR100, ModelNet) with limited private data
- Completes unlearning within seconds, demonstrating superior runtime efficiency compared to baselines
- Maintains strong model utility on retained data while effectively removing label information from data to be unlearned
- Demonstrates robustness in scenarios with multiple passive parties and varying unlearning targets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Manifold mixup augments forward embeddings to address label scarcity while flattening state distributions, making gradient ascent more effective.
- **Mechanism**: Mixup interpolates forward embeddings of different samples (rather than raw features) to create synthetic embeddings. This increases the effective number of labeled samples for unlearning without transferring original labels.
- **Core assumption**: The synthetic embeddings preserve sufficient label information structure while providing enough diversity to enable effective unlearning.
- **Evidence anchors**:
  - [abstract]: "Our method leverages a limited amount of labeled data, utilizing manifold mixup to augment the forward embedding of insufficient data"
  - [section 4.1]: "we adopt the manifold mixup mechanism (Verma et al., 2019) by interpolating hidden embeddings rather than directly mixing the features"
  - [corpus]: Weak - no direct citations to Verma et al. 2019 in neighbor papers
- **Break condition**: If the synthetic embeddings don't capture sufficient label-related information, gradient ascent will be ineffective at removing label influence.

### Mechanism 2
- **Claim**: Gradient ascent on augmented embeddings effectively erases label information from both active and passive models.
- **Mechanism**: The active party performs gradient ascent on the mixed embeddings to unlearn the active model, then transfers inverse gradients to passive parties to unlearn their models independently.
- **Core assumption**: The transferred gradients contain sufficient information for passive parties to unlearn without requiring original labels.
- **Evidence anchors**:
  - [abstract]: "followed by gradient ascent on the augmented embeddings to erase label information from the models"
  - [section 4.2]: "the active party performs gradient ascent on the mixed embeddings to unlearn the active model, and subsequently transfers the inverse gradients to the passive party to facilitate the unlearning of the passive model independently"
  - [corpus]: Missing - no neighbor papers discuss gradient ascent for unlearning
- **Break condition**: If the gradient transfer leaks label information or is insufficient for effective unlearning at passive parties.

### Mechanism 3
- **Claim**: Using only a small labeled dataset minimizes label privacy leakage risk during unlearning.
- **Mechanism**: By limiting the number of labels shared with passive parties (np << n), the method reduces the risk of label inference attacks during unlearning.
- **Core assumption**: The passive party's ability to infer labels is proportional to the number of labels they observe during unlearning.
- **Evidence anchors**:
  - [section 3.1]: "We assume that the passive party possesses corresponding labels for a limited number of features, defined as Dp = {(xpk, yp)}K k=1 = {{(xpk,i, yi)}np i=1}K k=1, where np << n"
  - [section 3.2]: "We assume that the active party discloses a limited number of labels to the passive party to facilitate the unlearning of a specific class"
  - [corpus]: Weak - no neighbor papers discuss privacy leakage in vertical federated unlearning
- **Break condition**: If passive parties can infer labels from gradients or other information despite limited label disclosure.

## Foundational Learning

- **Concept: Vertical Federated Learning (VFL)**
  - Why needed here: The paper specifically addresses unlearning in VFL where one party holds labels and others hold features, creating unique privacy challenges
  - Quick check question: What distinguishes VFL from horizontal federated learning in terms of data partitioning?

- **Concept: Manifold mixup**
  - Why needed here: The method uses mixup on hidden embeddings rather than raw features to create synthetic data for unlearning
  - Quick check question: How does manifold mixup differ from standard mixup, and why is this difference important for privacy?

- **Concept: Gradient ascent for unlearning**
  - Why needed here: The method uses gradient ascent (rather than descent) to maximize loss on data to be forgotten
  - Quick check question: Why would gradient ascent be used for unlearning instead of the more common gradient descent?

## Architecture Onboarding

- **Component map**: Passive parties -> Active party (via forward embeddings) -> Manifold mixup module -> Gradient ascent module -> Inverse gradients -> Passive parties (independent updates)

- **Critical path**:
  1. Forward pass: Passive parties compute embeddings → Active party performs mixup
  2. Unlearning: Active party performs gradient ascent → Transfers inverse gradients to passive parties
  3. Update: Passive parties update their models using received gradients

- **Design tradeoffs**:
  - Mixup vs. other augmentation: Mixup on embeddings preserves privacy better than raw feature mixing
  - Small learning rate vs. large: Small rate prevents model degradation but may require more epochs
  - Limited labels vs. full labels: Fewer labels reduce privacy risk but may reduce unlearning effectiveness

- **Failure signatures**:
  - Passive parties still can infer labels (indicates gradient leakage)
  - Model utility significantly degrades after unlearning (indicates overly aggressive unlearning)
  - Unlearning effectiveness is low (indicates insufficient gradient information or poor mixup quality)

- **First 3 experiments**:
  1. Single-class unlearning on MNIST with 1 passive party to verify basic functionality
  2. Two-class unlearning on CIFAR10 with 2 passive parties to test scalability
  3. Multi-class unlearning on CIFAR100 with 4 passive parties to test robustness under complex scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform in VFL settings with more than two passive parties?
- Basis in paper: [inferred] The paper states "we conduct an ablation study on the effectiveness of our method for different number of passive parties" but does not explore scenarios with more than two passive parties.
- Why unresolved: The experimental section only evaluates scenarios with one, two, and four passive parties, leaving uncertainty about scalability.
- What evidence would resolve it: Empirical results showing effectiveness and efficiency of the method with three or more passive parties in various datasets and unlearning scenarios.

### Open Question 2
- Question: What is the impact of varying the size of the private labeled data (Dp) on unlearning effectiveness?
- Basis in paper: [explicit] The paper mentions "we apply the gradient ascent with different size Dp to achieve unlearning" and shows results for two specific sizes, but does not explore a comprehensive range of sizes.
- Why unresolved: The study only compares two extreme cases (40 and 5000 samples), leaving uncertainty about the optimal size of Dp for different scenarios.
- What evidence would resolve it: A systematic study varying the size of Dp across a wide range and measuring its impact on unlearning effectiveness and utility across different datasets and models.

### Open Question 3
- Question: How does the proposed method perform under different privacy-preserving VFL mechanisms beyond differential privacy and gradient compression?
- Basis in paper: [inferred] The paper evaluates the method under differential privacy and gradient compression, but does not explore other privacy mechanisms like homomorphic encryption or secure multi-party computation.
- Why unresolved: The study is limited to two specific privacy mechanisms, leaving uncertainty about the method's applicability to other VFL privacy-preserving techniques.
- What evidence would resolve it: Empirical results showing the effectiveness of the proposed method under various privacy-preserving VFL mechanisms, including homomorphic encryption and secure multi-party computation, across different datasets and unlearning scenarios.

## Limitations

- Privacy guarantees for label unlearning remain theoretical with limited discussion of potential side-channel attacks or gradient leakage
- Method's scalability to large-scale VFL systems with many passive parties and high-dimensional features requires further validation
- Reliance on a small labeled dataset for unlearning may not generalize well when such data is scarce or when unlearning targets involve multiple complex classes

## Confidence

- **High confidence**: The core unlearning mechanism (manifold mixup + gradient ascent) is well-defined and empirically validated across multiple datasets
- **Medium confidence**: The privacy preservation claims, as they rely on the assumption that limited label disclosure prevents inference attacks without formal proof
- **Medium confidence**: The runtime efficiency claims, as the comparison focuses on specific baselines without comprehensive benchmarking across diverse hardware configurations

## Next Checks

1. **Gradient leakage analysis**: Systematically evaluate whether gradients transferred during unlearning contain sufficient information for passive parties to reconstruct label information, testing with various gradient compression techniques
2. **Scalability testing**: Evaluate the method's performance on VFL systems with 10+ passive parties and high-dimensional feature spaces (e.g., medical imaging or genomics data) to assess computational overhead and unlearning effectiveness
3. **Robustness to data scarcity**: Test the unlearning effectiveness when the available labeled dataset Dp is extremely limited (e.g., <1% of the target class samples) to determine the minimum viable dataset size for effective unlearning