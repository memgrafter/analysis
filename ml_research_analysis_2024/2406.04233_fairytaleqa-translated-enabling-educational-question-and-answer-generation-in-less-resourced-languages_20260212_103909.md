---
ver: rpa2
title: 'FairytaleQA Translated: Enabling Educational Question and Answer Generation
  in Less-Resourced Languages'
arxiv_id: '2406.04233'
source_url: https://arxiv.org/abs/2406.04233
tags:
- question
- text
- answer
- pairs
- translated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of Question Answering (QA) datasets
  for less-resourced languages, specifically focusing on educational purposes. The
  authors introduce machine-translated versions of FairytaleQA, a high-quality QA
  dataset designed to assess narrative comprehension skills in young children.
---

# FairytaleQA Translated: Enabling Educational Question and Answer Generation in Less-Resourced Languages

## Quick Facts
- arXiv ID: 2406.04233
- Source URL: https://arxiv.org/abs/2406.04233
- Authors: Bernardo Leite; Tomás Freitas Osório; Henrique Lopes Cardoso
- Reference count: 26
- Key outcome: Introduces machine-translated versions of FairytaleQA for Romance languages, establishing QA/QG benchmarks and a case study on Portuguese QA pair generation

## Executive Summary
This paper addresses the critical gap in educational QA datasets for less-resourced languages by introducing machine-translated versions of FairytaleQA, a high-quality dataset designed to assess narrative comprehension in young children. The authors establish baseline benchmarks for both QA and Question Generation tasks using fine-tuned T5 models on translated data, showing promising performance particularly for explicit questions. A case study demonstrates the potential for generating educational QA pairs in Portuguese, though challenges remain in ensuring effective alignment between generated questions and answers.

## Method Summary
The approach involves translating the FairytaleQA dataset (278 stories, 10,580 QA pairs) into Spanish, Portuguese (European and Brazilian), and French using DeepL, then fine-tuning monolingual T5 models for each language on the translated data. For QA and QG tasks, models are trained with specific hyperparameters (max 512 input tokens, 128 output tokens, 20 epochs, batch size 16, beam search 5). A separate Question-Answer Pair Generation (QAPG) model is trained using narrative labels as control attributes to generate diverse QA pairs. Human evaluation assesses generated pairs across five criteria: well-formedness, answerability, relevance, alignment, and children suitability.

## Key Results
- Fine-tuned T5 models achieve strong performance on QA tasks for explicit questions in translated datasets
- QG performance is generally lower than QA performance across all languages
- QAPG model can generate well-formed questions but struggles with alignment between questions and answers
- Back-translation reveals minor semantic discrepancies but overall preservation of meaning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine translation can produce usable QA datasets for less-resourced languages by leveraging existing high-quality datasets like FairytaleQA.
- Mechanism: By translating the original FairytaleQA dataset into multiple Romance languages using a commercial tool (DeepL), researchers can create new datasets that support QA and QG tasks in these languages.
- Core assumption: The quality of machine translation is sufficient to preserve the essential meaning and structure of questions and answers.
- Evidence anchors:
  - [abstract]: "To alleviate this gap, our paper introduces machine-translated versions of FairytaleQA, a renowned QA dataset designed to assess and enhance narrative comprehension skills in young children."
  - [section]: "We opted to translate all three components together, separated by a line break. This approach was chosen based on empirical evidence from preliminary test translations, indicating that translating questions and answers with improved contextualization of the section text led to greater coherence between these components."
  - [corpus]: Weak or missing - no direct evidence from corpus, but related work suggests machine translation is a common approach for extending datasets to new languages.
- Break condition: If the translation introduces significant errors or inconsistencies that compromise the quality of the QA pairs, such as changing the gender of subjects or losing the meaning of questions.

### Mechanism 2
- Claim: Fine-tuning modest-scale models on translated data can establish baseline benchmarks for QA and QG tasks in less-resourced languages.
- Mechanism: By using pre-trained T5 models and fine-tuning them on the translated FairytaleQA datasets, researchers can create models that perform QA and QG tasks in the target languages.
- Core assumption: The translated data is of sufficient quality to train effective models, and the T5 architecture is adaptable to different languages.
- Evidence anchors:
  - [abstract]: "By employing fine-tuned, modest-scale models, we establish benchmarks for both Question Generation (QG) and QA tasks within the translated datasets."
  - [section]: "Leveraging pre-trained T5 [17] encoder-decoder models, known for their remarkable performance in QA/QG tasks [20], we employ the t5-base version pre-trained on each language's data (monolingual)."
  - [corpus]: Weak or missing - no direct evidence from corpus, but related work suggests fine-tuning pre-trained models is a common approach for adapting models to new tasks or languages.
- Break condition: If the translated data is too noisy or inconsistent, or if the T5 models are not well-suited to the target languages, the performance of the fine-tuned models may be significantly degraded.

### Mechanism 3
- Claim: A Question-Answer Pair Generation (QAPG) model trained on translated data can generate QA pairs that are qualitatively similar to those used in real exams in a less-resourced language.
- Mechanism: By framing QAPG as a controllable QA pair generation task and using narrative labels as control attributes, researchers can train a model to generate diverse and multiple QA pairs from the same text, which can then be evaluated for quality.
- Core assumption: The QAPG model can learn to generate well-formed questions and aligned answers, and the evaluation criteria (well-formedness, answerability, relevance, and children suitability) are appropriate for assessing the quality of the generated QA pairs.
- Evidence anchors:
  - [abstract]: "Our motivation for generating both questions and answers is driven by the need for automated assessment of student answers in real-world educational scenarios."
  - [section]: "We frame QAPG as a controllable QA pair generation task, where narrative labels (recall Section 3.1) are used as control attributes."
  - [corpus]: Weak or missing - no direct evidence from corpus, but related work suggests that controllable generation and evaluation criteria are important for generating high-quality educational content.
- Break condition: If the QAPG model fails to generate well-formed questions, aligned answers, or QA pairs that meet the evaluation criteria, or if the evaluation criteria are not appropriate for the target language or audience.

## Foundational Learning

- Concept: Narrative comprehension
  - Why needed here: The FairytaleQA dataset is designed to assess narrative comprehension skills in young children, so understanding this concept is crucial for developing and evaluating QA and QG models.
  - Quick check question: What are the key elements of narrative comprehension, and how do they relate to the FairytaleQA dataset?

- Concept: Machine translation
  - Why needed here: Machine translation is used to create translated versions of the FairytaleQA dataset, so understanding its strengths and limitations is important for assessing the quality of the translated data.
  - Quick check question: What are the main challenges in machine translation, and how might they affect the quality of the translated FairytaleQA dataset?

- Concept: Fine-tuning pre-trained models
  - Why needed here: Fine-tuning is used to adapt pre-trained T5 models to the QA and QG tasks in the target languages, so understanding this process is crucial for developing and evaluating the models.
  - Quick check question: What are the key steps in fine-tuning a pre-trained model, and how might the quality of the translated data affect the fine-tuning process?

## Architecture Onboarding

- Component map: FairytaleQA dataset -> DeepL translation -> Monolingual T5 models -> QA/QG benchmarks -> QAPG model -> Human evaluation
- Critical path: 1. Translate FairytaleQA dataset into target languages using machine translation 2. Fine-tune pre-trained T5 models on translated data for QA and QG tasks 3. Generate QA pairs using QAPG model trained on translated data 4. Evaluate generated QA pairs using human evaluation
- Design tradeoffs: Using machine translation allows for rapid creation of datasets in multiple languages, but may introduce errors or inconsistencies; Fine-tuning pre-trained models is more efficient than training from scratch, but may not fully capture the nuances of the target languages; Human evaluation provides high-quality feedback on generated QA pairs, but is time-consuming and subjective
- Failure signatures: Low performance on QA and QG tasks despite fine-tuning; Generated QA pairs that are poorly formed, unanswerable, irrelevant, or unsuitable for children; High variance in human evaluation scores, indicating inconsistency in generated QA pairs
- First 3 experiments: 1. Evaluate the quality of the translated FairytaleQA dataset using human evaluation and automatic metrics 2. Fine-tune pre-trained T5 models on the translated dataset and evaluate their performance on QA and QG tasks 3. Train a QAPG model on the translated dataset and evaluate the quality of the generated QA pairs using human evaluation and automatic metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of modest-scale models trained on synthetic data compare to those trained on translated data for QA and QG tasks in less-resourced languages?
- Basis in paper: [inferred] The paper suggests comparing synthetic data generated by large language models (e.g., GPT-4) with translated data as a potential area for future work.
- Why unresolved: The paper does not provide experimental results comparing models trained on synthetic data versus translated data.
- What evidence would resolve it: Conduct an experiment training modest-scale models on both synthetic and translated data, then compare their performance on QA and QG tasks.

### Open Question 2
- Question: What are the specific types of errors made by modest-scale models in generating aligned QA pairs, and how can these errors be systematically addressed?
- Basis in paper: [explicit] The paper identifies the generation of aligned QA pairs as the primary challenge for the QAPG model and suggests exploring classification models to filter unwanted QA pairs.
- Why unresolved: The paper does not provide a detailed analysis of the specific error types or propose concrete solutions beyond using classification models.
- What evidence would resolve it: Conduct a detailed error analysis of QA pairs generated by modest-scale models and develop specific strategies to address each type of error.

### Open Question 3
- Question: How does the choice of translation tool and language pair affect the quality of translated QA datasets and the performance of downstream QA/QG models?
- Basis in paper: [explicit] The paper uses DeepL for translation and focuses on Romance languages, but does not explore the impact of different translation tools or language pairs.
- Why unresolved: The paper does not compare the performance of models trained on data translated using different tools or explore less common language pairs.
- What evidence would resolve it: Conduct experiments using different translation tools and language pairs, then compare the quality of the resulting datasets and the performance of QA/QG models trained on them.

## Limitations
- Quality of machine translation may introduce semantic discrepancies, particularly for complex narrative comprehension questions
- Monolingual T5 models may not capture cross-lingual relationships that could improve performance
- Human evaluation introduces subjectivity that may affect the reliability of QAPG quality assessments

## Confidence
**High Confidence**: The baseline QA/QG performance on translated FairytaleQA (particularly for explicit questions) is well-supported by the experimental results and aligns with expectations for fine-tuned T5 models.

**Medium Confidence**: The QAPG model's ability to generate qualitatively similar content to real educational materials is demonstrated but limited by the small scale of the case study and the subjective nature of human evaluation criteria.

**Low Confidence**: The generalizability of results to other less-resourced languages beyond Romance languages remains uncertain, as the translation approach and model architecture choices may not transfer equally well to linguistically distant languages.

## Next Checks
1. Conduct ablation studies comparing translation quality across different Romance languages to identify specific linguistic features that affect QA pair alignment.

2. Implement a bilingual T5 fine-tuning approach to test whether cross-lingual pre-training improves performance on translated FairytaleQA datasets.

3. Expand the QAPG case study to include automated alignment metrics (e.g., answerability prediction models) to complement human evaluation and reduce subjectivity in quality assessment.