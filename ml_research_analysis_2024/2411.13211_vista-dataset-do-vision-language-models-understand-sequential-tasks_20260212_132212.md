---
ver: rpa2
title: 'ViSTa Dataset: Do vision-language models understand sequential tasks?'
arxiv_id: '2411.13211'
source_url: https://arxiv.org/abs/2411.13211
tags:
- videos
- classes
- base
- clip
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ViSTa, a hierarchical dataset for evaluating
  vision-language models'' ability to understand sequential tasks. The dataset contains
  over 4,000 videos with step-by-step descriptions across three environments: virtual
  home, Minecraft, and real-world settings.'
---

# ViSTa Dataset: Do vision-language models understand sequential tasks?

## Quick Facts
- arXiv ID: 2411.13211
- Source URL: https://arxiv.org/abs/2411.13211
- Reference count: 40
- Key outcome: Vision-language models struggle with understanding sequential tasks despite strong object recognition, revealing fundamental limitations in current VLM architectures

## Executive Summary
This paper introduces ViSTa, a hierarchical dataset designed to evaluate vision-language models' (VLMs) ability to understand sequential tasks. The dataset contains over 4,000 videos across three environments (virtual home, Minecraft, and real-world) with step-by-step descriptions organized in eight complexity levels. The authors evaluate three state-of-the-art VLMs (CLIP, ViCLIP, and GPT-4o) using a video-description matching approach. While all models perform well on basic object recognition, they struggle significantly with understanding action order and sequential reasoning, with performance degrading as task complexity increases. Only GPT-4o achieves non-trivial performance on multi-step tasks, and even it shows substantial limitations in true sequential understanding.

## Method Summary
The paper evaluates vision-language models on sequential task understanding using the ViSTa dataset, which organizes 4,000+ videos into 8 hierarchical complexity levels across 3 environments. The evaluation uses video-description matching where models score similarity between videos and descriptions, selecting the highest-scoring match. Three VLMs are tested: CLIP (averaging frame embeddings), ViCLIP (using 8 frames), and GPT-4o (with frame-by-frame descriptions). Performance is measured using Macro F1 score across problem sets that isolate specific capabilities like object recognition, action order understanding, and sequential reasoning.

## Key Results
- All VLMs achieve high performance on basic object recognition tasks but show significant degradation on sequential understanding
- GPT-4o is the only model achieving non-trivial performance on multi-step tasks, yet still struggles with action order comprehension
- Models perform better on real-world videos than simulated environments, suggesting distributional shift issues
- Performance consistently degrades as task complexity increases, indicating fundamental limitations in sequential reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ViSTa's hierarchical structure enables fine-grained analysis of VLM sequential task understanding
- Mechanism: By organizing tasks from single actions to complex 8-step sequences, the dataset creates controlled complexity gradients that isolate when models fail
- Core assumption: Models' performance degradation is monotonic with task complexity
- Evidence anchors:
  - [abstract] "Its novel hierarchical structure -- basic single-step tasks composed into more and more complex sequential tasks -- allows a fine-grained understanding of how well VLMs can judge tasks with varying complexity"
  - [section 3] "ViSTa mirrors this structure: 1. Single-action videos (level 1)... 2. Multiple-action videos (levels 2 through 8)"
  - [corpus] Weak - only 5/25 neighbors mention sequential understanding, none discuss hierarchical task decomposition

### Mechanism 2
- Claim: Problem sets isolate specific capabilities (object recognition, action order) to diagnose model failures
- Mechanism: By grouping videos into classification problems targeting specific capabilities, the dataset distinguishes between object recognition failures and sequential reasoning failures
- Core assumption: Different capability types can be meaningfully separated through problem set design
- Evidence anchors:
  - [section 3.1] "ViSTa groups the video-description pairs into problem sets: classification problems testing specific capabilities"
  - [section 4.2] "We find the following: • Understanding action order is very hard... • General sequential tasks can be distinguished without understanding action order"
  - [corpus] Weak - no corpus evidence about capability isolation through problem set design

### Mechanism 3
- Claim: Multi-environment design tests VLM generalization across domains
- Mechanism: Including virtual home, Minecraft, and real-world environments creates distributional shifts that reveal whether models rely on environment-specific cues
- Core assumption: Performance differences across environments reflect genuine capability differences rather than dataset artifacts
- Evidence anchors:
  - [section 4.2] "Real videos are easier than simulations: Models do less well on tasks in the simulated virtual home environment, compared to analogous tasks in the real world"
  - [section 3.2] "ViSTa includes data from 3 different environments... to assess VLM capabilities in even more unfamiliar simulated environments"
  - [corpus] Weak - no corpus evidence about multi-environment VLM evaluation

## Foundational Learning

- Concept: Video-language matching as evaluation paradigm
  - Why needed here: The paper uses video-description matching to evaluate VLMs, requiring understanding of how similarity scoring works for multimodal embeddings
  - Quick check question: How does cosine similarity between video and text embeddings indicate matching quality?

- Concept: Hierarchical task decomposition
  - Why needed here: The dataset builds complexity by composing atomic actions, requiring understanding of how task sequences can be systematically constructed
  - Quick check question: If a 3-step task consists of actions A, B, and C, what would a 4-step task in the same hierarchy look like?

- Concept: Macro F1 score interpretation
  - Why needed here: The evaluation uses Macro F1 across problem sets, requiring understanding of how this metric handles class imbalance and performance aggregation
  - Quick check question: Why might Macro F1 be preferred over accuracy when evaluating models across problem sets with different numbers of classes?

## Architecture Onboarding

- Component map: Video collection → preprocessing → embedding generation → problem set construction → evaluation pipeline → performance aggregation

- Critical path:
  1. Load problem set definitions (video-description pairs grouped by capability)
  2. For each video, generate embeddings using appropriate model
  3. Compute similarity scores between video and all descriptions in problem set
  4. Select highest-scoring description as prediction
  5. Compute standardized scores using problem-set-specific normalization
  6. Aggregate results into performance metrics

- Design tradeoffs:
  - Frame rate vs. computational cost: CLIP uses 32 frames, ViCLIP uses 8, GPT-4o uses 16
  - Single vs. multi-label classification: Minecraft uses both depending on task ambiguity
  - Normalization scope: Global vs. problem-set-specific standardization affects comparability

- Failure signatures:
  - Near-random performance on permutation problems indicates inability to track action order
  - Consistent performance drop across levels suggests fundamental sequential reasoning limitations
  - Environment-specific performance gaps reveal over-reliance on domain-specific features

- First 3 experiments:
  1. Run baseline majority-class predictor to establish minimum performance threshold
  2. Test CLIP with varying frame counts (2, 4, 8, 16, 32) on level 1 object recognition to verify frame rate impact
  3. Evaluate GPT-4o on permutation problems at each level to identify the complexity threshold where action order understanding fails

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do vision-language models show consistent performance differences between simulated and real environments, or is this pattern environment-specific?
- Basis in paper: [explicit] The paper notes that models perform better on real videos than virtual home simulations, hypothesizing this is because simulations are more off-distribution.
- Why unresolved: The paper only compares one simulated environment (virtual home) to its real-world counterpart. The performance gap could be due to specific artifacts in virtual home or general simulation characteristics.
- What evidence would resolve it: Testing the same models on multiple simulated environments (like Habitat, iGibson, or custom simulations) versus their real-world counterparts would reveal whether this is a general pattern or specific to virtual home.

### Open Question 2
- Question: What specific aspects of sequential task understanding break down first as task complexity increases - action order, object tracking, or temporal continuity?
- Basis in paper: [inferred] The paper shows performance degradation with increasing task complexity but doesn't isolate which components fail first. They note models can match videos to descriptions using partial understanding of action order.
- Why unresolved: The experiments aggregate results across all aspects of sequential understanding without decomposing which specific capability (order tracking, object continuity, temporal reasoning) degrades first.
- What evidence would resolve it: Creating targeted problem sets that isolate specific aspects of sequential understanding (e.g., testing action order in isolation from object tracking) would reveal the breakdown pattern.

### Open Question 3
- Question: How do different frame rates and model architectures affect performance on general sequential tasks versus permutation tasks?
- Basis in paper: [explicit] The paper shows that frame rate and model scale are most important for general sequential tasks but less so for permutation problems, suggesting different mechanisms.
- Why unresolved: While the paper demonstrates this correlation, it doesn't explain why frame rate matters more for some tasks than others or what architectural features would optimize for each type.
- What evidence would resolve it: Systematic ablation studies varying frame rates, model architectures (attention mechanisms, temporal modeling), and task types would reveal which architectural choices best support different aspects of sequential understanding.

## Limitations
- The video-description matching paradigm may not fully capture genuine task comprehension, as models could exploit superficial correlations
- GPT-4o's evaluation method using frame-by-frame descriptions introduces potential prompt engineering artifacts not present in other models
- Performance differences across environments might be influenced by video quality differences rather than fundamental capability variations

## Confidence

- High confidence: CLIP and ViCLIP performance degradation across complexity levels (clear empirical pattern)
- Medium confidence: GPT-4o's superior performance reflects genuine understanding (complicated by frame description method)
- Medium confidence: Environment differences reflect model capabilities (alternative explanations like video quality differences exist)
- Low confidence: Hierarchical structure reveals specific failure points (non-monotonic performance not explored)

## Next Checks

1. Conduct ablation studies removing object recognition cues (e.g., masking objects in frames) to isolate true sequential reasoning capabilities
2. Test models on permuted descriptions with original videos to verify they're not relying on description-object correlations
3. Implement a control where models predict action order independently of video content to establish baselines for random sequential reasoning