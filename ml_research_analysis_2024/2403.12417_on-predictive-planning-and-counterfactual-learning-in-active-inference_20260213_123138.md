---
ver: rpa2
title: On Predictive planning and counterfactual learning in active inference
arxiv_id: '2403.12417'
source_url: https://arxiv.org/abs/2403.12417
tags:
- agent
- planning
- inference
- active
- dpefe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores two decision-making approaches within the
  active inference framework: planning-based methods and counterfactual learning from
  experience. The authors introduce a hybrid model that combines both strategies,
  allowing agents to dynamically balance predictive planning and experiential learning
  based on environmental feedback and confidence in prior knowledge.'
---

# On Predictive planning and counterfactual learning in active inference

## Quick Facts
- arXiv ID: 2403.12417
- Source URL: https://arxiv.org/abs/2403.12417
- Reference count: 9
- Primary result: Hybrid model combines planning-based and experience-based decision-making in active inference, achieving strong performance across varying environmental complexities

## Executive Summary
This paper introduces a hybrid decision-making model that combines predictive planning (DPEFE) and counterfactual learning (CL) within the active inference framework. The model dynamically balances these two approaches using an entropy-based bias parameter that updates online based on the relative confidence of each strategy. Tested in grid-world environments with environmental mutations, the hybrid approach demonstrates superior adaptability and performance compared to pure planning or pure experience-based methods.

The key contribution is a computationally efficient framework that explains how biological agents might balance multiple decision-making strategies while maintaining interpretability through internal parameter analysis. The model's ability to explain its own decision-making process through the evolution of risk and bias parameters provides insights into bio-mimetic artificial intelligence design.

## Method Summary
The hybrid model combines Dynamic Programming for Expected Free Energy (DPEFE) planning with Counterfactual Learning (CL) through a multiplicative decision rule weighted by an entropy-based bias parameter β. DPEFE performs predictive planning by evaluating expected free energy over future trajectories, while CL learns state-action mappings from experience with a risk-modulated update mechanism. The bias parameter β is updated online using the difference in Shannon entropy between the two strategies' action distributions, allowing the agent to dynamically allocate trust between planning and experience based on environmental feedback.

## Key Results
- Hybrid model successfully navigates the data-complexity tradeoff between pure planning and pure experience-based learning
- The model achieves strong performance across different task demands, including scenarios with environmental mutations
- Internal parameters (risk term and bias parameter) evolve to reflect the agent's decision-making strategy, providing explainability
- The approach demonstrates computational efficiency while maintaining adaptability to changing environments

## Why This Works (Mechanism)

### Mechanism 1
The mixed model balances planning and experiential learning by dynamically adjusting the bias parameter β based on the entropy of each strategy's action distribution. The agent computes Shannon entropy for both DPEFE and CL action distributions. The difference in entropy determines how much the agent should trust its prior experience (CL) versus predictive planning (DPEFE). Higher entropy in CL suggests more uncertainty, increasing reliance on DPEFE.

### Mechanism 2
The risk parameter Γ in the CL method reduces over time as the agent gains environmental knowledge, making decisions more exploitative. Γ starts high (0.9) when the agent is uncertain or at risk of failure, then decays linearly toward zero as the agent approaches the goal. This decay is inversely proportional to the remaining time to the goal.

### Mechanism 3
The hybrid decision rule PMM combines both strategies proportionally, enabling smooth transitions between exploration and exploitation. PMM = PCL^(1−β) · PDPEFE^β. When β is near 0, the agent relies on CL; when β is near 1, it relies on DPEFE. The interpolation ensures no abrupt switching.

## Foundational Learning

- **Concept: Variational Free Energy (VFE)**
  - Why needed here: VFE is the optimization objective in active inference; both DPEFE and CL are derived from minimizing different forms of VFE
  - Quick check question: What role does VFE play in distinguishing between planning-based and experience-based decision-making?

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: The generative model is POMDP-based, and the agent's inference of hidden states from observations is central to both decision-making schemes
  - Quick check question: How does the agent infer hidden states from observations in a POMDP, and why is this inference necessary for decision-making?

- **Concept: Entropy as Uncertainty Measure**
  - Why needed here: Entropy quantifies the confidence of action distributions, which drives the bias update mechanism in the hybrid model
  - Quick check question: Why is Shannon entropy a suitable measure for comparing the reliability of two different decision strategies?

## Architecture Onboarding

- **Component map:** Generative Model (POMDP) -> Perception Module -> DPEFE Module & CL Module -> Bias Module -> Decision Module -> Action Execution
- **Critical path:** 1. Perception → 2. Compute DPEFE and CL distributions → 3. Calculate entropies → 4. Update β → 5. Combine strategies → 6. Execute action
- **Design tradeoffs:** High planning depth (T) improves DPEFE accuracy but increases computational cost and may slow adaptation; Fast Γ decay speeds up exploitation but risks overfitting to early experiences; Low α in β update yields stable bias but may slow adaptation to environmental changes
- **Failure signatures:** Oscillation in β → α too high or entropy estimates unstable; Slow learning after mutation → Γ decay rate too aggressive or β stuck favoring old strategy; High variance in episode length → one strategy dominates unpredictably, indicating poor entropy calibration
- **First 3 experiments:** 1. Run DPEFE and CL separately in a static grid world; compare episode length convergence; 2. Test hybrid model in a mutating grid; monitor β and Γ evolution across episodes; 3. Vary planning depth (T) and Γ decay rate; measure performance trade-offs in complex environments

## Open Questions the Paper Calls Out

### Open Question 1
How do biological agents balance predictive planning and experiential learning under varying resource constraints? The paper introduces a mixed model that balances planning and experience-based learning, but does not explore how biological systems achieve this balance in practice or what neural mechanisms underlie this capability.

### Open Question 2
What are the optimal conditions for switching between planning-based and experience-based decision-making strategies? The paper shows that different decision-making schemes perform better in different environments, suggesting that environmental demands should influence strategy selection, but does not provide a framework for determining when to switch.

### Open Question 3
How does the risk parameter (Γt) in the counterfactual learning method relate to neurobiological measures of uncertainty or stress? The authors show that the risk parameter increases when the environment mutates and correlates with performance changes, but do not connect this to biological measures.

## Limitations
- The entropy-based bias mechanism assumes reliable entropy estimates, but no robustness analysis is provided for cases where either DPEFE or CL distributions are degenerate
- The risk parameter Γ decay rate is not explicitly tied to environmental complexity, potentially limiting adaptability in rapidly changing environments
- The hybrid model's computational overhead is not compared against state-of-the-art reinforcement learning methods

## Confidence

- **High**: The hybrid model architecture and basic decision-making framework are well-specified
- **Medium**: The entropy-based bias update mechanism works as described in controlled environments
- **Low**: The model's generalization to continuous or high-dimensional state spaces is unverified

## Next Checks
1. Test the hybrid model with varying α values in the bias update equation to identify optimal learning rates for different environmental complexities
2. Implement a robustness test where one strategy's distribution is intentionally made degenerate to verify the hybrid model's stability
3. Compare the model's performance and computational efficiency against standard Q-learning and policy gradient methods in identical grid-world environments