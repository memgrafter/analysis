---
ver: rpa2
title: Grounding Spatial Relations in Text-Only Language Models
arxiv_id: '2403.13666'
source_url: https://arxiv.org/abs/2403.13666
tags:
- spatial
- relations
- object
- sstd
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that text-only Language Models (LM) can learn
  to ground spatial relations like "left of" or "below" if they are provided with
  explicit location information of objects and they are properly trained to leverage
  those locations. We perform experiments on a verbalized version of the Visual Spatial
  Reasoning (VSR) dataset, where images are coupled with textual statements which
  contain real or fake spatial relations between two objects of the image.
---

# Grounding Spatial Relations in Text-Only Language Models

## Quick Facts
- arXiv ID: 2403.13666
- Source URL: https://arxiv.org/abs/2403.13666
- Authors: Gorka Azkune; Ander Salaberria; Eneko Agirre
- Reference count: 14
- Key outcome: Text-only LMs with spatial location tokens outperform Vision-and-Language Models on spatial reasoning, setting new state-of-the-art on VSR.

## Executive Summary
This paper demonstrates that text-only language models can effectively ground spatial relations like "left of" or "below" when provided with explicit location information of objects. The authors propose using location tokens to represent object positions in textual form, derived from normalized bounding box coordinates mapped to a discrete grid. By pretraining on a large synthetic dataset of spatial relations, the text-only models achieve state-of-the-art performance on the Visual Spatial Reasoning (VSR) dataset, surpassing multimodal vision-and-language models. The approach shows that spatial grounding can be achieved without explicit visual input, challenging the assumption that multimodal models are necessary for spatial reasoning tasks.

## Method Summary
The authors verbalize images using an object detector (VinVL) to extract object labels and bounding boxes. Bounding box coordinates are normalized and mapped to discrete grid cells, which are represented as textual tokens prepended to object labels. This creates a spatial signature for each object. The model is pretrained on a synthetic dataset (SSTD) generated using heuristic rules encoding spatial relations (horizontal/vertical alignment, diagonal alignment, perpendicular arrangements) between object pairs. The text-only LMs (BERT and T5) are then fine-tuned on the VSR dataset, where they must classify whether a spatial relation description matches the image content.

## Key Results
- Text-only LMs with location tokens achieve state-of-the-art accuracy on VSR, outperforming multimodal VLMs.
- Pretraining on the synthetic SSTD dataset significantly improves performance when using location tokens, but not without them.
- The models can generalize beyond the spatial relations seen in synthetic training to some extent, learning useful information beyond the heuristic rules.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Location tokens allow language models to ground spatial relations by encoding explicit positional information.
- Mechanism: Bounding box coordinates are normalized and mapped to discrete grid cells, which are represented as textual tokens prepended to object labels. This creates a spatial signature that the language model can interpret alongside the spatial relation description.
- Core assumption: The model can learn to interpret the positional tokens and relate them to the semantic meaning of spatial relations through fine-tuning.
- Evidence anchors:
  - [abstract] "we propose to use location tokens to represent the positions and spatial extent of objects in a scene"
  - [section 4.1] "we propose to use textual tokens in a novel way to represent real-world scenes and leverage pretrained LMs"
  - [corpus] Weak evidence: only one related paper explicitly discusses spatial tokens in grounding.
- Break condition: If the grid resolution is too coarse or too fine, the tokens may lose precision or become noisy, breaking the spatial grounding.

### Mechanism 2
- Claim: Synthetic spatial training dataset improves the model's ability to generalize spatial relations beyond what is seen in the test set.
- Mechanism: A large, automatically generated dataset provides diverse examples of spatial relations encoded via bounding boxes and heuristic rules, allowing the model to learn robust patterns.
- Core assumption: The heuristic rules capture the essential spatial reasoning needed to generalize to unseen relations.
- Evidence anchors:
  - [abstract] "pretraining the LM over a synthetic dataset automatically derived by us improves results significantly when using location tokens"
  - [section 5.1] "we do not observe any improvement when using locations, but pretraining the LM over a synthetic dataset automatically derived by us improves results significantly"
  - [corpus] Weak evidence: only one related paper discusses synthetic data for spatial reasoning.
- Break condition: If the heuristic rules are too simplistic or miss edge cases, the model may overfit to the synthetic patterns and fail on real data.

### Mechanism 3
- Claim: Text-only models with spatial grounding can outperform multimodal vision-language models on spatial reasoning tasks.
- Mechanism: The text-only model leverages explicit spatial tokens and large-scale synthetic training to capture spatial relationships more effectively than multimodal models that rely on implicit visual features.
- Core assumption: The spatial tokens provide richer, more explicit spatial information than the implicit representations learned by vision-language models.
- Evidence anchors:
  - [abstract] "our text-only LMs outperforming Vision-and-Language Models and setting the new state-of-the-art for the VSR dataset"
  - [section 5.2] "All our spatially trained LMs surpass that accuracy significantly"
  - [corpus] Weak evidence: no direct comparison with VLMs in the corpus.
- Break condition: If the spatial tokens fail to capture depth or occlusion cues, the model may underperform on relations requiring 3D understanding.

## Foundational Learning

- Concept: Bounding box representation and normalization
  - Why needed here: The method relies on converting bounding boxes to normalized coordinates and then to discrete grid cells for token generation.
  - Quick check question: Given a bounding box with coordinates (x0, y0, x1, y1) and an image size (W, H), how do you normalize the coordinates to the [0,1] range?

- Concept: Grid-based spatial discretization
  - Why needed here: The method divides the image into a regular grid and maps bounding box coordinates to discrete grid cell indices for token generation.
  - Quick check question: If the grid size is 4x4 and the normalized bounding box coordinates are (0.2, 0.3, 0.7, 0.8), what are the corresponding grid cell indices?

- Concept: Synthetic data generation with heuristic rules
  - Why needed here: The method generates a large synthetic dataset using simple rules to create diverse examples of spatial relations for training.
  - Quick check question: Given two bounding boxes, how would you determine if one object is "left of" the other using their coordinates?

## Architecture Onboarding

- Component map:
  - Object detector (VinVL) -> Bounding boxes and object labels
  - Grid discretization module -> Normalized coordinates -> Discrete grid cell indices
  - Token generator -> Grid cell indices + object labels -> Location tokens
  - Language model (BERT/T5) -> Location tokens + spatial relation caption -> Classification head
  - Synthetic data generator -> Images + heuristic rules -> (question, description, answer) triples

- Critical path:
  - Input image -> Object detection -> Bounding box extraction -> Coordinate normalization -> Grid discretization -> Location token generation -> Concatenation with caption -> LM input -> Classification

- Design tradeoffs:
  - Grid size: Larger grids provide finer spatial resolution but increase token vocabulary size and computational cost.
  - Synthetic data volume: More synthetic data improves generalization but may introduce noise if heuristic rules are imperfect.
  - Object detector choice: Higher accuracy detectors improve grounding quality but may be computationally expensive.

- Failure signatures:
  - Low accuracy on spatial relations requiring depth or occlusion understanding
  - Overfitting to synthetic patterns, leading to poor performance on real data
  - Inconsistent predictions when object labels or bounding boxes are noisy

- First 3 experiments:
  1. Train BERT-base with and without location tokens on the VSR dataset; compare accuracy to establish the benefit of location tokens.
  2. Train BERT-base with location tokens on SSTD; evaluate on VSR to measure the impact of synthetic spatial training.
  3. Compare text-only LMs with location tokens to multimodal VLMs on VSR to validate the effectiveness of the spatial grounding approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do location tokens perform compared to other spatial encoding methods (e.g., coordinate embeddings, grid-based representations) for grounding spatial relations in text-only LMs?
- Basis in paper: [explicit] The paper introduces location tokens as a way to represent object positions and extents in textual form for spatial grounding.
- Why unresolved: The paper only compares location tokens to not using any spatial information, not to other encoding methods.
- What evidence would resolve it: Experiments comparing the performance of text-only LMs using location tokens, coordinate embeddings, and grid-based representations on a spatial reasoning task like VSR.

### Open Question 2
- Question: To what extent can text-only LMs generalize spatial reasoning beyond the specific relations and rules encoded in the synthetic training dataset?
- Basis in paper: [explicit] The paper observes that the LMs trained on the synthetic dataset can generalize to some extent to spatial relations not seen in the training data, particularly those requiring depth information.
- Why unresolved: The paper does not quantify the generalization capabilities or identify the specific factors that enable generalization.
- What evidence would resolve it: Systematic experiments varying the coverage and complexity of spatial relations in the synthetic dataset and measuring the resulting generalization performance on a held-out test set.

### Open Question 3
- Question: How does the performance of text-only LMs on spatial reasoning tasks scale with model size, and is there a point of diminishing returns?
- Basis in paper: [explicit] The paper observes diminishing returns in performance improvements as the size of the text-only LMs increases.
- Why unresolved: The paper does not explore the scaling behavior in detail or identify the factors that limit the benefits of scaling.
- What evidence would resolve it: Experiments training and evaluating text-only LMs of varying sizes (e.g., BERT-base, BERT-large, T5-base, T5-large, T5-3B) on a spatial reasoning task like VSR, and analyzing the relationship between model size and performance.

## Limitations

- Small VSR dataset (12,077 examples) makes it difficult to draw definitive conclusions about the effectiveness of location tokens in isolation.
- The model's reliance on bounding boxes limits its ability to handle transparent objects, shadows, or other spatial phenomena that don't have clear rectangular boundaries.
- The heuristic rules used to generate the synthetic dataset may not capture the full complexity of real-world spatial relationships, potentially leading to overfitting.

## Confidence

**High Confidence Claims:**
- Location tokens can be generated from bounding box coordinates using grid discretization
- Pretraining on synthetic spatial data improves performance on spatial reasoning tasks
- The proposed text-only approach achieves state-of-the-art results on VSR

**Medium Confidence Claims:**
- Location tokens provide richer spatial information than implicit visual features in VLMs
- The synthetic dataset effectively captures essential spatial reasoning patterns
- The model can generalize beyond the spatial relations seen in synthetic training

**Low Confidence Claims:**
- Text-only models with spatial grounding fundamentally outperform multimodal approaches
- The learned spatial representations are meaningful rather than pattern-memorized
- The approach scales to more complex spatial reasoning tasks beyond VSR

## Next Checks

1. **Ablation Study on Dataset Size**: Conduct experiments training the model on progressively larger subsets of VSR (100, 500, 1000, 2000 examples) to determine the minimum dataset size required for location tokens to show consistent benefits, controlling for synthetic pretraining effects.

2. **Synthetic Data Quality Analysis**: Create a controlled test set of spatial relations that are intentionally absent from SSTD's heuristic rules (e.g., "partly inside," "surrounding," "intersecting") and evaluate whether the model can reason about these relations without explicit synthetic examples, measuring generalization beyond learned patterns.

3. **VLM Comparison on Controlled Tasks**: Design a minimal benchmark with simple geometric shapes and precise spatial relations where both the text-only approach and a VLM can be evaluated without object detection errors. Compare performance on relations requiring depth perception, occlusion handling, and non-rectangular arrangements to identify specific failure modes.