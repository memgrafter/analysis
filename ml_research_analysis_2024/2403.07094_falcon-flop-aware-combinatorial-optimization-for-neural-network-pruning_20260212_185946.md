---
ver: rpa2
title: 'FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning'
arxiv_id: '2403.07094'
source_url: https://arxiv.org/abs/2403.07094
tags:
- pruning
- flop
- falcon
- network
- flops
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FALCON, a novel combinatorial optimization
  framework for neural network pruning that jointly considers model accuracy, FLOPs,
  and sparsity constraints. The key idea is formulating pruning as an integer linear
  program (ILP) that optimizes both FLOPs and non-zero weight (NNZ) budgets.
---

# FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning

## Quick Facts
- arXiv ID: 2403.07094
- Source URL: https://arxiv.org/abs/2403.07094
- Reference count: 40
- Key result: Achieves 48% accuracy improvement over state-of-the-art for ResNet50 with 20% FLOPs retained

## Executive Summary
This paper introduces FALCON, a novel combinatorial optimization framework for neural network pruning that jointly considers model accuracy, FLOPs, and sparsity constraints. The key innovation is formulating pruning as an integer linear program (ILP) that optimizes both FLOPs and non-zero weight (NNZ) budgets. FALCON uses a discrete first-order method that iteratively solves the ILP, leveraging the low-rank structure of the approximated Hessian for efficiency. Experiments show FALCON achieves superior accuracy compared to existing methods under fixed FLOP budgets and in gradual pruning settings with retraining.

## Method Summary
FALCON formulates network pruning as an integer linear program that directly optimizes both FLOPs and NNZ constraints while preserving model accuracy. The method uses a discrete first-order approach that iteratively solves the ILP, leveraging the low-rank structure of the approximated Hessian for scalability. A multi-stage FALCON++ procedure progressively refines the quadratic model by tightening constraints, improving accuracy under aggressive compression. The framework is validated on ResNet20, MobileNetV1, and ResNet50 across CIFAR-10 and ImageNet datasets.

## Key Results
- Achieves 48% accuracy improvement over state-of-the-art for ResNet50 with 20% FLOPs retained
- Demonstrates superior performance in gradual pruning settings with retraining
- Shows effective balance between NNZ and FLOP constraints across different network architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The ILP formulation directly enforces both FLOPs and NNZ constraints, leading to more uniform sparsity across network layers.
- **Mechanism:** By modeling the pruning decision as binary variables with separate linear constraints for FLOPs and NNZ, the solver is forced to distribute sparsity more evenly, avoiding layer collapse.
- **Core assumption:** Parameters within the same layer have equal FLOPs contribution.
- **Evidence anchors:**
  - [abstract]: "Our approach achieves an optimal balance between NNZ and FLOP—proxies for inference time and memory usage"
  - [section]: "This balanced sparsity distribution enhances accuracy, as demonstrated by Kusupati et al. (2020); Singh and Alistarh (2020)"
- **Break condition:** If FLOPs per parameter vary within a layer (e.g., custom convolutions), the equal-FLOPs assumption breaks and the formulation may no longer enforce balanced sparsity.

### Mechanism 2
- **Claim:** The low-rank Hessian approximation enables scalable optimization for millions of parameters without dense matrix storage.
- **Mechanism:** The empirical Fisher Hessian has rank n (samples), allowing matrix-vector products via the n×p matrix X instead of the full p×p Hessian.
- **Core assumption:** n << p in practice (e.g., hundreds to thousands of samples vs millions of parameters).
- **Evidence anchors:**
  - [abstract]: "Using problem structure (e.g., the low-rank structure of approx. Hessian), we can address instances with millions of parameters"
  - [section]: "matrix H has a rank of n. In practice, p could reach millions while n remains below thousands"
- **Break condition:** If n becomes comparable to p (e.g., full-batch training), the low-rank advantage disappears and memory savings vanish.

### Mechanism 3
- **Claim:** The multi-stage FALCON++ procedure refines the quadratic model progressively, improving accuracy under aggressive compression.
- **Mechanism:** Gradually tightening NNZ and FLOPs budgets while re-optimizing the local quadratic model prevents degradation from poor local approximations.
- **Core assumption:** The local quadratic model remains a reasonable approximation of the loss when changes between stages are small.
- **Evidence anchors:**
  - [abstract]: "we propose a multi-stage process called FALCON++ that operates on a better approximation of the loss"
  - [section]: "This cautious approach helps maintain the validity of the local quadratic approximation, ensuring that each step is based on accurate information"
- **Break condition:** If the loss surface changes dramatically between stages (e.g., due to catastrophic forgetting), the quadratic model may no longer be valid.

## Foundational Learning

- **Concept: Integer Linear Programming (ILP)**
  - Why needed here: ILP allows exact formulation of discrete pruning decisions under hard resource constraints.
  - Quick check question: If we replace the ILP with a continuous relaxation only, what constraint might be violated in the final solution?

- **Concept: Low-rank matrix factorization**
  - Why needed here: Enables efficient Hessian-vector products without storing dense p×p matrices.
  - Quick check question: What is the storage complexity of the Hessian approximation using the low-rank form vs dense form?

- **Concept: Local quadratic approximation of loss**
  - Why needed here: Provides a tractable surrogate objective for pruning while preserving model accuracy.
  - Quick check question: Why is a ridge regularization term added to the quadratic approximation during pruning?

## Architecture Onboarding

- **Component map:** Pre-trained weights -> Gradient/Hessian estimation -> ILP projection -> DFO update -> Active set refinement
- **Critical path:**
  1. Estimate gradient and Hessian from n samples
  2. Build ILP with FLOPs and NNZ constraints
  3. Solve ILP approximately via LP relaxation + rounding
  4. Apply DFO update with active set restriction
  5. Refine support via backsolve if needed
- **Design tradeoffs:**
  - Memory vs accuracy: Block-wise Hessian approximation reduces memory but may hurt pruning quality
  - Speed vs precision: Solving ILP to higher accuracy improves results but increases runtime
  - Single-stage vs multi-stage: Simpler but may degrade under aggressive compression
- **Failure signatures:**
  - Memory overflow: n or p too large for available RAM during Hessian estimation
  - Slow convergence: Active set too large, reducing DFO update efficiency
  - Accuracy collapse: FLOPs budget too tight relative to NNZ budget, forcing poor sparsity distribution
- **First 3 experiments:**
  1. Run FALCON on a small ResNet variant (e.g., ResNet18) with moderate FLOPs reduction (50%) and verify accuracy retention
  2. Compare single-stage vs multi-stage FALCON on the same task to observe accuracy differences
  3. Profile memory usage during Hessian estimation to confirm low-rank advantage holds for the target network size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed FALCON framework perform when applied to different types of neural network architectures, such as recurrent neural networks (RNNs) or transformers?
- Basis in paper: [inferred] The paper primarily focuses on convolutional neural networks (CNNs) like ResNet and MobileNet. There is no explicit mention of applying FALCON to other architectures.
- Why unresolved: The paper does not provide any experimental results or analysis on the performance of FALCON with different types of neural network architectures.
- What evidence would resolve it: Conducting experiments to evaluate FALCON's performance on various neural network architectures, including RNNs and transformers, and comparing the results to other pruning methods.

### Open Question 2
- Question: What is the impact of using different gradient and Hessian approximation methods on the performance of FALCON?
- Basis in paper: [explicit] The paper mentions that FALCON uses stochastic gradient and empirical Fisher information matrix for gradient and Hessian approximations, respectively. However, it does not explore the impact of using different approximation methods.
- Why unresolved: The paper does not provide any analysis or comparison of FALCON's performance using different gradient and Hessian approximation methods.
- What evidence would resolve it: Conducting experiments to compare FALCON's performance using various gradient and Hessian approximation methods and analyzing the impact on pruning results.

### Open Question 3
- Question: How does the proposed multi-stage procedure (FALCON++) affect the convergence and accuracy of the pruned models compared to the single-stage approach (FALCON)?
- Basis in paper: [explicit] The paper introduces the multi-stage procedure FALCON++ and mentions that it iteratively refines the local quadratic models and solves them with gradually decreasing NNZ and FLOP budgets. However, it does not provide a detailed analysis of the impact of the multi-stage procedure on convergence and accuracy.
- Why unresolved: The paper does not provide a comprehensive comparison between the single-stage and multi-stage approaches in terms of convergence speed and final accuracy of the pruned models.
- What evidence would resolve it: Conducting experiments to compare the convergence speed and final accuracy of models pruned using FALCON and FALCON++, and analyzing the trade-offs between the two approaches.

## Limitations
- Scalability constraints: ILP formulation becomes computationally prohibitive for very large networks
- Dataset dependence: Effectiveness may vary significantly across different domains beyond vision tasks
- Solver dependency: Success heavily depends on ILP solver quality and configuration

## Confidence
- **High confidence:** The theoretical framework connecting ILP to combinatorial optimization is sound
- **Medium confidence:** Experimental results showing 48% accuracy improvement are compelling but based on specific baselines
- **Low confidence:** Claims about FALCON++'s superiority in gradual pruning lack ablation studies isolating component contributions

## Next Checks
1. Systematically vary ILP solver parameters (time limits, gap tolerances, solver choice) and measure impact on final accuracy and sparsity distribution across different network architectures
2. Apply FALCON to non-vision domains (language models, recommendation systems, or graph neural networks) with varying parameter importance distributions
3. Compare FALCON against variants using only FLOPs constraints, only NNZ constraints, or different Hessian approximation strategies to isolate design choice contributions