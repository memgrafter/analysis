---
ver: rpa2
title: 'Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP:
  Application, Robustness, and Self-Awareness'
arxiv_id: '2405.08151'
source_url: https://arxiv.org/abs/2405.08151
tags:
- corpus
- negative
- performance
- rals
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates retrieval-augmented large language models
  (RALs) in biomedical NLP, focusing on their application, robustness, and self-awareness.
  The study benchmarks RALs across five biomedical tasks using 11 datasets, introducing
  four testbeds: unlabeled robustness, counterfactual robustness, diverse robustness,
  and negative awareness.'
---

# Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness

## Quick Facts
- **arXiv ID:** 2405.08151
- **Source URL:** https://arxiv.org/abs/2405.08151
- **Reference count:** 0
- **One-line primary result:** RALs outperform standard LLMs on most biomedical tasks but struggle with robustness and self-awareness, particularly under counterfactual and diverse scenarios.

## Executive Summary
This paper evaluates retrieval-augmented large language models (RALs) in biomedical NLP, focusing on their application, robustness, and self-awareness. The study benchmarks RALs across five biomedical tasks using 11 datasets, introducing four testbeds: unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness. Results show that while RALs generally outperform standard LLMs, they still struggle with robustness and self-awareness, particularly under counterfactual and diverse scenarios. A Detect-and-Correct strategy and contrastive learning approach are proposed to improve RAL performance, showing significant gains in unlabeled and counterfactual robustness. The findings highlight the need for continued refinement to ensure reliability in high-stakes biomedical applications.

## Method Summary
The study evaluates RALs using 11 biomedical datasets across five tasks, employing three retrievers (BM25, Contriever, MedCPT) and five LLMs (LLaMA2-13B, MedLLaMA-13B, LLaMA3-8B, Phi4 14B, Qwen2.5 32B). The RAL pipeline retrieves relevant knowledge from a corpus, combines it with input sentences, and feeds the combined context to an LLM for output generation. Four testbeds modify corpus conditions to evaluate specific abilities: unlabeled robustness (removing labels), counterfactual robustness (injecting incorrect labels), diverse robustness (mixing task corpora), and negative awareness (all counterfactual). The proposed Detect-and-Correct method uses GPT-4 to revise labels, while contrastive learning improves negative awareness. Performance is measured using Micro F1, Recall, Precision, Macro-F1, and Negative Awareness Rate.

## Key Results
- RALs generally outperform standard LLMs on most biomedical tasks, but performance degrades significantly under counterfactual and diverse robustness scenarios.
- Unlabeled corpora lead to lower RAL performance compared to labeled corpora, though exceptions exist on Chemprot and Hetionet where unlabeled retrieval improved results.
- The Detect-and-Correct strategy and contrastive learning approach significantly improve performance in unlabeled and counterfactual robustness scenarios.
- RALs struggle with negative awareness, with true negative awareness rates of zero on PharmKG and BioNLI datasets.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RALs can outperform standard LLMs in biomedical NLP tasks by retrieving relevant knowledge from external databases, thereby reducing hallucination and improving adaptation to new knowledge.
- **Mechanism:** The RAL framework retrieves pertinent information from an established database, which is then fed into the LLM along with the input sentence to assist in generating the expected output. This process allows the model to access external knowledge that may not be present in its pre-training data.
- **Core assumption:** The retrieved knowledge is relevant and accurate enough to improve the model's output without introducing significant noise or errors.
- **Evidence anchors:**
  - [abstract]: "To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database."
  - [section]: "The most common method is to use the designed retriever to retrieve the knowledge that is relevant to the input sentence, subsequently, the retrieved knowledge, along with the input sentence, is fed into the LLM to assist in generating the expected output."
  - [corpus]: Weak evidence - The paper does not provide direct experimental data comparing retrieval-augmented outputs to standard LLM outputs without retrieval.

### Mechanism 2
- **Claim:** RALs demonstrate improved performance in label-intensive tasks like triple extraction and classification when using labeled corpora compared to unlabeled corpora.
- **Mechanism:** The labeled corpus provides specific examples with correct answers that guide the model's learning process. When the RAL retrieves these labeled examples, it can better understand the task requirements and generate more accurate outputs.
- **Core assumption:** Labeled data contains more task-specific information than unlabeled data, making it more valuable for model training and inference.
- **Evidence anchors:**
  - [abstract]: "RALs generally outperform standard LLMs on most biomedical tasks, but still struggle with robustness and self-awareness, particularly under counterfactual and diverse scenarios."
  - [section]: "RAL utilizing the unlabeled corpus exhibits lower performance compared to RAL utilizing the labeled corpus. RALs have demonstrated a strong dependence on the labeled corpus, especially on the label-intensive tasks."
  - [corpus]: Direct evidence from Table 2 showing performance differences between labeled and unlabeled corpora across multiple datasets.

### Mechanism 3
- **Claim:** Counterfactual robustness is challenging for RALs, with high counterfactual rates (80%) significantly reducing performance, but some datasets show resilience where counterfactual information can still improve performance.
- **Mechanism:** The RAL system attempts to retrieve relevant information even from counterfactual corpora, and in some cases, the structure and format of these examples provide useful templates for generation despite incorrect labels.
- **Core assumption:** The model can extract useful structural information from counterfactual examples even when the labels are incorrect.
- **Evidence anchors:**
  - [abstract]: "RALs struggle with robustness and self-awareness, particularly under counterfactual and diverse scenarios."
  - [section]: "Counterfactual corpus poses a challenge for RALs. On ADE, counterfactual instances significantly influence the model performance."
  - [corpus]: Direct evidence from Table 2 showing performance degradation with increasing counterfactual rates, but also some instances where performance improves.

## Foundational Learning

- **Concept:** Vector similarity and embedding space
  - **Why needed here:** RAL systems rely on retrievers that use vector representations to measure similarity between input sentences and corpus instances. Understanding how embeddings capture semantic meaning is crucial for grasping how retrievers select relevant information.
  - **Quick check question:** If two sentences have similar embeddings in the retrieval space, does this guarantee they are semantically equivalent?

- **Concept:** Contrastive learning and loss functions
  - **Why needed here:** The paper mentions using contrastive learning approaches to improve negative awareness. Understanding how triplet loss works and how it distinguishes between positive and negative instances is essential for implementing these methods.
  - **Quick check question:** In triplet loss, what happens to the distance between positive and negative instances as training progresses?

- **Concept:** Zero-shot and few-shot learning
  - **Why needed here:** RAL systems often operate in low-resource settings where labeled data is scarce. Understanding how models can perform tasks with minimal supervision through in-context learning is crucial for implementing the Detect-and-Correct methods described in the paper.
  - **Quick check question:** How does in-context learning differ from traditional fine-tuning in terms of parameter updates and data requirements?

## Architecture Onboarding

- **Component map:** Input sentence -> Retriever (BM25/Contriever/MedCPT) -> Corpus database -> LLM (LLaMA2-13B/MedLLaMA-13B/LLaMA3-8B/Phi4 14B/Qwen2.5 32B) -> Output generator -> Evaluation metrics

- **Critical path:**
  1. Receive input sentence
  2. Retriever computes vector representation of input
  3. Retriever computes vector representations of corpus instances
  4. Similarity scores calculated between input and corpus instances
  5. Top-k relevant instances retrieved
  6. Retrieved instances combined with input sentence
  7. LLM generates output based on combined context
  8. Output evaluated against ground truth

- **Design tradeoffs:**
  - Dense vs. sparse retrieval: Dense retrievers (Contriever, MedCPT) capture semantic meaning but are computationally expensive; sparse retrievers (BM25) are fast but rely on lexical overlap
  - Corpus size vs. relevance: Larger corpora increase coverage but may introduce noise; smaller corpora are more focused but may miss relevant information
  - Retrieval frequency: Frequent retrieval may improve accuracy but increases computational cost and latency

- **Failure signatures:**
  - Retriever returns irrelevant instances: Check embedding quality and similarity thresholds
  - LLM generates incorrect outputs despite relevant retrievals: Check prompt engineering and context window management
  - Performance degrades with unlabeled corpora: Verify that unlabeled data contains sufficient information for the task
  - Counterfactual instances mislead the model: Implement uncertainty estimation or confidence scoring

- **First 3 experiments:**
  1. Implement a simple BM25 retriever and test retrieval accuracy on a small biomedical corpus
  2. Create a controlled testbed with labeled and unlabeled versions of the same corpus to measure performance differences
  3. Implement the Detect-and-Correct method using GPT-4 for label correction and evaluate its impact on counterfactual robustness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do RALs perform in biomedical tasks when retrieving from unlabeled corpora, and what factors determine whether unlabeled retrieval can be as effective as labeled retrieval?
- **Basis in paper:** [explicit] The paper explicitly states that "RAL utilizing the unlabeled corpus exhibits lower performance compared to RAL utilizing the labeled corpus" but also notes exceptions on Chemprot and Hetionet where unlabeled retrieval improved performance.
- **Why unresolved:** The paper observes this phenomenon but doesn't explain the underlying mechanisms that make unlabeled retrieval effective in some cases but not others, or identify the characteristics of datasets/tasks where this approach works.
- **What evidence would resolve it:** Comparative analysis across more biomedical datasets showing which characteristics (task type, dataset size, knowledge domain) predict unlabeled retrieval success, plus experiments isolating whether this is due to model pretraining, corpus similarity, or task-specific knowledge patterns.

### Open Question 2
- **Question:** What is the relationship between counterfactual rate and model performance in RALs, and why does higher counterfactual rate sometimes improve performance rather than degrade it?
- **Basis in paper:** [explicit] The paper states "Counterfactual rates and model performance are not inversely proportional" and provides the specific example that "when the counterfactual rate is higher, the model performance also improves" on SemClass.
- **Why unresolved:** The paper observes this counterintuitive finding but doesn't explain the mechanisms by which misleading information could benefit model performance, or under what conditions this occurs.
- **What evidence would resolve it:** Systematic experiments varying counterfactual rates across multiple datasets, analysis of model attention patterns on counterfactual vs. factual examples, and investigation into whether this reflects template learning or other compensatory mechanisms.

### Open Question 3
- **Question:** What specific limitations prevent RALs from achieving negative awareness, and what architectural or training modifications could improve their ability to distinguish helpful from harmful retrieved information?
- **Basis in paper:** [explicit] The paper states that "RAL poses a challenge to the Negative Awareness" and reports that "true negative awareness rate on PharmKG and BioNLI was zero," with overall poor performance in detecting negative examples.
- **Why unresolved:** While the paper identifies the problem, it doesn't provide detailed analysis of why RALs fail at this task or what specific model components or training strategies could address this limitation.
- **What evidence would resolve it:** Ablation studies testing different retrieval architectures, attention mechanisms, or training objectives focused on negative example detection, plus analysis of where and why RALs misclassify negative examples.

## Limitations
- The study relies on pre-existing datasets and does not evaluate RAL performance in real-world deployment scenarios where retrieval latency and computational costs would matter.
- Performance gains from retrieval augmentation are primarily demonstrated through relative comparisons rather than absolute superiority over standard LLMs, making practical significance uncertain.
- The proposed Detect-and-Correct and contrastive learning methods lack detailed implementation specifications that would enable direct replication.

## Confidence
- **High confidence:** The finding that RALs outperform standard LLMs on most biomedical tasks when using labeled corpora
- **Medium confidence:** The observation that RALs struggle with counterfactual and diverse robustness scenarios
- **Low confidence:** The specific performance improvements claimed by the proposed Detect-and-Correct and contrastive learning methods due to limited methodological detail

## Next Checks
1. **Replication of core findings:** Recreate the RAL pipeline using BM25 retriever with one biomedical dataset (e.g., ChemProt) to verify the performance gap between labeled and unlabeled corpora
2. **Counterfactual robustness testing:** Implement a controlled testbed with 80% counterfactual instances to measure the exact performance degradation reported in the paper
3. **Ablation study of proposed methods:** Implement the Detect-and-Correct method with GPT-4 label revision on a single task to quantify the improvement in negative awareness rates