---
ver: rpa2
title: An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic
  Theses and Dissertations
arxiv_id: '2403.16424'
source_url: https://arxiv.org/abs/2403.16424
tags:
- subject
- lcsh
- chatgpt
- library
- headings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored using ChatGPT for generating Library of Congress
  Subject Headings (LCSH) for electronic theses and dissertations (ETDs). The authors
  designed prompts incorporating ETD titles and abstracts, plus examples, to generate
  MARC 21 formatted LCSH.
---

# An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations

## Quick Facts
- arXiv ID: 2403.16424
- Source URL: https://arxiv.org/abs/2403.16424
- Authors: Eric H. C. Chow; TJ Kao; Xiaoli Li
- Reference count: 26
- Key outcome: ChatGPT generated mostly correct MARC field coding (90% accuracy) and some valid LCSH terms for ETDs, but struggled with complex subdivision strings, achieving only 23% fully valid LCSH assignments

## Executive Summary
This study explored using ChatGPT to generate Library of Congress Subject Headings (LCSH) for electronic theses and dissertations (ETDs). The authors designed prompts incorporating ETD titles and abstracts, plus examples, to generate MARC 21 formatted LCSH. ChatGPT produced mostly correct MARC field coding (90% accuracy) and some valid LCSH terms, but struggled with complex subject strings involving subdivisions. Only 23% of LCSH assignments were fully valid, and about half adequately reflected content specificity and exhaustivity. The findings suggest LLMs like ChatGPT could support catalogers by generating initial LCSH suggestions, saving time and effort, while human oversight remains essential for ensuring accuracy and compliance with LCSH rules.

## Method Summary
The study used 30 ETD MARC records with titles and abstracts from WorldCat and UC Davis Library catalog. ChatGPT-3.5 via Azure OpenAI API with temperature=0 processed these records through Python scripts using prompts with context, instruction, output indicator, and three examples. A professional cataloger evaluated results for MARC coding correctness and LCSH validity against Library of Congress standards.

## Key Results
- ChatGPT produced mostly correct MARC field coding (90% accuracy) for LCSH assignments
- Only 23% of generated LCSH terms were fully valid and compliant with LCSH vocabularies
- Approximately 53% of LCSH assignments adequately reflected content specificity and exhaustivity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT can generate mostly correct MARC field coding for LCSH assignments in ETD records.
- Mechanism: ChatGPT's internal training corpus includes MARC records and LCSH vocabularies from public domain sources like the Library of Congress website and library/publisher MARC datasets. This allows it to produce syntactically correct MARC field structures (650, 600, 610, 630, 651) when prompted appropriately.
- Core assumption: The model has been trained on enough MARC records and LCSH data to reproduce correct field coding without explicit programming.
- Evidence anchors:
  - [section]: "ChatGPT’s outputs of MARC coding were mostly correct (90%), with only three records containing incorrect codes."
  - [section]: "Our results provide strong evidence that ChatGPT has been trained on LCSH vocabularies scraped from the Library of Congress website and MARC records from websites of libraries and publishers around the world."
  - [corpus]: Weak evidence - no direct corpus neighbor on MARC encoding accuracy, but general LLM capability studies support this inference.
- Break condition: If ChatGPT encounters unfamiliar LCSH structures or MARC conventions not present in its training corpus, accuracy drops below acceptable thresholds.

### Mechanism 2
- Claim: ChatGPT can generate some valid LCSH terms but struggles with complex subdivision strings and LCSH rule compliance.
- Mechanism: While ChatGPT can access basic LCSH terms from its training data, the intricate rules for constructing subdivision strings (topical, form, chronological, geographic) require specialized knowledge that goes beyond pattern matching from examples. The model often produces terms that are close but not compliant with LCSH standards.
- Core assumption: The LLM's training data includes LCSH terms but not comprehensive documentation of subdivision rules and their proper application sequences.
- Evidence anchors:
  - [section]: "ChatGPT produced valid LCSH for seven ETD records, or 23.3%, that completely conforms to the LCSH controlled vocabularies; in the remaining 23 ETD records, the assigned LCSH had varying degrees of validity."
  - [section]: "The model still falters in terms of specificity and exhaustivity, particularly with LCSH strings that involve subdivisions, thus failing to produce accurate results about half of the time."
  - [corpus]: Weak evidence - no direct corpus neighbor on LCSH subdivision complexity, but Zhang et al. (2023) study mentioned similar LLM limitations with discipline-specific classification rules.
- Break condition: When LCSH assignments require deep understanding of hierarchical relationships, cross-references, and subdivision application rules not captured in training examples.

### Mechanism 3
- Claim: ChatGPT can support catalogers by generating initial LCSH suggestions, reducing time and effort while human oversight ensures quality.
- Mechanism: By providing a starting point with mostly correct MARC structure and some valid LCSH terms, ChatGPT allows experienced catalogers to focus on refinement rather than creation from scratch. This complementary approach leverages LLM efficiency for basic tasks while preserving human expertise for complex validation.
- Core assumption: The time saved in generating initial suggestions outweighs the time needed for human review and correction, and catalogers can effectively evaluate and improve LLM-generated suggestions.
- Evidence anchors:
  - [abstract]: "LLMs such as ChatGPT have the potential to reduce cataloging time needed for assigning LCSH subject terms for ETDs as well as to improve the discovery of this type of resource in academic libraries."
  - [section]: "Our study suggests LLMs could be used as an economical tool for the immediate generation of the LCSH given the cost associated with using Microsoft’s OpenAI API in the present study amounted to approximately USD $0.25 and the total time used for processing the 30 ETDs was roughly 3 minutes."
  - [corpus]: Moderate evidence - related studies on AI-assisted cataloging validation (Brzustowicz 2023, Zhang et al. 2023) support the complementary human-LLM workflow concept.
- Break condition: If catalogers spend more time correcting errors than they would have spent creating headings from scratch, or if LLM suggestions consistently miss critical content elements.

## Foundational Learning

- Concept: MARC 21 format structure and field coding
  - Why needed here: Understanding how ChatGPT produces MARC field codes (650, 600, 610, etc.) and why certain coding errors occur requires knowledge of MARC structure
  - Quick check question: What MARC field code would you use for a topical subject heading versus a personal name subject heading?

- Concept: Library of Congress Subject Headings (LCSH) structure and subdivision rules
  - Why needed here: Evaluating ChatGPT's LCSH validity requires understanding of authorized headings, variant terms, and the complex rules for combining main headings with subdivisions
  - Quick check question: Why can't "Press coverage" be used as a main topical subject heading in LCSH?

- Concept: Subject analysis principles: specificity and exhaustivity
  - Why needed here: The study evaluates ChatGPT's performance based on whether headings are specific enough and comprehensive enough to cover the work's content
  - Quick check question: How would you balance specificity versus exhaustivity when assigning subject headings to a multidisciplinary thesis?

## Architecture Onboarding

- Component map: ETD record input -> Python script for batch processing -> ChatGPT API call -> Excel spreadsheet for results -> Human cataloger review -> Quality assessment framework
- Critical path: ETD record input -> Prompt generation -> ChatGPT API call -> MARC output parsing -> Validation against LCSH rules -> Human review for specificity/exhaustivity
- Design tradeoffs: Using ChatGPT (closed-source, no retraining possible) versus open-source LLMs that could be fine-tuned on LCSH data; balancing automation benefits against potential for systematic errors in subdivision construction
- Failure signatures: Incorrect MARC field codes (wrong field types), invalid LCSH terms (non-authorized vocabulary), improper subdivision combinations, insufficient specificity/exhaustivity in coverage
- First 3 experiments:
  1. Test ChatGPT with a small sample of ETD records to verify MARC field coding accuracy before full implementation
  2. Compare ChatGPT-generated LCSH against manually assigned headings for the same records to measure validity rates
  3. Implement a reconciliation tool check on ChatGPT output to identify invalid LCSH terms automatically before human review

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of LLMs in assigning LCSH be improved for complex subject strings involving subdivisions?
- Basis in paper: [explicit] The paper states that ChatGPT "often fails to produce complex subject strings that adhere to intricate subdivision rules" and suggests potential solutions like using faceted vocabularies or proposing new subject authority records.
- Why unresolved: The paper presents possible solutions but does not test their effectiveness in improving LLM performance on complex LCSH assignments.
- What evidence would resolve it: An experiment comparing the accuracy of LLM-generated LCSH using different approaches (e.g., faceted vocabularies, proposed new authority records) on complex subject strings involving subdivisions.

### Open Question 2
- Question: What is the impact of using LLMs for LCSH assignment on cataloging efficiency and discoverability of library resources?
- Basis in paper: [explicit] The paper mentions that using LLMs could potentially "reduce cataloging time needed to assign LCSH subject terms" and improve discoverability by allowing catalogers to insert LLM-generated terms in the 653 field. However, it does not provide concrete data on the actual impact.
- Why unresolved: The paper does not present quantitative data on the time saved or the improvement in discoverability resulting from using LLMs for LCSH assignment.
- What evidence would resolve it: A study comparing the time taken and the quality of LCSH assignment with and without LLM assistance, as well as an analysis of the impact on resource discoverability using metrics like click-through rates or circulation statistics.

### Open Question 3
- Question: How does the performance of LLMs in assigning LCSH vary across different types of library resources (e.g., books, audio-visual materials, etc.)?
- Basis in paper: [explicit] The paper mentions that the study focused on ETDs and suggests that "future experiments could explore subject assignment in other frameworks, such as FAST, as well as in other types of works (books, audio-visual materials, etc.)."
- Why unresolved: The study only examined the performance of LLMs on ETDs and did not investigate how it varies for other types of library resources.
- What evidence would resolve it: A comparative study evaluating the accuracy of LLM-generated LCSH across different types of library resources, including books, audio-visual materials, and other formats, using the same methodology as the original study.

## Limitations

- Limited generalizability beyond ETDs to other library materials like monographs or journal articles
- Single LLM model testing without comparison to other models or traditional automated tools
- Small sample size of 30 records provides limited statistical power across subject domains

## Confidence

- High confidence: ChatGPT produces mostly correct MARC field coding (90% accuracy) and its training corpus includes LCSH vocabularies and MARC records from public sources
- Medium confidence: ChatGPT can support catalogers by generating initial suggestions, though this depends on unmeasured cataloger workflow efficiency
- Low confidence: Generalizability of findings to other library materials, different LLMs, or large-scale production environments

## Next Checks

1. **Cross-model comparison**: Test the same ETD records with multiple LLM models (GPT-4, Claude, Llama) and traditional automated subject heading tools to establish relative performance benchmarks and identify whether ChatGPT's limitations are model-specific or represent broader LLM challenges.

2. **Domain diversity validation**: Expand testing to include monographs, journal articles, and other library materials across humanities, social sciences, and STEM disciplines to assess whether ChatGPT's performance varies significantly by material type or subject complexity.

3. **Cataloger workflow efficiency study**: Conduct time-motion studies comparing catalogers creating LCSH from scratch versus using ChatGPT suggestions with varying levels of refinement, measuring actual time savings and error correction patterns in realistic production environments.