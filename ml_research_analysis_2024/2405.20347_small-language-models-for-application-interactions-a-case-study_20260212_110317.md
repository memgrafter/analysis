---
ver: rpa2
title: 'Small Language Models for Application Interactions: A Case Study'
arxiv_id: '2405.20347'
source_url: https://arxiv.org/abs/2405.20347
tags:
- language
- arxiv
- plan
- slms
- demand
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates the use of small language models (SLMs) versus
  large language models (LLMs) for enabling natural-language interactions with a Microsoft
  cloud supply chain fulfillment application. The goal is to translate user queries
  into executable code for tasks such as data extraction, plan generation, and what-if
  analysis.
---

# Small Language Models for Application Interactions: A Case Study

## Quick Facts
- arXiv ID: 2405.20347
- Source URL: https://arxiv.org/abs/2405.20347
- Reference count: 29
- Small language models (SLMs) can achieve higher accuracy than large language models (LLMs) on application-specific tasks while significantly reducing inference costs and latency.

## Executive Summary
This paper evaluates the use of small language models (SLMs) versus large language models (LLMs) for enabling natural-language interactions with a Microsoft cloud supply chain fulfillment application. The goal is to translate user queries into executable code for tasks such as data extraction, plan generation, and what-if analysis. The evaluation compares SLMs—including Phi-3, Llama 3, and Mistral v0.2—with GPT-3.5-turbo and GPT-4-turbo using both fine-tuning and in-context learning. Results show that SLMs achieve higher overall accuracy (up to 95.86%) compared to LLMs (up to 88.22%) while incurring significantly lower inference costs and latency. SLMs also require only a modest number of training examples to reach strong performance, with accuracy improving as the number of examples increases. The findings suggest SLMs can outperform LLMs in both accuracy and efficiency for application-specific interactions.

## Method Summary
The study fine-tunes small language models (Phi-3, Llama 3, Mistral v0.2) with task-specific datasets (1000 examples per task) using LoRA, and compares their performance against large language models (GPT-3.5-turbo, GPT-4-turbo) using in-context learning (1-20 shots). The evaluation measures overall accuracy, coder accuracy, F-1 score, latency, and cost per query for translating user queries into executable code for a cloud supply chain fulfillment application. The fine-tuning process uses consistent hyperparameters (batch size 16, learning rate 0.0002, 100k steps).

## Key Results
- SLMs achieve higher overall accuracy (up to 95.86%) compared to LLMs (up to 88.22%) for application-specific tasks
- SLMs provide significantly lower inference latency (seconds vs 1-2 minutes) and cost compared to LLMs
- Fine-tuning SLMs with modest training examples (1000 per task) outperforms in-context learning with 1-20 examples for LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small Language Models (SLMs) can achieve higher accuracy than Large Language Models (LLMs) on application-specific tasks when fine-tuned with a modest number of training examples.
- Mechanism: SLMs leverage high-quality, task-specific training data to specialize in a narrow domain, reducing the need for broad generalization and thereby achieving higher precision on known tasks.
- Core assumption: The quality and relevance of training data outweigh the benefits of model size for narrow, well-defined application interactions.
- Evidence anchors:
  - [abstract] "SLMs achieve higher overall accuracy (up to 95.86%) compared to LLMs (up to 88.22%) while incurring significantly lower inference costs and latency."
  - [section 2.1] "These small models operate with orders of magnitude less parameters but are capable of rivaling their larger counterparts on key performance benchmarks."
  - [corpus] Weak evidence: No direct neighbor papers confirm the accuracy claim; only general SLM benchmarking studies.
- Break condition: If the application domain is too broad or the task variability is high, the SLM's narrow specialization will degrade accuracy relative to LLMs.

### Mechanism 2
- Claim: SLMs provide significantly lower inference latency and cost compared to LLMs for the same task.
- Mechanism: Reduced parameter count and token processing requirements in SLMs lead to faster inference times and lower computational resource usage.
- Core assumption: Model inference speed scales inversely with parameter count and input/output token complexity.
- Evidence anchors:
  - [section 4.2] "Due to the significantly lower average input token count, the fine-tuned Phi-3 can handle queries quickly, typically taking only a few seconds, compared to 1-2 minutes for GPT-4-turbo."
  - [section 4.1] "The cost per query for an SLM is estimated by the renting price of a GPU VM per hour divided by the number of queries that can be processed per hour."
  - [corpus] Weak evidence: No neighbor papers provide latency or cost data for SLMs vs LLMs in similar settings.
- Break condition: If the SLM is hosted on resource-constrained hardware or if batch processing is not optimized, latency gains may diminish.

### Mechanism 3
- Claim: Fine-tuning SLMs with task-specific examples improves performance more efficiently than increasing in-context examples for LLMs.
- Mechanism: Fine-tuning allows the model to internalize task patterns, whereas in-context learning requires repeated prompting with examples for each query.
- Core assumption: Model weights updated via fine-tuning capture task-specific patterns more permanently than prompt-based examples.
- Evidence anchors:
  - [section 4.1] "Figure 1 shows the results, where the x-axis represents the number of shots... Phi-3 and Mistral obtain high accuracy with relatively few shots, and then improve gradually with more examples."
  - [section 3] "Fine-tuning thus becomes an attractive alternative, as one can increase the number of training examples to capture the diversity."
  - [corpus] Weak evidence: No neighbor papers confirm the fine-tuning advantage over in-context learning.
- Break condition: If the training dataset is too small or not representative, fine-tuning may lead to overfitting and reduced generalization.

## Foundational Learning

- Concept: Natural language to code translation
  - Why needed here: The system translates user queries into executable code snippets for the application.
  - Quick check question: Can you describe how a query like "What is the standard deviation of supplier S's inventory in the last T weeks?" is converted into a SQL query?
- Concept: Fine-tuning vs in-context learning
  - Why needed here: The paper compares fine-tuning SLMs with in-context learning for LLMs to understand performance differences.
  - Quick check question: What is the key difference between fine-tuning a model and providing in-context examples during inference?
- Concept: Function calling and API integration
  - Why needed here: The system must correctly map user intent to the appropriate application APIs.
  - Quick check question: How does the system determine which API to call for a given user query?

## Architecture Onboarding

- Component map: User Interface -> Language Model (SLM) -> Application Layer -> Execution Engine
- Critical path:
  1. User query input
  2. SLM code generation
  3. API invocation
  4. Result formatting and output
- Design tradeoffs:
  - Accuracy vs latency: SLMs offer faster responses but may require careful fine-tuning to maintain accuracy.
  - Model size vs resource usage: Smaller models use less compute but may need more training data for robustness.
  - Fine-tuning vs in-context learning: Fine-tuning is more efficient for fixed task sets, while in-context learning offers flexibility for unseen tasks.
- Failure signatures:
  - Low coder accuracy: Indicates poor alignment between user queries and generated code.
  - High out-of-domain queries: Suggests the model struggles to filter unsupported requests.
  - Slow response times: May indicate inefficient model inference or resource constraints.
- First 3 experiments:
  1. Measure accuracy and latency for a small set of representative queries using a fine-tuned SLM vs an LLM.
  2. Vary the number of training examples per task to observe the impact on accuracy and convergence.
  3. Test the system's ability to filter out-of-domain queries and provide appropriate fallback responses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of small language models (SLMs) scale with increasing dataset size beyond 1000 training examples per task?
- Basis in paper: [explicit] The paper notes that SLMs require only a modest number of training examples to reach strong performance, with accuracy improving as the number of examples increases, but does not explore beyond 1000 examples.
- Why unresolved: The evaluation focuses on the impact of training examples up to 1000 shots, leaving the scalability and potential saturation points unexplored.
- What evidence would resolve it: Conducting experiments with larger training datasets (e.g., 2000-5000 examples per task) to determine if SLMs continue to improve or plateau in performance.

### Open Question 2
- Question: How do SLMs perform on tasks outside the specific domain of cloud supply chain fulfillment, and what is their generalization capability?
- Basis in paper: [inferred] The study focuses on a single case study (cloud supply chain fulfillment), and while it highlights the potential of SLMs, it does not test their performance on diverse or unrelated tasks.
- Why unresolved: The paper does not provide evidence of SLMs' effectiveness across different application domains or task types.
- What evidence would resolve it: Evaluating SLMs on a broader range of applications and tasks, such as healthcare, agriculture, or general-purpose queries, to assess their generalization ability.

### Open Question 3
- Question: What is the impact of fine-tuning SLMs on computational resources and latency compared to their inference performance?
- Basis in paper: [explicit] The paper mentions that the estimated training cost for SLMs is negligible, but it does not discuss the computational resources or latency during the fine-tuning process.
- Why unresolved: The focus is on inference performance and cost, with limited information on the fine-tuning phase's resource requirements.
- What evidence would resolve it: Measuring the time, GPU/CPU usage, and energy consumption during the fine-tuning process to provide a complete picture of SLMs' resource efficiency.

## Limitations

- The proprietary nature of the application domain and lack of publicly available evaluation datasets make exact reproduction challenging
- The study focuses on a single cloud supply chain application, limiting generalizability to other domains
- Cost analysis relies on specific GPU VM pricing that may vary across cloud providers and hardware configurations

## Confidence

- **High Confidence**: The comparative advantage of SLMs in terms of inference latency and cost is well-supported by the empirical evidence provided.
- **Medium Confidence**: The accuracy claims (95.86% for SLMs vs 88.22% for LLMs) are supported by the evaluation metrics, but the exact task distribution and query complexity are not fully specified.
- **Low Confidence**: The mechanism by which fine-tuning consistently outperforms in-context learning across different task types is demonstrated but not thoroughly explained.

## Next Checks

1. Apply the same SLM fine-tuning approach to a different application domain (e.g., customer support ticket classification) and measure whether similar accuracy and efficiency gains are observed.
2. Systematically vary query complexity, phrasing, and ambiguity to assess whether SLM performance degrades faster than LLMs when faced with out-of-distribution inputs.
3. Conduct a comprehensive cost-performance analysis across different GPU configurations and model sizes to identify the optimal SLM configuration for various workload profiles.