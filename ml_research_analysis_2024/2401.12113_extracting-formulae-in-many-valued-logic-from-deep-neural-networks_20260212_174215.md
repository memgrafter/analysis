---
ver: rpa2
title: Extracting Formulae in Many-Valued Logic from Deep Neural Networks
arxiv_id: '2401.12113'
source_url: https://arxiv.org/abs/2401.12113
tags:
- term
- relu
- networks
- network
- logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a connection between deep ReLU networks\
  \ and infinite-valued \u0141ukasiewicz logic, proposing an algorithm to extract\
  \ logical formulae from trained networks. The key insight is that ReLU networks\
  \ with integer weights naturally implement McNaughton functions, which characterize\
  \ the truth functions of many-valued logic."
---

# Extracting Formulae in Many-Valued Logic from Deep Neural Networks

## Quick Facts
- arXiv ID: 2401.12113
- Source URL: https://arxiv.org/abs/2401.12113
- Reference count: 29
- Key outcome: Deep ReLU networks with integer weights implement Łukasiewicz many-valued logic; algorithm extracts logical formulae, with deep networks yielding exponentially shorter representations than shallow ones

## Executive Summary
This paper establishes a fundamental connection between deep ReLU neural networks and infinite-valued Łukasiewicz logic, showing that ReLU networks with integer weights naturally implement McNaughton functions - the truth functions of many-valued logic. The authors propose an algorithm that converts trained ReLU networks into equivalent logical formulae by transforming them into CReLU networks, extracting logical terms from individual neurons, and composing these terms according to the network structure. The work demonstrates that deep networks provide exponentially shorter logical formulae compared to shallow networks or existing methods like Schauder hat and hyperplane approaches, offering a new perspective on the expressive power of deep architectures and a method for interpreting their learned representations.

## Method Summary
The method involves three main steps: First, converting a ReLU network to an equivalent CReLU network to enable systematic extraction of logical terms from bounded neurons. Second, iteratively extracting MV terms from individual neurons using Lemma 3.3, which establishes the relationship between σ(f) and σ(f∘) operations. Third, composing the extracted logical terms according to the layered structure of the network to obtain the final logical formula. The algorithm is designed to preserve the compositional structure of deep networks, converting functional compositionality into algebraic compositionality, which results in significantly shorter logical formulae compared to shallow network representations.

## Key Results
- ReLU networks with integer weights naturally implement McNaughton functions, characterizing Łukasiewicz infinite-valued logic
- Deep networks yield exponentially shorter logical formulae than shallow networks or existing methods like Schauder hat and hyperplane approaches
- The extraction algorithm works by converting ReLU to CReLU networks, extracting terms from individual neurons, and composing them according to network structure
- The method extends to rational and real weights, corresponding to extensions of Łukasiewicz logic

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReLU networks with integer weights implement McNaughton functions, which are the truth functions of Łukasiewicz infinite-valued logic
- Mechanism: The paper establishes that ReLU networks with integer weights naturally realize continuous piecewise linear functions with integer coefficients, which exactly characterize the class of McNaughton functions in Łukasiewicz logic
- Core assumption: The connection between ReLU networks and many-valued logic holds for networks with integer weights specifically
- Evidence anchors:
  - [abstract] "ReLU networks with integer weights naturally implement McNaughton functions, which characterize the truth functions of many-valued logic"
  - [section] "We show that neural networks with the ReLU nonlinearity ρ(x) = max{0, x} and integer weights 2 naturally implement statements in MV logic and vice versa"
- Break condition: If weights are not integers, the direct correspondence breaks down and different logic systems (rational or real-valued extensions) are needed

### Mechanism 2
- Claim: Deep networks yield shorter logical formulae than shallow networks through compositional structure preservation
- Mechanism: The extraction algorithm honors the compositional structure of deep networks, converting functional compositionality into algebraic compositionality, which results in significantly shorter logical formulae compared to shallow networks or existing methods
- Core assumption: The compositional structure present in deep networks contains algebraic information that can be preserved during formula extraction
- Evidence anchors:
  - [abstract] "deep networks yield shorter logical formulae than shallow networks or existing methods like the Schauder hat and hyperplane approaches, with the difference growing exponentially with network depth"
  - [section] "functional compositionality is hence turned into algebraic compositionality. In contrast, application of our algorithm to the shallow network representation... ignores the compositional structure of gs"
- Break condition: If the network lacks compositional structure or uses a shallow architecture, the algorithm cannot preserve this information and produces longer formulae

### Mechanism 3
- Claim: The algorithm iteratively extracts logical terms from individual neurons and composes them according to network structure
- Mechanism: The algorithm converts ReLU networks to CReLU networks, extracts MV terms from individual neurons using iterative application of Lemma 3.3, then composes these terms according to the layered structure to obtain the final logical formula
- Core assumption: Individual neurons in CReLU networks can be systematically converted to logical terms that can be composed to represent the entire network
- Evidence anchors:
  - [abstract] "The algorithm works by converting the ReLU network into an equivalent one using the CReLU nonlinearity, then iteratively extracting logical terms from individual neurons and composing them according to the network structure"
  - [section] "It consists of the following three steps... Step 2: Extract MV terms from individual σ-neurons... Step 3: Compose the MV terms corresponding to the individual σ-neurons according to the layered structure"
- Break condition: If neurons cannot be individually converted to logical terms or composition fails to preserve the network's functionality

## Foundational Learning

- Concept: McNaughton functions and their characterization
  - Why needed here: Understanding that McNaughton functions are continuous piecewise linear functions with integer coefficients is crucial for grasping why ReLU networks with integer weights implement Łukasiewicz logic
  - Quick check question: What mathematical property characterizes the truth functions in Łukasiewicz infinite-valued logic?

- Concept: CReLU nonlinearity transformation
  - Why needed here: The algorithm requires converting ReLU networks to CReLU networks to enable systematic extraction of logical terms from individual neurons
  - Quick check question: How does the transformation from ReLU to CReLU enable the extraction of logical terms from bounded neurons?

- Concept: Lemma 3.3 iterative application
  - Why needed here: This lemma provides the core mechanism for converting linear combinations in neurons to logical terms, which is fundamental to the extraction algorithm
  - Quick check question: What is the key relationship established by Lemma 3.3 between σ(f) and σ(f∘) operations?

## Architecture Onboarding

- Component map: Input layer (x1, x2, ..., xn) -> Hidden layers (ReLU/CReLU neurons with integer weights) -> Output layer (produces value in [0,1] representing logical formula evaluation) -> Extraction engine (implements Steps 1-3 of the algorithm)

- Critical path:
  1. Convert ReLU network to equivalent CReLU network
  2. Extract MV terms from each neuron using Lemma 3.3
  3. Compose extracted terms according to network architecture
  4. Validate that the resulting logical formula equals the original network function

- Design tradeoffs:
  - Integer weights provide direct Łukasiewicz logic correspondence but may limit expressiveness compared to real weights
  - Deep architectures yield shorter formulae but increase computational complexity of extraction
  - CReLU conversion adds preprocessing overhead but enables systematic term extraction

- Failure signatures:
  - Formula extraction produces incorrect results: Check CReLU conversion step and neuron term extraction
  - Extracted formula is excessively long: Network may lack compositional structure or use shallow architecture
  - Algorithm fails on certain weight values: Verify weight type matches expected logic system (integer, rational, or real)

- First 3 experiments:
  1. Implement network conversion: Take a simple ReLU network with integer weights and verify it converts correctly to CReLU form
  2. Test term extraction: Apply Lemma 3.3 to individual neurons and verify the extracted MV terms are correct
  3. Validate composition: Compose extracted terms from a small network and verify they reproduce the original network's functionality

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the content, several implicit questions arise:
- How does the proposed algorithm for extracting logical formulae from deep ReLU networks scale with network width and depth in terms of computational complexity?
- Can the proposed method for extracting logical formulae be extended to other activation functions beyond ReLU, such as sigmoid, tanh, or modern activation functions like GELU?
- How does the proposed method for extracting logical formulae compare to other interpretability techniques for deep neural networks, such as feature visualization, saliency maps, or causal analysis?

## Limitations

- The algorithm's practical performance on large-scale, real-world networks remains unvalidated
- The paper focuses primarily on theoretical analysis and simple examples, with unknown scalability to complex architectures
- The conversion process from ReLU to CReLU networks and the composition of extracted terms require careful implementation to maintain correctness

## Confidence

- **High Confidence**: The theoretical foundation connecting ReLU networks with integer weights to McNaughton functions in Łukasiewicz logic is well-established and mathematically rigorous
- **Medium Confidence**: The claim that deep networks yield exponentially shorter formulae than shallow networks is supported by theoretical analysis but needs empirical validation on practical datasets
- **Medium Confidence**: The extension to rational and real weights, while theoretically sound, requires further validation to confirm practical applicability

## Next Checks

1. **Scalability Test**: Apply the extraction algorithm to a pre-trained deep ReLU network on a standard benchmark dataset (e.g., MNIST or CIFAR-10) and evaluate the quality and interpretability of the extracted logical formulae

2. **Comparative Analysis**: Implement the Schauder hat and hyperplane methods and compare the length and complexity of extracted formulae against the proposed algorithm across various network architectures and depths

3. **Robustness Evaluation**: Test the algorithm's performance with different weight initializations, including cases with non-integer weights, to assess its robustness and identify any failure modes in practical scenarios