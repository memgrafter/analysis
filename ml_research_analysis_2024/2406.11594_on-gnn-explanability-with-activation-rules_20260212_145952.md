---
ver: rpa2
title: On GNN explanability with activation rules
arxiv_id: '2406.11594'
source_url: https://arxiv.org/abs/2406.11594
tags:
- activation
- rules
- graph
- graphs
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INSIDE-GNN, a method for explaining Graph
  Neural Networks (GNNs) by mining activation rules in hidden layers. The key idea
  is to construct binary activation matrices from the internal representations of
  the GNN and discover rules that capture important configurations for classification
  decisions.
---

# On GNN explanability with activation rules

## Quick Facts
- arXiv ID: 2406.11594
- Source URL: https://arxiv.org/abs/2406.11594
- Authors: Luca Veyrin-Forrer; Ataollah Kamal; Stefan Duffner; Marc Plantevit; Céline Robardet
- Reference count: 13
- Primary result: Introduces INSIDE-GNN method achieving up to 200% improvement in explanation fidelity for GNN decisions

## Executive Summary
This paper introduces INSIDE-GNN, a novel method for explaining Graph Neural Networks (GNNs) by mining activation rules in hidden layers. The approach constructs binary activation matrices from GNN internal representations and discovers rules that capture important configurations for classification decisions. Using information-theoretic measures from the FORSIED framework, the method quantifies rule interestingness while iteratively updating background knowledge to avoid redundancy. The extracted activation rules are characterized using interpretable pattern languages like numerical subgroups and graph subgroups, providing insights into the hidden features learned by the GNN.

## Method Summary
INSIDE-GNN works by first constructing binary activation matrices from the internal representations of a trained GNN, where each entry indicates whether a hidden dimension is activated for a given node. The method then iteratively discovers activation rules using an information-theoretic approach based on the FORSIED framework, which quantifies the subjective interestingness of rules while accounting for background knowledge. As rules are extracted, the background model is updated to prevent redundancy. Finally, the activation rules are characterized using interpretable pattern languages such as numerical subgroups and graph subgroups to describe the nodes that support each rule in a discriminating way compared to other nodes.

## Key Results
- Achieves up to 200% improvement in explanation fidelity compared to state-of-the-art methods
- Demonstrates significant improvements in fidelity metrics (Fidelity, Infidelity, Sparsity) against baselines including GNNExplainer, PGExplainer, PGM-Explainer, and Grad
- Reveals specific subgraphs and topological properties that GNNs capture through characterization of activation rules
- Shows effectiveness across multiple real-world graph datasets including BA2, Aids, BBBP, Mutagen, DD, and Proteins

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Activation rules in hidden layers provide a principled way to explain GNN decisions by identifying important component configurations
- Mechanism: The method constructs binary activation matrices from the internal representations of the GNN and discovers rules that capture important configurations for classification decisions using information-theoretic measures from the FORSIED framework
- Core assumption: The activated components of the GNN's hidden vectors play a role in the decision process
- Evidence anchors:
  - [abstract]: "The key idea is to construct binary activation matrices from the internal representations of the GNN and discover rules that capture important configurations for classification decisions"
  - [section]: "The problem is therefore not only to discover highly discriminant activation rules but also to provide a pattern set that covers all GNN decisions on the input graphs"
- Break condition: If activated components do not significantly contribute to GNN decision process or information-theoretic measures fail to capture rule interestingness

### Mechanism 2
- Claim: The background model and its iterative update enable discovery of non-redundant activation rules
- Mechanism: The background model is initialized with basic assumptions about the activation matrix and updated as rules are extracted, allowing evaluation of rule interest based on knowledge it brings relative to existing background knowledge
- Core assumption: The background model accurately represents initial knowledge about activation matrix and its update effectively integrates knowledge from each extracted rule
- Evidence anchors:
  - [section]: "We consider the discrete random variable H ℓrv, ks associated to the activation matrix xH ℓrv, ks2, and we model the background knowledge by the probability P pH ℓrv, ks " 1q"
  - [section]: "The model P integrates the rule Aℓ Ñ c as follows: @k such that pAℓqk " 1 and v such that xH ℓrv, ks " p Aℓqk, P pH ℓrv, ks " 1q is set to 1"
- Break condition: If background model does not accurately represent initial knowledge or its update fails to integrate knowledge from extracted rules

### Mechanism 3
- Claim: Characterization using interpretable pattern languages provides insights into hidden features learned by GNN
- Mechanism: Once activation patterns are found, they are characterized using interpretable pattern languages such as numerical subgroups and graph subgroups to describe nodes supporting each pattern in discriminating way
- Core assumption: Interpretable pattern languages accurately capture characteristics of nodes supporting activation patterns and these characteristics relate to hidden features learned by GNN
- Evidence anchors:
  - [abstract]: "The extracted activation rules are then characterized using interpretable pattern languages such as numerical subgroups and graph subgroups to provide insights into the hidden features learned by the GNN"
  - [section]: "Many pattern domains can be used to that end. In the following, we consider two of them: one based on numerical descriptions and the other one based on common subgraphs"
- Break condition: If interpretable pattern languages do not accurately capture node characteristics or these characteristics are not related to hidden features learned by GNN

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their internal representations
  - Why needed here: Understanding how GNNs work and represent data in hidden layers is crucial for understanding how INSIDE-GNN discovers and characterizes activation rules
  - Quick check question: What is the role of the ego-graphs centered at each node in a GNN, and how are they represented in the internal vectors of the GNN?

- Concept: Information theory and the FORSIED framework
  - Why needed here: The method uses information-theoretic measures rooted in the FORSIED framework to quantify subjective interestingness of activation rules while accounting for background knowledge
  - Quick check question: How does the FORSIED framework define the subjective interestingness of a pattern, and how does it use information theory to quantify this interestingness?

- Concept: Pattern languages and subgroup discovery
  - Why needed here: The method characterizes activation rules using interpretable pattern languages such as numerical subgroups and graph subgroups to describe nodes supporting each pattern
  - Quick check question: What is the difference between numerical subgroups and graph subgroups, and how are they used to characterize nodes supporting an activation pattern?

## Architecture Onboarding

- Component map: Binary activation matrix construction -> Iterative rule discovery using FORSIED framework -> Background model update -> Characterization using interpretable pattern languages -> Instance-level explanations or model insights
- Critical path: Discovery of activation rules through iterative algorithm involving construction of binary activation matrices, rule discovery, and characterization using interpretable pattern languages
- Design tradeoffs: Trades off between complexity of activation rules and their ability to capture GNN's decision-making process; more complex rules may capture nuanced aspects but be harder to interpret
- Failure signatures: If activation rules do not accurately capture GNN's decision-making process or interpretable pattern languages do not accurately characterize nodes supporting rules, method may fail to provide meaningful explanations
- First 3 experiments:
  1. Test construction of binary activation matrices on simple GNN model and synthetic graph dataset, verify matrices accurately capture activated components of internal representations
  2. Implement iterative algorithm for rule discovery on small graph dataset, verify it discovers non-redundant activation rules capturing GNN's decision-making process
  3. Test characterization of activation rules using interpretable pattern languages on graph dataset with known topological properties, verify characterizations accurately capture characteristics of nodes supporting rules

## Open Questions the Paper Calls Out
None

## Limitations
- The MaxEnt background model assumes independent activations across dimensions, which may not hold for real-world GNNs where activation patterns exhibit dependencies
- The claim of "up to 200% improvement" in fidelity lacks clear specification of baseline conditions and statistical significance measures
- Results are primarily demonstrated on GCN architectures, raising questions about generalizability to other GNN architectures like GAT or GraphSAGE

## Confidence

**Major Uncertainties:**
The primary limitation lies in the dependency on the MaxEnt background model's initialization, which assumes independent activations across dimensions. This assumption may not hold for real-world GNNs where activation patterns exhibit dependencies. Additionally, the claim of "up to 200% improvement" in fidelity lacks clear specification of the baseline conditions and statistical significance measures.

**Confidence Labels:**
- **High confidence**: The mechanism of constructing binary activation matrices from hidden representations and the general framework of iterative rule discovery
- **Medium confidence**: The effectiveness of information-theoretic measures (FORSIED framework) in quantifying rule interestingness
- **Low confidence**: The generalizability of results across diverse GNN architectures beyond GCNs, and the scalability claims for large datasets

## Next Checks

1. Test INSIDE-GNN on multiple GNN architectures (GAT, GraphSAGE) to verify if the activation rule discovery generalizes beyond GCNs
2. Conduct ablation studies removing the background model update mechanism to quantify its contribution to explanation quality
3. Evaluate the computational complexity on larger graph datasets (10K+ nodes) to verify scalability claims and identify performance bottlenecks