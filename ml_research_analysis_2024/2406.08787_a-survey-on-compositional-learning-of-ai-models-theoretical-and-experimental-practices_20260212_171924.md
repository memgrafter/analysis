---
ver: rpa2
title: 'A Survey on Compositional Learning of AI Models: Theoretical and Experimental
  Practices'
arxiv_id: '2406.08787'
source_url: https://arxiv.org/abs/2406.08787
tags:
- compositional
- learning
- language
- https
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey systematically reviews compositional learning in AI\
  \ models, covering theoretical foundations, experimental practices, and evaluation\
  \ benchmarks. The authors categorize compositionality into five measures\u2014systematicity,\
  \ productivity, substitutivity, localism, and overgeneralization\u2014and examine\
  \ how these relate to generalization in language and vision tasks."
---

# A Survey on Compositional Learning of AI Models: Theoretical and Experimental Practices

## Quick Facts
- arXiv ID: 2406.08787
- Source URL: https://arxiv.org/abs/2406.08787
- Authors: Sania Sinha; Tanawan Premsri; Parisa Kordjamshidi
- Reference count: 40
- This survey systematically reviews compositional learning in AI models, covering theoretical foundations, experimental practices, and evaluation benchmarks

## Executive Summary
This comprehensive survey examines compositional learning in AI models, focusing on how neural networks and transformers handle the ability to recombine learned concepts in novel ways. The authors systematically categorize compositionality into five distinct measures and survey the landscape of datasets, models, and evaluation methods. They identify critical gaps in current research, particularly the lack of benchmarks for substitutivity, localism, and overgeneralization measures, while highlighting challenges like error propagation and data contamination that affect compositional reasoning evaluation.

## Method Summary
The survey employs a systematic literature review methodology, categorizing compositional learning research into four main components: Compositional Learning Facets (five measures of compositionality), Datasets (evaluation benchmarks like SCAN, gSCAN, COGS, CREPE), Compositional Learning Models (basic neural networks, transformers, neurosymbolic architectures), and Evaluation (theoretical and empirical methods). The authors analyze both theoretical studies exploring limitations of current architectures and experimental practices using synthetic and realistic datasets to assess compositional generalization capabilities.

## Key Results
- Compositionality is categorized into five measures: systematicity, productivity, substitutivity, localism, and overgeneralization
- Current evaluation benchmarks primarily focus on systematicity and productivity, leaving substitutivity, localism, and overgeneralization underexplored
- Transformers show significant limitations in compositional reasoning, particularly for length generalization and error propagation
- Data contamination and synthetic data limitations pose major challenges for evaluating true compositional generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Systematicity is the most well-studied measure of compositionality because it directly tests a model's ability to recombine known concepts into novel expressions.
- Mechanism: Systematicity tasks create test splits where atomic concepts are seen separately during training but combined in new ways during testing, forcing the model to generalize beyond memorized patterns.
- Core assumption: The model learns representations of individual concepts that can be recombined during inference.
- Evidence anchors:
  - [abstract]: "They categorize compositionality into five measuresâ€”systematicity, productivity, substitutivity, localism, and overgeneralization"
  - [section]: "Systematicity is one of the most commonly used notions of compositionality in evaluating the performance of computational models... It has been defined as the ability to systematically recombine known parts and rules"
  - [corpus]: Weak - the corpus neighbors are mostly unrelated to systematicity evaluation
- Break condition: The model memorizes compound expressions instead of learning separable concept representations, causing failure on novel combinations.

### Mechanism 2
- Claim: Productivity/length generalization is challenging for transformers because their positional encodings limit their ability to handle sequences longer than training data.
- Mechanism: When tested on longer sequences than seen during training, transformers struggle because their attention mechanisms and positional encodings don't generalize well to new sequence lengths.
- Core assumption: The model's architecture imposes hard constraints on the maximum sequence length it can effectively process.
- Evidence anchors:
  - [abstract]: "Theoretical studies explore limitations of transformers and large language models in compositional reasoning, highlighting challenges like error propagation"
  - [section]: "Another commonly explored test for compositionality is length generalization or productivity... In this evaluation, models are tested on their performance with expressions or sequences that are longer than training data"
  - [corpus]: Weak - corpus contains no relevant evidence about length generalization limitations
- Break condition: The model's positional encoding scheme cannot effectively represent positions beyond the training distribution, causing attention patterns to break down.

### Mechanism 3
- Claim: Current evaluation benchmarks suffer from data contamination and synthetic data limitations, making it difficult to assess true compositional generalization.
- Mechanism: Language models trained on internet-scale data may have seen test examples during pre-training, and synthetic datasets may not capture the complexity of real-world compositional reasoning.
- Core assumption: Test data overlap with training data invalidates compositional generalization claims, and synthetic data doesn't reflect real-world complexity.
- Evidence anchors:
  - [abstract]: "They identify gaps in current evaluation methods, particularly the lack of benchmarks for substitutivity, localism, and overgeneralization"
  - [section]: "While some datasets in the computer vision community for compositional learning utilize realistic images, they address fewer aspects of compositionality"
  - [corpus]: Weak - corpus neighbors don't address data contamination issues
- Break condition: The model's performance on benchmark tasks is explained by memorization or pattern matching rather than genuine compositional reasoning.

## Foundational Learning

- Concept: Systematicity vs. Productivity distinction
  - Why needed here: Understanding these different measures of compositionality is crucial for interpreting evaluation results and designing appropriate tests
  - Quick check question: If a model sees "red car" and "blue hat" during training, what should it be able to generate during systematicity testing?

- Concept: Transformer architecture limitations
  - Why needed here: Knowing why transformers struggle with compositional tasks helps in selecting appropriate architectures or designing better models
  - Quick check question: What architectural component of transformers most directly limits their ability to handle sequences longer than training data?

- Concept: Data contamination in LLM evaluation
  - Why needed here: Understanding this issue is essential for interpreting LLM performance on compositional tasks and designing better evaluation protocols
  - Quick check question: Why might a language model perform well on a compositional task even if it doesn't truly understand compositionality?

## Architecture Onboarding

- Component map: Compositional Learning Facets -> Datasets -> Models -> Evaluation
- Critical path: Understanding the relationship between compositional measures and appropriate evaluation datasets, then selecting or designing models that can address the specific compositional challenges
- Design tradeoffs: Synthetic vs. realistic data (synthetic is cleaner but less realistic; realistic is more complex but better reflects real-world challenges), specialized vs. general architectures (specialized may perform better on specific tasks but lack generality)
- Failure signatures: Poor performance on out-of-distribution compositions, inability to handle longer sequences than training data, reliance on pattern matching rather than true compositional reasoning
- First 3 experiments:
  1. Test a basic transformer on SCAN dataset systematicity split to observe failure modes in compositional generalization
  2. Evaluate the same model on SCAN productivity split to assess length generalization capabilities
  3. Compare performance on synthetic vs. realistic compositional datasets to understand the impact of data complexity on model behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relative importance of systematicity versus productivity in compositional generalization for real-world applications?
- Basis in paper: [explicit] The paper notes that "Only Systematicity and Productivity have been well-researched and have established connections to evaluation benchmarks" while other measures remain underexplored.
- Why unresolved: While systematicity and productivity are well-studied, the paper suggests that their relative importance for practical AI systems is unclear, especially given the current focus on large language models.
- What evidence would resolve it: Comparative studies evaluating systematicity and productivity measures across diverse real-world datasets and tasks, showing their impact on model performance and generalization.

### Open Question 2
- Question: How can we develop evaluation benchmarks that effectively distinguish compositional generalization from memorization and data contamination?
- Basis in paper: [explicit] The paper identifies this as a major challenge: "One issue in current evaluations is that controlled and clean tests of compositonality are mostly synthesized" and "the difficulty in disentangling the compositional reasoning from the data contamination."
- Why unresolved: Current benchmarks often rely on synthetic data or realistic data with potential contamination, making it difficult to assess true compositional generalization.
- What evidence would resolve it: New benchmarks using diverse, realistic datasets with carefully controlled contamination levels and evaluation metrics that specifically target compositional reasoning.

### Open Question 3
- Question: What architectural modifications to transformers and large language models can improve their compositional generalization abilities?
- Basis in paper: [explicit] The paper discusses various architectural modifications explored in recent research, such as relative positional encodings, copy decoders, and intermediate representations, but notes that their effectiveness varies depending on the task.
- Why unresolved: While some architectural modifications have shown promise, the paper highlights the lack of a consistent methodology and the need for more research to identify the most effective approaches.
- What evidence would resolve it: Systematic studies comparing the performance of different architectural modifications on a wide range of compositional generalization tasks, with a focus on identifying generalizable principles.

## Limitations
- Most datasets are synthetic, limiting ecological validity
- Limited empirical validation of theoretical claims about transformer failures
- Incomplete coverage of substitutivity and localism measures

## Confidence
- Claims about systematicity evaluation methods: High
- Claims about productivity assessment and transformer limitations: Medium
- Claims about substitutivity and localism evaluation: Low

## Next Checks
1. Replicate SCAN systematicity experiments with multiple transformer architectures to quantify the frequency of compositional generalization failures and determine if they stem from architectural constraints or training procedures.

2. Conduct data contamination analysis by comparing model performance on benchmark tasks against probability of encountering similar examples during pre-training, establishing confidence intervals for compositional generalization claims.

3. Design and evaluate a minimal benchmark that isolates substitutivity and localism measures, testing whether current models can handle these compositional aspects independently of systematicity and productivity tasks.