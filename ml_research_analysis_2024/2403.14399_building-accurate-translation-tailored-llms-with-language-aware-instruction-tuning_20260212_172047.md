---
ver: rpa2
title: Building Accurate Translation-Tailored LLMs with Language Aware Instruction
  Tuning
arxiv_id: '2403.14399'
source_url: https://arxiv.org/abs/2403.14399
tags:
- translation
- language
- llms
- training
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of off-target translation in large
  language models (LLMs), particularly for low-resource languages. The core method
  involves a two-stage fine-tuning algorithm that first elicits basic translation
  capabilities using maximum likelihood estimation (MLE) loss on translation data,
  and then further improves instruction-following ability by introducing an unlikelihood
  loss on instruction-conflicting samples.
---

# Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning

## Quick Facts
- **arXiv ID**: 2403.14399
- **Source URL**: https://arxiv.org/abs/2403.14399
- **Reference count**: 40
- **Primary result**: Reduces off-target translation ratio by -53.3% while improving SacreBLEU by +5.7 and BLEURT by +16.4

## Executive Summary
This paper addresses the problem of off-target translation in large language models (LLMs), where models generate translations in the wrong language despite correct instructions. The authors propose a two-stage fine-tuning approach that first elicits basic translation capabilities using MLE loss, then improves instruction-following ability through unlikelihood loss on instruction-conflicting samples. Experiments on IWSLT and WMT benchmarks using LLaMA demonstrate significant improvements in translation accuracy and instruction adherence while preserving general task performance.

## Method Summary
The method involves a two-stage fine-tuning algorithm. Stage 1 uses maximum likelihood estimation (MLE) loss on translation data to elicit basic translation capabilities in the LLM. Stage 2 introduces unlikelihood loss on instruction-conflicting samples, which are created by randomly replacing the translation direction within the instruction. This contrastive training approach forces the model to associate correct instructions with correct outputs, thereby improving instruction-following ability and reducing off-target translations.

## Key Results
- Reduces off-target translation ratio by an average of -53.3%
- Improves translation quality with average +5.7 SacreBLEU scores
- Achieves +16.4 BLEURT score improvements
- Preserves general task performance on AlpacaEval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Off-target translation occurs because standard MLE training causes the model to overlook translation instructions (especially language direction) in favor of next-token prediction.
- Mechanism: MLE loss on translation data teaches the model to predict the next token given the instruction and input, but this objective does not enforce strict adherence to the language direction specified in the instruction.
- Core assumption: The language direction in the instruction is the primary signal for selecting the correct target language, and ignoring it leads to off-target translations.
- Evidence anchors: Abstract states the method doesn't improve instruction-following ability, section attributes problem to overlooking instruction information.
- Break condition: If the language direction signal in the instruction is ambiguous or missing, the mechanism would fail because the model has no explicit target to enforce.

### Mechanism 2
- Claim: Unlikelihood loss on instruction-conflicting samples forces the model to assign low probability to translations that do not match the prescribed language direction.
- Mechanism: Instruction-conflicting samples are created by replacing the correct instruction with a random wrong one while keeping input and output unchanged. During training, the unlikelihood loss penalizes the model for generating the correct output given the wrong instruction.
- Core assumption: The model can learn to distinguish correct instruction-output pairs from incorrect ones by contrastive training with unlikelihood loss.
- Evidence anchors: Abstract describes creating instruction-conflicting samples and introducing unlikelihood loss to learn them.
- Break condition: If the model overfits to the unlikelihood loss, translation quality could degrade.

### Mechanism 3
- Claim: Two-stage fine-tuning first elicits basic translation capability with MLE, then refines instruction adherence with unlikelihood loss, leading to better zero-shot translation.
- Mechanism: Stage 1 (MLE on translation data) unlocks the model's translation ability. Stage 2 adds instruction-conflicting samples and unlikelihood loss, which emphasizes the instruction's effect without harming the translation capability acquired in stage 1.
- Core assumption: The two-stage approach allows separation of translation capability elicitation from instruction-following refinement, preventing interference between the two objectives.
- Evidence anchors: Abstract describes the two-stage approach with MLE first, then unlikelihood on conflicting samples.
- Break condition: If stage 1 is insufficient to elicit translation capability, stage 2 cannot refine it; if stage 2 is too aggressive, it may hurt translation quality.

## Foundational Learning

- Concept: Maximum Likelihood Estimation (MLE) loss
  - Why needed here: MLE is the standard training objective for language models; understanding it is crucial to see why it alone is insufficient for enforcing instruction adherence.
  - Quick check question: In MLE training for translation, what is the model trying to maximize given an instruction and input?

- Concept: Unlikelihood training
  - Why needed here: Unlikelihood loss is the key innovation for mitigating off-target translation; understanding how it contrasts with MLE is essential.
  - Quick check question: How does unlikelihood loss differ from MLE loss in terms of what it encourages the model to do?

- Concept: Instruction-following in LLMs
  - Why needed here: The paper's focus is on improving the model's ability to follow instructions, especially language direction; understanding instruction tuning is foundational.
  - Quick check question: In instruction tuning, what are the three main components of each training instance?

## Architecture Onboarding

- Component map: Pre-trained LLaMA model -> Multilingual translation dataset -> Instruction-conflicting sample generator -> Two-stage training pipeline -> Evaluation metrics
- Critical path: 1) Load pre-trained LLaMA 2) Prepare multilingual translation data with formatted instructions 3) Stage 1: Fine-tune with MLE loss on translation pairs 4) Generate instruction-conflicting samples by replacing instruction with random wrong one 5) Stage 2: Fine-tune with combined MLE + unlikelihood loss 6) Evaluate on zero-shot translation directions
- Design tradeoffs: Mixing parameter α balances MLE and unlikelihood loss; too high causes overfitting, too low insufficient mitigation of off-target problem; number of unlikelihood training steps: more steps reduce OTR but risk overfitting; model size: larger models show better performance but increase computational cost
- Failure signatures: High OTR despite training: likely insufficient α or too few unlikelihood steps; Degraded translation quality: possible overfitting to unlikelihood loss (α too high); Off-target translations persist: instruction-conflicting samples not diverse enough or model not learning the contrast
- First 3 experiments: 1) Train Stage 1 only (MLE fine-tuning) and evaluate OTR to confirm baseline off-target problem 2) Train Stage 2 with α=0.05 and 100 steps; compare OTR and translation quality to Stage 1 3) Vary α (e.g., 0.01, 0.05, 0.1, 0.3) and measure OTR and BLEURT to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the unlikelihood training affect the model's performance on general tasks beyond translation, and is there a way to balance the trade-off between translation accuracy and general task performance?
- Basis in paper: The paper discusses the model's performance on general tasks using AlpacaEval and mentions that the method preserves general task performance.
- Why unresolved: While the paper shows that the method maintains general task performance, it does not explore the impact on other specific tasks or provide a detailed analysis of the trade-off between translation accuracy and general task performance.
- What evidence would resolve it: Conducting experiments on a wider range of tasks and analyzing the trade-off between translation accuracy and performance on these tasks would provide insights into the model's versatility and potential limitations.

### Open Question 2
- Question: Can the proposed method be extended to other types of hallucinations in LLMs, such as fact-conflicting or context-conflicting hallucinations, and what would be the impact on the model's performance?
- Basis in paper: The paper focuses on the off-target translation problem and uses unlikelihood training to address it, suggesting the potential for extending this approach to other hallucination types.
- Why unresolved: The paper does not explore the application of the method to other hallucination types or discuss its effectiveness in those contexts.
- What evidence would resolve it: Applying the method to datasets involving fact-conflicting or context-conflicting hallucinations and evaluating the model's performance would demonstrate its broader applicability and limitations.

### Open Question 3
- Question: How does the size of the translation dataset affect the model's performance, and is there an optimal dataset size for achieving the best balance between translation accuracy and model efficiency?
- Basis in paper: The paper discusses the impact of translation data size on performance and mentions that the benefits of increasing dataset size become negligible beyond a certain point.
- Why unresolved: While the paper provides insights into the relationship between dataset size and performance, it does not determine an optimal dataset size or explore the trade-off between translation accuracy and model efficiency.
- What evidence would resolve it: Conducting experiments with varying dataset sizes and analyzing the trade-off between translation accuracy and model efficiency would help identify the optimal dataset size for different use cases.

## Limitations

- Relies primarily on internal experimental evidence rather than external corpus validation
- Lacks detailed implementation specifications for critical components like instruction-conflicting sample generation and unlikelihood loss
- Does not provide ablation studies to quantify individual contributions of different training components

## Confidence

- Off-target translation problem identification: High
- Two-stage training methodology: Medium
- Reported performance improvements: Medium
- Generalizability to other language pairs: Low

## Next Checks

1. **Independent Reproduction**: Replicate the two-stage training pipeline on a held-out translation dataset to verify the reported OTR reduction and translation quality improvements, particularly focusing on the instruction-conflicting sample generation and unlikelihood loss implementation.

2. **Ablation Study**: Conduct controlled experiments to isolate the contributions of MLE fine-tuning versus unlikelihood fine-tuning to the overall performance gains, varying the mixing parameter α systematically.

3. **Cross-Lingual Generalization**: Test the method on additional low-resource language pairs not included in the original training data to assess the model's ability to generalize the instruction-following capability to truly unseen translation directions.