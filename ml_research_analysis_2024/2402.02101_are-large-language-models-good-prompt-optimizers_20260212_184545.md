---
ver: rpa2
title: Are Large Language Models Good Prompt Optimizers?
arxiv_id: '2402.02101'
source_url: https://arxiv.org/abs/2402.02101
tags:
- prompt
- answer
- steps
- prompts
- direction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the effectiveness of using large language
  models (LLMs) as prompt optimizers through a comprehensive study. The authors find
  that LLM-based reflection processes often generate repetitive and potentially invalid
  feedback, as optimizers may rely on prior knowledge rather than truly analyzing
  errors.
---

# Are Large Language Models Good Prompt Optimizers?

## Quick Facts
- arXiv ID: 2402.02101
- Source URL: https://arxiv.org/abs/2402.02101
- Authors: Ruotian Ma; Xiaolei Wang; Xin Zhou; Jian Li; Nan Du; Tao Gui; Qi Zhang; Xuanjing Huang
- Reference count: 40
- Large language models struggle as prompt optimizers due to repetitive feedback and poor error analysis

## Executive Summary
This work investigates whether large language models can effectively serve as prompt optimizers for improving task performance on black-box target models. Through comprehensive experiments across four BigBench tasks, the authors find that LLM-based optimizers consistently generate repetitive and potentially invalid feedback, relying more on prior knowledge than genuine error analysis. While semantic refinements can be made, these often fail to translate into improved target model behavior due to the gap between prompt generation and actual model capabilities. The study introduces a new "Automatic Behavior Optimization" paradigm that directly optimizes target model behavior through step-by-step instruction refinement and demonstration examples, showing significant performance improvements especially for tasks where target models previously struggled.

## Method Summary
The paper evaluates five LLM-based prompt optimization methods (Iterative-APE, APO, APO-Sum, PromptAgent, OPRO) on four BigBench tasks using GPT-4 as the optimizer. All methods use a unified experimental setting with LLM-based prompt initialization (4 examples), beam search strategy, and evaluation protocol. The methods differ in their prompt updating mechanisms: resampling, explicit reflection and refinement, or implicit reflection. The proposed Automatic Behavior Optimization paradigm directly optimizes target model behavior by refining step-by-step instructions with demonstration examples. Experiments are conducted on two target models (GPT-3.5-Turbo and Llama-2-70B-chat) with training and test score collection at each optimization step.

## Key Results
- LLM optimizers generate repetitive feedback regardless of error distribution patterns
- Semantic refinements often fail to translate into behavioral improvements due to target model limitations
- The Automatic Behavior Optimization paradigm significantly outperforms existing methods, especially for difficult tasks
- Target models struggle to follow complex instructions even when prompts are semantically refined

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs struggle to identify true causes of errors during reflection, instead relying on prior knowledge
- **Mechanism**: When generating feedback, LLM optimizers analyze error examples but tend to generate repetitive feedback based on their own task priors rather than genuine error analysis
- **Core assumption**: The reflection process requires understanding error patterns rather than pattern matching to known issues
- **Evidence anchors**:
  - [abstract] "LLM optimizers struggle to identify the true causes of errors during reflection, tending to be biased by their own prior knowledge"
  - [section] "The LLM optimizers consistently generate similar feedbacks regardless of the error distributions"
  - [corpus] Weak - no direct citations on reflection bias in prompt optimization
- **Break condition**: When error patterns require novel analysis outside the model's training distribution

### Mechanism 2
- **Claim**: LLMs fail to generate appropriate prompts even when semantic refinements are valid
- **Mechanism**: The gap between prompt generation and target model behavior causes prompts to be semantically correct but behaviorally ineffective
- **Core assumption**: Target models have inconsistent instruction-following capabilities that aren't captured during prompt refinement
- **Evidence anchors**:
  - [abstract] "LLM optimizers often fail to generate appropriate prompts in a single step due to the gap between prompt generation and target model behavior"
  - [section] "Target models face difficulties in appropriately following the generated instructions"
  - [corpus] Weak - limited corpus evidence on instruction-following gaps
- **Break condition**: When target model behavior is predictable and consistent

### Mechanism 3
- **Claim**: Direct behavior optimization is more effective than prompt optimization for improving target model performance
- **Mechanism**: By optimizing step-by-step instructions with demonstration examples, the approach ensures target models follow refinements correctly
- **Core assumption**: Target models can follow well-structured demonstrations better than natural language prompts
- **Evidence anchors**:
  - [abstract] "We introduce a new 'Automatic Behavior Optimization' paradigm, which directly optimizes target model's behavior in a more controllable manner"
  - [section] "Experimental results show that this approach significantly improves performance, especially for tasks where target models previously struggled"
  - [corpus] Weak - only mentions similar work but no direct evidence
- **Break condition**: When target models have strong instruction-following capabilities already

## Foundational Learning

- **Concept**: Error analysis and reflection mechanisms
  - **Why needed here**: Understanding how LLM optimizers identify and correct errors is central to the paper's findings
  - **Quick check question**: What distinguishes genuine error analysis from pattern matching in LLM reflection?

- **Concept**: Semantic space and prompt refinement
  - **Why needed here**: The paper evaluates whether LLM optimizers can navigate semantic spaces effectively
  - **Quick check question**: How does semantic similarity differ from behavioral effectiveness in prompt optimization?

- **Concept**: Instruction-following behavior in LLMs
  - **Why needed here**: The gap between prompt generation and target model behavior is a key finding
  - **Quick check question**: What factors influence an LLM's ability to follow complex multi-step instructions?

## Architecture Onboarding

- **Component map**: Error example → LLM reflection → Feedback generation → Prompt refinement → Target model evaluation → Score update
- **Critical path**: Error example → LLM reflection → Feedback generation → Prompt refinement → Target model evaluation → Score update
- **Design tradeoffs**: 
  - Reflection depth vs. computational cost
  - Semantic accuracy vs. behavioral effectiveness
  - Demonstration complexity vs. target model capacity
- **Failure signatures**:
  - Repetitive feedback patterns
  - Poor correlation between semantic changes and performance gains
  - Target model fails to follow refined instructions
- **First 3 experiments**:
  1. Implement uniform reflection vs. error-specific reflection comparison
  2. Test prompt refinement with and without instruction demonstrations
  3. Compare semantic similarity metrics vs. behavioral performance metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can large language models effectively identify the true causes of errors during prompt optimization, or are they merely making educated guesses based on their prior knowledge?
- Basis in paper: [explicit] The paper explicitly states that LLM optimizers "struggle to identify the true causes of errors during reflection, tending to be biased by their own prior knowledge rather than genuinely reflecting on the errors."
- Why unresolved: The paper demonstrates that LLM optimizers generate similar feedback regardless of the error distribution, suggesting they might not be effectively identifying the root causes of errors.
- What evidence would resolve it: A controlled experiment where LLM optimizers are presented with a variety of error types and their ability to correctly identify the cause of each error is measured and compared to human performance.

### Open Question 2
- Question: How can the gap between LLM-generated prompts and the actual performance of those prompts on target models be effectively bridged?
- Basis in paper: [inferred] The paper discusses the challenges faced by LLM optimizers in generating appropriate prompts for target models due to the gap between prompt generation and target model behavior.
- Why unresolved: The paper highlights the issue but does not provide a definitive solution for bridging this gap.
- What evidence would resolve it: Research into techniques that improve the alignment between LLM-generated prompts and the expected behavior of target models, such as incorporating target model feedback into the prompt generation process.

### Open Question 3
- Question: Is the "Automatic Behavior Optimization" paradigm a viable and scalable approach for improving the performance of large language models on complex tasks?
- Basis in paper: [explicit] The paper introduces the "Automatic Behavior Optimization" paradigm and demonstrates its effectiveness on certain tasks, particularly for models with limited capabilities.
- Why unresolved: The paper presents a preliminary investigation into the ABO paradigm, and its long-term effectiveness and scalability for a wider range of tasks and model types remain to be seen.
- What evidence would resolve it: Extensive testing of the ABO paradigm on a diverse set of tasks and model architectures, along with comparisons to existing prompt optimization methods, would provide insights into its viability and scalability.

## Limitations
- Limited to four BigBench tasks and two target model types, restricting generalizability
- Several critical implementation details remain unspecified, particularly for the ABO paradigm
- Evidence supporting mechanisms is primarily correlational rather than causal

## Confidence
**High Confidence**: LLM optimizers generate repetitive feedback regardless of error distributions; There exists a gap between semantic prompt refinement and target model behavior; The ABO paradigm shows significant performance improvements over existing methods; Target models struggle to follow complex instructions in refined prompts

**Medium Confidence**: LLMs rely on prior knowledge rather than genuine error analysis during reflection; Semantic refinements don't always translate to behavioral improvements; Demonstration-based approaches improve instruction-following

**Low Confidence**: The exact mechanism of how LLM optimizers fail to identify true error causes; The generalizability of findings across different task types and model architectures

## Next Checks
**Check 1: Mechanism Validation Experiment** Design a controlled experiment where error distributions are systematically manipulated while keeping semantic similarity constant. Compare feedback generation across different error patterns to isolate whether LLM optimizers truly analyze errors or merely apply pattern matching.

**Check 2: Cross-Domain Generalization Study** Evaluate the ABO paradigm on at least 10 diverse tasks spanning different domains (mathematical reasoning, code generation, creative writing, multi-modal tasks) and at least 3 different model families. Measure performance improvements and identify which task characteristics correlate with ABO effectiveness.

**Check 3: Instruction-Following Analysis** Create a comprehensive dataset of prompts with varying instruction complexity and format requirements. Systematically measure target model compliance rates for each instruction type, then correlate these compliance rates with ABO performance gains to validate the instruction-following gap hypothesis.