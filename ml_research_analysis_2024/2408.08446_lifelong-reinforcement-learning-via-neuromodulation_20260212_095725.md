---
ver: rpa2
title: Lifelong Reinforcement Learning via Neuromodulation
arxiv_id: '2408.08446'
source_url: https://arxiv.org/abs/2408.08446
tags:
- learning
- uncertainty
- neuroscience
- framework
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for adaptive reinforcement learning
  algorithms inspired by neuromodulatory systems in the brain. The authors hypothesize
  connections between specific neuromodulators (dopamine, noradrenaline, acetylcholine,
  serotonin) and RL hyperparameters (learning rate, inverse temperature, discount
  factor, TD-error).
---

# Lifelong Reinforcement Learning via Neuromodulation

## Quick Facts
- arXiv ID: 2408.08446
- Source URL: https://arxiv.org/abs/2408.08446
- Authors: Sebastian Lee; Samuel Liebana; Claudia Clopath; Will Dabney
- Reference count: 40
- One-line primary result: Framework connects neuromodulators to RL hyperparameters, showing superior performance on non-stationary multi-armed bandit tasks

## Executive Summary
This paper proposes a framework for adaptive reinforcement learning algorithms inspired by neuromodulatory systems in the brain. The authors hypothesize connections between specific neuromodulators (dopamine, noradrenaline, acetylcholine, serotonin) and RL hyperparameters (learning rate, inverse temperature, discount factor, TD-error). They instantiate this framework with an algorithm called the Doya-DaYu agent, which adapts learning rate and exploration based on estimated expected and unexpected uncertainties. The agent is evaluated on a non-stationary multi-armed bandit task, showing superior performance compared to baseline methods.

## Method Summary
The Doya-DaYu agent uses an ensemble of agents to estimate expected (aleatoric) and unexpected (epistemic) uncertainties in non-stationary environments. Learning rate and inverse temperature are adapted based on these uncertainties using specific functional forms. The framework draws inspiration from neuroscience literature on neuromodulators, mapping dopamine to TD-error, noradrenaline to inverse temperature (exploration), and acetylcholine to learning rate through uncertainty signals. The agent is evaluated on a non-stationary multi-armed bandit task with Gaussian payout distributions that change over time.

## Key Results
- Doya-DaYu agent shows superior performance compared to baseline methods (Discounted-UCB and Boltzmann policies) on non-stationary multi-armed bandit tasks
- Agent successfully models context switches and adapts to changing payout variances
- Proposed framework provides a novel approach to adaptive hyperparameter tuning in reinforcement learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neuromodulatory systems can be mapped to RL hyperparameters to enable adaptive learning rates and exploration strategies
- Mechanism: The framework connects dopamine to TD-error, noradrenaline to inverse temperature (exploration), and acetylcholine to learning rate through uncertainty signals
- Core assumption: The proposed mappings between neuromodulators and RL hyperparameters accurately capture their biological functions
- Evidence anchors:
  - [abstract] "We give a concrete instance of this framework built on literature surrounding the neuromodulators Acetylcholine (ACh) and Noradrenaline (NA)"
  - [section 3.1] "Doya [2002] hypothesises mappings from dopamine, noradrenaline, acetylcholine, and serotonin to TD-error, an exploration parameterβ(inverse temperature), learning rateα, and discount factor γrespectively"
  - [corpus] Weak - no direct corpus evidence for specific neuromodulator-RL hyperparameter mappings
- Break condition: If the biological functions of neuromodulators differ significantly from the assumed mappings, or if the uncertainty estimation methods fail to accurately capture expected/unexpected uncertainty

### Mechanism 2
- Claim: Expected and unexpected uncertainty can be estimated and used to adapt RL hyperparameters
- Mechanism: The framework estimates aleatoric (expected) uncertainty through ensemble variance and epistemic (unexpected) uncertainty through ensemble mean variance, then uses these to modulate learning rate and exploration
- Core assumption: Ensemble methods can effectively estimate the two types of uncertainty in non-stationary environments
- Evidence anchors:
  - [section 3.2] "In a series of papers, Yu and Dayan propose that ACh and NA signal expected and unexpected uncertainty respectively"
  - [section 3.3] "For the expected (aleatoric) uncertainty, we directly estimate the variance of the return distribution... We take the variance over the ensemble of the mean values to be the unexpected (epistemic) uncertainty"
  - [corpus] Weak - no direct corpus evidence for ensemble-based uncertainty estimation in RL
- Break condition: If ensemble methods cannot accurately estimate uncertainties in complex environments, or if the functional forms mapping uncertainties to hyperparameters are suboptimal

### Mechanism 3
- Claim: The Doya-DaYu agent outperforms baseline methods in non-stationary multi-armed bandit tasks
- Mechanism: By adapting learning rate and exploration based on estimated uncertainties, the agent can more effectively handle context switches and changing reward distributions
- Core assumption: The adaptive strategies based on neuromodulation-inspired uncertainty estimation are superior to fixed hyperparameter approaches
- Evidence anchors:
  - [abstract] "The agent is evaluated on a non-stationary multi-armed bandit task, showing superior performance compared to baseline methods"
  - [section 4.1] "Agents must model these context switches and explore to find the new high payout arm, while also navigating changing payout variances"
  - [corpus] Weak - no direct corpus evidence for performance comparisons with neuromodulation-inspired RL agents
- Break condition: If baseline methods with tuned hyperparameters can match or exceed the performance of the adaptive agent, or if the advantages diminish in more complex environments

## Foundational Learning

- Concept: Reinforcement Learning (RL) fundamentals
  - Why needed here: The paper builds on RL concepts like Markov Decision Processes, value functions, and temporal difference learning
  - Quick check question: What is the difference between the value function and the action-value function in RL?

- Concept: Uncertainty estimation in machine learning
  - Why needed here: The framework relies on estimating aleatoric and epistemic uncertainty to adapt hyperparameters
  - Quick check question: How do aleatoric and epistemic uncertainty differ, and why is this distinction important in non-stationary environments?

- Concept: Neuromodulatory systems in neuroscience
  - Why needed here: The framework draws inspiration from the functions of dopamine, noradrenaline, and acetylcholine in the brain
  - Quick check question: What are the proposed roles of dopamine, noradrenaline, and acetylcholine in learning and decision-making?

## Architecture Onboarding

- Component map: Uncertainty estimation module (ensemble-based) -> Hyperparameter adaptation module (functional forms) -> RL agent (tabular Q-learning or deep RL) -> Experiment evaluation module (multi-armed bandit tasks)

- Critical path: Uncertainty estimation → Hyperparameter adaptation → RL agent performance → Experiment evaluation

- Design tradeoffs:
  - Ensemble size vs. computational efficiency in uncertainty estimation
  - Functional form complexity vs. adaptability in hyperparameter adaptation
  - Model-based vs. model-free approaches in the RL agent
  - Task complexity vs. framework scalability

- Failure signatures:
  - Poor performance on non-stationary tasks compared to baseline methods
  - Unstable learning or exploration-exploitation trade-off
  - Inaccurate uncertainty estimates leading to suboptimal hyperparameter adaptations

- First 3 experiments:
  1. Implement the Doya-DaYu agent for a simple non-stationary bandit task and compare performance against a fixed hyperparameter baseline
  2. Test different ensemble sizes and uncertainty estimation methods to find the optimal configuration
  3. Extend the framework to a deep RL agent and evaluate on a more complex non-stationary environment (e.g., the proposed key-door gridworld)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed functional forms for mapping uncertainty estimates to hyperparameters (e.g., learning rate α and inverse temperature β) generalize to more complex RL tasks beyond multi-armed bandits?
- Basis in paper: [explicit] The paper presents specific functional forms (Equation 4) for the tabular bandit setting but acknowledges these may need adaptation for richer environments.
- Why unresolved: The functional forms are derived for a simple bandit task and their effectiveness in environments with continuous state/action spaces, delayed rewards, or complex dynamics is unknown.
- What evidence would resolve it: Empirical evaluation of the framework with various functional forms on standard RL benchmarks (e.g., Atari, MuJoCo) demonstrating improved performance compared to tuned baselines.

### Open Question 2
- Question: Can the neuromodulatory framework inspire novel regularization techniques for continual learning that go beyond standard methods like weight decay or dropout?
- Basis in paper: [inferred] The paper discusses potential connections between neuromodulators like serotonin and regularization, but does not explore this direction empirically.
- Why unresolved: While the paper mentions regularization as a potential application of the framework, it does not propose specific mechanisms or test them in a continual learning setting.
- What evidence would resolve it: Development and empirical validation of a neuromodulation-inspired regularization technique for continual learning, showing improved performance on benchmarks like Permuted MNIST or RL datasets.

### Open Question 3
- Question: How does the proposed closed-loop experimental paradigm (Figure 4) account for the complex interactions between different neuromodulatory systems in the brain?
- Basis in paper: [explicit] The paper acknowledges the limitations of studying individual neuromodulators in isolation and proposes a framework for integrating insights from multiple systems.
- Why unresolved: The paper focuses on individual neuromodulators (dopamine, noradrenaline, acetylcholine) but does not address how their interactions might influence learning and behavior in the proposed experimental paradigm.
- What evidence would resolve it: Experimental results from the proposed paradigm demonstrating the effects of manipulating multiple neuromodulatory systems simultaneously on learning and decision-making in mice.

## Limitations

- The framework's effectiveness is only demonstrated on simple non-stationary bandit tasks, not on more complex RL environments
- The biological plausibility of the exact functional forms mapping uncertainty to hyperparameters is not validated through experiments
- The computational overhead of ensemble-based uncertainty estimation in practical applications is not addressed

## Confidence

- High: The conceptual framework connecting neuromodulators to RL hyperparameters is well-founded in prior neuroscience literature
- Medium: The uncertainty estimation approach using ensemble methods shows theoretical validity but lacks extensive empirical validation
- Low: The claim of superior performance in lifelong learning scenarios, as only simple bandit tasks were evaluated

## Next Checks

1. Evaluate the Doya-DaYu agent on more complex non-stationary environments (e.g., gridworlds with multiple changing contexts) to test scalability
2. Conduct ablation studies to determine which components of the framework are most critical for performance gains
3. Implement the proposed neuroscience experiment to validate whether the framework's predictions about noradrenaline's role in exploration match biological reality