---
ver: rpa2
title: Towards Pedagogical LLMs with Supervised Fine Tuning for Computing Education
arxiv_id: '2411.01765'
source_url: https://arxiv.org/abs/2411.01765
tags:
- education
- computing
- llms
- error
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the risk that large language models (LLMs)
  can harm learning by providing students with solutions, contradicting constructivist
  pedagogy. To mitigate this, the authors fine-tuned ChatGPT 3.5 using 528 high-quality,
  manually filtered question/answer pairs from programming course forums.
---

# Towards Pedagogical LLMs with Supervised Fine Tuning for Computing Education

## Quick Facts
- **arXiv ID**: 2411.01765
- **Source URL**: https://arxiv.org/abs/2411.01765
- **Reference count**: 16
- **Key outcome**: Fine-tuning ChatGPT 3.5 with curated course forum data produces a more Socratic, pedagogically aligned model that prompts thinking rather than giving direct answers

## Executive Summary
This paper addresses the risk that large language models can harm learning by providing students with solutions, contradicting constructivist pedagogy. The authors fine-tuned ChatGPT 3.5 using 528 high-quality, manually filtered question/answer pairs from programming course forums. The fine-tuned model (FT2) adopted a more informal, Socratic tone that prompts students to think rather than simply giving answers, unlike the more instructive GPT-3.5. In evaluations with 50 real student questions, FT2 responses were more concise and pedagogically aligned than GPT-4o's verbose output. This work demonstrates that university course forums can be valuable for fine-tuning datasets and that supervised fine-tuning can improve LLMs' pedagogical alignment in computing education.

## Method Summary
The authors collected 2,500 question/answer pairs from programming course forums, applied automated cleansing to remove URLs and proper nouns while correcting grammatical issues, then manually filtered 500 pairs using inclusion criteria focused on correct, helpful, self-contained answers that suggest rather than solve problems. Five tutors reviewed the pairs to ensure formal but not dismissive tone, code blocks as examples only, and focus on programming understanding rather than specific assignments. The filtered dataset of 528 pairs was used to fine-tune ChatGPT 3.5 Turbo, which was then evaluated against GPT-3.5 and GPT-4o using 50 real student questions to measure pedagogical alignment, conciseness, and correctness.

## Key Results
- FT2 adopted a more informal, Socratic tone that prompts students to think rather than giving direct answers
- FT2 responses were more concise and pedagogically aligned than GPT-4o's verbose output
- The fine-tuned model showed improved pedagogical alignment compared to both GPT-3.5's instructive tone and GPT-4o's lengthy responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning with curated Q/A pairs from course forums improves pedagogical alignment by shifting LLM behavior from instructive to Socratic.
- Mechanism: The fine-tuning process uses high-quality tutor responses that follow constructivist principles (prompting rather than solving), which trains the model to adopt similar behaviors when responding to student questions.
- Core assumption: Tutor responses from course forums accurately represent pedagogically sound practices that align with constructivist learning theory.
- Evidence anchors:
  - [abstract] "The fine-tuned model (FT2) adopted a more informal, Socratic tone that prompts students to think rather than simply giving answers"
  - [section] "Compared to the instructive tone of GPT-3.5, where solutions are plainly stated and sometimes given, FT2 Socratically prompts the student to consider a particular approach"
  - [corpus] Weak evidence - the corpus contains related papers but no direct evidence about forum-based fine-tuning effectiveness
- Break condition: If the forum data contains incorrect or non-constructivist responses, the fine-tuned model will learn harmful behaviors instead of beneficial ones.

### Mechanism 2
- Claim: Manual filtering and quality control of Q/A pairs is essential for successful pedagogical fine-tuning.
- Mechanism: The iterative process of automated cleansing followed by tutor review ensures only pedagogically aligned responses are used for training, preventing the model from learning incorrect or overly instructive behaviors.
- Core assumption: Human expert review can effectively identify and filter responses that align with constructivist pedagogy.
- Evidence anchors:
  - [section] "Manual filtering in Step 3 was therefore motivated: five tutors each reviewed 500 of 2500 randomly assigned Q/A pairs, applying the following inclusion criteria"
  - [abstract] "The project utilised a proprietary dataset of 2,500 high quality question/answer pairs from programming course forums"
  - [corpus] No direct evidence in corpus about manual filtering effectiveness
- Break condition: If the inclusion criteria are too lenient or reviewers have inconsistent standards, the dataset quality will degrade and model performance will suffer.

### Mechanism 3
- Claim: Fine-tuning improves response quality by making outputs more concise and context-appropriate for terminal environments.
- Mechanism: The model learns to produce shorter, more focused responses that convey similar information to larger models but are better suited to the user interface constraints of integrated development environments.
- Core assumption: Response length and style impact usability in terminal-based educational tools.
- Evidence anchors:
  - [abstract] "FT2 responses were more concise and pedagogically aligned than GPT-4o's verbose output"
  - [section] "The FT2 responses are concise, while conveying similar information. Comparatively, GPT-4o responses are verbose: overwhelming the terminal environment"
  - [corpus] No corpus evidence about response length optimization
- Break condition: If students require more detailed explanations for learning, the conciseness could become a barrier rather than a benefit.

## Foundational Learning

- Concept: Constructivist pedagogy
  - Why needed here: The entire approach is built on shifting from instructivist to constructivist LLM behavior, so understanding this educational theory is fundamental
  - Quick check question: What is the key difference between constructivist and instructivist approaches to learning?

- Concept: Supervised fine-tuning process
  - Why needed here: The paper's methodology relies on supervised fine-tuning with curated data, which requires understanding how this differs from other training approaches
  - Quick check question: How does supervised fine-tuning differ from reinforcement learning from human feedback (RLHF)?

- Concept: Dataset curation and quality control
  - Why needed here: The success of the approach depends heavily on selecting appropriate Q/A pairs, requiring understanding of data preparation best practices
  - Quick check question: What are the key risks of using unfiltered forum data for model training?

## Architecture Onboarding

- Component map: Data pipeline (forum extraction → cleansing → manual filtering) → Fine-tuning process using ChatGPT 3.5 Turbo → Deployment layer integrated into DCC Help extension within Debugging C Compiler tool
- Critical path: Data collection → Automated cleansing → Manual filtering → Fine-tuning → Evaluation → Deployment
- Design tradeoffs: Using smaller, specialized models (ChatGPT 3.5) instead of larger proprietary models reduces cost and computational requirements but may limit the breadth of knowledge the model can access.
- Failure signatures: Poor pedagogical alignment (model gives direct answers instead of prompting), grammatical issues in responses, verbose outputs that overwhelm the terminal interface, incorrect technical information.
- First 3 experiments:
  1. Compare FT2 responses to GPT-3.5 and GPT-4o on identical student questions measuring conciseness, pedagogical alignment, and correctness
  2. Test the fine-tuned model with edge cases (highly complex questions, ambiguous questions) to identify failure modes
  3. Evaluate the impact of different filtering criteria thresholds on final model performance by training multiple variants

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fine-tuned model's performance compare to commercial LLMs in terms of actual learning outcomes and student performance?
- Basis in paper: [explicit] The paper mentions that future work will "rigorously measure FT2 quality utilising the methodology presented in [13]" but this evaluation has not been conducted yet.
- Why unresolved: The paper only provides preliminary qualitative comparisons and mentions future plans for rigorous evaluation, but no actual data on learning outcomes or student performance metrics is presented.
- What evidence would resolve it: Empirical studies measuring student learning outcomes, grades, or problem-solving abilities when using the fine-tuned model versus commercial LLMs, with proper control groups and statistical analysis.

### Open Question 2
- Question: What is the optimal balance between conversational tone and pedagogical effectiveness in LLM responses for computing education?
- Basis in paper: [explicit] The paper notes that FT2 adopted a more informal, Socratic tone that "prompts students to think rather than simply giving answers" and compares this to GPT-3.5's instructive tone and GPT-4o's verbose output.
- Why unresolved: While the paper observes differences in tone and length, it doesn't systematically investigate how different conversational styles impact learning effectiveness or student engagement.
- What evidence would resolve it: Controlled experiments testing different conversational styles (formal vs. informal, concise vs. detailed, Socratic vs. instructive) and measuring their impact on student learning, engagement, and problem-solving success rates.

### Open Question 3
- Question: Can the manual filtering process for creating fine-tuning datasets be automated or scaled to larger datasets without sacrificing quality?
- Basis in paper: [inferred] The paper describes a manual filtering process where "five tutors each reviewed 500 of 2500 randomly assigned Q/A pairs" with specific inclusion criteria, which was "time-consuming" but "vital in improving the dataset."
- Why unresolved: The paper acknowledges the effectiveness of manual filtering but also notes its time-consuming nature, suggesting a need for more scalable approaches.
- What evidence would resolve it: Development and validation of automated filtering techniques (using AI or machine learning) that can match or exceed the quality of manual filtering, tested across larger datasets and multiple courses or institutions.

## Limitations

- The proprietary nature of the dataset prevents independent verification and replication of the fine-tuning process and results
- The evaluation lacks systematic usability testing with actual students and only provides qualitative comparisons against baseline models
- The paper does not include rigorous studies measuring actual learning outcomes or student performance improvements when using the fine-tuned model

## Confidence

- **High Confidence**: The mechanism that supervised fine-tuning can improve pedagogical alignment by shifting from instructive to Socratic responses is well-supported by the empirical results showing FT2's improved performance over baseline models.
- **Medium Confidence**: The claim that manual filtering is essential for success is supported by the methodology but lacks comparative evidence showing what happens with less rigorous filtering approaches.
- **Medium Confidence**: The assertion that conciseness improves usability in terminal environments is based on qualitative observations rather than systematic usability testing with actual students.

## Next Checks

1. **Dataset Transparency Test**: Replicate the fine-tuning process using publicly available programming forum data with the same inclusion criteria to verify that the approach works independently of the proprietary dataset.
2. **Pedagogical Impact Study**: Conduct a controlled study with actual computing students comparing learning outcomes when using FT2 versus standard LLMs to determine if the pedagogical improvements translate to better learning gains.
3. **Generalization Assessment**: Test the fine-tuned model across different programming domains and educational contexts (e.g., web development, data science, algorithms) to evaluate whether the pedagogical alignment generalizes beyond the original course forums.