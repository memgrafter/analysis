---
ver: rpa2
title: Curricula for Learning Robust Policies with Factored State Representations
  in Changing Environments
arxiv_id: '2409.09169'
source_url: https://arxiv.org/abs/2409.09169
tags:
- learning
- shifting
- state
- curriculum
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores how curriculum design affects policy robustness\
  \ in reinforcement learning using factored state representations. The authors propose\
  \ that curricula focused on variables with high regret\u2014those causing the largest\
  \ performance drops when altered\u2014can significantly improve policy robustness\
  \ in non-stationary environments."
---

# Curricula for Learning Robust Policies with Factored State Representations in Changing Environments

## Quick Facts
- arXiv ID: 2409.09169
- Source URL: https://arxiv.org/abs/2409.09169
- Reference count: 33
- Key outcome: Curricula targeting high-regret variables with factored representations produce robust policies in non-stationary environments

## Executive Summary
This paper investigates how curriculum design affects policy robustness when using factored state representations in reinforcement learning. The authors propose that curricula focused on variables causing the largest performance drops (high regret) can significantly improve policy robustness in changing environments. Through experiments on a "Shifting Frozen Lake" environment, they demonstrate that while simple curricula are insufficient without factored representations, using factored representations enables domain randomization and shuffling diverse examples to produce robust policies. The findings highlight the importance of both curriculum design and factored representations for building adaptive, robust policies in dynamic environments.

## Method Summary
The authors evaluate different curriculum strategies for learning robust policies in a non-stationary "Shifting Frozen Lake" environment. They compare PPO and PPO-F (factored representation variant) agents across various curriculum types: random shifting, domain randomization, stored examples, and shuffling diverse examples. The key innovation is the use of factored state representations that decompose the environment into distinct variables (grid size, hole locations, goal location, agent location, distance matrix), enabling more efficient learning and better generalization. They measure policy robustness by evaluating performance across shifted environments and identify high-regret variables that cause the largest performance drops when altered.

## Key Results
- Without factored representations, simple curricula are insufficient for robust policy learning
- Using factored representations, domain randomization and shuffling diverse examples enable robust policies
- Curricula targeting high-regret variables (shifting goal, hole, or start locations) produce highly robust policies
- Shifting grid size causes low regret and does not significantly impact policy performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factored representations reduce the effective dimensionality of the state space, enabling faster learning and better generalization.
- Mechanism: By decomposing the state into distinct, low-dimensional variables, the agent learns to reason about each component independently rather than modeling the entire high-dimensional state space.
- Core assumption: The components are sufficiently independent that learning policies for each separately transfers well to novel combinations.
- Evidence anchors:
  - [abstract] "Factored representations, which break down complex state and action spaces into distinct components, can improve generalization and sample efficiency in policy learning."
  - [section 2] "Factored representations can decompose the state and action spaces into sets of variables, each representing different components of the environment."
  - [corpus] Weak - no direct evidence found in related papers
- Break condition: If components are highly correlated or dependent, the factored representation loses its efficiency advantage.

### Mechanism 2
- Claim: Curriculum learning improves policy robustness by progressively exposing the agent to more complex environments.
- Mechanism: Starting with simpler or static environments and gradually introducing variability allows the agent to build foundational skills before tackling full generalization.
- Core assumption: Learning is more efficient when complex tasks are broken down into simpler subtasks that build upon each other.
- Evidence anchors:
  - [abstract] "we explore how the curriculum of an agent using a factored state representation affects the robustness of the learned policy"
  - [section 1] "Curriculum learning...involves training an agent on a sequence of different tasks, enabling it to leverage the knowledge gained from simpler tasks to tackle more challenging ones."
  - [corpus] Weak - related papers mention curriculum learning but don't provide direct evidence for this specific mechanism
- Break condition: If curriculum progression is poorly designed, it may confuse the agent or lead to suboptimal policies.

### Mechanism 3
- Claim: Identifying and adjusting high-regret variables during training creates more robust policies.
- Mechanism: Variables that cause the largest performance drops when altered (high regret) are critical for policy robustness. By focusing curriculum on these variables, the agent learns to handle the most challenging environmental changes.
- Core assumption: Variables causing high regret when shifted alone are the most important for generalization across all possible shifts.
- Evidence anchors:
  - [abstract] "curricula targeting high-regret variables—such as shifting goal or hole locations—produce highly robust policies"
  - [section 5] "we find that shifting the goal location, start location, and hole locations leads to high regret while shifting the grid size does not"
  - [corpus] Weak - no direct evidence found in related papers
- Break condition: If regret measurement is inaccurate or if multiple variables interact in complex ways, focusing on single high-regret variables may miss important generalization challenges.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The entire framework of reinforcement learning and factored MDPs is built on MDP theory.
  - Quick check question: What are the five components of an MDP tuple (S, A, P, R, γ)?

- Concept: Dynamic Bayesian Networks
  - Why needed here: Factored representations use DBNs to model conditional dependencies between state variables over time.
  - Quick check question: How does a DBN represent the joint probability distribution over state variables at time t?

- Concept: Distribution shifts in reinforcement learning
  - Why needed here: The paper's core contribution is about learning policies that generalize across distribution shifts.
  - Quick check question: What are the three types of learning environments based on how training and testing distributions relate?

## Architecture Onboarding

- Component map:
  Environment -> Agent (PPO/PPO-F with factored representation) -> Curriculum manager -> Regret analyzer -> Training loop

- Critical path:
  1. Initialize environment with sampled variables
  2. Agent interacts and learns policy using factored representation
  3. Curriculum manager applies variable shifts based on strategy
  4. Regret analyzer evaluates policy performance on shifted environments
  5. Policy updates continue until convergence or curriculum completion

- Design tradeoffs:
  - Factored representation complexity vs. learning efficiency
  - Curriculum complexity vs. training time
  - Regret measurement accuracy vs. computational cost
  - Single-variable vs. multi-variable shifting strategies

- Failure signatures:
  - Stuck behavior (repeating left/right or up/down movements)
  - Failure to reach goal in any episode
  - High variance in performance across different grid configurations
  - Rapid performance degradation when shifting begins

- First 3 experiments:
  1. Test PPO vs PPO-F on single static example to verify factored representation advantage
  2. Apply random shifting from start to measure baseline generalization capability
  3. Implement stored examples curriculum to test minimum diversity needed for robust policies

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of empirical evidence directly supporting the three proposed mechanisms, particularly for high-regret variable identification
- Core assumptions about single-variable regret measurement may not hold when multiple variables interact
- Limited external validation of factored representation benefits beyond the source paper

## Confidence
- Mechanism 1 (Factored representations reduce dimensionality): Medium confidence
- Mechanism 2 (Curriculum learning improves robustness): Medium confidence
- Mechanism 3 (High-regret variable targeting): Low confidence

## Next Checks
1. Conduct a literature review focused specifically on empirical studies of factored representations in reinforcement learning to verify the claimed dimensionality reduction benefits and their impact on generalization.

2. Design controlled experiments to test whether high-regret variables identified in isolation remain high-regret when multiple variables shift simultaneously, addressing the core assumption about single-variable regret measurement.

3. Systematically vary curriculum design parameters (progression rate, variable selection criteria, diversity of examples) to quantify their individual contributions to policy robustness and identify optimal configurations.