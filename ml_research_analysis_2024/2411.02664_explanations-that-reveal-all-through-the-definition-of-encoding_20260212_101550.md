---
ver: rpa2
title: Explanations that reveal all through the definition of encoding
arxiv_id: '2411.02664'
source_url: https://arxiv.org/abs/2411.02664
tags:
- explanation
- encoding
- explanations
- eval-x
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of detecting \"encoding\" in\
  \ feature attribution explanations\u2014where explanations appear to predict labels\
  \ well but actually rely on irrelevant inputs selected through non-transparent mechanisms.\
  \ The authors introduce a formal statistical definition of encoding based on conditional\
  \ dependence, showing that encoding occurs when the explanation's selection carries\
  \ additional label information beyond the explanation's values."
---

# Explanations that reveal all through the definition of encoding

## Quick Facts
- arXiv ID: 2411.02664
- Source URL: https://arxiv.org/abs/2411.02664
- Authors: Aahlad Puli; Nhi Nguyen; Rajesh Ranganath
- Reference count: 40
- Key outcome: This paper introduces STRIPE-X, a new evaluation method that strongly detects "encoding" in feature attribution explanations by combining prediction quality with an encoding penalty term based on conditional dependence.

## Executive Summary
This paper addresses the challenge of detecting "encoding" in feature attribution explanations—where explanations appear to predict labels well but actually rely on irrelevant inputs selected through non-transparent mechanisms. The authors introduce a formal statistical definition of encoding based on conditional dependence, showing that encoding occurs when the explanation's selection carries additional label information beyond the explanation's values. They prove that existing evaluation methods (ROAR, FRESH, EVAL-X) either fail to detect encoding or only weakly detect it. To address this, they develop STRIPE-X, a new evaluation method that strongly detects encoding by combining EVAL-X with an encoding penalty term (ENCODE-METER) based on instantaneous conditional mutual information. Experiments demonstrate that STRIPE-X successfully identifies encoding in both simulated and real-world settings, including revealing encoding in LLM-generated explanations despite explicit instructions to avoid it.

## Method Summary
The paper defines encoding through conditional dependence—specifically that the label y is not independent of the explanation selection indicator Ev given the explanation values xv. This captures the extra predictive power that comes from knowing which inputs were selected, not just their values. STRIPE-X combines an EVAL-X component that measures prediction quality from explanation values with an ENCODE-METER component that measures conditional dependence between the label and selection indicator. The method subtracts α times the encoding cost from the EVAL-X score, pushing encoding explanations below non-encoding ones regardless of their prediction quality. The algorithm requires estimating both a surrogate model for prediction (EVAL-X) and the conditional mutual information between label and selection (ENCODE-METER).

## Key Results
- STRIPE-X successfully detects encoding in simulated binary datasets where control flow variables determine which feature predicts the label
- Existing methods (ROAR, FRESH, EVAL-X) fail to detect encoding or only weakly detect it compared to STRIPE-X
- STRIPE-X reveals encoding in LLM-generated explanations despite explicit instructions to include all words the model based its selection on
- The method achieves this by pricing out encoding explanations below the negative marginal entropy threshold

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoding occurs when the selection indicator Ev carries additional label information beyond the explanation's values xv
- Mechanism: The paper formalizes encoding through conditional dependence - specifically that y is not independent of Ev given xv. This captures the extra predictive power that comes from knowing which inputs were selected, not just their values.
- Core assumption: The selection mechanism Ev is not fully determined by the values xv alone
- Evidence anchors:
  - [abstract] "encoding occurs when the explanation's selection carries additional label information beyond the explanation's values"
  - [section] "The dependence in Def: Encoding means that for encoding explanations, there is a disconnect between how well the explanation xe(x) predicts the label versus only the explanation's valuesxv"
  - [corpus] Weak - no direct discussion of this specific mechanism

### Mechanism 2
- Claim: STRIPE-X successfully detects encoding by combining prediction quality with encoding penalty
- Mechanism: STRIPE-X subtracts α times the encoding cost (ENCODE-METER) from the EVAL-X score. For large enough α, this pushes encoding explanations below non-encoding ones regardless of their prediction quality.
- Core assumption: There exists a finite threshold α that sufficiently penalizes encoding explanations
- Evidence anchors:
  - [abstract] "STRIPE-X, a new evaluation method that strongly detects encoding by combining EVAL-X with an encoding penalty term"
  - [section] "For a large enough α, the STRIPE-X scores for any encoding explanations will be dominated by the information term, and thus will become smaller than any non-encoding explanation"
  - [corpus] Weak - no direct discussion of this specific detection mechanism

### Mechanism 3
- Claim: Non-encoding explanations have a "what you see is what you get" property
- Mechanism: When an explanation is non-encoding, the label y is independent of Ev given xv. This means all predictive information about y lives in the values xv themselves, making the explanation transparent.
- Core assumption: The independence condition y ⊥ Ev | xv holds for non-encoding explanations
- Evidence anchors:
  - [abstract] "non-encoding explanations contain all the informative inputs used to produce the explanation, giving them a 'what you see is what you get' property"
  - [section] "With non-encoding explanations, there exists no positive measure set of explanationsxe(x), where the explanation indicator has conditional dependence given the explanation's values"
  - [corpus] Weak - no direct discussion of this specific transparency mechanism

## Foundational Learning

- Concept: Conditional dependence and independence
  - Why needed here: The entire encoding detection framework relies on conditional independence tests (y ⊥ Ev | xv) to identify encoding
  - Quick check question: Given two random variables A and B, and conditioning variable C, what does A ⊥ B | C mean in terms of probability distributions?

- Concept: Information theory (mutual information, KL divergence)
  - Why needed here: ENCODE-METER uses instantaneous conditional mutual information, and the paper uses KL divergence to prove detection properties
  - Quick check question: If X and Y are independent given Z, what is the value of I(X;Y|Z)?

- Concept: Feature attribution and explanation methods
  - Why needed here: The paper evaluates different explanation methods (ROAR, FRESH, EVAL-X) and develops STRIPE-X for this domain
  - Quick check question: What is the difference between an explanation method that selects features vs one that provides importance scores?

## Architecture Onboarding

- Component map: Data generation (q(y,x)) -> Explanation method e(x) → produces xe(x) = (e(x), xv) -> EVAL-X component: predicts y from xv using a surrogate model -> ENCODE-METER component: measures conditional dependence between Ev and y given xv -> STRIPE-X: combines EVAL-X and ENCODE-METER with penalty weight α

- Critical path: Data → Explanation method → STRIPE-X evaluation → Detection result
- Design tradeoffs:
  - Model complexity vs estimation accuracy for EVAL-X and ENCODE-METER
  - Penalty weight α vs sensitivity to encoding
  - Generative vs predictive estimation methods for STRIPE-X
- Failure signatures:
  - Misestimation in EVAL-X model leads to incorrect scores
  - ENCODE-METER estimation fails when data is scarce
  - Penalty weight α set too low → encoding not detected
  - Penalty weight α set too high → non-encoding explanations penalized
- First 3 experiments:
  1. Run STRIPE-X on synthetic encoding example (discrete DGP) to verify detection
  2. Compare STRIPE-X vs EVAL-X on a simple image classification task
  3. Test STRIPE-X on LLM-generated explanations for sentiment analysis task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we extend the encoding definition to detect label leakage, where explanations have access to both inputs and observed labels?
- Basis in paper: [explicit] The paper mentions that label leakage is similar to encoding in that additional information is in the explanation, but focuses on explanations that have access to both inputs and observed labels. They leave extending Def: Encoding to leakage to the future.
- Why unresolved: The current encoding definition relies on conditional dependence properties that assume the explanation only has access to inputs, not labels. Extending it to cases where explanations can directly access labels requires new theoretical framework.
- What evidence would resolve it: A formal extension of Def: Encoding that accounts for label access, along with experimental validation showing it can distinguish between encoding and leakage in controlled settings.

### Open Question 2
- Question: What techniques can effectively prevent encoding in LLM-generated explanations beyond prompt engineering?
- Basis in paper: [explicit] The paper shows that despite instructing Llama 3 to include all words it based its selection on, the LLM still produced encoding explanations. They suggest building non-encoding explanations with LLMs may require extensive search over prompts or finetuning guided by STRIPE-X scores.
- Why unresolved: Prompt engineering alone proved insufficient, and the paper only suggests but doesn't implement finetuning approaches. The complexity of LLM internal reasoning mechanisms makes encoding difficult to prevent through surface-level interventions.
- What evidence would resolve it: Comparative experiments showing effectiveness of different finetuning strategies (e.g., reinforcement learning with STRIPE-X rewards) versus prompt engineering in producing non-encoding LLM explanations across multiple tasks.

### Open Question 3
- Question: How can we formally define and evaluate faithfulness for explanations beyond encoding detection?
- Basis in paper: [inferred] The paper notes that faithfulness asks explanations reflect how a label is predicted from inputs, but formalization doesn't exist. Jacovi and Goldberg [38] are cited as noting the need for formal faithfulness definition.
- Why unresolved: Faithfulness requires understanding the internal prediction mechanism, which is distinct from encoding (which only considers information transmission). The paper focuses on detecting encoding but acknowledges faithfulness remains an open challenge.
- What evidence would resolve it: A formal definition of faithfulness that captures when explanations accurately reflect model reasoning processes, validated through experiments showing it correlates with human understanding of model behavior.

### Open Question 4
- Question: How can we extend encoding detection to free-text rationales where explanations output generated text rather than input subsets?
- Basis in paper: [explicit] The paper mentions that explanations outputting subsets may not always help humans interpret prediction mechanisms, and suggests extending weak/strong detector definitions to evaluations of free-text rationales as an important direction.
- Why unresolved: Current encoding definition and STRIPE-X are designed for subset-based explanations. Free-text rationales introduce new challenges as encoding can occur through text generation itself, not just selection.
- What evidence would resolve it: Modified version of STRIPE-X or new evaluation metric that can detect encoding in free-text explanations, validated on datasets where both encoding and non-encoding text rationales are generated and compared.

## Limitations

- ENCODE-METER estimation may fail when data is scarce, potentially limiting STRIPE-X's reliability in low-data regimes
- The choice of penalty weight α appears critical but is not fully explored across different data regimes and explanation types
- Current encoding definition and STRIPE-X are designed for subset-based explanations and may not directly extend to free-text rationales

## Confidence

- **High Confidence**: The formal definition of encoding and its theoretical properties are well-established through rigorous proofs. The claim that existing methods (ROAR, FRESH, EVAL-X) fail to detect encoding is strongly supported by both theory and experiments.
- **Medium Confidence**: The STRIPE-X algorithm's ability to detect encoding is well-demonstrated in synthetic and controlled settings, but its performance on diverse real-world explanation methods requires further validation.
- **Low Confidence**: The practical implications of "what you see is what you get" transparency for non-encoding explanations are theoretically sound but may be difficult to verify in practice due to estimation challenges.

## Next Checks

1. **Estimation Accuracy Validation**: Systematically evaluate ENCODE-METER's estimation accuracy across varying sample sizes and data complexities to establish practical bounds on STRIPE-X's reliability.

2. **Cross-Method Generalization**: Test STRIPE-X on a broader range of explanation methods beyond those used in the paper, including gradient-based, perturbation-based, and surrogate model approaches, to verify consistent encoding detection.

3. **α Parameter Sensitivity**: Conduct a comprehensive sensitivity analysis of the penalty weight α across different domains and explanation types to determine robust default settings and identify scenarios where manual tuning is necessary.