---
ver: rpa2
title: 'Open Grounded Planning: Challenges and Benchmark Construction'
arxiv_id: '2406.02903'
source_url: https://arxiv.org/abs/2406.02903
tags:
- task
- plan
- steps
- planning
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "open grounded planning," a new task that
  requires AI models to generate executable plans grounded in open, variable action
  sets across diverse domains. To enable evaluation, the authors construct a benchmark
  spanning everyday life, tool use, and robot control scenarios, totaling over 76,000
  tasks with extensive action libraries.
---

# Open Grounded Planning: Challenges and Benchmark Construction

## Quick Facts
- arXiv ID: 2406.02903
- Source URL: https://arxiv.org/abs/2406.02903
- Reference count: 40
- Primary result: Open grounded planning is a novel task requiring AI to generate executable plans using open, variable action sets across diverse domains; current models struggle despite large benchmarks.

## Executive Summary
This paper introduces "open grounded planning," a new task requiring AI models to generate executable plans grounded in open, variable action sets across diverse domains. To enable evaluation, the authors construct a benchmark spanning everyday life, tool use, and robot control scenarios, totaling over 76,000 tasks with extensive action libraries. Five methods are evaluated, including retrieval-based and inference-based approaches, alongside a novel Retrieve and Rewrite framework. Results show that even state-of-the-art models like GPT-3.5 and Vicuna-7B struggle with this task, with fine-tuned models showing improved performance but still facing significant challenges in generalization and plan quality. The Retrieve and Rewrite method outperforms simpler approaches, highlighting the complexity of open grounded planning and the need for further research.

## Method Summary
The authors construct a large-scale benchmark for open grounded planning across three domains: everyday life, tool use, and robot control. The benchmark includes over 76,000 tasks with corresponding action libraries. Five methods are evaluated: retrieval-based, inference-based, and a novel Retrieve and Rewrite framework that combines both approaches. The evaluation focuses on plan validity and executability metrics to assess model performance.

## Key Results
- Even state-of-the-art models like GPT-3.5 and Vicuna-7B struggle with open grounded planning tasks
- Fine-tuned models show improved performance but still face significant challenges in generalization
- The Retrieve and Rewrite framework outperforms simpler retrieval or inference-only approaches

## Why This Works (Mechanism)
The paper demonstrates that open grounded planning is fundamentally challenging because it requires models to dynamically adapt to variable action sets rather than relying on fixed, known action spaces. The Retrieve and Rewrite framework works better by combining the strengths of both retrieval (accessing relevant actions) and inference (generating novel combinations), allowing more flexible plan generation across diverse domains.

## Foundational Learning

1. **Open Grounded Planning**: AI task requiring plans to be executable using variable, domain-specific action sets rather than fixed vocabularies.
   - Why needed: Traditional planning assumes fixed action spaces; this task reflects real-world variability
   - Quick check: Can the model generate valid plans when given entirely new action sets?

2. **Action Library Construction**: Process of creating domain-specific, executable action sets for each planning task.
   - Why needed: Without proper action grounding, plans cannot be executed in real environments
   - Quick check: Are all actions in the library executable and contextually appropriate?

3. **Plan Validity vs Executability**: Two distinct evaluation metrics measuring whether plans are logically sound and can be performed in practice.
   - Why needed: A plan can be valid logically but still impossible to execute due to missing or inappropriate actions
   - Quick check: Does the plan pass both validity and executability criteria?

## Architecture Onboarding

**Component Map**: Task Specification -> Action Library Retrieval -> Plan Generation -> Validity/Executability Evaluation

**Critical Path**: The core challenge is matching task requirements with appropriate actions from variable libraries. The Retrieve and Rewrite method's critical path involves: (1) retrieving relevant actions, (2) rewriting or combining them into coherent plans, (3) validating plan structure and executability.

**Design Tradeoffs**: The paper balances between retrieval (relying on existing action libraries) and inference (generating novel action combinations). Retrieval ensures action executability but may miss optimal solutions; inference allows creativity but risks generating invalid or non-executable actions.

**Failure Signatures**: Common failures include: (1) selecting inappropriate actions from libraries, (2) failing to chain actions coherently, (3) generating plans that are valid logically but non-executable in practice, (4) poor generalization to unseen action sets.

**3 First Experiments**:
1. Test plan generation with held-out action libraries to assess true generalization
2. Compare plan quality when using complete vs incomplete action libraries
3. Evaluate human vs automated assessment of plan executability

## Open Questions the Paper Calls Out
None

## Limitations
- The quality and real-world executability of constructed action libraries are not thoroughly validated
- The benchmark may contain biases toward more common scenarios despite broad domain coverage
- Evaluation focuses on plan validity and executability but not optimality or efficiency
- Performance gaps may be influenced by prompting strategies rather than inherent model capabilities

## Confidence

- **High**: Novelty of task definition and scale of benchmark construction
- **Medium**: Comparative method evaluations due to potential variability in prompting
- **Low to Medium**: Conclusions about model limitations given sensitivity to benchmark design choices

## Next Checks

1. Conduct human evaluation of plan quality and executability across domains to validate automated metrics
2. Test models on held-out action libraries to assess true generalization to unseen actions
3. Evaluate plan efficiency and optimality alongside validity to provide a more complete performance picture