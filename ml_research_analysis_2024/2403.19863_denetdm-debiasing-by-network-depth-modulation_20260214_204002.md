---
ver: rpa2
title: 'DeNetDM: Debiasing by Network Depth Modulation'
arxiv_id: '2403.19863'
source_url: https://arxiv.org/abs/2403.19863
tags:
- bias
- denetdm
- branch
- training
- shallow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeNetDM addresses the problem of neural network bias by leveraging
  depth modulation. The method is based on the observation that shallow networks prioritize
  core attributes while deeper networks emphasize biases.
---

# DeNetDM: Debiasing by Network Depth Modulation

## Quick Facts
- **arXiv ID**: 2403.19863
- **Source URL**: https://arxiv.org/abs/2403.19863
- **Reference count**: 40
- **Primary result**: Debiasing technique using depth modulation achieves 5% improvement over existing methods without bias annotations

## Executive Summary
DeNetDM addresses neural network bias by leveraging the observation that shallow networks prioritize core attributes while deeper networks emphasize biases. The method employs a Product of Experts framework with deep and shallow branches to create biased and debiased models, then distills knowledge to produce the target debiased model. This approach requires no bias annotations or explicit data augmentation, making it practical for real-world applications where bias sources are unknown or unlabeled.

## Method Summary
DeNetDM operates on the principle that network depth correlates with the type of features learned, with shallower layers capturing core attributes and deeper layers capturing biased correlations. The method constructs two parallel networks - a shallow branch for debiased learning and a deep branch that captures biases. These branches are combined using a Product of Experts framework to generate both biased and debiased predictions. Knowledge distillation is then applied to transfer the debiased knowledge from the shallow branch to the final model, while maintaining performance on core tasks. The approach eliminates the need for bias-specific annotations or data augmentation strategies.

## Key Results
- Achieves 5% performance improvement over existing debiasing techniques
- Effective across multiple synthetic and real-world datasets (Colored MNIST, Corrupted CIFAR-10, Biased FFHQ, BAR)
- Successfully identifies and mitigates spurious correlations without requiring bias labels or bias type specification
- Outperforms methods requiring explicit bias annotations or augmentation strategies

## Why This Works (Mechanism)
The method exploits the fundamental property that different network depths learn different types of features. Shallow layers tend to learn task-relevant features essential for classification, while deeper layers can capture spurious correlations between background elements and labels. By explicitly separating these learning processes and combining them through a Product of Experts framework, DeNetDM can isolate and suppress the biased information while preserving the core discriminative features. The knowledge distillation step ensures that the final model retains the debiased representation while maintaining task performance.

## Foundational Learning
- **Network depth feature learning**: Different network depths capture different types of features; shallow layers learn core attributes while deep layers learn biased correlations. Needed to understand why depth modulation works as a debiasing strategy.
- **Product of Experts framework**: Combines multiple probabilistic models to create a unified prediction. Quick check: verify that the PoE properly balances contributions from shallow and deep branches.
- **Knowledge distillation**: Transfers knowledge from one model to another, typically from a larger to smaller model. Quick check: ensure distillation loss effectively transfers debiased representations.

## Architecture Onboarding

**Component map**: Input -> Shallow Branch -> Debiased Output -> Knowledge Distillation -> Final Model
                    -> Deep Branch -> Biased Output -> Product of Experts -> Combined Output

**Critical path**: Input → Shallow Branch → Product of Experts → Knowledge Distillation → Final Model

**Design tradeoffs**: The method requires architectural modifications (separate shallow and deep branches) which may limit applicability to pre-trained models. However, this design eliminates the need for bias annotations and enables automatic bias detection.

**Failure signatures**: If the shallow and deep branches learn similar biased representations, the method will fail to debias effectively. This can occur when biases are present in low-level features or when the dataset bias is too subtle for depth-based separation.

**3 first experiments**:
1. Validate that shallow networks learn less biased representations than deep networks on a simple biased dataset
2. Test the Product of Experts combination with varying weightings to find optimal balance
3. Evaluate knowledge distillation effectiveness by comparing debiased representations before and after distillation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Reliance on architectural modifications may limit applicability to pre-trained models
- Performance improvements may vary across different bias types and dataset characteristics
- Scalability to extremely deep networks or complex architectures like transformers remains untested

## Confidence
- **High**: Effectiveness of depth modulation as a debiasing mechanism
- **Medium**: Generalizability across different bias types and datasets
- **Low**: Performance in real-world deployment scenarios with unknown or mixed bias sources

## Next Checks
1. Test DeNetDM on datasets with multiple simultaneous bias sources to evaluate performance in more realistic scenarios
2. Compare the computational overhead and parameter efficiency of DeNetDM against existing debiasing methods
3. Evaluate the method's robustness when the bias is not linearly separable by depth (i.e., when shallow and deep networks show similar bias patterns)