---
ver: rpa2
title: 'Quantum-Cognitive Neural Networks: Assessing Confidence and Uncertainty with
  Human Decision-Making Simulations'
arxiv_id: '2412.08010'
source_url: https://arxiv.org/abs/2412.08010
tags:
- neural
- quantum
- qt-nn
- network
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the quantum-tunnelling neural network (QT-NN)
  model, which combines quantum tunnelling with quantum cognition theory to mimic
  human-like perception and judgment. The QT-NN uses quantum tunnelling as an activation
  function, enabling probabilistic processing and faster training compared to classical
  models.
---

# Quantum-Cognitive Neural Networks: Assessing Confidence and Uncertainty with Human Decision-Making Simulations

## Quick Facts
- arXiv ID: 2412.08010
- Source URL: https://arxiv.org/abs/2412.08010
- Authors: Milan Maksimovic; Ivan S. Maksymov
- Reference count: 40
- Key outcome: Quantum-tunnelling neural network achieves up to 50x faster training while producing human-like classifications on Fashion MNIST

## Executive Summary
This paper introduces the quantum-tunnelling neural network (QT-NN), which combines quantum tunnelling physics with quantum cognition theory to create AI systems that better mimic human perception and judgment. The model uses quantum tunnelling probability as an activation function, enabling probabilistic processing of ambiguous inputs rather than deterministic classification. Tested on the Fashion MNIST dataset, QT-NN achieved high classification accuracy while producing results more aligned with human cognitive patterns, particularly in handling uncertain or ambiguous cases. The model demonstrated up to 50 times faster training compared to classical approaches and provided nuanced classifications that better reflect human decision-making uncertainty.

## Method Summary
The QT-NN model uses quantum tunnelling as an activation function, where neuron activation probability follows the algebraic expressions for electron transmission through potential barriers. The architecture consists of an input layer (28×28 nodes), hidden layer (800 nodes with quantum tunnelling activation), and output layer (10 nodes with softmax). Weights are initialized randomly in [-1, 1] and updated via backpropagation using the quantum tunnelling derivative. The model is trained on Fashion MNIST using 32 batches of 100 epochs each and compared against a classical neural network with ReLU activation. Results are analyzed using Jensen-Shannon divergence and Shannon entropy to quantify uncertainty and compare output distributions between models.

## Key Results
- Achieved up to 50 times faster training compared to classical neural networks
- Produced classifications more aligned with human cognition, particularly for ambiguous inputs
- Statistical analyses (JSD and Shannon entropy) confirmed superior handling of uncertainty and ambiguity compared to traditional ML algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantum tunnelling activation enables probabilistic processing that better mimics human perception of ambiguous inputs
- Mechanism: The QT-NN uses quantum tunnelling probability as an activation function, allowing neurons to "tunnel" through decision barriers rather than requiring deterministic activation thresholds. This creates a probabilistic decision pathway that can represent multiple interpretations simultaneously.
- Core assumption: Human cognitive processing of ambiguous stimuli involves quantum-like superposition of interpretations before collapsing to a decision.
- Evidence anchors:
  - [abstract] "The QT-NN uses quantum tunnelling as an activation function, enabling probabilistic processing and faster training compared to classical models."
  - [section] "The activation function ϕQT for the nodes in the hidden layers is defined using the algebraic expressions for the transmission coefficient T of an electron penetrating a potential barrier."
  - [corpus] No direct evidence found - corpus neighbors discuss uncertainty estimation but not quantum tunnelling specifically.
- Break condition: If the quantum tunnelling model doesn't actually capture human-like ambiguity handling better than other probabilistic approaches, or if the computational overhead outweighs the benefits.

### Mechanism 2
- Claim: QT-NN achieves up to 50x faster training by maintaining probability distributions across weights rather than converging to fixed values
- Mechanism: Instead of adjusting weights to specific values during training, QT-NN maintains probabilistic weight distributions that represent the full range of possible solutions. This eliminates the need for extensive weight convergence.
- Core assumption: The optimal solution for classification tasks can be represented as a probability distribution rather than a single point in weight space.
- Evidence anchors:
  - [abstract] "The model demonstrated up to 50 times faster training and provided nuanced classifications that better reflect human decision-making."
  - [section] "The QT-NN model differs fundamentally in its training approach... Rather than adjusting weights to fixed values, it aims to effectively use the entire range of possible weights represented by probability distributions."
  - [corpus] Weak evidence - corpus neighbors discuss calibration and uncertainty but don't specifically address training speed improvements from quantum tunnelling.
- Break condition: If the probabilistic weight approach actually degrades classification accuracy or if the claimed speed improvement is not reproducible across different datasets.

### Mechanism 3
- Claim: Jensen-Shannon divergence analysis shows QT-NN produces more nuanced, human-like classifications by better handling ambiguity
- Mechanism: Lower JSD between QT-NN and classical model outputs indicates that QT-NN captures similar patterns of uncertainty as human perception, particularly for ambiguous categories like "Pullover" vs "Shirt".
- Core assumption: Human-like classification should show similar patterns of uncertainty as measured by statistical divergence metrics.
- Evidence anchors:
  - [section] "The analysis of JSD and SE figures-of-merit provides insights into the prediction similarities and uncertainty levels between the QT-NN and the classical model."
  - [section] "A lower JSD score, closer to 0, indicates high alignment between the probability distributions of the models, as seen in the 'Trouser' category (JSD = 0.049)."
  - [corpus] No direct evidence - corpus neighbors discuss calibration and uncertainty quantification but not specifically JSD analysis of QT-NN outputs.
- Break condition: If JSD analysis doesn't actually correlate with human perception quality, or if the metric is being misapplied to validate the model.

## Foundational Learning

- Concept: Quantum tunnelling and transmission probability
  - Why needed here: The activation function is based on quantum tunnelling physics, specifically the transmission coefficient through potential barriers.
  - Quick check question: Can you explain how the transmission probability T varies with barrier height and particle energy in the one-dimensional potential barrier problem?

- Concept: Jensen-Shannon divergence and Shannon entropy
  - Why needed here: These metrics are used to quantify uncertainty and compare output distributions between QT-NN and classical models.
  - Quick check question: Given two probability distributions p and q, can you compute the JSD and explain what values close to 0 vs close to 1 mean for model comparison?

- Concept: Quantum cognition theory principles
  - Why needed here: The model is explicitly based on quantum cognition theory, which suggests human decision-making involves quantum-like superposition and contextuality.
  - Quick check question: How does quantum cognition theory explain the Necker cube illusion, and why would this be relevant to neural network design?

## Architecture Onboarding

- Component map: Input layer (28×28 nodes) → Hidden layer (800 nodes with quantum tunnelling activation) → Output layer (10 nodes with softmax)
- Critical path: Data preprocessing → Forward pass with QT activation → Error computation → Backward pass with QT derivative → Weight update
- Design tradeoffs: Quantum tunnelling activation provides probabilistic processing and faster training but adds mathematical complexity compared to ReLU. The probabilistic weight approach avoids overfitting but may reduce precision in some cases.
- Failure signatures: Overconfident predictions (low entropy but wrong classification), training instability if barrier parameters are poorly tuned, or failure to converge if the quantum tunnelling model doesn't match the data distribution.
- First 3 experiments:
  1. Compare QT-NN vs classical network on Fashion MNIST with identical architecture except activation function; measure training time and accuracy.
  2. Vary the barrier height parameter in the QT activation and measure its effect on confidence scores and classification accuracy.
  3. Compute JSD and SE metrics for both models on ambiguous test cases (e.g., items with visual similarities like Shirt vs Pullover) to validate human-like uncertainty handling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the QT-NN model's performance scale with larger, more complex image datasets beyond Fashion MNIST?
- Basis in paper: [inferred] The paper focuses on Fashion MNIST and mentions the potential for more complex data patterns.
- Why unresolved: The study only tested the QT-NN on the Fashion MNIST dataset, which is relatively simple compared to real-world image datasets.
- What evidence would resolve it: Testing the QT-NN on diverse, large-scale image datasets like ImageNet or medical imaging datasets would provide insights into its scalability and performance.

### Open Question 2
- Question: What is the optimal architecture for hybrid quantum-classical neural networks that maximize the benefits of both approaches?
- Basis in paper: [explicit] The paper discusses the potential of hybrid quantum-Bayesian neural networks (QBNNs) but does not explore specific architectures.
- Why unresolved: The paper proposes the concept of QBNNs but does not provide a detailed implementation or comparative analysis of different hybrid architectures.
- What evidence would resolve it: Developing and testing various hybrid quantum-classical architectures on benchmark tasks would reveal the most effective configurations.

### Open Question 3
- Question: How does the QT-NN model handle sequential or temporal data, such as video or time series?
- Basis in paper: [inferred] The paper focuses on static image classification and does not address sequential data.
- Why unresolved: The study does not explore the QT-NN's capabilities with temporal or sequential data, which are common in real-world applications.
- What evidence would resolve it: Extending the QT-NN to handle sequential data and testing it on tasks like video classification or time series prediction would demonstrate its versatility.

## Limitations

- The quantum cognition framework applied to neural networks lacks direct empirical validation that human decision-making operates via quantum superposition principles
- Claims about human-like decision-making and superiority of quantum cognition approaches lack direct empirical validation against human perception studies
- The 50x speed improvement claim lacks comparison against modern accelerated training techniques that might achieve similar gains through different mechanisms

## Confidence

- **High confidence**: The basic neural network architecture and Fashion MNIST classification task are well-established and reproducible
- **Medium confidence**: The quantum tunnelling activation function implementation appears mathematically sound, though its cognitive benefits are theoretical
- **Low confidence**: Claims about human-like decision-making and the superiority of quantum cognition approaches lack direct empirical validation against human perception studies

## Next Checks

1. Conduct human perception studies on ambiguous Fashion MNIST items (Shirt vs Pullover) and compare actual human uncertainty patterns against both QT-NN and classical model outputs using JSD analysis
2. Benchmark QT-NN training speed against classical networks with modern optimization techniques (AdamW, learning rate scheduling, mixed precision) on identical hardware to isolate the quantum tunnelling contribution
3. Test the QT-NN architecture on multiple datasets (CIFAR-10, CIFAR-100) to verify that the claimed benefits generalize beyond Fashion MNIST and aren't dataset-specific artifacts