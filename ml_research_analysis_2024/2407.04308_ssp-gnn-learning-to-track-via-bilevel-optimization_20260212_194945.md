---
ver: rpa2
title: 'SSP-GNN: Learning to Track via Bilevel Optimization'
arxiv_id: '2407.04308'
source_url: https://arxiv.org/abs/2407.04308
tags:
- tracking
- graph
- tracks
- algorithm
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel end-to-end trainable method for multi-object
  tracking using a graph neural network combined with a successive shortest paths
  (SSP) algorithm. The approach addresses the challenge of tracking objects in scenarios
  with high-dimensional features and complex dynamics by learning edge costs in a
  tracking graph via message passing, then optimizing globally with SSP.
---

# SSP-GNN: Learning to Track via Bilevel Optimization

## Quick Facts
- **arXiv ID:** 2407.04308
- **Source URL:** https://arxiv.org/abs/2407.04308
- **Authors:** Griffin Golias; Masa Nakura-Fan; Vitaly Ablavsky
- **Reference count:** 33
- **One-line primary result:** Achieves MOTA scores above 0.9 on synthetic data with high-dimensional features and complex dynamics

## Executive Summary
This paper introduces SSP-GNN, a novel end-to-end trainable method for multi-object tracking that combines graph neural networks with successive shortest paths (SSP) algorithms. The approach addresses the challenge of tracking objects in scenarios with high-dimensional features and complex dynamics by learning edge costs in a tracking graph via message passing, then optimizing globally with SSP. The model is trained through bilevel optimization, where the outer level learns GNN parameters to minimize a loss function defined on predicted and ground-truth tracks, and the inner level solves tracking as a shortest-path problem. Experiments on synthetic data show the method achieves strong performance, with MOTA scores above 0.9 even in challenging conditions with many false alarms or weak features.

## Method Summary
SSP-GNN addresses multi-object tracking by constructing a detection graph from input detections and using a message-passing network (MPN) to predict edge costs. These costs are then optimized using the successive shortest paths (SSP) algorithm to find globally optimal tracks. The entire system is trained end-to-end using bilevel optimization, where the outer optimization learns GNN parameters to minimize tracking loss, while the inner optimization solves the tracking problem. The method is tested on synthetic data generated using the Stone Soup framework with constant velocity and Ornstein-Uhlenbeck motion models, incorporating detections with kinematic information and re-identification features drawn from mixture of Gaussians distributions.

## Key Results
- Achieves MOTA scores above 0.9 on synthetic data even with high false alarm rates and weak features
- Outperforms baseline methods without global path optimization, particularly in scenarios with intersecting tracks or high clutter
- Demonstrates robustness to noisy feature dimensions and varying training set sizes
- Shows computational efficiency advantages over linear programming-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The SSP-GNN framework achieves globally optimal tracking by combining GNN-based edge cost prediction with a successive shortest paths algorithm.
- Mechanism: The GNN learns to map detection features to edge costs on the tracking graph, and SSP then finds the optimal set of node-disjoint paths. This two-stage process leverages the representational power of GNNs while guaranteeing constraint satisfaction.
- Core assumption: The SSP algorithm finds globally optimal paths given the learned edge costs, and the GNN can learn cost functions that differentiate correct from incorrect tracks.
- Evidence anchors:
  - [abstract] "Our method applies a successive shortest paths (SSP) algorithm to a tracking graph defined over a batch of frames. The edge costs in this tracking graph are computed via a message-passing network..."
  - [section] "In our outer optimization problem we learn parameters of a function that computes the cost of each edge in the tracking graph given detections that include attributes."
- Break condition: If the GNN cannot learn cost functions that reflect true track quality, or if SSP is unable to find optimal paths due to graph structure, the framework fails.

### Mechanism 2
- Claim: Bilevel optimization allows the GNN parameters to be learned based on tracking performance rather than direct feature matching.
- Mechanism: The outer optimization updates GNN parameters to minimize a loss function defined on predicted tracks versus ground truth, while the inner optimization solves the tracking problem using SSP. This creates a feedback loop where the GNN learns from tracking mistakes.
- Core assumption: The loss function properly captures tracking performance, and the bilevel optimization can effectively backpropagate gradients through the SSP solution.
- Evidence anchors:
  - [abstract] "The parameters of the GNN, and hence, the tracker, are learned end-to-end on a training set of example ground-truth tracks and detections. Specifically, learning takes the form of bilevel optimization..."
  - [section] "Bilevel optimization to learn MPN parametersWe formulate the following bilevel optimization problem...to find the optimal set of GNN parameters given ground-truth pathsP +"
- Break condition: If the bilevel optimization becomes too computationally expensive or the gradient estimation through SSP is unstable, learning may fail.

### Mechanism 3
- Claim: The graph neural network architecture effectively captures complex relationships between detections across time.
- Mechanism: The MPN uses message passing layers to aggregate information from neighboring detections, allowing it to capture temporal dependencies and feature relationships that inform edge cost predictions.
- Core assumption: The GNN architecture (number of layers, hidden dimension size) is sufficient to capture the necessary relationships in the detection data.
- Evidence anchors:
  - [abstract] "The edge costs in this tracking graph are computed via a message-passing network, a graph neural network (GNN) variant."
  - [section] "Letf θ be a function, parameterized byθfor computing edge costs in a graphG det = (V,E,Y)...Ourf θ takes the form of a message-passing network (MPN) [11]..."
- Break condition: If the GNN architecture is too shallow or narrow to capture necessary relationships, or if message passing fails to aggregate relevant information, edge cost predictions will be poor.

## Foundational Learning

- Concept: Graph Neural Networks and Message Passing
  - Why needed here: The GNN is the core component that learns to map detection features to edge costs, which is essential for the tracking framework.
  - Quick check question: Can you explain how message passing works in a GNN and why it's suitable for capturing relationships in graph-structured data?

- Concept: Bilevel Optimization
  - Why needed here: The learning process involves two optimization problems - learning GNN parameters and solving the tracking problem - which requires bilevel optimization.
  - Quick check question: What is the difference between single-level and bilevel optimization, and why is bilevel optimization necessary in this tracking framework?

- Concept: Successive Shortest Paths Algorithm
  - Why needed here: SSP provides the inner optimization that finds globally optimal tracks given the learned edge costs.
  - Quick check question: How does the SSP algorithm guarantee finding optimal paths, and what constraints does it satisfy in the tracking context?

## Architecture Onboarding

- Component map:
  - Input detections -> Detection graph construction -> Message Passing Network -> Edge cost computation -> Tracking graph -> Successive Shortest Paths -> Output tracks

- Critical path:
  1. Construct detection graph from input detections
  2. Compute edge costs using MPN
  3. Copy edge costs to tracking graph
  4. Run SSP algorithm to find optimal tracks
  5. Compute loss and update GNN parameters

- Design tradeoffs:
  - GNN depth vs. computational efficiency: More layers may capture better relationships but increase computation
  - Batch size vs. memory usage: Larger batches provide more context but require more memory
  - SSP vs. other optimization methods: SSP guarantees optimality but may be slower than heuristics

- Failure signatures:
  - Poor MOTA scores despite low loss during training (overfitting to training data)
  - High variance in performance across different scenarios
  - Tracks that frequently fragment or swap identities

- First 3 experiments:
  1. Validate GNN edge cost predictions on a simple tracking scenario with ground truth
  2. Test SSP algorithm independently with pre-computed edge costs
  3. Run end-to-end training on a small dataset and evaluate tracking performance

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several important limitations are acknowledged:

## Limitations
- Evaluation limited to synthetic data only, with no real-world dataset validation
- Computational complexity of bilevel optimization not thoroughly analyzed
- Limited ablation studies on architecture choices and hyperparameters

## Confidence
- Theoretical framework and algorithmic design: Medium
- Empirical results and performance claims: Low-Medium
- Scalability and real-world applicability: Low

## Next Checks
1. Test the SSP-GNN framework on established MOT benchmarks (e.g., MOTChallenge) to verify performance claims beyond synthetic data
2. Conduct systematic ablation studies on GNN architectures, message passing depths, and SSP variants to evaluate impact on tracking accuracy and computational efficiency
3. Measure and analyze time and memory requirements for the bilevel optimization process, particularly for larger tracking scenarios with more detections and longer track durations