---
ver: rpa2
title: Discourse-Aware In-Context Learning for Temporal Expression Normalization
arxiv_id: '2404.07775'
source_url: https://arxiv.org/abs/2404.07775
tags:
- temporal
- context
- normalization
- examples
- timex3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the use of large language models (LLMs) for
  temporal expression (TE) normalization, a task that typically requires domain-specific
  rule-based systems or large amounts of training data. The authors propose a discourse-aware
  in-context learning approach that leverages few-shot examples and a document-level
  temporal context window to inject task, document, and example information into the
  LLM.
---

# Discourse-Aware In-Context Learning for Temporal Expression Normalization

## Quick Facts
- arXiv ID: 2404.07775
- Source URL: https://arxiv.org/abs/2404.07775
- Reference count: 19
- Key outcome: LLM-based in-context learning with few-shot examples and document-level context achieves competitive results for temporal expression normalization across multiple languages and domains

## Executive Summary
This work explores using large language models (LLMs) for temporal expression (TE) normalization, a task traditionally requiring domain-specific rule-based systems or extensive training data. The authors propose a discourse-aware in-context learning approach that combines few-shot examples with document-level temporal context windows to inject task, document, and example information into the LLM. They evaluate various sample selection strategies for retrieving the most relevant examples and demonstrate competitive performance compared to task-specific models, with significant improvements in non-standard settings through dynamic example inclusion during inference.

## Method Summary
The method uses in-context learning with few-shot examples and a document-level temporal context window to normalize temporal expressions without task-specific training. For each target sentence containing temporal expressions, the approach retrieves semantically similar examples from training data using embedding similarity, constructs a prompt combining task description, context window, and examples, and sends it to an LLM (GPT-3.5-turbo or Zephyr) for inference. The LLM produces normalized temporal expressions in TimeML format, which are then parsed from JSON output and evaluated against ground truth using accuracy metrics.

## Key Results
- Competitive performance compared to task-specific models across six domains and seven languages
- Document-level context windows improve performance for relative and implicit temporal expressions
- Dynamic example selection strategies enhance few-shot learning effectiveness
- Significant improvements in non-standard settings through adaptive example inclusion during inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot examples with semantic similarity retrieval enable the LLM to generalize temporal normalization without task-specific training
- Mechanism: The LLM leverages its pre-existing knowledge by being prompted with examples that are semantically similar to the target text, allowing it to infer the normalization format and rules
- Core assumption: The LLM's pre-training has encoded sufficient temporal reasoning and normalization patterns that can be activated through in-context learning
- Evidence anchors:
  - [abstract] "explore various sample selection strategies to retrieve the most relevant set of examples"
  - [section 3.2] "we use the embedding model to create vector representations of text sequences and select examples based on the embedding similarity between candidate sentences and t"
  - [corpus] Weak - no explicit mention of pre-training data composition related to temporal expressions

### Mechanism 2
- Claim: Document-level context windows allow the LLM to resolve temporal dependencies across sentences
- Mechanism: By maintaining a running record of previously seen temporal expressions within the same document, the LLM can use earlier temporal expressions as anchors for normalizing subsequent relative or implicit expressions
- Core assumption: Temporal expressions within a document are often interdependent, and this interdependency can be captured through sequential processing with context windows
- Evidence anchors:
  - [abstract] "window-based prompt design approach, we can perform TE normalization across sentences"
  - [section 3.1] "we process all sentences from a document d containing temporal expressions (target sentences) sequentially"
  - [corpus] Weak - no explicit mention of document-level temporal dependencies in the studied datasets

### Mechanism 3
- Claim: Multilingual in-context learning allows the LLM to generalize temporal normalization across languages without language-specific training data
- Mechanism: The LLM can transfer knowledge from examples in one language to normalize temporal expressions in another language, leveraging its multilingual pre-training
- Core assumption: The LLM's multilingual pre-training has created cross-lingual representations that allow knowledge transfer for temporal normalization tasks
- Evidence anchors:
  - [abstract] "broad evaluation across six domains and seven languages"
  - [section 4.2] "study the effect of our method in 3 different settings on the multilingual AncientTimes corpus"
  - [corpus] Weak - no explicit mention of the LLM's multilingual capabilities or pre-training data composition

## Foundational Learning

- Concept: Temporal expression types and normalization formats
  - Why needed here: Understanding the different temporal expression types (DATE, TIME, DURATION, SET) and their normalization formats is essential for designing effective prompts and evaluating results
  - Quick check question: What are the four temporal types defined by TimeML, and how would you normalize "next week" if today is April 24, 2024?

- Concept: In-context learning and few-shot prompting
  - Why needed here: The entire approach relies on selecting and presenting few-shot examples effectively to guide the LLM's behavior without training
  - Quick check question: What are the key components of an effective in-context learning prompt for a text classification task?

- Concept: Semantic search and embedding similarity
  - Why needed here: The sample selection strategies rely on finding semantically similar examples using vector representations, which requires understanding embedding models and similarity metrics
  - Quick check question: How would you use cosine similarity between sentence embeddings to find the 5 most similar sentences to a given query?

## Architecture Onboarding

- Component map: Document parsing -> Sentence splitting -> Temporal expression identification -> Example retrieval (semantic search) -> Prompt construction -> LLM inference -> JSON output parsing -> Evaluation

- Critical path:
  1. Parse input document and identify sentences with temporal expressions
  2. For each target sentence, retrieve semantically similar examples from training data
  3. Construct prompt with task description, context window, and examples
  4. Send prompt to LLM and receive normalized temporal expressions
  5. Parse JSON output and store predictions
  6. Evaluate predictions against ground truth

- Design tradeoffs:
  - Number of examples vs. context length: More examples improve performance but may exceed LLM context limits
  - Context window length: Longer windows capture more dependencies but increase prompt size and cost
  - Sentence-level vs. document-level processing: Sentence-level allows more examples but may miss document-level context
  - Proprietary vs. open-source models: Proprietary models offer better performance but at higher cost and with API limitations

- Failure signatures:
  - Invalid JSON output: LLM fails to follow output format instructions
  - Missing temporal expressions: LLM fails to identify or normalize some temporal expressions
  - Incorrect normalization: LLM produces syntactically valid but semantically incorrect normalized values
  - Context window overflow: Combined prompt exceeds LLM context length limits

- First 3 experiments:
  1. Compare performance of Target-centric vs. Target-agnostic example selection on a single dataset
  2. Test different context window lengths (1, 3, 5 sentences) on document-level datasets
  3. Evaluate multilingual performance by testing monolingual vs. multilingual example pools on AncientTimes corpus

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of in-context learning for temporal expression normalization scale with increasing context window sizes beyond 5 sentences?
- Basis in paper: [inferred] The paper mentions that the best results are obtained with context lengths between one and five sentences, and that longer context sizes lead to a decrease in performance
- Why unresolved: The paper only explores context window lengths up to 5 sentences, and does not investigate the effects of larger context sizes
- What evidence would resolve it: Additional experiments with larger context window sizes, measuring the impact on normalization accuracy and computational efficiency

### Open Question 2
- Question: How do different sentence embedding models affect the sample selection strategies and overall performance of in-context learning for temporal expression normalization?
- Basis in paper: [explicit] The paper mentions using a specific embedding model for creating vector representations of text sequences and selecting examples based on embedding similarity
- Why unresolved: The paper does not compare the performance of different sentence embedding models or explore their impact on the sample selection strategies
- What evidence would resolve it: Experiments using different sentence embedding models and analyzing their effects on sample selection and normalization accuracy

### Open Question 3
- Question: Can the in-context learning approach be extended to handle temporal expressions in multimodal contexts, such as text with accompanying images or videos?
- Basis in paper: [inferred] The paper focuses solely on text-based temporal expression normalization and does not explore multimodal contexts
- Why unresolved: The paper does not investigate the applicability of the approach to multimodal data or consider the challenges of integrating temporal information from different modalities
- What evidence would resolve it: Experiments using multimodal data, such as text with images or videos, and evaluating the performance of the in-context learning approach in these scenarios

## Limitations

- The exact embedding model used for semantic search is not specified, which is critical for reproducibility and performance
- LLM API configuration details beyond temperature are missing, potentially affecting output quality and consistency
- The mechanism for cross-lingual transfer through in-context learning is not fully explained, despite multilingual evaluation claims

## Confidence

**High Confidence**: The core methodology of using in-context learning for temporal normalization is sound and well-established; the general approach of combining task description, context window, and few-shot examples follows standard ICL practices; the evaluation setup with accuracy metrics and multiple datasets is appropriate

**Medium Confidence**: The specific sample selection strategies and their relative performance differences; the effectiveness of document-level context windows compared to sentence-level processing; the generalizability of results across all seven languages and six domains

**Low Confidence**: The exact implementation details for embedding-based semantic search; the LLM-specific configurations beyond temperature; the mechanism and limitations of cross-lingual transfer in the multilingual setting

## Next Checks

1. **Reproduce Sample Selection**: Implement both Target-centric and Target-agnostic sample selection strategies using a standard embedding model (e.g., sentence-transformers/all-MiniLM-L6-v2) and verify that the semantic similarity-based retrieval produces relevant examples for temporal normalization tasks

2. **Test Context Window Impact**: Conduct controlled experiments varying context window sizes (1, 3, 5 sentences) on document-level datasets to quantify the performance gains from maintaining temporal context across sentences, measuring the point of diminishing returns

3. **Cross-Lingual Transfer Validation**: Design an experiment where temporal expressions are normalized using examples from a different language (e.g., English examples for French temporal expressions) to empirically validate the cross-lingual transfer capability claimed in the multilingual evaluation