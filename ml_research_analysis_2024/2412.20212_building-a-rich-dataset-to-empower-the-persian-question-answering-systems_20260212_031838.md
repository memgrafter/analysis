---
ver: rpa2
title: Building a Rich Dataset to Empower the Persian Question Answering Systems
arxiv_id: '2412.20212'
source_url: https://arxiv.org/abs/2412.20212
tags:
- dataset
- question
- persian
- questions
- answering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces NextQuAD, a comprehensive Persian open-domain
  question answering dataset comprising 7,515 contexts with 23,918 question-answer
  pairs. The dataset was constructed through three methods: translating SQuAD 2.0
  (845 high-quality records after manual filtering), extracting question-answer pairs
  from the "Quiz of Kings" mobile game (1,915 records), and manual collection from
  Persian websites by seven crowdworkers (21,000+ records).'
---

# Building a Rich Dataset to Empower the Persian Question Answering Systems

## Quick Facts
- **arXiv ID**: 2412.20212
- **Source URL**: https://arxiv.org/abs/2412.20212
- **Reference count**: 40
- **Primary result**: Introduces NextQuAD, a Persian QA dataset with 23,918 question-answer pairs, achieving 0.95 EM and 0.97 F1 on dev set

## Executive Summary
This paper introduces NextQuAD, a comprehensive Persian open-domain question answering dataset comprising 7,515 contexts with 23,918 question-answer pairs. The dataset was constructed through three methods: translating SQuAD 2.0 (845 high-quality records after manual filtering), extracting question-answer pairs from the "Quiz of Kings" mobile game (1,915 records), and manual collection from Persian websites by seven crowdworkers (21,000+ records). The dataset covers diverse subjects including householding, politics, art, sports, and health. A BERT-based QA model was trained using ParsBERT and XLM-RoBERTa pre-trained language models, with ensemble methods applied to improve performance. The model achieved 0.95 Exact Match and 0.97 F1 score on the development set. When evaluated on other Persian QA datasets (PersianQA and ParSQuAD), the model outperformed their original implementations, demonstrating NextQuAD's superior quality. The study highlights that dataset quality is more important than size for QA system performance.

## Method Summary
NextQuAD was constructed using three distinct data collection approaches. First, the SQuAD 2.0 dataset was translated into Persian, with 845 high-quality question-answer pairs retained after manual filtering. Second, the authors extracted question-answer pairs from the "Quiz of Kings" mobile game, yielding 1,915 records. Third, seven crowdworkers manually collected question-answer pairs from Persian websites, generating over 21,000 records. The dataset covers diverse subjects including householding, politics, art, sports, and health. For model training, a BERT-based QA architecture was implemented using both ParsBERT and XLM-RoBERTa pre-trained language models, with ensemble methods employed to improve performance. The model was evaluated on NextQuAD's development set and benchmarked against PersianQA and ParSQuAD datasets.

## Key Results
- NextQuAD contains 7,515 contexts with 23,918 question-answer pairs
- BERT-based QA model achieved 0.95 Exact Match and 0.97 F1 score on dev set
- Model outperformed original implementations on PersianQA and ParSQuAD datasets
- Dataset construction yielded 845 high-quality records from SQuAD 2.0 translation, 1,915 from Quiz of Kings, and 21,000+ from manual collection

## Why This Works (Mechanism)
The dataset construction methodology combines multiple high-quality data sources to create a comprehensive Persian QA dataset. By translating an established English dataset (SQuAD 2.0), extracting from an existing Persian game dataset, and collecting new data from Persian websites, the authors ensure diverse question types and subject coverage. The use of pre-trained language models (ParsBERT and XLM-RoBERTa) provides strong linguistic representations for Persian, while ensemble methods combine the strengths of different model variants. The emphasis on dataset quality over quantity is supported by the model's superior performance on benchmark datasets, suggesting that well-curated data with diverse, accurate question-answer pairs is more valuable than simply increasing dataset size.

## Foundational Learning
- **Persian Language Modeling**: Understanding Persian linguistic structures and tokenization is essential for effective QA system development. Quick check: Verify that the pre-trained models properly handle Persian-specific characters and word segmentation.
- **Question-Answer Pair Construction**: The quality of QA pairs depends on clear question formulation and unambiguous answers. Quick check: Ensure questions are specific enough to have definitive answers while covering diverse topics.
- **Dataset Annotation Consistency**: Multiple annotators require standardized guidelines to maintain quality across large datasets. Quick check: Calculate inter-annotator agreement scores for the manually collected data.
- **BERT Architecture for QA**: Understanding how BERT-based models handle span extraction for QA tasks. Quick check: Verify that the model correctly identifies answer boundaries within context passages.
- **Ensemble Methods**: Combining multiple model predictions can improve overall performance. Quick check: Compare ensemble performance against individual model variants to confirm improvement.
- **Evaluation Metrics**: F1 score and Exact Match are standard metrics for QA systems. Quick check: Ensure these metrics are calculated correctly, accounting for partial matches in F1 score.

## Architecture Onboarding

### Component Map
Crowdworkers + Persian websites -> Manual Collection -> Dataset Assembly -> BERT-based QA Model (ParsBERT + XLM-RoBERTa) -> Ensemble Prediction -> Evaluation

### Critical Path
Context extraction → Question-answer pair creation → Dataset filtering → Model training → Ensemble prediction → Performance evaluation

### Design Tradeoffs
The choice to use multiple data sources (translated, game-extracted, and manually collected) provides diversity but introduces quality control challenges. The decision to prioritize quality over quantity is validated by superior benchmark performance, though it limits dataset size. Using both ParsBERT and XLM-RoBERTa provides language coverage but increases computational complexity. The ensemble approach improves performance but requires careful model weight optimization.

### Failure Signatures
Performance degradation may occur due to annotation inconsistencies in the manually collected data, domain-specific weaknesses in certain subject areas, or insufficient coverage of certain question types. The model may also struggle with questions requiring complex reasoning or multi-hop inference that weren't adequately represented in the training data.

### First Experiments
1. Evaluate model performance on held-out subsets from each data source to identify quality variations
2. Compare individual model performance (ParsBERT vs XLM-RoBERTa) to quantify ensemble benefits
3. Analyze error patterns across different subject domains to identify coverage gaps

## Open Questions the Paper Calls Out
None

## Limitations
- Significant quality variation across the three data sources, with manual collection potentially introducing annotation inconsistencies
- Limited error analysis and ablation studies to understand performance drivers
- Lack of comprehensive distribution analysis showing domain balance across the dataset
- Claims about quality outweighing size require more systematic validation across different contexts

## Confidence
- **High Confidence**: Basic dataset construction methodology and BERT-based model training with reported performance metrics
- **Medium Confidence**: Claims about NextQuAD being the largest Persian dataset and quality being more important than size
- **Low Confidence**: Generalization that dataset quality consistently outweighs size across different QA contexts

## Next Checks
1. Conduct systematic quality assessment of the 21,000+ manually collected records to identify annotation inconsistencies and inter-annotator agreement scores
2. Perform ablation studies varying dataset size and quality independently to empirically test whether quality truly outweighs size for QA performance
3. Implement comprehensive error analysis across different subject domains to identify potential domain-specific weaknesses in the dataset