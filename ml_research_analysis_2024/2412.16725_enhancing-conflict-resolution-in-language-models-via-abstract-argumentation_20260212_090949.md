---
ver: rpa2
title: Enhancing Conflict Resolution in Language Models via Abstract Argumentation
arxiv_id: '2412.16725'
source_url: https://arxiv.org/abs/2412.16725
tags:
- arguments
- argumentation
- complete
- llms
- labelling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper enhances large language models' conflict resolution
  capabilities using abstract argumentation frameworks. It introduces a novel dataset
  with formal argumentation frameworks and process explanations, then fine-tunes models
  via supervised fine-tuning and reinforcement learning from human feedback.
---

# Enhancing Conflict Resolution in Language Models via Abstract Argumentation

## Quick Facts
- **arXiv ID**: 2412.16725
- **Source URL**: https://arxiv.org/abs/2412.16725
- **Reference count**: 40
- **One-line primary result**: Process explanations significantly improve LLM conflict resolution accuracy (97% vs 31% on grounded semantics) and generalization to complex frameworks

## Executive Summary
This paper introduces a novel approach to enhance large language models' conflict resolution capabilities using abstract argumentation frameworks (AAF). The authors create a comprehensive dataset with formal argumentation structures and detailed process explanations, then fine-tune models using supervised learning and reinforcement learning from human feedback. Their experiments demonstrate that models trained with process explanations achieve significantly higher accuracy in computing argumentation semantics compared to zero-shot chain-of-thought approaches, with improvements ranging from 66% to 37% across different semantics. The method shows particular strength in generalization to complex frameworks and improves conflict-free extension generation from 62% to 99%.

## Method Summary
The authors generate diverse argumentation frameworks (6-25 arguments) and compute their semantics using formal labeling algorithms. They create detailed process explanations for each framework and split the data into training (60k samples) and test sets (2k samples). Models are fine-tuned using LoRA with supervised learning, optionally followed by reinforcement learning from human feedback for improved explainability. The approach is evaluated using multiple metrics including extension view accuracy, argument view accuracy, conflict-free proportion, and at least one successful extension. Visual representations are also tested as multimodal inputs alongside text.

## Key Results
- SFT models with explanations achieve 97% accuracy on grounded semantics versus 31% baseline
- Process explanations enable superior generalization to complex frameworks (>25 arguments)
- Conflict-free extension generation improves from 62% to 99% with explanation-based training
- Visual inputs provide marginal but consistent improvements over text-only approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Process explanations enable models to learn the underlying logical structure of argumentation frameworks, improving generalization to unseen complexities.
- **Mechanism**: By providing step-by-step reasoning traces (e.g., "P, so Q" justifications), the model learns not just the final answer but the causal chain of inference. This mirrors symbolic computation and helps the model internalize the rules of conflict resolution.
- **Core assumption**: The model can extract and generalize logical patterns from natural language explanations rather than memorizing surface patterns.
- **Evidence anchors**:
  - [abstract] "Models trained with explanations exhibit superior generalization accuracy compared to those trained solely on question-answer data."
  - [section] "In the generalization stage, significant differences emerge between models trained with and without explanations. Models with explanations maintain relatively high accuracy, though performance gradually declines as complexity increases. Conversely, models without explanations experience a sharp drop in accuracy, sometimes falling to 30%, indicating poor generalization."
  - [corpus] No direct evidence; this is a novel contribution.
- **Break condition**: If the explanation format becomes too noisy or inconsistent, the model may learn incorrect patterns rather than the intended logical structure.

### Mechanism 2
- **Claim**: Fine-tuning with supervised learning and reinforcement learning from human feedback (RLHF) improves both accuracy and explainability of LLM outputs.
- **Mechanism**: SFT aligns the model with the task-specific data distribution, while RLHF optimizes for human-preferred explanations. This dual approach ensures both correct predictions and pedagogically useful reasoning traces.
- **Core assumption**: Human feedback can effectively guide the model toward more natural and interpretable explanations without sacrificing accuracy.
- **Evidence anchors**:
  - [abstract] "Models trained with explanations exhibit superior generalization accuracy compared to those trained solely on question-answer data."
  - [section] "Our findings suggest that integrating human feedback via RLHF can substantially improve the explainability of model-generated decisions, making them align more closely with human reasoning and teaching practices."
  - [corpus] Weak evidence; the study is novel and doesn't cite similar RLHF applications in argumentation.
- **Break condition**: If human annotators introduce inconsistent or incorrect explanations, RLHF may reinforce suboptimal patterns.

### Mechanism 3
- **Claim**: Visual representations of argumentation frameworks (e.g., graph images) provide marginal but consistent improvements in model understanding compared to text-only inputs.
- **Mechanism**: Multimodal inputs (text + visual) allow the model to cross-reference structural information, reducing ambiguity in complex attack/defense relationships.
- **Core assumption**: The model can effectively integrate visual and textual information to enhance reasoning, even if the improvement is small.
- **Evidence anchors**:
  - [section] "For the Qwen2.5-VL-SFT model, using visualized graph inputs together with text achieves slightly higher accuracy on both grounded extension and complete extension prediction compared to the text-only setting."
  - [corpus] No direct evidence; this is a minor contribution not emphasized in related work.
- **Break condition**: If the visual representation is unclear or inconsistent with the textual description, it may introduce confusion rather than clarity.

## Foundational Learning

- **Concept: Abstract Argumentation Frameworks (AAF)**
  - Why needed here: AAF provides the formal structure for modeling conflicts and resolutions, which is the core task the model must learn.
  - Quick check question: What are the three possible labels for an argument in AAF, and what do they represent?

- **Concept: Labelling Algorithms for Semantics**
  - Why needed here: The model must learn to execute these algorithms step-by-step to compute grounded, complete, preferred, and stable extensions.
  - Quick check question: How does the grounded labelling algorithm differ from the complete labelling algorithm in terms of process?

- **Concept: Conflict-Freeness and Defensibility**
  - Why needed here: These properties are essential for validating extensions and are used in the explanation and evaluation phases.
  - Quick check question: What does it mean for an extension to be conflict-free, and why is this important for complete semantics?

## Architecture Onboarding

- **Component map**: Data generation -> Dataset creation -> SFT fine-tuning -> (Optional) RLHF -> Evaluation
- **Critical path**:
  1. Generate diverse AAFs with ground truth semantics
  2. Add process explanations (algorithmic steps)
  3. Fine-tune LLM on instruction-tuning format
  4. Evaluate on held-out test set and generalization sets
  5. (Optional) Apply RLHF for improved explainability
- **Design tradeoffs**:
  - Text-only vs. visual inputs: Slight accuracy gain vs. increased complexity
  - Full explanations vs. masked explanations: Better generalization vs. more data
  - SFT only vs. SFT + RLHF: Faster training vs. better human alignment
- **Failure signatures**:
  - Hallucinations: Model references non-existent attacks or arguments
  - Labelling inconsistency: Model changes labels mid-process
  - Early termination: Model stops before all arguments are processed
  - Incomplete search: Model misses valid extensions in complete semantics
- **First 3 experiments**:
  1. **Zero-shot CoT baseline**: Run CoT pipeline on test set to establish performance floor.
  2. **SFT with explanations**: Fine-tune model and evaluate on test set to confirm accuracy gains.
  3. **Generalization test**: Evaluate model on AAFs with >25 arguments to measure robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs handle argumentation frameworks with cycles or more complex graph structures beyond the current dataset scope?
- Basis in paper: [explicit] The paper mentions that argumentation graphs from complex real-world situations may exhibit greater structural complexity, and the current data distribution may not comprehensively cover such edge cases.
- Why unresolved: The paper does not provide empirical results on LLMs' performance with complex graph structures like cycles or highly interconnected frameworks.
- What evidence would resolve it: Experiments testing LLMs on argumentation frameworks with cycles, multiple interconnected components, or varying density levels would provide clarity on their limitations.

### Open Question 2
- Question: What is the impact of varying the quality and diversity of human feedback on the final performance of RLHF-trained models?
- Basis in paper: [explicit] The paper notes that they collected human-annotated explanations from expert annotators and used them for RLHF training, but does not analyze how different qualities or types of feedback affect outcomes.
- Why unresolved: The paper does not explore whether more diverse human feedback leads to better generalization or if feedback quality impacts the model's ability to handle edge cases.
- What evidence would resolve it: A systematic study comparing RLHF-trained models using feedback from experts vs. non-experts, or feedback with varying levels of detail and accuracy.

### Open Question 3
- Question: How do LLMs perform when mapping real-world scenarios to formal argumentation frameworks, and what biases might emerge in this process?
- Basis in paper: [explicit] The paper acknowledges that mapping complex real-world situations to arguments and attack/support relations is an important issue where fairness and representational bias may arise.
- Why unresolved: The paper focuses on formal argumentation computation but does not investigate the preliminary step of translating real-world conflicts into formal frameworks.
- What evidence would resolve it: Empirical studies where LLMs are tasked with converting real-world conflict scenarios into argumentation frameworks, followed by analysis of the resulting biases and fairness implications.

## Limitations

- The study doesn't control for dataset size when comparing models with and without explanations, making it unclear if improvements are due to explanation format or additional training data.
- RLHF is applied only to a small subset (1k examples) and focuses solely on grounded semantics, limiting generalizability of explainability improvements.
- Visual input experiments are limited to a single model (Qwen2.5-VL) without systematic comparison across multiple architectures.

## Confidence

- **High confidence**: The empirical results showing SFT with explanations outperforms zero-shot CoT baselines (97% vs 31% on grounded semantics). The dataset construction methodology and evaluation metrics are clearly specified.
- **Medium confidence**: The generalization claims for complex frameworks, as the study shows performance decline with complexity but doesn't establish whether this represents true generalization failure or task difficulty scaling.
- **Medium confidence**: The explainability improvements from RLHF, given the limited scope (1k examples, single semantics) and lack of comparison to alternative explainability techniques.

## Next Checks

1. **Ablation study on explanation format**: Create parallel training runs varying explanation detail levels (full, masked, minimal) while holding dataset size constant to isolate the effect of explanation quality versus quantity.

2. **Zero-shot generalization test**: Evaluate zero-shot performance on out-of-distribution frameworks (different structures, argument counts, or attack patterns) to better understand true generalization capabilities beyond complexity scaling.

3. **Multi-semantics RLHF evaluation**: Extend RLHF training and evaluation across all four semantics (grounded, complete, preferred, stable) rather than the current single-semantics approach to assess consistency of explainability improvements.