---
ver: rpa2
title: 'CAMPHOR: Collaborative Agents for Multi-input Planning and High-Order Reasoning
  On Device'
arxiv_id: '2410.09407'
source_url: https://arxiv.org/abs/2410.09407
tags:
- agent
- user
- prompt
- information
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAMPHOR introduces an on-device small language model (SLM) multi-agent
  framework for personalized query parsing that achieves 35% better task completion
  F1 than closed-source LLMs while maintaining privacy. The approach uses hierarchical
  agents with parameter sharing and prompt compression to handle multiple user inputs
  and personal context locally.
---

# CAMPHOR: Collaborative Agents for Multi-input Planning and High-Order Reasoning On Device

## Quick Facts
- arXiv ID: 2410.09407
- Source URL: https://arxiv.org/abs/2410.09407
- Authors: Yicheng Fu; Raviteja Anantha; Jianpeng Cheng
- Reference count: 40
- Primary result: 35% better task completion F1 than closed-source LLMs while maintaining privacy

## Executive Summary
CAMPHOR introduces an on-device small language model (SLM) multi-agent framework for personalized query parsing that achieves 35% better task completion F1 than closed-source LLMs while maintaining privacy. The approach uses hierarchical agents with parameter sharing and prompt compression to handle multiple user inputs and personal context locally. A novel dataset of 35K+ prompt-completion pairs enables SLM fine-tuning that outperforms instruction-based LLM baselines. Prompt compression reduces token count by 98% with minimal accuracy loss, eliminating server-device communication while protecting user data.

## Method Summary
CAMPHOR employs a hierarchical multi-agent architecture where a high-order reasoning agent decomposes complex tasks and coordinates expert agents for personal context, tool interaction, and plan generation. The framework uses parameter sharing across agents to reduce model size while maintaining task completion accuracy. A novel prompt compression technique encodes function definitions into single-token embeddings, enabling SLMs to reason over full toolboxes without exceeding prompt budgets. The approach is validated through fine-tuning SLMs (Phi-3.5 and Gemma-2) on a novel CAMPHOR dataset containing 35K+ prompt-completion pairs, achieving superior performance compared to instruction-based LLM baselines while keeping all processing on-device.

## Key Results
- Fine-tuned SLMs achieve 35% better task completion F1 than closed-source LLMs
- Prompt compression reduces token count by 98% with minimal accuracy loss
- CAMPHOR outperforms instruction-based LLM baselines on task completion metrics
- On-device processing maintains user privacy while achieving superior performance

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical decomposition with parameter sharing enables SLM to match LLM performance. The high-order reasoning agent breaks tasks into subgoals and coordinates expert agents for personal context, tool interaction, and plan generation. Parameter sharing reduces model size while maintaining task completion accuracy.

### Mechanism 2
Prompt compression reduces token count by 98% while maintaining accuracy. Single-token embeddings encode function definitions, allowing SLMs to reason over full toolbox without exceeding prompt budgets. Rotary positional embeddings ensure proper attention patterns.

### Mechanism 3
Fine-tuned SLMs outperform instruction-based LLMs on task completion metrics. SLMs trained on multi-agent trajectories learn to parse queries and coordinate actions better than LLMs following static instructions.

## Foundational Learning

- Concept: Multi-agent task decomposition
  - Why needed here: CAMPHOR requires understanding how to break complex queries into coordinated sub-tasks across specialized agents
  - Quick check question: Given a query "schedule a meeting with my boss next week," what sequence of agent actions would CAMPHOR take?

- Concept: Function calling with personal context
  - Why needed here: CAMPHOR must map user queries to executable functions while retrieving personal data (contacts, calendar, etc.)
  - Quick check question: How does CAMPHOR retrieve the correct contact when a query mentions "my travel buddy" without knowing who that is?

- Concept: Prompt compression techniques
  - Why needed here: CAMPHOR needs to fit large toolboxes into SLM prompt budgets while maintaining reasoning capability
  - Quick check question: What is the difference between using gist tokens versus single-token embeddings for function definitions?

## Architecture Onboarding

- Component map: High-order reasoning agent → Personal context agent → Device information agent → User perception agent → External knowledge agent → Task completion agent
- Critical path: Query → High-order agent decomposes → Expert agents retrieve context → Task completion agent generates final function calls → Execution and response
- Design tradeoffs: Parameter sharing reduces model size but may limit agent specialization; prompt compression saves tokens but requires careful embedding design
- Failure signatures: If high-order agent fails to coordinate, you'll see incomplete context retrieval; if prompt compression fails, tool selection will be random
- First 3 experiments:
  1. Test each agent independently with canned inputs to verify tool calling works
  2. Chain two agents together to test basic coordination
  3. Run end-to-end on simple queries from the CAMPHOR dataset to validate complete pipeline

## Open Questions the Paper Calls Out
- How does CAMPHOR's performance scale when handling multi-turn conversational queries with system policies and user follow-ups?
- What is the impact of runtime error-handling logic (e.g., disambiguation requests for multiple search results) on CAMPHOR's task completion accuracy?
- How does the choice between fine-tuning and instruction-based prompting affect CAMPHOR's performance across different SLM model sizes?

## Limitations
- The CAMPHOR dataset has not yet been released, preventing independent validation of claimed performance improvements
- The paper does not address how CAMPHOR handles edge cases where personal context is ambiguous or when multiple interpretations of a query are possible
- Performance claims are based on proprietary datasets (RMD, Alime, Assist, SGD) that are not publicly available

## Confidence
- High confidence: The architectural design of hierarchical multi-agent framework with parameter sharing is well-specified and theoretically sound
- Medium confidence: The prompt compression technique showing 98% token reduction with minimal accuracy loss is promising but lacks independent validation
- Low confidence: The claim that fine-tuned SLMs can generalize to completely new queries not seen during training is not empirically tested in the paper

## Next Checks
1. **Dataset Validation**: Once the CAMPHOR dataset is released, conduct a replication study to verify the reported 35% improvement over LLM baselines on the same test sets (RMD, Alime, Assist, SGD).
2. **Compression Ablation Study**: Implement alternative prompt compression methods (such as gist tokens or hierarchical compression) and compare their performance against the single-token embedding approach.
3. **Generalization Testing**: Design a benchmark of queries that are semantically similar to the training data but structurally different, then evaluate whether the fine-tuned CAMPHOR agents can successfully parse and execute these novel queries.