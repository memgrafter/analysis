---
ver: rpa2
title: Emotion-driven Piano Music Generation via Two-stage Disentanglement and Functional
  Representation
arxiv_id: '2407.20955'
source_url: https://arxiv.org/abs/2407.20955
tags:
- music
- emotion
- generation
- valence
- lead
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of emotion-driven piano music
  generation by proposing a two-stage Transformer-based framework with a novel functional
  representation of symbolic music. The method disentangles valence and arousal modeling:
  the first stage generates lead sheets with valence conditions using a functional
  representation that captures note-chord-key interactions, while the second stage
  adds performance-level attributes for arousal modeling.'
---

# Emotion-driven Piano Music Generation via Two-stage Disentanglement and Functional Representation

## Quick Facts
- arXiv ID: 2407.20955
- Source URL: https://arxiv.org/abs/2407.20955
- Reference count: 0
- Primary result: Two-stage Transformer-based framework with functional representation achieves 71.5% emotion recognition accuracy, significantly outperforming end-to-end models (31.0%)

## Executive Summary
This paper introduces a novel two-stage framework for emotion-driven piano music generation that disentangles valence and arousal modeling. The approach uses a functional representation of symbolic music to encode note-chord-key interactions relative to key signatures, addressing the challenge of maintaining key consistency while expressing emotions. The first stage generates lead sheets conditioned on valence, while the second stage adds performance-level attributes for arousal modeling. Both objective metrics and subjective listening tests demonstrate significant improvements over baseline methods, with the model achieving higher accuracy in emotion recognition and better subjective ratings for emotional expressiveness.

## Method Summary
The proposed framework employs a two-stage Transformer-based architecture with a novel functional representation. The first stage generates lead sheets (chords and melody) conditioned on valence using Transformer decoders with cross-attention to encode note-chord-key relationships. The second stage adds performance-level attributes (dynamics, articulation) to capture arousal, using separate Transformer decoders conditioned on the first-stage outputs. The functional representation encodes notes and chords relative to the key signature, enabling better key consistency and emotional expressiveness. The framework is trained separately for each stage with distinct loss functions, and a latent fusion technique combines emotion embeddings from different models to enhance controllability.

## Key Results
- 71.5% emotion recognition accuracy versus 31.0% for end-to-end models
- Significant improvements in key consistency metrics compared to baseline methods
- Subjective listening tests confirm superior performance in conveying both valence and arousal emotions
- Enables flexible control of arousal levels under the same lead sheet

## Why This Works (Mechanism)
The framework's success stems from its disentangled approach to modeling valence and arousal separately, which aligns with psychological theories of emotion. The functional representation captures the hierarchical structure of music (notes within chords within keys) and their temporal dependencies, providing richer context for generation. By conditioning on key signatures and encoding relative positions, the model maintains musical coherence while allowing emotional expression. The two-stage design allows specialized modeling of harmonic content (valence) and performance details (arousal), avoiding the complexity of end-to-end approaches that struggle to capture both aspects simultaneously.

## Foundational Learning

**Symbolic Music Representation**
- Why needed: Traditional piano-roll representations lack musical context and hierarchical relationships
- Quick check: Can the model maintain key consistency across generated pieces?

**Disentangled Emotion Modeling**
- Why needed: Valence and arousal are independent dimensions of emotion that require different modeling approaches
- Quick check: Does separating valence and arousal improve emotional expressiveness compared to joint modeling?

**Functional Representation**
- Why needed: Captures note-chord-key interactions and temporal dependencies essential for coherent musical structure
- Quick check: Does the functional representation improve key consistency metrics?

## Architecture Onboarding

**Component Map**
Functional Representation Encoder -> Stage 1 Valence Transformer -> Stage 2 Arousal Transformer -> Piano Roll Decoder

**Critical Path**
Input music sequence → Functional representation encoding → Valence-conditioned lead sheet generation → Arousal-conditioned performance attributes → Final piano roll output

**Design Tradeoffs**
- Two-stage vs. end-to-end: Two-stage provides better disentanglement but requires more training; end-to-end is simpler but struggles with emotional expressiveness
- Functional vs. piano roll representation: Functional captures musical context but is more complex to implement; piano roll is simpler but lacks hierarchical structure

**Failure Signatures**
- Poor key consistency indicates issues with functional representation or cross-attention mechanisms
- Inability to control arousal suggests problems with the second-stage conditioning or latent fusion
- Generic-sounding music may indicate insufficient diversity in the training data or model capacity limitations

**3 First Experiments**
1. Test functional representation's impact on key consistency by comparing against piano roll baseline
2. Evaluate disentanglement effectiveness by measuring valence and arousal independence
3. Assess controllability by generating multiple pieces with same lead sheet but different arousal levels

## Open Questions the Paper Calls Out
None

## Limitations
- Functional representation may not capture all musical nuances relevant to emotional expression
- Reliance on lead sheets as intermediate representation could constrain musical diversity
- Model performance heavily dependent on quality and representativeness of training data

## Confidence

**High Confidence**
- Technical implementation of two-stage architecture and functional representation

**Medium Confidence**
- Comparative improvements over baseline methods
- Subjective listening test results

**Low Confidence**
- Model's generalizability to other musical instruments or genres beyond piano

## Next Checks
1. Conduct cross-cultural subjective evaluations with diverse listener groups to validate emotional perception across different contexts
2. Test framework's adaptability to other musical instruments and genres to assess generalizability beyond piano music
3. Implement ablation studies focusing on functional representation component to quantify its individual contribution to performance improvements