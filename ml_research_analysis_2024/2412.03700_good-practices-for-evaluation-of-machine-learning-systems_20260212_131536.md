---
ver: rpa2
title: Good practices for evaluation of machine learning systems
arxiv_id: '2412.03700'
source_url: https://arxiv.org/abs/2412.03700
tags:
- data
- evaluation
- will
- system
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper outlines essential practices for evaluating machine
  learning systems, emphasizing the importance of careful design of the evaluation
  protocol. It covers key aspects such as data selection, metric selection, and statistical
  significance.
---

# Good practices for evaluation of machine learning systems

## Quick Facts
- arXiv ID: 2412.03700
- Source URL: https://arxiv.org/abs/2412.03700
- Authors: Luciana Ferrer; Odette Scharenborg; Tom Bäckström
- Reference count: 13
- Key outcome: The paper outlines essential practices for evaluating machine learning systems, emphasizing careful design of the evaluation protocol, including data selection, metric selection, and statistical significance.

## Executive Summary
This paper provides comprehensive guidelines for evaluating machine learning systems, focusing on proper evaluation protocol design. It emphasizes the importance of using evaluation data that closely resembles deployment scenarios, selecting appropriate performance metrics that reflect user needs, and computing confidence intervals to account for variability. The paper introduces bootstrapping techniques for confidence interval computation and advocates for expected cost as the primary metric for classification tasks. It also addresses the challenge of spurious correlations in datasets and provides practical advice for ensuring evaluation results are reliable and generalizable.

## Method Summary
The paper outlines a comprehensive evaluation framework involving three data splits (training, development, and evaluation), appropriate metric selection based on task type, and confidence interval computation through bootstrapping. For data splitting, it recommends cross-validation for small datasets and nested cross-validation for very small datasets, ensuring evaluation data resembles deployment scenarios and remains separate from development decisions. The paper advocates for expected cost as the primary metric for classification tasks and suggests appropriate metrics for other task types. For confidence intervals, it describes bootstrapping techniques that can handle non-iid data by respecting sample dependencies like speaker identity, with implementations available in a GitHub repository.

## Key Results
- Bootstrapping produces confidence intervals that capture both test-set variability and random seed effects when properly implemented
- Expected cost (EC) metric that matches application's true cost structure yields more actionable evaluations than default accuracy-based metrics
- Splitting data into training, development, and evaluation sets that reflect deployment data distribution prevents overfitting to spurious correlations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bootstrapping produces confidence intervals that capture both test-set variability and random seed effects.
- Mechanism: By resampling the test set and pooling results across multiple random seeds, bootstrapping aggregates variance from both sources into a single empirical distribution.
- Core assumption: The resampling process preserves the correlation structure between samples (e.g., speaker identity).
- Evidence anchors:
  - [section] "Bootstrapping can properly handle datasets where the samples are not iid... Bootstrapping can be easily adapted to take these correlation-inducing factors into account by defining the bootstrap samples based on the condition."
  - [abstract] "The paper introduces bootstrapping techniques to compute confidence intervals, accounting for variability due to random factors like data selection and random seeds."
- Break condition: If the resampling breaks speaker or session dependencies, the intervals become too narrow and misleadingly optimistic.

### Mechanism 2
- Claim: Using an expected cost (EC) metric that matches the application's true cost structure yields more actionable evaluations than default accuracy-based metrics.
- Mechanism: EC directly incorporates task-specific misclassification costs, so optimization targets the actual utility of the system rather than an arbitrary proxy.
- Core assumption: The cost matrix used in EC accurately reflects the relative harm of different errors in the target deployment scenario.
- Evidence anchors:
  - [section] "The most general and principled metric for assessment of categorical decisions is the expected cost (EC)... We strongly recommend using this metric for all classification tasks."
  - [abstract] "It also discusses the choice of performance metrics, advocating for metrics that reflect user needs, such as expected cost for classification tasks."
- Break condition: If the cost matrix is guessed or mismatched to real user impact, EC optimization can degrade actual system usefulness.

### Mechanism 3
- Claim: Splitting data into training, development, and evaluation sets that reflect the deployment data distribution prevents overfitting to spurious correlations.
- Mechanism: By ensuring the evaluation set resembles deployment conditions and excluding it from any model selection, the reported performance is a realistic predictor of future performance.
- Core assumption: The evaluation data is collected under conditions (microphones, background noise, speakers) that mirror the eventual deployment environment.
- Evidence anchors:
  - [section] "In order for the evaluation results to be a good prediction of deployment results, the evaluation data should be as similar to the use-case data as possible... Importantly, the evaluation data should never be used for development."
  - [abstract] "It covers key aspects such as data selection... The paper highlights the need for evaluation data that closely resembles deployment scenarios and stresses that evaluation data should not be used for development decisions."
- Break condition: If evaluation data is drawn from the same source as training data but contains hidden spurious correlations, the model may appear to perform well while failing in deployment.

## Foundational Learning

- Concept: Spurious correlations in datasets
  - Why needed here: Understanding how hidden correlations can cause models to overfit to dataset-specific artifacts rather than true predictive signals.
  - Quick check question: If control and patient recordings are made in different rooms, what kind of bias might a model learn?

- Concept: Proper scoring rules vs. threshold-dependent metrics
  - Why needed here: Knowing when to evaluate posterior probabilities (e.g., cross-entropy) versus categorical decisions (e.g., expected cost).
  - Quick check question: Why is AUC not appropriate when the final system outputs a hard decision with a fixed threshold?

- Concept: Bootstrap resampling with dependent samples
  - Why needed here: Correctly computing confidence intervals when samples are not independent (e.g., multiple utterances per speaker).
  - Quick check question: How would you adjust bootstrap sampling if your test set contains multiple samples from the same speaker?

## Architecture Onboarding

- Component map:
  - Data pipeline: Training / Development / Evaluation splits
  - Metric engine: Expected cost calculator, cross-entropy, sequence metrics
  - Bootstrap module: Resampling with group constraints, pooling across seeds
  - Reporting layer: Confidence intervals, per-group breakdowns, reference baselines

- Critical path:
  1. Define deployment scenario → set evaluation data conditions
  2. Design cost matrix → implement expected cost metric
  3. Bootstrap test set with speaker/session grouping
  4. Train models with multiple random seeds
  5. Pool results → compute final confidence intervals

- Design tradeoffs:
  - More evaluation subsets → better bias detection but higher variance
  - Larger bootstrap sample size → tighter intervals but higher compute cost
  - Fixed random seed for reproducibility vs. multiple seeds for robustness

- Failure signatures:
  - Confidence intervals too narrow → ignored sample dependencies
  - Reported accuracy much higher than expected cost → mismatched metric to task
  - Performance drops on unseen speakers → spurious correlations in training data

- First 3 experiments:
  1. Train and evaluate on iid data; compare bootstrap intervals with and without speaker grouping.
  2. Implement expected cost vs. accuracy on an imbalanced dataset; observe differences in selected model.
  3. Introduce synthetic spurious correlation (e.g., background noise linked to class); evaluate impact on held-out test data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically detect and mitigate spurious correlations in ML datasets beyond the "silent part prediction" approach mentioned for speech processing?
- Basis in paper: [explicit] The paper discusses spurious correlations as a "sneaky problem" where models learn to predict classes based on correlated but irrelevant features, particularly in medical datasets where control subjects are recorded under different conditions than patients.
- Why unresolved: The paper only mentions one diagnostic approach (training predictors on silent audio portions) and suggests that even noise reduction doesn't guarantee complete elimination of spurious correlations. The problem is described as "usually not easy to tell whether spurious correlations exist."
- What evidence would resolve it: Development of a general framework or set of diagnostic tools that can systematically identify spurious correlations across different domains and types of data (not just speech), along with validated methods to quantify and mitigate their impact on model performance.

### Open Question 2
- Question: What is the optimal strategy for selecting evaluation metrics that balance user needs with model interpretability, particularly for tasks where defining the metric is inherently difficult?
- Basis in paper: [explicit] The paper emphasizes that "performance metrics should be designed as a numerical measure of the value that the user will be able to extract from the system" and criticizes the use of default metrics that may not reflect user needs. It discusses the difficulty of defining metrics for tasks like translation, emotion recognition, and ASR where ground truth is not unique or well-defined.
- Why unresolved: While the paper provides guidance on selecting metrics for specific scenarios (classification, sequence evaluation, regression), it acknowledges that for many tasks "defining the metric is difficult" and suggests focusing on one use case, but doesn't provide a systematic approach for this selection process when multiple stakeholders or conflicting objectives are involved.
- What evidence would resolve it: Development of a decision framework or set of principles that can guide metric selection for complex tasks involving multiple stakeholders, conflicting objectives, or subjective evaluation criteria, along with empirical validation showing improved alignment between selected metrics and actual user satisfaction.

### Open Question 3
- Question: How can we develop more efficient methods for computing confidence intervals that account for multiple sources of variability (training data, random seeds, test data) without the computational burden of extensive bootstrapping?
- Basis in paper: [explicit] The paper describes bootstrapping techniques for computing confidence intervals that account for variability due to random factors, but notes that when considering variability from training data, "the number of bootstrap sets in this case will probably have to be much smaller than when doing bootstrapping on the test dataset due to computational constraints."
- Why unresolved: The paper acknowledges that current bootstrapping approaches can be computationally expensive when multiple sources of variability need to be considered simultaneously, and that "many other approaches exist for this purpose" but focuses only on bootstrapping. The computational burden limits the practical applicability of comprehensive uncertainty quantification.
- What evidence would resolve it: Development and validation of alternative statistical methods (analytical approximations, Bayesian approaches, or efficient sampling techniques) that can provide comparable confidence intervals with significantly reduced computational cost, along with empirical comparisons showing similar or better accuracy than traditional bootstrapping approaches.

## Limitations

- The paper lacks specific implementation details and concrete code examples needed for direct reproduction of the evaluation methodology
- Recommendations are primarily based on speech processing experience and may not fully generalize to all ML domains without additional validation
- The expected cost framework's effectiveness depends heavily on accurate cost matrix specification, which can be challenging in real-world scenarios

## Confidence

- **High Confidence**: Claims about the importance of separating evaluation data from development data, and the need for evaluation data to resemble deployment scenarios. These are well-established practices in ML evaluation with strong empirical support.
- **Medium Confidence**: Claims about expected cost being the optimal metric for all classification tasks. While theoretically sound, the practical effectiveness depends heavily on accurate cost matrix specification, which can be challenging in real-world scenarios.
- **Medium Confidence**: Claims about bootstrapping properly handling sample dependencies. The theoretical foundation is solid, but practical implementation details and potential pitfalls are not fully explored.

## Next Checks

1. **Dataset Dependency Test**: Implement the evaluation methodology on both iid and non-iid datasets (e.g., with speaker/session groupings) and compare bootstrap confidence intervals to assess whether dependencies are properly handled.

2. **Cost Matrix Sensitivity Analysis**: Evaluate the same classification task using different cost matrices (including incorrect/mismatched ones) to determine how sensitive expected cost-based model selection is to cost specification errors.

3. **Cross-Domain Generalization**: Apply the recommended evaluation framework to a non-speech ML task (e.g., computer vision or recommendation systems) and assess whether the same principles hold or require adaptation for different data types and use cases.