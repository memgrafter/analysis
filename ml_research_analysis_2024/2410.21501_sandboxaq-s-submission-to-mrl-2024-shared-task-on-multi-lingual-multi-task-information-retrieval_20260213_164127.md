---
ver: rpa2
title: SandboxAQ's submission to MRL 2024 Shared Task on Multi-lingual Multi-task
  Information Retrieval
arxiv_id: '2410.21501'
source_url: https://arxiv.org/abs/2410.21501
tags:
- dspy
- arxiv
- translation
- languages
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates multilingual QA and NER performance using
  five LLMs (GPT-4o, GPT-4-turbo, Claude-3.5-sonnet, Aya-23-8B, and Aya-23-35B) across
  five languages (Igbo, Swiss German, Turkish, Azerbaijani, and Yoruba) using various
  prompting strategies including zero-shot, translation, and DSPy techniques. The
  study finds that while GPT-4 models generally outperform others in QA, the Aya models
  achieve comparable performance with advanced prompting techniques, particularly
  DSPy-ReAct with English translation.
---

# SandboxAQ's submission to MRL 2024 Shared Task on Multi-lingual Multi-task Information Retrieval

## Quick Facts
- arXiv ID: 2410.21501
- Source URL: https://arxiv.org/abs/2410.21501
- Reference count: 22
- Primary result: GPT-4 models outperform others in QA while Aya models achieve comparable performance with advanced prompting techniques

## Executive Summary
This paper evaluates multilingual QA and NER performance using five LLMs across five languages with various prompting strategies. The study finds that while GPT-4 models generally outperform others in QA, the Aya models achieve comparable performance with advanced prompting techniques, particularly DSPy-ReAct with English translation. For NER, GPT-4o and Claude-3.5-sonnet consistently perform best, though advanced prompting shows mixed results. The research reveals that model size doesn't directly correlate with multilingual performance, and translation effects vary significantly between tasks. These findings highlight the need for task-specific optimization in multilingual NLP systems.

## Method Summary
The study evaluates five language models (GPT-4o, GPT-4-turbo, Claude-3.5-sonnet, Aya-23-8B, and Aya-23-35B) on multilingual QA and NER tasks across five languages using six evaluation strategies: simple zero-shot prompting, English translation, DSPy Chain-of-Thought, DSPy Chain-of-Thought with translation, DSPy ReAct, and DSPy ReAct with translation. The XTREME-UP repository datasets were used for both tasks, with validation and test sets evaluated separately. Performance was measured using accuracy for QA and F1 score for NER, with results analyzed across language, model, and prompting method combinations.

## Key Results
- GPT-4 models generally outperform other models in QA, but Aya models achieve comparable performance with DSPy-ReAct prompting
- Model size does not directly correlate with multilingual performance, as Aya-23-35B and Aya-23-8B performed comparably in NER despite different sizes
- Translation effects vary significantly between tasks: helps QA performance for most models but generally decreases NER performance, especially for GPT-4o and Claude-3.5-sonnet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DSPy prompting techniques significantly improve QA performance across multiple languages compared to simple zero-shot prompting.
- Mechanism: DSPy provides algorithmic prompt optimization through techniques like Chain of Thought and ReAct that help models break down reasoning into steps and combine analytical steps with practical actions.
- Core assumption: Advanced prompting techniques that work well in English benchmarks will generalize to multilingual low-resource settings.
- Evidence anchors:
  - [abstract] "Our results show that while we were able to achieve very high results in both tasks using zero-shot approaches, there was variability in performance across models, methods, and languages."
  - [section] "DSPy-enhanced prompts boosted performance for all models, and especially for the open source Aya models."
  - [corpus] Weak evidence - related papers focus on different tasks (cultural reasoning, Arabic NER, multimodal translation) rather than multilingual QA with DSPy
- Break condition: When language-specific linguistic features or cultural contexts require approaches beyond algorithmic reasoning steps.

### Mechanism 2
- Claim: Translation to English as an intermediate step can improve or maintain performance in multilingual tasks.
- Mechanism: English serves as a bridge language with abundant NLP resources, allowing models to leverage their stronger English capabilities before translating back to target languages.
- Core assumption: Models trained on English data can better process tasks when inputs are first translated to English, even if the final task is in another language.
- Evidence anchors:
  - [abstract] "We saw that advanced prompting techniques generally improved QA performance but had mixed results for NER; and we observed that language difficulty patterns differed between tasks."
  - [section] "We further examined the impact of translating text to English before processing. Translation alone tended to maintain or improve performance compared to simple QA, with the largest average improvements of 4.72% occurring for the TR language."
  - [corpus] Weak evidence - corpus neighbors don't provide direct evidence about translation benefits in multilingual QA
- Break condition: When translation quality degrades for specific language pairs or when task-specific knowledge is lost in translation.

### Mechanism 3
- Claim: Model size does not directly correlate with multilingual task performance.
- Mechanism: Factors beyond parameter count, such as model architecture and prompt engineering, play significant roles in multilingual capabilities.
- Core assumption: The relationship between model size and performance is not linear for multilingual tasks, unlike in monolingual settings.
- Evidence anchors:
  - [abstract] "The complex interactions between model architectures, prompting strategies, and linguistic features suggest that future advancements may require more nuanced, language-specific approaches rather than one-size-fits-all solutions."
  - [section] "Our results additionally revealed a nuanced relationship between model size and performance; raw parameter count isn't the sole determinant of multilingual and multi-task capabilities."
  - [corpus] Weak evidence - corpus neighbors don't provide evidence about model size vs performance relationships
- Break condition: When task complexity or language diversity requires larger models to capture necessary linguistic patterns.

## Foundational Learning

- Concept: Multilingual NLP challenges
  - Why needed here: Understanding the unique difficulties of processing multiple languages, including limited resources for low-resource languages and varying linguistic structures
  - Quick check question: Why might a model that performs well on English QA struggle with Yoruba or Swiss German QA?

- Concept: Prompt engineering and optimization
  - Why needed here: The paper heavily relies on various prompting strategies (zero-shot, DSPy Chain of Thought, ReAct, translation) to enhance model performance
  - Quick check question: What is the difference between DSPy Chain of Thought and DSPy ReAct prompting strategies?

- Concept: Named Entity Recognition fundamentals
  - Why needed here: The paper evaluates NER performance across languages using BIO tagging scheme and multiple entity types
  - Quick check question: What are the three entity types annotated in the NER dataset and how are they tagged?

## Architecture Onboarding

- Component map: Data preprocessing → Prompt engineering → Model inference → Output processing → Evaluation → Cross-lingual analysis → Cross-model comparison
- Critical path: Prompt engineering → Model inference → Output processing → Evaluation
- Design tradeoffs: Zero-shot vs. advanced prompting (complexity vs. performance), translation vs. direct processing (potential information loss vs. leveraging English resources)
- Failure signatures: Inconsistent performance across languages with same prompting method, degradation when combining multiple techniques, translation quality issues
- First 3 experiments:
  1. Run simple zero-shot QA for all five languages with each model to establish baseline
  2. Apply DSPy Chain of Thought prompting to the best-performing model from experiment 1
  3. Compare performance of English translation vs. direct processing for the same model and language

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model size correlate with multilingual performance across different task types (QA vs NER)?
- Basis in paper: [explicit] The paper explicitly states "Our results additionally revealed a nuanced relationship between model size and performance; raw parameter count isn't the sole determinant of multilingual and multi-task capabilities" and shows that Aya-23-35B and Aya-23-8B performed comparably in NER despite different sizes.
- Why unresolved: The study shows inconsistent patterns - in QA, larger models generally performed better, while in NER, size didn't correlate with performance. The paper doesn't provide a clear explanation for why this discrepancy exists between tasks.
- What evidence would resolve it: A systematic study comparing multiple models of varying sizes across many tasks and languages, controlling for other variables like architecture and training data, would clarify whether size affects different tasks differently.

### Open Question 2
- Question: What is the optimal prompting strategy for each language-task combination?
- Basis in paper: [explicit] The paper concludes "the choice of QA method should be tailored to the specific language and model combination for optimal results" and shows that translation effects varied significantly between tasks and models.
- Why unresolved: While the paper identifies that optimal strategies vary by language and task, it doesn't provide clear guidelines for selecting the best approach for any given language-task pair. The effectiveness of different strategies (DSPy CoT, DSPy ReAct, translation) varied unpredictably across combinations.
- What evidence would resolve it: A comprehensive mapping of which prompting strategies work best for each language-task pair, possibly derived from extensive cross-validation across multiple language-task combinations, would provide actionable guidance.

### Open Question 3
- Question: Why do translation effects differ between QA and NER tasks?
- Basis in paper: [explicit] The paper notes that in QA, translation effects varied widely with GPT-4-turbo often improving, while in NER, translation generally led to decreased performance, especially for GPT-4o and Claude-3.5-sonnet.
- Why unresolved: The paper observes this difference but doesn't explain the underlying reasons why translation helps in QA but hurts in NER. This could be due to differences in task complexity, entity recognition challenges, or how information is processed in each task type.
- What evidence would resolve it: Detailed error analysis comparing how models handle translated vs native text in both tasks, examining specific failure modes and success factors, would reveal why translation has opposite effects on these tasks.

## Limitations
- The study focuses on only five languages and two specific tasks, limiting generalizability to other multilingual scenarios
- Heavy reliance on translation to English may not capture language-specific nuances and could introduce errors
- Significant performance drops for lower-resource languages like Yoruba and Swiss German suggest limited effectiveness for truly low-resource settings

## Confidence
- **High Confidence**: The finding that advanced prompting techniques (particularly DSPy) improve QA performance across models and languages
- **Medium Confidence**: The observation that model size does not directly correlate with multilingual performance
- **Medium Confidence**: The claim that translation effects vary significantly between tasks

## Next Checks
1. Cross-validation with additional low-resource languages: Test the same prompting strategies on a broader set of low-resource languages (e.g., indigenous languages from different language families) to verify if the observed patterns hold across more diverse linguistic contexts.

2. Translation quality assessment: Implement human evaluation of translation quality for each language pair to quantify how much translation errors might be contributing to performance variations, particularly for languages like Yoruba and Swiss German where performance drops significantly.

3. Model architecture analysis: Conduct ablation studies comparing models with similar parameter counts but different architectures (e.g., transformer variants) to better understand which architectural features contribute most to multilingual performance beyond raw parameter count.