---
ver: rpa2
title: Domain-specific Question Answering with Hybrid Search
arxiv_id: '2412.03736'
source_url: https://arxiv.org/abs/2412.03736
tags:
- retrieval
- answer
- system
- hybrid
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses domain-specific question answering in enterprise
  settings, proposing a hybrid retrieval approach that combines fine-tuned dense retrieval
  with keyword-based sparse search methods. The system uses a linear combination of
  relevance signals including cosine similarity from dense retrieval, BM25 scores,
  and URL host matching, each with tunable boost parameters.
---

# Domain-specific Question Answering with Hybrid Search

## Quick Facts
- arXiv ID: 2412.03736
- Source URL: https://arxiv.org/abs/2412.03736
- Authors: Dewang Sultania, Zhaoyu Lu, Twisha Naik, Franck Dernoncourt, David Seunghyun Yoon, Sanat Sharma, Trung Bui, Ashok Gupta, Tushar Vatsa, Suhas Suresha, Ishita Verma, Vibha Belavadi, Cheng Chen, Michael Friedrich
- Reference count: 4
- One-line primary result: Hybrid dense-sparse retrieval with tunable parameters achieves nDCG 0.847 and robust contextual grounding for enterprise Q&A

## Executive Summary
This paper presents a hybrid retrieval system for domain-specific question answering in enterprise settings. The approach combines fine-tuned dense retrieval with keyword-based sparse search methods, integrating multiple relevance signals through tunable boost parameters. The system demonstrates significant improvements over single-retriever approaches while maintaining robust contextual grounding and resilience against inappropriate content through implemented guardrails.

## Method Summary
The system employs a linear combination of relevance signals including cosine similarity from dense retrieval, BM25 scores, and URL host matching, each with tunable boost parameters. It uses a fine-tuned dense retriever combined with keyword-based sparse search on enterprise documentation indexed in Elasticsearch. The approach optimizes retrieval performance through empirical parameter tuning and includes guardrails for content filtering. The hybrid method was evaluated on a golden dataset with question-answer pairs and negative queries, achieving strong performance metrics.

## Key Results
- Achieved nDCG score of 0.847 with optimal parameter configuration
- Answer similarity score of 0.780 and groundedness of 0.983 when evaluated against human-annotated ground truth
- Demonstrated effectiveness in handling domain-specific queries while maintaining resilience against inappropriate content
- Outperformed single-retriever approaches across multiple evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid retrieval combining dense and sparse methods outperforms single-method approaches.
- Mechanism: Dense retrieval captures semantic meaning while sparse retrieval ensures exact term matching, providing complementary signals that enhance overall retrieval accuracy.
- Core assumption: Dense and sparse retrieval capture orthogonal aspects of query-document relevance.
- Evidence anchors:
  - [abstract] "Our system leverages a linear combination of relevance signals, including cosine similarity from dense retrieval, BM25 scores, and URL host matching"
  - [section] "The integration of dense and sparse retrieval methods allows the system to leverage both semantic understanding and exact keyword matching"
  - [corpus] Weak - no direct corpus evidence for hybrid effectiveness

### Mechanism 2
- Claim: Host boosting improves retrieval quality by prioritizing authoritative sources.
- Mechanism: Documents from preferred URL hosts receive additional weight in the scoring function, enhancing the reliability of retrieved information.
- Core assumption: Documents from certain URL hosts are inherently more authoritative for domain-specific queries.
- Evidence anchors:
  - [abstract] "Our system leverages a linear combination of relevance signals, including... URL host matching, each with tunable boost parameters"
  - [section] "A host boost value of 0.1 provided the best trade-off between relevance and authority"
  - [corpus] Weak - no direct corpus evidence for host boosting effectiveness

### Mechanism 3
- Claim: Tunable boost parameters enable optimization for specific domain requirements.
- Mechanism: Linear combination of relevance signals with adjustable weights allows fine-tuning of retrieval performance based on empirical validation.
- Core assumption: Optimal parameter values can be determined through iterative tuning on validation datasets.
- Evidence anchors:
  - [section] "The values for bm25 boost and host boost were empirically determined by optimizing for the highest average similarity score between generated outputs and a predefined golden set"
  - [section] "Table 2 and Table- 3 summarize how we select these parameters"
  - [corpus] Weak - no direct corpus evidence for parameter tuning methodology

## Foundational Learning

- Concept: Dense retrieval with semantic embeddings
  - Why needed here: Captures meaning beyond exact keyword matches, crucial for domain-specific terminology
  - Quick check question: How does cosine similarity between query and document embeddings indicate relevance?

- Concept: Sparse retrieval with BM25
  - Why needed here: Ensures exact term matching is not overlooked, providing complementary signal to semantic matching
  - Quick check question: What role does term frequency play in BM25 scoring?

- Concept: Linear combination of multiple relevance signals
  - Why needed here: Allows integration of diverse retrieval signals with tunable weights for optimal performance
  - Quick check question: How do boost parameters affect the relative importance of different retrieval signals?

## Architecture Onboarding

- Component map:
  Elasticsearch index for document storage -> Dense retriever (fine-tuned model) -> BM25 sparse retriever -> Host-based relevance scorer -> Linear combination module -> Guardrail mechanism for content filtering -> LLM-based answer generator

- Critical path:
  1. Query receives host-based filtering
  2. Dense retriever generates semantic scores
  3. BM25 retriever generates keyword scores
  4. Host-based scorer generates authority scores
  5. Linear combination produces final ranking
  6. Top documents retrieved and passed to LLM
  7. Guardrails applied to generated response

- Design tradeoffs:
  - Chunk size vs. retrieval granularity (optimal: 1000 chars with 100 overlap)
  - Number of retrieval signals vs. computational efficiency
  - Host boosting strength vs. potential bias toward certain sources
  - Guardrail strictness vs. system usability

- Failure signatures:
  - High null rate on valid queries: Possible guardrail misconfiguration
  - Low answer similarity: Retrieval quality issues
  - Inconsistent results: Parameter tuning problems
  - Slow response times: Computational efficiency bottlenecks

- First 3 experiments:
  1. Validate chunk size optimization by testing retrieval performance with different chunk sizes (500, 1000, 2000 chars)
  2. Tune BM25 boost parameter by evaluating NDCG scores across range (0.1 to 1.0)
  3. Test host boosting effectiveness by comparing retrieval results with and without host-based scoring

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between BM25 and dense retrieval scores in the hybrid approach, and how does this vary across different domains or content types?
- Basis in paper: [explicit] The paper discusses empirical tuning of boost parameters (BM25 boost and host boost) to optimize performance, suggesting that the optimal balance may depend on specific use cases.
- Why unresolved: While the paper presents tuned values for their specific enterprise setting, it acknowledges that the hybrid approach needs to be adaptable for other enterprise applications, implying that optimal parameter settings may vary.
- What evidence would resolve it: Systematic experiments across multiple domains with varying content types and query distributions, demonstrating how optimal parameter combinations shift between different application contexts.

### Open Question 2
- Question: How does the hybrid retrieval system perform when scaled to handle significantly larger document collections and higher query volumes typical of enterprise deployments?
- Basis in paper: [inferred] The paper mentions the framework is "production-ready" and "built on Elasticsearch," suggesting scalability is a consideration, but doesn't provide performance data at enterprise scale.
- Why unresolved: The evaluation focuses on retrieval accuracy metrics rather than performance characteristics under load, leaving questions about latency, throughput, and resource utilization unanswered.
- What evidence would resolve it: Benchmark studies measuring retrieval latency, throughput, and resource usage across varying document collection sizes (10x, 100x current scale) and query rates, including impact on hybrid scoring computation.

### Open Question 3
- Question: How does the system handle multimodal queries that combine text with visual elements (screenshots, diagrams) common in technical documentation and support scenarios?
- Basis in paper: [explicit] The "Future Work" section explicitly mentions "Multimodal Enhancement" as a potential direction, noting that incorporating visual content through multimodal embedding models could significantly enhance the system's utility.
- Why unresolved: The current system appears to be text-only, and while the authors acknowledge this limitation, they don't explore how the hybrid retrieval approach might need to be modified to handle multimodal inputs effectively.
- What evidence would resolve it: Implementation and evaluation of a multimodal extension to the hybrid retrieval system, comparing performance on text-only versus multimodal queries, and demonstrating how the scoring algorithm would need to be adapted to incorporate visual relevance signals.

## Limitations

- Host boosting mechanism assumes URL authority correlates with document quality, which may not hold across all domains
- Parameter tuning relies on empirical optimization without cross-validation for generalizability
- Guardrail effectiveness evaluation is limited to a small negative dataset without assessing false positive rates

## Confidence

**High Confidence**: The hybrid retrieval approach combining dense and sparse methods improves performance over single-retriever approaches. This claim is supported by multiple evaluation metrics (nDCG 0.847, answer similarity 0.780, groundedness 0.983) and aligns with established retrieval literature.

**Medium Confidence**: The optimal parameter values (BM25 boost=0.3, host boost=0.1) represent the best configuration for the tested dataset. While empirically validated, these parameters may not generalize across different enterprise domains or document collections.

**Low Confidence**: The assertion that host boosting significantly improves retrieval quality without introducing bias. The evidence is limited to one dataset and doesn't explore potential negative impacts on diversity or coverage.

## Next Checks

1. **Cross-domain validation**: Test the hybrid approach on multiple enterprise domains (legal, medical, technical documentation) to assess parameter generalizability and host boosting effectiveness across different knowledge domains.

2. **False positive analysis**: Systematically evaluate guardrail performance by testing legitimate queries that might trigger content filters, measuring the false positive rate and its impact on system usability.

3. **Ablation study on retrieval signals**: Conduct controlled experiments removing each retrieval signal (dense, sparse, host-based) to quantify their individual contributions and identify potential redundancy in the hybrid approach.