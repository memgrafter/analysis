---
ver: rpa2
title: 'Sanity Checks Revisited: An Exploration to Repair the Model Parameter Randomisation
  Test'
arxiv_id: '2401.06465'
source_url: https://arxiv.org/abs/2401.06465
tags:
- mprt
- explanation
- randomisation
- emprt
- smprt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses methodological issues in the Model Parameter
  Randomisation Test (MPRT) for evaluating explainability methods in AI. The original
  MPRT suffers from layer-order effects and sensitivity to noise in similarity measurements.
---

# Sanity Checks Revisited: An Exploration to Repair the Model Parameter Randomisation Test

## Quick Facts
- **arXiv ID**: 2401.06465
- **Source URL**: https://arxiv.org/abs/2401.06465
- **Reference count**: 40
- **Primary result**: Two variants of the Model Parameter Randomisation Test (sMPRT and eMPRT) outperform the original MPRT in reliability metrics, though no variant achieves perfect reliability.

## Executive Summary
This paper addresses critical methodological issues in the Model Parameter Randomisation Test (MPRT) for evaluating explainability methods in AI. The original MPRT suffers from layer-order effects and sensitivity to noise in similarity measurements. The authors propose two variants: Smooth MPRT (sMPRT), which denoises attributions by averaging over perturbed inputs, and Efficient MPRT (eMPRT), which replaces pairwise similarity with a complexity measure based on histogram entropy. Experiments show both variants outperform the original MPRT in reliability metrics, with eMPRT being computationally more efficient. However, no variant achieves perfect reliability, highlighting the need for cautious application of XAI evaluation metrics.

## Method Summary
The paper proposes two modifications to the Model Parameter Randomisation Test (MPRT) for evaluating explanation methods. Smooth MPRT (sMPRT) introduces a denoising step by averaging attributions over multiple perturbed inputs before computing similarity scores. Efficient MPRT (eMPRT) replaces the pairwise similarity measurement with a complexity measure based on histogram entropy, eliminating layer-order effects. Both methods are implemented in the Quantus framework and evaluated using meta-consistency scores across multiple datasets (ImageNet, MNIST, fMNIST) and models (VGG-16, ResNet-18, LeNet) with various explanation methods.

## Key Results
- sMPRT reduces noise sensitivity through attribution averaging, improving meta-consistency scores
- eMPRT eliminates layer-order bias by using histogram entropy instead of pairwise similarity
- Both variants show superior performance compared to original MPRT, with eMPRT offering computational advantages
- No variant achieves perfect reliability, indicating fundamental challenges in XAI evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Denoising attributions through sampling reduces the impact of shattering noise, improving the reliability of explanation evaluation.
- Mechanism: By averaging attributions over multiple perturbed inputs, the random noise inherent in gradient-based methods is smoothed out, allowing the underlying signal to be more clearly evaluated.
- Core assumption: The noise in attributions is random and averages out over multiple samples, while the true attribution signal remains consistent.
- Evidence anchors:
  - [abstract]: "sMPRT minimises the impact that noise has on the evaluation results through sampling"
  - [section 3.1]: "Inspired by [21, 25] we introduce a preprocessing step to the evaluation procedure—given an input x, generate N perturbed instances xˆi, compute their attributions Φ(xˆi,f,y;λ) and then perform the MPRT evaluation on the averaged denoised attribution estimates."
- Break condition: If the noise in attributions is not random but systematic, averaging will not effectively remove it and may even obscure meaningful signal.

### Mechanism 2
- Claim: Replacing pairwise similarity with a complexity measure based on histogram entropy removes bias towards noisy explanation methods.
- Mechanism: By measuring the change in complexity of the explanation after full model randomization, rather than pairwise similarity, the evaluation becomes less sensitive to the noise characteristics of the explanation method.
- Core assumption: Faithful explanations should become more complex (higher entropy) when the model is fully randomized, as they lose information about the model's structure.
- Evidence anchors:
  - [abstract]: "Efficient MPRT (eMPRT), which replaces pairwise similarity with a complexity measure based on histogram entropy"
  - [section 3.2]: "We introduce eMPRT. This test effectively removes the layer-by-layer pairwise comparison between e and eˆ and instead compute the relative rise in explanation complexity using only two model states, i.e., the original- and fully randomised model."
- Break condition: If the relationship between model randomization and explanation complexity is not consistent across different model architectures or datasets, the eMPRT may not provide reliable evaluations.

### Mechanism 3
- Claim: Performing bottom-up randomization preserves more information in the forward pass, leading to more meaningful changes in explanations.
- Mechanism: By randomizing layers from the input to the output, bottom-up randomization ensures that the information flow through the network is more thoroughly disrupted, leading to greater changes in explanations.
- Core assumption: Top-down randomization preserves too much information from lower layers, limiting the degree of change in explanations and potentially favoring certain explanation methods.
- Evidence anchors:
  - [section 2.1.1]: "top-down randomisation induces only modest alterations in the forward pass where (i) irrelevant features from lower, non-randomised layers persist in higher, randomised layers"
  - [supplement 6.2]: "Since bottom-up layer-order randomises the lowest layers first, such preservation cannot occur."
- Break condition: If the preservation of information in the forward pass is not the primary factor affecting explanation changes, or if bottom-up randomization introduces other biases, the benefits may not be realized.

## Foundational Learning

- Concept: Model Parameter Randomization Test (MPRT)
  - Why needed here: Understanding the original MPRT is crucial for appreciating the proposed modifications and their motivations.
  - Quick check question: What is the key evaluative principle of the MPRT, and how does it measure the quality of explanation methods?

- Concept: Explanation methods and their properties
  - Why needed here: Familiarity with different explanation methods (e.g., gradients, saliency, LRP) and their characteristics (e.g., faithfulness, stability, complexity) is necessary to interpret the experimental results and understand the implications of the proposed modifications.
  - Quick check question: What are some common explanation methods, and how do they differ in terms of their approach and properties?

- Concept: Statistical similarity measures and complexity measures
  - Why needed here: Understanding the differences between pairwise similarity measures (e.g., SSIM, Spearman correlation) and complexity measures (e.g., entropy) is essential for grasping the rationale behind the proposed modifications and their potential impact on evaluation outcomes.
  - Quick check question: How do pairwise similarity measures and complexity measures differ in terms of their sensitivity to noise and their ability to capture the faithfulness of explanations?

## Architecture Onboarding

- Component map:
  - Input: Original model, explanation method, dataset
  - MPRT: Top-down layer-wise randomization, pairwise similarity measure
  - sMPRT: Top-down layer-wise randomization, denoising via averaging over perturbed inputs, pairwise similarity measure
  - eMPRT: Full model randomization, complexity measure based on histogram entropy
  - Output: Evaluation score indicating explanation faithfulness

- Critical path:
  1. Choose explanation method and dataset
  2. Compute original explanation
  3. For MPRT: Randomize layers top-down, compute randomized explanations, measure pairwise similarity
     For sMPRT: Generate perturbed inputs, compute denoised explanation, randomize layers top-down, measure pairwise similarity
     For eMPRT: Randomize full model, compute randomized explanation, measure complexity change
  4. Aggregate results and compare across explanation methods

- Design tradeoffs:
  - MPRT vs. sMPRT: sMPRT reduces noise sensitivity but increases computational cost due to multiple perturbed inputs
  - MPRT vs. eMPRT: eMPRT avoids layer-order effects and similarity measure biases but may be less sensitive to fine-grained changes in explanations
  - Bottom-up vs. top-down randomization: Bottom-up randomization preserves less information but may introduce other biases

- Failure signatures:
  - High variance in evaluation scores across different random seeds or perturbations
  - Consistent ranking of explanation methods that contradicts other evaluation metrics or human judgment
  - Evaluation scores that do not correlate with known properties of explanation methods (e.g., noise levels, faithfulness to model)

- First 3 experiments:
  1. Implement and compare MPRT, sMPRT, and eMPRT on a simple dataset (e.g., MNIST) with a few explanation methods (e.g., gradients, saliency, LRP)
  2. Investigate the effect of the number of perturbed inputs (N) on sMPRT results and identify a suitable value for balancing noise reduction and computational cost
  3. Compare bottom-up and top-down randomization in MPRT and analyze the impact on explanation changes and evaluation outcomes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design a layer randomisation strategy that consistently produces fully random outputs across different neural network architectures (e.g., ResNets with skip connections, VGGs)?
- Basis in paper: [explicit] "However, several caveats of MPRT have been identified (b)-(d)... Layer-order: top-down randomisation of layers in MPRT does not yield a fully random output, preserving properties of the unrandomised lower layers and thus affecting the evaluation of faithful explanations."
- Why unresolved: The paper identifies this as a key limitation of MPRT but only briefly discusses bottom-up randomisation as a potential solution without thorough empirical validation across diverse architectures.
- What evidence would resolve it: Systematic experiments comparing different layer randomisation strategies (top-down, bottom-up, random layer selection) across multiple architectures (ResNets, VGGs, Transformers) measuring their effectiveness at producing fully random outputs as judged by entropy metrics.

### Open Question 2
- Question: What is the optimal balance between denoising attribution methods (as in sMPRT) and preserving method-specific noise properties that might be important for certain applications?
- Basis in paper: [explicit] "sMPRT introduces hyperparameters σ and N, which may not be tunable on any given data domain... the degree of noisiness is an arguable property of attribution methods and removing it before evaluation may yield non-representative or biased results"
- Why unresolved: The paper acknowledges that denoising may remove important properties of attribution methods but doesn't provide guidance on when to preserve vs remove noise.
- What evidence would resolve it: Empirical studies comparing real-world application performance (e.g., debugging, safety-critical systems) using denoised vs non-denoised attributions across diverse domains and attribution methods.

### Open Question 3
- Question: Can we develop a unified evaluation metric that combines the strengths of sMPRT and eMPRT while mitigating their individual weaknesses?
- Basis in paper: [explicit] "Since both sMPRT and eMPRT employ fundamentally different mechanisms, a combination of both holds promise and will be subject to future work."
- Why unresolved: The paper proposes two separate solutions to different problems but doesn't explore their potential combination.
- What evidence would resolve it: Development and experimental validation of a hybrid metric that incorporates both denoising and complexity measurement, comparing its performance against the individual variants across the same benchmark tasks.

## Limitations

- No variant achieves perfect reliability, indicating fundamental limitations in the MPRT framework itself
- sMPRT introduces hyperparameters (σ and N) that may not be tunable on any given data domain
- The denoising mechanism in sMPRT may remove important noise properties that are relevant for certain applications

## Confidence

- High confidence in the identification of layer-order effects in original MPRT (supported by direct empirical evidence)
- Medium confidence in the denoising mechanism of sMPRT (theoretical justification but limited validation)
- Medium confidence in the complexity measure of eMPRT (novel approach with theoretical basis but requires further validation)

## Next Checks

1. Test sMPRT's denoising effectiveness by comparing attribution variance before and after averaging across different perturbation magnitudes
2. Validate eMPRT's complexity measure across diverse model architectures (not just VGG-16 and ResNet-18) to assess generalizability
3. Conduct ablation studies removing the denoising step in sMPRT to quantify its contribution to improved reliability