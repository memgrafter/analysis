---
ver: rpa2
title: What's in an embedding? Would a rose by any embedding smell as sweet?
arxiv_id: '2406.06870'
source_url: https://arxiv.org/abs/2406.06870
tags:
- representation
- circle
- knowledge
- llms
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores the limitations of Large Language Models (LLMs)
  in terms of true understanding and reasoning capabilities, suggesting that LLMs
  develop a kind of empirical "geometric" understanding rather than a comprehensive
  "algebraic" one. This geometric understanding, while useful for many applications,
  makes LLMs unreliable, difficult to generalize, and lacking in inference capabilities
  and explanations.
---

# What's in an embedding? Would a rose by any embedding smell as sweet?

## Quick Facts
- arXiv ID: 2406.06870
- Source URL: https://arxiv.org/abs/2406.06870
- Authors: Venkat Venkatasubramanian
- Reference count: 0
- One-line primary result: Proposes Large Knowledge Models (LKMs) by integrating symbolic AI with LLMs to overcome limitations in reasoning and explainability.

## Executive Summary
This paper argues that Large Language Models (LLMs) develop an empirical "geometric" understanding of knowledge through high-dimensional vector embeddings, which is fundamentally different from true "algebraic" (symbolic) reasoning. This geometric representation, while useful for many applications, makes LLMs unreliable, difficult to generalize, and lacking in inference capabilities and explanations. To address these limitations, the author proposes integrating LLMs with an "algebraic" representation of knowledge, incorporating symbolic AI elements used in expert systems, to create Large Knowledge Models (LKMs) that possess deep, first-principles-based knowledge, reasoning, and explanation capabilities.

## Method Summary
The paper proposes creating Large Knowledge Models (LKMs) by integrating LLMs with symbolic AI elements. The method involves combining the geometric knowledge representation of LLMs (high-dimensional vector embeddings) with algebraic representations (rules, ontologies, frames) from expert systems. The proposed hybrid architecture would use symbolic knowledge to provide deep, first-principles-based reasoning while maintaining the pattern-matching capabilities of LLMs. The integration would allow for better generalization, improved reliability, and enhanced explanation capabilities compared to pure LLMs.

## Key Results
- LLMs develop a "geometric" understanding of knowledge through vector embeddings rather than true "algebraic" reasoning
- The geometric representation is unreliable due to being built from incomplete and noisy data
- Integration of symbolic AI elements with LLMs could create Large Knowledge Models (LKMs) with human-like expert capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs develop an empirical "geometric" understanding of knowledge through high-dimensional vector embeddings, allowing them to perform pattern matching and retrieval-like reasoning without symbolic computation.
- Mechanism: High-dimensional vector embeddings encode semantic relationships as spatial proximity; the model "sees" patterns and associations directly, bypassing symbolic inference.
- Core assumption: Semantic meaning can be captured sufficiently by geometric relationships in embedding space for many tasks.
- Evidence anchors:
  - [abstract] "We suggest that LLMs do develop a kind of empirical 'understanding' that is 'geometry'-like, which seems adequate for a range of applications..."
  - [section III] "Could the LLMs be using a 'geometric' representation rather than an 'algebraic' one for their knowledge internally? Therefore, instead of 'reasoning', they merely 'look up' in their internal knowledge base to answer queries..."
  - [corpus] Weak - corpus contains only tangentially related titles; no direct support for geometric vs algebraic distinction.
- Break condition: If embedding geometry fails to capture critical semantic distinctions (e.g., in reasoning tasks requiring abstraction or symbolic manipulation).

### Mechanism 2
- Claim: The "geometric" representation is incomplete and unreliable because it is built from noisy, incomplete training data, limiting generalization and inference.
- Mechanism: Sparse or noisy embeddings approximate the underlying data manifold imperfectly, leading to stochastic outputs and brittleness when extrapolating beyond the training distribution.
- Core assumption: Real-world data is inherently incomplete and noisy, preventing perfect geometric reconstruction of knowledge.
- Evidence anchors:
  - [abstract] "...this 'geometric' understanding, built from incomplete and noisy data, makes them unreliable, difficult to generalize, and lacking in inference capabilities..."
  - [section III] "In the real world, the equivalent of the 'perfect circle' is not available in most practical situations... This results in LLMs developing a noisy 'geometric' approximation of the world."
  - [corpus] Weak - no corpus support for data incompleteness or reliability issues.
- Break condition: If training data becomes dense, clean, and comprehensive enough to eliminate noise and sparsity.

### Mechanism 3
- Claim: Integrating symbolic (algebraic) representations with LLMs can create Large Knowledge Models (LKMs) that combine geometric intuition with deep, principled reasoning and explanation.
- Mechanism: Symbolic knowledge encodes invariants, rules, and first principles; hybrid architectures fuse symbolic reasoning with embedding-based pattern matching to produce explainable, reliable outputs.
- Core assumption: Symbolic representations can be mapped or aligned with embedding geometry in a computationally tractable way.
- Evidence anchors:
  - [abstract] "...LLMs should be integrated with an 'algebraic' representation of knowledge that includes symbolic AI elements used in expert systems... mimicking human expert capabilities."
  - [section IV] "The importance of using such human expertise has become evident... ChatGPT uses human experts' guidance as reinforcement learning using human feedback (RLHF)."
  - [corpus] Weak - corpus lacks concrete examples of hybrid symbolic-embedding systems.
- Break condition: If symbolic and geometric components cannot be aligned without prohibitive computational cost or loss of performance.

## Foundational Learning

- Concept: Vector embedding geometry
  - Why needed here: The paper's core thesis is that LLMs use geometric representations; understanding high-dimensional vector spaces and similarity metrics is essential to grasp the argument.
  - Quick check question: What property of vector embeddings allows LLMs to "see" semantic relationships without explicit reasoning?

- Concept: Knowledge representation paradigms (symbolic vs. connectionist)
  - Why needed here: The proposed shift from LLM to LKM relies on combining symbolic (algebraic) and connectionist (geometric) approaches; knowing the history and tradeoffs is critical.
  - Quick check question: What are the main strengths and weaknesses of symbolic AI compared to neural network-based approaches?

- Concept: Emergent behavior in large-scale systems
  - Why needed here: The paper argues that scaling LLMs changes their qualitative behavior (e.g., from autocomplete to emergent reasoning); understanding emergence is key to evaluating the claims.
  - Quick check question: How does increasing model size and training data alter the capabilities and limitations of neural language models?

## Architecture Onboarding

- Component map:
  Embedding encoder (transformer backbone) -> high-dimensional semantic space
  Symbolic knowledge base (rules, ontologies, frames) -> structured reasoning module
  Fusion layer -> integrates geometric similarity with symbolic inference
  Output generator -> produces final response with explanations

- Critical path:
  1. Encode input via LLM to obtain embedding
  2. Retrieve relevant symbolic knowledge (e.g., via nearest-neighbor or rule matching)
  3. Perform symbolic reasoning on retrieved knowledge
  4. Combine symbolic and embedding outputs for final answer

- Design tradeoffs:
  - Complexity vs. explainability: Adding symbolic modules increases interpretability but may reduce fluency and scalability.
  - Data vs. knowledge: Reliance on curated symbolic knowledge reduces data requirements but increases development overhead.
  - Modularity vs. integration: Loose coupling preserves modularity but may limit seamless fusion; tight integration improves performance but risks brittleness.

- Failure signatures:
  - Symbolic module fails to find relevant rules -> fallback to embedding-only output (risk of hallucination)
  - Embedding similarity is too coarse -> irrelevant symbolic knowledge retrieved
  - Fusion logic produces contradictions -> output inconsistency or errors

- First 3 experiments:
  1. Benchmark LKM on a dataset requiring both factual recall and reasoning (e.g., scientific QA) vs. baseline LLM.
  2. Stress-test symbolic retrieval accuracy by perturbing embeddings (e.g., adding noise) and measuring rule-matching performance.
  3. Evaluate explainability by comparing human judgments of clarity and correctness between LKM and LLM outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise nature of the "geometric" vs. "algebraic" knowledge representations in LLMs, and how can they be empirically distinguished?
- Basis in paper: [explicit] The paper explicitly contrasts "geometric" (empirical, lookup-based) and "algebraic" (symbolic, first-principles-based) representations, using the circle example to illustrate the differences.
- Why unresolved: While the paper provides a conceptual framework, it does not offer a rigorous, testable definition of these representations in the context of LLMs. The distinction is largely intuitive and needs empirical validation.
- What evidence would resolve it: Empirical studies comparing LLM performance on tasks that require lookup-based reasoning versus those requiring symbolic manipulation and first-principles reasoning. Development of metrics to quantify the "geometric" vs. "algebraic" nature of LLM knowledge.

### Open Question 2
- Question: How can the integration of "algebraic" representations into LLMs be practically achieved to create Large Knowledge Models (LKMs)?
- Basis in paper: [explicit] The paper proposes integrating LLMs with symbolic AI elements used in expert systems to create LKMs with deep, first-principles-based knowledge and reasoning capabilities.
- Why unresolved: The paper outlines the need for integration but does not provide a concrete methodology or framework for how this can be practically implemented. The challenges of combining connectionist and symbolic approaches are not fully addressed.
- What evidence would resolve it: Development and demonstration of a hybrid AI system that successfully integrates LLMs with symbolic AI components, showing improved performance and reasoning capabilities on complex tasks compared to pure LLMs.

### Open Question 3
- Question: What are the emergent properties of ultra-large language models like GPT-3.5, and how do they differ qualitatively from smaller models like GPT-1?
- Basis in paper: [explicit] The paper suggests that ultra-large models have qualitatively different capabilities, drawing an analogy to the transition from gas to liquid when density increases. It references Anderson's "More is Different" principle.
- Why unresolved: While the paper posits that larger models have emergent properties, it does not provide a detailed analysis of what these properties are or how they manifest. The qualitative differences are asserted but not empirically demonstrated.
- What evidence would resolve it: Comparative studies of smaller and larger LLMs on a range of tasks, identifying specific capabilities that emerge only in larger models. Development of theoretical frameworks to explain these emergent properties and their relationship to model size.

## Limitations

- No empirical validation of the geometric vs algebraic representation distinction
- Lacks concrete technical specifications for implementing the proposed LKM architecture
- Does not clearly define the boundary between tasks suited for geometric versus symbolic approaches

## Confidence

- Low confidence: The claim that LLMs fundamentally use geometric rather than algebraic representations internally
- Medium confidence: The observation that LLMs struggle with reliability, generalization, and explanation
- Medium confidence: The suggestion that integrating symbolic knowledge could improve LLM capabilities

## Next Checks

1. Design experiments to test whether LLM performance degrades predictably when geometric relationships in embedding space are perturbed, supporting the geometric representation hypothesis.

2. Build a minimal LKM prototype with rule-based symbolic knowledge integrated with an LLM, then test on tasks requiring both pattern matching and logical reasoning.

3. Conduct systematic benchmarks comparing LLM, symbolic AI, and hybrid approaches across tasks requiring different types of reasoning (inductive, deductive, abductive) to validate the geometric vs algebraic distinction.