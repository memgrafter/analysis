---
ver: rpa2
title: 'LoRA-GA: Low-Rank Adaptation with Gradient Approximation'
arxiv_id: '2407.05000'
source_url: https://arxiv.org/abs/2407.05000
tags:
- lora
- lora-ga
- initialization
- fine-tuning
- dout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LoRA fine-tuning of large language models is expensive and converges
  slowly compared to full fine-tuning. LoRA-GA addresses this by initializing the
  LoRA adapters using eigenvectors of the gradient matrix to approximate the gradient
  direction of full fine-tuning from the first step.
---

# LoRA-GA: Low-Rank Adaptation with Gradient Approximation

## Quick Facts
- arXiv ID: 2407.05000
- Source URL: https://arxiv.org/abs/2407.05000
- Authors: Shaowen Wang; Linxi Yu; Jian Li
- Reference count: 40
- Primary result: Accelerates LoRA convergence 2-4x and matches full fine-tuning performance on GLUE and Llama 2-7B benchmarks

## Executive Summary
LoRA-GA addresses the slow convergence of LoRA fine-tuning by initializing adapter matrices using eigenvectors of the full gradient matrix, ensuring the first optimization step closely approximates full fine-tuning. The method also introduces a rank-aware scaling factor to maintain stable forward/backward scales during training. Extensive experiments demonstrate LoRA-GA converges 2-4x faster than vanilla LoRA and achieves superior or comparable performance to full fine-tuning across multiple benchmarks.

## Method Summary
LoRA-GA modifies the initialization of LoRA adapters by performing SVD on the full gradient matrix at initialization and setting the adapter matrices A and B to the corresponding eigenvectors. This ensures the low-rank update closely approximates the full weight update direction from the first step. A rank-aware scaling factor ζ is introduced to maintain stable forward and backward scales during training. The method requires one additional backward pass per layer to compute the gradient matrix, after which the model is fine-tuned using standard LoRA procedures.

## Key Results
- On T5-Base with GLUE, LoRA-GA outperforms vanilla LoRA by 5.69% average accuracy
- On Llama 2-7B, improves MT-bench by 0.34, GSM8K by 11.52%, and Human-eval by 5.05%
- Achieves 2-4x faster convergence compared to vanilla LoRA
- Matches or exceeds full fine-tuning performance while using significantly fewer parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LoRA-GA accelerates convergence by initializing LoRA adapters to match the gradient direction of full fine-tuning from the first step.
- **Mechanism**: LoRA-GA performs SVD on the full gradient matrix at initialization and sets the LoRA adapter matrices A and B to the corresponding eigenvectors. This ensures the low-rank update BA closely approximates the full weight update W in the first optimization step.
- **Core assumption**: The first-step update direction strongly influences the subsequent trajectory of optimization, so aligning it with full fine-tuning leads to faster convergence.
- **Evidence anchors**:
  - [abstract] "LoRA-GA achieves a convergence rate comparable to that of full fine-tuning (hence being significantly faster than vanilla LoRA as well as various recent improvements)"
  - [section 3.2] "Our goal is to ensure that the first-step update ∆(ηBA) approximate the direction of the weight update ∆W , i.e., ∆(ηBA) ≈ ζ∆W for some non-zero positive constant ζ."
  - [corpus] Weak: No direct empirical evidence in related papers; theoretical claim based on low-rank subspace hypothesis.
- **Break condition**: If the gradient subspace is not low-rank or if the first-step approximation doesn't transfer to later steps, the benefit diminishes.

### Mechanism 2
- **Claim**: LoRA-GA maintains stable forward/backward scales across different ranks and input dimensions via a rank-aware scaling factor.
- **Mechanism**: The scaling factor ζ is set to Θ(dout/r²) for forward stability and Θ(din/r²) for backward stability, ensuring that the variance of adapter outputs and gradients remains invariant to rank and input dimension as they grow large.
- **Core assumption**: Stable variance during forward and backward passes prevents exploding/vanishing gradients and maintains training dynamics similar to full fine-tuning.
- **Evidence anchors**:
  - [section 3.3] "an adapter ηBA exhibits two distinct types of scale stabilities: Forward stability... Backward stability..."
  - [section 3.3] "Under these conditions, the adapters are forward scale-stable if ζ = Θr,dout,din √(dout/r²) and backward scale-stable if ζ = Θr,dout,din √(din/r²)."
  - [corpus] Missing: No direct empirical verification in related works; theoretical derivation only.
- **Break condition**: If the rank is too low relative to input/output dimensions, the scaling may not preserve stability.

### Mechanism 3
- **Claim**: LoRA-GA initialization reduces the number of fine-tuning steps needed to reach target performance by up to 2-4x compared to vanilla LoRA.
- **Mechanism**: By aligning the initial gradient direction and stabilizing scales, LoRA-GA reduces the optimization "distance" to the target solution, leading to fewer required steps for convergence.
- **Core assumption**: Gradient alignment at initialization translates into a more direct optimization path, reducing total FLOPs.
- **Evidence anchors**:
  - [abstract] "Additionally, we observe up to 2-4 times convergence speed improvement compared to vanilla LoRA"
  - [section 1] "LoRA requires 5-6x more iterations and FLOPs to reach the same performance as full fine-tuning"
  - [corpus] Weak: Only comparative claims; no ablation on step count alone in related papers.
- **Break condition**: If the problem landscape changes (e.g., different tasks or model sizes), the benefit may not transfer.

## Foundational Learning

- **Concept**: Singular Value Decomposition (SVD) and its role in low-rank approximation
  - Why needed here: SVD is used to extract the dominant eigenvector directions from the full gradient matrix to initialize LoRA adapters.
  - Quick check question: Given a matrix M, what do the columns of U and V in its SVD M = UΣVᵀ represent?

- **Concept**: Gradient descent update rules and their dependence on initialization
  - Why needed here: Understanding how initialization affects the first gradient step and subsequent optimization trajectory.
  - Quick check question: If two models have the same gradient at initialization but different initial weights, will their first update steps be identical?

- **Concept**: Forward and backward variance stability in neural networks
  - Why needed here: Ensures that LoRA adapters don't introduce exploding/vanishing gradients during training.
  - Quick check question: What is the effect on training if the variance of layer outputs increases exponentially with depth?

## Architecture Onboarding

- **Component map**: Frozen base model -> LoRA adapters (A and B matrices initialized via SVD) -> Gradient computation -> Parameter update
- **Critical path**: (1) Sample a batch, (2) Forward pass through the model, (3) Compute full gradient for one layer, (4) Perform SVD on gradient, (5) Initialize A and B from eigenvectors, (6) Adjust base weights to keep initial output unchanged
- **Design tradeoffs**: (1) Higher rank improves gradient approximation but increases parameters; (2) Larger sampled batch improves gradient estimate but increases initialization time
- **Failure signatures**: (1) Training diverges → check scaling factor ζ; (2) No speedup → verify gradient alignment; (3) Memory overflow → reduce rank or batch size
- **First 3 experiments**:
  1. Compare training loss curves of LoRA-GA vs vanilla LoRA on a small dataset (e.g., CoLA) to verify faster convergence
  2. Test different rank values (e.g., 8, 32, 128) to see impact on performance and convergence speed
  3. Evaluate stability by training with different initialization seeds and measuring variance in final performance

## Open Questions the Paper Calls Out

- How does LoRA-GA perform on larger models (e.g., Llama 2-70B) compared to smaller ones?
- Is LoRA-GA's performance consistent across all benchmark datasets or only specific ones?
- How does LoRA-GA interact with other LoRA variants like ReLoRA when combined?
- Does LoRA-GA's initialization affect the model's ability to generalize to unseen data?
- What are the potential risks of LoRA-GA in generating misleading information, and how can they be mitigated?

## Limitations

- Theoretical mechanisms rely heavily on assumptions about gradient subspace structure that are not empirically validated
- Scaling factor derivation is mathematically sound but lacks empirical verification across diverse model architectures
- Performance comparisons against full fine-tuning are based on specific benchmarks that may not generalize to other domains
- Computational resource constraints prevented testing on larger models like Llama 2-70B
- Limited evaluation scope to specific datasets (GLUE, Llama 2-7B) without broader generalization testing

## Confidence

- Mechanism 1 (gradient direction initialization): Medium - theoretical justification exists but lacks empirical validation
- Mechanism 2 (scale stability): Low - primarily theoretical derivation without experimental verification
- Mechanism 3 (convergence speedup): Medium - comparative results shown but dependent on specific experimental conditions

## Next Checks

1. Verify gradient alignment empirically by comparing the cosine similarity between LoRA-GA and full fine-tuning gradient directions across multiple training steps
2. Test scale stability across different rank values and input/output dimensions to confirm the scaling factor maintains consistent variance
3. Reproduce convergence speedup on a third independent dataset (e.g., SuperGLUE) to validate generalization beyond the reported benchmarks