---
ver: rpa2
title: Optimizing Contextual Speech Recognition Using Vector Quantization for Efficient
  Retrieval
arxiv_id: '2411.00664'
source_url: https://arxiv.org/abs/2411.00664
tags:
- biasing
- retrieval
- proc
- contextual
- phrases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational limitations of neural contextual
  biasing (NCB) in speech recognition, particularly the scalability issues with large
  biasing catalogues. The authors propose a two-stage approach using finite scalar
  quantization (FSQ) to efficiently retrieve relevant biasing entries from large catalogues.
---

# Optimizing Contextual Speech Recognition Using Vector Quantization for Efficient Retrieval

## Quick Facts
- arXiv ID: 2411.00664
- Source URL: https://arxiv.org/abs/2411.00664
- Reference count: 40
- Up to 71% relative error rate reduction in personal entity recognition

## Executive Summary
This paper addresses computational limitations in neural contextual biasing (NCB) for speech recognition, particularly the scalability issues with large biasing catalogues. The authors propose a two-stage approach using finite scalar quantization (FSQ) to efficiently retrieve relevant biasing entries from large catalogues. In the first stage, FSQ discretizes contextual embeddings to enable efficient dot-product computation between audio queries and biasing keys. In the second stage, retrieved entries are used for biasing through either full cross-attention or LLM prompting. The method achieves significant improvements in efficiency while maintaining high accuracy for personal entity recognition tasks.

## Method Summary
The proposed system builds on a CTC-AED model with conformer-based acoustic encoder and transformer-based context encoder. The key innovation is using finite scalar quantization (FSQ) to discretize contextual embeddings, replacing high-dimensional floating-point vectors with small integer indices. This enables efficient retrieval from large biasing catalogues through pre-computed score matrices. The system operates in two stages: first using quantized retrieval to shortlist relevant biasing entries, then applying cross-attention or LLM prompting to the retrieved entries. Training involves freezing pre-trained NCB model parameters and training only the FSQ parameters for 100k updates.

## Key Results
- 71% relative error rate reduction in personal entity recognition
- 20% reduction in compute time compared to standard dot-product cross-attention
- 85-95% reduction in memory usage for catalogues of up to one million entries

## Why This Works (Mechanism)

### Mechanism 1
FSQ quantization drastically reduces memory requirements for biasing embeddings by replacing high-dimensional vectors with small integer indices. The paper uses Finite Scalar Quantization (FSQ) to map each dimension of contextual embeddings to a small set of discrete integer values. Instead of storing full D-dimensional floating-point vectors for each biasing phrase, each phrase is represented by G groups of L indices. This reduces memory from O(|B|D) to O(|B||L|G).

### Mechanism 2
Efficient dot-product computation through pre-computed score matrices enables fast retrieval from large biasing catalogues. Instead of computing T · |B| D-dimensional dot-products between queries and keys, the paper pre-computes a score matrix San that contains all possible dot-products between queries and quantized key representations. During retrieval, it uses index selection and sum reduction to approximate the original dot-product efficiently.

### Mechanism 3
Two-stage retrieval followed by dense cross-attention provides optimal balance between scalability and accuracy. The system first uses quantized retrieval to shortlist K relevant biasing phrases from the large catalogue, then applies full cross-attention only to these shortlisted phrases. This reduces the computational burden while maintaining high accuracy.

## Foundational Learning

- **Vector Quantization**
  - Why needed here: Provides the foundation for reducing memory footprint of biasing embeddings while maintaining retrieval accuracy
  - Quick check question: How does vector quantization reduce the dimensionality of data representation?

- **Cross-Attention Mechanism**
  - Why needed here: Forms the basis for integrating contextual information with acoustic features in neural contextual biasing
  - Quick check question: What is the computational complexity of standard cross-attention between T queries and |B| keys?

- **Retrieval-Based Approaches**
  - Why needed here: Enables efficient scaling to large biasing catalogues by first identifying relevant entries before applying expensive operations
  - Quick check question: How does retrieval-based filtering improve computational efficiency in large-scale systems?

## Architecture Onboarding

- **Component map**: Acoustic encoder -> FSQ-based retrieval module -> Cross-attention biasing -> ASR decoder
- **Critical path**: The quantized retrieval module is the bottleneck that determines system scalability
- **Design tradeoffs**: Memory reduction vs. retrieval accuracy, speed vs. precision in the quantized approximation
- **Failure signatures**: Poor retrieval accuracy (low recall), excessive quantization error leading to confusion between similar phrases
- **First 3 experiments**:
  1. Measure memory usage and retrieval accuracy for different FSQ configurations (G, L)
  2. Compare WER/NEER for quantized vs. non-quantized systems with varying catalogue sizes
  3. Evaluate runtime performance scaling with catalogue size for different TopK settings

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of groups (G) and levels (L) for finite scalar quantization in contextual speech recognition across different biasing catalogue sizes? The paper experiments with various G and L configurations and notes that performance improves significantly for G ≥ 16, but does not establish an optimal configuration or analyze scaling behavior.

### Open Question 2
How does the proposed vector quantization-based retrieval approach compare to alternative efficient attention mechanisms like flash attention or ring attention in terms of accuracy and computational efficiency? The paper mentions these approaches exist but focuses exclusively on vector quantization without comparative analysis.

### Open Question 3
How does LLM prompting performance change when the LLM is fine-tuned or adapted specifically for speech recognition tasks versus using pre-trained weights? The paper notes that using pre-trained LLM weights without adaptation yielded limited gains for contact NEER reduction compared to retrieval-based cross-attention.

## Limitations

- Quantization accuracy vs. computational gains trade-off remains unclear across different catalogue sizes
- Dataset generalization is limited to specific dictation and assistant data characteristics
- Real-time performance validation lacks empirical measurements under realistic deployment conditions

## Confidence

- **High Confidence**: The core mechanism of using finite scalar quantization to reduce memory footprint is technically sound
- **Medium Confidence**: The claimed WER improvement and computational benefits are supported by experiments but may not generalize to all scenarios
- **Low Confidence**: The assertion that the method scales effectively to "any size" catalogues lacks empirical support beyond tested range

## Next Checks

1. **Scalability Boundary Testing**: Evaluate system performance with progressively larger biasing catalogues (10M, 100M entries) to identify practical scalability limits and characterize degradation patterns.

2. **Cross-Domain Generalization Study**: Test the quantized retrieval system on diverse speech domains (medical, legal, technical) with varying entity distributions to assess robustness and identify domain-specific failure modes.

3. **End-to-End Latency Validation**: Measure real-time factor and end-to-end latency on target deployment hardware (CPU, GPU, edge devices) under streaming conditions to verify practical efficiency gains.