---
ver: rpa2
title: Anytime Continual Learning for Open Vocabulary Classification
arxiv_id: '2409.08518'
source_url: https://arxiv.org/abs/2409.08518
tags:
- learning
- training
- data
- continual
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes AnytimeCL, a method for anytime continual learning
  in open vocabulary image classification that enables efficient incremental updates
  and maintains open vocabulary performance. The core idea is to dynamically weight
  predictions between a partially fine-tuned model and a fixed open vocabulary model,
  using online class-wise weighting to improve accuracy for seen labels while preserving
  generality.
---

# Anytime Continual Learning for Open Vocabulary Classification

## Quick Facts
- arXiv ID: 2409.08518
- Source URL: https://arxiv.org/abs/2409.08518
- Reference count: 40
- The paper proposes AnytimeCL, a method for anytime continual learning in open vocabulary image classification that enables efficient incremental updates and maintains open vocabulary performance.

## Executive Summary
This paper addresses the challenge of anytime continual learning for open vocabulary image classification, where a model must predict over arbitrary label sets at any time while efficiently updating with new training samples. The proposed AnytimeCL method dynamically weights predictions between a partially fine-tuned model and a fixed open vocabulary model, using online class-wise weighting to improve accuracy for seen labels while preserving generality. The approach also introduces attention-weighted PCA compression of training features to reduce storage and computation with minimal accuracy loss. Experiments show AnytimeCL outperforms previous state-of-the-art methods across task, class, and data incremental learning scenarios.

## Method Summary
AnytimeCL enables continual learning by partially fine-tuning the last transformer block of CLIP while keeping label embeddings fixed, allowing efficient online updates while maintaining open vocabulary capability. The method uses online class-wise weighting (OCW) to dynamically combine predictions from the tuned and original models based on per-label accuracy estimates. Additionally, it employs attention-weighted PCA compression to reduce storage requirements by compressing intermediate features with minimal accuracy loss. The system processes training examples sequentially, maintaining a compressed feature store for class-balanced sampling during online updates.

## Key Results
- AnytimeCL achieves 88.1% average accuracy across task, class, and data incremental settings on 8 benchmark datasets
- Outperforms state-of-the-art methods by 2.4% average accuracy while being 5.4x more efficient in storage
- Maintains strong performance in anytime inference settings with varying computational budgets per sample

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partial fine-tuning of the last transformer block while keeping label embeddings fixed preserves generality and enables efficient online learning.
- Mechanism: By fine-tuning only the final block, the model adapts to new data while retaining the broader feature space learned during pre-training. Fixed label embeddings maintain compatibility with the original open vocabulary capability.
- Core assumption: The last transformer block is the primary source of task-specific adaptation, and earlier layers contain general visual features.
- Evidence anchors:
  - [abstract] "We propose a dynamic weighting between predictions of a partially fine-tuned model and a fixed open vocabulary model that enables continual improvement when training samples are available for a subset of a task's labels."
  - [section] "We tune only the last image transformer block while keeping the label embedding fixed, which helps the features to stay correlated with the text modality and reduces overfitting to received labels."
- Break condition: If the task requires significant feature transformation beyond what the last block can provide, this approach may fail to capture necessary representations.

### Mechanism 2
- Claim: Online class-wise weighting (OCW) dynamically combines predictions from tuned and original models based on per-label accuracy estimates.
- Mechanism: The method maintains running estimates of each model's accuracy for every label using exponential moving averages. Predictions are weighted according to these accuracy estimates, giving more weight to the model more likely to be correct for each specific label.
- Core assumption: The relative accuracy of tuned vs. original models for each label can be reliably estimated online without a held-out validation set.
- Evidence anchors:
  - [abstract] "We propose a dynamic weighting between predictions of a partially fine-tuned model and a fixed open vocabulary model that enables continual improvement when training samples are available for a subset of a task's labels."
  - [section] "The tuned and original model's predictions are then weighted in proportion to their expected accuracy for each label."
- Break condition: If the accuracy estimation becomes unstable due to label distribution shifts or insufficient data, the weighting may become unreliable.

### Mechanism 3
- Claim: Attention-weighted PCA compression of intermediate features reduces storage and computation while maintaining accuracy.
- Mechanism: PCA is applied to the feature vectors within each image, weighted by attention scores to prioritize important tokens. This compresses 50 tokens down to 5 components with minimal accuracy loss.
- Core assumption: Feature vectors within an image contain redundancies that can be compressed without losing discriminative information, and attention weights can identify the most important tokens.
- Evidence anchors:
  - [section] "We apply a per-image weighted PCA, which provides a 30x reduction of data with little impact on prediction accuracy."
- Break condition: If the attention mechanism fails to identify truly important tokens, or if the compressed representation loses critical information for certain classes.

## Foundational Learning

- Concept: Continual learning and catastrophic forgetting
  - Why needed here: The system must learn from new data without forgetting previously learned information, which is central to anytime continual learning.
  - Quick check question: What happens to model performance on old tasks when training on new data without any mitigation strategies?

- Concept: Open vocabulary classification
  - Why needed here: The system must predict over arbitrary label sets at any time, requiring compatibility with a continuous label embedding space rather than discrete class indices.
  - Quick check question: How does the model handle predicting labels that were never seen during training?

- Concept: Feature extraction and transformer architectures
  - Why needed here: Understanding how visual features are extracted and processed through transformer blocks is essential for implementing the partial fine-tuning approach.
  - Quick check question: What role does the CLS token play in CLIP's classification pipeline?

## Architecture Onboarding

- Component map: Image → Encoder → Both decoders → Predictions → OCW weighting → Final output
- Critical path: Image → Encoder → Both decoders → Predictions → OCW weighting → Final output
- Design tradeoffs:
  - Partial vs. full fine-tuning: Partial is faster and maintains generality but may be less effective for task-specific features
  - Storage vs. computation: Storing compressed features saves space but requires decompression during training
  - Accuracy estimation vs. computation: OCW provides good estimates but requires maintaining state for each label
- Failure signatures:
  - Degraded performance on novel labels: Indicates OCW weighting is not properly identifying when to use original model
  - Slow incorporation of new classes: Suggests partial fine-tuning is insufficient for rapid adaptation
  - Memory issues: Indicates compression ratio is too aggressive or feature reconstruction is failing
- First 3 experiments:
  1. Test partial fine-tuning on a single new class with varying learning rates to find optimal balance between adaptation and stability
  2. Evaluate OCW weighting effectiveness by comparing with fixed weighting schemes on a dataset with clear seen/novel label separation
  3. Measure compression accuracy trade-off by varying PCA components and measuring impact on fine-tuning performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AnytimeCL scale when applied to multi-modal tasks such as semantic segmentation or visual question answering?
- Basis in paper: [inferred] The paper mentions that the partial fine-tuning approach is applicable to other tasks like semantic segmentation, visual question answering, and object detection.
- Why unresolved: The experiments in the paper are limited to image classification tasks, and there is no empirical evidence or results demonstrating the effectiveness of AnytimeCL on multi-modal tasks.
- What evidence would resolve it: Conducting experiments on multi-modal tasks like semantic segmentation or visual question answering to evaluate the performance of AnytimeCL compared to existing methods would provide insights into its scalability and effectiveness.

### Open Question 2
- Question: What is the impact of varying the number of PCA components used in the compression method on the trade-off between storage efficiency and model accuracy?
- Basis in paper: [explicit] The paper discusses the use of per-instance PCA compression with different numbers of components and mentions that using 5 components maintains good accuracy.
- Why unresolved: While the paper provides results for using 3, 5, 10, and 20 components, it does not explore the full range of possible components or provide a comprehensive analysis of how the number of components affects the balance between storage efficiency and accuracy.
- What evidence would resolve it: Conducting a detailed study with a wider range of PCA components to analyze the trade-offs between storage efficiency and accuracy would help in understanding the optimal number of components for different scenarios.

### Open Question 3
- Question: How does the performance of AnytimeCL change when applied in a federated learning setting with multiple clients?
- Basis in paper: [inferred] The paper mentions that combining tree-based clustering and feature compression can enable fully distributed training on inexpensive nodes, suggesting potential applicability in federated learning.
- Why unresolved: The paper does not provide any experimental results or analysis of AnytimeCL in a federated learning context, leaving the effectiveness and challenges of such an application unexplored.
- What evidence would resolve it: Implementing and testing AnytimeCL in a federated learning environment with multiple clients would provide insights into its performance, scalability, and any potential challenges or limitations in such a setting.

## Limitations
- The online accuracy estimation mechanism (OCW) relies on stable label distributions and sufficient samples per class to maintain reliable estimates.
- The compression approach using attention-weighted PCA shows 30x reduction with minimal accuracy loss, but the evaluation focuses primarily on CLIP-based methods.
- The generalizability of the compression strategy to other backbone architectures or feature types remains untested.

## Confidence
- **High Confidence**: The core mechanism of partial fine-tuning the last transformer block while maintaining fixed label embeddings is well-grounded in the literature on continual learning and catastrophic forgetting prevention.
- **Medium Confidence**: The OCW weighting scheme shows strong performance gains in the reported experiments, but the sensitivity to hyperparameters like the EMA decay rate needs more rigorous testing.
- **Medium Confidence**: The compression efficiency claims are supported by the reported experiments, but the evaluation could benefit from more systematic ablation studies on different compression ratios.

## Next Checks
1. Test OCW performance when the label distribution changes significantly between incremental steps, particularly in scenarios with imbalanced class frequencies or sudden appearance of novel classes.
2. Systematically measure the trade-off between compression ratio and accuracy across different backbone architectures and task types to establish practical limits of the attention-weighted PCA approach.
3. Conduct extensive experiments on the anytime inference setting with varying amounts of computational budget per sample to validate the claimed efficiency benefits in real-world deployment scenarios.