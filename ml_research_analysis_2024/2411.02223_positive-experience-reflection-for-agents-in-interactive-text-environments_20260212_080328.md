---
ver: rpa2
title: Positive Experience Reflection for Agents in Interactive Text Environments
arxiv_id: '2411.02223'
source_url: https://arxiv.org/abs/2411.02223
tags:
- agent
- action
- agents
- reflection
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitations of self-reflection mechanisms
  in LLM-based agents for text-based games, particularly poor performance when initially
  successful and reduced effectiveness with smaller models. The authors propose Sweet&Sour,
  a novel reflection technique that incorporates both positive and negative experiences
  to enhance the agent's context during decision-making.
---

# Positive Experience Reflection for Agents in Interactive Text Environments

## Quick Facts
- arXiv ID: 2411.02223
- Source URL: https://arxiv.org/abs/2411.02223
- Reference count: 40
- Primary result: Sweet&Sour achieves 54.6 success score with GPT-4o on ScienceWorld, outperforming Reflexion (36.0) by incorporating positive experience reflection

## Executive Summary
This paper addresses a critical limitation in LLM-based agents for text-based games: poor performance when initially successful and reduced effectiveness with smaller models. The authors propose Sweet&Sour, a novel reflection technique that combines positive and negative experiences to enhance agent context during decision-making. By reflecting on successful actions and reinforcing effective strategies alongside learning from failures, Sweet&Sour improves agent adaptability and reasoning across different LLM sizes.

## Method Summary
Sweet&Sour implements a managed memory approach with dual buffers (short-term and long-term) to store reflections from both successful and failed attempts. The method triggers positive reflections after sub-goal completion, where agents articulate what made their actions successful, while negative reflections occur after failures. These reflections are stored in appropriate memory buffers and used as additional context for future decisions. The approach is evaluated on the ScienceWorld benchmark using various LLM sizes including GPT-4o, Mistral Large 2, and Llama 3.1 8B.

## Key Results
- Sweet&Sour achieves 54.6 success score with GPT-4o on ScienceWorld compared to 36.0 for Reflexion
- Performance gap widens for smaller models: 32.5 (Sweet&Sour) vs 21.7 (Reflexion) on Llama 8B
- Method demonstrates particular effectiveness in resource-constrained scenarios with smaller LLMs
- Sweet&Sour reduces "tilt" behavior in medium-difficulty tasks by reinforcing successful strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Positive experience reflection reduces "tilt" in medium-difficulty tasks by reinforcing successful strategies.
- Mechanism: When agents complete sub-goals, they reflect on what made their actions successful and store these reflections in short-term memory, which is later moved to long-term memory.
- Core assumption: Agents can articulate why their actions were successful and extract generalizable patterns from these experiences.
- Evidence anchors:
  - [abstract] "By reflecting on successful actions and reinforcing effective strategies alongside learning from failures, Sweet&Sour improves agent adaptability and reasoning."
  - [section] "When the current policy is achieving rewards, we query the agent to extrapolate from it, encouraging the agent to verbalize what made its current policy successful and what can be generalized from this."
- Break condition: If agents cannot identify what made their actions successful, or if the reflections are too task-specific to generalize.

### Mechanism 2
- Claim: Managed memory with dual buffers improves retrieval of relevant reflections for decision-making.
- Mechanism: Short-term memory stores recent successful reflections, while long-term memory stores all reflections (both successful and failed attempts).
- Core assumption: The dual-buffer structure enables more efficient retrieval of contextually appropriate reflections than a single memory system.
- Evidence anchors:
  - [abstract] "The method uses a managed memory approach with dual buffers (short-term and long-term) to store and retrieve relevant reflections."
  - [section] "This is implemented using a dual-buffer structure, where experiences are stored in two categories: short-term memory and long-term memory, based on their outcome (success or failure) and recency."
- Break condition: If the retrieval mechanism cannot effectively match current context with stored reflections, or if the memory becomes too large to manage efficiently.

### Mechanism 3
- Claim: Positive and negative experience reflection is particularly beneficial for smaller LLMs that lack inherent task-solving capabilities.
- Mechanism: Smaller LLMs benefit more from explicit reflection because they have less built-in knowledge to draw upon.
- Core assumption: Smaller models require more explicit guidance through reflection to achieve comparable performance to larger models.
- Evidence anchors:
  - [abstract] "The performance gap widens for smaller models, with Sweet&Sour scoring 32.5 on Llama 8B versus 21.7 for Reflexion, demonstrating its effectiveness particularly in resource-constrained scenarios."
  - [section] "The performance gap between Sweet&Sour and the other methods widens for smaller models with a lower parameter count."
- Break condition: If the reflection mechanism becomes too verbose or complex for smaller models to process effectively.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: Text-based games are modeled as POMDPs where the agent only observes partial information about the environment state through natural language descriptions.
  - Quick check question: Why can't the agent directly observe the complete game state in a text-based game?

- Concept: Reinforcement Learning with Sparse Rewards
  - Why needed here: ScienceWorld provides sparse rewards only when sub-goals are completed, requiring the agent to learn from both positive and negative experiences to navigate the large search space effectively.
  - Quick check question: How does sparse reward structure affect the learning process compared to dense reward structures?

- Concept: Self-Reflection and Meta-Learning
  - Why needed here: The agent must learn not just to solve tasks but to improve its own problem-solving strategy through reflection on past experiences.
  - Quick check question: What distinguishes self-reflection from standard supervised learning in this context?

## Architecture Onboarding

- Component map: Environment (ScienceWorld) -> Agent Core (LLM-based actor) -> Reflection Module -> Managed Memory (dual-buffer) -> Evaluation System
- Critical path:
  1. Agent observes environment state
  2. Agent generates action using current policy
  3. Environment responds with new observation and reward
  4. If sub-goal reached or attempt ends, trigger reflection
  5. Store reflection in appropriate memory buffer
  6. Use stored reflections as additional context for future decisions
- Design tradeoffs:
  - Memory vs. Performance: Larger memory stores more reflections but increases retrieval complexity
  - Reflection Frequency vs. Efficiency: More frequent reflection provides better learning but slows down decision-making
  - Model Size vs. Reflection Quality: Smaller models benefit more from reflection but may struggle with complex reflection prompts
- Failure signatures:
  - Poor performance on medium-difficulty tasks (anti-tilt behavior)
  - Large performance gap between model sizes without reflection
  - Reflection prompts that are too vague or too specific to be useful
- First 3 experiments:
  1. Run Sweet&Sour with only positive reflections (no negative) to measure impact of success reinforcement
  2. Run with only negative reflections (baseline Reflexion) to establish comparison point
  3. Run with no reflection at all (ReAct baseline) to measure overall reflection benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Sweet&Sour method's performance scale with increasingly larger and more complex environments beyond ScienceWorld?
- Basis in paper: [inferred] The paper mentions that the evaluation is conducted using a single environment (ScienceWorld) which, while comprehensive, does not cover all types of interactive scenarios. The authors state they leave the exploration of additional environments to future work.
- Why unresolved: The current evaluation is limited to a single benchmark, ScienceWorld, which may not represent the full diversity and complexity of text-based game environments.
- What evidence would resolve it: Testing Sweet&Sour across multiple diverse text-based game benchmarks with varying complexity, size, and domain specificity would demonstrate its scalability and generalizability.

### Open Question 2
- Question: What is the optimal balance between positive and negative reflections in the Sweet&Sour method for different types of tasks or LLM sizes?
- Basis in paper: [explicit] The paper mentions that when they modify their method to only sample from failures, performance drops significantly. This suggests that positive reflections contribute significantly to performance, but the optimal ratio is not investigated.
- Why unresolved: The paper does not explore how the balance between positive and negative reflections affects performance across different task types or model sizes.
- What evidence would resolve it: Systematic ablation studies varying the ratio of positive to negative reflections across different task categories and LLM sizes would reveal optimal configurations.

### Open Question 3
- Question: How does the managed memory approach in Sweet&Sour compare to alternative memory architectures like external knowledge bases or episodic memory systems?
- Basis in paper: [explicit] The paper introduces a dual-buffer memory system (short-term and long-term) as part of Sweet&Sour, but acknowledges that other self-reflection methods focus on learning from failures and store reflections in something akin to long-term memory. The authors leave the study of these additional use cases for future work.
- Why unresolved: The paper does not compare the proposed dual-buffer memory system against alternative memory architectures that could potentially offer better retrieval, organization, or generalization of reflections.
- What evidence would resolve it: Comparative experiments testing Sweet&Sour's memory system against alternatives like external knowledge bases, episodic memory with retrieval mechanisms, or hierarchical memory structures would reveal relative strengths and weaknesses.

## Limitations
- Evaluation relies entirely on the ScienceWorld benchmark, limiting generalizability to broader text-based game environments
- Dual-buffer memory system implementation details are not fully specified, particularly reflection prioritization during retrieval
- Reflection quality depends heavily on agent's ability to articulate why actions were successful, which varies across LLM architectures

## Confidence
- High Confidence: Comparative performance improvements of Sweet&Sour over baseline methods are well-supported by reported success scores across multiple model sizes
- Medium Confidence: Claim that positive experience reflection specifically addresses "tilt" behavior in medium-difficulty tasks requires more direct evidence
- Low Confidence: Mechanism explaining why smaller models benefit more from positive reflections is largely theoretical with limited empirical evidence

## Next Checks
1. Test Sweet&Sour performance on additional text-based game benchmarks beyond ScienceWorld to evaluate generalizability
2. Conduct ablation studies isolating the impact of positive versus negative reflections to quantify their individual contributions
3. Implement the dual-buffer memory system with different reflection prioritization strategies to assess sensitivity to memory management choices