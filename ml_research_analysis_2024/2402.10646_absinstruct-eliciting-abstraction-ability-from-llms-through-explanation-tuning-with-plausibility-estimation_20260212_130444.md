---
ver: rpa2
title: 'AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning
  with Plausibility Estimation'
arxiv_id: '2402.10646'
source_url: https://arxiv.org/abs/2402.10646
tags:
- framework
- abstraction
- llms
- llama2
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ABSINSTRUCT, a framework that improves large
  language models' abstraction ability by using explanation traces and a plausibility
  estimator to select better training data. The framework builds instructions for
  abstraction detection with detailed rationales and combines them with general-purpose
  instructions.
---

# AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation

## Quick Facts
- **arXiv ID**: 2402.10646
- **Source URL**: https://arxiv.org/abs/2402.10646
- **Reference count**: 40
- **Primary result**: 6-10% performance gains on ABSPYRAMID dataset

## Executive Summary
This paper introduces ABSINSTRUCT, a framework that improves large language models' abstraction ability by using explanation traces and a plausibility estimator to select better training data. The framework builds instructions for abstraction detection with detailed rationales and combines them with general-purpose instructions. It was evaluated on ABSPYRAMID and out-of-domain datasets, showing 6-10% performance gains over baselines. The approach maintains general instruction-following ability while significantly enhancing abstraction detection.

## Method Summary
ABSINSTRUCT improves abstraction ability through instruction tuning by incorporating explanation traces and plausibility estimation. The framework first collects explanation traces for each example using GPT-4, then designs a plausibility estimator to select instruction data consistent with LLM knowledge. The selected data is combined with general-purpose instructions from datasets like Alpaca. The method was tested on multiple pre-trained LLMs including Llama2, Mistral, Falcon, and MPT models of various sizes. Evaluation metrics include accuracy and Macro F1-score on ABSPYRAMID test set, out-of-domain performance on AbstractATOMIC and Levy/Holt datasets, and instruction-following ability on SuperNI and SELF-INSTRUCT benchmarks.

## Key Results
- 6-10% performance gains on ABSPYRAMID test set compared to baselines
- Improved out-of-domain generalization on AbstractATOMIC and Levy/Holt datasets
- Maintained general instruction-following ability on SuperNI and SELF-INSTRUCT benchmarks

## Why This Works (Mechanism)
The framework leverages explanation traces to provide rich rationales for abstraction detection, which are then filtered through a plausibility estimator to ensure alignment with the model's knowledge. This dual approach of explanation and selection helps the model learn more effective abstraction patterns while avoiding contradictory or implausible examples. The combination of specialized abstraction instructions with general-purpose instructions ensures the model maintains broad instruction-following capabilities.

## Foundational Learning
- **Abstraction detection**: Identifying abstract relationships in text is crucial for tasks like summarization and reasoning. Quick check: Test model on basic abstraction tasks before and after ABSINSTRUCT training.
- **Explanation traces**: Detailed reasoning steps help models understand complex concepts. Quick check: Compare model performance with and without explanation traces in training data.
- **Plausibility estimation**: Filtering training data to match model knowledge prevents confusion and improves learning efficiency. Quick check: Measure performance impact of different plausibility thresholds.
- **Instruction tuning**: Adapting pre-trained models to follow specific instructions improves task performance. Quick check: Evaluate general instruction-following ability before and after ABSINSTRUCT training.

## Architecture Onboarding
**Component Map**: Pre-trained LLM -> Explanation Trace Generator -> Plausibility Estimator -> Instruction Tuner -> Fine-tuned LLM

**Critical Path**: The critical path flows from generating explanation traces using GPT-4, through the plausibility estimator that filters these traces, to the instruction tuner that combines selected traces with general instructions for final model training.

**Design Tradeoffs**: The framework trades increased computational cost (due to GPT-4 trace generation) for improved abstraction ability. Using 200 examples per relation balances abstraction gains against general-domain performance.

**Failure Signatures**: Poor performance may result from low-quality explanation traces, overly restrictive plausibility thresholds, or insufficient diversity in training data.

**First Experiments**:
1. Test different numbers of explanation traces per relation (50-250) to find optimal tradeoff
2. Evaluate the impact of explanation traces on a subset of the data
3. Compare performance with different plausibility estimation methods

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: What is the optimal number of explanation traces to collect per abstraction relation to balance performance gains and computational costs?
- **Basis in paper**: [inferred] The paper tests 200 examples per relation but notes "tradeoff between abstraction ability and general-domain ability" and suggests this number balances these factors.
- **Why unresolved**: The paper only tests 200 examples and briefly mentions 50-250 range. No systematic study of optimal K values across different model sizes or abstraction types.
- **What evidence would resolve it**: A comprehensive ablation study varying K from 50-500 examples per relation, measuring both abstraction performance and general instruction-following ability, would identify optimal trade-offs.

### Open Question 2
- **Question**: How do explanation traces from different LLM sources affect the plausibility estimator's effectiveness?
- **Basis in paper**: [inferred] The paper uses GPT-4 for generating explanation traces but doesn't explore alternative sources.
- **Why unresolved**: Only GPT-4 traces are tested; no comparison with traces from other models or human annotators.
- **What evidence would resolve it**: Comparative studies using explanation traces from multiple sources (different LLMs, humans) would reveal source-specific impacts on performance.

## Limitations
- Reliance on GPT-4 for explanation traces introduces potential biases and computational costs
- Limited out-of-domain generalization testing (only two additional datasets)
- Computational cost of generating explanation traces may be prohibitive for large-scale applications

## Confidence
- **High confidence**: Performance improvements on ABSPYRAMID dataset (6-10% gains verified through direct comparison with baselines)
- **Medium confidence**: Out-of-domain generalization (limited to two datasets, results positive but not comprehensive)
- **Medium confidence**: Maintenance of general instruction-following ability (tested on specific benchmarks but broader generalization unclear)

## Next Checks
1. Evaluate the framework on a more diverse set of abstraction tasks and domains to better assess generalization capabilities
2. Conduct ablation studies to isolate the specific contribution of the plausibility estimator versus the explanation traces
3. Test the computational efficiency and scalability of the approach with larger datasets and more complex models