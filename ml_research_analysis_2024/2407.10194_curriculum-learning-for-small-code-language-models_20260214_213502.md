---
ver: rpa2
title: Curriculum Learning for Small Code Language Models
arxiv_id: '2407.10194'
source_url: https://arxiv.org/abs/2407.10194
tags:
- code
- learning
- curriculum
- language
- difficulty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether curriculum learning can improve
  the performance of small code language models. The authors propose a novel code
  difficulty metric combining software complexity measures, and use it to categorize
  Python code snippets into easy, medium, and hard levels.
---

# Curriculum Learning for Small Code Language Models

## Quick Facts
- arXiv ID: 2407.10194
- Source URL: https://arxiv.org/abs/2407.10194
- Reference count: 15
- Key outcome: Hybrid curriculum learning achieves 79.23% accuracy on code execution tasks, outperforming baseline's 74.58%

## Executive Summary
This paper investigates whether curriculum learning can improve the performance of small code language models. The authors propose a novel code difficulty metric combining software complexity measures to categorize Python code snippets into easy, medium, and hard levels. They train GPT models with 1 million parameters using various curriculum learning schedules and evaluate them on code completion and execution tasks. The results demonstrate that a well-designed curriculum learning approach significantly improves accuracy on code execution tasks, while its effect on code completion is less pronounced.

## Method Summary
The authors propose a code difficulty metric that combines software complexity measures to categorize Python code snippets into difficulty levels. They train GPT models with 1 million parameters using various curriculum learning schedules, where training data is presented in increasing order of difficulty. The study evaluates these models on code completion and execution tasks, comparing performance against a baseline model trained on randomly ordered data. The curriculum schedules include fixed, adaptive, and hybrid approaches to determine the most effective training strategy.

## Key Results
- Hybrid curriculum learning schedule achieves 79.23% output accuracy on code execution tasks, outperforming baseline's 74.58%
- Curriculum learning shows significant improvement for code execution but less pronounced effects on code completion tasks
- The proposed code difficulty metric successfully categorizes Python code snippets for curriculum learning

## Why This Works (Mechanism)
The paper does not explicitly discuss the mechanism behind why curriculum learning works for small code language models. However, the approach leverages the principle that learning from easier examples first can help models build foundational understanding before tackling more complex code structures. By presenting code snippets in increasing order of difficulty, the model can gradually develop its reasoning capabilities, potentially leading to better generalization on complex code execution tasks.

## Foundational Learning
1. **Curriculum Learning** - A training strategy where data is presented in increasing order of difficulty; needed to structure learning progression and improve model performance on complex tasks
2. **Code Complexity Metrics** - Measures like cyclomatic complexity, Halstead metrics, and nesting depth; needed to quantify code difficulty for effective curriculum design
3. **Language Model Training** - Standard approaches to training autoregressive models on sequential data; needed as baseline for comparison
4. **Code Execution Evaluation** - Methods to assess whether generated code produces correct outputs; needed to measure practical model capabilities
5. **Software Engineering Metrics** - Traditional measures of code quality and maintainability; needed to inform difficulty assessment
6. **Transfer Learning** - Applying knowledge from simpler tasks to more complex ones; needed to understand why curriculum learning might improve performance

## Architecture Onboarding

**Component Map:**
Data Preparation -> Code Difficulty Metric -> Curriculum Schedule -> Model Training -> Evaluation

**Critical Path:**
The most critical path is the code difficulty metric and curriculum schedule design, as these directly determine the learning progression. The metric must accurately reflect pedagogical difficulty, and the schedule must present examples at an appropriate pace for effective learning.

**Design Tradeoffs:**
The paper balances model size (1M parameters) against training complexity, choosing a small model to demonstrate curriculum learning effectiveness. The difficulty metric trades comprehensive software engineering analysis for computational efficiency. The curriculum schedules balance between rigid progression and adaptive learning.

**Failure Signatures:**
If the difficulty metric poorly reflects actual learning difficulty, models may struggle with early examples or fail to benefit from progression. If the curriculum schedule is too aggressive, models may not adequately learn from easier examples. If evaluation tasks don't match the curriculum content, improvements may not transfer effectively.

**3 First Experiments:**
1. Train baseline model on randomly ordered data to establish performance without curriculum learning
2. Train model with fixed curriculum schedule (easy → medium → hard) to test basic curriculum effectiveness
3. Train model with hybrid curriculum schedule to evaluate adaptive progression benefits

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the findings suggest several areas for further investigation, including scalability to larger models, applicability to other programming languages, and refinement of the code difficulty metric.

## Limitations
- Evaluation constrained by small model size (1M parameters), limiting generalizability to larger, more capable code models
- Code difficulty metric relies on static complexity measures that may not fully capture pedagogical progression needs
- Study focuses exclusively on Python code snippets, limiting generalizability to other programming languages

## Confidence
- **High confidence**: Hybrid curriculum learning schedule shows consistent improvement in code execution accuracy (79.23% vs 74.58%)
- **Medium confidence**: Effectiveness of curriculum learning for code completion tasks is less conclusive with modest improvements
- **Medium confidence**: Proposed code difficulty metric successfully categorizes Python code snippets, though pedagogical validity requires further validation

## Next Checks
1. Test curriculum learning approach on larger code language models (10M-100M parameters) to assess scalability and determine if improvements persist with increased model capacity
2. Expand evaluation to multiple programming languages and diverse code domains (web development, data science, etc.) to test generalizability of difficulty metric and curriculum schedules
3. Conduct ablation studies on individual complexity measures within the difficulty metric to identify which factors most strongly influence learning effectiveness