---
ver: rpa2
title: Function Approximation for Reinforcement Learning Controller for Energy from
  Spread Waves
arxiv_id: '2404.10991'
source_url: https://arxiv.org/abs/2404.10991
tags:
- wave
- waves
- energy
- learning
- controller
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a multi-agent reinforcement learning (MARL)\
  \ controller for a three-legged wave energy converter (WEC) handling complex spread\
  \ waves from multiple directions. The authors explore various function approximation\
  \ architectures\u2014fully connected neural networks, LSTMs, and transformer variants\u2014\
  for the policy and critic networks, finding that transformer models with gated residual\
  \ connections (STrXL) significantly outperform traditional approaches."
---

# Function Approximation for Reinforcement Learning Controller for Energy from Spread Waves

## Quick Facts
- arXiv ID: 2404.10991
- Source URL: https://arxiv.org/abs/2404.10991
- Reference count: 40
- Multi-agent RL controller with transformer function approximators achieves 22.1% energy capture improvement over baseline for spread waves

## Executive Summary
This paper presents a multi-agent reinforcement learning (MARL) controller for a three-legged wave energy converter (WEC) designed to handle complex spread waves from multiple directions. The authors explore various function approximation architectures—fully connected neural networks, LSTMs, and transformer variants—for the policy and critic networks. The transformer models with gated residual connections (STrXL) significantly outperform traditional approaches, achieving an average 22.1% improvement in energy capture efficiency over the baseline spring damper controller for spread waves. The MARL controller also reduces mechanical stress by over 99% for angled waves and decreases yaw motion by over 98% under both normal and extreme wave conditions, significantly reducing maintenance requirements.

## Method Summary
The method employs a multi-agent PPO controller where each of the three agents controls a generator on one leg of a three-legged WEC. Different function approximators (FCN, LSTM, TrXL, GTrXL, STrXL) are trained and evaluated for both policy and critic networks. The STrXL architecture uses gated residual connections around multi-head attention, MLP, and transformer blocks. Training is conducted in a simulated CETO 6 WEC environment with wave data sampled from multiple locations. The state vector includes buoy positions/velocities/accelerations, yaw, tether extensions/velocities, and wave elevation/rate of change. The reward function combines power generation and yaw reduction through weighted coefficients.

## Key Results
- Transformer-based controllers (STrXL) achieve 22.1% average improvement in energy capture efficiency over baseline for spread waves
- MARL controller reduces mechanical stress by over 99% for angled waves and decreases yaw motion by over 98% under both normal and extreme wave conditions
- STrXL architecture demonstrates superior training stability and performance compared to FCN and LSTM alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer models with gated residual connections (STrXL) capture sequential dependencies in WEC dynamics better than traditional function approximators
- Mechanism: Multi-head attention explicitly models interactions between different time steps in the wave state sequence, while gated residual connections stabilize training
- Core assumption: Wave dynamics have long-term temporal dependencies that simple feed-forward networks cannot capture effectively
- Evidence anchors: Abstract mentions exploring sequential function approximations; section notes periodic nature of ocean waves requires sequential system models
- Break condition: If wave dynamics are truly Markovian (memoryless), transformers' advantage would diminish significantly

### Mechanism 2
- Claim: Multi-agent architecture with separate agents for each leg handles heterogeneity of wave impacts across the three-generator system
- Mechanism: Each agent learns to control its specific generator based on local observations while cooperating through shared reward structure
- Core assumption: Different legs experience different wave forces simultaneously, requiring independent control policies
- Evidence anchors: Abstract states MARL can handle complexities; section notes need for separate agents to accommodate heterogeneity
- Break condition: If wave impacts are sufficiently symmetric across all legs, single-agent controller might perform equally well with less complexity

### Mechanism 3
- Claim: Reward shaping including both power generation and yaw reduction effectively balances competing objectives
- Mechanism: Weighting power reward (α.Preward) and yaw penalty ((1-α).yaw) in total reward function enables learning to optimize energy capture while minimizing mechanical stress
- Core assumption: Yaw motion is significant source of mechanical stress impacting maintenance costs and system longevity
- Evidence anchors: Abstract mentions transformer controller eliminated mechanical stress from rotational yaw motion; section notes yaw motion causes tether wear and maintenance implications
- Break condition: If yaw motion is not actually significant source of stress, additional complexity of yaw-aware reward shaping may not be justified

## Foundational Learning

- Concept: Sequential modeling in reinforcement learning
  - Why needed here: WECs experience time-dependent dynamics where past states influence future behavior, requiring function approximators that can capture temporal dependencies
  - Quick check question: Why wouldn't a simple feed-forward neural network work well for modeling wave WEC dynamics?

- Concept: Multi-agent reinforcement learning cooperation vs. competition
  - Why needed here: Each generator leg experiences different wave forces simultaneously, requiring separate agents that must coordinate through reward structure balancing individual and collective performance
  - Quick check question: How does the team coefficient (η) in the reward function affect whether agents cooperate or compete?

- Concept: Function approximation bias-variance tradeoff in RL
  - Why needed here: Choice between FCN, LSTM, and Transformer architectures involves balancing model capacity (variance) with ability to generalize from limited training data (bias)
  - Quick check question: What happens to training stability as we increase depth of function approximators in RL?

## Architecture Onboarding

- Component map: Wave simulation environment -> State observation -> Function approximation -> Action selection -> Environment response -> Reward calculation -> Policy update
- Critical path: Wave simulation → State observation → Function approximation → Action selection → Environment response → Reward calculation → Policy update
- Design tradeoffs:
  - Model complexity vs. training stability: Transformers perform better but are harder to train than LSTMs or FCNs
  - Agent independence vs. coordination: Separate agents allow specialization but require careful reward design
  - Reward shaping: Balancing power generation against mechanical stress reduction
- Failure signatures:
  - Poor convergence: Indicates function approximator instability or inappropriate hyperparameters
  - High yaw values: Suggests insufficient weighting of yaw penalty in reward function
  - Asymmetric performance across legs: May indicate inadequate state representation or reward coordination
- First 3 experiments:
  1. Compare FCN vs. LSTM with baseline PPO hyperparameters on unidirectional waves to establish sequential modeling benefits
  2. Implement STrXL with identity map reordering and test on spread waves to evaluate training stability improvements
  3. Vary the yaw penalty weight (α) to find optimal balance between power generation and stress reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do transformer-based function approximators scale to more complex WEC designs with more than three legs or additional degrees of freedom?
- Basis in paper: Explicit mention that approach is tested on three-legged WEC with potential for applying RL controllers to more complex designs
- Why unresolved: Paper does not explore performance on WEC designs with more than three legs or additional degrees of freedom
- What evidence would resolve it: Experimental results demonstrating performance on WEC designs with more than three legs or additional degrees of freedom, comparing to baseline controllers and other RL methods

### Open Question 2
- Question: What are the limitations of the proposed MARL controller with STrXL function approximation in terms of computational resources and real-time implementation?
- Basis in paper: Inferred from discussion of training convergence and performance without computational requirements or real-time implementation information
- Why unresolved: Paper does not provide information on computational resources required to train and run the controller or its suitability for real-time implementation
- What evidence would resolve it: Analysis of computational requirements and real-time performance, including comparisons to other RL methods and baseline controllers

### Open Question 3
- Question: How does the proposed MARL controller with STrXL function approximation handle uncertainty in the wave environment, such as unexpected wave patterns or extreme weather conditions?
- Basis in paper: Inferred from discussion of performance on different wave conditions without explicit addressing of uncertainty or extreme weather events
- Why unresolved: Paper does not provide information on robustness to uncertainty or extreme weather conditions
- What evidence would resolve it: Experimental results demonstrating performance under various uncertain and extreme weather conditions, comparing to baseline controllers and other RL methods

## Limitations

- Limited real-world validation with evaluation conducted primarily in simulated environments
- Incomplete specification of critical hyperparameters and architectural details for exact reproduction
- Insufficient ablation studies on key design choices like gated residual connections in STrXL

## Confidence

**High confidence**: Sequential function approximation architectures (LSTMs, Transformers) outperform non-sequential ones (FCNs) for WEC control due to temporal nature of wave dynamics.

**Medium confidence**: STrXL with gated residual connections provides optimal balance of performance and training stability, though exact architectural specifications and hyperparameter choices limit definitive conclusions.

**Medium confidence**: Effectiveness of MARL approach with three separate agents, but insufficient evidence that added complexity is necessary compared to alternative single-agent approaches.

## Next Checks

1. Conduct hyperparameter sensitivity analysis by systematically varying learning rate, batch size, and network architecture dimensions for each function approximator to establish robustness of performance differences

2. Assess practical challenges and requirements for deploying trained MARL controller on actual wave energy converters, including sensor requirements, computational constraints, and safety considerations

3. Implement and test additional baseline controllers beyond spring damper approach (e.g., model predictive control, other RL methods) to establish relative performance in broader context