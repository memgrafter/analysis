---
ver: rpa2
title: LLMs are Not Just Next Token Predictors
arxiv_id: '2408.04666'
source_url: https://arxiv.org/abs/2408.04666
tags:
- page
- llms
- token
- next
- andsoon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues against the reductionist claim that LLMs are "just
  next token predictors." While LLMs are trained using next token prediction, this
  characterization undersells their capabilities. The authors draw an analogy with
  the "gene's-eye view" in biology, which reduces evolution and development to gene-level
  processes.
---

# LLMs are Not Just Next Token Predictors

## Quick Facts
- arXiv ID: 2408.04666
- Source URL: https://arxiv.org/abs/2408.04666
- Authors: Stephen M. Downes; Patrick Forber; Alex Grzankowski
- Reference count: 0
- Primary result: LLMs are more than "just next token predictors" - they learn association networks among tokens that enable coherent text assembly

## Executive Summary
The paper challenges the reductionist view that large language models (LLMs) are merely next token predictors. While LLMs are trained using next token prediction, this characterization undersells their capabilities. The authors argue that LLMs are both next token predictors and "token network mapping devices" that leverage learned associations to assemble coherent text. Drawing an analogy with the "gene's-eye view" in biology, they contend that focusing only on next token prediction misses important higher-order organizational structure in how LLMs operate.

## Method Summary
The paper uses philosophical and theoretical arguments, drawing on analogies from biology and discussing formal connections between reinforcement learning and evolutionary dynamics. It examines the role of attention mechanisms and association networks in LLMs, arguing these elements enable coherent text generation beyond simple next token prediction. The authors analyze how LLMs undergo function changes through fine-tuning and RLHF, evolving beyond their initial next token prediction function.

## Key Results
- LLMs leverage learned association networks among tokens to assemble coherent text beyond simple next token prediction
- The reductionist "just next token predictor" view blinds us to reinforcement learning's capability to achieve structural information learning
- LLMs evolve beyond their initial function through fine-tuning and RLHF, similar to how Play-Doh changed from wallpaper cleaner to toy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs are both next token predictors and token network mapping devices
- Mechanism: Models learn association networks among tokens during training, allowing them to leverage these learned connections to assemble coherent text beyond simple next token prediction
- Core assumption: Attention heads expand the scope of prediction by making available information concerning many previous tokens and their interrelations
- Evidence anchors: Abstract explicitly states the paper argues against the reductionist claim; corpus shows weak evidence with related papers focusing on technical aspects but not directly supporting token network mapping claim
- Break condition: If attention mechanisms were removed or disabled, models would lose ability to leverage token associations and revert to simple next token prediction

### Mechanism 2
- Claim: Reduction to "just next token predictor" loses important explanatory power
- Mechanism: The reductionist view blinds us to reinforcement learning's capability to achieve structural information learning and undermines ability to explain how such structural information is learned
- Core assumption: Deep formal connection between reinforcement learning and evolutionary dynamics makes comparison between LLMs and units of selection more than superficially similar
- Evidence anchors: Section discusses how the claim blinds us to reinforcement learning capabilities; corpus shows no direct evidence supporting this specific evolutionary dynamics analogy
- Break condition: If model architecture were fundamentally altered to remove ability to learn association networks, reductionist explanation would become sufficient

### Mechanism 3
- Claim: LLMs undergo function changes through fine-tuning and RLHF
- Mechanism: Just as Play-Doh changed from wallpaper cleaner to toy through changes in use and development, LLMs evolve beyond initial next token prediction function
- Core assumption: Functions can change when things designed or selected to do one thing can be recruited to do another and come to have new function
- Evidence anchors: Section discusses how LLMs are more like Play-Doh than WD-40 and should be seen as more than mere number crunchers; corpus shows no direct evidence supporting function change argument
- Break condition: If models were only trained with next token prediction objective without any fine-tuning or RLHF, function change argument would not apply

## Foundational Learning

- Concept: Token association networks
  - Why needed here: Understanding how LLMs learn connections between tokens is crucial to grasping why they're more than just next token predictors
  - Quick check question: How do attention heads enable LLMs to leverage token associations when generating text?

- Concept: Evolutionary dynamics and reinforcement learning
  - Why needed here: The paper draws an analogy between how evolution assembles complex genomes and how reinforcement learning enables LLMs to learn structural information
  - Quick check question: What is the formal connection between changing learning weights in reinforcement learning and changing population composition in evolutionary dynamics?

- Concept: Function change through use and development
  - Why needed here: The argument that LLMs are more than next token predictors relies on understanding how functions can evolve beyond initial design objectives
  - Quick check question: How does the Play-Doh example illustrate the concept of function change, and how does this apply to LLMs?

## Architecture Onboarding

- Component map: Input text → Attention mechanisms → Weight adjustment systems → Token association learning → Output generation. Attention heads are critical as they enable token association learning, while weight adjustment systems support both next token prediction and structural information acquisition.

- Critical path: Token → Attention processing → Association network lookup → Next token prediction. The attention mechanisms are critical as they enable the model to leverage learned associations rather than just predict based on immediate context.

- Design tradeoffs: The model sacrifices computational efficiency for expressive power by using attention mechanisms that consider many previous tokens rather than just immediate context. This enables richer text generation but increases computational cost.

- Failure signatures: If the model were "just" a next token predictor, it would produce incoherent or irrelevant text. The presence of coherent paragraphs and relevant answers indicates the token network mapping is functioning.

- First 3 experiments:
  1. Compare text generation quality with and without attention mechanisms enabled
  2. Test model performance on tasks requiring long-range dependencies versus immediate context prediction
  3. Analyze the effect of removing fine-tuning and RLHF on the model's ability to perform tasks beyond next token prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific higher-level organizational structures in LLM behavior and capabilities are lost when adopting the "just next token predictor" view?
- Basis in paper: [explicit] The paper argues that focusing only on next token prediction misses complex association networks among tokens that LLMs learn
- Why unresolved: The paper provides general claims about lost explanatory power but doesn't enumerate specific structural features or capabilities that require higher-level explanation
- What evidence would resolve it: Detailed analysis showing which LLM behaviors (e.g., maintaining coherence across paragraphs, handling anaphora, understanding context) cannot be explained by next-token prediction alone

### Open Question 2
- Question: How do the association networks among tokens in LLMs relate to human language understanding and meaning-making?
- Basis in paper: [inferred] The paper mentions that LLMs leverage "rich networks of connections between tokens" to assemble coherent text, suggesting a structural parallel to meaning
- Why unresolved: The relationship between distributional token associations and semantic meaning remains unclear and debated
- What evidence would resolve it: Empirical studies comparing LLM token association networks with human semantic networks and understanding

### Open Question 3
- Question: What are the limits of the "token network mapping" view of LLMs - when does it break down or become inadequate?
- Basis in paper: [explicit] The paper argues for a dual view of LLMs as both next token predictors and token network mapping devices
- Why unresolved: The paper doesn't specify edge cases or limitations where token network mapping fails to explain LLM behavior
- What evidence would resolve it: Identification of specific failure modes or limitations in LLM behavior that cannot be explained by token network mapping

## Limitations
- The paper lacks direct empirical evidence for the specific mechanisms by which attention heads leverage token associations
- The evolutionary dynamics analogy, while conceptually interesting, lacks concrete validation in the LLM context
- The paper relies heavily on theoretical reasoning rather than experimental demonstration of claimed capabilities

## Confidence
- Medium confidence in the core claim that LLMs are more than just next token predictors, as the theoretical framework is sound but empirical validation is incomplete
- Lower confidence in the biological analogy and evolutionary dynamics comparison, as these connections are more speculative and lack direct empirical support

## Next Checks
1. Design experiments to quantify the contribution of attention mechanisms to text coherence by systematically disabling different attention heads and measuring degradation in generated text quality
2. Conduct ablation studies comparing models trained with different levels of token association learning to isolate the specific role of association networks in producing coherent multi-sentence outputs
3. Develop metrics to measure the "distance" between simple next token prediction and the actual generation process, quantifying how much of the model's behavior cannot be explained by pure next token prediction