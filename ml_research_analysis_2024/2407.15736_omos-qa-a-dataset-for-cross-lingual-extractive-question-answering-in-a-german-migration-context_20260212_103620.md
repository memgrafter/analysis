---
ver: rpa2
title: 'OMoS-QA: A Dataset for Cross-Lingual Extractive Question Answering in a German
  Migration Context'
arxiv_id: '2407.15736'
source_url: https://arxiv.org/abs/2407.15736
tags:
- questions
- question
- language
- answer
- german
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OMoS-QA is a dataset for extractive question answering in a German
  migration context, addressing the need for high-quality information for immigrants.
  Questions are generated with an LLM and answers are annotated by crowd workers.
---

# OMoS-QA: A Dataset for Cross-Lingual Extractive Question Answering in a German Migration Context

## Quick Facts
- arXiv ID: 2407.15736
- Source URL: https://arxiv.org/abs/2407.15736
- Authors: Steffen Kleinle; Jakob Prange; Annemarie Friedrich
- Reference count: 40
- OMoS-QA is a dataset for extractive QA in a German migration context with German and English questions paired with relevant documents

## Executive Summary
OMoS-QA is a dataset containing 906 German and English question-answer pairs in a German migration context, designed to help immigrants access critical information. The dataset uses sentence-level answer extraction with high inter-annotator agreement (Jaccard index 0.86) to ensure quality. Experiments with five pretrained LLMs and a fine-tuned DeBERTa classifier demonstrate high precision (70-90%) in answer extraction, though recall is lower (20-50%). Cross-language QA results are promising, with similar performance when question and document languages differ, indicating the potential for multilingual applications.

## Method Summary
The OMoS-QA dataset was created by generating questions from LLM summaries of German migration documents, with answers annotated by crowd workers using sentence-level extraction. Two annotators independently select answer sentences, with gold answers determined by intersection and adjacent sentence inclusion rules. The dataset was evaluated using five pretrained LLMs (Mixtral-8x7B, Mistral-7B, Llama-3-8B, Llama-3-70B, GPT-3.5-Turbo) in both zero-shot and five-shot settings, plus a fine-tuned DeBERTa-v3-large classifier. Experiments covered monolingual QA in German and English, and cross-language QA scenarios.

## Key Results
- High precision (70-90%) in sentence-level answer extraction using intersection rules with high annotator agreement
- Cross-language QA performs comparably when question and document languages differ
- Mixtral-8x7B and Llama-3-70B show best performance among evaluated LLMs
- DeBERTa classifier achieves precision and recall in the 60-65% range, comparable to some LLM results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sentence-level answer extraction with multiple annotations yields high precision because noisy or partially correct sentences are filtered out via intersection and adjacency rules.
- Mechanism: Two annotators independently select answer sentences; only sentences marked by both annotators are retained as gold, with an expansion rule to include adjacent sentences within three sentences to account for minor disagreements.
- Core assumption: Annotators' selections overlap significantly enough to allow meaningful intersection, and small disagreements can be resolved by including adjacent sentences without introducing noise.
- Evidence anchors:
  - [abstract]: "answer sentences are selected by crowd workers with high agreement"
  - [section 3.2]: Describes the intersection and adjacent sentence inclusion rule
  - [corpus]: Average Jaccard index after filtering is 0.86 (chance corrected), indicating high agreement
- Break condition: If annotator agreement drops significantly (Jaccard < 0.5), the intersection would become too sparse and the dataset would lose quality.

### Mechanism 2
- Claim: Fine-tuning a DeBERTa-v3-large classifier for sentence-level answer extraction can achieve precision comparable to larger LLMs, with better balance between precision and recall.
- Mechanism: The classifier is trained on the OMoS-QA dataset to predict whether a sentence answers a given question, using the [CLS] token representation for classification.
- Core assumption: The pretrained DeBERTa-v3-large encoder has learned sufficient semantic representations to distinguish answer sentences from non-answer sentences in the context of the OMoS-QA dataset.
- Evidence anchors:
  - [abstract]: "a fine-tuned classifier" is used in experiments
  - [section 4.3]: Describes the DeBERTa classifier architecture and training
  - [corpus]: DeBERTa achieves precision and recall in the 60-65% range, comparable to some LLM results
- Break condition: If the dataset is too small or domain-specific, the classifier might overfit and not generalize well.

### Mechanism 3
- Claim: Cross-language QA performance is maintained when the question language differs from the document language, suggesting that models can leverage semantic understanding beyond lexical overlap.
- Mechanism: The same LLM model is used to answer questions in a different language than the document, relying on its multilingual capabilities and semantic understanding.
- Core assumption: The LLM has been trained on multilingual data and can understand the semantic content of questions and documents even when they are in different languages.
- Evidence anchors:
  - [abstract]: "Cross-language QA results are promising, with similar performance when the question language does not match the document language"
  - [section 4.5]: Presents cross-language QA results showing comparable performance
  - [corpus]: Limited multilingual experiments, but results suggest this mechanism works
- Break condition: If the language pair is too distant or the model's multilingual training data is insufficient, performance will degrade significantly.

## Foundational Learning

- Concept: Inter-annotator agreement (IAA)
  - Why needed here: To ensure the quality and reliability of the annotated dataset by measuring the consistency between annotators.
  - Quick check question: What is the Jaccard index and how is it used to measure IAA in this context?

- Concept: Chance correction
  - Why needed here: To account for the possibility of agreement occurring by chance and to provide a more accurate measure of true agreement.
  - Quick check question: How is the expected agreement calculated and why is it subtracted from the observed agreement?

- Concept: Sentence-level vs. question-level evaluation
  - Why needed here: To evaluate the performance of the models at different granularities and to understand their strengths and weaknesses.
  - Quick check question: What is the difference between precision and recall at the sentence level and at the question level?

## Architecture Onboarding

- Component map:
  Document retrieval -> Question generation (LLM) -> Answer extraction (LLM or fine-tuned classifier) -> Cross-language QA

- Critical path:
  Input question and document -> Answer extraction -> Output answer sentences

- Design tradeoffs:
  - Sentence extraction vs. text generation: Sentence extraction is chosen for faithfulness and to avoid hallucinations.
  - Few-shot vs. zero-shot: Few-shot learning generally improves performance but requires manually selected examples.
  - Multilingual vs. monolingual: Multilingual models can handle cross-language QA but may have lower performance on individual languages.

- Failure signatures:
  - Low precision: Model is extracting too many irrelevant sentences.
  - Low recall: Model is missing many relevant sentences.
  - High precision, low recall: Model is being too conservative and only extracting high-confidence answers.
  - Low unanswerability recall: Model is not identifying unanswerable questions correctly.

- First 3 experiments:
  1. Evaluate zero-shot performance of Mixtral-8x7B on German and English QA.
  2. Fine-tune DeBERTa classifier on OMoS-QA and compare performance to zero-shot LLMs.
  3. Conduct cross-language QA experiment with German, English, and Arabic questions/documents.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OMoS-QA models change when questions are posed in a mix of languages (code-mixing) or contain spelling errors, simulating realistic user interactions?
- Basis in paper: [inferred] from the discussion section mentioning future experiments with introducing noise like spelling errors or code-mixing to measure model robustness.
- Why unresolved: The paper acknowledges the need to test model robustness under realistic user interaction scenarios but does not provide any experimental results for these cases.
- What evidence would resolve it: Conducting experiments where questions are deliberately corrupted with spelling errors or code-mixing, and comparing model performance to the clean dataset results.

### Open Question 2
- Question: What is the impact of using different question generation strategies (e.g., using longer summaries or different prompt engineering techniques) on the quality and diversity of the generated questions in OMoS-QA?
- Basis in paper: [explicit] from the question generation section describing the use of full document prompts and three-word summary prompts, but not exploring other strategies.
- Why unresolved: The paper only explores two basic question generation strategies and does not investigate the potential benefits of more sophisticated approaches.
- What evidence would resolve it: Generating questions using various prompt engineering techniques and evaluating their impact on the quality, diversity, and difficulty of the generated questions.

### Open Question 3
- Question: How does the performance of OMoS-QA models vary when the dataset is expanded to include more languages, particularly those with significantly different linguistic structures from German and English?
- Basis in paper: [explicit] from the limitations section acknowledging the current dataset's limitation to German and English and the need for future work to include more languages.
- Why unresolved: The paper only provides a pilot study with a few additional languages and does not explore the full potential of cross-lingual QA in the migration context.
- What evidence would resolve it: Extending the dataset to include a wider range of languages and evaluating model performance across different language pairs and linguistic structures.

## Limitations
- Low recall (20-50%) due to strict intersection rules requiring annotator agreement
- Dataset limited to German migration context, limiting generalizability
- 5-shot prompting relies on manually selected examples, raising reproducibility concerns

## Confidence

- **High confidence**: The high precision achieved by sentence-level extraction with intersection rules is well-supported by the 0.86 average Jaccard index and multiple LLM experiments showing consistent results across different models.
- **Medium confidence**: The cross-language QA results showing comparable performance when question and document languages differ are promising but based on limited experiments with only German, English, and Arabic, requiring more extensive validation across diverse language pairs.
- **Low confidence**: The DeBERTa classifier's performance being comparable to larger LLMs is questionable given that it achieves 60-65% precision and recall versus 70-90% for the best LLMs, suggesting it may not be truly comparable in effectiveness.

## Next Checks

1. **Annotation Agreement Validation**: Conduct a follow-up study with additional annotators on a subset of the dataset to verify if the 0.86 Jaccard index holds consistently, particularly for more complex or ambiguous questions where agreement might drop significantly.

2. **Cross-Lingual Scalability Test**: Expand cross-language experiments to include more diverse language pairs (e.g., German-Arabic, English-Turkish) and evaluate whether the promising results observed with German-English-Arabic generalize to linguistically distant language combinations.

3. **Real-World Retrieval Integration**: Implement and evaluate a complete QA pipeline that includes document retrieval from the OMoS-QA corpus, measuring how retrieval errors affect overall QA performance compared to the perfect retrieval assumption used in current experiments.