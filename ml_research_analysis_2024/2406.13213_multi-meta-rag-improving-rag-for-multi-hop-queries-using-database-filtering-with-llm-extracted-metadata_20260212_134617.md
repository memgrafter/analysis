---
ver: rpa2
title: 'Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering
  with LLM-Extracted Metadata'
arxiv_id: '2406.13213'
source_url: https://arxiv.org/abs/2406.13213
tags:
- news
- queries
- metadata
- chunks
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-Meta-RAG improves retrieval-augmented generation for multi-hop
  questions by using LLM-extracted metadata to filter documents before retrieval.
  A metadata extraction step identifies relevant article sources and publication dates
  from queries, which are then used to filter vector database chunks before ranking.
---

# Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata

## Quick Facts
- arXiv ID: 2406.13213
- Source URL: https://arxiv.org/abs/2406.13213
- Reference count: 20
- Multi-Meta-RAG improves RAG performance on multi-hop questions using LLM-extracted metadata filtering

## Executive Summary
Multi-Meta-RAG enhances retrieval-augmented generation for multi-hop questions by extracting metadata (sources and dates) from queries using an LLM, then filtering document chunks during retrieval based on this metadata. Tested on the MultiHop-RAG benchmark, this approach improves retrieval metrics (MRR@10 from 0.60 to 0.66, Hits@4 from 0.66 to 0.77) and LLM generation accuracy (from 0.47 to 0.61 for PaLM, 0.56 to 0.63 for GPT-4). The method shows strongest gains on comparison and temporal queries.

## Method Summary
The system extracts article sources and publication dates from queries using an LLM with few-shot prompting, then uses this metadata to filter document chunks in a vector database (Neo4j) before retrieval and ranking. Document chunks are embedded using models like voyage-02, filtered by metadata, then reranked using bge-reranker-large. The top-K chunks are used as context for LLM answer generation. The approach requires queries to follow a specific structure with explicit source and date references.

## Key Results
- MRR@10 improved from 0.60 to 0.66 and Hits@4 from 0.66 to 0.77 using voyage-02 embeddings
- LLM generation accuracy increased from 0.47 to 0.61 for PaLM and 0.56 to 0.63 for GPT-4
- Strongest performance gains observed on comparison and temporal queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extracting article sources and publication dates from queries using an LLM provides precise filtering criteria for the vector database.
- Mechanism: The LLM identifies specific news sources (e.g., "Engadget", "The Verge") and dates mentioned in the query and constructs a MongoDB-style filter dictionary with $in or $nin operators.
- Core assumption: The query structure consistently contains explicit or implicit references to news sources and dates that can be parsed by a few-shot prompt.
- Evidence anchors: [abstract] "uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question." [section 3.1] "We can extract the query filter via helper LLM by constructing a few-shot prompt [1] with examples of extracted article sources and publishing dates as a filter."

### Mechanism 2
- Claim: Filtering chunks in the retrieval stage using the extracted metadata improves the precision of the top-K retrieved chunks.
- Mechanism: After embedding the query and retrieving top-20 chunks, the system applies the metadata filter to exclude chunks from irrelevant sources or dates before ranking with the reranker.
- Core assumption: The metadata filter can be efficiently applied at the vector database level (Neo4j in this case) without significant performance degradation.
- Evidence anchors: [section 3.2] "We also filter the chunks with LLM-extracted metadata in the same stage... select the top-K chunks using the Reranker." [section 4.1] "The experiment (Table 2) with RAG showed considerable improvement in both embeddings for all core metrics."

### Mechanism 3
- Claim: Using metadata-filtered retrieval improves LLM response accuracy by providing more relevant context.
- Mechanism: By ensuring that retrieved chunks come from the correct sources and dates, the LLM has a higher chance of generating an accurate answer without having to reason through irrelevant information.
- Core assumption: The LLM's generation accuracy is sensitive to the relevance of the retrieved context, and filtering out irrelevant sources/dates improves this relevance.
- Evidence anchors: [section 4.2] "We achieved substantial improvement in accuracy (Table 3) for both models compared to baseline RAG implementation." [section 4.2] "Table 4 shows the detailed evaluation results of different question types... Google PaLM performs significantly better for comparison and temporal queries than GPT-4."

## Foundational Learning

- Concept: Vector embeddings and similarity search
  - Why needed here: The system relies on embedding models (e.g., voyage-02, bge-large-en-v1.5) to convert both queries and document chunks into vectors for similarity-based retrieval.
  - Quick check question: What embedding model is used to convert document chunks into vectors in this system?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: The system is an improved RAG pipeline that retrieves relevant documents before generating answers with an LLM.
  - Quick check question: What is the primary purpose of using RAG in this system?

- Concept: Metadata filtering and database operations
  - Why needed here: The system uses LLM-extracted metadata to filter document chunks in the vector database using operators like $in and $nin.
  - Quick check question: What database operators are used to filter chunks based on extracted metadata?

## Architecture Onboarding

- Component map: LLM -> Embedding Model -> Vector Database (Neo4j) -> Reranker -> LLM
- Critical path: 1. Extract metadata from query using LLM 2. Embed query using embedding model 3. Retrieve top-20 chunks using vector similarity and metadata filter 4. Rerank top-20 chunks to select top-10 5. Generate answer using LLM with top-K chunks as context
- Design tradeoffs: Using metadata filtering improves precision but adds an extra LLM inference step, increasing latency and cost. Smaller chunk overlap (32) improves chunk variety but may reduce contextual coherence within chunks. The system is optimized for the MultiHop-RAG dataset format and may not generalize well to other domains without prompt adaptation.
- Failure signatures: If metadata extraction fails, the filter will be empty, and the system will fall back to baseline RAG performance. If the vector database does not support efficient metadata filtering, retrieval latency may increase significantly. If the reranker is not effective, the top-K chunks may still contain irrelevant information, reducing LLM accuracy.
- First 3 experiments: 1. Test metadata extraction accuracy on a sample of MultiHop-RAG queries using the provided prompt template. 2. Verify that the Neo4j vector store correctly applies metadata filters during retrieval by checking the source and date properties of retrieved chunks. 3. Compare baseline RAG and Multi-Meta-RAG performance on a small subset of MultiHop-RAG queries using a simple LLM (e.g., GPT-3.5) to validate the accuracy improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How generalizable is the LLM-extracted metadata filtering approach to domains beyond news articles and temporal/comparison queries?
- Basis in paper: [inferred] The paper notes that metadata extraction "requires a set of queries from a particular domain and question format" and suggests testing on datasets "from different domains" as future work.
- Why unresolved: The current implementation was only tested on the MultiHop-RAG benchmark with news articles. The paper doesn't explore how well the approach works with different types of queries or knowledge domains.
- What evidence would resolve it: Experiments applying Multi-Meta-RAG to benchmarks from other domains (e.g., scientific literature, legal documents, medical records) and evaluating performance across different query types.

### Open Question 2
- Question: What is the optimal chunk size and overlap for different embedding models when using metadata filtering?
- Basis in paper: [explicit] The paper mentions using "256 tokens" chunk size and "32" overlap, finding that "smaller chunk overlap leads to better variety of unique chunks" compared to the default of 200, but doesn't systematically explore this parameter space.
- Why unresolved: The paper only reports results for one chunk size and overlap configuration. Different embedding models might have different optimal parameters for balancing retrieval quality and computational efficiency.
- What evidence would resolve it: A systematic study varying chunk sizes and overlaps across multiple embedding models, measuring impact on MRR@10, MAP@10, Hits@10, and computational costs.

### Open Question 3
- Question: How does Multi-Meta-RAG perform with more recent knowledge cutoff dates?
- Basis in paper: [explicit] The paper suggests testing "alternative LLMs, like LLama 3.1, on datasets with more recent cut-off dates" as future work, implying this hasn't been tested.
- Why unresolved: The current experiments used MultiHop-RAG dataset from September to December 2023, which is still relatively close to the knowledge cutoff of the models tested (GPT-4 in June 2023, PaLM with unspecified cutoff).
- What evidence would resolve it: Testing Multi-Meta-RAG on benchmarks with knowledge cutoff dates well beyond the training data of the LLMs used, measuring performance degradation and improvement over baseline RAG.

## Limitations

- The approach is tightly coupled to the MultiHop-RAG dataset structure where queries explicitly mention sources and dates
- Performance depends on the Neo4j vector store's ability to efficiently filter by metadata during retrieval
- The metadata extraction LLM may not generalize to queries without explicit temporal or source references

## Confidence

- High confidence: The mechanism of metadata filtering improving retrieval precision is well-supported by the quantitative results (MRR@10 improvement from 0.60 to 0.66, Hits@4 from 0.66 to 0.77).
- Medium confidence: The LLM accuracy improvements (from 0.47 to 0.61 for PaLM, 0.56 to 0.63 for GPT-4) are compelling but may be influenced by the specific dataset characteristics and the choice of evaluation metric (word overlap).
- Low confidence: The generalizability of the metadata extraction approach to other domains or query types remains unclear without additional experiments.

## Next Checks

1. **Dataset Generalization Test**: Apply Multi-Meta-RAG to a different multi-hop QA dataset (e.g., HotpotQA) to assess whether metadata extraction and filtering still improve performance when queries lack explicit source/date mentions.

2. **Metadata Extraction Robustness**: Evaluate the metadata extraction LLM's accuracy on a held-out set of MultiHop-RAG queries, particularly for queries with implicit temporal or source references, to identify failure patterns.

3. **Database Performance Benchmark**: Measure the retrieval latency with and without metadata filtering on a scaled-up version of the Neo4j vector store to quantify the trade-off between precision gains and computational overhead.