---
ver: rpa2
title: 'Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost Edge
  Devices'
arxiv_id: '2410.21073'
source_url: https://arxiv.org/abs/2410.21073
tags:
- u1d47e
- u1d48c
- u1d488
- skip2-lora
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Skip2-LoRA, a lightweight on-device DNN fine-tuning
  method for resource-limited edge devices. The approach extends LoRA (low-rank adaptation)
  by inserting trainable LoRA adapters between the last layer and every other layer,
  enhancing network expressive power while keeping backward computation costs low.
---

# Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost Edge Devices

## Quick Facts
- arXiv ID: 2410.21073
- Source URL: https://arxiv.org/abs/2410.21073
- Authors: Hiroki Matsutani; Masaaki Kondo; Kazuki Sunaga; Radu Marculescu
- Reference count: 17
- Primary result: Achieves 90% reduction in fine-tuning time on Raspberry Pi Zero 2 W while preserving accuracy

## Executive Summary
This paper introduces Skip2-LoRA, a lightweight on-device DNN fine-tuning method designed for resource-limited edge devices. The approach extends traditional LoRA by inserting trainable adapters between the last layer and every other layer, enhancing network expressive power while keeping backward computation costs low. The architecture enables caching intermediate computation results during forward passes, allowing Skip2-LoRA to skip forward computation of seen samples as training progresses. The method was implemented and tested on a $15 Raspberry Pi Zero 2 W board, demonstrating that it reduces fine-tuning time by 90.0% on average compared to the baseline with the same number of trainable parameters while preserving accuracy.

## Method Summary
Skip2-LoRA extends the LoRA framework by inserting trainable LoRA adapters between the last layer and every other layer in a DNN. This architecture enhances network expressive power while keeping backward computation costs low. The method implements a cache system that stores intermediate feature maps from forward passes, allowing reuse of cached results for repeated samples across training epochs. The approach was implemented on a Raspberry Pi Zero 2 W and tested on three drifted datasets: Damage1, Damage2, and HAR. The method uses 3-layer DNNs with varying architectures depending on the dataset, and trains with stochastic gradient descent while only updating LoRA adapter parameters.

## Key Results
- Skip2-LoRA reduces fine-tuning time by 90.0% on average compared to LoRA-All baseline with same trainable parameters
- Total fine-tuning times are only 1.06-2.79 seconds across different datasets
- Achieves comparable accuracies to state-of-the-art methods while running on a $15 Raspberry Pi Zero 2 W board
- Effectively addresses the gap between pre-trained and deployed models for real-time edge AI applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Skip2-LoRA reduces forward computation by caching intermediate feature maps and reusing them for repeated samples.
- Mechanism: The method stores output feature maps of each FC layer in a cache (Skip-Cache) during the first epoch. When the same sample is seen again in later epochs, the cached results are reused, avoiding redundant matrix multiplications.
- Core assumption: Training samples are repeated across epochs, and cached intermediate results remain valid since weights are not updated except in LoRA adapters.
- Evidence anchors:
  - [abstract] "This architecture is well-suited to cache intermediate computation results of the forward pass and then can skip the forward computation of seen samples as training epochs progress."
  - [section] "Let /u1D438be the number of fine-tuning epochs. In the stochastic gradient descent, it is expected that the same training sample appears /u1D438times on average during a fine-tuning process."
  - [corpus] Weak. No corpus paper directly discusses sample reuse caching for forward pass optimization in LoRA-based methods.
- Break condition: If the cache lookup overhead exceeds the cost of recomputation, or if the training dataset has very low sample repetition.

### Mechanism 2
- Claim: Skip-LoRA architecture reduces backward computation by isolating LoRA adapters between the last layer and all other layers.
- Mechanism: LoRA adapters are inserted between the last FC layer and every other layer. Only these adapters are updated during training, so gradients are computed only for the adapter parameters, not the frozen backbone weights.
- Core assumption: The computational cost of LoRA adapter operations is negligible compared to full FC layer operations, especially when rank is small.
- Evidence anchors:
  - [abstract] "trainable LoRA (low-rank adaptation) adapters are inserted between the last layer and every other layer to enhance the network expressive power while keeping the backward compute cost low."
  - [section] "Please note that a LoRA adapter is a low-rank approximation of an FC layer; thus, we can expect /u1D445 << /u1D441, /u1D440."
  - [corpus] Weak. No corpus paper describes this exact Skip-LoRA architecture; most LoRA works focus on adding adapters to individual layers, not between last and all others.
- Break condition: If the rank of the LoRA adapters is increased to the point where their computation cost rivals that of the FC layers.

### Mechanism 3
- Claim: The combination of Skip-LoRA and Skip-Cache (Skip2-LoRA) achieves up to 90% reduction in fine-tuning time while preserving accuracy.
- Mechanism: Skip-LoRA reduces backward pass time by limiting gradient computation to LoRA adapters. Skip-Cache reduces forward pass time by reusing cached intermediate results. Together, they drastically cut total fine-tuning time.
- Core assumption: The accuracy of the model is not significantly degraded by the reduced computation and parameter updates.
- Evidence anchors:
  - [abstract] "Our results show that Skip2-LoRA reduces the fine-tuning time by 89.0% to 92.0% compared to the baseline while achieving comparable accuracies..."
  - [section] "Experimental results using three drifted datasets demonstrated that Skip2-LoRA reduces the fine-tuning time by 90.0% on average compared to LoRA-All that has the same number of trainable parameters while achieving comparable accuracies to the state-of-the-art method."
  - [corpus] Weak. No corpus paper provides similar magnitude of speedup claims for on-device fine-tuning with comparable accuracy preservation.
- Break condition: If the model requires more expressive power than Skip2-LoRA can provide, leading to accuracy degradation.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA enables efficient fine-tuning by decomposing weight updates into low-rank matrices, reducing the number of trainable parameters and computation.
  - Quick check question: In LoRA, if the rank /u1D445is set to 1, how many parameters are added to a weight matrix of size 256x96 compared to full fine-tuning?

- Concept: Forward and Backward Pass in Neural Networks
  - Why needed here: Understanding the computational cost of forward and backward passes is crucial to appreciate the efficiency gains from Skip2-LoRA's caching and adapter isolation strategies.
  - Quick check question: In a standard FC layer, which pass (forward or backward) is typically more computationally expensive, and why?

- Concept: Cache Coherence and Reuse in Iterative Algorithms
  - Why needed here: The Skip-Cache mechanism relies on the assumption that training samples repeat across epochs and that cached intermediate results remain valid.
  - Quick check question: What is the expected number of times a training sample appears across /u1D438epochs in stochastic gradient descent, assuming random sampling with replacement?

## Architecture Onboarding

- Component map: Input Layer -> FC Layers -> LoRA Adapters -> Cache -> Loss Function -> Optimizer
- Critical path:
  1. Forward pass through FC layers (with cache lookup and skip)
  2. Forward pass through LoRA adapters
  3. Loss computation
  4. Backward pass through LoRA adapters only
  5. Parameter update for LoRA adapters
  6. Cache update with new intermediate results
- Design tradeoffs:
  - Memory vs. Speed: Larger cache improves speed but increases memory usage
  - Rank vs. Accuracy: Higher LoRA rank improves expressive power but increases computation
  - Cache Hit Rate vs. Cache Size: Larger cache improves hit rate but requires more memory
- Failure signatures:
  - High cache miss rate: Indicates low sample repetition or cache eviction issues
  - Degraded accuracy: Suggests insufficient expressive power from low LoRA rank or cache staleness
  - Increased execution time: Could indicate cache lookup overhead exceeding benefits or inefficient implementation
- First 3 experiments:
  1. Measure cache hit rate and forward pass time reduction with varying cache sizes on a small dataset
  2. Compare accuracy and backward pass time with different LoRA ranks (e.g., 1, 2, 4) on a simple model
  3. Evaluate total fine-tuning time and accuracy on a drifted dataset with varying numbers of training epochs

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- The method's effectiveness depends heavily on sample repetition patterns across epochs, which weren't fully characterized
- The novel Skip-LoRA architecture lacks independent validation beyond this work
- Energy consumption measurements were not provided despite being critical for edge deployment claims

## Confidence
- Speedup claims: High - Well-supported by controlled experiments across three datasets
- Accuracy preservation: Medium - Comparable accuracy shown but no statistical significance testing
- Energy efficiency: Low - Power measurements missing despite being central to edge device claims

## Next Checks
1. Characterize cache hit rate patterns across different dataset sizes and class distributions to validate the sample reuse assumption
2. Measure power consumption and temperature profiles during fine-tuning on the Pi Zero 2 W to quantify energy efficiency claims
3. Test robustness to varying learning rates to determine cache invalidation thresholds and identify conditions where the approach breaks down