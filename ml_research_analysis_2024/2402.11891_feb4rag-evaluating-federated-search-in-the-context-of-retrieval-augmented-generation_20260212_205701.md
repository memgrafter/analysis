---
ver: rpa2
title: 'FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented
  Generation'
arxiv_id: '2402.11891'
source_url: https://arxiv.org/abs/2402.11891
tags:
- search
- information
- federated
- retrieval
- collection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FeB4RAG, a new dataset for evaluating federated
  search within Retrieval-Augmented Generation (RAG) pipelines. The dataset addresses
  limitations of existing collections by providing 790 information requests derived
  from 16 sub-collections of the BEIR benchmark, along with LLM-derived relevance
  judgements.
---

# FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2402.11891
- Source URL: https://arxiv.org/abs/2402.11891
- Authors: Shuai Wang; Ekaterina Khramtsova; Shengyao Zhuang; Guido Zuccon
- Reference count: 40
- Primary result: FeB4RAG dataset demonstrates that optimized federated search significantly outperforms naive approaches in RAG contexts

## Executive Summary
FeB4RAG introduces a new dataset specifically designed to evaluate federated search within Retrieval-Augmented Generation (RAG) pipelines. The collection addresses limitations of existing federated search datasets by providing 790 information requests derived from 16 sub-collections of the BEIR benchmark, along with LLM-derived relevance judgments. The authors demonstrate that LLM-based relevance labeling achieves substantial agreement with human annotations (Kappa=0.57), validating the approach for large-scale dataset creation. Using this dataset, they show that optimized federated search methods significantly outperform naive round-robin merging approaches across multiple evaluation criteria.

## Method Summary
The FeB4RAG dataset was created by rewriting 790 BEIR queries into more conversational formats suitable for RAG applications. Each of the 16 BEIR sub-collections was paired with state-of-the-art dense retrievers to simulate realistic search engines. LLM-based relevance judgments were generated using GPT-4 and other open-source models, with Cohen's Kappa agreement of 0.57 when compared to human annotations. The dataset was then used to compare naive federated search (round-robin merging of top results from all 16 search engines) against an optimized approach that selects resources based on graded precision scores.

## Key Results
- LLM-based relevance judgments achieve substantial agreement with human annotations (Kappa=0.57)
- Optimized federated search approach significantly outperforms naive round-robin merging across coverage, consistency, correctness, and clarity criteria
- The dataset successfully identifies limitations of existing federated search methods when applied to RAG contexts
- Conversational query rewriting effectively transforms BEIR queries for RAG evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based relevance judgments can achieve substantial agreement with human annotations, making them viable for creating large-scale evaluation datasets.
- Mechanism: The paper demonstrates that using GPT-4 and other open-source LLMs to generate relevance judgments results in a Cohen's Kappa of 0.57 when compared to human annotations. This is close to the agreement level between two human annotators (0.6) reported in TREC FedWeb 2013.
- Core assumption: LLM judgments can approximate human relevance judgments when provided with well-crafted prompts that guide the evaluation process.
- Evidence anchors: All datasets achieved a Cohen's Kappa above 0.4, which indicates moderate agreement. Notably, the aggregated labels from both LLMs often resulted in the highest average Kappa, 0.57, compared to 0.54 when using lgs-13b and 0.5 when using solar-11b.

### Mechanism 2
- Claim: The FeB4RAG collection addresses the limitations of existing federated search datasets by providing user requests that better represent real-world RAG applications.
- Mechanism: The dataset includes 790 information requests derived from BEIR sub-collections, rewritten to be more conversational and aligned with chatbot applications rather than short keyword queries typical of previous collections.
- Core assumption: RAG applications in conversational agents require more natural language queries that express specific information needs rather than terse keyword searches.
- Evidence anchors: Many queries, especially argumentative ones from ArguAna or paper titles from SCIDOCS, were deemed unsuitable for RAG settings in their original form.

### Mechanism 3
- Claim: The collection's use of state-of-the-art dense retrievers as search engines provides a more realistic evaluation environment for modern federated search methods.
- Mechanism: By pairing each BEIR dataset with top-performing dense retrieval models from the MTEB leaderboard, the collection simulates realistic search engines that reflect current neural information retrieval practices rather than older keyword-matching approaches.
- Core assumption: Modern federated search systems need to evaluate against neural retrieval models that better represent current technology rather than legacy keyword-based systems.
- Evidence anchors: Our selection process is guided by the BEIR benchmark... We only considered those top performing models that are based on dense retrievers, because dense retrievers are generally characterised by low latency and computational requirements.

## Foundational Learning

- Concept: Cohen's Kappa statistic
  - Why needed here: To measure the agreement between LLM-generated relevance judgments and human annotations, providing quantitative validation of the dataset's quality.
  - Quick check question: What does a Cohen's Kappa value of 0.57 indicate about the agreement between two raters, and how does this compare to typical human inter-annotator agreement?

- Concept: Federated search architecture
  - Why needed here: Understanding the distinction between resource selection and result merging is crucial for interpreting the collection's structure and how it can be used to evaluate different aspects of federated search systems.
  - Quick check question: In a federated search system, what are the two key tasks that must be performed, and how do they differ in terms of access to underlying search engine information?

- Concept: Retrieval-augmented generation (RAG) pipeline
  - Why needed here: The collection is specifically designed for evaluating federated search within RAG contexts, so understanding how RAG pipelines integrate retrieval with generation is essential.
  - Quick check question: In a RAG pipeline, what role does federated search play, and how does it differ from traditional information retrieval?

## Architecture Onboarding

- Component map: 16 search engines (BEIR datasets with dense retrievers) -> 790 user requests (rewritten conversational queries) -> LLM-based relevance judgments (4-level scale) -> Evaluation framework for resource selection and result merging -> Codebase for expansion and adaptation

- Critical path: 1. Select user request -> 2. Route to appropriate search engines (resource selection) -> 3. Retrieve top-k results from each selected engine -> 4. Merge results into unified ranking (result merging) -> 5. Generate response using LLM with retrieved context -> 6. Evaluate using relevance judgments

- Design tradeoffs: Using LLMs for labeling enables large-scale dataset creation but introduces potential consistency issues; Dense retrievers provide computational efficiency but may not capture all nuances of more complex retrieval methods; The collection balances between comprehensiveness (790 queries) and practical evaluation constraints

- Failure signatures: Low Cohen's Kappa between LLM and human judgments indicates poor label quality; Uneven distribution of relevant results across search engines suggests poor resource selection challenges; Queries that don't trigger appropriate search engines indicate mismatch between request types and engine capabilities

- First 3 experiments: 1. Run a baseline round-robin merging approach on the entire collection and measure performance degradation; 2. Test resource selection methods using the engine-level relevance scores to compare against random selection; 3. Evaluate how query rewriting affects retrieval effectiveness by comparing original BEIR queries with FeB4RAG versions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do resource selection strategies in federated search within RAG pipelines scale when the number of available resources grows significantly beyond the current 16 used in FeB4RAG?
- Basis in paper: The paper mentions that the current collection uses 16 search engines and that this fits many real applications, but doesn't explore scaling to hundreds or thousands of resources.
- Why unresolved: The paper focuses on demonstrating the need for federated search in RAG contexts rather than exploring scalability limits of resource selection methods.
- What evidence would resolve it: Empirical studies testing resource selection performance as the number of resources scales from 16 to 100+ resources, measuring accuracy, latency, and computational overhead.

### Open Question 2
- Question: What is the optimal trade-off between resource selection accuracy and latency/cost in federated search for RAG systems?
- Basis in paper: The paper discusses implications of obtaining information from irrelevant resources (increased chances of hallucination) and interacting with all resources (higher computational load, payment of API costs, higher latency), but doesn't quantify the optimal balance.
- Why unresolved: The paper identifies the problem but doesn't empirically determine the point at which additional resource selection accuracy no longer justifies the increased cost and latency.
- What evidence would resolve it: Systematic experiments varying resource selection thresholds and measuring both retrieval accuracy gains and cost/latency penalties across different RAG tasks.

### Open Question 3
- Question: How does the performance of LLM-based relevance judgments compare to human annotations for federated search tasks that involve more complex information needs or domain-specific content?
- Basis in paper: The paper demonstrates substantial agreement (Kappa=0.57) between LLM-derived labels and human annotations, but this is based on relatively general BEIR collections.
- Why unresolved: The paper validates LLM judgments on general collections but doesn't test performance on specialized domains or more complex information needs that might arise in specific RAG applications.
- What evidence would resolve it: Comparative studies of LLM vs human relevance judgments across specialized domains (legal, medical, technical) and for complex multi-faceted information needs.

## Limitations

- The dataset's LLM-derived relevance judgments, while showing substantial agreement (Kappa=0.57) with human annotations, still exhibit moderate consistency that may affect evaluation reliability.
- The collection focuses on conversational queries rather than traditional keyword searches, which may limit its applicability to non-RAG federated search scenarios.
- The use of dense retrievers as search engines, while computationally efficient, may not capture the full complexity of federated search systems that employ more sophisticated retrieval methods.

## Confidence

- **High confidence**: The dataset's structure and purpose are clearly defined, with well-documented methodology for query rewriting and LLM labeling.
- **Medium confidence**: The LLM labeling agreement (Kappa=0.57) is substantial but not perfect, indicating some uncertainty in label quality.
- **Medium confidence**: The effectiveness of optimized federated search approaches over naive methods is demonstrated but may vary with different system configurations.

## Next Checks

1. **Label Quality Validation**: Conduct a blind human evaluation on a random sample of 100 query-result pairs to independently verify the LLM labeling agreement and identify systematic biases in the relevance judgments.

2. **Generalization Testing**: Apply the FeB4RAG evaluation framework to a completely different federated search scenario (e.g., enterprise search across heterogeneous document repositories) to assess its broader applicability beyond RAG contexts.

3. **Algorithm Robustness Analysis**: Test the optimized federated search approach against adversarial query types (e.g., ambiguous requests, queries spanning multiple domains) to identify failure modes and robustness limitations.