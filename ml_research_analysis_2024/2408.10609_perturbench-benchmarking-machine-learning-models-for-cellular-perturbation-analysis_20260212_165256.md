---
ver: rpa2
title: 'PerturBench: Benchmarking Machine Learning Models for Cellular Perturbation
  Analysis'
arxiv_id: '2408.10609'
source_url: https://arxiv.org/abs/2408.10609
tags:
- perturbation
- cell
- perturbations
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PerturBench, a comprehensive framework for
  benchmarking machine learning models that predict cellular responses to genetic
  and chemical perturbations. The framework addresses the challenge of inconsistent
  benchmarks in this rapidly evolving field by providing standardized datasets, tasks,
  and evaluation metrics.
---

# PerturBench: Benchmarking Machine Learning Models for Cellular Perturbation Analysis

## Quick Facts
- arXiv ID: 2408.10609
- Source URL: https://arxiv.org/abs/2408.10609
- Reference count: 40
- One-line primary result: No single model architecture outperforms others across all tasks; simpler models often match or exceed complex ones

## Executive Summary
PerturBench introduces a comprehensive benchmarking framework for evaluating machine learning models that predict cellular responses to genetic and chemical perturbations. The framework addresses the field's inconsistent evaluation practices by providing standardized datasets, tasks, and evaluation metrics including rank-based measures that detect common failure modes like mode collapse. The study reveals that simpler model architectures often perform as well as or better than complex ones, particularly when scaling to larger datasets, and that removing strong inductive biases like adversarial training can improve performance.

## Method Summary
PerturBench provides a modular framework with standardized data preparation, model training, and evaluation components. The method uses six published single-cell RNA-seq datasets covering diverse perturbation types and cell lines, with models trained on nested data splits to simulate real-world scenarios. Models are evaluated using multiple metrics including RMSE, cosine similarity, rank metrics, MMD, and DEG recall. Hyperparameter optimization is performed using optuna, and the framework includes counterfactual prediction generation capabilities essential for perturbation response modeling.

## Key Results
- No single model architecture consistently outperforms others across all tasks and datasets
- Simpler architectures (Latent Additive, Decoder-Only) often match or exceed performance of complex models like CPA and SAMS-VAE
- Removing adversarial components from CPA and sparsity constraints from SAMS-VAE improves model performance
- Rank-based metrics are essential for detecting mode collapse and assessing model specificity beyond traditional fit measures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Simpler model architectures often match or outperform more complex models across diverse datasets and tasks.
- **Mechanism:** These simpler architectures reduce modeling assumptions and computational complexity, enabling better generalization when training data scales up or when biological states become more heterogeneous.
- **Core assumption:** Model performance correlates inversely with architectural complexity when sufficient training data is available.
- **Evidence anchors:**
  - [abstract]: "simpler architectures are generally competitive and scale well with larger datasets"
  - [section]: "simpler architectures with fewer modeling assumptions benefit more from additional training data"
  - [corpus]: Weak - corpus papers focus on specialized approaches rather than comparing architectural complexity directly
- **Break condition:** When datasets are small or highly structured, complex models with specialized inductive biases may outperform simpler alternatives.

### Mechanism 2
- **Claim:** Rank-based metrics complement traditional fit measures by detecting mode collapse and assessing model specificity.
- **Mechanism:** Rank metrics measure how well models order perturbations relative to each other, capturing specificity that RMSE and cosine similarity miss when models predict identical outputs for different inputs.
- **Core assumption:** In perturbation prediction, the ability to rank perturbations correctly is as important as predicting accurate absolute values.
- **Evidence anchors:**
  - [abstract]: "rank metrics which complement traditional model fit measures...for validating model effectiveness"
  - [section]: "mode or posterior collapse where a model always generates the same prediction irrespective of target perturbation may still result in decent cosine similarity or RMSE"
  - [corpus]: Weak - corpus papers mention ranking but don't establish rank metrics as primary evaluation tools
- **Break condition:** When perturbations have very similar effects or when absolute prediction accuracy is the sole priority.

### Mechanism 3
- **Claim:** Ablating complex model components often improves performance.
- **Mechanism:** Removing components that enforce strong assumptions (like disentanglement via adversarial training) reduces optimization difficulties and allows models to learn more flexible representations.
- **Core assumption:** Strong inductive biases can hinder learning when dataset characteristics don't align with those assumptions.
- **Evidence anchors:**
  - [abstract]: "Ablation studies reveal that removing adversarial components from CPA and sparsity constraints from SAMS-VAE improves performance"
  - [section]: "removing the adversary component or sparsity constraint leads to better performance"
  - [corpus]: Weak - corpus papers focus on model development rather than systematic ablation studies
- **Break condition:** When dataset characteristics strongly support the inductive biases being removed.

## Foundational Learning

- **Concept:** Cross-covariate generalization
  - **Why needed here:** Models must predict perturbation effects in cell types/lines where those perturbations haven't been observed
  - **Quick check question:** Can your model predict drug effects in a new cell line using only data from other cell lines?

- **Concept:** Counterfactual prediction
  - **Why needed here:** Single-cell RNA-seq destroys cells, so models must predict what a cell's state would be if perturbed
  - **Quick check question:** Does your model generate predictions by combining control cell states with perturbation representations?

- **Concept:** Mode collapse detection
  - **Why needed here:** Generative models may output identical predictions regardless of input, which rank metrics can detect
  - **Quick check question:** Does your model produce different outputs for different perturbations, or does it collapse to a single prediction?

## Architecture Onboarding

- **Component map:** Data module (Example, Dataset classes) → Model module (PerturbationModel base class) → Analysis module (evaluation metrics)
- **Critical path:** Dataset preparation → Model training with HPO → Counterfactual prediction generation → Comprehensive metric evaluation
- **Design tradeoffs:** Simpler models trade representational power for better generalization and scalability; complex models may overfit on smaller datasets
- **Failure signatures:** Mode collapse (identical predictions), poor rank metrics despite good RMSE, degradation with increased dataset size
- **First 3 experiments:**
  1. Train Latent Additive model on Srivatsan20 with default hyperparameters and evaluate rank metrics
  2. Train CPA model with and without adversarial component on same dataset to observe ablation effects
  3. Test Decoder-Only model performance on Jiang24 to assess whether perturbation information adds value over covariates alone

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the choice of loss function (MSE vs. NLL) impact the performance of CPA models across different datasets?
- **Basis in paper:** [explicit] The authors note that CPA models using MSE loss outperformed those using Gaussian or Negative Binomial NLL losses, and this was the primary reason their reimplemented CPA performed better than the public implementation.
- **Why unresolved:** The paper only compares MSE vs NLL losses for CPA on the Norman19 dataset. It's unclear if this trend holds across other datasets with different characteristics (e.g., chemical vs. genetic perturbations, different levels of data imbalance).
- **What evidence would resolve it:** Systematic ablation studies of CPA models trained with different loss functions (MSE, Gaussian NLL, Negative Binomial NLL) across all benchmark datasets, reporting performance on all metrics.

### Open Question 2
- **Question:** To what extent do perturbation embeddings (e.g., ESM embeddings, GEARS GO graph, RDKIT embeddings) improve the ability of perturbation response models to predict unseen perturbations?
- **Basis in paper:** [explicit] The authors mention that the performance of perturbation response models in predicting unseen perturbations is dependent on the quality of the perturbation representation, which is itself a complex task. They note that some models like GEARS and PerturbNet use perturbation embeddings, but this was outside the scope of their study.
- **Why unresolved:** The paper does not investigate the impact of perturbation embeddings on model performance. It's unclear whether incorporating high-quality perturbation embeddings could significantly improve the ability of models to generalize to unseen perturbations.
- **What evidence would resolve it:** Benchmarking perturbation response models with and without various perturbation embeddings (e.g., ESM, GEARS GO graph, RDKIT) on tasks involving unseen perturbations, reporting performance on rank-based metrics which are crucial for identifying effective perturbations.

### Open Question 3
- **Question:** How does the level of biological heterogeneity within a dataset (e.g., within-cell-type heterogeneity) affect the relative performance of autoencoder models versus simpler baseline models?
- **Basis in paper:** [inferred] The authors observe that autoencoder models (CPA, SAMS-VAE) generally outperform simpler baseline models (Latent Additive, Decoder-Only) on distributional metrics (MMD) in some datasets, but the gap varies across datasets. They speculate that this could be due to differences in within-cell-type heterogeneity.
- **Why unresolved:** The paper does not directly measure or control for biological heterogeneity within datasets. It's unclear whether the observed differences in model performance are driven by inherent differences in the datasets' biological complexity or other factors.
- **What evidence would resolve it:** Quantifying the level of biological heterogeneity within each dataset (e.g., using metrics like within-cell-type variance) and correlating this with the relative performance of autoencoder models versus baseline models across all benchmark datasets.

## Limitations
- The corpus analysis reveals limited direct support for the specific benchmarking claims, with an average neighbor FMR of 0.451 and no citations
- Preprocessing details for gene selection (top 4000 variable genes, top 25 DEGs per perturbation) are not fully specified
- Exact hyperparameter settings remain partially unspecified, which could affect reproducibility

## Confidence

- **Mechanism 1:** Medium - supported by abstract claims but weak corpus validation
- **Mechanism 2:** Low - novel approach with limited corpus precedent
- **Mechanism 3:** Medium - ablation studies are described but corpus lacks systematic comparisons

## Next Checks

1. Replicate the rank metric evaluation on a held-out test set from the Srivatsan20 dataset to confirm mode collapse detection capabilities
2. Perform systematic ablation studies comparing CPA with and without adversarial loss across all six datasets to verify performance improvements
3. Test cross-dataset generalization by training on one dataset and evaluating on perturbations from another to assess real-world applicability