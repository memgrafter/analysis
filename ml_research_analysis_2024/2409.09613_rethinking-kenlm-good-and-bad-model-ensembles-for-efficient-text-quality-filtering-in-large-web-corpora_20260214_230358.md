---
ver: rpa2
title: 'Rethinking KenLM: Good and Bad Model Ensembles for Efficient Text Quality
  Filtering in Large Web Corpora'
arxiv_id: '2409.09613'
source_url: https://arxiv.org/abs/2409.09613
tags:
- kenlm
- data
- good
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an ensemble approach using two contrasting
  KenLM models to improve text quality filtering in large web corpora. The traditional
  KenLM, trained only on high-quality data, struggles to detect low-quality content
  with similar linguistic patterns.
---

# Rethinking KenLM: Good and Bad Model Ensembles for Efficient Text Quality Filtering in Large Web Corpora

## Quick Facts
- **arXiv ID**: 2409.09613
- **Source URL**: https://arxiv.org/abs/2409.09613
- **Reference count**: 5
- **Primary result**: Ensemble of Good KenLM (high-quality data) and Bad KenLM (low-quality data) improves recall@30 and recall@60 by 9.76% and 2.50% compared to traditional methods.

## Executive Summary
This paper introduces an ensemble approach using two contrasting KenLM models to improve text quality filtering in large web corpora. The traditional KenLM, trained only on high-quality data, struggles to detect low-quality content with similar linguistic patterns. To address this, the authors propose combining a Good KenLM (trained on high-quality data like S2ORC and textbooks) with a Bad KenLM (trained on low-quality data like spam and social media). The ensemble method significantly improves recall@30 and recall@60 by 9.76% and 2.50%, respectively, compared to existing methods, while maintaining low computational overhead. This approach effectively filters out noisy content while preserving high-quality data, making it a practical solution for resource-constrained environments.

## Method Summary
The method involves training two separate KenLM models with n-gram size 6 and vocabulary size 65,536. Good KenLM is trained on S2ORC and Textbooks-are-all-you-need-lite datasets (300k samples each), while Bad KenLM is trained on Twitter (1M samples), spam messages (776k samples), and toxic datasets. The ensemble combines perplexity scores from both models using Z-score standardization and a weighted combination controlled by hyperparameter α. The approach is evaluated on the CC-MAIN-2024-10 dump from Fineweb-edu (211 million samples) using recall@30 and recall@60 metrics.

## Key Results
- Improved recall@30 by 9.76% compared to traditional methods
- Improved recall@60 by 2.50% compared to traditional methods
- Maintained low computational overhead suitable for resource-constrained environments
- Optimal α value found to be 0.7 for balancing quality preservation and noise removal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ensemble method captures complementary filtering signals that neither model alone can provide.
- Mechanism: Good KenLM excels at detecting high-quality text based on well-formed linguistic patterns but fails to identify low-quality text that mimics these patterns. Bad KenLM, conversely, identifies text with low-quality patterns but may incorrectly filter high-quality content that resembles its training distribution. By ensembling the two, the approach leverages Good KenLM's strength in preserving high-quality data and Bad KenLM's ability to remove low-quality content, achieving a balance that neither model achieves alone.
- Core assumption: The two models' error patterns are sufficiently independent and complementary.
- Evidence anchors:
  - [abstract] "We argue that the traditional KenLM does not explicitly learn the linguistic patterns of low-quality data."
  - [section 4.2] "Although Bad KenLM alone showed poor performance, our strategy of ensembling it with Good KenLM outperformed even FastText trained on the same data, improving Recall@30 and Recall@60 by 9.76% and 2.50%, respectively."
  - [corpus] Weak - limited comparison of independent error patterns.
- Break condition: If Good KenLM and Bad KenLM make correlated errors (e.g., both fail on a specific class of low-quality text), the ensemble provides little benefit.

### Mechanism 2
- Claim: The Z-score standardization aligns the scales of the two perplexity scores, enabling meaningful combination.
- Mechanism: Good KenLM and Bad KenLM are trained on different data distributions, resulting in different scales and distributions of perplexity scores. Z-score standardization transforms these scores to a common scale (mean 0, variance 1) based on their respective training data distributions, allowing for direct comparison and combination. This prevents one model's scores from dominating the ensemble due to scale differences.
- Core assumption: The perplexity score distributions of each model are roughly Gaussian and can be meaningfully standardized.
- Evidence anchors:
  - [section 3] "We perform Z-score standardization to align the scales of the two PPL scores assigned by each model, as they are trained on different datasets and therefore exhibit different distributions of PPL scores."
  - [corpus] Weak - no explicit distributional analysis provided.
- Break condition: If the perplexity score distributions are heavily skewed or multimodal, Z-score standardization may not adequately align the scales.

### Mechanism 3
- Claim: The hyperparameter α controls the trade-off between preserving high-quality content and filtering low-quality content.
- Mechanism: α determines the relative weight given to Good KenLM and Bad KenLM in the ensemble. A higher α emphasizes Good KenLM's ability to preserve high-quality content, while a lower α emphasizes Bad KenLM's ability to filter low-quality content. The optimal α balances these competing objectives, maximizing the ensemble's overall performance.
- Core assumption: The optimal α is stable across different datasets and quality distributions.
- Evidence anchors:
  - [section 4.3] "As depicted in Figure 1, Recall@30 and Recall@60 continuously improve as α increases to 0.7 and 0.6, respectively, and then gradually decrease."
  - [section 4.2] "These results suggest that when α is too small, the influence of Bad KenLM becomes overly dominant, resulting in poor preservation of high-quality content. Conversely, when α is too large, the influence of Good KenLM prevails, leading to the inclusion of some low-quality content."
  - [corpus] Weak - sensitivity analysis limited to one dataset.
- Break condition: If the optimal α varies significantly across different datasets or quality distributions, a single α may not be optimal for all scenarios.

## Foundational Learning

- Concept: Language model perplexity and its relationship to text quality.
  - Why needed here: Understanding how perplexity reflects text quality is fundamental to understanding how KenLM models filter data. Good KenLM assigns low perplexity to high-quality text, while Bad KenLM assigns low perplexity to low-quality text. This understanding is crucial for interpreting the ensemble's behavior.
  - Quick check question: If a text sample has a high perplexity score under Good KenLM, what does that indicate about its quality?

- Concept: N-gram language models and their computational efficiency.
  - Why needed here: KenLM is an n-gram language model known for its computational efficiency on CPUs. This efficiency is a key advantage of the approach, enabling filtering of large web corpora without requiring GPU resources.
  - Quick check question: Why is KenLM preferred over other language models for large-scale text filtering in resource-constrained environments?

- Concept: Ensemble methods and their benefits in machine learning.
  - Why needed here: The proposed approach is an ensemble method that combines the strengths of two contrasting models. Understanding the principles of ensemble methods, such as error complementarity and bias-variance trade-off, is essential for understanding the ensemble's effectiveness.
  - Quick check question: What is the primary advantage of using an ensemble method over a single model in this context?

## Architecture Onboarding

- Component map: Data Ingestion -> Good KenLM -> Perplexity Scoring -> Z-score Standardization -> Ensemble Scoring -> Filtering <- Bad KenLM <- Perplexity Scoring <- Z-score Standardization
- Critical path: Data Ingestion → Perplexity Scoring → Z-score Standardization → Ensemble Scoring → Filtering
- Design tradeoffs:
  - Training data selection: Balancing the quality and quantity of training data for both Good KenLM and Bad KenLM.
  - Hyperparameter α: Finding the optimal balance between preserving high-quality content and filtering low-quality content.
  - Computational overhead: Managing the additional processing time and cost of the ensemble approach compared to a single KenLM.
- Failure signatures:
  - Low recall of high-quality data: Indicates α is too low or Bad KenLM is overly aggressive in filtering.
  - High inclusion of low-quality data: Indicates α is too high or Good KenLM is not effectively detecting low-quality content.
  - High computational overhead: Indicates the ensemble approach is not suitable for the available resources.
- First 3 experiments:
  1. Evaluate the individual performance of Good KenLM and Bad KenLM on a validation set to understand their strengths and weaknesses.
  2. Tune the hyperparameter α to find the optimal balance between preserving high-quality content and filtering low-quality content.
  3. Compare the ensemble's performance to a single KenLM and other baseline methods (e.g., FastText) on a held-out test set.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations section, several questions emerge regarding the broader applicability and impact of the approach.

## Limitations
- Reliance on quality and representativeness of training data for both Good KenLM and Bad KenLM models.
- Potential degradation in performance when applied to datasets significantly different from the evaluation corpus.
- Need for dataset-specific tuning of hyperparameter α for optimal performance.

## Confidence
- **High Confidence**: The computational efficiency of KenLM and the general concept of using ensemble methods for complementary filtering signals.
- **Medium Confidence**: The effectiveness of the Z-score standardization in aligning perplexity score distributions and the specific value of the hyperparameter α.
- **Low Confidence**: The robustness of the ensemble approach to diverse types of low-quality content and its generalizability to datasets significantly different from the evaluation corpus.

## Next Checks
1. **Error Analysis**: Conduct a detailed error analysis to identify the types of low-quality content that the ensemble fails to filter and the types of high-quality content that it incorrectly removes. This will reveal whether Good KenLM and Bad KenLM make correlated errors and guide improvements to the training data or ensemble strategy.
2. **Cross-Dataset Evaluation**: Evaluate the ensemble approach on multiple datasets with varying quality distributions and characteristics to assess its generalizability and identify potential failure modes. This will help determine whether the optimal α is stable across different datasets.
3. **Computational Overhead Analysis**: Measure the computational overhead of the ensemble approach compared to a single KenLM on different hardware configurations and corpus sizes. This will provide a more accurate assessment of its practicality for resource-constrained environments and identify potential bottlenecks.