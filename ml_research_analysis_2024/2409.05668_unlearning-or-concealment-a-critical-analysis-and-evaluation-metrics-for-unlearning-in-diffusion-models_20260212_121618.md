---
ver: rpa2
title: Unlearning or Concealment? A Critical Analysis and Evaluation Metrics for Unlearning
  in Diffusion Models
arxiv_id: '2409.05668'
source_url: https://arxiv.org/abs/2409.05668
tags:
- knowledge
- unlearned
- concept
- unlearning
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a critical analysis of unlearning in diffusion
  models, demonstrating that existing methods primarily achieve concept concealment
  rather than true forgetting. The authors introduce two novel evaluation metrics
  - Concept Retrieval Score (CRS) and Concept Confidence Score (CCS) - which leverage
  a partial diffusion-based adversarial attack framework to assess unlearning effectiveness.
---

# Unlearning or Concealment? A Critical Analysis and Evaluation Metrics for Unlearning in Diffusion Models

## Quick Facts
- arXiv ID: 2409.05668
- Source URL: https://arxiv.org/abs/2409.05668
- Authors: Aakash Sen Sharma; Niladri Sarkar; Vikram Chundawat; Ankur A Mali; Murari Mandal
- Reference count: 40
- Key outcome: This paper presents a critical analysis of unlearning in diffusion models, demonstrating that existing methods primarily achieve concept concealment rather than true forgetting.

## Executive Summary
This paper critically examines diffusion model unlearning methods, revealing that they primarily achieve concept concealment rather than true forgetting. The authors introduce two novel evaluation metrics—Concept Retrieval Score (CRS) and Concept Confidence Score (CCS)—which leverage a partial diffusion-based adversarial attack framework to assess unlearning effectiveness. Testing five state-of-the-art unlearning methods, the results show that existing approaches fail to completely erase targeted concepts, with residual traces remaining in the model's latent space. The proposed metrics reveal significant shortcomings in current methods that traditional metrics like KID and CLIP scores miss, highlighting the need for more rigorous evaluation of unlearning in generative models.

## Method Summary
The authors develop a partial diffusion pipeline that divides the denoising process into initial steps using a fully trained model and remaining steps using an unlearned model. They create three datasets: λO (original model outputs), λU (unlearned model outputs), and λP (partially diffused outputs at varying ratios). For evaluation, they fine-tune three feature extractors (ResNet18, DenseNet121, EfficientNet-B0) for binary classification to distinguish between original and unlearned domain knowledge. CRS measures cosine similarity between feature embeddings of generated images and reference images, while CCS measures the probability of domain alignment. The framework tests five unlearning methods on Stable Diffusion 1.4 across three concept categories (art style, identity, NSFW content).

## Key Results
- Existing unlearning methods achieve concept concealment rather than true forgetting, with residual traces of concepts remaining in latent space
- CRS and CCS metrics reveal significant shortcomings in current unlearning methods that traditional metrics (KID, CLIP) miss
- At partial diffusion ratio ψ ≈ 0.55, supposedly forgotten concepts can be successfully retrieved from unlearned models
- The proposed metrics show high sensitivity in detecting concept presence at latent stages of diffusion, even when final outputs appear to have forgotten the concept

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Existing diffusion unlearning methods achieve concept concealment rather than true forgetting.
- Mechanism: The methods focus on decoupling prompts from noise predictions by increasing L2 loss rather than removing concept information from model parameters.
- Core assumption: The information about the concept remains encoded in the model's parameters even after unlearning.
- Evidence anchors:
  - [abstract] "We show that existing unlearning methods lead to decoupling of the targeted concepts (meant to be forgotten) for the corresponding prompts. This is concealment and not actual forgetting, which was the original goal."
  - [section 3.1] "The existing unlearning methods [12, 25] primarily increase the L2 loss for noise predictions related to the forget concept without explicitly removing the concept information from the model's parameters."
  - [corpus] Weak - corpus papers focus on alternative unlearning approaches but don't directly address the concealment vs. forgetting distinction

### Mechanism 2
- Claim: Partial diffusion can recover supposedly forgotten concepts from unlearned models.
- Mechanism: At certain partial diffusion ratios, the latent representation contains sufficient information about the concept for the unlearned model to recover it.
- Core assumption: The mutual information between latent representations and concepts increases gradually during the diffusion process.
- Evidence anchors:
  - [abstract] "We introduce two new evaluation metrics: Concept Retrieval Score (CRS) and Concept Confidence Score (CCS). These metrics are based on a successful adversarial attack setup that can recover forgotten concepts from unlearned diffusion models."
  - [section 3.1] "During the process of image generation, there exists a critical point where the mutual information between the latent representation and a specific concept becomes significant."
  - [corpus] Weak - corpus papers focus on unlearning methods but don't specifically address partial diffusion attacks

### Mechanism 3
- Claim: CCS and CRS metrics effectively detect concept concealment that traditional metrics miss.
- Mechanism: These metrics evaluate concept presence at latent stages of diffusion rather than just final outputs, using feature embeddings and probability scores.
- Core assumption: Concept information persists in latent representations even when final outputs appear to have forgotten the concept.
- Evidence anchors:
  - [abstract] "CRS measures the similarity between the latent representations of the unlearned and fully trained models after unlearning. It reports the extent of retrieval of the forgotten concepts with increasing amount of guidance."
  - [section 3.3] "The CRS quantifies the alignment between generated images and the original or unlearned domain knowledge, measured through cosine similarity of feature embeddings extracted from a fine-tuned model."
  - [section 4.1] "CCS and CRS scores effectively measure if the targeted (to be erased) concept has be completely unlearned or if the method just helped in concealment of concepts."

## Foundational Learning

- Concept: Mutual information in diffusion models
  - Why needed here: Understanding how concept information flows through the denoising process is crucial for grasping why partial diffusion attacks work
  - Quick check question: At what point in the diffusion process does the mutual information between latent representations and concepts become significant?

- Concept: Earth Mover's Distance (EMD) in optimal transport theory
  - Why needed here: EMD provides the mathematical framework for quantifying the difference between pre-unlearning and post-unlearning distributions
  - Quick check question: How does EMD differ from simpler distance metrics like KL divergence when measuring distribution changes?

- Concept: Feature embeddings and cosine similarity
  - Why needed here: CCS and CRS rely on comparing feature embeddings of generated images to reference images using cosine similarity
  - Quick check question: Why is cosine similarity preferred over Euclidean distance for comparing feature embeddings in this context?

## Architecture Onboarding

- Component map: Diffusion model (original vs. unlearned) -> Partial diffusion pipeline -> Feature extractor -> Evaluation metrics -> Reference datasets
- Critical path: Partial diffusion attack → Feature extraction → Metric calculation → Concealment detection
- Design tradeoffs:
  - Tradeoff between computational cost and evaluation rigor (partial diffusion vs. full generation)
  - Choice of feature extractor affects metric sensitivity
  - Balance between perturbation strength and meaningful evaluation
- Failure signatures:
  - High KID/CLIP scores but low CCS/CRS scores indicate concealment
  - Sudden drops in CCS/CRS at specific partial diffusion ratios suggest concept resurgence
  - Inconsistent behavior across different feature extractors may indicate metric instability
- First 3 experiments:
  1. Reproduce the partial diffusion attack on a known unlearning method (ESD) to verify concept recovery
  2. Compare CCS/CRS scores with traditional metrics (KID, CLIP) on the same unlearning method
  3. Test the sensitivity of CCS/CRS to different feature extractors (ResNet vs. EfficientNet)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed partial diffusion framework perform when applied to other generative model architectures beyond diffusion models, such as GANs or VAEs?
- Basis in paper: [inferred] The paper focuses exclusively on diffusion models but notes that the partial diffusion pipeline operates "independently of any specific modality" and "offers a partially denoised latent with an optional modality input to guide the model."
- Why unresolved: The paper only demonstrates the framework on diffusion models, leaving its generalizability to other generative architectures unexplored.
- What evidence would resolve it: Experiments applying the partial diffusion framework to GANs, VAEs, or other generative models with comparable metrics and evaluation procedures.

### Open Question 2
- Question: What are the theoretical bounds on the partial diffusion ratio (ψ) that guarantee complete unlearning versus mere concealment?
- Basis in paper: [explicit] The paper identifies a critical threshold at ψ ≈ 0.55 for concept retrieval but does not establish theoretical bounds or guarantees for unlearning effectiveness.
- Why unresolved: The paper provides empirical observations of thresholds but lacks rigorous mathematical bounds or proofs for when unlearning is guaranteed versus when concealment occurs.
- What evidence would resolve it: Formal mathematical proofs or theoretical analysis establishing conditions under which partial diffusion ratios guarantee complete unlearning rather than concealment.

### Open Question 3
- Question: How do the proposed CRS and CCS metrics correlate with human perceptual judgments of unlearning effectiveness?
- Basis in paper: [inferred] The paper introduces CRS and CCS as quantitative metrics but does not validate them against human judgments of whether concepts have been truly forgotten or merely concealed.
- Why unresolved: While the metrics provide quantitative measures, their alignment with human perception of unlearning effectiveness remains untested.
- What evidence would resolve it: User studies comparing human assessments of unlearning quality with the quantitative CRS and CCS scores to establish correlation and validity.

## Limitations
- The evaluation focuses primarily on Stable Diffusion 1.4 and five specific unlearning methods, limiting generalizability
- The paper demonstrates theoretical vulnerabilities but doesn't extensively explore security implications or robustness against different attack scenarios
- The computational overhead of the proposed evaluation framework may limit its practical adoption

## Confidence
- **High confidence**: Core claim that existing diffusion unlearning methods achieve concept concealment rather than true forgetting
- **Medium confidence**: Generalizability of findings to other diffusion models and unlearning approaches
- **Low confidence**: Practical implications for real-world deployment and security considerations

## Next Checks
1. **Cross-model validation**: Apply the CCS/CRS framework to other diffusion models (e.g., Stable Diffusion XL, DALL-E variants) to test generalizability beyond SD 1.4.

2. **Attack surface analysis**: Systematically vary the partial diffusion parameters (ratios, inference steps, guidance scales) to identify the most effective attack configurations and establish robustness bounds.

3. **Real-world deployment testing**: Evaluate the proposed metrics on models deployed in production environments with varying data distributions to assess practical utility and identify potential failure modes in real-world scenarios.