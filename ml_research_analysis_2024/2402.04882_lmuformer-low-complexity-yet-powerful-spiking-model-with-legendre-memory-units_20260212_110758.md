---
ver: rpa2
title: 'LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory
  Units'
arxiv_id: '2402.04882'
source_url: https://arxiv.org/abs/2402.04882
tags:
- lmuformer
- spiking
- performance
- arxiv
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of developing energy-efficient
  models for streaming applications at the edge, where devices are resource-constrained.
  While transformer models excel in performance, their high complexity and lack of
  sequential processing capability make them ill-suited for these scenarios.
---

# LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units

## Quick Facts
- arXiv ID: 2402.04882
- Source URL: https://arxiv.org/abs/2402.04882
- Authors: Zeyu Liu; Gourav Datta; Anni Li; Peter Anthony Beerel
- Reference count: 18
- Primary result: Achieves 96.12% accuracy on Speech Commands V2 while reducing parameters by 53x and FLOPs by 65x compared to transformers

## Executive Summary
LMUFormer addresses the challenge of developing energy-efficient models for streaming applications at the edge by augmenting the Legendre Memory Unit (LMU) with convolutional patch embedding and convolutional channel mixers. This architecture enables sequential data processing while maintaining high performance, achieving comparable accuracy to state-of-the-art transformers with significantly reduced complexity. The paper also presents a spiking version that further reduces computational complexity while establishing new state-of-the-art performance among spiking neural networks.

## Method Summary
The method involves developing LMUFormer by augmenting the Legendre Memory Unit with convolutional patch embedding (1D convolution with kernel size 3) and convolutional channel mixers (1x1 convolutions with BN and activations). The model is trained using Adam optimizer for psMNIST and AdamW with 1cycle learning rate policy for other datasets, with FFT-based parallel training to reduce complexity. A spiking version replaces floating-point operations with binary spike propagation using LIF neurons. The approach is evaluated on psMNIST, Speech Commands V2, and Long Range Arena benchmark datasets.

## Key Results
- LMUFormer achieves 96.12% accuracy on Speech Commands V2, establishing state-of-the-art among spiking neural networks
- Reduces parameters by 53x and FLOPs by 65x compared to transformer models while maintaining comparable performance
- Achieves 32.03% reduction in sequence length through real-time processing capabilities
- Outperforms transformer variants on LRA benchmark while maintaining streaming inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Convolutional patch embedding captures local sequential dependencies with minimal latency
- Mechanism: 1D convolution with kernel size 3 processes each output based on current and two neighboring samples, enabling streaming inference with only 9-sample delay
- Core assumption: Nearby samples contain sufficient correlation for prediction, and 3-sample receptive field captures these patterns without excessive latency
- Evidence anchors: Kernel size 3 dramatically improves performance with negligible latency; model processes input in real-time after only 9-sample delay
- Break condition: If data has long-range dependencies beyond 3 samples or latency budget is extremely tight (< 9 samples)

### Mechanism 2
- Claim: Convolutional channel mixer augments LMU's temporal mixing with efficient cross-channel feature fusion
- Mechanism: 1x1 convolution with BN and activation layers mixes information across feature channels after LMU produces state vector, preserving streaming inference while expanding representational capacity
- Core assumption: Cross-channel mixing is separate from temporal mixing; LMU handles time, channel mixer handles features
- Evidence anchors: Channel mixer consists of BN and non-linear activation layers followed by 1×1 convolutional layers that don't interact with temporal information
- Break condition: If cross-channel interactions aren't beneficial or 1x1 convolution overfits due to limited data

### Mechanism 3
- Claim: Spiking LMUFormer achieves ultra-low energy by converting intermediate states to binary spikes
- Mechanism: Replaces floating-point operations with binary spike propagation and accumulation-only updates using LIF neuron dynamics that integrate inputs and emit spikes when thresholds are crossed
- Core assumption: Binary spike propagation with accumulation-only updates approximates original computations closely enough while saving energy
- Evidence anchors: Spiking LMUFormer achieves state-of-the-art performance with 96.12% accuracy on Speech Commands dataset
- Break condition: If spike rates become too high or threshold dynamics are poorly tuned, energy advantage erodes and accuracy suffers

## Foundational Learning

- Concept: Legendre Memory Units (LMU)
  - Why needed here: Provides continuous-time state-space representation that remembers information over theoretically infinite horizons, addressing RNN forgetting without quadratic complexity
  - Quick check question: What mathematical basis does the LMU use to achieve long-range memory without exploding gradients?

- Concept: Spiking Neural Networks (SNNs) and LIF neurons
  - Why needed here: Enables event-driven, low-power computation by communicating via binary spikes, critical for edge deployment
  - Quick check question: In a LIF neuron, what happens to the membrane potential when a spike is emitted, and how does this reset behavior affect temporal integration?

- Concept: Convolutional patch embedding in sequential models
  - Why needed here: Processes one sample at a time unlike standard ViT patch embeddings, enabling streaming while still extracting local context
  - Quick check question: How does a 1D convolution with kernel size 3 differ from a fully connected layer when applied to streaming sequential data?

## Architecture Onboarding

- Component map: Input → Convolutional patch embedding (1D conv, kernel=3) → LMU block (stateful temporal mixing) → Convolutional channel mixer (1x1 conv + BN + activations) → Classification head (fully connected)

- Critical path: Sample arrives → patch embedding produces output after 9-sample delay → LMU updates state and produces output → channel mixer processes features → final prediction

- Design tradeoffs: Streaming vs full-sequence processing (small accuracy drop for real-time edge inference), parameter efficiency vs accuracy (53x fewer parameters), energy vs accuracy in spiking mode (binary operations reduce energy but require threshold tuning)

- Failure signatures: Accuracy plateaus early in sequence length tests (receptive field too small), overfitting on small datasets (excessive channel mixer depth), spike rate too high in SNN (energy savings disappear)

- First 3 experiments: 1) Ablation: Remove convolutional channel mixer and compare accuracy vs full LMUFormer, 2) Latency sweep: Vary kernel size in patch embedding and measure accuracy vs algorithmic delay, 3) Spiking threshold sweep: Adjust LIF threshold and leak parameters to find optimal spike sparsity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LMUFormer be effectively pre-trained on large-scale datasets and fine-tuned on downstream tasks?
- Basis in paper: Paper suggests pre-training empowers Transformers but notes no large-scale streaming dataset exists for LMUFormer
- Why unresolved: Authors hypothesize scaling requires architectural optimizations beyond stacking LMUFormer blocks
- What evidence would resolve it: Experiments pre-training LMUFormer on large-scale datasets and fine-tuning on downstream tasks, comparing performance to Transformers

### Open Question 2
- Question: How can LMUFormer's performance be improved for long-context scenarios?
- Basis in paper: LMUFormer outperforms transformer variants but cannot surpass S4 models on LRA dataset
- Why unresolved: Paper identifies this as important research direction without concrete solution
- What evidence would resolve it: Developing and testing architectural modifications that improve performance on long-context tasks like LRA

### Open Question 3
- Question: Can compute efficiency of spiking LMUFormer be further improved by optimizing data reuse scheme and hardware implementation?
- Basis in paper: Paper estimates compute energy but doesn't explore potential optimizations in data reuse and hardware implementation
- Why unresolved: Focus is on comparing compute energy of spiking vs non-spiking variants without delving into hardware optimizations
- What evidence would resolve it: Experiments evaluating impact of different data reuse schemes and hardware implementations on compute efficiency

## Limitations
- Spiking variant's energy efficiency claims lack experimental validation through hardware measurements or detailed energy profiling
- Generalization of results across diverse sequence lengths and domains remains unclear due to limited dataset coverage
- Paper lacks complete implementation details and extensive ablation studies to isolate individual component contributions

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Parameter reduction (53x fewer parameters) and computational complexity reduction (65x fewer FLOPs) | High |
| Accuracy improvements and streaming latency benefits | Medium |
| Energy efficiency claims for spiking variant | Low |

## Next Checks
1. **Ablation study**: Systematically remove convolutional channel mixer from LMUFormer and retrain to quantify its exact contribution to accuracy improvements

2. **Energy profiling**: Implement spiking LMUFormer on neuromorphic hardware or through detailed cycle-accurate simulation to measure actual energy consumption per inference

3. **Generalization testing**: Evaluate LMUFormer across a broader range of sequence lengths and on time-series domains not covered in current experiments to assess robustness to varying temporal dependencies