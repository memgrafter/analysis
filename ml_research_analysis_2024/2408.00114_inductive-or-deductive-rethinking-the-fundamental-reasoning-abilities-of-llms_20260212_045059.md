---
ver: rpa2
title: Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs
arxiv_id: '2408.00114'
source_url: https://arxiv.org/abs/2408.00114
tags:
- reasoning
- llms
- inductive
- deductive
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) are
  better at deductive or inductive reasoning by designing tasks that cleanly separate
  the two. It introduces SolverLearner, a framework that forces LLMs to infer a function
  from examples (inductive step) and then use an external code interpreter to apply
  it (avoiding deductive reasoning).
---

# Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs

## Quick Facts
- arXiv ID: 2408.00114
- Source URL: https://arxiv.org/abs/2408.00114
- Reference count: 40
- Primary result: SolverLearner achieves near-perfect inductive reasoning performance while LLMs struggle with deductive reasoning, especially counterfactual cases.

## Executive Summary
This paper challenges the assumption that large language models excel at both inductive and deductive reasoning by introducing a framework that cleanly separates these two types of reasoning. Through carefully designed tasks across arithmetic, syntactic, spatial, and cipher domains, the authors demonstrate that traditional in-context learning prompting (IO) primarily leverages deductive reasoning, which LLMs perform poorly on - particularly for counterfactual scenarios. In contrast, their proposed SolverLearner framework forces models to infer functions from examples (inductive step) and use external code execution to apply them, achieving near-perfect performance on inductive reasoning tasks.

## Method Summary
The paper introduces SolverLearner, a framework that separates inductive and deductive reasoning by having LLMs first infer a function from few-shot examples, then use an external code interpreter to apply it. For each task (arithmetic in various bases, modified English syntax, spatial coordinate systems, cipher decryption), four prompting methods are tested: zero-shot, 8-IO with manual function extraction, 8-IO without manual function extraction, and 8-shot SolverLearner using GPT-3.5 and GPT-4. The framework's key innovation is preventing deductive reasoning during execution by forcing code generation and external interpretation.

## Key Results
- SolverLearner achieves near-perfect accuracy (ACC=1) on inductive reasoning tasks across all domains tested
- Traditional IO prompting shows poor performance, especially on counterfactual reasoning tasks (non-standard bases, word orders)
- LLMs demonstrate strong inductive reasoning capabilities when deductive reasoning is explicitly prevented during execution
- The performance gap between inductive and deductive reasoning suggests LLMs fundamentally reason differently than assumed

## Why This Works (Mechanism)
The framework works by structurally preventing the model from using deductive reasoning during the execution phase. By forcing the model to first infer a function from examples (inductive step) and then rely on an external code interpreter to apply it, SolverLearner eliminates the need for the model to reason through individual cases deductively. This separation reveals that LLMs' apparent reasoning capabilities in traditional prompting setups largely stem from pattern matching and deductive inference rather than true function abstraction.

## Foundational Learning
- Inductive reasoning: Inferring general rules from specific examples; needed because it reveals LLMs' ability to abstract patterns rather than memorize; quick check: can model generalize to unseen but structurally similar inputs?
- Deductive reasoning: Applying general rules to specific cases; needed because it represents the traditional evaluation of reasoning; quick check: can model handle counterfactual or modified scenarios?
- Function abstraction: Extracting underlying computational rules from examples; needed because it separates true understanding from pattern matching; quick check: does generated code correctly implement the inferred function?
- Code interpretation: External execution of generated code; needed to prevent deductive reasoning during application; quick check: does interpreter produce correct outputs for the inferred function?

## Architecture Onboarding

**Component map:** Task examples -> LLM function proposal -> Python code generation -> External interpreter execution -> Output verification

**Critical path:** Few-shot examples → Function inference → Code generation → External execution → Answer production

**Design tradeoffs:** 
- Pros: Clean separation of reasoning types, near-perfect inductive performance, reveals true reasoning capabilities
- Cons: Relies on external interpreter, assumes code generation is pure inductive inference, limited to tasks expressible in code

**Failure signatures:** 
- Incorrect function inference from examples (fails early in SolverLearner pipeline)
- Poor counterfactual reasoning performance (indicates deductive reasoning limitations)
- Minimal improvement from increased few-shot examples (suggests reasoning limitations beyond data scale)

**3 first experiments:**
1. Replicate arithmetic tasks with base-9, base-11, and base-16 to verify counterfactual reasoning failures
2. Test SolverLearner on a new domain (e.g., logical reasoning puzzles) to assess generalizability
3. Compare SolverLearner performance with increasing numbers of in-context examples (4, 8, 16 shots) to identify performance plateaus

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of increasing the number of in-context examples beyond 16 on the inductive reasoning performance of LLMs?
- Basis in paper: [explicit] The paper states that "the results indicate that an increase in the number of in-context examples yields only slight improvements across both deductive and inductive reasoning scenarios."
- Why unresolved: The paper only tests up to 16-shot examples, and the marginal improvements suggest a potential plateau in performance gains.
- What evidence would resolve it: Conducting experiments with a wider range of in-context examples (e.g., 32, 64, 128) and analyzing the performance trends would provide insights into the scalability of the SolverLearner framework.

### Open Question 2
- Question: How does the SolverLearner framework perform on tasks that require complex, multi-step inductive reasoning beyond the tasks explored in the paper?
- Basis in paper: [inferred] The paper demonstrates the framework's effectiveness on arithmetic, syntactic, spatial, and cipher decryption tasks, but these tasks are relatively well-defined and constrained.
- Why unresolved: The paper does not explore the framework's performance on tasks with higher complexity or those requiring abstract reasoning.
- What evidence would resolve it: Applying the SolverLearner framework to tasks such as natural language inference, commonsense reasoning, or scientific problem-solving would provide a more comprehensive evaluation of its capabilities.

### Open Question 3
- Question: What are the underlying mechanisms that enable LLMs to achieve near-perfect inductive reasoning performance with the SolverLearner framework?
- Basis in paper: [inferred] The paper shows that LLMs can achieve remarkable inductive reasoning performance through SolverLearner, but it does not delve into the specific mechanisms or patterns that LLMs exploit to infer the underlying functions.
- Why unresolved: Understanding the inner workings of LLMs in the context of inductive reasoning would provide valuable insights into their reasoning capabilities and limitations.
- What evidence would resolve it: Conducting detailed analyses of the code generated by LLMs in the function proposal stage, as well as examining the intermediate reasoning steps, would shed light on the mechanisms employed by LLMs for inductive reasoning.

## Limitations
- The framework's assumption that code generation cleanly separates inductive from deductive reasoning may not hold in all cases
- Testing only GPT-3.5 and GPT-4 limits generalizability to other model families and sizes
- The paper doesn't provide the specific few-shot examples used, making exact replication difficult

## Confidence
- **High confidence**: LLMs struggle with counterfactual reasoning tasks (base-9/11/16 arithmetic, non-standard word orders)
- **Medium confidence**: SolverLearner enables near-perfect inductive reasoning performance
- **Medium confidence**: Overall conclusion that LLMs are better at inductive than deductive reasoning

## Next Checks
1. Test the same methodology across a broader range of model sizes and architectures (Claude, LLaMA, Mistral) to assess generalizability
2. Evaluate whether the inductive/deductive separation holds when using different code generation frameworks or when models must execute logic internally rather than via external interpreter
3. Conduct ablation studies varying the number and quality of few-shot examples to determine sensitivity of SolverLearner's performance to example selection