---
ver: rpa2
title: On time series clustering with k-means
arxiv_id: '2410.14269'
source_url: https://arxiv.org/abs/2410.14269
tags:
- k-means
- time
- clustering
- algorithm
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper identifies significant inconsistencies in how k-means\
  \ Lloyd\u2019s algorithm is configured across time series clustering (TSCL) literature,\
  \ which impedes fair comparison of TSCL algorithms. It proposes a standardised,\
  \ end-to-end Lloyd\u2019s implementation that consistently uses a single distance\
  \ function in all four algorithm stages (initialisation, assignment, update, stopping)."
---

# On time series clustering with k-means

## Quick Facts
- arXiv ID: 2410.14269
- Source URL: https://arxiv.org/abs/2410.14269
- Reference count: 21
- The paper identifies significant inconsistencies in how k-means Lloyd's algorithm is configured across time series clustering (TSCL) literature, which impedes fair comparison of TSCL algorithms.

## Executive Summary
This paper addresses the lack of standardization in k-means Lloyd's algorithm implementations for time series clustering (TSCL). The authors identify that existing implementations often use inconsistent distance functions across the four stages of Lloyd's algorithm (initialisation, assignment, update, stopping), which impedes fair comparison of TSCL algorithms. They propose a standardised, end-to-end Lloyd's implementation that consistently uses a single distance function throughout all stages. The study evaluates seven Lloyd's-based TSCL variants on the UCR archive and demonstrates that k-means-soft-dba performs best but is extremely slow, while k-means-DBA and MSM are significantly faster and form the top clique for most metrics.

## Method Summary
The study implements seven variants of Lloyd's algorithm for time series clustering: k-means-Euclidean, k-means-DTW, k-means-MSM, k-shape, k-means-DBA, k-SC, and k-means-soft-dba. All variants use a standardized configuration with Forgy initialization (10 restarts), inertia-based stopping (1e-6 tolerance), and empty cluster handling via maximum inertia reduction. The algorithms are evaluated on 112 UCR time series datasets using both train-test split and combined test-train configurations. Clustering quality is measured using CL-ACC, ARI, NMI, AMI, and RI metrics, while runtime is recorded for each algorithm-dataset pair.

## Key Results
- k-means-soft-dba achieves the best overall clustering performance but is extremely slow (17.62 hours per dataset)
- k-means-DBA and MSM form the top clique for most metrics while being significantly faster than k-means-soft-dba
- k-means-Euclidean performs well in terms of speed but provides lower clustering accuracy compared to elastic distance methods
- k-means-DTW with arithmetic averaging performs poorly, validating prior findings about naive DTW usage
- Standardizing distance functions across all Lloyd's stages significantly improves clustering consistency and performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using the same distance function end-to-end across all four Lloyd's stages significantly improves TSCL clustering consistency and performance.
- **Mechanism:** Standard k-means implementations often use Euclidean distance for initialisation and stopping but elastic distances (like DTW) only for assignment. This mismatch causes poor centroid updates and suboptimal convergence. Using one consistent distance function ensures the algorithm optimizes the same objective throughout.
- **Core assumption:** The distance function used for assignment is also appropriate for measuring cluster quality in the other stages.
- **Evidence anchors:**
  - [abstract] "It proposes a standardised, end-to-end Lloyd's implementation that consistently uses a single distance function in all four algorithm stages"
  - [section] "Our implementation of k-means uses the same distance function throughout the process: we say it employs an end-to-end distance function."
  - [corpus] No direct evidence in corpus papers about end-to-end distance usage. They focus on clustering speed/accuracy but not internal consistency.
- **Break condition:** The distance function becomes inappropriate for initialisation or stopping (e.g., if it doesn't satisfy triangle inequality or creates empty clusters frequently).

### Mechanism 2
- **Claim:** Proper initialization with restarts (Forgy or random with 10 restarts) reduces clustering variability and improves reproducibility compared to single random initializations.
- **Mechanism:** Random initialization can lead to poor local optima and empty clusters. Restarting multiple times with different initializations and selecting the best result based on inertia reduces sensitivity to initialization.
- **Core assumption:** The best initialization across restarts correlates with better clustering quality.
- **Evidence anchors:**
  - [section] "Celebi et al (2013) did not provide a definitive answer to the best initialisation strategy, but recommended approaches such as greedy k-means++... Alternatively, Forgy or random should be used with restarts"
  - [section] "We elect to use Forgy as it is the simplest option and because it is based on instances in the train data"
  - [corpus] No corpus evidence directly addresses initialization with restarts for TSCL specifically.
- **Break condition:** If the number of restarts is too small or the distance function creates empty clusters frequently, restarts won't improve consistency.

### Mechanism 3
- **Claim:** Empty cluster handling by selecting the time series furthest from its assigned centroid improves convergence compared to random reassignment or ignoring empty clusters.
- **Mechanism:** When a cluster becomes empty, randomly selecting a new centroid can disrupt convergence toward a good local optimum. Selecting the time series that would reduce inertia most ensures the new centroid meaningfully improves clustering quality.
- **Core assumption:** The time series furthest from its current centroid is the best candidate to become a new centroid when an empty cluster forms.
- **Evidence anchors:**
  - [section] "Choosing a time series that reduces inertia by the largest amount as the new centroid is generally a more effective strategy than random selection because it directly targets the objective of the clustering algorithm."
  - [section] "The latter approach is that adopted by scikit-learn (Pedregosa et al, 2011) to handle empty clusters in their k-means implementation."
  - [corpus] No corpus evidence about empty cluster handling strategies for TSCL.
- **Break condition:** If the algorithm forms empty clusters too frequently, this strategy becomes computationally expensive and may not prevent repeated empty cluster formation.

## Foundational Learning

- **Concept:** Dynamic Time Warping (DTW) distance
  - **Why needed here:** DTW is the most common elastic distance used in TSCL and is central to several of the algorithms compared (k-means-DTW, k-means-DBA)
  - **Quick check question:** How does DTW differ from Euclidean distance when comparing two time series of different lengths?

- **Concept:** Sum of Squared Errors (SSE) objective function
  - **Why needed here:** The paper's experimental evaluation and algorithm comparison are based on minimizing SSE, and stopping conditions depend on SSE changes
  - **Quick check question:** What is the relationship between SSE and the "inertia" metric mentioned in scikit-learn's k-means implementation?

- **Concept:** Time series averaging methods (DBA, Shape extraction, K-SC average)
  - **Why needed here:** The update step of Lloyd's algorithm requires computing new centroids, and different averaging methods are used depending on the distance function
  - **Quick check question:** Why can't we simply use arithmetic mean for centroids when using DTW distance?

## Architecture Onboarding

- **Component map:** The TSCL system has four main stages: initialization (select k centroids), assignment (assign each series to nearest centroid using distance function), update (compute new centroids using averaging method), and stopping (check convergence based on SSE change). Each stage requires distance calculations, and the key architectural decision is whether to use the same distance function throughout or different functions for different stages.

- **Critical path:** For each iteration: compute all pairwise distances between series and centroids → assign series to nearest centroid → compute new centroids using averaging method → check stopping condition (SSE change). The most computationally expensive step is typically the distance matrix computation, especially for elastic distances like DTW.

- **Design tradeoffs:** Using elastic distances (DTW, MSM) improves alignment but increases computation time significantly. The choice of initialization strategy trades off runtime (more restarts = longer) against clustering quality and reproducibility. Empty cluster handling adds complexity but prevents algorithm failure.

- **Failure signatures:** Poor clustering quality despite many iterations may indicate mismatched distance functions across stages. Extremely long runtimes with little SSE improvement suggests the distance function and averaging method are working against each other. Empty clusters forming frequently may indicate the distance function creates problematic cluster geometries.

- **First 3 experiments:**
  1. Run k-means with Euclidean distance using different initialization strategies (random, Forgy, k-means++) on a simple UCR dataset to observe the impact on final SSE and runtime.
  2. Compare k-means-DTW vs k-means-DBA on a dataset where series have similar patterns but different speeds, measuring both clustering quality and runtime.
  3. Implement empty cluster handling using both random reassignment and inertia-minimizing reassignment on a dataset prone to empty clusters, comparing the resulting cluster quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does k-means-soft-dba consistently outperform other Lloyd's-based algorithms across different time series datasets beyond the UCR archive?
- Basis in paper: [explicit] The paper states that k-means-soft-dba performs best overall but is extremely slow, suggesting potential limitations in practical applications.
- Why unresolved: The study primarily focuses on the UCR archive, which may not be representative of all time series data characteristics and distributions.
- What evidence would resolve it: Testing k-means-soft-dba on diverse time series datasets from various domains and comparing its performance against other Lloyd's-based algorithms would provide a more comprehensive understanding of its effectiveness.

### Open Question 2
- Question: How do the initialization strategies affect the convergence and performance of Lloyd's algorithm when applied to time series clustering with non-Euclidean distance functions?
- Basis in paper: [explicit] The paper discusses the importance of initialization strategies but primarily focuses on k-means with Euclidean distance, leaving the impact of these strategies on other distance functions unclear.
- Why unresolved: The interaction between initialization strategies and different time series distance functions has not been thoroughly explored, which could lead to suboptimal clustering results.
- What evidence would resolve it: Conducting experiments that systematically vary initialization strategies across different time series distance functions and analyzing their effects on convergence speed and clustering performance would provide insights into the best practices for Lloyd's algorithm in time series clustering.

### Open Question 3
- Question: What are the implications of using a single distance function throughout all stages of Lloyd's algorithm for time series clustering, and how does this approach compare to using different distance functions for different stages?
- Basis in paper: [explicit] The authors propose a standard Lloyd's-based model that adopts an end-to-end approach, using a single distance function in all stages, but do not explore the potential benefits or drawbacks of using different distance functions for different stages.
- Why unresolved: The assumption that a single distance function is optimal for all stages of the algorithm may not hold true for all time series datasets, and alternative configurations could potentially improve clustering results.
- What evidence would resolve it: Experimenting with different combinations of distance functions for each stage of Lloyd's algorithm and comparing the results to the end-to-end approach would provide empirical evidence on the effectiveness of using multiple distance functions in time series clustering.

## Limitations

- The study uses a standardized k-means implementation but doesn't explore the full parameter space of initialization strategies (e.g., k-means++ variants) or distance function parameters (e.g., DTW window sizes), which may affect results.
- Runtime measurements are not fully standardized across algorithms (e.g., some use GPU acceleration), making direct performance comparisons less reliable.
- The evaluation focuses on UCR archive datasets which may not represent all time series clustering scenarios, particularly high-dimensional or irregularly sampled data.

## Confidence

- **High confidence:** The core finding that inconsistent k-means configurations across literature impede fair comparison is well-supported by the survey of existing implementations.
- **Medium confidence:** Performance rankings of TSCL variants are reliable within the UCR archive context but may not generalize to all time series domains.
- **Medium confidence:** The claim that end-to-end consistent distance functions improve clustering quality is supported by experiments but lacks extensive ablation studies.

## Next Checks

1. Conduct parameter sensitivity analysis for key algorithms (DTW window size, soft-DTW gamma) to assess robustness of performance rankings.
2. Validate results on non-UCR datasets with different characteristics (e.g., higher dimensionality, irregular sampling) to test generalizability.
3. Perform ablation studies comparing end-to-end consistent distance functions against hybrid approaches across all four Lloyd's stages to quantify the impact of consistency.