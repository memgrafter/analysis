---
ver: rpa2
title: Stochastic Gradient Succeeds for Bandits
arxiv_id: '2402.17235'
source_url: https://arxiv.org/abs/2402.17235
tags:
- have
- gradient
- stochastic
- rmax
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes global convergence of the stochastic gradient
  bandit algorithm with a constant learning rate, resolving a long-standing open question.
  The key insight is that the sampling noise in the stochastic updates automatically
  diminishes as progress becomes small, eliminating the need for learning rate decay
  or other noise control techniques.
---

# Stochastic Gradient Succeeds for Bandits

## Quick Facts
- arXiv ID: 2402.17235
- Source URL: https://arxiv.org/abs/2402.17235
- Reference count: 40
- Primary result: Global convergence of stochastic gradient bandit algorithm with constant learning rate

## Executive Summary
This paper resolves a fundamental open question in bandit optimization by proving that stochastic gradient descent with a constant learning rate achieves global convergence in the bandit setting. The key insight is that sampling noise naturally diminishes as the algorithm approaches optimality, eliminating the need for learning rate decay or other noise control mechanisms. The authors establish an O(1/t) convergence rate to globally optimal policies, matching the asymptotic regret bounds of state-of-the-art bandit algorithms while using the simpler constant learning rate approach.

## Method Summary
The paper analyzes the stochastic gradient bandit algorithm where the agent updates its policy parameters using gradient estimates obtained through bandit feedback. Unlike previous approaches that required carefully decaying learning rates or sophisticated noise control techniques, this method uses a constant learning rate throughout training. The algorithm computes policy gradients using only the observed rewards from the chosen actions, without requiring knowledge of the reward function or access to full information feedback. The theoretical analysis demonstrates that as the policy improves and approaches optimality, the inherent sampling noise in the bandit feedback naturally decreases, ensuring convergence.

## Key Results
- Proves global convergence to optimal policies for stochastic gradient bandit algorithms with constant learning rates
- Establishes O(1/t) convergence rate matching state-of-the-art bandit algorithms
- Demonstrates that sampling noise automatically diminishes as progress becomes small
- Shows theoretical results extend to bandits with an additional O(âˆšT) regret term

## Why This Works (Mechanism)
The mechanism relies on the self-regularizing property of stochastic gradients in the bandit setting. As the policy approaches optimality, the magnitude of the policy gradients decreases, which means that the same level of sampling noise becomes proportionally smaller relative to the true gradient signal. This creates a natural annealing effect where the algorithm automatically reduces the impact of noise without explicit learning rate decay. The constant learning rate maintains sufficient exploration early in training while the diminishing gradient magnitudes ensure convergence later.

## Foundational Learning
- **Stochastic gradient descent fundamentals** - Understanding the basic update rule and convergence properties is essential for grasping how the algorithm works in the bandit setting. Quick check: Can you explain why SGD works in convex optimization?
- **Policy gradient methods** - The connection between bandit optimization and policy gradient reinforcement learning provides important context. Quick check: How do policy gradients differ from value-based methods?
- **Exploration-exploitation tradeoff** - The constant learning rate implicitly balances exploration and exploitation throughout training. Quick check: What happens if the learning rate is too small or too large?
- **Bandit feedback structure** - Understanding the limited information available in bandit problems compared to full information settings. Quick check: What is the key difference between bandit and full information feedback?

## Architecture Onboarding

Component Map: Policy Parameters -> Action Selection -> Reward Observation -> Gradient Estimation -> Parameter Update -> (loop)

Critical Path: The algorithm follows a straightforward loop where policy parameters are used to select actions, rewards are observed, gradients are estimated from the rewards, and parameters are updated using these gradients with a constant learning rate.

Design Tradeoffs: The main tradeoff is using a constant learning rate versus adaptive methods. While constant rates are simpler and eliminate the need for decay schedules, they require careful tuning and may converge more slowly than adaptive methods in practice.

Failure Signatures: Convergence failure typically manifests as oscillations or getting stuck in suboptimal policies. This can occur if the learning rate is too large, causing instability, or too small, preventing sufficient progress.

First Experiments:
1. Test convergence on a simple 2-armed bandit with known optimal solution
2. Verify the O(1/t) convergence rate empirically on a smooth reward function
3. Compare performance against decaying learning rate baselines on standard bandit benchmarks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several extensions are suggested by the work, including applying the results to non-smooth reward functions, extending to contextual bandits, and exploring the algorithm's performance in adversarial settings.

## Limitations
- Analysis focuses on full information setting with extension claims to bandits that are not fully proven
- Theoretical results assume smooth reward functions which may not hold in all practical applications
- Constant learning rate requires careful tuning and may be sensitive to problem characteristics
- Experimental validation is limited to relatively simple bandit problems

## Confidence
- Global convergence proof: High
- O(1/t) convergence rate: Medium
- Extension to bandit setting: Medium
- Practical performance: Medium

## Next Checks
1. Test the algorithm on bandit problems with non-smooth reward functions to verify robustness beyond the smooth reward assumption
2. Conduct experiments on high-dimensional bandit problems with sparse rewards to evaluate practical scalability
3. Compare the constant learning rate parameter sensitivity across different problem domains to establish practical tuning guidelines