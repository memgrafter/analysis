---
ver: rpa2
title: 'RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval'
arxiv_id: '2402.18510'
source_url: https://arxiv.org/abs/2402.18510
tags:
- will
- rnns
- retrieval
- sequence
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the representation gap between Recurrent Neural
  Networks (RNNs) and Transformers on algorithmic problems. While RNNs are more memory
  efficient for long sequences, they struggle with in-context retrieval tasks even
  with Chain-of-Thought prompting.
---

# RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval

## Quick Facts
- arXiv ID: 2402.18510
- Source URL: https://arxiv.org/abs/2402.18510
- Reference count: 40
- Primary result: RNNs struggle with in-context retrieval tasks but can match Transformer performance when augmented with explicit or implicit retrieval mechanisms

## Executive Summary
This paper analyzes the fundamental limitations of Recurrent Neural Networks (RNNs) in performing in-context retrieval tasks, a capability where Transformers excel. While RNNs are more memory-efficient for processing long sequences, they fail to solve algorithmic problems requiring retrieval of information from context, even with Chain-of-Thought prompting. The authors prove that this limitation extends to tasks that implicitly require retrieval, such as determining if a graph is a tree. Through both theoretical analysis and empirical validation, they demonstrate that enhancing RNNs with retrieval capabilities—either through explicit regular expressions or a single Transformer layer—enables them to solve all polynomial-time solvable problems with CoT and match Transformer performance on synthetic and natural language tasks.

## Method Summary
The authors conducted a comprehensive analysis comparing RNNs and Transformers on algorithmic problems requiring in-context retrieval. They first proved theoretically that RNNs cannot solve certain polynomial-time solvable problems that require implicit retrieval. They then designed two augmentation strategies to address this limitation: explicit retrieval using regular expressions and implicit retrieval by adding a single Transformer layer to RNNs. The experiments were conducted on synthetic benchmarks (parity checking, tree detection) and natural language tasks (proof problems, question answering). The augmented RNNs were compared against standard Transformers across these tasks to evaluate performance differences.

## Key Results
- RNNs fail to solve in-context retrieval tasks even with Chain-of-Thought prompting
- The retrieval limitation extends to tasks requiring implicit retrieval (e.g., graph tree detection)
- Retrieval-augmented RNNs (with regex or single Transformer layer) match Transformer performance on tested tasks
- A single Transformer layer provides implicit retrieval capabilities to RNNs

## Why This Works (Mechanism)
The paper establishes that RNNs' sequential processing architecture fundamentally limits their ability to retrieve information from context, which is essential for many algorithmic and reasoning tasks. Transformers overcome this limitation through their attention mechanism, which enables direct access to any position in the input sequence. By adding retrieval capabilities—either explicitly through regex patterns that extract relevant information or implicitly through a Transformer layer's attention mechanism—RNNs gain the ability to access context information effectively. This augmentation bridges the representational gap between RNNs and Transformers for tasks requiring retrieval, allowing RNNs to perform comparably while maintaining their memory efficiency advantages for long sequences.

## Foundational Learning

**Chain-of-Thought prompting**: A technique that breaks down complex problems into intermediate reasoning steps. Why needed: Essential for understanding how RNNs fail even with enhanced reasoning approaches. Quick check: Can be tested by implementing CoT on simple arithmetic problems.

**Implicit vs explicit retrieval**: Explicit retrieval involves directly extracting information using patterns/rules, while implicit retrieval is built into the model architecture. Why needed: Central to understanding the paper's contribution and the distinction between RNN and Transformer capabilities. Quick check: Can be tested by comparing regex-based extraction versus attention-based retrieval on structured data.

**Polynomial-time solvable problems**: Computational problems that can be solved in time polynomial to the input size. Why needed: The theoretical framework for proving RNN limitations is built on complexity theory. Quick check: Can be verified by analyzing algorithm complexity on small input sizes.

## Architecture Onboarding

Component map: Input sequence -> RNN layers -> (Optional: Retrieval mechanism) -> Output
Critical path: The retrieval mechanism is the critical bottleneck that determines whether RNNs can solve a given task
Design tradeoffs: RNNs offer memory efficiency for long sequences but lack retrieval capabilities; Transformers provide retrieval but are computationally expensive
Failure signatures: RNNs fail on tasks requiring access to non-local context or multiple pieces of information from different positions
First experiments:
1. Test vanilla RNN on parity checking task with CoT
2. Apply regex-based retrieval to the same parity task
3. Compare performance with a single Transformer layer added to RNN

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis primarily focused on synthetic benchmarks and specific algorithmic tasks
- Unclear whether findings generalize to broader natural language processing applications
- Theoretical proofs rely on specific problem formulations that may not cover all retrieval scenarios

## Confidence
High: RNNs struggle with implicit retrieval (supported by theoretical proofs and empirical results)
Medium-High: Retrieval augmentation closes the gap (consistent performance matching on tested benchmarks)
Medium: Generalization to real-world applications (limited testing scope on diverse NLP tasks)

## Next Checks
1. Test retrieval-augmented RNN approach on diverse real-world datasets requiring complex multi-hop reasoning, such as open-domain question answering or document-based dialogue systems
2. Evaluate whether the single Transformer layer augmentation introduces significant computational overhead that negates RNNs' memory efficiency advantages
3. Investigate whether alternative retrieval augmentation strategies (attention mechanisms, memory networks) could achieve similar performance improvements with less architectural modification