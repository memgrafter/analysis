---
ver: rpa2
title: Enhancing Action Recognition by Leveraging the Hierarchical Structure of Actions
  and Textual Context
arxiv_id: '2410.21275'
source_url: https://arxiv.org/abs/2410.21275
tags:
- action
- recognition
- actions
- hierarchical
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel action recognition approach that
  leverages hierarchical action structures and contextualized textual information.
  The method employs a vision-language transformer architecture that combines visual
  features from RGB and optical flow with textual embeddings representing contextual
  cues like location and previous actions.
---

# Enhancing Action Recognition by Leveraging the Hierarchical Structure of Actions and Textual Context

## Quick Facts
- arXiv ID: 2410.21275
- Source URL: https://arxiv.org/abs/2410.21275
- Authors: Manuel Benavent-Lledo; David Mulero-Pérez; David Ortiz-Perez; Jose Garcia-Rodriguez; Antonis Argyros
- Reference count: 40
- One-line primary result: Novel action recognition approach leveraging hierarchical action structures and contextualized textual information achieves over 17% improvement in top-1 accuracy on Hierarchical TSU, Assembly101, and IkeaASM datasets.

## Executive Summary
This paper introduces a transformer-based action recognition approach that combines visual features from RGB and optical flow with contextualized textual information representing location and previous actions. The method employs a joint loss function to train simultaneously for coarse- and fine-grained action classification. The authors create Hierarchical TSU by extending the Toyota Smarthome Untrimmed dataset with hierarchical annotations, enabling the first hierarchical dataset for Activities of Daily Living. Experiments demonstrate significant performance improvements over state-of-the-art methods across multiple datasets, with particular effectiveness in leveraging contextual information for fine-grained action recognition.

## Method Summary
The proposed method uses a vision-language transformer architecture that fuses visual features (RGB and optical flow) with textual embeddings representing contextual cues. A frozen ViT-H/14 extracts features from RGB frames while Inception v3 extracts optical flow features, both pre-trained on Kinetics-400. DistilBERT generates contextual embeddings from textual prompts describing location and previous actions. A fusion transformer encoder combines these modalities, with two separate classifiers trained simultaneously for coarse- and fine-grained action recognition using a joint loss function. The approach is evaluated on Hierarchical TSU, Assembly101, and IkeaASM datasets, demonstrating state-of-the-art performance with significant improvements in top-1 accuracy.

## Key Results
- The proposed method achieves over 17% improvement in top-1 accuracy compared to state-of-the-art approaches on Hierarchical TSU, Assembly101, and IkeaASM datasets.
- Hierarchical modeling and contextual cues provide significant benefits for action recognition, particularly for fine-grained actions.
- The method shows particular effectiveness in leveraging contextual information, with ablation studies confirming the benefits of hierarchical modeling and contextual cues.

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical and contextual information improve action recognition accuracy by providing additional supervision that disambiguates similar fine-grained actions. The model leverages both fine-grained and coarse-grained action labels during training while contextual embeddings from past actions and location provide semantic relationships that help distinguish between similar actions. This approach assumes that hierarchical relationships between action categories and temporal context are semantically meaningful and learnable by the model.

### Mechanism 2
Vision-language transformer architecture effectively fuses multimodal information through self-attention mechanisms that capture long-range temporal dependencies and cross-modal relationships. Separate video transformers for RGB and optical flow features are combined with textual embeddings through another transformer encoder, allowing the model to learn how visual and textual information complement each other for action classification.

### Mechanism 3
Contextual information from past actions and location provides discriminative cues for fine-grained action recognition by encoding temporal dependencies. DistilBERT extracts feature vectors from textual prompts describing previous actions and location, which are then fused with visual features to provide additional semantic context. This mechanism assumes that past actions and location correlate with current action and that language models can effectively encode this temporal information.

## Foundational Learning

- **Hierarchical action structures and their semantic relationships**: Understanding that fine-grained actions belong to broader coarse-grained categories is essential since the model exploits these relationships during training. *Quick check: Can you explain why "make coffee" might be a coarse-grained action while "open coffee maker" and "pour water" are fine-grained sub-actions?*

- **Transformer architecture and self-attention mechanisms**: The entire model architecture relies on transformers for video encoding, text encoding, and multimodal fusion, requiring understanding of how self-attention works across different modalities. *Quick check: How does self-attention in transformers differ from recurrent architectures like LSTMs in terms of capturing temporal dependencies?*

- **Multimodal learning and feature fusion strategies**: The model fuses RGB, optical flow, and textual embeddings, requiring understanding of when to fuse early vs late and how different modalities complement each other. *Quick check: What are the trade-offs between early fusion (concatenating features before any processing) and late fusion (processing each modality separately then combining)?*

## Architecture Onboarding

- **Component map**: Frozen feature extractors (ViT-H/14 for RGB, Inception v3 for optical flow) → video transformers → DistilBERT for text encoding → fusion transformer → two classification heads (fine-grained and coarse-grained)
- **Critical path**: RGB/optical flow frames → feature extractors → video transformers → fusion transformer → classification heads. The text path runs in parallel through DistilBERT and feeds into the fusion transformer.
- **Design tradeoffs**: Using frozen feature extractors reduces training complexity but may limit adaptation to the specific dataset; the joint loss function adds complexity but provides auxiliary supervision; late fusion allows modality-specific processing but requires careful temporal alignment.
- **Failure signatures**: If fine-grained accuracy drops while coarse-grained accuracy improves, the hierarchical supervision may be too noisy; if both accuracies drop, the fusion transformer may not be learning effective cross-modal relationships; if performance is similar to visual-only approaches, the contextual information may not be providing meaningful signal.
- **First 3 experiments**:
  1. Test visual-only performance with different input lengths (8, 16, 32, 64 blocks) to find optimal temporal context.
  2. Evaluate the impact of contextual information by comparing ground truth vs predicted context inputs.
  3. Test different fusion strategies (early vs late) with the same feature extractors to understand modality interaction effects.

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed method perform when using different numbers of past actions for contextual information? The paper evaluates the impact of using 1, 3, 5, and 7 past actions, finding that five yields the best performance, but leaves open whether even more past actions could further improve performance or if there is an optimal range.

### Open Question 2
How would the proposed method perform on datasets with different hierarchical structures or action categories? While the paper demonstrates effectiveness on three datasets with varying hierarchical structures, it does not explore performance on datasets with significantly different hierarchical structures, which could impact generalizability.

### Open Question 3
How would the proposed method perform on datasets with different video lengths or action durations? The paper uses datasets with varying video lengths and action durations but does not explicitly analyze the impact of these factors on performance, leaving open questions about the method's sensitivity to temporal characteristics.

## Limitations
- The reliance on external language models (GPT-3.5 and Llama 3) for generating contextual descriptions introduces potential biases and inconsistencies in the contextual data.
- The effectiveness of hierarchical supervision depends heavily on the quality and structure of hierarchical labels, which are not clearly defined for all datasets.
- The IkeaASM dataset presents challenges where fine-grained actions are assigned to multiple coarse-grained categories, potentially undermining the hierarchical learning assumption.

## Confidence

**High confidence**: The core architectural design and multimodal fusion approach, supported by ablation studies and comparative results.

**Medium confidence**: The effectiveness of hierarchical supervision, as results vary significantly across datasets and the IkeaASM results suggest potential issues with label quality.

**Medium confidence**: The generalizability of contextual information benefits, as the improvement is dataset-dependent and relies on external language model outputs.

## Next Checks

1. **Ablation study with varying label quality**: Test the model with different levels of hierarchical label consistency to determine the robustness of hierarchical learning to label noise.

2. **Contextual information sensitivity analysis**: Compare performance using ground truth contextual information versus model-generated descriptions to quantify the impact of language model quality.

3. **Cross-dataset generalization test**: Evaluate the model's performance when trained on one dataset and tested on another to assess the transferability of hierarchical and contextual learning benefits.