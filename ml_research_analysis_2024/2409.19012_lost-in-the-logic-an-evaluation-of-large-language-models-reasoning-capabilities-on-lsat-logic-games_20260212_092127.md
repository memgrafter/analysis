---
ver: rpa2
title: 'Lost in the Logic: An Evaluation of Large Language Models'' Reasoning Capabilities
  on LSAT Logic Games'
arxiv_id: '2409.19012'
source_url: https://arxiv.org/abs/2409.19012
tags:
- accuracy
- answer
- logic
- dataset
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis evaluates large language models (LLMs) on the Law School
  Admissions Test (LSAT) logic games section, which requires complex multi-step logical
  reasoning. A novel dataset of LSAT logic games with metadata (difficulty, game type,
  problem type) is constructed and released publicly.
---

# Lost in the Logic: An Evaluation of Large Language Models' Reasoning Capabilities on LSAT Logic Games
## Quick Facts
- arXiv ID: 2409.19012
- Source URL: https://arxiv.org/abs/2409.19012
- Authors: Saumya Malik
- Reference count: 0
- Primary result: Large language models struggle with complex logical reasoning on LSAT logic games, achieving only 23-33% accuracy with Chain-of-Thought prompting but improving to 46-70% on In-and-Out questions with self-correction capabilities

## Executive Summary
This thesis evaluates large language models' (LLMs) reasoning capabilities on the Law School Admissions Test (LSAT) logic games section, which requires complex multi-step logical reasoning. The study constructs a novel dataset of LSAT logic games with metadata including difficulty, game type, and problem type, and releases it publicly for research purposes. The research investigates both Chain-of-Thought prompting and an adapted Reflexion framework that enables self-reflection and error correction.

The findings reveal that while LLMs perform poorly on LSAT logic games using standard prompting techniques, implementing self-correction mechanisms significantly improves performance on specific question types. The study identifies particular logical challenges that models face, including handling contrapositives, implications, and mutual exclusivity. These results demonstrate that LLMs can improve their reasoning when given opportunities to self-correct, though they still fall short of human performance on these complex logical tasks.

## Method Summary
The study constructs a novel dataset of LSAT logic games with detailed metadata including difficulty levels, game types, and problem types. The dataset is designed to comprehensively represent the types of logical reasoning challenges found in the LSAT logic games section. The research evaluates two large language models - GPT-4 and GPT-3.5 - using both standard Chain-of-Thought prompting and an adapted Reflexion framework that allows for self-reflection and error correction. The evaluation focuses on measuring accuracy across different game types and problem types, with particular attention to identifying patterns in model performance and logical errors. The Reflexion framework adaptation enables models to recognize and correct their own mistakes during the reasoning process.

## Key Results
- Standard Chain-of-Thought prompting achieves low accuracy: GPT-4 at 33% and GPT-3.5 at 23% across the full LSAT logic games dataset
- Adapted Reflexion framework with self-correction capabilities significantly improves performance on In-and-Out questions: GPT-4 reaches 70% and GPT-3.5 reaches 46%
- Models demonstrate better performance on certain game types (In-and-Out games) while struggling with specific logical errors such as incorrect implications, missing contrapositives, and handling mutual exclusivity

## Why This Works (Mechanism)
The study demonstrates that LLMs can improve their logical reasoning performance when provided with mechanisms for self-reflection and error correction. The Reflexion framework adaptation works by allowing models to recognize patterns in their mistakes and adjust their reasoning process accordingly. This self-correction capability appears particularly effective for In-and-Out game types, where models can identify and fix logical inconsistencies in their reasoning chains. The improvement suggests that LLMs possess latent reasoning capabilities that can be activated through appropriate prompting strategies and self-correction mechanisms, rather than requiring fundamentally different architectures.

## Foundational Learning
- **Logical reasoning fundamentals**: Understanding formal logic, implications, contrapositives, and logical equivalences is essential for interpreting LSAT logic games and evaluating model performance
- **Chain-of-Thought prompting**: This technique requires understanding how to break down complex reasoning tasks into sequential steps that models can follow
- **Reflexion framework**: Knowledge of how self-reflection and error correction mechanisms work in AI systems is crucial for understanding the adapted approach used in this study
- **LSAT logic games structure**: Familiarity with the specific formats and requirements of LSAT logic games helps in evaluating the dataset construction and performance metrics
- **Multi-step reasoning evaluation**: Understanding how to assess complex reasoning processes and identify specific types of logical errors is necessary for interpreting the results
- **Statistical significance in NLP**: Knowledge of how to determine whether performance differences are meaningful or due to chance is important for evaluating the study's claims

## Architecture Onboarding
**Component Map**: Dataset Construction -> Model Evaluation -> Reflexion Framework Adaptation -> Performance Analysis -> Error Pattern Identification
**Critical Path**: Dataset creation with metadata → Initial Chain-of-Thought evaluation → Reflexion framework implementation → Performance comparison → Error analysis
**Design Tradeoffs**: The study prioritizes comprehensive dataset construction with detailed metadata over broader model coverage, choosing to deeply evaluate two models rather than superficially testing many. This enables detailed error analysis but limits generalizability.
**Failure Signatures**: Models consistently fail on contrapositives and implications, particularly when multiple logical steps are required. Performance varies significantly by game type, with In-and-Out games being most amenable to self-correction.
**3 First Experiments**:
1. Evaluate Chain-of-Thought prompting on a small subset of LSAT logic games to establish baseline performance
2. Implement basic self-correction mechanisms on the same subset to test improvement potential
3. Analyze error patterns from initial evaluations to inform Reflexion framework adaptation

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions for future research, focusing instead on presenting the evaluation results and error analysis. However, the findings naturally raise several important questions about LLM reasoning capabilities that could be explored in future work, including how to extend self-correction mechanisms to other game types, whether different prompting strategies could yield better results, and how model performance scales with increased training on logical reasoning tasks.

## Limitations
- Dataset representativeness and selection bias concerns, as the paper does not specify how games were selected or whether the sample size provides adequate coverage of all game types and difficulty levels
- Limited model evaluation scope, testing only two LLM models (GPT-4 and GPT-3.5) which constrains generalizability to other LLMs or newer model versions
- Unclear evaluation methodology for complex multi-step reasoning problems, with insufficient detail on validation procedures and determination of correct answers

## Confidence
- **High confidence**: LLM performance remains substantially below human capability on LSAT logic games
- **Medium confidence**: Reflexion framework adaptation shows promise for specific question types
- **Low confidence**: Generalizability of findings to other LLMs or reasoning tasks

## Next Checks
1. Expand dataset evaluation to include a more diverse sample of LSAT logic games across all difficulty levels and game types to address representativeness concerns
2. Test additional LLM models (including open-source alternatives) to assess generalizability of findings and determine if performance patterns hold across different architectures
3. Implement rigorous evaluation metrics with human validation for multi-step reasoning problems to establish clear standards for correctness and improve methodological clarity