---
ver: rpa2
title: Benchmark for Evaluation and Analysis of Citation Recommendation Models
arxiv_id: '2412.07713'
source_url: https://arxiv.org/abs/2412.07713
tags:
- citation
- recommendation
- papers
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a benchmark for evaluating citation recommendation
  models, addressing the challenge of comparing diverse approaches due to variations
  in datasets, evaluation metrics, and methodologies. The benchmark focuses on local
  citation recommendation systems and utilizes diagnostic datasets derived from the
  S2ORC and S2AG datasets, covering features like context length, citation location,
  context type, POS of surrounding words, and low-resource papers.
---

# Benchmark for Evaluation and Analysis of Citation Recommendation Models

## Quick Facts
- arXiv ID: 2412.07713
- Source URL: https://arxiv.org/abs/2412.07713
- Reference count: 40
- One-line primary result: BM25 consistently outperforms learned citation recommendation models across most diagnostic datasets, highlighting its robustness.

## Executive Summary
This paper presents a benchmark for evaluating citation recommendation models, addressing the challenge of comparing diverse approaches due to variations in datasets, evaluation metrics, and methodologies. The benchmark focuses on local citation recommendation systems and utilizes diagnostic datasets derived from the S2ORC and S2AG datasets, covering features like context length, citation location, context type, POS of surrounding words, and low-resource papers. The evaluation employs models such as BM25, NCN, LCR, and Galactica, using Recall and Mean Reciprocal Rank (MRR) as metrics. BM25 consistently outperforms other models across most datasets, highlighting its robustness. The benchmark provides a standardized framework for researchers to assess and compare citation recommendation models, enabling meaningful comparisons and guiding future research.

## Method Summary
The benchmark evaluates citation recommendation models using diagnostic datasets constructed from S2ORC and S2AG. Papers are hierarchically sampled across fields, years, and citation counts to ensure balanced representation. Citation contexts are extracted and stratified by features like context length, citation location, and POS patterns. Four models (BM25, NCN, LCR, Galactica) are evaluated using Recall@10 and MRR@10 across each diagnostic dataset class. BM25 leverages full S2AG metadata for retrieval, while learned models are trained on domain-specific datasets like Arxiv.

## Key Results
- BM25 consistently outperforms NCN, LCR, and Galactica across most diagnostic datasets.
- Model performance varies systematically across diagnostic dataset classes, revealing dataset-specific strengths and weaknesses.
- NCN shows improved performance with higher citation counts, while Galactica performs better for references at sentence ends.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diagnostic datasets enable granular performance assessment of citation recommendation models by stratifying data across multiple citation context features.
- Mechanism: The paper constructs eight diagnostic datasets, each targeting a specific feature (e.g., context length, citation location, POS patterns). By sampling papers hierarchically across fields, years, and citation counts, these datasets ensure balanced representation of citation scenarios. Models are evaluated on each dataset class independently, revealing strengths and weaknesses tied to context characteristics.
- Core assumption: That the features used (length, location, POS, etc.) meaningfully influence model performance, and that the stratified sampling yields representative subsets.
- Evidence anchors:
  - [abstract] "This benchmark will evaluate the performance of models on different features of the citation context and provide a comprehensive evaluation of the models across all these tasks"
  - [section 3.1] "Diagnostic datasets play a crucial role in error analysis, performance evaluation, and model comparison in citation recommendation"
  - [corpus] Weak - corpus neighbors discuss citation evaluation and related recommendation models but do not directly confirm the stratified sampling method described.
- Break condition: If the stratified sampling introduces bias (e.g., over/under-sampling certain fields), or if the assumed feature-importance relationship does not hold in practice, the diagnostic datasets may not provide reliable performance signals.

### Mechanism 2
- Claim: BM25 consistently outperforms learned models in local citation recommendation due to its use of full S2AG metadata and query-based retrieval.
- Mechanism: Unlike NCN, LCR, and Galactica, which are trained on domain-specific datasets (e.g., Arxiv), BM25 queries the entire S2AG corpus, providing a broader candidate pool. Its term-frequency-based ranking is robust to context variations and does not overfit to training data biases.
- Core assumption: That retrieval breadth and lack of domain-specific training bias are key to robust citation recommendation performance.
- Evidence anchors:
  - [section 4] "BM25 shows the best performance in terms of Recall and MRR for most datasets... BM25 consistently has a higher MRR across all classes in the location dataset"
  - [section 3.2.2] "BM25 has the advantage of querying the entire S2AG dataset, which comprises millions of records"
  - [corpus] Weak - corpus papers focus on evaluation frameworks and model-specific citation tasks, not on retrieval vs. learned model comparisons.
- Break condition: If the S2AG corpus is incomplete or biased, or if BM25's term-based matching fails on semantically rich but lexically different contexts, its advantage may diminish.

### Mechanism 3
- Claim: Model performance varies systematically across diagnostic dataset classes, revealing dataset-specific strengths and weaknesses.
- Mechanism: By evaluating each model on each diagnostic dataset class (e.g., fields, years, citation counts), the paper identifies patterns such as NCN's improved performance with higher citation counts, or Galactica's better results for references at sentence ends. These patterns stem from differences in training data and architectural inductive biases.
- Core assumption: That model architectures and training datasets impose systematic biases that manifest as performance differences across diagnostic classes.
- Evidence anchors:
  - [section 4] "Due to the varying training datasets, different models perform differently on the classes of the diagnostic datasets... Galactica models exhibit lower performance in the location dataset when referencing the start and middle of sentences compared to the end"
  - [section 3.1.5] "The POS of the preceding and following words in citations is analyzed to obtain the results"
  - [corpus] Weak - corpus does not directly address systematic model-class performance patterns, focusing instead on individual model capabilities.
- Break condition: If the observed performance patterns are due to random variance rather than systematic architectural or training effects, the diagnostic framework may not provide actionable insights.

## Foundational Learning

- Concept: Stratified sampling
  - Why needed here: To ensure balanced representation of citation scenarios across fields, years, and citation counts, preventing dataset bias in evaluation.
  - Quick check question: If a field represents 5% of the total S2ORC dataset, how would you sample it to maintain proportional representation while ensuring enough instances for robust evaluation?
- Concept: Part-of-speech (POS) tagging and its role in citation context
  - Why needed here: POS patterns around citations (e.g., preceding verbs vs. nouns) can influence model ability to predict relevant references; diagnostic datasets leverage this for fine-grained analysis.
  - Quick check question: How might a citation preceded by a preposition differ in recommendation difficulty compared to one preceded by a noun?
- Concept: Retrieval vs. learned ranking in information retrieval
  - Why needed here: BM25 (retrieval-based) vs. NCN/LCR/Galactica (learned) models exhibit different performance profiles; understanding this tradeoff is key to interpreting results.
  - Quick check question: What are the main advantages and disadvantages of using a term-frequency-based retriever like BM25 compared to a neural reranker in citation recommendation?

## Architecture Onboarding

- Component map: S2ORC -> S2AG -> Stratified Sampling -> Citation Context Extraction -> Diagnostic Datasets -> Model Evaluation -> Metrics (Recall@10, MRR@10)
- Critical path: Data extraction -> stratified sampling -> sentence parsing -> model inference -> metric computation
- Design tradeoffs:
  - Broad vs. deep sampling: Stratified sampling ensures coverage but may reduce per-class instance count.
  - Retrieval vs. learned models: BM25's broad retrieval vs. model-specific training biases.
  - Diagnostic granularity: More classes increase diagnostic power but also evaluation complexity.
- Failure signatures:
  - Poor Recall/MRR across all models -> data quality or sampling issues
  - Inconsistent model rankings across classes -> model overfitting or dataset bias
  - High variance in metrics -> insufficient per-class samples
- First 3 experiments:
  1. Validate stratified sampling by checking field/year/citation count distributions in each diagnostic dataset.
  2. Run BM25 and one learned model (e.g., LCR) on a single diagnostic class (e.g., short context length) and compare Recall/MRR.
  3. Test model performance on a low-resource field (e.g., philosophy) to confirm systematic domain-specific weaknesses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do citation recommendation models perform on extremely short versus extremely long citation contexts, and what is the optimal context length for different types of citations?
- Basis in paper: [inferred] The paper mentions that NCN, LCR, and Galactica models show superior performance in short-length contexts, while BM25 excels with longer contexts. However, it does not provide a detailed analysis of the performance across the entire spectrum of context lengths.
- Why unresolved: The paper only briefly mentions the performance differences without providing a comprehensive analysis of how context length affects model performance across various citation types and domains.
- What evidence would resolve it: A detailed study comparing model performance across a wide range of context lengths, including extremely short and long contexts, would provide insights into the optimal context length for different citation types and domains.

### Open Question 2
- Question: How do citation recommendation models handle low-resource fields, and what strategies can be employed to improve their performance in these areas?
- Basis in paper: [explicit] The paper mentions that low-resource fields include psychology, environmental science, engineering, business, geography, political science, sociology, geology, history, art, and philosophy, and that BM25 shows better recall in art and MRR in geography. However, it does not provide a detailed analysis of the performance across all low-resource fields.
- Why unresolved: The paper only briefly mentions the performance differences without providing a comprehensive analysis of how models perform in low-resource fields and what strategies can be employed to improve their performance.
- What evidence would resolve it: A detailed study comparing model performance across all low-resource fields and exploring strategies to improve their performance would provide insights into how to enhance citation recommendation in these areas.

### Open Question 3
- Question: How do different parts-of-speech (POS) patterns around citations affect the performance of citation recommendation models, and can this information be leveraged to improve model accuracy?
- Basis in paper: [explicit] The paper mentions that the performance of models varies based on the POS of the surrounding words, with NCN showing better performance for references with the following adjective POS, LCR for preceding noun POS, and BM25 for preceding pronoun POS. However, it does not provide a detailed analysis of how different POS patterns affect model performance.
- Why unresolved: The paper only briefly mentions the performance differences without providing a comprehensive analysis of how different POS patterns affect model performance and how this information can be leveraged to improve model accuracy.
- What evidence would resolve it: A detailed study comparing model performance across different POS patterns and exploring how this information can be leveraged to improve model accuracy would provide insights into how to enhance citation recommendation based on linguistic cues.

## Limitations
- Dataset Construction Bias: Stratified sampling from S2ORC and S2AG may introduce unintended biases if sampling ratios or stratification depth are not properly specified.
- Model Training Details: Lack of full training details (e.g., hyperparameters, preprocessing) for learned models limits reproducibility and comparison.
- Citation Context Extraction: Citation parser implementation and validation are not detailed, risking noisy or incorrect diagnostic datasets if parser misidentifies contexts.

## Confidence

- **High Confidence**: BM25's superior performance across most diagnostic datasets is well-supported by the evidence, as it leverages the full S2AG metadata and query-based retrieval, which are explicitly described and implemented.
- **Medium Confidence**: The systematic performance differences across diagnostic dataset classes (e.g., NCN's improved performance with higher citation counts) are plausible but depend on the assumption that the stratified sampling yields representative subsets. The paper provides some evidence but lacks detailed validation of the sampling process.
- **Low Confidence**: The claim that diagnostic datasets enable granular performance assessment is foundational but untested. The paper assumes the stratified sampling method is unbiased and that the selected features (context length, POS, etc.) meaningfully influence model performance, but these assumptions are not empirically validated.

## Next Checks
1. Validate stratified sampling by analyzing the distribution of fields, years, and citation counts in each diagnostic dataset to ensure proportional representation and absence of sampling bias.
2. Evaluate model performance on a low-resource field (e.g., philosophy) to confirm systematic domain-specific weaknesses and compare results to high-resource fields (e.g., computer science).
3. Manually inspect a sample of citation contexts from the diagnostic datasets to verify correct extraction and parsing, checking for errors such as misidentified citation boundaries or incorrect POS tagging.