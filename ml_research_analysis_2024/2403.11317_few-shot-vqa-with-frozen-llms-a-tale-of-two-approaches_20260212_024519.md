---
ver: rpa2
title: 'Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches'
arxiv_id: '2403.11317'
source_url: https://arxiv.org/abs/2403.11317
tags:
- image
- question
- approaches
- in-context
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work compares two approaches to few-shot visual question answering
  (VQA) with frozen large language models (LLMs). The first approach maps image embeddings
  directly into the LLM embedding space, while the second uses image captions as textual
  input.
---

# Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches

## Quick Facts
- **arXiv ID:** 2403.11317
- **Source URL:** https://arxiv.org/abs/2403.11317
- **Reference count:** 13
- **Primary result:** Caption-based approach outperforms embedding-based approach in zero-shot settings; embedding-based approach slightly better in few-shot settings with joint similarity selection

## Executive Summary
This work presents a controlled comparison between two approaches for few-shot visual question answering (VQA) using frozen large language models (LLMs). The first approach maps image embeddings directly into the LLM embedding space, while the second converts images to captions and uses them as textual input. Using Flan-T5 XL (3B parameters) with a non-linear mapping network, the authors find that the caption-based approach achieves superior performance in zero-shot settings, while the embedding-based approach shows slight advantages in few-shot settings when examples are selected using joint question and image similarity. The study emphasizes the importance of comparing both approaches and carefully selecting in-context examples for optimal performance.

## Method Summary
The study compares two distinct approaches for adapting frozen LLMs to VQA tasks. The embedding-based approach uses a non-linear mapping network to transform image embeddings into the LLM's embedding space, allowing direct integration of visual information into the language model's processing pipeline. The caption-based approach generates textual descriptions of images using an image captioning model, which are then processed by the LLM as natural language input. Both approaches employ in-context learning with carefully selected examples to adapt to new VQA tasks. The controlled experiment uses Flan-T5 XL (3B parameters) as the frozen LLM backbone, with evaluation across multiple shot settings and example selection strategies based on question and image similarity.

## Key Results
- Caption-based approach outperforms embedding-based approach in zero-shot VQA settings
- Embedding-based approach shows slight performance advantages in 1-, 2-, and 4-shot settings when examples are selected using joint question and image similarity
- Performance differences between approaches are highly sensitive to in-context example selection strategy
- Non-linear mapping network is essential for effective integration of image embeddings into LLM space

## Why This Works (Mechanism)
The caption-based approach leverages the LLM's strong natural language understanding capabilities by converting visual information into textual form, allowing the model to process images using its pre-trained language representations. The embedding-based approach attempts to bypass the language bottleneck by directly injecting visual features into the LLM's embedding space, but requires careful non-linear mapping to align visual and linguistic representations. The performance differences stem from how effectively each approach preserves semantic relationships during the multimodal fusion process, with captions maintaining richer contextual information while direct embedding mapping requires more sophisticated alignment mechanisms.

## Foundational Learning

**Visual Question Answering (VQA):** A task requiring models to answer questions about visual content in images. Why needed: This is the core problem being addressed. Quick check: Can the system correctly answer questions like "What color is the car?" given an appropriate image.

**Few-shot Learning:** Learning from very limited training examples (1-4 examples per task). Why needed: VQA datasets are often small and expensive to annotate. Quick check: Can the model generalize from 2-4 examples to correctly answer similar questions.

**In-context Learning:** Providing LLMs with task examples within the input prompt rather than fine-tuning. Why needed: Allows adaptation without modifying model weights. Quick check: Does the model's performance improve when relevant examples are included in the prompt?

**Embedding Space Alignment:** Mapping representations from one modality (images) to another (text) in a shared semantic space. Why needed: Enables cross-modal reasoning in frozen models. Quick check: Are similar concepts represented close together after alignment?

**Non-linear Mapping Networks:** Neural networks that transform embeddings through non-linear functions. Why needed: Linear transformations are insufficient for complex multimodal alignment. Quick check: Does the mapping preserve semantic relationships between visual and textual concepts?

## Architecture Onboarding

**Component Map:** Image Encoder -> Embedding Mapping Network -> LLM (Flan-T5 XL) -> Output Decoder
or Image Encoder -> Caption Generator -> LLM (Flan-T5 XL) -> Output Decoder

**Critical Path:** For embedding-based: Image features → Mapping network → LLM context window → Question answering
For caption-based: Image features → Caption generation → LLM context window → Question answering

**Design Tradeoffs:** Embedding-based approach offers direct visual integration but requires complex alignment; caption-based approach is simpler but may lose fine-grained visual details. Embedding-based approach has higher computational cost due to mapping network; caption-based approach depends on caption quality.

**Failure Signatures:** Poor image encoding leads to degraded performance in both approaches. In embedding-based approach, misalignment in the mapping network causes semantic drift. In caption-based approach, poor captioning quality directly impacts answer accuracy. Both approaches fail when in-context examples are poorly selected or irrelevant to the target task.

**First Experiments:**
1. Evaluate zero-shot performance of both approaches on a simple VQA dataset with clear visual-textual correspondences
2. Test the sensitivity of each approach to the quality of image embeddings/captions by introducing controlled noise
3. Compare performance when using different numbers of in-context examples (1, 2, 4, 8 shots) with systematic example selection

## Open Questions the Paper Calls Out
None

## Limitations
- Experiment limited to single LLM architecture (Flan-T5 XL, 3B parameters), limiting generalizability
- Performance differences highly sensitive to specific in-context example selection strategy with no systematic comparison of alternatives
- Evaluation focused on limited set of VQA tasks without examining broader visual reasoning domain generalization

## Confidence
- **High confidence:** Caption-based approach superiority in zero-shot settings, supported by consistent experimental evidence
- **Medium confidence:** Few-shot performance findings, dependent on specific implementation details of example selection
- **Low confidence:** General superiority claims for either approach, as results appear highly dependent on experimental conditions and hyperparameters

## Next Checks
1. Replicate experiments across multiple LLM architectures (including larger models) and image encoders to verify robustness of observed performance differences
2. Conduct systematic ablation studies on different in-context example selection strategies, including automated methods beyond manual selection
3. Extend evaluation to diverse VQA datasets covering different types of visual reasoning tasks to assess generalization beyond current task set