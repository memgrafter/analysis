---
ver: rpa2
title: Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering
  Benchmark
arxiv_id: '2402.19248'
source_url: https://arxiv.org/abs/2402.19248
tags:
- llms
- answer
- chinese
- questions
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CDQA, the first Chinese benchmark for evaluating
  LLMs on dynamic questions whose answers change over time. The benchmark is constructed
  through a semi-automatic pipeline that combines automatic entity extraction and
  question generation with human annotation and verification, resulting in 1,339 high-quality
  question-answer pairs covering various domains like finance, politics, and technology.
---

# Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark

## Quick Facts
- arXiv ID: 2402.19248
- Source URL: https://arxiv.org/abs/2402.19248
- Reference count: 8
- Primary result: Introduces first Chinese benchmark for evaluating LLMs on dynamic questions with changing answers over time

## Executive Summary
This paper introduces CDQA, the first Chinese benchmark designed to evaluate large language models on dynamic questions whose answers change over time. The benchmark is constructed through a semi-automatic pipeline combining automatic entity extraction, question generation, and human annotation, resulting in 1,339 high-quality question-answer pairs across domains like finance, politics, and technology. The questions are classified by answer change frequency to enable fine-grained evaluation of LLM capabilities.

The authors evaluate multiple Chinese and international LLMs on CDQA, finding that GPT-4 with search engine retrieval achieves the best performance, surpassing the second-best model Deepseek-67B-Chat by nearly 10 F1-recall points. The study also examines different prompting strategies, showing that Chain-of-Thought prompting with search results generally improves performance but can increase hallucination. Notably, Chinese-oriented models tend to have more internal knowledge but perform worse than GPT-4 when search engines are used, suggesting GPT-4 has stronger ability to learn from contexts.

## Method Summary
The CDQA benchmark is constructed through a semi-automatic pipeline that combines automatic entity extraction and question generation with human annotation and verification. The process involves extracting entities from various domains, generating dynamic questions based on these entities, and having human annotators verify and refine the question-answer pairs. The resulting dataset contains 1,339 high-quality question-answer pairs covering domains such as finance, politics, and technology. Questions are classified by answer change frequency (fast-changing, slow-changing, never-changing) to enable fine-grained evaluation of LLM capabilities. The benchmark is designed to test LLMs' ability to handle time-sensitive information and adapt to changing contexts.

## Key Results
- GPT-4 with search engine retrieval achieves the best performance, surpassing Deepseek-67B-Chat by nearly 10 F1-recall points
- Chain-of-Thought prompting with search results generally improves performance but increases hallucination risk
- Chinese-oriented models have more internal knowledge but underperform GPT-4 when search engines are used, suggesting GPT-4 has stronger ability to learn from contexts

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focus on dynamic questions whose answers change over time, creating a more challenging evaluation scenario than static question-answering tasks. The semi-automatic construction pipeline ensures high-quality data while maintaining scalability. The classification of questions by answer change frequency allows for nuanced assessment of LLM capabilities across different temporal dynamics. The combination of automatic generation with human verification helps balance efficiency with quality control.

## Foundational Learning
- Dynamic question answering: Evaluating models on questions whose answers change over time tests their ability to handle real-world scenarios where information becomes outdated
  - Why needed: Static benchmarks cannot capture LLMs' ability to process time-sensitive information
  - Quick check: Compare model performance on fast-changing vs never-changing questions

- Semi-automatic benchmark construction: Combining automatic generation with human verification creates scalable yet high-quality datasets
  - Why needed: Fully manual annotation is time-consuming, while fully automatic generation may lack quality
  - Quick check: Evaluate inter-annotator agreement on human-verified questions

- Answer change frequency classification: Categorizing questions by temporal dynamics enables fine-grained performance analysis
  - Why needed: Different temporal dynamics require different LLM capabilities and strategies
  - Quick check: Analyze performance differences across frequency categories

## Architecture Onboarding
Component map: Entity extraction -> Question generation -> Human annotation -> Model evaluation -> Performance analysis

Critical path: The pipeline follows a linear flow from data collection through evaluation, with human annotation serving as the quality control bottleneck. The evaluation phase measures both raw performance and prompting strategy effectiveness.

Design tradeoffs: The semi-automatic approach balances scalability with quality but relies heavily on human annotation, which limits scalability and introduces potential bias. The focus on Chinese language content provides valuable domain-specific insights but limits generalizability.

Failure signatures: Poor inter-annotator agreement indicates annotation inconsistency; large performance gaps between automatic and human-verified questions suggest quality issues in the automatic generation phase; consistently low performance across models may indicate benchmark difficulty or construction issues.

First experiments:
1. Measure inter-annotator agreement on a subset of questions to establish annotation reliability
2. Compare performance on automatically generated vs human-verified questions to assess generation quality
3. Test model performance on different answer change frequency categories to identify temporal dynamics capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on human annotation raises scalability concerns and potential annotation bias
- Limited generalizability due to focus on Chinese language content and cultural context
- Lack of systematic hallucination measurement across prompting strategies

## Confidence
Medium: The F1-recall score comparison between GPT-4 and other models provides reasonable evidence for performance claims, but the lack of error analysis and quantification of hallucination risks reduces confidence. The benchmark construction methodology, while innovative, lacks detailed reliability metrics.

## Next Checks
1. Conduct inter-annotator agreement analysis on a subset of the CDQA dataset to establish reliability metrics for the human annotation component of the benchmark construction pipeline.

2. Perform ablation studies isolating the search engine retrieval component to determine whether GPT-4's superior performance is due to its language understanding capabilities or its ability to effectively process retrieved information.

3. Implement systematic hallucination measurement across all prompting strategies to quantify the trade-offs between performance improvements and reliability concerns, particularly for the Chain-of-Thought approach.