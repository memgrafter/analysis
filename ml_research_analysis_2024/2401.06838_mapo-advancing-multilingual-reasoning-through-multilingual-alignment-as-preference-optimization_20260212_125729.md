---
ver: rpa2
title: 'MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference
  Optimization'
arxiv_id: '2401.06838'
source_url: https://arxiv.org/abs/2401.06838
tags:
- reasoning
- languages
- preference
- optimization
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAPO improves multilingual reasoning by aligning reasoning processes
  across languages using translation model scores as preference signals for optimization.
  The method employs DPO and PPO to optimize reasoning in non-dominant languages to
  match dominant language reasoning.
---

# MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization

## Quick Facts
- arXiv ID: 2401.06838
- Source URL: https://arxiv.org/abs/2401.06838
- Authors: Shuaijie She; Wei Zou; Shujian Huang; Wenhao Zhu; Xiang Liu; Xiang Geng; Jiajun Chen
- Reference count: 12
- Primary result: Achieves up to 16.2% accuracy improvement on multilingual reasoning benchmarks

## Executive Summary
MAPO introduces a novel approach to enhance multilingual reasoning in large language models by aligning reasoning processes across languages using translation model scores as preference signals. The framework employs preference optimization algorithms (DPO and PPO) to optimize non-dominant language reasoning to match dominant language reasoning patterns. By leveraging off-the-shelf translation models to compute alignment scores, MAPO achieves significant improvements in multilingual reasoning accuracy without requiring additional reasoning annotations.

## Method Summary
MAPO uses a two-stage framework: first, it computes alignment scores between reasoning processes in non-dominant and dominant languages using translation models; second, it optimizes the base LLM using these scores as preference signals through DPO or PPO. The method employs iterative preference optimization for additional performance gains and requires no additional reasoning annotations. Experiments on three benchmarks (MSVAMP, MGSM, MNumGLUESub) demonstrate accuracy improvements of up to 16.2%, 6.1%, and 13.3% respectively.

## Key Results
- Achieves 16.2% accuracy improvement on MSVAMP benchmark
- Achieves 6.1% accuracy improvement on MGSM benchmark
- Achieves 13.3% accuracy improvement on MNumGLUESub benchmark
- Demonstrates consistent performance across different translation model scales
- Shows robust performance across 10 different languages in test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual reasoning abilities can be aligned by optimizing non-dominant language outputs to match dominant language reasoning using translation model scores as preference signals.
- Mechanism: The method leverages an off-the-shelf translation model to compute alignment scores between reasoning processes in non-dominant and dominant languages. Higher alignment scores indicate more consistent reasoning, which