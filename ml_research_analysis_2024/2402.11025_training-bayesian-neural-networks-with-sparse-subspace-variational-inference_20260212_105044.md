---
ver: rpa2
title: Training Bayesian Neural Networks with Sparse Subspace Variational Inference
arxiv_id: '2402.11025'
source_url: https://arxiv.org/abs/2402.11025
tags:
- training
- ssvi
- sparse
- criteria
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sparse Subspace Variational Inference (SSVI) is a fully sparse
  Bayesian neural network framework that maintains high sparsity throughout both training
  and inference. It confines parameters to an adaptive low-dimensional sparse subspace,
  alternately optimizing the subspace basis and its associated parameters.
---

# Training Bayesian Neural Networks with Sparse Subspace Variational Inference

## Quick Facts
- arXiv ID: 2402.11025
- Source URL: https://arxiv.org/abs/2402.11025
- Authors: Junbo Li; Zichen Miao; Qiang Qiu; Ruqi Zhang
- Reference count: 38
- Key outcome: SSVI achieves 10-20x compression with under 3% performance drop on CIFAR datasets

## Executive Summary
Sparse Subspace Variational Inference (SSVI) is a fully sparse Bayesian neural network framework that maintains high sparsity throughout both training and inference. It confines parameters to an adaptive low-dimensional sparse subspace, alternately optimizing the subspace basis and its associated parameters. The key innovation is using novel weight distribution statistics-based criteria for subspace removal and addition, achieving significant model compression while preserving accuracy and uncertainty quantification.

## Method Summary
SSVI implements Bayesian inference within a sparse subspace by confining the variational posterior to a low-dimensional subspace S of dimension s. The method alternates between optimizing the posterior parameters (ϕ) using standard VI updates and optimizing the sparse subspace basis (γ) through a removal-and-addition strategy guided by weight distribution statistics. Training proceeds for 200 epochs with SGD optimizer, batch size 128, initial learning rate 0.01 with cosine annealing decay, and KL warm-up technique applied.

## Key Results
- Achieves 10-20x compression in model size with under 3% performance drop
- Reduces training FLOPs by up to 20x compared to dense VI
- Demonstrates superior accuracy and uncertainty quantification on CIFAR-10/100

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSVI maintains high sparsity throughout training by optimizing both the sparse subspace basis and the posterior parameters alternately.
- Mechanism: By starting from a low-dimensional sparse subspace and alternately optimizing the subspace basis selection (γ) and the associated posterior parameters (ϕ), SSVI ensures sparsity is preserved throughout the entire training process.
- Core assumption: The optimal sparse subspace can be approximated through a removal-and-addition strategy guided by weight distribution statistics.
- Evidence anchors:
  - [abstract] "Starting from a randomly initialized low-dimensional sparse subspace, our approach alternately optimizes the sparse subspace basis selection and its associated parameters."
  - [section 3.1.1] "Given the discrete nature of γ, directly applying gradient descent for both variables is infeasible. To address this challenge, we propose to alternatively update ϕ and γ at each iteration."
- Break condition: If the removal-and-addition strategy fails to approximate the optimal subspace, sparsity levels may degrade during training.

### Mechanism 2
- Claim: SSVI reduces training costs by confining parameter optimization to a sparse subspace.
- Mechanism: By restricting the posterior distribution to a low-dimensional sparse subspace, SSVI eliminates the need to compute gradients and perform updates for the majority of parameters, significantly reducing computational overhead.
- Core assumption: The sparse subspace contains sufficient information to maintain model performance while drastically reducing the parameter space.
- Evidence anchors:
  - [abstract] "Starting from a randomly initialized low-dimensional sparse subspace, our approach alternately optimizes the sparse subspace basis selection and its associated parameters."
  - [section 3.1] "To address this, we introduce SSVI, wherein we confine the support of qϕ(θ) to a subspace S of a pre-assigned dimension s within Rd, implying qϕ(θ) = 0 for θ not belonging to S."
- Break condition: If the sparse subspace becomes too restrictive, model performance may degrade despite computational savings.

### Mechanism 3
- Claim: SSVI achieves superior accuracy and uncertainty quantification by leveraging weight distribution statistics for subspace optimization.
- Mechanism: The novel criteria based on weight distribution statistics (e.g., SNRqϕ(|θ|)) provide more informative guidance for subspace optimization compared to traditional magnitude-based pruning, leading to better-preserved model capacity and uncertainty estimation.
- Core assumption: Weight distribution statistics contain sufficient information to identify important parameters in BNNs.
- Evidence anchors:
  - [abstract] "While basis selection is characterized as a non-differentiable problem, we approximate the optimal solution with a removal-and-addition strategy, guided by novel criteria based on weight distribution statistics."
  - [section 3.3.1] "Building upon this insight, we derive several novel criteria for removing indices in the subspace."
- Break condition: If the weight distribution statistics fail to capture parameter importance, the subspace optimization may degrade model performance.

## Foundational Learning

- Concept: Variational Inference (VI) in Bayesian Neural Networks
  - Why needed here: SSVI is built upon VI framework, understanding the basics of VI is crucial for grasping how SSVI optimizes the posterior distribution.
  - Quick check question: What is the main objective of variational inference in the context of Bayesian neural networks?

- Concept: Sparse Neural Networks
  - Why needed here: SSVI aims to create sparse BNNs, understanding the principles of sparse neural networks is essential for comprehending the benefits and challenges of SSVI.
  - Quick check question: How does sparsity in neural networks contribute to computational efficiency and model compression?

- Concept: Subspace Optimization
  - Why needed here: SSVI optimizes parameters within a low-dimensional subspace, understanding subspace optimization techniques is crucial for grasping how SSVI maintains sparsity throughout training.
  - Quick check question: What are the advantages and challenges of optimizing neural network parameters within a subspace?

## Architecture Onboarding

- Component map:
  Sparse Subspace Basis (γ) -> Posterior Parameters (ϕ) -> Weight Distribution Statistics -> KL Divergence and Likelihood Terms

- Critical path:
  1. Initialize sparse subspace basis γ and posterior parameters ϕ
  2. Alternately optimize ϕ (via standard VI updates) and γ (via removal-and-addition strategy)
  3. Use weight distribution statistics to guide subspace optimization
  4. Maintain sparsity throughout training and inference

- Design tradeoffs:
  - Sparsity vs. Performance: Higher sparsity levels may lead to reduced model capacity and degraded performance
  - Computational Efficiency vs. Accuracy: Optimizing within a sparse subspace reduces computational costs but may limit the model's ability to capture complex patterns
  - Hyperparameter Sensitivity: SSVI's performance may be sensitive to the choice of sparsity level and other hyperparameters

- Failure signatures:
  - Degraded performance despite high sparsity: Indicates that the sparse subspace is too restrictive or the optimization strategy is not effective
  - Unstable training: May suggest issues with the initialization or the alternation between ϕ and γ updates
  - Poor uncertainty quantification: Could indicate that the weight distribution statistics are not capturing parameter importance effectively

- First 3 experiments:
  1. Train SSVI on CIFAR-10 with varying sparsity levels (e.g., 50%, 90%, 95%) and compare performance to dense BNNs and other sparse BNN methods
  2. Ablation study: Evaluate the impact of different weight distribution statistics (e.g., |µ|, SNRqϕ(θ), Eqϕ|θ|) on SSVI's performance
  3. Analyze the evolution of the sparse subspace throughout training: Track the active parameters in γ and their corresponding importance scores to understand how the subspace adapts to the data

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the methodology and results, several areas warrant further investigation:

1. How does the choice of criteria for weight removal (e.g., SNRqϕ(|θ|) vs. Eqϕ |θ|) affect the final model performance and uncertainty estimation across different datasets and architectures?

2. What is the theoretical convergence guarantee for the alternating optimization procedure in SSVI, and how does the choice of hyperparameters (e.g., replacement rate rt, inner update steps M) affect convergence speed and final solution quality?

3. How does SSVI perform on extremely high-dimensional data (e.g., genomics, hyperspectral imaging) where the curse of dimensionality becomes a significant challenge, and can the subspace dimension s be adaptively determined during training?

## Limitations

- Limited validation on diverse network architectures beyond ResNet-18
- Long-term stability of sparsity patterns during extended training periods is not evaluated
- Impact of different initialization strategies on SSVI's performance is not thoroughly explored

## Confidence

- Claims about sparsity maintenance during training: Medium
- Claims about computational efficiency improvements: Medium
- Claims about uncertainty quantification: Medium

## Next Checks

1. Test SSVI on architectures with different sparsity characteristics (e.g., transformers, MLPs) to verify architecture independence
2. Conduct ablation studies comparing the weight distribution statistics criteria against simpler magnitude-based pruning methods
3. Evaluate SSVI's performance with varying KL warm-up schedules and maximum coefficients to assess hyperparameter sensitivity