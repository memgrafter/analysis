---
ver: rpa2
title: Haar-Laplacian for directed graphs
arxiv_id: '2411.15527'
source_url: https://arxiv.org/abs/2411.15527
tags:
- graph
- graphs
- laplacian
- matrix
- directed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Haar-Laplacian, a novel Hermitian matrix
  for directed graphs that preserves edge direction and weight information while maintaining
  spectral properties. The method is based on a Haar-like transformation using symmetrized
  and skew-symmetrized adjacency matrices.
---

# Haar-Laplacian for directed graphs

## Quick Facts
- arXiv ID: 2411.15527
- Source URL: https://arxiv.org/abs/2411.15527
- Authors: Theodor-Adrian Badea; Bogdan Dumitrescu
- Reference count: 40
- Primary result: Introduces Haar-Laplacian, a novel Hermitian matrix for directed graphs that preserves edge direction and weight information while maintaining spectral properties

## Executive Summary
This paper introduces the Haar-Laplacian, a novel Hermitian matrix for directed graphs that preserves edge direction and weight information while maintaining spectral properties. The method is based on a Haar-like transformation using symmetrized and skew-symmetrized adjacency matrices. Key properties include scaling robustness, sensitivity, continuity, and one-to-one mapping with the original adjacency matrix. The authors demonstrate its effectiveness in two applications: graph learning through a new spectral graph convolutional network (HaarNet) and graph signal processing for denoising.

## Method Summary
The Haar-Laplacian is constructed by decomposing the adjacency matrix into symmetrized (As = (A + AT)/2) and skew-symmetrized (Aa = (A - AT)/2) components, forming a Hermitian matrix Hh = As + iAa. From this, the diagonal matrix Dh = diag(|Hh|·1) is computed, and the Laplacian is defined as Lh = Dh - Hh. For graph learning, a spectral graph convolutional network (HaarNet) is built using this Laplacian, with polynomial approximation of spectral filters. The method is evaluated on link prediction (existence, three-class, weight prediction) and node classification tasks across five real-world directed graph datasets using 10-fold cross-validation and early stopping.

## Key Results
- HaarNet achieves up to 0.11% better accuracy than magnetic and sign-magnetic Laplacians on weight prediction tasks
- The HaarD-Laplacian variant provides better frequency domain interpretation for signal processing applications
- Superior denoising performance demonstrated on random geometric graphs compared to existing Hermitian Laplacians
- One-to-one mapping between adjacency matrix and Laplacian preserves all edge direction and weight information without ambiguity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Haar-Laplacian provides a one-to-one mapping from the adjacency matrix to the Laplacian, preserving both edge direction and weight information without ambiguity.
- Mechanism: The Hermitian matrix Hh = As + iAa combines the symmetrized (average flow) and skew-symmetrized (difference flow) adjacency matrices, ensuring all directional and weight information is encoded. This avoids the ambiguity present in magnetic or sign-magnetic Laplacians where antiparallel edges can collapse to identical values.
- Core assumption: The transformation A → Hh is bijective for simple graphs without self-loops, and the spectral properties of Hermitian matrices (real eigenvalues, orthogonal eigenvectors) carry over to the Laplacian construction.
- Evidence anchors:
  - [abstract] "not only in one-to-one relation with the adjacency matrix, preserving both direction and weight information"
  - [section] "So, the situation is similar with that of a Haar filter bank, although no explicit downsampling is involved; the number of independent elements is the same in A as in As and Aa together, and the transformation is linear."
  - [corpus] No direct mention of this exact mechanism; the claim is based on the paper's theoretical derivation.
- Break condition: If the graph contains self-loops or multiple edges between the same nodes, the one-to-one mapping property breaks down, and additional handling would be required.

### Mechanism 2
- Claim: The HaarD-Laplacian variant provides a meaningful frequency domain interpretation for signal processing applications, with eigenvalues ordered by absolute value.
- Mechanism: By using the total variation TVQ defined with the complex matrix Q (with α = -1, β = 1), the HaarD-Laplacian (Ds - Hh) becomes directly related to this variation measure. The eigenvalues are ordered by |λ| rather than λ, allowing low frequencies to correspond to low TVQ values and high frequencies to high TVQ values.
- Core assumption: The relationship TVQ(z) = |zH(L̃h)z| holds for all signals z, providing a sound basis for interpreting eigenvalues as frequencies. This ordering differs from the standard Laplacian where only non-negative eigenvalues exist.
- Evidence anchors:
  - [section] "Relation (26) between total variation and the HaarD-Laplacian suggests that, seen as graph frequencies, the eigenvalues of Λ̃ should be ordered in increasing order of their absolute value"
  - [section] "For such α, the matrix Q is no longer positive semidefinite if β ≠ 0. However, we can define a TV measure by using the absolute value of (21):"
  - [corpus] No direct mention; this is a theoretical contribution from the paper.
- Break condition: If the graph contains only symmetric edges or is nearly symmetric, the frequency ordering may become less meaningful, and the distinction between Haar and HaarD may diminish.

### Mechanism 3
- Claim: The Haar-Laplacian enables superior weight prediction in graph learning tasks by preserving continuous weight information in the Laplacian construction.
- Mechanism: Unlike magnetic or sign-magnetic Laplacians that encode directionality through phase but lose continuous weight information (especially for antiparallel edges), the Haar-Laplacian maintains the full continuous mapping from weights to Laplacian entries. This allows the graph neural network to learn weight patterns more effectively.
- Core assumption: The learning task benefits from having all weight information preserved in the Laplacian, and the neural network can effectively utilize this information through the spectral convolution operations.
- Evidence anchors:
  - [section] "The last series of experiments, addressing the weight prediction flavor of the problem, proves that our Haar Laplacian is indeed able to encode all graph information: structure, directionality, and weights."
  - [section] "We believe that the performance of HaarNet on Telegram and UC-Social is supported by the structure of the datasets, in the sense that Telegram contains an insignificant number of antiparallel edges and UC-Social does not contain any, and, moreover, both have a much larger spectrum of weights."
  - [corpus] No direct mention of this mechanism; the claim is supported by the experimental results in the paper.
- Break condition: If the learning task focuses primarily on graph structure or connectivity rather than weights, the advantage of preserving continuous weight information may be less significant.

## Foundational Learning

- Concept: Hermitian matrices and their spectral properties
  - Why needed here: The Haar-Laplacian is built on a Hermitian matrix Hh, and understanding its properties (real eigenvalues, orthogonal eigenvectors, unitary eigenvector matrix) is essential for both theoretical analysis and practical implementation.
  - Quick check question: Given a Hermitian matrix, what can you say about its eigenvalues and eigenvectors without computing them?

- Concept: Graph Fourier Transform and frequency domain interpretation
  - Why needed here: The paper relies on interpreting eigenvalues as graph frequencies, especially for the HaarD-Laplacian variant where eigenvalues are ordered by absolute value rather than magnitude. Understanding GFT is crucial for signal processing applications.
  - Quick check question: How does the Graph Fourier Transform differ from the classical Fourier Transform, and what role does the Laplacian play in this transformation?

- Concept: Spectral graph convolution and polynomial approximation
  - Why needed here: The HaarNet architecture uses spectral graph convolutions based on the proposed Laplacian. Understanding how spectral convolutions work and how they're approximated using Chebyshev polynomials is essential for implementing and modifying the architecture.
  - Quick check question: Why do we need to approximate the spectral filter using polynomials rather than directly using the eigen-decomposition, and what are the computational advantages?

## Architecture Onboarding

- Component map: Graph data (A, X) -> Haar-Laplacian construction (As, Aa, Hh, Dh, Lh) -> Spectral convolution layers (polynomial approximation) -> Unwind operation (link prediction) or Conv1D (node classification) -> Prediction layers -> Output

- Critical path:
  1. Construct the Hermitian matrix Hh from adjacency matrix A
  2. Build the diagonal matrix Dh from |Hh|
  3. Form the Haar-Laplacian Lh = Dh - Hh
  4. Compute the spectral convolution using polynomial approximation
  5. Apply nonlinearity (ReLU on real and imaginary parts separately)
  6. For link prediction: unwind operation and final prediction layer
  7. For node classification: concatenate real/imag parts and apply Conv1D

- Design tradeoffs:
  - Computational cost: Computing the Hermitian matrix and its diagonal is O(N²), which can be expensive for large graphs. Consider sparse implementations.
  - Memory usage: Complex-valued matrices require twice the storage of real matrices. Be mindful of memory constraints.
  - Numerical stability: The construction involves absolute values and square roots. Ensure proper handling of edge cases and numerical precision.
  - Generalization: The Haar-Laplacian preserves all information, which may lead to better performance on weight prediction but potentially slower convergence on tasks where this information is less relevant.

- Failure signatures:
  - Poor performance on existence/three-class prediction: May indicate that preserving all weight information is unnecessary for these tasks, or that the learning rate/architecture needs adjustment.
  - Unstable training or NaN values: Check the construction of the Hermitian matrix, especially the absolute value operations and diagonal matrix computation.
  - Slow convergence: The complex-valued operations may require more training iterations. Consider adjusting learning rate or using normalization techniques.
  - Memory errors: For large graphs, the complex matrix storage may exceed available memory. Consider using sparse representations or graph sampling techniques.

- First 3 experiments:
  1. Verify the Haar-Laplacian construction: Take a small directed graph with known adjacency matrix, manually compute As, Aa, Hh, Dh, and Lh. Check that the eigenvalues are real and non-negative.
  2. Compare frequency domain behavior: Apply the Haar-Laplacian GFT to a simple signal (e.g., linear function of node coordinates) and visualize the frequency coefficients. Compare with standard Laplacian behavior.
  3. Basic link prediction test: Use a small directed graph dataset (e.g., a subset of Bitcoin-Alpha) and train a minimal HaarNet with 2 layers and small dimension. Verify that the model can learn to predict edge existence before scaling to full experiments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the HaarD-Laplacian's frequency domain interpretation compare to other Hermitian Laplacians in terms of practical denoising performance across different graph structures?
- Basis in paper: [explicit] The paper demonstrates superior denoising performance of HaarD-Laplacian compared to magnetic and sign-magnetic Laplacians in geometric graphs, but only explores this on one graph type
- Why unresolved: The experiments only test on random geometric graphs with specific parameters (N=500, p=0.5 or p=1, specific weight ranges). The performance could vary significantly with different graph structures, sizes, or weight distributions
- What evidence would resolve it: Systematic comparison of HaarD-Laplacian denoising performance across diverse graph types (scale-free, small-world, community-structured) and varying parameters, measuring SNR improvement across different noise levels

### Open Question 2
- Question: Does the one-to-one mapping between adjacency matrix and Haar-Laplacian provide a fundamental advantage for link weight prediction that cannot be achieved through other means?
- Basis in paper: [explicit] The authors claim HaarNet's superiority in weight prediction stems from preserving the one-to-one relation with the adjacency matrix, achieving up to 0.11% better accuracy than magnetic Laplacians
- Why unresolved: While the paper shows empirical superiority, it doesn't prove that this property is the essential factor rather than other characteristics of the Haar-Laplacian. The experiments compare multiple Laplacians but don't isolate this specific property
- What evidence would resolve it: Ablation studies where different Laplacian properties are systematically removed or modified while keeping others constant, or theoretical proof that the one-to-one mapping is necessary for optimal weight prediction performance

### Open Question 3
- Question: What is the exact relationship between the eigenvalues of the Haar-Laplacian and the "total variation" measure defined in the paper, and how does this relationship enable better frequency interpretation than existing methods?
- Basis in paper: [explicit] The paper establishes Proposition 9 linking TVQ(z) to the HaarD-Laplacian through equality TVQ(z) = |zH˜Lhz|, but notes that while 1T˜Lh1 = 0, the vector 1 is not necessarily an eigenvector
- Why unresolved: The paper provides the theoretical connection but doesn't fully explore the practical implications of this relationship. The fuzzy interpretation of low frequencies and the ordering of eigenvalues by absolute value rather than magnitude is noted but not deeply investigated
- What evidence would resolve it: Detailed analysis of eigenvector behavior across different graph types, showing how the Haar-Laplacian's frequency interpretation enables better signal compression or denoising than magnetic Laplacians, particularly for signals with specific frequency characteristics

## Limitations

- The computational complexity of O(N²) for constructing the Hermitian matrix may limit scalability to very large graphs
- The one-to-one mapping property assumes simple graphs without self-loops, requiring additional handling for more general graph structures
- The performance advantage in weight prediction tasks may be dataset-dependent, particularly for graphs with limited antiparallel edges

## Confidence

- **High Confidence**: The mathematical construction of the Haar-Laplacian matrix and its Hermitian properties are rigorously derived and internally consistent. The basic spectral properties (real eigenvalues, orthogonal eigenvectors) follow directly from the Hermitian nature of the construction.

- **Medium Confidence**: The experimental results showing superior weight prediction performance are well-documented, but the advantage may be partially attributed to dataset characteristics (limited antiparallel edges in Telegram and UC-Social). The comparison with magnetic and sign-magnetic Laplacians is fair, but additional baseline methods could strengthen the claims.

- **Low Confidence**: The frequency domain interpretation for the HaarD-Laplacian variant relies on theoretical relationships that, while plausible, would benefit from more extensive empirical validation across diverse graph types and signal patterns.

## Next Checks

1. **Scalability Test**: Implement sparse matrix representations and test the Haar-Laplacian construction on graphs with 10K+ nodes to verify computational feasibility and identify performance bottlenecks.

2. **Dataset Dependency Analysis**: Systematically vary the proportion of antiparallel edges in synthetic datasets and measure the performance gap between Haar-Laplacian and magnetic Laplacians to quantify the impact of edge directionality patterns.

3. **Frequency Domain Validation**: Apply the HaarD-Laplacian to a variety of synthetic graph signals (low-frequency, high-frequency, band-limited) and verify that the eigenvalue ordering by absolute value correctly corresponds to the expected frequency content as predicted by the total variation relationship.