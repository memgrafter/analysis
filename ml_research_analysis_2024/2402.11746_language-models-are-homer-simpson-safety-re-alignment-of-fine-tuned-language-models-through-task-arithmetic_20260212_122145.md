---
ver: rpa2
title: Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language
  Models through Task Arithmetic
arxiv_id: '2402.11746'
source_url: https://arxiv.org/abs/2402.11746
tags:
- safety
- arxiv
- fine-tuning
- resta
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RESTA, a method to restore safety in fine-tuned
  language models that often lose safety guardrails. RESTA uses simple arithmetic
  addition of a safety vector to the model weights.
---

# Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic

## Quick Facts
- arXiv ID: 2402.11746
- Source URL: https://arxiv.org/abs/2402.11746
- Authors: Rishabh Bhardwaj; Do Duc Anh; Soujanya Poria
- Reference count: 7
- One-line primary result: RESTA reduces harmfulness from 18.6% to 5.1% (PEFT) and 9.2% to 1.5% (Full-FT) while maintaining performance

## Executive Summary
The paper introduces RESTA, a method to restore safety in fine-tuned language models that often lose safety guardrails. RESTA uses simple arithmetic addition of a safety vector to the model weights. It employs Drop and REscale (DARE) to enhance effectiveness. Evaluations show RESTA reduces harmfulness from 18.6% to 5.1% (PEFT) and 9.2% to 1.5% (Full-FT) while maintaining performance. It also generalizes across languages and evaluation benchmarks.

## Method Summary
RESTA addresses safety degradation in fine-tuned language models by adding a safety vector computed from the difference between aligned and unaligned models. The method employs Drop and REscale (DARE) to enhance effectiveness by reducing redundancy in delta parameters. It's tested on both parameter-efficient fine-tuning (PEFT) and full fine-tuning approaches, showing significant safety improvements while maintaining task performance across multiple languages and benchmarks.

## Key Results
- RESTA reduces harmfulness from 18.6% to 5.1% for PEFT models
- RESTA reduces harmfulness from 9.2% to 1.5% for Full-FT models
- Method maintains task performance while improving safety across multiple languages and benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding a safety vector to the weights of a fine-tuned model restores safety guardrails.
- Mechanism: The safety vector is computed as the difference between the aligned base model and an unaligned version. Adding this vector compensates for the degradation in safety caused by task-specific fine-tuning.
- Core assumption: The difference vector between aligned and unaligned models primarily captures safety degradation rather than task-specific capability changes.
- Evidence anchors:
  - [abstract] "At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model."
  - [section] "δsaf e = θ+base − θbase" and "We compute the safety vector δsaf e by identifying the shift of aligned mode from its unaligned counterpart"
  - [corpus] Weak. The corpus contains related work on safety re-alignment but does not directly validate the vector arithmetic mechanism.
- Break condition: If the unalignment process introduces changes unrelated to safety, or if the safety vector captures non-safety-related shifts, the restoration may fail.

### Mechanism 2
- Claim: DARE (Drop and REscale) enhances RESTA by reducing redundancy in delta parameters.
- Mechanism: DARE drops a fraction p of delta parameters and rescales the rest, reducing the impact of non-safety-related fine-tuning changes and giving more room for the safety vector to act.
- Core assumption: A large fraction of delta parameters from fine-tuning are redundant and can be zeroed without harming task performance.
- Evidence anchors:
  - [abstract] "We employ Drop and REscale (DARE) to enhance effectiveness."
  - [section] "We perform Drop and REscale operations (DARE) on the delta parameters... an extensive set of SFT delta parameters are redundant and can be zeroed out"
  - [corpus] Weak. No direct evidence in corpus about DARE's impact on RESTA effectiveness.
- Break condition: If delta parameters are not redundant or if dropping them harms task performance, DARE may reduce RESTA's effectiveness.

### Mechanism 3
- Claim: RESTA generalizes across languages and evaluation benchmarks.
- Mechanism: The safety vector is obtained from a small unalignment dataset and then applied to models fine-tuned on various tasks, languages, and evaluation datasets.
- Core assumption: The safety degradation patterns are similar across languages and tasks, so a single safety vector can restore safety broadly.
- Evidence anchors:
  - [abstract] "We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset"
  - [section] "We test R ESTA on two common LLM fine-tuning approaches... covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math"
  - [corpus] Weak. Related papers propose alternative safety re-alignment methods but do not validate cross-lingual generalization.
- Break condition: If safety degradation patterns differ significantly across languages or tasks, the single safety vector may not generalize.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT) methods like LoRA
  - Why needed here: RESTA is tested on both PEFT and full fine-tuning, so understanding how PEFT works is crucial for interpreting results.
  - Quick check question: How does LoRA modify model weights differently from full fine-tuning?

- Concept: Delta parameters and task vectors
  - Why needed here: RESTA and DARE both operate on delta parameters; understanding their role is essential.
  - Quick check question: What is the mathematical relationship between pre-training weights, fine-tuning weights, and delta parameters?

- Concept: Safety alignment via RLHF or DPO
  - Why needed here: The safety vector is derived from aligned vs. unaligned models; knowing how alignment works helps understand the vector's meaning.
  - Quick check question: How do RLHF and DPO jointly optimize for helpfulness and harmlessness?

## Architecture Onboarding

- Component map:
  Base model -> SFT model -> Unaligned model -> Safety vector -> RESTA model (with optional DARE)

- Critical path:
  1. Fine-tune base model on downstream task → SFT model
  2. Create unaligned model by fine-tuning aligned base on harmful QA
  3. Compute safety vector as difference between aligned and unaligned
  4. Apply DARE to SFT delta parameters (optional)
  5. Add safety vector to SFT model (with or without DARE)

- Design tradeoffs:
  - Single safety vector vs. task-specific vectors
  - DARE drop rate p: higher p → more aggressive safety restoration but potential performance loss
  - Safety vector scaling factor γ: higher γ → stronger safety restoration but potential over-correction

- Failure signatures:
  - Safety score does not improve after RESTA
  - Performance drops significantly after RESTA
  - DARE causes task performance degradation
  - Safety vector does not generalize to new languages/tasks

- First 3 experiments:
  1. Fine-tune Llama-2 on a small instruction-following dataset, apply RESTA, evaluate safety and performance
  2. Repeat experiment with DARE (p=0.3), compare safety and performance
  3. Apply RESTA to a full fine-tuned model, evaluate on out-of-domain safety benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of RESTA and DARE vary across different model sizes (e.g., Llama-2-7B vs Llama-2-70B)?
- Basis in paper: [inferred] The paper mentions that "we believe the effectiveness of DARE depends on the model size" but does not provide empirical evidence or analysis.
- Why unresolved: The authors were constrained by budget and could not evaluate larger models like Llama-2-70B, leaving the relationship between model size and the effectiveness of RESTA/DARE unexplored.
- What evidence would resolve it: Empirical results comparing RESTA and DARE effectiveness across multiple model sizes, including both small (7B) and large (70B) variants, would clarify how model scale affects the method's performance.

### Open Question 2
- Question: How transferable are safety vectors across different base models (e.g., from Llama-2 to GPT-3.5 or other architectures)?
- Basis in paper: [inferred] The authors mention that "another limitation of this work is analyzing the impact of safety vectors and their transferability across language models" but do not investigate cross-model transferability.
- Why unresolved: The paper only evaluates RESTA on Llama-2 variants, leaving the generalizability of safety vectors to other model architectures and families unexplored.
- What evidence would resolve it: Experiments applying safety vectors trained on one model (e.g., Llama-2) to other models (e.g., GPT-3.5, Mistral) would demonstrate whether safety vectors are model-specific or transferable across architectures.

### Open Question 3
- Question: What is the optimal hyperparameter configuration (γ and p) for RESTA across different fine-tuning domains and tasks?
- Basis in paper: [explicit] The paper states "we keep hyperparameters p = 0.3 and γ = 0.5 which works stably across tasks" but acknowledges that "for each task, one can optimize the hyperparameters to get to a sweet spot of performance and safety."
- Why unresolved: The authors used fixed hyperparameters across all experiments without systematic optimization, potentially missing task-specific configurations that could improve results.
- What evidence would resolve it: A comprehensive hyperparameter search (grid or Bayesian optimization) across different tasks and fine-tuning domains would identify optimal γ and p values for each scenario, potentially improving both safety and performance outcomes.

## Limitations

- The core mechanism of using a single safety vector for all tasks and languages needs stronger validation across diverse safety failures.
- DARE's theoretical justification for reducing redundancy in delta parameters lacks direct empirical evidence.
- The paper doesn't explore cross-model transferability of safety vectors or optimal hyperparameter tuning for different tasks.

## Confidence

- RESTA's safety restoration effectiveness: High
- DARE's contribution to RESTA: Medium
- Cross-lingual and cross-task generalization: Medium
- Mechanism validity of single safety vector: Low

## Next Checks

1. Verify RESTA's effectiveness on a new fine-tuning task not covered in the original evaluation set
2. Test whether DARE parameters (drop rate p) need task-specific tuning for optimal results
3. Evaluate whether the safety vector computed from one model can restore safety in a different model architecture