---
ver: rpa2
title: 'Knowledge Graphs are all you need: Leveraging KGs in Physics Question Answering'
arxiv_id: '2412.05453'
source_url: https://arxiv.org/abs/2412.05453
tags:
- question
- knowledge
- llms
- reasoning
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a novel pipeline that leverages knowledge graphs
  generated by large language models to decompose high school-level physics questions
  into sub-questions. The approach aims to enhance model response quality by constructing
  knowledge graphs that capture the internal logic of questions, which then guide
  the generation of more logically consistent sub-questions.
---

# Knowledge Graphs are all you need: Leveraging KGs in Physics Question Answering

## Quick Facts
- arXiv ID: 2412.05453
- Source URL: https://arxiv.org/abs/2412.05453
- Reference count: 8
- Success rates: 84.45% (numerical solving) and 74.58% (conceptual reasoning) with KG-based decomposition

## Executive Summary
This study proposes a novel pipeline that leverages knowledge graphs generated by large language models to decompose high school-level physics questions into sub-questions. The approach aims to enhance model response quality by constructing knowledge graphs that capture the internal logic of questions, which then guide the generation of more logically consistent sub-questions. A dataset of 8,000 physics questions was augmented with corresponding knowledge graphs and subqueries using Gemini Pro. Human evaluation with GPT-4 across 100 questions showed success rates of 84.45% for numerical solving and 74.58% for conceptual reasoning when using knowledge graph-based decomposition, compared to 77.62% and 68.53% with standard prompting.

## Method Summary
The paper introduces a pipeline that uses LLMs to construct knowledge graphs capturing the internal logic of physics questions, then generates sub-questions guided by these graphs. The process involves generating knowledge graphs using Gemini Pro, creating sub-questions based on these graphs, answering the sub-questions, and synthesizing final answers. The methodology was evaluated on a dataset of 8,000 high school physics questions, comparing performance against standard prompting and decomposition without knowledge graphs.

## Key Results
- Knowledge graph-based decomposition achieved 84.45% success rate for numerical solving versus 77.62% with standard prompting
- Conceptual reasoning success improved to 74.58% with KG approach versus 68.53% without
- Student survey of 5 participants indicated preference for KG approach due to improved clarity and logical consistency
- Sub-questions derived from knowledge graphs showed significantly improved fidelity to original question logic

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Knowledge graph-guided decomposition produces sub-questions that are more logically consistent with the original question.
- **Mechanism**: By constructing a knowledge graph that captures the internal logic and relationships within the original question, the model is guided to generate sub-questions that align with the original question's intent and structure.
- **Core assumption**: The knowledge graph accurately captures the essential relationships and logic of the original question.
- **Evidence anchors**: Knowledge graphs guide sub-question generation; sub-questions show improved fidelity to original question logic.

### Mechanism 2
- **Claim**: The knowledge graph approach reduces hallucinations and improves faithfulness in model responses.
- **Mechanism**: The knowledge graph provides a structured, factual foundation that constrains the model's reasoning path.
- **Core assumption**: Structured knowledge representations improve model faithfulness by providing verifiable facts.
- **Evidence anchors**: Enhanced capability to adhere to pertinent concepts; improved factual accuracy in related KG-enhanced LLM work.

### Mechanism 3
- **Claim**: Knowledge graph-guided decomposition improves performance on both numerical solving and conceptual reasoning tasks.
- **Mechanism**: The knowledge graph breaks down complex questions into manageable components with clear relationships.
- **Core assumption**: Complex physics problems benefit from decomposition into sub-questions with maintained logical connections.
- **Evidence anchors**: Similar reasoning paths for numerical solving; improved performance for conceptual reasoning requiring deep understanding.

## Foundational Learning

- **Concept**: Knowledge Graphs and their structure (nodes, edges, properties)
  - **Why needed here**: Understanding how KGs represent entities, relationships, and attributes is essential for implementing the knowledge graph construction and interpretation steps.
  - **Quick check question**: Given a physics problem about a circuit, what would be the nodes, edges, and properties in its knowledge graph representation?

- **Concept**: Question Decomposition techniques
  - **Why needed here**: The core innovation involves breaking down complex questions into sub-questions, so understanding various decomposition strategies provides context.
  - **Quick check question**: What are the key differences between rule-based question decomposition and knowledge graph-guided decomposition?

- **Concept**: Chain of Thought (CoT) prompting
  - **Why needed here**: Understanding this technique helps in comparing the knowledge graph approach with other reasoning enhancement methods.
  - **Quick check question**: How does Chain of Thought prompting differ from knowledge graph-guided decomposition in terms of how they structure the reasoning process?

## Architecture Onboarding

- **Component map**: Original Question → Knowledge Graph Generator (Gemini Pro) → Knowledge Graph → Sub-question Generator → Sub-questions → Answering Module → Sub-answers → Synthesis Module → Final Answer
- **Critical path**: Original Question → Knowledge Graph → Sub-questions → Answering → Synthesis → Final Answer
- **Design tradeoffs**:
  - **Pros**: Improved logical consistency, reduced hallucinations, better performance on complex questions
  - **Cons**: Increased inference costs, dependency on KG construction quality, potential scalability issues
  - **Alternative consideration**: Direct fine-tuning of LLMs on physics problems vs. using KGs for decomposition
- **Failure signatures**: Incorrect or incomplete knowledge graphs leading to irrelevant sub-questions; failure to synthesize sub-answers coherently
- **First 3 experiments**:
  1. **Unit test the knowledge graph generator**: Input physics questions and verify generated KGs capture essential entities, relationships, and properties correctly.
  2. **Validate sub-question generation**: Check that generated sub-questions are logically consistent with KG structure and address key components.
  3. **Compare end-to-end performance**: Run full pipeline on small dataset and compare final answers against ground truth.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the knowledge graph-based question decomposition approach perform across different educational domains beyond physics?
- **Basis in paper**: The methodology has been tested specifically for high school-level physics questions, but further research is needed to investigate its generalizability.
- **Why unresolved**: The current study is limited to physics domain, and authors acknowledge need for further research to assess effectiveness in other educational contexts.
- **What evidence would resolve it**: Conducting experiments applying the approach to educational questions from various domains (mathematics, chemistry, biology) and comparing performance with traditional methods.

### Open Question 2
- **Question**: What is the long-term impact of using LLM-based question answering with knowledge graph decomposition on student learning outcomes?
- **Basis in paper**: The paper suggests the approach could enhance learning experience but does not investigate long-term effects on student learning outcomes.
- **Why unresolved**: Current study focuses on immediate effectiveness in generating relevant sub-questions and improving model performance, but does not assess impact on student learning over time.
- **What evidence would resolve it**: Conducting longitudinal studies tracking student performance and learning outcomes over extended period when using this approach versus traditional methods.

### Open Question 3
- **Question**: How can the knowledge graph construction techniques be further improved to enhance the performance and versatility of the approach?
- **Basis in paper**: Authors mention future work should focus on developing more advanced knowledge graph construction techniques and incorporating external knowledge sources.
- **Why unresolved**: While current study demonstrates effectiveness of using knowledge graphs generated by LLMs, there is room for improvement in techniques used to construct these graphs and integrate additional knowledge sources.
- **What evidence would resolve it**: Experimenting with different knowledge graph construction methods, such as incorporating domain-specific ontologies or leveraging external knowledge bases, and evaluating their impact on quality of generated sub-questions and overall model performance.

## Limitations
- Evaluation relies heavily on human evaluation via GPT-4, introducing potential subjectivity
- Small evaluation sample size (100 questions for human evaluation, 5 students for survey)
- Knowledge graph generation depends on Gemini Pro without exploring alternative LLMs
- Assumes knowledge graph construction will consistently capture essential logic of physics questions

## Confidence
- **High confidence**: Overall methodology and experimental design are sound with clear implementation steps
- **Medium confidence**: Reported performance improvements are promising but require independent verification due to human evaluation component
- **Low confidence**: Student survey findings are limited by small sample size and lack of demographic information

## Next Checks
1. **Independent replication with alternative evaluation methods**: Replicate experiments using automated metrics (exact match, F1 score) in addition to human evaluation to verify reported success rates.
2. **Robustness testing across LLM variations**: Test pipeline with different LLMs (GPT-4, Claude) for knowledge graph generation to assess dependency on specific models.
3. **Scaling analysis**: Evaluate approach's performance on wider range of question complexities, including edge cases with ambiguous or multi-concept questions, to identify potential failure modes.