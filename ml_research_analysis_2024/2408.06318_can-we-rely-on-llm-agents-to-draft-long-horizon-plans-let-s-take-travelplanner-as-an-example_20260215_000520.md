---
ver: rpa2
title: Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner
  as an Example
arxiv_id: '2408.06318'
source_url: https://arxiv.org/abs/2408.06318
tags:
- feedback
- arxiv
- plan
- information
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LLM-based agents struggle with long-horizon planning tasks, as
  shown by poor performance on the TravelPlanner benchmark. This paper investigates
  four key questions: (1) LLM agents fail to attend to crucial parts of lengthy, noisy
  contexts; (2) increasing few-shot examples can hurt performance due to hallucination;
  (3) refinement is ineffective when using LLM feedback but works with heuristic feedback;
  (4) feedback-aware fine-tuning (FAFT) outperforms standard fine-tuning by leveraging
  both positive and negative feedback.'
---

# Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example

## Quick Facts
- arXiv ID: 2408.06318
- Source URL: https://arxiv.org/abs/2408.06318
- Reference count: 21
- LLM-based agents struggle with long-horizon planning tasks, as shown by poor performance on the TravelPlanner benchmark.

## Executive Summary
This paper investigates whether LLM-based agents can effectively handle long-horizon planning tasks using the TravelPlanner benchmark. The authors find that LLMs struggle with lengthy, noisy contexts and that increasing few-shot examples can actually hurt performance due to hallucination. They propose a feedback-aware fine-tuning (FAFT) approach that outperforms standard fine-tuning by leveraging both positive and negative feedback. FAFT significantly improves planning accuracy, highlighting the value of rich feedback over traditional RL approaches in data-scarce scenarios.

## Method Summary
The paper introduces a framework for evaluating LLM agents on long-horizon planning tasks using the TravelPlanner benchmark. The approach involves a Scrubber agent to filter reference information, a Planner agent that can use either in-context learning or fine-tuning approaches (SFT and FAFT), a Feedback Generator to provide feedback on plan quality, and a Refiner to improve plans based on feedback. The FAFT method incorporates both positive and negative feedback during fine-tuning, using feedback as thought chains to guide the model toward better planning decisions.

## Key Results
- LLM agents fail to attend to crucial parts of lengthy, noisy contexts during planning tasks
- Increasing few-shot examples can hurt performance due to hallucination in long-context scenarios
- Feedback-aware fine-tuning (FAFT) outperforms standard fine-tuning by leveraging both positive and negative feedback

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM agents fail to attend to crucial parts of lengthy, noisy contexts during planning tasks.
- **Mechanism**: In long contexts, attention scores become diluted across many tokens, causing the model to lose focus on the most relevant information needed for accurate planning.
- **Core assumption**: The model's attention distribution becomes flatter as context length increases, reducing the model's ability to discriminate important from irrelevant information.
- **Evidence anchors**:
  - [abstract] "LLMs often fail to attend to crucial parts of a long context, despite their ability to handle extensive reference information"
  - [section] "when they encounter a much longer context, the attention scores are diluted, and thus the score distribution becomes flat leading to information loss"
- **Break condition**: If the model can maintain high attention scores for relevant tokens despite context length, or if context compression techniques effectively preserve important information while reducing noise.

### Mechanism 2
- **Claim**: Increasing few-shot examples can hurt performance due to hallucination in long-context scenarios.
- **Mechanism**: More few-shot examples increase the chance of introducing irrelevant or contradictory patterns, causing the model to hallucinate entities or actions not present in the reference information.
- **Core assumption**: The model's context window has limited capacity to distinguish between relevant few-shot patterns and noise, leading to confusion.
- **Evidence anchors**:
  - [abstract] "increasing few-shot examples can hurt performance due to hallucination"
  - [section] "more shots in the context window may distract the LLM and lead to hallucination (e.g., using entities that do not exist in the given reference information)"
- **Break condition**: If the model can effectively filter and prioritize relevant few-shot examples, or if the reference information is sufficiently clean and concise to prevent confusion.

### Mechanism 3
- **Claim**: Feedback-aware fine-tuning (FAFT) outperforms standard fine-tuning by leveraging both positive and negative feedback.
- **Mechanism**: FAFT incorporates detailed feedback (both positive and negative) during training, which acts as thought chains that guide the model toward better planning decisions and help it learn from mistakes.
- **Core assumption**: Rich, diverse feedback provides more informative training signals than simple plan annotations alone.
- **Evidence anchors**:
  - [abstract] "feedback-aware fine-tuning (FAFT) outperforms standard fine-tuning by leveraging both positive and negative feedback"
  - [section] "elaborative and rich feedback acts as thought chains to improve the agent's planning"
- **Break condition**: If the feedback quality is poor or inconsistent, or if the model cannot effectively process and integrate the feedback during training.

## Foundational Learning

- **Concept**: Attention mechanisms in transformers
  - **Why needed here**: Understanding how attention works is crucial for diagnosing why LLMs fail to focus on relevant information in long contexts
  - **Quick check question**: How does the attention score distribution change as context length increases, and what effect does this have on information retrieval?

- **Concept**: In-context learning vs. fine-tuning
  - **Why needed here**: The paper compares few-shot prompting (in-context learning) with fine-tuning approaches, requiring understanding of their respective strengths and limitations
  - **Quick check question**: What are the key differences between in-context learning and fine-tuning, and when might each approach be more appropriate?

- **Concept**: Reinforcement learning from human feedback (RLHF)
  - **Why needed here**: FAFT is presented as an alternative to RL-based solutions, so understanding RLHF concepts helps contextualize the approach
  - **Quick check question**: How does FAFT differ from traditional RLHF approaches, and what advantages might it offer in data-scarce scenarios?

## Architecture Onboarding

- **Component map**: Query → Scrubber → Planner → Feedback Generator → Refiner → Evaluation → Final Plan
- **Critical path**: Query → Scrubber → Planner → Feedback Generator → Refiner → Evaluation → Final Plan
  - The Planner is the core component that can be implemented using different approaches (ICL, SFT, or FAFT)

- **Design tradeoffs**:
  - Scrubber complexity vs. reference information quality: More aggressive filtering may remove useful information, while less filtering leaves noise
  - Feedback generator choice: LLM-based feedback is more flexible but less reliable than heuristic rules
  - Refinement iterations: More iterations may improve quality but increase computation and risk of plan degradation

- **Failure signatures**:
  - Poor delivery rate: Indicates the planner struggles to generate valid plans
  - Low pass rates: Suggests the planner fails to meet specific constraints
  - High hallucination rate: Points to the planner generating entities not in reference information
  - Refinement stagnation: Shows the feedback generator or refiner are not effectively improving plans

- **First 3 experiments**:
  1. Test different context cleaning approaches (Scrubber) to see impact on planner performance
  2. Vary the number of few-shot examples to find the optimal balance between guidance and hallucination
  3. Compare refinement effectiveness using heuristic vs. LLM-based feedback generators

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does FAFT's performance improvement depend on the ratio of positive to negative feedback samples in the training data?
- Basis in paper: [inferred] The paper mentions that FAFT uses both positive and negative feedback, but doesn't explore how different ratios affect performance.
- Why unresolved: The authors note this as a limitation due to budget constraints and suggest further investigation is needed.
- What evidence would resolve it: Systematic experiments varying the positive-to-negative feedback ratio while keeping other factors constant would reveal if there's an optimal balance for FAFT performance.

### Open Question 2
- Question: How does the effectiveness of FAFT compare to traditional RLHF methods like PPO or DPO when sufficient annotated data is available?
- Basis in paper: [explicit] The authors mention that FAFT could be a promising alternative to RL-based solutions like PPO, but don't directly compare performance.
- Why unresolved: The paper focuses on FAFT's advantages in data-scarce scenarios and doesn't conduct head-to-head comparisons with RLHF methods.
- What evidence would resolve it: Direct performance comparisons between FAFT and PPO/DPO across multiple benchmarks with varying amounts of annotated data would clarify when each method is preferable.

### Open Question 3
- Question: Can the context cleaning performed by the Scrubber agent be further improved to better handle complex, noisy reference information?
- Basis in paper: [explicit] The authors propose the Scrubber agent to address LLM difficulties with lengthy, noisy contexts, but note it achieves only ~60% reduction in reference information length.
- Why unresolved: The paper demonstrates the Scrubber's effectiveness but doesn't explore more sophisticated context cleaning approaches or measure the impact of different cleaning strategies on planning performance.
- What evidence would resolve it: Experiments testing alternative context cleaning methods (e.g., semantic filtering, attention-based selection) and measuring their impact on planner performance would identify optimal approaches for handling noisy reference information.

## Limitations
- The findings are limited to a single travel planning domain, raising questions about broader applicability
- The evaluation relies on qualitative observations of attention patterns rather than quantitative analysis
- The paper doesn't directly compare FAFT against RLHF methods, leaving uncertainty about relative performance

## Confidence
- **High confidence**: The observation that LLM agents struggle with long-horizon planning tasks and the general trend that FAFT outperforms standard fine-tuning
- **Medium confidence**: The specific mechanisms of attention dilution and hallucination effects, as these rely on qualitative rather than quantitative evidence
- **Low confidence**: The claim that feedback-aware fine-tuning is superior to RL-based approaches in data-scarce scenarios, as this comparison is not directly tested

## Next Checks
1. Conduct quantitative analysis of attention score distributions across different context lengths to verify the attention dilution hypothesis
2. Test FAFT across multiple domains beyond travel planning to assess generalizability
3. Compare FAFT directly against RLHF approaches using the same dataset and evaluation metrics