---
ver: rpa2
title: 'Why the Agent Made that Decision: Contrastive Explanation Learning for Reinforcement
  Learning'
arxiv_id: '2411.16120'
source_url: https://arxiv.org/abs/2411.16120
tags:
- action
- right
- agent
- visionmask
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VisionMask, a novel contrastive learning
  framework for generating interpretable saliency maps that explain reinforcement
  learning agent decisions. VisionMask addresses the lack of faithfulness in existing
  perturbation-based explanations by learning saliency maps through contrastive training
  that explicitly contrasts target actions with alternatives.
---

# Why the Agent Made that Decision: Contrastive Explanation Learning for Reinforcement Learning

## Quick Facts
- arXiv ID: 2411.16120
- Source URL: https://arxiv.org/abs/2411.16120
- Reference count: 40
- This paper introduces VisionMask, a novel contrastive learning framework for generating interpretable saliency maps that explain reinforcement learning agent decisions

## Executive Summary
This paper addresses the challenge of generating faithful explanations for reinforcement learning (RL) agent decisions through a novel contrastive learning framework called VisionMask. The method generates saliency maps that highlight which visual features influence an agent's action selection by explicitly contrasting the target action with alternative actions. Unlike perturbation-based approaches that suffer from faithfulness issues, VisionMask learns explanations through contrastive training that maximizes the probability of the target action while minimizing non-target action probabilities. The framework demonstrates superior performance across six diverse RL environments, achieving high faithfulness metrics while maintaining robustness and sparseness.

## Method Summary
VisionMask is a contrastive learning framework that generates interpretable saliency maps for explaining RL agent decisions. It consists of a segmentation model (DeepLabv3) that takes states as input and outputs K saliency maps (one per action). The model is trained in a self-supervised manner by masking states and observing how the agent's action probabilities change, using a contrastive loss that encourages masks to highlight features that increase target action probability while decreasing non-target action probabilities. VisionMask employs both action-wise and feature-wise contrastive learning, along with regularization terms for area size control. The framework generates action-specific masks and enables counterfactual analysis by removing important regions to reveal causal relationships between features and actions.

## Key Results
- VisionMask achieves superior faithfulness metrics: 95.9% accuracy, 67.5% insertion, and 20.4% deletion scores
- Human studies show 89% of participants found VisionMask explanations helpful for understanding agent decisions
- The method maintains robustness (LLE scores) and sparseness while providing interpretable explanations
- VisionMask demonstrates effectiveness across six diverse RL environments including Atari games and autonomous driving scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning between target and non-target actions creates more faithful saliency maps by explicitly modeling action preferences
- Mechanism: VisionMask trains to generate saliency maps that maximize the probability of the target action while minimizing probabilities of non-target actions through contrastive loss. This directly addresses the contrastive nature of human reasoning ("why this action instead of that one?").
- Core assumption: The difference in action probabilities between the original state and masked state contains sufficient signal to train meaningful explanations
- Evidence anchors:
  - [abstract] "VisionMask is trained to generate explanations by explicitly contrasting the agent's chosen action with alternative actions"
  - [section 3.3] "Our primary goal is to learn explanations faithful to the a, making (a, p_a) the only positive pair"
  - [corpus] Weak evidence - neighboring papers discuss explanation methods but don't validate contrastive approaches for RL
- Break condition: If the agent's action probabilities are too similar across actions (nearly uniform distribution), the contrastive signal becomes too weak to train effectively

### Mechanism 2
- Claim: Feature-wise contrastive learning improves sparseness by distinguishing relevant from irrelevant features
- Mechanism: VisionMask treats the original mask and its complement as a negative pair, encouraging the model to focus only on relevant features while treating background/irrelevant regions as uniformly unimportant
- Core assumption: Background and irrelevant features should not influence action selection, making their exclusion from explanations beneficial
- Evidence anchors:
  - [abstract] "Feature-wise contrast: To exclude irrelevant features (e.g. background) from the saliency map, explanations also need to be discriminative"
  - [section 3.3] "we define negative entropy loss regarding ˜m as the following: L_ne(s) = 1/K^2 * Σ_ij ˜p_ij log ˜p_ij"
  - [corpus] No direct evidence - this specific feature-wise contrastive approach appears novel
- Break condition: If the environment has complex feature interactions where seemingly irrelevant features actually contribute to action selection

### Mechanism 3
- Claim: Self-supervised contrastive training eliminates need for ground truth explanations while maintaining faithfulness
- Mechanism: VisionMask generates its own training signal by masking states and observing how the agent's action probabilities change, creating a self-contained learning loop
- Core assumption: Changes in action probabilities due to feature masking provide reliable supervision signal for learning explanations
- Evidence anchors:
  - [abstract] "VisionMask is trained to generate explanations by explicitly contrasting the agent's chosen action with alternative actions in a given state using a self-supervised manner"
  - [section 3.3] "we carefully designed the training loss function L(s,a, θ) = L_a(s,a) + λ_ne L_ne(s) + λ_area L_area(s,a)"
  - [corpus] Moderate evidence - self-supervised approaches are validated in neighboring papers but not specifically for RL explanations
- Break condition: If the agent's decision boundary is too complex or non-smooth, small perturbations may not provide reliable training signals

## Foundational Learning

- Concept: Contrastive learning fundamentals
  - Why needed here: VisionMask relies on contrastive loss functions to learn which features distinguish between target and non-target actions
  - Quick check question: What is the difference between positive and negative pairs in contrastive learning, and how does this apply to action explanations?

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The paper frames RL problems as MDPs and VisionMask operates on states, actions, and policies from this framework
  - Quick check question: In the MDP formulation, what do S, A, P, R, and γ represent, and how do they relate to VisionMask's input/output?

- Concept: Saliency map generation and evaluation
  - Why needed here: VisionMask produces saliency maps and the paper evaluates them using insertion/deletion metrics and other XAI evaluation methods
  - Quick check question: How do insertion and deletion metrics work, and why are they appropriate for evaluating explanation faithfulness?

## Architecture Onboarding

- Component map: State → VisionMask segmentation → K saliency maps → Overlay with state → Query expert policy → Compute contrastive loss → Backpropagate → Update VisionMask weights

- Critical path: State → VisionMask segmentation → K saliency maps → Overlay with state → Query expert policy → Compute contrastive loss → Backpropagate → Update VisionMask weights

- Design tradeoffs: 
  - Using a learned model vs perturbation-based methods: Better faithfulness but requires training time
  - Action-wise vs feature-wise contrastive learning: Balances between action-specific and general feature importance
  - Reference value selection: Background vs black vs blur affects counterfactual quality

- Failure signatures:
  - Poor accuracy/insertion/deletion scores indicate unfaithful explanations
  - High LLE (Local Lipschitz Estimate) indicates lack of robustness to input variations
  - Low sparseness indicates explanations include too many irrelevant features

- First 3 experiments:
  1. Train VisionMask on a simple environment (like CartPole) and verify it produces meaningful saliency maps for the two actions
  2. Compare insertion/deletion metrics against a baseline perturbation method on a medium-complexity environment
  3. Test counterfactual analysis by removing important regions and verifying action changes match expectations

## Open Questions the Paper Calls Out
None

## Limitations
- The self-supervised training methodology relies heavily on the assumption that action probability changes due to masking provide reliable supervision signals
- Evaluation primarily focuses on faithfulness metrics without extensive validation of real-world utility or robustness to distribution shifts
- Human studies involved only 40 participants across 6 environments, limiting generalizability of usefulness findings

## Confidence

- **High**: VisionMask improves faithfulness metrics compared to baselines (95.9% accuracy, 67.5% insertion, 20.4% deletion)
- **Medium**: Contrastive learning provides better explanations than perturbation methods; the approach generalizes across diverse environments
- **Medium**: Human studies validate the usefulness of VisionMask explanations for understanding agent decisions

## Next Checks

1. Test VisionMask on environments with continuous action spaces to verify generalization beyond discrete actions
2. Evaluate explanation stability when training the RL agent with different random seeds to assess robustness
3. Conduct larger-scale human studies with domain experts to validate practical utility beyond initial findings