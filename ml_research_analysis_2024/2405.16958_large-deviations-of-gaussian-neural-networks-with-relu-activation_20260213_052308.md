---
ver: rpa2
title: Large Deviations of Gaussian Neural Networks with ReLU activation
arxiv_id: '2405.16958'
source_url: https://arxiv.org/abs/2405.16958
tags:
- neural
- gaussian
- activation
- function
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper establishes a large deviation principle for deep neural\
  \ networks with Gaussian weights and activation functions that grow at most linearly,\
  \ including ReLU. The key innovation is handling linearly growing activations (unlike\
  \ prior work limited to bounded functions) using exponential equivalence and the\
  \ multidimensional Cram\xE9r theorem rather than the Gartner-Ellis approach."
---

# Large Deviations of Gaussian Neural Networks with ReLU activation

## Quick Facts
- arXiv ID: 2405.16958
- Source URL: https://arxiv.org/abs/2405.16958
- Authors: Quirin Vogel
- Reference count: 3
- Primary result: Establishes large deviation principle for deep Gaussian neural networks with ReLU activation using exponential equivalence and Cramér theorem

## Executive Summary
This paper establishes a large deviation principle (LDP) for deep neural networks with Gaussian weights and activation functions that grow at most linearly, including ReLU. The key innovation is handling linearly growing activations (unlike prior work limited to bounded functions) using exponential equivalence and the multidimensional Cramér theorem rather than the Gartner-Ellis approach. This is crucial for ReLU, the most commonly used activation function. The paper also simplifies the rate function expression from previous work and provides a power-series expansion for the ReLU case.

## Method Summary
The paper establishes LDP for Gaussian neural networks by conditioning on previous layers and showing the next layer can be approximated by a Gaussian process transformation. Instead of using the Gartner-Ellis theorem (which fails for linearly growing activations as the moment generating function becomes infinite except at the origin), the proof leverages exponential equivalence and the multidimensional Cramér theorem. The approach involves computing conditional minimizers of the rate function to simplify its expression and deriving a power-series expansion for the ReLU case that makes the rate function computationally tractable.

## Key Results
- Establishes LDP for deep Gaussian neural networks with linearly growing activation functions (including ReLU)
- Simplifies the rate function expression from previous work by reducing a two-matrix minimization to a single-matrix problem
- Provides a power-series expansion for the ReLU activation function's moment generating function, making the rate function computationally tractable
- Handles linearly growing activations by replacing the Gartner-Ellis approach with exponential equivalence and multidimensional Cramér theorem

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The large deviation principle holds for deep Gaussian neural networks with linearly growing activation functions by leveraging exponential equivalence between the full network and a simplified Gaussian process.
- Mechanism: The paper replaces the Gartner-Ellis approach with exponential equivalence and the multidimensional Cramér theorem. For linearly growing activations, the moment generating function becomes infinite except at the origin, making the Gartner-Ellis theorem inapplicable. Instead, the proof conditions on previous layers and shows that the next layer can be approximated by a Gaussian process transformation.
- Core assumption: The activation function grows at most linearly (c_+ ≥ 0 in Assumption 1) and satisfies measurability, almost everywhere continuity, boundedness on compact sets, and non-triviality.
- Evidence anchors:
  - [abstract] "This generalises earlier work, in which bounded and continuous activation functions were considered."
  - [section] "Instead of working with the Gartner–Ellis theorem, we instead apply exponential equivalence and the multidimensional Cramér theorem."
  - [corpus] Weak - the corpus papers focus on ReLU properties but don't directly address large deviation principles.
- Break condition: If the activation function grows faster than linearly, the LDP would no longer be in the exponential class as the moment generating function becomes infinite everywhere except at the origin.

### Mechanism 2
- Claim: The rate function for the LDP can be simplified from a two-matrix minimization to a single-matrix problem with explicit formulas for the ReLU case.
- Mechanism: The paper reduces the complexity of the rate function from [MPT24] by computing conditional minimizers of the second matrix, resulting in the simplified expression in Equation (1.9). For ReLU specifically, a power-series expansion is provided that makes the rate function computationally tractable.
- Core assumption: The Gaussian weights and biases satisfy Assumptions 2 and 3, and the network architecture follows the specified recursive structure.
- Evidence anchors:
  - [abstract] "We furthermore simplify previous expressions for the rate function and provide a power-series expansions for the ReLU case."
  - [section] "We were able to compute the conditional minimizers of the second matrix and hence reduce the complexity of the minimization problem."
  - [corpus] Weak - corpus papers discuss ReLU properties but not rate function simplification or power series expansions.
- Break condition: If the network layers don't jointly tend to infinity as specified in Assumption 3, or if the Gaussian assumptions on weights and biases are violated.

### Mechanism 3
- Claim: The continuity of the map q → q^# (matrix square root) ensures the necessary conditions for the LDP proof to hold.
- Mechanism: The paper establishes that the map taking a positive definite matrix to its square root is continuous (Lemma 2.1), which is crucial for proving the exponential equivalence and the LDP continuity condition required by Theorem 2.
- Core assumption: The activation function satisfies the linear growth condition and the Gaussian process properties hold under the given assumptions.
- Evidence anchors:
  - [section] "Regarding the second claim, this follows from the singular value decomposition... The continuity of the map q → q^# then follows from the continuity of the square-root map on [0,∞)."
  - [section] "Next we prove the second statement... Note that for any ε > 0 there exists n₀ ≥ 1 large enough such that for all N ∈ R^d, α ∈ A and n ≥ n₀..."
  - [corpus] Weak - corpus papers don't discuss matrix square root continuity or its role in LDP proofs.
- Break condition: If the activation function has discontinuities that prevent the necessary continuity arguments, or if the positive definiteness conditions on matrices are violated.

## Foundational Learning

- Concept: Large Deviation Principle (LDP)
  - Why needed here: The paper's main result is establishing an LDP for Gaussian neural networks, which quantifies atypical behavior of network outputs.
  - Quick check question: What distinguishes a large deviation principle from standard convergence results in probability theory?

- Concept: Moment Generating Function and Gärtner-Ellis Theorem
  - Why needed here: Understanding why the Gärtner-Ellis theorem fails for linearly growing activations (finite moment generating function only at origin) and why the Cramér theorem is used instead.
  - Quick check question: Under what conditions does the Gärtner-Ellis theorem require the moment generating function to be finite in a neighborhood of the origin?

- Concept: Exponential Equivalence
  - Why needed here: The proof technique relies on showing that two probability measures are exponentially equivalent, allowing results to transfer between them.
  - Quick check question: How does exponential equivalence differ from other forms of probabilistic approximation?

## Architecture Onboarding

- Component map: Input layer (R^n0) -> Hidden layers (L layers with sizes n₁, n₂, ..., nL) -> Output layer (R^nL+1)
- Critical path: The proof structure follows an inductive approach - establishing the base case L=1, then extending to L>1 using the LDP continuity condition from [Cha97].
- Design tradeoffs: Using linearly growing activations like ReLU enables the LDP but requires more sophisticated proof techniques compared to bounded activations. The trade-off is between practical utility (ReLU is commonly used) and theoretical tractability.
- Failure signatures: If the activation function grows faster than linearly, the moment generating function becomes infinite everywhere except at the origin, breaking the LDP. If the Gaussian assumptions on weights/biases are violated, the conditional Gaussian process structure is lost.
- First 3 experiments:
  1. Verify the base case L=1 by computing the moment generating function for a single-layer network with ReLU activation and checking the Cramér theorem conditions.
  2. Test the exponential equivalence numerically by comparing the distribution of G^(L)_n(x) conditioned on different previous layer values.
  3. Implement the simplified rate function expression (1.9) and compare its computational efficiency against the original formulation from [MPT24].

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the large deviation principle extend to infinite-dimensional cases where the number of training examples grows without bound?
- Basis in paper: [explicit] The paper mentions that "A functional LDP is work in progress" and notes that the Dawson-Gartner theorem could extend results to product topology for infinitely many training examples.
- Why unresolved: The authors explicitly state this as future work and only establish the LDP for finite sets of training points.
- What evidence would resolve it: A rigorous proof extending the LDP to infinite-dimensional spaces or functional LDP, potentially using the Dawson-Gartner theorem framework mentioned.

### Open Question 2
- Question: Can the power series expansion for the ReLU activation function's moment generating function (Equation 3.5) be simplified or expressed in closed form?
- Basis in paper: [explicit] The authors provide a power series expansion for κ(η;q) in the ReLU case but leave open whether a simpler closed-form expression exists.
- Why unresolved: The authors present the expansion as computationally useful but note it may be complex for practical use in high dimensions.
- What evidence would resolve it: A derivation of a closed-form expression for κ(η;q) in the ReLU case, or a proof that no simpler form exists.

### Open Question 3
- Question: How do the large deviation results change for activation functions that grow faster than linearly?
- Basis in paper: [explicit] The authors state that "for faster growing functions, the LDP is no longer in the exponential class, as the moment generating function is infinite everywhere, except at the origin."
- Why unresolved: The authors explicitly identify this as a critical threshold but do not explore the non-exponential regime or characterize what happens beyond linear growth.
- What evidence would resolve it: Analysis of the LDP behavior for super-linear activation functions, including characterization of the rate function and scaling behavior in the non-exponential regime.

## Limitations
- The LDP proof is currently limited to Gaussian weights and biases with specific scaling assumptions (C_W/n_l and C_b = 1/2). Non-Gaussian or non-standard normal distributions require separate verification.
- The rate function simplification assumes joint convergence of all layer sizes to infinity, with specific growth rate conditions that are not fully characterized.
- The power series expansion for ReLU is computationally promising but its convergence properties for practical network sizes remain to be empirically validated.

## Confidence
- **High Confidence**: The theoretical framework using exponential equivalence and multidimensional Cramér theorem is sound, given the established continuity of the matrix square root map and the non-triviality of the moment generating function.
- **Medium Confidence**: The simplified rate function expression and its computational tractability through power series expansion for ReLU is theoretically derived but requires numerical validation.
- **Low Confidence**: The practical implications of the LDP for understanding network behavior in finite-width regimes are not yet clear from the theoretical results alone.

## Next Checks
1. **Numerical Verification of Rate Function**: Implement the simplified rate function (1.9) and its power series expansion for ReLU, then compare computational efficiency and accuracy against the original formulation from [MPT24] for various network architectures.
2. **Empirical LDP Testing**: Generate synthetic data from Gaussian neural networks with different activation functions (ReLU, sigmoid, etc.) and empirically verify the large deviation principle by measuring the probability of rare events as layer sizes scale.
3. **Non-Gaussian Extension**: Test the LDP framework with non-Gaussian weight distributions (e.g., uniform, Bernoulli) to assess the robustness of the results beyond the Gaussian assumption and identify which conditions are essential versus technical.