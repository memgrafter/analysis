---
ver: rpa2
title: Focused Large Language Models are Stable Many-Shot Learners
arxiv_id: '2408.13987'
source_url: https://arxiv.org/abs/2408.13987
tags:
- attention
- demonstrations
- number
- demonstration
- focus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that increasing the number of demonstrations
  in In-Context Learning can scatter model attention away from the query, leading
  to performance degradation. To address this, it proposes FOCUS ICL, which applies
  triviality filtering to mask unimportant tokens and uses hierarchical attention
  by batching demonstrations to maintain focus on the query.
---

# Focused Large Language Models are Stable Many-Shot Learners

## Quick Facts
- arXiv ID: 2408.13987
- Source URL: https://arxiv.org/abs/2408.13987
- Authors: Peiwen Yuan; Shaoxiong Feng; Yiwei Li; Xinglin Wang; Yueqi Zhang; Chuyi Tan; Boyuan Pan; Heda Wang; Yao Hu; Kan Li
- Reference count: 40
- Primary result: FOCUS ICL improves average accuracy by 5.2% over vanilla ICL across three LLMs and five benchmarks

## Executive Summary
This paper identifies that increasing the number of demonstrations in In-Context Learning can scatter model attention away from the query, leading to performance degradation. To address this, it proposes FOCUS ICL, which applies triviality filtering to mask unimportant tokens and uses hierarchical attention by batching demonstrations to maintain focus on the query. The method also includes an efficient hyperparameter search strategy based on model perplexity. Experiments show FOCUS ICL improves average accuracy by 5.2% over vanilla ICL across three LLMs and five benchmarks, while also scaling well with more demonstrations and reducing inference overhead.

## Method Summary
FOCUS ICL addresses the attention dispersion problem in many-shot ICL by implementing two key mechanisms: triviality filtering and hierarchical attention. The triviality filtering operation masks unimportant tokens in demonstrations based on their attention scores, ensuring that only relevant content receives attention. The hierarchical attention mechanism divides demonstrations into batches, performing intra-batch attention within each batch and inter-batch attention to integrate insights across batches. The method includes a hyperparameter search strategy that finds optimal values for the filtering threshold and batch size based on model perplexity of the demonstrations.

## Key Results
- FOCUS ICL achieves 5.2% average accuracy improvement over vanilla ICL across five benchmarks
- The method scales better with increasing demonstration numbers compared to vanilla ICL
- FOCUS ICL reduces inference overhead while maintaining or improving performance
- The triviality filtering operation effectively removes unimportant tokens without losing critical information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing demonstrations in ICL scatters model attention from the query, degrading performance
- Mechanism: Standard attention involves competition between query and demonstrations. More demonstrations decrease attention weight allocated to the query (λ(hr)), resulting in insufficient attention to the query
- Core assumption: Linear approximation of attention ignores competition between demonstrations and query
- Evidence: [abstract] "more demonstrations dispersing the model attention from the query"; [section 3.2] "increase in demonstrations will lead to larger λ(hr), decreasing model attention towards q"
- Break condition: If model uses attention mechanism without competition between query and demonstrations

### Mechanism 2
- Claim: Triviality filtering masks unimportant tokens based on attention distribution
- Mechanism: Calculates attention scores for tokens, masks those below threshold, allocating more attention to important contents
- Core assumption: Low attention score tokens are unimportant and can be masked without losing task-relevant information
- Evidence: [abstract] "conducts triviality filtering to avoid attention being diverted by unimportant contents"; [section 4.1] "filter the trivialities in demonstrations according to a pre-set threshold"
- Break condition: If threshold is too high, masking important tokens, or if attention distribution doesn't reflect token importance

### Mechanism 3
- Claim: Hierarchical attention divides demonstrations into batches with intra-batch and inter-batch operations
- Mechanism: Demonstrations divided into batches, intra-batch attention ensures sufficient query attention, inter-batch attention integrates results from different batches
- Core assumption: Limited demonstrations per batch ensure sufficient query attention while inter-batch attention integrates insights
- Evidence: [abstract] "operates hierarchical attention to further ensure sufficient attention towards current query"; [section 4.2] "controlling batch size B, we can ensure that the model maintains enough attention towards the query within each batch"
- Break condition: If batch size too small, excluding important demonstrations, or if inter-batch attention doesn't effectively integrate insights

## Foundational Learning

- **In-Context Learning (ICL)**: Fundamental learning paradigm FOCUS ICL aims to improve. Understanding ICL is crucial to grasp the motivation and mechanism.
  - Quick check: What is the main advantage of ICL compared to traditional finetuning?

- **Attention mechanism in transformers**: Central to how FOCUS ICL operates. Understanding attention in transformers is essential to understand the attention dispersion problem and solutions.
  - Quick check: How does the standard attention mechanism in transformers calculate attention weights?

- **Scaling laws in machine learning**: The paper discusses how ICL performance doesn't necessarily scale well with demonstration numbers. Understanding scaling laws helps appreciate this observation.
  - Quick check: What are scaling laws in machine learning, and how do they typically affect model performance?

## Architecture Onboarding

- **Component map**: Input query and demonstrations -> Calculate attention scores -> Triviality filtering -> Divide demonstrations into batches -> Intra-batch attention -> Inter-batch attention -> Generate response

- **Critical path**: 1) Input query and demonstrations, 2) Calculate attention scores for tokens, 3) Apply triviality filtering, 4) Divide demonstrations into batches, 5) Perform intra-batch attention, 6) Perform inter-batch attention, 7) Generate response

- **Design tradeoffs**: Filtering threshold (higher removes more unimportant tokens but may remove important information), Batch size (smaller ensures more attention to query but may exclude important demonstrations), Computational overhead (FOCUS ICL introduces additional computation)

- **Failure signatures**: Decreased performance (filtering threshold too high or batch size too small), Increased computational overhead (number of batches or filtering threshold needs adjustment), Unstable results (hyperparameter searching strategy needs improvement)

- **First 3 experiments**: 1) Test effect of different filtering thresholds on simple task to find optimal value, 2) Test effect of different batch sizes on task with varying demonstration numbers, 3) Compare FOCUS ICL with vanilla ICL on standard benchmark to validate improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does FOCUS ICL maintain effectiveness with models having context windows significantly larger than 32K tokens (e.g., 1M+ tokens)?
- Basis: Experiments primarily conducted on models with context windows up to 32K tokens, acknowledging need to verify applicability with larger demonstration numbers
- Why unresolved: No experiments with models having extremely large context windows
- Resolution: Experiments demonstrating FOCUS ICL's performance on models with context windows of 1M tokens or more

### Open Question 2
- Question: How does FOCUS ICL perform on encoder-decoder architectures or models with sliding window attention mechanisms?
- Basis: Paper primarily discusses standard transformer decoder architecture, mentions need to explore applicability on other LLM variants
- Why unresolved: No experiments with encoder-decoder architectures or sliding window attention models
- Resolution: Experiments demonstrating FOCUS ICL's performance on encoder-decoder models and models with sliding window attention

### Open Question 3
- Question: What is the interplay between FOCUS ICL and other methods like Self-Consistency and Chain-of-Thought in terms of performance gains?
- Basis: FOCUS ICL can be seen as method achieving performance gains through increased computation, notes need to explore interplay with other methods
- Why unresolved: No experiments combining FOCUS ICL with Self-Consistency and Chain-of-Thought
- Resolution: Experiments demonstrating combined performance of FOCUS ICL with Self-Consistency and Chain-of-Thought

### Open Question 4
- Question: How does FOCUS ICL perform in evaluation and dialogue tasks beyond the QA and inference tasks tested?
- Basis: Paper mentions need to explore application of FOCUS ICL in evaluation and dialogue tasks
- Why unresolved: No experiments on evaluation and dialogue tasks
- Resolution: Experiments demonstrating FOCUS ICL's performance on evaluation and dialogue tasks

## Limitations

- Theoretical analysis relies on approximations of attention operations that may not fully capture complex relationships in large language models
- Triviality filtering mechanism depends heavily on assumption that attention scores directly correlate with token importance
- Hyperparameter search strategy based on model perplexity may not always find globally optimal values across diverse tasks

## Confidence

**High Confidence Claims:**
- FOCUS ICL consistently improves accuracy over vanilla ICL across multiple benchmarks and models
- Attention dispersion problem in many-shot ICL is empirically observable and measurable
- FOCUS ICL scales better with increasing demonstration numbers compared to vanilla ICL

**Medium Confidence Claims:**
- Theoretical mechanism explaining attention dispersion through competitive attention allocation
- Triviality filtering effectively removes unimportant tokens without losing critical information
- Hierarchical attention mechanism successfully maintains focus on the query while integrating demonstration insights

**Low Confidence Claims:**
- Specific numerical improvements would generalize to all LLMs and tasks
- Perplexity-based hyperparameter search always finds optimal values
- Computational overhead reduction claims hold across all hardware configurations

## Next Checks

1. **Attention Distribution Validation**: Implement gradient-based attribution methods (e.g., Integrated Gradients) to verify that FOCUS ICL's triviality filtering actually removes genuinely unimportant tokens and that the hierarchical attention mechanism maintains query focus as claimed.

2. **Generalization Test**: Evaluate FOCUS ICL on a broader range of model sizes (beyond the 7-8B range tested) and task types (particularly long-form generation tasks) to assess whether the attention dispersion problem and proposed solution generalize beyond the current experimental scope.

3. **Ablation Study**: Conduct a systematic ablation study removing either the triviality filtering or hierarchical attention components to quantify their individual contributions to performance improvements, and test whether the combination provides synergistic benefits beyond their sum.