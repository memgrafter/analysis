---
ver: rpa2
title: Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel
  Collaboration
arxiv_id: '2404.12715'
source_url: https://arxiv.org/abs/2404.12715
tags:
- ensemble
- relative
- representation
- llms
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of combining the strengths of different
  large language models (LLMs) by creating an ensemble method. Existing methods focus
  on training additional models for selection or fusion, which can be challenging
  and ignore the rich information in internal model representations.
---

# Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration

## Quick Facts
- arXiv ID: 2404.12715
- Source URL: https://arxiv.org/abs/2404.12715
- Reference count: 40
- One-line primary result: DeePEn achieves consistent improvements across all tasks, even when ensembling models with different architectures or combining a specialist model with an LLM

## Executive Summary
This paper tackles the problem of combining the strengths of different large language models (LLMs) by creating an ensemble method. Existing methods focus on training additional models for selection or fusion, which can be challenging and ignore the rich information in internal model representations. To address this, the authors propose DeePEn, a training-free ensemble framework that fuses the probability distributions of different LLMs at each decoding step. The key challenge is the vocabulary discrepancy between LLMs, which makes direct averaging of distributions unfeasible. DeePEn overcomes this by mapping the probability distributions to a universal relative space based on relative representation theory, aggregating them, and then transforming the result back to the probability space of the main model. Extensive experiments on six benchmarks demonstrate that DeePEn achieves consistent improvements across all tasks, even when ensembling models with different architectures or combining a specialist model with an LLM. Furthermore, DeePEn complements other ensemble methods like voting.

## Method Summary
DeePEn is a training-free ensemble framework that fuses probability distributions of heterogeneous LLMs at each decoding step. The core innovation is mapping probability distributions from different models to a universal relative space using relative representation theory, which enables cross-model alignment despite vocabulary discrepancies. The method constructs relative representation matrices using anchor tokens, aggregates distributions in this relative space, and then performs a search-based inverse transformation to map back to the main model's probability space. This approach is evaluated on six benchmarks (MMLU, ARC-C, GSM8K, PIQA, TriviaQA, NQ) using six LLMs with different architectures and vocabularies.

## Key Results
- DeePEn achieves consistent improvements across all six benchmarks compared to single models
- The method works effectively even when ensembling models with different architectures (e.g., LLaMA-2-13B with Mistral-7B)
- DeePEn complements other ensemble methods like voting, showing additional performance gains when combined
- Performance peaks at top-4 or top-5 models in ensemble configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relative representation theory enables cross-model alignment of probability distributions
- Mechanism: By representing each token using embedding similarities to a set of anchor tokens, the relative representation captures token semantics in a way that is invariant across different LLMs, even when they have different vocabularies
- Core assumption: The relative representation of a token remains stable across models even when the absolute embedding space differs
- Evidence anchors:
  - [abstract] "To address this challenge, DEEPEN maps the probability distribution of each model from its own probability space to a universal relative space based on the relative representation theory"
  - [section 2.1] "relative representations possess cross-model invariance, i.e., the relative representation of the same sample keeps invariant across different models"
  - [corpus] Weak - corpus neighbors don't discuss relative representation theory directly
- Break condition: If anchor tokens are poorly chosen or too few, the relative representation may not capture sufficient semantic information, leading to poor alignment

### Mechanism 2
- Claim: Normalization of relative representations prevents outlier token degeneration
- Mechanism: Applying softmax to the relative representation matrix ensures that even tokens with zero vectors in relative space get probabilistic distributions, preventing them from being ignored during aggregation
- Core assumption: Outlier tokens (those far from other tokens) can become zero vectors in relative space without normalization
- Evidence anchors:
  - [section 3.2] "we perform normalization on the relative representation of all tokens by a softmax operation so that it becomes a probability distribution"
  - [section 5.2] "The result is shown in Tab. 4, the ensemble struggles to achieve improvements due to the ineffective representation of outlier words"
  - [corpus] Weak - corpus neighbors don't discuss outlier token handling
- Break condition: If normalization is too aggressive, it may distort the relative relationships between tokens, affecting the quality of aggregation

### Mechanism 3
- Claim: Search-based inverse transformation enables mapping from relative space back to absolute probability space
- Mechanism: Using iterative gradient descent to find the absolute representation whose relative representation matches the aggregated relative representation, guided by KL-divergence loss
- Core assumption: The aggregated relative representation can be mapped back to a valid probability distribution in the main model's vocabulary
- Evidence anchors:
  - [section 3.4] "This search is iteratively conducted under the guidance of the gradient of the loss in Eq.6 with respect to the absolute representation pi"
  - [section 5.3] "However, the loss is hard to reach zero, i.e., under-fitting" indicating the search process has limitations
  - [corpus] Weak - corpus neighbors don't discuss inverse transformation methods
- Break condition: If the search space is too large or the loss landscape is too complex, the iterative search may converge to suboptimal solutions or fail to converge

## Foundational Learning

- Concept: Relative representation theory
  - Why needed here: It provides the theoretical foundation for aligning probability distributions across models with different vocabularies
  - Quick check question: What is the key property of relative representation that makes it suitable for cross-model alignment?

- Concept: Embedding similarity and cosine distance
  - Why needed here: The relative representation is constructed using cosine similarities between token embeddings and anchor tokens
  - Quick check question: How is the relative representation of a token mathematically defined in terms of embedding similarities?

- Concept: KL-divergence and optimization
  - Why needed here: KL-divergence is used as the loss function for the search-based inverse transformation, and optimization techniques are needed to find the best absolute representation
  - Quick check question: Why is KL-divergence a suitable choice for measuring the distance between relative representations?

## Architecture Onboarding

- Component map: Input (N heterogeneous LLMs) -> Preprocessing (Construct relative representation matrices) -> Core (Transform to relative space, aggregate, inverse transform) -> Output (Next token from main model)

- Critical path:
  1. Construct relative representation matrices (preprocessing)
  2. At each decoding step:
     a. Get probability distributions from all models
     b. Transform to relative space
     c. Aggregate with collaboration weights
     d. Perform search-based inverse transformation
     e. Select next token from main model

- Design tradeoffs:
  - Anchor token selection: Using all common tokens provides better performance but increases computational cost
  - Search steps: More steps improve accuracy but increase latency
  - Collaboration weights: Uniform averaging is simple but adaptive weights could leverage complementary strengths better

- Failure signatures:
  - Poor performance across all benchmarks: Likely issues with anchor token selection or relative representation construction
  - Inconsistent performance across tasks: May indicate issues with the inverse transformation or search process
  - High latency: Could be due to excessive search steps or inefficient implementation

- First 3 experiments:
  1. Verify relative representation consistency: Compare relative representations of common tokens across models with same and different vocabularies
  2. Test normalization impact: Run ensemble with and without normalization on relative representation matrix
  3. Validate search-based inverse transformation: Check if the aggregated relative representation can be accurately mapped back to a valid probability distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DEEPEN scale with the number of ensemble models beyond 9?
- Basis in paper: [explicit] The authors conducted experiments ranging from 2-model to 9-model ensembles and observed that performance peaks at top-4 or top-5 models. They also mention the potential for future research on ensemble learning of a larger number of models.
- Why unresolved: The paper does not explore the performance of DEEPEN when ensembling more than 9 models, leaving the scalability of the approach an open question.
- What evidence would resolve it: Experiments demonstrating the performance of DEEPEN on ensembles of 10 or more models, and analysis of the trade-off between the number of models and the ensemble performance.

### Open Question 2
- Question: How does the choice of anchor tokens affect the performance of DEEPEN?
- Basis in paper: [explicit] The authors mention that the choice of anchor tokens is crucial for the relative representation capability and that selecting the full set of common words as anchors provides better performance. However, they do not explore the impact of different anchor selection strategies.
- Why unresolved: The paper does not investigate the effect of different anchor token selection methods on the performance of DEEPEN, leaving the optimal anchor selection strategy an open question.
- What evidence would resolve it: Experiments comparing the performance of DEEPEN with different anchor token selection strategies, such as random sampling, clustering-based selection, or domain-specific anchor selection.

### Open Question 3
- Question: How can the search-based inverse transformation in DEEPEN be made more efficient?
- Basis in paper: [explicit] The authors mention that the search-based inverse transformation process incurs extra latency and that the optimal value of the relative ensemble learning rate varies across different datasets. They also note that the loss is hard to reach zero, indicating underfitting.
- Why unresolved: The paper does not explore methods to improve the efficiency of the search-based inverse transformation or to address the underfitting issue, leaving the optimization of this process an open question.
- What evidence would resolve it: Experiments comparing the performance and efficiency of DEEPEN with different optimization techniques for the inverse transformation, such as gradient-based methods, adaptive learning rates, or early stopping criteria.

## Limitations

- The performance gains, while consistent, are sometimes modest (e.g., 1.2% improvement on MMLU), raising questions about whether the complexity of the approach is justified for all use cases
- The theoretical foundation of relative representation theory lacks rigorous mathematical proof, with experimental validation limited to empirical observations
- The computational overhead of the search-based inverse transformation is not thoroughly analyzed, which is crucial for practical deployment

## Confidence

**High Confidence:** The core mechanism of transforming probability distributions to a relative space and back shows consistent empirical improvements across all six benchmarks. The claim that DeePEn works as a training-free ensemble method is well-supported by experimental results.

**Medium Confidence:** The assertion that relative representation theory provides cross-model invariance is supported by empirical results but lacks rigorous mathematical proof. The claim that DeePEn complements existing ensemble methods like voting is demonstrated but could benefit from more extensive ablation studies.

**Low Confidence:** The paper's claim about handling models with completely different vocabularies (beyond just different vocabularies but same underlying language) is not thoroughly tested. The scalability claims regarding the number of LLMs in the ensemble are based on limited experimental evidence.

## Next Checks

1. **Cross-Architecture Invariance Test:** Conduct experiments with LLMs from different architecture families (e.g., transformer-based vs. non-transformer) to validate whether relative representation theory truly provides the claimed cross-model invariance, and measure the degradation in performance when mixing fundamentally different architectures.

2. **Search Process Analysis:** Perform a detailed ablation study varying the number of search steps and learning rate for the inverse transformation process, measuring both performance gains and computational overhead to determine the optimal tradeoff point and identify when the search process becomes a bottleneck.

3. **Vocabulary Gap Stress Test:** Systematically increase the vocabulary discrepancy between LLMs (e.g., by using models trained on different languages or specialized domains) to determine the breaking point where DeePEn's performance degrades significantly, helping establish the practical limits of the approach.