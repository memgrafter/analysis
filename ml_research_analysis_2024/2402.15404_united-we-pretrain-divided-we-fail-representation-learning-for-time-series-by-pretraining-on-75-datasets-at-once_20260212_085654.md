---
ver: rpa2
title: United We Pretrain, Divided We Fail! Representation Learning for Time Series
  by Pretraining on 75 Datasets at Once
arxiv_id: '2402.15404'
source_url: https://arxiv.org/abs/2402.15404
tags:
- time
- series
- datasets
- pretraining
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the prevailing belief that multi-dataset
  pretraining does not work for time series data. The authors propose a new self-supervised
  contrastive pretraining approach, called XIT, that learns a single encoding from
  many unlabeled and diverse time series datasets.
---

# United We Pretrain, Divided We Fail! Representation Learning for Time Series by Pretraining on 75 Datasets at Once

## Quick Facts
- arXiv ID: 2402.15404
- Source URL: https://arxiv.org/abs/2402.15404
- Authors: Maurice Kraus; Felix Divo; David Steinmann; Devendra Singh Dhami; Kristian Kersting
- Reference count: 14
- One-line primary result: XIT outperforms supervised training and other self-supervised pretraining methods when finetuning on low-data regimes

## Executive Summary
This paper challenges the prevailing belief that multi-dataset pretraining does not work for time series data. The authors propose a new self-supervised contrastive pretraining approach, called XIT, that learns a single encoding from many unlabeled and diverse time series datasets. XIT introduces two key components: XD-MixUp, an interpolation method that induces a shared latent representation for multiple datasets, and SICC (Soft Interpolation Contextual Contrasting), a loss function that aligns information in augmented time series context vectors. Empirical results demonstrate that XIT outperforms both supervised training and other self-supervised pretraining methods when finetuning on low-data regimes.

## Method Summary
XIT uses a contrastive learning framework with a 3-layer residual CNN encoder that processes time series data. The method introduces XD-MixUp for interpolating between time series from different datasets using Beta(α,α) sampling, and SICC for aligning context vectors of interpolated samples. During pretraining, time series from multiple datasets are encoded, summarized into context vectors using a transformer, and projected through an MLP. The model optimizes a combination of TC (temporal contrasting) and SICC losses that encourage both augmentation invariance and cross-dataset alignment. For evaluation, a linear classifier is trained on the frozen encoder representations.

## Key Results
- XIT outperforms supervised training and other self-supervised pretraining methods when finetuning on low-data regimes
- The method is evaluated on 75 time series datasets from the UCR archive, showing improved transfer classification performance on multiple small labeled target datasets
- This work disproves the common belief that multi-dataset pretraining is ineffective for time series

## Why This Works (Mechanism)

### Mechanism 1
Multi-dataset pretraining can learn general time series representations that transfer to unseen datasets. XIT uses XD-MixUp to interpolate between time series from different datasets, creating synthetic samples that bridge the distribution gaps between datasets. This induces a shared latent representation space where features from multiple datasets align. The SICC loss then reinforces these alignments by treating interpolated samples as soft positives/negatives based on the interpolation coefficient λ.

### Mechanism 2
Contrastive pretraining on interpolated samples improves robustness to augmentations and domain shifts. The TC loss maximizes similarity between differently augmented views of the same time series while minimizing similarity to others. Combined with XD-MixUp, this forces the encoder to learn augmentation-invariant features that generalize across domains. The SICC loss extends this by aligning context vectors of interpolated samples with their source series based on λ.

### Mechanism 3
Soft interpolation contextual contrasting (SICC) enables learning from multiple datasets by creating soft positive/negative pairs. SICC treats interpolated time series as partial positives to their source series (weighted by 1-λ and λ) while still treating other series as hard negatives. This creates a smoother optimization landscape that can align representations across dataset boundaries without forcing strict cluster separation.

## Foundational Learning

- Concept: **Contrastive learning and InfoNCE loss**
  - Why needed here: The paper builds on TS-TCC's contrastive framework and extends it with SICC. Understanding how InfoNCE works is crucial to grasp why TC and SICC losses are effective.
  - Quick check question: What is the key difference between the TC loss and the standard InfoNCE loss used in vision/NLP contrastive learning?

- Concept: **MixUp data augmentation**
  - Why needed here: XD-MixUp is inspired by MixUp and is central to the method's ability to bridge dataset distributions. Understanding MixUp's mechanism and benefits is important.
  - Quick check question: How does MixUp improve model robustness compared to traditional data augmentation techniques?

- Concept: **Transfer learning and negative transfer**
  - Why needed here: The paper's core contribution is showing that multi-dataset pretraining can avoid negative transfer. Understanding when transfer learning helps vs. hurts is key to appreciating the results.
  - Quick check question: Under what conditions does multi-dataset pretraining lead to negative transfer instead of positive transfer?

## Architecture Onboarding

- Component map: Time series data -> XD-MixUp interpolation -> Augmentation (weak/strong) -> Encoder Fθ -> Transformer S -> Projection head h -> TC + SICC losses -> Encoder update

- Critical path: 1) Sample mini-batch from multiple datasets 2) Apply XD-MixUp to create interpolated samples 3) Apply weak/strong augmentations to original and interpolated samples 4) Encode all samples through Fθ 5) Summarize embeddings to get context vectors 6) Project contexts through h 7) Compute TC and SICC losses 8) Backpropagate combined loss to update Fθ, S, and h

- Design tradeoffs: Single encoder vs. dataset-specific heads (simpler, more general, but may sacrifice dataset-specific feature extraction); Beta(α,α) interpolation vs. other mixing strategies (simple, differentiable, but may not capture complex dataset relationships); Soft vs. hard contrastive pairs (smoother optimization but potentially noisier gradients)

- Failure signatures: Poor transfer performance (shared latent space assumption may be invalid); Unstable training with high variance (λ sampling too extreme or batch size too small); Overfitting to pretraining datasets (insufficient regularization or too many pretraining epochs)

- First 3 experiments: 1) Implement basic TS-TCC on single dataset, verify it reproduces reported results 2) Add XD-MixUp to TS-TCC, observe if it improves single-dataset performance 3) Extend to multiple datasets, measure transfer performance vs. supervised baseline

## Open Questions the Paper Calls Out
- How would the XIT method perform on time series forecasting tasks rather than classification?
- Would using specialized time series interpolation methods like Dynamic Time Warping (DTW) instead of simple linear interpolation improve XIT's performance?
- How does XIT's performance scale with the number of pretraining datasets beyond 75?
- How does the learned representation's effectiveness vary across different domain types in the UCR repository?

## Limitations
- All experiments use UCR archive datasets with similar characteristics (univariate, fixed length ≤600, classification tasks), limiting generalizability to more diverse time series domains
- Computational requirements for pretraining on 75 datasets simultaneously may limit practical applicability, with no clear estimates of resource requirements provided
- The exact contribution of each component isn't fully isolated since the combined loss function (TC + SICC) makes it difficult to determine which mechanism drives improvements

## Confidence
- **High confidence**: The empirical demonstration that multi-dataset pretraining can work for time series classification on UCR datasets
- **Medium confidence**: The claim that XIT outperforms supervised training and other self-supervised methods in low-data regimes
- **Low confidence**: The generalizability of findings to truly diverse time series domains outside the UCR archive

## Next Checks
1. Apply XIT pretraining on UCR datasets, then evaluate transfer performance on time series from completely different domains (e.g., medical ECG data, financial market data, or multivariate industrial sensor streams) that weren't seen during pretraining
2. Measure and report the actual computational resources (GPU hours, memory usage) required for pretraining on 75 datasets, and analyze how performance scales with the number of pretraining datasets
3. Conduct a more granular ablation study where TC and SICC losses are evaluated separately (not just combined), and test alternative interpolation strategies beyond Beta(α,α) mixing to understand which mechanisms are essential vs. beneficial