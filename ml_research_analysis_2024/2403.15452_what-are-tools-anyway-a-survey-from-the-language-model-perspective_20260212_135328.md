---
ver: rpa2
title: What Are Tools Anyway? A Survey from the Language Model Perspective
arxiv_id: '2403.15452'
source_url: https://arxiv.org/abs/2403.15452
tags:
- tools
- tool
- arxiv
- tasks
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a unified survey of language model (LM) tool
  use, defining tools as external program functions that LMs can call to extend their
  capabilities or facilitate task-solving. The survey systematically categorizes tool
  use scenarios (e.g., knowledge access, computation, interaction with the world)
  and approaches (e.g., multi-tool selection, programmatic contexts, tool creation).
---

# What Are Tools Anyway? A Survey from the Language Model Perspective

## Quick Facts
- **arXiv ID**: 2403.15452
- **Source URL**: https://arxiv.org/abs/2403.15452
- **Reference count**: 30
- **Primary result**: Language models can extend capabilities by calling external tools, with significant performance gains in mathematical reasoning but minimal impact on machine translation

## Executive Summary
This survey provides a unified framework for understanding language model tool use, defining tools as external program functions that LMs can call to extend their capabilities or facilitate task-solving. The authors systematically categorize tool use scenarios (e.g., knowledge access, computation, interaction with the world) and approaches (e.g., multi-tool selection, programmatic contexts, tool creation). Through empirical analysis across diverse tasks and tooling methods, they reveal that tool use yields substantial performance improvements for some applications like mathematical reasoning, while showing minimal or even negative impact on others such as machine translation. The survey identifies critical evaluation gaps, particularly around tool quality, reliability, safety, and efficiency metrics, while emphasizing the importance of reproducible testing for tools interacting with real-world data. The work provides actionable guidance on when and how to effectively integrate tools with LMs, highlighting promising directions for future research in tool creation, evaluation, and safe deployment.

## Method Summary
The authors conducted a comprehensive literature review synthesizing existing research on language model tool use, systematically categorizing tool use scenarios and approaches through analysis of published works. They performed empirical analysis by examining reported performance results across multiple studies, focusing on tasks like mathematical reasoning and machine translation to understand when tool use provides benefits. The survey methodology involved identifying key themes in tool use research, extracting quantitative performance data where available, and analyzing efficiency trade-offs between different tooling approaches. While the paper does not present original experimental results, it critically evaluates the existing body of research to provide insights into the current state of LM tool use, highlighting gaps in evaluation methodologies and identifying promising research directions.

## Key Results
- Tool use provides substantial performance gains for mathematical reasoning tasks but minimal or negative impact on machine translation
- Efficiency of tooling methods varies dramatically, with some achieving large improvements using minimal compute while others require substantial resources
- Training costs can be amortized over multiple inferences, but inference-time efficiency remains critical for practical deployment
- Current evaluation frameworks lack comprehensive metrics for tool quality, reliability, safety, and efficiency

## Why This Works (Mechanism)
Language models benefit from tool use because external programs can provide capabilities that are difficult or impossible to encode directly in model parameters, such as real-time information retrieval, complex calculations, or interaction with dynamic systems. By calling external functions, LMs can access up-to-date knowledge, perform precise computations, and interface with specialized software, effectively extending their functional scope beyond what's possible through training alone. The mechanism works particularly well for tasks requiring exact computation or current information, where the LM's parametric knowledge may be outdated or imprecise. However, the effectiveness depends critically on the quality and reliability of the tools being called, as well as the LM's ability to correctly formulate tool requests and interpret results.

## Foundational Learning

**Tool Definition**: External program functions LMs can call to extend capabilities
*Why needed*: Establishes the conceptual boundary between model capabilities and external resources
*Quick check*: Can the function be called independently of the LM? Does it provide functionality not in the model's parameters?

**Tool Use Scenarios**: Knowledge access, computation, world interaction, multi-tool orchestration
*Why needed*: Different scenarios require different tooling approaches and have varying effectiveness
*Quick check*: What type of external capability is needed? Is it information retrieval, calculation, or action execution?

**Tool Selection Approaches**: Direct prompting, learned selection, programmatic contexts, tool creation
*Why needed*: Different selection methods have varying computational costs and success rates
*Quick check*: How does the LM decide which tool to use? Is it rule-based, learned, or context-driven?

**Evaluation Metrics Gap**: Missing standards for tool quality, reliability, safety, efficiency
*Why needed*: Current research lacks consistent ways to measure tool effectiveness and safety
*Quick check*: How do we quantify tool reliability? What metrics capture safety concerns?

## Architecture Onboarding

**Component Map**: LM model -> Tool selection module -> External tools -> Result processing -> Output generation
The LM generates requests for external tools, which are processed and executed, with results returned for integration into the final response.

**Critical Path**: Tool request formulation -> Tool execution -> Result interpretation -> Integration
Success depends on accurate tool selection, reliable execution, and proper result incorporation into the LM's output.

**Design Tradeoffs**: Training-time tooling vs. inference-time tooling; computational cost vs. performance gain; tool reliability vs. model autonomy
Different approaches balance resource usage against effectiveness, with implications for deployment feasibility.

**Failure Signatures**: Incorrect tool selection, tool execution errors, misinterpretation of tool outputs, over-reliance on unreliable tools
Failures can cascade through the system, leading to degraded performance or incorrect outputs.

**First Experiments**:
1. Compare mathematical reasoning performance with and without calculator tool integration
2. Test tool selection accuracy across different prompting strategies
3. Measure inference latency overhead when adding multiple tool options

## Open Questions the Paper Calls Out
- How to evaluate tool quality, reliability, safety, and efficiency comprehensively
- Best practices for reproducible testing of tools interacting with real-world data
- Methods for effective tool creation that balance automation with human oversight
- Integration of multimodal tools with language models
- Safety considerations for tools that interact with sensitive systems or data

## Limitations
- Categorization may not capture emerging paradigms like continuous learning tools or novel multimodal integrations
- Performance analysis relies on diverse studies with varying protocols, making direct comparisons difficult
- Efficiency metrics combine training and inference costs without clear optimization guidance
- Safety and reliability concerns are acknowledged but not deeply explored for real-world deployments
- Scope excludes specialized tool use cases in domains like scientific discovery or creative writing

## Confidence

**High**: Definition and systematic categorization of tool use scenarios and approaches
- The survey provides clear, consistent definitions and comprehensive categorization framework

**Medium**: Empirical performance trends across different task types
- Performance patterns are observable but comparisons across studies have inherent limitations

**Medium**: General efficiency trade-off analysis between training and inference costs
- Efficiency analysis is reasonable but lacks detailed optimization guidance for specific scenarios

## Next Checks
1. Replicate performance comparisons using standardized benchmarks across multiple tooling approaches to verify reported efficiency differences
2. Conduct controlled experiments isolating tool quality effects from model capabilities to validate claims about tool impact on task performance
3. Implement safety evaluation protocols for real-world tool interactions, measuring reliability and error rates in deployed systems