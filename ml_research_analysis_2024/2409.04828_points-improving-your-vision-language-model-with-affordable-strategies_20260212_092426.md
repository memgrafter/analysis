---
ver: rpa2
title: 'POINTS: Improving Your Vision-language Model with Affordable Strategies'
arxiv_id: '2409.04828'
source_url: https://arxiv.org/abs/2409.04828
tags:
- arxiv
- datasets
- chen
- preprint
- soup
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'POINTS improves vision-language models through three key strategies:
  1) establishing a strong baseline by integrating dynamic high resolution, consistent
  aspect ratio image splitting (CATTY), dual vision encoders, and effective data selection;
  2) filtering pre-training data using perplexity to select the top 20% lowest-perplexity
  samples from a 5M dataset, achieving better performance with fewer samples; 3) applying
  model soup to merge models fine-tuned on different datasets when further data selection
  yields diminishing returns. The resulting 9B-parameter model achieves competitive
  performance with state-of-the-art models on eight benchmarks, using only publicly
  available datasets and a 1M pre-training subset.'
---

# POINTS: Improving Your Vision-language Model with Affordable Strategies

## Quick Facts
- arXiv ID: 2409.04828
- Source URL: https://arxiv.org/abs/2409.04828
- Authors: Yuan Liu; Zhongyin Zhao; Ziyuan Zhuang; Le Tian; Xiao Zhou; Jie Zhou
- Reference count: 33
- One-line primary result: 9B-parameter model achieves competitive performance with state-of-the-art models on eight benchmarks using affordable strategies

## Executive Summary
POINTS introduces three affordable strategies to improve vision-language models: data filtering using perplexity, model soup for ensemble learning, and aspect ratio preservation during image processing. The approach establishes a strong baseline with dynamic high resolution, consistent aspect ratio image splitting (CATTY), dual vision encoders, and effective data selection. By filtering the pre-training dataset to select only the top 20% lowest-perplexity samples, POINTS achieves better performance with fewer training samples. The model soup technique merges models fine-tuned on different datasets when additional data yields diminishing returns.

## Method Summary
POINTS improves vision-language models through three key strategies: (1) establishing a strong baseline by integrating dynamic high resolution, consistent aspect ratio image splitting (CATTY), dual vision encoders, and effective data selection; (2) filtering pre-training data using perplexity to select the top 20% lowest-perplexity samples from a 5M dataset, achieving better performance with fewer samples; (3) applying model soup to merge models fine-tuned on different datasets when further data selection yields diminishing returns. The resulting 9B-parameter model achieves competitive performance with state-of-the-art models on eight benchmarks, using only publicly available datasets and a 1M pre-training subset.

## Key Results
- Achieves competitive performance with state-of-the-art models on eight benchmarks (MMBench, MMStar, MMMU, HallusionBench, MathVista, AI2D, OCRBench, MMVet)
- Uses only publicly available datasets and a 1M pre-training subset filtered from 5M
- Demonstrates that lower perplexity data leads to better model performance
- Shows model soup improves performance when fine-tuning on multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filtering pre-training data using perplexity improves model performance by removing low-quality or overly specialized data.
- Mechanism: Perplexity measures how well a language model predicts a given text. By selecting data with the lowest perplexity, the model focuses on data that is both high-quality and representative, avoiding data that is either too noisy or too specialized.
- Core assumption: Low perplexity data is both high-quality and representative of the general knowledge the model needs to learn.
- Evidence anchors:
  - [abstract] "Inspired by recent work on large language models, we filtered pre-training data using perplexity, selecting the lowest perplexity data for training."
  - [section] "Based on the 5 million data, we further selected a 1 million dataset for the final vision-language alignment by choosing the top 20% of data with the lowest perplexity value."
- Break condition: If low perplexity data is not representative or if high perplexity data contains valuable specialized knowledge that the model needs.

### Mechanism 2
- Claim: Model soup improves performance by combining the strengths of models fine-tuned on different datasets.
- Mechanism: Different datasets lead to different local optima during fine-tuning. By averaging the weights of these models, the combined model can leverage the strengths of each dataset.
- Core assumption: Different datasets lead to different local optima during fine-tuning, and combining these optima leads to a better overall model.
- Evidence anchors:
  - [abstract] "During the visual instruction tuning stage, we experimented with model soup on different datasets when further introducing more datasets into the training set brought marginal improvements."
- Break condition: If the models fine-tuned on different datasets do not converge to meaningfully different optima.

### Mechanism 3
- Claim: Using a consistent aspect ratio during image splitting improves OCR performance by reducing image distortion.
- Mechanism: Maintaining the original aspect ratio prevents distortion of text within images, which is crucial for OCR tasks.
- Core assumption: Image distortion negatively impacts OCR performance.
- Evidence anchors:
  - [section] "Compared to general visual feature extraction, the ability to extract text features from images is limited for CLIP-ViT... Consequently, we observe substantial improvements on OCRBench after integrating features from an additional ViT, post-trained on text-rich images."
- Break condition: If the improvement in OCR performance is not significant enough to justify the added complexity.

## Foundational Learning

- Concept: Perplexity as a measure of data quality
  - Why needed here: Perplexity is used to filter the pre-training dataset, so understanding what it measures is crucial.
  - Quick check question: What does perplexity measure in the context of language models?

- Concept: Model soup and weight averaging
  - Why needed here: Model soup is used to combine models fine-tuned on different datasets, so understanding how it works is essential.
  - Quick check question: How does model soup combine the weights of multiple models?

- Concept: Image resolution and aspect ratio
  - Why needed here: The paper discusses dynamic high resolution and consistent aspect ratio, so understanding their impact on image quality is important.
  - Quick check question: How does changing the aspect ratio of an image affect its content?

## Architecture Onboarding

- Component map:
  Vision Encoder (General ViT and OCR ViT) -> Projector (MLP) -> Large Language Model
  Additional components: Dynamic High Resolution (with CATTY), CapFusion, Model Soup

- Critical path:
  Pre-training (OCR ViT and Vision-Language Pre-training) -> Visual Instruction Tuning (with Model Soup)

- Design tradeoffs:
  - Using a smaller pre-training dataset with higher quality vs. a larger dataset with lower quality
  - Using model soup to combine models vs. using a single model
  - Using CATTY vs. traditional dynamic high resolution

- Failure signatures:
  - Low performance on OCR tasks (may indicate issues with image resolution or aspect ratio)
  - Overfitting to the training data (may indicate issues with dataset quality or size)
  - Poor generalization to new data (may indicate issues with model architecture or training process)

- First 3 experiments:
  1. Train a model using the full pre-training dataset and compare its performance to a model trained on the filtered dataset.
  2. Train a model using model soup and compare its performance to a model trained on a single dataset.
  3. Train a model using CATTY and compare its performance to a model using traditional dynamic high resolution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal perplexity threshold for filtering pre-training data to maximize model performance?
- Basis in paper: [explicit] The paper filters pre-training data by selecting the top 20% of samples with the lowest perplexity values, but does not explore different threshold percentages.
- Why unresolved: The paper only tests one threshold (20%) without comparing it to other percentages or methods for determining the optimal threshold.
- What evidence would resolve it: Systematic experiments testing multiple perplexity thresholds (e.g., 10%, 20%, 30%, 40%) and comparing model performance to identify the optimal filtering percentage.

### Open Question 2
- Question: How does the effectiveness of model soup vary with different combinations of fine-tuning datasets?
- Basis in paper: [explicit] The paper uses model soup to merge models fine-tuned on different datasets when additional data selection yields diminishing returns, but does not explore how different dataset combinations affect the effectiveness.
- Why unresolved: The paper only demonstrates that model soup improves performance but does not investigate which specific dataset combinations are most effective or whether certain datasets contribute more to the improvement.
- What evidence would resolve it: Controlled experiments testing model soup with various combinations of fine-tuning datasets and analyzing which combinations yield the greatest performance improvements.

### Open Question 3
- Question: What is the long-term impact of training on filtered pre-training data versus the full dataset?
- Basis in paper: [explicit] The paper shows that training on a filtered 1M dataset (top 20% lowest perplexity) achieves better performance than training on a 5M dataset, but does not examine the long-term effects on model generalization or robustness.
- Why unresolved: The paper focuses on immediate performance gains but does not investigate whether filtered data training affects the model's ability to handle diverse or novel inputs over time.
- What evidence would resolve it: Longitudinal studies comparing models trained on filtered versus full datasets across multiple benchmarks and real-world applications to assess differences in generalization, robustness, and adaptability.

## Limitations

- The perplexity-based filtering mechanism lacks theoretical grounding and empirical validation across different dataset domains
- The model soup approach assumes different datasets lead to meaningfully different optima without testing this assumption
- The evaluation scope is limited to eight benchmarks and may not capture full generalization capabilities

## Confidence

- **High confidence**: The baseline architecture components (dual vision encoders, dynamic high resolution) are well-established in the literature and their effectiveness is reasonably demonstrated through benchmark improvements
- **Medium confidence**: The perplexity-based filtering shows promising results but lacks ablation studies to confirm that the specific 20% threshold is optimal or that perplexity is the best metric for data selection
- **Low confidence**: The model soup benefits are presented without rigorous comparison to alternative ensembling strategies, and the assumption about different datasets leading to different optima remains unverified

## Next Checks

1. **Ablation study on perplexity threshold**: Systematically test filtering thresholds from 10% to 50% of the dataset to determine if 20% is optimal, and compare perplexity-based filtering against random sampling and other quality metrics like CLIP score or human evaluation.

2. **Model soup methodology comparison**: Implement and compare model soup against alternative ensembling strategies including weighted averaging, gating mechanisms, and traditional ensemble voting to verify that simple weight averaging provides optimal performance gains.

3. **Aspect ratio impact analysis**: Conduct controlled experiments varying image aspect ratios during training to quantify the specific contribution of CATTY to OCRBench performance improvements, separating the effects of resolution, aspect ratio, and image splitting strategy.