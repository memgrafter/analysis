---
ver: rpa2
title: 'Adaptive Pruning with Module Robustness Sensitivity: Balancing Compression
  and Robustness'
arxiv_id: '2410.15176'
source_url: https://arxiv.org/abs/2410.15176
tags:
- adversarial
- pruning
- robustness
- mrpf
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of balancing model compression
  with adversarial robustness in neural network pruning. Traditional pruning methods
  often compromise robustness by removing critical parameters.
---

# Adaptive Pruning with Module Robustness Sensitivity: Balancing Compression and Robustness

## Quick Facts
- arXiv ID: 2410.15176
- Source URL: https://arxiv.org/abs/2410.15176
- Authors: Lincen Bai; Hedi Tabia; Raúl Santos-Rodríguez
- Reference count: 40
- Key outcome: MRPF achieves 81.15% standard accuracy and 46.44% accuracy under AutoAttack at 90% sparsity on CIFAR-10, outperforming state-of-the-art methods by up to 7.16% in robustness.

## Executive Summary
This paper addresses the challenge of balancing model compression with adversarial robustness in neural network pruning. Traditional pruning methods often compromise robustness by removing critical parameters. The authors introduce Module Robustness Sensitivity (MRS), a novel metric that quantifies layer-wise sensitivity to adversarial perturbations, and propose Module Robust Pruning and Fine-Tuning (MRPF), an adaptive pruning algorithm that leverages MRS to dynamically adjust pruning ratios. Extensive experiments on SVHN, CIFAR-10/100, and Tiny-ImageNet across ResNet, VGG, and MobileViT architectures demonstrate that MRPF significantly enhances adversarial robustness while maintaining competitive accuracy and computational efficiency.

## Method Summary
The paper proposes a novel approach to adversarial pruning that combines Module Robustness Sensitivity (MRS) metric with adaptive pruning and adversarial fine-tuning. MRS quantifies how perturbing each layer's weights affects adversarial loss, identifying which layers are most critical for robustness. MRPF uses these sensitivity scores to dynamically allocate pruning ratios, preserving robustness-critical parameters while achieving compression. After pruning, the model is fine-tuned using adversarial examples to restore and enhance decision boundaries. The method is compatible with various adversarial training methods including TRADES, MART, and PGD.

## Key Results
- At 90% sparsity on CIFAR-10, MRPF achieves 81.15% standard accuracy and 46.44% accuracy under AutoAttack
- Outperforms state-of-the-art methods by up to 7.16% in robustness while maintaining competitive accuracy
- Demonstrates consistent improvements across ResNet, VGG, and MobileViT architectures on multiple datasets
- Shows that MRS-guided pruning preserves more robustness-critical parameters than uniform or magnitude-based pruning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The MRS metric effectively identifies layers whose removal would disproportionately degrade adversarial robustness.
- **Mechanism**: MRS is calculated by measuring the change in adversarial loss when weights of a layer are perturbed. Layers with higher MRS values are more sensitive to adversarial perturbations and thus more critical to robustness. By assigning higher pruning ratios to layers with lower MRS, the method preserves robustness-critical parameters while still achieving compression.
- **Core assumption**: The adversarial loss sensitivity of a layer directly reflects its importance for robustness preservation under pruning.
- **Evidence anchors**:
  - [abstract] "Module Robustness Sensitivity (MRS), a novel metric that quantifies layer-wise sensitivity to adversarial perturbations and dynamically informs pruning decisions."
  - [section] "We define the MRS to quantify the effect of perturbing each layer's weights on the adversarial loss: MRS(li) = Ladv(N(W pert li , AE), yAE) - Ladv(N(W orig li , AE), yAE)"
- **Break condition**: If adversarial loss sensitivity does not correlate with actual robustness impact, MRS would fail to guide effective pruning.

### Mechanism 2
- **Claim**: Fine-tuning with adversarial examples is more critical than pruning criteria for recovering and enhancing robustness.
- **Mechanism**: After pruning, the model is fine-tuned using a dataset augmented with adversarial examples. This adversarial fine-tuning restores decision boundaries that were compromised during pruning, leading to improved robustness even when significant compression has occurred.
- **Core assumption**: Adversarial fine-tuning can compensate for parameter loss by relearning robust decision boundaries.
- **Evidence anchors**:
  - [abstract] "Extensive experiments on SVHN, CIFAR, and Tiny-ImageNet across diverse architectures, including ResNet, VGG, and MobileViT, demonstrate that MRPF significantly enhances adversarial robustness while maintaining competitive accuracy and computational efficiency."
  - [section] "By incorporating adversarial fine-tuning, pruning strategies can better handle the trade-off between efficiency and robustness, as demonstrated in recent works like CSTAR [Phan et al., 2022]."
- **Break condition**: If adversarial fine-tuning fails to restore decision boundaries, robustness gains would not materialize despite effective pruning.

### Mechanism 3
- **Claim**: The dynamic allocation of pruning ratios based on MRS values enables better trade-offs between compression and robustness than uniform or magnitude-based pruning.
- **Mechanism**: Pruning ratios are computed as the inverse of MRS values, normalized across layers. This ensures that layers with lower sensitivity (lower MRS) are pruned more aggressively, while critical layers are preserved, leading to more balanced performance across accuracy and robustness metrics.
- **Core assumption**: Layer-wise sensitivity to adversarial perturbations varies significantly enough to justify non-uniform pruning.
- **Evidence anchors**:
  - [abstract] "Leveraging MRS, we propose Module Robust Pruning and Fine-Tuning (MRPF), an adaptive pruning algorithm compatible with any adversarial training method, offering both flexibility and scalability."
  - [section] "Layers with higher MRS values are more important for maintaining robustness, and their channels should be retained during pruning."
- **Break condition**: If layer sensitivities are uniform or the MRS metric fails to capture true robustness importance, adaptive pruning would not outperform uniform methods.

## Foundational Learning

- **Concept**: Adversarial examples and their generation
  - **Why needed here**: Understanding how adversarial examples are generated and used is fundamental to grasping how MRS works and why adversarial fine-tuning is necessary.
  - **Quick check question**: What is the difference between FGSM and PGD in terms of how they generate adversarial examples?

- **Concept**: Neural network pruning strategies (structured vs unstructured, magnitude-based vs importance-based)
  - **Why needed here**: The paper builds on existing pruning methods but introduces a novel criterion (MRS) for deciding which channels to prune.
  - **Quick check question**: Why might structured pruning be preferred over unstructured pruning in practice?

- **Concept**: Robustness-accuracy trade-off in deep learning
  - **Why needed here**: The paper explicitly addresses the tension between model compression (which reduces robustness) and the need for adversarial robustness.
  - **Quick check question**: According to the paper, what is one reason why over-parameterized networks tend to be more robust?

## Architecture Onboarding

- **Component map**: Pre-trained model -> Adversarial example generator (FGSM/PGD) -> MRS calculator (layer-wise sensitivity analysis) -> Pruning ratio allocator (MRS-based dynamic allocation) -> Pruner (channel removal based on importance scores) -> Adversarial fine-tuner (TRADES/MART/PGD-based) -> Pruned model (output)

- **Critical path**: Generate adversarial examples → Calculate MRS for each layer → Compute pruning ratios → Prune network → Fine-tune with adversarial examples → Evaluate robustness and accuracy

- **Design tradeoffs**:
  - Computational cost of MRS calculation vs. potential robustness gains
  - Choice of adversarial training method during fine-tuning vs. final robustness-accuracy balance
  - Layer-wise vs. global pruning ratio allocation strategy

- **Failure signatures**:
  - Poor robustness recovery after pruning → likely insufficient adversarial fine-tuning
  - High accuracy but low robustness → pruning may have removed critical robustness features
  - Computational inefficiency → MRS calculation or fine-tuning may be too intensive

- **First 3 experiments**:
  1. Implement MRS calculation on a pre-trained ResNet-18 and visualize layer-wise sensitivity scores
  2. Apply uniform pruning vs. MRS-guided pruning at 50% sparsity and compare robustness metrics
  3. Add adversarial fine-tuning after pruning and measure recovery in both accuracy and robustness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does MRPF's performance scale with deeper architectures like Vision Transformers and Large Language Models?
- **Basis in paper**: [explicit] The paper mentions future work exploring MRPF's application to Vision Transformers and large language models, suggesting current experiments focus on ResNet, VGG, and MobileViT architectures.
- **Why unresolved**: The paper only evaluates MRPF on ResNet, VGG, and MobileViT architectures. The behavior of MRPF on deeper, more complex architectures like Vision Transformers and Large Language Models remains untested.
- **What evidence would resolve it**: Experimental results showing MRPF's effectiveness on deeper architectures, comparing robustness, accuracy, and compression ratios against state-of-the-art methods for those architectures.

### Open Question 2
- **Question**: What is the optimal ratio of adversarial examples to include during fine-tuning to maximize robustness without sacrificing too much accuracy?
- **Basis in paper**: [explicit] The paper mentions that increasing the ratio of adversarial examples during fine-tuning improves robustness but also notes that higher ratios (e.g., 100%) lead to a sharp drop in accuracy.
- **Why unresolved**: The paper only tests specific ratios (20%, 50%, 100%) and does not explore the full spectrum of possible ratios to find the optimal balance.
- **What evidence would resolve it**: A detailed study varying the ratio of adversarial examples from 0% to 100% in smaller increments, measuring both accuracy and robustness at each step to identify the optimal ratio.

### Open Question 3
- **Question**: How does the choice of adversarial training method (TRADES, MART, PGD) interact with MRPF's Module Robustness Sensitivity (MRS) metric to affect final performance?
- **Basis in paper**: [explicit] The paper compares different adversarial training methods (TRADES, MART, PGD) but does not specifically analyze how these methods interact with MRS to influence the pruning process and final model performance.
- **Why unresolved**: The paper shows that adversarial training improves robustness but does not isolate the interaction between the specific adversarial training method and MRS in determining which layers are pruned and how this affects overall performance.
- **What evidence would resolve it**: Experiments comparing MRPF with MRS using different adversarial training methods, analyzing the layer-wise pruning decisions and their impact on robustness and accuracy to determine if certain combinations are more effective.

## Limitations

- The MRS metric relies on adversarial loss sensitivity as a proxy for robustness importance, which may not perfectly correlate with actual robustness impact in all scenarios.
- The computational overhead of MRS calculation and adversarial fine-tuning may limit scalability to very large models or datasets.
- The method's performance on non-image domains and extremely high sparsity levels (>95%) remains untested.

## Confidence

- **High Confidence**: The core mechanism of using layer-wise adversarial sensitivity for pruning decisions is well-grounded in established principles of adversarial machine learning and network pruning.
- **Medium Confidence**: The specific implementation details of MRS calculation and the normalization approach for pruning ratios are not fully specified, requiring some engineering decisions during reproduction.
- **Medium Confidence**: While results show significant improvements, the extent of robustness gains may vary depending on architecture choice and training hyperparameters.

## Next Checks

1. Verify the correlation between MRS values and actual robustness impact by conducting ablation studies that selectively prune high vs. low MRS layers
2. Test the method's performance on larger-scale datasets (ImageNet) and higher sparsity levels to assess scalability limits
3. Compare MRS-guided pruning against alternative robustness-aware pruning methods using consistent evaluation protocols