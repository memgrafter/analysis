---
ver: rpa2
title: On Using Quasirandom Sequences in Machine Learning for Model Weight Initialization
arxiv_id: '2408.02654'
source_url: https://arxiv.org/abs/2408.02654
tags:
- accuracy
- random
- normal
- qrng-based
- qrng
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Quasirandom sequences, particularly Sobol' sequences, can significantly
  improve neural network training efficiency compared to traditional pseudorandom
  number generators. The study examined four neural network architectures (MLP, CNN,
  LSTM, and Transformer) on three datasets (MNIST, CIFAR-10, and IMDB) using ten different
  initialization schemes.
---

# On Using Quasirandom Sequences in Machine Learning for Model Weight Initialization

## Quick Facts
- arXiv ID: 2408.02654
- Source URL: https://arxiv.org/abs/2408.02654
- Reference count: 40
- Quasirandom sequences (Sobol') improve neural network training efficiency compared to PRNGs

## Executive Summary
This study investigates whether quasirandom number generators (QRNGs), specifically Sobol' sequences, can improve neural network weight initialization compared to traditional pseudorandom number generators (PRNGs). Through 120 experiments across four neural network architectures (MLP, CNN, LSTM, Transformer) and three datasets (MNIST, CIFAR-10, IMDB), the research demonstrates that QRNG-based initializers either achieve higher accuracy or reach the same accuracy faster in 60% of cases. The approach shows particular promise for simpler architectures and shape-dependent initialization schemes, offering a practical method to accelerate training with minimal computational overhead.

## Method Summary
The research employed 120 experiments with 100 repetitions each, comparing PRNG-based and QRNG-based initializers across ten initialization schemes on three datasets and four architectures. Sobol' sequences were generated using SciPy's implementation and transformed to desired distributions via inverse transform sampling. The study used simplified architectures trained for 30 epochs with batch size 64, employing both SGD and Adam optimizers. A seed selection heuristic was implemented to optimize QRNG performance, though this introduced computational overhead. The experiments systematically varied architectures, datasets, optimizers, and initialization schemes to comprehensively evaluate QRNG effectiveness.

## Key Results
- QRNG-based initializers achieved higher accuracy or faster convergence in 60% of 120 experiments
- Top 25% of improvements showed accuracy gains between 0.0775 and 0.3550
- Shape-dependent initializers (Glorot, He, Lecun) showed 67% win rate when using QRNG
- MLP architecture showed 75% win rate, CNN showed 60% win rate with QRNG initialization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** QRNG-based initialization accelerates training by providing more uniform coverage of the parameter space than PRNG-based initialization.
- **Mechanism:** Sobol' sequences minimize discrepancy and fill high-dimensional spaces more evenly than PRNGs, leading to better starting conditions for optimization.
- **Core assumption:** More uniform initialization leads to faster convergence and/or better final accuracy.
- **Evidence anchors:** Abstract showing 60% improvement rate; section 1 discussing uniformity and parameter space examination.
- **Break condition:** If the architecture or dataset doesn't benefit from uniform initialization, or if optimizers compensate for poor initialization.

### Mechanism 2
- **Claim:** QRNG-based initialization is particularly effective for shape-dependent initializers due to their variance scaling properties.
- **Mechanism:** Shape-dependent initializers scale random numbers based on layer dimensions, and QRNGs provide more uniform scaling across dimensions.
- **Core assumption:** The interaction between QRNG uniformity and shape-dependent scaling leads to improved weight initialization.
- **Evidence anchors:** Section 3.3.3 showing 67% win rate for shape-dependent initializers.
- **Break condition:** If shape-dependent scaling is not beneficial for the architecture or dataset.

### Mechanism 3
- **Claim:** QRNG-based initialization can lead to faster convergence in models with simpler architectures.
- **Mechanism:** Simpler architectures with fewer parameters are more sensitive to initialization, and QRNGs provide a more stable starting point.
- **Core assumption:** Simpler architectures are more sensitive to initialization quality.
- **Evidence anchors:** Section 3.3.4 showing 75% win rate for MLPs and 60% for CNNs.
- **Break condition:** If architecture complexity makes initialization less critical, or optimizers compensate for poor initialization.

## Foundational Learning

- **Concept:** Low-discrepancy sequences and their properties.
  - **Why needed here:** Understanding how Sobol' sequences provide more uniform coverage of high-dimensional spaces than PRNGs is crucial to understanding QRNG benefits.
  - **Quick check question:** What distinguishes low-discrepancy sequences from PRNGs, and how does this benefit neural network initialization?

- **Concept:** Variance scaling in weight initialization.
  - **Why needed here:** Shape-dependent initializers scale random numbers based on layer dimensions, which interacts with QRNG properties.
  - **Quick check question:** How do shape-dependent initializers scale random numbers, and why is this scaling important for training?

- **Concept:** Neural network architectures and their training dynamics.
  - **Why needed here:** The study examines four different architectures and their sensitivity to initialization.
  - **Quick check question:** What are the key differences between MLPs, CNNs, LSTMs, and Transformers regarding initialization sensitivity?

## Architecture Onboarding

- **Component map:** Sobol' sequence generator -> Inverse transform sampling -> Custom Keras initializers -> Keras/TensorFlow models -> Training pipeline
- **Critical path:**
  1. Generate Sobol' sequences for each layer
  2. Apply inverse transform sampling for desired distributions
  3. Initialize network weights using generated random numbers
  4. Train model with specified optimizer and dataset
  5. Evaluate performance against PRNG-based baseline

- **Design tradeoffs:**
  - QRNG initialization is slower than PRNG due to Sobol' sequence recursion
  - Seed selection heuristic adds computational overhead but can improve results
  - Simplified architectures limit generalizability to production models

- **Failure signatures:**
  - Incorrect Sobol' implementation leads to non-functional QRNG initialization
  - Wrong inverse transform sampling produces incorrect distributions
  - Custom initializers not properly integrated prevent QRNG usage

- **First 3 experiments:**
  1. Train MLP on MNIST using PRNG vs QRNG Glorot Uniform initialization
  2. Train CNN on CIFAR-10 using PRNG vs QRNG He Normal initialization
  3. Train LSTM on IMDB using PRNG vs QRNG Orthogonal initialization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does Random Uniform initializer consistently underperform with QRNG?
- **Basis in paper:** Paper identifies Random Uniform wins only 25% of cases and mutates starting random numbers the least.
- **Why unresolved:** Paper identifies the pattern but doesn't explain the underlying mechanism.
- **What evidence would resolve it:** Comparative analysis of how different initializers transform input random sequences and their mathematical relationship with QRNG properties.

### Open Question 2
- **Question:** How does the number of weights in a layer affect QRNG vs PRNG performance difference?
- **Basis in paper:** Paper mentions weight patterns like 4, 8, 12 and questions whether incorporating this factor could improve QRNG initialization.
- **Why unresolved:** Study used specific architectures without systematically varying layer sizes.
- **What evidence would resolve it:** Systematic experiments varying weights across multiple orders of magnitude while measuring relative performance.

### Open Question 3
- **Question:** What is the optimal seed selection strategy for QRNG-based initialization?
- **Basis in paper:** Paper identifies computational penalty of current seed selection heuristic and suggests alternatives could improve results.
- **Why unresolved:** Current heuristic requires multiple training runs, partially negating speed benefits.
- **What evidence would resolve it:** Development of seed selection method achieving comparable results without multiple training runs, or validation that simple random seed selection suffices.

## Limitations
- Simplified architectures with reduced parameters may not generalize to production-scale models
- 30-epoch training duration may not capture long-term training dynamics
- Study doesn't address interactions with regularization techniques or architectural components

## Confidence
- **High Confidence:** Sobol' QRNGs provide more uniform parameter space coverage than PRNGs, leading to faster convergence in simpler architectures (75% MLP, 60% CNN improvement rates)
- **Medium Confidence:** QRNG works particularly well with shape-dependent initializers (67% win rate), but underlying mechanism needs further validation
- **Low Confidence:** Generalizability to complex architectures (Transformers at 55% improvement) and real-world scenarios with extended training remains uncertain

## Next Checks
1. **Architectural Generalization Test:** Implement QRNG initialization on production-scale architectures (ResNets, BERT variants) with full parameters and extended training to verify if improvements persist
2. **Interaction Effects Analysis:** Test QRNG initialization combined with batch normalization, dropout, and data augmentation to identify amplification or diminishment of benefits
3. **Distribution-Specific Validation:** Systematically test each initialization distribution across different QRNG sequences beyond Sobol' to determine if benefits are sequence-specific or represent broader QRNG advantage