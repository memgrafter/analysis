---
ver: rpa2
title: Influence Functions for Scalable Data Attribution in Diffusion Models
arxiv_id: '2410.13850'
source_url: https://arxiv.org/abs/2410.13850
tags:
- loss
- influence
- training
- diffusion
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops scalable influence function methods for data
  attribution in diffusion models. The authors address the challenge of understanding
  how training data influences diffusion model outputs, motivated by interpretability
  and potential copyright concerns.
---

# Influence Functions for Scalable Data Attribution in Diffusion Models

## Quick Facts
- arXiv ID: 2410.13850
- Source URL: https://arxiv.org/abs/2410.13850
- Reference count: 40
- This paper develops scalable influence function methods for data attribution in diffusion models using Kronecker-Factored Approximate Curvature approximations.

## Executive Summary
This paper addresses the challenge of understanding how training data influences diffusion model outputs through scalable influence function methods. The authors develop Kronecker-Factored Approximate Curvature (K-FAC) approximations specifically tailored to diffusion models, enabling efficient computation of influence scores for large-scale models. Their K-FAC Influence approach outperforms previous methods on Linear Data-modelling Score (LDS) metrics and demonstrates improved performance in retraining experiments where influential data points are removed.

## Method Summary
The method uses influence functions to approximate how removing training data would change model outputs, focusing on predicting changes in generation probability. The core innovation is formulating K-FAC approximations of the generalized Gauss-Newton matrix specifically for diffusion models, enabling scalable Hessian computations. The approach involves Monte Carlo sampling of auxiliary targets from the model's output distribution, eigenvalue-corrected K-FAC (EK-FAC) for improved accuracy, and 8-bit quantization of preconditioned gradients for computational efficiency. The method is evaluated across multiple datasets including CIFAR-10 and ArtBench-10, comparing against TRAK and D-TRAK baselines.

## Key Results
- K-FAC Influence approach outperforms previous methods on Linear Data-modelling Score (LDS) metric
- Demonstrated improvements in retraining experiments where top influences are removed
- EK-FAC improves performance notably over standard K-FAC
- Method successfully handles the unique challenges of diffusion models, including Monte Carlo sampling for auxiliary targets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: K-FAC approximates the generalized Gauss-Newton matrix for diffusion models by treating the model's output as the linearization target while keeping the ℓ²-loss function fixed.
- Mechanism: The Hessian approximation splits the loss function θ ↦ ‖ε(t) - εt(θ(x(t)))‖² into model output θ ↦ εt(θ(x(t))) and the ℓ²-loss function. This formulation allows K-FAC to approximate the resulting GGN efficiently using Kronecker products of pre-activation and gradient matrices.
- Core assumption: The neural network mapping from parameters to model output can be locally linearized, making the GGN an appropriate Hessian approximation.
- Evidence anchors:
  - [abstract] "We systematically develop K-FAC approximations based on generalised Gauss-Newton matrices specifically tailored to diffusion models"
  - [section] "We can consider the trivial decomposition of ℓ(·, x) into the log and the exponentiated loss, effectively 'linearising' exp(-ℓ(·, x))."
  - [corpus] Weak - no direct corpus evidence for diffusion model-specific K-FAC formulation
- Break condition: If the model output mapping is highly nonlinear in the region of interest, the linearization assumption breaks down and the approximation becomes inaccurate.

### Mechanism 2
- Claim: Monte Carlo sampling of auxiliary targets from the model's output distribution enables efficient computation of the Fisher approximation for diffusion models.
- Mechanism: The GGNmodel approximation FD(θ) = Exn[E˜t[Ex(˜t),ϵ(˜t)[gn(θ)gn(θ)⊺]]] is computed using MC samples of auxiliary targets ϵmod sampled from N(ε˜t(θ(x(˜t))), I), avoiding explicit Jacobian computation.
- Core assumption: The auxiliary targets sampled from the model's output distribution provide an unbiased estimate of the Fisher information needed for the GGN approximation.
- Evidence anchors:
  - [section] "This formulation lends itself to a Monte Carlo approximation, since we can now compute gradients using auxiliary targets ϵmod sampled from the model's output distribution"
  - [section] "where gn(θ) = ∇θ‖ϵmod - ε˜t(θ(x(˜t)))‖² ∈ Rdparam"
  - [corpus] Weak - no corpus evidence for MC sampling approach in diffusion models
- Break condition: If the model's output distribution is highly multimodal or has complex dependencies, MC sampling may require an impractically large number of samples for accurate estimation.

### Mechanism 3
- Claim: Eigenvalue-corrected K-FAC (EK-FAC) improves the accuracy of the Hessian approximation by correcting the eigenvalue spectrum while maintaining computational efficiency.
- Mechanism: EK-FAC modifies the standard K-FAC approximation by applying eigenvalue correction to better match the true Hessian spectrum, improving the quality of the influence function approximation.
- Core assumption: The eigenvalue spectrum of the true Hessian contains important information for influence function accuracy that is lost in the basic K-FAC approximation.
- Evidence anchors:
  - [section] "Additionally, we choose to use eigenvalue-corrected K-FAC (George et al., 2018, EK-FAC) in our experiments — as suggested by Grosse et al. (2023) — which improves performance notably"
  - [section] "EK-FAC improves performance notably and can be directly applied on top of our K-FAC approximation"
  - [corpus] Weak - no corpus evidence for EK-FAC specifically in diffusion models
- Break condition: If the computational overhead of eigenvalue correction outweighs the accuracy gains, or if the correction introduces numerical instability in high-dimensional settings.

## Foundational Learning

- Concept: Influence functions and their mathematical foundation
  - Why needed here: The entire paper builds on approximating how removing training data would change model outputs using influence functions, which requires understanding the implicit function theorem and Taylor expansions
  - Quick check question: What is the key mathematical assumption that allows influence functions to approximate the change in optimal parameters when removing training data?

- Concept: Kronecker-Factored Approximate Curvature (K-FAC)
  - Why needed here: K-FAC is the core computational technique that makes influence functions scalable for large diffusion models by approximating the Hessian efficiently
  - Quick check question: How does K-FAC exploit the structure of neural network layers to make Hessian inversion computationally tractable?

- Concept: Generalized Gauss-Newton matrix and its relationship to the Hessian
  - Why needed here: Understanding when and why the GGN approximates the Hessian is crucial for selecting the appropriate approximation in diffusion models
  - Quick check question: Under what conditions is the GGN exactly equal to the true Hessian, and when is it only an approximation?

## Architecture Onboarding

- Component map: Core computation -> K-FAC-based Hessian approximation using the GGNmodel formulation -> Sampling layer -> Monte Carlo sampling of auxiliary targets and diffusion trajectories -> Measurement module -> Various proxy measurements (loss, ELBO, marginal probability) -> Compression pipeline -> Int8 quantization of preconditioned gradients -> Evaluation suite -> LDS scores and retraining experiments

- Critical path: Compute K-FAC inverse → Precondition query gradients → Compress → Compute inner products with training gradients → Rank influences

- Design tradeoffs:
  - K-FAC vs EK-FAC: EK-FAC provides better accuracy but adds computational overhead
  - Expand vs Reduce variants: Expand is exact for deep linear networks but may be more memory-intensive
  - MC samples: More samples improve accuracy but increase computation time

- Failure signatures:
  - Poor LDS scores despite correct implementation → Likely issues with damping factor or Hessian approximation quality
  - Runtime explosion → Check gradient compression or K-FAC variant selection
  - Negative influences when removing data → Expected behavior, but extreme values may indicate numerical instability

- First 3 experiments:
  1. Implement K-FAC Influence on CIFAR-2 with default settings and verify it outperforms TRAK on LDS scores
  2. Ablate the damping factor on CIFAR-2 to find the optimal value for your specific hardware
  3. Compare K-FAC-expand vs K-FAC-reduce variants to confirm expand provides better performance in your setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of Hessian approximation (GGNmodel vs GGNloss) specifically impact the accuracy of data attribution predictions in diffusion models?
- Basis in paper: [explicit] The paper discusses two different GGN formulations (GGNmodel and GGNloss) and shows that GGNmodel performs better, but the specific reasons for this difference are not fully explored.
- Why unresolved: The paper notes that GGNmodel is a better-motivated approximation but doesn't provide a detailed analysis of why it outperforms GGNloss in the diffusion modeling context.
- What evidence would resolve it: Systematic experiments comparing the two GGN formulations across different diffusion model architectures and training conditions, including analysis of their approximation errors relative to the true Hessian.

### Open Question 2
- Question: What are the fundamental limitations of influence functions in predicting numerical changes in measurements for diffusion models?
- Basis in paper: [explicit] The paper identifies several limitations, including the tendency to overestimate negative influences and the poor prediction of numerical changes in measurements, particularly for marginal probability.
- Why unresolved: While the paper documents these limitations, it doesn't provide a theoretical explanation for why influence functions struggle with diffusion models specifically.
- What evidence would resolve it: Theoretical analysis of the diffusion modeling objective function that explains why the assumptions underlying influence functions break down in this setting, or development of alternative attribution methods that address these limitations.

### Open Question 3
- Question: How does the performance of influence functions scale with the size of the training dataset in diffusion models?
- Basis in paper: [explicit] The paper observes that for sufficiently large training set sizes, the diffusion model's marginal probability distribution becomes nearly constant, which complicates data attribution.
- Why unresolved: The paper doesn't systematically investigate how the effectiveness of influence functions changes as dataset size varies, or at what dataset size the constant-probability behavior becomes dominant.
- What evidence would resolve it: Empirical studies measuring the performance of influence functions on datasets of varying sizes, particularly focusing on the transition point where the constant-probability behavior emerges.

## Limitations
- The mathematical framework relies on the generalized Gauss-Newton matrix approximating the true Hessian, which may not hold for highly nonlinear diffusion model architectures
- Monte Carlo sampling introduces variance in the influence estimates, particularly for complex data distributions
- Empirical evaluation focuses primarily on image datasets (CIFAR-10, ArtBench-10), limiting generalizability to other data modalities

## Confidence
**High Confidence**: The mathematical formulation of K-FAC Influence for diffusion models is well-grounded in established influence function theory and the Kronecker-factored approximation literature. The improvements over TRAK/D-TRAK baselines on LDS scores and retraining experiments are demonstrated across multiple datasets.

**Medium Confidence**: The choice of EK-FAC over standard K-FAC is supported by empirical results but lacks theoretical justification specific to diffusion models. The observed tendency for influence functions to overestimate loss decreases when removing data points is documented but not fully explained.

**Low Confidence**: The generalization of results to non-image domains remains untested. The impact of different noise schedule parameterizations on influence function accuracy is not explored.

## Next Checks
1. **Ablation Study**: Systematically vary the damping factor and K-FAC variant (expand vs reduce) on CIFAR-2 to identify optimal configurations and test sensitivity to hyperparameters.

2. **Distributional Analysis**: Analyze the empirical distribution of influence scores across different data classes to verify that the method captures meaningful attribution patterns beyond overall LDS improvements.

3. **Cross-Domain Validation**: Apply K-FAC Influence to a non-image diffusion model (e.g., audio or text) to assess the method's generalizability beyond the visual domain.