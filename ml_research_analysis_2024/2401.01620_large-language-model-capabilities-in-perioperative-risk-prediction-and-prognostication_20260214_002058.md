---
ver: rpa2
title: Large Language Model Capabilities in Perioperative Risk Prediction and Prognostication
arxiv_id: '2401.01620'
source_url: https://arxiv.org/abs/2401.01620
tags:
- shot
- summary
- patient
- hospital
- admission
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether large language models (LLMs) such
  as GPT-4 Turbo can perform perioperative risk prediction and prognostication tasks
  using patient clinical notes and procedure descriptions. The study examines predictive
  performance on 8 tasks: ASA Physical Status Classification, hospital admission,
  ICU admission, unplanned admission, hospital mortality, PACU Phase 1 duration, hospital
  duration, and ICU duration.'
---

# Large Language Model Capabilities in Perioperative Risk Prediction and Prognostication

## Quick Facts
- arXiv ID: 2401.01620
- Source URL: https://arxiv.org/abs/2401.01620
- Reference count: 40
- Key outcome: GPT-4 Turbo achieves F1 scores of 0.81-0.86 for ICU admission and hospital mortality prediction using clinical notes and procedure descriptions, but struggles with numerical duration prediction tasks

## Executive Summary
This study investigates whether large language models can perform perioperative risk prediction and prognostication tasks using patient clinical notes and procedure descriptions. The researchers examine predictive performance on 8 tasks including ASA classification, hospital admission, ICU admission, mortality, and duration predictions. Results show that LLMs can assist clinicians in perioperative risk stratification for classification tasks with F1 scores of 0.50 for ASA classification, 0.81 for ICU admission, and 0.86 for hospital mortality. Few-shot and chain-of-thought prompting significantly improve performance for several tasks, but LLMs universally struggle with duration prediction tasks.

## Method Summary
The study uses GPT-4 Turbo to perform perioperative risk prediction on 8 tasks using clinical notes and procedure descriptions from EHR data. Researchers employ zero-shot, few-shot, and chain-of-thought prompting strategies, testing both original clinical notes and compressed note summaries. Performance is evaluated using F1 scores for classification tasks and mean absolute error for duration predictions. The dataset includes patients aged 18+ undergoing surgery with anesthesia, with outcomes extracted from EHR records. Balanced datasets are created for each task by downsampling and balancing outcome variables.

## Key Results
- LLMs achieve F1 scores of 0.81-0.86 for ICU admission and hospital mortality prediction
- Few-shot and chain-of-thought prompting improve performance on tasks requiring clinical knowledge synthesis
- Performance on duration prediction tasks (PACU, hospital, ICU length of stay) is universally poor across all prompting strategies
- Note summaries result in performance degradation for some tasks compared to using original notes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 Turbo can perform perioperative risk prediction tasks using clinical notes and procedure descriptions without domain-specific fine-tuning
- Mechanism: The LLM leverages its general language understanding and reasoning capabilities to extract relevant clinical features from narrative text and correlate them with outcome variables through prompting strategies
- Core assumption: The LLM's pretraining on diverse text corpora provides sufficient medical knowledge representation to perform clinical reasoning tasks
- Evidence anchors:
  - [abstract] "Current generation large language models can assist clinicians in perioperative risk stratification on classification tasks"
  - [section 2.2] "We examine predictive performance on 8 different tasks: prediction of ASA Physical Status Classification, hospital admission, ICU admission, unplanned admission, hospital mortality, PACU Phase 1 duration, hospital duration, and ICU duration"
  - [corpus] Weak - no direct corpus evidence found for this specific claim
- Break condition: If the LLM fails to understand medical terminology or cannot extract relevant clinical features from narrative text, performance would degrade significantly below baseline

### Mechanism 2
- Claim: Few-shot and chain-of-thought prompting significantly improve LLM performance on perioperative prediction tasks
- Mechanism: In-context learning provides examples that demonstrate the task pattern, while CoT prompting forces step-by-step reasoning that improves prediction accuracy for tasks requiring clinical judgment
- Core assumption: The LLM can learn task-specific patterns from a small number of examples and benefits from explicit reasoning steps
- Evidence anchors:
  - [abstract] "Few-shot and chain-of-thought prompting improves predictive performance for several of the tasks"
  - [section 3.2] "Few-shot and chain-of-thought prompting reveal significant gains in predictive performance in tasks where synthesizing prior clinical knowledge is important"
  - [section 3.2] "We observe strong overall performance for prediction of postoperative ICU admission and hospital mortality across all prompt strategies"
- Break condition: If the LLM cannot generalize from few examples or if reasoning steps don't improve predictions, prompting strategies would show no benefit over zero-shot approaches

### Mechanism 3
- Claim: LLMs struggle with continuous numerical prediction tasks but excel at binary classification tasks in the perioperative setting
- Mechanism: The discrete tokenization architecture of LLMs makes it difficult to interpolate between numerical values, but classification tasks align well with the model's natural output format
- Core assumption: The LLM's training objective and architecture are better suited for discrete classification than continuous regression
- Evidence anchors:
  - [abstract] "Performance on duration prediction tasks were universally poor across all prompt strategies"
  - [section 3.2] "In their current incarnation, LLMs struggle with regression tasks involving prediction of continuous or numerical outcomes"
  - [section 3.2] "Visualizing PACU phase 1 duration predictions revealed that without few-shot and CoT prompting, LLMs tend to predict quantized outputs"
- Break condition: If the LLM can learn to interpolate numerical values or if the task can be reframed as classification, performance on numerical tasks would improve

## Foundational Learning

- Concept: Understanding of ASA Physical Status Classification system
  - Why needed here: The study uses ASA-PS classification as one of the primary prediction tasks, requiring understanding of what each class represents
  - Quick check question: What are the six ASA Physical Status classes and what clinical characteristics define each class?

- Concept: Prompt engineering techniques (few-shot, chain-of-thought)
  - Why needed here: The study relies heavily on different prompting strategies to improve LLM performance, requiring understanding of how these techniques work
  - Quick check question: How does few-shot prompting differ from chain-of-thought prompting, and when would each be most effective?

- Concept: Evaluation metrics for classification vs regression tasks
  - Why needed here: The study uses different metrics (F1 score vs MAE) for different task types, requiring understanding of when to use each metric
  - Quick check question: When would you use F1 score versus mean absolute error as an evaluation metric?

## Architecture Onboarding

- Component map: EHR data extraction → task-specific dataset creation → prompt generation (procedure info + clinical notes) → LLM inference (GPT-4 Turbo API) → result processing → statistical analysis
- Critical path: Data extraction → Dataset creation → Prompt generation → LLM inference → Result processing → Analysis
- Design tradeoffs: Few-shot vs zero-shot prompting (performance vs cost), note summaries vs original notes (context compression vs information loss)
- Failure signatures: Poor performance on classification tasks indicates issues with prompt formulation or dataset quality; quantized outputs indicate numerical prediction limitations
- First 3 experiments:
  1. Test baseline zero-shot performance on ASA-PS classification with original notes
  2. Compare zero-shot performance using original notes vs note summaries
  3. Test few-shot performance with increasing numbers of examples on hospital admission prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the key architectural or pretraining modifications needed to enable current large language models to effectively predict continuous-valued outcomes such as PACU, hospital, and ICU length of stay?
- Basis in paper: [inferred] The authors explicitly note that LLMs struggle with regression tasks and suggest that the architectural design enforcing discrete tokenized output and lack of robust training for interpolating between numerical values are key limitations.
- Why unresolved: The paper identifies the problem but does not propose specific architectural or pretraining solutions. While it mentions potential strategies like multimodal enhancements and direct mapping of continuous values to embedding spaces, these remain speculative.
- What evidence would resolve it: Experimental results comparing performance of modified LLM architectures (e.g., with continuous-valued output layers, multimodal training, or tool use integration) against standard LLMs on the same perioperative duration prediction tasks.

### Open Question 2
- Question: Does domain-specific fine-tuning of large language models on clinical or EHR text significantly improve performance on perioperative risk prediction tasks compared to general-domain models?
- Basis in paper: [explicit] The authors state that "Future research is needed to evaluate whether clinical domain-specific language models, trained on clinical or EHR text shows improved performance" compared to the general-domain models used in their study.
- Why unresolved: The study only used general-domain models (GPT-4 Turbo) without any fine-tuning. While the models achieved reasonable performance, the potential benefits of domain-specific pretraining or fine-tuning remain untested.
- What evidence would resolve it: Comparative experiments training and evaluating domain-specific LLMs (e.g., on MIMIC-IV data) on the same perioperative prediction tasks, measuring performance differences against the general-domain baseline.

### Open Question 3
- Question: What is the clinical impact of using large language model explanations for perioperative risk stratification in real-world hospital settings?
- Basis in paper: [explicit] The authors note that "LLMs are unique in their ability to present natural language explanations understandable to human clinicians" and suggest this has "significant clinical utility," but they do not empirically evaluate the actual impact on clinical decision-making or patient outcomes.
- Why unresolved: The study focuses on technical performance metrics (F1 scores, MAE) but does not assess how clinicians interact with or benefit from LLM-generated explanations in practice.
- What evidence would resolve it: Prospective clinical studies measuring changes in perioperative risk assessment accuracy, decision quality, or patient outcomes when clinicians use LLM-generated explanations as decision support tools compared to standard practice.

## Limitations
- Retrospective design and single-center data source limit generalizability
- Focus on note-based predictions excludes other potentially valuable clinical data sources
- Performance drop when using note summaries suggests significant information loss during compression
- Consistent failure on numerical prediction tasks represents a significant limitation for comprehensive perioperative prognostication

## Confidence
- High confidence: LLMs can perform binary classification tasks reasonably well for specific perioperative outcomes
- Medium confidence: Few-shot and chain-of-thought prompting improve performance, though results vary by task
- Low confidence: Generalization of these findings to other clinical settings or different LLM architectures

## Next Checks
1. External validation: Test the same prompting strategies on an independent multicenter dataset to assess generalizability across different clinical settings and documentation styles.
2. Clinical utility assessment: Conduct a prospective study measuring clinician decision-making accuracy and time when using LLM predictions versus standard practice, including evaluation of explanation quality.
3. Architecture comparison: Evaluate whether domain-specific fine-tuning or alternative model architectures (such as multimodal models that can incorporate vital signs) can overcome the numerical prediction limitations observed in this study.