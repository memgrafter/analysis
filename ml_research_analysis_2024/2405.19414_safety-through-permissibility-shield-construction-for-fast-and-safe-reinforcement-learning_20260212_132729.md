---
ver: rpa2
title: 'Safety through Permissibility: Shield Construction for Fast and Safe Reinforcement
  Learning'
arxiv_id: '2405.19414'
source_url: https://arxiv.org/abs/2405.19414
tags:
- safety
- safe
- actions
- learning
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a permissibility-based framework for safe reinforcement
  learning that integrates safety and optimality by expanding the definition of permissibility
  to include unsafe actions. The method uses a shield to block actions that are either
  unsafe or non-permissible (cannot lead to optimal solutions), ensuring both safety
  and efficient learning.
---

# Safety through Permissibility: Shield Construction for Fast and Safe Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.19414
- Source URL: https://arxiv.org/abs/2405.19414
- Authors: Alexander Politowicz; Sahisnu Mazumder; Bing Liu
- Reference count: 40
- Key outcome: UNP shielding technique leads to faster convergence to optimal policies while maintaining safety, outperforming baseline methods and other shielding approaches

## Executive Summary
This paper proposes a permissibility-based framework for safe reinforcement learning that integrates safety and optimality by expanding the definition of permissibility to include unsafe actions. The method uses a shield to block actions that are either unsafe or non-permissible (cannot lead to optimal solutions), ensuring both safety and efficient learning. Experiments on three environments (CartPole, Lane Keeping, and FlappyBird) show that the proposed UNP shielding technique leads to faster convergence to optimal policies while maintaining safety, outperforming baseline methods and other shielding approaches. The approach requires less human effort than previous methods and generalizes unsafe behaviors effectively.

## Method Summary
The method introduces a permissibility-based shielding framework that combines unsafe actions with task failure through the concept of Unsafe Non-Permissible (UNP) actions. By defining unsafe actions as receiving minimal reward and terminating the episode, the shield can block both unsafe and non-permissible actions using the same Aunp(s) set. This action-based shielding eliminates the state prediction problem present in previous methods and allows RL agents to generalize unsafe behavior from limited UNP specifications, reducing the need for comprehensive safety specifications.

## Key Results
- UNP shielding technique achieves faster convergence to optimal policies compared to baseline DDQN/DDPG methods
- The approach maintains safety guarantees while improving learning efficiency
- Requires less human effort than traditional safety specification methods due to generalization capabilities
- Outperforms existing shielding approaches in both safety compliance and learning speed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding permissibility to include safety naturally integrates both goals without changing the optimization objective
- Mechanism: By defining unsafe actions as receiving minimal reward and terminating the episode (Assumption 1), unsafe actions become a subset of non-permissible actions. The shield blocks both unsafe and non-permissible actions using the same Aunp(s) set
- Core assumption: Unsafe actions can be treated as failure transitions with minimal reward and episode termination
- Evidence anchors:
  - [abstract]: "This paper shows that safety can be naturally incorporated into this framework, i.e. extending permissibility to include safety"
  - [section]: "Assumption 1 is reasonable because taking an action a ∈ Aunsafe(s) is unsafe and naturally should receive minimal reward and result in failure episode termination"
  - [corpus]: Weak evidence - no direct mention of this specific assumption mechanism
- Break condition: If unsafe actions cannot be modeled as minimal reward failure transitions (e.g., continuous safety violations without termination)

### Mechanism 2
- Claim: Action-based shielding eliminates the state prediction problem present in state-based shielding methods
- Mechanism: Instead of predicting if an action will lead to an unsafe state, the shield simply checks if the action belongs to Aunp(s). This avoids needing knowledge of τ or an accurate model
- Core assumption: It's easier to define unsafe actions than to predict unsafe states
- Evidence anchors:
  - [section]: "Note the distinction between the focus on states (Ssafe) in previous shield construction works and actions (Aunp(s)) in our proposed method"
  - [section]: "Shields that check if an action belongs to Aunsafe(s) avoid these issues"
  - [corpus]: Moderate evidence - similar action-based shielding approaches exist but don't use permissibility framework
- Break condition: If Aunp(s) cannot be efficiently computed or if action-space complexity makes lookup impractical

### Mechanism 3
- Claim: Combining unsafe actions with task failure through UNP bridges generalization in RL and manual user safety specification
- Mechanism: By treating unsafe actions as failures, RL agents can learn to generalize unsafe behavior across the state-action space from limited UNP specifications, reducing the need for comprehensive safety specifications
- Core assumption: RL agents can generalize from limited UNP specifications to cover all unsafe behaviors
- Evidence anchors:
  - [section]: "By combining unsafe actions and task failure through UNP, we have helped bridge the gap between generalization in RL and manual user safety specification"
  - [section]: "Agents can now take any UNP experience and generalize unsafe (or non-permissible) behavior across the state-action space"
  - [corpus]: Moderate evidence - some papers mention generalization but not in this specific context
- Break condition: If the RL agent fails to generalize from UNP specifications to cover all unsafe behaviors

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The entire framework is built on MDP theory where states, actions, rewards, and transitions define the problem
  - Quick check question: What are the five components of an MDP tuple (S, A, τ, R, γ)?

- Concept: Reinforcement Learning (RL) basics
  - Why needed here: The method uses RL algorithms (DDQN, DDPG) and concepts like policies, Q-values, and exploration
  - Quick check question: How does the Bellman equation relate to finding optimal policies in RL?

- Concept: Shielding in RL safety
  - Why needed here: The entire contribution is about a new shield construction method for safe RL
  - Quick check question: What are the two main properties a shield should satisfy (guaranteed correctness and minimal interference)?

## Architecture Onboarding

- Component map:
  - Environment -> Agent selects action -> Shield checks Aunp(s) -> If safe, execute; if unsafe, apply backup policy -> Update RL algorithm -> Repeat

- Critical path: Environment → Agent selects action → Shield checks Aunp(s) → If safe, execute; if unsafe, apply backup policy → Update RL algorithm → Repeat

- Design tradeoffs:
  - UNP shield vs. reward shaping: UNP maintains original optimization objective while reward shaping changes it
  - UNP shield vs. model-based shielding: UNP doesn't require models or τ knowledge
  - Comprehensive vs. non-comprehensive Aunp(s): Non-comprehensive requires RL to generalize but reduces human effort

- Failure signatures:
  - Poor convergence: May indicate incorrect Aunp(s) specification or backup policy issues
  - Safety violations: May indicate incomplete Aunp(s) coverage or model mismatch with Assumption 1
  - Slow learning: May indicate overly restrictive Aunp(s) blocking too many safe actions

- First 3 experiments:
  1. CartPole with simple Aunp(s) based on pole angle thresholds
  2. Lane Keeping with Aunp(s) based on vehicle position relative to road center
  3. FlappyBird with Aunp(s) based on bird position relative to pipe gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the permissibility-based shielding framework generalize to environments with stochastic dynamics or partial observability?
- Basis in paper: [inferred] The paper mentions that current analysis is limited to deterministic MDPs and suggests that other problem formulations would require significant additional design.
- Why unresolved: The paper does not provide experimental results or theoretical analysis for stochastic or partially observable environments.
- What evidence would resolve it: Experiments demonstrating the effectiveness of permissibility-based shielding in environments with stochastic dynamics or partial observability would provide evidence for its generalizability.

### Open Question 2
- Question: Can the permissibility-based shielding framework be extended to multi-agent reinforcement learning problems?
- Basis in paper: [explicit] The paper mentions that permissibility-based shielding extends to multi-agent domains with few adjustments, but more creative ways of adapting permissibility is an interesting challenge left for future work.
- Why unresolved: The paper does not provide experimental results or theoretical analysis for multi-agent reinforcement learning problems.
- What evidence would resolve it: Experiments demonstrating the effectiveness of permissibility-based shielding in multi-agent reinforcement learning problems would provide evidence for its applicability in this domain.

### Open Question 3
- Question: How does the permissibility-based shielding framework compare to other safety techniques in terms of computational efficiency and ease of implementation?
- Basis in paper: [explicit] The paper discusses the advantages of permissibility-based shielding over previous shielding techniques, including reduced human effort, generalization of unsafe behaviors, and no requirement for knowledge of the MDP dynamics or a model.
- Why unresolved: The paper does not provide a comprehensive comparison of computational efficiency and ease of implementation with other safety techniques.
- What evidence would resolve it: A detailed analysis comparing the computational efficiency and ease of implementation of permissibility-based shielding with other safety techniques would provide evidence for its relative advantages.

## Limitations

- The framework assumes unsafe actions cause immediate episode termination, which may not hold for all real-world scenarios where safety violations can occur gradually
- Quality of shielding heavily depends on correctly defining Aunp(s), requiring domain expertise without systematic guidance provided
- Computational overhead and scalability with state-action space complexity are not thoroughly analyzed

## Confidence

- **High Confidence:** The core claim that permissibility can be extended to include safety, and the mathematical framework connecting UNP, Asp(s), and Aunp(s) is well-defined and theoretically sound.
- **Medium Confidence:** The experimental results showing faster convergence with maintained safety are promising, but the limited number of environments (3) and the specific nature of these benchmarks warrant further validation on more diverse and complex tasks.
- **Low Confidence:** The claim about significantly reduced human effort compared to traditional safety specification methods lacks quantitative comparison metrics in the paper.

## Next Checks

1. **Cross-domain validation:** Test UNP shielding on environments where Assumption 1 (unsafe actions cause immediate termination) doesn't hold, such as continuous safety degradation scenarios, to evaluate framework robustness.

2. **Human effort quantification:** Conduct a controlled study comparing the time and expertise required to specify Aunp(s) versus traditional reward shaping or state-based safety specifications across multiple domains.

3. **Computational overhead analysis:** Measure and compare the runtime overhead of UNP shielding versus baseline methods across environments of increasing state-action space complexity to evaluate scalability.