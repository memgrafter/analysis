---
ver: rpa2
title: Why Line Search when you can Plane Search? SO-Friendly Neural Networks allow
  Per-Iteration Optimization of Learning and Momentum Rates for Every Layer
arxiv_id: '2406.17954'
source_url: https://arxiv.org/abs/2406.17954
tags:
- step
- methods
- search
- gradient
- momentum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new class of neural networks, called SO-friendly
  networks, where subspace optimization (SO) can be efficiently used to optimize learning
  rates and momentum rates. The authors demonstrate that for these networks, performing
  line searches or plane searches to set step sizes on each iteration has the same
  asymptotic cost as using a fixed step size.
---

# Why Line Search when you can Plane Search? SO-Friendly Neural Networks allow Per-Iteration Optimization of Learning and Momentum Rates for Every Layer

## Quick Facts
- arXiv ID: 2406.17954
- Source URL: https://arxiv.org/abs/2406.17954
- Authors: Betty Shea; Mark Schmidt
- Reference count: 24
- One-line primary result: Subspace optimization (SO) can efficiently optimize learning and momentum rates per iteration for a new class of neural networks (SO-friendly) without increasing asymptotic iteration cost.

## Executive Summary
This paper introduces SO-friendly neural networks, a class of networks where subspace optimization (SO) can be used to efficiently optimize learning and momentum rates on each iteration without increasing asymptotic iteration cost. The key insight is that for these networks, the cost of evaluating the objective is dominated by matrix multiplications with the data matrix X, allowing SO to be performed using only inexpensive additional operations. The paper demonstrates that this approach can be applied to 2-layer networks with tanh activation, leading to improved convergence compared to traditional optimization methods.

## Method Summary
The method involves training neural networks that are structured to be SO-friendly, where the cost of evaluating the objective function is dominated by matrix multiplications with the data matrix X. Subspace optimization is then used to optimize learning rates and momentum rates for each layer on each iteration. This is done by tracking intermediate products (like Mk = XWk) and solving low-dimensional SO problems using methods like Barzilai-Borwein with line search. The approach is demonstrated on logistic regression and 2-layer neural networks, showing improved performance compared to traditional methods like gradient descent and Adam.

## Key Results
- For SO-friendly networks, performing line searches or plane searches to set step sizes has the same asymptotic cost as using a fixed step size.
- SO can be used to optimize a separate learning rate and momentum rate for each layer, leading to faster convergence.
- Experiments on logistic regression and 2-layer neural networks demonstrate that using line optimization (LO) and subspace optimization (SO) significantly improves performance compared to traditional methods.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: For SO-friendly networks, line search (LS) and plane search (PS) can be performed without increasing asymptotic iteration cost compared to using fixed learning and momentum rates.
- Mechanism: The cost of evaluating the neural network objective is dominated by matrix multiplications with the data matrix X. By tracking intermediate products (like Mk = XWk), the optimization over step sizes only requires additional inexpensive operations (e.g., O(nr) for evaluating the objective at different step sizes), while the bottleneck matrix multiplications remain the same.
- Core assumption: The neural network structure is such that the forward/backward pass through the first layer dominates computational cost, and subsequent layers are cheap in comparison.
- Evidence anchors:
  - [abstract]: "SO-friendly networks have the property that performing a precise line search to set the step size on each iteration has the same asymptotic cost during full-batch training as using a fixed learning."
  - [section 4.1]: "We say that a neural network is SO-friendly if the cost of evaluating f is dominated by the cost of matrix multiplications with X."
- Break condition: If the number of hidden units in the second layer is large enough that the cost of applying the second layer is no longer negligible compared to the first layer, or if the network structure does not allow efficient tracking of intermediate products.

### Mechanism 2
- Claim: SO-friendly networks allow per-layer optimization of learning and momentum rates without increasing asymptotic iteration cost.
- Mechanism: By structuring the update to separately optimize step sizes for each layer while reusing the same intermediate products (Mk), the optimization over multiple step sizes only requires solving a low-dimensional problem (e.g., 4D for a 2-layer network) without additional matrix multiplications. The SO problem is solved efficiently using methods like Barzilai-Borwein with line search.
- Core assumption: The neural network can be decomposed into layers where each layer's parameters can be updated independently while maintaining the efficiency of tracking intermediate products.
- Evidence anchors:
  - [section 4.4]: "Consider now performing LO to set the step size... Analogous to LCPs, if we track the n Ã— r product Mk = XWk then we can perform LO using only two of the bottleneck matrix multiplications per iteration."
  - [section 4.5]: "We can consider adding previous iterations or gradients as additional directions as in supermemory methods, or use a scaling of each layer as in the scaled memory gradient method (7)."
- Break condition: If the SO problem becomes too expensive to solve numerically (e.g., if the function is highly non-convex and local minima are problematic), or if the per-layer optimization leads to instability (e.g., very large step sizes that cause divergence).

### Mechanism 3
- Claim: Augmenting quasi-Newton methods and Adam with SO improves performance without increasing asymptotic iteration cost for SO-friendly networks.
- Mechanism: For quasi-Newton methods, the update can be written to include momentum and optimized step sizes while still only requiring 2 matrix multiplications per iteration. For Adam, adding an additional momentum term or using multiple Adam directions allows SO to optimize step sizes efficiently.
- Core assumption: The structure of the quasi-Newton update and Adam update can be modified to include momentum and optimized step sizes while maintaining the efficiency of tracking intermediate products.
- Evidence anchors:
  - [section 5.1]: "The computational savings for SO in LCPs and SO-frinedly networks are preserved for this 'quasi-Newton with momentum' method... Similar to the GD+M update this only requires 2 matrix multiplications with X per iteration."
  - [section 5.2]: "We could alternately simply use a very-large fixed batch size... since large batch sizes makes SGD behave like a deterministic algorithm up to a fixed accuracy."
- Break condition: If the quasi-Newton or Adam updates cannot be modified to maintain efficiency, or if the additional directions added to Adam do not lead to improved performance.

## Foundational Learning

- Concept: Subspace optimization (SO)
  - Why needed here: SO allows efficient optimization over low-dimensional subspaces, which is crucial for setting step sizes in neural networks without increasing computational cost.
  - Quick check question: What is the key property of SO-friendly networks that allows SO to be performed efficiently?

- Concept: Linear composition problems (LCPs)
  - Why needed here: LCPs are a class of problems where SO can be performed efficiently, and many neural networks can be structured to be SO-friendly in a similar way.
  - Quick check question: How does the structure of LCPs allow SO to be performed efficiently?

- Concept: Matrix multiplications as the bottleneck operation
  - Why needed here: Understanding that matrix multiplications with the data matrix X are the dominant cost in SO-friendly networks is crucial for understanding why SO can be performed efficiently.
  - Quick check question: What is the dominant cost in evaluating the objective function for SO-friendly networks?

## Architecture Onboarding

- Component map:
  - Data matrix X (n x d) -> First layer weights W (d x r) -> Intermediate product Mk = XWk (n x r) -> Second layer weights v (r x 1 for single output) -> Objective function g(h(XW)v)

- Critical path:
  1. Compute Mk = XWk
  2. Compute gradient and direction (e.g., Rk)
  3. Form SO problem using Mk and direction
  4. Solve SO problem to get step sizes
  5. Update weights using optimized step sizes

- Design tradeoffs:
  - Using per-layer step sizes can improve performance but may lead to instability if not regularized.
  - Using momentum can improve performance but requires tracking additional products (e.g., Mk-1).
  - Using more directions in SO can improve performance but increases the cost of solving the SO problem.

- Failure signatures:
  - If the SO problem is not solved accurately enough, the step sizes may not lead to sufficient decrease in the objective.
  - If the per-layer step sizes are too large, the parameters may diverge.
  - If the momentum rates are too large, the updates may oscillate or diverge.

- First 3 experiments:
  1. Implement gradient descent with line search (LS) for a 2-layer network and compare performance to fixed step size.
  2. Implement gradient descent with momentum and line optimization (LO) for a 2-layer network and compare performance to LS.
  3. Implement gradient descent with momentum and plane search (PS) for a 2-layer network and compare performance to LO.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does using subspace optimization (SO) within stochastic gradient descent (SGD) lead to improved convergence compared to traditional SGD variants?
- Basis in paper: [explicit] The paper discusses the potential for using SO within SGD and mentions that recent works show some line search variants converge for over-parameterized models.
- Why unresolved: The paper focuses on deterministic methods and does not provide empirical results for SO within SGD.
- What evidence would resolve it: Experiments comparing the performance of SGD with and without SO on various over-parameterized models and datasets.

### Open Question 2
- Question: Can subspace optimization be efficiently applied to deep neural networks that are not SO-friendly?
- Basis in paper: [explicit] The paper discusses the limitations of SO for general deep neural networks and suggests exploring layer-wise training or partially linearizing the network as potential solutions.
- Why unresolved: The paper does not provide empirical results for applying SO to deep neural networks beyond the SO-friendly cases.
- What evidence would resolve it: Experiments applying SO to deep neural networks using layer-wise training or partial linearization, and comparing the results to traditional training methods.

### Open Question 3
- Question: Is there an optimal strategy for choosing the accuracy of the subspace optimization (SO) problem solver at each iteration?
- Basis in paper: [explicit] The paper mentions that the best performance might be obtained by increasing the accuracy of the SO problem solver as the algorithm runs, but it is not obvious how to optimally choose the accuracy at each step.
- Why unresolved: The paper does not provide a method for choosing the SO solver accuracy or empirical results demonstrating the impact of different accuracy strategies.
- What evidence would resolve it: Experiments comparing the performance of different SO solver accuracy strategies, such as varying the maximum number of iterations allowed based on the optimization stage or using a fixed accuracy throughout the optimization process.

## Limitations

- The definition of SO-friendly networks may not be applicable to all neural network architectures, particularly deeper networks or those with complex layer types.
- The specific implementation details of the subspace optimization solver, including the numerical method used to solve the low-dimensional SO problems, are not provided.
- It is unclear how well the SO approach would generalize to deeper networks or networks with different layer types (e.g., convolutional layers, recurrent layers).

## Confidence

- High Confidence: The theoretical framework for SO-friendly networks and the potential benefits of using SO to optimize learning and momentum rates are well-established.
- Medium Confidence: The experimental results demonstrating improved performance of SO-based methods compared to traditional approaches are promising, but further validation on a wider range of architectures and datasets is needed.
- Low Confidence: The computational efficiency claims, particularly the assertion that SO can be performed without increasing asymptotic iteration cost, are based on specific assumptions about the network architecture and may not hold in general.

## Next Checks

1. Implement the subspace optimization solver used in the experiments, including the numerical method for solving the low-dimensional SO problems. Validate the solver's accuracy and computational efficiency on a range of test problems.
2. Evaluate the performance of SO-based methods on a wider range of neural network architectures, including deeper networks and networks with different layer types (e.g., convolutional layers, recurrent layers). Compare the results to traditional optimization methods.
3. Perform a detailed analysis of the computational efficiency of SO-based methods, including the cost of solving the SO subproblems and the overall iteration time. Compare the results to traditional optimization methods on a range of hardware platforms.