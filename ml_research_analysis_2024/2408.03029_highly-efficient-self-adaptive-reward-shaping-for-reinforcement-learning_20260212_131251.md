---
ver: rpa2
title: Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning
arxiv_id: '2408.03029'
source_url: https://arxiv.org/abs/2408.03029
tags:
- uni00000013
- uni00000014
- reward
- learning
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a self-adaptive reward shaping mechanism
  (SASR) for reinforcement learning in extremely sparse-reward environments. The core
  innovation lies in using success rates from historical experiences, modeled as Beta
  distributions, to provide shaped rewards that evolve from stochastic to deterministic
  as more data accumulates.
---

# Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning

## Quick Facts
- arXiv ID: 2408.03029
- Source URL: https://arxiv.org/abs/2408.03029
- Authors: Haozhe Ma; Zhengding Luo; Thanh Vinh Vo; Kuankuan Sima; Tze-Yun Leong
- Reference count: 37
- Primary result: SASR significantly outperforms relevant baselines in sample efficiency, learning speed, and convergence stability across diverse tasks (MuJoCo, robotics, Atari, physical simulation)

## Executive Summary
This paper introduces a self-adaptive reward shaping mechanism (SASR) for reinforcement learning in extremely sparse-reward environments. The core innovation lies in using success rates from historical experiences, modeled as Beta distributions, to provide shaped rewards that evolve from stochastic to deterministic as more data accumulates. This naturally balances exploration and exploitation. Kernel Density Estimation with Random Fourier Features enables efficient computation of these distributions in high-dimensional continuous state spaces without additional learning models. Experiments across diverse tasks show SASR significantly outperforms relevant baselines in sample efficiency, learning speed, and convergence stability.

## Method Summary
SASR augments standard RL rewards with shaped rewards derived from historical success rates. States from successful and failed trajectories are stored in separate buffers, and KDE with RFF approximates their densities to estimate success/failure counts. These counts parameterize Beta distributions from which stochastic success rates are sampled. The shaped reward is the difference between current and next state success rates. As experience accumulates, Beta distributions concentrate around empirical success rates, transitioning from exploration-promoting stochasticity to exploitation-promoting determinism. SASR integrates with SAC through modified reward computation while maintaining policy invariance.

## Key Results
- SASR achieves faster learning and better final performance than SAC, SAC+SRS, and other baselines on AntStand, HumanStand, and HalfCheetah tasks
- The method demonstrates superior exploration behavior, collecting more diverse samples in early training phases
- SASR shows consistent improvements across continuous control (MuJoCo), robotic manipulation, Atari games, and physical simulation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The self-adaptive evolution from stochastic to deterministic rewards balances exploration and exploitation naturally.
- Mechanism: Success rates are modeled as Beta distributions whose shape parameters (α, β) increase with accumulated experience. Early on, low counts produce high-variance Beta distributions, yielding stochastic rewards that encourage exploration. As counts grow, distributions concentrate around the empirical success rate, yielding deterministic rewards that promote exploitation.
- Core assumption: The Beta distribution adequately represents uncertainty in success rate estimation from limited samples.
- Evidence anchors:
  - [abstract]: "The success rates are sampled from Beta distributions, which dynamically evolve from uncertain to reliable values as data accumulates."
  - [section]: "In the early phases, lower counts of NS(si) and NF (si) result in higher-variance Beta distributions, making the sampled rewards more stochastic... In the later phases, as the counts NS(si) and NF (si) increase, the Beta distribution gradually sharpens."
  - [corpus]: Weak. No direct corpus evidence for Beta distribution self-adaptation; only mentions general reward shaping.

### Mechanism 2
- Claim: KDE with RFF provides an efficient, non-parametric method for estimating success/failure densities in high-dimensional continuous spaces.
- Mechanism: KDE estimates densities ˜dX(si) from stored states in success/failure buffers. RFF approximates the Gaussian kernel via random feature mapping, reducing kernel computation from O(D) to O(M) per state, where D is buffer size and M is feature dimension.
- Core assumption: The Gaussian kernel is appropriate for modeling similarity in the state space.
- Evidence anchors:
  - [abstract]: "We apply Kernel Density Estimation (KDE) with Random Fourier Features (RFF) to derive Beta distributions, providing a computationally efficient solution for continuous and high-dimensional state spaces."
  - [section]: "RFF converts nonlinear kernel computations into linear vector operations, significantly speeding up computation by leveraging the vectorization capabilities of GPUs."
  - [corpus]: Weak. No direct corpus evidence for KDE+RFF in reward shaping; only mentions kernel methods generally.

### Mechanism 3
- Claim: Success rate as a shaped reward aligns with the agent's original objective, providing task-relevant guidance.
- Mechanism: Success rate is defined as the ratio of a state's presence in successful trajectories to total occurrences. This metric directly measures a state's contribution toward task completion, unlike novelty-based exploration bonuses.
- Core assumption: States that frequently appear in successful trajectories are indeed valuable for policy improvement.
- Evidence anchors:
  - [abstract]: "The success rate, defined as the ratio of a state's presence in successful trajectories to its total occurrences, serves as an auxiliary reward distilled from historical experience."
  - [section]: "This success rate assesses a state's contribution toward successful task completion, which closely aligns with the agent's original objectives, offering informative guidance for learning."
  - [corpus]: Weak. No direct corpus evidence for success rate shaping; only mentions task-specific reward shaping generally.

## Foundational Learning

- Concept: Beta distribution and Thompson sampling
  - Why needed here: Beta distributions model uncertainty in success rates, enabling stochastic reward sampling that balances exploration and exploitation.
  - Quick check question: If a state has been visited 3 times in successful trajectories and 7 times in failed trajectories, what are the parameters of its Beta distribution? (Answer: α=3, β=7)

- Concept: Kernel Density Estimation and Random Fourier Features
  - Why needed here: KDE estimates state densities from experience buffers, while RFF provides efficient kernel approximation for high-dimensional spaces.
  - Quick check question: What is the computational complexity of KDE with RFF compared to standard KDE? (Answer: O(M) vs O(D) per state)

- Concept: Reward shaping and potential-based methods
  - Why needed here: Understanding how shaped rewards integrate with environmental rewards without changing optimal policy.
  - Quick check question: What property must a potential function satisfy to preserve optimal policy under reward shaping? (Answer: Potential-based shaping must satisfy the difference between potential functions)

## Architecture Onboarding

- Component map:
  - State buffers (DS, DF) storing states from successful/failed trajectories
  - KDE module with RFF approximation for density estimation
  - Beta distribution sampler for success rate generation
  - SAC backbone with augmented reward function
  - Retention rate controller for buffer management

- Critical path:
  1. Collect transition and trajectory
  2. Store states in DS or DF based on success/failure
  3. Sample batch from replay buffer
  4. Estimate success/failure counts via KDE+RFF
  5. Sample success rate from Beta distribution
  6. Compute augmented reward and update SAC

- Design tradeoffs:
  - Buffer size vs. computational cost: Larger buffers improve density estimates but increase KDE computation
  - RFF dimension M vs. accuracy: Higher M improves kernel approximation but increases computation
  - Retention rate ϕ vs. exploration: Lower ϕ maintains uncertainty but may slow convergence

- Failure signatures:
  - Poor exploration: Shaped rewards too deterministic early on
  - Slow convergence: Beta distributions not sharpening sufficiently
  - High variance: KDE estimates unstable due to small buffer sizes

- First 3 experiments:
  1. Compare SASR vs. SAC on AntStand with identical hyperparameters
  2. Vary retention rate ϕ (0.01, 0.1, 1.0) and measure impact on learning curves
  3. Test SASR with and without Beta sampling on HumanStand to verify exploration benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SASR perform in dense reward environments compared to sparse reward environments?
- Basis in paper: [explicit] The paper mentions that while SASR is designed for sparse-reward environments, in dense-reward settings, the additional shaped rewards may be unnecessary, suggesting a potential area for future research.
- Why unresolved: The paper does not provide experimental results or theoretical analysis of SASR's performance in dense reward environments.
- What evidence would resolve it: Experimental results comparing SASR's performance in both sparse and dense reward environments would clarify its effectiveness and limitations in different reward structures.

### Open Question 2
- Question: What is the impact of the retention rate (ϕ) on SASR's performance, and how can an adaptive retention rate be designed?
- Basis in paper: [explicit] The paper discusses the influence of the retention rate on the confidence level of Beta distributions and mentions that developing an adaptive retention rate or improved buffer management mechanisms is crucial for future improvement.
- Why unresolved: The paper does not provide an adaptive retention rate mechanism or extensive experiments on how different retention rates affect performance.
- What evidence would resolve it: An adaptive retention rate mechanism and experimental results showing the impact of different retention rates on SASR's performance would provide insights into optimal buffer management strategies.

### Open Question 3
- Question: How sensitive is SASR to the choice of kernel function in KDE, and are there more efficient alternatives to Gaussian kernel?
- Basis in paper: [explicit] The paper uses a Gaussian kernel for KDE but does not explore the impact of different kernel functions on performance or computational efficiency.
- Why unresolved: The paper does not provide a comparative analysis of different kernel functions or their effects on SASR's performance and efficiency.
- What evidence would resolve it: Experimental results comparing SASR's performance with different kernel functions would clarify the impact of kernel choice on effectiveness and efficiency.

## Limitations

- The method assumes stationary task structure; performance may degrade when optimal policies change over time
- Computational efficiency claims rely on RFF approximations that may not scale well to extremely high-dimensional state spaces
- The Beta distribution approximation for success rate uncertainty may be inadequate in highly non-stationary environments

## Confidence

- High confidence: The basic mechanism of using historical success rates as shaped rewards (proven theoretically sound for policy invariance)
- Medium confidence: The self-adaptive properties of Beta distribution sampling for exploration-exploitation balance (empirical evidence provided but limited ablation studies)
- Low confidence: The computational efficiency claims regarding KDE+RFF in very high-dimensional spaces (no scalability analysis beyond demonstrated tasks)

## Next Checks

1. Conduct ablation studies on AntStand task with (a) deterministic success rates (no Beta sampling), (b) fixed small exploration bonus, and (c) random reward shaping to isolate the contribution of self-adaptive stochasticity.

2. Test SASR on a non-stationary sparse-reward task where optimal behavior changes during training to evaluate robustness to concept drift in success rate estimates.

3. Perform computational complexity analysis by scaling state dimensionality and buffer sizes on a synthetic task to verify the claimed efficiency gains of KDE with RFF versus standard KDE approaches.