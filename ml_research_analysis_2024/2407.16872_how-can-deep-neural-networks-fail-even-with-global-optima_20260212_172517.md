---
ver: rpa2
title: How Can Deep Neural Networks Fail Even With Global Optima?
arxiv_id: '2407.16872'
source_url: https://arxiv.org/abs/2407.16872
tags:
- function
- hidden
- where
- neural
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the question of whether global optima in
  deep neural network training always guarantee good generalization. The paper proposes
  a simple method to extend the expressive power of shallow networks to any depth
  while keeping the width of subsequent hidden layers minimal.
---

# How Can Deep Neural Networks Fail Even With Global Optima?
## Quick Facts
- arXiv ID: 2407.16872
- Source URL: https://arxiv.org/abs/2407.16872
- Authors: Qingguang Guan
- Reference count: 23
- Primary result: This work shows that achieving global optima (zero training loss) in deep neural networks does not guarantee good generalization, constructing examples where networks perform poorly on unseen data despite perfect training accuracy.

## Executive Summary
This paper investigates whether global optima in deep neural network training always guarantee good generalization. The authors propose a method to extend the expressive power of shallow networks to any depth while keeping subsequent hidden layers minimal. Using this technique, they construct examples of binary classification and function approximation problems where neural networks achieve global optima (zero training loss) but perform poorly on unseen data. The key finding is that even with global optima, the model can only make accurate predictions for data points near the training set, failing to generalize to other regions of the input space. This result holds for various activation functions and is demonstrated for both low-dimensional and high-dimensional problems.

## Method Summary
The paper proposes a simple trick to extend the expressive power of shallow neural networks to deeper networks without loss of approximation accuracy. The method involves attaching additional hidden layers with minimal width and using a specific input construction that preserves the original shallow network's output through Taylor expansion. The authors then construct classification and approximation problems where the networks achieve zero training loss by creating basis functions with compact support centered at training points. By setting biases close to 1 (or d for higher dimensions), the network output is forced to zero almost everywhere except near training samples, demonstrating poor generalization despite perfect training performance.

## Key Results
- Global optima (zero training loss) can be achieved in deep neural networks while the model performs poorly on unseen data
- The failure phenomenon occurs for various activation functions including ReLU, Parametric ReLU, and Sigmoid
- The poor generalization is not limited to low-dimensional problems but extends to high-dimensional spaces as well

## Why This Works (Mechanism)
### Mechanism 1
- Claim: By attaching additional hidden layers with minimal width and using the simple input construction described in equations (2.1)-(2.3), the expressive power of a shallow network can be preserved in a deep network without loss of approximation accuracy.
- Mechanism: The proof uses Taylor expansion of the activation function around a constant c to show that the input sequence generated in (2.1)-(2.2) converges back to the original shallow network output after being processed through the added layers. The small parameter ϵ controls the error, which can be made arbitrarily small.
- Core assumption: The activation function must have a bounded second derivative and a non-zero first derivative at c, ensuring the Taylor expansion is valid and the recurrence in (2.5) converges.
- Evidence anchors: [abstract] "extend the expressive power of shallow neural networks to networks of any depth using a simple trick", [section] "Theorem 2.1. Suppose a bounded scalar valued function f(x), x ∈ Rd, d ≥ 1 can be approximated by a fully connected neural network with K hidden layers, K ≥ 1, then after attaching N extra hidden layers with any width ≥ 1, the function can still be approximated by the deep neural network with K + N hidden layers."

### Mechanism 2
- Claim: For certain constructed classification and approximation problems, global optima can be achieved (zero training loss) while the network output is zero outside a small region around the training data, leading to poor generalization.
- Mechanism: The construction uses basis functions with compact support (equations like (3.1) and (4.3)) that are zero except near training points. By setting biases b1 and b2 close to 1 (or d for higher dimensions), the network output is forced to zero almost everywhere except near training samples, even though it fits the training data perfectly.
- Core assumption: The training data can be placed on a grid and the network width is sufficient to construct the required number of non-overlapping basis functions.
- Evidence anchors: [abstract] "construct examples of binary classification and function approximation problems where the neural networks achieve global optima (zero training loss) but still perform poorly on unseen data", [section] "Proposition 3.1.1...there exist weights and biases such that the loss function is zero...if dis (x, {xi}N i=1) > ϵ , then f2(x) = 0"

### Mechanism 3
- Claim: The failure phenomenon holds for different activation functions (ReLU, Parametric ReLU, Sigmoid) and in high-dimensional spaces, showing the result is not dependent on a specific architecture.
- Mechanism: The paper provides separate constructions for each activation type, adjusting the basis functions and the second hidden layer inputs accordingly. For Sigmoid, the construction uses large K to create narrow spikes, and large L to truncate negative values, achieving similar zero-outside-training-region behavior.
- Core assumption: Each activation function can be manipulated (via parameters like K, L, α, or c) to create the required compact-support-like behavior for the constructed examples.
- Evidence anchors: [abstract] "Different types of activation functions are considered, including ReLU, Parametric ReLU, and Sigmoid functions", [section] "In Section 5, we only consider function approximation problems for networks with Sigmoid activation functions" and corresponding propositions

## Foundational Learning
- Concept: Universal approximation theorem for shallow networks
  - Why needed here: The paper builds on the fact that shallow networks can approximate any continuous function, then extends this to deep networks using the simple trick.
  - Quick check question: Can a single hidden layer network with enough neurons approximate any continuous function on a compact set? (Yes, by the universal approximation theorem)

- Concept: Compact support and basis functions in neural networks
  - Why needed here: The failure examples rely on constructing basis functions (like equations (3.1) and (4.3)) that are non-zero only in small regions around training points.
  - Quick check question: What property of a function means it is zero outside a bounded interval? (Compact support)

- Concept: Global optima vs. generalization in optimization
  - Why needed here: The core finding is that achieving zero training loss (global optima) does not guarantee good performance on unseen data.
  - Quick check question: If a model achieves zero training error, does it always perform well on test data? (No, it can overfit)

## Architecture Onboarding
- Component map: Input layer (d neurons) -> Hidden layer 1 (H1 neurons) -> Hidden layer 2 (H2 neurons) -> Output layer (1 neuron)
- Critical path:
  1. Define training set on a uniform grid in [0,1]^d
  2. Construct basis functions with compact support centered at each training point
  3. Set up second hidden layer inputs as linear combinations of basis functions minus biases
  4. Choose biases close to 1 (or d) to force network output to zero outside training regions
  5. Verify zero training loss and poor generalization
- Design tradeoffs:
  - Width vs. depth: The construction requires specific minimum widths but can add arbitrary depth without changing the output
  - Bias values: Closer to 1/d gives worse generalization but still zero training loss
  - Activation function choice: Different functions require different basis function constructions
- Failure signatures:
  - Perfect training accuracy but near-zero output on most of the input space
  - Output only spikes near training points, zero elsewhere
  - Model cannot classify or approximate any point not extremely close to training data
- First 3 experiments:
  1. Implement the 1D binary classification example (Proposition 3.1.1) with N=6, verify f2(x) is zero except near training points
  2. Try the 2D classification example (Proposition 3.2.1) with N=4, visualize the "spikes" using a density plot
  3. Test the Sigmoid function approximation (Example 5.1) with varying b and K to observe the transition from good to poor approximation

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can we characterize the precise relationship between the number of hidden layers and the generalization performance of deep neural networks with global optima?
- Basis in paper: [explicit] The paper demonstrates that adding more hidden layers to networks with global optima does not necessarily improve generalization performance.
- Why unresolved: While the paper shows that increasing depth can maintain poor generalization, it does not provide a comprehensive framework for understanding the trade-offs between depth and generalization.
- What evidence would resolve it: Theoretical analysis and empirical studies comparing generalization performance across networks with varying depths but fixed global optima.

### Open Question 2
- Question: Can the phenomenon of poor generalization with global optima be generalized to other types of neural network architectures, such as Recurrent Neural Networks and Convolutional Neural Networks?
- Basis in paper: [explicit] The paper concludes by mentioning the intention to extend the analysis to Recurrent Neural Networks and Convolutional Neural Networks.
- Why unresolved: The current study focuses on fully connected deep neural networks, leaving the generalization of results to other architectures as an open question.
- What evidence would resolve it: Empirical and theoretical investigations into the generalization capabilities of RNNs and CNNs with global optima.

### Open Question 3
- Question: What are the implications of the findings on global optima and generalization for the design of neural network architectures and training algorithms?
- Basis in paper: [inferred] The paper highlights the limitations of relying solely on global optima for deep neural network training, suggesting a need for better understanding and control of generalization.
- Why unresolved: While the paper identifies the issue, it does not provide concrete recommendations for architectural or algorithmic changes to address the problem.
- What evidence would resolve it: Development and evaluation of new architectures or training methods that explicitly account for generalization beyond achieving global optima.

## Limitations
- The constructions rely on specific network architectures with carefully chosen widths and biases, which may not reflect typical training scenarios
- The examples assume training data placed on uniform grids, which is an idealized scenario
- The mechanisms require precise parameter tuning (like bias values close to 1) that may be difficult to achieve in practice

## Confidence
- High confidence in the theoretical proofs showing the construction method works as described
- Medium confidence in the practical relevance of these specific failure modes, as they represent carefully constructed edge cases
- Low confidence in the extent to which these phenomena occur in real-world training scenarios without deliberate manipulation

## Next Checks
1. Implement the 1D binary classification example and verify the network outputs are zero outside small regions around training points while maintaining zero training loss
2. Test the sensitivity of the constructions to perturbations in bias values to determine how robust these failure modes are to parameter variations
3. Apply standard optimization algorithms (SGD, Adam) to the constructed problems to see if they can naturally discover the global optima described in the paper