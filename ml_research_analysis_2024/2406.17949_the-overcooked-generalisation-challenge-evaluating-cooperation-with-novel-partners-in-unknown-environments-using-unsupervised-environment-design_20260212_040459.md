---
ver: rpa2
title: 'The Overcooked Generalisation Challenge: Evaluating Cooperation with Novel
  Partners in Unknown Environments Using Unsupervised Environment Design'
arxiv_id: '2406.17949'
source_url: https://arxiv.org/abs/2406.17949
tags:
- learning
- agents
- training
- environment
- layouts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Overcooked Generalisation Challenge (OGC) is a novel benchmark
  for evaluating cooperative reinforcement learning agents on their ability to generalise
  to both novel environments and unknown partners simultaneously. Unlike prior work
  that treats these challenges separately, OGC combines unsupervised environment design
  with multi-agent coordination, providing the first open-source testbed for this
  dual generalisation problem.
---

# The Overcooked Generalisation Challenge: Evaluating Cooperation with Novel Partners in Unknown Environments Using Unsupervised Environment Design

## Quick Facts
- arXiv ID: 2406.17949
- Source URL: https://arxiv.org/abs/2406.17949
- Authors: Constantin Ruhdorfer; Matteo Bortoletto; Anna Penzkofer; Andreas Bulling
- Reference count: 40
- Primary result: Current methods fail to produce agents that generalise effectively to novel layouts and partners in the Overcooked Generalisation Challenge

## Executive Summary
The Overcooked Generalisation Challenge (OGC) introduces a novel benchmark for evaluating cooperative reinforcement learning agents on their ability to generalise to both novel environments and unknown partners simultaneously. Unlike prior work that treats these challenges separately, OGC combines unsupervised environment design with multi-agent coordination, providing the first open-source testbed for this dual generalisation problem. Extensive experiments with state-of-the-art DCD algorithms (DR, PLR, PAIRED, ACCEL) and scalable architectures (CNN-LSTM, SoftMoE-LSTM, CNN-S5) reveal that current methods fail to produce agents that generalise effectively to novel layouts and partners, with only PAIRED with SoftMoE-LSTM achieving partial success.

## Method Summary
OGC extends Overcooked-AI to support procedural training via dual curriculum design methods, generating diverse layouts with complex spatial dependencies and object interactions. The benchmark evaluates agents across three settings: self-play on transformed layouts, ad-hoc teamwork with diverse policy populations, and zero-shot coordination with independently trained partners. JAX-accelerated training with MAPPO algorithm uses 32 parallel environments for 30,000 iterations. Agents tested include CNN-LSTM, SoftMoE-LSTM, and CNN-S5 architectures, with DCD methods generating training layouts via teacher policies or layout mutation.

## Key Results
- Current DCD algorithms (DR, PLR, PAIRED, ACCEL) struggle to train agents that generalise across novel layouts and unknown partners
- PAIRED with SoftMoE-LSTM is the only method showing partial success in the joint generalisation challenge
- Agents consistently fail on layouts with narrow corridors or large object distances, revealing inability to learn spatially invariant coordination strategies
- Performance drops significantly when tested on systematically transformed layouts (mirrored, rotated, squeezed)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OGC's procedural layout generation exposes agents to a wider range of spatial coordination structures than fixed-layout benchmarks
- Mechanism: The dual curriculum design framework procedurally places walls, objects, and agents on a grid during training, creating diverse task geometries that require novel coordination strategies
- Core assumption: Procedural generation creates layouts with varying spatial complexities and interaction bottlenecks that cannot be covered by a small set of fixed layouts
- Evidence anchors:
  - [abstract] "Compared to prior DCD benchmarks, where designers manipulate only minimal elements of the environment, OGC introduces a significantly richer design space: full kitchen layouts with multiple objects that require the designer to account for interaction dynamics between agents."
  - [section 2.2] "The OGC thus contributes the first open-source cooperative multi-agent UED environment in which agents are exposed to novel partners during evaluation."
  - [corpus] Weak: No direct evidence that procedural generation produces more diverse layouts than fixed layouts, but the claim is plausible given the design space description
- Break condition: If the procedural generator consistently produces only a narrow subset of layout types (e.g., only simple corridor-based designs), the diversity benefit disappears

### Mechanism 2
- Claim: The teacher-based environment generation in OvercookedUED creates adaptive difficulty by placing objects based on agent performance
- Mechanism: A generator policy sequentially places objects onto a layout grid, with the teacher selecting positions that match the agent's current capabilities, creating a curriculum of increasing complexity
- Core assumption: The generator can effectively estimate agent capabilities and adjust layout difficulty accordingly to provide appropriate learning signals
- Evidence anchors:
  - [section 4.2] "OvercookedUED implements a teacher environment where a generator policy sequentially places objects onto a layout grid. At each step t, the teacher selects a grid cell and places one object from a fixed sequence."
  - [abstract] "This enables us to evaluate generalisation in a more challenging and realistic way, where agents encounter entirely novel environments without prior exposure."
  - [corpus] Weak: No direct evidence that the teacher-based approach actually produces effective curricula, but the mechanism is theoretically sound
- Break condition: If the teacher policy fails to generate solvable layouts or produces layouts that are consistently too easy/hard, the adaptive curriculum breaks down

### Mechanism 3
- Claim: The combination of environment and partner generalisation creates a more realistic test of cooperative AI than either challenge alone
- Mechanism: Agents must simultaneously adapt to new spatial layouts and new partner behaviors, requiring them to learn coordination strategies that transfer across both dimensions rather than memorizing specific patterns
- Core assumption: Real-world cooperation requires flexibility across both environmental and social dimensions, and training on only one dimension is insufficient preparation
- Evidence anchors:
  - [abstract] "The OGC poses a strictly harder challenge: agents must learn to cooperate in entirely novel environments and with unseen partners, with no prior exposure to evaluation layouts or their structure."
  - [section 2.3] "While many benchmarks focus on either environment or partner generalisation (Lowe et al., 2017; Foerster et al., 2018; Hu et al., 2020), few evaluate both simultaneously."
  - [corpus] Weak: No direct evidence that joint generalisation is more realistic, but the claim aligns with real-world requirements
- Break condition: If agents can achieve good performance by learning separate strategies for environment and partner generalisation (rather than integrated strategies), the joint challenge may not be as fundamental as claimed

## Foundational Learning

- Concept: Unsupervised Environment Design (UED)
  - Why needed here: OGC uses UED to generate training layouts procedurally rather than relying on fixed test layouts, creating a more challenging and realistic generalization problem
  - Quick check question: What is the key difference between UED and domain randomisation in terms of level generation?

- Concept: Dual Curriculum Design (DCD)
  - Why needed here: DCD provides the framework for co-evolving the environment generator (designer) and level selector (curator) to construct adaptive training curricula
  - Quick check question: How does the teacher-based approach in DCD differ from the random approach when generating training layouts?

- Concept: Zero-Shot Coordination (ZSC)
  - Why needed here: OGC evaluates agents on their ability to coordinate with partners they haven't trained with, which is fundamental to real-world human-AI collaboration
  - Quick check question: Why does self-play often fail to produce agents that generalize to novel partners in zero-shot coordination settings?

## Architecture Onboarding

- Component map: OvercookedUED -> Overcooked Mutator -> JaxMARL -> minimax -> Policy architectures (CNN-LSTM, SoftMoE-LSTM, CNN-S5)

- Critical path:
  1. Initialize OvercookedUED with teacher policy
  2. Generate training layouts procedurally during DCD training
  3. Train agents using MAPPO across generated layouts
  4. Evaluate on human-authored test layouts in self-play, ad-hoc teamwork, and zero-shot coordination settings

- Design tradeoffs:
  - Fixed vs variable layout sizes: Fixed sizes enable JAX acceleration but limit layout diversity
  - Teacher vs random generation: Teacher provides adaptive curricula but requires learning; random is simpler but may produce less useful layouts
  - Architecture complexity: SoftMoE-LSTM showed best performance but requires more parameters and training time

- Failure signatures:
  - Agents consistently fail on layouts with narrow corridors or large object distances
  - Performance drops significantly when tested on systematically transformed layouts (mirrored, rotated, squeezed)
  - Agents achieve good training returns but poor generalization to evaluation layouts

- First 3 experiments:
  1. Run a single layout with random agent policies to verify the environment mechanics work correctly
  2. Train a simple CNN-LSTM agent on a small set of procedurally generated layouts and evaluate on fixed test layouts
  3. Compare performance of teacher-based vs random layout generation on a fixed agent architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can unsupervised environment design methods be improved to generate training layouts that better prepare agents for the complex coordination challenges found in Overcooked evaluation levels?
- Basis in paper: [explicit] "Our results reveal a key limitation: current dual curriculum design (DCD) methods with hand-crafted or weak procedural designers fail to generate sufficiently diverse and structured training levels"
- Why unresolved: The paper demonstrates that current methods produce many unsolvable or trivially easy layouts, and PAIRED is the only method showing partial success, but the specific mechanisms for generating more effective training environments remain unclear
- What evidence would resolve it: Comparative experiments testing different level generation strategies (e.g., constraint-based generation, reward shaping during level creation, or learned generators with stronger inductive biases) that show improved generalization performance on evaluation layouts

### Open Question 2
- Question: What architectural modifications could enable agents to develop spatially invariant coordination strategies that generalize across layout transformations?
- Basis in paper: [explicit] "performance fluctuates widely... revealing that agents fail to learn spatially invariant coordination strategies" and "current methods often overfit to superficial spatial patterns rather than acquiring abstract cooperation skills"
- Why unresolved: While the paper identifies this failure mode and shows that SoftMoE-LSTM performs best among tested architectures, it does not explore architectural innovations specifically designed to capture spatial invariance or abstract coordination patterns
- What evidence would resolve it: Experiments comparing standard architectures against those incorporating explicit spatial reasoning modules, attention mechanisms focused on task-relevant spatial relationships, or graph-based representations of kitchen layouts

### Open Question 3
- Question: How can zero-shot coordination be improved when agents must simultaneously generalize to both novel environments and unknown partners?
- Basis in paper: [explicit] "even state-of-the-art DCD algorithms struggle to train agents that generalise across layouts and partners" and "current methods often overfit to superficial spatial patterns"
- Why unresolved: The paper establishes this as the core challenge but does not test specific techniques for joint environment-partner generalization, such as meta-learning approaches, adaptive partner modeling, or curriculum designs that explicitly vary partner diversity alongside environmental complexity
- What evidence would resolve it: Experiments showing significant performance gains when using methods that explicitly model partner uncertainty, adapt to partner behavior during evaluation, or use partner-aware curriculum generation compared to standard DCD approaches

## Limitations
- The effectiveness of procedural layout generation versus expanded fixed layouts has not been directly validated through empirical comparison
- The teacher-based curriculum generation's effectiveness in producing appropriately challenging layouts remains untested against simpler random generation methods
- No direct evidence that the 32 transformed layouts provide additional generalization signal beyond what the 5 human-authored layouts offer

## Confidence
- **High Confidence**: The experimental results showing current DCD methods fail to produce agents that generalize effectively to novel layouts and partners
- **Medium Confidence**: The claim that joint environment-partner generalization is more realistic and challenging than either alone
- **Low Confidence**: The assertion that procedural layout generation inherently provides superior diversity compared to fixed layouts

## Next Checks
1. Conduct a systematic comparison of layout diversity between procedural generation and an expanded set of fixed layouts, measuring coverage of spatial coordination structures and interaction patterns
2. Compare teacher-based layout generation against random generation by training agents with both methods and measuring learning curves and final generalization performance
3. Test whether the 32 transformed layouts (mirrored, rotated, squeezed) actually provide additional generalization signal beyond what the 5 human-authored layouts offer, or if they simply measure the same capabilities