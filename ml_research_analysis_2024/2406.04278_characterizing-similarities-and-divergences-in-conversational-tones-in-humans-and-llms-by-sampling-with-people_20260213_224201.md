---
ver: rpa2
title: Characterizing Similarities and Divergences in Conversational Tones in Humans
  and LLMs by Sampling with People
arxiv_id: '2406.04278'
source_url: https://arxiv.org/abs/2406.04278
tags:
- conversational
- tone
- tones
- human
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to characterize conversational tones
  in humans and LLMs by iteratively eliciting sentences and tone labels from both
  groups. Humans and GPT-4 alternately generated sentences based on a conversational
  tone and labeled the tone of given sentences.
---

# Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People

## Quick Facts
- arXiv ID: 2406.04278
- Source URL: https://arxiv.org/abs/2406.04278
- Reference count: 40
- One-line primary result: Humans and GPT-4 show different conversational tone distributions, with some tones highly aligned and others poorly aligned

## Executive Summary
This paper introduces a novel iterative method for characterizing conversational tones in humans and large language models (LLMs). By employing a Gibbs sampling-inspired approach where participants alternately generate sentences based on tones and identify tones in sentences, the study creates a shared dataset of 80 sentences and 40 tones. The research reveals significant differences in tone distributions between humans and GPT-4, with some tones like "joyful" and "pleased" showing high alignment while others like "proud" and "apologetic" show low alignment. The quality-of-fit ratings from 1,339 human participants enable the creation of interpretable geometric representations of tone relations and provide a benchmark for semantic alignment methods.

## Method Summary
The study employs a Sampling with People (SP) paradigm inspired by cognitive science methods. Participants alternate between two tasks: identifying the tone of a given sentence and generating a sentence based on a given tone. This iterative process, resembling Gibbs sampling, is conducted with both humans and GPT-4 to create a shared dataset. After collecting sentences and tones, quality-of-fit ratings are gathered where participants evaluate how well each tone matches each sentence on a Likert scale. These ratings are used to construct semantic embeddings through correlation matrices, which are then projected into a 2D space using Multidimensional Scaling (MDS) to visualize the relationships between tones in humans and GPT-4.

## Key Results
- Human and GPT-4 tone distributions differ significantly, with some tones highly aligned (e.g., "joyful," "pleased") and others poorly aligned (e.g., "proud," "apologetic")
- Bilingual Lexicon Induction outperforms other semantic alignment methods when benchmarked on the collected data
- The MDS projection creates an interpretable geometric representation showing tone relations within and across humans and GPT-4
- The method enables cross-cultural tone comparison and can be applied to low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The iterative Gibbs sampling process alternates between sentence generation and tone labeling, enabling both humans and GPT to explore the joint distribution of sentences and tones.
- Mechanism: By constraining each participant to see only the output of the previous iteration, the process satisfies the Markovian property required for Gibbs sampling, ensuring that the final sample is representative of the true distribution.
- Core assumption: The alternating tasks (S and T) are valid sampling operations from the conditional distributions p(S|T) and p(T|S), respectively.
- Evidence anchors:
  - [abstract]: "Inspired by methods from cognitive science, we propose an iterative method for simultaneously eliciting conversational tones and sentences, where participants alternate between two tasks: (1) one participant identifies the tone of a given sentence and (2) a different participant generates a sentence based on that tone."
  - [section]: "Under a probability theory framework, “S” and “T” tasks are essentially sampling operations, respectively from the conditional distribution of a sentence given a conversational tone p(S|T), and the conditional distribution of a tone given a sentence p(T|S)."

### Mechanism 2
- Claim: The quality-of-fit ratings provide a quantitative measure of the alignment between tones and sentences, allowing for the creation of semantic embeddings.
- Mechanism: By having independent raters evaluate all sentence-tone pairs, the ratings capture the degree to which each tone matches each sentence, resulting in a vector representation of tone similarity.
- Core assumption: The Likert scale ratings accurately capture the strength of the relationship between tones and sentences.
- Evidence anchors:
  - [abstract]: "In an additional experiment, humans and GPT-4 annotated all sentences with all tones. With data from 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4 queries, we show how our approach can be used to create an interpretable geometric representation of relations between conversational tones in humans and GPT-4."
  - [section]: "Specifically, after SP sampling, we collect quality-of-fit ratings of all sentences with all tones... We use these to construct a geometric embedding that can be used to evaluate the alignment between human and LLM conversational tones."

### Mechanism 3
- Claim: The MDS projection of the cross-domain correlation matrix reveals the geometric structure of tone relations, enabling direct comparison between humans and GPT.
- Mechanism: By projecting the high-dimensional tone embeddings into a 2D space, MDS preserves the relative distances between tones, allowing for visual interpretation of similarities and differences.
- Core assumption: The cross-domain correlation matrix accurately captures the relationships between tones across humans and GPT.
- Evidence anchors:
  - [abstract]: "With data from 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4 queries, we show how our approach can be used to create an interpretable geometric representation of relations between conversational tones in humans and GPT-4."
  - [section]: "We use the resulting cross-domain similarity matrices to obtain a geometric representation of both human and GPT data within the same space... The space thus represents not only the relation between tones within humans but also the way they relate to GPT."

## Foundational Learning

- Concept: Gibbs Sampling
  - Why needed here: The iterative process of alternating between sentence generation and tone labeling is an instance of Gibbs sampling, which is used to explore the joint distribution of sentences and tones.
  - Quick check question: What is the key property that the alternating tasks must satisfy for the Gibbs sampling process to be valid?

- Concept: Multidimensional Scaling (MDS)
  - Why needed here: MDS is used to project the high-dimensional tone embeddings into a 2D space, preserving the relative distances between tones and enabling visual interpretation.
  - Quick check question: What is the main goal of MDS in this context, and how does it achieve this goal?

- Concept: Semantic Alignment
  - Why needed here: The quality-of-fit ratings and MDS projection are used to create semantic embeddings that capture the relationships between tones, enabling comparison between humans and GPT.
  - Quick check question: What is the main purpose of creating semantic embeddings in this study, and how do they facilitate comparison between humans and GPT?

## Architecture Onboarding

- Component map:
  1. Sampling with People (SP) experiments: Collect sentences and tones from humans and GPT.
  2. Quality-of-fit rating experiments: Have independent raters evaluate all sentence-tone pairs.
  3. MDS projection: Project the high-dimensional tone embeddings into a 2D space.
  4. Semantic alignment benchmarking: Compare the performance of different alignment methods.

- Critical path:
  1. Run SP experiments to collect sentences and tones.
  2. Conduct quality-of-fit rating experiments.
  3. Compute cross-domain correlation matrix.
  4. Perform MDS projection.
  5. Benchmark semantic alignment methods.

- Design tradeoffs:
  - Using a finite number of sampling chains with only 100 generations may limit the representativeness of the final sample.
  - Employing a shared array of conversational tones enables direct comparability but may not capture all possible tone variations.
  - Conducting experiments with participants from a single country (UK) limits the generalizability of the results.

- Failure signatures:
  - If the SP experiments do not generate a diverse set of sentences and tones, the final sample may not be representative.
  - If the quality-of-fit ratings are not reliable or do not accurately capture the tone-sentence relationship, the resulting embeddings may not be meaningful.
  - If the MDS projection does not accurately preserve the relative distances, the resulting geometric representation may not be meaningful.

- First 3 experiments:
  1. Run a pilot SP experiment with a small number of participants to test the task instructions and data collection process.
  2. Conduct a quality-of-fit rating experiment with a subset of sentence-tone pairs to assess the reliability of the ratings.
  3. Perform an MDS projection on a subset of the data to test the visualization and interpretation of the geometric representation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are the observed differences in conversational tone alignment between humans and GPT-4 across different LLM architectures and training paradigms?
- Basis in paper: [explicit] The paper acknowledges that it only tested GPT-4 and did not explore other LLMs or parameters like temperature, limiting generalizability.
- Why unresolved: The study used a single LLM model, so the observed patterns may not generalize to other architectures or training methods.
- What evidence would resolve it: Testing multiple LLM models with varying architectures and training approaches to see if the tone alignment patterns hold.

### Open Question 2
- Question: Can the Sampling with People method be effectively applied to study conversational tones in low-resource languages where large text corpora are unavailable?
- Basis in paper: [inferred] The paper mentions that their approach can be used with participants in any language, which could help create AI systems for low-resource languages.
- Why unresolved: The method's effectiveness in low-resource language contexts is not demonstrated or tested in the current study.
- What evidence would resolve it: Conducting the Sampling with People experiments with speakers of low-resource languages to create conversational tone datasets and evaluate alignment.

### Open Question 3
- Question: How do conversational tone representations differ across cultures, and can these differences be captured using the proposed method?
- Basis in paper: [explicit] The paper states that the method can be extended to study cross-cultural differences in tone of voice.
- Why unresolved: The study only tested participants in the UK, so cross-cultural variations are not explored.
- What evidence would resolve it: Applying the method to participants from diverse cultural backgrounds and comparing the resulting conversational tone representations.

### Open Question 4
- Question: What is the impact of increasing the number of iterations and sampling chains in the Sampling with People method on the quality and diversity of the elicited conversational tones and sentences?
- Basis in paper: [inferred] The paper used 90 chains with 100 iterations each but acknowledges this as a limitation, suggesting potential for further exploration.
- Why unresolved: The study used a fixed number of iterations and chains, so the effect of varying these hyperparameters is unknown.
- What evidence would resolve it: Experimenting with different numbers of iterations and chains to assess their impact on the diversity and representativeness of the elicited data.

## Limitations
- The iterative Gibbs sampling process uses only 100 generations across finite sampling chains, potentially limiting the representativeness of the final sample
- The study uses a fixed set of 40 conversational tones, which may not capture the full spectrum of human tone variations
- Experiments were conducted with participants from a single country (UK), limiting generalizability to cross-cultural tone interpretations

## Confidence

- **High Confidence**: The core finding that human and GPT-4 tone distributions differ significantly, with some tones like "joyful" and "pleased" showing high alignment while others like "proud" and "apologetic" show low alignment. This is supported by extensive human judgments (33,370) and GPT-4 queries (29,900).
- **Medium Confidence**: The validity of the Gibbs sampling approach for tone-sentence joint distribution exploration. While the theoretical framework is sound, the practical implementation details and finite sampling constraints introduce uncertainty.
- **Medium Confidence**: The semantic alignment benchmarking results, particularly the superiority of Bilingual Lexicon Induction. While methodologically rigorous, the specific performance metrics and comparisons could benefit from additional validation.

## Next Checks

1. **Replication with expanded sampling**: Run additional sampling chains with increased iterations (e.g., 200-500 generations) to assess the stability and representativeness of the tone-sentence distributions.

2. **Cross-cultural validation**: Repeat the SP experiments with participants from diverse geographical regions to test the generalizability of tone interpretations across cultures.

3. **Alternative alignment method comparison**: Implement and compare additional semantic alignment methods (e.g., Canonical Correlation Analysis, Procrustes alignment) to validate the superiority of Bilingual Lexicon Induction across different evaluation metrics.