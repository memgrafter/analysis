---
ver: rpa2
title: The Role of Fibration Symmetries in Geometric Deep Learning
arxiv_id: '2408.15894'
source_url: https://arxiv.org/abs/2408.15894
tags:
- symmetries
- graph
- nodes
- graphs
- bration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Geometric Deep Learning (GDL) leverages symmetries to introduce
  inductive biases into machine learning models. However, the current formulation
  of GDL is limited to global symmetries, which are often not found in real-world
  problems.
---

# The Role of Fibration Symmetries in Geometric Deep Learning

## Quick Facts
- arXiv ID: 2408.15894
- Source URL: https://arxiv.org/abs/2408.15894
- Authors: Osvaldo Velarde; Lucas Parra; Paolo Boldi; Hernan Makse
- Reference count: 40
- Key outcome: Fibration symmetries enable graph compression and tighter bounds on GNN expressive power

## Executive Summary
This paper introduces fibration symmetries as a relaxation of global symmetries in Geometric Deep Learning, enabling more realistic and effective inductive biases for real-world problems. By leveraging these local symmetries, the authors demonstrate improved computational efficiency through graph compression and provide a tighter theoretical bound on GNN expressive power compared to the Weisfeiler-Lehman test. The work bridges theoretical graph theory with practical deep learning applications, showing significant compression factors (up to 74% on QM9) while maintaining model performance.

## Method Summary
The method centers on identifying fibration symmetries in graphs using minimal balanced coloring algorithms, then constructing compressed base graphs where synchronized nodes are collapsed into fibers. For GNNs, this involves training on either original graphs or their compressed bases while preserving the same computational dynamics. For MLPs, the Fibration-Gradient Descent algorithm dynamically detects node synchronization during training and reduces network size by collapsing synchronized nodes. The approach is validated across multiple datasets including QM9 for molecular property prediction, TCGA-BRCA for survival analysis, and MNIST/KMNIST/Fashion for image classification.

## Key Results
- Fibration symmetries provide a tighter upper bound on GNN expressive power than the Weisfeiler-Lehman test
- Graph compression using fibration symmetries achieves ~74% average compression factor on QM9 dataset
- Fibration-Gradient Descent compresses MLP networks by 5-6x in hidden layers without sacrificing performance (within ±2% of original accuracy)

## Why This Works (Mechanism)

### Mechanism 1
Fibration symmetries identify nodes with isomorphic input trees, forcing them to synchronize during GNN propagation. By collapsing these synchronized nodes into a base graph, computations are preserved with fewer nodes, enabling computational savings without loss of representational fidelity. The core assumption is that node states depend only on in-neighborhoods, and synchronization preserves required dynamics.

Evidence anchors:
- [abstract] "Fibration symmetries can be used to compress graph-structured data, improving memory scalability and computations in GNNs. For example, in the QM9 dataset, the average compression factor across all samples is ~74%."
- [section 2.1.3] "We show that the evolution of the loss function over the training steps for both forms is similar... The performance of the network trained with Fibration-Gradient Descent (accuracy = 95.73%) remains within ±2% of the original network and gradient descent (97.67%)."

### Mechanism 2
The expressive power of GNNs is bounded by the Fibration Test's ability to distinguish graphs, which is tighter than the Weisfeiler-Lehman test because it only considers input trees. Since GNNs aggregate solely from in-neighbors, their expressive power cannot exceed that of the Fibration Test.

Evidence anchors:
- [section 2.1.1] "We show that GNNs apply the inductive bias of fibration symmetries and derive a tighter upper bound for their expressive power... GNNs are particular functions that satisfy the bias induced by fibrations (see green line inside the yellow area), as they depend solely on the in-neighborhoods."
- [abstract] "Fibration symmetries provide a tighter upper bound on the expressive power of Graph Neural Networks (GNNs) compared to the Weisfeiler-Lehman test."

### Mechanism 3
During MLP training, gradient descent causes some nodes to converge to identical activation patterns across all training samples, forming fibers. By collapsing these fibers into single nodes, the network size is reduced while preserving the input-output mapping through the Fibration-Gradient Descent algorithm.

Evidence anchors:
- [section 2.2.1] "We hypothesized that during learning the weights adjust to give synchronized network activity... we analyze the activity of the nodes in MLPs that were trained and achieve accuracy of 97.67%, 90.58%, and 88.42%... clusters of synchronized nodes are detected."
- [section 2.2.2] "With this new version Fibration-Gradient Descent (FB-GradDesc), we construct a smaller network B (i.e., base) composed of fibers from the original network G... the performance of the network trained with FB-GradDesc (accuracy = 95.73%) remains within ±2% of the original network and gradient descent (97.67%)."

## Foundational Learning

- Concept: Graph symmetries (automorphisms, coverings, fibrations)
  - Why needed here: Understanding the hierarchy of symmetry levels is crucial for grasping why fibrations are more flexible than automorphisms and how they can be used to compress graphs while preserving dynamics.
  - Quick check question: What is the key difference between automorphisms and fibrations in terms of what graph structure they preserve?

- Concept: Input and output trees in directed graphs
  - Why needed here: Fibration symmetries are defined in terms of isomorphic input trees. Recognizing this allows you to understand why node synchronization occurs and how graph compression works.
  - Quick check question: If two nodes have isomorphic input trees but different output trees, are they in the same fiber under fibrations?

- Concept: Expressive power of graph neural networks
  - Why needed here: The paper connects the expressive power of GNNs to symmetry-based graph tests. Knowing this helps you see why fibrations provide a tighter bound than WL tests and what that means for model design.
  - Quick check question: If a GNN's update rule only uses in-neighborhoods, what is the tightest upper bound on its expressive power according to the paper?

## Architecture Onboarding

- Component map:
  - Graph symmetry detection -> Minimal balanced coloring algorithm (Kamei & Cock)
  -> Fiber identification -> Base graph construction -> GNN training
  - MLP training -> Node activation monitoring -> Fiber detection with threshold ε
  -> Network reduction -> Fibration-Gradient Descent training

- Critical path:
  1. Detect fibers in dataset graphs or MLPs
  2. Construct base graphs or reduced networks
  3. Train model on reduced representation
  4. Evaluate performance vs. original representation

- Design tradeoffs:
  - Stronger symmetry (automorphism) → more expressive power but fewer real-world graphs have such symmetries
  - Weaker symmetry (fibration) → more common in real graphs, better compression, but lower expressive power bound
  - Compression factor vs. performance: Higher compression risks performance loss if fibers are misdetected or if out-neighborhoods matter

- Failure signatures:
  - If compression degrades performance, check if fibers were correctly identified or if the base graph construction preserved necessary information
  - If no fibers are found in an MLP, verify the threshold ε and ensure the dataset is large enough for synchronization to emerge
  - If base graph operations don't match original GNN performance, confirm the lifted equations correctly preserve in-neighborhood dynamics

- First 3 experiments:
  1. Run minimal balanced coloring on a small graph dataset (e.g., QM9 subset) and verify compression factor and base graph construction
  2. Train a GNN on original vs. base graphs for a regression task (e.g., dipole moment) and compare loss curves
  3. Train an MLP on MNIST, detect fibers at epoch T, collapse them, and retrain; compare accuracy before and after compression

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the expressive power of GNNs with fibration symmetries compare to other advanced GNN architectures (e.g., higher-order GNNs, equivariant GNNs) on complex graph tasks?
- Basis in paper: [explicit] The paper establishes that fibration symmetries provide a tighter upper bound on GNN expressive power compared to the Weisfeiler-Leiman test, but does not directly compare to other advanced architectures.
- Why unresolved: The paper focuses on theoretical bounds and empirical results on specific datasets, without benchmarking against the full spectrum of modern GNN architectures.
- What evidence would resolve it: Comparative experiments on diverse graph benchmarks (e.g., molecular property prediction, social network analysis) between fibration-based GNNs and state-of-the-art GNN variants.

### Open Question 2
- Question: Can the concept of fibration symmetries be extended to non-graph domains (e.g., point clouds, meshes, or manifolds) in geometric deep learning, and what would be the theoretical implications?
- Basis in paper: [inferred] The paper mentions that the mathematical extension applies beyond graphs to manifolds, bundles, and grids, but does not explore these extensions.
- Why unresolved: The paper focuses on graph-structured data and does not provide a framework for generalizing fibration symmetries to other geometric structures.
- What evidence would resolve it: Development of fibration-like symmetry concepts for other geometric domains and theoretical analysis of their impact on model expressive power and inductive bias.

### Open Question 3
- Question: How does the performance of the Fibration-Gradient Descent method scale with network depth and width, and what are its limitations in very deep or very wide networks?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of Fibration-Gradient Descent on relatively small MLPs (two hidden layers), but does not explore scaling to deeper or wider architectures.
- Why unresolved: The paper only provides empirical results on limited network architectures and does not analyze the method's behavior in more complex scenarios.
- What evidence would resolve it: Systematic experiments on MLPs and deep CNNs with varying depths and widths, analyzing convergence, compression ratios, and performance trade-offs.

## Limitations

- The theoretical analysis of expressive power bounds relies on specific assumptions about GNN update rules depending solely on in-neighborhoods.
- The compression factor of ~74% for QM9 is reported as an average but individual sample variation is not provided.
- The Fibration-Gradient Descent method's performance on MLPs is demonstrated primarily on MNIST, with limited testing on other datasets.

## Confidence

- High Confidence: Fibration symmetries provide computational benefits through graph compression and node synchronization in MLPs.
- Medium Confidence: Fibration symmetries provide a tighter upper bound on GNN expressive power than the Weisfeiler-Leiman test.
- Medium Confidence: The Fibration-Gradient Descent method can compress MLP networks by 5-6x without significant performance loss.

## Next Checks

1. Verify expressive power bounds: Implement multiple GNN architectures and test their performance on graph pairs distinguished by the Fibration Test but not by the WL test.
2. Analyze compression variability: Compute fibration bases for all molecules in QM9 and report the distribution of compression factors.
3. Test Fibration-Gradient Descent generalization: Apply the method to convolutional neural networks on image datasets and measure compression factors and performance changes across different network depths and architectures.