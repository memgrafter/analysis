---
ver: rpa2
title: 'SciFaultyQA: Benchmarking LLMs on Faulty Science Question Detection with a
  GAN-Inspired Approach to Synthetic Dataset Generation'
arxiv_id: '2412.11988'
source_url: https://arxiv.org/abs/2412.11988
tags:
- faulty
- questions
- question
- llms
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of large language models (LLMs)
  answering logically or scientifically flawed questions without recognizing their
  inherent faults. To benchmark LLMs on this task, the authors introduce SciFaultyQA,
  a novel dataset of intentionally faulty science questions.
---

# SciFaultyQA: Benchmarking LLMs on Faulty Science Question Detection with a GAN-Inspired Approach to Synthetic Dataset Generation

## Quick Facts
- arXiv ID: 2412.11988
- Source URL: https://arxiv.org/abs/2412.11988
- Reference count: 7
- Primary result: LLMs struggle with faulty question detection (16% baseline), but multi-agent systems and tool integration improve performance (up to 65%)

## Executive Summary
This paper addresses a critical gap in LLM evaluation: their inability to recognize logically or scientifically flawed questions. The authors introduce SciFaultyQA, a novel dataset of 1,333 faulty science questions generated using a GAN-inspired approach where multiple LLMs iteratively create and critique each other's outputs. The study reveals that current LLMs have limited capability to detect faulty questions, with GPT-4o achieving only 16% detection rate. However, integrating web search tools and multi-agent systems significantly improves performance, increasing detection rates to 65% in some cases. The work highlights the need for better fault detection mechanisms and proposes methods to enhance LLM robustness in handling flawed inputs.

## Method Summary
The authors develop SciFaultyQA using a GAN-inspired synthetic dataset generation approach where multiple LLMs (generators) create faulty versions of valid science questions from SciQ and SciQA datasets, while a discriminator LLM evaluates whether the generated questions are sufficiently flawed. This iterative process continues until convergence. The resulting dataset contains 1,333 faulty science questions with associated fault types and explanations. LLMs are then evaluated on their ability to detect these faulty questions, both individually and when integrated with tools or used in multi-agent configurations.

## Key Results
- GPT-4o achieves only 16% detection rate on faulty science questions without tools
- Integration of web search tools and multi-agent systems increases detection rates to 65% in some cases
- Different LLMs exhibit varying expertise across different types of fallacies (logical, numerical, physical law violations)
- The GAN-inspired generation process produces diverse faulty questions that challenge current LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The GAN-inspired iterative feedback loop improves faulty question generation quality by leveraging multiple LLMs with different strengths.
- Mechanism: Three different LLMs generate faulty versions of the same question, then a fourth LLM (discriminator) evaluates whether each generated question is actually faulty. The feedback from the discriminator is fed back to the generators, who refine their outputs. This process repeats until the discriminator can no longer find faults, indicating convergence on a sufficiently faulty question.
- Core assumption: Different LLMs have complementary weaknesses and strengths in detecting and generating faulty questions, creating a useful adversarial dynamic.
- Evidence anchors:
  - [abstract] "A GAN-inspired approach is used to generate synthetic faulty questions by having multiple LLMs iteratively create and critique each other's outputs until the questions are sufficiently flawed."
  - [section] "If we take two different LLMs, ask one LLM say LLM1 to generate faulty question and then ask another LLM, say LLM2 to answer it, a lot of time the LLM2 can not detect the faulty question and gives answer to the faulty question."
  - [corpus] Weak evidence - no directly comparable papers in corpus about GAN-style question generation for fault detection.
- Break condition: The process stops when the discriminator LLM can no longer identify faults in any of the generated faulty versions, or when a predefined maximum number of iterations is reached.

### Mechanism 2
- Claim: Multi-agent systems combining different specialized LLMs improve faulty question detection rates.
- Mechanism: Instead of relying on a single LLM to detect faulty questions, multiple LLMs are deployed in parallel where they verify each other's responses before delivering a final answer. This leverages the varying areas of expertise among different models.
- Core assumption: Different LLMs have varying strengths and weaknesses in different domains, and combining them creates a more robust detection system.
- Evidence anchors:
  - [abstract] "integrating web search tools and multi-agent systems significantly improves performance, increasing detection rates to 65% in some cases."
  - [section] "We observed that LLMs often proceed to answer these flawed questions without recognizing their inherent issues, producing results that are logically or scientifically invalid."
  - [section] "Our data suggests that different LLMs have varying areas of expertise. By combining multiple models in a multi-agent framework, we can harness these strengths to create a more robust application capable of effectively addressing flawed questions."
  - [corpus] Weak evidence - no directly comparable papers in corpus about multi-agent systems for faulty question detection.
- Break condition: When the combined system achieves stable performance that doesn't significantly improve with additional agents or iterations.

### Mechanism 3
- Claim: Tool integration (especially web search) provides additional context that helps LLMs recognize faulty questions.
- Mechanism: When LLMs have access to external tools like web search, they can verify the factual accuracy of the question's premises before attempting to answer, reducing the likelihood of responding to nonsensical questions.
- Core assumption: LLMs often fail to recognize faulty questions because they lack access to real-world context that would reveal the impossibility or illogical nature of the premise.
- Evidence anchors:
  - [abstract] "integrating web search tools and multi-agent systems significantly improves performance, increasing detection rates to 65% in some cases."
  - [section] "When the LLM has access to the internet it performs much better than without it. We believe it is happening because of access to more information available to it instead of relying on training where it might happen that LLM didn't capture the true concept."
  - [corpus] Weak evidence - no directly comparable papers in corpus about tool integration for faulty question detection.
- Break condition: When the tool integration no longer provides meaningful performance improvements or when the cost-benefit ratio becomes unfavorable.

## Foundational Learning

- Concept: GAN (Generative Adversarial Network) architecture and its adaptation to synthetic data generation
  - Why needed here: The paper adapts GAN principles to generate synthetic faulty questions by having "generator" LLMs create questions and a "discriminator" LLM evaluate them iteratively.
  - Quick check question: Can you explain how the generator-discriminator dynamic in traditional GANs translates to this question generation context?

- Concept: Multi-agent systems and their coordination patterns
  - Why needed here: The paper proposes using multiple LLMs in coordinated fashion to improve faulty question detection through cross-verification.
  - Quick check question: What are the key differences between a simple ensemble and the multi-agent approach described in this paper?

- Concept: Fault injection techniques and their evaluation
  - Why needed here: Understanding how to systematically introduce and measure faults in questions is central to both the dataset generation and evaluation methodology.
  - Quick check question: What are the different types of faults that can be injected into science questions, and how would you categorize them?

## Architecture Onboarding

- Component map: SciQ/SciQA datasets -> Generator LLMs (GPT-4o, Gemini Pro, Llama 3.1, Mixtral) -> Discriminator LLM (GPT-4) -> Multi-agent coordinator -> Tool integrations (web search, WolframAlpha, calculators) -> Evaluation metrics (detection rate)

- Critical path: Dataset generation → LLM evaluation → Multi-agent implementation → Tool integration → Performance measurement

- Design tradeoffs:
  - Multiple generator LLMs provide diversity but increase computational cost
  - Iterative refinement improves quality but may lead to diminishing returns
  - Tool integration improves accuracy but adds latency and dependency on external services
  - Human evaluation is gold standard but not scalable for large datasets

- Failure signatures:
  - Generator LLMs converge to similar faulty patterns, reducing diversity
  - Discriminator becomes too permissive, accepting invalid questions
  - Multi-agent coordination overhead outweighs benefits
  - Tool integrations introduce new failure modes or inconsistencies

- First 3 experiments:
  1. Implement the basic GAN-inspired generation loop with two LLMs and evaluate the quality of generated faulty questions using human evaluation
  2. Test multi-agent coordination with three different LLMs on a small subset of SciFaultyQA and measure improvement in detection rates
  3. Add web search tool integration to a single LLM and compare detection rates against the baseline on the full dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the diffusion-inspired approach to fault injection produce a fundamentally different distribution of faulty questions compared to the GAN-inspired method?
- Basis in paper: [explicit] The authors propose investigating a diffusion-inspired dataset generation methodology alongside their GAN-inspired approach, stating "We want to inject faults into the valid questions using diffusion process" and questioning whether iterative training on synthetic data would capture "a completely different distribution."
- Why unresolved: The paper only mentions this as a work-in-progress and does not provide experimental results or comparisons between the two methods.
- What evidence would resolve it: Comparative analysis showing the types, frequencies, and characteristics of faulty questions generated by both approaches, along with LLM performance differences on each dataset type.

### Open Question 2
- Question: What is the fundamental limit to how many ways a question can be made faulty, and is this finite or infinite?
- Basis in paper: [explicit] The authors explicitly ask "What makes a question faulty: We ask the question of what makes a question faulty. In how many fundamental ways someone can make a correct question faulty? Is it finite or infinite? Depend on domain of knowledge? type of questions - subjective or objective or MCQ?"
- Why unresolved: The paper does not provide a systematic categorization or enumeration of fault types, nor does it establish whether the space of possible faults is bounded.
- What evidence would resolve it: A comprehensive taxonomy of fault types across different domains, along with statistical analysis of fault diversity in generated datasets to determine if new fault types continue emerging or converge to a finite set.

### Open Question 3
- Question: How does fine-tuning LLMs on progressively generated synthetic faulty question datasets affect their ability to recognize and handle flawed questions?
- Basis in paper: [explicit] The authors propose "if we generate a new synthetic dataset and then fine-tune or train a new model and then generate another synthetic data, and train another LLM, if we keep iterating this process will the final LLM be capturing a completely different distribution? Like having a diffusion kind of approach."
- Why unresolved: This iterative fine-tuning approach is only proposed as a future direction without experimental validation or results demonstrating its effectiveness.
- What evidence would resolve it: Experimental results comparing baseline LLMs, models fine-tuned on single iterations of synthetic data, and models fine-tuned through multiple iterations, measuring detection accuracy and generalization to novel faulty questions.

### Open Question 4
- Question: Which specific components of faulty questions (logical, numerical, physical law violations) do different LLMs specialize in detecting?
- Basis in paper: [explicit] The authors note "Our findings indicate that current LLMs exhibit varying degrees of expertise across different types of fallacies" and mention identifying fault types like "logical fallacies, unrealistic scenarios, or violations of physical laws."
- Why unresolved: The paper does not provide detailed error analysis breaking down detection performance by fault type, nor does it systematically characterize which models excel at which types of fault detection.
- What evidence would resolve it: Granular performance metrics showing detection rates for each fault category (logical, numerical, physical, etc.) across different LLMs, revealing patterns of specialization and potential for model combination strategies.

## Limitations
- The evaluation methodology for detection rates is not fully specified, particularly regarding how model responses are scored
- The study focuses exclusively on science questions, limiting generalizability to other domains
- The dataset size of 1,333 questions may not capture the full diversity of faulty question patterns

## Confidence

- Medium confidence in the core claim that current LLMs struggle with faulty question detection (16% baseline for GPT-4o), as this is directly measured but the methodology details are sparse
- Medium confidence in the claim that multi-agent systems and tool integration improve performance (up to 65%), as results are reported but the exact implementation details and evaluation criteria are unclear
- Low confidence in the scalability and practical applicability of the proposed methods, as the paper does not address computational costs, real-world deployment challenges, or performance on out-of-distribution questions

## Next Checks

1. Replicate the baseline detection rate using the provided dataset and a standard LLM (GPT-4o or equivalent) with the exact evaluation methodology to verify the 16% detection rate claim
2. Implement the multi-agent coordination approach with at least three different LLMs on a subset of SciFaultyQA and measure the improvement in detection rates compared to single-LLM baselines
3. Test the model's performance on faulty questions from domains outside the science domain (e.g., history or mathematics) to evaluate generalizability of the detection capabilities