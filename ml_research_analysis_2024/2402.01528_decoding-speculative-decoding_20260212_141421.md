---
ver: rpa2
title: Decoding Speculative Decoding
arxiv_id: '2402.01528'
source_url: https://arxiv.org/abs/2402.01528
tags:
- decoding
- draft
- throughput
- speculative
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how to optimize draft models for speculative\
  \ decoding in large language model inference. Through over 350 experiments with\
  \ models like LLaMA-65B and OPT-66B, the authors find that draft model latency\u2014\
  especially from model depth\u2014is the primary bottleneck, while language modeling\
  \ accuracy does not strongly correlate with speculative decoding performance."
---

# Decoding Speculative Decoding

## Quick Facts
- arXiv ID: 2402.01528
- Source URL: https://arxiv.org/abs/2402.01528
- Reference count: 36
- Authors: Minghao Yan; Saurabh Agarwal; Shivaram Venkataraman
- Key outcome: New draft models that trade depth for width achieve up to 111% higher throughput than existing models while reducing KV cache usage by 37%.

## Executive Summary
This paper investigates how to optimize draft models for speculative decoding in large language model inference. Through over 350 experiments with models like LLaMA-65B and OPT-66B, the authors find that draft model latency—especially from model depth—is the primary bottleneck, while language modeling accuracy does not strongly correlate with speculative decoding performance. Based on these insights, they design new draft models that trade depth for width while keeping parameter count constant, achieving up to 111% higher throughput than existing models. Their approach also reduces KV cache usage by 37% and generalizes across multiple LLaMA model versions and fine-tuned variants.

## Method Summary
The authors conducted over 350 experiments using LLaMA-65B and OPT-66B as target models, testing various draft models from OPT and LLaMA families. They implemented speculative decoding using Microsoft DeepSpeed library with greedy decoding, batch size 1, on 4 Nvidia A100 80GB GPUs. The key innovation was designing new draft models by pruning larger models (LLaMA-7B) to trade depth for width while maintaining parameter count, using Sheared-LLaMA framework without fine-tuning. They systematically varied draft model architectures to measure the impact on throughput and token acceptance rates.

## Key Results
- Draft model latency, not language modeling accuracy, is the primary bottleneck in speculative decoding performance
- New draft models that trade depth for width achieve up to 111% higher throughput than existing models
- The approach reduces KV cache usage by 37% and generalizes across multiple LLaMA model versions and fine-tuned variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Draft model latency, not language modeling accuracy, is the primary bottleneck in speculative decoding performance.
- Mechanism: Autoregressive decoding in the draft model is inherently sequential and memory-bandwidth bound, making model depth the dominant factor in latency. This sequential latency dominates even when the target model verification step is parallelized.
- Core assumption: The sequential nature of autoregressive decoding in the draft model creates a hard latency bottleneck regardless of the draft model's accuracy.
- Evidence anchors:
  - [abstract] "our experiments indicate that the performance of speculative decoding depends heavily on the latency of the draft model, and the draft model's capability in language modeling does not correlate strongly with its performance in speculative decoding."
  - [section 3.2] "In the first benchmark (Figure 3a), we vary the number of attention heads, feed-forward dimension, and layers in a model to keep the model parameters at around 350M. The detailed configuration for each model can be found in Table 13 in the Appendix. The plot shows that autoregressive decoding latency scales linearly with layer depth despite similar total model parameters."
- Break condition: If the draft model's autoregressive decoding could be parallelized or if the draft model's language modeling accuracy became a limiting factor for token acceptance rate.

### Mechanism 2
- Claim: Trading depth for width in draft models can maintain parameter count while significantly reducing latency and improving speculative decoding throughput.
- Mechanism: By reducing the number of layers and increasing layer width (attention heads, intermediate size, model dimension), the draft model achieves lower autoregressive decoding latency without sacrificing parameter count. This design prioritizes throughput over language modeling accuracy.
- Core assumption: The draft model's primary function is to generate candidate tokens quickly, not to achieve high language modeling accuracy, allowing for optimization of the depth-width tradeoff specifically for latency.
- Evidence anchors:
  - [abstract] "Based on these insights we explore a new design space for draft models and design hardware-efficient draft models for speculative decoding. Our newly designed draft model can provide 111% higher throughput than existing draft models."
  - [section 4.1] "We use two configurations: the first configuration was provided by the Sheared-LLAMA authors (NoFT-1.3B), and we designed the second configuration (NoFT-Wide-1.3B) to optimize for better speculative decoding throughput. Table 1 shows the detailed configuration of the two models. We slash the number of layers by half, from 24 to 12, and keep the total parameter count roughly the same by increasing the intermediate size from 5504 to 9280, the number of attention heads from 16 to 20, and the corresponding model dimension from 2048 to 2560."
- Break condition: If the reduction in model depth causes the draft model's language modeling accuracy to drop below the threshold needed for acceptable token acceptance rates, or if the increased width causes other performance bottlenecks.

### Mechanism 3
- Claim: The design space for draft models should prioritize throughput optimization over language modeling accuracy, as these two objectives are not strongly correlated in the context of speculative decoding.
- Mechanism: Existing draft models are optimized for language modeling accuracy given a parameter budget, but this approach is suboptimal for speculative decoding. By explicitly designing draft models to minimize latency while maintaining acceptable token acceptance rates, significant throughput improvements can be achieved.
- Core assumption: The token acceptance rate (TAR) in speculative decoding depends more on the draft model's ability to generate plausible candidate tokens quickly rather than on achieving high language modeling accuracy.
- Evidence anchors:
  - [abstract] "Our experiments indicate that the performance of speculative decoding depends heavily on the latency of the draft model, and the draft model's capability in language modeling does not correlate strongly with its performance in speculative decoding."
  - [section 3.3] "We plot the accuracy of a model against the TAR it achieves in Figure 5. Surprisingly, we find that TAR correlates little to the model's accuracy on a task."
- Break condition: If future workloads or target models require higher language modeling accuracy from draft models to maintain acceptable token acceptance rates, or if the throughput gains from reduced latency are outweighed by drops in token acceptance rate.

## Foundational Learning

- Concept: Speculative Decoding mechanism
  - Why needed here: Understanding how speculative decoding works is fundamental to grasping why draft model latency is the bottleneck and why trading depth for width is beneficial.
  - Quick check question: In speculative decoding, what are the two main phases that occur sequentially, and which one is typically the bottleneck?

- Concept: Autoregressive decoding vs. parallel processing
  - Why needed here: The sequential nature of autoregressive decoding in the draft model versus the parallel verification in the target model is key to understanding the latency bottleneck.
  - Quick check question: Why does autoregressive decoding in the draft model create a bottleneck even when the target model verification is parallelized?

- Concept: Model depth vs. width tradeoffs in transformer architectures
  - Why needed here: The paper's key insight is that trading depth for width can maintain parameter count while reducing latency, which requires understanding how these architectural choices affect performance.
  - Quick check question: How does reducing the number of layers while increasing layer width affect the autoregressive decoding latency in transformer models?

## Architecture Onboarding

- Component map: Target LLM (65B parameters) -> Draft model (5M-6.7B parameters) -> Speculative decoding framework -> Output
- Critical path: The critical path in speculative decoding is the draft model's autoregressive token generation, followed by the target model's parallel verification. The draft model's sequential decoding latency dominates the overall performance.
- Design tradeoffs: The primary tradeoff is between draft model depth (which increases language modeling accuracy but also latency) and width (which maintains parameter count while reducing latency). Another tradeoff is between throughput optimization and maintaining acceptable token acceptance rates.
- Failure signatures: If the draft model's latency is too high relative to the target model, speculative decoding provides minimal speedup. If the draft model's language modeling accuracy drops too low, token acceptance rates suffer, reducing throughput. If the draft model is too small, it may not generate plausible enough tokens for the target model to accept.
- First 3 experiments:
  1. Benchmark speculative decoding throughput using existing draft models (OPT-125M to OPT-6.7B) with LLaMA-65B as target, measuring latency breakdown and token acceptance rates.
  2. Create draft model variants with fixed parameter count but varying depth-width ratios, measuring autoregressive decoding latency to confirm the depth-latency relationship.
  3. Compare throughput of depth-optimized draft models (NoFT-1.3B) versus width-optimized draft models (NoFT-Wide-1.3B) on the same target model to validate the design approach.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach may not generalize to all LLM architectures beyond LLaMA and OPT families
- Results were obtained on 4 Nvidia A100 80GB GPUs and may not transfer directly to different hardware
- The relationship between draft model architecture and token acceptance rate stability across different domains is unclear

## Confidence
**High Confidence**:
- Draft model latency is the primary bottleneck in speculative decoding performance
- Autoregressive decoding latency scales linearly with model depth
- Trading depth for width while maintaining parameter count reduces latency

**Medium Confidence**:
- Language modeling accuracy does not strongly correlate with speculative decoding performance
- The specific NoFT-Wide-1.3B configuration provides 111% higher throughput than baselines
- The approach generalizes across multiple LLaMA model versions and fine-tuned variants

**Low Confidence**:
- The 37% reduction in KV cache usage is consistent across all target models and datasets
- NoFT-1.3B performance relative to NoFT-Wide-1.3B is consistent across different tasks
- The specific layer count thresholds (minimum 5 layers) are optimal for all scenarios

## Next Checks
1. Test the depth-for-width draft model design with different target LLM architectures (e.g., Mistral, Gemma, or custom MoE models) to verify if the throughput improvements generalize beyond LLaMA and OPT families.
2. Reproduce key experiments on different GPU configurations (e.g., H100, A40, or mixed-precision setups) to quantify how sensitive the throughput improvements are to hardware variations and identify potential optimization opportunities.
3. Evaluate the draft models on specialized datasets (medical, legal, or code generation) to assess whether the weak correlation between language modeling accuracy and speculative decoding performance holds across diverse domains and whether TAR stability is maintained.