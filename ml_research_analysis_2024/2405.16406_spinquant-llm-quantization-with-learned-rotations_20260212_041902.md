---
ver: rpa2
title: 'SpinQuant: LLM quantization with learned rotations'
arxiv_id: '2405.16406'
source_url: https://arxiv.org/abs/2405.16406
tags:
- quantization
- rotation
- gptq
- arxiv
- spinquanthad
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpinQuant addresses the challenge of post-training quantization
  (PTQ) in large language models (LLMs), where outliers in weight and activation distributions
  lead to quantization errors. The core method introduces learned rotation matrices
  that reduce outliers while preserving the full-precision network output.
---

# SpinQuant: LLM quantization with learned rotations

## Quick Facts
- arXiv ID: 2405.16406
- Source URL: https://arxiv.org/abs/2405.16406
- Reference count: 40
- Key outcome: SpinQuant achieves 2.9-point accuracy gap to full precision on LLaMA-2 7B with 4-bit quantization, outperforming previous methods by up to 25.0 points

## Executive Summary
SpinQuant addresses the challenge of post-training quantization (PTQ) in large language models (LLMs), where outliers in weight and activation distributions lead to quantization errors. The core method introduces learned rotation matrices that reduce outliers while preserving the full-precision network output. SpinQuant optimizes these rotations using Cayley SGD on the Stiefel manifold to minimize quantization loss. The approach achieves state-of-the-art results, narrowing the accuracy gap to full precision to 2.9 points on LLaMA-2 7B with 4-bit quantization of weights, activations, and KV cache. It outperforms previous methods like LLM-QAT by 19.1 points and SmoothQuant by 25.0 points.

## Method Summary
SpinQuant introduces learned rotation matrices to mitigate quantization errors caused by outliers in LLM weight and activation distributions. The method optimizes rotation matrices using Cayley SGD on the Stiefel manifold to minimize quantization loss while preserving full-precision model outputs. Rotations are absorbed into weights during optimization, eliminating runtime overhead. The approach supports both symmetric and asymmetric quantization and includes optional online Hadamard rotations for activation and KV cache quantization. Optimization uses 800 calibration samples from WikiText-2, with GPTQ applied after rotation optimization.

## Key Results
- Achieves 2.9-point accuracy gap to full precision on LLaMA-2 7B with 4-bit quantization
- Outperforms LLM-QAT by 19.1 points and SmoothQuant by 25.0 points
- Reduces activation kurtosis from approximately 18 to 3 using random Hadamard rotations
- Shows 45.1% improvement over concurrent work QuaRot on LLaMA-3 8B models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rotation matrices transform outlier-heavy weight and activation distributions into more Gaussian-like distributions, reducing quantization error.
- Mechanism: Random rotation statistically mixes large and small values across channels, evening out the distribution and reducing kurtosis. Learned rotations further optimize this process by minimizing quantization loss directly.
- Core assumption: Transformer outputs are invariant to orthonormal rotations applied in specific positions (e.g., residual stream, attention blocks).
- Evidence anchors:
  - [abstract]: "Rotating activation or weight matrices helps remove outliers and benefits quantization."
  - [section]: "After multiplying these activations with a random rotation matrix, the κ across all layers becomes approximately 3, indicating a more Gaussian-shaped distribution that is easier to quantize."
- Break condition: If the rotation breaks the residual connections or is applied where non-linearities exist without compensation, the full-precision invariance fails.

### Mechanism 2
- Claim: Cayley SGD efficiently optimizes rotation matrices on the Stiefel manifold without altering full-precision model outputs.
- Mechanism: Cayley transform updates ensure the rotation matrix stays orthonormal while minimizing the quantization loss. The rotation only affects intermediate activations, making them more quantization-friendly.
- Core assumption: The Stiefel manifold constraint (orthonormality) is necessary and sufficient for maintaining model equivalence in full precision.
- Evidence anchors:
  - [section]: "TheCayley-optimized rotation exhibits minimal variance when initiated from different random seeds."
  - [section]: "This optimization does not alter the full-precision network output but refines the intermediate activations and weights, making them more quantization-friendly."
- Break condition: If the Cayley update step size is too large or the projection is incorrect, orthonormality may be violated, breaking invariance.

### Mechanism 3
- Claim: SpinQuantno had merges rotation into weights, avoiding architectural changes and runtime overhead.
- Mechanism: Rotations R1 and R2 are absorbed into the corresponding weight matrices during optimization. At inference, only the rotated weights are used, eliminating the need for extra matrix multiplications.
- Core assumption: The rotation matrices can be merged into weights without increasing parameter count or changing the floating-point network behavior.
- Evidence anchors:
  - [section]: "SpinQuantno had merges rotation matrices into pre-trained weights without altering the network architecture."
  - [section]: "During inference, the original weights are simply replaced with the rotated quantized weights, eliminating the need for modification in the forward pass."
- Break condition: If the rotation is not perfectly absorbed (e.g., due to quantization before merging), the network output may drift from the original.

## Foundational Learning

- Concept: Stiefel manifold and orthonormal matrices
  - Why needed here: Rotations must remain orthonormal to preserve model invariance and numerical stability during optimization.
  - Quick check question: What property must a rotation matrix have to preserve vector norms and be invertible?

- Concept: Quantization error and outlier sensitivity
  - Why needed here: Understanding how extreme values dominate quantization range explains why outlier mitigation is critical for low-bit quantization.
  - Quick check question: Why does a single large outlier in a tensor force more bits to be allocated to it at the expense of smaller values?

- Concept: Cayley transform and skew-symmetric parameterization
  - Why needed here: Provides an efficient, numerically stable way to update rotation matrices on the Stiefel manifold without matrix exponentials.
  - Quick check question: How does the Cayley transform maintain orthonormality during gradient updates?

## Architecture Onboarding

- Component map:
  - Rotation matrices (R1, R2) absorbed into weights
  - Optional online Hadamard rotations (R3, R4) applied in MLP and KV cache
  - Cayley SGD optimizer for learned rotations
  - GPTQ for final weight quantization
  - Symmetric/asymmetric quantization settings for activations and KV cache

- Critical path:
  1. Initialize rotations (random Hadamard)
  2. Optimize rotations using Cayley SGD on calibration set
  3. Merge learned rotations into weights
  4. Apply GPTQ to rotated weights
  5. Deploy with optional online Hadamard kernels

- Design tradeoffs:
  - Learned vs random rotations: Learned reduces variance but adds optimization time.
  - Merge vs online rotations: Merging avoids runtime overhead but may be less flexible for activation quantization.
  - Symmetric vs asymmetric quantization: Asymmetric often better for activations but may need extra scaling logic.

- Failure signatures:
  - Large variance in quantized accuracy across random seeds → likely need learned rotations.
  - Significant accuracy drop after rotation → possible orthonormality violation or incorrect merge.
  - Slow inference with hadamard → online rotations may be too costly for the target hardware.

- First 3 experiments:
  1. Apply random Hadamard rotation to LLaMA-2 7B weights, measure activation kurtosis before/after.
  2. Optimize rotation with Cayley SGD for 100 iterations, compare zero-shot accuracy vs random.
  3. Merge learned rotation into weights, apply GPTQ, measure end-to-end accuracy and latency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we derive a closed-form solution for the optimal rotation matrix that evenly distributes magnitude across different axes given an activation distribution with known outlier axes and magnitudes?
- Basis in paper: [explicit] The paper discusses the potential for a closed-form solution for optimal rotation matrices based on activation distributions, particularly in the illustrative analysis section where they discuss rotating distributions by 45° to equalize value representation ranges.
- Why unresolved: While the paper provides theoretical justification and examples of how rotation helps quantization, it does not provide or derive a closed-form solution for optimal rotation matrices based on known activation distributions.
- What evidence would resolve it: Mathematical derivation showing that a closed-form solution exists for optimal rotation matrices given activation distribution statistics, followed by empirical validation showing this solution outperforms learned rotations in terms of quantization accuracy.

### Open Question 2
- Question: What is the theoretical limit of quantization accuracy improvements achievable through rotation optimization, and how does this compare to architectural modifications or quantization-aware training approaches?
- Basis in paper: [inferred] The paper demonstrates significant improvements using learned rotations but does not establish theoretical bounds or compare against alternative approaches like quantization-aware training or architectural modifications.
- Why unresolved: The paper focuses on demonstrating the effectiveness of learned rotations but does not explore the fundamental limits of this approach or compare it to other methods that could achieve similar or better results through different mechanisms.
- What evidence would resolve it: A comprehensive study comparing learned rotations against quantization-aware training, architectural modifications, and other outlier suppression techniques, establishing both theoretical bounds and empirical performance ceilings.

### Open Question 3
- Question: How can we systematically identify and reduce outliers in the first token's activation channels before applying SpinQuant to further enhance quantization accuracy?
- Basis in paper: [explicit] The distribution visualization section notes that in several activation layers, the first token displays substantial values in multiple channels, which could be a potential source of quantization error.
- Why unresolved: While the paper identifies this phenomenon, it does not investigate the source of these outliers or propose methods to address them prior to rotation optimization.
- What evidence would resolve it: Analysis tracing the origin of first-token outliers through the transformer architecture, followed by proposed modifications to the model or preprocessing steps that reduce these outliers before rotation, with empirical validation of improved quantization performance.

## Limitations
- Scalability uncertainty for very large models (e.g., LLaMA-3 70B) where rotation optimization may become computationally prohibitive
- Reliance on calibration data quality and representativeness for rotation optimization
- Does not address dynamic activation outliers that may emerge during inference in long sequences

## Confidence

- **High Confidence**: The core claim that learned rotations reduce quantization error by mitigating outliers is strongly supported by empirical results and statistical analysis of activation distributions.
- **Medium Confidence**: The claim that Cayley SGD on the Stiefel manifold is necessary for stable optimization is plausible but not rigorously proven.
- **Low Confidence**: The assertion that SpinQuant is robust to random initialization is based on variance measurements but lacks systematic study across diverse architectures.

## Next Checks

1. **Ablation of Optimization Method**: Replace Cayley SGD with a simpler optimizer (e.g., Adam with explicit orthonormality projection) and measure the impact on final accuracy and training stability.

2. **Cross-Domain Calibration Robustness**: Evaluate SpinQuant on models fine-tuned for specialized domains (e.g., biomedical or code generation) using calibration sets from those domains.

3. **Dynamic Activation Outlier Analysis**: Instrument the inference pipeline to log activation statistics (kurtosis, max/min values) for each layer during evaluation on long sequences or diverse prompts.