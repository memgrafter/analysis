---
ver: rpa2
title: 'From Coarse to Fine: Efficient Training for Audio Spectrogram Transformers'
arxiv_id: '2401.08415'
source_url: https://arxiv.org/abs/2401.08415
tags:
- training
- audio
- phase
- transformer
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to improve the training efficiency
  of audio spectrogram transformers (ASTs) by introducing a multi-phase curriculum
  learning approach. The key idea is to train the model initially on lower-resolution
  audio spectrograms to learn general features quickly, then progressively fine-tune
  on higher-resolution data.
---

# From Coarse to Fine: Efficient Training for Audio Spectrogram Transformers

## Quick Facts
- arXiv ID: 2401.08415
- Source URL: https://arxiv.org/abs/2401.08415
- Reference count: 0
- The paper proposes a multi-phase curriculum learning approach for audio spectrogram transformers, achieving on-par or better performance while saving 18-58% of FLOPs and training time across four audio classification datasets.

## Executive Summary
This paper introduces a method to improve training efficiency of audio spectrogram transformers (ASTs) by using a coarse-to-fine curriculum learning approach. The key innovation is to initially train the model on lower-resolution audio spectrograms to learn general features quickly, then progressively fine-tune on higher-resolution data. This approach achieves comparable or better performance than standard training while significantly reducing computational resources and training time across multiple audio classification benchmarks.

## Method Summary
The method employs a two-phase (or three-phase) training strategy where the model first learns from temporally compressed, lower-resolution mel-spectrograms, then fine-tunes on the original high-resolution data. Three temporal compression methods are explored: frame-shift, pooling, and flexible patchification. During phase transitions, positional embeddings are resized via bilinear interpolation to accommodate the change in token count. The approach leverages the temporal redundancy in audio spectrograms to reduce computational complexity while maintaining model performance.

## Key Results
- Achieves on-par or better performance compared to baseline AST across four audio classification datasets
- Saves 18-58% of FLOPs and training time while maintaining or improving accuracy
- Demonstrates generalization to other AST-based methods including HTS-AT and SSAST
- Shows improved convergence speed with fewer computational resources required

## Why This Works (Mechanism)

### Mechanism 1
Curriculum learning with progressively higher temporal resolution improves model convergence by starting from coarse, generalizable features before refining with fine-grained data. The model first learns broad, scale-invariant patterns from lower-resolution spectrograms, reducing redundancy and focusing on essential structure. This initial phase provides stable weight initialization. Later, fine-tuning on full-resolution data refines learned representations for task-specific details without destabilizing earlier learning.

### Mechanism 2
Reducing temporal resolution reduces computational cost and training time because fewer tokens mean fewer self-attention computations. By shrinking the time axis, the number of tokens is reduced by a factor of C. Since transformer self-attention scales quadratically with token count, this yields proportional drop in FLOPs and wall-clock time during the initial phase.

### Mechanism 3
Adapting positional embeddings when transitioning phases ensures the model can interpret token positions correctly at different resolutions. During phase transition, trained positional embeddings are resized via bilinear interpolation to match the new token count. This preserves learned spatial relationships while accommodating different input structure.

## Foundational Learning

- Concept: Temporal redundancy in audio spectrograms
  - Why needed here: Justifies why starting with lower-resolution data is viable and efficient
  - Quick check question: If two adjacent frames in a spectrogram are nearly identical, what does that imply about the information content and how might it influence model design?

- Concept: Quadratic complexity of self-attention in transformers
  - Why needed here: Explains why reducing token count yields large computational savings
  - Quick check question: If an input has 100 tokens, how many pairwise self-attention computations are performed? What about with 50 tokens?

- Concept: Curriculum learning in deep learning
  - Why needed here: Provides the theoretical motivation for phased training with increasing difficulty/resolution
  - Quick check question: In a standard curriculum learning setup, what typically happens to the model's generalization when easy examples are presented first?

## Architecture Onboarding

- Component map: Input: Mel-spectrogram (F x T) -> Temporal compression: Fshift, Pool, or Patch -> Tokenization: Converts spectrogram to (f x t/C) tokens -> Transformer encoder: Standard self-attention layers -> Positional embeddings: Resized during phase transitions -> Output: Classification head

- Critical path: 1. Generate mel-spectrogram 2. Apply temporal compression 3. Tokenize and add positional embeddings 4. Pass through transformer encoder 5. Output classification

- Design tradeoffs:
  - Compression factor C vs. accuracy loss: Larger C saves more compute but risks losing detail
  - Compression method choice: Fshift is simple but alters audio content; Pooling is model-agnostic; Patchification is flexible but requires careful interpolation
  - Phase scheduling: Too short initial phase risks poor generalization; too long wastes efficiency gains

- Failure signatures:
  - Accuracy drops sharply after phase transition → positional embedding interpolation failed or resolution gap too large
  - No training speedup despite lower token count → implementation bug in compression or tokenization
  - Convergence stalls in initial phase → compression too aggressive, insufficient information for learning

- First 3 experiments:
  1. Train baseline AST on AudioSet for 5 epochs; record mAP and FLOPs
  2. Apply Fshift compression with C=2 for 1 epoch, then fine-tune on full resolution; compare mAP and FLOPs to baseline
  3. Repeat experiment 2 with Pooling compression; verify if results are method-agnostic

## Open Questions the Paper Calls Out

### Open Question 1
How does the optimal number of training phases (beyond the initial low-resolution phase) vary with dataset size and complexity? Is there a principled way to determine this? The paper only tests 2-phase and 3-phase training without exploring more phases or providing a theoretical framework for determining the optimal number.

### Open Question 2
Can the coarse-to-fine training approach be effectively combined with other efficiency techniques like token pruning or low-rank approximations to further reduce computational costs? The paper presents the coarse-to-fine approach in isolation without investigating potential synergies with other efficiency techniques.

### Open Question 3
How does the coarse-to-fine training approach affect the learned representations' robustness to noise or domain shifts compared to standard training? The paper focuses on accuracy and efficiency but does not analyze the robustness properties of the learned representations.

## Limitations

- The assumption of temporal redundancy may not hold uniformly across all audio classification tasks, particularly those requiring precise temporal localization
- The effectiveness of positional embedding interpolation across large resolution gaps is not extensively validated
- Evaluation scope is limited to four datasets without exploring scenarios with varying audio lengths, sampling rates, or domain-specific characteristics

## Confidence

- High confidence: Computational efficiency gains (18-58% FLOPs reduction) are well-supported by quadratic complexity of self-attention
- Medium confidence: Claim that curriculum learning improves convergence speed and generalization is supported by experimental results but needs deeper analysis
- Medium confidence: Assertion that approach generalizes to other AST variants is plausible but not rigorously proven

## Next Checks

1. **Resolution Gap Sensitivity Test**: Systematically evaluate performance degradation as a function of compression factor C (e.g., test C=2, 4, 8, 16) to identify maximum viable gap between coarse and fine phases.

2. **Positional Embedding Interpolation Robustness**: Conduct ablation studies comparing bilinear interpolation with alternative methods and analyze impact on phase transition performance.

3. **Domain Transfer Analysis**: Apply method to audio classification tasks with different temporal characteristics to test whether temporal redundancy assumption holds across domains.