---
ver: rpa2
title: 'Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation'
arxiv_id: '2404.12784'
source_url: https://arxiv.org/abs/2404.12784
tags:
- segmentation
- scene
- gaussian
- gaussians
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Contrastive Gaussian Clustering, a novel
  approach for 3D scene segmentation using 3D Gaussian Splatting (3DGS). The method
  addresses the challenge of limited availability of annotated 3D scene datasets by
  leveraging 2D segmentation masks and a combination of contrastive learning and spatial
  regularization.
---

# Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation

## Quick Facts
- **arXiv ID**: 2404.12784
- **Source URL**: https://arxiv.org/abs/2404.12784
- **Reference count**: 40
- **Primary result**: 8% improvement in IoU accuracy over state-of-the-art methods on two datasets

## Executive Summary
This paper introduces Contrastive Gaussian Clustering, a novel approach for 3D scene segmentation using 3D Gaussian Splatting (3DGS). The method addresses the challenge of limited availability of annotated 3D scene datasets by leveraging 2D segmentation masks and a combination of contrastive learning and spatial regularization. The core innovation is embedding a 3D feature field in a 3DGS model, enabling novel view synthesis of segmentation features and 3D scene segmentation through clustering. The method can be trained on inconsistent 2D segmentation masks while still learning a 3D feature field consistent across all views, achieving state-of-the-art performance on two benchmark datasets.

## Method Summary
The method extends 3D Gaussian Splatting by embedding learnable feature vectors in each Gaussian. During training, the model optimizes Gaussians to minimize a rendering loss (standard 3DGS), a contrastive clustering loss applied to rendered feature maps every 50 iterations, and a spatial-similarity regularization loss applied every 100 iterations. The contrastive loss enforces multi-view consistency by maximizing similarity within segments and minimizing similarity across segments in the rendered feature space. Spatial regularization ensures that nearby Gaussians in 3D space have similar feature vectors. After training, 3D segmentation is performed by clustering Gaussians based on their feature vectors, and novel view segmentation masks are generated by α-blending the clustered features.

## Key Results
- Achieves 8% improvement in IoU accuracy over the best competitor on two benchmark datasets
- Demonstrates superior performance in boundary quality and 3D object segmentation
- Can be trained on inconsistent 2D segmentation masks while learning a consistent 3D feature field
- Renders novel segmentation masks in 0.005 seconds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Contrastive learning on rendered 2D feature maps enables multi-view consistency in 3D segmentation even when 2D masks are inconsistent.
- **Mechanism**: By computing a contrastive loss that maximizes similarity within segments and minimizes similarity across segments in the rendered feature space, the model learns a 3D feature field that is coherent across views.
- **Core assumption**: Inconsistent 2D masks still contain reliable information about which Gaussians belong to the same object, and contrastive learning can exploit this.
- **Evidence anchors**: [abstract] "Using a combination of contrastive learning and spatial regularization, our method can be trained on inconsistent 2D segmentation masks, and still learn to generate segmentation masks consistent across all views."; [section 3.2] "Our objective is to find features that minimize the following loss function: LCC = -1/Nk ∑p=1 1/|{fp}| ∑q=1 log exp(f_pq·f̄_p/φ_p) / ∑s=1 exp(f_pq·f̄_s/φ_s)"
- **Break condition**: If the 2D masks are too noisy or contain systematic labeling errors, contrastive learning may converge to incorrect segmentations.

### Mechanism 2
- **Claim**: Spatial-similarity regularization enforces that nearby Gaussians in 3D space have similar feature vectors, improving segmentation accuracy.
- **Mechanism**: A regularization term encourages cosine similarity between features of nearby Gaussians while discouraging similarity between distant Gaussians.
- **Core assumption**: Segmentation features should vary smoothly in 3D space, reflecting the spatial continuity of objects.
- **Evidence anchors**: [section 3.2] "We include spatial-similarity regularization to enforce spatial continuity of the feature vectors, encouraging adjacent 3D Gaussians to have similar segmentation feature vectors while discouraging faraway Gaussians from having the same segmentation features."; [section 4.3] "The results, reported in Table 3, show that on most scenes the spatial-similarity loss results in a significant performance improvement, of on average 78.8% against 80.3%."
- **Break condition**: If the regularization strength is too high, it may oversmooth features and merge distinct objects.

### Mechanism 3
- **Claim**: Embedding learnable feature vectors in 3D Gaussians allows direct access to 3D segmentation features for clustering and novel view synthesis.
- **Mechanism**: Each Gaussian stores a fixed feature vector in addition to position, color, and opacity, enabling segmentation via clustering and mask generation via α-blending of features.
- **Core assumption**: A fixed, view-independent feature vector per Gaussian is sufficient to encode instance segmentation information.
- **Evidence anchors**: [abstract] "we train a model to include also a segmentation feature vector for each Gaussian. These can then be used for 3D scene segmentation, by clustering Gaussians according to their feature vectors."; [section 3.2] "The 3D feature field is a collection of learnable-vectors stored on the 3D Gaussians, that encode the instance segmentation of the scene."
- **Break condition**: If the feature dimensionality is too low, it may not capture sufficient discriminative information for accurate clustering.

## Foundational Learning

- **Concept**: 3D Gaussian Splatting (3DGS)
  - **Why needed here**: 3DGS provides the underlying representation of scenes as collections of 3D Gaussians, which is extended with segmentation features.
  - **Quick check question**: What are the key parameters that define a 3D Gaussian in 3DGS, and how are they used during rendering?

- **Concept**: Contrastive learning
  - **Why needed here**: Contrastive learning is used to enforce multi-view consistency by maximizing similarity within segments and minimizing similarity across segments in feature space.
  - **Quick check question**: How does the temperature parameter in the contrastive loss affect the learning dynamics?

- **Concept**: Spatial regularization in feature space
  - **Why needed here**: Spatial regularization ensures that segmentation features vary smoothly with 3D geometry, improving segmentation quality.
  - **Quick check question**: What is the effect of varying the λnear and λfar parameters in the spatial-similarity regularization term?

## Architecture Onboarding

- **Component map**: 3D Gaussians -> Rendering pipeline -> Loss functions (rendering, contrastive, spatial regularization) -> Training loop (optimization, pruning, densification)

- **Critical path**:
  1. Initialize 3D Gaussians from SfM point cloud
  2. Extract 2D segmentation masks from input images
  3. Optimize Gaussians to minimize rendering loss and contrastive clustering loss
  4. Apply spatial-similarity regularization
  5. Prune and densify Gaussians as needed

- **Design tradeoffs**:
  - Feature dimensionality vs. memory usage: Higher dimensionality allows better discrimination but increases memory.
  - Contrastive loss frequency vs. training speed: More frequent updates improve consistency but slow training.
  - Regularization strength vs. segmentation accuracy: Stronger regularization improves smoothness but may oversmooth.

- **Failure signatures**:
  - Poor segmentation accuracy: May indicate insufficient feature dimensionality or incorrect contrastive loss parameters.
  - Inconsistent masks across views: May indicate insufficient contrastive learning or incorrect spatial regularization.
  - High memory usage: May indicate too many Gaussians or too high feature dimensionality.

- **First 3 experiments**:
  1. Train a basic 3DGS model without segmentation features to establish baseline rendering quality.
  2. Add segmentation features and contrastive loss, train on a simple scene, and evaluate 2D mask consistency.
  3. Introduce spatial-similarity regularization and evaluate its impact on 3D segmentation accuracy.

## Open Questions the Paper Calls Out
None explicitly mentioned in the paper.

## Limitations
- Scalability to very large scenes is constrained by memory requirements for storing feature vectors on potentially millions of Gaussians.
- Performance may be sensitive to hyperparameter choices for contrastive learning and spatial regularization.
- Limited evaluation to only two benchmark datasets, though results are strong.

## Confidence
- **High confidence** in the core mechanism of using contrastive learning on 2D features for multi-view consistency
- **Medium confidence** in the spatial regularization component, as ablation studies show clear benefits but performance gains vary across scenes
- **Medium confidence** in the weak supervision claims, as the paper demonstrates effectiveness with inconsistent 2D masks but doesn't extensively explore scenarios with highly noisy annotations

## Next Checks
1. Test model sensitivity to contrastive loss temperature parameter across a wider range to identify optimal values for different scene complexities.
2. Evaluate performance degradation when 2D segmentation masks contain increasing levels of noise or systematic errors to quantify robustness limits.
3. Measure memory usage and runtime scaling as scene complexity increases to determine practical constraints for real-world applications.