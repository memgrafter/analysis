---
ver: rpa2
title: Why Larger Language Models Do In-context Learning Differently?
arxiv_id: '2405.19592'
source_url: https://arxiv.org/abs/2405.19592
tags:
- learning
- in-context
- have
- larger
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates why larger language models exhibit different
  in-context learning (ICL) behaviors compared to smaller models, specifically focusing
  on their sensitivity to noise in test contexts. The authors analyze two stylized
  theoretical settings: (1) linear regression with one-layer single-head linear transformers
  and (2) parity classification with two-layer multiple attention heads transformers.'
---

# Why Larger Language Models Do In-context Learning Differently?

## Quick Facts
- arXiv ID: 2405.19592
- Source URL: https://arxiv.org/abs/2405.19592
- Reference count: 40
- Primary result: Larger language models are more sensitive to noise in test contexts because they attend to more features, including unimportant ones, while smaller models are more robust by focusing on key features.

## Executive Summary
This paper investigates why larger language models exhibit different in-context learning (ICL) behaviors compared to smaller models, particularly their sensitivity to noise in test contexts. Through theoretical analysis of stylized settings—linear regression with rank-constrained transformers and parity classification with multi-head attention—the authors derive closed-form optimal solutions showing that smaller models focus on important hidden features while larger ones cover more features, including noisy ones. This leads to smaller models being more robust to noise during evaluation, while larger models are more easily distracted. The theoretical findings are supported by preliminary experimental results on large base and chat models.

## Method Summary
The authors analyze two stylized theoretical settings: (1) linear regression with one-layer single-head linear transformers and (2) parity classification with two-layer multiple attention heads transformers. They derive closed-form optimal solutions for both settings, showing how model scale (rank or head count) controls feature coverage and noise sensitivity. The analysis reveals that smaller models achieve robustness by focusing on high-variance feature directions, while larger models degrade by overfitting to in-context examples and attending to noisy directions. The theoretical findings are supported by preliminary experimental results on large base and chat models.

## Key Results
- Smaller models achieve robustness by focusing attention on high-variance feature directions, filtering out noise in low-variance ones
- Larger models degrade ICL performance by overfitting to in-context examples and attending to noisy feature directions
- The trade-off between signal coverage and noise sensitivity is formalized mathematically, showing that increasing model scale captures more useful signals but also more noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller models achieve robustness by focusing attention on high-variance feature directions, ignoring noise in low-variance ones.
- Mechanism: During pretraining, rank-constrained transformers learn optimal weight matrices that concentrate on top eigenvectors of the token covariance. At evaluation, noise distributed across all directions is filtered out by ignoring the low-variance components.
- Core assumption: Noise is evenly distributed across all feature directions while signal concentrates in high-variance ones.
- Evidence anchors:
  - [abstract]: "smaller models emphasize important hidden features while larger ones cover more hidden features; thus, smaller models are more robust to noise"
  - [section]: Theorem 4.1 shows optimal rank-r solution keeps only first r eigenvectors of token covariance
  - [corpus]: Weak - no direct mentions of noise robustness in related works

### Mechanism 2
- Claim: Larger models degrade because they overfit to in-context examples, overriding pretrained biases.
- Mechanism: With more attention heads, transformers learn to cover all hidden features including less important ones. When label noise is injected, the larger model shifts attention toward the noisy directions, losing the original bias.
- Core assumption: Pretraining has established strong semantic biases that smaller models preserve.
- Evidence anchors:
  - [abstract]: "larger models may have a worse ICL ability than the small language models... larger language models may overfit into the prompts"
  - [section]: Theorem 5.1 shows optimal solution for more heads covers additional non-important features
  - [corpus]: Weak - related works discuss ICL but not overfitting to prompts specifically

### Mechanism 3
- Claim: The trade-off between signal coverage and noise sensitivity drives ICL behavior differences.
- Mechanism: Model scale (rank or head count) controls a bias-variance trade-off: increasing scale captures more useful signals but also more noise. Smaller models underfit useful signals but are robust; larger models overfit noise but capture more signal.
- Core assumption: There is a fixed set of important vs. unimportant features, and noise is present in both.
- Evidence anchors:
  - [abstract]: "smaller models emphasize important hidden features while larger ones cover more hidden features; thus, smaller models are more robust to noise"
  - [section]: Theorem 4.2 and Theorem 5.2 formalize this trade-off mathematically
  - [corpus]: Weak - related works do not explicitly discuss this bias-variance trade-off in ICL

## Foundational Learning

- Concept: Eigenvalue decomposition of token covariance matrix
  - Why needed here: Underlies how transformers decompose input space into hidden feature directions; key to understanding what features are covered at different scales.
  - Quick check question: Given a symmetric positive semidefinite matrix, what does its eigendecomposition reveal about its action on vectors?

- Concept: Rank-constrained optimization in neural nets
  - Why needed here: Explains how model scale is operationalized as a rank constraint on attention matrices; determines which features can be represented.
  - Quick check question: What is the effect of constraining a matrix to have rank ≤ r on its expressiveness and generalization?

- Concept: Bias-variance trade-off in statistical learning
  - Why needed here: Provides the conceptual framework for why larger models can both improve (more signal) and degrade (more noise) performance.
  - Quick check question: In a fixed-feature regression, how does increasing model complexity affect bias and variance?

## Architecture Onboarding

- Component map: Token embedding matrix E -> Attention matrices WKQ, WP V -> Activation (ReLU for parity) -> Head aggregation -> Prediction

- Critical path:
  1. Parse prompt → build E
  2. Compute attention via WKQ, WP V
  3. Apply activation (ReLU for parity) and head aggregation
  4. Compare prediction to true label
  5. Update via gradient descent on pretraining loss

- Design tradeoffs:
  - Rank vs. model size: more rank → more features covered → potentially more noise sensitivity
  - Linear vs. non-linear models: non-linear (parity case) allows richer feature combinations but more complex optimization
  - Sparse vs. dense features: sparsity in parity case makes certain features more important

- Failure signatures:
  - Performance degrades on noisy prompts but improves on clean ones → likely overfit to prompt
  - Consistent underperformance across prompt types → possibly underfitting (too small rank)
  - Performance varies wildly with prompt length → possibly unstable attention patterns

- First 3 experiments:
  1. Vary rank r in synthetic linear regression with Gaussian noise; measure test loss vs. training set size.
  2. Vary head count m in parity classification with sparse labels; inject varying label noise; measure accuracy.
  3. Apply same rank/h and noise setup to a real dataset (e.g., SST-2) with synthetic irrelevant context; measure attention magnitude to irrelevant tokens.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal solution structure change when extending from linear regression to non-linear models with more complex attention mechanisms?
- Basis in paper: The paper analyzes linear regression with one-layer single-head linear transformers and parity classification with two-layer multiple-head transformers, suggesting the structure may generalize.
- Why unresolved: The paper focuses on specific stylized settings and does not explore more complex model architectures or non-linear attention mechanisms beyond the parity classification case.
- What evidence would resolve it: Extending the analysis to models with multiple layers, non-linear attention mechanisms, and more complex data distributions, and deriving optimal solutions and characterizing their behavior.

### Open Question 2
- Question: How do different pretraining strategies affect the robustness of large language models to noise during in-context learning?
- Basis in paper: The paper discusses pretraining noise and its interaction with inference noise, but does not explore different pretraining strategies.
- Why unresolved: The paper focuses on the effect of model scale and noise during inference, but does not investigate how different pretraining approaches might influence the model's robustness.
- What evidence would resolve it: Conducting experiments comparing the performance of large language models with different pretraining strategies (e.g., different data sources, pretraining objectives) on in-context learning tasks with varying levels of noise.

### Open Question 3
- Question: How can we design training objectives or architectures that improve the robustness of large language models to noise during in-context learning?
- Basis in paper: The paper identifies the trade-off between model scale and robustness to noise, but does not propose solutions to mitigate this issue.
- Why unresolved: The paper provides insights into the behavior of large language models during in-context learning but does not offer concrete methods to improve their robustness.
- What evidence would resolve it: Developing and evaluating new training objectives or architectural modifications that explicitly encourage robustness to noise during in-context learning, and demonstrating their effectiveness through experiments.

## Limitations

- The theoretical analysis is confined to highly stylized synthetic settings and may not generalize to real-world transformers with nonlinearities and residual connections
- Experimental validation is preliminary and limited to showing consistency with previous observations rather than testing predictions across diverse conditions
- The paper doesn't address whether the same principles apply to other forms of noise (input vs. label noise, structured vs. unstructured)

## Confidence

- **High Confidence**: The core mathematical framework for analyzing feature coverage in rank-constrained transformers is sound, with rigorous derivation of optimal solutions
- **Medium Confidence**: The claim that larger models "overfit to prompts" is plausible but may oversimplify the mechanism
- **Low Confidence**: Generalization of these findings to practical large language models with billions of parameters and complex pretraining objectives

## Next Checks

1. **Empirical Test of Feature Coverage**: Train transformers of varying ranks/head counts on synthetic data with known important vs. unimportant features. Measure attention magnitudes to each feature type and verify that larger models attend to more unimportant features as predicted.

2. **Noise Sensitivity Experiment**: Using the same setup, inject varying levels of noise into test prompts and measure performance degradation. Compare this to the theoretical prediction that smaller models are more robust to noise while larger models are more sensitive.

3. **Real-World Generalization**: Apply the theoretical framework to a practical ICL task (e.g., few-shot classification on SST-2 with irrelevant context). Measure whether model scale correlates with attention to irrelevant tokens and whether this predicts noise sensitivity in evaluation.