---
ver: rpa2
title: Investigating large language models for their competence in extracting grammatically
  sound sentences from transcribed noisy utterances
arxiv_id: '2410.05099'
source_url: https://arxiv.org/abs/2410.05099
tags:
- llms
- utterances
- language
- speech-specific
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  extract grammatically sound sentences from transcriptions of noisy spoken utterances.
  The study uses a dataset of Polish dialogues annotated with speech-specific elements
  like pauses, disfluencies, and restarts.
---

# Investigating large language models for their competence in extracting grammatically sound sentences from transcribed noisy utterances

## Quick Facts
- **arXiv ID**: 2410.05099
- **Source URL**: https://arxiv.org/abs/2410.05099
- **Reference count**: 19
- **Primary result**: GPT-4 and GPT-3.5 achieve high recall (94-97%) in extracting complete structures but retain many speech-specific elements, while Mistral shows high precision (TNR 91%) in filtering speech noise but often removes required syntactic components

## Executive Summary
This study investigates whether large language models can extract grammatically sound sentences from transcriptions of noisy spoken utterances using Polish dialogue data. The research evaluates three tasks: extracting well-structured utterances, filtering discourse elements, and removing disfluencies and restarts. Five LLMs (GPT-4, GPT-3.5, Mistral, Llama, and Bielik) are tested against a gold-standard dataset annotated with speech-specific elements. Results show that while GPT models excel at preserving complete structures, they struggle with filtering speech-specific noise, whereas Mistral performs well on precision but often removes essential grammatical components. The study concludes that LLMs demonstrate only superficial language competence compared to human processing of noisy utterances, particularly struggling with false starts and other speech-specific phenomena.

## Method Summary
The study uses the DiaBiz dataset of Polish dialogues annotated with speech-specific elements including pauses, disfluencies, and restarts. Researchers created a probing dataset where each token is labeled as well-structured (True) or speech-specific (False). Five LLMs (GPT-4, GPT-3.5, Mistral, Llama, and Bielik) were prompted using few-shot examples to identify and extract tokens composing well-structured utterances. Performance was evaluated using accuracy, precision, recall, F1-measure, and true negative rate (TNR) against a gold-standard dataset using Universal Dependencies tree approximation. The evaluation focused on three tasks: extracting complete syntactic-semantic structures, filtering discourse elements, and removing disfluencies and restarts.

## Key Results
- GPT-4 and GPT-3.5 achieve high recall (94-97%) in extracting complete structures but retain many speech-specific elements
- Mistral achieves high precision in filtering speech-specific elements (TNR 91%) but often removes required syntactic components
- All LLMs struggle particularly with false starts and restarts, with GPTs showing only 30% restart detection accuracy
- LLMs demonstrate superficial language competence compared to humans in processing noisy utterances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT models retain more speech-specific tokens compared to Mistral, resulting in lower precision for filtering speech noise.
- Mechanism: GPTs adopt a cautious filtering strategy that prioritizes avoiding false negatives (over-filtering), leading to higher recall but lower precision in speech noise removal.
- Core assumption: GPTs are trained with a bias toward preserving potentially relevant content to maintain coherence, even at the cost of retaining some speech-specific noise.
- Evidence anchors:
  - [abstract] "while GPT-3.5 and GPT-4 perform well on extracting complete structures (recall 94-97%), they retain many speech-specific elements"
  - [section 4.1] "The TNR scores, indicating the quality of detected speech-specific segments, are lower in comparison to the accuracy scores... GPTs and Bielik incorporate many infrequent speech-specific tokens into the ultimate utterances"
- Break condition: If the task requires strict precision in filtering speech noise rather than completeness of extracted utterances.

### Mechanism 2
- Claim: Mistral achieves high precision in filtering speech-specific elements but often removes required syntactic components, leading to grammatical errors.
- Mechanism: Mistral employs an aggressive filtering strategy that removes not only speech-specific noise but also essential syntactic elements like arguments and predicates.
- Core assumption: Mistral's filtering algorithm lacks the nuanced understanding to distinguish between speech noise and grammatically necessary components.
- Evidence anchors:
  - [abstract] "Mistral achieves high precision in filtering speech-specific elements (TNR 91%) but often removes required syntactic components"
  - [section 4.1] "Mistral demonstrates remarkable efficacy in filtering discourse, reparandum and restart segments... However, its filtering tends to be overly aggressive, excluding not only speech noise but also elements of predicate-argument structure"
- Break condition: When the task requires maintaining grammatical integrity while filtering speech noise.

### Mechanism 3
- Claim: LLMs struggle with false starts (restarts) because they cannot identify unfinished statements intended to be replaced by restarts.
- Mechanism: LLMs treat false starts as syntactically or semantically sound parts of utterances rather than recognizing them as semantically irrelevant in the current context.
- Core assumption: LLMs lack the contextual awareness to identify that a clause is a false start when it could be acceptable in other contexts.
- Evidence anchors:
  - [abstract] "The study concludes that LLMs have superficial language competence compared to humans in processing noisy utterances, struggling particularly with false starts and other speech-specific phenomena"
  - [section 4.2] "As evidenced by the low restart values, such as 30% for GPTs, 40-50% for Bielik and 66% for Llama, LLMs struggle to recognise the restart phenomenon"
- Break condition: If LLMs develop the ability to understand contextual relevance of speech elements within specific dialogue contexts.

## Foundational Learning

- Concept: Syntactic-semantic structure (AS)
  - Why needed here: Understanding how LLMs process utterances requires knowledge of how humans internally represent grammatical and meaningful structures
  - Quick check question: What is the difference between syntactic structure and semantic structure, and why might they be combined into an abstract syntactic-semantic structure?

- Concept: Speech-specific phenomena (disfluencies, restarts, pauses)
  - Why needed here: The paper's core evaluation depends on distinguishing between grammatically relevant content and speech-specific noise
  - Quick check question: How do filled pauses differ from false starts in terms of their syntactic and semantic impact on utterance comprehension?

- Concept: Universal Dependencies framework
  - Why needed here: The evaluation methodology uses UD trees to approximate abstract syntactic-semantic structures
  - Quick check question: What are the key differences between UD dependency types and traditional phrase structure grammar?

## Architecture Onboarding

- Component map:
  - Input transcription -> LLM processing with prompt -> Output generation -> Evaluation against gold standard -> Performance metrics calculation

- Critical path: Raw Polish dialogue transcriptions with speech-specific annotations → LLM processing with task-specific prompts → Output generation of well-structured utterances → Evaluation using UD tree approximation against gold standard → Calculation of accuracy, precision, recall, F1, and TNR metrics

- Design tradeoffs:
  - Precision vs recall in filtering speech noise (Mistral favors precision, GPTs favor recall)
  - Aggressive vs cautious filtering strategies
  - Language-specific vs universal linguistic rules
  - Few-shot examples vs zero-shot prompting effectiveness

- Failure signatures:
  - Missing core arguments in predicate-argument structures
  - Incorrect filtering of false starts (treating them as valid content)
  - Generation of out-of-vocabulary words instead of filtering
  - Inconsistent handling of speech-specific phenomena across different models

- First 3 experiments:
  1. Test each LLM with a controlled set of simple false starts to measure restart detection accuracy
  2. Compare aggressive vs cautious filtering strategies on the same dataset to quantify precision-recall tradeoffs
  3. Evaluate model performance on Polish-specific vs English-specific speech phenomena to assess cross-linguistic generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs internalize abstract syntactic-semantic structures (AS) in a way similar to human language processing, or do they rely on different mechanisms?
- Basis in paper: [explicit] The paper states "LLM is expected to internalise ASs, akin to human language processing" but questions whether this occurs, noting that "LLMs struggle to identify complete and coherent sentences in noisy utterances" suggesting "superficial competence" or inability to "apply all syntactic-semantic rules."
- Why unresolved: The study tests LLM outputs against gold-standard structures but cannot directly inspect LLM internal representations to verify if they construct or represent ASs similarly to humans. The paper notes they "lack insight into LLMs' layers where speech-specific elements are recognised and syntactic-semantic structures are internalised."
- What evidence would resolve it: Direct probing of LLM internal representations to identify regions corresponding to AS construction, or psycholinguistic experiments comparing LLM behavior to human processing patterns.

### Open Question 2
- Question: What specific psycholinguistic factors might influence LLMs' inability to effectively apply acquired syntactic-semantic rules when processing noisy utterances?
- Basis in paper: [explicit] The paper mentions "psycholinguistic factors, such as shallow heuristics mixed with syntactic algorithms (Ferreira, 2003) or rational statistical inference (Gibson et al., 2013), could impact the behaviour of LLMs" as suggested by a reviewer.
- Why unresolved: The study focuses on evaluating LLM outputs rather than investigating underlying cognitive mechanisms. The paper acknowledges this limitation, stating "we do not inspect LLM's internal architectures to identify specific regions related to distinct linguistic features."
- What evidence would resolve it: Controlled experiments applying psycholinguistic theories to LLM behavior, or comparative analysis of LLM processing patterns against established psycholinguistic models of human language comprehension.

### Open Question 3
- Question: How do cross-linguistic differences affect LLMs' ability to process speech-specific phenomena in languages other than Polish?
- Basis in paper: [inferred] The paper acknowledges a limitation in using only Polish data, noting it "poses an additional challenge for LLMs, requiring them to process a non-dominant language and a non-dominant text domain" and suggesting "certain conclusions can also be drawn regarding LLMs' competence in cross-linguistically capturing universal linguistic properties."
- Why unresolved: The study is restricted to Polish due to dataset availability and contamination concerns, making generalization to other languages uncertain. The paper explicitly states this is a limitation and hopes "this research will be positively received by the NLP community, creating opportunities for broader research in the future."
- What evidence would resolve it: Replication of the experiments with annotated datasets in multiple languages, particularly those with different syntactic structures or speech patterns than Polish.

## Limitations

- The study is limited to Polish dialogue data, which may not generalize to other languages or different types of spontaneous speech
- The evaluation methodology using UD tree approximation may not fully capture the complexity of human speech processing and may oversimplify syntactic-semantic structures
- The study cannot directly inspect LLM internal representations to verify whether models internalize abstract syntactic-semantic structures in a manner similar to human language processing

## Confidence

- **High confidence**: GPT models retain more speech-specific tokens while achieving higher recall, and Mistral achieves higher precision but at the cost of grammatical completeness
- **Medium confidence**: The specific numerical performance metrics (recall 94-97%, TNR 91%) are based on a single dataset and may vary with different speech corpora
- **Low confidence**: The generalizability of these findings to other languages beyond Polish and to spontaneous speech in different domains

## Next Checks

1. **Cross-linguistic validation**: Test the same LLMs on English and other language dialogue corpora with speech-specific annotations to determine if the observed patterns of GPTs favoring completeness over precision, and Mistral favoring precision over completeness, hold across languages.

2. **Error analysis on false starts**: Conduct a detailed qualitative analysis of the specific types of false starts that LLMs fail to identify, distinguishing between grammatical false starts (that could be valid in other contexts) and semantically irrelevant ones, to better understand the nature of the models' limitations.

3. **Human baseline comparison**: Collect human judgments on the same dataset to establish baseline performance for speech noise filtering, allowing for a more nuanced comparison between human and LLM processing of noisy utterances beyond the current UD-based approximation.