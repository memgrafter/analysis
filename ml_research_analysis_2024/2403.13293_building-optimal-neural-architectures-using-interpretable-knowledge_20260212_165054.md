---
ver: rpa2
title: Building Optimal Neural Architectures using Interpretable Knowledge
arxiv_id: '2403.13293'
source_url: https://arxiv.org/abs/2403.13293
tags:
- search
- architectures
- autobuild
- stage
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoBuild learns to score architecture modules by aligning GNN
  embeddings with performance labels, enabling direct construction of high-quality
  neural networks without full search. By mining a small set of labeled architectures,
  it assigns interpretable importance scores to operations, subgraphs, and configurations.
---

# Building Optimal Neural Architectures using Interpretable Knowledge

## Quick Facts
- arXiv ID: 2403.13293
- Source URL: https://arxiv.org/abs/2403.13293
- Reference count: 40
- One-line primary result: AutoBuild learns to score architecture modules by aligning GNN embeddings with performance labels, enabling direct construction of high-quality neural networks without full search.

## Executive Summary
AutoBuild is a method for constructing high-performance neural architectures by learning interpretable importance scores for operations, subgraphs, and configurations. It uses a GNN to embed architecture DAGs and trains with a ranking loss that aligns embedding norms with performance labels. By mining a small set of labeled architectures, AutoBuild assigns scores to architecture components and constructs superior architectures with reduced search space compared to traditional methods.

## Method Summary
AutoBuild uses a Graph Attention Network (GATv2) to embed neural architectures represented as directed acyclic graphs. The GNN is trained with a hop-level ranking loss based on Spearman's Rank Correlation Coefficient, forcing embeddings to rank according to performance labels. A Feature Embedding MLP (FE-MLP) scores individual node features independently. Subgraph importance scores from different hop levels are compared using distribution shift normalization. The method constructs architectures by combining top-scoring subgraphs from different stages while maintaining consistent channel constraints.

## Key Results
- AutoBuild constructs superior architectures compared to both labeled architectures and exhaustive search baselines
- Reduces search space by directly scoring architecture modules instead of exhaustive evaluation
- Demonstrates effectiveness across ImageNet classification, panoptic segmentation, and Stable Diffusion models
- Works with as few as 68 labeled architectures for Stable Diffusion U-Net

## Why This Works (Mechanism)

### Mechanism 1
Learning a magnitude-ranked embedding space enables interpretable importance scoring of architecture modules. The GNN predictor is trained with a ranking loss that aligns embedding norms with ground-truth performance labels, so higher-performing architectures induce larger embedding norms for their constituent subgraphs and nodes.

### Mechanism 2
Hop-level subgraph embeddings can be compared across different sizes via distribution shift normalization. After training, node embedding norms from different hop levels are standardized to their own distributions, then unstandardized to the target performance distribution for the corresponding subgraph size, enabling cross-hop comparison.

### Mechanism 3
Feature embedding MLPs enable direct importance scoring of individual node features without exhaustive evaluation. Each node feature category is processed by a separate MLP to produce a scalar, concatenated and passed through an absolute nonlinearity to form the 0-hop embedding; the ranking loss ensures these scalars reflect feature importance.

## Foundational Learning

- **Graph Neural Networks (GNNs) and message passing**: AutoBuild uses a GNN to embed architecture DAGs and learn subgraph importance scores. *Quick check*: What is the receptive field of a node after m message passing layers in a GNN?
- **Ranking correlation and differentiable ranking metrics**: The hop-level ranking loss uses Spearman's Rank Correlation Coefficient to align embedding norms with performance labels. *Quick check*: How does differentiable Spearman's Rank Correlation enable its use as a loss function in training?
- **Distribution normalization and standardization**: Subgraph scores from different hop levels are compared by standardizing to their own distributions and then unstandardizing to the target performance distribution. *Quick check*: Why is it necessary to standardize to N(µh m, σh m) before unstandardizing to N(µy u,l, σy u,l)?

## Architecture Onboarding

- **Component map**: GATv2 GNN backbone with 4 message passing layers -> Feature Embedding MLP (FE-MLP) for node feature importance -> Magnitude Ranked Embedding Space training with hop-level ranking loss -> Distribution shift normalization for cross-hop subgraph comparison
- **Critical path**: Train GNN predictor with ranking loss → Compute subgraph importance scores → Construct/select architectures based on scores → Evaluate performance
- **Design tradeoffs**: Hop-level ranking loss vs. standard regression loss enables interpretability but may require more training data; Independent feature MLPs vs. joint feature processing is simpler but may miss feature interactions; Distribution shift normalization vs. direct comparison accounts for size biases but adds complexity
- **Failure signatures**: Low ranking correlation (SRCC) after training indicates poor embedding space learning; Subgraph scores not reflecting known performance trends indicate normalization issues; Constructed architectures performing worse than random indicates importance scoring errors
- **First 3 experiments**:
  1. Train AutoBuild predictor on a small set of labeled architectures and verify high SRCC at all hop levels.
  2. Compute and compare subgraph importance scores before and after distribution shift normalization to verify the effect.
  3. Construct a few architectures using top-scoring subgraphs and evaluate their performance against the training set to verify the effectiveness of the importance scoring.

## Open Questions the Paper Calls Out

### Open Question 1
How does AutoBuild's performance scale with dataset size for stable diffusion U-Net architectures, particularly when the number of labeled architectures is extremely small (e.g., fewer than 50)? The paper only tested with 68 samples and mentions potential issues with even smaller datasets, but doesn't provide empirical results or theoretical analysis for the extreme case.

### Open Question 2
What is the theoretical relationship between hop-level embedding norm distributions and target metric distributions that would allow optimal distribution shift calculations without relying on empirical statistics? The paper uses empirical statistics for distribution shifts but doesn't explore the theoretical foundation or alternative approaches that might work with limited data.

### Open Question 3
How can AutoBuild's target equation design process be automated to eliminate manual crafting while maintaining or improving performance? The paper notes "our equations are manually inferred from the data which is a limitation" and mentions "Automating the equation-creation process is a future research direction."

## Limitations
- Reliance on a small labeled dataset for training the GNN predictor may limit generalization to architectures outside the training distribution
- Distribution shift normalization approach assumes consistent performance distribution patterns across different subgraph sizes, which may not hold for all tasks or domains
- FE-MLP's independent feature scoring approach may miss complex interactions between node features that could affect importance scoring accuracy

## Confidence

- **High Confidence**: The core mechanism of using GNN embeddings with ranking correlation for architecture scoring is well-established and the experimental results demonstrate consistent improvements across multiple tasks and domains.
- **Medium Confidence**: The distribution shift normalization approach for cross-hop subgraph comparison is theoretically sound but requires empirical validation across diverse architectural spaces.
- **Medium Confidence**: The FE-MLP's ability to score individual node features without exhaustive evaluation is promising but may have limitations in capturing feature interactions.

## Next Checks

1. **Generalization Test**: Evaluate AutoBuild's performance on architectures with novel combinations of operations and subgraph sizes not present in the training set to assess generalization capability.

2. **Normalization Robustness**: Test the distribution shift normalization approach on tasks where larger subgraphs do not necessarily correlate with higher performance to validate its robustness.

3. **Feature Interaction Analysis**: Compare AutoBuild's feature importance scores with those obtained from exhaustive evaluation of feature combinations to quantify the impact of independent feature scoring.