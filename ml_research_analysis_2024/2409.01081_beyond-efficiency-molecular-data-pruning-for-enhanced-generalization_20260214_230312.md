---
ver: rpa2
title: 'Beyond Efficiency: Molecular Data Pruning for Enhanced Generalization'
arxiv_id: '2409.01081'
source_url: https://arxiv.org/abs/2409.01081
tags:
- data
- pruning
- learning
- dataset
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of efficient training for molecular
  tasks using large pretrained models, where traditional data pruning methods fail
  due to domain shifts between pretraining and downstream tasks. The authors propose
  MolPeg, a source-free data pruning framework that maintains two models with different
  update paces to perceive both source and target domains.
---

# Beyond Efficiency: Molecular Data Pruning for Enhanced Generalization

## Quick Facts
- **arXiv ID**: 2409.01081
- **Source URL**: https://arxiv.org/abs/2409.01081
- **Reference count**: 40
- **Key outcome**: MolPeg achieves lossless pruning (60-70% data reduction) while surpassing full-dataset performance on molecular tasks, particularly HIV and PCBA datasets.

## Executive Summary
This work addresses the challenge of efficient training for molecular tasks using large pretrained models, where traditional data pruning methods fail due to domain shifts between pretraining and downstream tasks. The authors propose MolPeg, a source-free data pruning framework that maintains two models with different update paces to perceive both source and target domains. MolPeg uses a novel scoring function based on loss discrepancy to identify informative samples, retaining both easy (representative) and hard (challenging) samples. Experiments on four molecular tasks show that MolPeg achieves lossless pruning while surpassing full-dataset performance, particularly on HIV and PCBA datasets. The method is robust across different pretraining strategies and molecular modalities, offering a promising path to enhanced efficiency and generalization in transfer learning for molecular applications.

## Method Summary
MolPeg is a source-free data pruning framework that maintains an online model (fast updating via backprop) and a reference model (slow updating via EMA) to perceive cross-domain knowledge. During training, both models infer each sample and calculate the absolute loss discrepancy between them. This discrepancy serves as an informativeness score, identifying both easy (representative) and hard (challenging) samples. The framework consists of two stages: scoring samples using the loss discrepancy function, then selecting the top-K most informative samples for training. The method is designed to work with large pretrained molecular models and addresses the domain shift problem that makes traditional pruning approaches ineffective for transfer learning scenarios.

## Key Results
- MolPeg achieves lossless pruning (60-70% data reduction) while surpassing full-dataset performance on HIV and PCBA classification tasks
- The method shows consistent improvement across different pretraining strategies (GraphMAE, GraphCL) and molecular modalities (2D graphs, 3D geometries)
- Dynamic pruning strategies significantly outperform static approaches, with MolPeg demonstrating superior generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Cross-domain perception via dual models with different update paces enables source-free data pruning.
- **Mechanism**: Maintaining an online model (fast update, target-focused) and a reference model (slow update via EMA, source-focused) creates a representation gap that reflects sample informativeness.
- **Core assumption**: The pretrained model encodes useful source-domain knowledge that can be preserved via EMA.
- **Evidence anchors**: [abstract] "By maintaining two models with different updating paces during training, we introduce a novel scoring function to measure the informativeness of samples based on the loss discrepancy." [section] "During the finetuning stage, apart from online encoder undergoing gradient optimization via back-propagation, we further maintain a reference encoder updated with exponential moving average (EMA) to perceive the cross-domain knowledge."

### Mechanism 2
- **Claim**: Loss discrepancy between models serves as an effective informativeness score.
- **Mechanism**: Absolute difference in losses from online and reference models identifies both easy (representative) and hard (challenging) samples, which are both retained.
- **Core assumption**: Loss discrepancy correlates with sample difficulty and representativeness in the transfer learning context.
- **Evidence anchors**: [abstract] "we design a novel scoring function to simultaneously select easy (representative) and hard (challenging) samples by comparing the absolute discrepancy between model losses." [section] "we adopt both online and reference encoder to infer each sample and calculate the absolute loss discrepancy between them"

### Mechanism 3
- **Claim**: Dynamic pruning during training outperforms static selection.
- **Mechanism**: Continuously updating sample scores allows adaptation to changing model behavior, capturing shifting difficulty patterns.
- **Core assumption**: Optimal training subsets change as model parameters evolve during training.
- **Evidence anchors**: [abstract] "Our systematic study suggests the following trends: (i) Dynamic DP strategies significantly outperform static DP strategies." [section] "MolPeg framework is divided into two stages, scoring and selection."

## Foundational Learning

- **Concept**: Exponential Moving Average (EMA)
  - **Why needed here**: EMA provides a stable reference model that captures source-domain knowledge while allowing the online model to adapt to the target domain.
  - **Quick check question**: What happens to the reference model if β = 1 versus β = 0 in the EMA update?

- **Concept**: Transfer Learning and Domain Shift
  - **Why needed here**: Understanding why traditional data pruning fails when applied to pretrained models requires recognizing the distribution shift between source and target domains.
  - **Quick check question**: Why can't we simply apply standard data pruning techniques designed for train-from-scratch scenarios to pretrained models?

- **Concept**: Loss Functions and Gradient Analysis
  - **Why needed here**: The scoring function relies on comparing losses between two models, requiring understanding of how loss relates to model behavior and sample difficulty.
  - **Quick check question**: How does the absolute loss discrepancy between two models indicate sample informativeness?

## Architecture Onboarding

- **Component map**: Pretrained model → Online/Reference model training → Loss discrepancy calculation → Sample scoring → Top-K selection → Training on pruned dataset
- **Critical path**: Pretrained model (θ₀ = ξ₀ = θ_S) → Online model (θ_t, fast updating via backprop) → Reference model (ξ_t, slow updating via EMA) → Loss discrepancy calculation → Sample scoring → Top-K selection → Training on pruned dataset
- **Design tradeoffs**: EMA pace (β) balances between source retention and target adaptation; scoring function complexity vs. computational overhead; dynamic vs. static pruning frequency
- **Failure signatures**: Poor performance on easy samples: β too small, reference model not capturing source knowledge; poor performance on hard samples: β too large, reference model too close to online model; no improvement over random pruning: loss discrepancy not discriminative enough
- **First 3 experiments**: 1) Compare MolPeg with static random pruning on HIV dataset at 60% ratio; 2) Vary EMA pace β across [0.001, 0.01, 0.1, 0.5, 0.9] to find optimal value; 3) Test MolPeg with different pretrained models (GraphMAE vs GraphCL) on same downstream task

## Open Questions the Paper Calls Out

- **Open Question 1**: How would MolPeg perform on source-free data pruning tasks in natural language processing or computer vision domains?
  - **Basis in paper**: [explicit] The paper mentions that MolPeg is designed for molecular tasks but notes that source-free data pruning has broader applications in other fields like LLMs and vision models.
  - **Why unresolved**: The authors acknowledge this as a limitation and future research direction, but have not yet tested MolPeg on non-molecular tasks.
  - **What evidence would resolve it**: Conducting experiments on source-free data pruning tasks using large language models or vision models would demonstrate whether MolPeg's effectiveness generalizes beyond molecular applications.

- **Open Question 2**: Can alternative methods for perceiving source and target domain knowledge improve MolPeg's performance beyond using loss discrepancy?
  - **Basis in paper**: [explicit] The authors state that their method "only made simple attempts at perceiving upstream and downstream knowledge via loss discrepancy" and suggest this leaves "significant potential to be explored."
  - **Why unresolved**: The paper presents loss discrepancy as their chosen method but acknowledges it as a first attempt rather than an optimal solution.
  - **What evidence would resolve it**: Testing MolPeg with alternative domain perception methods (such as feature similarity metrics, domain adversarial training, or contrastive learning approaches) and comparing performance would identify more effective approaches.

- **Open Question 3**: How sensitive is MolPeg's performance to the choice of EMA update pace parameter β?
  - **Basis in paper**: [explicit] The authors conduct sensitivity analysis showing moderate performance sensitivity to β, with β=0.5 typically performing best, but they don't explore the full parameter space or provide theoretical guidance.
  - **Why unresolved**: While the paper tests a limited range of β values, it doesn't establish theoretical bounds or explore how performance scales across the full range of possible values.
  - **What evidence would resolve it**: Systematic experimentation across a wider range of β values, coupled with theoretical analysis of the EMA dynamics, would clarify optimal parameter selection and robustness.

## Limitations
- The effectiveness of MolPeg relies heavily on the assumption that pretrained models capture transferable source knowledge that remains relevant for downstream tasks.
- The study focuses on molecular property prediction tasks with specific model architectures (GIN, PaiNN, SchNet), limiting generalization to other domains.
- Performance gains are inconsistent across datasets, with more modest improvements on QM9 compared to HIV and PCBA.

## Confidence
- **High confidence**: The dual-model architecture with EMA updates is technically sound and well-supported by experimental results showing consistent improvement over baselines.
- **Medium confidence**: The loss discrepancy scoring function effectively captures sample informativeness, though the exact relationship between loss differences and true sample difficulty could benefit from further theoretical analysis.
- **Medium confidence**: The claim that MolPeg "surpasses full-dataset performance" is supported for specific tasks (HIV, PCBA) but not universally across all tested datasets.

## Next Checks
1. **Cross-domain validation**: Test MolPeg on non-molecular tasks (e.g., image classification with pretrained vision models) to assess domain generality of the dual-model pruning approach.
2. **Ablation study on EMA parameters**: Systematically vary the EMA update pace β across a wider range to identify the sensitivity of performance to this hyperparameter and determine optimal values for different task types.
3. **Theoretical analysis of loss discrepancy**: Conduct a deeper investigation into why loss discrepancy between online and reference models correlates with sample informativeness, potentially through visualization of learned representations or gradient analysis.