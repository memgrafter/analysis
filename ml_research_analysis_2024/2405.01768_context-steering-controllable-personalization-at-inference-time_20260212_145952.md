---
ver: rpa2
title: 'Context Steering: Controllable Personalization at Inference Time'
arxiv_id: '2405.01768'
source_url: https://arxiv.org/abs/2405.01768
tags:
- context
- generation
- bias
- personalized
- movie
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Context Steering (CoS), a training-free\
  \ decoding approach that enables controllable personalization of LLM outputs by\
  \ amplifying or attenuating the influence of user-provided context during inference.\
  \ The method computes the likelihood difference between LLM outputs with and without\
  \ context, and scales this difference by a parameter \u03BB to control the degree\
  \ of personalization."
---

# Context Steering: Controllable Personalization at Inference Time

## Quick Facts
- arXiv ID: 2405.01768
- Source URL: https://arxiv.org/abs/2405.01768
- Authors: Jerry Zhi-Yang He; Sashrika Pandey; Mariah L. Schrum; Anca Dragan
- Reference count: 40
- Primary result: Training-free decoding approach that enables controllable personalization of LLM outputs by amplifying or attenuating the influence of user-provided context during inference

## Executive Summary
This paper introduces Context Steering (CoS), a training-free decoding approach that enables controllable personalization of LLM outputs by amplifying or attenuating the influence of user-provided context during inference. The method computes the likelihood difference between LLM outputs with and without context, and scales this difference by a parameter λ to control the degree of personalization. Experiments show CoS successfully increases personalization in movie recommendations and outperforms baseline methods in implicit hate classification and quantification tasks, demonstrating its flexibility and effectiveness for inference-time personalization without requiring model retraining or additional data collection.

## Method Summary
Context Steering (CoS) is a training-free decoding approach that enables controllable personalization of LLM outputs by computing the likelihood difference between LLM outputs with and without context, then scaling this difference by a parameter λ. The method calculates contextual influence by comparing output probabilities from two LLM forward passes - one including the context and one without - and adjusts token probabilities accordingly. CoS can be inverted to function as a Bayesian generative model for inferring correlations between open-ended texts. The approach requires no model retraining or additional data collection, making it flexible for different use cases.

## Key Results
- Successfully increases personalization in movie recommendations with user study (average score 3.9/5) and GPT-4 pairwise comparisons showing 62% win rate for personalization
- Outperforms baseline methods in implicit hate classification (accuracy 0.81 vs 0.71) and quantification tasks (Spearman correlation 0.67 vs 0.61)
- Maintains coherence and diversity while providing flexible control over personalization levels through λ parameter

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoS captures the influence of context by computing the likelihood difference between LLM outputs with and without context
- Mechanism: CoS calculates the contextual influence function FC,P(xi) = LLM(xi|C, P) - LLM(xi|∅, P), which represents how much more likely a token xi is to be generated under context C compared to no context. This difference is then scaled by λ to control the degree of personalization
- Core assumption: The difference in token probabilities between context-present and context-absent scenarios directly measures the influence of that context on the model's output
- Evidence anchors:
  - [abstract] "CoS computes contextual influence by comparing the output probabilities from two LLM forward passes: one that includes the context and one that does not"
  - [section 2.2] "The contextual influence function captures how much more likely it is for some token xi to be generated under the context C compared to when no contextual information is provided"
  - [corpus] Weak - no direct evidence in corpus about this specific mechanism
- Break condition: If the LLM's attention mechanism doesn't meaningfully incorporate context, or if the context is too short/long to have measurable influence, the likelihood difference may not capture true contextual influence

### Mechanism 2
- Claim: CoS can be used as a Bayesian generative model to infer correlations between open-ended texts
- Mechanism: By inverting the forward probability model PCoS,λ(X|C, P), CoS computes the posterior distribution P(Λ=λ|X, C, P) to assess how strongly a statement X reflects the underlying tone C
- Core assumption: The likelihood of a statement being generated under different λ values follows a probabilistic structure that can be inverted to infer the underlying context strength
- Evidence anchors:
  - [abstract] "we show that CoS can function as a Bayesian Generative model to infer and quantify correlations between open-ended texts"
  - [section 2.3] "By leveraging Bayesian Inference, we can effectively 'invert' this forward probability model to compute the posterior distribution of λ"
  - [corpus] Weak - no direct evidence in corpus about this specific Bayesian inference mechanism
- Break condition: If the model's generation process doesn't follow the assumed probabilistic structure, or if the normalizing constant Z cannot be approximated adequately, the Bayesian inference will be unreliable

### Mechanism 3
- Claim: CoS enables controllable personalization without requiring model retraining or additional data collection
- Mechanism: By adjusting λ during inference time, practitioners can amplify or attenuate the influence of context, allowing flexible control over personalization levels without modifying the underlying model
- Core assumption: The LLM's internal representations capture meaningful relationships between context and appropriate responses that can be modulated through likelihood scaling
- Evidence anchors:
  - [abstract] "CoS allows practitioners to flexibly control the degree of personalization for different use cases" and "without requiring model retraining or additional data collection"
  - [section 1] "Current solutions, such as prompt-engineering and fine-tuning, require collection of contextually appropriate responses as examples, making them time-consuming and less flexible"
  - [corpus] Strong - multiple related papers discuss inference-time personalization techniques that avoid retraining
- Break condition: If the LLM's representations are too rigid or if context influence is non-linear in ways that simple scaling cannot capture, the control may be ineffective or produce incoherent outputs

## Foundational Learning

- Concept: Autoregressive language modeling
  - Why needed here: Understanding how LLMs generate text token-by-token is essential to grasp how CoS modifies token probabilities
  - Quick check question: How does an autoregressive LLM generate the next token given previous tokens and context?

- Concept: Likelihood difference as a measure of influence
  - Why needed here: CoS fundamentally relies on computing probability differences to quantify contextual influence
  - Quick check question: What does the difference LLM(xi|C, P) - LLM(xi|∅, P) represent in terms of context influence?

- Concept: Bayesian inference for model inversion
  - Why needed here: CoS uses Bayesian methods to infer the strength of underlying contexts from generated text
  - Quick check question: How does computing P(Λ=λ|X, C, P) help quantify the relationship between context and generated text?

## Architecture Onboarding

- Component map: Context input module -> Prompt input module -> LLM forward pass engine (with/without context) -> Contextual influence calculator -> Scaling module -> Output generation system
- Critical path: Context → Prompt → LLM forward passes → Likelihood difference computation → λ scaling → Token probability adjustment → Text generation
- Design tradeoffs:
  - Computational cost: CoS requires two LLM forward passes per generation step, roughly doubling computation
  - Control granularity: λ provides continuous control but extreme values may produce numerical instability
  - Context handling: Single context vs. multiple contexts affects implementation complexity and scalability
- Failure signatures:
  - Incoherent outputs at extreme λ values (numerical instability)
  - Minimal personalization changes when context is too generic or too specific
  - Computational bottlenecks during inference due to doubled forward passes
  - Incorrect Bayesian inference when normalizing constant cannot be properly estimated
- First 3 experiments:
  1. Verify basic functionality: Test CoS on a simple task like personalizing movie recommendations with different λ values and verify that higher λ produces more context-aware responses
  2. Test numerical stability: Generate outputs across a range of λ values (including negative and large positive values) to identify the stable operating range
  3. Evaluate Bayesian inference: Use CoS to classify implicit hate speech and measure classification accuracy compared to baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the context's position in the prompt affect the quality of generated text?
- Basis in paper: [explicit] The paper investigates the effect of context position on generation quality in Appendix C.1, finding that the context's position has small effects on the generation.
- Why unresolved: While the paper finds small effects, it does not provide a detailed analysis of why the position has minimal impact or explore scenarios where position might matter more.
- What evidence would resolve it: A more extensive study varying context positions across different prompt lengths and model architectures, along with an analysis of attention mechanisms, would clarify the impact of context position.

### Open Question 2
- Question: How does Context Steering (CoS) perform with long input sequences?
- Basis in paper: [inferred] The paper mentions in the discussion section that it is unclear how well CoS can handle long input sequences, as the effect of the context may diminish on long input sequences.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on the scalability of CoS with long input sequences.
- What evidence would resolve it: Empirical studies comparing CoS performance on long vs. short input sequences, along with analysis of context retention in attention mechanisms, would provide insights into scalability.

### Open Question 3
- Question: Can CoS be effectively composed with multiple regions of contextual input to guide different parts of language generation?
- Basis in paper: [explicit] The paper states in the discussion section that the main limitation of CoS lies in its composability, and it is unclear how to modulate the influence of multiple regions of contextual input.
- Why unresolved: The paper does not explore methods for composing multiple contexts or provide experimental results on such compositions.
- What evidence would resolve it: Experiments demonstrating the effectiveness of CoS with multiple, distinct contexts, along with analysis of how different contexts interact, would address composability challenges.

### Open Question 4
- Question: How does Context Steering affect the factuality and creativity of LLM outputs?
- Basis in paper: [explicit] The paper investigates the impact of CoS on factuality in Appendix C.2, finding a small decrease in accuracy with higher λ values. It also explores creativity in Appendix C.4, finding that λ has little influence on coherence and diversity.
- Why unresolved: The paper provides limited quantitative analysis and does not explore the underlying mechanisms of how CoS affects factuality and creativity.
- What evidence would resolve it: A comprehensive study measuring factuality and creativity across various tasks and contexts, along with analysis of token-level changes, would clarify the impact of CoS on these aspects.

## Limitations

- The method requires two LLM forward passes per generation step, doubling computational costs compared to standard inference
- Extreme λ values can lead to numerical instability and degenerate text generations, creating a narrow effective operating range
- The effectiveness of CoS depends on the underlying LLM's ability to meaningfully incorporate context, which may vary significantly across different model architectures and sizes

## Confidence

- **High confidence** in the core mechanism: Computing likelihood differences to measure contextual influence is well-grounded in probabilistic modeling and has been validated across multiple experiments with consistent results
- **Medium confidence** in the personalization effectiveness: While the method shows clear improvements in personalization tasks, the improvements are task-dependent and may not generalize to all personalization scenarios equally
- **Medium confidence** in the Bayesian inference claims: The framework for using CoS as a generative model is theoretically sound, but the approximation of the normalizing constant Z and the reliability of posterior inference in practice remain partially validated

## Next Checks

1. **Compute cost validation**: Measure and report the exact wall-clock time difference between CoS and standard inference across different model sizes (7B, 13B, 70B parameters) to quantify the practical computational overhead
2. **Numerical stability boundary testing**: Systematically test CoS with λ values in [-4, 4] increments on a representative task to identify the precise boundaries where numerical instability begins and document the quality degradation patterns
3. **Cross-architecture generalization**: Implement CoS on at least two different LLM architectures (e.g., transformer-based and state-space model-based) to verify that the contextual influence computation works consistently across architectural variations