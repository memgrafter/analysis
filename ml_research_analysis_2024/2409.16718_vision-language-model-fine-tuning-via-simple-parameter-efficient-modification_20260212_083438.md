---
ver: rpa2
title: Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification
arxiv_id: '2409.16718'
source_url: https://arxiv.org/abs/2409.16718
tags:
- clipfit
- fine-tuning
- bias
- tuning
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLIPFit, a simple parameter-efficient method
  for fine-tuning Vision-Language Models (VLMs) like CLIP without introducing external
  parameters. CLIPFit fine-tunes only specific bias terms in the text encoder and
  layer normalization in the image encoder, achieving a 7.33% improvement in harmonic
  mean accuracy compared to zero-shot CLIP on the 16-shot base-to-new setting.
---

# Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification

## Quick Facts
- arXiv ID: 2409.16718
- Source URL: https://arxiv.org/abs/2409.16718
- Authors: Ming Li; Jike Zhong; Chenxin Li; Liuzhuozheng Li; Nie Lin; Masashi Sugiyama
- Reference count: 33
- Primary result: CLIPFit improves zero-shot CLIP by 7.33% harmonic mean accuracy using only bias and LayerNorm updates

## Executive Summary
This paper introduces CLIPFit, a parameter-efficient fine-tuning method for Vision-Language Models that modifies only specific bias terms in the text encoder and layer normalization parameters in the image encoder. The method achieves significant performance improvements (7.33% harmonic mean accuracy gain) while avoiding catastrophic forgetting through knowledge distillation. CLIPFit demonstrates strong generalization across 11 datasets and outperforms existing parameter-efficient fine-tuning methods.

## Method Summary
CLIPFit fine-tunes Vision-Language Models by selectively updating only projection layer biases in the text encoder's feed-forward networks and all layer normalization parameters in the image encoder. The method employs knowledge distillation loss to prevent catastrophic forgetting of pre-trained knowledge during adaptation. Training uses standard cross-entropy loss combined with KD loss, with selective parameter updates designed to maintain efficiency while achieving strong performance on downstream image classification tasks.

## Key Results
- Achieves 7.33% improvement in harmonic mean accuracy compared to zero-shot CLIP on 16-shot base-to-new setting
- Demonstrates strong generalization across 11 diverse image classification datasets
- Outperforms traditional parameter-efficient methods while using fewer trainable parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning only bias terms of projection layers in FFNs preserves pre-trained knowledge while adapting to downstream tasks.
- Mechanism: Bias terms in projection layers control the final output transformation of each FFN block. By only modifying these, CLIPFit can adjust task-specific feature mappings without disrupting the learned internal representations in earlier layers.
- Core assumption: The projection layer bias is the most critical factor in adapting high-level semantic features for downstream classification.
- Evidence anchors:
  - [abstract] "We demonstrate that by only fine-tuning the specific bias terms and normalization layers, ClipFit can improve the performance of zero-shot CLIP by 7.27% average harmonic mean accuracy."
  - [section] "For the text encoder, instead of fine-tuning all the bias terms, CLIPFit fine-tunes only the bias terms of projection linear layers in feed-forward networks (FFNs)."
- Break condition: If projection layer biases are not the primary locus of task-specific adaptation, this selective tuning would fail to improve performance.

### Mechanism 2
- Claim: Updating LayerNorm in the image encoder corrects distribution shifts between pre-training and downstream data.
- Mechanism: LayerNorm parameters (gain and bias) re-center and re-scale normalized activations. Downstream data often has different feature statistics than pre-training data, so updating LayerNorm adapts the model to these shifts without modifying core learned weights.
- Core assumption: The mismatch between pre-training and downstream data distributions is primarily corrected through normalization layer adaptation.
- Evidence anchors:
  - [abstract] "For the image encoder, layer normalization (LayerNorm) (Ba et al., 2016) aims to normalize the distributions of intermediate layers. Since the distributions of pre-training and downstream data might be divergent, pre-trained LayerNorm might lead to sub-optimal performance for downstream data inference."
  - [section] "Therefore, CLIPFit proposes to further update only the parameters of the image encoder's LayerNorm."
- Break condition: If the distribution shift is not the primary source of performance degradation, updating LayerNorm alone would be insufficient.

### Mechanism 3
- Claim: Knowledge distillation loss prevents catastrophic forgetting of pre-trained knowledge during fine-tuning.
- Mechanism: The KD loss penalizes divergence between the fine-tuned model's predictions and the original zero-shot CLIP's predictions, encouraging the model to retain useful pre-trained knowledge while learning new task-specific information.
- Core assumption: The original zero-shot CLIP contains valuable generic knowledge that should be preserved during fine-tuning.
- Evidence anchors:
  - [abstract] "Previous studies (Yao et al., 2023) have shown that generic pre-trained knowledge is easily forgotten in the fine-tuning stage. Therefore, we explored two different regularization strategies for alleviating forgetting: (i) using the knowledge distillation (KD) loss (Hinton et al., 2015) to guide CLIPFit to learn from the zero-shot CLIP."
  - [section] "The training loss and KD loss of CLIPFit are defined by L = Lce + βLkg, where Lkg = 1/K Σ cos(wclip_i, w_i)"
- Break condition: If the pre-trained knowledge is not relevant to the downstream task, KD loss would hinder rather than help performance.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning
  - Why needed here: Traditional fine-tuning of large VLMs is computationally expensive and risks catastrophic forgetting. CLIPFit needs to achieve good performance with minimal parameter changes.
  - Quick check question: What is the key difference between CLIPFit and full fine-tuning in terms of parameter modification?

- Concept: Layer normalization and its role in domain adaptation
  - Why needed here: CLIPFit updates LayerNorm parameters to handle distribution shifts between pre-training and downstream data, which is crucial for maintaining performance.
  - Quick check question: How does updating LayerNorm parameters help when the input data distribution changes?

- Concept: Knowledge distillation and preventing catastrophic forgetting
  - Why needed here: CLIPFit uses KD loss to prevent the model from losing useful pre-trained knowledge while adapting to new tasks, which is essential for maintaining generalization.
  - Quick check question: What role does the KD loss play in CLIPFit's training objective?

## Architecture Onboarding

- Component map: Input image -> Image encoder (LayerNorm updated) -> Text encoder (projection biases updated) -> Class weights -> Cosine similarity -> Predictions
- Critical path:
  1. Preprocess input images (resize, augment)
  2. Encode images with image encoder (LayerNorm updated)
  3. Generate class weights from text encoder (projection biases updated)
  4. Compute cosine similarity between image features and class weights
  5. Apply cross-entropy loss with KD regularization
- Design tradeoffs:
  - Selective parameter tuning vs. full fine-tuning: CLIPFit trades potential maximum performance for efficiency and reduced overfitting risk
  - KD loss weighting: Higher β values preserve more pre-trained knowledge but may slow task adaptation
  - LayerNorm vs. other image encoder modifications: LayerNorm is computationally efficient but may not capture all needed adaptations
- Failure signatures:
  - Poor performance on new classes: May indicate insufficient adaptation or too strong KD regularization
  - Catastrophic forgetting on base classes: May indicate too weak KD regularization or excessive bias tuning
  - No improvement over zero-shot: May indicate incorrect parameter selection or insufficient training
- First 3 experiments:
  1. Compare CLIPFit with BitFit (fine-tuning all biases) on base-to-new setting to verify selective tuning advantage
  2. Test CLIPFit with and without KD loss on few-shot learning to measure forgetting prevention
  3. Evaluate CLIPFit on distribution shift datasets (ImageNet-V2, ImageNet-Sketch) to assess robustness

## Open Questions the Paper Calls Out
None

## Limitations
- The claim of "only fine-tuning specific bias terms" requires verification that these parameters alone capture sufficient task adaptation capacity without additional modifications
- The distribution shift assumption underlying LayerNorm updates may not generalize across all dataset domains
- The optimal KD loss weighting (β=8) appears dataset-specific and may not transfer well to other VLMs or tasks

## Confidence
- High: CLIPFit successfully achieves parameter-efficient fine-tuning with minimal modifications (7.33% harmonic mean improvement verified through multiple datasets)
- Medium: The selective bias tuning mechanism is effective for text encoder adaptation (supported by ablation studies but requires further exploration of alternative parameter selections)
- Low: LayerNorm updates alone sufficiently handle distribution shifts for all image domains (assumption-driven, limited empirical validation across diverse data distributions)

## Next Checks
1. **Cross-Domain Robustness Test:** Evaluate CLIPFit on systematically shifted datasets (different lighting, angles, or domains) to verify LayerNorm adaptation effectiveness beyond standard benchmarks
2. **Parameter Sensitivity Analysis:** Systematically vary the number of fine-tuned bias terms and LayerNorm layers to determine optimal parameter selection trade-offs
3. **Alternative VLM Generalization:** Apply CLIPFit to other VLMs (ALFWorld, BLIP) to validate whether the same parameter-efficient approach generalizes beyond CLIP architecture