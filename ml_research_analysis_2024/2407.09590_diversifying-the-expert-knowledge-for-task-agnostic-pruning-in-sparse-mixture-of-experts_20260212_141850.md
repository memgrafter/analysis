---
ver: rpa2
title: Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse Mixture-of-Experts
arxiv_id: '2407.09590'
source_url: https://arxiv.org/abs/2407.09590
tags:
- experts
- layer
- pruning
- expert
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of memory efficiency in deploying
  large-scale Mixture-of-Experts (MoE) architectures for large language models (LLMs).
  The authors propose a task-agnostic method to prune redundant experts within MoE
  layers to improve parameter efficiency without significant performance loss.
---

# Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse Mixture-of-Experts

## Quick Facts
- arXiv ID: 2407.09590
- Source URL: https://arxiv.org/abs/2407.09590
- Authors: Zeliang Zhang; Xiaodong Liu; Hao Cheng; Chenliang Xu; Jianfeng Gao
- Reference count: 40
- One-line primary result: Task-agnostic pruning method for MoE layers achieves 40% memory reduction with <2% performance degradation

## Executive Summary
This paper addresses the challenge of memory efficiency in deploying large-scale Mixture-of-Experts (MoE) architectures for large language models. The authors propose a novel task-agnostic method to prune redundant experts within MoE layers, significantly reducing parameter count without substantial performance loss. By identifying similar experts using Centered Kernel Alignment (CKA) and merging them in weight space, the approach effectively diversifies knowledge across layers while preserving model capabilities.

## Method Summary
The approach works in two stages: first discovering similar experts through data-centric (using a small calibration dataset) or model-centric (using expert weights) strategies, then merging these experts while preserving router functions. The method uses CKA to measure similarity between expert outputs, groups similar experts, and merges them using uniform coefficients. This task-agnostic pruning technique is evaluated on state-of-the-art MoE architectures including Mixtral, DeepSeek-MoE, and Qwen, demonstrating significant memory savings with minimal performance degradation.

## Key Results
- Outperforms baseline pruning methods on MMLU, BoolQ, OpenBookQA, and RTE tasks
- Achieves 40% reduction in experts per MoE layer with less than 2% performance degradation
- Model-centric approach shows robustness across different model scales without requiring access to training data
- Demonstrates consistent effectiveness across Mixtral-8x7B, DeepSeek-MoE-16B, and Qwen2-57B-14A architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Experts with similar behavior on shared inputs encode redundant knowledge
- Mechanism: Centered Kernel Alignment (CKA) measures similarity between expert outputs on the same input batch, revealing redundant experts
- Core assumption: Experts producing similar outputs on shared inputs contain overlapping knowledge
- Evidence anchors:
  - [abstract] "Our empirical study reveals that some experts encode redundant knowledge during pre-training"
  - [section 3.3] "experts exhibiting similar behaviors likely contain redundant knowledge"
  - [corpus] Weak - neighboring papers focus on pruning but don't explicitly validate CKA as similarity measure
- Break condition: If expert outputs diverge significantly across different input distributions

### Mechanism 2
- Claim: Merging similar experts while preserving their routing weights maintains model performance
- Mechanism: Group similar experts and merge them in weight space while preserving router functions to diversify knowledge
- Core assumption: The combined knowledge of merged experts can be represented by a single expert with weighted parameters
- Evidence anchors:
  - [abstract] "merges these experts in the weight space to diversify knowledge across layers"
  - [section 3.4] "merge experts in the weight space to diversify the knowledge in different MoE layers"
  - [corpus] Weak - neighboring papers mention expert merging but don't detail weight-space merging approach
- Break condition: If merged expert cannot effectively represent combined knowledge across all inputs

### Mechanism 3
- Claim: Model weights encode sufficient information about expert similarity without requiring access to training data
- Mechanism: Vectorized or surrogate weights can serve as proxy for expert representations when computing similarity
- Core assumption: Weight distributions reflect the knowledge learned from training data
- Evidence anchors:
  - [section 3.5] "weights already encode valuable data information, which can be utilized to group experts for pruning"
  - [corpus] Weak - neighboring papers don't discuss weight-based similarity computation
- Break condition: If weight similarity doesn't correlate with functional similarity

## Foundational Learning

- Concept: Centered Kernel Alignment (CKA)
  - Why needed here: Provides quantitative measure of similarity between expert outputs
  - Quick check question: How does CKA differ from simple correlation when measuring neural network representation similarity?

- Concept: Sparse Mixture-of-Experts architecture
  - Why needed here: Understanding MoE layer structure is essential for grasping pruning approach
  - Quick check question: In MoE layers, how does the router function determine which experts are activated?

- Concept: Neural network weight analysis
  - Why needed here: Model-centric approach relies on analyzing expert weights directly
  - Quick check question: What information about learned features can be extracted from examining neural network weights?

## Architecture Onboarding

- Component map:
  - Input layer → Router function → Expert selection → Expert computation → Weighted output combination
  - Each MoE layer contains N experts, each being a feed-forward network
  - Router matrix determines expert selection per token

- Critical path:
  - Compute expert similarity → Group similar experts → Merge grouped experts in weight space → Update router weights

- Design tradeoffs:
  - Data-centric vs model-centric approaches (data access requirements vs robustness)
  - Number of experts to prune vs performance retention
  - Merging strategy (uniform vs learned coefficients) vs computational efficiency

- Failure signatures:
  - Performance degradation beyond acceptable threshold (typically >2-3% on benchmarks)
  - Model instability or crashes when specific expert combinations are removed
  - Inconsistent performance across different tasks

- First 3 experiments:
  1. Implement CKA-based similarity computation on a small MoE model and visualize expert similarity matrix
  2. Test merging of two highly similar experts and evaluate performance impact
  3. Compare data-centric and model-centric pruning approaches on Mixtral-8x7B with 2 experts removed per layer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we push MoE compression to its limits while maintaining acceptable performance?
- Basis in paper: [explicit] "Third, in our work, we prune the same experts across different MoE layers, despite each layer having varying levels of redundancy. A key question remains: how can we push MoE compression to its limits while maintaining acceptable performance?"
- Why unresolved: The paper acknowledges this as a limitation but doesn't explore layer-specific pruning strategies or provide a framework for determining optimal compression ratios for different layers.
- What evidence would resolve it: Experimental results comparing layer-specific vs. uniform pruning across different MoE architectures, with analysis of trade-offs between compression ratio and performance degradation.

### Open Question 2
- Question: What causes performance differences across pruning strategies for different models?
- Basis in paper: [explicit] "First, we designed various strategies to prune the MoE, and we observed that different models require different strategies to achieve optimal post-pruning performance. It remains unclear what causes these performance differences across strategies."
- Why unresolved: The paper notes that DeepSeek-MoE and Qwen architectures show different responses to pruning strategies, but doesn't investigate the architectural or training factors that contribute to these differences.
- What evidence would resolve it: Detailed analysis of architectural differences (e.g., shared experts in DeepSeek vs. dense initialization in Qwen) and their impact on expert similarity, along with systematic testing of pruning strategies across architectures.

### Open Question 3
- Question: How can we efficiently find optimal merging coefficients for expert merging?
- Basis in paper: [explicit] "Second, while the learning strategy at the merging step can bring slightly performance improvement, the cost is also large. The question of how to efficiently find the optimal merging coefficients remains."
- Why unresolved: The paper compares three merging strategies but doesn't explore more sophisticated optimization approaches or investigate whether the learned coefficients transfer across different layers or models.
- What evidence would resolve it: Comparison of various optimization approaches for finding merging coefficients (e.g., gradient-based, meta-learning, or transfer learning approaches) with analysis of computational efficiency and generalization across different pruning scenarios.

## Limitations

- The method requires either access to a small calibration dataset (data-centric approach) or relies on the assumption that weight distributions sufficiently capture expert similarity (model-centric approach)
- The approach focuses on reducing experts per layer rather than exploring alternative pruning strategies like expert skipping or dynamic routing
- The effectiveness of the method may vary across different MoE architectures, with some models showing better performance with specific pruning strategies

## Confidence

- **High confidence**: The core mechanism of using CKA to identify similar experts is well-established in the literature and the empirical results show consistent performance improvements over baseline pruning methods
- **Medium confidence**: The merging strategy using uniform coefficients is effective but may not be optimal; learned merging coefficients could potentially yield better results
- **Medium confidence**: The claim that model weights encode sufficient information for similarity measurement is supported by results but requires more extensive ablation studies

## Next Checks

1. **Ablation study on CKA variants**: Compare the performance of different similarity metrics (linear CKA vs RBF CKA vs cosine similarity) to quantify the contribution of the specific CKA choice to overall performance

2. **Cross-dataset robustness test**: Evaluate the data-centric approach on calibration datasets from different domains to assess whether the method generalizes beyond the C4 dataset used in the main experiments

3. **Transferability analysis**: Test whether experts pruned using the data-centric approach from one model can be successfully transferred and merged in a differently initialized model of the same architecture, examining the universality of the similarity measure