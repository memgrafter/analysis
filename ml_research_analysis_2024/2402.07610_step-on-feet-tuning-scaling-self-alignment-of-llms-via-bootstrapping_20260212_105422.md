---
ver: rpa2
title: 'Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping'
arxiv_id: '2402.07610'
source_url: https://arxiv.org/abs/2402.07610
tags:
- assistant
- self-alignment
- user
- bootstrapping
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether bootstrapping self-alignment can
  improve large language models (LLMs) by iteratively fine-tuning on self-generated
  data. It finds that naive bootstrapping causes overfitting, but introducing a diverse
  in-context learning (ICL) example pool and training in an easy-to-hard order mitigates
  this issue and enhances performance.
---

# Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping

## Quick Facts
- arXiv ID: 2402.07610
- Source URL: https://arxiv.org/abs/2402.07610
- Reference count: 40
- Primary result: Bootstrapping self-alignment with diverse ICL examples and easy-to-hard training order outperforms single-round alignment and approaches state-of-the-art models

## Executive Summary
This paper investigates whether bootstrapping self-alignment can improve large language models (LLMs) by iteratively fine-tuning on self-generated data. The authors find that naive bootstrapping causes overfitting to simple responses, but introducing a diverse in-context learning (ICL) example pool and training in an easy-to-hard order mitigates this issue and enhances performance. Their method, Step-On-Feet Tuning (SOFT), leverages the model's improved few-shot ability to boost zero/one-shot performance and includes a validation set to prevent collapse. Experiments on classification and generation tasks show SOFT outperforms single-round alignment and approaches state-of-the-art models.

## Method Summary
The Step-On-Feet Tuning (SOFT) method involves iterative self-alignment where the model is fine-tuned on its own generated responses. Key components include a diverse 48-example ICL pool to prevent overfitting, easy-to-hard training order based on response perplexity, and a validation set for early stopping. The process involves generating responses using ICL examples, fine-tuning on these responses, and evaluating performance while monitoring for collapse. The method aims to leverage the model's continuously enhanced few-shot ability to improve overall performance across both classification and generation tasks.

## Key Results
- Bootstrapping self-alignment with diverse ICL examples significantly outperforms single-round alignment in early stages
- Easy-to-hard training order improves performance by reducing error accumulation
- SOFT prevents performance collapse through validation set monitoring and achieves results approaching state-of-the-art models
- Later-stage performance drops are attributed to Data Processing Inequality and sharper output distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse ICL examples prevent overfitting to simple responses in bootstrapping self-alignment.
- Mechanism: The ICL example pool introduces varied formats and complexity in few-shot demonstrations, breaking the model's tendency to memorize and overfit to a narrow set of response patterns.
- Core assumption: Model overfitting occurs when few-shot examples are too homogeneous, leading to repetitive and uninformative outputs.
- Evidence anchors:
  - [abstract] "Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning."
  - [section 4.1] "We enhance the complexity and diversity of ICL examples, conduct experiments based on these modifications, and demonstrate the effectiveness of bootstrapping self-alignment in early stages."
  - [corpus] Weak evidence - no direct citations found, but related works mention importance of data diversity.

### Mechanism 2
- Claim: Easy-to-hard training order reduces error accumulation and improves model performance.
- Mechanism: Sorting training data by response perplexity ensures the model learns simpler tasks first, building a strong foundation before tackling complex prompts, thereby reducing error propagation.
- Core assumption: Early-stage errors compound and degrade later performance; learning easier tasks first improves label quality.
- Evidence anchors:
  - [section 4.3] "We sorted the dataset P with its perplexity from small to large, and mark it as P′. Afterward, we segment P′ into ordered subsets P′t to do bootstrapping self-alignment again..."
  - [section 4.3] "We observe improved performance against simple bootstrapping self-alignment on these benchmarks."
  - [corpus] No direct citations, but related works on curriculum learning support this approach.

### Mechanism 3
- Claim: Bootstrapping self-alignment leverages the model's continuously enhanced few-shot ability to improve zero/one-shot performance.
- Mechanism: Each iteration of self-alignment generates higher-quality labels using the improved few-shot capability of the previous model, creating a positive feedback loop that enhances overall performance.
- Core assumption: Few-shot ability and zero-shot ability are correlated; improving one enhances the other.
- Evidence anchors:
  - [abstract] "SOFT leverages model's continuously enhanced few-shot ability to boost zero or one-shot performance."
  - [section 4.2] "On the Truthful QA benchmark, the model has demonstrated continuous improvement across all two iteration settings..."
  - [corpus] No direct citations, but this aligns with findings in related works on few-shot learning.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: ICL is central to the self-alignment process, where the model generates responses based on few-shot examples.
  - Quick check question: What is the primary difference between few-shot and zero-shot learning in the context of ICL?

- Concept: Data Processing Inequality
  - Why needed here: Explains why model performance may degrade in later stages due to information loss during self-training.
  - Quick check question: How does Data Processing Inequality relate to the concept of information loss in self-training loops?

- Concept: Perplexity as a measure of difficulty
  - Why needed here: Used to sort tasks from easy to hard, ensuring the model learns simpler tasks first.
  - Quick check question: How is perplexity calculated, and why is it a suitable metric for task difficulty?

## Architecture Onboarding

- Component map: Pretrained LLM -> ICL Example Pool -> Training Data -> Validation Set -> Fine-tuning Pipeline
- Critical path: 1. Generate responses using ICL examples. 2. Fine-tune model on generated responses. 3. Evaluate performance on benchmarks. 4. Check validation set for early stopping.
- Design tradeoffs:
  - Larger ICL pool increases diversity but requires more curation effort.
  - Easy-to-hard ordering improves performance but may not capture all task complexities.
  - Validation set prevents collapse but adds computational overhead.
- Failure signatures:
  - Overfitting to simple responses (e.g., repetitive refusal messages).
  - Performance degradation in later stages due to information loss.
  - Inconsistent results between classification and generation tasks.
- First 3 experiments:
  1. Test overfitting with fixed vs. diverse ICL examples.
  2. Compare easy-to-hard vs. random training order.
  3. Evaluate early stopping using the validation set.

## Open Questions the Paper Calls Out

Open Question 1
- Question: How does the effectiveness of bootstrapping self-alignment vary with model size and capacity?
- Basis in paper: Inferred from discussion of multi-round self-alignment on different models (LLaMA-2-7b, LLaMA-2-13b) and mention of potential for larger models
- Why unresolved: The paper only experiments with 7B and 13B parameter models, but doesn't systematically explore the relationship between model capacity and bootstrapping effectiveness
- What evidence would resolve it: Experiments comparing bootstrapping effectiveness across a wider range of model sizes (e.g., 7B, 13B, 33B, 70B) while controlling for other variables

Open Question 2
- Question: What is the optimal frequency and timing for introducing new ICL examples during multi-round self-alignment?
- Basis in paper: Explicit mention of ICL example pool and its importance, but no systematic study of when/how often to refresh examples
- Why unresolved: The paper uses a fixed pool of 48 examples throughout all rounds without exploring the impact of varying this strategy
- What evidence would resolve it: Experiments comparing different schedules for introducing new ICL examples (e.g., every round, every 2 rounds, only when performance plateaus)

Open Question 3
- Question: How does the order of training examples within each easy/hard subset affect final performance?
- Basis in paper: The paper discusses easy-to-hard ordering of subsets but doesn't explore ordering within each subset
- Why unresolved: While the paper investigates easy-to-hard ordering between subsets, it doesn't examine whether the sequence of examples within each difficulty level matters
- What evidence would resolve it: Experiments comparing different ordering strategies within each difficulty level (e.g., random, by topic, by length, by complexity)

Open Question 4
- Question: What is the relationship between output distribution sharpness and model collapse across different task types?
- Basis in paper: Explicit discussion of sharper output distribution and its different effects on classification vs generation tasks
- Why unresolved: The paper observes this phenomenon but doesn't systematically measure or quantify the relationship between distribution sharpness and task-specific collapse
- What evidence would resolve it: Quantitative analysis correlating output distribution metrics (e.g., entropy, effective vocabulary size) with performance degradation across different task types

Open Question 5
- Question: How does the diversity of the initial ICL example pool affect the maximum achievable performance through bootstrapping?
- Basis in paper: Explicit emphasis on ICL example diversity as crucial for preventing overfitting
- Why unresolved: The paper uses a specific pool size (48 examples) but doesn't explore how different levels of diversity affect the upper bound of bootstrapping effectiveness
- What evidence would resolve it: Experiments comparing bootstrapping performance with ICL pools of varying diversity (measured by example variety, format diversity, topic coverage)

## Limitations
- Data diversity dependency: Effectiveness heavily relies on quality and diversity of ICL example pool
- Model-specific behavior: Results may not directly translate to other model architectures or sizes
- Validation set scope: Specific tasks and metrics used for validation are not detailed

## Confidence

**High Confidence**:
- Bootstrapping with diverse ICL examples improves early-stage performance
- Easy-to-hard training order reduces error accumulation

**Medium Confidence**:
- SOFT leverages enhanced few-shot ability to improve zero/one-shot performance
- Validation set prevents performance collapse

**Low Confidence**:
- Later-stage performance drops due to Data Processing Inequality

## Next Checks
1. Test SOFT with ICL pools of varying sizes and diversity levels to determine minimum requirements for effective bootstrapping
2. Apply SOFT to different base models (e.g., GPT-3.5, BLOOM) to evaluate cross-model applicability
3. Conduct ablation studies on validation set configurations to assess robustness of early stopping mechanism