---
ver: rpa2
title: 'ShortGPT: Layers in Large Language Models are More Redundant Than You Expect'
arxiv_id: '2403.03853'
source_url: https://arxiv.org/abs/2403.03853
tags:
- layer
- pruning
- layers
- language
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors analyze layer redundancy in large language models and
  propose a simple pruning method that removes entire layers based on a metric called
  Block Influence (BI). They show that this method outperforms previous pruning approaches
  while maintaining about 90% of model performance with a 25% reduction in parameters.
---

# ShortGPT: Layers in Large Language Models are More Redundant Than You Expect

## Quick Facts
- arXiv ID: 2403.03853
- Source URL: https://arxiv.org/abs/2403.03853
- Authors: Xin Men; Mingyu Xu; Qingyu Zhang; Bingning Wang; Hongyu Lin; Yaojie Lu; Xianpei Han; Weipeng Chen
- Reference count: 40
- Primary result: Simple layer removal method maintains ~90% performance with 25% parameter reduction

## Executive Summary
This paper investigates layer redundancy in large language models (LLMs) and proposes a simple pruning method that removes entire layers based on a metric called Block Influence (BI). The authors observe that transformer layers in pre-norm architectures exhibit high input-output similarity, making them redundant. Their BI metric effectively identifies less important layers by measuring cosine similarity between consecutive hidden states. The approach outperforms existing pruning methods while maintaining model performance and is compatible with quantization techniques.

## Method Summary
The authors propose a layer pruning method based on Block Influence (BI) metric, which measures the cosine similarity between input and output hidden states of each transformer layer. They compute BI scores across all layers using a calibration dataset, then remove layers with the lowest BI scores. The method exploits the observation that pre-norm architectures create high layer redundancy due to bounded transformation components. The pruned models are evaluated across multiple benchmarks including MMLU, CMMLU, and generative tasks, demonstrating that coarse-grained layer removal often outperforms fine-grained parameter pruning methods.

## Key Results
- BI metric effectively identifies redundant layers, achieving 90% performance retention with 25% parameter reduction
- Layer removal approach outperforms fine-grained pruning methods like LLMPruner and SliceGPT
- The method is orthogonal to quantization, maintaining performance when combined with quantization techniques
- Generative tasks (XSum, C3) show more performance degradation than classification tasks after pruning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-norm architecture creates high cosine similarity between layer inputs and outputs, leading to layer redundancy
- Mechanism: When layer normalization is applied before self-attention and feed-forward layers, the hidden states accumulate over layers while the transformation components remain bounded, resulting in high input-output similarity for deeper layers
- Core assumption: The layer normalization in pre-norm architecture causes hidden states to grow while transformations remain bounded, making deeper layers less transformative
- Evidence anchors:
  - [abstract] "Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs"
  - [section 2.1] "However, we observe that when pre-norm is adopted, the similarity between the input and output of transformer layers tends to be higher, as illustrated in Figure 2"
  - [corpus] Weak evidence - corpus papers discuss layer pruning but don't directly address pre-norm architecture effects
- Break condition: If the model uses post-norm architecture where normalization occurs after transformations, the similarity between inputs and outputs would decrease with depth

### Mechanism 2
- Claim: Block Influence (BI) metric effectively identifies redundant layers by measuring hidden state transformation
- Mechanism: BI scores layers by computing 1 - cosine similarity between input and output hidden states, where lower scores indicate less transformation and thus lower importance
- Core assumption: Layers that produce minimal changes to hidden states contribute less to model functionality and can be removed without significant performance loss
- Evidence anchors:
  - [section 3.1] "BIi = 1 − EX,t XT i,tXi+1,t ||Xi,t||2||Xi+1,t||2, (1) where Xi,t means the tth row of hidden states of ith layer"
  - [section 3.1] "Lower BI score imply that Xi and Xi+1 exhibit high cosine similarity, suggesting that the layer makes minimal transformations to the hidden states and is therefore less important"
  - [corpus] Moderate evidence - corpus papers propose various pruning metrics but don't specifically use cosine similarity-based block influence
- Break condition: If a layer has high BI score but is still functionally redundant due to other architectural properties not captured by the metric

### Mechanism 3
- Claim: Layer removal is more effective than fine-grained pruning because LLM outputs are robust to individual layer removal
- Mechanism: Removing entire layers exploits the model's inherent robustness, whereas fine-grained pruning requires identifying specific redundant parameters within layers
- Core assumption: The model architecture is robust enough that removing any single layer has minimal impact, making coarse-grained pruning simpler and more effective
- Evidence anchors:
  - [section 4.2] "The results show that coarse-grained pruning methods, such as removing entire layers, often outperform fine-grained approaches like Slice GPT or LLM Pruner"
  - [section 4.2] "We speculate that the reason is that the large language model is actually very robust, as shown in Figure 1, removing any deep layer individually actually has very little impact on the final output"
  - [corpus] Moderate evidence - corpus papers discuss various pruning approaches but don't explicitly compare coarse vs fine-grained effectiveness
- Break condition: If fine-grained pruning methods are improved to better identify redundant parameters within layers, they could outperform layer removal

## Foundational Learning

- Concept: Transformer architecture and pre-norm vs post-norm configurations
  - Why needed here: Understanding the architectural difference is crucial because the redundancy phenomenon is specifically tied to pre-norm configurations
  - Quick check question: In pre-norm transformers, where is layer normalization applied relative to the self-attention and feed-forward layers?

- Concept: Cosine similarity and its use as a similarity metric
  - Why needed here: BI metric relies on cosine similarity between hidden states to measure layer importance
  - Quick check question: What does a high cosine similarity between two vectors indicate about their directional relationship?

- Concept: Structured vs unstructured pruning in neural networks
  - Why needed here: The paper contrasts its structured layer removal approach with unstructured parameter pruning methods
  - Quick check question: What is the key difference between structured and unstructured pruning in terms of what components are removed?

## Architecture Onboarding

- Component map: Input sequence → Token embeddings → Stacked transformer layers (LayerNorm → Self-Attention → Feed-Forward → LayerNorm) → Final feed-forward layer → Language model head → Output probabilities
- Critical path: Input sequence → Token embeddings → Stacked transformer layers → Final feed-forward layer → Language model head → Output probabilities
- Design tradeoffs: Layer removal offers simplicity and hardware compatibility but may affect generative tasks more than classification tasks. The tradeoff is between model compression and task-specific performance.
- Failure signatures: Performance degradation in generative tasks (XSum, C3) while maintaining multiple-choice task performance, sudden drops in MMLU scores at specific pruning ratios indicating critical layers
- First 3 experiments:
  1. Compute BI scores across all layers using a calibration dataset and visualize the distribution to identify potential redundant layers
  2. Remove the lowest-scoring layer(s) and evaluate performance degradation on a held-out validation set to verify the pruning effectiveness
  3. Test the pruned model with different quantization schemes to verify the orthogonality claim with quantization methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the fundamental reason for layer redundancy in large language models, and can this be addressed during training rather than through post-hoc pruning?
- Basis in paper: [inferred] The paper observes significant layer redundancy and suggests this "warrants further investigation," noting that "our work suggests potential avenues for improving the efficiency of model training by reducing inherent redundancy in the future."
- Why unresolved: The authors identify the phenomenon but don't explore the underlying causes of why certain layers become redundant during training, or whether architectural modifications could prevent this redundancy from emerging.
- What evidence would resolve it: Experiments comparing different training methodologies (curriculum learning, layer-wise training schedules, architectural modifications) that reduce layer redundancy, or theoretical analysis of why pre-norm transformers in particular develop this redundancy pattern.

### Open Question 2
- Question: How does layer redundancy vary across different model scales, and is there a threshold where redundancy becomes more pronounced?
- Basis in paper: [inferred] The paper notes that "the performance decline was not as significant on the larger model of the 13B" compared to smaller models, and mentions that "RWKV model appears less redundant than Mamba and Transformer models."
- Why unresolved: The authors observe differences in redundancy between model sizes and architectures but don't systematically investigate the relationship between model scale and redundancy patterns.
- What evidence would resolve it: Comprehensive experiments across a wide range of model sizes (from 1B to 100B+ parameters) measuring redundancy metrics, or analysis of how redundancy scales with model depth and width.

### Open Question 3
- Question: Can the Block Influence metric be used as a training-time diagnostic tool to optimize layer allocation and prevent redundancy?
- Basis in paper: [explicit] The authors introduce BI as a metric to "quantify how much the hidden state changes after passing through each layer" and demonstrate its effectiveness for pruning, but don't explore its use during training.
- Why unresolved: While BI is validated as an effective pruning metric, the paper doesn't investigate whether monitoring BI during training could guide architectural decisions or training procedures to reduce redundancy from the start.
- What evidence would resolve it: Experiments using BI as a regularizer during training, or developing training schedules that minimize BI variance across layers, showing whether this leads to more parameter-efficient models without requiring pruning.

## Limitations
- The study focuses primarily on English benchmarks, raising questions about cross-lingual applicability
- The layer removal approach may not be optimal for all downstream tasks, particularly generative tasks
- The pre-norm architecture's role in creating layer redundancy is presented but not extensively explored across alternative architectures

## Confidence
- **High confidence**: The empirical results showing 90% performance retention with 25% parameter reduction are well-supported by the presented experiments across multiple models and benchmarks
- **Medium confidence**: The mechanism explaining pre-norm architecture's role in layer redundancy is plausible but could benefit from more theoretical analysis and exploration of alternative architectures
- **Medium confidence**: The claim that layer removal outperforms fine-grained pruning methods is supported by experiments but doesn't rule out the possibility that more sophisticated fine-grained methods could be equally effective

## Next Checks
1. **Cross-architecture validation**: Test the BI metric and layer removal approach on post-norm and mixed-norm transformer architectures to determine if the redundancy phenomenon is truly architecture-dependent
2. **Task-specific ablation**: Perform detailed ablation studies on individual layers for both generative and classification tasks to identify task-specific critical layers and understand why performance differs across task types
3. **Alternative similarity metrics**: Compare BI metric performance using different similarity measures (Euclidean distance, KL divergence) to assess whether cosine similarity is the optimal choice for layer importance quantification