---
ver: rpa2
title: 'SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain'
arxiv_id: '2407.19584'
source_url: https://arxiv.org/abs/2407.19584
tags:
- arxiv
- legal
- preprint
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SaulLM-54B and SaulLM-141B, two large language
  models (LLMs) specialized for the legal domain using a three-stage approach: continued
  pretraining on over 540B legal tokens, instruction fine-tuning with legal-specific
  prompts, and preference alignment via synthetic legal data. The models, based on
  the Mixtral architecture with 54B and 141B parameters, outperform previous open-source
  legal models on LegalBench-Instruct, achieving mean balanced accuracy of 73.32%
  (SaulLM-medium) and 73.35% (SaulLM-large), surpassing GPT-4 and Llama3 on legal
  benchmarks.'
---

# SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain

## Quick Facts
- arXiv ID: 2407.19584
- Source URL: https://arxiv.org/abs/2407.19584
- Reference count: 40
- Primary result: SaulLM-54B/141B achieve 73.32%/73.35% mean balanced accuracy on LegalBench-Instruct, surpassing GPT-4 and Llama3 on legal benchmarks

## Executive Summary
This paper introduces SaulLM-54B and SaulLM-141B, two large language models specialized for the legal domain through a three-stage approach: continued pretraining on over 540B legal tokens, instruction fine-tuning with legal-specific prompts, and preference alignment via synthetic legal data. The models, based on the Mixtral architecture with 54B and 141B parameters, demonstrate significant improvements in legal domain adaptation, achieving state-of-the-art performance on LegalBench-Instruct while maintaining competitive results on general benchmarks. The study highlights the effectiveness of scaling both model size and corpus size for domain-specific adaptation.

## Method Summary
The method employs a three-stage pipeline for legal domain adaptation. First, continued pretraining is conducted on a diverse corpus of 540B+ legal tokens from sources like FreeLaw, EDGAR, and EuroParl, using Mixtral-54B/141B architectures with AdamW optimization. Second, instruction fine-tuning is applied using datasets like UltraInteract and synthetic legal dialogues to teach task execution. Finally, preference alignment via DPO incorporates synthetic legal preference data to align outputs with human preferences. The approach uses 98% legal tokens with 2% replay for general knowledge retention.

## Key Results
- SaulLM-54B achieves 73.32% mean balanced accuracy on LegalBench-Instruct, outperforming GPT-4 and Llama3
- SaulLM-141B achieves 73.35% mean balanced accuracy, demonstrating scaling benefits
- Models maintain strong performance on general benchmarks (MMLU, TruthfulQA) despite heavy legal specialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling up model and corpus size improves legal domain adaptation performance
- Mechanism: Larger models and more legal tokens provide richer representations of legal concepts, enabling better comprehension and generation of legal text
- Core assumption: The base Mixtral architecture benefits from increased scale and legal-specific data
- Evidence anchors: Scaling up both model and corpus size significantly enhances legal domain adaptation
- Break condition: If additional scaling provides diminishing returns or harms performance (inverse scaling laws observed in some tasks)

### Mechanism 2
- Claim: Three-stage domain adaptation (continued pretraining, instruction fine-tuning, preference alignment) improves legal performance
- Mechanism: Each stage progressively specializes the model: continued pretraining builds legal knowledge, instruction fine-tuning teaches legal task execution, and preference alignment ensures outputs align with human legal preferences
- Core assumption: The sequential application of these adaptation methods is synergistic rather than redundant
- Evidence anchors: Three-stage approach with continued pretraining, instruction fine-tuning, and preference alignment via synthetic legal data
- Break condition: If later stages degrade performance or if one stage alone is sufficient

### Mechanism 3
- Claim: Synthetic data can effectively replace or augment human preference data for alignment
- Mechanism: Synthetic legal scenarios can simulate complex legal reasoning, allowing models to learn preference alignment without extensive human annotation
- Core assumption: Synthetic data can capture the complexity and nuance of human legal reasoning
- Evidence anchors: Integration of synthetically generated data enhances the models' capabilities
- Break condition: If synthetic data fails to capture legal nuance or introduces harmful biases

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Mixtral models use MoE to efficiently handle large parameter counts while maintaining computational efficiency
  - Quick check question: How does the MoE layer select which experts to activate for a given input?

- Concept: Continued pretraining vs. fine-tuning
  - Why needed here: The paper uses continued pretraining to build legal knowledge before fine-tuning for specific tasks
  - Quick check question: What's the key difference between continued pretraining and standard fine-tuning in terms of objective and data?

- Concept: Preference alignment (DPO)
  - Why needed here: DPO aligns model outputs with human preferences, critical for legal applications where accuracy matters
  - Quick check question: How does DPO differ from traditional RLHF in terms of implementation and computational requirements?

## Architecture Onboarding

- Component map: Mixtral-54B/MoE: 32 layers, 4096 model dimension, 14,336 hidden dimension, 8 experts, 2 active per input. Mixtral-141B/MoE: 56 layers, 6144 model dimension, 16,384 hidden dimension, 8 experts, 2 active per input. All with 8,192 token context
- Critical path: Pretraining → Instruction Fine-tuning → Preference Alignment → Evaluation
- Design tradeoffs: MoE provides parameter efficiency but adds routing complexity; larger models improve performance but increase computational cost
- Failure signatures: Inverse scaling in some tasks, model verbosity affecting benchmark parsing
- First 3 experiments:
  1. Run pretraining with legal corpus and measure loss reduction over epochs
  2. Apply instruction fine-tuning with legal instructions and evaluate on LegalBench-Instruct
  3. Apply DPO with synthetic legal preference data and compare performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SaulLM-54B and SaulLM-141B compare to proprietary legal LLMs like Harvey and CaseText's CoCounsel?
- Basis in paper: The paper extensively compares SaulLM models to open-source models (GPT-4, Llama3, Mixtral) but does not mention or compare against proprietary legal LLMs
- Why unresolved: The paper focuses on open-source comparisons, leaving the relative performance against proprietary legal models unknown
- What evidence would resolve it: Benchmarking SaulLM models against proprietary legal LLMs on the same LegalBench-Instruct tasks

### Open Question 2
- Question: What is the optimal ratio of legal to general tokens in the continued pretraining corpus for maximizing legal domain adaptation?
- Basis in paper: The paper uses a 98% legal / 2% replay ratio but does not experiment with different ratios to find the optimal balance
- Why unresolved: The study uses a fixed ratio without exploring how different proportions affect performance
- What evidence would resolve it: Systematic experiments varying the legal-to-general token ratio in pretraining and measuring impact on legal benchmark performance

### Open Question 3
- Question: How does the performance of SaulLM models vary across different legal jurisdictions (e.g., civil law vs. common law systems)?
- Basis in paper: The paper acknowledges the diversity of legal systems but only evaluates on benchmarks based on American legal frameworks
- Why unresolved: Evaluation is limited to American legal tasks, despite the model being trained on diverse international legal corpora
- What evidence would resolve it: Testing SaulLM models on jurisdiction-specific legal benchmarks from different legal traditions (e.g., European civil law, UK common law, Asian legal systems)

## Limitations
- The base Mixtral-54B/141B architectures used are not explicitly specified, creating uncertainty about the actual parameter scaling achieved
- Synthetic data generation methodology for legal reasoning is described but not detailed, making it difficult to assess whether synthetic scenarios truly capture legal complexity
- The study lacks analysis of real-world deployment scenarios or error analysis on challenging legal cases

## Confidence

**High Confidence**: The claim that SaulLM models outperform previous open-source legal models on LegalBench-Instruct (73.32% vs previous best of 64.83%)

**Medium Confidence**: The mechanism that scaling up both model and corpus size improves legal domain adaptation, given strong benchmark results but limited ablation studies on scaling effects

**Medium Confidence**: The three-stage adaptation approach is synergistic, though the relative contribution of each stage is not quantified

## Next Checks
1. Conduct ablation studies to isolate the impact of each adaptation stage (pretraining, instruction fine-tuning, preference alignment) on final performance
2. Test model performance on real-world legal tasks beyond curated benchmarks, including qualitative error analysis on complex legal reasoning cases
3. Evaluate the effectiveness of synthetic data by comparing preference alignment results using synthetic vs human-annotated legal preference data on the same tasks