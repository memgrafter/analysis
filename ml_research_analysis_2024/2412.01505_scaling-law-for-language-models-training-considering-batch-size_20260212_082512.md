---
ver: rpa2
title: Scaling Law for Language Models Training Considering Batch Size
arxiv_id: '2412.01505'
source_url: https://arxiv.org/abs/2412.01505
tags:
- batch
- size
- training
- learning
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how global batch size affects large language
  model (LLM) training, aiming to establish scaling laws that guide optimal training
  strategies under resource constraints. The authors train GPT-style models ranging
  from 125M to 2.6B parameters on up to 300B high-quality tokens, exploring batch
  sizes from 1M to 32M tokens.
---

# Scaling Law for Language Models Training Considering Batch Size

## Quick Facts
- arXiv ID: 2412.01505
- Source URL: https://arxiv.org/abs/2412.01505
- Authors: Xian Shuai; Yiding Wang; Yimeng Wu; Xin Jiang; Xiaozhe Ren
- Reference count: 17
- One-line primary result: Optimal batch size scales as Bopt ∝ C^0.102 for fixed compute and Bopt ∝ D^0.264 for fixed data, with optimal learning rate growing sublinearly with batch size

## Executive Summary
This paper investigates how global batch size affects large language model (LLM) training and establishes scaling laws that guide optimal training strategies under resource constraints. The authors train GPT-style models ranging from 125M to 2.6B parameters on up to 300B high-quality tokens, exploring batch sizes from 1M to 32M tokens. They establish a basic scaling law relating model size N and training data D, then examine how varying batch sizes and learning rates impact convergence and generalization. The study derives batch size scaling laws for two cases: with fixed compute budget and with fixed training data. Extrapolation experiments on 4.3B and 7B models validate these laws, showing that their predicted batch size and learning rate combinations achieve lower loss than baselines.

## Method Summary
The authors conduct extensive experiments training GPT-style transformer models ranging from 125M to 2.6B parameters on up to 300B high-quality tokens using distributed synchronous SGD on Huawei Ascend clusters. They systematically vary batch sizes from 1M to 32M tokens and test different learning rate scaling schemes (linear and square root with batch size). The experiments explore three scenarios: basic scaling law validation, step-loss and token-loss tradeoffs, and batch size scaling law validation. The authors use AdamW optimizer with β1=0.9 and β2=0.95, linear warm-up, and cosine decay learning rate schedules. They perform grid searches over batch sizes and learning rates to identify optimal configurations and validate their findings through extrapolation experiments on 4.3B and 7B models.

## Key Results
- Established basic scaling law: Loss decreases with model size N and training data D following Eq. 15
- Optimal batch size scales as Bopt ∝ C^0.102 for fixed compute budget and Bopt ∝ D^0.264 for fixed dataset size
- Optimal learning rate grows sublinearly with batch size (LRopt ∝ B^γ, γ ∈ [0.75,1])
- Extrapolation experiments show predicted batch size and learning rate combinations achieve lower loss than baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large batch sizes reduce gradient noise, enabling higher learning rates without divergence up to a plateau.
- Mechanism: Larger batch sizes provide a less noisy gradient estimate (lower gradient noise scale Bnoise). According to gradient noise scale theory, when batch size B is much smaller than Bnoise, the optimal learning rate grows approximately linearly with B. As B increases, this relationship continues but with diminishing returns, eventually plateauing because of a theoretical upper bound on the learning rate beyond which the model diverges.
- Core assumption: The gradient noise scale Bnoise remains relatively stable across training steps, so the relationship between B and the optimal learning rate can be treated as continuous.
- Evidence anchors:
  - [section]: "Eq. 6 indicates that to achieve a loss improvement ∆Lmax, the full-batch gradient descend only needs a single step, while it needs to take δS = (1+ Bnoise/B) steps when using batch size B... the optimal learning rate under the noisy gradient is smaller than the one under the ideal case, i.e., ηopt(B) < η max."
  - [section]: "When B ≪ Bnoise, the optimal learning rate grows almost with the square root of B, which aligns with the suggestion in previous works [Hoffer et al., 2017; Granziol et al., 2022]."
  - [corpus]: Weak support; corpus lacks direct gradient noise scale measurements.
- Break condition: If the gradient noise scale increases significantly during training, the relationship between B and optimal LR will deviate from the predicted sublinear growth.

### Mechanism 2
- Claim: Under a fixed compute budget, the optimal batch size scales as Bopt ∝ C^0.102, balancing model size and training steps.
- Mechanism: The total compute C is split between model size N and training steps S. The study finds that for a given compute budget, Nopt ∝ C^0.464 and Sopt ∝ C^0.434. Since the total data D is the product of steps and batch size (D = S × B), and D is also fixed by the scaling law Dopt ∝ C^0.536, the batch size must scale as Bopt = Dopt / Sopt ∝ C^(0.536-0.434) ≈ C^0.102.
- Core assumption: The compute-efficient frontier holds, meaning N, D, and B are jointly optimized to minimize loss for a fixed compute budget.
- Evidence anchors:
  - [section]: "This indicates that given a fixed compute budget, to achieve the lowest loss, the compute should be allocated with Nopt ∝ C^0.464, Sopt ∝ C^0.434, Bopt ∝ C^0.102."
  - [section]: "The dashed line represents the FLOP-loss frontier across several model sizes, and the dots on each model's curve show the optimal points for that size model regarding the compute-loss effectiveness."
  - [corpus]: No direct corpus evidence; claim relies on internal regression analysis.
- Break condition: If the model cannot efficiently utilize large batch sizes (e.g., due to communication overhead or memory constraints), the predicted scaling may overestimate the feasible Bopt.

### Mechanism 3
- Claim: With a fixed dataset size, the optimal batch size scales sublinearly as Bopt ∝ D^0.264, increasing the number of iterations.
- Mechanism: When N and D are fixed (not necessarily on the compute-efficient frontier), the optimal batch size must balance the number of training steps and the batch size to minimize loss. Empirically, the study finds that as D increases, Bopt grows sublinearly (exponent 0.264). This means that for larger datasets, proportionally larger batch sizes are used, but not as fast as the dataset grows, ensuring enough training steps remain.
- Core assumption: The fixed dataset scenario is distinct from the compute-efficient case, and the optimal B is determined by the interplay between batch size and the number of training steps required for convergence.
- Evidence anchors:
  - [section]: "Specifically: Bopt ≈ 3.24 × 10^3 · D^0.264 (17) Notably, this relationship will not yield FLOP-Loss optimality. Instead, it applies to scenarios where both N and D are pre-determined and may not necessarily satisfy the compute-efficient frontier condition in Eq. 14."
  - [section]: "First, when the training data amount is relatively limited (e.g., less than 10B), the optimal batch size increases linearly with the data amount (i.e., the exponent term of D is 1), meaning that the number of iterations should remain above a minimum threshold."
  - [corpus]: No corpus evidence; relationship is derived from experimental data.
- Break condition: If the dataset is too small to support the minimum number of training steps, the linear growth regime will dominate, and the sublinear scaling will not apply.

## Foundational Learning

- Concept: Gradient noise scale (Bnoise)
  - Why needed here: Understanding how batch size affects gradient noise is central to explaining why larger batches can support higher learning rates and how the optimal learning rate scales with batch size.
  - Quick check question: What happens to the optimal learning rate as batch size increases when B ≪ Bnoise, and why?

- Concept: Compute-efficient frontier
  - Why needed here: The study's scaling laws depend on the relationship between model size, dataset size, and batch size under a fixed compute budget. Understanding the compute-efficient frontier is key to interpreting the Bopt ∝ C^0.102 result.
  - Quick check question: How are Nopt and Dopt related to the compute budget C on the compute-efficient frontier?

- Concept: Sublinear scaling
  - Why needed here: The study finds that the optimal batch size grows sublinearly with both compute and dataset size. Understanding sublinear scaling is crucial for predicting how to allocate resources as scale increases.
  - Quick check question: What is the exponent in the relationship Bopt ∝ D^0.264, and what does this imply about the growth of batch size relative to dataset size?

## Architecture Onboarding

- Component map:
  - Data -> Model (GPT-style transformer) -> Training (distributed SGD) -> Loss tracking -> Hyperparameter tuning
  - Model (125M-2.6B params) -> Optimizer (AdamW) -> Learning rate schedule (linear warm-up, cosine decay) -> Batch size variation

- Critical path:
  1. Data preparation and curation (300B tokens)
  2. Model initialization and hyperparameter setup
  3. Training loop with distributed data parallelism
  4. Loss tracking and convergence monitoring
  5. Hyperparameter tuning (batch size and learning rate grid search)
  6. Extrapolation experiments on larger models

- Design tradeoffs:
  - Batch size vs. learning rate: Larger batch sizes reduce gradient noise but require fewer training steps, necessitating higher learning rates to compensate. The optimal learning rate grows sublinearly with batch size.
  - Compute allocation: For a fixed compute budget, increasing model size requires proportionally more data and fewer training steps, leading to larger optimal batch sizes.
  - Dataset size vs. batch size: With a fixed dataset, the optimal batch size grows sublinearly with dataset size to ensure sufficient training steps.

- Failure signatures:
  - Divergence: Learning rate too high for the batch size, causing loss to spike or model to fail to converge
  - Underutilization: Batch size too small, leading to slow convergence and higher loss than predicted by scaling laws
  - Overfitting: Insufficient data for the model size, resulting in poor generalization despite low training loss
  - Communication bottleneck: Batch size too large for the distributed infrastructure, causing slow training due to synchronization overhead

- First 3 experiments:
  1. Reproduce the basic scaling law (Eq. 15) by training models of varying sizes (125M to 2.6B) on 300B tokens with a fixed batch size and learning rate, and regress the loss as a function of N and D
  2. Investigate the step-loss and token-loss tradeoff by training a 350M model on 100B tokens with batch sizes from 1M to 32M and varying learning rate scaling schemes, and plot the loss vs. steps and loss vs. tokens
  3. Validate the batch size scaling laws by training models at the edge of the predicted optimal batch size for a given compute or dataset size, and compare the achieved loss to the predicted minimum

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do scaling laws change for models larger than 7B parameters, and what are the optimal batch size and learning rate combinations for such models?
- Basis in paper: [explicit] The paper mentions that extrapolation experiments did not involve models larger than 7B or with more training data due to compute resource constraints.
- Why unresolved: The authors explicitly state they couldn't test models larger than 7B due to compute limitations, leaving a gap in understanding scaling behavior for truly large LLMs.
- What evidence would resolve it: Training and evaluating models in the 10B-100B+ parameter range with varying batch sizes and learning rates, measuring loss convergence and downstream task performance.

### Open Question 2
- Question: What is the explicit relationship between optimal batch size and model size (N), beyond the implicit trends observed in Fig. 6?
- Basis in paper: [inferred] The authors note in the discussion that they "do not obtain an explicit relationship between the optimal batch size and the model size" despite observing "subtle differences in their trends" across different model sizes.
- Why unresolved: The paper only provides scaling laws for batch size as a function of compute budget (C) or data amount (D), not as a direct function of model size (N), which would be useful for practical training.
- What evidence would resolve it: Conducting controlled experiments across a wider range of model sizes (e.g., 125M to 10B+) while keeping other factors constant to derive a clear N-dependent batch size scaling law.

### Open Question 3
- Question: How do different learning rate scheduling strategies (beyond simple linear scaling) affect the optimal batch size and overall training efficiency for large batch training?
- Basis in paper: [explicit] The authors mention in the discussion that they use a learning rate scaling factor instead of different learning rate warm-up and decay schedules, which "considered an influence of the training progress."
- Why unresolved: The paper focuses on three basic learning rate schemes (original, sqrt scaling, linear scaling) and does not explore more sophisticated scheduling strategies that might better optimize large batch training.
- What evidence would resolve it: Systematically comparing various learning rate schedules (e.g., cosine decay with restarts, step decay, OneCycle) across different batch sizes and model sizes to determine their impact on convergence speed and final loss.

## Limitations
- Scaling laws may not generalize beyond GPT-style architectures or the specific training configuration used
- Absence of direct gradient noise scale measurements introduces uncertainty in theoretical underpinnings
- Fixed sequence length and optimizer settings limit applicability to other training setups

## Confidence
- **High Confidence**: The basic scaling law relating model size N and training data D (Eq. 15) is well-supported by experimental data and aligns with prior work. The relationship Bopt ∝ C^0.102 for fixed compute is also robust, derived from the compute-efficient frontier.
- **Medium Confidence**: The sublinear scaling of optimal batch size with dataset size (Bopt ∝ D^0.264) is supported by experiments but less theoretically grounded. The optimal learning rate scaling with batch size (LRopt ∝ B^γ, γ ∈ [0.75,1]) is empirically derived and may vary with different training setups.
- **Low Confidence**: Extrapolation to much larger models (e.g., beyond 7B parameters) is speculative and may not hold due to communication overhead, memory constraints, or changes in training dynamics at extreme scales.

## Next Checks
1. Measure the gradient noise scale Bnoise directly during training across different batch sizes and model sizes to verify the theoretical relationship between batch size and optimal learning rate
2. Test the scaling laws on different model architectures (e.g., decoder-only vs. encoder-decoder, varying attention mechanisms) to assess their generalizability
3. Investigate the impact of communication overhead and memory constraints on the optimal batch size by varying the distributed training setup and measuring actual training efficiency at different batch sizes