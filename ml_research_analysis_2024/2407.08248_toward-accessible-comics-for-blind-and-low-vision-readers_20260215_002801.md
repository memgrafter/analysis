---
ver: rpa2
title: Toward accessible comics for blind and low vision readers
arxiv_id: '2407.08248'
source_url: https://arxiv.org/abs/2407.08248
tags:
- character
- text
- comics
- script
- comic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to automatically generate detailed
  comic book scripts for improved accessibility to blind and low vision readers. The
  approach uses computer vision and OCR to extract content such as panels, characters,
  text, and reading order from comic strip images.
---

# Toward accessible comics for blind and low vision readers

## Quick Facts
- arXiv ID: 2407.08248
- Source URL: https://arxiv.org/abs/2407.08248
- Reference count: 40
- Method automatically generates detailed comic book scripts using computer vision, OCR, and LLMs for blind and low vision readers

## Executive Summary
This paper presents a novel method to automatically generate detailed comic book scripts for improved accessibility to blind and low vision readers. The approach uses computer vision and OCR to extract content such as panels, characters, text, and reading order from comic strip images. Character identification and clustering are performed to associate dialogues with named characters. A large language model is then used with prompt engineering techniques to generate a contextual panel description including character appearances, postures, moods, and dialogues. The generated script can be used to produce audiobooks with different voices for characters, captions, and sound effects.

## Method Summary
The method combines computer vision and OCR techniques to extract content from comic strip images, including panels, characters, text, reading order, and speech balloon association. Character clustering is performed using CLIP ViT-L/14 model with HDBSCAN algorithm to group characters by visual similarities. A large language model with prompt engineering techniques and chain-of-prompt approach is used to generate contextual panel descriptions. The system artificially adds character name identifiers as white rectangles on character bodies to improve VLM panel description accuracy. The final output is a detailed script that can be used for text-to-speech synthesis with different voices for characters, captions, and sound effects.

## Key Results
- Text type classification achieved F1-score of 96.55% and accuracy of 97.45%
- Character clustering successfully grouped more than 12 instances for each character in experiments
- Generated panel descriptions showed good semantic similarity to human annotations
- The method demonstrates promise for improving comic accessibility for blind and low vision readers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using prompt engineering with chain-of-thought improves LLM accuracy without requiring additional training data
- Mechanism: The method breaks down complex reasoning tasks into sequential prompts that guide the model through progressive stages of analysis, allowing it to leverage existing knowledge more effectively
- Core assumption: LLMs have sufficient reasoning capacity to handle chained prompts and can maintain context across multiple reasoning steps
- Evidence anchors:
  - [abstract] "we investigate prompt engineering techniques such as chain-of-prompt to make the most of this technique and dramatically improves prediction quality without requiring additional data or annotation time"
  - [section 3.2] "We propose a chain-of-prompt of four prompts to guide the model throughout its knowledge exploration"
  - [corpus] Weak - the corpus shows related work on prompt engineering but no direct evidence for this specific chain-of-thought approach

### Mechanism 2
- Claim: Character clustering using CLIP embeddings and HDBSCAN effectively groups character instances across panels
- Mechanism: CLIP's vision transformer extracts visual features that capture character appearance, which are then reduced via UMAP and clustered using density-based methods that automatically determine cluster count
- Core assumption: Character appearances are visually consistent enough across panels for CLIP embeddings to group them together, and the clustering algorithm can handle variations in pose, lighting, and style
- Evidence anchors:
  - [section 3.1] "For character clustering, we compute image feature vectors on each character instance using a variant of Contrastive Language-Image Pre-training (CLIP) associated with Vision Transformer (ViT)"
  - [section 4.3] "The algorithm grouped more than 12 instance for each cluster in our experiment"
  - [corpus] Weak - corpus contains related work on character clustering but no specific evidence for this CLIP+HDBSCAN combination

### Mechanism 3
- Claim: Adding character names as white rectangles on character bodies improves VLM panel description accuracy
- Mechanism: By providing explicit text labels in the image, the VLM can more reliably associate character identities with their visual representations and textual descriptions from the script
- Core assumption: VLMs can effectively process overlaid text in images and use this information to improve their understanding of character identities and relationships
- Evidence anchors:
  - [section 3.3] "Given that these models can also interpret text [39] like OCR, we introduce character name's identifier artificially into panels"
  - [section 4.4] "We believe this information plays a major role in the similarity measure"
  - [corpus] Weak - corpus shows related work on multimodal models but no specific evidence for this annotation technique

## Foundational Learning

- Concept: CLIP (Contrastive Language-Image Pre-training) model architecture
  - Why needed here: The system relies on CLIP's image embedding capabilities for character clustering and potentially for other visual understanding tasks
  - Quick check question: How does CLIP's zero-shot classification capability work without requiring task-specific fine-tuning?

- Concept: Vision Transformer (ViT) architecture
  - Why needed here: CLIP uses ViT to extract visual features, which are crucial for the character clustering mechanism
  - Quick check question: What advantages does ViT have over traditional CNN architectures for image feature extraction?

- Concept: Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN)
  - Why needed here: The system uses HDBSCAN for character clustering because it can automatically determine the number of clusters and handle noise
  - Quick check question: How does HDBSCAN's density-based approach differ from k-means clustering in handling variable cluster sizes?

## Architecture Onboarding

- Component map: Image pre-processing → OCR extraction → Text type classification → Character detection → Character clustering → Script generation → LLM name inference → VLM panel description → Output script

- Critical path:
  1. Character detection and clustering must complete before name inference
  2. Script generation must include all text types before LLM processing
  3. Panel description requires both script context and annotated images

- Design tradeoffs:
  - Using zero-shot models (CLIP, LLM, VLM) vs. fine-tuning on comics-specific data
  - Balancing annotation effort (white rectangles) against description accuracy
  - Choosing between real-time processing vs. batch processing for comic volumes

- Failure signatures:
  - Character clustering fails: Similar-looking characters get merged, or the same character gets split into multiple clusters
  - Name inference fails: Characters remain unnamed or get wrong names
  - Panel description fails: Generic descriptions that don't match the script context

- First 3 experiments:
  1. Test character clustering on a simple comic with clearly distinct characters to verify the CLIP+HDBSCAN pipeline
  2. Validate the name inference chain-of-prompts with a script containing obvious character references
  3. Measure VLM description accuracy with and without the character name annotations to quantify the improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of character clustering vary with different image quality and styles of comics?
- Basis in paper: [inferred] The authors note that the title "Patents" showed less confusion in character clustering compared to "Escape with me," possibly due to better image quality and more colorful characters. They also mention the need to extend evaluation to other styles in the future.
- Why unresolved: The current study only experimented on two public domain English comics with different styles and qualities. There is no systematic analysis of how character clustering performance is affected by variations in image quality and comic styles.
- What evidence would resolve it: Conducting experiments on a diverse dataset of comics with varying image qualities and styles, and analyzing the clustering accuracy for each subset.

### Open Question 2
- Question: What is the impact of character background masking on the accuracy of character clustering and name inference?
- Basis in paper: [inferred] The authors observed that some clustering errors were due to background similarities between characters, suggesting that masking the background could improve clustering. They also note that errors in clustering might propagate to name inference and panel description.
- Why unresolved: The current study did not explore the effect of background masking on clustering and name inference accuracy. It remains unclear how much background information affects the performance of these tasks.
- What evidence would resolve it: Implementing background masking techniques and comparing the clustering and name inference results with and without masking on a dataset of comics.

### Open Question 3
- Question: How can the text-to-speech synthesis be improved by incorporating additional information from balloon contour analysis?
- Basis in paper: [explicit] The authors mention that complementary information such as balloon contour analysis could be used to include speech tone (e.g., shouted, whispered, thought) into the script and modulate the tone and speed of speech synthesis.
- Why unresolved: The current study did not explore the integration of balloon contour analysis for enhancing text-to-speech synthesis. The potential benefits and challenges of incorporating this information are not yet known.
- What evidence would resolve it: Developing a method to extract speech tone information from balloon contours and evaluating its impact on the quality and naturalness of text-to-speech synthesis for comics.

## Limitations
- Limited evaluation on diverse comic styles and artistic conventions
- Character clustering may struggle with dramatic character transformations or similar-looking characters
- Current pipeline requires manual character name provision, creating a bottleneck for full automation

## Confidence
- Character clustering mechanism: Medium-High
- Prompt engineering approach: Medium-High
- VLM annotation technique: Medium

## Next Checks
1. **Cross-Style Generalization Test**: Evaluate the complete pipeline on comics with varying artistic styles (manga, American comics, European bande dessinée) to assess robustness across visual conventions and character designs.

2. **Character Variation Challenge**: Create a controlled test set with characters undergoing dramatic appearance changes (costumes, age progression, disguises) to stress-test the CLIP+HDBSCAN clustering mechanism's ability to maintain character identity across transformations.

3. **Annotation Method Comparison**: Conduct an ablation study comparing VLM description quality with different annotation approaches: no character names, white rectangles, embedded text overlays, and character name captions outside the panel area to determine optimal text integration strategy.