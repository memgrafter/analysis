---
ver: rpa2
title: A Self-matching Training Method with Annotation Embedding Models for Ontology
  Subsumption Prediction
arxiv_id: '2402.16278'
source_url: https://arxiv.org/abs/2402.16278
tags:
- embeddings
- axioms
- annotation
- ontology
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a self-matching training method with two ontology
  embedding models, InME and CoME, for concept subsumption prediction. The self-matching
  training method improves the robustness of a binary classifier by recognizing the
  similarity between subclasses and superclasses, addressing the difficulties of similar
  and isolated entities.
---

# A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction

## Quick Facts
- arXiv ID: 2402.16278
- Source URL: https://arxiv.org/abs/2402.16278
- Reference count: 36
- Outperforms existing ontology embeddings on GO and FoodOn ontologies

## Executive Summary
This paper proposes a self-matching training method with two annotation embedding models (InME and CoME) for concept subsumption prediction in ontologies. The method captures global and local information from annotation axioms using inverted-index and co-occurrence matrices, respectively. By including self-matching training samples, the approach improves robustness when predicting similar and isolated entities. Experiments on GO, FoodOn, and HeLiS ontologies demonstrate superior performance compared to existing ontology embedding methods.

## Method Summary
The method extracts annotation axioms from OWL ontologies and constructs inverted-index (X_global) and co-occurrence (X_local) matrices to capture global and local information, respectively. Autoencoders compress these high-dimensional matrices into low-dimensional word embeddings, which are then averaged to generate entity embeddings. A self-matching training method enhances a binary classifier by including positive samples of the form (e, e), improving prediction of isolated entities. The Random Forest classifier predicts whether entity e1 is a subclass of entity e2 based on concatenated embeddings.

## Key Results
- Self-matching training method improves robustness when predicting similar and isolated entities
- InME and CoME outperform existing ontology embeddings on GO and FoodOn ontologies
- CoME concatenated with OWL2Vec* outperforms existing methods on HeLiS ontology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-matching training improves prediction of isolated entities by including positive samples of the form (e, e).
- Mechanism: The binary classifier learns that every entity is its own superclass, enabling better generalization to isolated entities not seen in positive training pairs.
- Core assumption: Isolated entities have no positive training pairs, so the model must infer their relationships from self-matching samples.
- Evidence anchors:
  - [abstract]: "The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology."
  - [section 5.1]: "The self-matching training method enhances a binary classifier to recognize the similarity between subclasses and superclasses."
  - [corpus]: Weak—no explicit citation to prior work on self-matching training for isolated entities.
- Break condition: If isolated entities have very different representations from their self-matching embeddings, the method fails to generalize.

### Mechanism 2
- Claim: InME captures global information by indicating the occurring locations of each word in a set of annotation axioms.
- Mechanism: The inverted-index matrix X global represents whether word wi appears in the annotations of entity ej, providing entity-to-word relationship information.
- Core assumption: Global information (entity-to-word occurrence) is more effective than local information (co-occurrence) for concept subsumption prediction.
- Evidence anchors:
  - [abstract]: "The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom."
  - [section 5.2]: "As global information, the locations of the words 'cranial,' 'facial,' and 'nerve' are identified by entities in a set of annotation axioms."
  - [corpus]: Weak—no direct comparison to prior global-information embeddings.
- Break condition: If local information (co-occurrence) is more important than global information for the specific ontology domain.

### Mechanism 3
- Claim: CoME captures local information by representing the co-occurrences of words in each annotation axiom.
- Mechanism: The co-occurrence matrix X local indicates whether word wi and wj appear together in an annotation axiom, capturing local lexical patterns.
- Core assumption: Local information about word co-occurrences improves the distinction between similar entities.
- Evidence anchors:
  - [abstract]: "The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom."
  - [section 5.2]: "As local information, these words co-occur as follows: 'cranial' co-occurs with: cranial, nerve, formation."
  - [corpus]: Weak—no explicit citation showing superiority of local co-occurrence information.
- Break condition: If the co-occurrence matrix becomes too sparse or if global information is more discriminative.

## Foundational Learning

- Concept: Binary classifier for concept subsumption prediction
  - Why needed here: The paper uses Random Forest to predict whether entity e1 is a subclass of entity e2 based on concatenated embeddings.
  - Quick check question: What type of classifier is used to predict concept subsumption in this paper?

- Concept: Inverted-index matrix for global information extraction
  - Why needed here: InME uses this matrix to capture where each word appears across all annotation axioms for each entity.
  - Quick check question: What does the inverted-index matrix X global represent in InME?

- Concept: Co-occurrence matrix for local information extraction
  - Why needed here: CoME uses this matrix to capture which words appear together in the same annotation axiom.
  - Quick check question: What does the co-occurrence matrix X local represent in CoME?

## Architecture Onboarding

- Component map:
  Annotation axioms extraction from OWL ontologies -> Inverted-index matrix construction (InME) -> Co-occurrence matrix construction (CoME) -> Autoencoder compression to low-dimensional embeddings -> Entity embedding generation by averaging word embeddings -> Self-matching training data generation -> Random Forest binary classifier training -> Concept subsumption prediction ranking

- Critical path:
  1. Extract annotation axioms from ontology
  2. Build inverted-index and co-occurrence matrices
  3. Compress matrices with autoencoder to get word embeddings
  4. Generate entity embeddings by averaging word embeddings
  5. Create self-matching training samples
  6. Train Random Forest classifier
  7. Predict and rank concept subsumptions

- Design tradeoffs:
  - Global vs local information: InME focuses on entity-to-word occurrence, CoME on word co-occurrence within axioms
  - Matrix size: High-dimensional matrices require autoencoder compression
  - Self-matching vs conventional training: Including self-pairs improves isolated entity prediction but increases training data size
  - Annotation-only vs logical axioms: The method relies solely on annotations, potentially missing logical structure information

- Failure signatures:
  - Poor performance on ontologies with very few annotation axioms
  - Overfitting when autoencoder dimension is too high
  - Random Forest not improving over distance-based ranking for isolated entities
  - Inverted-index or co-occurrence matrices becoming too sparse

- First 3 experiments:
  1. Implement InME without self-matching training and compare MRR on GO ontology
  2. Implement CoME with self-matching training and compare MRR on FoodOn ontology
  3. Concatenate InME and CoME embeddings with self-matching training and evaluate on HeLiS ontology

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the self-matching training method perform on ontologies with more complex logical structures, such as those involving negation or cardinality restrictions?
- Basis in paper: [inferred] The paper mentions future work to extend to more complex classes and reasoning tasks, suggesting that the current method's performance on such ontologies is unknown.
- Why unresolved: The experiments were conducted on ontologies with simpler logical structures (DL SRI and SRIQ), and the method's effectiveness on more complex ontologies is not evaluated.
- What evidence would resolve it: Experiments on ontologies with complex logical structures, comparing the self-matching method's performance to existing methods on these ontologies.

### Open Question 2
- Question: What is the impact of the choice of embedding dimension on the performance of InME and CoME, and how does this compare to the impact on other ontology embedding models?
- Basis in paper: [explicit] The paper mentions fine-tuning the hyper-parameter of the compressed dimension n for InME and CoME, but does not provide a detailed analysis of its impact on performance.
- Why unresolved: The paper does not explore the sensitivity of InME and CoME to the embedding dimension or compare it to the sensitivity of other models.
- What evidence would resolve it: Experiments varying the embedding dimension for InME, CoME, and other models, and analyzing the impact on performance.

### Open Question 3
- Question: How does the self-matching training method affect the interpretability of the ontology embeddings, and can the learned representations be used to gain insights into the ontology's structure?
- Basis in paper: [inferred] The paper focuses on the performance of the embeddings for prediction tasks, but does not discuss the interpretability of the learned representations.
- Why unresolved: The paper does not explore the interpretability of the embeddings or their potential to provide insights into the ontology's structure.
- What evidence would resolve it: Analysis of the learned embeddings to identify interpretable patterns or relationships that correspond to the ontology's structure, and evaluation of the embeddings' ability to support knowledge discovery tasks.

## Limitations
- Performance heavily depends on availability of annotation axioms, which may be sparse in some ontologies
- Self-matching training's effectiveness for isolated entities lacks empirical validation against alternative approaches
- Method's generalizability to ontologies with complex logical structures remains untested

## Confidence
- **High confidence**: The self-matching training method's basic mechanism and its positive impact on binary classifier robustness
- **Medium confidence**: The relative effectiveness of InME's global information capture versus CoME's local information capture
- **Low confidence**: The method's performance on ontologies with limited annotation axioms, particularly HeLiS

## Next Checks
1. Implement ablation study: Compare InME and CoME performance with and without self-matching training on GO and FoodOn ontologies to quantify the exact contribution of self-matching to isolated entity prediction.

2. Test alternative information capture: Implement a modified CoME that uses global information (similar to InME) instead of local co-occurrence information, and compare performance on the same ontologies to determine whether global or local information is more effective.

3. Evaluate on additional ontologies: Test the method on additional ontologies with varying annotation densities (e.g., SNOMED CT, UMLS) to validate generalizability and identify failure conditions related to annotation sparsity.