---
ver: rpa2
title: Continual SFT Matches Multimodal RLHF with Negative Supervision
arxiv_id: '2411.14797'
source_url: https://arxiv.org/abs/2411.14797
tags:
- nsft
- image
- rlhf
- llav
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses whether multimodal RLHF is superior to continual\
  \ SFT during the preference alignment stage of VLMs. The authors observe that the\
  \ key success of multimodal RLHF lies in its negative supervision\u2014the logit\
  \ of rejected responses."
---

# Continual SFT Matches Multimodal RLHF with Negative Supervision

## Quick Facts
- arXiv ID: 2411.14797
- Source URL: https://arxiv.org/abs/2411.14797
- Reference count: 40
- Primary result: Negative supervised finetuning (nSFT) matches multimodal RLHF performance while being more memory efficient

## Executive Summary
This paper challenges the assumption that multimodal RLHF is inherently superior to continual SFT for VLM alignment. Through careful analysis, the authors identify that the key success factor in RLHF is its negative supervision mechanism - specifically the logit of rejected responses. They propose nSFT, a method that disentangles this negative supervision and achieves comparable performance using simple SFT loss. The approach is rigorously validated across multiple datasets, base models, and evaluation metrics, demonstrating that nSFT can match or exceed RLHF while being more computationally efficient.

## Method Summary
The nSFT method works by first generating model responses that serve as rejected answers, then using GPT-4 with a vision error codebook to identify specific errors and construct corrective conversations. Training uses a simple SFT loss on both ground truth responses and the constructed corrections. The approach can be enhanced with per-token KL constraints to further improve performance. The method requires significantly fewer models than traditional RLHF (1 vs 2-4), making it more memory efficient while maintaining comparable alignment quality.

## Key Results
- nSFT matches multimodal RLHF performance across multiple datasets and base models
- Achieves an overall best result across evaluation metrics (VQA, multimodal comprehension, hallucination detection)
- Provides 2-4x memory efficiency improvement over multimodal RLHF
- Adding KL constraints further improves nSFT performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative supervision in RLHF (logit of rejected responses) is the key factor for superior performance compared to naive SFT
- Mechanism: RLHF's DPO loss implicitly incorporates negative supervision by contrasting chosen vs rejected responses, while naive SFT only uses positive examples
- Core assumption: The gradient contribution from negative supervision is critical for proper alignment
- Evidence anchors:
  - [abstract] "we observe that the inherent value of multimodal RLHF lies in its negative supervision, the logit of the rejected responses"
  - [section 3.2] "we found practically that multimodal RLHF is not as perfect as it does, since it will face GPU memory shortage [46] and usually confronts unstable training issue [29]"

### Mechanism 2
- Claim: Disentangling negative supervision allows using simple SFT loss instead of complex RLHF optimization
- Mechanism: The proposed nSFT method uses an LLM to identify and reconstruct negative supervision from rejected responses, then trains with standard SFT loss
- Core assumption: The LLM can accurately identify and verbalize errors in rejected responses using the vision error codebook
- Evidence anchors:
  - [abstract] "Our nSFT disentangles this negative supervision in RLHF paradigm, and continually aligns VLMs with a simple SFT loss"
  - [section 3.3] "we involve a construction function G(·) (an LLM like GPT-4), and define our loss as Lsft(yc) + Lsft(G(yr))"

### Mechanism 3
- Claim: Adding KL constraint to nSFT further improves performance by regularizing token-level predictions
- Mechanism: Per-token KL divergence constraint encourages consistency with reference model while maintaining diversity
- Core assumption: The KL constraint helps prevent catastrophic forgetting and maintains model stability
- Evidence anchors:
  - [section 4.5] "we show that adding a per-token constraint to nSFT will further improve the results"
  - [section 4.5 Table 7] "cont. SFT ✓ 64.2 57.8 67.7" vs "cont. SFT ✗ 63.6 56.6 67.3"

## Foundational Learning

- Concept: Gradient analysis of loss functions
  - Why needed here: Understanding how different loss functions (DPO vs SFT) affect model updates is crucial for the theoretical foundation
  - Quick check question: What is the relationship between DPO gradient and SFT gradient according to the paper's derivation?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: The paper builds on RLHF methodology and improves upon it with negative supervision
  - Quick check question: How does DPO simplify the RLHF optimization process compared to traditional methods?

- Concept: Vision error codebook and error identification
  - Why needed here: The nSFT method relies on identifying specific error types in rejected responses using the codebook
  - Quick check question: What are the two levels of error classification used in the vision error codebook?

## Architecture Onboarding

- Component map:
  Vision encoder + connector → LLM → Output
  GPT-4 (or similar) for error identification and conversation construction
  Vision error codebook for error type classification
  Deepspeed framework with ZeRO-3 optimization

- Critical path:
  1. Input image and question → vision encoder
  2. Generate response using LLM
  3. Compare with ground truth to identify errors
  4. Use GPT-4 with error codebook to construct correction conversation
  5. Train using SFT loss on both ground truth and constructed corrections

- Design tradeoffs:
  - Memory efficiency vs. performance: nSFT uses one model vs. 2-4 models for RLHF
  - Error identification accuracy vs. computation: Using GPT-4 for error identification adds overhead
  - Dataset diversity vs. model stability: More diverse data may improve generalization but could introduce noise

- Failure signatures:
  - Model degradation on specific error types if vision error codebook is incomplete
  - Training instability if error identification by GPT-4 is inconsistent
  - Overfitting to training data if dataset is too small or homogeneous

- First 3 experiments:
  1. Implement basic nSFT pipeline with synthetic error data to verify the core mechanism
  2. Test error identification accuracy using GPT-4 on a small validation set
  3. Compare nSFT vs. baseline SFT on a controlled dataset with known error patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed nSFT approach perform when applied to larger vision-language models beyond LLaVa-1.5 and LLaVa-NeXT?
- Basis in paper: [explicit] The paper mentions that nSFT was tested on LLaVa-1.5-13B and LLaVa-NeXT-13B, but further scaling to even larger models is not explored.
- Why unresolved: The experiments only go up to 13B parameter models, leaving open the question of performance on models with 30B+ parameters or those with different architectural designs.
- What evidence would resolve it: Testing nSFT on a range of larger VLMs (e.g., 30B, 70B, or even trillion-parameter models) and comparing performance to RLHF methods on these models would provide the needed evidence.

### Open Question 2
- Question: Can the negative supervision construction method used in nSFT be effectively applied to natural language processing (NLP) tasks?
- Basis in paper: [explicit] The paper acknowledges that multimodal RLHF focuses on image-level hallucinations, while NLP RLHF aims to reduce toxic or offensive outputs, suggesting a potential difference in approach.
- Why unresolved: The paper does not explore whether the error identification and correction process used in nSFT can be adapted to NLP tasks with different objectives.
- What evidence would resolve it: Implementing nSFT on NLP tasks (e.g., text generation, sentiment analysis) and comparing its effectiveness to standard RLHF methods would demonstrate its generalizability.

### Open Question 3
- Question: What is the optimal data scale for training VLMs with nSFT, and how does it compare to other alignment methods?
- Basis in paper: [explicit] The paper shows that nSFT improves performance with 5k to 10k data, but the scaling behavior beyond this range is not fully explored.
- Why unresolved: While the paper provides some insights into data scaling, it does not determine the optimal amount of data needed for nSFT to match or exceed RLHF performance, nor does it compare this to other methods.
- What evidence would resolve it: Conducting experiments with varying data scales (e.g., 1k, 5k, 10k, 20k, 50k) for nSFT and comparing the results to RLHF methods would clarify the optimal data requirements.

## Limitations

- The error identification pipeline relies heavily on GPT-4's accuracy, which may vary across different error types and domains
- The vision error codebook approach assumes errors can be reliably categorized into discrete types, potentially missing nuanced failure modes
- Evaluation focuses primarily on reasoning and hallucination tasks, with limited assessment of open-ended generation quality or long-form reasoning capabilities

## Confidence

- **High Confidence**: The core empirical finding that nSFT matches multimodal RLHF performance across multiple datasets and base models is well-supported by extensive ablation studies and comparison experiments.
- **Medium Confidence**: The theoretical analysis of why negative supervision matters is logically sound but relies on several assumptions about error distribution and the effectiveness of the error codebook.
- **Low Confidence**: The generalization of these findings to other VLM architectures beyond LLaVA and LLaVA-NeXT variants is not well-established.

## Next Checks

1. **Error Identification Reliability Test**: Conduct a human evaluation study where multiple annotators assess the accuracy of GPT-4 error identification against ground truth, measuring both precision and recall of error type classification across the vision error codebook.

2. **Cross-Dataset Generalization**: Apply nSFT to a held-out dataset with substantially different characteristics (e.g., medical imaging QA or scientific diagram interpretation) to test whether the negative supervision approach generalizes beyond the tested domains.

3. **Long-form Generation Analysis**: Evaluate the models on extended conversational tasks or multi-step reasoning problems to assess whether the preference alignment benefits observed in short-answer tasks translate to more complex generation scenarios.