---
ver: rpa2
title: Executable Code Actions Elicit Better LLM Agents
arxiv_id: '2402.01030'
source_url: https://arxiv.org/abs/2402.01030
tags:
- action
- code
- arxiv
- codeact
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CodeAct, a framework that uses executable
  Python code as actions for large language model (LLM) agents, replacing traditional
  text or JSON-based actions. CodeAct allows LLMs to leverage Python's control and
  data flow features, dynamically adjust actions based on new observations, and access
  existing software packages.
---

# Executable Code Actions Elicit Better LLM Agents

## Quick Facts
- arXiv ID: 2402.01030
- Source URL: https://arxiv.org/abs/2402.01030
- Reference count: 40
- This paper introduces CodeAct, a framework using executable Python code as actions for LLM agents, achieving up to 20% improvement in success rates on complex tasks.

## Executive Summary
This paper introduces CodeAct, a framework that uses executable Python code as actions for LLM agents instead of traditional text or JSON-based actions. By leveraging Python's control and data flow features, CodeAct enables LLMs to dynamically adjust actions based on new observations and access existing software packages. Experiments with 17 LLMs show CodeAct improves success rates by up to 20% on complex tasks requiring multiple tool interactions while reducing the number of interactions needed.

## Method Summary
CodeAct integrates Python code execution as the primary action format for LLM agents. The framework uses a sandbox environment to execute generated code actions, with the LLM receiving execution results or error messages to inform subsequent actions. The authors develop CodeActInstruct, a dataset of 7k multi-turn interaction trajectories generated using GPT-4 and GPT-3.5-turbo, and fine-tune open-source models (Llama2 and Mistral) to create CodeActAgent. The framework is evaluated against traditional text and JSON action formats on API-Bank and M3ToolEval benchmarks.

## Key Results
- CodeAct achieves up to 20% absolute improvement over baselines on success rate for complex tasks
- CodeAct requires up to 30% fewer actions compared to text and JSON baselines
- CodeActAgent shows strong performance on code-based tasks while maintaining general capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CodeAct improves performance because LLMs are already familiar with Python code from pre-training.
- Mechanism: LLMs that have seen large amounts of Python code during pre-training can naturally generate executable code actions, reducing the need for specialized fine-tuning.
- Core assumption: The pre-training corpus contains substantial Python code covering the tools and patterns needed for CodeAct tasks.
- Evidence anchors:
  - [abstract]: "Code data is widely used in pre-training today's LLMs [58]. These models are already familiar with structured programming languages, allowing cost-effective adoption of CodeAct."
  - [section 2.2]: "CodeAct achieves comparable or better performance than the baselines... open-source LLMs, CodeAct's improvements are more prominent... code data is usually more accessible for fine-tuning open-source LLMs than the specialized JSON or text tool-calling format."
- Break condition: If the model's pre-training corpus lacks relevant Python patterns or tool usage examples, the advantage disappears.

### Mechanism 2
- Claim: Code's control and data flow features let CodeAct handle complex tasks more efficiently than text or JSON.
- Mechanism: Code can naturally express loops, conditionals, and variable reuse, so a single code action can replace multiple tool calls, reducing interaction turns.
- Core assumption: The task requires repeated tool usage or conditional logic that can be expressed in code.
- Evidence anchors:
  - [abstract]: "Code inherently supports control and data flow, allowing for the storage of intermediate results as variables for reuse and the composition of multiple tools... thereby unlocking LLMs' potential to tackle complex tasks."
  - [section 2.3]: "CodeAct achieves up to a 20% absolute improvement over baselines on the success rate... while requiring up to 30% fewer actions."
- Break condition: If tasks are simple and require only single tool calls, the control and data flow advantage is irrelevant.

### Mechanism 3
- Claim: Multi-turn interaction with error feedback allows CodeAct to self-debug and improve actions over time.
- Mechanism: Python's error messages provide actionable feedback that LLMs can use to revise code actions in subsequent turns.
- Core assumption: The Python environment and tools return clear, actionable error messages in natural language.
- Evidence anchors:
  - [abstract]: "CodeAct can execute code actions and dynamically revise prior actions or emit new action based on observations it receives through multiple turns of interactions."
  - [section 2.4]: "Error messages from the environment further enable it to rectify errors autonomously through self-debugging in multi-turn interaction."
- Break condition: If error messages are cryptic or absent, the self-debugging loop fails.

## Foundational Learning

- Concept: Python code generation from natural language instructions.
  - Why needed here: CodeAct relies on the LLM generating correct Python code actions from task descriptions.
  - Quick check question: Can the LLM generate syntactically correct Python code for a simple task like "print the sum of two numbers"?

- Concept: Multi-turn agent interaction and state management.
  - Why needed here: CodeActAgent must maintain context across turns and update actions based on execution feedback.
  - Quick check question: Given a failed code execution with an error message, can the agent propose a corrected code block in the next turn?

- Concept: Instruction tuning with specialized agent datasets.
  - Why needed here: CodeActInstruct is used to fine-tune models for agent tasks; understanding dataset curation and filtering is essential.
  - Quick check question: What is the purpose of filtering trajectories that show self-improvement from errors?

## Architecture Onboarding

- Component map: LLM model -> Python interpreter (sandbox) -> Tool APIs -> Feedback loop -> Next action
- Critical path:
  1. Receive task instruction
  2. LLM generates Python code action
  3. Execute code in sandbox
  4. Return execution result or error
  5. LLM revises or emits new action
  6. Repeat until solution or max turns
- Design tradeoffs:
  - Security vs. expressiveness: sandboxing limits what code can do but keeps execution safe
  - Prompt complexity vs. generalization: detailed prompts help but may reduce zero-shot ability
  - Dataset size vs. quality: more data helps but noisy examples hurt self-debugging capability
- Failure signatures:
  - Code syntax errors that the LLM cannot fix (pre-training gap)
  - Error messages that are too cryptic for meaningful self-correction
  - Tool APIs not aligned with the model's code generation patterns
- First 3 experiments:
  1. Test zero-shot code generation on a simple tool-calling task (e.g., web search).
  2. Measure performance on a multi-turn task requiring loops/conditionals (e.g., iterating over a list).
  3. Evaluate self-debugging on a task that initially fails with a syntax error but succeeds after one correction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CodeAct's advantage over text and JSON actions stem primarily from the models' pre-training on code data or from the inherent advantages of Python's control and data flow features?
- Basis in paper: [inferred] The paper shows CodeAct performs comparably or better on atomic tool use (ablation of control/data flow advantages) but gains more on complex tasks requiring multiple tool interactions.
- Why unresolved: The paper doesn't isolate these factors experimentally. It's unclear whether models' familiarity with code format or Python's features drive the improvements.
- What evidence would resolve it: A controlled experiment comparing CodeAct against a custom action format that mimics Python's control/data flow but isn't actual code.

### Open Question 2
- Question: What is the optimal balance between CodeActInstruct data and general conversation data for training CodeActAgent?
- Basis in paper: [explicit] The ablation study shows both contribute to agent tasks while general conversations are essential for maintaining general capabilities.
- Why unresolved: The paper uses a fixed mixture ratio without exploring alternatives. Different task domains might benefit from different ratios.
- What evidence would resolve it: Systematic experiments varying the ratio of CodeActInstruct to general conversation data and measuring performance on both agent and general tasks.

### Open Question 3
- Question: Why does CodeActAgent with Llama2 backbone show no improvement on M3ToolEval while the Mistral variant improves significantly?
- Basis in paper: [explicit] The paper notes this anomaly and hypothesizes it could be due to training artifacts or weaker fundamental capabilities in Llama2.
- Why unresolved: The paper doesn't test these hypotheses or explore alternative explanations.
- What evidence would resolve it: Comparing the pre-training corpora, testing other Llama2 variants, or conducting capability assessments on fundamental reasoning skills.

## Limitations
- Effectiveness relies on LLMs' pre-training on Python code, but pre-training corpora are not characterized.
- Self-debugging capability depends on the quality of Python error messages, which may not always be actionable.
- Dataset curation involves filtering for self-improvement, but specific filtering criteria are not disclosed.

## Confidence

- High confidence: The core observation that executable code actions can reduce the number of tool interactions needed for complex tasks, supported by quantitative results on M3ToolEval.
- Medium confidence: The claim that pre-training familiarity with Python code drives CodeAct's effectiveness, as this mechanism is plausible but not directly tested.
- Medium confidence: The self-debugging capability, as the paper shows CodeActAgent can correct errors but doesn't systematically evaluate performance across different error types or message qualities.

## Next Checks

1. **Pre-training analysis**: Characterize the Python code content in the pre-training corpora of evaluated models (Llama2, Mistral, GPT-4, etc.) to verify the mechanism that pre-training familiarity enables CodeAct's effectiveness.

2. **Error message robustness**: Systematically evaluate CodeActAgent's self-debugging performance across different types of Python errors, including cryptic errors, to assess the robustness of the error-based correction mechanism.

3. **Distribution shift analysis**: Compare the quality and diversity of CodeActInstruct trajectories generated by GPT-4/GPT-3.5-turbo with those that could be generated by the fine-tuned CodeActAgent, to understand potential training-serving skew.