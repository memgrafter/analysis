---
ver: rpa2
title: Enhancing Knowledge Retrieval with Topic Modeling for Knowledge-Grounded Dialogue
arxiv_id: '2405.04713'
source_url: https://arxiv.org/abs/2405.04713
tags:
- knowledge
- dialogue
- topic
- retrieval
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of knowledge retrieval in knowledge-grounded
  dialogue systems. The authors propose a method that utilizes topic modeling on the
  knowledge base to improve retrieval accuracy and, as a result, improve response
  generation.
---

# Enhancing Knowledge Retrieval with Topic Modeling for Knowledge-Grounded Dialogue

## Quick Facts
- arXiv ID: 2405.04713
- Source URL: https://arxiv.org/abs/2405.04713
- Authors: Nhat Tran; Diane Litman
- Reference count: 10
- Primary result: Topic modeling-based clustering improves retrieval accuracy and response generation quality in knowledge-grounded dialogue systems.

## Executive Summary
This paper addresses the challenge of knowledge retrieval in knowledge-grounded dialogue systems by proposing a method that utilizes topic modeling on the knowledge base to improve retrieval accuracy. The authors use contextual topic modeling to cluster the knowledge base and train a separate encoder for each cluster, incorporating the topic distribution of the input query into the similarity score to find the top-K passages. They also experiment with using ChatGPT as the response generator, with and without the retrieved knowledge. Experimental results on two datasets show that their approach can increase retrieval and generation performance, and that ChatGPT is a better response generator when relevant knowledge is provided.

## Method Summary
The authors propose using contextual topic modeling (CTM) to partition the knowledge base into T topical clusters. Each cluster gets its own document encoder (BERT_i^d) while using a shared query encoder (BERT_q). During retrieval, the query's topic distribution from CTM weights the similarity scores from each cluster-specific encoder, and the top-K passages are retrieved based on this weighted sum. The retrieved passages are then used to condition a response generator (either BART or ChatGPT) to produce knowledge-grounded responses. The approach is evaluated on MultiDoc2Dial and KILT-dialogue datasets, measuring retrieval performance with P@1 and generation quality with F1 and KILT-F1 metrics.

## Key Results
- Topic modeling-based clustering improves retrieval accuracy compared to baseline RAG models
- ChatGPT generates better responses than fine-tuned BART when provided with relevant retrieved knowledge
- Optimal number of clusters T depends on validation set performance rather than topic coherence scores alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering knowledge base into topical clusters improves retrieval precision by reducing semantic noise during nearest-neighbor search.
- Mechanism: The knowledge base is partitioned into T clusters via contextual topic modeling. Each cluster gets its own document encoder. During retrieval, the query's topic distribution weights the contribution of each cluster-specific encoder's similarity score.
- Core assumption: Passages within the same topic cluster share sufficient semantic similarity that a single encoder can represent them effectively.
- Evidence anchors:
  - [abstract] "we use topic modeling to cluster the knowledge base and train a separate encoder for each cluster"
  - [section] "Due to impressive performance across various natural language processing (NLP) tasks of large language models (LLMs) such as ChatGPT, we also experiment with using ChatGPT as the response generator"
  - [corpus] Weak: no direct citations for cluster-based encoding, but related to general dense retrieval literature
- Break condition: If topic clusters are too heterogeneous or too fine-grained, a single encoder per cluster cannot generalize, leading to degraded retrieval.

### Mechanism 2
- Claim: Incorporating topic distribution weights into the similarity score improves relevance ranking by modulating the influence of each cluster based on the query's topical alignment.
- Mechanism: The dot product between query and document encodings is multiplied by the query's topic probability for that cluster.
- Core assumption: The topic distribution from CTM accurately reflects the topical relevance of the query to each cluster.
- Evidence anchors:
  - [abstract] "we then incorporate the topic distribution of the input query into the similarity score to find the top-K passages"
  - [section] "Given the topic distribution of the dialogue history H calculated using CTM as w = ( w1, w2, ..., wT ), to find the top-K passages, we first retrieve the top-K passages from each cluster ti"
  - [corpus] Moderate: consistent with weighted fusion methods in multi-vector retrieval
- Break condition: If the CTM topic distribution is inaccurate or poorly calibrated, the weighted similarity scores may mislead the ranking.

### Mechanism 3
- Claim: ChatGPT generates better responses when provided with relevant retrieved knowledge than when it must rely on its internal knowledge alone.
- Mechanism: By conditioning ChatGPT's generation on retrieved passages, the model can ground its output in specific factual content rather than relying on potentially outdated or incomplete parametric knowledge.
- Core assumption: ChatGPT's internal knowledge is incomplete or stale for certain knowledge-grounded dialogue domains.
- Evidence anchors:
  - [abstract] "The results also indicate that ChatGPT is a better response generator for knowledge-grounded dialogue when relevant knowledge is provided"
  - [section] "Although ChatGPT (OpenAI, 2022) has shown great performance in various NLP tasks (Laskar et al., 2023), recent works in knowledge-grounded dialogue (Li et al., 2022; Zhao et al., 2022; Wu et al., 2022; Gowriraj et al., 2023) have not utilized it as a response generator"
  - [corpus] Moderate: aligns with prior findings that retrieval-augmented generation improves factuality
- Break condition: If retrieved passages are irrelevant or noisy, conditioning on them may degrade response quality.

## Foundational Learning

- Concept: Dense passage retrieval and bi-encoder architectures
  - Why needed here: The retriever relies on dual BERT encoders (query and document) to map text into a shared vector space for similarity computation.
  - Quick check question: What is the role of the FAISS index in DPR, and how does it enable fast retrieval?

- Concept: Topic modeling and topic coherence
  - Why needed here: CTM is used to partition the knowledge base into coherent topical clusters, and topic coherence scores help select the optimal number of clusters.
  - Quick check question: How does the CTM's topic distribution vector w influence the retrieval scoring in the proposed method?

- Concept: Retrieval-augmented generation (RAG) framework
  - Why needed here: The paper modifies both the retriever and generator components of RAG to improve knowledge-grounded dialogue.
  - Quick check question: In RAG, what is the difference between the non-parametric retriever and the parametric generator during training vs inference?

## Architecture Onboarding

- Component map:
  Knowledge base -> CTM clustering -> Cluster-specific document encoders -> FAISS indices -> Weighted similarity scoring -> Top-K retrieval -> Generator (BART/ChatGPT)

- Critical path:
  1. Preprocess KB → train CTM → cluster KB
  2. Initialize DPR encoders → fine-tune per cluster
  3. Index each cluster in FAISS
  4. At inference: encode query → get topic dist → retrieve top-K per cluster → rank by weighted dot product → return top-K
  5. Feed retrieved passages + dialogue history to generator (BART or ChatGPT)

- Design tradeoffs:
  - Number of clusters T vs retrieval accuracy vs computational cost
  - Shared vs separate query encoder (this work uses shared)
  - Weighted sum vs max pooling across cluster scores
  - Using ChatGPT vs fine-tuning BART for generation

- Failure signatures:
  - Retrieval performance drops sharply if clusters are too broad or too narrow
  - Generation quality degrades if retrieved passages are irrelevant
  - Training instability if fine-tuning separate encoders per cluster with limited data

- First 3 experiments:
  1. Vary T from 1 to 10; plot topic coherence vs R@5 on validation set; pick optimal T
  2. Compare RAG-topic vs baseline RAG on retrieval (P@1) and generation (F1, KILT-F1)
  3. Test ChatGPT with vs without retrieved knowledge; compare F1 and KILT-F1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of topic modeling-based retrieval scale with the size of the knowledge base?
- Basis in paper: [inferred] The paper mentions that computational requirements increase proportionally with the number of topics, as each topic requires a separate document encoder. It also states that this method does not scale well if the optimal number of topics is large.
- Why unresolved: The paper does not provide experiments or analysis on how retrieval performance changes as the knowledge base size increases, particularly when the optimal number of topics is large.
- What evidence would resolve it: Experiments comparing retrieval performance on knowledge bases of varying sizes, with different numbers of topics, would provide insights into the scalability of the approach.

### Open Question 2
- Question: How does the quality of topics generated by the contextual topic model (CTM) impact retrieval and generation performance?
- Basis in paper: [explicit] The paper reports topic coherence scores for different numbers of topics but notes that higher topic coherence scores do not necessarily lead to higher retrieval results. It suggests using validation set performance to choose the optimal number of topics.
- Why unresolved: The paper does not explore the relationship between topic quality (as measured by coherence) and downstream task performance, nor does it investigate ways to improve topic quality beyond the default CTM parameters.
- What evidence would resolve it: Experiments varying CTM parameters or using alternative topic modeling approaches, while measuring their impact on retrieval and generation performance, would clarify the importance of topic quality.

### Open Question 3
- Question: How do different large language models (LLMs) compare in their ability to generate knowledge-grounded responses when provided with relevant knowledge?
- Basis in paper: [explicit] The paper experiments with ChatGPT as a response generator and finds that it outperforms retrieval-based models when given relevant knowledge. However, it notes the lack of diversity in open-source LLMs and relies solely on automatic metrics for evaluation.
- Why unresolved: The paper only tests one LLM (ChatGPT) and does not compare its performance to other LLMs or conduct human evaluations to assess the quality of generated responses.
- What evidence would resolve it: Experiments comparing multiple LLMs (including open-source models) on knowledge-grounded dialogue tasks, using both automatic and human evaluations, would provide a more comprehensive understanding of LLM performance in this setting.

## Limitations
- The approach's computational cost scales linearly with the number of clusters, limiting scalability for large knowledge bases requiring many topics
- Performance depends heavily on CTM's ability to produce coherent, semantically meaningful clusters; poor clustering leads to degraded retrieval
- Only one LLM (ChatGPT) was tested for generation, with no human evaluation of response quality

## Confidence

| Claim | Confidence |
|-------|------------|
| Clustering knowledge base improves retrieval precision | Medium-High |
| Weighted similarity scoring based on topic distribution improves ranking | Medium-High |
| ChatGPT generates better responses than BART when provided with relevant knowledge | Medium |

## Next Checks

1. **Cluster Coherence Validation**: Verify that the CTM produces topically coherent clusters by computing topic coherence metrics (e.g., UMass coherence) and examining cluster purity through manual inspection of representative passages from each cluster.

2. **Encoder Specialization Analysis**: Compare the performance of cluster-specific encoders against a single shared encoder to quantify the benefit of specialization, and analyze whether the separate encoders actually learn distinct semantic representations or simply memorize cluster-specific artifacts.

3. **Retrieval Ablation Study**: Conduct an ablation study removing the topic distribution weighting from the similarity score to isolate the contribution of the weighted fusion mechanism versus the baseline DPR retrieval, and test performance across different cluster counts to identify the optimal trade-off.