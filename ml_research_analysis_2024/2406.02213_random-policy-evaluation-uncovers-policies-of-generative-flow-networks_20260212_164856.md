---
ver: rpa2
title: Random Policy Evaluation Uncovers Policies of Generative Flow Networks
arxiv_id: '2406.02213'
source_url: https://arxiv.org/abs/2406.02213
tags:
- policy
- gflownets
- flow
- evaluation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a theoretical connection between GFlowNets
  and standard reinforcement learning through policy evaluation. The authors show
  that evaluating a uniform policy with appropriately transformed rewards yields state
  value functions that are scaled versions of GFlowNets' flow functions under specific
  structural conditions.
---

# Random Policy Evaluation Uncovers Policies of Generative Flow Networks

## Quick Facts
- arXiv ID: 2406.02213
- Source URL: https://arxiv.org/abs/2406.02213
- Reference count: 40
- Key outcome: RPE achieves competitive performance to GFlowNets through simplified random policy evaluation with transformed rewards

## Executive Summary
This paper establishes a theoretical connection between GFlowNets and standard reinforcement learning through policy evaluation. The authors show that evaluating a uniform policy with appropriately transformed rewards yields state value functions that are scaled versions of GFlowNets' flow functions under specific structural conditions. Based on this insight, they propose Rectified Policy Evaluation (RPE), a simplified method that achieves the same reward-matching capability as GFlowNets by evaluating a fixed random policy. Experiments on TF Bind generation, RNA sequence design, and molecule generation tasks demonstrate that RPE achieves competitive or superior performance compared to existing GFlowNets and maximum entropy RL approaches in terms of accuracy and mode discovery.

## Method Summary
RPE reformulates GFlowNets training into a random policy evaluation process with rectification. The method learns flow functions by evaluating a fixed uniform policy with transformed rewards, then derives the sampling policy PF(s'|s) = Fθ(s')PB(s'|s)/Fθ(s). The algorithm consists of a flow function approximator (MLP with 2 hidden layers of 2048 units), a fixed uniform policy for evaluation, and a transformation function g(s) that scales rewards based on action branching ratios. Training proceeds by sampling trajectories, calculating g(s) values, evaluating the uniform policy with transformed rewards, and updating flow parameters via MSE loss.

## Key Results
- RPE achieves competitive accuracy and mode discovery performance compared to GFlowNets and MaxEnt RL baselines
- The method successfully generates TF Bind sequences, RNA structures, and molecular compounds
- Performance improvements are particularly notable in mode discovery metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evaluating a uniform random policy with transformed rewards yields state values proportional to GFlowNets' flow functions under specific structural conditions
- Mechanism: The paper shows that when rewards are scaled by the product of available actions along each trajectory path, policy evaluation for a uniform policy produces values that match GFlowNets' flow functions through a relationship V(st) = F(st) * g(τ, st)
- Core assumption: The path-invariance condition must hold where any trajectories τ1 and τ2 visiting state st satisfy g(τ1, st) = g(τ2, st)
- Evidence anchors: [abstract] "Surprisingly, we find that the value function obtained from evaluating a uniform policy is closely associated with the flow functions in GFlowNets through the lens of flow iteration under certain structural conditions"

### Mechanism 2
- Claim: RPE achieves reward-matching capability through simplified policy evaluation instead of GFlowNets' flow-matching objectives
- Mechanism: By reformulating GFlowNets' training into a random policy evaluation process with rectification, RPE learns flow functions by evaluating a fixed uniform policy with transformed rewards, then derives the sampling policy PF(s'|s) = Fθ(s')PB(s'|s)/Fθ(s)
- Core assumption: The transformed reward structure correctly accounts for the branching structure of the decision process
- Evidence anchors: [abstract] "Building upon these insights, we introduce a rectified random policy evaluation (RPE) algorithm, which achieves the same reward-matching effect as GFlowNets based on simply evaluating a fixed random policy in these cases"

### Mechanism 3
- Claim: The connection between GFlowNets and policy evaluation provides theoretical understanding that enables simpler implementation
- Mechanism: By viewing flow iteration as a form of policy evaluation, the paper establishes that sophisticated GFlowNets objectives can be reinterpreted as specific forms of value estimation in RL with transformed reward structures
- Core assumption: The equivalence between flow iteration and policy evaluation holds under the specified structural conditions
- Evidence anchors: [abstract] "This work establishes a link between GFlowNets and standard (non-MaxEnt) RL through one of its most basic building blocks: policy evaluation"

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and policy evaluation
  - Why needed here: The paper's theoretical connection relies on understanding how policy evaluation computes state values in MDPs through iterative updates
  - Quick check question: In policy evaluation, how is the value of a state updated based on its successor states and the policy's action probabilities?

- Concept: Directed Acyclic Graphs (DAGs) and tree structures
  - Why needed here: GFlowNets operate on DAGs where objects are constructed through sequential decisions, and the paper's theorems distinguish between tree and non-tree cases
  - Quick check question: What's the key difference between a tree-structured DAG and a general DAG in terms of state transitions and path uniqueness?

- Concept: Flow consistency and flow matching
  - Why needed here: GFlowNets require flow consistency where incoming flow equals outgoing flow for all internal states, which is central to understanding the flow-value connection
  - Quick check question: What does it mean for a flow to be "consistent" in GFlowNets, and why is this property important for reward-matching?

## Architecture Onboarding

- Component map: Trajectory sampling -> Calculate g(s) for states -> Evaluate uniform policy with transformed rewards -> Update flow parameters via MSE loss -> Derive sampling policy
- Critical path: Sample trajectory → Calculate g(s) for states → Evaluate uniform policy with transformed rewards → Update flow parameters via MSE loss between g(s)Fθ(s) and V(s) → Derive sampling policy
- Design tradeoffs: RPE trades off the expressiveness of GFlowNets' flow-matching objectives for simplicity of fixed policy evaluation; it works well when path-invariance holds but may underperform when complex backward policies are needed
- Failure signatures: Poor accuracy indicates path-invariance assumption violation; slow convergence suggests inadequate reward transformation; mode discovery issues point to insufficient exploration
- First 3 experiments:
  1. Implement RPE on tree-structured TF Bind task with simple left-to-right generation to verify equivalence
  2. Test RPE on set generation task to validate path-invariance condition in non-tree DAGs
  3. Compare RPE performance against GFlowNets baselines on RNA sequence generation with varying reward functions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the path-invariance condition be relaxed or automatically detected in general DAGs to extend RPE's applicability beyond the current structural constraints?
- Basis in paper: [explicit] The paper explicitly states that Theorem 4.2 requires path-invariance where g(τ1, st) = g(τ2, st) for any trajectories reaching the same state, and notes this doesn't hold for all DAGs like HyperGrid.
- Why unresolved: The authors acknowledge this as a limitation and suggest it as a future direction, but don't provide concrete methods for relaxing or detecting this condition.

### Open Question 2
- Question: How does RPE perform in continuous or high-dimensional state spaces where the tabular representation used in the set generation task is not feasible?
- Basis in paper: [inferred] The experiments focus on discrete, relatively small state spaces (TF Bind, RNA, molecules), and the set generation task uses tabular representation. The paper doesn't explore continuous domains.
- Why unresolved: The paper demonstrates RPE's effectiveness in discrete combinatorial spaces but doesn't address scalability to continuous or high-dimensional problems.

### Open Question 3
- Question: What is the computational complexity of RPE compared to GFlowNets, particularly in terms of the forward pass computation for sampling policy PF(s'|s)?
- Basis in paper: [explicit] The paper mentions in the discussion that "the current implementation computes PF(s'|s) through separate forward passes for each child state s'," suggesting potential computational inefficiency.
- Why unresolved: While the paper notes this as an area for optimization, it doesn't provide a detailed complexity analysis or compare computational requirements with existing GFlowNets methods.

## Limitations
- The path-invariance condition required for RPE may not hold in complex environments with boundary effects or asymmetric transition structures
- Performance in continuous or high-dimensional state spaces remains unexplored
- Computational efficiency of sampling policy computation needs optimization

## Confidence

- **High confidence**: The core theoretical insight that uniform policy evaluation with transformed rewards yields proportional flow functions under path-invariance conditions. This is supported by rigorous mathematical proofs in Theorem 4.2 and Corollary 4.1.
- **Medium confidence**: The practical effectiveness of RPE across diverse tasks. While experiments show competitive performance, the results are based on a limited set of synthetic and bioinformatics tasks.
- **Medium confidence**: The claim that RPE achieves the same reward-matching capability as GFlowNets. This holds under the specified structural conditions but may break down in environments requiring complex backward policies.

## Next Checks

1. **Boundary Effect Analysis**: Systematically test RPE on environments with explicit boundary conditions (like the HyperGrid example) to quantify performance degradation when path-invariance fails, and measure how often this occurs in practical applications.

2. **Architecture Ablation Study**: Conduct controlled experiments varying the flow function architecture (depth, width, activation functions) and policy evaluation parameters to identify the minimal requirements for maintaining performance equivalence with GFlowNets.

3. **Real-World Application Testing**: Apply RPE to more complex, real-world generative modeling tasks beyond the current bioinformatics focus, particularly in domains with high-dimensional, unstructured state spaces to stress-test the path-invariance assumption.