---
ver: rpa2
title: 'KerasCV and KerasNLP: Vision and Language Power-Ups'
arxiv_id: '2405.20247'
source_url: https://arxiv.org/abs/2405.20247
tags:
- keras
- kerascv
- kerasnlp
- data
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KerasCV and KerasNLP are domain-specific extensions of the Keras
  API for computer vision and natural language processing workflows. They provide
  pretrained task models, foundational components, and backbones for popular architectures
  like Stable Diffusion, YOLOv8, GPT2, BERT, Mistral, CLIP, Gemma, and T5.
---

# KerasCV and KerasNLP: Vision and Language Power-Ups

## Quick Facts
- arXiv ID: 2405.20247
- Source URL: https://arxiv.org/abs/2405.20247
- Reference count: 8
- Key outcome: KerasCV and KerasNLP are domain-specific extensions of the Keras API for computer vision and natural language processing workflows

## Executive Summary
KerasCV and KerasNLP are domain-specific extensions of the Keras API designed to enable fast experimentation in computer vision and natural language processing. Built on top of Keras 3, these libraries provide pretrained task models, foundational components, and backbones for popular architectures while supporting JAX, TensorFlow, and PyTorch backends. The libraries adopt a modular, layered design approach that separates foundational components, pretrained backbones, and task models, enabling users to mix and match components for custom workflows while leveraging pretrained models for quick prototyping.

## Method Summary
The paper evaluates KerasCV and KerasNLP performance on various models including SAM, Gemma, BERT, and Mistral using different backends (TensorFlow, JAX, PyTorch). Benchmarks were conducted on a single NVIDIA A100 GPU with 40GB memory using Google Cloud Compute Engine. The evaluation measured training and inference time per step (ms/step) for each model-backend combination, comparing Keras 3 performance against Keras 2. The methodology involved downloading pretrained models from Kaggle Models and running consistent benchmarks across different batch sizes where possible.

## Key Results
- Keras 3 with optimal backend selection consistently outperforms Keras 2 across all evaluated models
- All models support XLA compilation for accelerated training via TensorFlow operations
- Pretrained task models can be used with zero-configuration fine-tuning on raw inputs
- All pretrained models are published on Kaggle Models and accessible in internet-off mode

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KerasCV and KerasNLP provide fast experimentation through modular, layered API design
- Mechanism: The layered approach separates foundational components, pretrained backbones, and task models, enabling users to mix and match components for custom workflows while leveraging pretrained models for quick prototyping
- Core assumption: Users need both flexibility to build custom models and convenience of pretrained components for rapid experimentation
- Evidence anchors:
  - [abstract]: "These domain packages are designed to enable fast experimentation, with a focus on ease-of-use and performance. We adopt a modular, layered design..."
  - [section]: "We adopt a layered approach to API design. Our library has three primary levels of abstraction: Foundational Components, Pretrained Backbones, and Task Models."
- Break condition: If the modular design introduces significant overhead or complexity that outweighs the benefits of flexibility

### Mechanism 2
- Claim: KerasCV and KerasNLP deliver state-of-the-art training and inference performance through backend flexibility and XLA compilation
- Mechanism: Users can select the optimal backend (JAX, TensorFlow, or PyTorch) for their specific model, and all models support XLA compilation for accelerated training
- Core assumption: Different backends have varying performance characteristics for different models, and XLA compilation provides significant speedups when applicable
- Evidence anchors:
  - [abstract]: "Because these domain packages are written on top of Keras 3, all of their modeling components natively support JAX, TensorFlow, and PyTorch..."
  - [section]: "To enable efficient training, we support XLA compilation for all models, and run all preprocessing via a compiled graph of TensorFlow operations using the tf.data API."
  - [table]: "Keras 3 offers flexibility by letting users select the fastest framework for their task. Picking the fastest backend for a given model consistently outperforms Keras 2..."
- Break condition: If XLA compilation introduces prohibitive restrictions on model architecture or if backend switching overhead negates performance gains

### Mechanism 3
- Claim: KerasCV and KerasNLP lower the barrier to using large pretrained models by providing easy access to pretrained weights and zero-configuration fine-tuning
- Mechanism: Task models combine preprocessing, pretrained weights, and fine-tuning capabilities into a unified interface that can operate directly on raw inputs
- Core assumption: Researchers and practitioners need access to large pretrained models but lack resources to pretrain them, and require simple fine-tuning interfaces
- Evidence anchors:
  - [abstract]: "At the library's highest level of abstraction, we provide pretrained 'task' models for popular architectures... Task models have built-in preprocessing, pretrained weights, and can be fine-tuned on raw inputs."
  - [section]: "Pretrained task models can be used by using presets trained on different datasets... To use a pretrained task model with a preset, one simply needs to import the keras_cv.models or keras_nlp.models module and then call the from_preset() method..."
- Break condition: If the zero-configuration approach proves insufficient for complex fine-tuning scenarios or if pretrained weights become outdated

## Foundational Learning

- Concept: Layered API design
  - Why needed here: Enables progressive disclosure of complexity, allowing users to start with high-level task models and gradually adopt lower-level components as needed
  - Quick check question: What are the three primary levels of abstraction in the KerasCV and KerasNLP API?

- Concept: Backend flexibility and XLA compilation
  - Why needed here: Different models perform optimally on different backends, and XLA compilation provides significant speedups for compatible models
  - Quick check question: Which backends do KerasCV and KerasNLP models natively support?

- Concept: Pretrained models and fine-tuning
  - Why needed here: Pretraining large models is cost-prohibitive for many researchers, but fine-tuning pretrained models enables practical access to state-of-the-art performance
  - Quick check question: Where are all pretrained models of KerasCV and KerasNLP published?

## Architecture Onboarding

- Component map:
  - keras_cv: Computer vision components including preprocessing layers, backbones, and task models
  - keras_nlp: Natural language processing components including tokenizers, backbones, and task models
  - keras: Base Keras 3 API providing the underlying framework support
  - Backend modules: JAX, TensorFlow, and PyTorch implementations

- Critical path: Import domain package → Select task model or build from components → Load pretrained weights (if using task model) → Prepare data → Train or fine-tune → Deploy

- Design tradeoffs:
  - Flexibility vs. simplicity: Modular design offers flexibility but may increase complexity
  - Backend choice vs. consistency: Multiple backend support enables optimal performance but may complicate deployment
  - Zero-configuration vs. customization: Task models offer simplicity but may limit fine-grained control

- Failure signatures:
  - Performance issues: Likely caused by suboptimal backend selection or lack of XLA compatibility
  - Memory errors: May indicate batch size too large for available GPU memory
  - Training instability: Could result from improper data preprocessing or incorrect model configuration

- First 3 experiments:
  1. Load and run inference on a pretrained task model (e.g., BERT for text classification)
  2. Fine-tune a task model on a small custom dataset using default settings
  3. Build a custom model using foundational components and a pretrained backbone for transfer learning

## Open Questions the Paper Calls Out
None

## Limitations

- Performance benchmarks rely on a single hardware configuration (NVIDIA A100 40GB GPU) and may not generalize across different setups
- XLA compilation benefits have "notable restrictions" on model architecture that are not detailed in the paper
- Zero-configuration fine-tuning may prove insufficient for complex domain-specific adaptation scenarios

## Confidence

- **High confidence**: The modular, layered API design approach and its implementation are well-documented and align with established software engineering principles
- **Medium confidence**: Performance improvements over Keras 2 are demonstrated through benchmarks but generalization across hardware configurations requires further validation
- **Low confidence**: Practical utility of zero-configuration fine-tuning for complex real-world scenarios is not empirically validated

## Next Checks

1. **Benchmark generalization**: Replicate the performance benchmarks across diverse hardware configurations (including consumer GPUs and cloud instances) to validate the backend selection optimization claims beyond the A100 40GB environment

2. **XLA compatibility mapping**: Create a comprehensive mapping of which model architectures and operations are compatible with XLA compilation to quantify the practical limitations of this optimization approach

3. **Fine-tuning flexibility assessment**: Conduct a comparative study measuring the performance gap between zero-configuration fine-tuning and custom fine-tuning strategies on domain-specific datasets to evaluate the practical limitations of the task model abstraction