---
ver: rpa2
title: What can LLM tell us about cities?
arxiv_id: '2411.16791'
source_url: https://arxiv.org/abs/2411.16791
tags:
- very
- data
- features
- cities
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explores the capability of large language models (LLMs)
  to provide knowledge about cities on a global scale through two approaches: directly
  querying LLMs for target variable values and extracting features correlated with
  those variables. Experiments across 41 diverse city-related tasks show that LLMs
  embed varying but generally useful knowledge across global cities, with machine
  learning models trained on LLM-derived features consistently improving predictive
  accuracy.'
---

# What can LLM tell us about cities?

## Quick Facts
- arXiv ID: 2411.16791
- Source URL: https://arxiv.org/abs/2411.16791
- Reference count: 40
- Primary result: LLMs embed varying but generally useful knowledge across global cities, with ML models trained on LLM-derived features consistently improving predictive accuracy

## Executive Summary
This study explores the capability of large language models (LLMs) to provide knowledge about cities on a global scale through two approaches: directly querying LLMs for target variable values and extracting features correlated with those variables. Experiments across 41 diverse city-related tasks show that LLMs embed varying but generally useful knowledge across global cities, with machine learning models trained on LLM-derived features consistently improving predictive accuracy. While LLMs demonstrate broad knowledge across continents, they clearly indicate when they lack specific information by generating generic or random outputs. The results suggest LLMs can enable data-driven decision-making for city research, especially for locations with limited or no data availability.

## Method Summary
The study employs two main approaches: Direct-Ask, where LLMs are prompted to directly predict target variables, and Feature-Extraction, where features are extracted from LLM hidden layers or through explicit prompting and then used to train machine learning models. The experiments use 41 diverse datasets covering global cities across 8 domains including environmental, transportation, energy, crime, and industry variables. The LLM-derived features are fed into various ML models (Decision Trees, Random Forests, Gradient Boosting, XGBoost, AdaBoost, Linear Regressor) with 5-fold cross-validation, and performance is evaluated using Root Mean Square Error (RMSE) compared to ground truth values.

## Key Results
- LLMs embed a broad but varying degree of knowledge across global cities, with ML models trained on LLM-derived features consistently leading to improved predictive accuracy
- LLMs clearly indicate when they lack knowledge by generating generic or random outputs for unfamiliar tasks
- Feature-based approach using LLM-extracted features outperforms Direct-Ask approach across multiple datasets and ML models
- English prompts work better for global cities while Chinese works similarly for Chinese cities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs can serve as knowledge bases for city-related tasks even when they don't have direct access to target variables.
- **Mechanism**: LLMs encode implicit and explicit features through their hidden layers and natural language understanding that correlate with target variables like traffic patterns, energy consumption, or crime rates. These features can be extracted and used as inputs to machine learning models to predict target values.
- **Core assumption**: The LLM's training data included sufficient diverse city-related information to create meaningful correlations between its internal representations and real-world city metrics.
- **Evidence anchors**:
  - [abstract]: "LLMs embed a broad but varying degree of knowledge across global cities, with ML models trained on LLM-derived features consistently leading to improved predictive accuracy"
  - [section]: "LLMs naturally do not have access to this specific information. When directly asked to generate a 24-hour curve, an LLM will either acknowledge that it lacks this data or produce random numbers. However, LLMs can provide insights into the functions of these zones within NYC"
  - [corpus]: Weak - No direct evidence in corpus papers about city knowledge extraction specifically
- **Break condition**: The LLM's training data lacks sufficient coverage of the target city or domain, leading to generic/random outputs instead of meaningful features.

### Mechanism 2
- **Claim**: LLMs can detect when they lack knowledge about specific city tasks and indicate this through their outputs.
- **Mechanism**: When LLMs encounter unfamiliar tasks or cities, they generate either generic placeholder values (like consistently outputting "50") or show high variance across multiple queries, signaling uncertainty about their predictions.
- **Core assumption**: The LLM's generation process is sufficiently sensitive to training data coverage that it can distinguish between known and unknown information.
- **Evidence anchors**:
  - [abstract]: "it is evident when they lack knowledge, as they tend to generate generic or random outputs for unfamiliar tasks"
  - [section]: "If the outputs of the LLM regarding the same query exhibit large variance, this suggests that the model may be uncertain about the information and is generating responses based on random guesses rather than a solid knowledge"
  - [corpus]: Weak - No direct evidence in corpus papers about uncertainty detection in LLMs
- **Break condition**: The LLM's generation becomes consistently confident even when incorrect, or the variance detection method fails to capture subtle knowledge gaps.

### Mechanism 3
- **Claim**: Explicit feature extraction through interactive prompting is more effective than implicit feature extraction from hidden layers.
- **Mechanism**: By asking LLMs to identify relevant features and then extract their values for specific cities, the resulting features are more coherent and better correlated with target variables than raw hidden layer representations, leading to improved model performance.
- **Core assumption**: LLMs can effectively reason about which features are relevant to a target variable and provide meaningful descriptions that can be converted to numerical values.
- **Evidence anchors**:
  - [abstract]: "ML models trained on LLM-derived features consistently leading to improved predictive accuracy"
  - [section]: "It is more effective to ask LLM about the features to use. LLM not only serves as feature extraction but also feature identification in this method"
  - [corpus]: Weak - No direct evidence in corpus papers about interactive feature extraction methods
- **Break condition**: The LLM fails to identify relevant features or provides inconsistent feature values across different queries.

## Foundational Learning

- **Concept**: Feature engineering and correlation analysis
  - Why needed here: Understanding how to identify and extract relevant features from LLM outputs that correlate with target variables is crucial for the feature-based prediction approach
  - Quick check question: How would you determine if an LLM-extracted feature is actually useful for predicting a target variable like traffic patterns?

- **Concept**: Machine learning model training and evaluation
  - Why needed here: The extracted LLM features need to be fed into ML models (Decision Trees, Random Forests, etc.) and evaluated using metrics like RMSE to assess their predictive power
  - Quick check question: What does it mean if an LLM-derived feature set reduces RMSE compared to using average values?

- **Concept**: Prompt engineering and interactive prompting
  - Why needed here: The effectiveness of both explicit and implicit feature extraction depends heavily on how prompts are formulated to elicit useful information from the LLM
  - Quick check question: Why might you choose to ask an LLM to identify relevant features before extracting their values, rather than just extracting values directly?

## Architecture Onboarding

- **Component map**: Data preprocessing pipeline → LLM interaction module → Feature extraction (explicit/implicit) → ML model training → Evaluation
- **Critical path**: The feature-based approach requires the most components: data preprocessing, LLM interaction, feature extraction, ML model training, and evaluation. Direct-Ask is simpler but less reliable.
- **Design tradeoffs**:
  - Explicit vs. implicit feature extraction: Explicit is more interpretable but requires careful prompt design; implicit captures hidden patterns but is less transparent
  - Model size vs. performance: Larger models like GPT-4o generally perform better but cost more to query
  - Language selection: English works better for global cities, but local language may improve performance for region-specific tasks
- **Failure signatures**:
  - Consistently generic outputs (like repeated "50" values) indicate the LLM lacks knowledge about the task
  - High variance across multiple queries suggests uncertainty or random guessing
  - Poor ML model performance despite feature extraction indicates weak feature-target correlations
- **First 3 experiments**:
  1. Test Direct-Ask on a simple, well-known dataset (like NYC taxi data) to verify basic LLM functionality
  2. Implement explicit feature extraction on a dataset with clear feature-target relationships (like PNT data) to validate the feature engineering approach
  3. Compare explicit vs. implicit feature extraction on the same dataset to demonstrate the effectiveness of the interactive approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically determine when LLMs lack specific knowledge about cities, beyond the qualitative indicators described in the paper?
- Basis in paper: [explicit] The paper identifies detecting consistent generic values (e.g., repeated "50" outputs) and inconsistent values across generations as indicators of LLM uncertainty, but acknowledges this is difficult to prove without ground truth.
- Why unresolved: The paper only provides observational indicators but doesn't establish a rigorous framework for quantifying LLM knowledge gaps or uncertainty thresholds.
- What evidence would resolve it: A systematic evaluation framework that quantifies LLM confidence through statistical measures of output variance, entropy of generated distributions, or comparison with known ground truth values across multiple trials.

### Open Question 2
- Question: To what extent can LLM-extracted features generalize across different city contexts and cultural regions, particularly when comparing Western and Eastern urban environments?
- Basis in paper: [inferred] The paper shows English prompts work better for global cities while Chinese works similarly for Chinese cities, but doesn't explore how features transfer across cultural contexts or whether LLM knowledge encodes cultural biases.
- Why unresolved: The experiments focus on individual tasks without examining cross-cultural feature transferability or investigating whether LLM embeddings capture culturally-specific urban patterns.
- What evidence would resolve it: Systematic experiments testing feature transferability across culturally distinct city pairs, measuring performance degradation when applying features trained in one cultural context to another.

### Open Question 3
- Question: What are the fundamental limitations of using LLMs for city research compared to traditional data collection methods, particularly for tasks requiring high precision or temporal granularity?
- Basis in paper: [explicit] The paper acknowledges LLMs generate features with "some randomness" and don't output precise values, noting they're better for testing hypotheses than deriving rigorous scientific findings.
- Why unresolved: The paper doesn't establish clear boundaries for LLM applicability or quantify the precision trade-offs compared to traditional methods across different urban research domains.
- What evidence would resolve it: Comparative studies measuring LLM prediction accuracy against ground truth across tasks requiring different precision levels, establishing confidence intervals for LLM-derived estimates versus traditional data collection accuracy.

## Limitations

- Geographic coverage limitations: The study demonstrates LLM capabilities across global cities, but confidence is higher for English-speaking regions and major metropolitan areas
- Knowledge extraction reliability: The mechanisms for detecting knowledge gaps (variance analysis, generic outputs) have not been rigorously validated
- Feature correlation uncertainty: The paper does not establish whether correlations reflect genuine city knowledge or dataset artifacts

## Confidence

**High confidence**: The core finding that LLM-derived features improve predictive accuracy compared to baseline methods (average values) is well-supported by the experimental results across multiple datasets and ML models.

**Medium confidence**: The claim that LLMs embed "varying but generally useful knowledge across global cities" is supported by the broad task coverage, but the degree of usefulness varies significantly by domain and geographic region.

**Low confidence**: The assertion that LLMs can reliably detect their own knowledge gaps through variance analysis and generic output generation requires further validation, as these mechanisms are not systematically tested.

## Next Checks

1. **Cross-lingual validation**: Test the same feature extraction pipeline on datasets from non-English speaking cities to quantify performance degradation and identify language-specific limitations.

2. **Knowledge gap detection benchmark**: Create a controlled dataset where LLM knowledge is known to be incomplete, then systematically evaluate the variance-based detection method against ground truth knowledge gaps.

3. **Feature correlation analysis**: Conduct ablation studies to determine which LLM-derived features contribute most to predictive accuracy, and whether these features show meaningful correlations with actual city metrics or are artifacts of the extraction process.