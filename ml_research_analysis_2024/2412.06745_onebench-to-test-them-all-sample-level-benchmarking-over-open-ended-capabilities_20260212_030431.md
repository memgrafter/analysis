---
ver: rpa2
title: 'ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities'
arxiv_id: '2412.06745'
source_url: https://arxiv.org/abs/2412.06745
tags:
- arxiv
- data
- rankings
- evaluation
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ONEBench introduces a new approach to benchmarking foundation models
  by consolidating evaluation datasets into a dynamic sample pool, enabling custom
  benchmarks for arbitrary capabilities. It addresses the limitations of fixed test
  sets by allowing continuous expansion and reducing dataset bias through sample reuse
  across benchmarks.
---

# ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities

## Quick Facts
- arXiv ID: 2412.06745
- Source URL: https://arxiv.org/abs/2412.06745
- Reference count: 40
- Consolidates evaluation datasets into a dynamic sample pool for custom benchmarks

## Executive Summary
ONEBench introduces a novel approach to foundation model benchmarking that consolidates evaluation datasets into a dynamic sample pool, enabling custom benchmarks for arbitrary capabilities. The framework addresses key limitations of fixed test sets by allowing continuous expansion and reducing dataset bias through sample reuse across benchmarks. Using the Plackett-Luce model from social choice theory, ONEBench aggregates heterogeneous and incomplete model measurements into robust rankings while maintaining sample efficiency, achieving high correlation with ground-truth rankings even with up to 95% missing data.

## Method Summary
ONEBench creates a unified sample pool from multiple evaluation datasets, enriched with metadata describing tested capabilities. Models are evaluated using heterogeneous metrics (binary, numeric, ordinal) at the sample level. The core innovation uses the Plackett-Luce model to convert all measurements to ordinal rankings and aggregate them, handling both heterogeneity (different metric types) and incompleteness (non-overlapping model evaluations). This enables capability-specific querying through semantic and metadata-based retrieval, allowing users to dynamically generate benchmarks tailored to their needs while maintaining ranking accuracy with significantly reduced evaluation costs.

## Key Results
- Achieves 0.88-0.99 Kendall's τ correlation with ground-truth rankings across multiple benchmarks
- Maintains effectiveness with up to 95% missing data, reducing evaluation cost by up to 20x
- Reveals substantial variation in top-10 model rankings across different domains and concepts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Plackett-Luce model enables robust aggregation of heterogeneous and incomplete model measurements by converting all measurements to ordinal rankings.
- **Mechanism:** ONEBench treats each data sample as a "voter" expressing preferences over models, then uses the Plackett-Luce framework to aggregate these ordinal preferences into global model rankings. This handles both heterogeneity (different metric types) and incompleteness (non-overlapping model evaluations).
- **Core assumption:** Model utilities follow a Plackett-Luce random utility model where preferences are based on comparing sampled utilities.
- **Evidence anchors:**
  - [abstract] "The framework tackles heterogeneity (aggregating diverse metrics) and incompleteness (comparing models tested on different data subsets) by employing the Plackett-Luce model from social choice theory"
  - [section 3.1] "Specifically, we use a random-utility model... we focus on the Plackett–Luce model (Plackett, 1975; Luce, 1977), the only known exception that allows for tractable MLE"
  - [corpus] Weak evidence - the corpus contains related work on benchmarking but no direct evidence about Plackett-Luce aggregation performance
- **Break condition:** If the random utility model assumption doesn't hold (e.g., if model utilities don't follow the Plackett-Luce distribution), the aggregation will produce biased rankings.

### Mechanism 2
- **Claim:** Sample-level evaluation with ordinal rankings enables evaluation cost reduction of up to 20x while maintaining ranking accuracy.
- **Mechanism:** By aggregating sample-level rankings rather than requiring all models to be evaluated on all samples, ONEBench can achieve accurate rankings with only 5% of the original measurements, reducing evaluation costs by up to 20 times.
- **Core assumption:** The Plackett-Luce model is sample-efficient and can recover accurate rankings from sparse data.
- **Evidence anchors:**
  - [abstract] "it remains effective with up to 95% missing data, reducing evaluation cost by up to 20x with little-to-no change in model rankings"
  - [section 3.2.3] "our method maintains stable performance even with up to 95% samples missing, demonstrating that it can achieve accurate rankings with up to 20 times fewer data points"
  - [corpus] Moderate evidence - contains related work on compressed subsets but no direct evidence about the 20x reduction claim
- **Break condition:** If the data becomes too sparse (beyond 95% missing), the Plackett-Luce model may fail to converge to accurate rankings.

### Mechanism 3
- **Claim:** Capability-specific querying reveals meaningful variations in model performance that global leaderboards obscure.
- **Mechanism:** ONEBench enables users to perform semantic and metadata-based retrieval to generate custom benchmarks for specific capabilities, revealing that top models vary significantly across different domains and concepts.
- **Core assumption:** Models specialize in different capabilities and their relative performance varies across domains.
- **Evidence anchors:**
  - [abstract] "By combining samples across test sets, ONEBench captures real-world diversity... Users can perform semantic searches and apply structured query filters to dynamically generate benchmarks tailored to their needs"
  - [section 4.3] "Top models vary significantly across queries... Our results reveal substantial variation in the top-k models across domains and concepts"
  - [corpus] Weak evidence - contains related work on dynamic evaluation but no direct evidence about capability-specific ranking variations
- **Break condition:** If model performance were actually consistent across all domains (no specialization), capability-specific querying would offer limited practical value.

## Foundational Learning

- **Concept: Social Choice Theory and Preference Aggregation**
  - Why needed here: ONEBench uses social choice theory to aggregate ordinal preferences from samples into global model rankings, treating samples as voters and models as candidates.
  - Quick check question: How does the Plackett-Luce model differ from simple averaging when aggregating model rankings?

- **Concept: Random Utility Models**
  - Why needed here: The theoretical foundation of ONEBench's aggregation relies on random utility models to justify converting cardinal measurements to ordinal rankings.
  - Quick check question: What assumptions about model utilities are required for the Plackett-Luce model to work correctly?

- **Concept: Information Theory and Data Processing Inequality**
  - Why needed here: Understanding why converting cardinal to ordinal measurements (which loses information) can still be beneficial requires knowledge of the data processing inequality and calibration issues.
  - Quick check question: Under what circumstances can ordinal measurements outperform cardinal measurements despite information loss?

## Architecture Onboarding

- **Component map:** Sample pool -> Metadata enrichment -> Semantic search engine -> Retrieval module -> Plackett-Luce aggregator -> Model rankings
- **Critical path:** User query → sample retrieval (semantic + metadata search) → ordinal ranking aggregation (Plackett-Luce) → ranked model output. The bottleneck is typically sample retrieval and the Plackett-Luce optimization.
- **Design tradeoffs:** Uses ordinal rankings to handle heterogeneity and incompleteness, trading information loss for robustness. Sample-level aggregation enables flexibility but requires efficient retrieval. The Plackett-Luce model provides theoretical guarantees but requires careful implementation.
- **Failure signatures:** Rankings become unstable when data is too sparse (>95% missing), aggregation produces incorrect orderings when random utility assumptions fail, retrieval returns irrelevant samples when semantic search is poor, or the system becomes biased when certain models are over-represented in the data pool.
- **First 3 experiments:**
  1. Implement a minimal Plackett-Luce aggregator with synthetic ordinal data to verify basic functionality and convergence properties.
  2. Create a small prototype with 5-10 models and 50-100 samples to test the full retrieval-aggregation pipeline with both semantic and metadata search.
  3. Benchmark the system's robustness to missing data by systematically removing samples and measuring ranking stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ONEBench's aggregation method perform when benchmark samples have conflicting or contradictory metadata labels (e.g., a sample tagged as both "mathematics" and "literature")?
- Basis in paper: [explicit] The paper discusses metadata-based retrieval and structured querying, but doesn't address conflicts in sample labeling
- Why unresolved: The framework assumes clean, non-conflicting metadata but real-world benchmarks may have inconsistent or overlapping labels
- What evidence would resolve it: Experimental results showing model rankings when samples have conflicting metadata, and how the aggregation algorithm handles such cases

### Open Question 2
- Question: What is the impact of using different embedding models for semantic search across different capability domains (e.g., all-MiniLM-L6-v2 for language vs SigLIP-B16 for vision-language)?
- Basis in paper: [explicit] The paper mentions using different embedding models for different tasks but doesn't analyze the impact of these choices
- Why unresolved: The choice of embedding model could significantly affect retrieval quality and subsequent rankings, but this hasn't been empirically studied
- What evidence would resolve it: Comparative analysis of capability probing results using different embedding models for the same queries

### Open Question 3
- Question: How does the sample efficiency of ONEBench scale with the number of models being evaluated? Does the Ω(|M| log |M|)/k bound hold in practice for large model sets?
- Basis in paper: [explicit] The paper states theoretical sample efficiency bounds but doesn't empirically validate them for large model sets
- Why unresolved: The theoretical guarantees were derived for specific conditions that may not hold for large, diverse model collections
- What evidence would resolve it: Empirical studies measuring Kendall's τ correlation vs. number of samples for increasing numbers of models

## Limitations

- Performance depends heavily on the Plackett-Luce model's assumptions about random utilities, which are not empirically validated across all tested domains
- The method's effectiveness with heterogeneous metrics beyond binary, numeric, and ordinal remains uncertain
- Retrieval mechanism details for capability-specific querying lack specifications about semantic search thresholds and configuration

## Confidence

- **High Confidence:** The Plackett-Luce aggregation method's theoretical foundations and its effectiveness with up to 95% missing data are well-supported by the presented correlation results (0.88-0.99 Kendall's τ).
- **Medium Confidence:** The 20x cost reduction claim is supported but could benefit from more diverse benchmark testing across different domains and evaluation scales.
- **Medium Confidence:** Capability-specific querying reveals meaningful variations, but the practical significance and consistency across different domains need further validation.

## Next Checks

1. Test the Plackett-Luce aggregation method's robustness with additional heterogeneous metric types (e.g., multi-class categorical, interval data) beyond the current binary/numeric/ordinal examples to verify generalization.

2. Conduct systematic ablation studies removing different proportions of samples to identify the exact breaking point where the 95% missing data threshold becomes invalid, and characterize the nature of failures beyond this point.

3. Validate the capability-specific querying system with cross-domain benchmarks (e.g., testing whether top-LLM models identified for medical queries actually perform well on medical tasks) to confirm the practical utility of capability-based model selection.