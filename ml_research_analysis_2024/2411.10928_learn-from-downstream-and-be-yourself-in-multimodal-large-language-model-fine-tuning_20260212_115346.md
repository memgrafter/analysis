---
ver: rpa2
title: Learn from Downstream and Be Yourself in Multimodal Large Language Model Fine-Tuning
arxiv_id: '2411.10928'
source_url: https://arxiv.org/abs/2411.10928
tags:
- fine-tuning
- parameter
- language
- mllm
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in Multimodal Large
  Language Model (MLLM) fine-tuning, where models lose pre-trained generalization
  abilities while adapting to downstream tasks. The authors propose SPIDER, which
  measures parameter importance for both pre-training (using weight magnitude) and
  fine-tuning (using accumulated gradient values), then selectively updates parameters
  based on their relative importance.
---

# Learn from Downstream and Be Yourself in Multimodal Large Language Model Fine-Tuning

## Quick Facts
- arXiv ID: 2411.10928
- Source URL: https://arxiv.org/abs/2411.10928
- Authors: Wenke Huang; Jian Liang; Zekun Shi; Didi Zhu; Guancheng Wan; He Li; Bo Du; Dacheng Tao; Mang Ye
- Reference count: 40
- Primary result: Achieves 66.61 CIDEr on COCO-Caption and 69.68 accuracy on ScienceQA while maintaining pre-training performance

## Executive Summary
This paper addresses catastrophic forgetting in Multimodal Large Language Model (MLLM) fine-tuning, where models lose pre-trained generalization abilities while adapting to downstream tasks. The authors propose SPIDER, which measures parameter importance for both pre-training (using weight magnitude) and fine-tuning (using accumulated gradient values), then selectively updates parameters based on their relative importance. This approach identifies and preserves parameters crucial for generalization while optimizing those important for specialization. Experiments on image captioning and visual question answering tasks using VILA and LLaVA architectures show SPIDER outperforms baselines while maintaining strong performance on pre-training datasets.

## Method Summary
SPIDER implements selective parameter updates by measuring two types of importance: pre-trained weight magnitude (I) for generalization and accumulated fine-tuning gradients (G) for specialization. During fine-tuning, the method computes an importance discrepancy matrix and creates an Importance Selection Mask (ISM) that determines which parameters to update. Parameters where G[v] > I[v] are selected for updates, with the magnitude of updates weighted by G[v]/(G[v]+I[v]). This continuous weighting scheme prevents the update variance issues that occur with simple binary masking. The method requires computing importance metrics for all parameters but only updates a subset, preserving generalization while improving downstream performance.

## Key Results
- Achieves 66.61 CIDEr on COCO-Caption and 69.68 accuracy on ScienceQA
- Outperforms baselines including Full Fine-Tuning, L2-Regularization, DARE, and Tailor
- Maintains strong performance on pre-training datasets while improving downstream task accuracy
- Demonstrates effectiveness across different model sizes and training durations
- Does not require architectural modifications or hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Catastrophic forgetting occurs because fine-tuning on small downstream datasets shifts parameter importance distributions away from those learned during pre-training
- Mechanism: Parameters crucial for generalization develop stable weight magnitudes during pre-training, while parameters important for specialization develop large gradients during fine-tuning. Indiscriminate fine-tuning updates both sets, causing loss of pre-trained generalization capabilities
- Core assumption: Weight magnitude correlates with parameter importance for pre-training tasks, and gradient magnitude correlates with parameter importance for fine-tuning tasks
- Evidence anchors: Uses weight magnitude for pre-trained parameters and accumulated gradient values for fine-tuning, as stated in abstract and section 3.2.1
- Break condition: If weight magnitude does not correlate with generalization importance, or if gradient magnitude does not correlate with specialization importance

### Mechanism 2
- Claim: By selectively updating only parameters more important for specialization than generalization, the model can improve downstream performance while preserving generalization
- Mechanism: Creates importance discrepancy matrix comparing pre-trained weight magnitudes (I) with accumulated fine-tuning gradients (G). Updates parameters where G[v] > I[v] while preserving others using frozen pre-trained weights
- Core assumption: Meaningful difference exists between parameters important for pre-training versus fine-tuning, quantifiable using proposed metrics
- Evidence anchors: Abstract states "selectively updating relatively important parameters for downstream tasks"; section 3.2.1 discusses differentiating between generic and task-specific parameters
- Break condition: If downstream task requires updating parameters also important for generalization, or if importance discrepancy is not sufficiently large

### Mechanism 3
- Claim: Importance Selection Mask (ISM) with weighted aggregation prevents update variance issues that occur with simple binary masking
- Mechanism: Uses continuous weighting M[v] = G[v]/(G[v]+I[v]) for selected parameters instead of binary mask, ensuring parameters with larger importance discrepancies receive larger updates while maintaining relative update scale
- Core assumption: Continuous weighting provides better optimization dynamics than binary selection
- Evidence anchors: Section 3.2.2 discusses how binary masking introduces no variance and results in degradation, proposing ISM to reconstruct aggregation weight
- Break condition: If continuous weighting does not provide meaningful advantages over binary selection, or if it introduces optimization instability

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Core problem is that fine-tuning MLLMs on downstream tasks causes loss of generalization capabilities learned during pre-training
  - Quick check question: What happens to a neural network's performance on previously learned tasks when fine-tuned on new tasks without regularization?

- Concept: Parameter importance measurement using magnitude and gradients
  - Why needed here: Method relies on using pre-trained weight magnitudes to measure generalization importance and fine-tuning gradients to measure specialization importance
  - Quick check question: How can you determine which parameters are most important for a model's current behavior during training?

- Concept: Partial fine-tuning strategies
  - Why needed here: Method selectively updates only a subset of parameters rather than entire model, distinguishing it from standard fine-tuning approaches
  - Quick check question: What are advantages and disadvantages of updating only a subset of parameters during fine-tuning compared to full fine-tuning?

## Architecture Onboarding

- Component map: Visual encoder (frozen) -> Connector module (trainable) -> LLM component (partially trainable) -> Importance Discrepancy Measurement (computes I and G) -> Importance Selection Mask (computes M and applies weighted updates)

- Critical path: During each training step, compute gradients → measure parameter importance → create selection mask → apply weighted updates → evaluate on both pre-training and downstream tasks

- Design tradeoffs:
  - Memory vs accuracy: Computing and storing importance metrics requires additional memory
  - Selective vs full updates: Selective updates preserve generalization but may limit specialization
  - Continuous vs binary masking: Continuous weighting provides smoother optimization but adds complexity

- Failure signatures:
  - If generalization performance drops significantly on pre-training datasets
  - If specialization performance does not improve compared to full fine-tuning
  - If method requires extensive hyperparameter tuning to work

- First 3 experiments:
  1. Compare SPIDER against full fine-tuning on simple downstream task while measuring performance on pre-training dataset
  2. Test different threshold values for parameter selection to find optimal balance
  3. Evaluate method on multiple downstream tasks with varying similarity to pre-training distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does effectiveness of SPIDER scale with number of downstream tasks, particularly when fine-tuning on multiple related tasks sequentially?
- Basis in paper: [explicit] Paper mentions catastrophic forgetting in MLLM fine-tuning but does not explore multi-task scenarios where models are adapted to multiple downstream tasks sequentially
- Why unresolved: Experimental evaluation focuses on single downstream tasks (image captioning and visual QA) without examining performance degradation or adaptation when switching between multiple related tasks
- What evidence would resolve it: Systematic experiments measuring generalization performance across multiple sequential fine-tuning tasks, comparing SPIDER against baselines in multi-task settings

### Open Question 2
- Question: What is impact of varying pre-training dataset size and diversity on SPIDER's effectiveness in mitigating catastrophic forgetting?
- Basis in paper: [inferred] Paper acknowledges MLLM's strong generalization comes from extensive pre-training datasets, but does not investigate how SPIDER's performance varies with different pre-training data characteristics
- Why unresolved: All experiments use pre-trained models with fixed pre-training data, without exploring whether SPIDER's importance ranking mechanism depends on breadth or depth of pre-training data
- What evidence would resolve it: Comparative studies using models pre-trained on datasets of varying sizes and domains, measuring SPIDER's parameter selection accuracy and forgetting mitigation across these conditions

### Open Question 3
- Question: How does SPIDER perform when applied to different connector architectures beyond simple linear projection used in LLaVA and VILA?
- Basis in paper: [explicit] Paper uses connector modules in tested architectures but does not explore how SPIDER's parameter importance ranking adapts to more complex connector designs like cross-attention or multi-layer perceptrons
- Why unresolved: Experimental validation is limited to specific connector implementations, leaving open whether SPIDER's IDM and ISM components generalize to different architectural choices for visual-text fusion
- What evidence would resolve it: Empirical evaluation of SPIDER on MLLM architectures with diverse connector designs, measuring both specialization performance and preservation of pre-trained knowledge across these variants

## Limitations

- Effectiveness relies heavily on assumption that weight magnitude correlates with generalization importance and gradient magnitude correlates with specialization importance, lacking strong empirical validation
- Introduces computational overhead for tracking and comparing importance metrics, which may not be justified for all use cases
- Claim that continuous weighting provides significant advantages over binary masking is supported only by brief statement without comprehensive comparison or ablation studies

## Confidence

**High confidence**: Experimental results demonstrating improved H-Average scores across multiple tasks and architectures are robust and well-documented. Methodology for measuring and comparing parameter importance is clearly specified.

**Medium confidence**: Theoretical justification for using weight magnitude as proxy for generalization importance has some support from cited works but lacks direct empirical validation within this paper. Mechanism explaining why approach prevents catastrophic forgetting is plausible but not rigorously proven.

**Low confidence**: Claim that continuous weighting in Importance Selection Mask provides significant advantages over binary masking is supported only by brief statement about preventing update variance, without comprehensive comparison or ablation studies.

## Next Checks

1. **Correlation validation**: Systematically measure actual correlation between pre-trained weight magnitudes and parameter importance for generalization across different network layers and architectures to validate core assumption.

2. **Binary vs continuous masking comparison**: Conduct controlled experiments comparing proposed continuous weighting scheme against simple binary masking to quantify actual benefits in terms of both performance and optimization stability.

3. **Cross-domain generalization test**: Evaluate SPIDER's performance on downstream tasks significantly different from both pre-training and fine-tuning distributions to test limits of its generalization preservation capability.