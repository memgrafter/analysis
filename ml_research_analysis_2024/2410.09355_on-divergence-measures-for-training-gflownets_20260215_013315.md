---
ver: rpa2
title: On Divergence Measures for Training GFlowNets
arxiv_id: '2410.09355'
source_url: https://arxiv.org/abs/2410.09355
tags:
- learning
- gflownets
- inference
- divergence
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces and evaluates divergence-based training objectives\
  \ for Generative Flow Networks (GFlowNets), including Renyi-\u03B1, Tsallis-\u03B1\
  , and both forward and reverse Kullback-Leibler (KL) divergences. It demonstrates\
  \ that these divergence measures often lead to faster convergence and better accuracy\
  \ than traditional flow-matching losses like trajectory balance."
---

# On Divergence Measures for Training GFlowNets

## Quick Facts
- arXiv ID: 2410.09355
- Source URL: https://arxiv.org/abs/2410.09355
- Reference count: 40
- Primary result: Divergence measures (Renyi-α, Tsallis-α, KL) provide faster convergence and better accuracy than trajectory balance in GFlowNet training across multiple generative tasks

## Executive Summary
This paper introduces divergence-based training objectives for Generative Flow Networks (GFlowNets), demonstrating that measures like Renyi-α, Tsallis-α, and both forward and reverse KL divergences often lead to faster convergence and better accuracy than traditional trajectory balance loss. The authors develop control variates based on REINFORCE leave-one-out and score-matching estimators to significantly reduce gradient variance in stochastic gradient estimates. Extensive experiments across set generation, sequence generation, Bayesian phylogenetic inference, and sampling from Gaussian mixtures and banana-shaped distributions validate the effectiveness of this approach, establishing theoretical connections to variational inference.

## Method Summary
The paper trains GFlowNets by minimizing divergences between forward and backward policies, including Renyi-α, Tsallis-α, and KL divergences. Control variates are designed using REINFORCE leave-one-out and score-matching estimators to reduce gradient variance. The method is evaluated across multiple generative tasks with parameterized forward policies (MLPs, GINs) and fixed backward policies. Hyperparameters include learning rates, batch sizes, and network architectures specific to each task. Training compares divergence measures against trajectory balance loss using metrics like L1 distance for discrete distributions and Jensen-Shannon divergence for continuous distributions.

## Key Results
- Divergence measures achieve faster convergence and better accuracy than trajectory balance across multiple generative tasks
- REINFORCE leave-one-out estimators significantly reduce gradient variance without introducing bias
- α = 0.5 generally provides optimal performance for Renyi-α and Tsallis-α divergences
- Theoretical connections established between GFlowNets and variational inference for general topological spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: REINFORCE leave-one-out estimator reduces variance without introducing bias in gradient estimates for divergence-based GFlowNet training
- Mechanism: For each trajectory in a batch, the estimator subtracts the average contribution of all other trajectories from the current trajectory's contribution, creating an unbiased estimate of the gradient
- Core assumption: Trajectories are independent samples, so leaving one out doesn't bias the estimate
- Evidence anchors:
  - [abstract]: "design control variates based on the REINFORCE leave-one-out and score-matching estimators to reduce the variance of the learning objectives' gradients"
  - [section]: "we use a leave-one-out estimator [84]; see below. For the first term, we use ∇θ log pFθ as a control variate, which satisfies EPF (so,·) [∇θ log pFθ(τ |so; θ)] = 0"
- Break condition: If trajectories are correlated or if the batch size is very small (N=1), the leave-one-out estimator breaks down

### Mechanism 2
- Claim: Divergence minimization (KL, Renyi-α, Tsallis-α) provides faster convergence than trajectory balance loss in many generative tasks
- Mechanism: These divergences directly measure the discrepancy between forward and backward policies, providing better optimization signals than the squared log-difference used in trajectory balance
- Core assumption: The variance reduction techniques effectively reduce gradient noise enough to realize the theoretical convergence benefits
- Evidence anchors:
  - [abstract]: "divergence measures often lead to faster convergence and better accuracy than traditional flow-matching losses like trajectory balance"
  - [section]: "minimizing divergence-based measures accelerates the training convergence of GFlowNets, whereas Figure 4 (for the banana-shaped distribution) and Table 1 highlight that we obtain a more accurate model with a fix compute budget"
- Break condition: If variance reduction is inadequate or if the task has very sparse rewards, trajectory balance might still outperform

### Mechanism 3
- Claim: The linear approximation of the optimal control variate baseline provides computationally efficient variance reduction
- Mechanism: Instead of computing the exact optimal baseline (which depends non-linearly on sample gradients), the paper uses a delta method approximation that can be computed efficiently with automatic differentiation
- Core assumption: The linear approximation is sufficiently accurate for practical purposes
- Evidence anchors:
  - [section]: "we consider a linear approximation of both numerator and denominator defining a⋆ in Proposition 2, which may be interpreted as an instantiation of the delta method"
  - [section]: "Consequently, we consider a linear approximation of both numerator and denominator defining a⋆ in Proposition 2"
- Break condition: If the true optimal baseline is far from the linear approximation, variance reduction may be suboptimal

## Foundational Learning

- Concept: Variational Inference and f-divergences
  - Why needed here: The paper builds on VI theory to justify using KL, Renyi-α, and Tsallis-α divergences for training GFlowNets
  - Quick check question: What's the key difference between forward and reverse KL divergence in terms of mode-seeking vs mass-covering behavior?

- Concept: Control Variates and Variance Reduction
  - Why needed here: The effectiveness of divergence-based training depends on reducing the high variance of gradient estimators
  - Quick check question: How does the REINFORCE leave-one-out estimator ensure unbiasedness?

- Concept: Automatic Differentiation and Gradient Computation
  - Why needed here: The paper relies on efficient gradient computation through frameworks like JAX/PyTorch for implementing the variance reduction techniques
  - Quick check question: Why can't we directly compute the optimal control variate baseline efficiently with autodiff?

## Architecture Onboarding

- Component map: Trajectory generator -> Divergence calculator -> Control variate module -> Optimizer -> Baseline estimator
- Critical path:
  1. Sample batch of trajectories
  2. Compute gradient estimates with control variates
  3. Update policy parameters
  4. Repeat until convergence
- Design tradeoffs:
  - Batch size vs variance: Larger batches reduce variance but increase computation
  - Choice of divergence: Different divergences have different exploration-exploitation properties
  - Control variate complexity: More sophisticated control variates may provide better variance reduction but at computational cost
- Failure signatures:
  - High gradient variance despite control variates → Increase batch size or try different control variate design
  - Poor convergence → Check if the chosen divergence is appropriate for the task (mode-seeking vs mass-covering)
  - Numerical instability → Verify the implementation of the log-target density computation and consider log-space operations
- First 3 experiments:
  1. Verify basic functionality on a simple Gaussian mixture task without control variates
  2. Compare training curves with and without control variates on the same task
  3. Test different divergence measures (KL, Renyi-α with α=0.5, Tsallis-α with α=0.5) on a discrete distribution task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different values of α in Renyi-α and Tsallis-α divergences affect the convergence rate and quality of learned distributions in GFlowNets across various generative tasks?
- Basis in paper: [explicit] The paper states that "our early experiments suggested the persistence of such dependence on α for diverse learning tasks, with α = 0.5 leading to the best results" and notes that "the choice of α provides a mechanism for controlling which trajectories are preferentially sampled during training."
- Why unresolved: While the paper uses α = 0.5 for experiments and mentions that different α values might strike a better explore-exploit tradeoff, it does not provide a comprehensive analysis of how varying α affects performance across different tasks and datasets.
- What evidence would resolve it: A systematic study varying α across multiple generative tasks (set generation, sequence generation, Bayesian phylogenetic inference, Gaussian mixtures, banana-shaped distributions) with quantitative comparisons of convergence speed and distributional accuracy for each α value.

### Open Question 2
- Question: Can variance reduction techniques be effectively designed for gradient estimation of flow-network-based objectives like trajectory balance (TB) loss, and would they improve GFlowNet training convergence?
- Basis in paper: [explicit] The paper notes that "it is unclear how to implement computationally efficient variance reduction techniques in this case" for TB loss, and states that "designing control variates for estimating the gradients of these objectives would not be significantly helpful" based on the observation of low variance in TB gradients.
- Why unresolved: The paper does not explore potential variance reduction methods specifically tailored to TB loss gradients, leaving open the question of whether such techniques could further improve training stability and speed.
- What evidence would resolve it: Implementation and empirical comparison of variance reduction techniques (e.g., control variates, leave-one-out estimators) specifically designed for TB loss gradients, with quantitative analysis of their impact on training convergence across multiple generative tasks.

### Open Question 3
- Question: How does the performance of GFlowNets trained with f-divergence measures compare to other generative models (e.g., diffusion models, GANs) on complex continuous distribution tasks?
- Basis in paper: [inferred] The paper demonstrates that GFlowNets trained with f-divergence measures perform well on tasks like Gaussian mixtures and banana-shaped distributions, but does not compare against other state-of-the-art generative models on more complex continuous distributions.
- Why unresolved: The paper focuses on benchmarking GFlowNets against traditional TB loss and does not provide comparisons with other generative model architectures that might have different strengths in modeling complex continuous distributions.
- What evidence would resolve it: Direct comparison of GFlowNets (trained with f-divergences) against diffusion models, GANs, and other generative models on benchmark datasets like CIFAR-10, CelebA, or complex multimodal distributions, with quantitative metrics for sample quality, diversity, and training efficiency.

## Limitations
- Computational overhead of variance reduction techniques is not fully explored
- Choice of divergence measure remains task-dependent without clear selection criteria
- Theoretical connections to variational inference remain somewhat abstract without practical demonstration

## Confidence
- High confidence: Divergence-based objectives can outperform trajectory balance in certain GFlowNet training scenarios
- High confidence: REINFORCE leave-one-out estimators reduce variance based on theoretical grounding
- Medium confidence: Universal superiority of divergence measures across all tasks
- Medium confidence: Effectiveness of linear approximation for optimal control variate baseline

## Next Checks
1. Ablation study: Systematically compare each control variate (leave-one-out vs score-matching) across all divergence measures to quantify their individual contributions to variance reduction and convergence speed
2. Computational overhead analysis: Measure and compare wall-clock training times between trajectory balance and divergence-based methods, accounting for the additional complexity of variance reduction techniques
3. Failure case investigation: Design experiments with highly correlated trajectories or very small batch sizes to test the robustness of the REINFORCE leave-one-out estimator under adverse conditions