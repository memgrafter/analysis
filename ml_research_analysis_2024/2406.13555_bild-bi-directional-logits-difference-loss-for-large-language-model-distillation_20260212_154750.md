---
ver: rpa2
title: 'BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation'
arxiv_id: '2406.13555'
source_url: https://arxiv.org/abs/2406.13555
tags:
- logits
- loss
- distillation
- bild
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of knowledge distillation for
  large language models (LLMs), focusing on the task-specific transfer of knowledge
  at the logit level. The authors investigate the characteristics of LLM logits and
  find that they exhibit a more pronounced long-tail distribution compared to vision
  models, with hidden noise in the long tail affecting distillation performance.
---

# BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation

## Quick Facts
- arXiv ID: 2406.13555
- Source URL: https://arxiv.org/abs/2406.13555
- Authors: Minchong Li, Feng Zhou, Xiaohui Song
- Reference count: 12
- Primary result: BiLD loss outperforms supervised fine-tuning and five other distillation methods across 13 NLP datasets using BLOOM and Qwen1.5 models

## Executive Summary
This paper addresses knowledge distillation for large language models by proposing the Bi-directional Logits Difference (BiLD) loss. The authors identify that LLM logits exhibit a pronounced long-tail distribution with hidden noise, and existing methods fail to effectively utilize internal ranking information. BiLD filters out this noise by using only top-k logits and preserves ranking information through logits differences, consisting of teacher-led and student-led components combined bidirectionally.

The method achieves state-of-the-art results across various NLP tasks, demonstrating superiority over supervised fine-tuning and vanilla KL loss. The paper evaluates BiLD on 13 datasets using BLOOM and Qwen1.5 model families, showing consistent improvements in distillation performance through better utilization of teacher knowledge.

## Method Summary
BiLD implements task-specific LLM distillation by focusing on logit-level knowledge transfer. The method extracts logits from teacher and student models, selects the top-k logits from each, constructs pairwise differences between these logits, converts the differences to probabilities using temperature scaling, and computes KL divergence between teacher and student probability distributions. The final loss combines teacher-led logits difference (t-LD) and student-led logits difference (s-LD) components. Training follows a three-epoch supervised fine-tuning phase followed by eight epochs of distillation with batch size 64, learning rate 2e-5, and temperature T=3.

## Key Results
- BiLD loss with top-8 logits outperforms supervised fine-tuning, vanilla KL loss, and five other distillation methods from both NLP and CV fields
- Achieves state-of-the-art results across 13 NLP datasets including SuperGLUE benchmarks
- Demonstrates consistent improvement across different model sizes (BLOOM-7B to BLOOM-1B, Qwen-4B to Qwen-0.5B)
- Top-1024 logits cover over 99% of probability mass in both teacher models

## Why This Works (Mechanism)

### Mechanism 1
Filtering out long-tail noise by using only top-k logits improves distillation quality. By restricting comparison to top-8 logits, the method avoids noise from rare tokens with high variance or low signal, focusing the student on the most relevant knowledge in the teacher's top predictions. The assumption is that top-k logits contain the majority of useful predictive information while the long-tail contains mostly noise. This is supported by statistics showing top-1024 logits cover over 99% of probability mass. The method may lose information if important knowledge is encoded in lower-ranked logits for rare or nuanced cases.

### Mechanism 2
Constructing logit differences preserves internal ranking information that vanilla KL loss ignores. BiLD builds differences between pairs of top-k logits, forcing the student to match not only absolute logit values but also relative ordering of tokens. This captures the teacher's preference structure, which is important for generation quality since ranking impacts selection in top-k and top-p sampling strategies. The assumption is that ranking consistency between teacher and student logits is important for generation quality. If ranking structure is not meaningful for the task (e.g., balanced classification tasks), the difference-based loss may add unnecessary complexity.

### Mechanism 3
Bi-directional logits difference (t-LD + s-LD) ensures balanced perspectives from both teacher and student. Computing two KL divergences - one from teacher to student and one from student to teacher - encourages the student to imitate the teacher while preventing the teacher from being overly dominant, maintaining stability. The assumption is that symmetric loss leads to more stable and accurate imitation than unidirectional approaches. If student and teacher distributions are very different in shape, the bidirectional comparison may introduce instability.

## Foundational Learning

- Concept: Kullback-Leibler (KL) divergence as a measure of probability distribution similarity.
  - Why needed here: BiLD loss ultimately minimizes KL divergence between reconstructed probability distributions derived from logit differences.
  - Quick check question: Given teacher probability p = [0.7, 0.2, 0.1] and student probability q = [0.6, 0.3, 0.1], is KL(p||q) finite? (Answer: Yes, because p_i > 0 wherever q_i > 0.)

- Concept: Top-k sampling and its dependence on logit ranking.
  - Why needed here: Motivation for preserving ranking information is rooted in how top-k sampling works in text generation.
  - Quick check question: If a model uses top-k=3 sampling, which tokens are eligible for output? (Answer: The tokens corresponding to the top 3 logit values.)

- Concept: Logit normalization via temperature scaling.
  - Why needed here: BiLD loss uses temperature T to convert logits into probabilities before computing KL divergence.
  - Quick check question: What happens to the output probability distribution when T approaches 0? (Answer: It becomes a one-hot distribution concentrated on the max logit.)

## Architecture Onboarding

- Component map: Teacher model → Logits extraction → Top-k selection → Logit difference construction → Probability conversion → KL divergence computation → Student model optimization
- Critical path: Logits extraction → Top-k clipping → Difference construction → KL loss → Backpropagation to student
- Design tradeoffs: Using fewer top-k logits reduces noise but may discard useful information; using bidirectional KL adds stability but doubles computation
- Failure signatures: If average accuracy drops but overlap@k increases, it may indicate overfitting to ranking at the expense of absolute prediction quality
- First 3 experiments:
  1. Run distillation with k=1 (minimum) to verify failure due to overly short logits
  2. Run with k=8 (default) to confirm improvement over vanilla KL
  3. Run with k=32 to test the trade-off between accuracy and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
How does the BiLD loss perform when teacher and student models have different vocabularies or token spaces? The paper mentions BiLD requires shared vocabularies for vector space alignment but doesn't explore scenarios with mismatched vocabularies. This is unresolved because the paper doesn't investigate performance with different vocabularies, which is common in practical applications. Evidence would come from experiments comparing BiLD performance with shared vs. mismatched vocabularies and analyzing how vocabulary alignment affects logits difference calculation.

### Open Question 2
What is the optimal value of k for BiLD loss in different task domains (e.g., text generation vs. text classification)? The paper sets k=8 based on a trade-off between computation time and performance but notes different values may be optimal for different tasks. This is unresolved because the paper doesn't systematically explore k's impact across different NLP tasks. Evidence would come from comprehensive experiments varying k across multiple NLP tasks to determine task-specific optimal values.

### Open Question 3
How does BiLD loss compare to other distillation methods when applied to multimodal models (e.g., models handling both text and images)? The paper focuses on text-based LLMs and doesn't explore BiLD's application to multimodal models, despite their increasing prevalence. This is unresolved because the paper doesn't investigate effectiveness for multimodal models, which may have different logits characteristics. Evidence would come from experiments comparing BiLD performance with other methods on multimodal models, analyzing how BiLD handles complex output spaces.

## Limitations

- The assumption that long-tail logits contain mostly noise rather than useful information is not rigorously tested
- The necessity of bidirectional design (t-LD + s-LD) is asserted but not empirically proven
- Comparison with other distillation methods is limited to five existing approaches, none specifically addressing long-tail noise or ranking preservation

## Confidence

- High confidence: Experimental methodology is sound, implementation details are clear, and results show consistent improvements across multiple datasets and model sizes. Top-k filtering mechanism is straightforward and well-justified.
- Medium confidence: Ranking preservation mechanism through logits differences is logically sound and supported by connection to sampling strategies, but lacks direct empirical validation that ranking is the causal factor.
- Low confidence: Necessity of bidirectional design is asserted but not empirically proven. Paper doesn't test unidirectional variants or provide ablation studies showing t-LD + s-LD is better than either component alone.

## Next Checks

1. **Ablation on rare token importance**: Test whether rare tokens in the long tail contain task-specific information by comparing BiLD with a variant that preserves the full logit distribution but applies noise filtering only to the lowest-ranked tokens.

2. **Ranking vs. probability validation**: Create a synthetic experiment where teacher and student have identical top-k rankings but different absolute probabilities, and measure whether BiLD's ranking preservation leads to better generation quality than probability matching alone.

3. **Bidirectional necessity test**: Implement and compare three variants: (a) t-LD only, (b) s-LD only, and (c) t-LD + s-LD (the proposed method) to provide empirical evidence for whether bidirectional design is essential.