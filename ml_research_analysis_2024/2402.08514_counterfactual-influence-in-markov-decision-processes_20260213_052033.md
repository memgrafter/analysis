---
ver: rpa2
title: Counterfactual Influence in Markov Decision Processes
arxiv_id: '2402.08514'
source_url: https://arxiv.org/abs/2402.08514
tags:
- counterfactual
- influence
- observed
- path
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses a fundamental problem in counterfactual inference
  for Markov Decision Processes (MDPs): as counterfactual states and actions progressively
  diverge from observed ones over time, the observation may no longer influence the
  counterfactual world, resulting in interventional rather than counterfactual outcomes.
  The authors introduce a formal notion of counterfactual influence based on comparing
  counterfactual and interventional distributions.'
---

# Counterfactual Influence in Markov Decision Processes

## Quick Facts
- arXiv ID: 2402.08514
- Source URL: https://arxiv.org/abs/2402.08514
- Authors: Milad Kazemi; Jessica Lally; Ekaterina Tishchenko; Hana Chockler; Nicola Paoletti
- Reference count: 40
- Primary result: Algorithm that constructs counterfactual MDPs satisfying influence constraints through polynomial-time pruning approach

## Executive Summary
This paper addresses a fundamental problem in counterfactual inference for Markov Decision Processes (MDPs): as counterfactual states and actions progressively diverge from observed ones over time, the observation may no longer influence the counterfactual world, resulting in interventional rather than counterfactual outcomes. The authors introduce a formal notion of counterfactual influence based on comparing counterfactual and interventional distributions, and develop an algorithm that constructs counterfactual MDPs automatically satisfying influence constraints through a polynomial-time pruning approach.

The method derives counterfactual policies that remain optimal while maintaining influence from the observed trajectory. Experiments on Grid World, epidemic, and sepsis models demonstrate that near-optimal policies can be obtained while remaining significantly influenced by observations. The approach also reduces MDP state space size, improving computational efficiency, and provides a framework for generating counterfactual explanations that remain tailored to individual observations.

## Method Summary
The framework operates on MDPs with known transition probabilities, using a Gumbel-max Structural Causal Model (SCM) encoding to enable counterfactual inference. Given an observed path, the algorithm calculates k-step influence by checking support overlap between observed and counterfactual state-action pairs using reverse BFS. The counterfactual MDP is constructed by pruning transitions that don't satisfy influence constraints, then unreachable states are removed. Dynamic programming is applied to find optimal (k,m)-CF policies that maximize cumulative reward while ensuring k-step influence from the observation. The method supports varying k values to balance influence strength against policy optimality.

## Key Results
- Introduces formal notion of counterfactual influence based on comparing counterfactual and interventional distributions
- Develops polynomial-time algorithm for constructing counterfactual MDPs satisfying influence constraints
- Demonstrates trade-off between influence and optimality: near-optimal policies can be obtained while remaining significantly influenced by observations
- Shows computational benefits: pruning reduces MDP state space size, improving efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Gumbel-max SCM encoding enables counterfactual inference in MDPs by sampling from posterior distributions of Gumbel variables given observed transitions.
- Mechanism: The SCM encodes MDP transitions as argmax operations over log probabilities plus Gumbel noise. When observing a transition, rejection sampling or top-down sampling infers posterior Gumbel values that align with the observation, allowing counterfactual predictions under different actions.
- Core assumption: The transition probabilities PM are known and the Gumbel-max trick correctly represents categorical distributions.
- Evidence anchors:
  - [section]: "We can achieve (approximate) posterior inference of P ((Gs,t)s∈S | st, at, st+1) through rejection sampling: we draw samples from the prior (gs,t)s∈S ∼ P ((Gs,t)s∈S) and discard all instances where f (st, at, (gs,t)s∈S) ̸= st+1."
  - [corpus]: Weak evidence - no directly matching papers found in corpus.
- Break condition: When transition probabilities are unknown or when the Gumbel-max trick doesn't accurately represent the categorical distribution (e.g., when counterfactual stability doesn't hold).

### Mechanism 2
- Claim: Counterfactual influence exists when interventional and counterfactual distributions differ, ensuring observations inform counterfactual outcomes.
- Mechanism: The algorithm checks whether supports of transition distributions overlap between observed and counterfactual state-action pairs. If supports are disjoint, no influence exists; otherwise, influence is present. The pruning algorithm removes non-influenced transitions from the counterfactual MDP.
- Core assumption: Transition probabilities PM are known, allowing precise calculation of support overlap.
- Evidence anchors:
  - [section]: "Definition 3 (1-step influence) Let τ be a path of an MDP M of length T , and let Mτ be the corresponding counterfactual MDP . Given a timet < T and counterfactual state s′t and action a′t in Mτ , we say that τ exerts an immediate (1- step) influence on s′t and a′t at time t if and only if the supports of the distributions PM(· | s′t, a′t) and PM(· | st, at) are not disjoint."
  - [abstract]: "We introduce a formal characterisation of influence based on comparing counterfactual and interventional distributions."
- Break condition: When transition probabilities are unknown or estimated with significant error, making support overlap calculations unreliable.

### Mechanism 3
- Claim: Relaxing influence constraints from k=1 to higher values enables better trade-offs between influence and policy optimality.
- Mechanism: The k-step influence definition allows influence to hold at least once over paths of length k, rather than at every step. This enables pruning fewer transitions initially, preserving more of the counterfactual MDP structure and allowing better policies while maintaining some observation influence.
- Core assumption: The observed path provides sufficient information to influence counterfactual outcomes within k steps.
- Evidence anchors:
  - [section]: "Definition 4 (k-step influence) Let M, T , τ, and Mτ be as in Definition 3. Given a time t < T , horizon k, and counterfactual state s′t and action a′t, if t + k ≤ T we say that τ exerts a k-step influence on s′t and a′t at time t if there exists a path τ ′ of Mτ of length k starting in (s′t, a′t) and such that τ exerts a 1-step influence on at least one state of τ ′."
  - [abstract]: "we further extend the notion of influence to encompass multiple steps, so that influence constraints must hold at least once (as opposed to always) over paths of a given length."
- Break condition: When k is too large relative to the observed path length, influence constraints become trivial and no pruning occurs.

## Foundational Learning

- Concept: Structural Causal Models (SCMs) and counterfactual inference
  - Why needed here: The entire framework relies on SCMs to define counterfactual distributions and compute influence by comparing counterfactual vs interventional distributions.
  - Quick check question: What is the key difference between interventional and counterfactual distributions in SCMs?

- Concept: Gumbel-max trick and categorical distribution sampling
  - Why needed here: The Gumbel-max SCM encoding is the specific mechanism used to represent MDP transitions and enable counterfactual inference in discrete state spaces.
  - Quick check question: How does the Gumbel-max trick relate sampling from categorical distributions to argmax operations over log probabilities plus Gumbel noise?

- Concept: Markov Decision Processes and policy optimization
  - Why needed here: The framework operates on MDPs, requiring understanding of state transitions, policies, and value iteration for finding optimal policies in both original and counterfactual MDPs.
  - Quick check question: What is the difference between the original MDP and the counterfactual MDP constructed in this work?

## Architecture Onboarding

- Component map: Observation processing -> Influence calculation -> Counterfactual MDP construction -> Policy optimization -> Evaluation
- Critical path: Observation → Influence Calculation → Counterfactual MDP Construction → Policy Optimization → Evaluation
- Design tradeoffs:
  - Influence strength (k value) vs policy optimality: Lower k preserves more observation influence but may restrict optimal policies; higher k allows better policies but reduces observation influence
  - Computational complexity vs accuracy: More Gumbel samples improve counterfactual probability estimates but increase computation time
  - State space pruning aggressiveness vs policy quality: Aggressive pruning ensures strong influence but may eliminate good policies
- Failure signatures:
  - No improvement in policy value when k is too low (all influenced paths lead to poor outcomes)
  - Policy values plateau at population-level optimality when k is too high (losing individual tailoring)
  - Computational timeouts when state space remains too large after pruning
  - Counterfactual paths that diverge completely from observation when influence constraints are too weak
- First 3 experiments:
  1. Grid World with catastrophic path: Verify that k ≥ 7 is needed to avoid dangerous states while maintaining some influence
  2. Epidemic model with "do nothing" baseline: Test how different k values affect infection rates and policy quality
  3. Sepsis model with diabetic patient: Compare k-CF policies against population-optimal policies to demonstrate individual tailoring benefits

## Open Questions the Paper Calls Out
None

## Limitations
- Performance critically depends on having accurate transition probabilities PM, which may not be available in many real-world scenarios
- Gumbel-max trick approximation for counterfactual inference could introduce sampling error, particularly in high-dimensional state spaces where rejection sampling becomes inefficient
- Computational complexity of exact k-step influence checking grows exponentially with k, potentially limiting scalability for longer horizons

## Confidence
- **High confidence**: The formal definition of counterfactual influence and the pruning algorithm's correctness (when transition probabilities are known)
- **Medium confidence**: The Gumbel-max SCM encoding's effectiveness for counterfactual inference, as limited empirical validation is provided
- **Medium confidence**: The policy optimization approach in counterfactual MDPs, as the experiments show trade-offs but don't fully characterize when k-CF policies outperform population-level policies

## Next Checks
1. **Robustness to transition probability uncertainty**: Evaluate the framework when transition probabilities are estimated from data rather than known exactly, measuring how estimation error affects influence calculation and policy quality.

2. **Scalability analysis**: Test the algorithm on MDPs with larger state spaces (e.g., >10,000 states) to identify computational bottlenecks and evaluate the effectiveness of the state space pruning in reducing computational burden.

3. **Cross-validation of influence measures**: Compare the proposed support-based influence measure against alternative metrics (e.g., KL divergence between counterfactual and interventional distributions) to validate that support overlap adequately captures meaningful influence.