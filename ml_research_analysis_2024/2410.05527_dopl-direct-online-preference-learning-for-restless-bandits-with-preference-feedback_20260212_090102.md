---
ver: rpa2
title: 'DOPL: Direct Online Preference Learning for Restless Bandits with Preference
  Feedback'
arxiv_id: '2410.05527'
source_url: https://arxiv.org/abs/2410.05527
tags:
- preference
- rmab
- feedback
- state
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Pref-RMAB, a restless multi-armed bandit\
  \ model where agents observe pairwise preference feedback instead of scalar rewards.\
  \ The authors propose DOPL, an algorithm that maintains confidence sets for transition\
  \ dynamics, infers preference matrices via a novel \"preference inference\" step\
  \ that reduces comparison complexity from O(B\xB2) to B-1 per step, and constructs\
  \ a direct index policy that solves a linear program in preference space."
---

# DOPL: Direct Online Preference Learning for Restless Bandits with Preference Feedback

## Quick Facts
- arXiv ID: 2410.05527
- Source URL: https://arxiv.org/abs/2410.05527
- Reference count: 40
- One-line primary result: Achieves Õ(√T ln T) regret in restless multi-armed bandits with only preference feedback

## Executive Summary
This paper introduces DOPL, an algorithm for restless multi-armed bandits (RMAB) where agents observe pairwise preference feedback instead of scalar rewards. The algorithm achieves sublinear regret by maintaining confidence sets for transition dynamics and using a novel preference inference technique that reduces comparison complexity from O(B²) to B-1 per step. DOPL constructs a direct index policy by solving a linear program in preference space, matching the performance of standard RMAB algorithms despite the information loss from preference feedback.

## Method Summary
DOPL addresses Restless Multi-Armed Bandits with Preference Feedback (Pref-RMAB) where the decision maker observes pairwise comparisons rather than scalar rewards. The algorithm maintains confidence sets for transition dynamics using Hoeffding bounds, estimates preference matrices through pairwise comparisons and a novel inference technique, and constructs a direct index policy via linear programming relaxation. The preference inference step leverages the Bradley-Terry model to reduce comparison complexity while preserving sufficient information for decision-making. DOPL adds an optimism bonus to encourage exploration and solves an extended linear program that incorporates both transition confidence sets and overestimated preferences to construct the index policy.

## Key Results
- Achieves regret bound of Õ(√T ln T) despite using only preference feedback instead of scalar rewards
- Reduces comparison complexity from O(B²) to B-1 per step through preference inference technique
- Outperforms baselines on synthetic app marketing, CPAP therapy, and maternal healthcare applications
- Maintains sublinear regret while approaching oracle performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DOPL achieves sublinear regret by combining confidence sets for transition dynamics with preference inference that reduces comparison complexity.
- **Mechanism**: The algorithm maintains confidence sets for transition functions and uses preference inference to estimate preference matrices with bounded error, enabling direct index policy construction without full pairwise comparisons.
- **Core assumption**: Bradley-Terry model accurately represents preference feedback, and arms can be compared with a reference arm to infer other preferences.
- **Evidence anchors**:
  - [abstract]: "reduces comparison complexity from O(B²) to B-1 per step"
  - [section]: "Our key insight here is that although some arms in some states may not be visited frequently, we can still infer the empirical average of its preference via the other arms' empirical preference estimations"
- **Break condition**: The Bradley-Terry model assumption fails, or the preference matrix structure doesn't allow for the inference trick to work.

### Mechanism 2
- **Claim**: The direct index policy constructed via solving a linear program in preference space achieves performance close to the offline optimal index policy.
- **Mechanism**: DOPL relaxes the instantaneous constraint to an average constraint, solves a linear program in terms of occupancy measures and preference feedback, then constructs a feasible index policy that respects the original constraints.
- **Core assumption**: The relaxed problem provides a tight bound for the original problem, and the index policy constructed from the LP solution is asymptotically optimal under ergodicity conditions.
- **Evidence anchors**:
  - [abstract]: "constructs a direct index policy that solves a linear program in preference space"
  - [section]: "Inspired by Whittle (1988), we first relax the instantaneous constraint... this relaxed problem can be equivalently transformed into a LP"
- **Break condition**: The ergodicity assumption fails, or the LP relaxation becomes too loose for the problem structure.

### Mechanism 3
- **Claim**: The overestimation bonus in preference learning encourages sufficient exploration while maintaining bounded regret.
- **Mechanism**: DOPL adds an optimism bonus δ to the estimated preference values, which encourages exploration of less-visited arms/states while still providing theoretical guarantees.
- **Core assumption**: The overestimation doesn't cause the algorithm to explore too much, making the regret term from exploration manageable.
- **Evidence anchors**:
  - [abstract]: "constructs a direct index policy that solves a linear program in preference space"
  - [section]: "we further construct a biased overestimated preference estimator based on the inferred preference to further encourage exploration"
- **Break condition**: The bonus term becomes too large relative to the true preference values, causing excessive exploration that dominates the regret bound.

## Foundational Learning

- **Concept**: Restless Multi-Armed Bandits (RMAB)
  - Why needed here: This paper extends RMAB to handle preference feedback instead of scalar rewards, so understanding the standard RMAB framework is essential
  - Quick check question: What is the key computational challenge in standard RMAB that makes index policies attractive?

- **Concept**: Linear Programming Relaxation for RMAB
  - Why needed here: DOPL relies on solving a relaxed LP to construct the index policy, so understanding this relaxation technique is crucial
  - Quick check question: How does relaxing the instantaneous constraint to an average constraint enable the problem to be transformed into a linear program?

- **Concept**: Bradley-Terry Preference Model
  - Why needed here: The algorithm assumes preference feedback follows the Bradley-Terry model to enable preference inference and reward representation
  - Quick check question: How does the Bradley-Terry model express the probability of preferring one arm over another in terms of their latent rewards?

## Architecture Onboarding

- **Component map**:
  Confidence Set Construction -> Preference Inference Engine -> Linear Program Solver -> Index Policy Constructor -> Execution Module

- **Critical path**: Observation → Confidence Set Update → Preference Inference → LP Solution → Index Policy → Action → Regret Accumulation

- **Design tradeoffs**:
  - Comparison complexity vs. information loss: Reducing from O(B²) to B-1 comparisons saves computation but relies on the inference assumption
  - Exploration vs. exploitation: The overestimation bonus balances between trying new options and exploiting known good ones
  - Computational complexity vs. regret: Solving the LP provides good indices but adds computational overhead

- **Failure signatures**:
  - High regret despite sublinear theoretical bound: Could indicate the preference inference is failing or the bonus is too large
  - Slow convergence: May suggest insufficient exploration or poor preference estimation
  - Numerical instability in LP solving: Could occur if the preference estimates are too noisy

- **First 3 experiments**:
  1. Verify the preference inference works by testing on a small synthetic problem where ground truth preferences are known
  2. Test the confidence set construction by checking if true transitions stay within bounds with high probability
  3. Validate the index policy performance by comparing against oracle policy on a simple RMAB instance with known rewards

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DOPL's performance degrade when the preference feedback violates the Bradley-Terry model assumptions?
- Basis in paper: [inferred] The paper assumes a Bradley-Terry model for preference feedback but doesn't test robustness to model violations.
- Why unresolved: The theoretical analysis and experiments all assume the BT model holds, leaving open whether DOPL remains effective under different preference structures.
- What evidence would resolve it: Experiments comparing DOPL's performance under various preference models (e.g., Thurstone-Mosteller, non-transitive preferences) would show its robustness limits.

### Open Question 2
- Question: What is the impact of the choice of reference arm and reference state on DOPL's regret bounds and empirical performance?
- Basis in paper: [explicit] The paper mentions selecting reference arm and state but doesn't provide guidance on optimal selection strategies.
- Why unresolved: While Proposition 1 shows any reference can be used, the paper doesn't analyze how different choices affect convergence rates or performance.
- What evidence would resolve it: Comparative experiments testing different reference arm/state selection strategies and theoretical analysis of their impact on regret bounds.

### Open Question 3
- Question: How does DOPL scale with the number of arms N and states |S| compared to standard RMAB algorithms?
- Basis in paper: [explicit] The regret bound includes terms with N and |S|, but computational complexity analysis is missing.
- Why unresolved: The paper provides theoretical regret bounds but doesn't analyze computational complexity or scaling behavior in practice.
- What evidence would resolve it: Runtime analysis comparing DOPL with baseline algorithms as N and |S| vary, showing scaling behavior and identifying computational bottlenecks.

## Limitations
- The theoretical analysis assumes Bradley-Terry model for preferences, which may not hold in all real-world scenarios
- Computational complexity of solving the extended linear program may become prohibitive for large state and arm spaces
- The overestimation bonus mechanism lacks sensitivity analysis showing optimal settings across different problem regimes

## Confidence
- Mechanism 1 (Preference inference): Medium - theoretically sound but limited empirical validation
- Mechanism 2 (Direct index policy): High - follows established Whittle index methodology
- Mechanism 3 (Overestimation bonus): Low - mechanism described but limited justification for specific design choices

## Next Checks
1. Conduct ablation studies removing the preference inference component to quantify information loss from reducing comparison complexity
2. Test DOPL on preference structures that violate transitivity assumptions to identify failure modes
3. Implement a sensitivity analysis varying the overestimation bonus magnitude to identify optimal settings for different problem regimes