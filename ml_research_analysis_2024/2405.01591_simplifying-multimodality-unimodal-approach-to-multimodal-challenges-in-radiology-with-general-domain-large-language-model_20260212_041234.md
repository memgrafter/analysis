---
ver: rpa2
title: 'Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in Radiology
  with General-Domain Large Language Model'
arxiv_id: '2405.01591'
source_url: https://arxiv.org/abs/2405.01591
tags:
- image
- data
- arxiv
- language
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MID-M, a multimodal framework that leverages
  a general-domain large language model (LLM) for medical image analysis by converting
  images to textual descriptions via a domain-specific classifier. Unlike existing
  multimodal models, MID-M does not require extensive fine-tuning or pretraining on
  multimodal data and uses only 3 billion parameters.
---

# Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in Radiology with General-Domain Large Language Model

## Quick Facts
- arXiv ID: 2405.01591
- Source URL: https://arxiv.org/abs/2405.01591
- Reference count: 13
- Primary result: MID-M achieves comparable or superior performance to task-specific fine-tuned LMMs with significantly fewer parameters across full and corrupted medical datasets

## Executive Summary
This paper introduces MID-M, a novel multimodal framework that leverages general-domain large language models (LLMs) for medical image analysis by converting images to textual descriptions through a domain-specific classifier. Unlike traditional approaches requiring extensive fine-tuning on multimodal data, MID-M uses only 3 billion parameters while achieving performance comparable to or better than fine-tuned models with 9-14 billion parameters. The framework demonstrates exceptional robustness to data quality degradation, maintaining high performance even when 30-50% of text is masked in medical datasets.

## Method Summary
MID-M converts medical images to textual descriptions using a domain-specific classifier (CheXpert for chest X-rays), then processes these descriptions with a general-domain LLM through in-context learning. The framework uses BM25 retrieval to select relevant examples for few-shot learning and generates concise medical impressions from image findings. The approach requires minimal fine-tuning, relying instead on the LLM's in-context learning capabilities to adapt to medical reasoning tasks while maintaining parameter efficiency and interpretability.

## Key Results
- MID-M outperforms task-specific fine-tuned LMMs (9-14B parameters) using only 3B parameters
- Highest ROUGE scores achieved in the most corrupted settings (30-50% text masking)
- Maintains comparable performance to baselines in all data quality conditions despite using significantly fewer parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MID-M leverages general-domain LLMs for medical image analysis by converting images to textual descriptions via a domain-specific classifier
- Mechanism: By transforming medical images into text-based descriptions, the framework enables a general-domain LLM to process multimodal medical data without requiring extensive fine-tuning on multimodal datasets
- Core assumption: A domain-specific image classifier can generate sufficiently accurate textual descriptions of medical images for downstream processing by a general-domain LLM
- Evidence anchors:
  - [abstract] "MID-M, a novel framework that leverages the in-context learning capabilities of a general-domain Large Language Model (LLM) to process multimodal data via image descriptions"
  - [section] "Classification-via-Description... we employ a Description-via-Classification strategy... We utilize a SOTA model (Chong et al., 2023) for CheXpert, a publicly available large chest X-ray classification dataset"
  - [corpus] Weak evidence: Related papers focus on multimodal biomedical AI but do not specifically validate the image-to-text conversion approach for general-domain LLMs
- Break condition: The domain-specific classifier fails to provide accurate descriptions of medical images, or the general-domain LLM cannot effectively process these descriptions for medical reasoning tasks

### Mechanism 2
- Claim: MID-M achieves comparable or superior performance to task-specific fine-tuned LMMs with significantly fewer parameters
- Mechanism: The framework uses only 3 billion parameters compared to 9-14 billion in fine-tuned LMMs, demonstrating that smaller models can achieve similar results when leveraging in-context learning capabilities
- Core assumption: In-context learning can be as effective as task-specific fine-tuning for medical image analysis tasks
- Evidence anchors:
  - [abstract] "without the extensive domain-specific training or pre-training on multimodal data, with significantly fewer parameters"
  - [section] "MID-M achieves a comparable or superior performance to task-specific fine-tuned LMMs and other general-domain ones, without the extensive domain-specific training or pre-training on multimodal data, with significantly fewer parameters"
  - [corpus] Weak evidence: While related papers discuss biomedical AI applications, none directly compare parameter efficiency between in-context learning approaches and fine-tuned models
- Break condition: The in-context learning approach fails to generalize effectively to medical domain tasks, or the performance gap between MID-M and fine-tuned models becomes too large to justify the parameter savings

### Mechanism 3
- Claim: MID-M demonstrates exceptional robustness to data quality degradation through systematic masking experiments
- Mechanism: By masking 30-50% of text in medical datasets, the framework maintains high ROUGE scores and semantic coherence, outperforming fine-tuned models that degrade significantly
- Core assumption: The combination of image descriptions and in-context learning provides sufficient context to maintain performance even with substantial text corruption
- Evidence anchors:
  - [abstract] "robustness of MID-M against data quality issues demonstrates its practical utility in real-world medical domain applications"
  - [section] "experimental results are presented in Table 3. MID-M achieves performance comparable to that of other baseline models in all settings, despite using only a third of the parameters. It even surpasses other models in experiments with corrupted data"
  - [corpus] Weak evidence: Related papers discuss multimodal medical AI but do not specifically address robustness to data quality degradation through systematic masking
- Break condition: The masking rate exceeds the framework's ability to maintain semantic coherence, or the combination of image descriptions and in-context learning fails to provide sufficient context for accurate medical reasoning

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Enables the framework to leverage general-domain LLMs for medical tasks without extensive fine-tuning
  - Quick check question: How does in-context learning differ from traditional fine-tuning approaches in terms of computational requirements and adaptation speed?

- Concept: Multimodal data processing
  - Why needed here: Allows the framework to handle both image and text inputs for comprehensive medical analysis
  - Quick check question: What are the key challenges in processing multimodal data, and how does the image-to-text conversion approach address these challenges?

- Concept: Data quality assessment
  - Why needed here: Critical for evaluating the framework's robustness to real-world medical data imperfections
  - Quick check question: How can we systematically measure data quality degradation, and what metrics best capture the impact on model performance?

## Architecture Onboarding

- Component map: Domain-specific image classifier → Image description generator → General-domain LLM with in-context learning → Text-based medical reasoning
- Critical path: Image → Classification → Description → Prompt construction → LLM reasoning → Output generation
- Design tradeoffs: Parameter efficiency vs. fine-tuning performance; interpretability vs. raw performance; robustness vs. optimal accuracy
- Failure signatures: Classifier misclassification leading to incorrect descriptions; LLM unable to process medical terminology in descriptions; degradation in semantic coherence with increased masking
- First 3 experiments:
  1. Evaluate the accuracy of the domain-specific classifier in generating medical image descriptions
  2. Test the LLM's ability to process image descriptions without additional context (zero-shot setting)
  3. Measure performance degradation with increasing masking rates to establish robustness benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MID-M scale with different sizes of general-domain LLMs, and what is the minimum effective model size?
- Basis in paper: [inferred] The paper demonstrates that MID-M uses a 3B parameter model and outperforms larger fine-tuned models, suggesting potential for scaling analysis
- Why unresolved: The paper only evaluates one specific LLM size (Flan-T5-xl with 3B parameters) without exploring the performance impact of using smaller or larger general-domain models
- What evidence would resolve it: Systematic experiments testing MID-M with LLMs of varying sizes (e.g., 1B, 3B, 7B, 13B parameters) while keeping all other components constant, measuring performance across different data quality levels

### Open Question 2
- Question: How does the choice of domain-specific classifier affect MID-M's performance, and what are the trade-offs between different classifier architectures?
- Basis in paper: [explicit] The paper mentions using a SOTA classifier for CheXpert but notes that the approach can be extended to other domains by replacing the classifier
- Why unresolved: The paper only uses one specific classifier without comparing alternative architectures or examining sensitivity to classifier performance
- What evidence would resolve it: Comparative experiments using different classifier architectures (e.g., different backbones, ensemble methods) and measuring the downstream impact on MID-M's final performance metrics

### Open Question 3
- Question: What is the impact of different text corruption strategies on MID-M's performance, and how does it compare to other models under realistic clinical data quality scenarios?
- Basis in paper: [explicit] The paper introduces a systematic masking approach but acknowledges it differs from real-world word-level masking of sensitive information
- Why unresolved: The masking approach is acknowledged as artificial and may not fully represent real clinical data quality issues
- What evidence would resolve it: Experiments using more realistic corruption methods (e.g., word-level masking targeting PHI, common OCR errors, inconsistent terminology) while measuring performance across different models under the same conditions

## Limitations
- Framework performance heavily depends on the accuracy of the domain-specific image classifier, which is not thoroughly validated
- Limited evaluation to chest X-ray domain only, with unclear generalizability to other medical imaging modalities
- Masking experiments use synthetic corruption that may not fully represent real-world data quality issues

## Confidence

**High Confidence** in the parameter efficiency claim:
- The framework clearly demonstrates using 3 billion parameters versus 9-14 billion in comparable models
- This claim is directly supported by the abstract and method section

**Medium Confidence** in performance claims:
- While the paper reports comparable or superior performance to fine-tuned models, the experimental setup and baseline details are limited
- The robustness claims are based on controlled masking experiments that may not reflect real-world conditions

**Low Confidence** in mechanism generalization:
- The framework's reliance on a specific domain classifier (CheXpert) raises questions about adaptability to other medical domains
- The image-to-text conversion approach's effectiveness for complex medical reasoning tasks needs further validation

## Next Checks

1. **Classifier Accuracy Validation**
   - Measure the accuracy of the CheXpert classifier in generating medical image descriptions across different disease categories
   - Compare the classifier's performance to human radiologists' descriptions of the same images

2. **Cross-Domain Generalization Test**
   - Apply MID-M to a different medical imaging modality (e.g., CT scans or MRI) using the same general-domain LLM
   - Evaluate performance degradation when using the same image-to-text approach without domain-specific fine-tuning

3. **Real-World Data Quality Assessment**
   - Test MID-M on actual clinical datasets with known quality issues (blurry images, incomplete reports, etc.)
   - Compare performance degradation against fine-tuned models when exposed to real-world data imperfections