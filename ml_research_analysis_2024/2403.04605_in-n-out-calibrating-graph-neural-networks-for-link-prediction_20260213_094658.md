---
ver: rpa2
title: 'In-n-Out: Calibrating Graph Neural Networks for Link Prediction'
arxiv_id: '2403.04605'
source_url: https://arxiv.org/abs/2403.04605
tags:
- calibration
- gnns
- vgae
- in-n-out
- sage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IN-N-OUT, the first method for calibrating
  graph neural networks (GNNs) for link prediction. While GNNs often exhibit underconfidence
  for node-level classification, the authors show they display a more complex mixed
  behavior for link prediction, being overconfident for positive predictions but underconfident
  for negative ones.
---

# In-n-Out: Calibrating Graph Neural Networks for Link Prediction

## Quick Facts
- arXiv ID: 2403.04605
- Source URL: https://arxiv.org/abs/2403.04605
- Reference count: 40
- Primary result: First method for calibrating GNNs for link prediction with significant ECE improvements

## Executive Summary
This paper introduces IN-N-OUT, a novel method for calibrating graph neural networks (GNNs) for link prediction. While GNNs typically exhibit underconfidence for node-level classification, the authors demonstrate they show more complex behavior for link prediction—being overconfident for negative predictions but underconfident for positive ones. IN-N-OUT addresses this by parameterizing temperature scaling based on how much an edge embedding changes when the edge is added to the graph, modulated by the predicted class. Experiments on 7 datasets and 5 GNN architectures show IN-N-OUT significantly outperforms standard calibration methods, achieving lower expected calibration error in 29 out of 35 cases while maintaining or improving link prediction accuracy.

## Method Summary
IN-N-OUT is a post-hoc calibration method for GNNs that computes temperature scaling factors based on the discrepancy between edge embeddings computed with and without the edge present. The method uses two separate MLP networks to handle the asymmetric miscalibration behavior observed in GNNs: one for positive predictions and another for negative predictions. During training, the method combines negative log-likelihood loss with a calibration penalty that pushes probabilities toward 0.5 for wrong predictions and toward 0 or 1 for correct ones. The calibration is performed through an auxiliary forward pass that adds each edge to the graph and measures how much the embedding changes, with the magnitude of this change used to modulate the temperature scaling factor.

## Key Results
- IN-N-OUT achieves lower ECE than baseline methods in 29 out of 35 experimental conditions
- The method improves or maintains Hits@20 performance in 24 out of 35 cases
- IN-N-OUT outperforms temperature scaling, isotonic regression, histogram binning, and BBQ across multiple GNN architectures
- The calibration method successfully addresses the asymmetric miscalibration pattern where GNNs are overconfident for negative predictions and underconfident for positive ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The difference between edge embeddings computed with and without the edge (γ(huv, h+uv)) captures the model's surprise when observing an edge that contradicts its prediction.
- Mechanism: When a GNN predicts a low probability for an edge but the edge exists (or vice versa), adding that edge causes a large change in the embedding, which IN-N-OUT uses to adjust the temperature scaling factor.
- Core assumption: The embedding difference γ(huv, h+uv) is a reliable signal of prediction correctness.
- Evidence anchors:
  - [abstract]: "if our GNN points to two nodes connecting with high probability, observing this edge should not change our knowledge about it dramatically, i.e. the edge embedding should not vary substantially"
  - [section 4]: "When h+uv and huv differ (in some sense) by a large margin, adding edge (u,v) conflicts with the previous information that we had on the graph"
- Break condition: If the embedding difference is not predictive of prediction correctness, the temperature scaling will fail to improve calibration.

### Mechanism 2
- Claim: The two separate calibration networks (MLPc1 and MLPc2) capture the asymmetric miscalibration behavior where positive predictions are underconfident and negative predictions are overconfident.
- Mechanism: MLPc1 is used when the model predicts a positive edge (suv > 0), while MLPc2 is used for negative predictions, allowing the method to learn different temperature adjustment patterns for each case.
- Core assumption: Positive and negative predictions exhibit fundamentally different miscalibration patterns that require separate treatment.
- Evidence anchors:
  - [abstract]: "they may be overconfident in negative predictions while being underconfident in positive ones"
  - [section 3]: "GNNs tend to attribute higher probability than they should to the positive class... improving calibration implies increasing the probability for the negative class"
- Break condition: If the miscalibration pattern becomes symmetric or changes direction, using two separate networks would be suboptimal.

### Mechanism 3
- Claim: The calibration loss (LCal) that pushes probabilities toward 0.5 for wrong predictions and toward 0 or 1 for correct ones complements the NLL loss by directly optimizing for calibration quality.
- Mechanism: LCal provides a direct signal to the calibration network about how well it's achieving well-calibrated probabilities, not just accurate ones.
- Core assumption: NLL alone is insufficient for achieving calibration because it focuses on accuracy rather than confidence quality.
- Evidence anchors:
  - [section 4]: "LCal can be seen as a simplified version (for binary outcomes) of the calibration penalty proposed by Wang et al. [7]"
  - [section 4]: "Recall that temperature scaling does not change the most likely classes, so Equation 4 drives the probability p̂uv to 0.5 if the GNN classifies the example incorrectly"
- Break condition: If the NLL loss already produces well-calibrated probabilities, adding LCal could hurt performance by over-constraining the model.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: IN-N-OUT relies on computing edge embeddings through message passing, and the core insight depends on how embeddings change when edges are added to the graph
  - Quick check question: Can you explain how a GCN updates node embeddings through message passing?

- Concept: Calibration and reliability diagrams
  - Why needed here: The entire method is designed to address calibration issues, and understanding reliability diagrams is crucial for interpreting results
  - Quick check question: What does it mean if a reliability diagram shows points below the diagonal?

- Concept: Temperature scaling and post-hoc calibration
  - Why needed here: IN-N-OUT is a temperature-scaling method, and understanding standard temperature scaling helps grasp how IN-N-OUT modifies it
  - Quick check question: How does temperature scaling affect the entropy of predicted probabilities?

## Architecture Onboarding

- Component map: Base GNN forward pass -> compute huv -> auxiliary GNN forward pass (with edge) -> compute h+uv -> calculate γ -> select MLP based on suv sign -> compute Tuv -> scale logit and apply sigmoid

- Critical path: Base GNN forward pass → compute huv → auxiliary GNN forward pass (with edge) → compute h+uv → calculate γ → select MLP based on suv sign → compute Tuv → scale logit and apply sigmoid

- Design tradeoffs: Using the Euclidean distance vs. direct difference for γ involves a tradeoff between sensitivity to large changes vs. preserving directional information. Using two separate MLPs allows asymmetric calibration but doubles the parameter count.

- Failure signatures: Poor calibration improvement suggests the embedding difference γ is not predictive; calibration that hurts accuracy suggests the temperature scaling is too aggressive; overfitting to calibration data suggests the MLPs are too complex.

- First 3 experiments:
  1. Implement the auxiliary GNN forward pass on a simple graph (like Cora) and verify that adding an edge changes the embeddings as expected
  2. Compare γ computed with Euclidean distance vs. direct difference on a validation set to see which correlates better with prediction correctness
  3. Implement IN-N-OUT with a single MLP (instead of two) to verify that the asymmetric calibration behavior is indeed beneficial

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would IN-N-OUT perform on graph classification tasks beyond link prediction?
- Basis in paper: [explicit] The authors mention that an interesting direction for future work is assessing calibration in other tasks like graph classification, suggesting this hasn't been explored yet.
- Why unresolved: The paper focuses exclusively on link prediction and doesn't test IN-N-OUT on graph classification datasets or tasks.
- What evidence would resolve it: Experiments applying IN-N-OUT to graph classification benchmarks like MUTAG, PROTEINS, or IMDB-BINARY with performance metrics similar to those used in the paper.

### Open Question 2
- Question: What is the computational overhead of IN-N-OUT compared to baseline calibration methods during inference?
- Basis in paper: [inferred] The paper discusses training complexity but doesn't explicitly analyze inference time or memory requirements compared to other calibration methods.
- Why unresolved: While the paper states IN-N-OUT doesn't increase asymptotic complexity, it doesn't provide concrete measurements of inference speed or memory usage.
- What evidence would resolve it: Benchmarking studies measuring inference time and memory consumption of IN-N-OUT versus temperature scaling, isotonic regression, and other methods on representative graphs.

### Open Question 3
- Question: How sensitive is IN-N-OUT to the choice of discrepancy measure γ between embeddings?
- Basis in paper: [explicit] The authors mention considering two options for γ (Euclidean distance and difference) and treating this as a hyperparameter, but don't provide systematic analysis of sensitivity.
- Why unresolved: The ablation study in Table 3 only compares IN-N-OUT against a simple baseline, not different choices of γ.
- What evidence would resolve it: Experiments testing multiple γ functions (e.g., cosine similarity, Mahalanobis distance) across datasets and measuring their impact on ECE and prediction accuracy.

### Open Question 4
- Question: Can IN-N-OUT be extended to temporal link prediction where graphs evolve over time?
- Basis in paper: [explicit] The authors mention temporal link prediction as an interesting future direction for calibration research.
- Why unresolved: The paper's formulation assumes static graphs and doesn't address how to handle dynamic graphs or time-evolving edge embeddings.
- What evidence would resolve it: Modified IN-N-OUT formulation that incorporates temporal information and experiments on temporal graph datasets like Wikipedia or social network evolution data.

## Limitations

- The method's effectiveness depends on the assumption that embedding changes correlate with prediction correctness, which may not hold for all graph structures and GNN architectures
- The asymmetric miscalibration pattern (underconfident positives, overconfident negatives) needs further validation across more diverse graph types and prediction thresholds
- The paper doesn't provide systematic analysis of how sensitive IN-N-OUT is to the choice of discrepancy measure γ between embeddings

## Confidence

- **High confidence**: The empirical results showing IN-N-OUT's superior calibration performance (29/35 cases with lower ECE) are well-supported by the experimental methodology and statistical analysis
- **Medium confidence**: The theoretical justification for why embedding differences capture prediction correctness is intuitive but not rigorously proven, relying on qualitative arguments rather than formal guarantees
- **Medium confidence**: The claim that positive predictions are underconfident while negative predictions are overconfident appears supported by the data but could be dataset-dependent and may not generalize universally

## Next Checks

1. **Mechanism validation**: Systematically test whether γ correlates with prediction correctness by computing Pearson correlation coefficients between embedding differences and calibration error on validation sets across different datasets and GNN architectures

2. **Ablation study**: Implement a variant of IN-N-OUT with a single MLP (instead of two) to quantify the actual contribution of asymmetric calibration and determine whether the performance gain is primarily from this architectural choice

3. **Robustness testing**: Evaluate IN-N-OUT on datasets with different graph characteristics (e.g., bipartite graphs, directed graphs, graphs with heterogeneous node types) to assess whether the calibration pattern holds across more diverse graph structures