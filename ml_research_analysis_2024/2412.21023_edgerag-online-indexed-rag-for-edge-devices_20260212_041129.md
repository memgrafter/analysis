---
ver: rpa2
title: 'EdgeRAG: Online-Indexed RAG for Edge Devices'
arxiv_id: '2412.21023'
source_url: https://arxiv.org/abs/2412.21023
tags:
- embeddings
- edgerag
- latency
- embedding
- index
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying Retrieval-Augmented
  Generation (RAG) systems on memory-constrained edge devices. The proposed EdgeRAG
  system tackles this by pruning second-level embeddings in a two-level IVF index,
  generating embeddings on-demand during retrieval, and caching expensive embeddings
  to minimize redundant computations.
---

# EdgeRAG: Online-Indexed RAG for Edge Devices

## Quick Facts
- arXiv ID: 2412.21023
- Source URL: https://arxiv.org/abs/2412.21023
- Authors: Korakit Seemakhupt; Sihang Liu; Samira Khan
- Reference count: 8
- Key outcome: EdgeRAG achieves 1.8× faster Time-to-First-Token latency on Jetson Orin Nano while fitting large datasets into 8GB memory through online embedding generation and selective caching

## Executive Summary
EdgeRAG addresses the challenge of deploying Retrieval-Augmented Generation (RAG) systems on memory-constrained edge devices by optimizing the two-level IVF index structure. The system prunes second-level embeddings and generates them on-demand during retrieval, pre-computes embeddings for large tail clusters to avoid latency spikes, and employs adaptive caching to minimize redundant computations. Evaluated on a Jetson Orin Nano platform, EdgeRAG achieves significant latency improvements while maintaining similar generation quality compared to baseline IVF implementations.

## Method Summary
EdgeRAG optimizes IVF indexing for edge devices by pruning second-level embeddings within clusters and generating them on-demand during retrieval, pre-computing embeddings for clusters with high generation latency, and implementing adaptive caching to minimize redundant computations. The system clusters embeddings using FAISS K-means, profiles cluster generation latencies to identify large tail clusters for pre-computation, and caches frequently accessed embeddings based on generation cost and access frequency. Evaluation uses BEIR benchmark datasets on a Jetson Orin Nano with 8GB LPDDR5 memory, comparing Time-to-First-Token latency and memory footprint against Flat and baseline IVF configurations.

## Key Results
- 1.8× faster Time-to-First-Token latency compared to baseline IVF index
- Achieves similar generation quality and recall as baseline implementations
- Successfully fits all evaluated datasets into 8GB memory on Jetson Orin Nano

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning second-level embeddings reduces memory footprint without significant accuracy loss
- Mechanism: The system prunes embeddings within clusters that are only used for second-level search, generating them on-demand during retrieval instead of storing them
- Core assumption: Most embeddings in a cluster are not accessed during typical retrieval operations
- Evidence anchors:
  - [abstract] "EdgeRAG which addresses the memory constraint by pruning embeddings within clusters and generating embeddings on-demand during retrieval"
  - [section] "First, we prune the vector embedding of the data embeddings within centroid clusters which are only used for second-level search to save the memory capacity"
  - [corpus] Weak evidence - corpus papers focus on different aspects of edge RAG deployment, not specifically on pruning second-level embeddings
- Break condition: If the retrieval pattern changes such that most clusters need to be accessed frequently, the benefit of pruning diminishes significantly

### Mechanism 2
- Claim: Pre-computing and storing embeddings for large tail clusters prevents long-tail latency
- Mechanism: During indexing, clusters with high embedding generation latency are identified and their embeddings are precomputed and stored, bypassing the need for online generation during retrieval
- Core assumption: Embedding generation latency follows a skewed distribution with a few clusters having significantly higher costs
- Evidence anchors:
  - [abstract] "To avoid the latency of generating embeddings for large tail clusters, EdgeRAG pre-computes and stores embeddings for these clusters"
  - [section] "To mitigate long-tail latency in large clusters, EdgeRAG employs a hybrid approach... Clusters exceeding the latency threshold are identified and their embeddings are precomputed and stored"
  - [section] "Figure 5 presents the distribution of embedding generation times for various clusters within the nq datasets. The majority of clusters exhibit generation latencies under 500 milliseconds. Nevertheless, a subset of clusters, albeit infrequent, may experience generation times exceeding 2 seconds."
- Break condition: If the latency threshold for determining "large tail clusters" is set too low or too high, the system may either waste storage on unnecessary precomputation or fail to mitigate long-tail latency effectively

### Mechanism 3
- Claim: Adaptive caching of generated embeddings minimizes redundant computation while controlling memory overhead
- Mechanism: EdgeRAG employs a cost-aware caching strategy that selectively caches embeddings from clusters with high generation costs, using a dynamic threshold to prevent caching of low-cost embeddings
- Core assumption: There is significant reuse of embeddings across queries, making caching beneficial
- Evidence anchors:
  - [abstract] "EdgeRAG adaptively caching remaining embeddings to minimize redundant computations and further optimize latency"
  - [section] "To optimize cache utilization, EdgeRAG strategically avoids caching embeddings from smaller clusters with lower generation latencies"
  - [section] "Table 2 through the chunk reuse ratio. The significant reuse observed across all datasets implies that embeddings for clusters associated with these frequently accessed data chunks must be repeatedly generated"
- Break condition: If the cache hit rate remains low despite caching (due to low reuse patterns), the memory overhead of caching may not be justified by the performance gains

## Foundational Learning

- Concept: Inverted File (IVF) Index and its two-level structure
  - Why needed here: EdgeRAG is built upon the IVF index, and understanding its clustering and retrieval process is fundamental to grasping EdgeRAG's optimizations
  - Quick check question: In a two-level IVF index, what are the two stages of the retrieval process, and what happens at each stage?

- Concept: Vector similarity search and its computational challenges in high-dimensional spaces
  - Why needed here: The motivation for EdgeRAG stems from the limitations of vector similarity search on edge devices, particularly memory and latency constraints
  - Quick check question: Why are traditional techniques like sorting less effective for finding similar embeddings in high-dimensional spaces?

- Concept: Memory thrashing and its impact on performance
- Why needed here: EdgeRAG specifically addresses memory thrashing that occurs when large datasets exceed available memory, leading to poor performance
  - Quick check question: What is memory thrashing, and how does it affect the performance of a system when accessing data that exceeds available memory?

## Architecture Onboarding

- Component map: First-level Index -> Second-level Index -> Embedding Cache -> Storage
- Critical path: Query → First-level lookup → Second-level lookup (cache hit/miss) → Data chunk retrieval → Embedding generation (if needed) → Cache update (if needed) → Retrieval result
- Design tradeoffs:
  - Memory vs. Latency: Pruning second-level embeddings saves memory but may increase latency if online generation is slow
  - Storage vs. Computation: Precomputing embeddings for large tail clusters reduces latency but requires additional storage
  - Cache Size vs. Hit Rate: Larger cache can improve hit rate but increases memory overhead
- Failure signatures:
  - High cache miss rate despite caching: Indicates low reuse patterns or incorrect caching threshold
  - Long tail latency even with precomputed embeddings: Suggests incorrect identification of large tail clusters or threshold setting
  - Memory thrashing persists: Indicates insufficient pruning or caching, or datasets still too large for available memory
- First 3 experiments:
  1. Measure cache hit rate and overall latency with different cache sizes to find optimal cache configuration
  2. Vary the latency threshold for identifying large tail clusters and measure the impact on overall latency and storage usage
  3. Test the system with different datasets to evaluate how well the pruning and caching strategies generalize across various data distributions and access patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EdgeRAG's performance scale with increasingly larger datasets that exceed available memory by orders of magnitude?
- Basis in paper: [explicit] The paper evaluates datasets up to 18.5 GB but doesn't test scenarios where dataset size is 10x or 100x larger than available memory
- Why unresolved: The evaluation only covers datasets 2-3x larger than available memory (8 GB), leaving uncertainty about performance at extreme scale
- What evidence would resolve it: Empirical results from testing EdgeRAG on datasets 10x-100x larger than available memory, measuring TTFT latency and memory utilization

### Open Question 2
- Question: What is the optimal cache size for EdgeRAG's adaptive caching mechanism across different workload patterns?
- Basis in paper: [explicit] The paper mentions caching uses 7% additional memory but doesn't explore optimal cache sizing or sensitivity to different reuse patterns
- Why unresolved: The evaluation uses a fixed cache size without exploring how cache hit rates and latency vary with different cache capacities
- What evidence would resolve it: Empirical study varying cache sizes from 1% to 50% of available memory, measuring cache hit rates, TTFT latency, and memory utilization across diverse workloads

### Open Question 3
- Question: How does EdgeRAG's pruning strategy affect long-term system behavior with dynamic data updates and deletions?
- Basis in paper: [explicit] The paper describes insertion and removal processes but doesn't evaluate performance over time with continuous data changes
- Why unresolved: The evaluation only tests static datasets without examining how the system maintains efficiency with ongoing updates
- What evidence would resolve it: Long-term evaluation tracking EdgeRAG performance over thousands of insertions and deletions, measuring retrieval latency degradation and pruning effectiveness

## Limitations

- Evaluation constrained to single edge platform (Jetson Orin Nano with 8GB memory), limiting generalizability
- Adaptive caching performance heavily depends on access pattern distribution which varies across use cases
- Comprehensive analysis of storage requirements trade-offs for precomputed embeddings not provided

## Confidence

- **High Confidence**: The core mechanism of pruning second-level embeddings and generating them on-demand is technically sound and well-supported by experimental results showing 1.8× faster TTFT latency
- **Medium Confidence**: The effectiveness of pre-computing embeddings for large tail clusters is supported by empirical evidence but optimal threshold determination remains application-dependent
- **Medium Confidence**: The adaptive caching strategy's benefits are demonstrated through cache hit rate analysis, but long-term performance under varying query distributions is not fully characterized

## Next Checks

1. **Cross-Platform Generalization**: Evaluate EdgeRAG on different edge devices with varying memory capacities (e.g., Raspberry Pi, Coral Dev Board) to assess performance portability and identify hardware-specific optimization opportunities

2. **Dynamic Threshold Analysis**: Conduct experiments varying the SLO thresholds for large tail cluster identification and adaptive caching thresholds to determine sensitivity of overall system performance to these critical parameters

3. **Long-term Performance Stability**: Implement a continuous evaluation framework that simulates realistic query patterns over extended periods to assess how cache hit rates, latency, and memory usage evolve with changing access patterns