---
ver: rpa2
title: A Survey on Knowledge Distillation of Large Language Models
arxiv_id: '2402.13116'
source_url: https://arxiv.org/abs/2402.13116
tags:
- llms
- language
- data
- knowledge
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically explores knowledge distillation (KD)
  of large language models (LLMs), detailing how to transfer advanced capabilities
  from proprietary models like GPT-4 to open-source counterparts such as LLaMA. It
  introduces data augmentation as a key paradigm for generating skill-specific training
  data, enabling open-source models to approximate proprietary LLMs' contextual adeptness
  and deep semantic insights.
---

# A Survey on Knowledge Distillation of Large Language Models

## Quick Facts
- **arXiv ID:** 2402.13116
- **Source URL:** https://arxiv.org/abs/2402.13116
- **Reference count:** 40
- **Primary result:** Comprehensive survey of knowledge distillation techniques for transferring capabilities from proprietary LLMs to open-source models through data augmentation and specialized algorithms

## Executive Summary
This survey systematically explores knowledge distillation (KD) of large language models (LLMs), detailing how to transfer advanced capabilities from proprietary models like GPT-4 to open-source counterparts such as LLaMA. It introduces data augmentation as a key paradigm for generating skill-specific training data, enabling open-source models to approximate proprietary LLMs' contextual adeptness and deep semantic insights. The work covers algorithms for knowledge elicitation and distillation, skill enhancement across tasks like reasoning and multi-modality, and verticalization in domains such as law and healthcare. By offering a comprehensive overview of current methodologies, it aims to guide researchers and practitioners toward more accessible and efficient AI solutions while advocating compliance with legal terms governing LLM use.

## Method Summary
The survey provides a comprehensive overview of knowledge distillation methodologies for LLMs, covering algorithms for knowledge elicitation and transfer, data augmentation strategies for skill-specific training, and applications across various domains and tasks. It systematically categorizes different KD approaches and discusses their applications in reasoning, multi-modality, and specialized vertical domains like law and healthcare.

## Key Results
- Data augmentation emerges as a critical paradigm for generating skill-specific training data to transfer proprietary LLM capabilities to open-source models
- KD algorithms enable open-source models to approximate advanced capabilities in reasoning, multi-modality, and domain-specific knowledge
- The survey provides comprehensive coverage of current methodologies while addressing legal compliance considerations for LLM use

## Why This Works (Mechanism)
Knowledge distillation works by transferring learned representations and capabilities from larger, more capable LLMs to smaller open-source models through carefully designed training processes. The mechanism leverages data augmentation to create skill-specific training examples that capture the nuanced behaviors of proprietary models, allowing open-source alternatives to approximate these capabilities while maintaining computational efficiency and accessibility.

## Foundational Learning
- **Knowledge Distillation Fundamentals** - Why needed: Core mechanism for transferring capabilities between models; Quick check: Verify teacher-student model architecture compatibility
- **Data Augmentation Techniques** - Why needed: Enables creation of skill-specific training data that captures proprietary model behaviors; Quick check: Assess augmentation quality through downstream task performance
- **Domain Adaptation** - Why needed: Critical for applying KD to specialized domains like healthcare and law; Quick check: Validate domain-specific knowledge retention through expert evaluation
- **Multi-modal Learning** - Why needed: Essential for transferring capabilities across different data types (text, image, audio); Quick check: Test cross-modal consistency and performance
- **Legal Compliance** - Why needed: Ensures responsible use of proprietary model capabilities; Quick check: Review terms of service and licensing agreements for compliance

## Architecture Onboarding
**Component Map:** Data Augmentation -> Knowledge Elicitation -> Distillation Algorithm -> Open-source Model
**Critical Path:** Quality training data generation → effective knowledge transfer → model fine-tuning → evaluation and validation
**Design Tradeoffs:** Computational efficiency vs. capability transfer accuracy; model size vs. performance; data quality vs. quantity
**Failure Signatures:** Poor performance on reasoning tasks indicates inadequate knowledge elicitation; domain-specific failures suggest insufficient augmentation quality; legal compliance issues arise from improper data handling
**First Experiments:** 1) Benchmark standard KD performance on simple text classification; 2) Test data augmentation effectiveness on reasoning tasks; 3) Evaluate domain adaptation capabilities in controlled environments

## Open Questions the Paper Calls Out
None

## Limitations
- Rapidly evolving LLM technology may quickly render survey findings outdated
- Limited empirical validation of data augmentation strategies across diverse tasks and domains
- Insufficient practical guidance on computational resource requirements for effective KD
- Superficial treatment of legal compliance considerations without detailed risk analysis

## Confidence
- **High confidence:** Categorization of KD algorithms and systematic organization of knowledge distillation landscape
- **Medium confidence:** Data augmentation paradigm and multi-modal/reasoning capabilities through KD
- **Low confidence:** Domain-specific verticalization claims and legal compliance recommendations

## Next Checks
1. Conduct systematic benchmarking of KD performance across different data augmentation strategies on standardized reasoning and multi-modal tasks to quantify effectiveness and identify optimal configurations
2. Perform rigorous evaluation of domain-specific distilled models in healthcare and legal applications using domain expert assessments and regulatory compliance testing
3. Analyze computational resource requirements and efficiency trade-offs of different KD methodologies through controlled experiments comparing training costs, inference latency, and model quality across various model sizes and architectures