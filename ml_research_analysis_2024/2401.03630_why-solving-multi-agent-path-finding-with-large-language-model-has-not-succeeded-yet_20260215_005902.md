---
ver: rpa2
title: Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded
  Yet
arxiv_id: '2401.03630'
source_url: https://arxiv.org/abs/2401.03630
tags:
- agent
- mapf
- arxiv
- agents
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the use of large language models (LLMs) like
  GPT-4 for solving the multi-agent path finding (MAPF) problem. The authors test
  LLMs in both easy scenarios (empty maps) and harder ones (maps with obstacles),
  finding that while LLMs can succeed in simple cases, they fail to generate valid
  solutions in more complex environments.
---

# Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet

## Quick Facts
- arXiv ID: 2401.03630
- Source URL: https://arxiv.org/abs/2401.03630
- Authors: Weizhe Chen; Sven Koenig; Bistra Dilkina
- Reference count: 6
- Primary result: LLMs can solve MAPF in simple environments but fail in complex scenarios due to context limits, obstacle understanding, and reasoning limitations

## Executive Summary
This paper evaluates the use of large language models (LLMs) like GPT-4 for solving the multi-agent path finding (MAPF) problem. The authors test LLMs in both easy scenarios (empty maps) and harder ones (maps with obstacles), finding that while LLMs can succeed in simple cases, they fail to generate valid solutions in more complex environments. The authors attribute these failures to three main issues: the context length limit of LLMs, their inability to understand and remember obstacle locations, and a lack of reasoning capability for complex planning tasks. They support their findings with extensive experiments and discuss potential solutions, including using multimodal inputs and incorporating external tools. The paper concludes by highlighting the challenges of using LLMs for real-world MAPF scenarios and suggesting future research directions.

## Method Summary
The authors use GPT-4-turbo to recommend actions for each agent step-by-step in MAPF scenarios, validating solutions with a high-level conflict checker. They test on empty maps, room maps with obstacles, and maze maps from the standard MAPF benchmark. The method involves providing scenario information (map size, obstacle locations, agent start/goal positions) to the LLM, which generates actions iteratively. The conflict checker validates each solution, providing feedback for corrections until a valid solution is found or failure criteria are met. Experiments track success rates and normalized makespan across varying numbers of agents and map complexities.

## Key Results
- LLMs can solve MAPF in simple environments (empty maps) but fail in complex scenarios with obstacles
- Performance degrades significantly with increased map complexity and number of agents
- Multimodal inputs (images) do not improve LLM performance compared to text-only descriptions
- Failures attributed to context length limits, obstacle understanding issues, and reasoning limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can solve MAPF in simple environments by leveraging step-by-step generation with a high-level conflict checker.
- Mechanism: The LLM generates actions for each agent at every timestep, and a rule-based checker validates the solution, providing feedback for corrections. This iterative process allows the LLM to correct mistakes and eventually produce a valid solution.
- Core assumption: The LLM can understand and follow the constraints of MAPF (no collisions, 4-connected grid) when given explicit instructions and feedback.
- Evidence anchors:
  - [abstract] "We first show the motivating success on an empty room map without obstacles, then the failure to plan on the harder room map and maze map of the standard MAPF benchmark."
  - [section] "We introduce a high-level conflict checker to ensure the plan generated by LLM is valid. We inform the LLM about the mistake in the solution, if any."
  - [corpus] Found 25 related papers, but limited direct evidence on LLM success in MAPF. Average neighbor FMR=0.546 suggests moderate relevance.
- Break condition: The mechanism breaks when the environment becomes too complex (e.g., many obstacles or agents), as the LLM fails to generate valid solutions despite feedback.

### Mechanism 2
- Claim: The LLM's performance in MAPF is limited by its context length, understanding of obstacle locations, and reasoning capabilities.
- Mechanism: The LLM struggles with long contexts (limiting the number of agents and timesteps it can handle), understanding complex map information (especially with obstacles), and planning ahead to avoid future collisions.
- Core assumption: The LLM's training data does not adequately cover complex MAPF scenarios, leading to poor performance in understanding and reasoning about these problems.
- Evidence anchors:
  - [abstract] "We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis."
  - [section] "We found that the reason for failing in MAPF is different from the reasons for failing in general planning tasks... Based on our observations, we break down the reasons into three parts from the perspective of natural language models: limitation from the model, understanding, and reasoning."
  - [corpus] Weak evidence; most related papers focus on MAPF solutions without LLM involvement.
- Break condition: The mechanism breaks when the LLM encounters scenarios requiring complex reasoning or understanding of obstacles, leading to failure in generating valid solutions.

### Mechanism 3
- Claim: Multimodal inputs (images) do not improve the LLM's performance in MAPF compared to text-only descriptions.
- Mechanism: The LLM struggles to understand and reason about obstacle locations when provided as images, as it lacks the ability to translate high-level intuitions into concrete path planning decisions.
- Core assumption: The LLM's training data does not include sufficient examples of understanding and reasoning about map images in the context of path planning.
- Evidence anchors:
  - [abstract] "We incorporate a list of experiments featuring various prominent prompt design alternatives, such as image-based and text-only inputs..."
  - [section] "Because of this lack of information, our main prompt included guidance on what action could be taken in the current step to increase the success rate a little bit... we propose three different models... We test the performance of these three variants, and show the results in Table. 3. We found that with an image-based input, the results are even worse."
  - [corpus] No direct evidence on multimodal input performance; assumption based on the paper's findings.
- Break condition: The mechanism breaks when the LLM encounters complex map images, as it cannot effectively translate visual information into path planning decisions.

## Foundational Learning

- Concept: Multi-Agent Path Finding (MAPF)
  - Why needed here: Understanding MAPF is crucial for grasping the problem the LLM is trying to solve and the challenges it faces.
  - Quick check question: What is the objective of MAPF, and what are the main constraints agents must follow?

- Concept: Large Language Models (LLMs)
  - Why needed here: Knowing how LLMs work and their limitations is essential for understanding why they struggle with MAPF and what aspects of their architecture contribute to these difficulties.
  - Quick check question: What are the main components of an LLM's architecture, and how do they influence its performance on tasks like MAPF?

- Concept: Context Length and Token Limits
  - Why needed here: Understanding the limitations imposed by context length and token limits helps explain why LLMs struggle with large-scale MAPF problems involving many agents and timesteps.
  - Quick check question: How do context length and token limits affect an LLM's ability to process and generate solutions for complex problems like MAPF?

## Architecture Onboarding

- Component map:
  - LLM (e.g., GPT-4-turbo) -> High-level conflict checker -> Prompt design

- Critical path:
  1. Provide scenario information to the LLM (map, start and goal locations).
  2. LLM generates actions for each agent at the current timestep.
  3. High-level conflict checker validates the solution.
  4. If invalid, provide feedback to the LLM and repeat steps 2-4 until a valid solution is generated or a failure condition is met.
  5. Proceed to the next timestep and repeat the process until all agents reach their goals or a failure condition is met.

- Design tradeoffs:
  - Using a step-by-step generation approach vs. generating the entire plan at once: Step-by-step allows for iterative corrections but may be slower, while generating the entire plan at once is faster but more prone to failure.
  - Providing single-step observation (SSO) information vs. relying on the LLM's understanding of the map: SSO improves success rates but increases the prompt length and iterations required.

- Failure signatures:
  - LLM fails to generate a plan within 3 times the optimal makespan.
  - LLM fails 5 consecutive times in a single timestep after receiving feedback.
  - LLM encounters a token limit error when processing long prompts.

- First 3 experiments:
  1. Test the LLM's performance on an empty map with varying numbers of agents to establish a baseline for simple scenarios.
  2. Introduce obstacles into the map and test the LLM's ability to generate valid solutions, observing how performance degrades with increased complexity.
  3. Compare the LLM's performance using text-only inputs versus multimodal inputs (images) to assess the impact of input format on solution quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multimodal input (combining images and text) improve LLM performance in solving MAPF problems compared to text-only approaches?
- Basis in paper: [explicit] The paper discusses three different input methods: multimodal input (MM), text-only describing obstacles (TOO), and text-only describing the whole map (TOM). The authors found that the multimodal approach performed worse than text-only methods.
- Why unresolved: While the paper presents initial findings that multimodal input did not improve performance, the reasons for this failure are not fully explored. The authors suggest that LLMs struggle to translate high-level visual intuitions into concrete reasoning for path planning, but this hypothesis needs further investigation.
- What evidence would resolve it: Further experiments with improved multimodal approaches, perhaps with better integration of visual and textual information, could determine if this method can eventually outperform text-only approaches.

### Open Question 2
- Question: How can external tools be effectively integrated with LLMs to solve MAPF problems without compromising the reasoning capability of the LLM?
- Basis in paper: [explicit] The paper discusses the challenges of using external tools with LLMs for MAPF. The authors note that while tools like single-agent planners (e.g., A*) could solve MAPF, they would eliminate the need for LLM reasoning, making the LLM redundant.
- Why unresolved: The paper identifies the paradox of needing tools to enhance LLM reasoning while avoiding tools that solve the problem entirely. However, it does not provide a clear solution to this dilemma.
- What evidence would resolve it: Development and testing of hybrid approaches that strategically use external tools to augment LLM reasoning without fully automating the solution process could provide insights into effective integration methods.

### Open Question 3
- Question: What specific reasoning capabilities are missing in current LLMs that prevent them from solving complex MAPF problems, and how can these be developed?
- Basis in paper: [explicit] The authors identify three main areas where current LLMs fall short: context length limitations, understanding and remembering obstacle locations, and reasoning capability for complex planning tasks. They provide examples, such as the symmetry-breaking problem, to illustrate these shortcomings.
- Why unresolved: While the paper outlines the areas where LLMs struggle, it does not propose specific methods to enhance these reasoning capabilities within the LLM framework.
- What evidence would resolve it: Research focused on enhancing LLM architectures or training methods to improve their ability to handle longer contexts, better understand spatial information, and perform complex reasoning tasks could provide solutions to these challenges.

## Limitations

- The exact prompt formats and system configurations used in experiments are not fully specified, limiting reproducibility
- The implementation details of the conflict checker are not described, making it difficult to assess the quality of solution validation
- The comparison between text-only and image-based inputs lacks clarity on how images were processed and formatted for the LLM

## Confidence

- High confidence: The observed failure of LLMs in complex MAPF scenarios with obstacles is well-supported by the experimental results presented
- Medium confidence: The attribution of failures to context length limitations, obstacle understanding issues, and reasoning capabilities is reasonable but requires further validation
- Low confidence: The claim about multimodal inputs performing worse than text-only inputs needs additional experimental support

## Next Checks

1. Implement a controlled reproduction using the exact prompt formats and conflict checker logic to verify the reported success rates and failure patterns
2. Conduct ablation studies to isolate the impact of context length versus reasoning capabilities by testing LLMs with varying context windows on the same MAPF instances
3. Evaluate alternative LLM architectures (e.g., fine-tuned models, specialized prompting strategies) to determine if the observed limitations are inherent to current LLM designs or can be mitigated through architectural changes