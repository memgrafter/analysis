---
ver: rpa2
title: Probing Large Language Models for Scalar Adjective Lexical Semantics and Scalar
  Diversity Pragmatics
arxiv_id: '2404.03301'
source_url: https://arxiv.org/abs/2404.03301
tags:
- scalar
- scale
- probing
- language
- adjectives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper probes Large Language Models (LLMs) for their understanding
  of scalar adjectives (SAs) and scalar diversity. It uses both direct and indirect
  probing methods to evaluate LLMs' knowledge of SAs' scale membership and intensity
  information.
---

# Probing Large Language Models for Scalar Adjective Lexical Semantics and Scalar Diversity Pragmatics

## Quick Facts
- arXiv ID: 2404.03301
- Source URL: https://arxiv.org/abs/2404.03301
- Reference count: 0
- Primary result: LLMs encode rich lexical-semantic information about scalar adjectives but struggle with scalar diversity pragmatics

## Executive Summary
This paper investigates how Large Language Models (LLMs) understand scalar adjectives and scalar diversity pragmatics. Using both direct and indirect probing methods, the authors evaluate eight different models across three datasets for scale membership and scalar intensity tasks, plus three datasets for scalar diversity reasoning. The study reveals that while LLMs generally encode rich lexical-semantic information about scalar adjectives, this knowledge does not translate into effective understanding of scalar diversity, which requires pragmatic inference beyond lexical semantics.

## Method Summary
The authors employ direct probing (analyzing internal representations) and indirect probing (testing performance on tasks) to evaluate LLMs' knowledge of scalar adjectives. For scale membership, they use cosine similarity between adjective embeddings and scale vectors derived from weakest/strongest adjectives. Scalar intensity is measured through pairwise comparisons using both concatenated adjective strings and natural contexts. Scalar diversity is tested via binary classification with debiasing strategies. Eight models are evaluated: BERT-base/large, RoBERTa-base/large, Falcon-7B-instruct, GPT-4, and Flan-T5-xl/xxl.

## Key Results
- LLMs generally encode rich lexical-semantic information about scalar adjectives
- LLMs have unsatisfying performance in capturing scalar diversity despite encoding rich lexical-semantic information
- The size of the LLMs does not correlate with their performance on these tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Scalar adjective embeddings in LLMs encode scale membership information through vector similarity patterns.
- **Mechanism**: The model represents half-scales as vector sums of the weakest and strongest adjectives. Each adjective's embedding is compared via cosine similarity to all scale vectors, and the highest similarity determines scale membership.
- **Core assumption**: The scale vector derived from the two extremes adequately captures the semantic field of the entire half-scale.
- **Evidence anchors**: [abstract] "LLMs generally encode rich lexical-semantic information about SAs"; [section 3.2.2] "All LLMs encode rich scale information as they mostly outperform the baseline"
- **Break condition**: If the model's internal representations do not preserve relative distances between scale members, or if scale boundaries are fuzzy, the similarity-based ranking will fail.

### Mechanism 2
- **Claim**: Scalar diversity reasoning requires pragmatic inference beyond lexical semantics.
- **Mechanism**: Scalar diversity involves inferring that a weaker adjective pragmatically implies the negation of a stronger alternative. This requires reasoning about speaker choice and alternative statements, not just knowing adjective intensities.
- **Core assumption**: Scalar diversity is a pragmatic phenomenon that cannot be reduced to lexical semantic knowledge.
- **Evidence anchors**: [abstract] "the rich lexical-semantic knowledge does not entail a good understanding of scalar diversity"; [section 5.4] "the gap between semantic and pragmatic capabilities"
- **Break condition**: If scalar diversity could be reduced to a pattern in training data rather than requiring pragmatic inference, then models with better lexical semantics might perform better on scalar diversity.

### Mechanism 3
- **Claim**: Larger model size does not guarantee better performance on scalar adjective tasks.
- **Mechanism**: Architectural differences and training objectives matter more than size alone. For example, RoBERTa underperforms smaller BERT despite being larger, and Falcon (larger) underperforms smaller encoder models.
- **Core assumption**: Architectural features and training objectives can outweigh model size in determining performance on these tasks.
- **Evidence anchors**: [abstract] "larger models are not always better"; [section 5.3] "scaling does not fully explain the pattern of results"
- **Break condition**: If scaling laws held perfectly across all model families and tasks, larger models would always perform better regardless of architecture.

## Foundational Learning

- **Concept**: Scalar adjectives and scalar implicatures
  - Why needed here: The entire paper investigates how LLMs handle scalar adjectives and the pragmatic inferences they trigger.
  - Quick check question: What is the difference between "likely" and "certain" on the likelihood scale, and what scalar implicature might "likely" trigger?

- **Concept**: Direct vs. indirect probing methods
  - Why needed here: The paper uses both direct probing (analyzing internal representations) and indirect probing (testing performance on tasks) to evaluate LLMs.
  - Quick check question: Why might indirect probing provide a lower bound on model capabilities compared to direct probing?

- **Concept**: Vector similarity and cosine distance
  - Why needed here: The paper uses cosine similarity between adjective embeddings and scale vectors to determine scale membership.
  - Quick check question: If adjective A has a cosine similarity of 0.8 with scale vector S1 and 0.6 with scale vector S2, which scale is A more likely to belong to according to the paper's methodology?

## Architecture Onboarding

- **Component map**: Data preparation -> Probing method selection -> Model evaluation -> Results analysis -> Interpretation
- **Critical path**: Data preparation → Probing method selection → Model evaluation → Results analysis → Interpretation
- **Design tradeoffs**: Direct probing provides better estimates but requires access to model internals; indirect probing is more general but provides lower bounds
- **Failure signatures**: Models perform well on direct probing but poorly on indirect probing (suggests inability to reason about embeddings in context)
- **First 3 experiments**:
  1. Implement direct scale membership probing using cosine similarity between adjective embeddings and scale vectors derived from weakest/strongest adjectives
  2. Compare scalar intensity estimation using concatenated adjective strings vs. natural contexts with single adjectives
  3. Test scalar diversity reasoning using binary classification with debiasing strategies for yes/no preferences

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do different training objectives affect LLMs' ability to capture scalar adjective lexical semantics and scalar diversity pragmatics?
- **Basis in paper**: The paper suggests that models with sentence relation training tend to learn fine-grained lexical semantics easier than those without.
- **Why unresolved**: The paper provides initial observations but does not conduct a systematic investigation of the impact of different training objectives.
- **What evidence would resolve it**: A controlled study comparing the performance of LLMs with different training objectives on scalar adjective lexical semantics and scalar diversity pragmatics tasks.

### Open Question 2
- **Question**: What is the relationship between LLMs' scalar adjective lexical semantics knowledge and their ability to draw pragmatic inferences about scalar diversity?
- **Basis in paper**: The paper finds that good lexical-semantic knowledge does not always give rise to good performance in pragmatic reasoning tasks.
- **Why unresolved**: The paper provides initial observations but does not conduct a systematic investigation of the relationship between semantic and pragmatic capabilities.
- **What evidence would resolve it**: A study that measures LLMs' performance on both scalar adjective lexical semantics and scalar diversity pragmatics tasks, and analyzes the correlation between the two.

### Open Question 3
- **Question**: How do LLMs handle scalar diversity pragmatics in more naturalistic settings, beyond the binary classification task used in this paper?
- **Basis in paper**: The paper notes that the datasets used for the scalar diversity task are relatively small and only constrained to scalar adjectives.
- **Why unresolved**: The paper uses a simplified binary classification task for scalar diversity, which may not fully capture the complexity of scalar diversity pragmatics in naturalistic settings.
- **What evidence would resolve it**: A study that uses more naturalistic and diverse datasets for scalar diversity pragmatics, including different types of scalar triggers and considering the possibility of cancellation.

## Limitations
- The paper demonstrates that LLMs encode scalar adjective semantics but struggles to explain why this knowledge fails to transfer to scalar diversity tasks
- Architectural differences between models are cited as explanations for performance variations, but the paper does not conduct controlled experiments isolating specific architectural features
- The debiasing strategies for scalar diversity are described but their implementation details remain unclear

## Confidence
- **High confidence**: LLMs encode rich lexical-semantic information about scalar adjectives
- **Medium confidence**: The size of LLMs does not correlate with performance on scalar adjective tasks
- **Medium confidence**: LLMs' poor scalar diversity performance reflects a pragmatic reasoning gap

## Next Checks
1. **Controlled architectural ablation study**: Systematically disable specific architectural features across model families to isolate which components most affect scalar adjective and diversity task performance.

2. **Pragmatic reasoning validation**: Design auxiliary tasks that specifically test whether models can make pragmatic inferences about speaker choice and alternatives, distinguishing lexical semantic knowledge from pragmatic reasoning capabilities.

3. **Training data analysis**: Conduct a quantitative analysis of scalar adjective and scalar diversity examples in the training corpora of different models to determine if observed performance gaps correlate with exposure to relevant linguistic patterns.