---
ver: rpa2
title: Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction
  of Business Processes
arxiv_id: '2403.09232'
source_url: https://arxiv.org/abs/2403.09232
tags:
- counterfactual
- process
- counterfactuals
- explanations
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating feasible and plausible
  counterfactual explanations for business process outcome prediction. The authors
  propose REVISED+, a method that incorporates process-specific constraints into a
  variational autoencoder framework.
---

# Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes

## Quick Facts
- arXiv ID: 2403.09232
- Source URL: https://arxiv.org/abs/2403.09232
- Authors: Alexander Stevens; Chun Ouyang; Johannes De Smedt; Catarina Moreira
- Reference count: 40
- Primary result: REVISED+ improves counterfactual plausibility (55.78% vs 44.41% baseline) and quantity (7.6 vs 6.28 per factual) for business process outcome prediction

## Executive Summary
This paper addresses the challenge of generating feasible and plausible counterfactual explanations for business process outcome prediction. The authors propose REVISED+, a method that incorporates process-specific constraints into a variational autoencoder framework. REVISED+ learns sequential patterns from process data using Declare language templates and ensures counterfactuals adhere to these patterns. The approach is evaluated on nine event logs, demonstrating improved plausibility rates and quantity of generated counterfactuals compared to a baseline method.

## Method Summary
REVISED+ extends the REVISE+ method by incorporating Declare language constraints into a variational autoencoder (VAE) framework for counterfactual generation. The approach first preprocesses event logs through temporal splitting, trace cutting, and adding End of Sequence tokens. It then trains an LSTM-VAE with a modified loss function that includes Trace Declare Constraints (TDC) and Label-specific Declare Constraints (LDC). Counterfactuals are generated through gradient descent in the latent space, with a hinge loss function allowing for diverse solutions. The method is evaluated on nine event logs, measuring plausibility, feasibility, proximity, sparsity, and diversity of generated counterfactuals.

## Key Results
- Improved plausibility rate: REVISED+ achieves 55.78% vs 44.41% for the baseline REVISE+ method
- Increased counterfactual quantity: REVISED+ generates 7.6 counterfactuals per factual vs 6.28 for REVISE+
- Successfully generates counterfactuals that adhere to both general process patterns (TDC) and outcome-specific patterns (LDC)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: REVISED+ generates more feasible and plausible counterfactuals by restricting them to high-density regions of the process data manifold.
- Mechanism: The approach uses a variational autoencoder (VAE) to learn the underlying data distribution of process traces. Counterfactuals are then generated through gradient descent in the latent space, ensuring they remain within regions of high probability density.
- Core assumption: The data manifold learned by the VAE accurately represents the feasible and plausible region of process traces.
- Evidence anchors:
  - [abstract]: "REVISED+ learns sequential patterns from process data using Declare language templates and ensures counterfactuals adhere to these patterns."
  - [section]: "The idea of the counterfactual generation algorithm is to perform gradient steps in the latent space of the learned data manifold and to generate counterfactuals from the latent sample of the initial data point."

### Mechanism 2
- Claim: Incorporating Declare language templates into the loss function improves plausibility by enforcing sequential process constraints.
- Mechanism: The approach adds two types of Declare constraints to the loss function: Trace Declare Constraints (TDC) for all traces and Label-specific Declare Constraints (LDC) for traces with the desired outcome. This ensures generated counterfactuals adhere to both general process patterns and outcome-specific patterns.
- Core assumption: The Declare constraints mined from the data accurately capture the sequential patterns that define feasible and plausible process executions.
- Evidence anchors:
  - [abstract]: "REVISED+ learns sequential patterns from process data using Declare language templates and ensures counterfactuals adhere to these patterns."
  - [section]: "The revised loss function of the VAE is now: LV AE = λN LL · LN LL + λKL · LKL (P ||Q) + λDT C · LDT C"

### Mechanism 3
- Claim: Using hinge loss instead of BCE loss allows for more diverse counterfactual solutions.
- Mechanism: The approach uses hinge loss to penalize high probabilities for the undesired label, but only requires the predicted probability to exceed a predefined threshold rather than achieving a specific target value.
- Core assumption: A relaxed target for the counterfactual probability allows the optimization process to explore a wider range of solutions.
- Evidence anchors:
  - [section]: "Third, we use hinge loss for the class loss to allow for more counterfactual solutions as we only require the predicted probability ˆyCF i = F (xCF 1 , . . . , xCF m ) to be greater than a predefined threshold (e.g. 0.5)."

## Foundational Learning

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: VAEs learn the underlying data distribution, allowing counterfactuals to be generated within feasible regions of the process space.
  - Quick check question: How does a VAE differ from a standard autoencoder, and why is this difference important for generating plausible counterfactuals?

- Concept: Declare Language Templates
  - Why needed here: Declare templates capture sequential patterns in process traces, which are used to define constraints for generating plausible counterfactuals.
  - Quick check question: What is the difference between Trace Declare Constraints and Label-specific Declare Constraints, and why are both needed?

- Concept: Counterfactual Properties (Proximity, Sparsity, Diversity, Plausibility, Feasibility)
  - Why needed here: These properties define what makes a good counterfactual explanation, and the approach is designed to optimize for them.
  - Quick check question: How does the approach balance the tradeoff between proximity (closeness to the original trace) and plausibility (adherence to process constraints)?

## Architecture Onboarding

- Component map:
  - Event Log Preprocessing -> VAE Training -> Declare Constraint Mining -> Counterfactual Generation -> Evaluation

- Critical path:
  1. Preprocess event logs
  2. Train LSTM-VAE with Declare constraints
  3. Mine Declare constraints from data
  4. Generate counterfactuals using REVISED+ algorithm
  5. Evaluate counterfactual properties

- Design tradeoffs:
  - Balancing VAE reconstruction loss and KL divergence for accurate data manifold learning
  - Setting appropriate weights for Declare constraints in loss function
  - Choosing threshold for counterfactual probability flip
  - Balancing proximity and plausibility in counterfactual generation

- Failure signatures:
  - VAE fails to learn data manifold: Poor reconstruction, high KL divergence, or Declare constraint violations in generated counterfactuals
  - Counterfactual generation fails: No feasible counterfactuals found, or all counterfactuals violate Declare constraints
  - Evaluation metrics show poor performance: Low plausibility rate, high proximity to original trace, or insufficient diversity among counterfactuals

- First 3 experiments:
  1. Train VAE on event log and visualize latent space to verify it captures data manifold
  2. Mine Declare constraints and manually verify they capture meaningful process patterns
  3. Generate counterfactuals for a small subset of traces and manually inspect for plausibility and feasibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively measure the feasibility of counterfactual explanations in business process analytics beyond the manifold learning approach?
- Basis in paper: [explicit] The paper mentions calculating remaining time as a "cost" and using entropic relevance measures or sequence probability as potential feasibility metrics, but states their approach is already restricted to the data manifold
- Why unresolved: The paper abstracts from these feasibility metrics because their counterfactual generation algorithm is already restricted to the data manifold, but acknowledges that alternative feasibility measures could provide additional insights
- What evidence would resolve it: Empirical studies comparing counterfactuals generated using different feasibility metrics (remaining time, entropic relevance, sequence probability) against those generated using only manifold restrictions, measuring their practical utility and actionability

### Open Question 2
- Question: What is the optimal balance between proximity and plausibility when generating counterfactual explanations for process outcome prediction?
- Basis in paper: [explicit] The results show that adding plausible constraints increases the distance between counterfactuals and factuals, indicating a trade-off between proximity and plausibility
- Why unresolved: The paper demonstrates that enhancing plausibility comes at the cost of proximity, but does not provide a systematic method for determining the optimal trade-off point
- What evidence would resolve it: User studies or decision-maker evaluations comparing counterfactuals generated with different weightings of proximity vs plausibility constraints, measuring their interpretability and actionability

### Open Question 3
- Question: How can we extend counterfactual explanation methods to handle multi-class process outcomes rather than just binary outcomes?
- Basis in paper: [inferred] The paper focuses exclusively on binary outcomes and does not address the challenges of extending to multi-class scenarios
- Why unresolved: The current framework and evaluation metrics are specifically designed for binary classification, and the hinge loss function and threshold-based stopping criteria are tailored for two-class problems
- What evidence would resolve it: Development and evaluation of modified counterfactual generation algorithms that can handle multi-class outcomes, with appropriate loss functions and stopping criteria, tested on event logs with multiple outcome classes

## Limitations

- Dependence on quality of Declare constraint mining: If the iBCM algorithm fails to capture relevant sequential patterns, generated counterfactuals may not reflect feasible process executions
- Assumption that all relevant process knowledge can be captured through Declare templates, potentially missing other important constraints
- Evaluation focuses on plausibility and feasibility but does not assess the actionability of generated counterfactuals for end-users

## Confidence

- High Confidence: The mechanism of using VAE to learn data manifolds and generate counterfactuals within high-density regions is well-established and empirically supported by the plausibility rate improvements.
- Medium Confidence: The integration of Declare constraints into the loss function effectively improves plausibility, but the exact contribution of each constraint type (TDC vs LDC) to the overall performance remains unclear.
- Medium Confidence: The use of hinge loss instead of BCE allows for more diverse solutions, though the optimal threshold setting requires further investigation.

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of Trace Declare Constraints and Label-specific Declare Constraints to counterfactual plausibility.
2. Test the approach on event logs with varying levels of noise and complexity to assess robustness across different process domains.
3. Implement a user study to evaluate whether the generated counterfactuals are actionable and interpretable for process analysts and managers.