---
ver: rpa2
title: 'Self-Supervised Alignment with Mutual Information: Learning to Follow Principles
  without Preference Labels'
arxiv_id: '2404.14313'
source_url: https://arxiv.org/abs/2404.14313
tags:
- principles
- should
- responses
- post
- constitution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SAMI, an iterative algorithm that self-aligns
  language models by increasing the mutual information between constitutions and model
  responses without preference labels or demonstrations. SAMI uses contrastive learning
  to optimize a lower bound on the conditional mutual information between generated
  responses and constitutions given user queries.
---

# Self-Supervised Alignment with Mutual Information: Learning to Follow Principles without Preference Labels

## Quick Facts
- arXiv ID: 2404.14313
- Source URL: https://arxiv.org/abs/2404.14313
- Reference count: 40
- Key outcome: SAMI self-aligns language models using mutual information between constitutions and responses, achieving 66-77% win rates against base models and 55-57% against instruction-finetuned models without preference labels

## Executive Summary
This paper introduces SAMI, an iterative self-alignment algorithm that trains language models to follow behavioral principles (constitutions) without requiring preference labels or demonstrations. SAMI optimizes a lower bound on conditional mutual information between constitutions and model responses using contrastive learning. The method demonstrates that pretrained models can learn to follow constitutions through self-supervision, achieving strong performance on dialogue and summarization tasks. SAMI generalizes to diverse principles and scales to larger models like llama3-70b while maintaining competitive win rates against established baselines.

## Method Summary
SAMI is an iterative algorithm that fine-tunes a pretrained language model to increase the mutual information between constitutions (behavioral principles) and self-generated responses. The method uses contrastive learning to optimize a lower bound on conditional mutual information I(Y;C|X) between responses Y and constitutions C given user queries X. The algorithm generates constitutions using a principle-writing model, samples responses from the current model, creates contrastive pairs, and updates model parameters using cross-entropy loss. Regularization through limited gradient updates prevents "gibberish" outputs while maintaining alignment effectiveness.

## Key Results
- SAMI-trained mistral-7b achieved 66-77% win rates against base models and 55-57% against instruction-finetuned baselines on single-turn dialogue and summarization
- The method generalized to diverse summarization principles with 68% win rates for learned and 67% for held-out principles
- SAMI scaled effectively to stronger models (llama3-70b) while maintaining competitive performance

## Why This Works (Mechanism)

### Mechanism 1
SAMI increases conditional mutual information between constitutions and responses, strengthening the statistical connection between behavioral principles and model behavior. By optimizing a lower bound on I(Y;C|X) through contrastive learning, SAMI encourages the model to generate responses more likely under relevant constitutions than contrastive ones. The approach assumes pretrained models already contain weak statistical connections that can be amplified through self-supervision.

### Mechanism 2
The two-sided contrastive objective (contrast over constitutions AND responses) provides more stable lower bounds than one-sided approaches. By combining p(y|c,x) and p(c|x,y) perspectives through symmetry of mutual information, the objective reduces variance and prevents trivial solutions. This dual approach creates a more robust optimization landscape for alignment.

### Mechanism 3
Regularization against the initial model distribution prevents "gibberish" outputs while enabling meaningful alignment. Limiting gradient updates and using few iterations keeps the model close to its pretrained distribution while optimizing the MI objective. This balance prevents catastrophic forgetting while allowing sufficient alignment with constitutions.

## Foundational Learning

- **Concept: Mutual Information and InfoNCE bounds** - SAMI directly optimizes a lower bound on conditional mutual information using InfoNCE-style contrastive learning. Quick check: What is the difference between I(X;Y) and I(X;Y|Z) in SAMI's context?

- **Concept: Contrastive learning with tractable conditionals** - SAMI requires computing log probabilities p(y|c,x) which are tractable for autoregressive models. Quick check: Why does SAMI use row-wise and column-wise normalization in contrastive pairs?

- **Concept: Regularization in alignment methods** - Prevents overfitting to the contrastive objective and maintains response coherence. Quick check: What happens if SAMI is trained for too many iterations without proper regularization?

## Architecture Onboarding

- **Component map**: Pretrained LM (πBASE) → Principle Writer (ω) → SAMI Iterative Finetuning → Aligned Model
- **Critical path**: Generate constitutions → Sample responses → Compute contrastive pairs → Update model parameters
- **Design tradeoffs**: Regularization strength vs. alignment effectiveness; number of constitutions vs. computational cost
- **Failure signatures**: Gibberish outputs (overfitting), no improvement in win rates (insufficient initial connection), length bias in responses
- **First 3 experiments**:
  1. Implement SAMI on HH-RLHF with mistral-7b and simple helpful/harmless principles
  2. Add length correction and compare to mistral-7b-instruct baseline
  3. Test with weak principle writer (mistral-7b-instruct) and strong base model (mixtral-8x7b) on TL;DR

## Open Questions the Paper Calls Out

The paper highlights several open questions: How does constitution diversity affect SAMI's performance and generalization across domains? Can SAMI effectively scale to more capable instruction-following models? What are the long-term effects of SAMI training on model behavior and performance? These questions remain unresolved and suggest directions for future research.

## Limitations

- SAMI depends on the initial statistical connection between pretrained models and behavioral principles, which may not exist universally
- Evaluation methodology using GPT-4 win rates may introduce biases and doesn't fully capture alignment quality
- The iterative nature requires multiple rounds of constitution generation, making computational cost potentially prohibitive for larger models

## Confidence

**High Confidence**: SAMI increases win rates against both base and instruction-finetuned models in tested scenarios; method demonstrates generalization across constitution types; regularization prevents catastrophic forgetting.

**Medium Confidence**: Two-sided contrastive objective provides more stable optimization than one-sided alternatives; conditional mutual information reflects alignment quality; SAMI scales effectively to larger models.

**Low Confidence**: SAMI will work equally well across all domains without modification; method can replace supervised alignment techniques entirely.

## Next Checks

1. **Zero-shot Constitution Transfer**: Test SAMI with constitutions from one domain applied to a model trained on a different domain to validate domain-general applicability.

2. **Quantitative MI Correlation**: Measure actual conditional mutual information throughout training and correlate with independent behavioral evaluations to test if optimized MI bounds track alignment quality.

3. **Minimal Pretraining Connection Test**: Systematically ablate the statistical connection between pretrained models and constitutions, then test whether SAMI can recover alignment to establish minimum required initial connection.