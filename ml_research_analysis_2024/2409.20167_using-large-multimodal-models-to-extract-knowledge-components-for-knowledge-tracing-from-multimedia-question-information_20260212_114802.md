---
ver: rpa2
title: Using Large Multimodal Models to Extract Knowledge Components for Knowledge
  Tracing from Multimedia Question Information
arxiv_id: '2409.20167'
source_url: https://arxiv.org/abs/2409.20167
tags:
- knowledge
- data
- performance
- components
- tracing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel approach using instruction-tuned
  large multimodal models (LMMs) to automatically extract knowledge components (KCs)
  from educational multimedia content. By leveraging GPT-4o to process text, images,
  and audio, the method generates detailed KCs without human labeling, clustering
  them via sentence embeddings.
---

# Using Large Multimodal Models to Extract Knowledge Components for Knowledge Tracing from Multimedia Question Information

## Quick Facts
- arXiv ID: 2409.20167
- Source URL: https://arxiv.org/abs/2409.20167
- Authors: Hyeongdon Moon; Richard Davis; Seyed Parsa Neshaei; Pierre Dillenbourg
- Reference count: 40
- Primary result: LMM-generated KCs achieved comparable or superior performance to human-defined KCs across multiple knowledge tracing models

## Executive Summary
This study introduces a novel approach using instruction-tuned large multimodal models (LMMs) to automatically extract knowledge components (KCs) from educational multimedia content. By leveraging GPT-4o to process text, images, and audio, the method generates detailed KCs without human labeling, clustering them via sentence embeddings. Evaluated across five domains (e.g., computing, psychology) and multiple knowledge tracing models, LMM-generated KCs achieved comparable or superior performance to human-defined KCs, with Additive Factor Model (AFM) RMSE scores of 0.363 (French) and 0.395 (Computing) versus human baselines. The approach addresses cold-start issues in intelligent tutoring systems and supports zero-shot knowledge tracing, advancing automated assessment while maintaining alignment with learning science frameworks. Publicly released benchmarks enable further research in content-aware knowledge tracing.

## Method Summary
The method uses GPT-4o API to extract knowledge components from educational multimedia content across five OLI domains. Raw content (HTML, SWF, MP3) is parsed to extract text and images, with audio converted to text using whisper-large-v2. GPT-4o generates KC descriptions with name and description fields, which are clustered using sentence embeddings via K-means with silhouette optimization. These KC clusters are mapped to problem IDs and fed into knowledge tracing models (IRT, PFA, DAS3H, SAKT, DKT) for evaluation using RMSE and AUC metrics. The approach enables zero-shot knowledge tracing by generalizing KC labels across unseen problem instances.

## Key Results
- LMM-generated KCs achieved RMSE scores of 0.363 (French) and 0.395 (Computing) with Additive Factor Model, outperforming or matching human-defined KCs
- Across five domains, LMM-extracted KCs maintained comparable performance to human-tagged labels in knowledge tracing tasks
- Zero-shot knowledge tracing capability demonstrated through KC generalization across problem instances
- Publicly released benchmark datasets enable further research in automated KC extraction and knowledge tracing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LMMs can generate semantically rich and valid knowledge components (KCs) without human labeling.
- Mechanism: The GPT-4o API is instructed to produce KC descriptions with name and description fields from raw multimedia content, then sentence embeddings cluster these into coherent groups, replacing traditional manual KC tagging.
- Core assumption: The instruction-tuned LMM can reliably interpret diverse content formats (text, images, audio) and distill them into meaningful KC descriptors.
- Evidence anchors:
  - [abstract] "automatically extract knowledge components from educational content using instruction-tuned large multimodal models"
  - [section] "Using the OpenAI API for the GPT-4o model, we extracted the knowledge components for each problem"
  - [corpus] Weak; only one neighbor study (Neshaei et al.) mentions LLMs for performance modeling, not KC extraction.
- Break condition: LMM fails to generalize across domains or produces inconsistent KC descriptions; clustering degrades if embeddings are noisy.

### Mechanism 2
- Claim: LMM-generated KCs improve knowledge tracing (KT) performance equivalently to or better than human-defined KCs.
- Mechanism: KCs extracted by LMM are mapped into KT models (PFA, DAS3H, SAKT, DKT) and compared to human-tagged KC baselines; performance measured via RMSE and AUC.
- Core assumption: KT models can leverage LMM-generated KCs as effectively as human-defined ones because the underlying skill-signal is preserved.
- Evidence anchors:
  - [abstract] "LMM-generated KCs achieved comparable or superior performance to human-defined KCs"
  - [section] "Our results indicate that the automatically extracted knowledge components can effectively replace human-tagged labels"
  - [corpus] Weak; related work focuses on fine-tuning LLMs for KT, not using them for KC extraction.
- Break condition: LMM KCs miss critical domain-specific nuances or introduce spurious correlations; KT models overfit to LMM noise.

### Mechanism 3
- Claim: Zero-shot KT becomes feasible by leveraging LMM-generated KCs to generalize to unseen items.
- Mechanism: A train-test split is created where test items share KC labels with train items but are never seen during training; logistic regression uses KC features to predict outcomes.
- Core assumption: KC labels generalize across problem instances; semantic similarity in KC clusters enables zero-shot transfer.
- Evidence anchors:
  - [abstract] "supports zero-shot knowledge tracing"
  - [section] "We implemented zero-shot knowledge tracing using the same codebase... The logistic regression setup was configured to experiment with various combinations of features"
  - [corpus] No explicit neighbor evidence; this is a novel contribution.
- Break condition: KC clusters are too coarse or too fine; zero-shot predictions degrade when KC semantics don't transfer.

## Foundational Learning

- Concept: Knowledge Components (KCs) as units of cognitive function in the Knowledge-Learning-Instruction (KLI) framework.
  - Why needed here: KCs are the bridge between content and student performance modeling; understanding their role clarifies why automatic extraction matters.
  - Quick check question: What distinguishes KCs from generic "skills" or "tags" in educational modeling?

- Concept: Knowledge Tracing (KT) and its evolution from Bayesian KT to deep learning models like DKT, SAKT, and PFA.
  - Why needed here: KT models are the evaluation target for LMM-generated KCs; knowing their inputs and assumptions is critical for interpreting results.
  - Quick check question: Which KT models can handle multiple KCs per item versus single KC assignments?

- Concept: Sentence embeddings and clustering for semantic grouping of KC descriptions.
  - Why needed here: LMM outputs natural language; embeddings enable automated grouping into reusable KC clusters.
  - Quick check question: How does silhouette score guide the choice of number of KC clusters?

## Architecture Onboarding

- Component map: Content parsing -> LMM KC extraction -> Clustering -> KT model training -> Evaluation (RMSE/AUC)
- Critical path: Content parsing → LMM KC extraction → Clustering → KT model training → Evaluation (RMSE/AUC)
- Design tradeoffs:
  - Prompt simplicity vs. output richness: fixed prompt avoids tuning overhead but may miss domain nuances
  - Cluster granularity: more clusters improve fit but risk sparsity; fewer clusters improve coverage but coarsen semantics
  - Embedding choice: OpenAI vs. Sentence-T5-XXL—better embeddings yield better KC quality but at higher cost
- Failure signatures:
  - LMM outputs generic or irrelevant KC names → check prompt wording and domain examples
  - Clustering produces singleton clusters → inspect embedding quality or reduce cluster count
  - KT performance drops vs. random baseline → verify KC mapping accuracy and feature engineering
- First 3 experiments:
  1. Run LMM on a small subset of problems; manually inspect KC names/descriptions for semantic validity.
  2. Vary cluster count (e.g., 10, 30, 50) and plot silhouette vs. AFM RMSE to find knee point.
  3. Compare KT performance (PFA) using LMM KCs vs. human KCs on one domain (e.g., statics) before scaling up.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-generated knowledge components compare to human-generated ones across different knowledge tracing model architectures?
- Basis in paper: [explicit] The paper states that LLM-generated KCs achieved comparable or superior performance to human-defined KCs across multiple KT models (IRT, PFA, DAS3H, SAKT, DKT) in five domains.
- Why unresolved: While the paper shows comparable performance, it does not provide a detailed analysis of which KT model architectures benefit most from LLM-generated KCs versus human-generated ones, or under what conditions one might outperform the other.
- What evidence would resolve it: Systematic ablation studies testing each KT model architecture with both LLM-generated and human-generated KCs across multiple datasets, with statistical significance testing to determine performance differences.

### Open Question 2
- Question: What is the optimal number of knowledge components to generate for different educational domains and how does this impact knowledge tracing performance?
- Basis in paper: [explicit] The paper notes that AFM performance improves with more clusters up to 200, but this is limited by computational resources and the need for each KC to appear in both train and test splits.
- Why unresolved: The paper does not determine the theoretical optimal number of KCs for different domains, and the practical constraints of zero-shot learning limit the exploration of higher cluster counts.
- What evidence would resolve it: Developing more efficient algorithms for determining optimal KC numbers that can scale beyond 200 clusters, combined with systematic testing across multiple domains to identify domain-specific optimal KC counts.

### Open Question 3
- Question: How can the preprocessing losses and data quality issues be addressed to improve the reliability of LLM-generated knowledge components?
- Basis in paper: [explicit] The paper identifies significant data losses during preprocessing, including extraction from Flash content, audio-to-text conversion errors, and exclusion of uncertain mappings, which reduced transaction data by over half.
- Why unresolved: The paper acknowledges these limitations but does not propose solutions or evaluate the impact of these losses on the quality of generated KCs or KT performance.
- What evidence would resolve it: Developing improved preprocessing pipelines that minimize data loss while maintaining accuracy, and conducting experiments to quantify the relationship between preprocessing quality and KC generation performance.

## Limitations
- Limited to OLI datasets from CMU Datashop, restricting domain coverage and generalizability
- Fixed prompt design may not capture domain-specific KC nuances across diverse educational content
- Evaluation focuses on RMSE and AUC metrics without deeper qualitative analysis of KC semantic validity

## Confidence

**Confidence levels:**
- **High confidence**: LMMs can generate KC descriptions from multimedia content, and clustering improves KT model performance versus random baselines
- **Medium confidence**: LMM-generated KCs perform equivalently to human-defined KCs across diverse domains and KT models
- **Medium confidence**: Zero-shot KT capability through KC generalization, though evaluation was limited to logistic regression

## Next Checks

1. Conduct qualitative expert review of LMM-generated KC descriptions across all five domains to assess semantic validity and domain coverage
2. Test prompt sensitivity by varying prompt wording and structure to identify optimal KC extraction strategies
3. Evaluate transfer learning performance by testing KC generalization across datasets from different institutions beyond CMU Datashop