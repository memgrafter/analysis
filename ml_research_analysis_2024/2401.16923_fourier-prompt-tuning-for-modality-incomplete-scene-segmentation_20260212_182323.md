---
ver: rpa2
title: Fourier Prompt Tuning for Modality-Incomplete Scene Segmentation
arxiv_id: '2401.16923'
source_url: https://arxiv.org/abs/2401.16923
tags:
- missing
- modalities
- prompt
- modality
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of modality-incomplete semantic
  segmentation in autonomous driving, where sensor failures or data absence can degrade
  model performance. To tackle this, the authors introduce a Missing-aware Modal Switch
  (MMS) strategy that randomly drops modalities during training, improving robustness
  without predefined missing ratios.
---

# Fourier Prompt Tuning for Modality-Incomplete Scene Segmentation

## Quick Facts
- arXiv ID: 2401.16923
- Source URL: https://arxiv.org/abs/2401.16923
- Authors: Ruiping Liu; Jiaming Zhang; Kunyu Peng; Yufan Chen; Ke Cao; Junwei Zheng; M. Saquib Sarfraz; Kailun Yang; Rainer Stiefelhagen
- Reference count: 40
- Key outcome: Introduces MMS and FPT to improve robustness in modality-incomplete semantic segmentation, achieving 5.84% mIoU improvement in missing scenarios

## Executive Summary
This paper addresses the challenge of modality-incomplete semantic segmentation in autonomous driving, where sensor failures or data absence can degrade model performance. The authors propose a Missing-aware Modal Switch (MMS) strategy that randomly drops modalities during training to improve robustness without predefined missing ratios. Additionally, they introduce Fourier Prompt Tuning (FPT), a parameter-efficient method that injects spectral information into prompts using Fast Fourier Transformation (FFT) to enhance model adaptation. The combined approach significantly outperforms state-of-the-art methods, demonstrating strong performance in both complete and incomplete modality conditions.

## Method Summary
The method combines Missing-aware Modal Switch (MMS) for data augmentation with Fourier Prompt Tuning (FPT) for parameter-efficient fine-tuning. MMS randomly drops modalities during training using a bit-level batch-wise sampling approach, ensuring at least one dense modality remains to prevent over-reliance on specific inputs. FPT enhances model robustness by injecting spectral information into learnable prompts through FFT and cross-attention with feature tokens. The approach is implemented on a multi-modal backbone (ViT + ConvNeXt decoder) and validated on DeLiVER and Cityscapes datasets for autonomous driving semantic segmentation.

## Key Results
- MMS+FPT achieves 5.84% mIoU improvement in modality-missing scenarios compared to baseline methods
- FPT outperforms AdaptFormer by 1.64%, 0.27%, and 2.84% mIoU in RGB-Depth, RGB missing, and Depth missing cases respectively
- The approach maintains strong performance with 3.8% mIoU improvement over baseline in complete modality scenarios
- Demonstrates effectiveness across both DeLiVER (RGB-Depth-LiDAR-Event) and Cityscapes (RGB-Depth) datasets

## Why This Works (Mechanism)

### Mechanism 1
The Missing-aware Modal Switch (MMS) mitigates predominant modality reliance by randomly dropping modalities during training using bit-level batch-wise sampling. During training, a binary switch is assigned independently to each modality, with automatic reset to ensure at least one dense modality remains. This introduces uniform missing ratios across modalities and avoids manual tuning that can lead to over-reliance on certain modalities.

### Mechanism 2
Fourier Prompt Tuning (FPT) enhances robustness by injecting spectral information into learnable prompts using Fast Fourier Transformation (FFT). FPT separates prompt tokens and feature tokens, applies FFT to extract spectral information, and rectifies this through cross-attention with feature tokens. This leverages global spectral information that is less affected by missing or noisy modalities compared to spatial information.

### Mechanism 3
The combination of MMS and FPT achieves superior performance through complementary effects. MMS improves robustness to missing modalities during training as a data augmentation strategy, while FPT provides parameter-efficient adaptation by incorporating spectral information into prompts. Their synergy leads to improved performance in both complete and incomplete modality conditions.

## Foundational Learning

- **Multi-modal semantic segmentation**: Why needed: Addresses scene perception using multiple modalities (RGB, Depth, LiDAR, Event) for autonomous driving. Quick check: What are the advantages of using multiple modalities for semantic segmentation compared to using a single modality?
- **Prompt tuning**: Why needed: Provides parameter-efficient adaptation of pre-trained models to downstream tasks without full fine-tuning. Quick check: How does prompt tuning differ from full fine-tuning in terms of tunable parameters and impact on pre-trained models?
- **Fast Fourier Transformation (FFT)**: Why needed: Extracts spectral information from prompt tokens to enhance robustness against modality incompleteness. Quick check: What are the two key properties of FFT that are utilized in FPT, and how do they contribute to effectiveness?

## Architecture Onboarding

- **Component map**: Multi-modal backbone (ViT + ConvNeXt) -> MMS for data augmentation -> FPT module for parameter-efficient fine-tuning -> Segmentation head
- **Critical path**: 1) Input multi-modal data processed by backbone 2) MMS randomly drops modalities during training 3) FPT injects spectral information into prompts 4) Segmentation head generates semantic output
- **Design tradeoffs**: MMS vs. manual missing ratio (avoids over-reliance), FPT vs. other prompt tuning (incorporates spectral info), parameter efficiency vs. potential performance trade-offs
- **Failure signatures**: Poor missing modality performance (MMS issues), no improvement over baselines (FPT integration problems), overfitting (aggressive MMS dropping)
- **First 3 experiments**: 1) Evaluate model with/without MMS on missing modality datasets 2) Compare FPT with other prompt tuning methods 3) Analyze different FFT configurations on FPT performance

## Open Questions the Paper Calls Out

### Open Question 1
How does FPT performance compare to full fine-tuning in terms of accuracy and computational cost across various multi-modal segmentation tasks? The paper demonstrates FPT efficacy within parameter-efficient tuning but lacks direct comparison to full fine-tuning regarding computational resources.

### Open Question 2
How does MMS generalize to datasets with more than four modalities or different sensor failures not covered in DeLiVER and Cityscapes? The paper's evaluation is limited to specific datasets and failure modes, raising questions about robustness in diverse conditions.

### Open Question 3
What is the impact of varying prompt token count in FPT on performance in modality-incomplete scenarios? The paper uses 200 tokens without exploring how changing this number affects results, leaving uncertainty about optimal configuration.

## Limitations
- Weak corpus evidence supporting the three proposed mechanisms, relying heavily on author assertions
- Implementation details for FFT integration and prompt token configurations not fully specified
- Evaluation limited to DeLiVER and Cityscapes datasets, potentially limiting generalizability to all autonomous driving scenarios

## Confidence

| Assessment | Label |
|------------|-------|
| General problem formulation and empirical performance improvements | High |
| MMS training strategy logic for preventing modality over-reliance | Medium |
| FPT mechanism validity without independent validation | Low |

## Next Checks
1. Cross-dataset validation: Test MMS+FPT approach on additional autonomous driving datasets (nuScenes, KITTI) to verify generalizability beyond DeLiVER and Cityscapes
2. Ablation study of spectral components: Systematically evaluate FPT performance with and without FFT spectral information, comparing against spatial-only prompt tuning variants
3. Robustness under varying missing patterns: Evaluate model performance across different missing modality patterns (simultaneous RGB+LiDAR vs. random single modality missing) to verify MMS effectiveness