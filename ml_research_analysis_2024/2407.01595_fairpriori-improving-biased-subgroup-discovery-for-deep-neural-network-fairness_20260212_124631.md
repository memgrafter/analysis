---
ver: rpa2
title: 'Fairpriori: Improving Biased Subgroup Discovery for Deep Neural Network Fairness'
arxiv_id: '2407.01595'
source_url: https://arxiv.org/abs/2407.01595
tags:
- fairness
- fairpriori
- subgroup
- bias
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fairpriori, a method for detecting intersectional
  bias in deep learning models. Fairpriori integrates fairness metric calculations
  into the frequent itemset generation algorithm to efficiently identify subgroups
  experiencing discrimination.
---

# Fairpriori: Improving Biased Subgroup Discovery for Deep Neural Network Fairness

## Quick Facts
- arXiv ID: 2407.01595
- Source URL: https://arxiv.org/abs/2407.01595
- Reference count: 39
- Primary result: Reduces computation time from minutes to seconds while offering more interpretable results for detecting intersectional bias in deep learning models

## Executive Summary
This paper introduces Fairpriori, a method for detecting intersectional bias in deep learning models by integrating fairness metric calculations into the frequent itemset generation algorithm. The approach efficiently identifies subgroups experiencing discrimination across multiple fairness metrics (demographic parity, predictive parity, predictive equality, and equalized opportunities). Compared to state-of-the-art methods, Fairpriori demonstrates superior efficiency and effectiveness in identifying biased subgroups while providing interpretable outputs that support actionable fairness improvements.

## Method Summary
Fairpriori is a novel method for detecting intersectional bias in deep learning models that integrates fairness metric calculations directly into the Apriori frequent itemset generation algorithm. Instead of first generating subgroups and then calculating fairness metrics separately, Fairpriori computes both simultaneously in a single pass through the data. The method decomposes each fairness metric into numerator and denominator components that are computed incrementally during itemset generation, allowing multiple metrics to be calculated in parallel without separate passes. Fairpriori outputs a DataFrame containing subgroups, their support, metric values, and differences from baseline, with default settings (minimum support 50%, maximum subgroup length 2) ensuring interpretability and computational tractability.

## Key Results
- Reduces computation time from minutes to seconds compared to state-of-the-art methods
- Provides more interpretable results through focus on larger, actionable subgroups
- Consistently outperforms Themis, FairFictPlay, and TestSGD in both efficiency and effectiveness across COMPAS and Diabetes Hospitals datasets

## Why This Works (Mechanism)

### Mechanism 1
Fairpriori achieves superior efficiency by integrating fairness metric calculation directly into the Apriori frequent itemset generation process, avoiding repeated sampling or external data access. Instead of first generating subgroups and then calculating fairness metrics for each separately, Fairpriori computes both in a single pass, with each candidate itemset immediately producing support, numerator, and denominator counts, allowing the metric value to be calculated on-the-fly. This assumes fairness metrics can be expressed as ratios (numerator/denominator) so partial sums can be accumulated incrementally during itemset generation.

### Mechanism 2
Fairpriori supports multiple fairness metrics by decomposing each into numerator and denominator components that are computed in parallel during itemset generation. Each metric's calculation is broken into numerator and denominator parts, with each instance contributing to the appropriate counts for each metric, allowing all metrics to be computed simultaneously without separate passes. This relies on the assumption that all target fairness metrics can be expressed as ratios of aggregated counts over subgroups.

### Mechanism 3
Fairpriori's default configuration (minimum support 50%, max subgroup length 2) ensures interpretability and computational tractability while still detecting meaningful intersectional bias. By setting high minimum support, Fairpriori focuses on larger subgroups that are more actionable and interpretable, while limiting subgroup length to 2 keeps results simple and avoids combinatorial explosion. This assumes intersectional bias often manifests in relatively large, well-defined subgroups, and stakeholders benefit more from interpretable results than exhaustive subgroup discovery.

## Foundational Learning

- **Concept: Intersectional bias** - Bias affecting subgroups defined by combinations of multiple sensitive attributes (e.g., race AND gender), not just individual attributes. Why needed: Fairpriori is designed to detect this specific type of bias. Quick check: If a model is fair for women overall but unfair for Black women specifically, which type of bias is this?

- **Concept: Fairness metrics decomposition** - Expressing fairness metrics as ratios of counts so they can be computed incrementally during itemset generation. Why needed: Fairpriori relies on this decomposition for its efficiency. Quick check: Given a subgroup with 10 true positives and 20 total positive predictions, what is the predictive parity?

- **Concept: Frequent itemset mining (Apriori algorithm)** - Efficiently generating candidate subgroups based on attribute-value combinations. Why needed: Fairpriori uses Apriori as the basis for bias detection. Quick check: If items {A}, {B}, and {C} are frequent, which 2-itemsets will the Apriori algorithm consider as candidates?

## Architecture Onboarding

- **Component map**: DataFrame -> Apriori candidate generation -> Incremental numerator/denominator calculation -> Metric value computation -> Output DataFrame
- **Critical path**: DataFrame → Apriori candidate generation → Incremental numerator/denominator calculation → Metric value computation → Output DataFrame
- **Design tradeoffs**:
  - Support threshold vs. sensitivity: Higher thresholds reduce computation but may miss small but significant biased subgroups
  - Subgroup length limit vs. expressiveness: Shorter limits improve interpretability but may miss complex intersectional effects
  - Single-pass calculation vs. flexibility: Integrated calculation is faster but limits support for metrics requiring more complex logic
- **Failure signatures**:
  - Empty result set: Support threshold too high or data too homogeneous
  - Unexpectedly large runtime: Support threshold too low or subgroup length limit too high, causing combinatorial explosion
  - Zero differences across all subgroups: No meaningful bias detected or inappropriate fairness metric selected
- **First 3 experiments**:
  1. Run Fairpriori on COMPAS dataset with default settings (support=50%, length=2) to verify basic functionality
  2. Vary support threshold from 10% to 90% to observe effect on result set size and runtime
  3. Switch fairness metric to predictive parity and compare results with demographic parity on the same dataset

## Open Questions the Paper Calls Out

### Open Question 1
How would Fairpriori perform on datasets with a larger number of attributes and more complex subgroup structures? The paper mentions that Fairpriori's efficiency advantage may not be as robust under more demanding scenarios with larger datasets and more stringent parameters. This remains unresolved because the current evaluation focuses on two specific datasets, and the paper acknowledges that more varied scenarios could provide further insight into Fairpriori's performance.

### Open Question 2
Can Fairpriori be extended to support multiclass classification scenarios beyond binary classification? The paper explicitly states that Fairpriori currently does not support multiclass classification, which is a limitation compared to some other methods. This remains unresolved because the paper does not provide any information on potential approaches or feasibility of extending Fairpriori to handle multiclass classification.

### Open Question 3
How can Fairpriori be integrated into the training process of machine learning models to directly improve fairness during model development? The paper mentions that Fairpriori does not align as effectively with incorporating subgroup fairness directly into the training of machine learning models, unlike some other methods. This remains unresolved because the paper does not discuss potential approaches or feasibility of integrating Fairpriori into the model training process to directly improve fairness.

## Limitations
- Efficiency gains rely on the assumption that fairness metrics can be decomposed into incremental numerator/denominator calculations, which may not hold for more complex metrics
- Focus on larger subgroups (50% minimum support) may miss subtle but significant biases affecting smaller populations
- Current implementation is limited to categorical attributes, restricting applicability to datasets with continuous features

## Confidence

- **High Confidence**: The efficiency claims are well-supported by the integration mechanism described, which avoids separate sampling and calculation steps
- **Medium Confidence**: The effectiveness claims across different fairness metrics are reasonable but require empirical validation on diverse datasets
- **Low Confidence**: The generalizability to datasets with continuous attributes or complex fairness metrics not expressible as simple ratios remains uncertain

## Next Checks

1. Test Fairpriori's performance on a dataset with continuous attributes by discretizing features and comparing results against methods designed for continuous data
2. Implement a more complex fairness metric (e.g., conditional demographic parity) to verify whether the decomposition approach scales to non-ratio-based calculations
3. Systematically vary the support threshold from 10% to 90% on both COMPAS and Diabetes Hospitals datasets to quantify the trade-off between computational efficiency and bias detection sensitivity