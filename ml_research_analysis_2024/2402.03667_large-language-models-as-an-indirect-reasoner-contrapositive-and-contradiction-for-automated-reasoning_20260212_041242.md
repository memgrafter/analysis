---
ver: rpa2
title: 'Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction
  for Automated Reasoning'
arxiv_id: '2402.03667'
source_url: https://arxiv.org/abs/2402.03667
tags:
- reasoning
- llms
- department
- conclusion
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DIR, a method that combines direct reasoning
  (DR) and indirect reasoning (IR) for large language models (LLMs). It leverages
  contrapositive and contradiction principles to prompt LLMs to perform IR, generating
  multiple reasoning paths.
---

# Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning

## Quick Facts
- arXiv ID: 2402.03667
- Source URL: https://arxiv.org/abs/2402.03667
- Reference count: 21
- Primary result: DIR improves reasoning accuracy by 33.4% on LogiQA compared to direct reasoning alone

## Executive Summary
This paper introduces DIR (Direct-Indirect Reasoning), a method that enhances large language model reasoning by combining direct reasoning with indirect reasoning through contrapositive and contradiction principles. The approach generates multiple reasoning paths to improve logical inference accuracy. DIR is designed to be simple and compatible with existing Chain-of-Thought methods, showing significant performance gains across multiple reasoning datasets.

## Method Summary
DIR combines direct reasoning (DR) and indirect reasoning (IR) to improve LLM logical inference. The method leverages contrapositive and contradiction principles to prompt models to generate alternative reasoning paths. By creating multiple reasoning trajectories, DIR enables models to explore different logical perspectives on the same problem. The approach is designed to be modular and can be integrated with existing CoT-based methods without requiring fundamental changes to the underlying reasoning framework.

## Key Results
- DIR improves reasoning accuracy by over 10% on multiple datasets compared to direct reasoning alone
- Achieves 33.4% improvement on LogiQA dataset when compared to direct reasoning baseline
- Demonstrates consistent performance gains across different LLMs and reasoning tasks

## Why This Works (Mechanism)
The method works by exploiting logical principles (contrapositive and contradiction) to generate multiple reasoning paths. When an LLM encounters a reasoning problem, DIR prompts it to not only follow the direct logical path but also to construct contrapositive arguments and identify contradictions. This multi-path approach allows the model to cross-validate its reasoning through different logical frameworks, reducing the likelihood of arriving at incorrect conclusions through flawed reasoning chains.

## Foundational Learning
- **Contrapositive reasoning**: Why needed - provides alternative logical path that is logically equivalent to original statement; Quick check - verify that P→Q has same truth value as ¬Q→¬P
- **Contradiction detection**: Why needed - helps identify inconsistencies in reasoning chains; Quick check - test if assuming ¬P leads to logical impossibility
- **Chain-of-Thought prompting**: Why needed - establishes baseline reasoning framework to extend; Quick check - verify standard CoT improves performance over direct answering
- **Multi-path validation**: Why needed - enables cross-verification of reasoning through different logical approaches; Quick check - compare agreement rates between DR and IR paths
- **Logical equivalence**: Why needed - ensures contrapositive reasoning maintains validity; Quick check - test if transformed statements preserve original truth conditions
- **Inference path generation**: Why needed - creates multiple reasoning trajectories for the same problem; Quick check - measure diversity of generated reasoning paths

## Architecture Onboarding

**Component map**: Problem Input -> Direct Reasoning Path -> Contrapositive Generation -> Contradiction Analysis -> Multiple Reasoning Paths -> Answer Synthesis

**Critical path**: The critical execution path involves generating the direct reasoning path first, then using that as foundation for constructing contrapositive and contradiction-based reasoning paths. The synthesis of these multiple paths determines the final answer.

**Design tradeoffs**: The method trades increased computational cost (generating multiple reasoning paths) for improved accuracy. Simpler problems may not benefit from IR generation, while complex logical reasoning tasks show the most significant improvements.

**Failure signatures**: DIR may fail when logical principles are misapplied by the model, when reasoning paths become too convoluted, or when the synthesis mechanism cannot effectively reconcile conflicting paths from DR and IR.

**First 3 experiments**:
1. Benchmark DIR against direct reasoning on simple logical syllogisms to establish baseline improvement
2. Test DIR integration with existing CoT methods like least-to-most prompting on multi-step problems
3. Evaluate computational overhead by measuring token usage and inference time across problem complexity levels

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily dependent on specific LLMs and prompting configurations
- Limited evaluation on reasoning domains beyond logical inference
- No quantitative analysis of computational overhead versus accuracy gains

## Confidence

**High confidence**: The core methodology of combining direct and indirect reasoning is clearly described and implemented. The reported accuracy improvements over direct reasoning baselines are substantial and statistically significant based on the experimental results presented.

**Medium confidence**: The claim that DIR "significantly improves reasoning accuracy" is supported by the experimental results, but the lack of detailed statistical analysis (confidence intervals, p-values) and the absence of comparisons with state-of-the-art CoT methods beyond direct reasoning limits the strength of this claim.

**Low confidence**: The assertion that DIR can be "easily integrated with existing CoT-based methods" is not empirically validated. No experiments demonstrate integration with other reasoning frameworks or show how DIR complements or enhances them.

## Next Checks
1. Test DIR across a wider range of LLM families (including open-source models) and sizes to assess the method's robustness and generalizability beyond the specific models used in the study.

2. Quantify the trade-off between accuracy gains and computational costs by measuring inference time and token usage for DIR compared to baseline methods across different problem complexities.

3. Design experiments that explicitly test how DIR can be integrated with established CoT methods like least-to-most prompting or self-consistency, demonstrating the claimed ease of integration and potential for combined performance improvements.