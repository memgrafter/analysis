---
ver: rpa2
title: Large Language Models As MOOCs Graders
arxiv_id: '2402.03776'
source_url: https://arxiv.org/abs/2402.03776
tags:
- grading
- answers
- each
- llms
- rubrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores using large language models (LLMs) to replace
  peer grading in massive open online courses (MOOCs). The authors evaluate two state-of-the-art
  LLMs, GPT-4 and GPT-3.5, across three MOOC subjects using three prompting strategies
  based on a modified zero-shot chain-of-thought technique.
---

# Large Language Models As MOOCs Graders

## Quick Facts
- arXiv ID: 2402.03776
- Source URL: https://arxiv.org/abs/2402.03776
- Reference count: 36
- GPT-4 with instructor-provided answers and rubrics outperforms peer grading in MOOC assignments

## Executive Summary
This study investigates using large language models (LLMs) to automate grading in massive open online courses (MOOCs), replacing traditional peer grading systems. The authors evaluate GPT-4 and GPT-3.5 across three MOOC subjects using modified zero-shot chain-of-thought prompting strategies. Their findings demonstrate that GPT-4, when provided with instructor answers and rubrics, produces grades more closely aligned with instructor evaluations than peer grading. The research highlights both the potential and limitations of LLM-based grading, particularly in subjects requiring imaginative thinking.

## Method Summary
The study evaluates two LLMs (GPT-4 and GPT-3.5) on grading writing assignments from three MOOC courses using three prompting strategies based on zero-shot chain-of-thought prompting. The authors use 10 writing assignments per course from Introductory Astronomy, Astrobiology, and History and Philosophy of Astronomy. They compare LLM-generated grades against instructor-assigned grades and peer grading using bootstrap resampling (10,000 samples) for statistical validation. The prompting strategies include: instructor-provided answers only, instructor-provided answers with rubrics, and instructor-provided answers with LLM-generated rubrics.

## Key Results
- GPT-4 with instructor-provided answers and rubrics produces grades more aligned with instructor-assigned grades than peer grading
- GPT-4 outperforms GPT-3.5 across all prompting strategies
- LLM-generated rubrics perform comparably to instructor-provided rubrics
- History and Philosophy of Astronomy presents greater challenges for LLM grading compared to scientific subjects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 with instructor-provided correct answers and rubrics achieves higher alignment with instructor-assigned grades than GPT-3.5 or peer grading.
- Mechanism: The prompt provides GPT-4 with both the correct answer and the grading rubric, allowing it to mimic the instructor's evaluation criteria and scoring logic. This in-context learning enables GPT-4 to apply the rubric consistently across assignments.
- Core assumption: GPT-4 can interpret and apply grading rubrics accurately when provided in the prompt context.
- Evidence anchors:
  - [abstract] "Our results show that Zero-shot-CoT, when integrated with instructor-provided answers and rubrics, produces grades that are more aligned with those assigned by instructors compared to peer grading."
  - [section] "Table 2 displays the results after the implementation of bootstrap resampling. Our examination unfolds several key insights: ... (1) Both Table 1 and Table 2 reveal that GPT-4 overall outperforms GPT-3.5 by generating grades that are more closely aligned with the grades provided by instructors..."

### Mechanism 2
- Claim: GPT-4-generated rubrics can perform as well as instructor-provided rubrics in automated grading.
- Mechanism: GPT-4, trained on extensive educational data, can create rubrics that better capture instructor preferences and grading nuances than human-created rubrics. This enables fully automated grading without requiring human rubric creation.
- Core assumption: LLMs have sufficient domain knowledge to create effective rubrics that align with human grading preferences.
- Evidence anchors:
  - [abstract] "Finally, our study reveals a promising direction for automating grading systems for MOOCs, especially in subjects with well-defined rubrics."
  - [section] "(4) As expected, the LLM-produced rubrics, on average, as displayed in Table 2, can perform on the same level as the instructor-provided rubrics."

### Mechanism 3
- Claim: Zero-shot-CoT prompting with explicit reasoning produces more reliable grades than zero-shot prompting alone.
- Mechanism: The chain-of-thought approach forces GPT-4 to verbalize its reasoning process, making its grading decisions more transparent and reducing hallucination risk. This explicit reasoning helps the model follow rubric criteria more carefully.
- Core assumption: Making reasoning explicit improves consistency and reduces errors in grading decisions.
- Evidence anchors:
  - [section] "We develop three prompts, which incorporate Zero-shot-CoT in combination with: (1) instructor-provided correct answers, (2) instructor-presented answers and rubrics, and (3) correct answers furnished by the instructor, along with the LLM-generated rubric."
  - [section] "After conducting a series of preliminary experiments, we found that the Zero-shot-CoT method yields scores more closely aligned with those given by instructors in contrast to vanilla zero-shot prompting."

## Foundational Learning

- Concept: Zero-shot chain-of-thought prompting
  - Why needed here: This technique enables the model to reason through grading decisions without requiring fine-tuning, making it adaptable to different MOOC subjects and grading rubrics.
  - Quick check question: How does zero-shot chain-of-thought prompting differ from standard prompting in terms of output format and model behavior?

- Concept: Bootstrap resampling
  - Why needed here: With limited budget for grading full MOOC datasets, bootstrap resampling allows statistical validation of LLM grading performance on representative samples, approximating results on the full population.
  - Quick check question: Why is bootstrap resampling particularly useful when working with small sample sizes in educational grading studies?

- Concept: Prompt engineering for educational contexts
  - Why needed here: Effective grading requires precise instruction on how to apply rubrics and evaluate student responses, which demands careful prompt design beyond standard text generation tasks.
  - Quick check question: What are the key differences between prompts designed for creative writing versus prompts designed for objective grading tasks?

## Architecture Onboarding

- Component map: Data preprocessing pipeline (student assignments, instructor answers, rubrics) -> Prompt generation module (Zero-shot-CoT with various configurations) -> LLM inference layer (GPT-4 and GPT-3.5 APIs) -> Bootstrap resampling module (statistical validation) -> Evaluation module (comparison with instructor and peer grades)

- Critical path:
  1. Load student assignments and corresponding instructor materials
  2. Generate appropriate prompts for each grading configuration
  3. Submit prompts to LLM API with temperature=0
  4. Collect and store grades
  5. Apply bootstrap resampling to generate confidence intervals
  6. Compare results against ground truth grades

- Design tradeoffs:
  - Cost vs. sample size: Using only 10 students per course balances budget constraints with statistical validity through resampling
  - Prompt complexity vs. token limits: More detailed prompts provide better guidance but consume more tokens and may require truncation
  - GPT-4 vs. GPT-3.5: GPT-4 provides better alignment but at higher cost and slower response times

- Failure signatures:
  - Consistently high variance in bootstrap results indicates model instability or insufficient sample size
  - Systematic bias away from instructor grades suggests prompt engineering issues or rubric misinterpretation
  - Out-of-memory errors on API calls indicate prompt size exceeding token limits

- First 3 experiments:
  1. Test Zero-shot-CoT with instructor answers only on a single question from Introductory Astronomy to establish baseline performance
  2. Add instructor rubrics to the same prompt and measure improvement in alignment with instructor grades
  3. Generate LLM-produced rubrics and compare performance against instructor rubrics on the same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform when grading open-ended essay questions that require critical thinking and personal reflection, compared to structured scientific questions?
- Basis in paper: [inferred] The paper notes that History and Philosophy of Astronomy, which requires imaginative thinking, was more challenging for both LLMs and peer grading compared to other courses.
- Why unresolved: The study focused on specific types of assignments in Astronomy, Astrobiology, and History/Philosophy of Astronomy, but did not explore diverse essay formats that might present different challenges.
- What evidence would resolve it: Comparative studies testing LLMs on various essay formats (narrative, analytical, argumentative) across different disciplines with varying cognitive demands.

### Open Question 2
- Question: Can LLM-generated rubrics maintain consistency and fairness when applied to large-scale, diverse student populations with varying cultural and educational backgrounds?
- Basis in paper: [explicit] The paper mentions that LLM-generated rubrics performed comparably to instructor-provided rubrics, but did not examine cultural or educational bias in the generated rubrics.
- Why unresolved: The study did not analyze potential biases in LLM-generated rubrics or test their performance across diverse demographic groups.
- What evidence would resolve it: Cross-cultural studies testing LLM-generated rubrics on student populations from different educational systems and cultural contexts.

### Open Question 3
- Question: What is the optimal combination of prompt engineering techniques and model parameters for different subject areas to maximize grading accuracy?
- Basis in paper: [explicit] The study tested three prompting strategies but did not explore other prompt engineering techniques or fine-tuning of model parameters for specific subjects.
- Why unresolved: The research focused on a limited set of prompt variations and did not investigate the full parameter space or alternative prompting strategies.
- What evidence would resolve it: Systematic experiments varying prompt structures, model parameters (temperature, context length), and subject-specific fine-tuning to identify optimal configurations for different disciplines.

## Limitations
- Limited sample size (10 students per course) restricts generalizability
- Reliance on de-identified proprietary datasets prevents full reproducibility
- Performance gap in subjects requiring imaginative thinking suggests LLM limitations with subjective assessments

## Confidence

- **High Confidence**: GPT-4 with instructor-provided answers and rubrics outperforms both GPT-3.5 and peer grading in score alignment with instructor-assigned grades.
- **Medium Confidence**: LLM-generated rubrics can perform comparably to instructor-provided rubrics, though this requires further validation across diverse subjects and rubric types.
- **Medium Confidence**: Zero-shot-CoT prompting improves grading consistency, but the study doesn't fully explore alternative prompting strategies or their comparative effectiveness.

## Next Checks
1. Test LLM grading performance on a larger, more diverse sample of MOOC assignments across different subjects and difficulty levels to assess generalizability.
2. Conduct blind studies comparing LLM grading with human expert grading across multiple rubric types to evaluate bias and consistency.
3. Implement a hybrid grading system that combines LLM and peer grading, measuring whether this approach improves overall accuracy and fairness compared to either method alone.