---
ver: rpa2
title: Serial Position Effects of Large Language Models
arxiv_id: '2406.15981'
source_url: https://arxiv.org/abs/2406.15981
tags:
- prompt
- prompts
- gpt3
- these
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates serial position effects (SPE) in Large
  Language Models (LLMs), demonstrating that both encoder-decoder and decoder-only
  architectures exhibit these cognitive biases across multiple tasks. Through experiments
  with diverse model families and tasks, the authors show that primacy and recency
  effects are widespread, with carefully designed prompts offering inconsistent mitigation.
---

# Serial Position Effects of Large Language Models

## Quick Facts
- arXiv ID: 2406.15981
- Source URL: https://arxiv.org/abs/2406.15981
- Reference count: 40
- Large Language Models exhibit serial position effects (primacy and recency) across diverse architectures and tasks

## Executive Summary
This paper investigates serial position effects (SPE) in Large Language Models, demonstrating that both encoder-decoder and decoder-only architectures exhibit these cognitive biases across multiple tasks. Through experiments with diverse model families and tasks, the authors show that primacy and recency effects are widespread, with carefully designed prompts offering inconsistent mitigation. The findings reveal that model architecture has a stronger influence on SPE than prompt design, highlighting the need for greater attention to these biases in LLM applications.

## Method Summary
The study conducts label shuffling experiments across multiple model families (GPT-3.5, GPT-4, Llama2, SOLAR, T5, FlanT5) and datasets (Banking77, GoEmotions, MASSIVE, TACRED, RE-TACRED, and summarization datasets). Researchers implement label shuffling experiments by randomizing labels within prompts for classification tasks and analyzing the distribution of predicted labels. They also conduct summarization tasks with shuffled articles and calculate normalized BERTScore for each article to determine model focus across positions. Various prompts, including Chain-of-Thought (CoT), are designed and tested to evaluate their effectiveness in mitigating SPE, with results analyzed using statistical measures like Jensen-Shannon divergence.

## Key Results
- Serial position effects manifest as primacy and recency biases across both encoder-decoder and decoder-only LLM architectures
- Prompt engineering provides inconsistent mitigation of SPE, with effectiveness varying across model families
- Model architecture has a stronger influence on SPE than prompt design
- CoT prompts can partially mitigate SPE by forcing explicit consideration of all options, but don't eliminate underlying positional bias

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Serial position effects in LLMs arise from how positional embeddings and attention weights interact, causing models to attend more strongly to earlier and later items in a sequence.
- **Mechanism:** During self-attention, the dot-product between query and key vectors is modulated by positional embeddings. If positional embeddings encode a strong "first" and "last" bias, the resulting attention scores will be higher for items at the beginning and end of the sequence, regardless of content. This bias persists in zero-shot settings because fine-tuning (which could override it) is absent.
- **Core assumption:** Positional embeddings in transformers encode a sinusoidal or learned bias toward sequence boundaries, and attention weights are directly shaped by these embeddings without sufficient content-based compensation.
- **Evidence anchors:**
  - [abstract] "These biases, attributed to factors such as diminished attention (Crano, 1977), rehearsal strategies (Tan and Ward, 2000), and memory system dynamics (Li, 2010), have been extensively studied in cognitive science."
  - [section] "We utilize tailored prompts designed to steer model focus and assess the impact of SPE under these conditions."
  - [corpus] Weak; corpus neighbors do not discuss positional embedding mechanisms in detail.
- **Break condition:** If positional embeddings are content-dependent or if attention mechanisms normalize positional biases (e.g., through learned rescaling), the mechanism fails.

### Mechanism 2
- **Claim:** Prompt-induced shifts in attention are unreliable because the model's inherent sequence processing biases override prompt instructions.
- **Mechanism:** Prompts attempt to direct attention by modifying the input sequence or adding explicit instructions. However, the underlying transformer architecture processes the input as a sequence, and the learned attention patterns favor certain positions. Even if the prompt asks to focus on the middle, the positional bias in attention weights can cause the model to revert to primacy or recency effects.
- **Core assumption:** The attention mechanism's learned weights encode strong positional preferences that are difficult to override via input-level prompt modifications.
- **Evidence anchors:**
  - [abstract] "We also discovered that while carefully designed prompts can somewhat mitigate these biases, their effectiveness is inconsistent."
  - [section] "Although approaches like few-shot learning and fine-tuning could potentially reduce SPE, they risk introducing bias in scenarios lacking ground truth due to the particular examples used."
  - [corpus] Weak; corpus neighbors focus on cognitive biases in prompts but not on the robustness of positional bias against prompt manipulation.
- **Break condition:** If the model is fine-tuned with positional diversity or if prompt engineering is combined with architectural changes (e.g., attention masking), the mechanism fails.

### Mechanism 3
- **Claim:** Chain-of-thought reasoning can partially mitigate SPE by forcing the model to explicitly consider all options, but it does not eliminate the underlying positional bias.
- **Mechanism:** CoT prompts the model to generate intermediate reasoning steps, effectively redistributing attention across the sequence. This can reduce the dominance of boundary positions by making the model process all items more equally. However, the final selection step may still be influenced by residual positional bias in the attention weights.
- **Core assumption:** Intermediate reasoning steps can override some of the positional attention bias, but the final decision-making step still relies on the same attention mechanism.
- **Evidence anchors:**
  - [abstract] "Experiments with and without prompts reveal the pervasive influence of SPE and its challenging nature."
  - [section] "We also examine whether the CoT approach can guide models to thoroughly analyze all options before making decisions in multiple-choice settings."
  - [corpus] Weak; corpus neighbors do not discuss CoT in the context of positional bias mitigation.
- **Break condition:** If the reasoning steps themselves are biased by position, or if the model skips reasoning for boundary items, the mechanism fails.

## Foundational Learning

- **Concept:** Positional embeddings in transformers
  - Why needed here: Understanding how positional embeddings encode sequence position is critical to grasping why LLMs exhibit serial position effects.
  - Quick check question: What is the difference between sinusoidal positional embeddings and learned positional embeddings, and how might each affect serial position effects?
- **Concept:** Attention mechanism and attention weights
  - Why needed here: The attention mechanism determines how the model processes each item in the sequence, and its interaction with positional embeddings underlies the serial position effects.
  - Quick check question: How do query-key dot-products in self-attention get modulated by positional embeddings, and what happens if these embeddings are biased?
- **Concept:** Zero-shot vs. fine-tuned learning
  - Why needed here: Serial position effects are more pronounced in zero-shot settings because fine-tuning could mitigate them, so understanding the difference is key.
  - Quick check question: Why might fine-tuning reduce serial position effects, and what are the risks of using fine-tuning in scenarios without ground truth labels?

## Architecture Onboarding

- **Component map:** Input embedding layer (includes positional embeddings) -> Self-attention layers (multi-head attention with positional modulation) -> Feed-forward layers -> Output layer (for classification or generation)
- **Critical path:** Input sequence → Positional embeddings → Self-attention (query-key dot-products) → Attention weights → Output (classification or generation)
- **Design tradeoffs:**
  - Sinusoidal vs. learned positional embeddings: Sinusoidal are fixed and may encode predictable biases; learned embeddings can adapt but may overfit to training data.
  - Attention normalization: Techniques like relative positional encoding or attention sinks could mitigate positional bias but may reduce model performance on other tasks.
- **Failure signatures:**
  - Consistent primacy or recency effects across diverse tasks and prompts
  - Inability to mitigate effects with prompt engineering alone
  - Residual effects even after CoT reasoning
- **First 3 experiments:**
  1. **Positional embedding ablation:** Remove or randomize positional embeddings and measure changes in serial position effects.
  2. **Attention weight analysis:** Visualize attention weights for different positions and correlate with output selection.
  3. **Prompt engineering robustness:** Test a wide range of prompts (e.g., explicit instructions, CoT, adversarial prompts) and measure consistency of effect mitigation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying mechanism causing encoder-decoder models to exhibit serial position effects despite their different attention architecture compared to decoder-only models?
- Basis in paper: [explicit] The paper found that encoder-decoder models (T5 and FlanT5) showed serial position effects similar to decoder-only models, but did not investigate the architectural differences that might explain this similarity.
- Why unresolved: The study did not conduct ablation studies or architectural analysis to isolate which components of the encoder-decoder architecture contribute to these biases.
- What evidence would resolve it: Systematic comparison of different attention mechanisms (cross-attention vs self-attention), ablation studies removing specific architectural components, or analysis of attention patterns across different model architectures.

### Open Question 2
- Question: How do serial position effects in LLMs change over time with continued pre-training or fine-tuning, and can these effects be intentionally minimized through training objectives?
- Basis in paper: [inferred] The paper mentions that previous research suggests RLHF may modulate SPE, and that pre-training may influence the emergence of specific SPE patterns, but does not explore how different training methods affect these biases.
- Why unresolved: The study focused on inference-time interventions rather than examining how different training approaches could systematically reduce SPE.
- What evidence would resolve it: Longitudinal studies tracking SPE across different stages of training, controlled experiments comparing SPE across models with different training objectives and data distributions, and investigation of whether specific training techniques can be designed to minimize these biases.

### Open Question 3
- Question: Are serial position effects in LLMs language-dependent, and how do they manifest across different linguistic structures and cultural contexts?
- Basis in paper: [explicit] The paper explicitly acknowledges that all experiments were conducted in English and states that conclusions might be limited to English contexts.
- Why unresolved: The study did not include multilingual experiments or examine how different language structures might influence the manifestation of SPE.
- What evidence would resolve it: Comparative studies across multiple languages with different syntactic structures, analysis of how cultural factors in training data affect SPE patterns, and examination of whether certain languages exhibit stronger or weaker SPE effects.

## Limitations

- The effectiveness of prompt engineering as a mitigation strategy remains uncertain due to lack of detailed prompt formulations and variations
- The study focuses primarily on zero-shot settings, leaving open questions about whether fine-tuning or few-shot learning could more effectively address SPE
- The claim that model architecture has a stronger influence on SPE than prompt design lacks detailed architectural analysis and systematic investigation of specific architectural components

## Confidence

- **High confidence:** The presence of SPE across multiple model families and tasks is well-supported by the experimental results.
- **Medium confidence:** The claim that architecture matters more than prompt design is supported but could benefit from more detailed architectural analysis.
- **Medium confidence:** The observation that prompt engineering provides inconsistent mitigation is well-documented, though specific prompt strategies need more thorough exploration.

## Next Checks

1. **Architectural ablation study:** Systematically test how modifications to positional embeddings and attention mechanisms affect SPE to identify which architectural components are most influential.

2. **Fine-tuning comparison:** Conduct controlled experiments comparing zero-shot, few-shot, and fine-tuned models to better understand when and how SPE can be mitigated through additional training.

3. **Prompt engineering robustness:** Develop and test a comprehensive set of prompt strategies (including varying complexity, explicit instructions, and adversarial prompts) to establish more definitive conclusions about prompt effectiveness.